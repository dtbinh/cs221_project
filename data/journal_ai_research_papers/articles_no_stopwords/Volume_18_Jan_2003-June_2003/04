Journal Articial Intelligence Research 18 (2003) 149-181

Submitted 10/02; published 02/03

Wrapper Maintenance: Machine Learning Approach
Kristina Lerman

lerman@isi.edu

USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Steven N. Minton

minton@fetch.com

Fetch Technologies
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Craig A. Knoblock

knoblock@isi.edu

USC Information Sciences Institute Fetch Technologies
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Abstract
proliferation online information sources led increased use wrappers
extracting data Web sources. previous research focused
quick ecient generation wrappers, development tools wrapper maintenance received less attention. important research problem Web
sources often change ways prevent wrappers extracting data correctly.
present ecient algorithm learns structural information data positive
examples alone. describe information used two wrapper maintenance applications: wrapper verication reinduction. wrapper verication system
detects wrapper extracting correct data, usually Web source
changed format. reinduction algorithm automatically recovers changes
Web source identifying data Web pages new wrapper may generated
source. validate approach, monitored 27 wrappers period year.
verication algorithm correctly discovered 35 37 wrapper changes, made
16 mistakes, resulting precision 0.73 recall 0.95. validated reinduction algorithm ten Web sources. able successfully reinduce wrappers,
obtaining precision recall values 0.90 0.80 data extraction task.

1. Introduction
tremendous amount information available online, much information
formatted easily read human users, computer applications. Extracting
information semi-structured Web pages increasingly important capability
Web-based software applications perform information management functions,
shopping agents (Doorenbos, Etzioni, & Weld, 1997) virtual travel assistants (Knoblock,
Minton, Ambite, Muslea, Oh, & Frank, 2001b; Ambite, Barish, Knoblock, Muslea, Oh, &
Minton, 2002), among others. applications, often referred agents, rely
Web wrappers extract information semi-structured sources convert
structured format. Semi-structured sources explicitly specied
grammar schema, implicit grammar used identify relevant
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiLerman, Minton & Knoblock

information page. Even text sources email messages structure
heading exploited extract date, sender, addressee, title, body
messages. sources, online catalogs, regular structure
exploited extract data automatically.
Wrappers rely extraction rules identify data eld extracted. Semiautomatic creation extraction rules, wrapper induction, active area
research recent years (Knoblock, Lerman, Minton, & Muslea, 2001a; Kushmerick, Weld,
& Doorenbos, 1997). advanced wrapper generation systems use machine
learning techniques learn extraction rules example. instance, wrapper
induction tool developed USC (Knoblock et al., 2001a; Muslea, Minton, & Knoblock,
1998) commercialized Fetch Technologies, allows user mark data
extracted several example pages online source using graphical user interface.
system generates landmark-based extraction rules data rely
page layout. USC wrapper tool able eciently create extraction rules
small number examples; moreover, extract data pages contain lists,
nested structures, complicated formatting layouts.
comparison wrapper induction, wrapper maintenance received less attention.
important problem, even slight changes Web page layout break
wrapper uses landmark-based rules prevent extracting data correctly.
paper discuss approach wrapper maintenance problem, consists
two parts: wrapper verication reinduction. wrapper verication system monitors
validity data returned wrapper. site changes, wrapper may extract
nothing data correct. verication system detect data
inconsistency notify operator automatically launch wrapper repair process.
wrapper reinduction system repairs extraction rules wrapper works
changed pages.

Pages
labeled

Web
pages

Reinduction System

GUI

Labeled
Web pages

Wrapper
Induction
System

Wrapper
Extracted
data
Change
detected

Automatic
Re-labeling

Wrapper
Verification

Figure 1: Life cycle wrapper
Figure 1 graphically illustrates entire life cycle wrapper. shown gure,
wrapper induction system takes set web pages labeled examples data
extracted. output wrapper induction system wrapper, consisting set
150

fiWrapper Maintenance

extraction rules describe locate desired information Web page.
wrapper verication system uses functioning wrapper collect extracted data.
learns patterns describing structure data. patterns used verify
wrapper correctly extracting data later date. change detected, system
automatically repair wrapper using structural information locate examples
data new pages re-running wrapper induction system examples.
core wrapper maintenance applications machine learning algorithm
learns structural information common data elds. paper introduce
algorithm, DataProG, describe application wrapper maintenance tasks
detail. Though focus web applications, learning technique web-specic,
used data validation general.
Note distinguish two types extraction rules: landmark-based rules extract data exploiting structure Web page, content-based rules,
refer content patterns simply patterns, exploit structure eld itself.
previous work focused learning landmark rules information extraction (Muslea,
Minton, & Knoblock, 2001). current work shows augmenting rules
content-based patterns provides foundation sophisticated wrapper maintenance applications.

2. Learning Content Patterns
goal research extract information semi-structured information sources.
typically involves identifying small chunks highly informative data formatted
pages (as opposed parsing natural language text). Either convention design,
elds usually structured: phone numbers, prices, dates, street addresses, names, schedules, etc. Several examples street addresses given Fig. 2. Clearly, strings
arbitrary, share similarities. objective work learn
structure elds.
4676 Admiralty Way
10924 Pico Boulevard
512 Oak Street
2431 Main Street
5257 Adams Boulevard

Figure 2: Examples street address eld

2.1 Data Representation
previous work, researchers described elds extracted Web pages characterlevel grammar (Goan, Benson, & Etzioni, 1996) collection global features,
number words density numeric characters (Kushmerick, 1999). employ
intermediate word-level representation balances descriptive power specicity
character-level representation compactness computational eciency
global representation. Words, accurately tokens, strings generated
151

fiLerman, Minton & Knoblock

alphabet containing dierent types characters: alphabetic, numeric, punctuation,
etc. use tokens character types assign one syntactic categories:
alphabetic, numeric, etc. categories form hierarchy depicted Fig. 3,
arrows point general less general categories. unique specic token type
created every string appears least k examples, determined preprocessing
step. hierarchical representation allows multi-level generalization. Thus, token
Boulevard belongs general token types Alphanum (alphanumeric strings), Alpha
(alphabetic strings), Upper (capitalized words), well specic type representing
string Boulevard. representation exible may expanded include
domain specic information. example, numeric type divided categories
include range information number Large (larger 1000), Medium
(medium numbers, 10 1000) Small (smaller 10) number
digits: 1, 2, 3digit. Likewise, may explicitly include knowledge type
information parsed, e.g., 5-digit numbers could represented zipcode.

TOKEN

PUNCT

ALPHANUM

ALPHA

UPPER

NUMBER

LOWER

SMALL MEDIUM LARGE

ALLCAPS

CA

HTML

1Digit

Boulevard

2Digit

3Digit

310

Figure 3: Portion token type syntactic hierarchy
found sequence specic general token types useful
describing content information character-level nite state representations
used previous work (Carrasco & Oncina, 1994; Goan et al., 1996). character-level
description far ne grained compactly describe data and, therefore, leads poor
generality. coarse-grained token-level representation appropriate Web
data types. addition, data representation schemes used previous work attempt
describe entire data eld, use starting ending sequences,
patterns, tokens capture structure data elds. reason
similar one above: using starting ending patterns allows us generalize
structural information many complex elds lot variability.
elds, e.g., addresses, usually regularity start end
exploit. call starting ending patterns collectively data prototype.
example, consider set street addresses Fig. 2. examples start
152

fiWrapper Maintenance

pattern <Number Upper> end specic type <Boulevard> generally
<Upper>. Note pattern language allow loops recursion. believe
recursive expressions useful representations types data trying
learn, harder learn lead over-generalization.
2.2 Learning Positive Examples
problem learning data prototype set examples labeled
belonging (or not) class may stated one two related ways: classication
conservation task. classication task, positive negative instances
class used learn rule correctly classify new examples. Classication
algorithms, FOIL (Quinlan, 1990), use negative examples guide specialization
rule. construct discriminating descriptions satised
positive examples negative examples. conservation task, hand,
attempts nd characteristic description (Dietterich & Michalski, 1981) conserved
patterns (Brazma, Jonassen, Eidhammer, & Gilbert, 1995), set positive examples
class. Unlike discriminating description, characteristic description often include
redundant features. example, learning description street addresses, city
names serving negative examples, classication algorithm learn <Number>
good description, street addresses start none city names
do. capitalized word follows number addresses redundant feature,
add discriminating power learned description. However,
application using description encounters zipcode future, incorrectly
classify street address. problem could avoided <Number Upper>
learned description street addresses. Therefore, negative examples
available learning algorithm, description capture regularity
data, including redundant features, order correctly identify new instances
class dierentiate classes. Ideally, characteristic description
learned positive examples alone discriminating description learned
classication algorithm positive negative examples, negative examples
drawn innitely many classes. widely used machine learning
algorithms (e.g., decision trees (Quinlan, 1993), inductive logic programming (Muggleton,
1991)) solve classication task, fewer algorithms learn characteristic
descriptions.
applications, appropriate source negative examples problematic; therefore,
chose frame learning problem conservation task. introduce algorithm
learns data prototypes positive examples data eld alone. algorithm
nds statistically signicant sequences tokens. sequence token types signicant
occurs frequently would expected tokens generated randomly
independently one another. words, sequence constitutes pattern
describes many positive examples data highly unlikely
generated chance.
algorithm estimates baseline probability token types occurrence
proportion types examples data eld type. Suppose
learning description set street addresses Fig. 2, already found
153

fiLerman, Minton & Knoblock

signicant token sequence e.g., pattern consisting single token <Number>
want determine whether specic pattern, <Number Upper>,
signicant pattern. Knowing probability occurrence type Upper,
compute many times Upper expected follow Number completely chance.
observe considerably greater number sequences, conclude longer
pattern signicant.
use hypothesis testing (Papoulis, 1990) decide whether pattern signicant.
null hypothesis observed instances pattern generated chance,
via random, independent generation individual token types. Hypothesis testing
decides, given condence level, whether data supports rejecting null hypothesis.
Suppose n identical sequences generated random source. probability
token type (whose overall probability occurrence p) next type
k sequences binomial distribution. large n, binomial distribution
approaches normal distribution P (x, , ) = np 2 = np(1p). cumulative
probability probability observing least n1 events:
P (k n1 ) =


n1

P (x, , )dx

(1)

use polynomial approximation formulas (Abramowitz & Stegun, 1964) compute
value integral.
signicance level test, , probability null hypothesis rejected
even though true, given cumulative probability above. Suppose set
= 0.05. means expect observe least n1 events 5% time
null hypothesis. number observed events greater, reject null hypothesis
(at given signicance level), i.e., decide observation signicant. Note
hypothesis test derived observation (data). constraint reduces
number degrees freedom test; therefore, must subtract one number
observed events. prevents anomalous case single occurrence
rare event judged signicant.
2.3 DataProG Algorithm
describe DataProG, algorithm nds statistically signicant patterns
set token sequences. preprocessing step text tokenized, tokens
assigned one syntactic types (see Figure 3). patterns encoded
type prex tree, node corresponds token type. DataProG relies
signicance judgements grow tree prune nodes. Every path
resulting tree starting root node corresponds signicant pattern found
algorithm. section, focus discussion version algorithm
learns starting patterns. algorithm easily adapted learn ending patterns.
present pseudocode DataProG algorithm Table 1. DataProG grows
pattern tree incrementally (1) nding signicant specializations (i.e., longer patterns) pattern (2) pruning less signicant generalizations (or specializations) among patterns length. last step, DataProG extracts
signicant patterns pattern tree, including generalizations (i.e., shorter patterns) found signicant given specic (i.e., longer) patterns.
154

fiWrapper Maintenance

DATAPROG MAIN LOOP
Create root node tree;
next node Q tree
Create children Q;
Prune nodes;
Extract patterns tree;

CREATE CHILDREN Q
token type next position examples
Let C = NewNode;
Let C.token = T;
Let C.examples = Q.examples followed T;
Let C.count = |C.examples|;
Let C.pattern = concat(Q.pattern );
Signicant(C.count, Q.count, T.probability)
AddChildToTree(C, Q);
End
End loop

PRUNE NODES
child C Q
sibling C s.t. S.pattern C.pattern
Let N = C.count S.count
Not(Signicant(N, Q.count, C.token.probability))
Delete C;
break;
Else
Delete S;
End
End loop
End C loop

EXTRACT PATTERNS TREE
Create empty list;
every node Q tree
every child C Q
Let N = C.count (Si .count|Si Children(C))
Signicant( N, Q.count, C.token.probability)
Add C.pattern list;
Return (list patterns);

Table 1: Pseudocode DataProG algorithm

155

fiLerman, Minton & Knoblock

tree empty initially, children added root node. children
represent tokens occur rst position training examples often
expected chance. example, learning addresses examples
Fig. 2, root two child nodes: Alphanum Number. tree extended
incrementally node Q. new child added Q every signicant specialization
pattern ending Q. explained previously, child node judged signicant
respect parent node number occurrences pattern ending
child node suciently large, given number occurrences pattern ending
parent node baseline probability token type used extend pattern.
illustrate addresses example, suppose already found pattern <Number
Upper> signicant. ways extend tree (see Fig. 4) given data:
<Number Upper Alphanum>, <Number Upper Alpha>, <Number Upper Upper>,
<Number Upper Street>, <Number Upper Boulevard>, <Number Upper Way>.
last patterns judged signicant = 0.05. example,
<Number Upper Upper> signicant, Upper follows pattern <Number
Upper> times,1 probability observing least many longer
sequences purely chance 0.0002.2 Since probability less , judge
sequence signicant.

ROOT
NUMBER
UPPER

ALPHANUM

ALPHA

UPPER

Boulevard

Street

Figure 4: Pattern tree describes structure addresses. Dashed lines link nodes
deleted pruning step.

next step prune tree. algorithm examines pair sibling nodes,
one general other, eliminates less signicant pair.
precisely, algorithm iterates newly created children Q,
least general, every pair children Ci Cj , Ci .pattern
Cj .pattern (i.e., Cj .pattern strictly general Ci .pattern), algorithm keeps
Cj explains signicantly data; otherwise, keeps Ci . 3
1. small numbers used illustrative purposes typical data sets
patterns learned much larger.
2. calculation cumulative probability depends occurrence probability Upper. count
occurrence token type independently others. example, occurrence probability
(relative fraction) type Upper 0.18.
3. DataProG based earlier version algorithm, DataPro, described conference paper (Lerman & Minton, 2000). Note original version algorithm, specic patterns
always kept, regardless whether general patterns found signicant not.

156

fiWrapper Maintenance

Let us illustrate pruning step example pattern tree Fig. 4. eliminate node AlphaNum, examples match pattern <Number Upper Alphanum> match pattern <Number Upper Alpha> thus, Alphanum
signicant given specialization Alpha. eliminate node Alpha similar
reason. Next, check whether <Number Upper Upper> signicant given patterns
<Number Upper Boulevard> <Number Upper Street>. 2 instances
address eld match pattern <Number Upper Boulevard>, 2 addresses
match <Number Upper Street>. <Number Upper Upper> matches signicantly
4 addresses, retained specic patterns pruned
tree; otherwise, deleted specic ones kept. every example
described one pattern given length, pruning step ensures size
tree remains polynomial number tokens, thereby, guaranteeing reasonable
performance algorithm.
entire tree expanded, nal step extract signicant patterns
tree. Here, algorithm judges whether shorter (more general) pattern, e.g.,
<Number Upper>, signicant given longer specializations it, e.g., <Number
Upper Boulevard> <Number Upper Street>. amounts testing whether
excess number examples explained shorter pattern, longer
patterns, signicant. pattern ends terminal node tree signicant.
Note set signicant patterns may cover examples data set,
fraction occur frequently expected chance (at signicance
level). Tables 24 show examples several data elds yellow pages source (Bigbook)
stock quote source (Y ahoo Quote), well starting patterns learned
eld.

3. Applications Pattern Learning
explained introduction, wrapper induction systems use information
layout Web pages create data extraction rules therefore vulnerable changes
layout, occur frequently site redesigned. cases wrapper
continues extract, data longer correct. output wrapper may
change format source data changed: e.g., $ dropped
price eld (9.95 instead $9.95), book availability changes Ships
immediately Stock: ships immediately. applications, Web
agents (Ambite et al., 2002; Chalupsky et al., 2001), rely data extracted wrappers,
wrapper maintenance important research problem. divide wrapper maintenance problem two parts, described separately paper. Wrapper verication
automatically detects wrapper extracting data correctly Web source,
wrapper reinduction automatically xes broken wrappers. applications learn
description data, patterns learned DataProG signicant part.

introduced strong bias specic patterns results, led high proportion
false positives wrapper verication experiments. Eliminating specicity bias, improved
performance algorithm verication task.

157

fiLerman, Minton & Knoblock

BUSINESS NAME
Chado Tea House
Saladang
Information Sciences Institute
Chaya Venice
Acorda Therapeutics
Cajun Kitchen
Advanced Medical Billing Services
Vega 1 Electrical Corporation
21st Century Foundation
TIS Season Gift Shop
Hide Sushi Japanese Restaurant
Aoat Sushi
Prebica Coee & Cafe
L Orangerie
Emils Hardware
Natalee Thai Restaurant
Casablanca
Antica Pizzeria
NOBU Photographic Studio
Lotus Eaters
Essex Coney
National Restaurant
Siam Corner Cafe
Grand Casino French Bakery
Alejo Presto Trattoria
Titos Tacos Mexican Restaurant Inc
Killer Shrimp
Manhattan Wonton CO
Starting patterns
<Alpha Upper>
<Alpha Upper Upper Restaurant>
<Alpha >

ADDRESS
8422 West 1st Street
363 South Fair Oaks Avenue
4676 Admiralty Way
110 Navy Street
330 West 58th Street
420 South Fairview Avenue
9478 River Road
1723 East 8th Street
100 East 85th Street
15 Lincoln Road
2040 Sawtelle Boulevard
87 East Colorado Boulevard
4325 Glencoe Avenue
903 North La Cienega Boulevard
2525 South Robertson Boulevard
998 South Robertson Boulevard
220 Lincoln Boulevard
13455 Maxella Avenue
236 West 27th Street
182 5th Avenue
1359 Coney Island Avenue
273 Brighton Beach Avenue
10438 National Boulevard
3826 Main Street
4002 Lincoln Boulevard
11222 Washington Place
523 Washington Boulevard
8475 Melrose Place
<Number Upper Upper>
<Number Upper Upper Avenue>
<Number Upper Upper Boulevard>

Table 2: Examples business name address elds Bigbook source,
patterns learned

158

fiWrapper Maintenance

CITY
Los Angeles
Pasadena
Marina Del Rey
Venice
New York
Goleta
Marcy
Brooklyn
New York
Bualo
Los Angeles
Pasadena
Marina Del Rey
West Hollywood
Los Angeles
Los Angeles
Venice
Marina Del Rey
New York
New York
Brooklyn
Brooklyn
Los Angeles
Culver City
Marina Del Rey
Culver City
Marina Del Rey
West Hollywood
Starting patterns
<Upper Upper>
<Upper Upper Rey>

STATE
CA
CA
CA
CA
NY
CA
NY
NY
NY
NY
CA
CA
CA
CA
CA
CA
CA
CA
NY
NY
NY
NY
CA
CA
CA
CA
CA
CA

PHONE
( 323 ) 655
( 626 ) 793
( 310 ) 822
( 310 ) 396
( 212 ) 376
( 805 ) 683
( 315 ) 793
( 718 ) 998
( 212 ) 249
( 716 ) 839
( 310 ) 477
( 626 ) 792
( 310 ) 823
( 310 ) 652
( 310 ) 839
( 310 ) 855
( 310 ) 392
( 310 ) 577
( 212 ) 924
( 212 ) 929
( 718 ) 253
( 718 ) 646
( 310 ) 559
( 310 ) 202
( 310 ) 822
( 310 ) 391
( 310 ) 578
( 323 ) 655

<AllCaps>

<( 3digit ) 3digit - Large>

-

2056
8123
1511
1179
7552
8864
1871
2550
3612
5090
7242
9779
4446
9770
8571
9380
5751
8182
7840
4800
1002
1225
1357
6969
0095
5780
2293
6030

Table 3: Examples city, state phone number elds Bigbook source,
patterns learned

159

fiLerman, Minton & Knoblock

PRICE CHANGE
+ 0 . 51
+ 1 . 51
+ 4 . 08
+ 0 . 83
+ 2 . 35
- 10 . 84
- 1 . 24
- 1 . 59
- 2 . 94
+ 1 . 04
- 0 . 81
+ 4 . 45
+ 0 . 16
- 3 . 48
+ 0 . 49
- 3 . 38
+ 1 . 15
- 2 . 86
- 6 . 46
- 0 . 82
+ 2 . 00
+ 0 . 13
- 1 . 63
Starting patterns
<Punct 1digit . 2digit>

TICKER
INTC
IBM
AOL

LU
ATHM
COMS
CSCO
GTE
AAPL
MOT
HWP
DELL
GM
CIEN
EGRP
HLIT
RIMM
C
GPS
CFLO
DCLK
NT
BFRE
QCOM

VOLUME
17 , 610 , 300
4 , 922 , 400
24 , 257 , 300
8 , 504 , 000
9 , 789 , 300
5 , 646 , 400
15 , 388 , 200
19 , 135 , 900
1 , 414 , 900
2 , 291 , 800
3 , 599 , 600
2 , 147 , 700
40 , 292 , 100
1 , 398 , 100
4 , 120 , 200
7 , 007 , 400
543 , 400
307 , 500
6 , 145 , 400
1 , 023 , 600
157 , 700
1 , 368 , 100
4 , 579 , 900
149 , 000
7 , 928 , 900

<AllCaps>

<Number , 3digit , 3digit>

PRICE
122 3 / 4
109 5 / 16
63 13 / 16
53 1 / 16
68
29 7 / 8
57 11 / 32
134 1 / 2
65 15 / 16
117 3 / 4
169 1 / 4
145 5 / 16
57 3 / 16
77 15 / 16
142
25 7 / 8
128 13 / 16
132 1 / 4
49 15 / 16
44 5 / 8
103 1 / 4
106
124 1 / 8
46 9 / 16
128 1 / 16
<Medium 1digit / Number>
<Medium 15 / 16 >

Table 4: Data examples ahoo Quote source, patterns learned

160

fiWrapper Maintenance

3.1 Wrapper Verification
data extracted wrapper changes signicantly, indication
Web source may changed format. wrapper verication system uses examples
data extracted wrapper past known correct order
acquire description data. learned description contains features two types:
patterns learned DataProG global numeric features, density tokens
particular type. application checks description still applies
new data extracted wrapper. Thus, wrapper verication specic instance
data validation task.
verication algorithm works following way. set queries used retrieve
HTML pages wrapper extracts (correct) training examples. algorithm
computes values vector features, "k, describes eld training
examples. features include patterns describe common beginnings (or
endings) eld. verication phase, wrapper generates set (new)
test examples pages retrieved using set queries, computes feature
vector "r associated eld test examples. two distributions, "k "r
(see Fig. 5), statistically (at signicance level), wrapper judged
extracting correctly; otherwise, judged failed.

16
training set
test set

feature value

14
12
10
8
6
4
2
0
1

2

3

4

5

6

7

feature

Figure 5: hypothetical distribution features training test examples

eld described vector, whose ith component value ith feature,
number examples match pattern j. addition patterns, use
following numeric features describe sets training test examples: average
number tuples-per-page, mean number tokens examples, mean token length,
density alphabetic, numeric, HTML-tag punctuation types. use goodness
method (Papoulis 1990) decide whether two distributions same. use
goodness method, must rst compute Pearsons test statistic data.
Pearsons test statistic dened as:
161

fiLerman, Minton & Knoblock

q=



(ti ei )2
i=1

ei

(2)

ti observed value ith feature test data, ei expected
value feature, number features. patterns ei = nri /N ,
ri number training examples explained ith patter, N number
examples training set n number examples test set. numeric
features ei simply value feature training set. test statistic q
chi-squared distribution 1 independent degrees freedom. q < 2 (m 1; ),
conclude signicance level two distributions same; otherwise,
conclude dierent. Values 2 dierent values looked
statistics table calculated using approximation formula.
order use test statistic reliably, helps use many independent features
possible. series verication experiments reported (Lerman & Minton, 2000),
used starting ending patterns average number tuples-per-page feature
computing value q. found method tended overestimate
test statistic, features (starting ending patterns) independent.
experiments reported paper, use starting patterns, order
increase number features, added numeric features description data.
3.1.1 Results
monitored 27 wrappers (representing 23 distinct Web sources) period ten
months, May 1999 March 2000. sources listed Table 5. wrapper,
results 1530 queries stored periodically, every 710 days. used
query set source, except hotel source, accepted dated queries,
change dates periodically get valid results. set new results
(test examples) compared last correct wrapper output (training examples).
verication algorithm used DataProG learn starting patterns numeric
features eld training examples made decision high signicance
level (corresponding = 0.001) whether test set statistically similar
training set. none starting patterns matched test examples data
found changed signicantly data eld, concluded wrapper failed
extract correctly source; otherwise, data elds returned statistically
similar data, concluded wrapper working correctly.
manual check 438 comparisons revealed 37 wrapper changes attributable
changes source layout data format.4 verication algorithm correctly discovered 35 changes made 15 mistakes. mistakes, 13 false positives,
means verication program decided wrapper failed reality
working correctly. two errors important false negatives,
meaning algorithm detect change data source. numbers
4. Seven were, fact, internal wrapper itself, wrapper modied extract
$22.00 instead 22.00 price eld. actions mostly outside control,
chose classify wrapper changes.

162

fiWrapper Maintenance

Source
airport
altavista
Amazon
arrowlist

Type
tuple/list
list
tuple
list

Bigbook
Barnes&N oble
borders
cuisinenet

tuple
tuple
list
list

geocoder
hotel
mapquest
northernlight
parking
Quote
Smartpages
showtimes
theatre
W ashington P ost
whitepages
yahoo people
ahoo Quote
yahoo weather
cia f actbook

tuple
list
tuple
list
list
tuple
tuple
list
list
tuple
list
list
tuple
tuple
tuple

Data Fields
airport code, name
url, title
book author, title, price, availability, isbn
part number, manufacturer, price, status,
description, url
business name, address, city, state, phone
book author, title, price, availability, isbn
book author, title, price, availability
restaurant name, cuisine, address, city, state,
phone, link
latitude, longitude, street, city, state
name, price, distance, url
hours, minutes, distance, url
url, title
lotname, dailyrate
stock ticker, price, pricechange, volume
name, address, city, state, phone
movie, showtimes
theater name, url, address
taxi price
business name, address, city, state, phone
name, address, city, state, phone
stock ticker, price, pricechange, volume
temperature, forecast
country area, borders, population, etc.

Table 5: List sources used experiments data elds extracted them. Source
type refers much data source returns response query single
tuple list tuples. airport source, type changed single tuple
list time.

163

fiLerman, Minton & Knoblock

result following precision, recall accuracy values:
P

=

R =
=

true positives
= 0.73 ,
true positives + f alse positives
true positives
= 0.95 ,
true positives + f alse negatives
true positives + true negatives
= 0.97 .
positives + negatives

results improvement reported (Lerman & Minton, 2000),
produced P = 0.47, R = 0.95, = 0.91. poor precision value reported
work due 40 false positives obtained data set. attribute improvements eliminating specicity bias patterns learned DataProG
changing feature set include starting patterns additional numeric
features. Note improvement result simply adding numeric features.
check this, ran verication experiments subset data (the last 278 comparisons) using global numeric features obtained P = 0.92 R = 0.55, whereas
using patterns numeric features results values P = 0.71 R = 1.00
data set.
3.1.2 Discussion Results
Though succeeded signicantly reducing number false positives,
managed eliminate altogether. number reasons presence,
point limitations approach.
split types errors roughly three entirely independent classes:
improper tokenization, incomplete data coverage, data format changes. URL eld
(Table 6) accounted signicant fraction false positives, large part due
design tokenizer, splits text strings punctuation marks. URL
contains embedded punctuation (as part alphanumeric key associated user
session id), split varying number tokens, hard capture
regularity eld. solution rewrite tokenizer recognize URLs
well dened specications exist. address problem ongoing work.
algorithm failed sometimes (e.g., arrowlist, showtimes) learned long
specic descriptions. worth pointing out, however, performed correctly
two dozen comparisons sources. types errors caused incomplete
data coverage: larger, varied training data set would produce general patterns,
would perform better verication task. striking example data coverage
problem occurred stock quotes source: day training data collected,
many movements stock price up, opposite true
day test data collected. result, price change elds two days
dissimilar. Finally, DataProG learns format data, false positives
inevitably result changes data format indicate problem
algorithm. case factbook source, units area changed
km2 sq km.
164

fiWrapper Maintenance

hotel, mapquest (5 cases): URL eld contains alphanumeric keys, embedded punctuation symbols. tokenizer splits eld many tokens. key
format changes from:
http://. . .&Stamp=Q4aaiEGSp68*itn/hot%3da11204,itn/agencies/newitn. . .
http://. . .&Stamp=8bEgGEQrCo*itn/hot%3da11204,itn/agencies/newitn. . .
one occasion, server name inside URL changed:
http://enterprise.mapquest.com/mqmapgend?MQMapGenRequest=. . .
http://sitemap.mapquest.com/mqmapgend?MQMapGenRequest=. . .
showtimes, arrowlist (5 cases ): Instance showtimes eld part number
description elds (arrowlist) long. Many long, overly specic patterns
learned elds: e.g.,
<( Number : 2digit AllCaps ) , ( Small : 2digit ) , ( Small : 2digit ) , ( 4 : 2digit
) , 6 : 2digit , 7 : 2digit , 9 : 2digit , 10 : 2digit >

altavista (1 case): Database search engine appears updated. dierent
set results returned query.
quote (1 case): Data changed many positive negative price movements test examples
factbook (1 case): Data format changed:
f rom <Number km2 >
<Number sq km >
Table 6: List sources false positive results verication task

165

fiLerman, Minton & Knoblock

3.2 Wrapper Reinduction
wrapper stops extracting correctly, next challenge rebuild automatically (Cohen, 1999). extraction rules wrappers (Muslea et al., 2001), well
many others (cf. (Kushmerick et al., 1997; Hsu & Dung, 1998)), generated machine
learning algorithm, takes input several pages source labeled examples
data extract page. assumed user labeled examples correctly. label least pages wrapper fails correctly identifying
examples data them, use examples input induction algorithm,
STALKER,5 generate new extraction rules.6 Note need identify data every page depending regular data layout is, Stalker
learn extraction rules using small number correctly labeled pages. solution
bootstrap wrapper induction process (which learns landmark-based rules) learning
content-based rules. want re-learn landmark-based rules, types
sites use, rules tend much accurate ecient content-based
rules.
employ method takes set training examples, extracted source
wrapper known working correctly, set pages
source, uses mixture supervised unsupervised learning techniques identify
examples data eld new pages. assume format data
change. Patterns learned DataProG play signicant role reinduction task.
addition patterns, features, length training examples
structural information pages used. fact, page structure used
critical step algorithm, discuss approach learning detail next
paragraph.
3.2.1 Page Template Algorithm
Many Web sources use templates, page skeletons, automatically generate pages
results database query. evident example Fig. 6.
template consists heading RESULTS, followed number results match
query, phrase Click links associated businesses information,
heading LISTINGS, followed anchors map, driving directions, add
Directory bolded phrase Appears Category. Obviously, data
part template rather, appears slots template elements.
Given two example pages source, induce template
used generate (Table 7). template nding algorithm looks sequences
tokens HTML tags text appear exactly page.
algorithm works following way: pick smallest page set template
seed. Starting rst token page, grow sequence appending tokens
5. matter, fact, matter wrapper induction system used. easily replace
Stalker HLRT (Kushmerick et al., 1997) generate extraction rules.
6. paper discuss wrapper reinduction information sources return single tuple
results per page, detail page. order create data extraction rules sources return lists
tuples, Stalker wrapper induction algorithm requires user specify rst last elements
list, well least two consecutive elements. Therefore, need able identify
data elements high degree certainty.

166

fiWrapper Maintenance

(a)

(b)
Figure 6: Fragments two Web pages source displaying restaurant information.

167

fiLerman, Minton & Knoblock

it, subject condition sequence appears every page. managed
build sequence thats least three tokens long7 , sequence appears exactly
page, becomes part page template. Templates play important role
helping identify correct data examples pages.
input:
P = set N Web pages
output:
= page template
begin
p = shortest(P )
= null
= null
= rsttoken(p) lasttoken(p)
= concat(s, t)
( appears every page P )
=
continue
else

n= N
page=1 count(s, page)
( n = N length(s) 3 )
add-to-template(T, s)
end
= null
end
end
end
Table 7: Pseudocode template nding algorithm

3.2.2 Automatic Labeling Algorithm
Figure 7 schematic outline reinduction algorithm, consists automatic
data labeling wrapper induction. latter aspect described detail
work (Muslea et al., 2001), focus discussion automatic data
labeling algorithm.
First, DataProG learns starting ending patterns describe set training examples. training examples collected wrappers normal operation, correctly extracting data Web source. patterns used
identify possible examples data eld new pages. addition patterns,
calculate mean (and variance) number-of-tokens training examples. new page scanned identify text segments begin one
starting patterns end one ending patterns. Text segments con7. best value minimum length page template element determined empirically
three.

168

fiWrapper Maintenance

extract

extracted
data

Wrapper

learn
labeled
Web pages

Wrapper
Induction
System

patterns
apply

Web
pages

extracts
score

group

Figure 7: Schematic outline reinduction algorithm
tain signicantly fewer tokens expected based old number-of-tokens
distribution, eliminated set candidate extracts. learned patterns
often general match many, possibly hundreds, text segments page.
Among spurious text segments correct example data eld. rest
discussion concerned identifying correct examples data pages.
exploit simple priori assumptions structure Web pages help
us separate interesting extracts noise. expect examples data eld
appear roughly position context page. example,
Fig. 6 shows fragments two Web pages source displaying restaurant information. pages relevant information restaurant appears
heading LISTINGS phrase Appears Category:. Thus,
expect eld, e.g., address, appear place, slot, within page
template. Moreover, information trying extract usually part
page template; therefore, candidate extracts part page template
eliminated consideration. Restaurant address always follows restaurant name (in
bold) precedes city zip code, i.e., appears context every page.
given eld either visible user every page, invisible (part HTML
tag) every page. order use information separate extracts, describe
candidate extract feature vector, includes positional information, dened
(page template) slot number context. context captured adjacent tokens:
one token immediately preceding candidate extract one token immediately following it. use binary feature value one token visible
user, zero part HTML tag. candidate extracts assigned
feature vectors, split groups, within group, candidate extracts
described feature vector.
next step score groups based similarity training examples.
expect highest scoring group contain correct examples data eld. One scoring
method involves assigning rank groups based many extracts
common training examples. technique generally works well,
least data usually remains Web page layout changes.
169

fiLerman, Minton & Knoblock

course, assumption apply data changes frequently, weather
information, ight arrival times, stock quotes, etc. However, found even
sources, enough overlap data approach works. scoring
algorithm assigns zero groups, i.e., exist extracts common training
examples, second scoring algorithm invoked. scoring method follows wrapper
verication procedure nds group similar training examples
based patterns learned training examples.
nal step wrapper reinduction process provide extracts top
ranking group Stalker wrapper induction algorithm (Muslea et al., 2001) along
new pages. Stalker learns data extraction rules changed pages. Note
examples provided Stalker required correct examples eld.
set automatically labeled examples includes false positives, Stalker learn
correct extraction rules eld. False negatives problem, however.
reinduction algorithm could nd correct example data page, page
simply used wrapper induction stage.
3.2.3 Results
evaluate reinduction algorithm used ten sources (listed Table 5)
returned single tuple results per page, detail page.8 method data collection
described Sec. 3.1.1. period October 1999 March 2000
eight format changes sources. Since set much small evaluation
purposes, created articial test set considering ten data sets collected
source period. evaluated algorithm using extract data
Web pages correct output known. Specically, took ten tuples set
collected one date, used information extract data ten pages (randomly
chosen) collected later date, regardless whether source actually changed
not. reserved remaining pages collected later date testing learned
Stalker rules.
output reinduction algorithm list tuples extracted ten pages,
well extraction rules generated Stalker pages. Though cases
able extract every eld every pages, still learn good extraction rules
Stalker long examples eld correctly labeled. evaluated
reinduction algorithm two stages: rst, checked many data elds source
identied successfully; second, checked quality learned Stalker rules
using extract data test pages.
Extracting content-based rules judged data eld successfully extracted automatic labeling algorithm able identify correctly least two
ten pages. minimum number examples Stalker needs create extraction rules. practice, low success rate occurred one eld two
8. use geocoder cia f actbook wrappers experiments. geocoder wrapper
accessed source another application; therefore, pages available us analysis.
reason excluding f actbook plain text source, methods apply Web
pages. Note verication experiments, two wrappers mapquest source,
extracting dierent data. experiments described below, used one contained
data time period.

170

fiWrapper Maintenance

sources: Quote ahoo Quote. sources, eld successfully
extracted, correctly identied least three, cases almost all,
pages set. false positive occurred reinduction algorithm incorrectly identied text page correct example data eld. many cases, false positives
consisted partial elds, e.g., Cloudy rather Mostly Cloudy (yahoo weather).
false negative occurred algorithm identify examples data eld.
ran reinduction experiment attempting extract elds listed Table 8.
second column table lists fractions data sets eld successfully
extracted. able correctly identify elds 277 times across data sets making
61 mistakes, 31 attributed false positives 30 false negatives.
several reasons reinduction algorithm failed operate perfectly. many
cases reason small training set.9 achieve better learning yellowpages-type sources Bigbook Smartpages using training examples (see Fig. 8).
two cases, errors attributable changes format data, resulted
failure patterns capture structure data correctly: e.g., airport source
changed airport names capitalized words allcaps, Quote source
patterns able identify negative price changes learned
data set stocks positive price change. two sources
reinduction algorithm could distinguish correct examples eld
examples data type: Quote source, cases extracted
opening price high price stock price eld, yahoo weather source,
extracted high low temperature, rather current temperature. problem
evident Smartpages source, city name appeared several places
page. cases, user intervention meta-analysis elds may necessary
improve results data extraction.
Extracting landmark-based rules nal validation experiment consisted
using automatically generated wrappers extract data test pages. last three
columns Table 8 list precision, recall accuracy extracting data test pages.
performance good elds, notable exception STATE eld
Bigbook source. eld, pattern <Allcaps> overly general, wrong
group received highest score scoring step reinduction algorithm.
average precision recall values P = 0.90 R = 0.80.
Within data set studied, sources, listed Table 9, experienced total seven
changes. addition sources, airport source changed format data
returned, since simultaneously changed presentation data detail page
list, could use data learn Stalker rules. Table 9 shows performance
automatically reinduced wrappers changed sources. elds precision P ,
important performance measures, close maximum value, indicating
false positives. However, small values recall indicate
examples elds extracted. result traced limitation
approach: eld appears dierent context, one rule necessary
9. Limitations data collection procedure prevented us accumulating large data sets
sources; therefore, order keep methodology uniform across sources, decided use
smaller training sets.

171

fiLerman, Minton & Knoblock

source/field
airport code
airport name
Amazon author
Amazon title
Amazon price
Amazon ISBN
Amazon availability
Barnes&N oble author
Barnes&N oble title
Barnes&N oble price
Barnes&N oble ISBN
Barnes&N oble availability
Bigbook name
Bigbook street
Bigbook city
Bigbook state
Bigbook phone
mapquest time
mapquest distance
Quote pricechange
Quote ticker
Quote volume
Quote shareprice
Smartpages name
Smartpages street
Smartpages city
Smartpages state
Smartpages phone
ahoo Quote pricechange
ahoo Quote ticker
ahoo Quote volume
ahoo Quote shareprice
W ashington P ost price
W eather temp
W eather outlook
average

ex %
100
90
100
70
100
100
60
100
80
90
100
90
70
90
70
100
90
100
100
50
63
100
38
80
80
0
100
100
100
100
100
80
100
40
90
83

p
1.0
1.0
97.3
98.8
1.0
1.0
1.0
0.93
0.96
1.0
1.0
1.0
1.0
1.0
0.91
0.04
1.0
1.0
1.0
0.38
0.93
1.0
0.46
1.0
1.0
0.68
1.0
0.99
1.0
1.0
1.0
1.0
1.0
0.36
0.83
0.90

r
1.0
1.0
0.92
0.81
0.99
0.91
0.86
0.96
0.62
0.68
0.95
0.92
0.76
0.87
0.98
0.50
0.30
0.98
0.98
0.36
0.87
0.88
0.60
0.82
0.52
0.58
0.70
1.0
0.41
0.98
0.99
0.59
1.0
0.82
1.0
0.80

Table 8: Reinduction results ten Web sources. rst column lists fraction
elds source correctly extracted pattern-based algorithm.
judged eld extracted algorithm correctly identied least
two examples it. last two columns list precision recall data
extraction task using reinduced wrappers.

172

fiWrapper Maintenance

100

extraction accuracy (%)

80

60

40
PHONE
STATE
CITY
NAME
STREET

20

0
0

5

10

15

20

25

number training examples

Figure 8: Performance reinduction algorithm elds Smartpages source
size training set increased

source/field
Amazon author
Amazon title
Amazon price
Amazon ISBN
Amazon availability
Barnes&N oble author
Barnes&N oble title
Barnes&N oble price
Barnes&N oble ISBN
Barnes&N oble availability
Quote pricechange
Quote ticker

P
1.0
1.0
0.9
1.0
1.0
1.0
1.0
1.0
1.0
1.0
0.0
1.0

R
1.0
0.7
0.9
0.9
0.9
0.5
0.8
1.0
1.0
1.0
0.0
1.0


1.0
0.7
0.9
0.9
0.9
0.5
0.8
1.0
1.0
1.0
0.0
1.0

source/field
Smartpages name
Smartpages street
Smartpages city
Smartpages state
Smartpages phone
ahoo Quote pricechange
ahoo Quote ticker
ahoo Quote volume
ahoo Quote shareprice
Quote volume
Quote shareprice

P
1.0
N/A
0.0
1.0
N/A
1.0
1.0
1.0
1.0

R
0.9
0.0
0.0
0.9
0.0
0.2
0.5
0.7
0.7


0.9
0.0
0.0
0.9
0.0
0.2
0.5
0.7
0.7

1.0
0.0

1.0
N/A

1.0
0.0

Table 9: Precision, recall, accuracy learned STALKER rules changed
sources

extract source. cases, extract subset examples
share context, ignore rest examples.
mentioned earlier, believe achieve better performance yellow-pagestype sources Bigbook Smartpages using training examples. Figure 8 shows
eect increasing size training example set performance automatically
generated wrappers Smartpages source. number training examples goes
up, accuracy extracted elds goes up.
173

fiLerman, Minton & Knoblock

3.2.4 Lists
applied reinduction algorithm extract data pages containing lists
tuples, and, many cases, successfully extracted least several examples
eld several pages. However, order learn correct extraction rules sources
returning lists data, Stalker requires rst, last least two consecutive
list elements correctly specied. methods presented cannot guarantee
required list elements extracted, unless list elements extracted.
currently working new approaches data extraction lists (Lerman, Knoblock, &
Minton, 2001) enable us use Stalker learn correct data extraction rules.

4. Previous Work
signicant amount research activity area pattern learning.
section discuss two approaches, grammar induction relational learning,
compare performance DataProG tasks Web wrapper application
domain. Section 4.2 review previous work topics related wrapper maintenance,
Section 4.3 discuss related work information extraction wrapper induction.
4.1 Pattern Learning
4.1.1 Grammar induction
Several researchers addressed problem learning structure, patterns, describing text data. particular, grammar induction algorithms used past
learn common structure set strings. Carrasco Oncina proposed ALERGIA (Carrasco & Oncina, 1994), stochastic grammar induction algorithm learns
regular language positive examples language. ALERGIA starts nite
state automaton (FSA) initialized prex tree represents strings
language. ALERGIA uses state-merging approach (Angluin, 1982; Stolcke & Omohundro, 1994) FSA generalized merging pairs statistically similar (at
signicance level) subtrees. Similarity based purely relative frequencies
substrings encoded subtrees. end result minimum FSA consistent
grammar.
Goan et al. (Goan et al., 1996) found applied data domains commonly
found Web, addresses, phone numbers, etc., ALERGIA tended merge
many states, resulting over-general grammar. proposed modications
ALERGIA, resulting algorithm WIL, aimed reducing number faulty merges.
modications motivated observation symbol string belong one
following syntactic categories: NUMBER, LOWER, UPPER DELIM.
viewed syntactic level, data strings contain additional structural information
eectively exploited reduce number faulty merges. WIL merges two subtrees
similar (in ALERGIA sense) if, every level, contain nodes
syntactic type. WIL adds wildcard generalization step
transitions corresponding symbols category approximately evenly
distributed range syntactic type (e.g., 09 numerals) replaced
single transition corresponding type (e.g., NUMBER). Goan et al. demonstrated
174

fiWrapper Maintenance

grammars learned WIL eective recognizing new strings several
relevant Web domains.
compared performance WIL DataProG wrapper verication task.
used WIL learn grammar token level using data examples extracted
wrappers, character level done Goan et al.Another dierence
Goan et al. that, whereas needed order 100 strings arrive high
accuracy rate, order 2030 examples work with. Note
longer apply wildcard generalization step FSA would need many
examples decide whether token approximately evenly distributed
syntactic type. Instead, compare DataProG two versions WIL: one without
wildcard generalization (WIL1), one every token initial FSA replaced
syntactic type (WIL2). addition syntactic types used Goan et al.,
introduce another type ALNUM consistent patterns learned
DataProG. Neither version WIL allows multi-level generalization.
algorithms tested data extracted wrappers 26 Web sources ten
dierent occasions period several months (see Sec. 3.1). Results 2030 queries
stored every time. wrapper, one data set used training examples,
data set extracted next date used test examples. used WIL1
WIL2 learn grammar eld training examples used
grammar recognize test examples. grammar recognized 80%
test examples data eld, concluded recognized entire data eld; otherwise,
concluded grammar recognize eld, possibly data
changed. procedure used wrapper verication experiments,
described greater detail Section 3.1.1. period time covered
data, 21 occasions Web site changed, thereby causing data
extracted wrapper change well. precision recall values WIL1
(grammar induction specic tokens) P = 0.20, R = 0.81; WIL2 (grammar
induction wildcards representing tokens syntactic categories) values P = 0.55
R = 0.76. WIL1 learned overly specic grammar, resulted high rate
false positives verication task, WIL2 learned overly general grammar,
resulting slightly false negatives. recall precision value DataProG
data P = 0.73 R = 1.0.
Recently Thollard et al. (Thollard, Dupont, & de la Higuera, 2000) introduced MDI,
extension ALERGIA. MDI shown generate better grammars least one
domain reducing number faulty merges states . MDI replaces ALERGIAs
state merging criterion global measure attempts minimize KullbackLeibler divergence learned automaton training sample
time keeping size automaton small possible. clear whether MDI
(or combination MDI/WIL) lead better grammars common Web data types.
suspect not, regular grammars capture multitude data types
found Web. example, business names, restaurant names shown Table 2
may well dened structure, yet many start two capitalized words
end word Restaurant constitute patterns learned DataProG.
175

fiLerman, Minton & Knoblock

4.1.2 Relational learning
sequence n tokens, pattern viewed non-recursive n-ary predicate.
Therefore, use relation-learning algorithm FOIL (Quinlan, 1990) learn them.
Given set positive negative examples class, FOIL learns rst order predicate
logic clauses dening class. Specically, nds discriminating description covers
many positive none negative examples.
used Foil.6 no-negative-literals option learn patterns describing several
dierent data elds. cases closed world assumption used construct negative
examples known objects: thus, Bigbook source, names addresses
negative examples phone number class. used following encoding
translate training examples allow foil.6 learn logical relations. data eld,
FOIL learned clauses form
data f ield(A) := P (A) f ollowed by(A, B) P (B) ,

(3)

denition eld, B tokens, terms right hand side
predicates. predicate f ollowed by(A, B) expresses sequential relation
tokens. predicate P (A) allows us specify token specic token (e.g.,
John(A)) general type (e.g., Upper(A), Alpha(A)), thus, allowing FOIL
multi-level generalization capability DataProG.
ran Foil.6 examples associated Bigbook (see Tables 23).
relational denitions learned Foil.6 examples shown Table 10.
many cases, similarities denitions learned FOIL
patterns learned DataProG, though clauses learned FOIL tended overly
general. Another problem given examples class little structure,
names book titles, FOIL tended create clauses covered single examples,
failed nd clauses. general, description learned FOIL depended critically
supplied negative examples eld. example, trying
learn denition book titles presence prices, FOIL would learn something
starts capitalized word title. author names supplied negative
examples well, learned denition would dierent. Therefore, using FOIL
situations complete set negative examples known available,
problematic.
4.2 Wrapper Maintenance
Kushmerick (Kushmerick, 1999) addressed problem wrapper verication proposing
algorithm Rapture verify wrapper correctly extracts data Web page.
work, data eld described collection global features,
word count, average word length, density types, i.e., proportion characters
training examples HTML, alphabetic, numeric type. Rapture calculated
mean variance features distribution training examples. Given
set queries wrapper output known, Rapture generates new result
query calculates probability generating observed value every feature.
Individual feature probabilities combined produce overall probability
wrapper extracted data correctly. probability exceeds certain threshold,
176

fiWrapper Maintenance

*** Warning:
NAME(A)
NAME(A)
NAME(A)

following definition cover 23 tuples relation
:= AllCaps(A), followed by(A,B)
:= Upper(A), followed by(A,B), Number(B)
:= followed by(A,B), Venice(B)

STREET(A) := Large(A), followed by(A,B)
STREET(A) := Medium(A), followed by(A,B), AlphaNum(B)

** Warning:
CITY(A)
CITY(A)
CITY(A)
CITY(A)
CITY(A)

following definition cover 9 tuples relation
:= Los(A)
:= Marina(A)
:= New(A)
:= Brooklyn(A)
:= West(A), followed by(A,B), Alpha(B)

STATE(A) := CA(A)
STATE(A) := NY(A)
PHONE(A) := ((A)

Table 10: Denitions learned foil.6 Bigbook source
Rapture decides wrapper correct; otherwise, failed. Kushmerick
found HTML density alone correctly identify almost changes
sources monitored. fact, adding features probability calculation
signicantly reduced algorithms performance. compared Raptures performance
verication task approach, found Rapture missed 17 wrapper changes
(false negatives) relied solely HTML density feature. 10
relatively little prior work wrapper reinduction problem. Cohen (Cohen, 1999) adapted WHIRL, soft logic incorporates notion statistical
text similarity, recognize page structure narrow class pages: containing
simple lists simple hotlists (dened anchor-URL pairs). Previously extracted data,
combined page structure recognition heuristics, used reconstruct wrapper
page structure changed. Cohen conducted wrapper maintenance experiments using original data corrupted data examples WHIRL. However, procedure
corrupting data neither realistic representative data Web changes.
Although cannot present guarantee good performance algorithm wrapper reinduction sources containing lists, handle realistic data changes Web
sources returning detail pages.
10. Although use dierent statistical test cannot compare performance algorithm
Rapture directly, doubt would outperform algorithm data set used global
numeric features, because, noted Section 3.1.1, using patterns well global numeric features
verication task outperforms using numeric features only.

177

fiLerman, Minton & Knoblock

4.3 Information Extraction
system, used reinduction task, related spirit many information
extraction (IE) systems developed group others uses learned
representation data extract information specic texts. wrapper induction
systems (see (Muslea et al., 2001; Kushmerick et al., 1997; Freitag & Kushmerick, 2000)),
domain independent works best semi-structured data, e.g., Web pages.
handle free text well systems, AutoSlog (Rilo, 1993)
Whisk (Soderland, 1999), free text fewer non-trivial regularities algorithm
exploit. Unlike wrapper induction, extract data based features
appear near text, rather based content data itself. However, unlike Whisk,
learns content rules, reinduction system represents eld independently
elds, advantage, instance, web source changes
order data elds appear. Another dierence system designed run
automatically, without requiring user interaction label informative examples.
main part purely automatic, reinduction system fails achieve accuracy
IE systems rely labeled examples train system; however,
see major limitation, since designed complement existing extraction
tools, rather supersede them. words, consider reinduction task
successful accurately extract sucient number examples use wrapper
induction system. system use resulting wrapper accurately extract
rest data source.
many similarities approach used RoadRunner system, developed concurrently system reported recently (Crescenzi,
Mecca, & Merialdo, 2001b, 2001a). goal system automatically extract
data Web sources exploiting similarities page structure across multiple pages.
RoadRunner works inducing grammar Web pages comparing several pages
containing long lists data. grammar expressed HTML tag level,
similar extraction rules generated Stalker. RoadRunner system
shown successfully extract data several Web sites. two signicant dierences
work (i) way detecting changes know
wrapper rebuilt (ii) reinduction algorithm works detail pages
only, RoadRunner works lists. believe data-centric approach
exible allow us extract data diverse information sources
RoadRunner approach looks page structure.

5. Conclusion
paper described DataProG algorithm, learns structural information data eld set examples eld. use patterns
two Web wrapper maintenance applications: (i) verification detecting wrapper
stops extracting data correctly Web source, (ii) reinduction identifying new
examples data eld order rebuild wrapper stops working. verication algorithm performed accuracy 97%, much better results reported
earlier work (Lerman & Minton, 2000). reinduction task, patterns
used identify large number data elds Web pages, turn used
178

fiWrapper Maintenance

automatically learn Stalker rules Web sources. new extraction rules
validated using successfully extract data sets test pages.
remains work done wrapper maintenance. current algorithms
sucient automatically re-generate Stalker rules sources return lists tuples.
However, preliminary results indicate (Lerman et al., 2001) feasible combine
information structure data priori expectations structure
Web pages containing lists automatically extract data lists assign rows
columns. believe techniques eventually eliminate need user
mark Web pages enable us automatically generate wrappers Web sources. Another exciting direction future work using DataProG algorithm automatically
create wrappers new sources domain given existing wrappers sources
domain. example, learn author, title price elds
AmazonBooks source, use extract elds Barnes&N obleBooks
source. Preliminary results show indeed feasible. Automatic wrapper generation
important cornerstone information-based applications, including Web agents.

6. Acknowledgments
would thank Priyanka Pushkarna carrying wrapper verication experiments.
research reported supported part Defense Advanced Research
Projects Agency (DARPA) Air Force Research Laboratory contract/agreement
numbers F30602-01-C-0197, F30602-00-1-0504, F30602-98-2-0109, part Air Force
Oce Scientic Research grant number F49620-01-1-0053, part Integrated
Media Systems Center, National Science Foundation (NSF) Engineering Research Center,
cooperative agreement number EEC-9529152 part NSF award number
DMI-0090978. U.S. Government authorized reproduce distribute reports
Governmental purposes notwithstanding copy right annotation thereon. views
conclusions contained herein authors interpreted
necessarily representing ocial policies endorsements, either expressed implied,
organizations person connected them.

References
Abramowitz, M., & Stegun, I. A. (1964). Handbook mathematical functions formulas, graphs mathematical tables. Applied Math. Series 55. National Bureau
Standards, Washington, D.C.
Ambite, J.-L., Barish, G., Knoblock, C. A., Muslea, M., Oh, J., & Minton, S. (2002).
Getting there: Interactive planning agent execution optimizing
travel. Fourteenth Innovative Applications Articial Intelligence Conference
(IAAI-2002), Edmonton, Alberta, Canada, 2002.
Angluin, D. (1982). Inference reversible languages. Journal ACM, 29 (3), 741765.
179

fiLerman, Minton & Knoblock

Brazma, A., Jonassen, I., Eidhammer, I., & Gilbert, D. (1995). Approaches automatic discovery patterns biosequences. Tech. rep., Department Informatics,
University Bergen.
Carrasco, R. C., & Oncina, J. (1994). Learning stochastic regular grammars means
state merging method. Lecture Notes Computer Science, 862, 139.
Chalupsky, H., et al. (2001). Electric elves: Applying agent technology support human
organizations. Proceedings Thirteenth Annual Conference Innovative
Applications Articial Intelligence (IAAI-2001), Seattle, WA.
Cohen, W. W. (1999). Recognizing structure web pages using similarity queries. Proc.
16th National Conference Articial Intelligence (AAAI-1999), pp. 5966.
Crescenzi, V., Mecca, G., & Merialdo, P. (2001a). Automatic web information extraction
roadrunner system. Proceedings International Workshop Data
Semantics Web Information Systems (DASWIS-2001).
Crescenzi, V., Mecca, G., & Merialdo, P. (2001b). RoadRunner: Towards automatic data
extraction large web sites. Proceedings 27th Conference Large
Databases (VLDB) Rome, Italy.
Dietterich, T., & Michalski, R. (1981). Inductive learning structural descriptions.. Articial Intelligence, 16, 257294.
Doorenbos, R. B., Etzioni, O., & Weld, D. S. (1997). scalable comparison-shopping
agent world-wide webs. Proceeding First International Confence
Autonomous Agents, Marina del Rey.
Freitag, D., & Kushmerick, N. (2000). Boosted wrapper induction. Proceedings 7th
Conference Articial Intelligence (AAAI-2000), pp. 577583. AAAI Press, Menlo
Park, CA.
Goan, T., Benson, N., & Etzioni, O. (1996). grammar inference algorithm world
wide web.. Proceedings AAAI Spring Symposium Machine Learning Information Access, Stanford University, CA.
Hsu, C.-N., & Dung, M.-T. (1998). Generating nite-state transducers semi-structured
data extraction web. Journal Information Systems, 23, 521538.
Knoblock, C. A., Lerman, K., Minton, S., & Muslea, I. (2001a). Accurately reliably
extracting data web: machine learning approach. IEEE Data Engineering
Bulletin, 23 (4), 3341.
Knoblock, C. A., Minton, S., Ambite, J. L., Muslea, M., Oh, J., , & Frank, M. (2001b).
Mixed-initiative, multi-source information assistants. Tenth International
World Wide Web Conference (WWW10), Hong Kong.
Kushmerick, N. (1999). Regression testing wrapper maintenance.. Proceedings
14th National Conference Articial Intelligence (AAAI-1999).
180

fiWrapper Maintenance

Kushmerick, N., Weld, D. S., & Doorenbos, R. B. (1997). Wrapper induction information extraction. Proceedings Intl. Joint Conference Articial Intelligence
(IJCAI), pp. 729737.
Lerman, K., Knoblock, C. A., & Minton, S. (2001). Automatic data extraction lists
tables web sources. Proceedings workshop Advances Text Extraction
Mining (IJCAI-2001) Menlo Park. AAAI Press.
Lerman, K., & Minton, S. (2000). Learning common structure data. Proceedings
15th National Conference Articial Intelligence (AAAI-2000) Menlo Park.
AAAI Press.
Muggleton, S. (1991). Inductive logic programming. New Generation Computing, 8, 295
318.
Muslea, I., Minton, S., & Knoblock, C. (1998). Wrapper induction semistructured webbased information sources.. Proceedings Conference Automated Learning
Discovery (CONALD).
Muslea, I., Minton, S., & Knoblock, C. A. (2001). Hierarchical wrapper induction
semistructured information sources. Autonomous Agents Multi-Agent Systems, 4,
93114.
Papoulis, A. (1990). Probability Statistics. Prentice Hall, Englewood Clis, NJ.
Quinlan, J. R. (1990). Learning logical denitions relations.. Machine Learning, 5 (3),
239266.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, San
Mateo, CA.
Rilo, E. (1993). Automatically constructing dictionary information extraction tasks.
Proceedings 11th National Conference Articial Intelligence, pp. 811816
Menlo Park, CA, USA. AAAI Press.
Soderland, S. (1999). Learning information extraction rules semi-structured free
text. Machine Learning, 34 (1-3), 233272.
Stolcke, A., & Omohundro, S. (1994). Inference nite-state probabilistic grammars.
Proceedings 2nd Int. Colloquium Grammar Induction, (ICGI-94), pp. 106
118.
Thollard, F., Dupont, P., & de la Higuera, C. (2000). Probabilistic DFA inference using
Kullback-Leibler divergence minimality. Proceedings 17th International
Conf. Machine Learning, pp. 975982. Morgan Kaufmann, San Francisco, CA.

181


