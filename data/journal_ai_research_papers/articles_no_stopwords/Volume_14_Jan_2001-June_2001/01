journal artificial intelligence

submitted published

speeding convergence value iteration
partially observable markov decision processes
nevin l zhang
weihong zhang

department computer science
hong kong university science technology
clear water bay road kowloon hong kong china

lzhang cs ust hk
wzhang cs ust hk

abstract

partially observable markov decision processes pomdps recently become popular among many ai researchers serve natural model
uncertainty value iteration well known finding optimal policies
pomdps typically takes large number iterations converge proposes
method accelerating convergence value iteration method evaluated array benchmark found effective enabled
value iteration converge iterations test

introduction
pomdps model sequential decision making effects actions nondeterministic state world known certainty attracted
many researchers operations artificial intelligence potential applications wide range areas monahan cassandra b one
uncertainty unfortunately still significant gap potential actual applications primarily due lack effective solution methods
reason much recent effort devoted finding ecient pomdps
e g parr russell hauskrecht b cassandra hansen kaelbling
et al zhang et al
value iteration well known pomdps smallwood sondik
puterman starts initial value function iteratively performs dynamic
programming dp updates generate sequence value functions sequence converges optimal value function value iteration terminates predetermined
convergence condition met
value iteration performs typically large number dp updates converges
dp updates notoriously expensive develop technique reducing
number dp updates
dp update takes finite representation value function input returns
finite representation another value function output value function closer
optimal input value function sense say dp update improves
input propose approximation dp update called point dp update pointbased dp update improves input possibly lesser degree standard dp
update hand computationally much cheaper value iteration
c ai access foundation morgan kaufmann publishers rights reserved

fizhang zhang
perform point dp update number times two standard dp updates
number standard dp updates reduced way since point dp update
improves input reduction come high cost since point dp
update takes little time
rest organized follows next section shall give brief
review pomdps value iteration basic idea behind point dp update
explained section theoretical preparations section shall work
details point dp update section empirical reported
section possible variations evaluated section finally shall discuss related
work section provide concluding remarks section

pomdps value iteration
pomdps
partially observable markov decision process pomdp sequential decision model
agent acts stochastic environment partial knowledge
state environment set possible states environment referred
state space denoted point time environment one
possible states agent directly observe state rather receives
observation denote set possible observations z receiving
observation agent chooses action set possible actions executes
action thereafter agent receives immediate reward environment evolves
stochastically next state
mathematically pomdp specified three sets z reward function
r transition probability function p js observation probability function
p z js reward function characterizes dependency immediate reward
current state current action transition probability characterizes
dependency next state current state current action
observation probability characterizes dependency observation z next time
point next state current action

policies value functions
since current observation fully reveal identity current state agent
needs consider previous observations actions choosing action information current state contained current observation previous observations
previous actions summarized probability distribution state space
astrom probability distribution sometimes called belief state denoted
b possible state b probability current state set
possible belief states called belief space denote b
policy prescribes action possible belief state words
mapping b associated policy value function v belief
state b v b expected total discounted reward agent receives following


fispeeding value iteration pomdps
policy starting b


x
r

v b e b







rt reward received time
discount factor known
exists policy v b v b policy belief
state b puterman policy called optimal policy value function
optimal policy called optimal value function denote v positive
number policy optimal

v b v b b b

value iteration

explain value iteration need consider belief state evolves time let b
current belief state belief state next point time determined
current belief state current action next observation z denote baz
state baz given

baz

ps p z js b



p z jb
p
p z js p z js p js p z jb p z js b renormal

ization constant notation suggests constant interpreted
probability observing z taking action belief state b
define operator takes value function v returns another value function
tv follows
x
tv b max
r b p z jb v baz b b


z

p
r b r b expected immediate reward taking action belief
state b given value function v policy said v improving
x p zjb v ba b b
b arg max

r

b




z

z



value iteration finding optimal policies starts initial
value function v iterates following formula
vn tvn
known e g puterman theorem vn converges v n goes
infinity value iteration terminates bellman residual maxb jvn b vn b j falls
vn improving policy optimal e g puterman
since infinitely many belief states value functions cannot explicitly represented fortunately value functions one encounters process value iteration
admit implicit finite representations explaining first introduce several technical concepts notations


fizhang zhang










figure illustration technical concepts

technical notational considerations

convenience view functions state space vectors size jsj use lower
case greek letters refer vectors script letters v u refer sets
vectors contrast upper case letters v u refer value functions
functions belief space b note belief state function state space
hence viewed vector
set v vectors induces value function follows

f b max
ffb b b
v

p

ffb inner product b ffb b convenience
shall abuse notation use v denote set vectors value function induced
set convention quantity f b written v b
vector set extraneous removal affect function set
induces useful otherwise set vectors parsimonious contains extraneous
vectors
given set v vector v define open witness region r v closed
witness region r v w r v regions belief space b respectively given

r v fb bjffb b vnfffgg
r v fb bjffb b vnfffgg
literature belief state open witness region r v usually called witness
point since testifies fact useful shall call belief
state closed witness region r v witness point
figure diagrammatically illustrates aforementioned concepts line
bottom depicts belief space pomdp two states point left end
represents probability distribution concentrates masses one states
point right end represents one concentrates masses
state four vectors four slanting lines represent


fispeeding value iteration pomdps

v

vi













f
u

dp update v
maxb ju b v b j
r v u
g r
return u

r

figure value iteration pomdps
linear functions ffi b b value function induced four vectors
represented three bold line segments top vector extraneous
removal affect induced function vectors useful first
segment line bottom witness region second segment
last segment

finite representation value functions value iteration

value function v represented set vectors equals value function induced
set value function representable finite set vectors
unique parsimonious set vectors represents function littman et al
sondik shown value function v representable finite set
vectors value function tv process obtaining parsimonious
representation tv parsimonious representation v usually referred
dynamic programming dp update let v parsimonious set vectors represents
v convenience use v denote parsimonious set vectors represents
tv
practice value iteration pomdps carried directly terms value
functions rather carried terms sets vectors represent
value functions figure one begins initial set vectors v iteration
one performs dp update previous parsimonious set v vectors obtains
parsimonious set vectors u one continues bellman residual maxb ju b v b j
determined solving sequence linear programs falls threshold

point dp update idea
section explains intuitions behind point dp update begin
called backup operator

backup operator
let v set vectors b belief state backup operator constructs

vector three steps



fizhang zhang
action observation z vector v maximum inner
product bza belief state case z observed executing action
belief state b one vector break ties lexicographically
littman denote vector found fia z
action construct vector fia
x
fia r p zjs fia z
z

vector among fia maximum inner product b
one vector break ties lexicographically denote vector found
backup b v
shown smallwood sondik littman backup b v
member v set vectors obtained performing dp update v moreover b
witness point backup b v
fact corner stone several dp update one pass
sondik linear support cheng relaxed region
cheng operate following way first systematically search
witness points vectors v obtain vectors backup operator
witness kaelbling et al employs similar idea

point dp update

systematically searching witness points vectors v computationally expensive point dp update instead uses heuristics come
collection belief points backs points might miss witness points
vectors v hence approximation standard dp update
obviously backing different belief states might vector
words backup b v backup b v might equal two different belief states b
b possible one gets vectors many backups one issue
design point dp update avoid address issue witness
points
point dp update assumes one knows witness point vector
input set backs points rationale witness points vectors
given set scatter belief space hence chance creating duplicate
vectors low experiments confirmed intuition
assumption made point dp update reasonable input
output standard dp update another point dp update standard
dp update computes products witness point output vectors
seen later point dp update shares property design

use point dp update

indicated introduction propose perform point dp update number
times two standard dp updates specific propose modify
seen later point dp update backs points



fispeeding value iteration pomdps

vi









v





f
u

dp update v
maxb ju b v b j
r v point vi u
g r
return u

r

u
f

v u

u point dpu v
g stop u v false
return v
point vi

figure modified value iteration pomdps
value iteration way shown figure note change line
instead assigning u directly v pass subroutine point vi
assign output subroutine v subroutine functions way
value iteration except performs point dp updates rather standard dp
updates hence call point value iteration
figure illustrates basic idea behind modified value iteration contrast value
iteration initial value function properly selected sequence value functions produced value iteration converges monotonically optimal value function
convergence usually takes long time partially standard dp updates indicated
fat upward arrows computationally expensive modified value iteration interleaves
standard dp updates point dp updates indicated thin upward
arrows point dp update improve value function much standard dp
update however complexity much lower consequence modified value iteration
hopefully converge less time
idea interleaving standard dp updates approximate updates back
finite number belief points due cheng work differs cheng
method mainly way select belief points detailed discussion differences
given section
modified value iteration raises three issues first stopping criterion
use point value iteration second guarantee stopping
criterion eventually satisfied third guarantee convergence
modified value iteration address issues introduce concept
uniformly improvable value functions
section



fizhang zhang









standard update

point update

value iteration

modified value iteration

figure illustration basic idea behind modified value iteration

uniformly improvable value functions

suppose v u two value functions say u dominates v write v u
v b u b every belief state b value function v said uniformly improvable
v tv set u vectors dominates another set v vectors value function induced
u dominates induced v set vectors unformly improvable value
function induces

lemma operator isotone sense two value functions v
u v u implies tv tu
lemma obvious well known mdp community puterman
nonetheless enables us explain intuition behind term uniformly improvable
suppose v uniformly improvable value function suppose value iteration starts
v sequence value functions generated monotonically increasing
converges optimal value function v implies v tv v tv b
closer v b v b belief states b
following lemma used later address issues listed end
previous section

lemma consider two value functions v u v uniformly improvable
v u tv u uniformly improvable
proof since v u tv tu lemma condition u tv
consequently u tu u uniformly improvable
corollary value function v uniformly improvable tv

point dp update

point dp update approximation standard dp update designing
point dp update try strike balance quality approximation


fispeeding value iteration pomdps
computational complexity need guarantee modified value iteration
converges

backing witness points input vectors
let v set vectors going perform point dp update
mentioned earlier assume know witness point vector v denote

witness point vector w point dp update first backs
points thereby obtains set vectors specific begins
following subroutine

v
u
v

backup w v

u

w
w

u u fffg
return u
backupwitnesspoints

subroutine line makes sure resulting set u contains duplicates
line takes note fact w witness point w r v

retaining uniform improvability

address convergence issues assume input point dp update
uniformly improvable require output uniformly improvable
explain later assumption facilitated requirement guarantees
convergence modified value iteration subsection discuss
requirement fulfilled
point dp update constructs vectors backing belief points
vectors members v hence output point dp update trivially
dominated v output dominates v must uniformly improvable
lemma question guarantee output dominates v
consider set u resulted backupwitnesspoints dominate v
must exist belief state b u b v b consequently must exist vector
v u b b gives us following subroutine testing whether
u dominates v case adding vectors u
subroutine called backuplppoints belief points found solving linear
programs

u v
v

f
backuplppoints





b

b

dominancecheck
null
backup




u

b v



fizhang zhang




w



b

u u fffg
g b null

subroutine examines vectors v one one v calls another subroutine
try belief point b u b b point found
backs resulting vector line property backup
operator b witness point w r v line cannot vector u
equals consequently vector simply added u without checking duplicates
line process repeats dominancecheck returns null
belief points b u b b backuplppoints terminates
u b b vector v belief point b hence u dominates v
subroutine dominancecheck u first checks whether exists vector u
pointwise dominates states exists returns
null right away otherwise solves following linear program lp u returns
solution point b optimal value objective function positive returns
null otherwise
dominancecheck

u
variables x b state
maximize x
constraints
ps b x ps b b u

ps b b states

lp



complete description point dp update first backs witness
points input vectors solves linear programs identify belief points
backs output dominates input hence uniformly improvable
point dpu v
u backupwitnesspoints v
backuplppoints u v
return u
terms computational complexity point dp update performs exactly jvj
backups first step jt vj backups second step solves linear
programs second step number linear programs solved upper bounded
jt vj jvj usually much smaller bound numbers constraints
linear programs upper bounded jt vj
since b witness w r v ffb v b since v uniformly improvable
v b v b together obvious fact v b b condition b u b
ffb u b consequently cannot vector u equals
actual implementation solution point b used backup even optimal value
objective function negative case duplication check needed



fispeeding value iteration pomdps
several standard dp update among incremental
pruning zhang liu shown ecient
theoretically empirically cassandra et al empirical section reveal
point dp update much less expensive incremental pruning number
test noted however proved
case

stopping point value iteration

consider loop point vi figure starting initial set
vectors generates sequence sets initial set uniformly improvable
value functions represented sets monotonically increasing upper bounded
optimal value function converge value function
necessarily optimal value function question stop loop
straightforward method would compute distance maxb ju b v b j
two consecutive sets u v stop distance falls threshold compute
distance one needs solve juj jvj linear programs time consuming use
metric less expensive compute specific stop loop

max
ju w v w j
u
words calculate maximum difference u v witness points
vectors u stop loop quantity larger
threshold bellman residual terminating value iteration number
experiments set

convergence modified value iteration
let vn vn sets vectors respectively generated vi figure vi figure

line iteration n suppose initial set uniformly improvable lemma
corollary one prove induction vn vn uniformly improvable
n induced value functions increase n moreover vn dominates vn
dominated optimal value function well known vn converges optimal
value function therefore vn must converge optimal value function
question make sure initial set uniformly improvable
following lemma answers question

lemma let mins r c ffc vector whose components
c singleton set fffc g uniformly improvable
proof use v denote value function induced singleton set belief

state b

tv b max
r b



x p zjb v ba
z

z

fizhang zhang
max
r b


x p zjb c
z

max
r b


v b
therefore value function hence singleton set uniformly improvable
experiments section shown vi ecient vi number test
noted however proved case
moreover complexity papadimitriou tsitsiklis implies task
finding optimal policies pomdps pspace complete hence worst case
complexity remain

computing bellman residual

modified value iteration input v standard dp update
uniformly improvable output u dominates input fact used
simplify computation bellman residual matter fact bellman residual
maxb ju b v b j reduces maxb u b v b
compute latter quantity one goes vectors u one one
vector one solves linear program lp v quantity simply maximum
optimal values objective functions linear programs without uniformly
improvability would repeat process one time roles v
u exchanged

empirical discussions

experiments conducted empirically determine effectiveness point
dp update speeding value iteration eight used experiments
literature commonly referred x co cheese x part
painting tiger shuttle network aircraft id obtained files
tony cassandra information sizes summarized following table
jsj jzj jaj
x co

x


tiger


network



jsj jzj jaj
cheese


painting


shuttle


aircraft id



effectiveness point dp update determined comparing standard
value iteration vi modified value iteration vi implementation standard value iteration used experiments borrowed hansen
modified value iteration implemented top hansen code discount factor
set round precision set experiments conducted
ultrasparc ii machine
implementation available request



fispeeding value iteration pomdps
table shows amounts time vi vi took compute optimal policies
test see vi consistently ecient vi especially
larger times faster vi
first seven respectively aircraft id vi able compute
optimal policy less hours vi able produce optimal
policy hours
x co cheese x paint tiger shuttle network aircraft
vi


vi






table time computing optimal policies seconds
statistics given table highlight computational properties
vi explain superior performance numbers standard dp updates carried
vi vi shown rows see vi performed
standard updates test vi performed indicates
point update effective cutting number standard updates
required reach convergence consequence vi spent much less time vi
standard updates row

x co cheese x paint tiger shuttle network
dpu




vi
time



dpu







time







vi pbdpu




time






quality ratio







complexity ratio




table detailed statistics
row shows numbers point updates carried vi see
numbers actually larger numbers standard updates performed vi
expected see recall point update approximation standard
update let v set vectors uniformly improvable use v denote
sets vectors resulted performing point update v belief state b
v b v b v b means point update improves v
much standard update consequently use point update increases total
note times shown include time testing stopping condition



fizhang zhang
number iterations e number standard updates plus number point
updates
intuitively better point update approximation standard update
less difference total number iterations vi vi need take
ratio two numbers used certain extent
measurement quality point update shall refer
quality ratio point update row shows quality ratios seven test
see quality point update fairly good stable across

row shows test ratio average time standard
update performed vi point update performed vi ratios
measure certain extent complexity point update relative standard update
hence referred complexity ratios point update see
predicted analysis section point update consistently less expensive
standard update differences times last four
summary statistics suggest quality point update relative
standard update fairly good stable complexity much lower together
fact point update drastically reduces number standard updates
explain superior performance vi
close section let us note vi finds policies quality close
predetermined criterion vi usually finds much better ones table
vi checks policy quality standard update vi
point updates

x co cheese x paint tiger shuttle network
vi


vi

table quality policies found vi vi

variations point dp update
studied several possible variations point update
ideas drawn existing literature none variations able significantly
enhance effectiveness accelerating value iteration nonetheless brief
discussion still worthwhile discussion provides insights
shows compares related work discussed
detail next section
variations divide two categories aimed improving quality
point update aimed reducing complexity shall discuss one
one
quality policy estimated bellman residual



fispeeding value iteration pomdps

improving quality point dp update
natural way improve quality point update back additional
belief points explored use randomly generated points cassandra
additional product points projected points hauskrecht additional byproduct points refer points generated stages standard update excluding
witness points already used projected points points reachable
one step points given rise useful vectors
table shows test number standard updates amount
time vi took without projected points see use
projected points reduce number standard updates one x co cheese
shuttle however increased time complexity test except network
two kinds points combinations three significantly improve
vi contrary often significantly degraded performance vi
w

w


x co cheese x paint tiger shuttle network aircraft





























table number standard dp updates time vi took without
projected points
close examination experimental data reveals plausible explanation point
update stands already reduce number standard updates
among last two three time consuming possibility
reducing number standard updates low even reduced
effect roughly shift time consuming standard updates earlier consequently
unlikely achieve substantial gains hand use additional points
increases overheads

reducing complexity point dp update
solving linear programs expensive operation point update obvious
way speed avoid linear programs point update solves linear programs
backs belief points found guarantee uniform improvability
linear programs skipped must way guarantee uniform
improvability easy solution suppose v set vectors
try update uniformly improvable let u set obtained v
backing witness points done without solving linear programs
set u might might uniformly improvable however union v u
guaranteed uniformly improvable therefore reprogram point update


fizhang zhang
return union hope reduce complexity resulting variation called
non lp point dp update
another way reduce complexity simplify backup operator section
idea behind modified policy iteration e g puterman backing
set vectors v belief point operator considers possible actions picks
one optimal according v improving policy speed one simply
use action found belief point previous standard update resulting
operator called mpi backup operator mpi stands modified policy
iteration v output previous standard update two actions often
however usually different v several point updates
following standard update
table shows test number standard updates amount
time vi took non lp point update used together standard
backup operator comparing statistics point update tables
see number standard updates increased test
amount time increased except first three plausible
reasons first clear non lp point update improve set vectors
much point update consequently less effective reducing number
standard updates second although solve linear programs non lp point
update produces extraneous vectors means might need deal large
number vectors later iterations hence might ecient point
update
x co cheese x paint tiger shuttle network aircraft














table number standard dp updates time vi took non lp pointbased update used
extraneous vectors pruned matter fact prune vectors
pointwise dominated others hence extraneous experiments inexpensive
pruning extraneous vectors however requires solution linear programs
expensive zhang et al discussed done
ecient way still good table
explored combination non lp point update mpi backup
operator good table reason
mpi backup operator compromises quality point update
quality non lp point update improved gauss seidel
asynchronous update denardo suppose updating set v idea
vector created backup add copy vector set v right away
hope increase components later vectors tested idea preparing
zhang et al found costs almost exceed benefits reason


fispeeding value iteration pomdps
asynchronous update introduces many extraneous vectors synchronous
update
conclusion point conceptually simple clean compared
complex variations seems effective accelerating value iteration

related work
work presented three levels point dp update bottom pointbased value iteration middle modified value iteration top section
discuss previous relevant work three levels

point dp update standard dp update

mentioned section point update closely related several exact standard update namely one pass sondik linear support cheng
relaxed region cheng backup finite number belief points
difference exact generate points systematically
expensive point update generate points heuristically
several exact standard dp update enumeration reduction monahan eagle incremental pruning zhang
liu cassandra et al first generate set vectors parsimonious prune extraneous vectors solving linear programs point dp
update never generates extraneous vectors might generate duplicate vectors however
duplicates pruned without solving linear programs witness kaelbling
et al two stages first stage considers actions one one
action constructs set vectors finite number systematically generated
belief points operator similar backup operator second stage vectors
different actions pooled together extraneous vectors pruned
proposals carry standard update approximately dropping vectors
marginally useful e g kaelbling et al hansen one idea
along line empirically evaluated recall achieve optimality
stopping threshold bellman residual idea drop
marginally useful vectors stages standard update keeping overall
error stop bellman residual falls easy see
optimality still guaranteed way tried start large error
tolerance hope prune vectors gradually decrease tolerance level
reasonable improvements observed especially one need quality
policy high however approximate updates much expensive
point updates context modified value iteration
suitable alternatives standard updates point update

point value iteration value function approximation

point value iteration starts set vectors generates sequence vector
sets repeatedly applying point update last set used approximate
optimal value function


fizhang zhang
methods approximating optimal value function developed
previously compare point value iteration along two dimensions whether map one set vectors another whether
interleaved standard updates whether guarantee convergence interleaved standard updates
lovejoy proposes approximate optimal value function v pomdp
optimal value function underlying markov decision process mdp
latter function state space v approximated one vector
littman et al b extend idea approximate v jaj vectors
corresponds q function underlying mdp extension recently
introduced zubek dietterich idea base approximation
underlying mdp rather called even odd pomdp identical original
pomdp except state fully observable even time steps platzman
suggests approximating v value functions one fixed suboptimal policies
constructed heuristically methods start set vectors
hence map set vectors another however easily adapted
however put predetermined limit number output vectors consequently
convergence guaranteed interleaved standard updates
fast informed bound hauskrecht q function curve fitting littman et al b
softmax curve fitting parr russell map set vectors another however differ drastically point value iteration
ways deriving next set vectors current one regardless size
current set fast informed bound q function curve fitting produces jaj vectors
one action softmax curve fitting number vectors determined
priori although necessarily related number actions methods
interleaved standard dp updates unlike point value iteration
may converge hauskrecht even cases converge
resulting interleaving standard updates necessarily
converge due priori limits number vectors
grid interpolation extrapolation methods lovejoy brafman hauskrecht
b approximate value functions discretizing belief space fixed variable
grid maintaining values grid points values non grid points estimated interpolation extrapolation needed methods cannot interleaved
standard dp updates work sets vectors
grid methods work sets vectors lovejoy method lower
bound optimal value function lovejoy instance falls category
method actually identical point value iteration except way derives
next set vectors current one instead point update backs
grid points regular grid convergence method guaranteed
resulting interleaving standard updates may converge
hauskrecht conducted extensive survey previous value function approximation methods
empirically compared terms among criteria complexity quality would
interesting include point value iteration empirical comparison done
present focus point value iteration speed value iteration
rather value function approximation method



fispeeding value iteration pomdps
incremental linear function method hauskrecht roughly corresponds
variation point value iteration uses non lp point update section
augmented gauss seidel asynchronous update method access
witness point starts purpose backup extreme points belief space
supplement projected points choice points appears poor
leads large number vectors consequently backup process usually stopped
well convergence hauskrecht

previous work related modified value iteration
basic idea modified value iteration vi add two
consecutive standard updates operations inexpensive hope
operations significantly improve quality vector set hence reduce number
standard updates
several previous work fashion differences lie operations inserted standard updates reward revision white
et al constructs iteration second pomdp current set
vectors runs value iteration second pomdp predetermined number steps
output used modify current set vectors resulting set vectors
fed next standard update
reward revision expected speed value iteration let v value function
represented current set vectors second pomdp constructed way
shares optimal value function original pomdp v optimal
one would expect two pomdps similar optimal value functions v
close optimal consequently running value iteration second pomdp
improve current value function inexpensive second
pomdp fully observable
reward revision conceptually much complex vi seems less
ecient according white et al reward revision average reduce
number standard updates computational time tables
see differences vi vi much larger
iterative discretization procedure idp proposed cheng similar
vi two main differences vi uses point update idp uses non lp
point update point update vi backs witness points belief
points found linear programs non lp point update idp backs extreme
points witness regions found products cheng linear support relaxed region

cheng conducted extensive experiments determine effectiveness idp
accelerating value iteration found idp cut number standard updates
much amount time much much less
significant reductions presented tables
hansen policy iteration pi maintains policy form finite state
controller node controller represents vector iteration standard
update performed set vectors represented current policy resulting


fizhang zhang
set vectors used improve current policy improved policy evaluated
solving system linear equations gives rise third set vectors
fed next standard update
compared performance hansen pi vi table shows
test number standard updates amount time took
comparing statistics vi table see pi performed standard
updates vi indicates policy improvement evaluation less effective
point value iteration cutting number standard updates terms
time pi ecient vi first three significantly less ecient

x co cheese x paint tiger shuttle network aircraft














table number standard updates time pi took compute optimal
policies
might possible combine vi pi specific one probably
insert policy improvement evaluation step two point updates pointbased value iteration figure accelerate point value iteration
hence vi possibility benefits yet investigated

conclusions future directions
value iteration popular finding optimal policies pomdps typically performs large number dp updates convergence dp updates
notoriously expensive developed technique called point dp
update reducing number standard dp updates technique conceptually
simple clean easily incorporated existing pomdp value iteration empirical studies shown point dp update drastically
cut number standard dp updates hence significantly speeding value
iteration moreover point dp update compares favorably complex
variations think compares favorably policy iteration
presented still requires standard dp updates limits
capability solving large pomdps one future direction investigate properties
point value iteration approximation another direction
design ecient standard dp updates special currently
exploring latter direction
hansen writings policy improvement includes dp update substep dp update
considered part policy improvement



fispeeding value iteration pomdps

acknowledgments
supported hong kong grants council grant hkust e
authors thank tony cassandra eric hansen sharing us programs
grateful three anonymous reviewers provided insightful comments
suggestions earlier version

references

astrom k j optimal control markov decision processes incomplete
state estimation journal computer system sciences
brafman r heuristic variable grid solution pomdps proceedings
fourteenth national conference artificial intelligence aaai
cassandra r littman l zhang n l incremental pruning
simple fast exact method partially observable markov decision processes
proceedings thirteenth conference uncertainty artificial intelligence
cassandra r exact approximate partially observable
markov decision processes phd thesis department computer science brown
university
cassandra r b survey pomdp applications working notes aaai
fall symposium partially observable markov decision processes
denardo e v dynamic programming applications prentice hall
eagle j n optimal search moving target search path
constrained operations
cheng h partially observable markov decision processes ph
thesis university british columbia
hansen e solving pomdps searching policy space proceedings
fourteenth conference uncertainty artificial intelligence
hauskrecht incremental methods computing bounds partially observable
markov decision processes proceedings fourteenth national conference
artificial intelligence aaai
hauskrecht b control stochastic domains imperfect information phd thesis department electrical engineering computer science
massachusetts institute technology
hauskrecht value function approximations partially observable markov
decision processes journal artificial intelligence


fizhang zhang
littman l cassandra r kaelbling l p ecient dynamicprogramming updates partially observable markov decision processes technical
report cs brown university
littman l cassandra r kaelbling l p b learning policies partially observable environments scaling proceedings fifteenth conference
machine learning
littman l sequential decision making ph thesis department computer science brown university
kaelbling l p littman l cassandra r acting
partially observable stochastic domains artificial intelligence vol
lovejoy w computationally feasible bounds partially observed markov
decision processes operations
lovejoy w suboptimal policies bounds parameter adaptive decision
processes operations
monahan g e survey partially observable markov decision processes theory management science
parr r russell approximating optimal policies partially observable
stochastic domains proceedings fourteenth international joint conference
artificial intelligence
papadimitriou c h tsitsiklis j n complexity markov decision processes
mathematics operations
platzman l k optimal infinite horizon undiscounted control finite probabilistic systems siam journal control optimization
puterman l markov decision processes p heyman j sobel
eds handbooks ms vol elsevier science publishers
smallwood r sondik e j optimal control partially observable
processes finite horizon operations
sondik e j optimal control partially observable markov processes phd
thesis stanford university
sondik e j optimal control partially observable markov processes
infinite horizon operations
white c c iii scherer w solution procedures partially observed
markov decision processes operations
zhang n l lee zhang w method speeding value iteration
partially observable markov decision processes proc th conference
uncertainties artificial intelligence


fispeeding value iteration pomdps
zhang n l w liu model approximation scheme stochastic
domains journal artificial intelligence
zubek v b dietterich g pomdp approximation anticipates need observe appear proceedings pacific rim conference
artificial intelligence pricai lecture notes computer science
york springer verlag




