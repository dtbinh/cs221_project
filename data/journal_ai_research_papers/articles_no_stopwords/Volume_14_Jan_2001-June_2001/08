Journal Artificial Intelligence Research 1 (2001) 231-252

Submitted 3/00; published 5/01

Technical Paper Recommendation: Study Combining
Multiple Information Sources
Chumki Basu

cbasu@cs.rutgers.edu

Haym Hirsh

hirsh@cs.rutgers.edu

Department Computer Science, Rutgers University, 110 Frelinghuysen Road,
Piscataway NJ 08854-8019
Telcordia Technologies, Inc., 445 South Street,
Morristown NJ 07960-6438
Department Computer Science, Rutgers University, 110 Frelinghuysen Road,
Piscataway NJ 08854-8019

William W. Cohen

wcohen@whizbang.com

Craig Nevill-Manning

nevill@cs.rutgers.edu

WhizBang! Labs, WhizBang Labs - East, 4616 Henry Street,
Pittsburgh PA 15213

Department Computer Science, Rutgers University, 110 Frelinghuysen Road,
Piscataway NJ 08854-8019

Abstract

growing need manage exploit proliferation online data sources opening new opportunities bringing people closer resources need. instance,
consider recommendation service researchers receive daily pointers
journal papers fields interest. survey known approaches
problem technical paper recommendation ask extended deal
multiple information sources. specifically, focus variant problem
{ recommending conference paper submissions reviewing committee members {
offers us testbed try different approaches. Using WHIRL { information integration system { able implement different recommendation algorithms derived
information retrieval principles. use novel autonomous procedure gathering
reviewer interest information Web. evaluate approach compare
methods using preference data provided members AAAI-98 conference
reviewing committee along data actual submissions.

1. Introduction
define paper recommendation problem follows:
Given representation interests, find relevant papers.

fact, replace papers definition name artifact
choice, yet another instantiation recommendation problem.
makes paper recommendation interesting?
231

fiBasu, Hirsh, Cohen, & Nevill-Manning

ability automatically filter large set papers find
aligned one's research interests advantages. growing number
publications, many online, dicult keep latest research, even
it's within one's field. timeliness information becoming critical,
desirable paper reach target audience minimal latency. Although
straightforward approach finding relevant papers may look close matches
person's interests paper's content, less clear represent
interests researchers contents papers.
Another feature sets paper recommendation apart variant problem
must dealt regular basis numerous conference chairs. Conferences
offer venue large number fairly specific papers must distributed smaller
number reviewers, within tight timeframe. Even scope problem
constrained degree topic, conference organizers and/or reviewers still must
expend great deal time effort begin reviewing process.
would suggest real value finding ways automating filtering process
would make less burdensome potential consumers.
consider algorithms recommending focused sets technical papers. use
conference reviewing platform explore series questions relating recommendation process. new interest AI community problem
recently since proposed \challenge" task IJCAI-97 (Geller, 1997). focus
conference reviewing turns natural choice since obtain data
set papers, i.e., conference submissions, obtain information
preferences set reviewers submissions. following section, discuss related work addresses conference reviewing problem. consider
work area recommender systems { e.g., recommending articles newsgroup
readers recommending Web pages Web site visitors { contribute task.
However, focus varying sources information data representations,
thereby allowing us formulate different recommendation algorithms based recombine sources computing similarity. show indeed difference
performance vary amount source data, compared baseline
using single source information data representations. compare
recommendation algorithms other, collaborative filtering,
random assignment papers reviewers. apply methods experimental data
involving reviewer preferences conference abstracts AAAI-98 conference. 1

2. Know Paper Recommendation
already know recommending papers reviewers, generally,
arbitrary researcher, trying selective choosing papers ultimately reach consumer based relevance interests expertise. However, finding
papers conference reviewers necessarily complex task, since papers may
assigned reviewers based criteria. instance, reviewer load balancing
con ict-resolution reviewer-author aliations may two criteria. addition,
1. data obtained permission AAAI, AAAI reviewers, appropriate,
authors submitted papers.

232

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

reviewer's reviewing preferences may uenced considerations paper's
readability novelty. example, preference novelty may lead reviewer choose
paper simply relevant interests.
methods suited address latter issues number reasons. First,
confidentiality purposes, lack information related author identity aliation
submitted conference papers. Secondly, since constraint-satisfaction main
concern { primarily interested finding best papers person without
regard whether multiple people receive paper { incorporate
criteria selection procedure. way represent \novelty"
paper respect consumer, thereby means recognizing
it. Finally, methods distinguish notion interest expertise
respect reviewers. general recommendation problem, researcher
may want retrieve papers areas outside expertise, case separate
representation would needed.
Previous work area assigning conference papers reviewers approached
problem one content-based information retrieval. Dumais Nielsen (1992) used
data provided 15 members reviewing committee HYPERTEXT '91 conference. reviewers submitted abstracts papers and/or interests,
provided complete relevance assessments 117 papers submitted conference.
Using information retrieval method known latent semantic indexing (LSI), compared reviewer abstracts submissions, ranking submissions
least similar reviewer. results, noticed, based performance
metric evaluates number relevant articles returned Top 10,
could achieve average 48% improvement using automated methods compared
random assignment articles reviewers.
results encouraging, believe widespread availability online
resources introduces opportunities exploring new issues. reviewers
weren't asked supply interest information? process gleaning reviewer interest
data automated simple methods? well retrieving relevant papers
using \approximation" reviewer interests? automatic collection reviewer
interest information Web, effectively removes reviewer loop,
novel aspect research.
Yarowsky Florian (1999) attempted similar task ACL'99 conference. However, primary focus classification { assignment every paper exactly
one six conference committees. used 92 papers submitted ACL
conference electronic form requested committee members provide representative papers. number papers returned members insucient,
augmented collection papers downloaded online sources. used
content-based retrieval (within context vector-space model (Salton, 1989)) one
routing strategies. main algorithm first computed centroid reviewer
based representative papers computed centroid committee
sum reviewer centroids. Then, paper classified (assigned committee)
computing cosine similarity committee centroids choosing one
highest rank. Amongst approaches, experimented Naive Bayes classifier
assessment similarity reviewing committee members authors cited
233

fiBasu, Hirsh, Cohen, & Nevill-Manning

papers. Based system performance relative human judges
task (evaluated actual assignments provided program chair conference), extrapolated automated methods could effective human judges,
especially cases judges may less experienced.
dealing large conferences several hundred papers covering variety areas, information load even greater conference organizers reviewers
alike. cases, getting evaluative relevance judgments submitted (or even accepted) papers reviewers feasible. (As example, AAAI conference,
reviewers even state preferences papers potentially
review. Instead, stop scanning list soon filled quota
\bids"{ papers expressed interest reviewing.) Therefore, focus building
extensible framework recommendation { defining process whereby systematically incorporate information formulating recommendation algorithms,
purpose generating better recommendations.
Content-based information retrieval, known content-based filtering, popular
recommendation method: consider systems recommend Web pages Syskill &
Webert (Pazzani & Billsus, 1997). number systems WebWatcher
Fab content-based filtering, mainly part hybrid approach
involves collaborative filtering. Whereas content-based filtering looks contents
artifact (e.g., words Web page), collaborative filtering consider
opinions like-minded people respect artifacts. Collaborative filtering
used recommend NetNews articles (Konstan, Miller, Maltz, Herlocker, Gordon,
& Riedl, 1997), movies (Hill, Stead, Rosenstein, & Furnas, 1995; Basu, Hirsh, & Cohen,
1998), music (Cohen & Fan, 2000; Shardanand & Maes, 1995), even jokes (Gupta,
Digiovanni, Narita, & Goldberg, 1999). Since content-based collaborative methods
use data orthogonal one another, opportunities come hybrid
approaches use combinations data. work movie recommendation
provides another example design hybrid system. Hybrid systems exploit data
multiple sources expectation better compensating
limiting factor data sparseness associated single source.
current study, would identify different sources information describe
papers reviewers, expectation individual pieces themselves, along
knowledge combine them, make difference recommendations.
Although share common goal combining data multiple sources
hybrid recommendation approaches, algorithms develop strictly contentbased. evaluative purposes, compare algorithms results
applying collaborative filtering methods set reviewer preferences.

3. Representing Papers Reviewers
approach recommendation represent entity using variety information
sources, enumerate different combinations sources, evaluate effectiveness combinations using ranked-retrieval methods. paper recommendation
problem, two types entities | papers consumers (reviewers,
case). entity, represent salient features entity sequence
234

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

one information sources. addition, need another type information
source relates reviewer paper, namely, reviewers' actual preferences
papers. begin discussion choice information sources |
choices based data typically used assign papers reviewers,
usually provided explicitly papers' authors, choices rely implicit
knowledge mined semi-structured data available Web.

3.1 Paper Information Sources

experiments based compilation submitted abstracts obtained
AAAI AAAI-98 conference. 466 papers submitted conference.
AAAI gave us collection 256 papers use experiments | abstracts 144
accepted papers abstracts 112 papers rejected whose authors
granted AAAI permission provide abstract work. excluded
papers authored authors paper.
submission obtained title, abstract, set user-assigned keywords
prespecified list. Therefore, paper associated set three information sources provided papers' authors. Although one may consider
body paper another source, information available reviewers
(nor us), use source.

3.2 Reviewer Information Sources

far, seen example entity paper represented
multiple information sources mainly composed distinct units title,
abstract, etc. However, another case may want multiply-represent
entity. Consider trying automatically compose representation reviewer's interests.
may try first go reviewer's home page. there, may decide look
around reviewer's papers. sources offer different point-of-view
reviewer's interests, therefore, considered separate unit. focus
sources { reviewer's entry-level home page papers referenced
home page { substitute asking reviewer provide interest information.
believe home pages online papers credible information sources since
likely fair number conference reviewers stated research interests
either sources. Since one paper information sources paper abstract,
decided represent reviewer \abstract interests". case home
pages, entire text reviewers's entry-level home page taken abstract
reviewer's interests. case PostScript files, define abstract first
300 words extracted paper.
extracted information Web using pre-existing utilities. find
reviewers' home pages, fed names aliations members review
committee Ahoy,2 home page finding engine (Shakes, Langheinrich, & Etzioni, 1997).
Ahoy returned least one match, supplied URL starting point
w3mir,3 HTTP service retrieves files contents Web sites. used
2. http://ahoy.cs.washington.edu:6060.
3. http://www.math.uio.no/janl/w3mir.

235

fiBasu, Hirsh, Cohen, & Nevill-Manning

w3mir download HTML files PostScript files accessible entry-level
home page residing site.4 Since person's papers may directly
available one site, additionally retrieved cross-references sites
contained PostScript files, using w3mir. PostScript files converted
ASCII using PreScript (Nevill-Manning, Reed, & Witten, 1998).
PostScript files retrieved reviewer treated uniformly. Although would
desirable attempt future work, make attempt determine
timeliness paper, especially respect reviewer's current interests.
distinguish journal papers, conference papers, even lecture notes.
reason attempt detailed analysis contents
files (e.g., automatically extract titles, abstracts, etc.). Instead, rely heuristics
looking first N words approximate paper's abstract. Although detailed
analysis likely valuable paper recommendation process, immediate goal
obtain gross sense usability various sources semi-structured information.

3.3 Reviewer Preferences
evaluate queries need \ground truth" | set data specifying
papers reviewer selected suitable review. information,
evaluate different approaches perform making choices. note
approximation full set abstracts reviewer might liked
| reviewing process requires reviewer find minimum quota papers,
quota reached, reviewer need look papers find more.
view optimistically yielding close approximation reviewer's full set
preferences would be, since reviewers able peruse abstracts keywords often
attempt inspect least subset papers labeled keywords areas
knowledgeable.
experiments ground truth comes actual preferences stated 122 (of
230) AAAI-98 reviewers gave AAAI permission release preference information papers considered work. point data ects
reviewers' initial preferences reviewing. data papers
reviewers actually received following AAAI reviewer assignment process.
course, one potential limitation data based portion data
may representative entire data conference. example,
preference data approximately half reviewers predicting preferences
collection papers whose distribution skewed towards accepted papers.
issue whether AAAI researchers representative much larger community
researchers large. (We ask similar question user populations
conferences well). However, consider acceptable limitations resulting
use conference reviewing platform paper recommendation.
4. moment, focus PostScript convenience, reason limit
one file format; main constraint able extract words document.

236

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

4. Recommendation Methodology

section, examine collaborative content-based methods recommendation. methods allow us explore use different subsets data described
previous section.

4.1 Recommending Reviewer Paper Information Sources

following sections, outline content-based recommendation framework uses
data describing papers well data describing reviewers make recommendations. reviewer preference data used evaluation purposes, input
recommendation process.
order locate papers closely match reviewer interest data rely ad hoc
similarity metrics commonly used information retrieval community. describe
methods section WHIRL. brief, reviewer compare
given reviewer representation appropriate paper information source(s).
comparisons implemented query returns rank-ordered list
papers. consequently compute precision Top N , proportion papers
returned actually preferred reviewer, query. final score
query average value, computed subset 50 reviewers (from
larger set reviewers gave us permission).
recommendation algorithms take different paper reviewer information sources
inputs. Since data plotted along two dimensions, let Reviewer set
information sources describing reviewers Paper set information sources describing
papers. construct Reviewer Paper matrix entry matrix
score measuring effectiveness using respective sources, (Reviewer ; Paper ),
compute similarity reviewers papers performing ranked-retrieval.
instance, given paper reviewer representations described, construct
2 3 matrix, gives us 6 possible evaluations scores. refer matrix
recommendation sources matrix.
Conceptually, extend recommendation sources matrix along dimension,
considering combinations rows columns. refer augmented matrix
source combinations matrix. define recommendation algorithm
combination method procedure applied one rows/columns source
combinations matrix. introduces another dimension comparison { combination
method { consider looking replicates source combinations matrix.
Now, pose following questions experimental analysis:


j

recommendation algorithms incorporate information lead better performance?
so, method combining data used algorithm make difference?
4.1.1 WHIRL

queries, use WHIRL, system specifically designed informationintegration tasks (Cohen, 1998b; Cohen & Hirsh, 1998). tasks, often necessary manipulate general way information obtained many heterogeneous online
237

fiBasu, Hirsh, Cohen, & Nevill-Manning

sources, potentially data organization terminology. particular,
WHIRL makes possible integrate information decomposed represented
clean, modular way. example, would information home pages
PostScript papers represented separately, using information integration tool
resolve sources information.
WHIRL conventional DBMS extended use ad hoc similarity metrics
developed information retrieval community. Using metrics, reason
pieces text culled heterogeneous sources based similarity values rather
strict equality. WHIRL computes similarity using \vector-space" representation
model text (Salton, 1989). text object represented vector term weights
(where terms stemmed using Porter's algorithm (Porter, 1980)) based
TFIDF weighting scheme. Similarity two vectors computed using cosine
similarity metric. answers query presented rank-ordering generated
tuples, tuples similar pairs attribute fields appearing first.
example, using WHIRL, pose following query:
SELECT Reviewer.Name, Paper.ID
Paper Reviewer
Reviewer.Descriptor SIM Paper.Abstract

query return list reviewer names paper IDs papers whose abstracts
similar reviewer's interest descriptor. Rather returning tuples
descriptor abstract fields identical, would performed traditional
database join, query returns Name ID pairs tuples whose fields contain
similar terms, ordered according decreasing value similarity. advantage
ad hoc joins without requiring textual fields identical one another important
text comes multiple sources thereby may use different terminology.
important perspective comparing relative importance different fields
one another ecient way.
use WHIRL data must stored form WHIRL relations.
data constructed two relations, one representing different information sources.
conference submission, form Paper relation containing id, abstract, keywords,
title. every reviewer, form Reviewer relation contains single tuple
attributes representing reviewer's name representation reviewer's
interests (for example, based reviewer's home page).
far, discussed use WHIRL formulate queries involving single
information source reviewers papers. However, advantage WHIRL
approach lies simplicity extend queries incorporate multiple sources. primary advantage using WHIRL work ease
measure impact conjunctive queries incorporating data multiple sources.
form conjunctive queries adding multiple conditions clause:
SELECT Reviewer.Name, Paper.ID
Paper Reviewer
Reviewer.Descriptor SIM Paper.Abstract
Reviewer.Descriptor SIM Paper.Keywords
238

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

WHIRL clause contains multiple conditions, similarity scores
individual conjuncts combined taking product though
independent probabilities. Since similarity scores independent probabilities,
use convenient way combine scores, albeit one offers straightforward
approach combination previously studied (Cohen, 1998a).
query, WHIRL would assign score ects similarity submitted
paper's abstract reviewer's descriptor, well similarity submitted
paper's keywords reviewer's descriptor.
4.1.2 Combining Information Sources Query Expansion

mean recommendation algorithm combine data multiple information sources? means enumerating information sources used possible
inputs algorithm, defining way use sources compute similarity.
instance, suppose look 1 reviewer source 2 paper sources given collection
reviewers papers. decide whether paper likely interest reviewer,
compute similarity reviewer source paper sources
combine two similarity scores. Alternatively, compute single similarity score
first combining two paper sources single representation computing
similarity respect reviewer source.
idea combining two sources single representation implemeted
appending terms sources. information retrieval, terms relevant sources
often appended baseline representation query process query
reformulation. usually referred query expansion. Since methods bear
resemblance query expansion, make analogy. expansion methods
described following sections. course, prior knowledge
relevance sources, sense, differ information retrieval
implementation query expansion.
compare relative performance recommendation algorithms,
multiple dimensions along compare results. differentiate results
based methods used combine data compute similarity differentiate results based information sources used comparison.
words, set inputs, one method query expansion perform
better another? want compare merit single source, consider
two groups algorithms { include given source input algorithm,
exclude source. simply count number times algorithms
include source outperform algorithms exclude it, determine relative
merit source.
4.1.3 Concatenation Method

One way \add" information new data source append terms appearing
source original WHIRL query. type query, always single
WHIRL conjunct textual fields appearing conjunct \grow"
addition new terms. call method, queryConcat.
239

fiBasu, Hirsh, Cohen, & Nevill-Manning

Suppose, example, start base query previous section
compares reviewer descriptors paper abstracts. Now, suppose want compare
reviewer descriptors paper abstracts paper keywords. One
way use queryConcat method. form new field representing
union words appearing paper abstract paper keywords fields
substitute original query. Let Paper.Descriptor = Paper.Abstract [ Paper.Keywords.
new query is:
SELECT Reviewer.Name, Paper.ID
Paper Reviewer
Reviewer.Descriptor SIM Paper.Descriptor

Similarly, replace Paper.Descriptor clause represent different
combinations fields, Paper.Abstract, Paper.Keywords Paper.Title using union
operator.
4.1.4 Conjunction Method

previously stated, important motivation using WHIRL ability execute
conjunctive queries, use combine information sources recommendation process. type query, instead adding terms particular text field,
add conjuncts original WHERE. refer method reformulating queries
queryConjunct.
enumerate query combinations considered queryConjunct follows.
Using sources queryConcat, begin queries before,
SELECT Reviewer.Name, Paper.ID
Paper Reviewer


now, replacing body clause following:
A: Reviewer.Descriptor SIM Paper.Abstract
K: Reviewer.Descriptor SIM Paper.Keywords
T: Reviewer.Descriptor SIM Paper.Title
AK: Reviewer.Descriptor SIM Paper.Abstract
Reviewer.Descriptor SIM Paper.Keywords
AT: Reviewer.Descriptor SIM Paper.Abstract
Reviewer.Descriptor SIM Paper.Title
KT: Reviewer.Descriptor SIM Paper.Keywords
Reviewer.Descriptor SIM Paper.Title
AKT: Reviewer.Descriptor SIM Paper.Abstract
Reviewer.Descriptor SIM Paper.Keywords
Reviewer.Descriptor SIM Paper.Title

assign labels, (abstract), K (keywords), (title) queries identify
paper sources used. (We use labels comparable fashion queryConcat
method, representing information sources concatenated together.)
240

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

queries, vary source data used represent
reviewers. first variant accounts case reviewer's descriptor contains
words reviewer's home page; second accounts case descriptor
contains union first 300 words extracted PostScript file obtained
reviewer's Web pages.
decided try yet another combination see whether using representations
reviewers would improve performance. simplicity, chose test hypothesis
expanded conjunctive query involving single extra conjunct. constructed
Reviewer table contains two attributes: Papers (consisting abstracts
reviewer's PostScript papers) Homepage (consisting reviewer's home page).
ran queries, additional conjunct appearing
clause:
Reviewer.Homepage SIM Paper.Keywords

chose use Keywords Paper.Descriptor based intuitions paper's
keywords reviewer's homepage would greater number words common.

4.2 Recommending Reviewer Preferences

Since evaluations reviewers common set papers, one approach
recommending papers would take information use collaborative
filtering. note actual conference reviewing problem, collaborative filtering
method assigning papers may practical. Although benefit
using preferences set reviewers study, information generally
available reviewers making selections, thereby making
dicult base predictions preferences others. Nevertheless, worthwhile
measure impact using reviewer preferences purpose recommending papers.
recommendation methodology collaborative filtering approaches implemented follows: reviewer presented recommended paper online
manner. paper presented reviewer tells system paper relevant. was, paper assigned rating 1 paper said rated
positively. paper relevant, assigned rating 0 said rated
negatively. Let Rating (R; P ) represent rating assigned paper P
reviewer R. paper relevant, reviewer provides single relevant
paper positive example order condition future recommendations. Since know
papers liked reviewers, simulate process data
have. experiment two collaborative filtering algorithms: kNN (Hill et al., 1995;
Cohen & Fan, 2000) Extended Direct Bayes (Cohen & Fan, 2000). let P1 ,P2 ,...,P ,1
represent papers previously rated reviewer , 1 trials.
kNN algorithm uses following distance metric locate reviewers, R , closest
current reviewer respect papers already rated:




Dist(R; R0) = jRating (R; P1) , Rating (R0; P1)j + ::: + jRating (R; P ,1) , Rating (R0; P ,1)j




compute score arbitrary paper, P , respect ratings
k closest reviewers, R1,...,R , follows:
k

241

fiBasu, Hirsh, Cohen, & Nevill-Manning

Score(P ) = Rating (R1; P ) + ::: + Rating (R ; P )
k

According methodology, highest scoring paper presented
reviewer next recommendation.
Extended Direct Bayes viewed ad hoc extension direct Bayesian approach recommendation. define R(P ; P ) represent Laplace-corrected estimate
prior probability reviewer give P positive rating. (R(P ; P )
thought measuring \relatedness" two papers.) consider arbitrary
trial let P1 ,P2,...,P ,1 represent papers rated positively
reviewer previous trials consider arbitrary trial t.
use following scoring function rank paper P :


j

j



j



Score(P ) = 1 , ((1 , R(P; P1)) ::: (1 , R(P; P ,1)))


subtrahend expression represents probability P related
P (assuming P 's independent).




4.3 Evaluation Methodology

following sections, evaluate performance recommendation algorithms.
collaborative filtering, compute recommendations reviewer run
positive examples use feedback. reviewer's list recommendations,
measure precision Top N ; gives us proportion items returned
Top N given reviewer actually preferred reviewer. Although
possible use evaluation metrics, compute precision different levels papers
returned since well-suited conference reviewing task. Since reviewer may get
list 10 papers review, would simulate recommending Top
10 papers returned methods. computing precision, measure percentage
papers list would matched reviewer's preferences. metric
commonly used literature. instance, Dumais Nielsen (1992) mostly used
measure, i.e., number relevant articles Top 10, reporting results
since constituted reasonable reviewer load. additionally report results precision
Top 30. kNN algorithm, set k = 10 experiments.
recommendation algorithms seen choice query expansion method
crossed choice input data sources. methods queryConjunct
queryConcat, ran 3 7 queries detailed previous section. resulted 21
runs per reviewer, per method. run returned ordered list paper IDs. run,
measure precision Top N (for N = 10 N = 30). discussion,
refer run using abstracts based reviewer's papers p run. Similarly, h runs
based reviewer's home page. Finally, ph runs combine sources information
(using extra conjunct). results report represent precision values averaged
across reviewers. order us compare performance across different information
sources, need evaluation using population reviewers.
reviewers provided preference data home pages and/or papers available online.
Therefore, performed set runs using 50 reviewers randomly chosen set
242

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

Source(s)
p(Top10)
h(Top10)
ph(Top10)
p(Top30)
h(Top30)
ph(Top30)


0.248
0.210
0.334
0.194
0.169
0.245

K
0.260
0.284
0.304
0.201
0.217
0.219


0.234
0.232
0.332
0.177
0.183
0.233

AK
0.266
0.288
0.312
0.198
0.226
0.224


0.274
0.270
0.342
0.195
0.199
0.241

KT
0.308
0.320
0.286
0.220
0.232
0.211

AKT
0.330
0.332
0.374
0.232
0.232
0.249

Table 1: Average Precision Scores Top 10 Top 30 Papers Returned using queryConjunct.
reviewers home pages papers available online, report results averaged
across 50 reviewers.
mentioned earlier, reviewer choices may uenced variety factors
ranging person's curiosity paper's readability. Many factors dicult
model. Furthermore, human judges may assign papers reviewers according criteria
relevance paper contents reviewer interests, individual opinions
may vary. Therefore, highly unlikely proposed methods achieve 100%
precision. Unfortunately, given nature problem, able get
assessment human judges would done task. Nevertheless,
evaluate recommendation framework built content-based information retrieval
principles compare relative performance reasonable baseline approaches.

5. Results
number questions would keep mind analyze results.
course experiments vary amount information input
algorithms method query expansion used algorithms. One questions
would answer algorithm set algorithms suited task
hand? ask whether choice inputs results measurable differences
performance. tabulation results provides basis analyzing contentbased algorithms presented Table 1 Table 2. baseline method
compare algorithms random assignment. method assigns reviewer random
collection papers. method, expect precision 7.0%. words,
means select papers randomly, average, reviewer would
fewer 1 14 papers selected.
Table 1 Table 2 replicates source combinations matrix discussed
earlier. Since ran two trials Top N papers returned, table actually
concatenated representation matrices Top 10 Top 30 experiments.
first three rows Table 1 Table 2, report precision figures Top 10
papers returned queryConjunct method queryConcat method, respectively.
243

fiBasu, Hirsh, Cohen, & Nevill-Manning

Precision Top 10 Top 30
0.4
compare.dat
x

0.38
0.36
0.34
queryConjunct method

0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
queryConcat method

0.35

0.4

Figure 1: Comparison Two Query Methods
Similarly, show results Top 30 papers returned bottom three rows
tables. Since view rows representing reviewer sources used query
columns representing paper sources, measure impact adding data
two ways. reading across row, across groups columns representing N information
sources, gauge results vary paper data included queries.
Similarly, reading column, gauge differences results
reviewer data included queries.
Given information, say performance recommendation
algorithms used different methods query expansion? compare relative
performance two methods queryConjunct queryConcat based values listed
Table 1 Table 2. Note cases performance methods exceeds
random selection, accuracies factor 2 5 times better. Figure 1,
record information data point every query uses two sources
information (since methods differ combine data two sources,
meaningless plot points refer queries using single source). figure,
x-axis represents queries expanded using queryConcat method y-axis represents
queries expanded using queryConjunct method. point falls x = line,
two methods yielded performance query using information
sources. points fall area x = line mark queries
queryConjunct higher precision queryConcat. data reveal almost
cases, queryConjunct higher precision queryConcat, thereby making queryConjunct
dominant two query expansion methods preferred method two
task hand.
expectation increase source data notice increase
precision. Specifically, note queryConjunct, query uses
information paper submission majority cases performs statistically significantly
244

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

Source(s)
p(Top10)
h(Top10)
ph(Top10)
p(Top30)
h(Top30)
ph(Top30)


0.248
0.210
0.258
0.194
0.169
0.197

K
0.260
0.284
0.272
0.201
0.217
0.212


0.234
0.232
0.242
0.177
0.183
0.180

AK
0.264
0.226
0.262
0.202
0.179
0.203


0.266
0.226
0.260
0.202
0.179
0.203

KT
0.276
0.308
0.300
0.201
0.199
0.209

AKT
0.266
0.222
0.274
0.209
0.184
0.211

Table 2: Average Precision Scores Top 10 Top 30 Papers Returned using queryConcat.
better5 queries use less information case performs statistically significantly worse.
note adding information always lead monotonically better
results. Notice queryConjunct, case Top 30 papers returned, hKT indistinguishable hAKT. note phT performs better (though statistically
significantly better) phKT. similar cases queryConcat. explain gaps? indeed gaps, i.e., true statistical differences,
may consider explanation adding information may increasing amount
noise representations. Consider, example, keywords fixed list
often poor match real subject matter paper. special cases, use
keywords source could lead degradation retrieval performance.
Analogous analysis paper sources, examine column
Table 1 Table 2 measure effect adding information reviewer representation. queryConjunct, majority time, find queries incorporating
information (ph entries) perform statistically significantly better single source
queries (p h entries).
far, illustrated move across groups columns blocks
rows source combinations matrix, adding sources queries
improvement. significant gains realize this? Focusing
queryConjunct, every reviewer source, consider queries contained data
single paper source lowest precision. pair queries
corresponding query row matrix made use paper sources
report resulting improvement precision Table 3. Top 10 results,
note best case, gain improvement precision 58% going
single-source multi-source query, Top 30 results, gain improvement
5. comparisons two queries Qi Qj made using two-tailed sign test. Specifically,
consider set Rij reviewers r precision(Qi ; r) 6= precision(Qj ; r) estimate
probability
pij = P rob(precision(Qi ; r ) > precision(Qj ; r ) j r 2 Rij )
consider difference statistically significant one reject confidence > 0.95 null
hypothesis pij generated j Rij j independent ips fair coin.

245

fiBasu, Hirsh, Cohen, & Nevill-Manning

Single-Source Queries Improvement Adding Two Sources
pT(Top 10)
41%
hA(Top 10)
58%
phK(Top 10)
23%
pT(Top 30)
31%
hA(Top 30)
37%
phK(Top 30)
14%

Table 3: Comparision Single-Source vs. Multi-Source Queries.
Methods(s)

Top 10 Top 30
kNN
0.294 0.154
ExtendedDirectBayes 0.300 0.129

Table 4: Average Precision Scores Top 10 Top 30 Papers Returned using Collaborative Filtering Methods.
37%. results support intuitions incorporating information
queries, quality retrieval results improves. Since different paper
source single-source queries row Table 3, note impact
given paper source dependent reviewer representation use.
still come assessment sources significant conference reviewing task? queryConjunct, present series figures (Figure 2 Figure 6)
illustrate impact source plotting precision values queries exclude
source along x-axis precision values queries include source along
y-axis (for N = 10 N = 30). point falls x = line, queries
exactly performance | choice source irrelevant. points fall
area x = line mark queries higher precision compared
query counterparts contain source.
simply counting number times queries include source outperform
queries include source, one way ranking sources
decreasing order importance. case, queries include abstract source
papers home page source reviewers highest rates success (when
compared information sources papers reviewers, respectively).
Now, natural question ask whether trends noticed queryConjunct
hold queryConcat. answer no, means queryConcat
give us definitive answer question whether information really better.
noticed query performance linked reviewer paper
sources, find linked query expansion method.
246

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

Precision Top 10 Top 30
0.4
A.dat
x

0.38
0.36
0.34

Queries Abstract

0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Abstract

0.35

0.4

Figure 2: Role Abstract Information Source

Precision Top 10 Top 30
0.4
K.dat
x

0.38
0.36

Queries Keywords

0.34
0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Keywords

0.35

0.4

Figure 3: Role Keywords Information Source

247

fiBasu, Hirsh, Cohen, & Nevill-Manning

Precision Top 10 Top 30
0.4
T.dat
x

0.38
0.36
0.34

Queries Titles

0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Titles

0.35

0.4

Figure 4: Role Title Information Source

Precision Top 10 Top 30
0.4
P.dat
x

0.38
0.36

Queries Reviewer Papers

0.34
0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Reviewer Papers

0.35

0.4

Figure 5: Role Papers Information Source

248

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

Precision Top 10 Top 30
0.4
H.dat
x

0.38

Queries Reviewer Homepages

0.36
0.34
0.32
0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16
0.15

0.2

0.25
0.3
Queries without Reviewer Homepages

0.35

0.4

Figure 6: Role Homepage Information Source
Table 4, show results collaborative filtering runs. report averages
precision values computed Top N (for N =10 N =30) papers returned
based reviewer recommendation lists. Since stop recommending
exhausted set positive examples reviewer, reviewer recommendation lists
varying lengths. cases size list less N , still compute
precision Top N , assuming remaining items incorrect predictions. methods
collaborative filtering exceed random selection significant margin.
Top 10 papers returned, collaborative recommendation methods competitive best performance queryConcat. already interesting observation,
since methods differ, method using different data make recommendations. state use queryConjunct information
sources recommend 10 papers, average almost four papers coincide reviewer's
preferences. Compared random selection, collaborative filtering, queryConcat,
method yields papers interest reviewers.
summary, learned experiments? found within
context peer reviewing papers, make recommendation process less
\people intensive". recommendation systems require users provide samples
preferences used extrapolate future behaviors. Collaborative
methods go even using preference information across multiple users predict
preferences single user. automatically collecting reviewer interest information
Web sources precomputing similarities profiles paper content,
require less input reviewers. Furthermore, content-based retrieval methods
exceed performance collaborative methods task.
believe recommendation framework provides extensible way
formulating queries provides control information content queries.
control much information include queries
incorporate information. new data become available, evaluate data
249

fiBasu, Hirsh, Cohen, & Nevill-Manning

sources and/or combinations effective, thereby fine-tuning query formulation
process.

6. Related Work Query Reformulation

Since work expanding queries using WHIRL viewed type query
reformulation, review related work information retrieval community
topic. Salton (1989) describes process query reformulation \moving"
given query towards relevant items away nonrelevant ones. context
vector-space model retrieval, means given query expression form
(Salton, 1997):
Q 0 = (q 1 ; q 2 ; :::; qt)

q number 0 1 representing weight assigned term , want
arrive new query expression:




Q 0 = (q 1 ; q 2 ; :::; qt)
0

0

0

0

weights adjusted new terms introduced vector
representation, terms effectively removed reducing respective
weights 0.
Harman (1992) describes operational procedure underlying process merging document query vectors. specifically, means query terms
original query appearing relevant documents added initial query
expression. expansion occurs using positive negative weights, depending
whether terms appears relevant non-relevant document.
description assumes relevance judgments documents
system return. Practically speaking, type information hard come by.
Therefore, people seeking compensate lack information expanding
queries using variety techniques use thesauri relevance feedback.
latter case, query reformulation part iterative interactive process whereby
users presented results retrieval asked supply feedback regarding
relative importance results.
Comparing approach methods query reformulation, make couple
observations. First, query reformulation driven knowledge precomputed
data colection. Given entities papers abstracts, keywords,
titles, make sense vary amount information queries?
equivalent Table 3 collection, table lookup run time determine
formulations promising.
note way construct queries queryConjunct method combines
aspects Boolean vector-space models query formulation hybrid approach. case Boolean queries, relevance feedback lead new query expressions
consisting term conjuncts (Salton, 1997):
(Term Term Term )
Notice replace Term Vector expression,
query expression formulated according queryConjunct method.


j



k



250

fiTechnical Paper Recommendation: Study Combining Multiple Information Sources

7. Conclusions

paper, shown collect information reviewers automatically
Web, use part recommendation framework route papers
reviewers. treat problem one decomposing reviewer interest paper
contents information sources, combining information sources using
different query formulations. experiments, compared two ways formulating
queries using content-based information retrieval one collaborative approach.
found recommendation algorithm using conjunctive queries outperforms
approaches. looked using different subsets information sources
algorithms, case optimal algorithm, found using information
generally lead better performance.
practical setting, recommendation method choice likely depend
number factors ranging availability information ease use. one
hand, framework provides exible alternative simple keyword-based searches
less intrusive alternative collaborative methods. hand, methods
assume obtain data reliable, accurate, timely. Based results,
optimistic Web provide credible information sources used
successfully recommendation process.

8. Acknowledgments

extend thanks AAAI, AAAI reviewers, AAAI paper authors, members
Rutgers Machine Learning Research Group, reviewers paper
inputs work.
note following property respective companies listed:
WHIRL (AT&T Labs { Research), LSI (Telcordia Technologies, Inc.).

References

Basu, C., Hirsh, H., & Cohen, W. (1998). Recommendation classification: Using social
content-based information recommendation. Proceedings AAAI-98.
Cohen, W. (1998a). Integration heterogeneous databases without common domains using
queries based textual similarity. Proceedings ACM SIGMOD-98.
Cohen, W. (1998b). whirl approach information integration. IEEE Intelligent
Systems. IEEE Press.
Cohen, W., & Fan, W. (2000). Web-collaborative filtering: Recommending music crawling web. Proceedings WWW-2000.
Cohen, W., & Hirsh, H. (1998). Joins generalize: Text classification using whirl.
Proceedings KDD-98.
Dillon, M., & Desper, J. (1980). Automatic relevance feedback boolean retrieval systems.
Journal Documentation, 36.
251

fiBasu, Hirsh, Cohen, & Nevill-Manning

Dumais, S., & Nielsen, J. (1992). Automating assignment submitted manuscripts
reviewers. Proceedings ACM SIGIR-92.
Geller, J. (1997). Challenge: ijcai 1999 prove value ai using ai.
Proceedings IJCAI-97.
Gupta, D., Digiovanni, M., Narita, H., & Goldberg, K. (1999). Jester 2.0: new lineartime collaborative filtering algorithm applied jokes. Workshop Recommender
Systems ACM SIGIR-99.
Harman, D. (1992). Relevance feedback revisited. Proceedings ACM SIGIR-92.
Hill, W., Stead, L., Rosenstein, M., & Furnas, G. (1995). Recommending evaluating
choices virtual community use. Proceedings CHI-95.
Konstan, J., Miller, B., Maltz, D., Herlocker, L., Gordon, L., & Riedl, J. (1997). Grouplens:
Applying collaborative filtering usenet news.. Vol. 40.
Nevill-Manning, C., Reed, T., & Witten, I. (1998). Extracting text postscript. Software
Practice Experience, 28 (5).
Pazzani, M., & Billsus, D. (1997). Learning revising user profiles: identification
interesting web sites. Machine Learning, 27, 313{331.
Porter, M. (1980). algorithm sux stripping. Program, 14, 130{137.
Salton, G. (1989). Automatic Text Processing. Addison Wesley.
Salton, G. (1997). Improving retrieval performance relevance feedback. Readings
Information Retrieval.
Shakes, J., Langheinrich, M., & Etzioni, O. (1997). Dynamic reference sifting: case study
homepage domain. Proceedings WWW-97.
Shardanand, U., & Maes, P. (1995). Social information filtering: Algorithms automating
\word mouth". Proceedings CHI-95.
Yarowsky, D., & Florian, R. (1999). Taking load conference chairs: towards
digital paper-routing assistant. Proceedings 1999 Joint SIGDAT Conference
Empirical Methods NLP Very-Large Corpora.

252


