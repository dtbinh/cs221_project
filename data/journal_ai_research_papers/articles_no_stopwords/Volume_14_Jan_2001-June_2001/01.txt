Journal Artificial Intelligence Research 14 (2001) 29-51

Submitted 6/00; published 2/01

Speeding Convergence Value Iteration
Partially Observable Markov Decision Processes
Nevin L. Zhang
Weihong Zhang

Department Computer Science
Hong Kong University Science & Technology
Clear Water Bay Road, Kowloon, Hong Kong, CHINA

lzhang@cs.ust.hk
wzhang@cs.ust.hk

Abstract

Partially observable Markov decision processes (POMDPs) recently become popular among many AI researchers serve natural model planning
uncertainty. Value iteration well-known algorithm finding optimal policies
POMDPs. typically takes large number iterations converge. paper proposes
method accelerating convergence value iteration. method evaluated array benchmark problems found effective: enabled
value iteration converge iterations test problems.

1. Introduction
POMDPs model sequential decision making problems effects actions nondeterministic state world known certainty. attracted
many researchers Operations Research Artificial Intelligence potential applications wide range areas (Monahan 1982, Cassandra 1998b), one
planning uncertainty. Unfortunately, still significant gap potential actual applications, primarily due lack effective solution methods.
reason, much recent effort devoted finding ecient algorithms POMDPs
(e.g., Parr Russell 1995, Hauskrecht 1997b, Cassandra 1998a, Hansen 1998, Kaelbling
et al. 1998, Zhang et al. 1999).
Value iteration well-known algorithm POMDPs (Smallwood Sondik 1973,
Puterman 1990). starts initial value function iteratively performs dynamic
programming (DP) updates generate sequence value functions. sequence converges optimal value function. Value iteration terminates predetermined
convergence condition met.
Value iteration performs typically large number DP updates converges
DP updates notoriously expensive. paper, develop technique reducing
number DP updates.
DP update takes (the finite representation of) value function input returns (the
finite representation of) another value function. output value function closer
optimal input value function. sense, say DP update improves
input. propose approximation DP update called point-based DP update. Pointbased DP update improves input, possibly lesser degree standard DP
update. hand, computationally much cheaper. value iteration,
c 2001 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiZhang & Zhang
perform point-based DP update number times two standard DP updates.
number standard DP updates reduced way since point-based DP update
improves input. reduction come high cost since point-based DP
update takes little time.
rest paper organized follows. next section shall give brief
review POMDPs value iteration. basic idea behind point-based DP update
explained Section 3. theoretical preparations Section 4, shall work
details point-based DP update Section 5. Empirical results reported
Section 6 possible variations evaluated Section 7. Finally, shall discuss related
work Section 8 provide concluding remarks Section 9.

2. POMDPs Value Iteration
2.1 POMDPs
partially observable Markov decision process (POMDP) sequential decision model
agent acts stochastic environment partial knowledge
state environment. set possible states environment referred
state space denoted . point time, environment one
possible states. agent directly observe state. Rather, receives
observation it. denote set possible observations Z . receiving
observation, agent chooses action set possible actions executes
action. Thereafter, agent receives immediate reward environment evolves
stochastically next state.
Mathematically, POMDP specified by: three sets , Z , A; reward function
r(s; a); transition probability function P (s0 js; a); observation probability function
P (z js0 ; a). reward function characterizes dependency immediate reward
current state current action a. transition probability characterizes
dependency next state s0 current state current action a.
observation probability characterizes dependency observation z next time
point next state s0 current action a.

2.2 Policies Value Functions
Since current observation fully reveal identity current state, agent
needs consider previous observations actions choosing action. Information current state contained current observation, previous observations,
previous actions summarized probability distribution state space
(Astrom 1965). probability distribution sometimes called belief state denoted
b. possible state s, b(s) probability current state s. set
possible belief states called belief space. denote B.
policy prescribes action possible belief state. words,
mapping B A. Associated policy value function V . belief
state b, V (b) expected total discounted reward agent receives following
30

fiSpeeding Value Iteration POMDPS
policy starting b,

1
X
r ];

V (b) = E;b[

t=0



(1)

rt reward received time
(0<1) discount factor. known
exists policy V (b)V (b) policy belief
state b (Puterman 1990). policy called optimal policy. value function
optimal policy called optimal value function. denote V . positive
number , policy -optimal

V (b) + V (b) 8b 2 B:

2.3 Value Iteration

explain value iteration, need consider belief state evolves time. Let b
current belief state. belief state next point time determined
current belief state, current action a, next observation z . denote baz .
state s0 , baz (s0 ) given

baz (s0 ) =

Ps P (z; s0js; a)b(s)

;
(2)
P (z jb; a)
P
P (z; s0 js; a)=P (z js0 ; a)P (s0 js; a) P (z jb; a)= s;s0 P (z; s0 js; a)b(s) renormal-

ization constant. notation suggests, constant interpreted
probability observing z taking action belief state b.
Define operator takes value function V returns another value function
TV follows:
X
TV (b) = max
[r(b; a) + P (z jb; a)V (baz )] 8b 2 B
(3)

z

P
r(b; a) = r(s; a)b(s) expected immediate reward taking action belief
state b. given value function V , policy said V -improving
X P (zjb; a)V (ba)] 8b 2 B:
(b) = arg max
[
r
(
b;

)
+

z

z

(4)

Value iteration algorithm finding -optimal policies. starts initial
value function V0 iterates using following formula:
Vn = TVn,1 :
known (e.g., Puterman 1990, Theorem 6.9) Vn converges V n goes
infinity. Value iteration terminates Bellman residual maxb jVn (b) , Vn,1 (b)j falls
(1 , )=2. does, Vn -improving policy -optimal (e.g., Puterman 1990).
Since infinitely many belief states, value functions cannot explicitly represented. Fortunately, value functions one encounters process value iteration
admit implicit finite representations. explaining why, first introduce several technical concepts notations.
31

fiZhang & Zhang

1
2
3
4

(1, 0)

(0, 1)

Figure 1: Illustration Technical Concepts.

2.4 Technical Notational Considerations

convenience, view functions state space vectors size jSj. use lower
case Greek letters refer vectors script letters V U refer sets
vectors. contrast, upper case letters V U always refer value functions,
functions belief space B. Note belief state function state space
hence viewed vector.
set V vectors induces value function follows:

f (b) = max
ffb 8b 2 B;
ff2V

P

ffb inner product b, ffb= ff(s)b(s). convenience,
shall abuse notation use V denote set vectors value function induced
set. convention, quantity f (b) written V (b).
vector set extraneous removal affect function set
induces. useful otherwise. set vectors parsimonious contains extraneous
vectors.
Given set V vector V , define open witness region R(ff; V ) closed
witness region R(ff; V ) w.r.t V regions belief space B respectively given

R(ff; V ) = fb 2 Bjffb > ff0 b; 8ff0 2 Vnfffgg
R(ff; V ) = fb 2 Bjffb ff0 b; 8ff0 2 Vnfffgg
literature, belief state open witness region R(ff; V ) usually called witness
point since testifies fact useful. paper, shall call belief
state closed witness region R(ff; V ) witness point ff.
Figure 1 diagrammatically illustrates aforementioned concepts. line
bottom depicts belief space POMDP two states. point left end
represents probability distribution concentrates masses one states,
point right end represents one concentrates masses
state. four vectors ff1 , ff2 , ff3 , ff4 . four slanting lines represent
32

fiSpeeding Value Iteration POMDPS

V ; ):

VI(

1.
2.
3.
4.
5.
6.
7.



(1 , )=2.

f
U

DP-UPDATE(V ).
maxb jU (b) , V (b)j;
(r > ) V U .
g ( r > ).
return U .

r

Figure 2: Value Iteration POMDPs.
linear functions ffi b (i=1; 2; 3; 4) b. value function induced four vectors
represented three bold line segments top. Vector ff3 extraneous
removal affect induced function. vectors useful. first
segment line bottom witness region ff1 , second segment
ff2 , last segment ff4 .

2.5 Finite Representation Value Functions Value Iteration

value function V represented set vectors equals value function induced
set. value function representable finite set vectors,
unique parsimonious set vectors represents function (Littman et al. 1995a).
Sondik (1971) shown value function V representable finite set
vectors, value function TV . process obtaining parsimonious
representation TV parsimonious representation V usually referred
dynamic programming (DP) update. Let V parsimonious set vectors represents
V . convenience, use V denote parsimonious set vectors represents
TV .
practice, value iteration POMDPs carried directly terms value
functions themselves. Rather, carried terms sets vectors represent
value functions (Figure 2). One begins initial set vectors V . iteration,
one performs DP update previous parsimonious set V vectors obtains new
parsimonious set vectors U . One continues Bellman residual maxb jU (b) ,V (b)j,
determined solving sequence linear programs, falls threshold.

3. Point-Based DP Update: Idea
section explains intuitions behind point-based DP update. begin
so-called backup operator.

3.1 Backup Operator
Let V set vectors b belief state. backup operator constructs new

vector three steps:

33

fiZhang & Zhang
1. action observation z , find vector V maximum inner
product bza | belief state case z observed executing action
belief state b. one vector, break ties lexicographically
(Littman 1996). Denote vector found fia;z .
2. action a, construct vector fia by:
X
fia (s) = r(s; a) + P (s0 ; zjs; a)fia;z (s0); 8s 2 :
z;s0

3. Find vector, among fia 's, maximum inner product b.
one vector, break ties lexicographically. Denote vector found
backup(b; V ).
shown (Smallwood Sondik 1973, Littman 1996) backup(b; V )
member V | set vectors obtained performing DP update V . Moreover, b
witness point backup(b; V ).
fact corner stone several DP update algorithms. one-pass
algorithm (Sondik 1971), linear-support algorithm (Cheng 1988), relaxed-region
algorithm (Cheng 1988) operate following way: first systematically search
witness points vectors V obtain vectors using backup operator.
witness algorithm (Kaelbling et al. 1998) employs similar idea.

3.2 Point-Based DP Update

Systematically searching witness points vectors V computationally expensive. Point-based DP update this. Instead, uses heuristics come
collection belief points backs points. might miss witness points
vectors V hence approximation standard DP update.
Obviously, backing different belief states might result vector.
words, backup(b; V ) backup(b0 ; V ) might equal two different belief states b
b0. such, possible one gets vectors many backups. One issue
design point-based DP update avoid this. address issue using witness
points.
Point-based DP update assumes one knows witness point vector
input set. backs points.1 rationale witness points vectors
given set \scatter belief space" hence chance creating duplicate
vectors low. experiments confirmed intuition.
assumption made point-based DP update reasonable input
either output standard DP update another point-based DP update. Standard
DP update computes, by-products, witness point output vectors.
seen later, point-based DP update shares property design.

3.3 Use Point-Based DP Update

indicated introduction, propose perform point-based DP update number
times two standard DP updates. specific, propose modify
1. seen later, point-based DP update backs points.

34

fiSpeeding Value Iteration POMDPS

VI1(

1.
2.
3.
4.
5.
6.
7.

V ; ):



(1 , )=2.

f
U

DP-UPDATE(V ).
maxb jU (b) , V (b)j;
(r > ) V POINT-BASED-VI(U ; ).
g ( r > ).
return U .

r

U ; ):
1. f
2.
V U.
3.
U POINT-BASED-DPU(V )
4. g (STOP(U ; V ; )= false).
5. return V .
POINT-BASED-VI(

Figure 3: Modified Value Iteration POMDPs.
value iteration way shown Figure 3. Note change line
5. Instead assigning U directly V , pass subroutine POINT-BASED-VI
assign output subroutine V . subroutine functions way
value iteration, except performs point-based DP updates rather standard DP
updates. Hence call point-based value iteration.
Figure 4 illustrates basic idea behind modified value iteration contrast value
iteration. initial value function properly selected,2 sequence value functions produced value iteration converges monotonically optimal value function.
Convergence usually takes long time partially standard DP updates, indicated
fat upward arrows, computationally expensive. Modified value iteration interleaves
standard DP updates point-based DP updates, indicated thin upward
arrows. Point-based DP update improve value function much standard DP
update. However, complexity much lower. consequence, modified value iteration
hopefully converge less time.
idea interleaving standard DP updates approximate updates back
finite number belief points due Cheng (1988). work differs Cheng's
method mainly way select belief points. detailed discussion differences
given Section 8.
modified value iteration algorithm raises three issues. First, stopping criterion
use point-based value iteration? Second, guarantee stopping
criterion eventually satisfied? Third, guarantee convergence
modified value iteration algorithm itself? address issues, introduce concept
uniformly improvable value functions.
2. show Section 5.5.

35

fiZhang & Zhang

.
.
.

.
.
.

standard update

point-based update

Value iteration

Modified value iteration

Figure 4: Illustration Basic Idea behind Modified Value Iteration.

4. Uniformly Improvable Value Functions

Suppose V U two value functions. say U dominates V write V U
V (b)U (b) every belief state b. value function V said uniformly improvable
V TV . set U vectors dominates another set V vectors value function induced
U dominates induced V . set vectors unformly improvable value
function induces is.

Lemma 1 operator isotone sense two value functions V
U , V U implies TV TU . 2
lemma obvious well known MDP community (Puterman 1990).
Nonetheless, enables us explain intuition behind term \uniformly improvable".
Suppose V uniformly improvable value function suppose value iteration starts
V . sequence value functions generated monotonically increasing
converges optimal value function V . implies V TV V . is, TV (b)
closer V (b) V (b) belief states b.
following lemma used later address issues listed end
previous section.

Lemma 2 Consider two value functions V U . V uniformly improvable
V U TV , U uniformly improvable.
Proof: Since V U , TV TU Lemma 1. condition U TV .
Consequently, U TU . is, U uniformly improvable. 2
Corollary 1 value function V uniformly improvable, TV . 2

5. Point-Based DP Update: Algorithm

Point-based DP update approximation standard DP update. designing
point-based DP update, try strike balance quality approximation
36

fiSpeeding Value Iteration POMDPS
computational complexity. need guarantee modified value iteration
algorithm converges.

5.1 Backing Witness Points Input Vectors
Let V set vectors going perform point-based DP update.
mentioned earlier, assume know witness point vector V . Denote

witness point vector w(ff). Point-based DP update first backs
points thereby obtains new set vectors. specific, begins
following subroutine:

V ):
1. U ;.
2. 2 V
3.
backup(w(fi ); V ).
4.
2= U
5.
w(ff)
w(fi ).
6.
U U [ fffg.
7. return U .
backUpWitnessPoints(

subroutine, line 4 makes sure resulting set U contains duplicates
line 5 takes note fact w(fi ) witness point (w.r.t V ).

5.2 Retaining Uniform Improvability

address convergence issues, assume input point-based DP update
uniformly improvable require output uniformly improvable.
explain later assumption facilitated requirement guarantees
convergence modified value iteration algorithm. subsection, discuss
requirement fulfilled.
Point-based DP update constructs new vectors backing belief points
new vectors members V . Hence output point-based DP update trivially
dominated V . output dominates V , must uniformly improvable
Lemma 2. question guarantee output dominates V .
Consider set U resulted backUpWitnessPoints. dominate V ,
must exist belief state b U (b)<V (b). Consequently, must exist vector
V U (b)<fi b. gives us following subroutine testing whether
U dominates V for, case, adding vectors U does.
subroutine called backUpLPPoints belief points found solving linear
programs.

U ; V ):
1. 2 V
2.
f
backUpLPPoints(

3.
4.
5.

b

b 6

dominanceCheck(
= NULL,
backup(
).



fi; U ).

b; V

37

fiZhang & Zhang
6.
7.
8.

w(

ff)

b.

U U [ fffg.
g (b 6= NULL).

subroutine examines vectors V one one. V , calls another subroutine
try find belief point b U (b)<fi b. point found,
backs it, resulting new vector (line 5). property backup
operator, b witness point w.r.t V (line 6). cannot vector U
equals ff.3 Consequently, vector simply added U without checking duplicates
(line 7). process repeats dominanceCheck returns NULL,
belief points b U (b)<fi b. backUpLPPoints terminates,
U (b)fi b vector V belief point b. Hence U dominates V .
subroutine dominanceCheck(fi; U ) first checks whether exists vector U
pointwise dominates , ff(s)fi (s) states s. exists, returns
NULL right away. Otherwise, solves following linear program LP(fi; U ). returns
solution point b optimal value objective function positive returns
NULL otherwise:4
dominanceCheck

fi; U ):
1. Variables: x, b(s) state
2. Maximize: x.
3. Constraints:
Ps fi(s)b(s) x+ Ps ff(s)b(b) ff2U
4.
Ps b(s) = 1, b(s) 0 states s.
5.
LP(

5.3 Algorithm

complete description point-based DP update. first backs witness
points input vectors. Then, solves linear programs identify belief points
backs output dominates input hence uniformly improvable.
POINT-BASED-DPU(V ):
1. U backUpWitnessPoints(V )
2. backUpLPPoints(U ; V )
3. return U .
terms computational complexity, point-based DP update performs exactly jVj
backups first step jT Vj backups second step. solves linear
programs second step. number linear programs solved upper bounded
jT Vj+jVj usually much smaller bound. numbers constraints
linear programs upper bounded jT Vj + 1.
3. Since b witness w.r.t V , ffb=T V (b). Since V uniformly improvable,
V (b)V (b). Together obvious fact V (b)fi b condition b>U (b),
ffb>U (b). Consequently, cannot vector U equals ff.
4. actual implementation, solution point b used backup even optimal value
objective function negative. case, duplication check needed.

38

fiSpeeding Value Iteration POMDPS
several algorithms standard DP update. Among them, incremental
pruning algorithm (Zhang Liu 1997) shown ecient
theoretically empirically (Cassandra et al. 1997). Empirical results (Section 6) reveal
point-based DP update much less expensive incremental pruning number
test problems. noted, however, proved always
case.

5.4 Stopping Point-Based Value Iteration

Consider do-while loop POINT-BASED-VI (Figure 2). Starting initial set
vectors, generates sequence sets. initial set uniformly improvable,
value functions represented sets monotonically increasing upper bounded
optimal value function. such, converge value function (which
necessarily optimal value function). question stop do-while loop.
straightforward method would compute distance maxb jU (b) ,V (b)j
two consecutive sets U V stop distance falls threshold. compute
distance, one needs solve jUj+jVj linear programs, time consuming. use
metric less expensive compute. specific, stop do-while loop

max
jU (w(ff)) , V (w(ff))j 1 :
ff2U
words, calculate maximum difference U V witness points
vectors U stop do-while loop quantity larger 1 .
threshold Bellman residual terminating value iteration 1 number
0 1. experiments, set 0:1.

5.5 Convergence Modified Value Iteration
Let Vn Vn0 sets vectors respectively generated VI (Figure 1) VI1 (Figure

2) line 3 iteration n. Suppose initial set uniformly improvable. Using Lemma 2
Corollary 1, one prove induction Vn Vn0 uniformly improvable
n induced value functions increase n. Moreover, Vn0 dominates Vn
dominated optimal value function. well known Vn converges optimal
value function. Therefore, Vn0 must converge optimal value function.
question make sure initial set uniformly improvable.
following lemma answers question.

Lemma 3 Let m= mins;a r(s; a), c = m=(1 , ), ffc vector whose components
c. singleton set fffc g uniformly improvable.
Proof: Use V denote value function induced singleton set. belief

state b,

TV (b) = max
[r(b; a) +

39

X P (zjb; a)V (ba)]
z

z

fiZhang & Zhang
= max
[r(b; a) +


X P (zjb; a)c]
z

= max
[r(b; a) + m=(1 , )]

+ m=(1 , )
= m=(1 , ) = V (b):
Therefore value function, hence singleton set, uniformly improvable. 2
Experiments (Section 6) shown VI1 ecient VI number test
problems. noted, however, proved always case.
Moreover, complexity results Papadimitriou Tsitsiklis (1987) implies task
finding -optimal policies POMDPs PSPACE-complete. Hence, worst-case
complexity remain same.

5.6 Computing Bellman Residual

modified value iteration algorithm, input V standard DP update always
uniformly improvable. such, output U dominates input. fact used
simplify computation Bellman residual. matter fact, Bellman residual
maxb jU (b),V (b)j reduces maxb (U (b),V (b)).
compute latter quantity, one goes vectors U one one.
vector, one solves linear program LP(ff; V ). quantity simply maximum
optimal values objective functions linear programs. Without uniformly
improvability, would repeat process one time roles V
U exchanged.

6. Empirical Results Discussions

Experiments conducted empirically determine effectiveness point-based
DP update speeding value iteration. Eight problems used experiments.
literature, problems commonly referred 4x3CO, Cheese, 4x4, Part
Painting, Tiger, Shuttle, Network, Aircraft ID. obtained problem files
Tony Cassandra. Information sizes summarized following table.
Problem jSj jZj jAj
4x3CO 11
4 11
4x4 16
2
4
Tiger 2
2
3
Network 7
2
4

Problem jSj jZj jAj
Cheese 11
4
7
Painting 4
4
2
Shuttle 8
2
3
Aircraft ID 12
5
6

effectiveness point-based DP update determined comparing standard
value iteration algorithm VI modified value iteration algorithm VI1. implementation standard value iteration used experiments borrowed Hansen.
Modified value iteration implemented top Hansen's code.5 discount factor
set 0:95 round-off precision set 10,6 . experiments conducted
UltraSparc II machine.
5. implementation available request.

40

fiSpeeding Value Iteration POMDPS
Table 1 shows amounts time VI VI1 took compute 0.01-optimal policies
test problems. see VI1 consistently ecient VI, especially
larger problems. 1.3, 2.8, 5, 62, 141, 173, 49 times faster VI
first seven problems respectively. Aircraft ID problem, VI1 able compute
0.01-optimal policy less 8 hours, VI able produce 33-optimal
policy 40 hours.
4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
VI
3.2
13.9 27.15 37.84 79.14 5,199 12,478
VI1
2.4
5.0 5.30
.61
.56
30
253 27,676
Table 1: Time Computing 0.01-Optimal Policies Seconds.
Various statistics given Table 2 highlight computational properties
VI1 explain superior performance. numbers standard DP updates carried
VI VI1 shown rows 1 3. see VI1 performed 5
standard updates test problems, VI performed 125. indicates
point-based update effective cutting number standard updates
required reach convergence. consequence, VI1 spent much less time VI
standard updates (row 2 4).6
Problem
4x3CO Cheese 4x4 Paint Tiger Shuttle Network
DPU #
125
129 130 127 163
174
214
VI
Time
2.00
7.63 17.83 33.39 70.44 3,198
8,738
DPU #
4
4
3
3
3
5
5
Time
.05
.09
.15
.21
.09
13
82
VI1 PBDPU #
377
219 173 244 515
455
670
Time
2.32
4.86 5.09
.37
.45
10
139
Quality Ratio
.33
.58
.74
.51
.31
0.31
.32
Complexity Ratio
.38
.37
.21 .0057 .002 .0012
.005
Table 2: Detailed Statistics.
Row 5 shows numbers point-based updates carried VI1. see
numbers actually larger numbers standard updates performed VI.
expected. see why, recall point-based update approximation standard
update. Let V set vectors uniformly improvable. Use 0 V denote
sets vectors resulted performing point-based update V . belief state b,
V (b)T 0 V (b)T V (b). means point-based update improves V
much standard update. Consequently, use point-based update increases total
6. Note times shown include time testing stopping condition.

41

fiZhang & Zhang
number iterations, i.e number standard updates plus number point-based
updates.
Intuitively, better point-based update approximation standard update,
less difference total number iterations VI1 VI need take. So,
ratio two numbers problem used, certain extent,
measurement quality point-based update problem. shall refer
quality ratio point-based update. Row 7 shows quality ratios seven test
problems. see quality point-based update fairly good stable across
problems.
Row 8 shows, test problem, ratio average time standard
update performed VI point-based update performed VI1. ratios
measure, certain extent, complexity point-based update relative standard update
hence referred complexity ratios point-based update. see that,
predicted analysis Section 5.3, point-based update consistently less expensive
standard update. differences 200 times last four problems.
summary, statistics suggest quality point-based update relative
standard update fairly good stable complexity much lower. Together
fact point-based update drastically reduces number standard updates,
explain superior performance VI1.
close section, let us note VI finds policies quality7 close
predetermined criterion, VI1 usually finds much better ones (Table 3).
VI checks policy quality (standard) update, VI1
point-based updates.

Problem 4x3CO Cheese 4x4 Paint Tiger Shuttle Network
VI
.0095 .0099 .0099 .01
.0098 .0097 .0098
VI1
.0008 .0008 .0009 .0007 .0007 .00015 .001
Table 3: Quality Policies Found VI VI1.

7. Variations Point-Based DP Update
studied several possible variations point-based update. based
ideas drawn existing literature. None variations able significantly
enhance effectiveness algorithm accelerating value iteration. Nonetheless brief
discussion still worthwhile. discussion provides insights
algorithm shows compares related work discussed
detail next section.
variations divide two categories: aimed improving quality
point-based update aimed reducing complexity. shall discuss one
one.
7. Quality policy estimated using Bellman residual.

42

fiSpeeding Value Iteration POMDPS

7.1 Improving Quality Point-Based DP Update
natural way improve quality point-based update back additional
belief points. explored use randomly generated points (Cassandra 1998a),
additional by-product points, projected points (Hauskrecht 2000). additional byproduct points refer points generated various stages standard update, excluding
witness points already used. Projected points points reachable
one step points given rise useful vectors.
Table 4 shows, test problem, number standard updates amount
time VI1 took without using projected points. see use
projected points reduce number standard updates one 4x3CO, Cheese,
Shuttle. However, increased time complexity test problems except Network.
two kinds points combinations three significantly improve
VI1 either. contrary, often significantly degraded performance VI1.
w/o

w/o


4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
4
4
3
3
3
5
5
7
3
3
3
3
3
4
5
7
2.4
5.0 5.3
.61
.56
30
253 27,676
3.2
5.6 7.4
.69
2.3
33
140 35,791

Table 4: Number Standard DP Updates Time VI1 Took Without
Using Projected Points.
close examination experimental data reveals plausible explanation. Point-based
update, stands, already reduce number standard updates
among last two three time-consuming. such, possibility
reducing number standard updates low even reduced,
effect roughly shift time-consuming standard updates earlier. Consequently,
unlikely achieve substantial gains. hand, use additional points
always increases overheads.

7.2 Reducing Complexity Point-Based DP Update
Solving linear programs expensive operation point-based update. obvious
way speed avoid linear programs. Point-based update solves linear programs
backs belief points found guarantee uniform improvability.
linear programs skipped, must way guarantee uniform
improvability. easy solution problem. Suppose V set vectors
try update uniformly improvable. Let U set obtained V
backing witness points, done without solving linear programs.
set U might might uniformly improvable. However, union V [ U
guaranteed uniformly improvable. Therefore reprogram point-based update
43

fiZhang & Zhang
return union hope reduce complexity. resulting variation called
non-LP point-based DP update.
Another way reduce complexity simplify backup operator (Section 3.1) using
idea behind modified policy iteration (e.g., Puterman 1990). backing
set vectors V belief point, operator considers possible actions picks
one optimal according V -improving policy. speed up, one simply
use action found belief point previous standard update. resulting
operator called MPI backup operator, MPI stands modified policy
iteration. V output previous standard update, two actions often
same. However, usually different V result several point-based updates
following standard update.
Table 5 shows, test problem, number standard updates amount
time VI1 took non-LP point-based update used (together standard
backup operator). Comparing statistics point-based update (Tables 1
2), see number standard updates increased test problems
amount time increased except first three problems. plausible
reasons. First, clear non-LP point-based update improve set vectors
much point-based update. Consequently, less effective reducing number
standard updates. Second, although solve linear programs, non-LP point-based
update produces extraneous vectors. means might need deal large
number vectors later iterations hence might ecient point-based
update all.
4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
4
5
8
4
4
7
10
8
2.38
2.38 3.4
.75
.88
44
599 32,281
Table 5: Number Standard DP Updates Time VI1 Took Non-LP PointBased Update Used.
Extraneous vectors pruned. matter fact, prune vectors
pointwise-dominated others (hence extraneous) experiments. inexpensive.
Pruning extraneous vectors, however, requires solution linear programs
expensive. Zhang et al. (1999), discussed done
ecient way. Still results good Table 5. paper,
explored combination non-LP point-based update MPI backup
operator. again, results good Table 5. reason
MPI backup operator compromises quality point-based update.
quality non-LP point-based update improved using Gauss-Seidel
asynchronous update (Denardo 1982). Suppose updating set V . idea to,
vector created backup, add copy vector set V right away.
hope increase components later vectors. tested idea preparing
Zhang et al. (1999) found costs almost always exceed benefits. reason
44

fiSpeeding Value Iteration POMDPS
asynchronous update introduces many extraneous vectors synchronous
update.
conclusion, point-based conceptually simple clean. compared
complex variations, seems effective accelerating value iteration.

8. Related Work
Work presented paper three levels: point-based DP update bottom, pointbased value iteration middle, modified value iteration top. section,
discuss previous relevant work three levels.

8.1 Point-Based DP Update Standard DP Update

mentioned Section 3.1, point-based update closely related several exact algorithms standard update, namely one-pass (Sondik 1971), linear-support (Cheng 1988),
relaxed-region (Cheng 1988). backup finite number belief points.
difference exact algorithms generate points systematically,
expensive, point-based update generate points heuristically.
several exact algorithms standard DP update. enumeration/reduction algorithms (Monahan 1982, Eagle 1984) incremental pruning (Zhang
Liu 1997, Cassandra et al. 1997) first generate set vectors parsimonious prune extraneous vectors solving linear programs. Point-based DP
update never generates extraneous vectors. might generate duplicate vectors. However,
duplicates pruned without solving linear programs. witness algorithm (Kaelbling
et al. 1998) two stages. first stage, considers actions one one.
action, constructs set vectors based finite number systematically generated
belief points using operator similar backup operator. second stage, vectors
different actions pooled together extraneous vectors pruned.
proposals carry standard update approximately dropping vectors
marginally useful (e.g., Kaelbling et al. 1998, Hansen 1998). one idea
along line empirically evaluated. Recall achieve -optimality,
stopping threshold Bellman residual = (1 , )=2. idea drop
marginally useful vectors various stages standard update keeping overall
error =2 stop Bellman residual falls =2. easy see
-optimality still guaranteed way. tried start large error
tolerance hope prune vectors gradually decrease tolerance level =2.
Reasonable improvements observed especially one need quality
policy high. However approximate updates much expensive
point-based updates. context modified value iteration algorithm,
suitable alternatives standard updates point-based update.

8.2 Point-Based Value Iteration Value Function Approximation

Point-based value iteration starts set vectors generates sequence vector
sets repeatedly applying point-based update. last set used approximate
optimal value function.
45

fiZhang & Zhang
Various methods approximating optimal value function developed
previously.8 compare point-based value iteration along two dimensions: (1) Whether map one set vectors another, whether
interleaved standard updates, (2) do, whether guarantee convergence interleaved standard updates.
Lovejoy (1993) proposes approximate optimal value function V POMDP
using optimal value function underlying Markov decision process (MDP).
latter function state space. V approximated one vector.
Littman et al. (1995b) extend idea approximate V using jAj vectors,
corresponds Q-function underlying MDP. extension recently
introduced Zubek Dietterich (2000). idea base approximation
underlying MDP, rather so-called even-odd POMDP identical original
POMDP except state fully observable even time steps. Platzman (1980)
suggests approximating V using value functions one fixed suboptimal policies
constructed heuristically. methods start set vectors
hence map set vectors another. However, easily adapted so.
However, put predetermined limit number output vectors. Consequently,
convergence guaranteed interleaved standard updates.
Fast informed bound (Hauskrecht 1997a), Q-function curve fitting (Littman et al. 1995b),
softmax curve fitting (Parr Russell 1995) map set vectors another. However, differ drastically point-based value iteration
ways deriving next set vectors current one. Regardless size
current set, fast informed bound Q-function curve fitting always produces jAj vectors,
one action. softmax curve fitting, number vectors determined
priori, although necessarily related number actions. methods
interleaved standard DP updates. Unlike point-based value iteration,
may converge (Hauskrecht 2000). Even cases converge themselves,
algorithms resulting interleaving standard updates necessarily
converge due priori limits number vectors.
Grid-based interpolation/extrapolation methods (Lovejoy 1991, Brafman 1997, Hauskrecht
1997b) approximate value functions discretizing belief space using fixed variable
grid maintaining values grid points. Values non-grid points estimated interpolation/extrapolation needed. methods cannot interleaved
standard DP updates work sets vectors.
grid-based methods work sets vectors. Lovejoy's method lower
bound optimal value function (Lovejoy 1991), instance, falls category.
method actually identical point-based value iteration except way derives
next set vectors current one. Instead using point-based update, backs
grid points regular grid. Convergence method guaranteed. algorithm
resulting interleaving standard updates may converge either.
8. Hauskrecht (2000) conducted extensive survey previous value function approximation methods
empirically compared terms of, among criteria, complexity quality. would
interesting include point-based value iteration empirical comparison. done
present paper focus using point-based value iteration speed value iteration,
rather using value function approximation method.

46

fiSpeeding Value Iteration POMDPS
incremental linear-function method (Hauskrecht 2000) roughly corresponds
variation point-based value iteration uses non-LP point-based update (Section 7.2)
augmented Gauss-Seidel asynchronous update. method access
witness point. starts, purpose backup, extreme points belief space
supplement projected points. choice points appears poor
leads large number vectors consequently backup process \usually stopped
well before" convergence (Hauskrecht 2000).

8.3 Previous Work Related Modified Value Iteration
basic idea modified value iteration algorithm VI1 add, two
consecutive standard updates, operations inexpensive. hope
operations significantly improve quality vector set hence reduce number
standard updates.
Several previous algorithms work fashion. differences lie operations inserted standard updates. reward revision algorithm (White
et al. 1989) constructs, iteration, second POMDP based current set
vectors. runs value iteration second POMDP predetermined number steps.
output used modify current set vectors resulting set vectors
fed next standard update.
reward revision expected speed value iteration? Let V value function
represented current set vectors. second POMDP constructed way
shares optimal value function original POMDP V optimal.
such, one would expect two POMDPs similar optimal value functions V
close optimal. Consequently, running value iteration second POMDP
improve current value function. inexpensive second
POMDP fully observable.
Reward revision conceptually much complex VI1 seems less
ecient. According White et al. (1989), reward revision can, average, reduce
number standard updates 80% computational time 85%. Tables 1
2, see differences VI1 VI much larger.
iterative discretization procedure (IDP) proposed Cheng (1988) similar
VI1. two main differences. VI1 uses point-based update, IDP uses non-LP
point-based update. point-based update VI1 backs witness points belief
points found linear programs, non-LP point-based update IDP backs extreme
points witness regions found by-products Cheng's linear-support relaxed region
algorithms.
Cheng conducted extensive experiments determine effectiveness IDP
accelerating value iteration. found IDP cut number standard updates
much 55% amount time much 80%. much less
significant reductions presented Tables 1 2.
Hansen's policy iteration (PI) algorithm maintains policy form finite-state
controller. node controller represents vector. iteration, standard
update performed set vectors represented current policy. resulting
47

fiZhang & Zhang
set vectors used improve current policy9 improved policy evaluated
solving system linear equations. gives rise third set vectors,
fed next standard update.
compared performance Hansen's PI algorithm VI1. Table 6 shows,
test problem, number standard updates amount time algorithm took.
Comparing statistics VI1 (Table 4), see PI performed standard
updates VI1. indicates policy improvement/evaluation less effective
point-based value iteration cutting number standard updates. terms
time, PI ecient VI1 first three problems significantly less ecient
problems.
4x3CO Cheese 4x4 Paint Tiger Shuttle Network Aircraft
3
7
7
10
14
9
18
9
.14
.87 3.4
3.8
4.5
60
1,109 66,964
Table 6: Number Standard Updates Time PI Took Compute 0.01-Optimal
Policies.
might possible combine VI1 PI. specific, one probably
insert policy improvement/evaluation step two point-based updates pointbased value iteration (Figure 2). accelerate point-based value iteration
hence VI1. possibility benefits yet investigated.

9. Conclusions Future Directions
Value iteration popular algorithm finding -optimal policies POMDPs. typically performs large number DP updates convergence DP updates
notoriously expensive. paper, developed technique called point-based DP
update reducing number standard DP updates. technique conceptually
simple clean. easily incorporated existing POMDP value iteration algorithms. Empirical studies shown point-based DP update drastically
cut number standard DP updates hence significantly speeding value
iteration. Moreover, point-based DP update compares favorably complex
variations think also. compares favorably policy iteration.
algorithm presented paper still requires standard DP updates. limits
capability solving large POMDPs. One future direction investigate properties
point-based value iteration approximation algorithm itself. Another direction
design ecient algorithms standard DP updates special models. currently
exploring latter direction.
9. Hansen's writings, policy improvement includes DP update substep. DP update
considered part policy improvement.

48

fiSpeeding Value Iteration POMDPS

Acknowledgments
Research supported Hong Kong Research Grants Council Grant HKUST6125/98E.
authors thank Tony Cassandra Eric Hansen sharing us programs.
grateful three anonymous reviewers provided insightful comments
suggestions earlier version paper.

References

Astrom, K. J. (1965). Optimal control Markov decision processes incomplete
state estimation. Journal Computer System Sciences, 10, 174-205.
Brafman, R. I. (1997). heuristic variable grid solution POMDPs. Proceedings
Fourteenth National Conference Artificial Intelligence(AAAI-97), 727-733.
Cassandra, A. R., Littman, M. L., Zhang, N. L. (1997). Incremental pruning:
simple, fast, exact method partially observable Markov decision processes.
Proceedings Thirteenth Conference Uncertainty Artificial Intelligence, 54-61.
Cassandra, A. R. (1998a). Exact approximate algorithms partially observable
Markov decision processes, PhD thesis, Department Computer Science, Brown
University.
Cassandra, A. R. (1998b). survey POMDP applications, Working Notes AAAI
1998 Fall Symposium Planning Partially Observable Markov Decision Processes, 17-24.
Denardo, E. V. (1982). Dynamic Programming: Models Applications Prentice-Hall.
Eagle, J. N.(1984). optimal search moving target search path
constrained. Operations Research, 32(5), 1107-1115.
Cheng, H. T.(1988). Algorithms partially observable Markov decision processes. Ph
thesis, University British Columbia.
Hansen, E. A. (1998). Solving POMDPs searching policy space. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, 211-219.
Hauskrecht, M.(1997a). Incremental methods computing bounds partially observable
Markov decision processes. Proceedings Fourteenth National Conference
Artificial Intelligence (AAAI-97), 734-749.
Hauskrecht, M.(1997b). Planning control stochastic domains imperfect information. PhD thesis, Department Electrical Engineering Computer Science,
Massachusetts Institute Technology.
Hauskrecht, M. (2000). Value function approximations partially observable Markov
decision processes, Journal Artificial Intelligence Research, 13, 33-95.
49

fiZhang & Zhang
Littman, M. L., Cassandra, A. R. Kaelbling, L. P. (1995a). Ecient dynamicprogramming updates partially observable Markov decision processes. Technical
Report CS-95-19, Brown University.
Littman, M. L., Cassandra, A. R. Kaelbling, L. P. (1995b). Learning policies partially observable environments, scaling up. Proceedings Fifteenth Conference
Machine Learning, 362-370.
Littman, M. L. (1996). Algorithms sequential decision making. Ph thesis, Department Computer Science, Brown University.
Kaelbling, L. P., Littman. M. L. Cassandra, A. R.(1998). Planning acting
partially observable stochastic domains, Artificial Intelligence, Vol 101.
Lovejoy, W. S. (1991). Computationally feasible bounds partially observed Markov
decision processes. Operations Research, 39, 192-175.
Lovejoy, W. S. (1993). Suboptimal policies bounds parameter adaptive decision
processes. Operations Research, 41, 583-599.
Monahan, G. E. (1982). survey partially observable Markov decision processes: theory, models, algorithms. Management Science, 28 (1), 1-16.
Parr, R., Russell, S. (1995). Approximating optimal policies partially observable
stochastic domains. Proceedings Fourteenth International Joint Conference
Artificial Intelligence 1088-1094.
Papadimitriou, C. H., Tsitsiklis, J. N.(1987). complexity Markov decision processes.
Mathematics Operations Research, 12(3), 441-450.
Platzman, L. K.(1980). Optimal infinite-horizon undiscounted control finite probabilistic systems. SIAM Journal Control Optimization, 18, 362-380.
Puterman, M. L. (1990), Markov decision processes, D. P. Heyman M. J. Sobel
(eds.), Handbooks & MS., Vol. 2, 331-434, Elsevier Science Publishers.
Smallwood, R. D. Sondik, E. J. (1973). optimal control partially observable
processes finite horizon. Operations Research, 21, 1071-1088.
Sondik, E. J. (1971). optimal control partially observable Markov processes. PhD
thesis, Stanford University.
Sondik, E. J. (1978). optimal control partially observable Markov processes
infinite horizon, Operations Research, 21, 1071-1088.
White, C. C. III Scherer, W. T. (1989). Solution procedures partially observed
Markov decision processes, Operations Research, 37(5), 791-797.
Zhang, N. L., Lee, S. S., Zhang, W.(1999). method speeding value iteration
partially observable Markov decision processes, Proc. 15th Conference
Uncertainties Artificial Intelligence.
50

fiSpeeding Value Iteration POMDPS
Zhang, N. L. W. Liu (1997). model approximation scheme planning stochastic
domains, Journal Artificial Intelligence Research, 7, 199-230.
Zubek, V. B. Dietterich, T. G.(2000). POMDP approximation algorithm anticipates need observe. appear Proceedings Pacific Rim Conference
Artificial Intelligence (PRICAI-2000), Lecture Notes Computer Science, New
York: Springer-Verlag.

51


