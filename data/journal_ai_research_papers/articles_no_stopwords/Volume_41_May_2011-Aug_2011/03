journal artificial intelligence

submitted published

efficient multi start strategies local search
andras gyorgy

gya szit bme hu

machine learning group
computer automation institute
hungarian academy sciences
budapest hungary

levente kocsis

kocsis sztaki hu

data mining web search group informatics laboratory
computer automation institute
hungarian academy sciences
budapest hungary

abstract
local search applied optimization often suffer getting
trapped local optimum common solution deficiency restart
progress observed alternatively one start multiple instances
local search allocate computational resources particular processing
time instances depending behavior hence multi start strategy
decide dynamically allocate additional resources particular instance
start instances propose multi start strategies motivated
works multi armed bandit lipschitz optimization unknown
constant strategies continuously estimate potential performance
instance supposing convergence rate local search unknown
constant every phase allocate resources instances could converge
optimum particular range constant asymptotic bounds given
performance strategies particular prove quadratic increase
number times target function evaluated needed achieve performance
local search started attraction region optimum experiments
provided spsa simultaneous perturbation stochastic approximation kmeans local search indicate proposed strategies work
well practice cases studied need logarithmically evaluations
target function opposed theoretically suggested quadratic increase

introduction
local search applied optimization often suffer getting trapped
local optimum moreover local search guaranteed converge
global optimum conditions simulated annealing simultaneous
perturbation stochastic approximation spsa see e g spall hill stark usually
converge slow pace conditions satisfied hand
employed aggressive settings much faster convergence local
optima achievable guarantee global optimum common soluc

ai access foundation rights reserved

figyorgy kocsis

tion escape local optimum restart progress observed
see e g mart moreno vega duarte zabinsky bulger khompatraporn
references therein
alternatively one start multiple instances local search allocate
computational resources particular processing time instances depending
behavior instances started time number instances may grow
time depending allocation strategy see e g chapter battiti brunato
mascia references therein type computational cost
usually measured total number steps made search instances
often reflects situation evaluation target function optimized
expensive costs related determine use next negligible
compared former e g clearly case task tune parameters
system whose performance tested via lengthy experiments see e g bartzbeielstein hutter hoos leyton brown stutzle address
dynamically starting several instances local search
allocating resources instances potential performance
knowledge solutions heuristics
assumption local optima search converge
extreme value distribution see section propose multi start
strategies mild conditions target function attractive theoretical
practical properties supposing convergence rate local search
unknown constant strategies continuously estimate potential performance
instance every phase allocate resources instances could
converge optimum particular range constant selection mechanism
analogous direct jones perttunen stuckman finkel
kelley horn optimizing lipschitz functions unknown constant
preference given rectangles may contain global optimum optimum
within rectangle estimated optimistic way estimate depends
size rectangle strategies use function describing convergence rate
local search similar way size rectangles used
direct
since proposed multi start strategies potential performance local search
continuously estimated currently best value target function
returned method restricted work local search
return best known value target function step case
example certain meta learning goal good parameter
setting learning search space parameter space learning
one step local search methods means running learning
completely possibly large data set hand local search
sort gradient search optimizing error function training data
value target function usually available case batch learning
potentially cheap computations gradient estimated
samples
rest organized follows section summarizes related
defined formally section multi start local search strategies


fiefficient multi start strategies local search

described analyzed section section deal selection
mechanism among fixed number instances local search
addition simple schedules starting instances considered section
natural extensions case finitely many local search instances
section concludes discussion section simulation
real synthetic data provided section conclusions future work
described section

related work
allocating resources among several instances search
comfortably handled generalized version maximum k armed bandit
original version consists several rounds round one
chooses one k arms receives reward depending choice goal
maximizing highest reward received several rounds model easily used
considering local search instance arm pulling
arm means taking one additional step corresponding evaluating
target function point suggested reward received
value target function sampled point generic standard
maximum k armed bandit reward assumed independent
identical distribution provided adam called reservation price
instance introduced gives maximum amount resources worth spend
instance instance achieves reservation price useless select
computation reservation price depends model
learnt specific constraints
consider scenario several instances possibly randomized local
search run goal maximizing expected performance instance run terminates scenario natural assume
values returned instances usually local optima target function independent furthermore since good search follow usually heuristic
procedures yield substantially better pure random guessing cicirello
smith suggested rewards evaluated target function values
search instances may viewed maximum many random variables instances run sufficiently long time hence may modeled extreme value
distributions several assumption hence developed
maximum k armed bandit returns following generalized extreme value
distributions cicirello smith apply somewhat heuristic methods use
extreme value distribution assumption decision point meta learning
streeter smith use model obtain upper confidence bounds
performance estimate type used try
best expected latter theoretically justified example natural
strategy probe instances estimate future performance
trial phase use promising
time remaining streeter smith b proposed distribution free


figyorgy kocsis

combines multi armed bandit exploration strategy heuristic selection among
available arms
standard maximum k armed bandit rewards round
assumed independent clearly case situation
instances run parallel reward evaluating target function
point improvement upon current maximum since samples chosen local
search usually depend previous samples nevertheless ideas lessons
learnt maximum k armed bandit used case well
example threshold ascent streeter smith b gives reasonably
good solutions case principle probing instances
promising time remaining carries situation easily
first exploration exploitation phase referred
sequel explore exploit class simple rules
suggested beck freuder predict future performance
carchrae beck employ bayesian prediction
another related fast among several ones solve
precisely several instances available produce
correct answer certain question run sufficiently long time time needed
instance answer assumed random quantity
independent identical distributions instances goal combine
given minimize expected running time answer found
distribution running time known optimal non adaptive time allocation strategy
perform sequence runs certain cut time depends distribution
luby sinclair zuckerman distribution unknown particular running
time sequence chosen expected total running time
logarithmic factor larger optimum achievable distribution known note
strategy among provide schedule increases number
instances set specialized goal
optimal approximation optimum running time number steps
needed given search achieve approximation note
case running time instance providing suboptimal solution
defined infinity luby et al remain valid optimal solution
found positive probability kautz horvitz ruan
gomes selman proposed allocation strategy updating dynamically
belief run time distribution concerning latter hoos stutzle
found empirically run time distributions approximately exponential certain nphard ribeiro rosseti vallejos dealt comparison
different run time distributions
finally set time allocation strategies available optimization solved several times one use standard multi armed bandit framework
done gagliolo schmidhuber
running several instances several parallel selecting
among intensively studied example area meta non adaptive time allocation strategy running time instance fixed advance
measured performance instances effect schedule



fiefficient multi start strategies local search

learning vilalta drissi automatic configuration hutter et al
underlying similar cases automatic configuration
usually refers tuning search meta learning used subset
tuning machine learning latter often allows specific use
data main allocate time slices particular
aim maximizing best returned allocation may depend intermediate
performance automatic configuration metalearning systems use heuristics explore space parameters
see e g hutter et al
finally important note although multi start local search strategies solve
global optimization concentrate maximizing performance given
underlying family local optimization methods since choice latter major
effect achievable performance compare vast literature
global optimization

preliminaries
assume wish maximize real valued function f dimensional unit hypercube
goal maximizer x f x f
f max f x
x

denotes maximum f simplicity assume f continuous
continuity f implies existence x particular f bounded
therefore without loss generality assume f non negative
form f known explicitly search usually evaluate f several
locations return estimate x f observations
obvious trade number samples used e number points
target function f evaluated quality estimate performance
search strategy may measured accuracy achieves estimating f
constraint number samples used
given local search general strategy finding good approximation
optimum x run several instances initialized different starting points
approximate f maximum f value observed concentrate local search
defined formally sequence possibly randomized sampling functions
sn dn n evaluates f locations x x xi
si x xi starting point x chosen uniformly random
n observations returns estimate x maximum f respectively

bn argmax f xk
bn
x

f x
kn

ties argmax function may broken arbitrarily samples xk
bn chosen avoid ambiguity
achieve maximum x
easily extended arbitrary valued bounded piecewise continuous functions
finitely many continuous components



figyorgy kocsis

simplify notation following unless stated explicitly otherwise adopt
convention use argmax denote maximizing sample smallest index
remain valid choice break ties
simplicity consider starting single local search different
random points although work extended allow varying
parameters including situation running different local search
parameter would choose actually employed search well allow
dependence among initializations starting point parameters
local search instance may depend information previously obtained target
function
clear starting points sampled uniformly
bn converges
evaluated starting point strategy consistent f x
maximum f probability number instances tends infinity
worst case perform random search known converge maximum almost
surely hand favorable properties possible
design multi start strategies still keep random search consistency
provide much faster convergence optimum terms number evaluations
f
bn bounded non decreasing converges matter
since sequence f x
random effects occur search next lemma proved appendix shows
high probability convergence cannot arbitrarily slow
bn f p e
lemma f let e denote event limn f x
bn f
event e e p e f x
uniformly almost everywhere e words exists non negative nonincreasing function g n limn g n



bt f
b f x
bn g n nfi lim f x
p lim f x





certain cases g n en shown nesterov gradient
optimization convex functions gerencser vago noise free spsa
convex functions kieffer k means clustering lloyds one
dimension log concave densities pertain simple situation
one local optimum global one many
extended general situations observed exponential rate convergence
experiments functions many local maxima
convergence property local search guaranteed lemma
exploited next section derive efficient multi start search strategies

multi start search strategies
standard multi start search strategies run instance seems converge
location hope beat currently best approximation f
practice usually assume local search converge local optima f may
assumed local optimum



fiefficient multi start strategies local search

alternative way multiple instances local search run
parallel round decide take extra step
may estimating potential performance local search
lemma note g known obvious way would run instance
possible performances become separated high probability sense
margin performance actually best second best
large actually best guaranteed best long run
high probability could pick best instance run given
computational budget exhausted would simple adaptation explore andexploit idea choosing best trial phase beck freuder
carchrae beck
practice however g usually known certain classes local
search may known belong function class example g may
known multiplicative constant factor example constant may
depend certain characteristics f maximum local steepness even
latter case best instance still cannot selected high probability matter
large margin g may arbitrarily large however ideas general
methodology lipschitz optimization unknown constant jones et al
get around estimate certain optimistic way potential
performance instance round step promising
ones
main idea resulting strategy summarized follows assume
k instances denoted ak let xi n k denote
location f evaluated ai nth time take step xi
starting point ai estimate location maximum ai n
samples steps
bi n argmax f xi
x
tn

bi n
maximum value function estimated n f x
let limn n denote limiting estimate maximum f
provided ai let g defined lemma largest values
f max
k

since f best achievable estimate maximum f given actual
ak g gives high probability convergence rate provide
best estimate maximum long run note assumption deals
limiting estimate usually local maximum separately assumption
made whose limiting estimates less f ai evaluates f
ni r points end rth round ai converges best achievable estimate
f lemma probability least
ni r g ni r


ni r g ni r




figyorgy kocsis

optimistic estimate f ai suboptimal sense limn n f
estimate still optimistic rate convergence slower g
pessimistic rate convergence slower g latter desirable sense
negatively biased estimate expected performance
want use waste samples suboptimal choices
practice g usually known exactly estimate often cannot constructed hand g known constant factor construct
family estimates scales let g denote normalized version g
g g n g n constant n construct family estimates
ni r cg ni r



c ranges positive reals reasonable choose round
take another step provide largest estimate values c
typically gives largest estimate c c interval
containing c provides largest estimate c
way get around fact know real scaling factor g
certainly use provide largest value c g g
discussed later waste many samples
maximize values c optimistic estimate similar spirit
optimistic estimates standard upper confidence bound type solution
multi armed bandit auer cesa bianchi fischer well known
search hart nilsson raphael
however exact local convergence rate known even constant factor
many local search even corresponding bounds usually
meaningful asymptotic regime often practical interest therefore
give freedom design going use estimate
form
ni r ch ni r

similarly requirements g h positive monotone decreasing function
limn h n assume without loss generality h
actual form h theoretical analysis resulting
heuristic considerations essentially use h functions converge zero
exponentially fast agreement exponentially fast local convergence rates
examples given lemma optimal choice h given example g
known left future work
constant number instances
idea translated metamax k shown figure
consider case fixed number instances goal perform
almost well best hindsight minimum number
br
evaluations f note slight abuse notation metamax k x
fr denote estimates r rounds r steps samples
first part step metamax sweep positive c select local
search maximize estimate easy see ai maximizes


fiefficient multi start strategies local search

metamax k multi start strategy k
instances
parameters k positive monotone decreasing function h
limn h n
initialization k take step ai
let ni f xi
round r
k select ai exists c
ni r ch ni r fj nj r ch nj r



j k ni r ni r nj r fj nj r
several values selected step number
ni r keep one selected uniformly random
b step selected ai update variables set ni r
ni r ai selected ni r ni r otherwise
bi n
selected ai evaluate f xi ni r compute estimates x
r
ni r
c let ir argmaxi k ni r denote index
currently largest estimate f estimate location
br x
bir n
maximum x
value fr fir nir r
ir r
figure metamax k
particular c u closed interval containing u ai
maximizes c therefore round strategy metamax k selects
local search ai corresponding point h ni r ni r
corner upper convex hull set
pr h nj r fj nj r j k max fj nj r
jk



selection mechanism illustrated figure
avoid confusion note random selection step metamax k implies
exactly state ni r ni r nj r fj nj r
j one selected uniformly random pathological situation
may arise e g beginning local search give
estimate f range step numbers apart case one
least used provides currently best estimate happens surely
first round usually happen later includes previous pathological case
guaranteed round use least two one largest


figyorgy kocsis





f x




















h n









figure selecting instances metamax points represent
instances lie corners upper convex hull
drawn blue lines selected

estimate ni r fr small values c one smallest step number
nj r large values c thus usually half total number function
calls f used optimal local search observation gives practical lower bound valid apart pathological situation mentioned
proportion function calls f made optimal local search surprisingly
theorem shows lower bound achieved asymptotically
randomization step precludes multiple instances
step number introduced speed certain pathological cases
example converges correct estimate ak
produce estimate round independently samples inferior
estimates use randomization half calls compute f
made without randomization would drop k
round would use furthermore could take step
lie convex hull similar pathological examples constructed
beneficial use corners hand almost never
happens practice three lie line typically
never fall non corner points convex hull
remainder section analyze performance metamax k
proposition shows consistent sense performance asymptotically achieves best instance number rounds
increases understand better lemma provides general sufficient condition instance advanced given round
lemma provides conditions ensure suboptimal instances
used round stepped many times e evaluated f
many points lemma gives upper bound number


fiefficient multi start strategies local search

instances used round lemmas used theorems
remark optimal instances used asymptotically least
minimum frequency turn yields asymptotic rate convergence
metamax k
following proposition shows metamax k consistent
sense
proposition metamax k consistent sense fr f
r

n
f lim n
f lim fr min
r

k

n

proof proof follows trivially fact selected infinitely
often limr ni r see latter every k rounds
number steps taken least used mini k ni r guaranteed
increase one k
min ni kk k

k



described round select exactly one made
least number steps thus k minimum step number
per increase k rounds completes proof

metamax k efficient suboptimal step
often next lemma provides sufficient conditions used
given round
lemma instance aj used round r metamax k
ai ak ni r fj nj r fk nk r
ni r nj r


h nj r
h nj r


fj nj r ni r

fk nk r
h nk r
h nk r
proof round corners convex hull pr used
easy see aj used round r ai
ak ni r fj nj r fk nk r ni r nj r
ni r fk nk r
ni r fj nj r


h nj r h ni r
h nk r h ni r



finish proof implies latter indeed equivalent
h nk r ni r fj nj r h nj r ni r fk nk r h ni r fk nk r fj nj r
last term right hand side inequality negative assumptions
inequality satisfied
h nk r ni r fj nj r h nj r ni r fk nk r


figyorgy kocsis

equivalent



lemma provides conditions instances certain
round depend actual performance instances next gives similar
conditions however best estimates usually local optima achievable
let limr ni r asymptotic estimate ai f
let f max ik denote best estimate achievable ak
let k set optimal converge best estimate
f let denote cardinality e number
optimal instances note random variable depends actual
realizations possibly randomized search sequences next lemma
shows j aj used round r used often far
lemma let

f max fj
j

denote margin estimates best second best
u random index r u j aj
used metamax k round r r u




f
j
min ni r

nj r h h


k
f u
furthermore let let g denote convergence rate
ai guaranteed lemma let g n maxio g n n p r u
g generalized inverse g suboptimal
kg u f fk


aj j used r kg u probability least given

limiting estimates f fk
proof let since limn n f assumption implies
u
limr ni r almost surely finite random index ri
u

r ri f n
u
r
f fr u



lemma easily derive high probability upper bound r u since
r kg u r implies ni r g u lemma yields



p ni r u r r f

follows probability least ni r u
thus
implies r u chosen p r u r f fk
prove lemma enough
clearly pick estimate one best
r u rounds let ak least number steps taken end


fiefficient multi start strategies local search

round r nk r mini ni r fk nk r fj nj r aj used round r
moreover since aj fj nj r fj fr case aj used round r
nj r nir r recall nir r number evaluations f initiated actually
best ir therefore lemma implies aj used r r

fj nj r fk nk r
h nj r h nk r

fr fk n
k r

clearly satisfied
h nj r h min ni r


fj
f u





u since


fj
fj nj r fk nk r
fj nj r



fr fk nk r
fr
f u

applying inverse h sides proves hence lemma



provides individual condition suboptimal
used round hand one optimal
stepped sufficiently many times give cumulative upper bound number
suboptimal used round
lemma assume h decreases asymptotically least exponentially fast
exist n h n
h n n n assume r large
enough ni r n let r maxi n

fr
r

ni r
fr



r
ln
ln stepped round r x denotes smallest integer
least large x

proof let im denote indices chosen round r
r r fim r fr lemma implies ni r ni r nim r
fr fik r fr fik r
k repeated application inequality implies
fr r fr fim r fr fim r fr r fr
yields

ln r
ln
assumed chosen round r fact finishes
proof





figyorgy kocsis

lemmas next theorem shows local search
converges fast enough exponentially dependent rate faster exponential half function calls evaluate f correspond optimal
instances
theorem assume performance ai k
k suppose


fj
h n
lim sup


min
j
h n
n
f
asymptotically least half function
respond optimal
p
ni r

p lim inf pio

k
r

ni r

calls evaluate f metamax k cor



f fk



furthermore constant r
p

k
n
r
f fr g






simultaneously r r
probability least given f fk

g defined lemma threshold r
depends h g

g defined lemma

proof suboptimal aj chosen large enough r nj r mink nk r
lemma sufficient prove large enough r

fj


min nk r h
h min nk r
k
k
f u
u recall r larger r u almost surely finite
random index lemma
minimum taken finite set follows exists small
enough positive u


fj
h n
lim sup


min
j
h n
n
f u
clearly implies limr mink nk r fact finishes proof
first part theorem
next prove let nu threshold holds n nu
furthermore lemma union bound holds local search
ai g place g simultaneously probability least


fiefficient multi start strategies local search

slight modification lemma imply holds simultaneously

r k max g u nu r probability least given f fk
since
round two used r r c
p
n
r
c r
pio
c kr
high probability since latter bounded
k
n
r

pk
p
ni r
c r k io ni r
r r r r k


l p highmprobability ai used least
k


ni r


rounds implying statement theorem via lemma



remark proof theorem lemma proof lemma
possible since setting minj fj f lemma implies
large enough r r round approximately length lemma
may happen although decay rate h exponential quite fast
enough satisfy optimal scenarios theorem hold
case turns number converging local maximum
plays key role determining usage frequency optimal
theorem assume estimates provided ak converge
n distinct limit points k converge f k k kn
converge suboptimal limit points respectively suppose furthermore
h decreases asymptotically least exponentially fast
lim supn h n
h n
p lim inf
r

p

io ni r
pk
ni r




kmax

f fk

k k kmax

kmax max ki
furthermore definitions lemma
constant threshold r


pk
k
n
max
r
f fr cg
k k kmax
simultaneously r r
probability least given f fk
r depends f sfk convergence rate
given fix random trajectories
proof suppose f fk
single suboptimal statement trivial kmax k k kmax
round least two used one
suboptimal one assume least two suboptimal
assume aj ak converge suboptimal local maxima strictly less f
r large enough optimal ai better suboptimal ones

instead convergence rate r may defined dependent g



figyorgy kocsis

ai converges f ni r fj nj r fk nk r assume without loss generality
fj nj r fk nk r nj r nk r clearly aj chosen round r assume
nj r nk r since fj nj r fk nk r convergent sequences r large enough
r j k


fj nj r fk nk r
j k tj k


n fk n
r



k r

tj k ln j k ln positive integer note aj ak converge
point limr fj nj r fk nk r second term left hand side
converges j k chosen implying tj k rearranging
inequality one obtains
tj k ni r tj k fk nk r fj nj r



nj nk tj k conditions h fact nj r nk r tend
infinity r recall imply large enough r h nj r h nk r tj k since
ni r fk nk r large enough r obtain


h nj r
h nj r
tj k
tj k

fk nk r
ni r
fj nj r ni r fk nk r
h nk r
h nk r
thus lemma r large enough aj cannot used round r nj r nk r tj k
since nj r nk r tend infinity follows large enough r
nj r nk r tj k



two suboptimal aj ak note fact implies
set suboptimal converging point eventually one
used round since corresponding thresholds tj k
clearly implies nj r nk r grow linearly r since differences bounded limr nj r nk r therefore suboptimal
aj limn nj r r kmax maximal rate elements
largest group suboptimal converging local optimum finally
optimal used round r large enough r
p
p
io ni r
io ni r
p
lim inf pk
lim inf p
r
r
n

r
n
ni r
io
r

kmax
r
lim


r
r r k k
k k kmax
kmax
used fact b increasing function b since

inequality holds realizations trajectories ak given f fk
first statement theorem follows
second statement follows similarly theorem since exact value
r particular interest derivation omitted




fiefficient multi start strategies local search

remark main message theorem somewhat surprising observation
suboptimal slowed large group suboptimal
converging local optimum rate suboptimal used bounded
size largest group
unbounded number instances
clear local search consistent e achieve
global optimum f despite favorable properties metamax k strategy
inconsistent however increase number infinity
get consistency random search still keeping reasonably fast convergence
rate metamax k
clearly one needs balance exploration exploitation
control often introduce one solution let metamax
solve metamax given figure extension metamax k able run infinitely many local search
instances major issue local search started
time time ensures converge global maximum f
since performs random search implemented modifying step
metamax k randomly initialized local search introduced round randomly selecting one uniformly infinitely
many possible used far obviously skip initialization
step metamax k start samples better control
length round e exploration round r allow use different
function h denoted hr may depend value measured round r
suppressed notation assume hr hr n monotone decreasing n limn hr n r typically make hr dependent
pkr
total number steps e function calls evaluate f tr
ni r made
round r kr number instances used
round r note kr r r start exactly one
round
desired although number local search grows infinity
number times best local search advanced metamax
approaches infinity reasonably fast somewhat relaxing random initialization
condition may imagine situation local search initialized
clever deterministic way first steps better
value initial guesses optimal may viewed
clever initialization may provide example identical estimates
first three steps easy see stepped
exactly twice thus convergence optimum would found third
step achieved although random initialization search guarantees
consistency metamax see proposition robust behavior even
pathological cases preferred
achieved slight modification round local search
overtakes currently best ir ir air


figyorgy kocsis

metamax multi start strategy infinitely many
instances
parameters hr set positive monotone decreasing functions
limn hr n
round r
initialize ar setting nr r fr
b r select ai exists c
ni r chr ni r fj nj r chr nj r
j r ni r ni r nj r fj nj r
several values selected step number
ni r keep one selected uniformly random
c step selected ai update variables set ni r
ni r ai selected ni r ni r otherwise
bi n
selected ai evaluate f xi ni r compute estimates x
r
ni r

let ir argmaxi r ni r denote index
currently largest estimate f estimate location
br x
bir n
maximum x
value fr fir nir r
ir r
figure metamax

stepped several times used times air resulting
called metamax given figure note metamax
metamax conceptually differ one place step c extended step c
technical modification appears step
simplify presentation metamax slight insignificant modification
introduced step b see discussion
modification metamax really significant practical examples
studied see section number steps taken overtakes
currently best grows quickly metamax since
metamax overtake usually introduces short rounds close minimum length
two many cases leading becomes used one goal
modification step b synchronize choice optimal
steps b c equally good solution would choose case tie step
way achieve actually best dominates others terms accuracy
number calls made compute target function type
dominance used hutter et al slightly different context



fiefficient multi start strategies local search

metamax multi start strategy infinitely many
instances
parameters hr set positive monotone decreasing functions
limn hr n
round r
initialize ar setting nr r fr
b r select ai exists c
ni r chr ni r fj nj r chr nj r
j r ni r ni r nj r fj nj r
several values selected step number
ni r keep one smallest index
c step selected ai update variables set ni r
ni r ai selected ni r ni r otherwise
bi n
selected ai evaluate f xi ni r compute estimates x
r

ni r

c let ir argmaxi r ni r denote index
currently largest estimate f case ir unique choose
one smallest number steps ni r ir ir step
air nir r nir r times set nir r nir r
br x
bir n
estimate location maximum x

ir r
value fr fir nir r
figure metamax

c used current round note
modifications currently best index ir taken steps
extra number steps taken step c indeed positive important consequence
modifications round r number steps taken local search
air best end round r r see theorem

rest section devoted theoretical analysis metamax
metamax following lines analysis provided metamax k first proposition shown consistent solution found
actually converges f lemma counterpart lemma shows
suboptimal make finitely many steps lemma gives upper
bound length round main theoretical section apply


figyorgy kocsis

metamax theorem gives lower bound number steps taken
actually best end given round consequence theorem shows rate convergence function total number
steps e function calls evaluate f used instances turns
quadratically steps needed generic local search instance
converges optimum
since metamax metamax strategies perform random search
number tends infinity length round finite
consistent
proposition strategies metamax metamax consistent

lim fr f
r

almost surely
proof clearly event fr converge f written
n


n




lim fr f
fr f n

r



n r r r

continuity f implies n x chosen uniformly
qn p
p f x f n thus round r p fr f n qn r



r p fr f n finite therefore borel cantelli lemma see e g ash
doleans dade implies

n





p
fr f n
r r r

n together finishes proof
p



lim fr f

r








x

n

p

n



r r r

fr f n









reminder section assume local search achieve
almost optimal value eventually converge optimum
assumption let f r denote set local maxima f let f
supff f f f assume ai n f
n limn n f
local search converge local optima reasonable assumption
practice assumption usually satisfied situation
hold pathological case f infinitely many local maxima set
maxima dense global maximum


fiefficient multi start strategies local search

assumption prove similarly lemma suboptimal selected limited number times increases h
r particular
hr h r large enough suboptimal chosen finitely many
times
lemma suppose assumption let q p f x f x uniformly
distributed metamax metamax
suboptimal aj started round r used round r probability
least q r




nj r hr

f
addition hr n non decreasing function r n
lim sup
r

h
r

n
j r



f



almost surely



particular hr constant function hr h r limr nj r
almost surely
remark note second
part lemma coulddrop
monotonicity




assumption hr replace hr f max r r hr f
proof consider aj used round r first note probability
least q r fr f furthermore newly introduced ar
used yet nr r fr thus lemma aj used


h nj r


fr hr nj r
fj nj r fr
hr
since equivalent
nj r

h
r

fj nj r

fr

h
r

fj nj r

fr







h
r








f



fr fj nj r

fr f
f f






f
f
fr
fr
first statement proof follows
b denote first round optimal
prove second part let r


ai ni rb f suboptimal aj first part
b
lemma implies r r












b
b
max r hr

nj r max r max
h
r r r
f
f


figyorgy kocsis

equality holds since h
r n non decreasing r thus




b
nj r
r

lim sup max





lim sup



h

r h
r
h
r f
r f
r f




b

r




max



h
h




f



f

b finite finite
used h
non decreasing r since r
r
probability

simple modification lemma implies optimal sample point
found limited number suboptimal chosen round
lemma consider metamax metamax suppose assumption
holds assume f fr r anyround r r hr n rn


r n

ln f
ln r

chosen

estimates fj f

proof proof follows lemma taking account suboptimal
aj satisfies fj f least one optimal chosen round
r r similarly r defined lemma bounded r f

l
f
ln
ln r

number suboptimal used round r bounded ln

ln
r
r

finally derive convergence rate metamax first bound
number steps taken currently best terms number
rounds total number steps taken local search
theorem consider metamax end round r number
steps taken currently best r r
r nir r r



furthermore number calls nir r evaluate f currently
best air
pr
bounded function total number times tr ni r target function f
evaluated local search instances

tr
nir r



proof first statement lemma simple since round actually
best takes one step overtaking one two steps


fiefficient multi start strategies local search

overtaking indeed round r overtaking ir ir
nir r nir r otherwise ir ir nir r nir r since
nir r nir r
nir r nir r
situations since first round clearly used takes step
ni follows
prove second part notice round r nir r
stepped step c used taken steps
currently best one step c extra samples used overtaking
case overtaking air advanced step c well air
nir r extra steps taken air therefore
tr tr nir r
thus since overtaking happens round obtain
tr

r
x

nis




tr

r
x


r r nir r nir r

yields



note proof used crude estimate length usual round without
overtaking relative example lemma however affects
constant factor long able bound number rounds number
extra steps taken overtaking happens since effect overtakings
introduces quadratic dependence proof experimental section
see figure number instances turn number r
rounds usual growth rate tr ln tr taken account may sharpen
bound often best chosen
assumption random search component metamax implies eventually optimal best point convergence
rate optimal local search determine performance search
number steps taken best local search bounded theorem
theorem suppose assumption holds almost surely finite random
index r rounds r r estimate fr metamax
total number steps tr taken local search end round r
satisfies







r

f fr g

probability least g defined lemma global maximum f


figyorgy kocsis

remark value r bounded high probability properties
uniform random search actual would yield similar bounds
theorems metamax k ii note exploration exploitation
trade metamax value r potentially decreased introduce
often nir r reduced time iii theorems
imply asymptotically metamax needs quadratically function
evaluations local search
psthat ensured converge optimum
particular f form f x
x isi x si form partition
isi denotes indicator function si belong nicely behaving
function class local search started si converges maximum
si e g f piecewise concave function exponential convergence rate
spsa used sufficiently small step size preserve
performance local search original function class price
asymptotically quadratic increase number function calls evaluate f e
total number steps taken local search instances
discussion
sense theoretical presented previous sections weak
consistency metamax k follows easily fact
local search used infinitely many times consistency metamax
metamax follows consistency random search performance bounds
provided disadvantage asymptotic sense hold
possibly large number rounds weakness bounds minimum number
rounds obtained properties uniform random search sampling particular
neglecting attractive properties fact quite easy
construct scheduling strategies consistent asymptotically arbitrarily
large fraction function evaluations even almost used optimal local search
explore exploit achieve goals number
function evaluations used known ahead use arbitrarily small fraction
evaluations target function f exploration compare performance
explore exploit section particular match
performance guarantees metamax family use spend half
time exploration half exploitation exploration part
uniform allocation strategy used finite number local search
schedule luby et al used infinitely many local search
although theoretical guarantees proved metamax family hold
explore exploit experiments metamax family seems
behave superior compared expected
theoretical give sufficient guidance chose parameter
h hr time varying version h considered metamax k
simplicity ease presentation require sufficiently fast
exponential decay h dependent cannot determined advance
sufficiently fast decay rate would ensure example metamax k
could use stronger theorem would never deal


fiefficient multi start strategies local search

case bound theorem holds one may easily choose h
function decreases super exponentially would make asymptotic bounds work
however would slow exploration extreme case hr n excluded
conditions exploration would performed would use
actually best local search practice found
appropriate chose hr decay exponentially furthermore found even
effective gradually decrease decay rate enhance exploration time elapses
rationale behind assumption good
less converged may greater potential exploration
improve estimates finally connection g h
investigated
keeping limitations theoretical mind still believe
theoretical analyses given provide important insight may guide
potential user practical applications especially since properties metamax
family proved asymptotic regime e g rounds quite short
usually observed practice well furthermore think possible
improve analysis bound thresholds become valid
reasonable values would require different therefore left
future work

experiments
variants metamax tested synthetic real examples since
negligible difference performance metamax metamax
following present metamax k metamax first demonstrate performance optimizing synthetic function spsa
local search next behavior tested standard data
sets metamax applied tuning parameters machine learning
classification task solved neural network parameters
training back propagation fine tuned metamax combined spsa
metamax applied boost performance k means clustering end
section compare experiments theoretical bounds obtained
section
experiments accordance simplifying assumptions introduced
section main difference individual runs particular local search
starting point obviously general diversification techniques exist
example parameters local search could vary instance
instance including running instances different local search parameter would select actually employed search initialization starting
point parametrization instance could depend delivered
example relative difference average error emetamax metamax
emetamax metamax optimizing parameters multi layer perceptron learning letter
data set see section especially figure right details standard
deviation averaged
experiments relative difference defined


fiemetamax emetamax max emetamax emetamax



figyorgy kocsis

existing instances although metamax strategies could applied
general scenarios behavior better studied simpler scenario hence
experiments correspond setup
optimizing parameters spsa
section compare two versions metamax six multi start
strategies including three constant three variable number
instances strategies run fixed time steps target function
evaluated times together local search instances note several reference
strategies use parameter
used spsa simultaneous perturbation stochastic approximation spall
base local search cases spsa local search sampling
function uses gradient descent stochastic approximation derivative
actual location xt xt xt spsa estimates lth partial derivative f
f xt bt f xt bt

ft l xt l
bt l
bt l bernoulli random variables components vector
bt uses sampling function st xt xt ft xt choose next point
sampled
xt l xt l ft l xt l
l scalar parameters
implementation followed guidelines provided
spall gain sequence perturbation size
values vary
different experiments chosen heuristically experience similar
cause goal experiments
provide fast solutions global optimization hand demonstrate
behavior multi start compared addition two evaluations
required perturbed points evaluate function current point xt
starting point chosen randomly function evaluated first point
six reference metamax k metamax compared following
unif selects constant number instances spsa uniformly
implementation instance mod k selected time k denotes
number instances
thrasc threshold ascent streeter smith b begins selecting fixed number instances phase
time step thrasc selects best estimates produced far
instances ai k previous time steps ai counts
many estimates produced ai denoting latter value si time
selects instance index argmaxi u si ni ni ni


fiefficient multi start strategies local search

number times ith instance selected time
u n



p
n
n

ln k parameters experiments
best value appeared set note threshold
ascent developed maximum k armed bandit nevertheless
provides sufficiently good performance setup test experiments
rand random search seen running sequence spsa
instance used exactly one step evaluation
random starting point spsa
luby work luby et al method runs several
instances spsa sequentially ith instance run ti steps
ti defined

k
k
ti
ti k
k k
definition produces schedule first k instances
one run k steps two k steps four k steps
ee unif instance explore exploit first
steps unif used exploration subsequently exploration
phase spsa instance achieved highest value exploration phase
selected
ee luby similar ee unif except luby used exploration
versions metamax tested motivated fact spsa
known converge global optimum exponentially fast f satisfies restrictive
conditions gerencser vago chose hr n decays exponentially fast
control exploration far suboptimal instances allowed hr n
time varying function changes tr total number function calls
evaluate f equally total number steps taken far thus
round r used


hr n en

tr



note used time varying version hr case metamax k
latter easily extended situation omitted simplify
presentation
fixed number local search instances metamax k unif
ee unif thrasc number instances k set simulations
choice provided reasonably good performance analyzed
multi start tested two versions synthetic function
tuning parameters learning two standard data sets


figyorgy kocsis

synthetic function slightly modified version griewank function griewank




xl x x l
cos
f x

l
l
l
x x xd xl constrained interval
dimensional dimensional cases
parameters spsa dimensional case
dimensional case performance search
measured error defined difference maximum value
function case best obtained search given
number steps multi start strategies two dimensional test functions shown figure error curve averaged
runs strategy run steps iterations one may observe
cases two versions metamax converge fastest thrasc
better unif luby seems fairly competitive two two exploreand exploit type ee unif ee luby similar performance dimensional function clearly better non exploiting base
dimensional function behavior somewhat pathological sense low
values performances best among increasing
error actually increases respective base achieve smaller errors
values random search seems option dimensional function
similar obtained dimensions pathological behavior
explore exploit start appear gradually starting dimensional
function pronounced dimensions onwards limited experimental data
obtained higher dimensions averaged hundred runs shows
superiority metamax preserved high dimensional well
reason pathological behavior explore exploit strategies e
error curves monotone decreasing number iterations illustrated
follows assume two spsa instances one converging global optimum
another one converging suboptimal local optimum assume first
steps optimal gives better suboptimal takes
reaches local maximum run even optimal
beats suboptimal one exploration stopped first last
regime explore exploit choose first optimal local search instance
whose performance may get quite close global optimum exploitation phase
even stopped first regime exploration stopped middle regime
suboptimal search instance selected exploitation whose performance may
even get close global optimum scenario error exploitation
phase e end lower small increases higher values decrease
error increasing assured optimal instance converges
exploration phase past suboptimal local optima selecting optimal
local search instance exploitation scenario error decrease fast
modification made order significant differences values function
global maximum local maxima















average error

average error

efficient multi start strategies local search










rand
luby
unif
thrasc
ee luby
ee unif
metamax
metamax

e

e





rand
luby
unif
thrasc
ee luby
ee unif
metamax
metamax

e









e







iteration









iteration

figure average error multi start strategies dimensional left
dimensional right modified griewank function confidence intervals
shown color corresponding curves note
intervals small








average error

average error






rand
luby
unif
thrasc
ee luby
ee unif
metamax
metamax

e

e







rand
luby
unif
thrasc
ee luby
ee unif
metamax
metamax

e











iteration

e













iteration

figure average error multi start strategies tuning parameters multilayer perceptron vehicle data set left letter data set right
confidence intervals shown color corresponding
curves

initially increase may decrease till converges
quite similar observe figure right pathological behavior
becomes transparent many local search length
exploitation phase scales number local search instances length
exploration instance kept fixed analyzing experimental data shows
complex versions scenario outlined occurred simulations
main cause observed pathological behavior non monotonicity error
curves
tuning parameters learning used two standard data
sets uci machine learning repository asuncion newman vehicle


figyorgy kocsis

letter multilayer perceptron learning weka witten frank
back propagation used training phase two parameters
tuned learning rate momentum range size
hidden layer multilayer perceptron set number epochs
parameters spsa
dimensional griewank function previous experiment parameters
chosen experience rate correctly classified items test set
vehicle multilayer perceptron varying values two parameters shown
figure highest rate similarly classification rate letter
shown figure highest rate
error rates optimized multilayer perceptron data sets vehicle
letter shown figure parameters learning tuned
multi start strategies error cases difference
best classification rate obtained respectively
best classification rate obtained multi start strategies given number steps
shown averaged runs observe metamax
increasing number instances converged fastest average three
strategies fixed number instances nearly identical luby
explore exploit variant slightly worse random search
slowest although performed nearly badly synthetic functions
reason random search relatively better performance relative
used spsa could twofold large parts error surface offer fairly small error
ii error surface less smooth therefore spsa less successful
gradient information explore exploit variants performed well vehicle data
set initially performance worsened larger values compared metamax
extent coupled observation figure
right would suggest explore exploit variants competitive small values
despite asymptotic guarantees
summary metamax increasing number instances provided far best performance tests usually requiring significantly
fewer steps optimum e g letter data set
metamax found global optimum runs time steps
conclude metamax converged faster multi start strategies investigated four test cases notable advantage difficult surfaces least
gradient optimization viewpoint induced classification tasks
k means clustering
section consider partitioning set dimensional real vectors
xj rd j n clusters cluster si represented center
reconstruction point ci rd n cost function minimized sum
distances
pn x pci data points corresponding centers want
minimize xsi x ci two necessary conditions optimality see e g
linde buzo gray gersho gray n
si x x ci x cj j n




fiefficient multi start strategies local search

figure classification rate vehicle data set rates plotted subtracting
thus global optima scattered black spots
corresponding value equal

figure classification rate letter data set rates plotted subtracting thus global optima scattered black spots
corresponding value equal

ties broken arbitrarily
ci argmin
crd

x

xsi



x c



figyorgy kocsis

p

x



usual choice squared euclidean distance case ci xs
si according necessary conditions k means generalized lloyd
see e g linde et al gersho gray alternates partitioning data set according centers fixed recomputing
centers partitioning kept fixed easily seen cost
error cannot increase steps hence converges
local minimum cost function practice stops
insufficient decrease cost function however k means often trapped
local optimum whose value influenced initial set centers spsa
restarting k means different initialization may finding global optimum
consider two initialization techniques first termed k means chooses centers
uniformly random data points second k means arthur vassilvitskii
chooses initial center uniformly random data set chooses
centers data points probability proportional distance
data point closest center already selected

k means usually terminates relatively small number steps
thus multi start strategies bounded number instances would run active local
search therefore appear particularly attractive however
natural domain consider strategy starts instance previous
finished strategy referred subsequently serial mentioned
considerations test metamax variant applicable
unbounded number instances experiments spsa used hr
note theoretical indicate k means may converge exponential
rate particular kieffer showed rate convergence exponential
random variables log concave densities dimension provided logarithm
density piecewise affine
two multi start strategies serial metamax tested data set cloud
uci machine learning repository asuncion newman data set
employed arthur vassilvitskii well number clusters set
ten performance multi start strategies defined difference
smallest cost function obtained strategy given number steps smallest
cost seen experiments averaged runs
plotted figure initialization methods metamax strategy converges
faster serial strategy note data set k means
clever initialization procedure yields faster convergence standard k means
uniform initialization consistent presented arthur
vassilvitskii

extension clustering random variables well known straightforward omitted
consider clustering finite data sets
note metamax practical modification local search
terminated chosen anymore clearly improves performance
chosen anymore improvement observed



fiefficient multi start strategies local search







average error

average error


















serial kmeans
metamax kmeans













iteration

e

serial kmeans
metamax kmeans













iteration

figure average error multi start strategies k means left kmeans right confidence intervals shown color
corresponding curves

practical considerations
experiments metamax presented observed
number instances r shown figure grows rate tr ln tr recall
tr total number function calls evaluate f total number steps
instances end round r hand derivation

theoretical bounds see theorem theorem used bound r tr
contrast quadratic penalty suggested theorem plugging tr ln tr
estimate r theorem would logarithmic factor calls
evaluate f total number steps needed achieve performance search
started attraction region optimum
finally perhaps main practical question concerning metamax family multistart decide use rule thumb say
sufficiently large performance difference average run
local search best one clearly single local search produces
acceptable worth effort run several instances local search
especially complicated schedule many real often case
relatively easy get close optimum may acceptable
applications approaching optimum greater precision hard latter
importance metamax variants may useful last one may
wonder computational costs discussed
consider case evaluation target function expensive clearly
case griewank function used demonstrate basic properties
holds many optimization practice including
experiments considered function evaluation
indeed expensive depends available data overhead introduced
metamax depends number rounds metamax k
upper convex hull set k points round
worst case take long k calculations practice usually


figyorgy kocsis

number instances ln



griewank
griewank
vehicle
letter
k means
k means
min
max






















iteration

figure number instances r metamax average number
instances shown six benchmarks griewank function dimensional parameter tuning multilayer perceptron vehicle
letter data set clustering k means k means
maximum minimum number instances runs benchmarks
shown one notice larger values tr tr ln tr r
tr ln tr

much cheaper upper convex hull determined point corresponds
actually best estimate point corresponds least used
requires k computations even less special ordering tricks
introduced since target function f evaluated least twice round average
k computational overhead needed evaluation f worst
case practically reduced k even less similar considerations hold
metamax metamax resulting average r worst case
overhead call f r rounds closer r even less practice
examples considered apart case griewank function amount
overhead negligible relative computational resources needed evaluate
f single point

conclusions
provided multi start strategies local search strategies
continuously estimate potential performance instance optimistic
way supposing convergence rate local search unknown
constant every phase resources allocated instances could converge
optimum particular range constant three versions
presented one able follow performance best fixed number
local search instances two gradually increasing number
local search achieve global consistency theoretical analysis asymptotic


fiefficient multi start strategies local search

behavior given specifically mild conditions
function maximized e g set values local maxima dense
global maximum best metamax preserves performance local
search original function class quadratic increase
number times target function needs evaluated asymptotically simulations
demonstrate work quite well practice
theoretical bound suggests target function evaluated
quadratic factor times achieve performance search started
attraction region optimum experiments found logarithmic
penalty clear whether difference slightly conservative
asymptotic analysis choice experimental settings finite sample
analysis interest experiments indicate metamax
provides good performance even relatively small number steps taken
local search sense provides speed compared
approaches even number times target function evaluated e total
number steps taken together relatively small finally
future work needed clarify connection convergence rate optimal
g function hr used exploration

acknowledgments
authors would thank anonymous referees numerous insightful
constructive comments supported part mobile innovation
center hungary national development agency hungary
technological innovation fund ktia otka cnk pascal network
excellence ec grant parts presented ecml
kocsis gyorgy

appendix proof lemma
bn since un almost everywhere e egoroffs
fix let un f f x
theorem see e g ash doleans dade implies event e e
p e un uniformly almost everywhere e second part
lemma follows definition uniform convergence


references
adam k learning searching best alternative journal economic
theory
arthur vassilvitskii k means advantages careful seeding
proceedings th annual acm siam symposium discrete pp

ash r b doleans dade c probability measure theory academic press


figyorgy kocsis

asuncion newman j uci machine learning repository
auer p cesa bianchi n fischer p finite time analysis multiarmed
bandit machine learning
bartz beielstein experimental evolutionary computation
experimentalism natural computing series springer york
battiti r brunato mascia f reactive search intelligent optimization
vol operations computer science interfaces springer verlag
beck c j freuder e c simple rules low knowledge selection
regin j c rueher eds cpaior lecture notes computer science
pp springer
carchrae beck j c low knowledge control proceedings
nineteenth national conference artificial intelligence aaai pp
cicirello v smith f heuristic selection stochastic search optimization
modeling solution quality extreme value theory proceedings th
international conference principles practice constraint programming pp
springer
cicirello v smith f max k armed bandit model exploration
applied search heuristic selection proceedings twentieth national
conference artificial intelligence pp
finkel e kelley c convergence analysis direct tech
rep crsc tr ncsu mathematics department
gagliolo schmidhuber j learning dynamic portfolios annals
mathematics artificial intelligence ai math special
issue
gagliolo schmidhuber j learning restart strategies veloso ed
ijcai twentieth international joint conference artificial intelligence
vol pp aaai press
gagliolo schmidhuber j selection bandit
unbounded losses blum c battiti r eds learning intelligent optimization vol lecture notes computer science pp springer
berlin heidelberg
gerencser l vago z mathematics noise free spsa proceedings
ieee conference decision control pp
gersho gray r vector quantization signal compression kluwer
boston
griewank generalized descent global optimization journal optimization theory applications
hart p nilsson n raphael b formal basis heuristic determination
minimum cost paths systems science cybernetics ieee transactions



fiefficient multi start strategies local search

hoos h h stutzle towards characterisation behaviour stochastic
local search sat artificial intelligence
horn optimal global optimization case unknown lipschitz
constant journal complexity
hutter f hoos h h leyton brown k stutzle paramils automatic
configuration framework journal artificial intelligence

jones r perttunen c stuckman b e lipschitzian optimization without
lipschitz constant journal optimization theory applications

kautz h horvitz e ruan gomes c selman b dynamic restart policies proceedings eighteenth national conference artificial intelligence
aaai pp
kieffer j c exponential rate convergence lloyds method ieee trans
inform theory
kocsis l gyorgy efficient multi start strategies local search
buntine w grobelnik mladenic shawe taylor j eds machine
learning knowledge discovery databases vol lecture notes computer science pp springer berlin heidelberg
linde buzo gray r vector quantizer design ieee
transactions communications com
luby sinclair zuckerman optimal speedup las vegas
information processing letters
mart r moreno vega j duarte advanced multi start methods gendreau potvin j eds handbook metaheuristics nd edition edition
springer
nesterov introductory lectures convex optimization basic course
kluwer academic publishers
ribeiro c rosseti vallejos r use run time distributions evaluate
compare stochastic local search stutzle birattari hoos
h eds engineering stochastic local search designing implementing
analyzing effective heuristic vol lecture notes computer science
pp springer berlin heidelberg
spall j hill stark theoretical framework comparing several stochastic
optimization approaches calafiore g dabbene f eds probabilistic
randomized methods design uncertainty chap pp springerverlag london
spall j c multivariate stochastic approximation simultaneous perturbation
gradient approximation ieee transactions automatic control
spall j c implementation simultaneous perturbation stochastic optimization ieee transactions aerospace electronic systems


figyorgy kocsis

streeter j smith f asymptotically optimal max
k armed bandit proceedings twenty first national conference
artificial intelligence eighteenth innovative applications artificial intelligence conference pp
streeter j smith f b simple distribution free max
k armed bandit principles practice constraint programming cp th international conference cp nantes france september
proceedings pp
vilalta r drissi perspective view survey meta learning artificial
intelligence review
witten h frank e data mining practical machine learning tools
techniques nd edition morgan kaufmann san francisco
zabinsky z b bulger khompatraporn c stopping restarting strategy
stochastic sequential search global optimization j global optimization





