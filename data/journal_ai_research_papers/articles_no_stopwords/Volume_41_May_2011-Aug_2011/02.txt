Journal Artificial Intelligence Research 41 (2011) 69-95

Submitted 10/10; published 05/11

Value Information Lattice: Exploiting Probabilistic
Independence Effective Feature Subset Acquisition
Mustafa Bilgic

mbilgic@iit.edu

Illinois Institute Technology
Chicago, IL 60616 USA

Lise Getoor

getoor@cs.umd.edu

University Maryland
College Park, MD 20742 USA

Abstract
address cost-sensitive feature acquisition problem, misclassifying instance costly expected misclassification cost reduced acquiring
values missing features. acquiring features costly well, objective acquire right set features sum feature acquisition
cost misclassification cost minimized. describe Value Information Lattice
(VOILA), optimal efficient feature subset acquisition framework. Unlike common
practice, acquire features greedily, VOILA reason subsets features.
VOILA efficiently searches space possible feature subsets discovering exploiting
conditional independence properties features reuses probabilistic inference computations speed process. empirical evaluation five
medical datasets, show greedy strategy often reluctant acquire features,
cannot forecast benefit acquiring multiple features combination.

1. Introduction
often need make decisions take appropriate actions complex uncertain
world. important subset decisions formulated classification problem,
instance described set features one finite categorical options
chosen based features. Examples include medical diagnosis patients
described lab tests diagnosis made disease state patient,
spam detection email described content email client needs
decide whether email spam.
Much research done learn effective efficient classifiers assuming
features describing entities fully given. Even though complete data
assumption might hold domains, practice features describe entities
often missing values. certain domains medical diagnosis decision
made based number features include laboratory test results, missing feature
values acquired cost performing related tests. cases, need
decide tests perform order. answer question, course,
depends important get correct classification decision. Put alternatively,
cost incorrect classification (e.g., misdiagnosis) determines much
willing spend expensive tests. Thus, need devise feature acquisition policy
determine tests perform order stop make
c
2011
AI Access Foundation. rights reserved.

fiBilgic & Getoor

final classification decision total incurred cost, feature acquisition cost
expected misclassification cost, minimized.
Devising optimal policy general requires considering possible permutations
features expected values. provide intuition, features might
useful acquired together, cost benefit acquiring features
depend features acquired values turned
be. devising optimal policy intractable general, previous work
greedy (Gaag & Wessels, 1993; Yang, Ling, Chai, & Pan, 2006), approximated value
information calculations (Heckerman, Horvitz, & Middleton, 1993), developed
heuristic feature scoring techniques (Nunez, 1991; Turney, 1995).
greedy approach, however, least two major limitations. First,
considers feature isolation, cannot accurately forecast value acquiring
multiple features together, causing produce sub-optimal policies. Second, greedy
strategy assumes features acquired sequentially value feature
observed acquiring next one. assumption, however, often
practical. example, doctors typically order batches measurements simultaneously
blood count, cholesterol level, etc., possibly order another batch
results arrive. two limitations greedy approach make necessary reason
sets features.
Reasoning sets features, hand, poses serious tractability challenges.
First all, number subsets exponential size feature set. Second,
judging value acquiring set features requires taking expectation
possible values features set, exponential number
features. good news, however, need consider possible subsets
features practice; certain features render features useless, features
useful acquired together. example, X-Ray might render skin test
useless diagnosing tuberculosis. Similarly, chest pain alone might useful
differentiating cold heart disease; becomes useful combined
features, blood test.
article, describe data structure discovers exploits types
constraints (features render features useless features useful
acquired together) underlying probability distribution. propose Value
Information Lattice (VOILA) reduces space possible subsets exploiting
constraints features. Additionally, VOILA makes possible share value
information calculations different feature sets reduce computation time.
article builds upon earlier work (Bilgic & Getoor, 2007). contributions
article include:
introduce two additional techniques sharing computations different
subsets features. new techniques based information caching
utilizing paths underlying Bayesian network.
experiment asymmetric misclassification costs addition symmetric
costs. asymmetric setup reflects realistic case provides new insights.
addition feature acquisition costs defined Turney (1995), generate
experiment synthetic feature costs. synthetic feature costs capture
70

fiValue Information Lattice

complex feature acquisition costs allows leeway various acquisition
strategies differ.
remainder article organized follows. describe notation
problem formulation Section 2. describe reduce search space
share computations using VOILA Section 3. show experimental results Section 4,
discuss related work Section 5, discuss future work Section 6. conclude
Section 7.

2. Notation Problem Formulation
main task classify given instance missing feature values incur
minimum acquisition misclassification cost. Let instance described set
features X = {X1 , X2 , . . . , Xn } let random variable representing class.
assume joint probability distribution P (Y, X) given concern
feature acquisition inference (note conditional distribution P (Y |X)
appropriate, features assumed unobserved initially). purpose
article, assume given Bayesian network, joint probabilistic
model allows us efficiently answer conditional independence queries used.
notation, bold face letter represents set random variables non-bold face
letter represents single random variable. example X represents set features,
whereas Xi X represents feature X represents class variable. Additionally,
capital letter represents random variable, lowercase letter represents particular
value variable; applies individual variables sets variables.
example, represents variable, represents particular value take.
addition probabilistic model, given cost models specify
feature acquisition costs misclassification costs. Formally, assume
feature acquisition cost function given subset features, S, set features
whose values known (evidence) E, returns non-negative real number C(S | e).
assume misclassification cost model returns misclassification
cost cij incurred assigned yi correct assignment yj . cost
functions, model non-static feature acquisition costs; is, cost acquiring
feature Xi depend acquired far values
(e) well acquired conjunction feature (S \ {Xi }). Moreover,
misclassification cost model assume symmetric costs; different kids errors (false
positives negatives) different costs.
Figure 1 shows simple example configuration two features, X1 X2 ,
class variable . simple example, joint distribution P (X, ) represented
table, feature costs simple independent costs X1 X2 , misclassification cost symmetric types misclassifications cost correct
classification cost anything.
diagnostic policy decision tree node represents feature
branches nodes represent possible values features. path
policy, ps , represents ordered sequence feature values s. often use ps
represent ordered version s. Typically, order features set
important computing feature costs, cost feature depend values
71

fiBilgic & Getoor

Figure 1: Example configuration two features X1 X2 class variable .
table left right represent: joint probability distribution P (X1 , X2 , ),
feature costs, misclassification costs.

previously acquired features. order features irrelevant computing
probability P (s). example conditional policy using example configuration
Figure 1 given Figure 2.
policy two types costs: feature acquisition cost misclassification
cost. costs defined terms costs associated following paths
policy. first describe compute feature acquisition cost path
describe compute associated expected misclassification cost. Finally, show
compute expected total cost policy using total costs associated
path.
naive version, feature cost path ps sum costs
features appear path. However, practice, cost feature depend
features acquired far observed values acquired features.
example, performing treadmill test (asking patient run treadmill
measure heart beat, etc.) riskier ordered cholesterol test
result turned high, putting patient high risk heart disease. account
types costs, order features ps matters, total feature cost
path summation individual feature costs conditioned values
features precede features consideration:
F C(ps ) =

n
X

C(ps [j] | ps [1 : j])

j=1

ps [j] represents j th feature ps ps [1 : j] represents feature values 1
j ps .
reach end path, need make classification decision.
case, simply utilize Bayesian decision theory choose decision minimum
risk (i.e., misclassification cost). find decision using probabilistic model
72

fiValue Information Lattice

Figure 2: example conditional policy features X1 , X2 class variable .
non-leaf node represents feature acquisition, probability distribution
possible values, cost feature. path (e.g., X1 = T, X2 = )
acquisition cost expected misclassification cost. policy overall
expected total cost ETC, sum total costs path, weighted
probability following path.

compute probability distribution P (Y | ps ) choose value leads
minimum expected cost. Note order features values matter
case; P (Y | ps ) = P (Y | s). expected misclassification path, EM C(ps ),
73

fiBilgic & Getoor

computed follows:
EM C(ps ) = EM C(s) = min
yi

X

P (Y = yj | s) cij

(1)

yj

total cost incur following path policy simply sum feature
expected misclassification costs path:
C(ps ) = F C(ps ) + EM C(ps )
Finally, compute expected total cost policy using total costs
individual paths ps . path ps probability occurrence real world.
probability easily computed generative probability model assumed. simply P (s). expected total cost policy sum total
cost path, C(ps ), weighted probability following path, P (s):
ET C() =

X

P (s)T C(ps )

(2)

ps

objective feature acquisition inference is, given joint probabilistic
model cost models acquisition misclassification, find policy
minimum expected total cost. However, building optimal decision tree known
NP-complete (Hyafil & Rivest, 1976). Thus, research greedy choosing
best feature reduces misclassification costs lowest cost (e.g.,
Gaag & Wessels, 1993; Dittmer & Jensen, 1997) developed heuristic feature scoring
techniques (e.g., Nunez, 1991; Tan, 1990).
greedy strategy, path policy extended feature reduces
misclassification cost lowest cost. specifically, path
ps replaced new paths psx1 , psx2 , . . . , psxni x1i , x2i , . . . , xni values


Xi take Xi feature highest benefit. define benefit
feature Xi given path ps reduction total cost path path
expanded possible values Xi . formally,
Benef it(Xi | ps ) , C(ps )

n
X

P (xji | s)T C(psxj )


j=1

= F C(ps ) + EM C(s)

n
X



P (xji | s) F C(psxj ) + EM C(s xji )


j=1


= F C(ps )

n
X


P (xji | s)F C(psxj ) + EM C(s)


j=1

n
X

P (xji | s)EM C(s xji )

j=1

= C(Xi | s) + EM C(s)

P (xji | s)EM C(s xji )

j=1

= F C(ps ) (F C(ps ) + C(Xi | s)) + EM C(s)
n
X

n
X

P (xji | s)EM C(s xji )

j=1

74

fiValue Information Lattice

Note that, last two terms equivalent definition expected value information, EVI, (Howard, 1966):
EV I(Xi | s) = EM C(s)

n
X

P (xji | s)EM C(s xji )

(3)

j=1

Substituting EVI, definition benefit becomes intuitive:
Benef it(Xi | ps ) = Benef it(Xi | s) = EV I(Xi | s) C(Xi | s)

(4)

definition, greedy strategy iteratively finds feature highest
positive benefit (value cost difference), acquires it, stops acquisition
features positive benefit value.
note straightforward define EVI Benefit set S0 features
single feature. difference expectation needs
taken joint assignments, s0 , features set S0 .
EV I(S0 | s) = EM C(s)

X

P (s0 | s)EM C(s s0 )

(5)

s0

and,
Benef it(S0 | s) = EV I(S0 | s) C(S0 | s)

(6)

problems greedy strategy mentioned
P earlier. First,
short-sighted. exist sets X Benef it(S) >
Benef it(Xi ).
Xi

easier see, example, XOR function, = X1 XOR X2 , X1 X2
alone useful determinative together. Due relationship, greedy
policy guaranteed optimal. Moreover, greedy policy prematurely stop
acquisition single feature seems provide positive benefit.
second problem greedy strategy often need acquire set
features simultaneously. example, doctor orders set lab tests s/he sends
patient lab, blood count, cholesterol level, etc. rather ordering single test,
waiting result ordering next one. However, traditional greedy strategy
cannot handle reasoning sets features naturally.
would able reason sets features two reasons.
objective article is, given existing potentially empty set already observed
features E observed values e, find set highest benefit:
L(X | e) , argmax Benef it(S | e)

(7)

SX\E

two problems formulation: first, number subsets X \ E
exponential size X \ E, second, set S, need take expectation
joint assignments features set. address two problems using
data structure describe next.
75

fiBilgic & Getoor

3. Value Information Lattice (VOILA)
VOILA makes reasoning sets features tractable reducing space possible
sets allowing sharing EVI computations different sets. section,
first explain reduce space explain techniques computation
sharing.
3.1 Reducing Space Possible Sets
domains, often complex interactions features
class label. Contrary Naive Bayes assumption, features often conditionally
independent given class label. features useless features
already acquired. example chest X-Ray typically determinative skin
test tuberculosis. Similarly, features useless alone unless accompanied
features. example, chest pain alone might due variety sicknesses;
accompanied high cholesterol, could indicate heart disease, whereas
combined fever, cold might probable. types interactions
features allow us reduce space candidate feature sets.
mentioned problem formulation, assumed already
joint probabilistic model features class variable, P (Y, X). find
two types feature interactions asking probabilistic independence queries using
P (Y, X). Specifically, assume given Bayesian network represents
P (Y, X). Bayesian network allow us find types interactions
standard d-separation algorithms.
Definition 1 set X \ E irreducible respect evidence e Xi S, Xi
conditionally independent given e \ {Xi }.
Given Bayesian network X , straightforward check irreducibility
d-separation (Pearl, 1988).
Proposition 1 Let S0 maximal irreducible subset respect e. Then, EV I(S |
e) = EV I(S0 | e).
Proof: Let S00 = \ S0 . S0 maximal irreducible set, S0 E d-separates S00 .
Otherwise, could make S0 larger including non-d-separated element(s) S00
S0 . Thus, P (Y | e, s) = P (Y | e, S0 , S00 ) = P (Y | e, S0 ). Substitution Equations
1 5 yields desired property.
Note assumption C(S0 | e) C(S | e) S0 S, suffices
consider irreducible sets find optimal solution objective function
Equation (7). VOILA data structure contains irreducible feature subsets
X, respect particular set evidence e. next define VOILA formally.
Definition 2 VOILA V directed acyclic graph node corresponding
possible irreducible set features, directed edge feature set
node corresponds direct (maximal) subset S. subset relationships
lattice defined directed paths V.
76

fiValue Information Lattice

(a)

(b)

Figure 3: (a) simple Bayesian network illustrating dependencies attributes
class variable. (b) VOILA corresponding network.

Figure 3(a) shows simple Bayesian network corresponding VOILA, respect
empty evidence set, shown Figure 3(b). Notice VOILA contains
irreducible subsets given Bayesian network; instance, VOILA contain
sets include X1 X2 X1 d-separates X2 . observe
number irreducible subsets 9 contrast 24 = 16 possible subsets. Moreover,
note largest subset size 3 contrast 4. smaller feature sets sizes
dramatic effect value information calculations. fact, savings
make solving objective function optimally (Equation (7)) feasible practice.
3.2 Sharing EVI Calculations
Finding set highest Benefit (Equation 6) requires computing EV I(S)
(Equation 5). However, computing EV I(S) requires taking expectation possible
values features S. Moreover, searching best set among irreducible sets
requires us compute EVI irreducible sets. make computations tractable
practice, VOILA allows computation sharing nodes. article, describe
three possible ways sharing computations nodes VOILA.
77

fiBilgic & Getoor

3.2.1 Subset Relationships
VOILA exploits subset relationships different feature sets order avoid
computing EVI nodes. First all, directed path node S1 S2
VOILA, S1 S2 thus EV I(S1 | e) EV I(S2 | e)1 . assume
directed path Si Sj EV I(Si | e) = EV I(Sj | e). Then, nodes
path EVI, thus need computation
subsets. algorithm makes use observation given Algorithm 1.
Algorithm 1: Efficient EVI computation using VOILA.
Input: VOILA V current evidence E
Output: VOILA updated correct EVI values
1 root node(s)
2
value EV I(S | e); ub(S) value; lb(S) value
3
ub(descendants(S)) value
4
5
6
7
8
9
10

leaf node(s)
value EV I(S | e); ub(S) value; lb(S) value
lb(ancestors(S)) value
node lb(S) 6= ub(S)
value EV I(S | e); ub(S) value; lb(S) value
lb(ancestors(S)) value
ub(descendants(S)) value

important point nodes VOILA irreducible sets. Unless
totally useless features change P (Y ) observed,
two distinct nodes EVI values exactly equal. However, statement
true context-specific independencies (independencies hold
certain assignments variables) underlying Bayesian network.
description implementation, used standard d-separation variable level; one
imagine going one step define irreducible sets variable
level d-separation context specific independencies.
order share computations different nodes lattice, keep lower
upper bounds EVI node. lower bound determined values
descendants node whereas upper bound determined values
ancestors. First, initialize bounds computing value information
boundary lattice, i.e., root node(s) leaf node(s) (lines 16) 2 . Then,
loop nodes whose upper bounds lower bounds equal (line 710),
computing values updating bounds ancestors descendants.
algorithm terminates upper bounds lower bounds nodes become
tight. order choose nodes line 7 number sets
value calculated minimum still open question. possible heuristic perform
1. superset always higher equivalent EVI (Equation (5)) subset.
2. need compute EVI root nodes; suffices compute node corresponds
Markov blanket . explained detail next section.

78

fiValue Information Lattice

binary search choose middle node path two nodes values
already calculated.
3.2.2 Information Pathways Underlying Bayesian Network
second mechanism VOILA uses share EVI computations edges
underlying Bayesian network. specifically make use following fact:
Proposition 2 S1 S2 , S1 d-separates S2 respect e,
EV I(S1 | e) EV I(S2 | e).
Proof: Consider S12 = S1 S2 . subset relationship, know EV I(S12 |
e) EV I(S1 | e) EV I(S12 | e) EV I(S2 | e).
EV I(S12 | e) = EM C(Y | e)

X

P (s12 | e)EM C(Y | e, s12 )

s12

= EM C(Y | e)

XX
s1

= EM C(Y | e)

XX
s1

= EM C(Y | e)

X

P (s1 , s2 | e)EM C(Y | e, s1 , s2 )

s2

P (s1 , s2 | e)EM C(Y | e, s1 )

s2

P (s1 | e)EM C(Y | e, s1 )

s1

= EV I(S1 | e)
EV I(S2 | e)
third line follows second fact S1 d-separates S2 thus
P (Y | s1 , s2 ) = P (Y | s1 ).
Corollary: Markov blanket , (i.e., parents, children, childrens
parents), set highest EVI search space, d-separates
remaining variables . Using corollary, need compute EVI
root nodes Algorithm 1; compute EVI root node corresponds
Markov blanket serves upper bound EVI remaining
root nodes.
relationships well exploited exploited subset relationships
above. Instead using subset relationships, use subset independence relationships. One simple way make use Algorithm 1 without modification
add edges S1 S2 independence property holds. example S1 S2 according toy network Figure 3(a) would S1 = {X1 }
S2 = {X2 }. Thus, add directed edge X1 X2 VOILA Figure 3(b)
Algorithm 1 work fine.
3.2.3 Incremental Inference
third last mechanism VOILA uses computation sharing
caching probabilities nodes. candidate set V, need compute
EV I(S | e) requires computing P (S | e) EM C(Y | S, e). cache
79

fiBilgic & Getoor

conditional probabilities node V, compute
P (S | e), find one
P
supersets Si = {Xi } compute P (S | e) = xi P (S, Xi = xi | e).
Computing EM C(Y | S, e) requires computing P (Y | S, e). perform computation efficiently, cache state junction tree node VOILA. Then,
find subset, Sj , = Sj {Xj }. compute P (Y | S, e) integrating
extra evidence junction tree node Sj used compute P (Y | Sj , e).
3.3 Constructing VOILA
Efficient construction VOILA straightforward task. brute force approach
would enumerate possible subsets X \ E subset check whether
irreducible. However, brute force approach clearly impractical. number
nodes VOILA expected much fewer number possible subsets X\E,
smart sets consider inclusion V, construct
efficiently. is, instead generating possible candidates checking whether
irreducible not, try generate irreducible sets. first introduce
notion dependency constraint explain use dependency constraints
efficiently construct VOILA.
Definition 3 dependency constraint feature Xi respect E
constraint E ensures dependency Xi exists.
instance, running example, dependency constraint X2 X1 ;
words, order X2 relevant, X1 included E. Similarly,
dependency constraint X4 X3 , meaning X3 must included SE. Specifically,
dependency constraint feature Xi requires Xj path Xi
included E Xj part v-structure; Xj part v-structure,
either Xj one descendants must included E (we refer latter
constraints positivity constraints). algorithm uses ideas compute
dependency constraints feature given Algorithm 2.
Algorithm 2: Dependency constraint computation Xi .
Input: Xi ,
Output: Dependency constraint Xi , denoted DC(Xi )
1 DC(Xi ) false
2 undirected path pj Xi
3
DCj (Xi ) true
4
Xk path pj
5
Xk cause v-structure
6
DCj (Xi ) DCj (Xi ) Xk
7
else
8
DCj (Xi ) DCj (Xi ) (Xk Descendants(Xk ))
9

DC(Xi ) DC(Xi ) DCj (Xi )

80

fiValue Information Lattice

dependency constraints used check whether set irreducible
potentially irreducible. Intuitively, set potentially irreducible irreducible
possible make set irreducible adding features it. formally,
Definition 4 set X \ E potentially irreducible respect evidence e if,
irreducible exists non-empty set features S0 X \ {E S}
S0 irreducible.
Potential irreducibility possible due non-monotonic nature d-separation. is,
feature d-separated become dependent consider combination
features. example, running example, {X4 } irreducible, X4
d-separated , whereas {X3 , X4 } irreducible.
use dependency constraints check whether set irreducible potentially
irreducible. set irreducible dependency elements
exists, dependency constraint set conjunction dependency
constraints members. irreducibility checked setting elements
E true setting remaining elements X false evaluating
sets dependency constraint. running example, dependency constraint set
{X2 , X4 } X1 X3 . Assuming E = , set members {X2 , X4 } true,
set remaining features, X1 X3 , false, X1 X3 evaluates false thus
set irreducible. makes sense given evidence, X4 independent
, {X2 } useful feature set consider acquisition, {X2 , X4 } not.
Checking potential irreducibility similar. Set elements E true
above. Then, set positivity constraints members true. Finally,
set everything else false. Using example above, check whether {X2 , X4 }
potentially irreducible, set X2 = true, X4 = true. set X3 = true
positivity constraint X4 . Set remaining features, X1 , false. Evaluating
constraint X1 X3 yields true, showing {X2 , X4 } potentially irreducible (while
irreducible).
Given definitions irreducibility potential irreducibility mechanisms
check properties notion dependency constraints, next describe
algorithm construct VOILA.
VOILA construction proceeds bottom fashion, beginning lowest level,
initially contains empty set constructs new irreducible feature sets
adding one feature time VOILA structure. Algorithm 3 gives details
algorithm. algorithm keeps track irreducible feature sets IS, set
potentially irreducible feature sets PS. done processing feature Xij ,
remove PS potentially irreducible set cannot become irreducible Xij
re-considered (line 11).
3.3.1 Analysis VOILA Construction Algorithm
construction algorithm inserts node VOILA corresponding set
irreducible (lines 6 7). Moreover, keeping track potentially irreducible sets
(lines 810), generate every possible irreducible set generated. Thus, VOILA
contains possible irreducible subsets X.
81

fiBilgic & Getoor

Algorithm 3: VOILA construction algorithm.
Input: Set features X class variable .
Output: VOILA data structure V, given E.
1 Pick ordering elements X = Xi1 , Xi2 , . . . , Xin
2 {}; PS
3 j = 1 n
4
PS
5
S0 Xij ; DC(S0 ) DC(S) DC(Xij )
6
S0 irreducible
7
{S0 }; Add node corresponding S0 V
8
else
9
S0 potentially irreducible
10
PS PS {S0 }
11
12
13
14
15
16
17

Remove PS sets longer potentially irreducible
max = size largest IS; = {S | |S| = l}
l = 0 max 1

S0 Ll+1
S0
Add edge S0 V

worst-case running time algorithm still exponential number
initially unobserved features, X \ E, number irreducible sets potentially
exponential. running time practice, though, depends structure
Bayesian network VOILA based upon ordering variables line 1.
example, Bayesian network naive Bayes, subsets irreducible (no
feature d-separates feature class variable); thus, search space cannot
reduced all. However, naive Bayes makes extremely strong assumptions
unlikely hold practice. fact, empirically show experiments section five
real-world datasets, features often conditionally independent given class variable;
complex interactions thus number irreducible
subsets substantially smaller number possible subsets.
loop line 4 iterates irreducible potentially irreducible sets
generated far, number potentially-irreducible sets generated
depends ordering chosen. good ordering processes features literals
positivity constraints features dependency constraints earlier. is,
undirected path Xi includes Xj v-structure, good ordering puts Xj
earlier ordering everything Xj Xi . instance, sample
Bayesian network Figure 3(a), consider X3 earlier X4 . refer
ordering perfect satisfies positivity constraints. perfect ordering used,
VOILA construction algorithm never generates potentially irreducible set. Unfortunately,
82

fiValue Information Lattice

always possible find perfect ordering. perfect ordering possible two
features positivity constraint literal dependency constraints.
case occurs loop two v-structures
(Note even though Bayesian network directed acyclic graph, still contain
loops, i.e., undirected cycles). perfect ordering possible four five real world
datasets used.
3.4 Using VOILA Feature-value Acquisition
VOILA makes searching space possible subsets tractable practice. Using
flexibility, possible devise several different acquisition policies. describe two
policies example policies section.
first acquisition policy aims capture practical setting one
feature acquired once. policy constructed using VOILA follows.
path ps policy (which initially empty) repeatedly extended acquiring
set S0 V best Benef it(S0 | s, e). policy construction ends path
extended, i.e., candidate sets non-positive Benefit values path .
second acquisition policy adds look-ahead capability greedy policy.
is, rather repeatedly extending path ps policy feature Xi
highest Benef it(Xi | s, e), add look-ahead capability, first find set S0 V
highest Benef it(S0 | s, e). Then, instead acquiring features S0
once, policy, find feature Xi S0 highest
Benef it(Xi | s, e) acquire extend ps .

4. Experiments
experimented five real-world medical datasets Turney (1995) described
used paper. datasets Bupa Liver Disorders, Heart Disease, Hepatitis,
Pima Indians Diabetes, Thyroid Disease, available UCI Machine
Learning Repository (Frank & Asuncion, 2010). datasets varying number
features ranging five 20. Four five datasets binary labels, whereas
Thyroid dataset three labels.
dataset, first learned Bayesian Network provides joint
probability distribution P (Y, X) efficiently answers conditional independence queries
thorough d-separation (Pearl, 1988). built VOILA dataset using learned
Bayesian Network. first present statistics dataset, number features
number nodes VOILA, compare various acquisition policies.
4.1 Search Space Reduction
Table 1 shows aggregate statistics dataset, describing number features,
number possible subsets, number subsets VOILA, percent reduction
search space. table shows, number irreducible subsets substantially
fewer possible subsets. Thyroid Disease dataset, example, number
possible subsets million whereas number irreducible subsets fewer
83

fiBilgic & Getoor

Table 1: Aggregate statistics dataset. number irreducible subsets, i.e.,
number nodes VOILA, substantially fewer number possible
subsets.
Dataset
Bupa Liver Disorders
Pima Indians Diabetes
Heart Disease
Hepatitis
Thyroid Disease

Features

Subsets

Nodes VOILA

Reduction

5
8
13
19
20

32
256
8,192
524,288
1,048,576

26
139
990
18,132
28,806

19%
46%
88%
97%
97%

thirty thousand. enormous reduction search space makes searching
possible sets features tractable practice.
4.2 Expected Total Cost Comparisons
compared expected total costs (Equation 2) four different acquisition policies
dataset. policies follows:
Acquisition: policy acquire features; aims minimize
expected misclassification cost based prior probability distribution class
variable, P (Y ).
Markov Blanket: policy acquires every relevant feature, regardless misclassification costs. Market Blanket Bayesian network defined
parents, children, childrens parents (Pearl, 1988). Intuitively,
minimal set X (X \ S) | S.
Greedy: policy repeatedly expands path ps initially empty policy
acquiring feature Xi highest positive Benef it(Xi | s) (Equation
4). policy construction ends path extended feature
positive Benefit value.
Greedy-LA: policy adds look-ahead capability Greedy strategy.
policy repeatedly expands path ps initially empty policy first finding
set S0 highest positive Benef it(S0 | s) (Equation 6) acquiring
feature Xi S0 maximum Benef it(Xi | s) (Equation 4). policy
construction ends set positive Benefit value found path
policy.
feature costs dataset described detail Turney (1995). summary,
feature either independent cost, belong group features,
first feature group incurs additional cost. example, first feature
group blood measurements incurs overhead cost drawing blood patient.
feature costs based data Ontario Ministry Health (1992).
84

fiValue Information Lattice

Table 2: Example misclassification cost matrix (cij ) symmetric asymmetric misclassification costs. cij set way achieve prior expected misclassification
cost 1. symmetric cost case, choosing probable class leads
EM C = 1, whereas, asymmetric cost case, choosing either class
indifferent leads EMC 1.
Actual Class

Prior Probability

Pred. Class

Symm. Cost

Asymm. Cost

y1

P (y1 ) = 0.6510

y1
y2

0
2.866

0
2.866

y2

P (y2 ) = 0.3490

y1
y2

2.866
0

1.536
0

observed features assigned cost. example, four
five features Bupa Liver Disorders dataset, 13 19 features Hepatitis
dataset, six eight features Diabetes dataset, 16 20 features
Thyroid Disease dataset assigned cost. costs similar,
problem practically equivalent finding minimum size decision tree. provide
structure feature acquisition costs, experimented randomly generated
feature group costs. feature, randomly generated cost 1 100,
group generated cost 100 200. repeated experiments
three different seeds dataset.
misclassification costs defined paper Turney (1995). One reason
could easier define feature costs, defining cost misclassification non-trivial. Instead, Turney tests different acquisition strategies using
various misclassification costs. follow similar technique slight modification.
compare acquisition policies symmetric (cij = cji ) asymmetric
misclassification costs. able judge misclassification cost structure affects
feature acquisition, unify presentation, compare different acquisition strategies
priori expected misclassification costs, defined Equation (1). Specifically, compare acquisition policies various priori EMC achieved
varying cij accordingly. show example misclassification table EMC value
1 Table 2. real feature cost case, varied EMC 0 2000,
varied 0 4000 synthetic feature cost case.
compare Greedy, Greedy-LA, Markov Blanket policies plotting
much cost policy saves respect Acquisition policy. X axis
plots, vary priori expected misclassification cost using methodology
described above. plot savings axis. dataset, plot four different
scenarios: cross product {symmetric, asymmetric} misclassification costs, {real,
synthetic} feature costs.
results Liver Disorders, Diabetes, Heart Disease, Hepatitis, Thyroid
Disease given Figures 4, 5, 6, 7, 8 respectively. figure, symmetric
misclassification cost scenarios given sub-figures (a) (c), whereas asymmetric
85

fiBilgic & Getoor

(a)

(b)

(c)

(d)

Figure 4: Expected Total Cost (ETC ) comparisons Bupa Liver Disorders dataset.
priori class distribution follows: P (Y ) = [0.4959, 0.5041].

misclassification cost scenarios presented (b) (d). Similarly, real feature cost
scenarios given (a) (b) synthetic feature cost scenarios presented
(c) (d). next summarize results.
found Greedy policy often prematurely stopped acquisition, performing
even worse Markov Blanket strategy. true datasets,
regardless feature misclassification cost structures. fact Greedy
strategy perform worse Markov Blanket strategy really troubling. first,
might seem rather unintuitive Greedy strategy perform worse Markov
Blanket strategy. Part reason features belong groups first
feature group incurs overhead cost. Greedy strategy feature
considered isolation, overhead costs outweigh single features benefit,
Greedy look ahead, reluctant commit acquiring
first feature group.
86

fiValue Information Lattice

(a)

(b)

(c)

(d)

Figure 5: Expected Total Cost (ETC ) comparisons Pima Indian Diabetes dataset.
priori class distribution follows: P (Y ) = [0.6510, 0.3490].

Greedy-LA strategy never performs worse strategy setting.
misclassification cost structure (symmetric asymmetric) considerable
effect policies behaved. differences symmetric asymmetric cases particularly evident datasets class distribution
imbalanced, Diabetes (Figure 5), Hepatitis (Figure 7), Thyroid
Disease (Figure 8) datasets. differences due misclassification cost structure
summarized follows:
class distribution imbalanced misclassification cost symmetric, acquiring information cannot change classification decisions
easily due class imbalance, thus features high EVI values.
hand, misclassification costs asymmetric, features tend
higher EVI values. Thus, Greedy Greedy-LA strategies start
acquiring features earlier X axis asymmetric cases compared
87

fiBilgic & Getoor

(a)

(b)

(c)

(d)

Figure 6: Expected Total Cost (ETC ) comparisons Heart Disease dataset. priori
class distribution follows: P (Y ) = [0.5444, 0.4556].

symmetric counterparts. example, Thyroid disease dataset
real feature costs, Greedy strategy starts acquisition EMC
greater 600 symmetric misclassification costs (Figure 8(a)) whereas
starts acquiring EMC reaches 100 asymmetric case (Figure 8(b)). synthetic feature costs, results dramatic; neither
Greedy Greedy-LA acquires features symmetric cost case (Figure 8(c)), whereas start acquisition EM C = 200 asymmetric
case (Figure 8(d)).
realm results, slope savings asymmetric case much higher compared symmetric case.
misclassification cost structure causes differences Greedy
Greedy-LA policies cases. Diabetes dataset Greedy policy performs worse misclassification costs symmetric (Figures 5(a)
88

fiValue Information Lattice

(a)

(b)

(c)

(d)

Figure 7: Expected Total Cost (ETC ) comparisons Hepatitis dataset. priori class
distribution follows: P (Y ) = [0.7908, 0.2092].

5(c)), whereas Hepatitis dataset, performs worse asymmetric
misclassification costs (Figures 7(b) 7(d)).
Greedy policy sometimes erratic, unpredictable, unreliable performance expected misclassification changes. possibly hits local minima, gets
later, hits local minima (Figures 6 8(d)).
finally present aggregate summary results Table 3. Table 3 shows
much Greedy policy Greedy-LA policy saves Markov Blanket policy.
results presented average saving various intervals, [0-500).
table shows, Greedy-LA policy never loses compared Markov Blanket
policy, one would expect. Additionally, Greedy-LA policy wins Greedy
policy cases, never looses. Finally, Greedy policy prematurely stops
acquisition, negative savings respect Markov Blanket strategy.
89

fiBilgic & Getoor

(a)

(b)

(c)

(d)

Figure 8: Expected Total Cost (ETC ) comparisons Thyroid Disease dataset.
priori class distribution follows: P (Y ) = [0.0244, 0.0507, 0.9249].

5. Related Work
Decision theoretic value information calculations provide principled methodology
information gathering general (Howard, 1966; Lindley, 1956). Influence diagrams,
example, popular tools representing decisions utility functions (Howard &
Matheson, 1984). However, devising optimal acquisition policy (i.e., constructing optimal decision tree) intractable general, approaches feature
acquisition myopic (Dittmer & Jensen, 1997), greedily acquiring one feature
time. greedy approaches typically differ i) problem setup assume, ii)
way features scored, iii) classification model learned. review
existing work here, highlighting differences different techniques three
dimensions.
Gaag Wessels (1993) consider problem evidence gathering diagnosis
using Bayesian Network. setup, gather evidence (i.e., observe values
variables) hypothesis confirmed disconfirmed desired extent.
90

fiValue Information Lattice

Table 3: Savings Greedy (GR) Greedy-LA (LA) respect Markov Blanket
policy, averaged different intervals. entry bold worse
Greedy-LA, red worse Markov Blanket.

Liver
GR

LA

Diabetes
GR
LA

Heart
GR

LA

Hepatitis
GR
LA

Thyroid
GR
LA

Real Feature Costs & Symmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000]

6.77
-18.84
-42.12
-67.59

9.08
2.70
2.66
2.85

15.49
-18.28
-48.35
-81.43

24.27
17.06
17.35
17.34

240.59
121.31
79.07
-24.98

243.31
144.87
116.68
111.34

4.19
-6.06
-14.32
-23.40

5.86
3.90
3.90
3.85

28.07
13.90
13.41
13.41

28.07
13.90
13.41
13.41

5.84
2.57
2.57
2.57

17.7
1.56
1.56
1.56

17.7
1.56
1.56
1.56

231.93
106.54
96.39
88.14
79.88
71.63
68.67
63.66

298.01
277.70
257.40
237.09
216.79
196.48
176.18
153.84

298.01
277.70
257.40
237.09
216.79
196.48
176.18
153.84

276.32
213.60
162.52
113.82
65.12
28.72
0.78
-18.10

276.32
213.60
162.52
113.82
68.39
34.73
14.67
9.50

Real Feature Costs & Asymmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000]

7.33
-16.78
-38.26
-61.88

9.3
2.66
3.04
2.97

22.74
9.85
3.99
-2.54

23.84
13.31
11.7
13.7

245.79
131.36
46.20
-40.96

245.79
143.3
114.23
107.14

-9.55
-47.61
-84.79
-125.69

Synthetic Feature Costs & Symmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000)
[2000-2500)
[2500-3000)
[3000-3500)
[3500-4000]

307.39
160.95
60.30
31.86
10.43
-14.60
-39.64
-67.18

307.39
160.95
79.76
53.97
53.01
62.66
59.96
63.68

418.34
245.75
163.80
138.78
108.69
78.90
48.83
15.75

418.34
288.65
224.09
163.45
163.78
164.75
172.76
172.13

723.36
579.25
444.42
378.43
364.03
268.00
171.91
109.91

723.36
585.72
539.88
490.23
482.24
458.89
422.06
412.11

231.93
63.59
96.39
88.14
79.88
71.63
63.38
54.30

Synthetic Feature Costs & Asymmetric Misclassification Costs
[0-500)
[500-1000)
[1000-1500)
[1500-2000)
[2000-2500)
[2500-3000)
[3000-3500)
[3500-4000]

306.19
156.78
66.57
37.47
14.84
-9.19
-33.22
-59.66

306.19
156.78
79.79
60.62
55.70
58.85
59.33
63.13

441.29
341.28
260.09
201.19
161.24
144.24
132.84
126.43

441.29
341.28
261.21
204.31
164.17
151.22
139.54
136.51

728.57
599.02
505.80
420.56
320.32
211.26
248.73
206.06

728.57
603.12
517.37
519.29
512.75
500.75
400.16
389.32

219.04
88.34
-16.86
-54.31
-64.75
-101.93
-139.11
-180.01

219.04
91.53
49.39
61.90
61.90
61.90
61.90
61.90

propose acquisition algorithm greedily computes expected utility acquiring
feature chooses one highest utility. define utility absolute
value change probability distribution hypothesis tested.
recent work, Sent Gaag (2007) consider problem acquiring
single feature step. define subgoals cluster features subgoal.
subgoals clustering features provided domain experts. Then,
non-myopic case, pick cluster calculating expected values. However,
91

fiBilgic & Getoor

clusters big, calculating expected value cluster problematic;
thus, provide semi-myopic algorithm pick cluster
best (myopic) feature.
Nunez (1991) introduces decision tree algorithm called EG2 sensitive
feature costs. Rather splitting decision tree feature high information
gain, EG2 chooses feature least information cost function, defined
ratio features cost discriminative efficiency. EG2 is, however, directly
optimized balance misclassification cost feature acquisition cost; rather
optimized 0/1 loss taking feature costs account. Similarly, Tan (1990)
modifies ID3 algorithm (Quinlan, 1986) account feature costs. Tan considers
domain robot needs sense, recognize, act, number features
large. robot act efficiently, needs trade-off accuracy efficiency.
Turney (1995) builds decision tree called ICET (standing Inexpensive Classification
Expensive Tests) using genetic search algorithm (Grefenstette, 1986) using
Nunezs (1991) criteria build C4.5 decision trees (Quinlan, 1993). Unlike Nunez, Turney
takes misclassification costs account (in addition feature costs) evaluate
given decision tree looks good decision tree using genetic search algorithms.
Yang et al. (2006) build cost-sensitive decision trees Naive Bayes classifiers
take feature costs misclassification costs account. Unlike Nunez (1991),
scores features based information gain cost ratio, Yang et al. score features based
expected reduction total cost (i.e., sum feature cost misclassification
cost) training data. so, take feature costs misclassification costs
account directly learning time.
Bayer-Zubek (2004) formulates feature acquisition problem Markov Decision
Process provides greedy systematic search algorithms develop diagnostic
policies. Bayer-Zubek takes feature cost misclassification costs account
automatically finds acquisition plan balances two costs. introduces
admissible heuristic AO* search describes regularization techniques reduce overfitting training data.
Saar-Tsechansky, Melville, Provost (2009) consider active feature acquisition
classifier induction. Specifically, given training data missing feature values, cost matrix defines cost acquiring feature value, describe
incremental algorithm select best feature acquire iteratively build
model expected high future performance. utility acquiring feature
estimated terms expected performance improvement per unit cost. two characteristics make work different previous work i) authors
assume fixed budget priori; rather build model incrementally, ii)
feature different cost instance.
Finally, Greiner, Grove, Roth (2002) analyze sample complexity dynamic
programming algorithms performs value iteration search best diagnostic
policies. analyze problem learning optimal policy, using variant
probably-approximately-correct (PAC) model. show learning achieved
efficiently active classifier allowed perform (at most) constant number
tests show learning optimal policy often intractable general
environments.
92

fiValue Information Lattice

6. Future Work
article, scratched surface incorporating constraints
features order reduce search space make reasoning sets tractable.
discovered two types constraints (features render features useless,
features useless without features) purely underlying probability
distribution. shown automatically discovered constraints helped reduce
search space dramatically. practice, possible discover additional types
constraints potentially used reduce search space (for e.g., ordering
constraints certain procedures always precede procedures). Constraints
defined based observed feature values; example, treadmill test might
performed patients old age. Patients decline certain procedures medications.
Eliciting constraints domain experts utilizing reduce
search space promising future direction.
existing feature acquisition frameworks, including one, major
simplification happens practice; assumed acquiring values
features change class value values variables. However, practice,
feature value measurements side-effects, example, medical diagnosis
certain measurements non-invasive change status patient, others
might include medications affect outcome. Similarly, fault diagnosis
repair, purpose diagnose repair fault, actions
fact repair fault, essence changing class value. Taking extra side-effects
account make feature acquisition frameworks realistic.

7. Conclusion
typical approach feature acquisition greedy past primarily due
sheer size possible subsets features. described general technique
optimally prune search space exploiting conditional independence relationships
features class variable. empirically showed exploiting conditional independence relationships substantially reduce number possible subsets.
introduced novel data structure called Value Information Lattice (VOILA)
efficiently reduce search space using conditional independence relationships share probabilistic inference computations different subsets
features. using VOILA, able add full look-ahead capability greedy
acquisition policy, would practical otherwise. experimentally showed
five real-world medical datasets greedy strategy often stopped feature acquisition
prematurely, performing worse even policy acquires features.

Acknowledgments
thank reviewers helpful constructive feedback. material
based work supported National Science Foundation Grant No. 0746930.
93

fiBilgic & Getoor

References
Bayer-Zubek, V. (2004). Learning diagnostic policies examples systematic search.
Annual Conference Uncertainty Artificial Intelligence.
Bilgic, M., & Getoor, L. (2007). VOILA: Efficient feature-value acquisition classification.
AAAI Conference Artificial Intelligence, pp. 12251230.
Dittmer, S., & Jensen, F. (1997). Myopic value information influence diagrams.
Annual Conference Uncertainty Artificial Intelligence, pp. 142149.
Frank, A., & Asuncion, A. (2010). UCI machine learning repository..
Gaag, L., & Wessels, M. (1993). Selective evidence gathering diagnostic belief networks.
AISB Quarterly, pp. 2334.
Grefenstette, J. (1986). Optimization control parameters genetic algorithms. IEEE
Transactions Systems, Man Cybernetics, 16 (1), 122128.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.
Artificial Intelligence, 139 (2), 137174.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). approximate nonmyopic computation value information. IEEE Transactions Pattern Analysis Machine
Intelligence, 15 (3), 292298.
Howard, R. A., & Matheson, J. E. (1984). Readings Principles Applications
Decision Analysis, chap. Influence Diagrams. Strategic Decision Group.
Howard, R. A. (1966). Information value theory. IEEE Transactions Systems Science
Cybernetics, 2 (1), 2226.
Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees NPComplete. Information Processing Letters, 5 (1), 1517.
Lindley, D. V. (1956). measure information provided experiment. Annals
Mathematical Statistics, 27, 9861005.
Nunez, M. (1991). use background knowledge decision tree induction. Machine
Learning, 6 (3), 231250.
Health, O. M. (1992). Schedule benefits: Physician services health insurance
act..
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann, San
Francisco.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1 (1), 81106.
Quinlan, J. R. (1993). C4.5: programs machine learning. Morgan Kaufmann Publishers
Inc., San Francisco, CA, USA.
Saar-Tsechansky, M., Melville, P., & Provost, F. (2009). Active feature-value acquisition.
Management Science, 55 (4), 664684.
Sent, D., & Gaag, L. C. (2007). Enhancing automated test selection probabilistic networks. Proceedings 11th conference Artificial Intelligence Medicine,
pp. 331335.
94

fiValue Information Lattice

Tan, M. (1990). CSL: cost-sensitive learning system sensing grasping objects.
IEEE International Conference Robotics Automation.
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation hybrid genetic
decision tree induction algorithm. Journal Artificial Intelligence Research, 2, 369
409.
Yang, Q., Ling, C., Chai, X., & Pan, R. (2006). Test-cost sensitive classification data
missing values. IEEE Transactions Knowledge Data Engineering, 18 (5),
626638.

95


