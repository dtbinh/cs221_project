Journal Artificial Intelligence Research 41 (2011) 527-551

Submitted 03/11; published 08/11

Controlling Complexity Part-of-Speech Induction
Joo V. Graa

JOAO . GRACA @ L 2 F. INESC - ID . PT

L2 F INESC-ID
Lisboa, Portugal

Kuzman Ganchev

KUZMAN @ GOOGLE . COM

Google Inc.
New York, NY, USA

Lusa Coheur

LUISA . COHEUR @ L 2 F. INESC - ID . PT

2

L F INESC-ID
Lisboa, Portugal

Fernando Pereira

PEREIRA @ GOOGLE . COM

Google Inc.
Mountain View, CA, USA

Ben Taskar

TASKAR @ CIS . UPENN . EDU

Computer & Information Science
University Pennsylvania

Abstract
consider problem fully unsupervised learning grammatical (part-of-speech) categories unlabeled text. standard maximum-likelihood hidden Markov model
task performs poorly, weak inductive bias large model capacity. address
problem refining model modifying learning objective control capacity via parametric non-parametric constraints. approach enforces word-category association sparsity,
adds morphological orthographic features, eliminates hard-to-estimate parameters rare
words. develop efficient learning algorithm much computationally intensive standard training. provide open-source implementation algorithm.
experiments five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve
significant improvements compared previous methods task.

1. Introduction
Part-of-speech (POS) categories elementary building blocks syntactic analysis text
play important role many natural-language-processing tasks, machine translation
information extraction. English handful languages fortunate enough
comprehensive POS-annotated corpora Penn Treebank (Marcus, Marcinkiewicz,
& Santorini, 1993), worlds languages extremely limited linguistic resources.
unrealistic expect annotation efforts catch explosion unlabeled electronic
text anytime soon. lack supervised data likely persist near future
investment required accurate linguistic annotation: took two years annotate 4,000 sentences
syntactic parse trees Chinese Treebank (Hwa, Resnik, Weinberg, Cabezas, & Kolak,
2005) four seven years annotate 50,000 sentences across range languages (Abeill,
2003).

c
2011
AI Access Foundation. rights reserved.

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

Supervised learning taggers POS-annotated training text well-studied task,
several methods achieving near-human tagging accuracy (Ratnaparkhi, 1996; Toutanova, Klein,
Manning, & Singer, 2003; Shen, Satta, & Joshi, 2007). However, POS induction one
access labeled corpus difficult task much room improvement.
recent literature, POS induction used refer two different tasks. first one,
addition raw text, given dictionary containing possible tags word
goal disambiguate tags particular word occurrence (Merialdo, 1994). second
task, given raw text, dictionary provided; goal cluster words
grammatical behavior. work, target latter, challenging, unsupervised POS
induction task.
Recent work task typically relies distributional morphological features, since words
grammatical function tend occur similar contexts common morphology (Brown, deSouza, Mercer, Pietra, & Lai, 1992; Schtze, 1995; Clark, 2003). However,
statistical regularities enough overcome several challenges. First, algorithm
decide many clusters use broad syntactic categories (for instance, whether distinguish
plural singular nouns). Second, category size distribution tends uneven. example, vast majority word types open class (nouns, verbs, adjectives), even among
open class categories, many nouns adjectives. runs contrary learning
biases commonly-used statistical models. common failure models clump several
rare categories together split common categories.
individual word types, third challenge arises ambiguity grammatical role
word sense. Many words take different POS tags different occurrences, depending
context occurrence (the word run either verb noun). approaches assume
(for computational statistical simplicity) word one tag, aggregating
local contexts distributional clustering (Schtze, 1995). one-tag-per-word
assumption clearly wrong, across many languages annotated corpora,
methods perform competitively methods assign different tags word
different contexts (Lamar, Maron, Johnson, & Bienenstock, 2010). partly due typical
statistical dominance one tags word, especially corpus includes single genre,
news. reason less restrictive models encode useful bias
words typically take small number tags.
approaches make one-tag-per-word assumption take form hidden
Markov model (HMM) hidden states represent word classes observations
word sequences (Brown et al., 1992; Johnson, 2007). Unfortunately, standard HMMs trained
maximize likelihood perform poorly, since learned hidden classes align well true
POS tags. Besides potential model estimation errors due non-convex optimization involved
training, pernicious problem. Typical maxima likelihood align well
maxima POS tag accuracy (Smith & Eisner, 2005; Graa, Ganchev, Pereira, & Taskar, 2009),
suggesting serious mismatch model data.
work, significantly reduce modeling mismatch combining three ideas:
standard HMM treats words atomic units, without using orthographic morphological information. information critical generalization many languages (Clark,
2003). address problem, reparameterize standard HMM replacing multinomial emission distributions maximum-entropy models (similar work Berg528

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

Kirkpatrick, Bouchard-Ct, DeNero, & Klein, 2010 Graa, 2010). allows use
orthographic morphological features emission model. Moreover, standard
HMM model large number parameters: number tags times number
word types. presents extremely rich model space capable fitting irrelevant correlations data. address problem dramatically reduce number parameters
model discarding features small support corpus, is, involving
rare words word parts.
HMM model allows high level ambiguity tags word. result,
maximizing marginal likelihood, common words typically tend associated every
tag non-trivial probability (Johnson, 2007). However, natural property POS
categories across many languages annotation standards word small
number allowed tags. address problem use posterior regularization (PR)
framework (Graa, Ganchev, & Taskar, 2007; Ganchev, Graa, Gillenwater, & Taskar, 2010)
constrain ambiguity word-tag associations via sparsity-inducing penalty
model posteriors (Graa et al., 2009).
show proposed extensions improves standard HMM performance,
moreover, gains nearly additive. improvements significant across different
metrics previously proposed task. instance, 1-Many metric, method attains
10.4% average improvement regular HMM. compare proposed method
eleven previously proposed approaches. languages English metrics except 1-1,
method achieves best published results. Furthermore, method appears stable
across different testing scenarios always shows competitive results. Finally, show
induced tags used improve performance supervised POS tagging system
limited labeled data scenario. open-source software POS induction evaluation
available http://code.google.com/p/pr-toolkit/.
paper organized follows. Section 2 describes basic HMM POS induction
maximum-entropy extension. Section 3 describes standard EM sparsity-inducing estimation
method. Section 4 presents comprehensive survey previous fully unsupervised POS induction
methods. Section 5 provide detailed experimental evaluation method. Finally,
Section 6, summarize results suggest ideas future work.

2. Models
model experiments based first order HMM. denote sequence
words sentence boldface x sequence hidden states correspond partof-speech tags boldface y. sentence length l, thus l hidden state variables
yi {1, . . . , J}, 1 l J number possible POS tags, l observation variables
xi {1, . . . , V }, 1 l, V number word types. simplify notation, assume
every tag sequence prefixed conventional start tag y0 = start, allowing us write
p(y1 |y0 ) initial state probability HMM.
probability sentence x along particular hidden state sequence given by:
p(x, y) =

l


pt (yi | yi1 )po (xi | yi ),

i=1

529

(1)

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

po (xi | yi ) probability observing word xi given state yi (emission
probability), pt (yi | yi1 ) probability state yi , given previous hidden
state yi1 (transition probability).
2.1 Multinomial Emission Model
Standard HMMs use multinomial emission transition probabilities. is, generic word
xi tag yi , observation probability po (xi | yi ) transition probability pt (yi | yi1 )
multinomial distributions. experiments refer model simply HMM. model
large number parameters large number word types (see Table 1).
common convention follow lowercase words well map words occurring
corpus special token unk.
2.2 Maximum Entropy Emission Model
work, use simple modification HMM model discussed previous section:
represent conditional probability distributions maximum entropy (log-linear) models. Specifically, emission probability expressed as:
exp( f (x, y))
0
x0 exp( f (x , y))

po (x|y) = P

(2)

f (x, y) feature function, x ranges word types, model parameters.
refer model HMM+ME. addition word identity, features include orthographyand morphology-inspired cues presence capitalization, digits, common suffixes.
feature sets described Section 5. idea replacing multinomial models HMM
maximum entropy models new applied different domains (Chen,
2003), well POS induction (Berg-Kirkpatrick et al., 2010; Graa, 2010). key advantage
representation allows much tighter control expressiveness
model. many languages helpful exclude word identity features rare words order
constrain model force generalization across words similar features. Unlike mapping
rare words unk token multinomial setting, maxent model still captures
information word features. Moreover, reduce number
parameters even using lowercase word identities still keeping case information
using case feature. Table 1 shows number features used different corpora. Note
reduced feature set order magnitude fewer parameters multinomial model.

3. Learning
Section 5 describe experiments comparing HMM model model three
learning scenarios: maximum likelihood training using EM algorithm (Dempster, Laird, & Rubin, 1977) HMM HMM+ME, gradient-based likelihood optimization HMM+ME
model, PR sparsity constraints (Graa et al., 2009) HMM HMM+ME.
section describes three learning algorithms.
following, denote whole corpus, list sentences, X = (x1 , x2 , . . . , xN )
corresponding tag sequences = (y1 , y2 , . . . , yN ).

530

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

3.1 Maximum Likelihood EM
Standard HMM training seeks model parameters maximize log-likelihood observed
data:
X
p (X, Y)
(3)
Log-Likelihood: L() = log


X whole corpus. Since model assumes independence sentences ,
log

X

p (X, Y) =

N
X

log

X

(4)

yn

n=1



p (xn , yn ),

use corpus notation consistency Section 3.3. latent variables
Y, log-likelihood function HMM model convex model parameters,
model fitted using EM algorithm. EM maximizes L() via block-coordinate ascent lower
bound F (q, ) using auxiliary distribution latent variables q(Y) (Neal & Hinton, 1998).
Jensens inequality, define lower-bound F (q, ) as:
L() = log

X

q(Y)



p (X, Y) X
p (X, Y)

q(Y) log
= F (q, ).
q(Y)
q(Y)

(5)



rewrite F (q, ) as:
F (q, ) =

X

q(Y) log(p (X)p (Y|X))



X

q(Y) log q(Y)

(6)



q(Y)
q(Y) log
p (Y|X)

(7)

= L() KL(q(Y)||p (Y|X)).

(8)

= L()

X


Using interpretation, view EM performing coordinate ascent F (q, ). Starting
initial parameter estimate 0 , algorithm iterates two block-coordinate ascent steps
convergence criterion reached:
E : q t+1 = arg max F (q, ) = arg min KL(q(Y) k pt (Y | X))
q

(9)

q

: t+1 = arg max F (q t+1 , ) = arg max Eqt+1 [log p (X, Y)]


(10)



E-step corresponds maximizing Eq. 8 respect q M-step corresponds
maximizing Eq. 6 respect . EM algorithm guaranteed converge local
maximum L() mild conditions (Neal & Hinton, 1998). HMM POS tagger,
E-Step computes posteriors pt (y|x) latent variables (POS tags) given observed
variables (words) current parameters sentence. accomplished forwardbackward algorithm HMMs. EM algorithm together forward-backward algorithm
HMMs usually referred BaumWelch algorithm (Baum, Petrie, Soules, & Weiss,
1970).
step uses q t+1 (qnt+1 posteriors given sentence) fill values tags
estimate parameters t+1 . Since HMM model locally normalized features used
531

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

depend tag word identities particular position occur,
optimization decouples following way:
Eqt+1 [log p (X, Y)] =
=

N
X

Eqnt+1 [log

n=1
ln
N X
X

ln


n
pt (yin | yi1
)po (xni | yin )]

(11)

i=1
n
Eqnt+1 log pt (yin | yi1
) + Eqnt+1 log po (xni | yin )



(12)

n=1 i=1

multinomial emission model, optimization particularly easy simply involves
normalizing (expected) counts parameter. maximum-entropy emission model parameterized Equation 2, closed form solution need solve unconstrained
optimization problem. possible hidden tag value solve two problems: estimate emission probabilities po (x|y) estimate transition probabilities pt (y 0 |y),
gradient one given


Eqt+1 [log p (X, Y)]
= Eqt+1 f (X, Y) Ep (X0 |Y) [f (X0 , Y)] ,
(13)

similar gradient supervised models, except expectation
q t+1 (Y) instead observed Y. optimization done using L-BFGS Wolfes rule
line search (Nocedal & Wright, 1999).
3.2 Maximum Likelihood Direct Gradient
likelihood traditionally optimized EM, Berg-Kirkpatrick et al. (2010) find
HMM maximum entropy emission model, higher likelihood better accuracy
achieved gradient-based likelihood-optimization method. use L-BFGS
experiments. derivative likelihood is,
L()



1

1
X
log p (X) =
p (X) =
p (X, Y)

p (X)
p (X)

X 1
X p (X, Y)

=
p (X, Y) =
log p (X, Y)
p (X)
p (X)


X

=
p (Y|X) log p (X, Y),

=

(14)
(15)
(16)



exactly derivative M-Step. Equation 14 apply chain
rule take derivative log p (X), Equation 15 apply chain rule reverse
direction. biggest difference EM procedure direct gradient EM
fix counts E-Step optimize model using counts. directly
optimizing likelihood need recompute counts parameter setting,
expensive. Appendix gives detailed discussion methods.
3.3 Controlling Tag Ambiguity PR
One problem unsupervised HMM POS tagging maximum likelihood objective may
encourage tag distributions allow many different tags word given context.
532

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

10

6

Supervised
HMM
HMM+ME
HMM+Sp
HMM+ME+Sp

8
L1L

8
L1L

10

Supervised
HMM
HMM+ME
HMM+Sp
HMM+ME+Sp

4
2

6
4
2

0

0
0

1000 2000 3000 4000 5000 6000 7000 8000

0

200 400 600 800 1000 1200 1400 1600 1800

rank word L1L

rank word L1L

Figure 1: ambiguity measure (`1 /` ) word type two corpora supervised
model, EM training (HMM, HMM+ME), train ambiguity penalty
described Section 3.3 (HMM+Sp, HMM+ME+Sp). Left:En, Right:Pt.

find actual text linguist-designed tags, tags designed informative
words grammatical role. following paragraphs describe measure tag ambiguity
proposed Graa et al. (2009) attempt control. easier understand measure
hard tag assignments, start thene extend discussion distributions
tags.
Consider word stock. Intuitively, would occurrences stock
tagged small subset possible tags (noun verb, case). hard assignment
tags entire corpus, Y, could count many different tags used occurrences
word stock.
instead single tagging corpus, distribution q(Y) assignments,
need generalize ambiguity measure. Instead asking particular tag ever used
word stock, would ask maximum probability particular tag
used word stock. instead counting number tags, would sum
probabilities.
motivation, Figure 1 shows distribution tag ambiguity across words two corpora.
see Figure 1, train using EM procedure described Section 3.1,
HMM models grossly overestimates tag ambiguity almost words. However
models trained using PR penalize tag ambiguity, models (HMM+Sp,
HMM+ME+Sp) achieve tag ambiguity closer truth.
formally, Graa et al. (2009) define measure terms constraint features (X, Y).
Constraint feature wvj (X, Y) takes value 1 j th occurrence word type w X assigned
tag v tag assignment Y. Consequently, probability j th occurrence word w
tag v label distribution q(Y) Eq [wvj (X, Y)]. ambiguity measurement
word type w becomes:
X
Ambiguity Penalty word type w:
max Eq(Y) [wvj (X, Y)] .
(17)
v

j

sum maxima called `1 /` mixed norm. brevity use norm notation ||Eq [w ]||1/ . computational reasons, add penalty term based ambiguity model distribution p (Y|X), instead introduce auxiliary distribution q(Y)
533

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

must close p must low ambiguity. modified objective becomes
X
||Eq [w (X, Y)]||1/ .
max L() KL(q(Y)||p (Y|X))
,q

(18)

w

Graa et al. (2009) optimize objective using algorithm similar EM. added complexity implementing algorithm lies computing Kullback-Leibler projection
modified E-Step. However, computation involves choosing distribution exponentially
many objects (label assignments). Luckily, Graa et al. (2009) show dual formulation
E-Step manageable. given by:
!
X
X
max log
p (Y|X) exp( (X, Y))
s. t.
wvj
(19)
0

j



vector dual parameters wvj , one wvj . projected distribution
given by: q(Y) p (Y|X) exp ( (X, Y)). Note p given HMM, q
sentence expressed
q(yn )




n
pt (yin | yi1
)qo (xni | yin ),

(20)

i=1

qo (xi |yi ) = po (xi |yi ) exp(xi yi j ) act modified (unnormalized) emission probabilities.
objective Equation 19 negative sum log probabilities sentences
q plus constant. compute running forward-backward corpus, similar
E-Step normal EM. gradient objective computed using forwardbackward algorithm. Note objective Eq. 19 concave respect
optimized using variety methods. perform dual optimization projected gradient,
using fast simplex projection algorithm described Bertsekas, Homer, Logan, Patek
(1995). experiments found taking projected gradient steps enough,
performing optimization convergence helps results.

4. Related Work
POS tags place words classes share commonalities (classes of) words
cooccur with. Therefore, natural ask whether word clustering methods based word
context distributions might able recover word classification inherent POS tag set.
Several influential methods, notably mutual-information clustering (Brown et al., 1992),
used cluster words according immediately contiguous words distributed.
Although methods explicitly designed POS induction, resulting clusters capture syntactic information (see Martin, Liermann, & Ney, 1998, different method
similar objective). Clark (2003) refined distributional clustering approach adding
morphological word frequency information, obtain clusters closely resemble POS
tags.
forms distributional clustering go beyond immediate neighbors word represent whole vector coocurrences target word within text window, compare
vectors using suitable metric, cosine similarity. However, wider-range similarities problems capturing local regularities. instance, adjective noun might
534

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

look similar noun tends used noun-noun compounds; similarly, two adjectives
different semantics selectional preferences might used different contexts. Moreover,
problem aggravated data sparsity. example, infrequent adjectives modify different nouns tend completely disjoint context vectors (but even frequent words
might completely different context vectors, since articles used disjoint right
contexts). alleviate problems, Schtze (1995) used frequency cutoffs, singular-value decomposition co-occurrence matrices, approximate co-clustering two stages SVD,
clusters first stage used instead individual words provide vector representations second-stage clustering.
Lamar, Maron Johnson (2010) recently revised two-stage SVD model Schtze
(1995) achieve close state-of-the-art performance. revisions relatively small,
touch several important aspects model: singular vectors scaled singular values
preserve geometry original space; latent descriptors normalized unit length;
cluster centroids computed weighted average constituent vectors based word
frequency, rare common words treated differently centroids initialized
deterministic manner.
final class approaches include work paper uses sequence model,
HMM, represent probabilistic dependencies consecutive tags.
approaches, observation corresponds particular word hidden state corresponds
cluster. However, noted Clark (2003) Johnson (2007), using maximum likelihood
training models achieve good results: maximum likelihood training tends result
ambiguous distributions common words, contradiction rather sparse wordtag distribution. Several approaches proposed mitigate problem. Freitag (2004)
clusters frequent words using distributional approach co-clustering. cluster
remaining (infrequent) words, author trains second-order HMM emission probabilities frequent words fixed clusters found earlier emission probabilities
remaining words uniform.
Several studies propose using Bayesian inference improper Dirichlet prior favor
sparse model parameters hence indirectly reduce tag ambiguity (Johnson, 2007; Gao & Johnson, 2008; Goldwater & Griffiths, 2007). refined Moon, Erk, Baldridge
(2010) representing explicitly different ambiguity patterns function content words.
Lee, Haghighi, Barzilay (2010) take direct approach reducing tag ambiguity explicitly modeling set possible tags word type. model first generates tag
dictionary assigns mass one tag word type reflect lexicon sparsity. dictionary used constrain Dirichlet prior emission probabilities drawn
support word-tag pairs dictionary. token-level HMM using
emission parameters transition parameters draw symmetric Dirichlet prior used
tagging entire corpus. authors show improvements using morphological features
creating dictionary. system achieves state-of-art results several languages.
noted common issue sparsity-inducing approaches sparsity
imposed parameter level, probability word given tag, desired sparsity
posterior level, probability tag given word. Graa et al. (2009) use PR framework
penalize ambiguous posteriors distributions words given tokens, achieves better results
Bayesian sparsifying Dirichlet priors.

535

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

recently, Berg-Kirkpatrick et al. (2010) Graa (2010) proposed replacing multinomial distributions HMM maximum entropy (ME) distributions. allows use features capture morphological information, achieve promising results. Berg-Kirkpatrick
et al. (2010) find optimizing likelihood L-BFGS rather EM leads substantial improvements, show case beyond English.
note briefly POS induction methods rely prior tag dictionary indicating
word type POS tags have. POS induction task then, word token
corpus, disambiguate possible POS tags, described Merialdo (1994). Unfortunately, availability large manually-constructed tag dictionary unrealistic much
later work tries reduce required dictionary size different ways, generalizing
small dictionary handful entries (Smith & Eisner, 2005; Haghighi & Klein, 2006;
Toutanova & Johnson, 2007; Goldwater & Griffiths, 2007). However, although approach greatly
simplifies problem words one tag and, furthermore, cluster-tag mappings predetermined, thus removing extra level ambiguity accuracy methods
still significantly behind supervised methods. address remaining ambiguity imposing
additional sparsity, Ravi Knight (2009) minimize number possible tag-tag transitions
HMM via integer program. Finally, Snyder, Naseem, Eisenstein, Barzilay (2008) jointly
train POS induction system parallel corpora several languages, exploiting fact
different languages present different ambiguities.

5. Experiments
section present encouraging results validating proposed method six different testing
scenarios according different metrics. highlights are:
maximum-entropy emission model Markov transition model trained ambiguity penalty improves regular HMM cases average improvement
10.4% (according 1-Many metric).
compared broad range recent POS induction systems, method produces
best results languages except English. Furthermore, method seems less sensitive
particular test conditions previous methods.
induced clusters useful features training supervised POS taggers, improving test
accuracy much clusters learned competing methods.
5.1 Corpora
experiments test several POS induction methods five languages help manually POS-tagged corpora languages. Table 1 summarizes characteristics test corpora:
Wall Street Journal portion Penn Treebank (Marcus et al., 1993) (we consider
17-tag version Smith & Eisner, 2005 (En17) 45-tag version (En45)); Bosque subset
Portuguese Floresta Sinta(c)tica Treebank (Afonso, Bick, Haber, & Santos, 2002) (Pt);
Bulgarian BulTreeBank (Simov et al., 2002) (Bg) (with 12 coarse tags); Spanish corpus Cast3LB treebank (Civit & Mart, 2004) (Es); Danish Dependency Treebank
(DDT) (Kromann, Matthias T., 2003) (Dk).

536

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

En
Pt
Bg
Es
Dk

1
Sentences
49208
9359
14187
3512
5620

2
Types
49206
29489
34928
16858
19400

3
LUnk
49.28%
37.83%
39.26%
37.73%
26.30%

4
Tokens
1173766
212545
214930
95028
65057

5
Tags
17 (45)
22
12
47
30

6
Avg. `1 /`
1.08 (1.11)
1.02
1.02
1.05
1.05

7
Total `1 /`
6523.6 (6746.2)
1144.6
3252.9
818.9
795.8

8
|w|1
54334
33293
38633
18962
21678

9
|w|2
7856
2114
2287
951
969

Table 1: Corpus statistics. third column shows percentage word types lower-casing
eliminating word types occurring once. sixth seventh columns show
information word ambiguity corpus average totality (corresponding penalty Equation 17). eighth ninth columns show number
parameters different feature sets, described Section 5.3.

5.2 Experimental Setup
compare work two kinds methods: induce single cluster word
type (type-level tagging), allow different tags different occurrences word type
(token-level tagging). type-level tagging, use two standard baselines, B ROWN C LARK,
described Brown et al. (1992)1 Clark (2003)2 . Following Headden, McClosky, Charniak (2008), trained C LARK system 5 10 hidden states letter HMM
ran 10 iterations; B ROWN system run according instructions accompanying
code. ran recently proposed LDC system (Lamar, Maron, & Bienenstock, 2010)3 ,
configuration described paper PTB45 PTB17, PTB17 configuration
corpora. noted carry experiments SVD2
system (Lamar, Maron Johnson, 2010), since SVD2 superseded LDC according
authors.
token-level tagging, experimented feature-rich HMM presented BergKirkpatrick et al. (2010), trained using EM training (BK+EM) direct gradient (BK+DG),
using configuration provided authors4 . report results type-level HMM
(TLHMM) (Lee et al., 2010) applicable, since able run system. Moreover,
compared systems implementation various HMM-based approaches:
HMM multinomial emission probabilities (Section 2.1), HMM maximumentropy emission probabilities (Section 2.2) trained EM (HMM+ME), trained direct gradient (HMM+ME+DG), trained using PR ambiguity penalty, described Section 3.3
(HMM+Sp multinomial emissions, HMM+ME+Sp maximum-entropy emissions).
addition, compared multinomial HMM sparsifying Dirichlet prior parameters (HMM+VB) trained using variational Bayes (Johnson, 2007).
Following standard practice, multinomial HMMs use morphological information, lowercase corpora replace unique words special unknown token,
improves multinomial HMM results decreasing number parameters eliminating
1.
2.
3.
4.

Implementation: http://www.cs.berkeley.edu/~pliang/software/brown-cluster-1.2.zip
Implementation: http://www.cs.rhul.ac.uk/home/alexc/pos2.tar.gz
Implementation provided Lamar, Maron Bienenstock (2010).
Implementation provided Berg-Kirkpatrick et al. (2010).

537

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

rare words (mostly nouns). Since maximum-entropy emission models access morphological features, preprocessing steps improve performance perform
case.
start EM, randomly initialize implementations HMM-based models
posteriors, obtained running E-step HMM model set random
parameters: close uniform random uniform jitter 0.01. means random
seed, initialization identical models.
EM variational Bayes training, train model 200 iterations, since found
typically models tend converge iteration 100. HMM+VB model fix
transition prior5 0.001 test emission prior equal 0.1 0.001, corresponding
best values reported Johnson (2007).
PR training, initialize 30 EM iterations run 170 iterations PR, following Graa et al. (2009). used results worked best English (En17) (Graa et al.,
2009), regularizing words occur least 10 times, = 32, use configuration scnenarios. setting specifically tuned test languages,
might optimal every language. Setting parameters unsupervised manner
difficult task address (Graa, 2010 discusses experiments different
values parameters).
obtain hard assignments using posterior decoding, position pick label
highest posterior probability, since showed small consistent improvements Viterbi
decoding. experiments required random initialization parameters report
average 5 random seeds.
experiments run using number true tags number clusters, results
obtained test set portion corpus. evaluate systems using four common metrics
POS induction: 1-Many mapping, 1-1 mapping (Haghighi & Klein, 2006), variation information (VI) (Meila, 2007), validity measure (V) (Rosenberg & Hirschberg, 2007). metrics
described detail Appendix B.
5.3 HMM+ME+Sp Performance
section compares gains using feature-rich representation ambiguity penalty, described Section 3.3. Experiments show feature-rich representation always improves performance, ambiguity penalty always improves
performance. Then, see improvements two methods combine additively,
suggesting address independent aspects POS induction.
use two different feature sets: large feature set Berg-Kirkpatrick et al. (2010),
reduced feature set described Graa (2010). apply count-based feature selection identity suffix features. Specifically, add identity features words
occurring least 10 times suffix features words occurring least 20 times. add
punctuation feature. follows, refer large feature set feature set 1 reduced
feature set 2. total number features model language given Table 1.
results experiments summarized Table 2.
Table 2 shows results 10 training methods across six corpora four evaluation metrics,
resulting 240 experimental conditions. simplify discussion, focus 1-Many metric
5. transition prior significantly affect results, report results different values.

538

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6
7
8
9
10

HMM
HMM+Sp
HMM+ME1 Prior 1
HMM+ME1 Prior 10
HMM+ME2 Prior 1
HMM+ME2 Prior 10
HMM+ME+Sp1 Prior 1
HMM+ME+Sp1 Prior 10
HMM+ME+Sp2 Prior 1
HMM+ME+Sp2 Prior 10

En45
62.4
67.5
67.1
70.0
70.3
69.9
71.4
68.8
71.6
71.1

En17
65.6
70.3
72.2
69.2
71.8
71.0
71.7
71.1
72.5
72.0

1-Many
PT BG
64.9 58.9
71.2 65.0
72.3 61.1
66.8 58.2
73.9 62.4
74.1 63.9
75.1 63.0
71.6 62.2
73.4 60.9
76.9 67.1

DK
60.0
67.5
65.1
63.9
66.5
67.8
64.5
68.2
65.0
72.0

1-1
ES PTB45 PTB17 PT BG DK ES
60.2 42.5
43.5 42.2 40.6 37.4 30.6
69.0 46.1
52.5 47.7 46.3 40.0 35.3
71.8 45.0
51.1 46.3 46.1 42.6 40.8
66.2 48.4
45.6 40.8 41.7 41.1 35.1
72.9 45.1
49.8 46.8 46.7 45.0 42.6
73.4 47.3
51.2 48.8 48.8 44.7 37.1
72.8 49.4
52.5 46.6 46.7 43.1 41.1
69.3 45.1
52.1 48.0 49.7 42.0 38.5
72.1 52.5
53.9 45.5 49.5 43.0 38.6
75.2 46.7
48.5 49.6 53.4 48.7 40.8

HMM
HMM+Sp
HMM+ME1 Prior 1
HMM+ME1 Prior 10
HMM+ME2 Prior 1
HMM+ME2 Prior 10
HMM+ME+Sp1 Prior 1
HMM+ME+Sp1 Prior 10
HMM+ME+Sp2 Prior 1
HMM+ME+Sp2 Prior 10

En45
4.22
3.64
3.77
3.31
3.59
3.28
3.20
3.46
3.21
3.41

En17
3.75
3.20
3.11
3.38
3.12
3.24
3.09
3.15
3.04
3.25

VI
PT BG
3.90 4.04
3.27 3.49
3.21 3.46
3.66 3.83
3.05 3.46
3.12 3.44
3.00 3.38
3.15 3.43
3.16 3.37
2.86 3.12

DK
4.55
3.85
3.77
4.13
3.71
3.74
3.80
3.73
3.72
3.35

V
ES PTB45 PTB17 PT BG DK ES
4.89 .558
.479 .490 .383 .432 .474
3.85 .616
.549 .573 .467 .518 .581
3.56 .606
.564 .583 .460 .519 .608
4.11 .649
.530 .527 .406 .482 .553
3.46 .626
.559 .600 .460 .528 .617
3.58 .652
.546 .596 .471 .530 .610
3.49 .660
.560 .608 .478 .514 .617
3.76 .637
.557 .591 .470 .532 .589
3.46 .658
.567 .591 .473 .523 .616
3.34 .644
.541 .631 .519 .578 .636

Table 2: Results different HMMs. HMM HMM+Sp HMMs multinomial emission
functions trained using EM PR sparsity constraints, respectively. HMM+ME
HMM+ME+Spare HMMs maximum entropy emission model trained using EM
PR sparsity constraints. feature-rich models, superscript 1 represents large
feature set, superscript 2 represents reduced feature set. Prior 1 10 refers
regularization strength emission model. Table entries results averaged
5 runs. Bold indicates best system overall.

(top left tab Table 2), observe conclusions hold three evaluation
metrics also. Table 2 conclude following:
Adding penalty high word-tag ambiguity improves performance multinomial
HMM. multinomial HMM trained EM (line 1 Table 2) always worse
multinomial HMM trained PR ambiguity penalty, 6.5% average (line 2
Table 2).
feature-rich maximum entropy HMMs (lines 3-6 Table 2) almost always perform better
multinomial HMM. true feature sets regularization strengths
used, average increase 6.4%. exceptions possibly due suboptimal regularization.
Adding penalty high word-tag ambiguity maximum-entropy HMM improves performance. almost cases, comparing lines 3-6 lines 7-10 Table 2, sparsity
constraints improve performance (average improvement 1.6%). combined system al539

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

always outperforms multinomial HMM trained using ambiguity penalty
average improvement 1.6%. every corpus best performance achieved
model ambiguity penalty maximum-entropy emission probabilities.
every language except English 17 tags particular feature configuration, reducing feature set excluding rare features improves performance average 2.3% (lines
5-6 better lines 3-4 Table 2).
Regularizing maximum-entropy model important many features
word-tag ambiguity penalty. Lines 3-4 Table 2
maximum-entropy HMM many features, see tight parameter prior
almost always out-performs looser prior. contrast, looking lines 9-10 Table 2 see ambiguity penalty fewer features looser prior
almost always better tighter parameter prior. observed Graa (2010).
encouraging see improvements using feature-rich model additive
effects penalizing tag-ambiguity. especially surprising since optimize strength tag-ambiguity penalty maximum-entropy emission HMM, rather
used value reported Graa et al. (2009) work multinomial emission HMM. Experiments reported Graa (2010) show tuning parameter improve performance.
Nevertheless, methods regularize objective different ways interaction
accounted for. would interesting use L1 regularization models, instead
L22 regularization together feature count cutoff. way model could learn features discard, instead requiring predefined parameter depends particular corpus
characteristics.
reported Berg-Kirkpatrick et al. (2010), way objective optimized
big impact overall results. However, due non-convex objective function
unclear optimization method works better why. briefly analyze question
Appendix leave open question future work.
5.4 Error Analysis
Figure 2 shows distribution true tags clusters HMM model (left)
HMM+ME+Sp model (right) En17 corpus. bar represents cluster, labeled tag
assigned performing 1-Many mapping. colors represent number words
corresponding true tag. reduce clutter, true tags never used label cluster
grouped Others.
observe models split common tags nouns several hidden states.
splitting accounts many errors models. using 5 states nouns instead
7, HMM+ME+Sp able use states adjectives. Another improvement comes
better grouping prepositions. example grouped punctuation HMM
HMM+ME+Sp correctly mapped prepositions. Although correct
behavior, actually hurts, since tagset special tag occurrences word
incorrectly assigned, resulting loss 2.2% accuracy. contrast, HMM state
mapped tag word comprises one fifth state. common
error made HMM+ME+Sp include word second noun induced tag
Figure 2 (Right). induced tag contains mostly capitalized nouns pronouns, often
540

fiPREP
DET

N
ADJ

RPUNC
POS

V
INPUNC

CONJ


EPUNC
Others

PREP
DET

N
ADJ

RPUNC
POS

V
INPUNC

CONJ


CONJ

ENDPUNC

V

INPUNC

V

V

ADJ

RPUNC

ADJ

N

ADJ

N

N

N

N

DET

PREP



ENDPUNC

CONJ

V

INPUNC

V

POS

N

ADJ

N

N

N

N

N

N

DET

PREP

C ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

EPUNC
Others

Figure 2: Induced tags HMM model (Left), HMM+ME+Sp model (Right)
En17 corpus. column represents hidden state, labeled 1-Many
mapping. Unused true tags grouped cluster named Others.

25000

25000

20000

20000

15000

15000

10000

10000

5000

5000

0

0
art
n
adj

prop
v-fin
prp

punc
num
adv

v-pcp
v-inf
conj-c

pron-pers
sumOthers

art
n
adj

prop
v-fin
prp

punc
num
adv

v-pcp
v-inf
conj-c

pron-pers
sumOthers

Figure 3: Induced tags HMM model (Left), HMM+ME+Sp model (Right)
Pt corpus. column represents hidden state, labeled 1-Many mapping.
Unused true tags grouped cluster named Others.

precede nouns induced tags. suspect capitalization feature cause
error.
better performance feature-based models Portuguese relative English may due
ability features better represent richer morphology Portuguese. Figure 3 shows
induced clusters Portuguese. HMM+ME+Sp model improves HMM tags
except adjectives. models trouble distinguishing nouns adjectives. reduced
accuracy adjectives HMM+ME+Sp explained mapping single cluster containing
adjectives adjectives HMM model nouns HMM+ME+Sp model.
Removing noun-adjective distinction, suggested Zhao Marcus (2009), would increase
performance models 6%. Another qualitative difference observed
HMM+ME+Sp model used single induced cluster proper nouns rather spreading
across different clusters.

541

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

5.5 State-of-the-Art Comparison
compare best POS induction system (based settings line 10 Table 2),
recent systems. Results summarized Table 3. previously done Table 2,
focus discussion 1-Many evaluation metric, results qualitatively
VI V metrics, 1-1 metric shows variance across languages.
1-Many
PT BG
69.6 63.2
66.0 62.3
67.1 57.0
69.2 61.1
64.9 58.9
61.5 51.5
63.2 53.5
71.2 65.0
72.3 64.3
72.5 56.1
74.5
72.0 76.9 67.1

DK
69.6
57.3
58.2
60.9
60.0
51.2
56.6
67.5
62.8
60.6
61.2
72.0

1-1
ES PTB45 PTB17 PT BG DK ES
69.7 53.3
56.6 43.8 47.9 44.3 41.2
67.6 52.3
43.1 47.3 50.3 37.5 37.4
70.1 52.3
44.2 48.1 45.2 37.5 40.0
67.9 48.6
50.0 42.6 50.1 35.2 38.8
60.2 42.5
43.5 42.2 40.6 37.4 30.6
45.5 48.1
50.3 51.4 42.0 42.7 35.9
55.9 44.1
51.4 45.1 38.3 38.1 34.4
69.0 46.1
52.5 47.7 46.3 40.0 35.3
72.0 48.3
54.4 45.5 50.6 41.5 37.2
73.7 54.5
47.9 42.9 38.8 41.5 40.4
68.9 50.9
64.1
52.1 58.3
75.2 46.7
48.5 49.6 53.4 48.7 40.8

VI
PT BG
3.34 3.30
3.38 3.30
3.28 3.60
3.50 3.51
3.90 4.04
3.65 3.90
3.97 4.07
3.27 3.49
3.19 3.30
3.29 3.89
2.86 3.12

DK
3.41
3.97
3.99
4.24
4.55
4.20
4.41
3.85
3.90
4.15
3.35

V
ES PTB45 PTB17 PT BG DK ES
3.40 .648
.559 .564 .473 .554 .613
3.76 .660
.498 .545 .475 .490 .588
3.55 .663
.499 .557 .424 .485 .610
4.00 .626
.585 .546 .450 .474 .569
4.89 .558
.479 .490 .383 .432 .474
4.40 .534
.500 .477 .368 .405 .402
4.69 .535
.501 .471 .368 .437 .474
3.85 .616
.549 .573 .467 .518 .581
4.15 .645
.568 .582 .479 .477 .596
3.56 .678
.534 .574 .392 .477 .611
3.34 .644
.541 .631 .519 .578 .636

1
2
3
4
5
6
7
8
9
10
11
12

En45
B ROWN
68.7
C LARK5
72.4
C LARK10
72.5
LDC
67.5
HMM
62.4
HMM+VB0.1
55.0
HMM+VB0.001
58.6
HMM+Sp
67.5
BK+EM
69.1
BK+DG
75.8
TLHMM
62.2
HMM+ME+Sp2 Prior 10 71.1

En17
68.7
63.5
63.2
74.7
65.6
67.2
67.7
70.3
72.1
67.9

1
2
3
4
5
6
7
8
9
10
12

En45
B ROWN
3.17
C LARK5
3.23
C LARK10
3.20
LDC
3.43
HMM
4.22
HMM+VB0.1
4.10
HMM+VB0.001
4.38
HMM+Sp
3.64
BK+EM
3.31
BK+DG
3.01
HMM+ME+Sp2 Prior 10 3.41

En17
3.07
3.45
3.46
3.00
3.75
3.52
3.55
3.20
3.04
3.21
3.25

Table 3: Comparing HMM+ME+Sp2 several POS induction systems. results
models random initialization run (systems: 2,3,5,6,7,8,9,10,12) represent
average 5 runs. See Section 5.5 details discussion.

Lines 1-3 Table 3 show clustering algorithms based information gain various metrics. B ROWN wins 5/6 times (in scenarios fewer clusters) C LARK system, despite
fact C LARK uses morphology. Comparing lines 1-3 Table 3 line 4, see
LDC system particularly strong En17 achieves state-of-the-art results, behaves
worse B ROWN system every corpus.
HMMs multinomial emissions (lines 5-8 Table 3), maximum likelihood training (HMM) parameter sparsity (HMM+VB) perform worse adding ambiguity penalty
(HMM+Sp). holds evaluation metrics, exception 1-1. confirms previous results Graa et al. (2009). Comparing models lines 5-8 lines 1-3, see

542

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

best HMM (HMM+Sp) performs comparably best clustering (B ROWN), one
model winning 3 languages remaining 3.
feature rich HMMs (BK+EM BK+DG) perform well, achieving results
better HMM+Sp 4 6 tests. Even though optimize objective, achieve
different results different corpora. explore training procedure detail Appendix A, comparing implementation Berg-Kirkpatrick et al. (2010). brevity,
Table 3 contains results implementation Berg-Kirkpatrick et al. (2010).
implementation produces comparable, quite identical results.
Lines 11-12 Table 3 display two methods attempt control tag ambiguity
feature-rich representation capture morphological information. results TLHMM
taken Lee et al. (2010), report results En17 Bg corpora. Also,
able rerun experiments TLHMM, able compute
information-theoretic metrics. Consequently, comparison TLHMM slightly less complete
methods. TLHMM HMM+ME+Sp perform competitively better
systems. surprising since ability model morphological
regularity penalizing high ambiguity. Comparing TLHMM HMM+ME+Sp, see
HMM+ME+Sp performs better 1-Many metric. contrast, TLHMM performs better
1-1. One possible explanation underlying model TLHMM Bayesian HMM
sparsifying Dirichlet priors. noted Graa et al. (2009), models trained way tend
cluster distribution closely resemble true POS distribution (some clusters lots
words words) favors 1-1 metric (a description particularity
1-1 metric discussed Appendix B).
summarize, non-English languages metrics except 1-1, HMM+ME+Sp
system performs better systems. English, BK+DG wins 45-tag corpus,
LDC wins 17-tag corpus. HMM+ME+Sp system fairly robust, performing well
corpora best several them, allow us conclude tuned
particular corpus evaluation metric.
performance HMM+ME+Sp tightly related performance underlying
HMM+ME system. Appendix present discussion performance different optimization methods HMM+ME. compare HMM+ME implementation BK+EM
BK+DG show significant differences performance. However,
clear results one better, performs better given situation.
mentioned Clark (2003), morphological information particularly useful rare words.
Table 4 compares different models accuracy words according frequency. compare clustering models based information gain without morphological information
(B ROWN,C LARK), distributional information-based model (LDC), feature rich HMM
tag ambiguity control (HMM+ME+Sp). expected see systems using morphology
better rare words. Moreover systems improve almost categories except
common words (words occurring 50 times). Comparing HMM+ME+Sp C LARK,
see even condition C LARK overall works better (En45), still performs worse
rare words HMM+ME+Sp.

543

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

1
5
10
50
> 50

B ROWN
50.07
61.76
64.32
67.13
68.94

C LARK
49.48
62.89
66.53
67.62
73.87

1
5
10
50
> 50

B ROWN
31.44
48.06
63.14
66.30
80.61

C LARK
44.53
53.30
64.33
67.52
74.68

1
5
10
50
> 50

B ROWN
41.04
61.75
72.10
64.44
86.43

C LARK
49.93
64.03
69.76
57.67
73.69

En45
LDC
50.65
58.70
61.78
64.28
68.04
PT
LDC
19.19
36.67
54.16
62.56
84.58
ES
LDC
38.75
51.94
61.69
56.74
81.94

HMM+ME+Sp2
70.12
72.12
70.59
70.80
71.49

B ROWN
29.42
43.38
50.94
59.16
72.14

C LARK
60.62
69.30
71.13
71.50
62.04

HMM+ME+Sp2
63.51
68.60
72.35
71.32
79.52

B ROWN
32.55
48.18
56.59
60.15
74.01

C LARK
52.25
65.00
69.51
68.95
61.89

HMM+ME+Sp2
68.65
72.05
73.35
59.94
82.11

B ROWN
37.71
49.54
58.90
60.42
82.82

C LARK
43.58
48.62
51.92
55.87
60.88

En17
LDC
53.39
64.03
67.35
68.02
77.14
BG
LDC
40.61
53.58
60.82
62.06
67.02
DK
LDC
35.65
40.17
47.96
46.12
77.91

HMM+ME+Sp2
75.79
76.50
74.29
75.31
71.40
HMM+ME+Sp2
68.17
73.54
71.53
68.59
65.89
HMM+ME+Sp2
64.41
66.98
65.21
61.83
73.26

Table 4: 1-Many accuracy word frequency different corpora.
5.6 Using Clusters
comparison different POS induction methods, experiment simple
semisupervised scheme use learned clusters features supervised POS tagger.
basic supervised model features HMM+ME model, except use
word identities suffixes regardless frequency. trained supervised model using
averaged perceptron number iterations chosen follows: split training set 20%
development 80% training pick number iterations optimize accuracy
development set. Finally, trained full training set using iterations report results
500 sentence test set.
augmented standard features learned hidden state current token,
unsupervised method (B ROWN,C LARK,LDC, HMM+ME+Sp). Figure 4 shows average
accuracy supervised model varied type unsupervised features. average
taken 10 random samples training set training set size. see Figure 4
using sem-supervised features models improves performance even
500 labeled sentences. Moreover, see HMM+ME+Sp either performs well better
models.

544

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

LDC
Brown
HMM+ME+Sp
Clark

10
8
6
4
2
0

100 200 300 400 500
# Training samples

ES

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

10
8
6
4
2
0

Improvement:

En17

10
8
6
4
2
0

Improvement:

BG

Improvement:

10
8
6
4
2
0

100 200 300 400 500
# Training samples

10
8
6
4
2
0

Improvement:

LDC
Brown
HMM+ME+Sp
Clark

Improvement:

En45

Improvement:

10
8
6
4
2
0

PT

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

DK

LDC
Brown
HMM+ME+Sp
Clark

100 200 300 400 500
# Training samples

Figure 4: Error reduction using induced clusters features semi-supervised model
function labeled data size. Top Left: En45. Top Middle: En17. Top Right: PT.
Bottom Left: BG. Bottom Middle: ES. Bottom Right: DK.

6. Conclusion
work investigated task fully unsupervised POS induction five different languages.
identified proposed solutions three major problems simple hidden Markov model
used extensively task: i) treating words atomically, ignoring orthographic
morphological information addressed replacing multinomial word distributions
small maximum-entropy models; ii) excessive number parameters allows models
fit irrelevant correlations adressed discarding parameters small support
corpus; iii) training regime (maximum likelihood) allows high word ambiguity
addressed training using PR framework word ambiguity penalty. show
solutions improve model performance improvements additive. Comparing
regular HMM achieve impressive improvement 10.4% average.
compared system main competing systems show approach
performs better every language except English. Moreover, approach performs well across
languages learning conditions, even hyperparameters tuned conditions.
induced clusters used features semi-supervised POS tagger trained small
amount supervised data, show significant improvements. Moreover, clusters induced
system always perform well better clusters produced systems.

545

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

Acknowledgments
Joo V. Graa supported fellowship Fundao para Cincia e Tecnologia (SFRH/
BD/ 27528/ 2006) FCT project CMU-PT/HuMach/0039/2008 FCT (INESC-ID multiannual funding) PIDDAC Program funds. Kuzman Ganchev partially supported
NSF ITR EIA 0205448. Ben Taskar partially supported DARPA CSSG 2009 Award
ONR 2010 Young Investigator Award. Lusa Coheur partially supported FCT (INESC-ID
multiannual funding) PIDDAC Program funds.

Appendix A. Unsupervised Optimization
Berg-Kirkpatrick et al. (2010) describe feature-rich HMM show training model
using direct gradient rather EM lead better results. However, report results
En45 corpus. Table 5 compares implementation training regimes (BK+EM,
BK+DG) different languages. Comparing two training regimes, see
clear winner. BK+EM wins 3 cases (Bg,En17,Dk) loses three.
clear predict method suitable. follow discussion
6 authors propose difference arises algorithm starts fine-tune
weights rare features relative trains weights common features short
suffixes. case direct gradient training, start optimization, weights common
features change rapidly weight gradient proportional feature frequency.
training progresses, weight transferred rarer features. contrast, EM training,
optimization done completion M-Step, even first iterations EM
counts mostly random, rarer features get lot weight mass. prevents
model generalizing, optimization terminates local maximum closer starting
point. allow EM use common features longer tried small experiments
initially permissive stopping criteria M-step. EM iterations
permissive stopping criteria, require stricter stopping criteria. tended improve EM,
find principled method setting schedule convergence criteria M-step.
Furthermore, small experiments explain direct gradient better EM
languages worse others.
related study (Salakhutdinov et al., 2003) compares convergence rate EM direct
gradient training, identifies conditions EM achieves Newton-like behavior,
achieves first-order convergence. conditions based amount missing information,
case approximated number hidden states. Potentially, difference
lead different local maxima, mainly due non-local nature line search procedure gradient based methods. fact, looking results, DG training seems work better
corpora higher number hidden states (En45, Es) work worse corpora
fewer hidden states (Bg,En17).
Table 5 compare implementation HMM+ME model implementation
Berg-Kirkpatrick et al. (2010), using conditions (regularization parameter, feature set,
convergence criteria, initialization) observe significant differences results. Communication
code-comparison revealed small implementation differences: use bias feature
not; random seed, parameters initialized differently theirs;
6. http://www.cs.berkeley.edu/~tberg/gradVsEM/main.html

546

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

En45
BK+EM 69.1
BK+DG 75.8
HMM+ME 67.1

1-Many
En17 Pt
Bg Dk Es
72.1 72.3 64.3 62.8 72.0
67.9 72.5 56.1 60.6 73.7
72.2 72.3 61.1 65.1 71.8

En45
48.3
54.5
45.0

1-1
En17 Pt
Bg Dk
Es
54.4 45.5 50.6 41.5 37.2
47.9 42.9 38.8 41.5 40.4
51.1 46.3 46.1 42.6 40.8

En45
BK+EM 3.31
BK+DG 3.01
HMM+ME 3.77

VI
En17 Pt
Bg Dk Es
3.04 3.19 3.30 3.90 4.15
3.21 3.29 3.89 4.15 3.56
3.11 3.21 3.46 3.77 3.56

En45
.645
.678
.606

V
En17 Pt
Bg Dk
Es
.568 .582 .479 .477 .596
.534 .574 .392 .477 .611
.564 .583 .460 .519 .608

Table 5: EM vs direct gradient Berg-Kirkpatrick et al. (2010) implementation compared
implementaion EM HMM maximum-entropy emission probabilities.
rows starting BK Berkeley implementation, rows starting
implementation.

different implementations optimization algorithm; different number iterations.
corpora differences result better performance implementation,
corpora implementation gets better results. leave details well better
understanding differences optimization procedure future work, since
main focus present paper.

Appendix B. Evaluation Metrics
compare performance different models one needs evaluate quality induced
clusters. Several evaluation metrics clustering proposed previous work. metrics
use evaluate divided two types (Reichart & Rappoport, 2009): mapping-based
information theoretic. Mapping based metrics require post-processing step map cluster
POS tag evaluate accuracy supervised POS tagging. Information-theoretic (IT)
metrics compare induced clusters directly true POS tags.
1-Many mapping 1-1 mapping (Haghighi & Klein, 2006) two widely-used mapping
metrics. 1-Many mapping, hidden state mapped tag cooccurs
most. means several hidden states mapped tag, tags might
used all. 1-1 mapping greedily assigns hidden state single tag. case
number tags hidden states same, give 1-1 correspondence. major
drawback latter mapping fails express information hidden states.
Typically, unsupervised models prefer explain frequent tags several hidden states,
combine rare tags. example Pt corpus 3 tags occur
corpus. Grouping together subdividing nouns still provides lot information
true tag assignments. However, would captured 1-1 mapping. metric
tends favor systems produce exponential distribution size induced cluster
independent clusters true quality, correlate well information theoretic
metrics (Graa et al., 2009). Nevertheless, 1-Many mapping drawbacks, since
distinguish clusters based frequent tag. So, cluster split almost evenly

547

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

nouns adjectives, cluster number nouns, mixture
words different tags gives 1-Many accuracy.
information-theoretic measures use evaluation variation information (VI)
(Meila, 2007) validity-measure (V) (Rosenberg & Hirschberg, 2007). based
entropy conditional entropy tags induced clusters. VI desirable geometric properties metric convexly additive (Meila, 2007). However, range VI values
dataset-dependent (VI lies [0, 2 log N ] N number POS tags) allow
comparison across datasets different N . validity-measure (V) entropy-based
measure always lies range [0, 1], satisfy geometric properties
VI. reported give high score large number clusters exist, even
low quality (Reichart & Rappoport, 2009). information-theoretic measures
proposed better handle different numbers clusters, instance NVI (Reichart & Rappoport,
2009). However, work testing conditions corpora number clusters problem exist. Christodoulopoulos, Goldwater, Steedman (2010)
present extensive comparison evaluation metrics. related work Maron, Lamar,
Bienenstock (2010) present another empirical study metrics conclude VI metric
produce results contradict true quality induced clustering, giving high
scores simple baseline systems, instance assigning label words.
point several problems 1-1 metric explained previously. Since
metric comparison focus work compare methods using four metrics
described section.

References
Abeill, A. (2003). Treebanks: Building Using Parsed Corpora. Springer.
Afonso, S., Bick, E., Haber, R., & Santos, D. (2002). Floresta Sinta(c)tica: treebank Portuguese. Proc. LREC, pp. 16981703.
Baum, L., Petrie, T., Soules, G., & Weiss, N. (1970). maximization technique occurring
statistical analysis probabilistic functions Markov chains. Annals Mathematical
Statistics, 41(1), 164171.
Berg-Kirkpatrick, T., Bouchard-Ct, A., DeNero, J., & Klein, D. (2010). Painless unsupervised
learning features. Proc. NAACL.
Bertsekas, D., Homer, M., Logan, D., & Patek, S. (1995). Nonlinear programming. Athena Scientific.
Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-based n-gram
models natural language. Computational Linguistics, 18, 467479.
Chen, S. (2003). Conditional joint models grapheme-to-phoneme conversion. Proc.
ECSCT.
Christodoulopoulos, C., Goldwater, S., & Steedman, M. (2010). Two decades unsupervised POS
induction: far come?. Proc. EMNLP, Cambridge, MA.
Civit, M., & Mart, M. (2004). Building cast3lb: spanish treebank. Research Language &
Computation, 2(4), 549574.

548

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

Clark, A. (2003). Combining distributional morphological information part speech induction. Proc. EACL.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data via
EM algorithm. Journal Royal Statistical Society. Series B (Methodological), 39(1).
Freitag, D. (2004). Toward unsupervised whole-corpus tagging. Proc. COLING. Association
Computational Linguistics.
Ganchev, K., Graa, J., Gillenwater, J., & Taskar, B. (2010). Posterior regularization structured
latent variable models. Journal Machine Learning Research, 11, 20012049.
Gao, J., & Johnson, M. (2008). comparison Bayesian estimators unsupervised hidden
Markov model POS taggers. Proc. EMNLP, pp. 344352, Honolulu, Hawaii. ACL.
Goldwater, S., & Griffiths, T. (2007). fully Bayesian approach unsupervised part-of-speech
tagging. Proc. ACL, Vol. 45, p. 744.
Graa, J., Ganchev, K., Pereira, F., & Taskar, B. (2009). Parameter vs. posterior sparisty latent
variable models. Proc. NIPS.
Graa, J., Ganchev, K., & Taskar, B. (2007). Expectation maximization posterior constraints.
Proc. NIPS. MIT Press.
Graa, J. a. d. A. V. (2010). Posterior Regularization Framework: Learning Tractable Models
Intractable Constraints. Ph.D. thesis, Universidade Tcnica de Lisboa, Instituto Superior
Tcnico.
Haghighi, A., & Klein, D. (2006). Prototype-driven learning sequence models. Proc. HTLNAACL. ACL.
Headden, III, W. P., McClosky, D., & Charniak, E. (2008). Evaluating unsupervised part-of-speech
tagging grammar induction. Proc. COLING, pp. 329336.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers via
syntactic projection across parallel texts. Special Issue Journal Natural Language
Engineering Parallel Texts, 11(3), 311325.
Johnson, M. (2007). doesnt EM find good HMM POS-taggers. Proc. EMNLP-CoNLL.
Kromann, Matthias T. (2003). Danish Dependency Treebank underlying linguistic
theory. Second Workshop Treebanks Linguistic Theories (TLT), pp. 217220, Vxj,
Sweden.
Lamar, M., Maron, Y., & Bienenstock, E. (2010). Latent-descriptor clustering unsupervised POS
induction. Proceedings 2010 Conference Empirical Methods Natural Language
Processing, pp. 799809, Cambridge, MA. Association Computational Linguistics.
Lamar, M., Maron, Y., Johnson, M., & Bienenstock, E. (2010). SVD clustering unsupervised POS tagging. Proceedings ACL 2010 Conference: Short Papers, pp. 215219,
Uppsala, Sweden. Association Computational Linguistics.
Lee, Y. K., Haghighi, A., & Barzilay, R. (2010). Simple type-level unsupervised POS tagging.
Proceedings 2010 Conference Empirical Methods Natural Language Processing,
pp. 853861, Cambridge, MA. Association Computational Linguistics.

549

fiG RAA , G ANCHEV, C OHEUR , P EREIRA , & TASKAR

Marcus, M., Marcinkiewicz, M., & Santorini, B. (1993). Building large annotated corpus
English: Penn Treebank. Computational linguistics, 19(2), 313330.
Maron, Y., Lamar, M., & Bienenstock, E. (2010). Evaluation criteria unsupervised POS induction. Tech. rep., Indiana University.
Martin, S., Liermann, J., & Ney, H. (1998). Algorithms bigram trigram word clustering.
Speech Communication, pp. 12531256.
Meila, M. (2007). Comparing clusteringsan information based distance. J. Multivar. Anal., 98(5),
873895.
Merialdo, B. (1994). Tagging English text probabilistic model. Computational linguistics,
20(2), 155171.
Moon, T., Erk, K., & Baldridge, J. (2010). Crouching Dirichlet, hidden Markov model: Unsupervised POS tagging context local tag generation. Proc. EMNLP, Cambridge, MA.
Neal, R. M., & Hinton, G. E. (1998). new view EM algorithm justifies incremental,
sparse variants. Jordan, M. I. (Ed.), Learning Graphical Models, pp. 355368.
Kluwer.
Nocedal, J., & Wright, S. J. (1999). Numerical optimization. Springer.
Ratnaparkhi, A. (1996). maximum entropy model part-of-speech tagging. Proc. EMNLP.
ACL.
Ravi, S., & Knight, K. (2009). Minimized models unsupervised part-of-speech tagging.
Proc. ACL.
Reichart, R., & Rappoport, A. (2009). NVI clustering evaluation measure. Proc. CONLL.
Rosenberg, A., & Hirschberg, J. (2007). V-measure: conditional entropy-based external cluster
evaluation measure. EMNLP-CoNLL, pp. 410420.
Salakhutdinov, R., Roweis, S., & Ghahramani, Z. (2003). Optimization EM expectationconjugate-gradient. Proc. ICML, Vol. 20.
Schtze, H. (1995). Distributional part-of-speech tagging. Proc. EACL, pp. 141148.
Shen, L., Satta, G., & Joshi, A. (2007). Guided learning bidirectional sequence classification.
Proc. ACL, Prague, Czech Republic.
Simov, K., Osenova, P., Slavcheva, M., Kolkovska, S., Balabanova, E., Doikoff, D., Ivanova, K.,
Simov, A., Simov, E., & Kouylekov, M. (2002). Building Linguistically Interpreted Corpus
Bulgarian: BulTreeBank. Proc. LREC.
Smith, N., & Eisner, J. (2005). Contrastive estimation: Training log-linear models unlabeled
data. Proc. ACL. ACL.
Snyder, B., Naseem, T., Eisenstein, J., & Barzilay, R. (2008). Unsupervised multilingual learning
POS tagging. Proceedings Conference Empirical Methods Natural Language
Processing, pp. 10411050. Association Computational Linguistics.
Toutanova, K., & Johnson, M. (2007). Bayesian LDA-based model semi-supervised part-ofspeech tagging. Proc. NIPS, 20.

550

fiC ONTROLLING C OMPLEXITY PART- -S PEECH NDUCTION

Toutanova, K., Klein, D., Manning, C., & Singer, Y. (2003). Feature-rich part-of-speech tagging
cyclic dependency network. Proc. HLT-NAACL.
Zhao, Q., & Marcus, M. (2009). simple unsupervised learner POS disambiguation rules given
minimal lexicon. Proc. EMNLP.

551


