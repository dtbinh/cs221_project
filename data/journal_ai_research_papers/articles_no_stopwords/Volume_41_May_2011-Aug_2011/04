journal artificial intelligence

submitted published

controlling complexity part speech induction
joo v graa

joao graca l f inesc id pt

l f inesc id
lisboa portugal

kuzman ganchev

kuzman google com

google inc
york ny usa

lusa coheur

luisa coheur l f inesc id pt



l f inesc id
lisboa portugal

fernando pereira

pereira google com

google inc
mountain view ca usa

ben taskar

taskar cis upenn edu

computer information science
university pennsylvania

abstract
consider fully unsupervised learning grammatical part speech categories unlabeled text standard maximum likelihood hidden markov model
task performs poorly weak inductive bias large model capacity address
refining model modifying learning objective control capacity via parametric non parametric constraints enforces word category association sparsity
adds morphological orthographic features eliminates hard estimate parameters rare
words develop efficient learning much computationally intensive standard training provide open source implementation
experiments five diverse languages bulgarian danish english portuguese spanish achieve
significant improvements compared previous methods task

introduction
part speech pos categories elementary building blocks syntactic analysis text
play important role many natural language processing tasks machine translation
information extraction english handful languages fortunate enough
comprehensive pos annotated corpora penn treebank marcus marcinkiewicz
santorini worlds languages extremely limited linguistic resources
unrealistic expect annotation efforts catch explosion unlabeled electronic
text anytime soon lack supervised data likely persist near future
investment required accurate linguistic annotation took two years annotate sentences
syntactic parse trees chinese treebank hwa resnik weinberg cabezas kolak
four seven years annotate sentences across range languages abeill


c

ai access foundation rights reserved

fig raa g anchev c oheur p ereira taskar

supervised learning taggers pos annotated training text well studied task
several methods achieving near human tagging accuracy ratnaparkhi toutanova klein
manning singer shen satta joshi however pos induction one
access labeled corpus difficult task much room improvement
recent literature pos induction used refer two different tasks first one
addition raw text given dictionary containing possible tags word
goal disambiguate tags particular word occurrence merialdo second
task given raw text dictionary provided goal cluster words
grammatical behavior work target latter challenging unsupervised pos
induction task
recent work task typically relies distributional morphological features since words
grammatical function tend occur similar contexts common morphology brown desouza mercer pietra lai schtze clark however
statistical regularities enough overcome several challenges first
decide many clusters use broad syntactic categories instance whether distinguish
plural singular nouns second category size distribution tends uneven example vast majority word types open class nouns verbs adjectives even among
open class categories many nouns adjectives runs contrary learning
biases commonly used statistical common failure clump several
rare categories together split common categories
individual word types third challenge arises ambiguity grammatical role
word sense many words take different pos tags different occurrences depending
context occurrence word run verb noun approaches assume
computational statistical simplicity word one tag aggregating
local contexts distributional clustering schtze one tag per word
assumption clearly wrong across many languages annotated corpora
methods perform competitively methods assign different tags word
different contexts lamar maron johnson bienenstock partly due typical
statistical dominance one tags word especially corpus includes single genre
news reason less restrictive encode useful bias
words typically take small number tags
approaches make one tag per word assumption take form hidden
markov model hmm hidden states represent word classes observations
word sequences brown et al johnson unfortunately standard hmms trained
maximize likelihood perform poorly since learned hidden classes align well true
pos tags besides potential model estimation errors due non convex optimization involved
training pernicious typical maxima likelihood align well
maxima pos tag accuracy smith eisner graa ganchev pereira taskar
suggesting serious mismatch model data
work significantly reduce modeling mismatch combining three ideas
standard hmm treats words atomic units without orthographic morphological information information critical generalization many languages clark
address reparameterize standard hmm replacing multinomial emission distributions maximum entropy similar work berg

fic ontrolling c omplexity part peech nduction

kirkpatrick bouchard ct denero klein graa allows use
orthographic morphological features emission model moreover standard
hmm model large number parameters number tags times number
word types presents extremely rich model space capable fitting irrelevant correlations data address dramatically reduce number parameters
model discarding features small support corpus involving
rare words word parts
hmm model allows high level ambiguity tags word
maximizing marginal likelihood common words typically tend associated every
tag non trivial probability johnson however natural property pos
categories across many languages annotation standards word small
number allowed tags address use posterior regularization pr
framework graa ganchev taskar ganchev graa gillenwater taskar
constrain ambiguity word tag associations via sparsity inducing penalty
model posteriors graa et al
proposed extensions improves standard hmm performance
moreover gains nearly additive improvements significant across different
metrics previously proposed task instance many metric method attains
average improvement regular hmm compare proposed method
eleven previously proposed approaches languages english metrics except
method achieves best published furthermore method appears stable
across different testing scenarios shows competitive finally
induced tags used improve performance supervised pos tagging system
limited labeled data scenario open source software pos induction evaluation
available http code google com p pr toolkit
organized follows section describes basic hmm pos induction
maximum entropy extension section describes standard em sparsity inducing estimation
method section presents comprehensive survey previous fully unsupervised pos induction
methods section provide detailed experimental evaluation method finally
section summarize suggest ideas future work


model experiments first order hmm denote sequence
words sentence boldface x sequence hidden states correspond partof speech tags boldface sentence length l thus l hidden state variables
yi j l j number possible pos tags l observation variables
xi v l v number word types simplify notation assume
every tag sequence prefixed conventional start tag start allowing us write
p initial state probability hmm
probability sentence x along particular hidden state sequence given
p x

l


pt yi yi po xi yi







fig raa g anchev c oheur p ereira taskar

po xi yi probability observing word xi given state yi emission
probability pt yi yi probability state yi given previous hidden
state yi transition probability
multinomial emission model
standard hmms use multinomial emission transition probabilities generic word
xi tag yi observation probability po xi yi transition probability pt yi yi
multinomial distributions experiments refer model simply hmm model
large number parameters large number word types see table
common convention follow lowercase words well map words occurring
corpus special token unk
maximum entropy emission model
work use simple modification hmm model discussed previous section
represent conditional probability distributions maximum entropy log linear specifically emission probability expressed
exp f x

x exp f x

po x p



f x feature function x ranges word types model parameters
refer model hmm addition word identity features include orthographyand morphology inspired cues presence capitalization digits common suffixes
feature sets described section idea replacing multinomial hmm
maximum entropy applied different domains chen
well pos induction berg kirkpatrick et al graa key advantage
representation allows much tighter control expressiveness
model many languages helpful exclude word identity features rare words order
constrain model force generalization across words similar features unlike mapping
rare words unk token multinomial setting maxent model still captures
information word features moreover reduce number
parameters even lowercase word identities still keeping case information
case feature table shows number features used different corpora note
reduced feature set order magnitude fewer parameters multinomial model

learning
section describe experiments comparing hmm model model three
learning scenarios maximum likelihood training em dempster laird rubin hmm hmm gradient likelihood optimization hmm
model pr sparsity constraints graa et al hmm hmm
section describes three learning
following denote whole corpus list sentences x x x xn
corresponding tag sequences yn



fic ontrolling c omplexity part peech nduction

maximum likelihood em
standard hmm training seeks model parameters maximize log likelihood observed
data
x
p x

log likelihood l log


x whole corpus since model assumes independence sentences
log

x

p x

n
x

log

x



yn

n



p xn yn

use corpus notation consistency section latent variables
log likelihood function hmm model convex model parameters
model fitted em em maximizes l via block coordinate ascent lower
bound f q auxiliary distribution latent variables q neal hinton
jensens inequality define lower bound f q
l log

x

q



p x x
p x

q log
f q
q
q





rewrite f q
f q

x

q log p x p x



x

q log q





q
q log
p x



l kl q p x



l

x


interpretation view em performing coordinate ascent f q starting
initial parameter estimate iterates two block coordinate ascent steps
convergence criterion reached
e q arg max f q arg min kl q k pt x
q



q

arg max f q arg max eqt log p x






e step corresponds maximizing eq respect q step corresponds
maximizing eq respect em guaranteed converge local
maximum l mild conditions neal hinton hmm pos tagger
e step computes posteriors pt x latent variables pos tags given observed
variables words current parameters sentence accomplished forwardbackward hmms em together forward backward
hmms usually referred baumwelch baum petrie soules weiss

step uses q qnt posteriors given sentence fill values tags
estimate parameters since hmm model locally normalized features used


fig raa g anchev c oheur p ereira taskar

depend tag word identities particular position occur
optimization decouples following way
eqt log p x


n
x

eqnt log

n
ln
n x
x

ln


n
pt yin yi
po xni yin




n
eqnt log pt yin yi
eqnt log po xni yin





n

multinomial emission model optimization particularly easy simply involves
normalizing expected counts parameter maximum entropy emission model parameterized equation closed form solution need solve unconstrained
optimization possible hidden tag value solve two estimate emission probabilities po x estimate transition probabilities pt
gradient one given


eqt log p x
eqt f x ep x f x


similar gradient supervised except expectation
q instead observed optimization done l bfgs wolfes rule
line search nocedal wright
maximum likelihood direct gradient
likelihood traditionally optimized em berg kirkpatrick et al
hmm maximum entropy emission model higher likelihood better accuracy
achieved gradient likelihood optimization method use l bfgs
experiments derivative likelihood
l






x
log p x
p x
p x

p x
p x

x
x p x


p x
log p x
p x
p x


x


p x log p x









exactly derivative step equation apply chain
rule take derivative log p x equation apply chain rule reverse
direction biggest difference em procedure direct gradient em
fix counts e step optimize model counts directly
optimizing likelihood need recompute counts parameter setting
expensive appendix gives detailed discussion methods
controlling tag ambiguity pr
one unsupervised hmm pos tagging maximum likelihood objective may
encourage tag distributions allow many different tags word given context


fic ontrolling c omplexity part peech nduction





supervised
hmm
hmm
hmm sp
hmm sp


l l


l l



supervised
hmm
hmm
hmm sp
hmm sp



















rank word l l

rank word l l

figure ambiguity measure word type two corpora supervised
model em training hmm hmm train ambiguity penalty
described section hmm sp hmm sp left en right pt

actual text linguist designed tags tags designed informative
words grammatical role following paragraphs describe measure tag ambiguity
proposed graa et al attempt control easier understand measure
hard tag assignments start thene extend discussion distributions
tags
consider word stock intuitively would occurrences stock
tagged small subset possible tags noun verb case hard assignment
tags entire corpus could count many different tags used occurrences
word stock
instead single tagging corpus distribution q assignments
need generalize ambiguity measure instead asking particular tag ever used
word stock would ask maximum probability particular tag
used word stock instead counting number tags would sum
probabilities
motivation figure shows distribution tag ambiguity across words two corpora
see figure train em procedure described section
hmm grossly overestimates tag ambiguity almost words however
trained pr penalize tag ambiguity hmm sp
hmm sp achieve tag ambiguity closer truth
formally graa et al define measure terms constraint features x
constraint feature wvj x takes value j th occurrence word type w x assigned
tag v tag assignment consequently probability j th occurrence word w
tag v label distribution q eq wvj x ambiguity measurement
word type w becomes
x
ambiguity penalty word type w
max eq wvj x

v

j

sum maxima called mixed norm brevity use norm notation eq w computational reasons add penalty term ambiguity model distribution p x instead introduce auxiliary distribution q


fig raa g anchev c oheur p ereira taskar

must close p must low ambiguity modified objective becomes
x
eq w x
max l kl q p x
q



w

graa et al optimize objective similar em added complexity implementing lies computing kullback leibler projection
modified e step however computation involves choosing distribution exponentially
many objects label assignments luckily graa et al dual formulation
e step manageable given

x
x
max log
p x exp x

wvj



j



vector dual parameters wvj one wvj projected distribution
given q p x exp x note p given hmm q
sentence expressed
q yn




n
pt yin yi
qo xni yin





qo xi yi po xi yi exp xi yi j act modified unnormalized emission probabilities
objective equation negative sum log probabilities sentences
q plus constant compute running forward backward corpus similar
e step normal em gradient objective computed forwardbackward note objective eq concave respect
optimized variety methods perform dual optimization projected gradient
fast simplex projection described bertsekas homer logan patek
experiments found taking projected gradient steps enough
performing optimization convergence helps

related work
pos tags place words classes share commonalities classes words
cooccur therefore natural ask whether word clustering methods word
context distributions might able recover word classification inherent pos tag set
several influential methods notably mutual information clustering brown et al
used cluster words according immediately contiguous words distributed
although methods explicitly designed pos induction resulting clusters capture syntactic information see martin liermann ney different method
similar objective clark refined distributional clustering adding
morphological word frequency information obtain clusters closely resemble pos
tags
forms distributional clustering go beyond immediate neighbors word represent whole vector coocurrences target word within text window compare
vectors suitable metric cosine similarity however wider range similarities capturing local regularities instance adjective noun might


fic ontrolling c omplexity part peech nduction

look similar noun tends used noun noun compounds similarly two adjectives
different semantics selectional preferences might used different contexts moreover
aggravated data sparsity example infrequent adjectives modify different nouns tend completely disjoint context vectors even frequent words
might completely different context vectors since articles used disjoint right
contexts alleviate schtze used frequency cutoffs singular value decomposition co occurrence matrices approximate co clustering two stages svd
clusters first stage used instead individual words provide vector representations second stage clustering
lamar maron johnson recently revised two stage svd model schtze
achieve close state art performance revisions relatively small
touch several important aspects model singular vectors scaled singular values
preserve geometry original space latent descriptors normalized unit length
cluster centroids computed weighted average constituent vectors word
frequency rare common words treated differently centroids initialized
deterministic manner
final class approaches include work uses sequence model
hmm represent probabilistic dependencies consecutive tags
approaches observation corresponds particular word hidden state corresponds
cluster however noted clark johnson maximum likelihood
training achieve good maximum likelihood training tends
ambiguous distributions common words contradiction rather sparse wordtag distribution several approaches proposed mitigate freitag
clusters frequent words distributional co clustering cluster
remaining infrequent words author trains second order hmm emission probabilities frequent words fixed clusters found earlier emission probabilities
remaining words uniform
several studies propose bayesian inference improper dirichlet prior favor
sparse model parameters hence indirectly reduce tag ambiguity johnson gao johnson goldwater griffiths refined moon erk baldridge
representing explicitly different ambiguity patterns function content words
lee haghighi barzilay take direct reducing tag ambiguity explicitly modeling set possible tags word type model first generates tag
dictionary assigns mass one tag word type reflect lexicon sparsity dictionary used constrain dirichlet prior emission probabilities drawn
support word tag pairs dictionary token level hmm
emission parameters transition parameters draw symmetric dirichlet prior used
tagging entire corpus authors improvements morphological features
creating dictionary system achieves state art several languages
noted common issue sparsity inducing approaches sparsity
imposed parameter level probability word given tag desired sparsity
posterior level probability tag given word graa et al use pr framework
penalize ambiguous posteriors distributions words given tokens achieves better
bayesian sparsifying dirichlet priors



fig raa g anchev c oheur p ereira taskar

recently berg kirkpatrick et al graa proposed replacing multinomial distributions hmm maximum entropy distributions allows use features capture morphological information achieve promising berg kirkpatrick
et al optimizing likelihood l bfgs rather em leads substantial improvements case beyond english
note briefly pos induction methods rely prior tag dictionary indicating
word type pos tags pos induction task word token
corpus disambiguate possible pos tags described merialdo unfortunately availability large manually constructed tag dictionary unrealistic much
later work tries reduce required dictionary size different ways generalizing
small dictionary handful entries smith eisner haghighi klein
toutanova johnson goldwater griffiths however although greatly
simplifies words one tag furthermore cluster tag mappings predetermined thus removing extra level ambiguity accuracy methods
still significantly behind supervised methods address remaining ambiguity imposing
additional sparsity ravi knight minimize number possible tag tag transitions
hmm via integer program finally snyder naseem eisenstein barzilay jointly
train pos induction system parallel corpora several languages exploiting fact
different languages present different ambiguities

experiments
section present encouraging validating proposed method six different testing
scenarios according different metrics highlights
maximum entropy emission model markov transition model trained ambiguity penalty improves regular hmm cases average improvement
according many metric
compared broad range recent pos induction systems method produces
best languages except english furthermore method seems less sensitive
particular test conditions previous methods
induced clusters useful features training supervised pos taggers improving test
accuracy much clusters learned competing methods
corpora
experiments test several pos induction methods five languages help manually pos tagged corpora languages table summarizes characteristics test corpora
wall street journal portion penn treebank marcus et al consider
tag version smith eisner en tag version en bosque subset
portuguese floresta sinta c tica treebank afonso bick haber santos pt
bulgarian bultreebank simov et al bg coarse tags spanish corpus cast lb treebank civit mart es danish dependency treebank
ddt kromann matthias dk



fic ontrolling c omplexity part peech nduction

en
pt
bg
es
dk


sentences







types







lunk







tokens







tags







avg







total







w







w






table corpus statistics third column shows percentage word types lower casing
eliminating word types occurring sixth seventh columns
information word ambiguity corpus average totality corresponding penalty equation eighth ninth columns number
parameters different feature sets described section

experimental setup
compare work two kinds methods induce single cluster word
type type level tagging allow different tags different occurrences word type
token level tagging type level tagging use two standard baselines b rown c lark
described brown et al clark following headden mcclosky charniak trained c lark system hidden states letter hmm
ran iterations b rown system run according instructions accompanying
code ran recently proposed ldc system lamar maron bienenstock
configuration described ptb ptb ptb configuration
corpora noted carry experiments svd
system lamar maron johnson since svd superseded ldc according
authors
token level tagging experimented feature rich hmm presented bergkirkpatrick et al trained em training bk em direct gradient bk dg
configuration provided authors report type level hmm
tlhmm lee et al applicable since able run system moreover
compared systems implementation hmm approaches
hmm multinomial emission probabilities section hmm maximumentropy emission probabilities section trained em hmm trained direct gradient hmm dg trained pr ambiguity penalty described section
hmm sp multinomial emissions hmm sp maximum entropy emissions
addition compared multinomial hmm sparsifying dirichlet prior parameters hmm vb trained variational bayes johnson
following standard practice multinomial hmms use morphological information lowercase corpora replace unique words special unknown token
improves multinomial hmm decreasing number parameters eliminating





implementation http www cs berkeley edu pliang software brown cluster zip
implementation http www cs rhul ac uk home alexc pos tar gz
implementation provided lamar maron bienenstock
implementation provided berg kirkpatrick et al



fig raa g anchev c oheur p ereira taskar

rare words mostly nouns since maximum entropy emission access morphological features preprocessing steps improve performance perform
case
start em randomly initialize implementations hmm
posteriors obtained running e step hmm model set random
parameters close uniform random uniform jitter means random
seed initialization identical
em variational bayes training train model iterations since found
typically tend converge iteration hmm vb model fix
transition prior test emission prior equal corresponding
best values reported johnson
pr training initialize em iterations run iterations pr following graa et al used worked best english en graa et al
regularizing words occur least times use configuration scnenarios setting specifically tuned test languages
might optimal every language setting parameters unsupervised manner
difficult task address graa discusses experiments different
values parameters
obtain hard assignments posterior decoding position pick label
highest posterior probability since showed small consistent improvements viterbi
decoding experiments required random initialization parameters report
average random seeds
experiments run number true tags number clusters
obtained test set portion corpus evaluate systems four common metrics
pos induction many mapping mapping haghighi klein variation information vi meila validity measure v rosenberg hirschberg metrics
described detail appendix b
hmm sp performance
section compares gains feature rich representation ambiguity penalty described section experiments feature rich representation improves performance ambiguity penalty improves
performance see improvements two methods combine additively
suggesting address independent aspects pos induction
use two different feature sets large feature set berg kirkpatrick et al
reduced feature set described graa apply count feature selection identity suffix features specifically add identity features words
occurring least times suffix features words occurring least times add
punctuation feature follows refer large feature set feature set reduced
feature set total number features model language given table
experiments summarized table
table shows training methods across six corpora four evaluation metrics
resulting experimental conditions simplify discussion focus many metric
transition prior significantly affect report different values



fic ontrolling c omplexity part peech nduction























hmm
hmm sp
hmm prior
hmm prior
hmm prior
hmm prior
hmm sp prior
hmm sp prior
hmm sp prior
hmm sp prior

en











en











many
pt bg











dk












es ptb ptb pt bg dk es





















hmm
hmm sp
hmm prior
hmm prior
hmm prior
hmm prior
hmm sp prior
hmm sp prior
hmm sp prior
hmm sp prior

en











en











vi
pt bg











dk











v
es ptb ptb pt bg dk es





















table different hmms hmm hmm sp hmms multinomial emission
functions trained em pr sparsity constraints respectively hmm
hmm spare hmms maximum entropy emission model trained em
pr sparsity constraints feature rich superscript represents large
feature set superscript represents reduced feature set prior refers
regularization strength emission model table entries averaged
runs bold indicates best system overall

top left tab table observe conclusions hold three evaluation
metrics table conclude following
adding penalty high word tag ambiguity improves performance multinomial
hmm multinomial hmm trained em line table worse
multinomial hmm trained pr ambiguity penalty average line
table
feature rich maximum entropy hmms lines table almost perform better
multinomial hmm true feature sets regularization strengths
used average increase exceptions possibly due suboptimal regularization
adding penalty high word tag ambiguity maximum entropy hmm improves performance almost cases comparing lines lines table sparsity
constraints improve performance average improvement combined system al

fig raa g anchev c oheur p ereira taskar

outperforms multinomial hmm trained ambiguity penalty
average improvement every corpus best performance achieved
model ambiguity penalty maximum entropy emission probabilities
every language except english tags particular feature configuration reducing feature set excluding rare features improves performance average lines
better lines table
regularizing maximum entropy model important many features
word tag ambiguity penalty lines table
maximum entropy hmm many features see tight parameter prior
almost performs looser prior contrast looking lines table see ambiguity penalty fewer features looser prior
almost better tighter parameter prior observed graa
encouraging see improvements feature rich model additive
effects penalizing tag ambiguity especially surprising since optimize strength tag ambiguity penalty maximum entropy emission hmm rather
used value reported graa et al work multinomial emission hmm experiments reported graa tuning parameter improve performance
nevertheless methods regularize objective different ways interaction
accounted would interesting use l regularization instead
l regularization together feature count cutoff way model could learn features discard instead requiring predefined parameter depends particular corpus
characteristics
reported berg kirkpatrick et al way objective optimized
big impact overall however due non convex objective function
unclear optimization method works better briefly analyze question
appendix leave open question future work
error analysis
figure shows distribution true tags clusters hmm model left
hmm sp model right en corpus bar represents cluster labeled tag
assigned performing many mapping colors represent number words
corresponding true tag reduce clutter true tags never used label cluster
grouped others
observe split common tags nouns several hidden states
splitting accounts many errors states nouns instead
hmm sp able use states adjectives another improvement comes
better grouping prepositions example grouped punctuation hmm
hmm sp correctly mapped prepositions although correct
behavior actually hurts since tagset special tag occurrences word
incorrectly assigned resulting loss accuracy contrast hmm state
mapped tag word comprises one fifth state common
error made hmm sp include word second noun induced tag
figure right induced tag contains mostly capitalized nouns pronouns often


fiprep
det

n
adj

rpunc
pos

v
inpunc

conj


epunc
others

prep
det

n
adj

rpunc
pos

v
inpunc

conj


conj

endpunc

v

inpunc

v

v

adj

rpunc

adj

n

adj

n

n

n

n

det

prep



endpunc

conj

v

inpunc

v

pos

n

adj

n

n

n

n

n

n

det

prep

c ontrolling c omplexity part peech nduction

epunc
others

figure induced tags hmm model left hmm sp model right
en corpus column represents hidden state labeled many
mapping unused true tags grouped cluster named others
























art
n
adj

prop
v fin
prp

punc
num
adv

v pcp
v inf
conj c

pron pers
sumothers

art
n
adj

prop
v fin
prp

punc
num
adv

v pcp
v inf
conj c

pron pers
sumothers

figure induced tags hmm model left hmm sp model right
pt corpus column represents hidden state labeled many mapping
unused true tags grouped cluster named others

precede nouns induced tags suspect capitalization feature cause
error
better performance feature portuguese relative english may due
ability features better represent richer morphology portuguese figure shows
induced clusters portuguese hmm sp model improves hmm tags
except adjectives trouble distinguishing nouns adjectives reduced
accuracy adjectives hmm sp explained mapping single cluster containing
adjectives adjectives hmm model nouns hmm sp model
removing noun adjective distinction suggested zhao marcus would increase
performance another qualitative difference observed
hmm sp model used single induced cluster proper nouns rather spreading
across different clusters



fig raa g anchev c oheur p ereira taskar

state art comparison
compare best pos induction system settings line table
recent systems summarized table previously done table
focus discussion many evaluation metric qualitatively
vi v metrics metric shows variance across languages
many
pt bg













dk














es ptb ptb pt bg dk es


























vi
pt bg












dk












v
es ptb ptb pt bg dk es




































en
b rown

c lark

c lark

ldc

hmm

hmm vb

hmm vb

hmm sp

bk em

bk dg

tlhmm

hmm sp prior

en























en
b rown

c lark

c lark

ldc

hmm

hmm vb

hmm vb

hmm sp

bk em

bk dg

hmm sp prior

en












table comparing hmm sp several pos induction systems
random initialization run systems represent
average runs see section details discussion

lines table clustering information gain metrics b rown wins times scenarios fewer clusters c lark system despite
fact c lark uses morphology comparing lines table line see
ldc system particularly strong en achieves state art behaves
worse b rown system every corpus
hmms multinomial emissions lines table maximum likelihood training hmm parameter sparsity hmm vb perform worse adding ambiguity penalty
hmm sp holds evaluation metrics exception confirms previous graa et al comparing lines lines see



fic ontrolling c omplexity part peech nduction

best hmm hmm sp performs comparably best clustering b rown one
model winning languages remaining
feature rich hmms bk em bk dg perform well achieving
better hmm sp tests even though optimize objective achieve
different different corpora explore training procedure detail appendix comparing implementation berg kirkpatrick et al brevity
table contains implementation berg kirkpatrick et al
implementation produces comparable quite identical
lines table display two methods attempt control tag ambiguity
feature rich representation capture morphological information tlhmm
taken lee et al report en bg corpora
able rerun experiments tlhmm able compute
information theoretic metrics consequently comparison tlhmm slightly less complete
methods tlhmm hmm sp perform competitively better
systems surprising since ability model morphological
regularity penalizing high ambiguity comparing tlhmm hmm sp see
hmm sp performs better many metric contrast tlhmm performs better
one possible explanation underlying model tlhmm bayesian hmm
sparsifying dirichlet priors noted graa et al trained way tend
cluster distribution closely resemble true pos distribution clusters lots
words words favors metric description particularity
metric discussed appendix b
summarize non english languages metrics except hmm sp
system performs better systems english bk dg wins tag corpus
ldc wins tag corpus hmm sp system fairly robust performing well
corpora best several allow us conclude tuned
particular corpus evaluation metric
performance hmm sp tightly related performance underlying
hmm system appendix present discussion performance different optimization methods hmm compare hmm implementation bk em
bk dg significant differences performance however
clear one better performs better given situation
mentioned clark morphological information particularly useful rare words
table compares different accuracy words according frequency compare clustering information gain without morphological information
b rown c lark distributional information model ldc feature rich hmm
tag ambiguity control hmm sp expected see systems morphology
better rare words moreover systems improve almost categories except
common words words occurring times comparing hmm sp c lark
see even condition c lark overall works better en still performs worse
rare words hmm sp



fig raa g anchev c oheur p ereira taskar







b rown






c lark












b rown






c lark












b rown






c lark






en
ldc





pt
ldc





es
ldc






hmm sp






b rown






c lark






hmm sp






b rown






c lark






hmm sp






b rown






c lark






en
ldc





bg
ldc





dk
ldc






hmm sp





hmm sp





hmm sp






table many accuracy word frequency different corpora
clusters
comparison different pos induction methods experiment simple
semisupervised scheme use learned clusters features supervised pos tagger
basic supervised model features hmm model except use
word identities suffixes regardless frequency trained supervised model
averaged perceptron number iterations chosen follows split training set
development training pick number iterations optimize accuracy
development set finally trained full training set iterations report
sentence test set
augmented standard features learned hidden state current token
unsupervised method b rown c lark ldc hmm sp figure shows average
accuracy supervised model varied type unsupervised features average
taken random samples training set training set size see figure
sem supervised features improves performance even
labeled sentences moreover see hmm sp performs well better




fic ontrolling c omplexity part peech nduction

ldc
brown
hmm sp
clark


training samples

ldc
brown
hmm sp
clark









training samples

es

ldc
brown
hmm sp
clark


training samples








improvement

en








improvement

bg

improvement









training samples








improvement

ldc
brown
hmm sp
clark

improvement

en

improvement








pt

ldc
brown
hmm sp
clark


training samples

dk

ldc
brown
hmm sp
clark


training samples

figure error reduction induced clusters features semi supervised model
function labeled data size top left en top middle en top right pt
bottom left bg bottom middle es bottom right dk

conclusion
work investigated task fully unsupervised pos induction five different languages
identified proposed solutions three major simple hidden markov model
used extensively task treating words atomically ignoring orthographic
morphological information addressed replacing multinomial word distributions
small maximum entropy ii excessive number parameters allows
fit irrelevant correlations adressed discarding parameters small support
corpus iii training regime maximum likelihood allows high word ambiguity
addressed training pr framework word ambiguity penalty
solutions improve model performance improvements additive comparing
regular hmm achieve impressive improvement average
compared system main competing systems
performs better every language except english moreover performs well across
languages learning conditions even hyperparameters tuned conditions
induced clusters used features semi supervised pos tagger trained small
amount supervised data significant improvements moreover clusters induced
system perform well better clusters produced systems



fig raa g anchev c oheur p ereira taskar

acknowledgments
joo v graa supported fellowship fundao para cincia e tecnologia sfrh
bd fct project cmu pt humach fct inesc id multiannual funding piddac program funds kuzman ganchev partially supported
nsf itr eia ben taskar partially supported darpa cssg award
onr young investigator award lusa coheur partially supported fct inesc id
multiannual funding piddac program funds

appendix unsupervised optimization
berg kirkpatrick et al describe feature rich hmm training model
direct gradient rather em lead better however report
en corpus table compares implementation training regimes bk em
bk dg different languages comparing two training regimes see
clear winner bk em wins cases bg en dk loses three
clear predict method suitable follow discussion
authors propose difference arises starts fine tune
weights rare features relative trains weights common features short
suffixes case direct gradient training start optimization weights common
features change rapidly weight gradient proportional feature frequency
training progresses weight transferred rarer features contrast em training
optimization done completion step even first iterations em
counts mostly random rarer features get lot weight mass prevents
model generalizing optimization terminates local maximum closer starting
point allow em use common features longer tried small experiments
initially permissive stopping criteria step em iterations
permissive stopping criteria require stricter stopping criteria tended improve em
principled method setting schedule convergence criteria step
furthermore small experiments explain direct gradient better em
languages worse others
related study salakhutdinov et al compares convergence rate em direct
gradient training identifies conditions em achieves newton behavior
achieves first order convergence conditions amount missing information
case approximated number hidden states potentially difference
lead different local maxima mainly due non local nature line search procedure gradient methods fact looking dg training seems work better
corpora higher number hidden states en es work worse corpora
fewer hidden states bg en
table compare implementation hmm model implementation
berg kirkpatrick et al conditions regularization parameter feature set
convergence criteria initialization observe significant differences communication
code comparison revealed small implementation differences use bias feature
random seed parameters initialized differently
http www cs berkeley edu tberg gradvsem main html



fic ontrolling c omplexity part peech nduction

en
bk em
bk dg
hmm

many
en pt
bg dk es




en





en pt
bg dk
es




en
bk em
bk dg
hmm

vi
en pt
bg dk es




en




v
en pt
bg dk
es




table em vs direct gradient berg kirkpatrick et al implementation compared
implementaion em hmm maximum entropy emission probabilities
rows starting bk berkeley implementation rows starting
implementation

different implementations optimization different number iterations
corpora differences better performance implementation
corpora implementation gets better leave details well better
understanding differences optimization procedure future work since
main focus present

appendix b evaluation metrics
compare performance different one needs evaluate quality induced
clusters several evaluation metrics clustering proposed previous work metrics
use evaluate divided two types reichart rappoport mapping
information theoretic mapping metrics require post processing step map cluster
pos tag evaluate accuracy supervised pos tagging information theoretic
metrics compare induced clusters directly true pos tags
many mapping mapping haghighi klein two widely used mapping
metrics many mapping hidden state mapped tag cooccurs
means several hidden states mapped tag tags might
used mapping greedily assigns hidden state single tag case
number tags hidden states give correspondence major
drawback latter mapping fails express information hidden states
typically unsupervised prefer explain frequent tags several hidden states
combine rare tags example pt corpus tags occur
corpus grouping together subdividing nouns still provides lot information
true tag assignments however would captured mapping metric
tends favor systems produce exponential distribution size induced cluster
independent clusters true quality correlate well information theoretic
metrics graa et al nevertheless many mapping drawbacks since
distinguish clusters frequent tag cluster split almost evenly



fig raa g anchev c oheur p ereira taskar

nouns adjectives cluster number nouns mixture
words different tags gives many accuracy
information theoretic measures use evaluation variation information vi
meila validity measure v rosenberg hirschberg
entropy conditional entropy tags induced clusters vi desirable geometric properties metric convexly additive meila however range vi values
dataset dependent vi lies log n n number pos tags allow
comparison across datasets different n validity measure v entropy
measure lies range satisfy geometric properties
vi reported give high score large number clusters exist even
low quality reichart rappoport information theoretic measures
proposed better handle different numbers clusters instance nvi reichart rappoport
however work testing conditions corpora number clusters exist christodoulopoulos goldwater steedman
present extensive comparison evaluation metrics related work maron lamar
bienenstock present another empirical study metrics conclude vi metric
produce contradict true quality induced clustering giving high
scores simple baseline systems instance assigning label words
point several metric explained previously since
metric comparison focus work compare methods four metrics
described section

references
abeill treebanks building parsed corpora springer
afonso bick e haber r santos floresta sinta c tica treebank portuguese proc lrec pp
baum l petrie soules g weiss n maximization technique occurring
statistical analysis probabilistic functions markov chains annals mathematical
statistics
berg kirkpatrick bouchard ct denero j klein painless unsupervised
learning features proc naacl
bertsekas homer logan patek nonlinear programming athena scientific
brown p f desouza p v mercer r l pietra v j lai j c class n gram
natural language computational linguistics
chen conditional joint grapheme phoneme conversion proc
ecsct
christodoulopoulos c goldwater steedman two decades unsupervised pos
induction far come proc emnlp cambridge
civit mart building cast lb spanish treebank language
computation



fic ontrolling c omplexity part peech nduction

clark combining distributional morphological information part speech induction proc eacl
dempster laird n rubin maximum likelihood incomplete data via
em journal royal statistical society series b methodological
freitag toward unsupervised whole corpus tagging proc coling association
computational linguistics
ganchev k graa j gillenwater j taskar b posterior regularization structured
latent variable journal machine learning
gao j johnson comparison bayesian estimators unsupervised hidden
markov model pos taggers proc emnlp pp honolulu hawaii acl
goldwater griffiths fully bayesian unsupervised part speech
tagging proc acl vol p
graa j ganchev k pereira f taskar b parameter vs posterior sparisty latent
variable proc nips
graa j ganchev k taskar b expectation maximization posterior constraints
proc nips mit press
graa j v posterior regularization framework learning tractable
intractable constraints ph thesis universidade tcnica de lisboa instituto superior
tcnico
haghighi klein prototype driven learning sequence proc htlnaacl acl
headden iii w p mcclosky charniak e evaluating unsupervised part speech
tagging grammar induction proc coling pp
hwa r resnik p weinberg cabezas c kolak bootstrapping parsers via
syntactic projection across parallel texts special issue journal natural language
engineering parallel texts
johnson doesnt em good hmm pos taggers proc emnlp conll
kromann matthias danish dependency treebank underlying linguistic
theory second workshop treebanks linguistic theories tlt pp vxj
sweden
lamar maron bienenstock e latent descriptor clustering unsupervised pos
induction proceedings conference empirical methods natural language
processing pp cambridge association computational linguistics
lamar maron johnson bienenstock e svd clustering unsupervised pos tagging proceedings acl conference short papers pp
uppsala sweden association computational linguistics
lee k haghighi barzilay r simple type level unsupervised pos tagging
proceedings conference empirical methods natural language processing
pp cambridge association computational linguistics



fig raa g anchev c oheur p ereira taskar

marcus marcinkiewicz santorini b building large annotated corpus
english penn treebank computational linguistics
maron lamar bienenstock e evaluation criteria unsupervised pos induction tech rep indiana university
martin liermann j ney h bigram trigram word clustering
speech communication pp
meila comparing clusteringsan information distance j multivar anal

merialdo b tagging english text probabilistic model computational linguistics

moon erk k baldridge j crouching dirichlet hidden markov model unsupervised pos tagging context local tag generation proc emnlp cambridge
neal r hinton g e view em justifies incremental
sparse variants jordan ed learning graphical pp
kluwer
nocedal j wright j numerical optimization springer
ratnaparkhi maximum entropy model part speech tagging proc emnlp
acl
ravi knight k minimized unsupervised part speech tagging
proc acl
reichart r rappoport nvi clustering evaluation measure proc conll
rosenberg hirschberg j v measure conditional entropy external cluster
evaluation measure emnlp conll pp
salakhutdinov r roweis ghahramani z optimization em expectationconjugate gradient proc icml vol
schtze h distributional part speech tagging proc eacl pp
shen l satta g joshi guided learning bidirectional sequence classification
proc acl prague czech republic
simov k osenova p slavcheva kolkovska balabanova e doikoff ivanova k
simov simov e kouylekov building linguistically interpreted corpus
bulgarian bultreebank proc lrec
smith n eisner j contrastive estimation training log linear unlabeled
data proc acl acl
snyder b naseem eisenstein j barzilay r unsupervised multilingual learning
pos tagging proceedings conference empirical methods natural language
processing pp association computational linguistics
toutanova k johnson bayesian lda model semi supervised part ofspeech tagging proc nips



fic ontrolling c omplexity part peech nduction

toutanova k klein manning c singer feature rich part speech tagging
cyclic dependency network proc hlt naacl
zhao q marcus simple unsupervised learner pos disambiguation rules given
minimal lexicon proc emnlp




