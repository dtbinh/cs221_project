Journal Artificial Intelligence Research 41 (2011) 397-406

Submitted 05/11; published 07/11

Research Note
Policy Invariance Reward Transformations
General-Sum Stochastic Games
Xiaosong Lu
Howard M. Schwartz

LUXIAOS @ SCE . CARLETON . CA
SCHWARTZ @ SCE . CARLETON . CA

Department Systems Computer Engineering
Carleton University
1125 Colonel Drive, Ottawa, K1S 5B6 Canada

Sidney N. Givigi Jr.

IDNEY.G IVIGI @ RMC . CA

Department Electrical Computer Engineering
Royal Military College Canada
13 General Crerar Cres, Kingston, K7K 7B4 Canada

Abstract
extend potential-based shaping method Markov decision processes multi-player
general-sum stochastic games. prove Nash equilibria stochastic game remains
unchanged potential-based shaping applied environment. property policy
invariance provides possible way speeding convergence learning play stochastic
game.

1. Introduction
reinforcement learning, one may suffer temporal credit assignment problem (Sutton &
Barto, 1998) reward received sequence actions. delayed reward lead
difficulty distributing credit punishment action long sequence actions
cause algorithm learn slowly. example problem found
episodic tasks soccer game player given credit punishment
goal scored. number states soccer game large, take long time
player learn equilibrium policy.
Reward shaping technique improve learning performance reinforcement learner
introducing shaping rewards environment (Gullapalli & Barto, 1992; Mataric, 1994).
state space large, delayed reward slow learning dramatically.
speed learning, learner may apply shaping rewards environment supplement
delayed reward. way, reinforcement learning algorithm improve learning
performance combining "good" shaping reward function original delayed reward.
applications reward shaping found literature (Gullapalli & Barto, 1992;
Dorigo & Colombetti, 1994; Mataric, 1994; Randlv & Alstrm, 1998). Gullapalli Barto (1992)
demonstrated application shaping key-press task robot trained press keys
keyboard. Dorigo Colombetti (1994) applied shaping policies robot perform
predefined animate-like behavior. Mataric (1994) presented intermediate reinforcement function
group mobile robots learn foraging task. Randlv Alstrm (1998) combined reinforcement learning shaping make agent learn drive bicycle goal. theoretical
c
2011
AI Access Foundation. rights reserved.

fiL U , CHWARTZ , & G IVIGI

analysis reward shaping found literature (Ng, Harada, & Russell, 1999; Wiewiora,
2003; Asmuth, Littman, & Zinkov, 2008). Ng et al. (1999) presented potential-based shaping
reward guarantee policy invariance single agent Markov decision process
(MDP). Ng et al. proved optimal policy keeps unchanged adding potential-based
shaping reward MDP environment. Following Ng et al., Wiewiora (2003) showed effects potential-based shaping achieved particular initialization Q-values agents
using Q-learning. Asmuth et al. (2008) applied potential-based shaping reward model-based
reinforcement learning approach.
articles focus applications reward shaping single agent MDP.
applications reward shaping general-sum games, Babes, Munoz de Cote, Littman (2008)
introduced social shaping reward players learn equilibrium policies iterated
prisoners dilemma game. theoretical proof policy invariance reward
transformation. research, prove Nash equilibria potential-based shaping
reward transformation (Ng et al., 1999) Nash equilibria original game
framework general-sum stochastic games. Note similar work Devlin Kudenko
(2011) published article review. Devlin Kudenko proved
sufficiency based proof technique introduced Asmuth et al. (2008), prove
sufficiency necessity using different proof technique article.

2. Framework Stochastic Games
Stochastic games first introduced Shapley (1953). stochastic game, players choose
joint action move one state another state based joint action choose.
section, framework stochastic games, introduce Markov decision processes, matrix
games stochastic games respectively.
2.1 Markov Decision Processes
Markov decision process tuple (S, A, T, , R) state space, action space,
: [0, 1] transition function, [0, 1] discount factor R : R
reward function. transition function denotes probability distribution next states
given current state action. reward function denotes received reward next state
given current action current state. Markov decision process following Markov
property: players next state reward depend players current state action.
players policy : defined probability distribution players actions given
state. optimal policy maximize players discounted future reward. MDP,
exists deterministic optimal policy player (Bertsekas, 1987).
Starting current state following optimal policy thereafter, get optimal
state-value function expected sum discounted rewards (Sutton & Barto, 1998)
)
(
V (s) = E




j rk+ j+1 |sk = s,

(1)

j=0

k current time step, rk+ j+1 received immediate reward time step k + j + 1,
[0, 1] discount factor, final time step. (1), task
infinite-horizon task task run infinite period. task episodic,
398

fiP OLICY NVARIANCE



R EWARD RANSFORMATIONS

defined terminal time episode terminated time step . call
state episode ends terminal state sT . terminal state, state-value function
always zero V (sT ) = 0 sT S. Given current state action a, following
optimal policy thereafter, define optimal action-value function (Sutton & Barto, 1998)
h



(2)
Q (s, a) = (s, a, ) R(s, a, ) + V (s )


(s, a, ) = Pr {sk+1 = |sk = s, ak = a} probability next state sk+1 =
given current state sk = action ak = time step k, R(s, a, ) = E{rk+1 |sk = s, ak = a,
sk+1 = } expected immediate reward received state given current state action
a. terminal state, action-value function always zero Q(sT , a) = 0 sT S.
2.2 Matrix Games
matrix game tuple (n, A1 , . . . , , R1 , . . . , Rn ) n number players, Ai (i = 1, . . . , n)
action set player Ri : A1 R payoff function player i.
matrix game game involving multiple players single state. player i(i = 1, . . . , n)
selects action action set Ai receives payoff. player payoff function Ri
determined players joint action joint action space A1 . two-player matrix
game, set matrix element containing payoff joint action pair.
payoff function Ri player i(i = 1, 2) becomes matrix. two players game
fully competitive, two-player zero-sum matrix game R1 = R2 .
matrix game, player tries maximize payoff based players strategy.
players strategy matrix game probability distribution players action set. evaluate players strategy, introduce following concept Nash equilibrium. Nash equilibrium
matrix game collection players policies (1 , , n )
Vi (1 , , , , n ) Vi (1 , , , , n ), , = 1, , n

(3)

Vi () expected payoff player given players current strategies
strategy player strategy space . words, Nash equilibrium collection
strategies players player better changing strategy given
players continue playing Nash equilibrium policies (Basar & Olsder, 1999). define
Qi (a1 , . . . , ) received payoff player given players joint action a1 , . . . , , (ai )
(i = 1, . . . , n) probability player choosing action a1 . Nash equilibrium defined
(3) becomes





Qi (a1 , . . . , )1 (a1 ) (ai ) n (an )

a1 ,...,an A1

Qi (a1 , . . . , )1 (a1 ) (ai ) n (an ), , = 1, , n

(4)

a1 ,...,an A1

(ai ) probability player choosing action ai player Nash equilibrium
strategy .
two-player matrix game called zero-sum game two players fully competitive.
way, R1 = R2 . zero-sum game unique Nash equilibrium sense
expected payoff. means that, although player may multiple Nash equilibrium
399

fiL U , CHWARTZ , & G IVIGI

strategies zero-sum game, value expected payoff Vi Nash equilibrium
strategies same. players game fully competitive summation
players payoffs zero, game called general-sum game. general-sum game,
Nash equilibrium longer unique game might multiple Nash equilibria. Unlike
deterministic optimal policy single player MDP, equilibrium strategies multiplayer matrix game may stochastic.
2.3 Stochastic Games
Markov decision process contains single player multiple states matrix game contains
multiple players single state. game one player multiple states,
define stochastic game (or Markov game) combination Markov decision processes
matrix games. stochastic game tuple (n, S, A1 , . . . , , T, , R1 , . . . , Rn ) n
number players, : A1 [0, 1] transition function, Ai (i = 1, . . . , n)
action set player i, [0, 1] discount factor Ri : A1 R
reward function player i. transition function stochastic game probability
distribution next states given current state joint action players. reward
function Ri (s, a1 , . . . , , ) denotes reward received player state taking joint
action (a1 , . . . , ) state s. Similar Markov decision processes, stochastic games
Markov property. is, players next state reward depend current state
players current actions.
solve stochastic game, need find policy : Ai maximize player
discounted future reward discount factor . Similar matrix games, players policy
stochastic game probabilistic. example soccer game introduced Littman (Littman,
1994) agent offensive side must use probabilistic policy pass unknown
defender. literature, solution stochastic game described Nash equilibrium
strategies set associated state-specific matrix games (Bowling, 2003; Littman, 1994).
state-specific matrix games, define action-value function Qi (s, a1 , . . . , ) expected reward player players take joint action a1 , . . . , state follow
Nash equilibrium policies thereafter. value Qi (s, a1 , . . . , ) known states,
find player Nash equilibrium policy solving associated state-specific matrix game
(Bowling, 2003). Therefore, state s, matrix game find Nash
equilibrium strategies matrix game. Nash equilibrium policies game
collection Nash equilibrium strategies state-specific matrix game states.
2.4 Multi-Player General-Sum Stochastic Games
multi-player general-sum stochastic game, want find Nash equilibria game
know reward function transition function game. Nash equilibrium stochastic
game described tuple n policies (1 , . . . , n ) = 1, , n,
Vi (s, 1 , . . . , , . . . , n ) Vi (s, 1 , . . . , , . . . , n )

(5)

set policies available player Vi (s, 1 , . . . , n ) expected sum
discounted rewards player given current state players equilibrium policies.
simplify notation, use Vi (s) represent Vi (s, 1 , , n ) state-value function Nash
equilibrium policies. define action-value function Q (s, a1 , , ) expected
400

fiP OLICY NVARIANCE



R EWARD RANSFORMATIONS

sum discounted rewards player given current state current joint action
players, following Nash equilibrium policies thereafter. get



Vi (s) =

Qi (s, a1 , , )1 (s, a1 ) n (s, ),

(6)

a1 , ,an A1

Qi (s, a1 , . . . , ) =

(s, a1 , . . . , , )






Ri (s, a1 , . . . , , ) + Vi (s ) ,

(7)

(s, ai ) PD(Ai ) probability distribution action ai player Nash equilibrium policy, (s, a1 , . . . , , ) = Pr {sk+1 = |sk = s, a1 , . . . , } probability next state
given current state joint action (a1 , . . . , ), Ri (s, a1 , . . . , , ) expected
immediate reward received state given current state joint action (a1 , . . . , ). Based
(6) (7), Nash equilibrium (5) rewritten



Qi (s, a1 , . . . , )1 (s, a1 ) (s, ai ) n (s, )

a1 ,...,an A1



Qi (s, a1 , . . . , )1 (s, a1 ) (s, ai ) n (s, ).

(8)

a1 ,...,an A1

3. Potential-Based Shaping General-Sum Stochastic Games
Ng et al. (1999) presented reward shaping method deal credit assignment problem
adding potential-based shaping reward environment. combination shaping
reward original reward may improve learning performance reinforcement learning
algorithm speed convergence optimal policy. theoretical studies potentialbased shaping methods appear published literature consider case single agent
MDP (Ng et al., 1999; Wiewiora, 2003; Asmuth et al., 2008). research, extend
potential-based shaping method Markov decision processes multi-player stochastic games.
prove Nash equilibria potential-based shaping reward transformation
Nash equilibria original game framework general-sum stochastic games.
define potential-based shaping reward (s, ) player
(s, ) = (s ) (s),

(9)

: R real-valued shaping function (sT ) = 0 terminal state sT .
define multi-player stochastic game tuple = (S, A1 , . . . , , T, , R1 , . . . , Rn ) set
states, A1 , . . . , players action sets, transition function, discount factor,
Ri (s, a1 , . . . , , )(i = 1, . . . , n) reward function player i. adding shaping reward
function (s, ) reward function Ri (s, a1 , . . . , , ), define transformed multi-player
stochastic game tuple = (S, A1 , . . . , , T, , R1 , . . . , Rn ) Ri (i = 1, . . . , n) new
reward function given Ri (s, a1 , . . . , , ) = (s, ) + Ri (s, a1 , . . . , , ). Inspired Ng et al.
(1999)s proof policy invariance MDP, prove policy invariance multi-player
general-sum stochastic game follows.
Theorem 1. Given n-player discounted stochastic game = (S, A1 , . . . , , T, , R1 , . . . , Rn ),
define transformed n-player discounted stochastic game = (S, A1 , . . . , , T, , R1 + F1 , . . . , Rn +
Fn ) shaping reward function player i. call potential-based shaping
function form (9). Then, potential-based shaping function necessary
sufficient condition guarantee Nash equilibrium policy invariance
401

fiL U , CHWARTZ , & G IVIGI

(Sufficiency) (i = 1, . . . , n) potential-based shaping function, every Nash equilibrium policy Nash equilibrium policy (and vice versa).
(Necessity) (i = 1, . . . , n) potential-based shaping function, may exist
transition function reward function R Nash equilibrium policy
Nash equilibrium policy M.
Proof. (Proof Sufficiency)
Based (8), Nash equilibrium stochastic game represented set policies
= 1, . . . , n, Mi






QMi (s, a1 , . . . , )M
(s, a1 )
(s, ai )
(s, )
1

n

a1 ,...,an A1





(s, a1 ) Mi (s, ai )
(s, ).
QMi (s, a1 , . . . , )M
1
n

(10)

a1 ,...,an A1

subtract (s) sides (10) get



a1 ,...,an A1




QMi (s, a1 , . . . , )M
(s, a1 )
(s, ai )
(s, ) (s)
1

n





QMi (s, a1 , . . . , )M
(s, a1 ) Mi (s, ai )
(s, ) (s).
1
n

(11)

a1 ,...,an A1
(s, ) (s, ) (s, ) = 1, get
Since a1 ,...,an A1
1

n
Mi
Mn
1






(s, a1 )
(s, ai )
(s, )
[QMi (s, a1 , . . . , ) (s)]M
1

n

a1 ,...,an A1





[QMi (s, a1 , . . . , ) (s)]M
(s, a1 ) Mi (s, ai )
(s, ).
1
n

(12)

a1 ,...,an A1

define
QMi (s, a1 , . . . , ) = QMi (s, a1 , . . . , ) (s).

(13)

get



a1 ,...,an A1






QMi (s, a1 , . . . , )M
(s, a1 )
(s, ai )
(s, )
1

n

a1 ,...,an A1



(s, a1 ) Mi (s, ai )
(s, ).
QMi (s, a1 , . . . , )M
1
n

(14)

use algebraic manipulations rewrite action-value function Nash equilibrium (7) player stochastic game

QMi (s, a1 , . . . , ) (s) = (s, a1 , . . . , , ) RMi (s, a1 , . . . , , ) + VM (s )



+ (s ) (s ) (s).

(15)

Since (s, a1 , . . . , , ) = 1, equation becomes
QMi (s, a1 , . . . , ) (s) =

(s, a1 , . . . , , )





RMi (s, a1 , . . . , , )


+ (s ) (s) + VM (s ) (s ) .
402

(16)

fiP OLICY NVARIANCE



R EWARD RANSFORMATIONS

According (6), rewrite equation
QMi (s, a1 , . . . , ) (s) =



+

a1 ,...,an A1

=
+



a1 ,...,an A1

(s, a1 , . . . , , )





RMi (s, a1 , . . . , , ) + (s ) (s)







(s
,

)



(s
,

)

QMi (s , a1 , . . . , )M



(s
)

1

n
1


(s, a1 , . . . , , )



RMi (s, a1 , . . . , , ) + (s ) (s)






(s , a1 )
(s , ) .
QMi (s , a1 , . . . , ) (s )
1


(17)

Based definitions (s, ) (9) QMi (s, a1 , . . . , ) (13), equation becomes
QMi (s, a1 , . . . , ) =
+



a1 ,...,an A1

(s, a1 , . . . , , )





RMi (s, a1 , . . . , , ) + Fi(s, )




QMi (s , a1 , . . . , )
(s , a1 )
(s , ) .
1


(18)

Since equations (14) (18) form equations (6)-(8), conclude
QMi (s, a1 , . . . , ) action-value function Nash equilibrium player stochastic game . Therefore, obtain
QMi (s, a1 , . . . , ) = QM (s, a1 , . . . , ) = QMi (s, a1 , . . . , ) (s).


(19)

state terminal state sT , QMi (sT , a1 , . . . , ) = QMi (sT , a1 , . . . , )
(sT ) = 0 0 = 0. Based (14) QMi (s, a1 , . . . , ) = QM (s, a1 , . . . , ), find

Nash equilibrium Nash equilibrium . state-value function
Nash equilibrium stochastic game given
VM (s) = VM (s) (s).


(20)

(Proof Necessity)
(i = 1, . . . , n) potential-based shaping function, (s, ) 6= (s ) (s).
Similar Ng et al. (1999)s proof necessity, define = (s, ) [ (s ) (s)].
build stochastic game giving following transition function player 1s reward
function RM1 ()
(s1 , a11 , a2 , . . . , , s3 ) = 1,
(s1 , a21 , a2 , . . . , , s2 ) = 1,
(s2 , a1 , . . . , , s3 ) = 1,
(s3 , a1 , . . . , , s3 ) = 1,

RM1 (s1 , a1 , . . . , , s3 ) = ,
2
RM1 (s1 , a1 , . . . , , s2 ) = 0,
RM1 (s2 , a1 , . . . , , s3 ) = 0,
RM1 (s3 , a1 , . . . , , s3 ) = 0,
403

(21)

fiL U , CHWARTZ , & G IVIGI

a11
S3

S1

a12
S2
Figure 1: possible states stochastic model proof necessity

ai (i = 1, . . . , n) represents possible action ai Ai player i, a11 a21 represent
player 1s action 1 action 2 respectively. Equation (s1 , a11 , a2 , . . . , , s3 ) = 1 (21) denotes
that, given current state s1 , player 1s action a11 lead next state s3 matter
joint action players take. Based transition function reward function,
get game model including states (s1 , s2 , s3 ) shown Figure 1. define 1 (si ) =
F1 (si , s3 )(i = 1, 2, 3). Based (6), (7), (19), (20) (21), obtain player 1s action-value
function state s1

,
2
QM1 (s1 , a21 , . . . ) = 0,

QM1 (s1 , a11 , . . . ) =


QM (s1 , a11 , . . . ) = F1 (s1 , s2 ) + F1 (s2 , s3 ) ,
1
2
QM (s1 , a21 , . . . ) = F1 (s1 , s2 ) + F1 (s2 , s3 ).
1

Nash equilibrium policy player 1 state s1


(s1 , a1 ) =

1

1
a1 > 0,


a21


,
(s1 , a1 ) =
1

otherwise

2
a1 > 0,


a11

.

(22)

otherwise

Therefore, case, Nash equilibrium policy player 1 state s1
Nash equilibrium policy .

analysis shows potential-based shaping reward form (s, ) =
(s ) (s) guarantees Nash equilibrium policy invariance. question becomes
select shaping function (s) improve learning performance learner. Ng
et al. (1999) showed (s) = VM (s) good candidate improving players learning
404

fiP OLICY NVARIANCE



R EWARD RANSFORMATIONS

performance MDP. substitute (s) = VM (s) (18) get
QMi (s, a1 , . . . , ) = QM (s, a1 , . . . , )


=
+

(s, a1 , . . . , , )







RMi (s, a1 , . . . , , ) + (s, )



RMi (s, a1 , . . . , , ) + (s, )






QM (s , a1 , . . . , )

(s
,

)



(s
,

)
1

n
1



a1 ,...,an A1

=

(s, a1 , . . . , , )




+ (VM (s ) (s ))


= (s, a1 , . . . , , ) RMi (s, a1 , . . . , , ) + (s, ) .

(23)



Equation (23) shows action-value function QM (s, a1 , . . . , ) state easily obtained

checking immediate reward RMi (s, a1 , . . . , , ) + (s, ) player received state .
However, practical applications, information environment
(s, a1 , . . . , , ) Ri (s, a1 , . . . , , ). means cannot find shaping function (s)
(s) = VM (s) without knowing model environment. Therefore, goal
designing shaping function find (s) good approximation VM (s).

4. Conclusion
potential-based shaping method used deal temporal credit assignment problem
speed learning process MDPs. article, extend potential-based shaping
method general-sum stochastic games. prove proposed potential-based shaping reward applied general-sum stochastic game change original Nash equilibrium
game. analysis result article potential improve learning performance
players stochastic game.

References
Asmuth, J., Littman, M. L., & Zinkov, R. (2008). Potential-based shaping model-based reinforcement learning. Proceedings 23rd AAAI Conference Artificial Intelligence,
pp. 604609.
Babes, M., Munoz de Cote, E., & Littman, M. L. (2008). Social reward shaping prisoners
dilemma. Proceedings 7th International Joint Conference Autonomous Agents
Multiagent Systems (AAMAS 2008), pp. 13891392.
Basar, T., & Olsder, G. J. (1999). Dynamic Noncooperative Game Theory. SIAM Series Classics
Applied Mathematics 2nd, London, U.K.
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic Stochastic Models. PrenticeHall, Englewood Cliffs, NJ.
Bowling, M. (2003). Multiagent Learning Presence Agents Limitations. Ph.D. thesis,
School Computer Science, Carnegie Mellon University, Pittsburgh, PA.
405

fiL U , CHWARTZ , & G IVIGI

Devlin, S., & Kudenko, D. (2011). Theoretical considerations potential-based reward shaping
multi-agent systems.. Proceedings 10th International Conference Autonomous
Agents Multiagent Systems (AAMAS), Taipei, Taiwan.
Dorigo, M., & Colombetti, M. (1994). Robot shaping: developing autonomous agents
learning. Artificial Intelligence, 71, 321370.
Gullapalli, V., & Barto, A. (1992). Shaping method accelerating reinforcement learning.
Proceedings 1992 IEEE International Symposium Intelligent Control, pp. 554 559.
Littman, M. L. (1994). Markov games framework multi-agent reinforcement learning.
Proceedings 11th International Conference Machine Learning, pp. 157163.
Mataric, M. J. (1994). Reward functions accelerated learning. Proceedings 11th
International Conference Machine Learning.
Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance reward transformations: theory
application reward shaping. Proceedings 16th International Conference
Machine Learning, pp. 278287.
Randlv, J., & Alstrm, P. (1998). Learning drive bicycle using reinforcement learning
shaping. Proceedings 15th International Conference Machine Learning.
Shapley, L. S. (1953). Stochastic games. Proceedings National Academy Sciences,
Vol. 39, pp. 10951100.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, Massachusetts.
Wiewiora, E. (2003). Potential-based shaping Q-value initialization equivalent. Journal
Artificial Intelligence Research, 19, 205208.

406


