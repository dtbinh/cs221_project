journal artificial intelligence

submitted published

note
policy invariance reward transformations
general sum stochastic games
xiaosong lu
howard schwartz

luxiaos sce carleton ca
schwartz sce carleton ca

department systems computer engineering
carleton university
colonel drive ottawa k b canada

sidney n givigi jr

idney g ivigi rmc ca

department electrical computer engineering
royal military college canada
general crerar cres kingston k k b canada

abstract
extend potential shaping method markov decision processes multi player
general sum stochastic games prove nash equilibria stochastic game remains
unchanged potential shaping applied environment property policy
invariance provides possible way speeding convergence learning play stochastic
game

introduction
reinforcement learning one may suffer temporal credit assignment sutton
barto reward received sequence actions delayed reward lead
difficulty distributing credit punishment action long sequence actions
cause learn slowly example found
episodic tasks soccer game player given credit punishment
goal scored number states soccer game large take long time
player learn equilibrium policy
reward shaping technique improve learning performance reinforcement learner
introducing shaping rewards environment gullapalli barto mataric
state space large delayed reward slow learning dramatically
speed learning learner may apply shaping rewards environment supplement
delayed reward way reinforcement learning improve learning
performance combining good shaping reward function original delayed reward
applications reward shaping found literature gullapalli barto
dorigo colombetti mataric randlv alstrm gullapalli barto
demonstrated application shaping key press task robot trained press keys
keyboard dorigo colombetti applied shaping policies robot perform
predefined animate behavior mataric presented intermediate reinforcement function
group mobile robots learn foraging task randlv alstrm combined reinforcement learning shaping make agent learn drive bicycle goal theoretical
c

ai access foundation rights reserved

fil u chwartz g ivigi

analysis reward shaping found literature ng harada russell wiewiora
asmuth littman zinkov ng et al presented potential shaping
reward guarantee policy invariance single agent markov decision process
mdp ng et al proved optimal policy keeps unchanged adding potential
shaping reward mdp environment following ng et al wiewiora showed effects potential shaping achieved particular initialization q values agents
q learning asmuth et al applied potential shaping reward model
reinforcement learning
articles focus applications reward shaping single agent mdp
applications reward shaping general sum games babes munoz de cote littman
introduced social shaping reward players learn equilibrium policies iterated
prisoners dilemma game theoretical proof policy invariance reward
transformation prove nash equilibria potential shaping
reward transformation ng et al nash equilibria original game
framework general sum stochastic games note similar work devlin kudenko
published article review devlin kudenko proved
sufficiency proof technique introduced asmuth et al prove
sufficiency necessity different proof technique article

framework stochastic games
stochastic games first introduced shapley stochastic game players choose
joint action move one state another state joint action choose
section framework stochastic games introduce markov decision processes matrix
games stochastic games respectively
markov decision processes
markov decision process tuple r state space action space
transition function discount factor r r
reward function transition function denotes probability distribution next states
given current state action reward function denotes received reward next state
given current action current state markov decision process following markov
property players next state reward depend players current state action
players policy defined probability distribution players actions given
state optimal policy maximize players discounted future reward mdp
exists deterministic optimal policy player bertsekas
starting current state following optimal policy thereafter get optimal
state value function expected sum discounted rewards sutton barto


v e




j rk j sk



j

k current time step rk j received immediate reward time step k j
discount factor final time step task
infinite horizon task task run infinite period task episodic


fip olicy nvariance



r eward ransformations

defined terminal time episode terminated time step call
state episode ends terminal state st terminal state state value function
zero v st st given current state action following
optimal policy thereafter define optimal action value function sutton barto
h




q r v


pr sk sk ak probability next state sk
given current state sk action ak time step k r e rk sk ak
sk expected immediate reward received state given current state action
terminal state action value function zero q st st
matrix games
matrix game tuple n r rn n number players ai n
action set player ri r payoff function player
matrix game game involving multiple players single state player n
selects action action set ai receives payoff player payoff function ri
determined players joint action joint action space two player matrix
game set matrix element containing payoff joint action pair
payoff function ri player becomes matrix two players game
fully competitive two player zero sum matrix game r r
matrix game player tries maximize payoff players strategy
players strategy matrix game probability distribution players action set evaluate players strategy introduce following concept nash equilibrium nash equilibrium
matrix game collection players policies n
vi n vi n n



vi expected payoff player given players current strategies
strategy player strategy space words nash equilibrium collection
strategies players player better changing strategy given
players continue playing nash equilibrium policies basar olsder define
qi received payoff player given players joint action ai
n probability player choosing action nash equilibrium defined
becomes





qi ai n



qi ai n n





ai probability player choosing action ai player nash equilibrium
strategy
two player matrix game called zero sum game two players fully competitive
way r r zero sum game unique nash equilibrium sense
expected payoff means although player may multiple nash equilibrium


fil u chwartz g ivigi

strategies zero sum game value expected payoff vi nash equilibrium
strategies players game fully competitive summation
players payoffs zero game called general sum game general sum game
nash equilibrium longer unique game might multiple nash equilibria unlike
deterministic optimal policy single player mdp equilibrium strategies multiplayer matrix game may stochastic
stochastic games
markov decision process contains single player multiple states matrix game contains
multiple players single state game one player multiple states
define stochastic game markov game combination markov decision processes
matrix games stochastic game tuple n r rn n
number players transition function ai n
action set player discount factor ri r
reward function player transition function stochastic game probability
distribution next states given current state joint action players reward
function ri denotes reward received player state taking joint
action state similar markov decision processes stochastic games
markov property players next state reward depend current state
players current actions
solve stochastic game need policy ai maximize player
discounted future reward discount factor similar matrix games players policy
stochastic game probabilistic example soccer game introduced littman littman
agent offensive side must use probabilistic policy pass unknown
defender literature solution stochastic game described nash equilibrium
strategies set associated state specific matrix games bowling littman
state specific matrix games define action value function qi expected reward player players take joint action state follow
nash equilibrium policies thereafter value qi known states
player nash equilibrium policy solving associated state specific matrix game
bowling therefore state matrix game nash
equilibrium strategies matrix game nash equilibrium policies game
collection nash equilibrium strategies state specific matrix game states
multi player general sum stochastic games
multi player general sum stochastic game want nash equilibria game
know reward function transition function game nash equilibrium stochastic
game described tuple n policies n n
vi n vi n



set policies available player vi n expected sum
discounted rewards player given current state players equilibrium policies
simplify notation use vi represent vi n state value function nash
equilibrium policies define action value function q expected


fip olicy nvariance



r eward ransformations

sum discounted rewards player given current state current joint action
players following nash equilibrium policies thereafter get



vi

qi n





qi








ri vi



ai pd ai probability distribution action ai player nash equilibrium policy pr sk sk probability next state
given current state joint action ri expected
immediate reward received state given current state joint action
nash equilibrium rewritten



qi ai n





qi ai n





potential shaping general sum stochastic games
ng et al presented reward shaping method deal credit assignment
adding potential shaping reward environment combination shaping
reward original reward may improve learning performance reinforcement learning
speed convergence optimal policy theoretical studies potentialbased shaping methods appear published literature consider case single agent
mdp ng et al wiewiora asmuth et al extend
potential shaping method markov decision processes multi player stochastic games
prove nash equilibria potential shaping reward transformation
nash equilibria original game framework general sum stochastic games
define potential shaping reward player




r real valued shaping function st terminal state st
define multi player stochastic game tuple r rn set
states players action sets transition function discount factor
ri n reward function player adding shaping reward
function reward function ri define transformed multi player
stochastic game tuple r rn ri n
reward function given ri ri inspired ng et al
proof policy invariance mdp prove policy invariance multi player
general sum stochastic game follows
theorem given n player discounted stochastic game r rn
define transformed n player discounted stochastic game r f rn
fn shaping reward function player call potential shaping
function form potential shaping function necessary
sufficient condition guarantee nash equilibrium policy invariance


fil u chwartz g ivigi

sufficiency n potential shaping function every nash equilibrium policy nash equilibrium policy vice versa
necessity n potential shaping function may exist
transition function reward function r nash equilibrium policy
nash equilibrium policy
proof proof sufficiency
nash equilibrium stochastic game represented set policies
n mi






qmi

ai



n







mi ai

qmi

n





subtract sides get








qmi

ai



n





qmi
mi ai


n




get
since


n
mi
mn








ai

qmi


n







qmi
mi ai


n





define
qmi qmi



get










qmi

ai



n





mi ai

qmi

n



use algebraic manipulations rewrite action value function nash equilibrium player stochastic game

qmi rmi vm







since equation becomes
qmi







rmi


vm




fip olicy nvariance



r eward ransformations

according rewrite equation
qmi




















rmi



















qmi








n







rmi








qmi





definitions qmi equation becomes
qmi












rmi




qmi







since equations form equations conclude
qmi action value function nash equilibrium player stochastic game therefore obtain
qmi qm qmi




state terminal state st qmi st qmi st
st qmi qm

nash equilibrium nash equilibrium state value function
nash equilibrium stochastic game given
vm vm




proof necessity
n potential shaping function
similar ng et al proof necessity define
build stochastic game giving following transition function player reward
function rm





rm

rm
rm
rm




fil u chwartz g ivigi








figure possible states stochastic model proof necessity

ai n represents possible action ai ai player represent
player action action respectively equation denotes
given current state player action lead next state matter
joint action players take transition function reward function
get game model including states shown figure define si
f si obtain player action value
function state



qm

qm


qm f f


qm f f


nash equilibrium policy player state

















otherwise











otherwise

therefore case nash equilibrium policy player state
nash equilibrium policy

analysis shows potential shaping reward form
guarantees nash equilibrium policy invariance question becomes
select shaping function improve learning performance learner ng
et al showed vm good candidate improving players learning


fip olicy nvariance



r eward ransformations

performance mdp substitute vm get
qmi qm













rmi



rmi






qm














n













vm


rmi





equation shows action value function qm state easily obtained

checking immediate reward rmi player received state
however practical applications information environment
ri means cannot shaping function
vm without knowing model environment therefore goal
designing shaping function good approximation vm

conclusion
potential shaping method used deal temporal credit assignment
speed learning process mdps article extend potential shaping
method general sum stochastic games prove proposed potential shaping reward applied general sum stochastic game change original nash equilibrium
game analysis article potential improve learning performance
players stochastic game

references
asmuth j littman l zinkov r potential shaping model reinforcement learning proceedings rd aaai conference artificial intelligence
pp
babes munoz de cote e littman l social reward shaping prisoners
dilemma proceedings th international joint conference autonomous agents
multiagent systems aamas pp
basar olsder g j dynamic noncooperative game theory siam series classics
applied mathematics nd london u k
bertsekas p dynamic programming deterministic stochastic prenticehall englewood cliffs nj
bowling multiagent learning presence agents limitations ph thesis
school computer science carnegie mellon university pittsburgh pa


fil u chwartz g ivigi

devlin kudenko theoretical considerations potential reward shaping
multi agent systems proceedings th international conference autonomous
agents multiagent systems aamas taipei taiwan
dorigo colombetti robot shaping developing autonomous agents
learning artificial intelligence
gullapalli v barto shaping method accelerating reinforcement learning
proceedings ieee international symposium intelligent control pp
littman l markov games framework multi agent reinforcement learning
proceedings th international conference machine learning pp
mataric j reward functions accelerated learning proceedings th
international conference machine learning
ng harada russell policy invariance reward transformations theory
application reward shaping proceedings th international conference
machine learning pp
randlv j alstrm p learning drive bicycle reinforcement learning
shaping proceedings th international conference machine learning
shapley l stochastic games proceedings national academy sciences
vol pp
sutton r barto g reinforcement learning introduction mit press
cambridge massachusetts
wiewiora e potential shaping q value initialization equivalent journal
artificial intelligence




