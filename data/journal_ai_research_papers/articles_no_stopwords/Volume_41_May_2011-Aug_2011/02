journal artificial intelligence

submitted published

value information lattice exploiting probabilistic
independence effective feature subset acquisition
mustafa bilgic

mbilgic iit edu

illinois institute technology
chicago il usa

lise getoor

getoor cs umd edu

university maryland
college park md usa

abstract
address cost sensitive feature acquisition misclassifying instance costly expected misclassification cost reduced acquiring
values missing features acquiring features costly well objective acquire right set features sum feature acquisition
cost misclassification cost minimized describe value information lattice
voila optimal efficient feature subset acquisition framework unlike common
practice acquire features greedily voila reason subsets features
voila efficiently searches space possible feature subsets discovering exploiting
conditional independence properties features reuses probabilistic inference computations speed process empirical evaluation five
medical datasets greedy strategy often reluctant acquire features
cannot forecast benefit acquiring multiple features combination

introduction
often need make decisions take appropriate actions complex uncertain
world important subset decisions formulated classification
instance described set features one finite categorical options
chosen features examples include medical diagnosis patients
described lab tests diagnosis made disease state patient
spam detection email described content email client needs
decide whether email spam
much done learn effective efficient classifiers assuming
features describing entities fully given even though complete data
assumption might hold domains practice features describe entities
often missing values certain domains medical diagnosis decision
made number features include laboratory test missing feature
values acquired cost performing related tests cases need
decide tests perform order answer question course
depends important get correct classification decision put alternatively
cost incorrect classification e g misdiagnosis determines much
willing spend expensive tests thus need devise feature acquisition policy
determine tests perform order stop make
c

ai access foundation rights reserved

fibilgic getoor

final classification decision total incurred cost feature acquisition cost
expected misclassification cost minimized
devising optimal policy general requires considering possible permutations
features expected values provide intuition features might
useful acquired together cost benefit acquiring features
depend features acquired values turned
devising optimal policy intractable general previous work
greedy gaag wessels yang ling chai pan approximated value
information calculations heckerman horvitz middleton developed
heuristic feature scoring techniques nunez turney
greedy however least two major limitations first
considers feature isolation cannot accurately forecast value acquiring
multiple features together causing produce sub optimal policies second greedy
strategy assumes features acquired sequentially value feature
observed acquiring next one assumption however often
practical example doctors typically order batches measurements simultaneously
blood count cholesterol level etc possibly order another batch
arrive two limitations greedy make necessary reason
sets features
reasoning sets features hand poses serious tractability challenges
first number subsets exponential size feature set second
judging value acquiring set features requires taking expectation
possible values features set exponential number
features good news however need consider possible subsets
features practice certain features render features useless features
useful acquired together example x ray might render skin test
useless diagnosing tuberculosis similarly chest pain alone might useful
differentiating cold heart disease becomes useful combined
features blood test
article describe data structure discovers exploits types
constraints features render features useless features useful
acquired together underlying probability distribution propose value
information lattice voila reduces space possible subsets exploiting
constraints features additionally voila makes possible share value
information calculations different feature sets reduce computation time
article builds upon earlier work bilgic getoor contributions
article include
introduce two additional techniques sharing computations different
subsets features techniques information caching
utilizing paths underlying bayesian network
experiment asymmetric misclassification costs addition symmetric
costs asymmetric setup reflects realistic case provides insights
addition feature acquisition costs defined turney generate
experiment synthetic feature costs synthetic feature costs capture


fivalue information lattice

complex feature acquisition costs allows leeway acquisition
strategies differ
remainder article organized follows describe notation
formulation section describe reduce search space
share computations voila section experimental section
discuss related work section discuss future work section conclude
section

notation formulation
main task classify given instance missing feature values incur
minimum acquisition misclassification cost let instance described set
features x x x xn let random variable representing class
assume joint probability distribution p x given concern
feature acquisition inference note conditional distribution p x
appropriate features assumed unobserved initially purpose
article assume given bayesian network joint probabilistic
model allows us efficiently answer conditional independence queries used
notation bold face letter represents set random variables non bold face
letter represents single random variable example x represents set features
whereas xi x represents feature x represents class variable additionally
capital letter represents random variable lowercase letter represents particular
value variable applies individual variables sets variables
example represents variable represents particular value take
addition probabilistic model given cost specify
feature acquisition costs misclassification costs formally assume
feature acquisition cost function given subset features set features
whose values known evidence e returns non negative real number c e
assume misclassification cost model returns misclassification
cost cij incurred assigned yi correct assignment yj cost
functions model non static feature acquisition costs cost acquiring
feature xi depend acquired far values
e well acquired conjunction feature xi moreover
misclassification cost model assume symmetric costs different kids errors false
positives negatives different costs
figure shows simple example configuration two features x x
class variable simple example joint distribution p x represented
table feature costs simple independent costs x x misclassification cost symmetric types misclassifications cost correct
classification cost anything
diagnostic policy decision tree node represents feature
branches nodes represent possible values features path
policy ps represents ordered sequence feature values often use ps
represent ordered version typically order features set
important computing feature costs cost feature depend values


fibilgic getoor

figure example configuration two features x x class variable
table left right represent joint probability distribution p x x
feature costs misclassification costs

previously acquired features order features irrelevant computing
probability p example conditional policy example configuration
figure given figure
policy two types costs feature acquisition cost misclassification
cost costs defined terms costs associated following paths
policy first describe compute feature acquisition cost path
describe compute associated expected misclassification cost finally
compute expected total cost policy total costs associated
path
naive version feature cost path ps sum costs
features appear path however practice cost feature depend
features acquired far observed values acquired features
example performing treadmill test asking patient run treadmill
measure heart beat etc riskier ordered cholesterol test
turned high putting patient high risk heart disease account
types costs order features ps matters total feature cost
path summation individual feature costs conditioned values
features precede features consideration
f c ps

n
x

c ps j ps j

j

ps j represents j th feature ps ps j represents feature values
j ps
reach end path need make classification decision
case simply utilize bayesian decision theory choose decision minimum
risk e misclassification cost decision probabilistic model


fivalue information lattice

figure example conditional policy features x x class variable
non leaf node represents feature acquisition probability distribution
possible values cost feature path e g x x
acquisition cost expected misclassification cost policy overall
expected total cost etc sum total costs path weighted
probability following path

compute probability distribution p ps choose value leads
minimum expected cost note order features values matter
case p ps p expected misclassification path em c ps


fibilgic getoor

computed follows
em c ps em c min
yi

x

p yj cij



yj

total cost incur following path policy simply sum feature
expected misclassification costs path
c ps f c ps em c ps
finally compute expected total cost policy total costs
individual paths ps path ps probability occurrence real world
probability easily computed generative probability model assumed simply p expected total cost policy sum total
cost path c ps weighted probability following path p
et c

x

p c ps



ps

objective feature acquisition inference given joint probabilistic
model cost acquisition misclassification policy
minimum expected total cost however building optimal decision tree known
np complete hyafil rivest thus greedy choosing
best feature reduces misclassification costs lowest cost e g
gaag wessels dittmer jensen developed heuristic feature scoring
techniques e g nunez tan
greedy strategy path policy extended feature reduces
misclassification cost lowest cost specifically path
ps replaced paths psx psx psxni x x xni values


xi take xi feature highest benefit define benefit
feature xi given path ps reduction total cost path path
expanded possible values xi formally
benef xi ps c ps

n
x

p xji c psxj


j

f c ps em c

n
x



p xji f c psxj em c xji


j


f c ps

n
x


p xji f c psxj em c


j

n
x

p xji em c xji

j

c xi em c

p xji em c xji

j

f c ps f c ps c xi em c
n
x

n
x

p xji em c xji

j



fivalue information lattice

note last two terms equivalent definition expected value information evi howard
ev xi em c

n
x

p xji em c xji



j

substituting evi definition benefit becomes intuitive
benef xi ps benef xi ev xi c xi



definition greedy strategy iteratively finds feature highest
positive benefit value cost difference acquires stops acquisition
features positive benefit value
note straightforward define evi benefit set features
single feature difference expectation needs
taken joint assignments features set
ev em c

x

p em c






benef ev c



greedy strategy mentioned
p earlier first
short sighted exist sets x benef
benef xi
xi

easier see example xor function x xor x x x
alone useful determinative together due relationship greedy
policy guaranteed optimal moreover greedy policy prematurely stop
acquisition single feature seems provide positive benefit
second greedy strategy often need acquire set
features simultaneously example doctor orders set lab tests sends
patient lab blood count cholesterol level etc rather ordering single test
waiting ordering next one however traditional greedy strategy
cannot handle reasoning sets features naturally
would able reason sets features two reasons
objective article given existing potentially empty set already observed
features e observed values e set highest benefit
l x e argmax benef e



sx e

two formulation first number subsets x e
exponential size x e second set need take expectation
joint assignments features set address two
data structure describe next


fibilgic getoor

value information lattice voila
voila makes reasoning sets features tractable reducing space possible
sets allowing sharing evi computations different sets section
first explain reduce space explain techniques computation
sharing
reducing space possible sets
domains often complex interactions features
class label contrary naive bayes assumption features often conditionally
independent given class label features useless features
already acquired example chest x ray typically determinative skin
test tuberculosis similarly features useless alone unless accompanied
features example chest pain alone might due variety sicknesses
accompanied high cholesterol could indicate heart disease whereas
combined fever cold might probable types interactions
features allow us reduce space candidate feature sets
mentioned formulation assumed already
joint probabilistic model features class variable p x
two types feature interactions asking probabilistic independence queries
p x specifically assume given bayesian network represents
p x bayesian network allow us types interactions
standard separation
definition set x e irreducible respect evidence e xi xi
conditionally independent given e xi
given bayesian network x straightforward check irreducibility
separation pearl
proposition let maximal irreducible subset respect e ev
e ev e
proof let maximal irreducible set e separates
otherwise could make larger including non separated element
thus p e p e p e substitution equations
yields desired property
note assumption c e c e suffices
consider irreducible sets optimal solution objective function
equation voila data structure contains irreducible feature subsets
x respect particular set evidence e next define voila formally
definition voila v directed acyclic graph node corresponding
possible irreducible set features directed edge feature set
node corresponds direct maximal subset subset relationships
lattice defined directed paths v


fivalue information lattice



b

figure simple bayesian network illustrating dependencies attributes
class variable b voila corresponding network

figure shows simple bayesian network corresponding voila respect
empty evidence set shown figure b notice voila contains
irreducible subsets given bayesian network instance voila contain
sets include x x x separates x observe
number irreducible subsets contrast possible subsets moreover
note largest subset size contrast smaller feature sets sizes
dramatic effect value information calculations fact savings
make solving objective function optimally equation feasible practice
sharing evi calculations
finding set highest benefit equation requires computing ev
equation however computing ev requires taking expectation possible
values features moreover searching best set among irreducible sets
requires us compute evi irreducible sets make computations tractable
practice voila allows computation sharing nodes article describe
three possible ways sharing computations nodes voila


fibilgic getoor

subset relationships
voila exploits subset relationships different feature sets order avoid
computing evi nodes first directed path node
voila thus ev e ev e assume
directed path si sj ev si e ev sj e nodes
path evi thus need computation
subsets makes use observation given
efficient evi computation voila
input voila v current evidence e
output voila updated correct evi values
root node

value ev e ub value lb value

ub descendants value








leaf node
value ev e ub value lb value
lb ancestors value
node lb ub
value ev e ub value lb value
lb ancestors value
ub descendants value

important point nodes voila irreducible sets unless
totally useless features change p observed
two distinct nodes evi values exactly equal however statement
true context specific independencies independencies hold
certain assignments variables underlying bayesian network
description implementation used standard separation variable level one
imagine going one step define irreducible sets variable
level separation context specific independencies
order share computations different nodes lattice keep lower
upper bounds evi node lower bound determined values
descendants node whereas upper bound determined values
ancestors first initialize bounds computing value information
boundary lattice e root node leaf node lines
loop nodes whose upper bounds lower bounds equal line
computing values updating bounds ancestors descendants
terminates upper bounds lower bounds nodes become
tight order choose nodes line number sets
value calculated minimum still open question possible heuristic perform
superset higher equivalent evi equation subset
need compute evi root nodes suffices compute node corresponds
markov blanket explained detail next section



fivalue information lattice

binary search choose middle node path two nodes values
already calculated
information pathways underlying bayesian network
second mechanism voila uses share evi computations edges
underlying bayesian network specifically make use following fact
proposition separates respect e
ev e ev e
proof consider subset relationship know ev
e ev e ev e ev e
ev e em c e

x

p e em c e



em c e

xx


em c e

xx


em c e

x

p e em c e



p e em c e



p e em c e



ev e
ev e
third line follows second fact separates thus
p p
corollary markov blanket e parents children childrens
parents set highest evi search space separates
remaining variables corollary need compute evi
root nodes compute evi root node corresponds
markov blanket serves upper bound evi remaining
root nodes
relationships well exploited exploited subset relationships
instead subset relationships use subset independence relationships one simple way make use without modification
add edges independence property holds example according toy network figure would x
x thus add directed edge x x voila figure b
work fine
incremental inference
third last mechanism voila uses computation sharing
caching probabilities nodes candidate set v need compute
ev e requires computing p e em c e cache


fibilgic getoor

conditional probabilities node v compute
p e one
p
supersets si xi compute p e xi p xi xi e
computing em c e requires computing p e perform computation efficiently cache state junction tree node voila
subset sj sj xj compute p e integrating
extra evidence junction tree node sj used compute p sj e
constructing voila
efficient construction voila straightforward task brute force
would enumerate possible subsets x e subset check whether
irreducible however brute force clearly impractical number
nodes voila expected much fewer number possible subsets x e
smart sets consider inclusion v construct
efficiently instead generating possible candidates checking whether
irreducible try generate irreducible sets first introduce
notion dependency constraint explain use dependency constraints
efficiently construct voila
definition dependency constraint feature xi respect e
constraint e ensures dependency xi exists
instance running example dependency constraint x x
words order x relevant x included e similarly
dependency constraint x x meaning x must included se specifically
dependency constraint feature xi requires xj path xi
included e xj part v structure xj part v structure
xj one descendants must included e refer latter
constraints positivity constraints uses ideas compute
dependency constraints feature given
dependency constraint computation xi
input xi
output dependency constraint xi denoted dc xi
dc xi false
undirected path pj xi

dcj xi true

xk path pj

xk cause v structure

dcj xi dcj xi xk

else

dcj xi dcj xi xk descendants xk


dc xi dc xi dcj xi



fivalue information lattice

dependency constraints used check whether set irreducible
potentially irreducible intuitively set potentially irreducible irreducible
possible make set irreducible adding features formally
definition set x e potentially irreducible respect evidence e
irreducible exists non empty set features x e
irreducible
potential irreducibility possible due non monotonic nature separation
feature separated become dependent consider combination
features example running example x irreducible x
separated whereas x x irreducible
use dependency constraints check whether set irreducible potentially
irreducible set irreducible dependency elements
exists dependency constraint set conjunction dependency
constraints members irreducibility checked setting elements
e true setting remaining elements x false evaluating
sets dependency constraint running example dependency constraint set
x x x x assuming e set members x x true
set remaining features x x false x x evaluates false thus
set irreducible makes sense given evidence x independent
x useful feature set consider acquisition x x
checking potential irreducibility similar set elements e true
set positivity constraints members true finally
set everything else false example check whether x x
potentially irreducible set x true x true set x true
positivity constraint x set remaining features x false evaluating
constraint x x yields true showing x x potentially irreducible
irreducible
given definitions irreducibility potential irreducibility mechanisms
check properties notion dependency constraints next describe
construct voila
voila construction proceeds bottom fashion beginning lowest level
initially contains empty set constructs irreducible feature sets
adding one feature time voila structure gives details
keeps track irreducible feature sets set
potentially irreducible feature sets ps done processing feature xij
remove ps potentially irreducible set cannot become irreducible xij
considered line
analysis voila construction
construction inserts node voila corresponding set
irreducible lines moreover keeping track potentially irreducible sets
lines generate every possible irreducible set generated thus voila
contains possible irreducible subsets x


fibilgic getoor

voila construction
input set features x class variable
output voila data structure v given e
pick ordering elements x xi xi xin
ps
j n

ps

xij dc dc dc xij

irreducible

add node corresponding v

else

potentially irreducible

ps ps








remove ps sets longer potentially irreducible
max size largest l
l max



add edge v

worst case running time still exponential number
initially unobserved features x e number irreducible sets potentially
exponential running time practice though depends structure
bayesian network voila upon ordering variables line
example bayesian network naive bayes subsets irreducible
feature separates feature class variable thus search space cannot
reduced however naive bayes makes extremely strong assumptions
unlikely hold practice fact empirically experiments section five
real world datasets features often conditionally independent given class variable
complex interactions thus number irreducible
subsets substantially smaller number possible subsets
loop line iterates irreducible potentially irreducible sets
generated far number potentially irreducible sets generated
depends ordering chosen good ordering processes features literals
positivity constraints features dependency constraints earlier
undirected path xi includes xj v structure good ordering puts xj
earlier ordering everything xj xi instance sample
bayesian network figure consider x earlier x refer
ordering perfect satisfies positivity constraints perfect ordering used
voila construction never generates potentially irreducible set unfortunately


fivalue information lattice

possible perfect ordering perfect ordering possible two
features positivity constraint literal dependency constraints
case occurs loop two v structures
note even though bayesian network directed acyclic graph still contain
loops e undirected cycles perfect ordering possible four five real world
datasets used
voila feature value acquisition
voila makes searching space possible subsets tractable practice
flexibility possible devise several different acquisition policies describe two
policies example policies section
first acquisition policy aims capture practical setting one
feature acquired policy constructed voila follows
path ps policy initially empty repeatedly extended acquiring
set v best benef e policy construction ends path
extended e candidate sets non positive benefit values path
second acquisition policy adds look ahead capability greedy policy
rather repeatedly extending path ps policy feature xi
highest benef xi e add look ahead capability first set v
highest benef e instead acquiring features
policy feature xi highest
benef xi e acquire extend ps

experiments
experimented five real world medical datasets turney described
used datasets bupa liver disorders heart disease hepatitis
pima indians diabetes thyroid disease available uci machine
learning repository frank asuncion datasets varying number
features ranging five four five datasets binary labels whereas
thyroid dataset three labels
dataset first learned bayesian network provides joint
probability distribution p x efficiently answers conditional independence queries
thorough separation pearl built voila dataset learned
bayesian network first present statistics dataset number features
number nodes voila compare acquisition policies
search space reduction
table shows aggregate statistics dataset describing number features
number possible subsets number subsets voila percent reduction
search space table shows number irreducible subsets substantially
fewer possible subsets thyroid disease dataset example number
possible subsets million whereas number irreducible subsets fewer


fibilgic getoor

table aggregate statistics dataset number irreducible subsets e
number nodes voila substantially fewer number possible
subsets
dataset
bupa liver disorders
pima indians diabetes
heart disease
hepatitis
thyroid disease

features

subsets

nodes voila

reduction

























thirty thousand enormous reduction search space makes searching
possible sets features tractable practice
expected total cost comparisons
compared expected total costs equation four different acquisition policies
dataset policies follows
acquisition policy acquire features aims minimize
expected misclassification cost prior probability distribution class
variable p
markov blanket policy acquires every relevant feature regardless misclassification costs market blanket bayesian network defined
parents children childrens parents pearl intuitively
minimal set x x
greedy policy repeatedly expands path ps initially empty policy
acquiring feature xi highest positive benef xi equation
policy construction ends path extended feature
positive benefit value
greedy la policy adds look ahead capability greedy strategy
policy repeatedly expands path ps initially empty policy first finding
set highest positive benef equation acquiring
feature xi maximum benef xi equation policy
construction ends set positive benefit value found path
policy
feature costs dataset described detail turney summary
feature independent cost belong group features
first feature group incurs additional cost example first feature
group blood measurements incurs overhead cost drawing blood patient
feature costs data ontario ministry health


fivalue information lattice

table example misclassification cost matrix cij symmetric asymmetric misclassification costs cij set way achieve prior expected misclassification
cost symmetric cost case choosing probable class leads
em c whereas asymmetric cost case choosing class
indifferent leads emc
actual class

prior probability

pred class

symm cost

asymm cost



p












p










observed features assigned cost example four
five features bupa liver disorders dataset features hepatitis
dataset six eight features diabetes dataset features
thyroid disease dataset assigned cost costs similar
practically equivalent finding minimum size decision tree provide
structure feature acquisition costs experimented randomly generated
feature group costs feature randomly generated cost
group generated cost repeated experiments
three different seeds dataset
misclassification costs defined turney one reason
could easier define feature costs defining cost misclassification non trivial instead turney tests different acquisition strategies
misclassification costs follow similar technique slight modification
compare acquisition policies symmetric cij cji asymmetric
misclassification costs able judge misclassification cost structure affects
feature acquisition unify presentation compare different acquisition strategies
priori expected misclassification costs defined equation specifically compare acquisition policies priori emc achieved
varying cij accordingly example misclassification table emc value
table real feature cost case varied emc
varied synthetic feature cost case
compare greedy greedy la markov blanket policies plotting
much cost policy saves respect acquisition policy x axis
plots vary priori expected misclassification cost methodology
described plot savings axis dataset plot four different
scenarios cross product symmetric asymmetric misclassification costs real
synthetic feature costs
liver disorders diabetes heart disease hepatitis thyroid
disease given figures respectively figure symmetric
misclassification cost scenarios given sub figures c whereas asymmetric


fibilgic getoor



b

c



figure expected total cost etc comparisons bupa liver disorders dataset
priori class distribution follows p

misclassification cost scenarios presented b similarly real feature cost
scenarios given b synthetic feature cost scenarios presented
c next summarize
found greedy policy often prematurely stopped acquisition performing
even worse markov blanket strategy true datasets
regardless feature misclassification cost structures fact greedy
strategy perform worse markov blanket strategy really troubling first
might seem rather unintuitive greedy strategy perform worse markov
blanket strategy part reason features belong groups first
feature group incurs overhead cost greedy strategy feature
considered isolation overhead costs outweigh single features benefit
greedy look ahead reluctant commit acquiring
first feature group


fivalue information lattice



b

c



figure expected total cost etc comparisons pima indian diabetes dataset
priori class distribution follows p

greedy la strategy never performs worse strategy setting
misclassification cost structure symmetric asymmetric considerable
effect policies behaved differences symmetric asymmetric cases particularly evident datasets class distribution
imbalanced diabetes figure hepatitis figure thyroid
disease figure datasets differences due misclassification cost structure
summarized follows
class distribution imbalanced misclassification cost symmetric acquiring information cannot change classification decisions
easily due class imbalance thus features high evi values
hand misclassification costs asymmetric features tend
higher evi values thus greedy greedy la strategies start
acquiring features earlier x axis asymmetric cases compared


fibilgic getoor



b

c



figure expected total cost etc comparisons heart disease dataset priori
class distribution follows p

symmetric counterparts example thyroid disease dataset
real feature costs greedy strategy starts acquisition emc
greater symmetric misclassification costs figure whereas
starts acquiring emc reaches asymmetric case figure b synthetic feature costs dramatic neither
greedy greedy la acquires features symmetric cost case figure c whereas start acquisition em c asymmetric
case figure
realm slope savings asymmetric case much higher compared symmetric case
misclassification cost structure causes differences greedy
greedy la policies cases diabetes dataset greedy policy performs worse misclassification costs symmetric figures


fivalue information lattice



b

c



figure expected total cost etc comparisons hepatitis dataset priori class
distribution follows p

c whereas hepatitis dataset performs worse asymmetric
misclassification costs figures b
greedy policy sometimes erratic unpredictable unreliable performance expected misclassification changes possibly hits local minima gets
later hits local minima figures
finally present aggregate summary table table shows
much greedy policy greedy la policy saves markov blanket policy
presented average saving intervals
table shows greedy la policy never loses compared markov blanket
policy one would expect additionally greedy la policy wins greedy
policy cases never looses finally greedy policy prematurely stops
acquisition negative savings respect markov blanket strategy


fibilgic getoor



b

c



figure expected total cost etc comparisons thyroid disease dataset
priori class distribution follows p

related work
decision theoretic value information calculations provide principled methodology
information gathering general howard lindley influence diagrams
example popular tools representing decisions utility functions howard
matheson however devising optimal acquisition policy e constructing optimal decision tree intractable general approaches feature
acquisition myopic dittmer jensen greedily acquiring one feature
time greedy approaches typically differ setup assume ii
way features scored iii classification model learned review
existing work highlighting differences different techniques three
dimensions
gaag wessels consider evidence gathering diagnosis
bayesian network setup gather evidence e observe values
variables hypothesis confirmed disconfirmed desired extent


fivalue information lattice

table savings greedy gr greedy la la respect markov blanket
policy averaged different intervals entry bold worse
greedy la red worse markov blanket

liver
gr

la

diabetes
gr
la

heart
gr

la

hepatitis
gr
la

thyroid
gr
la

real feature costs symmetric misclassification costs



















































































































real feature costs asymmetric misclassification costs








































synthetic feature costs symmetric misclassification costs








































































synthetic feature costs asymmetric misclassification costs

















































































propose acquisition greedily computes expected utility acquiring
feature chooses one highest utility define utility absolute
value change probability distribution hypothesis tested
recent work sent gaag consider acquiring
single feature step define subgoals cluster features subgoal
subgoals clustering features provided domain experts
non myopic case pick cluster calculating expected values however


fibilgic getoor

clusters big calculating expected value cluster problematic
thus provide semi myopic pick cluster
best myopic feature
nunez introduces decision tree called eg sensitive
feature costs rather splitting decision tree feature high information
gain eg chooses feature least information cost function defined
ratio features cost discriminative efficiency eg however directly
optimized balance misclassification cost feature acquisition cost rather
optimized loss taking feature costs account similarly tan
modifies id quinlan account feature costs tan considers
domain robot needs sense recognize act number features
large robot act efficiently needs trade accuracy efficiency
turney builds decision tree called icet standing inexpensive classification
expensive tests genetic search grefenstette
nunezs criteria build c decision trees quinlan unlike nunez turney
takes misclassification costs account addition feature costs evaluate
given decision tree looks good decision tree genetic search
yang et al build cost sensitive decision trees naive bayes classifiers
take feature costs misclassification costs account unlike nunez
scores features information gain cost ratio yang et al score features
expected reduction total cost e sum feature cost misclassification
cost training data take feature costs misclassification costs
account directly learning time
bayer zubek formulates feature acquisition markov decision
process provides greedy systematic search develop diagnostic
policies bayer zubek takes feature cost misclassification costs account
automatically finds acquisition plan balances two costs introduces
admissible heuristic ao search describes regularization techniques reduce overfitting training data
saar tsechansky melville provost consider active feature acquisition
classifier induction specifically given training data missing feature values cost matrix defines cost acquiring feature value describe
incremental select best feature acquire iteratively build
model expected high future performance utility acquiring feature
estimated terms expected performance improvement per unit cost two characteristics make work different previous work authors
assume fixed budget priori rather build model incrementally ii
feature different cost instance
finally greiner grove roth analyze sample complexity dynamic
programming performs value iteration search best diagnostic
policies analyze learning optimal policy variant
probably approximately correct pac model learning achieved
efficiently active classifier allowed perform constant number
tests learning optimal policy often intractable general
environments


fivalue information lattice

future work
article scratched surface incorporating constraints
features order reduce search space make reasoning sets tractable
discovered two types constraints features render features useless
features useless without features purely underlying probability
distribution shown automatically discovered constraints helped reduce
search space dramatically practice possible discover additional types
constraints potentially used reduce search space e g ordering
constraints certain procedures precede procedures constraints
defined observed feature values example treadmill test might
performed patients old age patients decline certain procedures medications
eliciting constraints domain experts utilizing reduce
search space promising future direction
existing feature acquisition frameworks including one major
simplification happens practice assumed acquiring values
features change class value values variables however practice
feature value measurements side effects example medical diagnosis
certain measurements non invasive change status patient others
might include medications affect outcome similarly fault diagnosis
repair purpose diagnose repair fault actions
fact repair fault essence changing class value taking extra side effects
account make feature acquisition frameworks realistic

conclusion
typical feature acquisition greedy past primarily due
sheer size possible subsets features described general technique
optimally prune search space exploiting conditional independence relationships
features class variable empirically showed exploiting conditional independence relationships substantially reduce number possible subsets
introduced novel data structure called value information lattice voila
efficiently reduce search space conditional independence relationships share probabilistic inference computations different subsets
features voila able add full look ahead capability greedy
acquisition policy would practical otherwise experimentally showed
five real world medical datasets greedy strategy often stopped feature acquisition
prematurely performing worse even policy acquires features

acknowledgments
thank reviewers helpful constructive feedback material
work supported national science foundation grant


fibilgic getoor

references
bayer zubek v learning diagnostic policies examples systematic search
annual conference uncertainty artificial intelligence
bilgic getoor l voila efficient feature value acquisition classification
aaai conference artificial intelligence pp
dittmer jensen f myopic value information influence diagrams
annual conference uncertainty artificial intelligence pp
frank asuncion uci machine learning repository
gaag l wessels selective evidence gathering diagnostic belief networks
aisb quarterly pp
grefenstette j optimization control parameters genetic ieee
transactions systems man cybernetics
greiner r grove j roth learning cost sensitive active classifiers
artificial intelligence
heckerman horvitz e middleton b approximate nonmyopic computation value information ieee transactions pattern analysis machine
intelligence
howard r matheson j e readings principles applications
decision analysis chap influence diagrams strategic decision group
howard r information value theory ieee transactions systems science
cybernetics
hyafil l rivest r l constructing optimal binary decision trees npcomplete information processing letters
lindley v measure information provided experiment annals
mathematical statistics
nunez use background knowledge decision tree induction machine
learning
health schedule benefits physician services health insurance
act
pearl j probabilistic reasoning intelligent systems morgan kaufmann san
francisco
quinlan j r induction decision trees machine learning
quinlan j r c programs machine learning morgan kaufmann publishers
inc san francisco ca usa
saar tsechansky melville p provost f active feature value acquisition
management science
sent gaag l c enhancing automated test selection probabilistic networks proceedings th conference artificial intelligence medicine
pp


fivalue information lattice

tan csl cost sensitive learning system sensing grasping objects
ieee international conference robotics automation
turney p cost sensitive classification empirical evaluation hybrid genetic
decision tree induction journal artificial intelligence

yang q ling c chai x pan r test cost sensitive classification data
missing values ieee transactions knowledge data engineering





