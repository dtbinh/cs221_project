Journal Artificial Intelligence Research 31 (2008) 83-112

Submitted 06/07; published 01/08

CUI Networks: Graphical Representation Conditional
Utility Independence
Yagil Engel
Michael P. Wellman

yagil@umich.edu
wellman@umich.edu

University Michigan, Computer Science & Engineering
2260 Hayward St, Ann Arbor, MI 48109-2121, USA

Abstract
introduce CUI networks, compact graphical representation utility functions
multiple attributes. CUI networks model multiattribute utility functions using
well-studied widely applicable utility independence concept. show conditional
utility independence leads effective functional decomposition exhibited
graphically, local, compact data graph nodes used calculate
joint utility. discuss aspects elicitation, network construction, optimization,
contrast new representation previous graphical preference modeling.

1. Introduction
Modern AI decision making based notion expected utility, probability distributions used weigh utility values possible outcomes.
representation probability distribution functions Markov Bayesian networks
(Pearl, 1988)exploiting conditional independence achieve compactness computational efficiencyhas led plethora new techniques applications. Despite
equal importance decision making, preferences utilities generally received
level attention AI researchers devoted beliefs probabilities.
(increasing) efforts develop representations inference methods utility achieved
degree success comparable impact graphical models probabilistic reasoning.
Recognizing utility functions multidimensional domains may amenable
factoring based independence (Keeney & Raiffa, 1976), several aimed develop
models analogous benefits (Bacchus & Grove, 1995; Boutilier, Bacchus, & Brafman,
2001; La Mura & Shoham, 1999; Wellman & Doyle, 1992). goal well,
compare approach methods Related Work section (2.2).
development compact representations multiattribute utility begins
notion preferential independence (PI), separability subdomains outcome space.
subdomain outcomes separable PI sense preference order subdomain depend rest domain. subsets attributes induce
separable subdomains, ordinal utility (value) function decomposes additively
variables (Debreu, 1959; Fishburn, 1965; Gorman, 1968). cardinal utility function represents preferences outcomes notion strength preferences,
notably represent preferences actions uncertain outcomes, lotteries. Direct
adaptation PI concept cardinal utility requires generalization notion:
set attributes utility independent (UI) preference order lotteries

c
2008
AI Access Foundation. rights reserved.

fiEngel & Wellman

induced subdomain depend values rest attributes. stronger
judgement assert preference order joint domain depends
margins attribute subsets. latter leads powerful additive decompositions,
either fully additive (when subsets attributes disjoint), generalized,
additive decomposition overlapping subsets (Fishburn, 1967; Bacchus & Grove, 1995).
Utility independence leads less convenient decompositions, multilinear (Keeney
& Raiffa, 1976) hierarchical (Von Stengel, 1988; Wellman & Doyle, 1992). previous
efforts AI community adapt modern graphical modeling utility functions employ generalized additive decomposition (Bacchus & Grove, 1995; Boutilier et al., 2001;
Gonzales & Perny, 2004). contrast, work continues thread, based
weaker utility independence assumption. elaborate difference types
independence following presentation formal definitions.

2. Background
utility-theoretic terminology follows definitive text Keeney Raiffa (1976).
multiattribute utility framework, outcome represented vector values
n variables, called attributes. decision makers preferences represented
total pre-order, , set outcomes. common applications decision makers
ability choose certain outcome, rather action results
probability distribution outcomes, called lottery. decision maker hence
set possible lotteries. Given standard set
assumed preference order

axioms, represented real-valued utility function outcomes, U (),
numeric ranking probabilistic outcomes expected utility respects ordering
utility function unique positive affine transformations. positive linear
.
transform U () represents preferences, thus strategically equivalent.
ability represent utility probability distributions function outcomes
provides structure, multiattribute settings outcome space n-dimensional.
Unless n quite small, therefore, explicit (e.g., tabular) representation U ()
generally practical. Much research multiattribute utility theory aims
identify structural patterns enable compact representations. particular,
subsets attributes respect various independence relationships, utility function may
decomposed combinations modular subutility functions smaller dimension.
Let = {x1 , . . . , xn } set attributes. following definitions (and rest
work) capital letters denote subsets attributes, small letters (with without numeric
subscripts) denote specific attributes, X denotes complement X respect
S. denote (joint) domain X D(X), indicate specific attribute assignments
prime signs superscripts. represent instantiation subsets X
time use sequence instantiation symbols, X 0 0 .
order meaningfully discuss preferences subsets attributes, need notion
preferences subset given fixed values rest attributes.
0

0

Definition 1. Outcome 0 conditionally preferred outcome 00 given , 0
0
0
00 . denote conditional preference order given 0 .

84

fiCUI networks

0
Similarly define conditional preference order lotteries. preference order

0
lotteries represented conditional utility function, U (Y, ).
Definition 2. Preferential Independent (PI) 0 depend value
0
chosen .
Preferential independence useful qualitative preference assessment. Firstorder preferential independence (i.e., independence single attribute rest)
natural assumption many domains. example, typical purchase decisions greater
quantity higher quality desirable regardless values attributes. Preferential independence higher order, however, requires invariance tradeoffs among
attributes respect variation others, stringentthough still often
satisfiableindependence condition. standard PI condition applies subset respect full complement remaining attributes. conditional version PI specifies
independence respect subset complement, holding remaining attributes
fixed.
Definition 3. Conditionally Preferential Independent (CPI) X given Z (Z = XY ),
Z 0 , X 0 Z 0 depend value chosen X 0 . denote relationship
CPI(Y, X | Z).
counterpart preferential independence considers probability distributions
outcomes called utility independence.
Definition 4. Utility Independent (UI) , conditional preference order
0 , depend value chosen 0 .
lotteries ,

notations, apply UI conditions defined sets attributes
specific attributes.
Given UI(Y, X), taking X = , conditional utility function given X 0
invariant positive affine transformations, fixed value X 0 . fact
expressed decomposition
U (X, ) = f (X) + g(X)U (X 0 , ),

g() > 0.

Note functions f () g() may different particular choice X 0 . Since
U (X 0 , ) function , sometimes use notation UX 0 (Y ).
Utility independence conditional version well.
Definition 5. Conditionally Utility Independent (CUI) X given Z (Z = XY )
X 0 Z 0 depend value chosen X 0 . denote relationship
Z 0 ,
CUI(Y, X | Z).
CUI supports functional decomposition. Z 0 , conditional utility function
given X 0 Z 0 strategically equivalent function given different instantiation
X. However, transformation depends X, Z 0 . Hence
write:
U (X, Y, Z) = f (X, Z) + g(X, Z)U (X 0 , Y, Z), g() > 0.
(1)
85

fiEngel & Wellman

is, fix X arbitrary level X 0 use two transformation functions f
g get value U () levels X. stronger, symmetric form independence
leads additive decomposition utility function called additive independence.
provide definition conditional version.
Definition 6. X Conditionally Additive Independent given Z, CAI(X, | Z),
Z 0 depends marginal conditional probability disif instantiation Z 0 ,
0
tributions XZ Z 0 .
means value Z 0 , two probability distributions p, q
p(X, , Z 0 ) q(X, , Z 0 ), p(, Y, Z 0 ) q(, Y, Z 0 ), decision maker indifferent
p q. necessary (but always sufficient) condition hold
utility differences U (X 0 , Y, Z 0 ) U (X 00 , Y, Z 0 ) (for X 0 , X 00 ) depend
value .
CAI leads following decomposition (Keeney & Raiffa, 1976):
U (X, Y, Z) = f (X, Z) + g(Y, Z).
variations utility independence considered theoretical literature,
leading various decomposition results (Fishburn, 1975; Krantz, Luce, Suppes, & Tversky,
1971; Fuhrken & Richter, 1991).
2.1 Motivation
obvious benefit model based (conditional) utility independence
generality admitted weaker independence condition, comparison additive independence. Whereas additivity practically excludes interaction utility one
attribute subset (X Definition 6) value another (Y ), utility independence
allows substitutivity complementarity relationships, long risk attitude towards one variable affected value another. One could argue UI
particularly intuitive, based invariance condition preference order.
contrast, (conditional) additive independence requires judgment effects joint
versus marginal probability distributions. Moreover, additive independence symmetric,
whereas condition U I(X, ) allow preference order depend X.
Bacchus Grove exemplify difference additive utility independence
simple state space two boolean attributes: Health Wealth. example,
shown Table 1, attributes additive independent (it immediately seen
using preference differences), H W complements: worth
sum one without other. would considered
two attributes substitutes if, example, U (W, H) = 4 U (W, H) = 3. cases
H W nonetheless preferential independent, since always prefer richer (all
else equal) healthier (all else equal). boolean variables, preferential
utility independence equivalent (we always prefer lotteries give higher probability
preferred level) therefore Health Wealth UI other.
(Conditional) additive independence resulting additive decomposition
generalized multiple subsets necessarily disjoint. condition called

86

fiCUI networks

W
W

H
5
2

H
1
0

Table 1: Utility values Health Wealth example (Bacchus & Grove, 1995).
generalized additive independence (GAI). GAI holds, U () decomposes sum independent functions () GAI subsets Xi . shown Bacchus Grove, CAI
conditions accumulated global GAI decomposition (see Section 2.2). latter
may exist without CAI conditions leading it, GAI condition hard
identify: whereas CAI condition corresponds independence two attributes
two subsets, global GAI condition intuitive interpretation.
next example, cardinal independence condition exists, except non symmetric CUI. example shows difference PI UI, hence requires
domains H W include least three values each. add third attribute outcome space, location (L), indicating whether live city
countryside (Table 2). order show U I(H, {W, L}) hold enough
find violated one pair lotteries. Given partial outcome Wr , Lci
prefer equal chance lottery < Hf , Hs >, whose expected utility 12+5
2 , sure
outcome Hg (value 8), whereas given Wp , Lci indifferent (expected utility 2
lotteries). Intuitively, may case additional value get fitness
(over good health) higher rich, making significant value
Hg adds Hs . Similarly, U I(W, {H, L}) hold, comparing even-chance
gamble < Wr , Wp > sure outcome Wm , first given Hf , Lci given
Hs , Lci .
W H therefore utility independent, preferential independent.
L, however, not: rich would rather live city, way
round poor, except case poor sick prefer
city.

Wr
Wm
Wp

Hf
12
6
3

Lci
Hg
8
4
2

Hs
5
3
1

Hf
10
6
4

Lco
Hg
6
3
1.5

Hs
4
2
0

Table 2: Utility values Health, Wealth Location example. Wr means rich, Wm
medium income, Wp means poor. Hf healthy top fitness, Hg means good health,
Hs means sick. Lci stands city location, Lco means countryside location.
Therefore, symmetric independence condition exists here, rules additive multiplicative independence, conditional not, subsets attributes.
Also, since single variable unconditionally UI, subset unconditionally
UI. Further, fact preferences L depend combination H W rules
GAI decomposition form {W, L}, {W, H}, {H, L}.
87

fiEngel & Wellman

can, however, achieve decomposition using CUI. case CUI(W, L|H),
since column left matrix (Lci ) affine transformation counterpart
right side (Lco ). example, transform first column (Hf ), multiply 23
add 2.
example illustrates subtlety utility independence. particular, whereas
preferences L depend W , W may still (conditionally) UI L. CAI assumption attributes must inevitably ignore reversal preferences L
different values W , hence decision maker queried preferences
assumption may able provide meaningful answers.
interaction system requires preference representation normally requires
identification structure, population utility values required
compact representation. therefore important two aspects
simplified possible, whereas functional form handled system may
sophisticated. exactly tradeoff made CUI nets, compared GAI-based
representation: GAI condition based CAI, CUI nets achieve lower dimensionality
(Section 7), therefore easier elicitation. GAI condition based collection
CAI conditions, hard identify. CUI nets simplify bottleneck aspects,
driving complexity algorithms functional form handled
behind scenes.
2.2 Related Work
Perhaps earliest effort exploit separable preferences graphical model extension influence diagrams Tatman Shachter (1990) decompose value functions
sums products multiple value nodes. structure provided computational
advantages, enabling use dynamic programming techniques exploiting value separability.
Bacchus Grove (1995) first develop graphical model based conditional
independence structure. particular, establish CAI condition perfect
map (Pearl & Paz, 1989); is, graph attribute nodes node separation
reflects exactly set CAI conditions S. specifically, two sets nodes
X, S, CAI(X, |XY ) holds direct edge node X
node . use term CAI map referring graph reflects
perfect map CAI conditions, context preference order D(S). Bacchus
Grove go show utility function GAI decomposition set
maximal cliques CAI map. show Section 7, CUI network representation
developed achieves weakly better dimensionality CAI maps due greater
generality independence assumption.
Initiating another important line work, Boutilier et al. (1999) introduced CP networks, efficient representation ordinal preferences multiple attributes.
CP network, variable conditionally PI rest given parents. Ordinal multiattribute preference representation schemes (for decision making certainty),
especially CP networks, dramatically simplify preference elicitation process, based
intuitive relative preference statements avoid magnitude considerations.
However, limited expressive power CP networks may suffice complex decision

88

fiCUI networks

problems, tradeoff resolution may hinge complicated way attribute settings
rich domains. problem particularly acute continuous almost continuous
attributes involved, money time.
Boutilier et al. (2001) subsequently extended approach numeric, cardinal utility
UCP networks, graphical model utilizes GAI decomposition combined
CP-net topology. requires dominance relations parents children,
somewhat limiting applicability representation. GAI structure
applied graphical models Gonzales Perny (2004), employ clique graph
CAI map (the GAI network ) elicitation purposes.
earlier work, La Mura Shoham (1999) redefine utility independence symmetric multiplicative condition, taking closer probability analog, supporting
Bayes-net representation. Although multiplicative independence different additive independence, necessarily weaker. Recent work Abbas (2005) defines
subclass utility functions multiplicative notion UI obeys analog
Bayess rule.
graphical decomposition suggested past utility functions based
original, non-symmetric notion utility independence utility tree (Von Stengel,
1988, see Wellman Doyle, 1992, discussion AI context). utility tree
decomposes utility function using multilinear multiplicative decomposition (Keeney
& Raiffa, 1976), tries decompose subset similarly. Using
hierarchical steps utility function becomes nested expression functions
smallest separable subsets complements.
2.3 Graphical Models CUI
concluding remarks, Bacchus Grove (1995) suggest investigating graphical
models independence concepts, particular utility independence. Founding
graphical model UI difficult, however, utility independence decompose
effectively additive independence. particular, condition U I(Y, X) ensures
subutility function, since X one harder carry
decomposition X. Hence case X large dimensionality
representation may remain high. approach therefore employs CUI conditions
large subsets , case decomposition driven decomposing
conditional utility function using CUI conditions.
sequel show serial application CUI leads functional decomposition.
corresponding graphical model, CUI network, provides lower-dimension representation utility function function vertex depends node
parents. demonstrate use CUI networks constructing example
relatively complex domain. Next elaborate technical semantic properties model knowledge required construct it. Subsequent technical sections
present optimization algorithms techniques reducing complexity
representation.

89

fiEngel & Wellman

3. CUI Networks
begin constructing DAG representing set CUI conditions, followed derivation functional decomposition nodes DAG.
3.1 CUI DAG
Suppose obtain set CUI conditions variable set = {x1 , . . . , xn },
x S, contains condition form
CUI(S \ ({x} P (x)) , x | P (x)).
words, exists set P (x) separates rest variables x.
P (x) always exists, P (x) = \ {x} condition trivially holds. set
represented graphically following procedure, name procedure C.
1. Define order set (for convenience assume ordering x1 , . . . , xn ).
2. Define set parents x1 P a(x1 ) = P (x1 ).
3. = 2, . . . , n
), set nodes
Define set intermediate descendants xi , Dn(x
x1 , . . . , xi1 turned descendants xi , xi
) smallest
parent another descendant xi parent. Formally, Dn(x
set satisfies following condition:
) P a(xj )
j {1, . . . , 1}, [xi P a(xj ), k {1, . . . , 1}.xk Dn(x
).] (2)
xj Dn(x
Define parents xi nodes P (xi ) already descendants xi ,
).
P a(xi ) = P (xi ) \ Dn(x
procedure defines DAG. denote Dn(x) final set descendants x.
set defined Equation (2), replacing {1, . . . , 1} {1, . . . , n}).

definitions, Dn(x) Dn(x),
hence

P a(x) Dn(x) P a(x) Dn(x)
= P (x).

(3)

Proposition 1. Consider DAG defined procedure C set attributes S.
x S,
CUI(S \ ({x} P a(x) Dn(x)) , x | P a(x) Dn(x)).
(4)

Proof. definitions P a(x) P (x), (4) holds replacing Dn(x) Dn(x).
definition CUI, straightforward
CUI(S \ (Y W ) , | W ) CUI(S \ (Y W Z) , | W Z),
invariance preference order \ (Y W ) implies invariance preference
order subset \ (Y W Z), difference set Z fixed. Given (3),


taking W = P a(x) Dn(x)
Z = Dn(x) \ Dn(x),
get (4).
90

fiCUI networks

example, show construction structure small set variables
= {x1 , x2 , x3 , x4 , x5 , x6 }, given following set CUI conditions:
= {CUI({x4 , x5 , x6 }, x1 | {x2 , x3 }), CUI({x4 , x3 , x6 }, x2 | {x1 , x5 }),
CUI({x2 , x4 , x6 }, x3 | {x1 , x5 }), CUI({x1 , x3 , x5 }, x4 | {x2 , x6 }),
CUI(x6 , x5 | {x1 , x2 , x3 , x4 }), CUI({x1 , x2 , x3 , x5 }, x6 | x4 )}.
Construction network using order implied indices results CUI
DAG illustrated Figure 1. minimal separating set x1 {x2 , x3 }. x2 , get
2 ) = {x1 }, non-descendant variable required separate
Dn(x
rest x5 , therefore parent. rest graph constructed
similar way. x4 placed, find P (x4 ) = {x2 , x6 }. Therefore, x4 becomes
2 ) {x4 } = {x1 , x4 }.
descendant x2 x2 placed, words Dn(x2 ) = Dn(x
ix6

ix5
Z

Z
=x

~ ix
Z

2



?
=x


4

3

Z

Z

~i
Z
=x

1

Figure 1: CUI DAG given order x1 ,. . . ,x6 .

Definition 7. Let U (S) utility function representing cardinal preferences D(S).
CUI DAG U () DAG, x S, (4) holds.
Procedure C yields CUI DAG Proposition 1. direction, given CUI
DAG G (in parents descendants denoted P aG (), DnG (), respectively)
constructed using C, follows. Define P (x) = P aG (x) DnG (x) variable
ordering according reverse topological order G, complete execution C.
straightforward show set parents selected xi exactly P aG (xi ),
hence result DAG identical G.
3.2 CUI Decomposition
show CUI conditions, guaranteed Proposition 1, applied iteratively decompose U () lower dimensional functions. first pick variable ordering
agrees reverse topological order CUI DAG. simplify presentation,
rename variables ordering x1 , . . . , xn . CUI condition (4) x1
implies following decomposition, according (1):
U (S) = f1 (x1 , P a(x1 ), Dn(x1 )) + g1 (x1 , P a(x1 ), Dn(x1 ))Ux01 (S \ {x1 }).

(5)

Note Dn(x1 ) = .
assume specified reference point 0 , arbitrary value chosen
attribute x S, denoted x0 . Ux01 () right hand side conditional
91

fiEngel & Wellman

utility function given x1 fixed reference point x01 . convenience omit
attributes whose values fixed list arguments.
applying decomposition based CUI condition x2 conditional
utility function Ux01 (), get
Ux01 (S \ {x1 }) = f2 (x, P a(x2 ), Dn(x2 )) + g2 (x2 , P a(x2 ), Dn(x2 ))Ux01 ,x02 (S \ {x1 , x2 }). (6)
Note Dn(x2 ) {x1 }, x1 fixed x01 , hence f2 g2 effectively depend
x2 P a(x2 ). point exploited below.
Substituting Ux01 () (5) according (6) yields:
U (S) = f1 + g1 (f2 + g2 Ux01 ,x02 (S \ {x1 , x2 })) = f1 + g1 f2 + g1 g2 Ux01 ,x02 (S \ {x1 , x2 }).
list arguments functions fj , gj always (xj , P a(xj ), Dn(xj )), omit
readability.
continue fashion get
U (S) =

i1
X

k1


(fk

j=1

k=1

gj ) +




gj Ux01 ,...,x0 (xi , . . . , xn ),
i1

j=1

apply CUI condition xi ,
Ux01 ,...,x0 (xi , xi+1 , . . . xn ) =
i1

(xi , P a(xi ), Dn(xi )) + gi (xi , P a(xi ), Dn(xi ))Ux01 ,...,x0 (xi+1 , . . . , xn ). (7)


convenience, define constant function fn+1 Ux01 ,...,x0n (). Ultimately obtain
U (S) =

n+1
X

i1


i=1

j=1

(fi (xi , P a(xi ), Dn(xi ))

gj (xj , P a(xj )), Dn(xj )).

(8)

variable ordering restricted agree reverse topological order graph,
hence (7), Dn(xi ) {x1 , . . . , xi1 }. Therefore, variables Dn(xi ) righthand side (7) fixed reference points, gi depend xi
P a(xi ). Formally, let y1 , . . . , yk variables Dn(xi ). abuse notation,
define:
(xi , P a(xi )) = (xi , P a(xi ), y10 , . . . , yk0 ),
gi (xi , P a(xi )) = gi (xi , P a(xi ), y10 , . . . , yk0 ).

(9)

(8) becomes
U (S) =

n+1
X

i1


i=1

j=1

(fi (xi , P a(xi ))

gj (xj , P a(xj ))).

(10)

term decomposition multiattribute utility function lower dimensional
functions, whose dimensions depend number variables P a(x). result,
92

fiCUI networks

dimensionality representation reduced (as Bayesian networks) maximal
number parents node plus one.
illustrate utility function decomposed example Figure 1.
pick ordering x4 , x1 , x6 , x3 , x2 , x5 agrees reverse topological order
graph (note renaming variables here). simplify notation
denote conditional utility function xi fixed reference point adding
subscript U ().
U (S) = f4 (x4 x2 x6 ) + g4 (x4 x2 x6 )U4 (S \ {x4 })
U4 (S \ {x4 }) = f1 (x1 x2 x3 ) + g1 (x1 x2 x3 )U1,4 (S \ {x4 x1 })
U1,4 (S \ {x4 x1 }) = f6 (x6 ) + g6 (x6 )U1,4,6 (x2 x3 x5 )
U1,4,6 (x2 x3 x5 ) = f3 (x3 x5 ) + g3 (x3 x5 )U1,3,4,6 (x2 x5 )
U1,3,4,6 (x2 x5 ) = f2 (x2 x5 ) + g2 (x2 x5 )U1,2,3,4,6 (x5 )
U1,2,3,4,6 (x5 ) = f5 (x5 ) + g5 (x5 )U1,2,3,4,5,6 ()
Note gi depends xi parents. Merging equations,
using definition f7 U1,2,3,4,5,6 () produces
U (S) = f4 + g4 f1 + g4 g1 f6 + g4 g1 g6 f3 + g4 g1 g6 g3 f2 + g4 g1 g6 g3 g2 f5 + g4 g1 g6 g3 g2 g5 f7 . (11)
established U (S) represented using set functions F, includes,
x S, functions (fx , gx ) resulting decomposition (1) based CUI
condition (4). means fully specify U (S) sufficient obtain data
functions F (this aspect discussed Section 5).
Definition 8. Let U (S) utility function representing cardinal preferences D(S).
CUI network U () triplet (G, F, 0 ). G = (S, E) CUI DAG U (S), 0
reference point, F set functions {fi (xi , P a(xi )), gi (xi , P a(xi )) | = 1 . . . , n}
defined above.
utility value assignment calculated CUI network
according (10), using variable ordering agrees reverse topological order
DAG. example, choose different variable ordering one used
above, x1 , x3 , x4 , x2 , x5 , x6 , leading following expression.
U (S) = f1 + g1 f3 + g1 g3 f4 + g1 g3 g4 f2 + g1 g3 g4 g2 f5 + g1 g3 g4 g2 g5 f6 + g1 g3 g4 g2 g5 g6 f7 .
sum product different one (11). However, based CUI
decompositions therefore functions (fi , gi ).
3.3 Properties CUI Networks
Based Procedure C decomposition following it, conclude following.
Proposition 2. Let set attributes, set CUI conditions S.
includes condition form CUI(S \ (x Zx ), x | Zx ) x S,
represented CUI network whose dimensionality exceed maxx (|Zx | + 1).
93

fiEngel & Wellman

Note Zx denotes minimal set attributes (variables) renders rest
CUI x. bound dimensionality obtained regardless variable
ordering. expect maximal dimension lower network constructed
using good variable ordering. good heuristic determining ordering would
use attributes smaller dependent sets first, attributes dependents
would descendants. Based ordering would expect
less important attributes lower topology, crucial attributes
would either present higher larger number parents.
point usually omit third argument referring CUI condition,
CUI(X, ), taken equivalent CUI(X, | \ (X )).
order achieve low dimensional CUI networks, required detect CUI
conditions large sets. may difficult task, address
example Section 4. task made somewhat easier fact set
CUI single variable; note condition CU I(Y, x) weaker condition
CU I(Y, X) x X. Furthermore, Section 7 shows dimensionality
reduced initial CUI decomposition sufficiently effective.
Based properties CUI, read additional independence conditions
graph. First, observe CUI composition property second argument.
Lemma 3. Let CUI(X, ) [X, S], CUI(A, B) [A, B S].
CUI(A X , B).
property leads following claim, allows us derive additional CUI
conditions graph constructed.
Proposition
4. Consider
CUI network set attributes S. Define P a(X) =

P
a(x)

Dn(X)
=
xX Dn(x). X S,
xX
CUI(S \ (X P a(X) Dn(X)) , X).
Proof. recursion X, using Lemma 3 Proposition 1.
consider direction, defining set nodes renders set CUI
rest. dual perspective becomes particularly useful optimization (Section 6),
optimization based preference order attribute meaningful
holding enough attributes fixed make CPI CUI rest. Let Ch(X)
denote union children nodes X, let An(X) denote ancestors
nodes X, cases excluding nodes X.
Proposition 5. Consider CUI network set attributes S. CUI(X, \(X An(X)
Ch(X))) X S.
Proof. Let
/ X Ch(X) An(X). clearly x X, x
/ P a(y) Dn(y). Hence
Proposition 1, CUI(X, y). apply Lemma 3 iteratively
/ X Ch(X) An(X)
(note first argument X CUI condition, X result well),
get desired result.
conclude section relating CUI networks CAI maps.
94

fiCUI networks

Proposition 6. Let G = (X, E) CAI map, x1 , . . . , xn ordering nodes
X. Let G0 = (X, E 0 ) DAG directed arc (xi , xj ) E 0 iff < j
(xi , xj ) E. G0 CUI network.
note, however, CAI maps decompose utility function maximal
cliques, whereas CUI networks decompose nodes parents. Section 7 bridges
gap. addition, result used Section 6.3.

4. CUI Modeling Example
demonstrate potential representational advantage CUI networks require domain difficult simplify otherwise. example use choice software
package enterprise wishes automate sourcing (strategic procurement) process. focus softwares facilities running auction RFQ (request quotes)
events, tools select winning suppliers either manually automatically.
identified nine key features kinds software packages. choice scenario,
buyer evaluates package nine features, graded discrete scale (e.g.,
one five). features are, brief:
Interactive Negotiations (IN ) allows separate bargaining procedure supplier.
Multi-Stage (MS ) allows procurement event comprised separate stages different types.
Cost Formula (CF ) buyers formulate total cost business
supplier.
Supplier Tracking (ST ) allows long-term tracking supplier performance.
MultiAttribute (MA) bidding multiattribute items, potentially using scoring function.1
Event Monitoring (EM ) provides interface running events real-time graphical
views.
Bundle Bidding (BB ) bidding bundles goods.
Grid Bidding (GB ) adds bidding dimension corresponding aspect time
region.
Decision Support (DS ) tools optimization aiding choice best
supplier(s).
observe first additive independence widely apply domain.
example, Multi-Stage makes several features useful important: Interactive
Negotiations (often useful last stage), Decision Support (to choose suppliers
1. hope fact software may include facilities multiattribute decision making
cause undue confusion. Naturally, consider important feature.

95

fiEngel & Wellman

proceed next stage), Event Monitoring (helps keep track useful
stage reducing costs). Conversely, circumstances Multi-Stage substitute
functionality features: MultiAttribute (by bidding different attributes different stages), Bundle Bidding (bidding separate items different stages), Grid Bidding
(bidding different time/regions different stages) Supplier Tracking (by extracting
supplier information Request Information stage). potential dependencies
attribute shown Table 3.
Attr
EM

CF
ST

MS
DS
GB
BB

Complements
CF ST MS
ST MS
EM MS DS GB BB
EM MS DS
DS CF
DS EM ST
CF GB ST BB MS
CF DS
CF DS

Substitutes

DS

MS BB ST GB
GB BB CF
MS BB
MS GB

CUI set
IN,DS,MA,GB,BB
EM,CF,ST,DS,GB,BB
MA,GB,BB
IN,CF,GB,BB
GB,BB
MA,GB,BB
IN,EM
MA,BB
MA,GB

Table 3: Dependent independent sets attribute.
presence complement substitute relation precludes additive independence.
fact identify set six attributes must mutually (additive) dependent: {BB , GB , DS , MA, MS , CF }. consequence, best-case dimensionality achieved
CAI map (and CAI-based representations, see Section 2.2), domain would
six, size largest maximal clique.
order construct CUI network first identify, attribute x, set
CUI it. first guess set according complement/substitute information
Table 3; typically, set attributes neither complements substitutes would
CUI. approach taken attributes EM DS . However, attributes
complements substitutes may still CUI other, therefore attempt
detect verify potentially larger CUI sets. Keeney Raiffa (1976) provide several
useful results help detection UI, results generalized CUI.
particular show first detect conditional preferential independence
(CPI) condition one element CUI. Based result, order verify
example
CUI({BB , GB , MA}, CF | \ {BB , GB , MA, CF }),

(12)

following two conditions sufficient:
CPI({BB , GB , MA}, CF | \ {BB , GB , MA, CF }),

(13)

CUI(BB , {GB , MA, CF } | \ {BB , GB , MA, CF }).

(14)

Detection verification conditions discussed Keeney Raiffa (1976).
example, observe features BB , GB , add qualitative
96

fiCUI networks

element bidding. bidding element best exploited cost formulation
available, complements CF . complementarity similar feature, thus
implying (13). Moreover, BB crucial feature therefore risk attitude towards
expected vary level CF , MA, GB , implies (14), together
leading (12).
similar fashion, observe nature substitutivity three
mechanisms BB , GB , MS similar: simulated using multiple stages.
means tradeoffs among three depend MS , meaning
CPI({BB , GB , MA}, MS ) holds. Next, dependency among triplet {BB , GB , MA}
result option substitute one another. result, pair CPI
third. Finally, find complementarity ST marginal
affect tradeoffs attributes. therefore verify following conditions:
CUI({BB , GB , MA}, MS ), CUI({BB , GB }, MA), CUI({ST , EM , CF , DS , GB , BB }, ),
CUI({GB , BB , CF , }, ST ). resulting maximal CUI sets attribute
shown Table 3.
construct network start variable largest CUI set, ,
needs MS parents, EM gets CF , MS , ST
parents. Next, consider ST needs four attributes conditional set, EM
descendant, therefore DS , MS , needed parents. next variable
choose MS , needs CF DS parents since dependant variables
descendants. chosen CF MS would needed four parents: , MS ,
ST , DS (note although CUI CF set {BB , GB , MA},
case union {BB , GB , MA, }). choose CF MS
MS , ST , descendants therefore DS parent. complete variable
ordering , EM , ST , MS , CF , DS , MA, GB , BB , resulting CUI network
depicted Figure 2. maximal dimension four.
structure obtained utility function example based largely
objective domain knowledge, may common various sourcing departments.
demonstrates important aspect graphical modeling captured CUI networks:
encoding qualitative information domain, thus making process extracting
numeric information easier. structure cases differs among decision makers,
cases (as above) makes sense extract data domain experts
reuse structure across decision makers.

5. Representation Elicitation
section, derive expression local node data terms conditional utility functions, discuss elicit utility information judgments relative
preference differences.
5.1 Node Data Representation
Representing U CUI network requires determine f g functions
CUI condition. node functions f, g represent affine transformation
conditional utility function U (x0 , Y, Z) (here Z = P a(x)) strategically equivalent utility
functions values x. transformation functions UI (Keeney & Raiffa,
97

fiEngel & Wellman

Figure 2: CUI network example. maximal number parents 3, leading
dimension 4.

1976), transformation functions CUI represented terms conditional
utility functions U (x, 1 , Z) U (x, 2 , Z) suitable values 1 2 (see below).
determine f g solving system two equations below, based
applying (1) specific values :
U (x, 1 , Z) = f (x, Z) + g(x, Z)U (x0 , 1 , Z),
U (x, 2 , Z) = f (x, Z) + g(x, Z)U (x0 , 2 , Z),
yielding
U (x, 2 , Z) U (x, 1 , Z)
,
U (x0 , 2 , Z) U (x0 , 1 , Z)
f (x, Z) = U (x, 1 , Z) g(x, Z)U (x0 , 1 , Z).
g(x, Z) =

(15)
(16)

restriction choice 1 , 2 decision maker must
indifferent given x0 current assignment Z. example, 1 , 2
may differ single attribute strictly essential.
5.2 Elicitation Measurable Value Functions
utility function used choosing action leads known probability
distribution outcomes, obtained elicitation preferences
lotteries, example using even-chance gambles certainty equivalents (Keeney &
Raiffa, 1976). Based preceding discussion, fully specify U () via CUI network,
need obtain numeric values conditional utility functions U (x, 1 , P a(x))
U (x, 2 , P a(x)) node x. significantly easier obtaining full
n-dimensional function, general done using methods described preference
98

fiCUI networks

elicitation literature (Keeney & Raiffa, 1976). section show elicitation
conducted cases choice assumed done certain outcomes,
cardinal representation nevertheless useful.
particular applications point specific attributes used
measurement others. common example preferences quasi-linear
special attribute money time. kind preferences represented
measurable value function, MVF (Krantz et al., 1971; Dyer & Sarin, 1979). MVF
cardinal utility function defined certainty represents preference differences.
shown (Dyer & Sarin, 1979) UI analogous interpretation MVF
similar resulting decomposition. extension CUI straightforward.
case monetary scaling, preference difference pair outcomes
represents difference willingness pay (wtp) each. potential way elicit
MVF asking decision maker provide wtp improve one outcome
another, particularly outcomes differ single attribute.
interpretation, first observe (15) g(x, Z) elicited terms
preference differences, outcomes possibly differ single attribute.
result convey qualitative preference information. Assume 2 1 x.x0 x.
g(x, Z) ratio preference difference 1 2 given x
difference given x0 (Z fixed outcomes). Hence, x complements
g(x, Z) > 1 increasing x. x substitutes, g(x, Z) < 1 decreasing
x. holds regardless choice 1 , 2 , since CUI(Y, x | Z) attributes
maintain complementarity substitutivity relationship x. Note
g(x, Z) = 1 iff CAI(Y, x | Z). Another important observation though x
may depend Z, practice expect level dependency x
depend particular value Z. case g becomes single-dimensional function,
independent Z.
f (x, Z), intuitively speaking, measurement wtp improve x0 x.
value U (x0 , 1 , Z) multiplied g(x, Z) compensate interaction
x, allowing f () independent . perform elicitation obeying
topological order graph, function U (x0 , 1 , Z) readily calculated
new node data stored predecessors. Choose 1 = 0 , let Z = {z1 , . . . , zk },
ordered children precede parents. Since Y, x fixed reference point,
k
i1
X

U (x , , Z) =
(fzi
gzj )fn+1 ().
0

0

i=1

j=1

obtain f (x, Z) follows: first elicit preference difference function
e(x, Z) = U (x, 1 , Z) U (x0 , 1 , Z). Then, assuming g(x, Z) already obtained, calculate:
f (x, Z) = e(x, Z) (g(x, Z) 1)U (x0 , 1 , Z).

6. Optimization
One primary uses utility functions support optimal choices, selecting
outcome action. complexity choice depends specific properties
99

fiEngel & Wellman

environment. choice among limited set definite outcomes, recover
utility outcome using compact representation choose one
highest value. instance, software example Section 4 would normally choose
among enumerated set vendors packages. procurement scenario assume
utility MVF, usually choose outcome yields highest utility
net price. case decision uncertainty, choice among actions
lead probability distributions outcomes, optimal choice selected computing
expected utility action. action involves reasonably bounded number
outcomes non-zero probability, done exhaustive computation.
Nevertheless, often useful directly identify maximal utility outcome given
quantitative representation utility. case direct choice constrained outcome
space, optimization algorithm serves subroutine systematic optimization procedures, adapted probabilistic reasoning literature (Nilsson, 1998).
algorithm may useful heuristic aid optimization expected utility
net utility mentioned above, set possible outcomes large explicit,
exhaustive choice.
section, develop optimization algorithms discrete domains, show
many cases CUI networks provide leverage optimization CAI maps.
typical graphical models, optimization algorithm particularly efficient
graph restricted tree.
6.1 Optimization CUI Trees
Definition 9. CUI tree CUI network node one child.
Note type graph corresponds upside-down version standard directed tree (or forest).
Let CUI tree. assume WLOG connected (a forest turned
tree adding arcs). upside-down sort tree, number roots,
single leaf. denote root nodes ai {a1 , . . . , ak }, child ai bi ,
on. root node ai , define function
hai (bi ) = arg 0 max U (bi , a0i ),
ai D(ai )

denoting selection optimal value ai corresponding given value child.
Proposition 5, hai depend reference values chosen \ {ai , bi }.
function hai (), call optimal value function (OVF) ai , stored node ai
since used descendants described below.
Next, bi children single child ci , number parents. simplicity exposition present case bi two parents, ai aj . maximization
function bi defined
hbi (ci ) = arg 0 max U (ci , b0i , hai (b0i ), haj (b0i )).
bi D(bi )

words, pick optimal value bi assignment child parents.
since already know optimum parents value bi , need
consider optimum evaluation domain bi .
100

fiCUI networks

(a)

(b)

Figure 3: CUI networks optimization examples: (a) Tree (b) Non-tree
external child set {ai , aj , bi } ci , external ancestors,
hence {ai , aj , bi } CUI rest given ci , therefore maximization
depend reference values rest attributes. Similarly, computing
hci (di ) child ci bi , value ci fixes bi (and parents ci ),
fixes ai aj (and ancestors ci ). last computation, leaf x, evaluates
value x. value x0 causes cascade fixed values ancestors,
meaning finally get optimal choice comparing |D(x)| complete assignments.
illustrate execution algorithm CUI tree Figure 3a. compute
ha (c) optimal value value c, similarly hb (c). Next,
compute hc (e), value e0 e compare outcomes (e0 , c0 , ha (c0 ), hb (c0 )), c0 D(c).
node compute hd (f ), independent nodes. node e compute
(f ) = arg maxe0 U (f, e0 , hc (e0 ), hb (hc (e0 )), ha (hc (e0 )) (node ignored here)
node f
hf () = arg max U (f 0 , (f 0 ), hd (f 0 ), hc (he (f 0 )), hb (hc (he (f 0 ))), ha (hc (he (f 0 ))).
f 0 D(f )

Note candidate value f causes cascade optimal values
ancestors. solution hf () resulting values ancestors.
optimization algorithm iterates nodes topological order, xi
calculates OVF hxi (xj ), xj child xi . calculation uses values
OVF stored parents, therefore involves comparison |D(xi )||D(xj )|
outcomes. case numeric data nodes available, factoring time takes
recover utility value outcome (which O(n)), algorithm runs time
O(n2 maxi |D(xi )|2 ).
6.2 Optimization General DAGs
common way graphical models apply tree algorithms non-trees using
junction graph. However, common notion junction graph DAG polytree,

101

fiEngel & Wellman

whereas algorithm specialized (unit) tree. Instead, optimize CUI
network directly generalizing tree algorithm.
tree case, fixing value child node x sufficient order separate
x rest graph, excluding ancestors. consider value child
time, determines values ancestors. general DAG longer
sufficient OVF depend children, provide sufficient
information determine values An(x). Hence generalize notion
scope x (Sc(x), defined below), set nodes OVF x must
depend, order iterative computation OVF sound.
generalization, DAG algorithm similar tree algorithm. Let G
CUI network, x1 , . . . , xn variable ordering agrees topological order
G (parents precede children). xi (according ordering), compute hxi (Sc(xi ))
instantiation Sc(xi ). optimal instantiation selected backwards
hxn (), since node xi reached values Sc(xi ) already selected.
Sc(xi ) computed follows: scan variables xi+1 , . . . , xn order. scanning
xj , add xj Sc(xi ) following conditions hold:
1. undirected path xj xi .
2. path blocked node already Sc(xi ).
conditions, Sc(xi ) includes children xi , non xi ancestor since
precede xi ordering. addition, Sc(xi ) includes nodes needed
block paths reach xi ancestors. example, xk , xj children
ancestor xa xi , k < < j, xj must Sc(xi ), path
xa . children xj blocked xj , unless another path xi
Sc(xi ). children xk , ordered later xi , Sc(xi ) (but
children not), on.
Figure 3b example CUI network tree. consider scopes
variable ordering a, b, . . . , j. scope roots always equals set children
(because path reaching them), meaning Sc(a) = {d, e}, Sc(b) = {d, e, f },
Sc(c) = {e, f, h}, Sc(i) = {j}. scope must include child g siblings e
f . paths h, j, blocked g, e, f therefore Sc(d) = {g, e, f }. e,
must include child g, younger sibling f . h blocked path e
f Sc(e), non-blocked one c
/ Sc(e), therefore Sc(e) = {g, f, h}.
Similarly, g h scope f due paths b c respectively, hence
Sc(f ) = {g, h, j}. g, addition child h add j whose path g f, b, e
blocked (Sc(g) = {h, j}) finally Sc(h) = Sc(i) = {j} Sc(j) = {}.
next step, computing OVF, requires compare set outcomes
differ xi Co(xi ), Co(xi ) set nodes whose OVF determined
xi Sc(xi ) (hence covered xi ). maximization valid, condition
CUI(xi Co(xi ), \ (xi Co(xi ) Sc(xi ))) must hold. formally define Co(xi ),
establish result proved appendix.
Definition 10. Co(xi ) smallest set nodes satisfied following condition
j < i, Sc(xj ) ({xi } Sc(xi ) Co(xi )) xj Co(xi ).
102

(17)

fiCUI networks

Intuitively, xj covered xi node xk 6= xi scope, either scope xi
determined (according scope) covered xi . Figure 3b, f Co(g)
Sc(f ) = {g} Sc(g). e Co(g) Sc(e) {g} Sc(g) {f }. Moreover,
Sc(d) = {g, e, f } hence Co(g) well, similarly find a, b Co(g).
example nodes preceding g ordering covered, necessarily
always case.
Lemma 7. assignment xi Sc(xi ) sufficient determine hxj () xj
Co(xi ).
Lemma 8. node xi , CUI({xi } Co(xi ), \ ({xi } Co(xi ) Sc(xi ))). Meaning
xi nodes covers CUI rest given Sc(xi ).
algorithm reaches node xi , every choice assignment Sc(xi ) {xi } determines optimal values Co(xi ) (Lemma 7). compare |D(xi )| assignments
differ values xi Co(xi ), select optimal one value hxi (Sc(xi )).
optimum depend nodes \({xi }Co(xi )Sc(xi ))) due Lemma 8.
illustrate, examine happens algorithm reaches node g Figure 3b.
point hx (Sc(x)) known x precedes g. showed, nodes
Co(g). Indeed, assignment Sc(g) = {g, h, j} directly determines value
hf (), together hf () determines value (), cascades
rest nodes. CUI network shows CUI({a, b, c, d, e, f, g}, {i}) (given
{h, j}) therefore maximization operation (over choice value g) valid
regardless value i.
performance optimization algorithm exponential size largest
scope (plus one). Note would seriously affected choice variable ordering. note, case tree algorithm specializes tree optimization
above, since node path ancestor xi , except ancestors xi must precede xi ordering. Therefore always case
Sc(xi ) = Ch(xi ), meaning hxi () function single child. Based that,
expect algorithm perform better similar CUI network tree.
6.3 CUI Tree Optimization CAI Maps
optimization procedure CUI trees particularly attractive due relatively
low amount preference information requires. cases comparison
done directly, without even data comprises utility function. Aside
direct benefit CUI networks, interested applying structure
optimization CAI maps. domains CAI map simple effective way
decompose utility function. However, optimization CAI maps exponential
size tree width, requires full data terms utility functions
maximal cliques. CAI map happens simple structure, tree,
CP condition, faster optimization algorithms used. However, could case
CAI map tree, subtle CUI conditions might exist cannot
captured CAI conditions. enough conditions could detected turn CAI
map CUI tree (or close enough tree), could take advantage simple
optimization procedure.
103

fiEngel & Wellman

(a)

(b)

(c)

Figure 4: (a) CAI map containing cycle. (b) Enhanced CAI map, expressing CUI
{a, d, f } b. (c) equivalent CUI tree.
Definition 11. Let G = (V, E) CAI map. enhanced CAI map directed graph
G0 = (V, A), pair arcs (u, v), (v, u) implies dependency
edge (u, v) E, addition node x, CUI(S \ ({x} In(x)), x) (In(x) denoting
set nodes (y, x) A). call pair arcs (u, v), (v, u) hard
link arc (u, v) s.t. (v, u)
/ weak link.
CAI map, enhanced CAI map generated replacing edge (u, v)
arcs (u, v) (v, u). require additional CUI conditions
entailed CAI map. However, additional CUI conditions
detected, might able remove one (or both) directions. Figure 4a shows
CAI map contains cycle. could detect CUI({a, d, f }, b), could remove
direction (a, b) get enhanced CAI map Figure 4b. set CUI conditions
implied enhanced CAI map expressed CUI tree, Figure 4c.
Proposition 9. Consider enhanced CAI map G. Let ordering nodes
G, G0 DAG result removing arcs (u, v) whose direction
agree . removed arc, v ancestor u G0 , G0 CUI
network.
hard links, removal (u, v) leaves v parent u, condition trivially
holds. obtain CUI tree, key therefore find variable ordering
enough weak links removed turn graph tree, maintaining condition
Proposition 9. large number variables, exhaustive search variable orderings
may feasible. However many cases effectively constrained, restricting
number orderings need consider. example, order break cycle
Figure 4b clear weak link (b, a) must implied ordering,
could ancestor b. way happen (given existing hard links),
c parent b, parent c, parent d.
Proposition 10. Let c = (y1 , . . . , yk ) cycle enhanced CAI map G. Assume
c contains exactly one weak link: (yi , yi+1 ) < k, (yk , y1 ). Let variable
104

fiCUI networks

ordering agree order path p = (yi+1 , yi+2 , . . . , yk , y1 , . . . , yi ).
CUI network constructed G (by Proposition 9), tree.
Therefore cycle contains one weak link leads constraint variable
ordering. Cycles one weak link lead constraints. c
another weak link (yj , yj+1 ), one two links must removed, ordering must
agree either path p path p0 = (yj+1 , yj+2 , . . . , yk , y1 , . . . , yj ). Assuming
WLOG j > i, paths (yi+1 , . . . , yj ) (yj+1 , . . . , yi ) required p
p0 , therefore used constraints. Similarly find intersection
paths implied number weak links cycle.
Sometimes constraint set lead immediate contradiction, case
search redundant. not, significantly reduce search space. However,
major bottleneck preference handling usually elicitation, rather computation.
Therefore, given good variable ordering may lead reduction optimization
problem simpler, qualitative task, eliminating need full utility elicitation,
would worthwhile invest required computation time.

7. Nested Representation
Section 5.1 conclude node data represented conditional utility functions depending node parents. may best dimensionality
achieved network. Perhaps set Z = P a(x) internal structure,
sense subgraph induced Z maximal dimension lower |Z|.
case could recursively apply CUI decomposition conditional utility functions
subgraph. approach somewhat resembles hierarchical decomposition done
utility trees (Keeney & Raiffa, 1976; Von Stengel, 1988). example, represent f1
network Figure 1, require conditional utility function U (x1 , x14 , x15 , x16 , x2 , x3 ).
However network see CUI(x3 , x2 | x1 , x4 , x5 , x6 ). Hence decompose conditional utility:
U (x1 , x14 , x15 , x16 , x2 , x3 ) = f 0 (x1 , x2 ) + g 0 (x1 , x2 )U (x1 , x3 , x02 , x14 , x15 , x16 ).
use notation f 0 g 0 since f g functions
top level decomposition.
nested representation generated systematically (Algorithm 1), decomposing
local function node x (xs utility factors) whose argument set Z P a(x)
form clique. performing complete CUI decomposition subgraph
induced Z (keeping mind resulting factors depend x).
Proposition 11. Let G CUI network utility function U (S). U (S)
represented set conditional utility functions, depending set attributes
corresponding (undirected) cliques G.
7.1 Discussion
result reduces maximal dimensionality representation size
largest maximal clique CUI network. instance, applying example
105

fiEngel & Wellman

Data: CUI Utility factors U (x, P a(x), ), U (x, P a(x), ) node x
/* note: , D(Y ) */
Determine order x1 , . . . , xn ;
j = 1, . . . , n /* initialization */
Kj1 = {xj } P a(xj ) /* scope utility factors */ ;
Yj1 = \ Kj1 /* rest variables */ ;
Q1j = P a(xj );
A1j = , dj = 1;
end
j = 1, . . . , n
= 1, . . . , dj /* loop factors node j*/
Qij 6= Kji clique
Let Gij subgraph induced Qij ;
Decompose Uji (Kji ) according CUI network Gij ;
foreach xr Qij
Let dr = dr + 1 (current num. factors xr ) denote = dr ;
Adr = Aij {xj }, Qdr = P a(xr ) Qij ;
Krd = Adr {xr } Qdr , Yrd = \ Krd ;
Store new CUI factors xr : U (Krd , Yrd ), U (Krd , Yrd );
/*Yrd , Yrd fixed assignments Yrd */
end
Remove factors U (Kji , Yji ), (Kji , Yji );
end
end
end
Algorithm 1: Recursive CUI decomposition. Process node reverse topological
order (outermost loop). Decompose factor stored current node, whose parents
form clique. parent xr (innermost loop) store resulting new
factors. defined xr , xr parents P a(xi ) (this
Qdr ), clique Adr original factor depends. time factor
decomposed set Q shrinks. empty, K clique.

106

fiCUI networks

Section 4 reduces dimensionality four three. important implication
somewhat relax requirement find large CUI sets. variables end
many parents, reduce dimensionality using technique. example
illustrates, technique aggregates lower order CUI conditions effective
decomposition.
procedure may generate complex functional form, decomposing function multiple
times factors become restricted clique. ultimate number factors
required represent U (S) exponential number nesting levels. However,
decomposition based CUI network subgraph, therefore typically
reduces number entries maintained.
expect typical application technique composition rather
decomposition. execute Algorithm 1 without actual data, resulting list factors
per node (that conditional utility functions cliques graph). means
elicitation purposes restrict attention conditional utility functions
maximal cliques. obtained, sufficient data factors.
recover original, convenient CUI-network representation function
store (more example below). Therefore, effective dimensionality
elicitation maximal cliques. storage efficient usage requires
potentially higher dimension original CUI network, typically less
concern.
result Proposition 6, CUI networks shown always achieve weakly
better dimensionality CAI maps, since representations reduce dimensionality
size maximal clique.
7.2 Example
illustrate result using simple example. Consider domain four attributes
(a, b, c, d), following CUI conditions:
CUI(b, c), CUI(c, b), CUI(d, a)
CUI network corresponding variable ordering a, b, c, depicted Figure 5.
Since CUI sets small (a single variable each), variable ordering must
node two parents, meaning dimensionality three. nesting operation
combines lower order conditions reduce dimensionality two.
Initially, utility function represented using conditional utility functions listed
according corresponding nodes column Level 0 Table 4. remove
three-dimensional factors, need decompose functions node according
CUI network {b, c}, contains arcs. proceeds follows:
U (a, b, c, d1 ) =
fb1 (a, c) + gb1 (a, c)U (a, b, c0 , d1 ) = fb1 (a, c) + gb1 (a, c)(fc1 (a, b) + gc1 (a, b)U (a, b0 , c1 , d1 ))
U (a, b, c, d2 ) =
fb2 (a, c) + gb2 (a, c)U (abc0 d2 ) = fb2 (a, c) + gb2 (a, c)(fc2 (a, b) + gc2 (a, b)U (a, b0 , c0 , d2 ))
107

fiEngel & Wellman

Figure 5: Nesting example

resulting functions fbi (a, c), gbi (a, c), fci (a, b), gci (a, b), = 1, 2. functions
gbi (a, c) represented using conditional utility functions U (a, b1 , c, di )
U (a, b2 , c, di ), similarly two functions. delete factors a,
U (a, b, c, di ), add new lower dimensional factors second column parents
b c. Though multiply number factors store four, new
factors conditional utility functions subdomains (deleted) higher dimensional
factors. algorithm continues node b, loops six factors. factors
defined set parents b clique decomposes store
new factors next table column. case factor column level 1 could
decomposed, would add level 2 column store result. simple example
decomposition possible.
fbi (a, c)

Attr

b
c


Level 0 (CUI net)
U (a, b, c, d1 ), U (a, b, c, d1 )
U (a0 , b, c1 , d),
U (a0 , b, c2 , d)
U (a0 , b1 , c, d),
U (a0 , b2 , c, d)
U (a0 , b0 , c0 , d)

Level 1
U (a, b, c1 , d1 ),
U (a, b, c1 , d2 ),
U (a, b1 , c, d1 ),
U (a, b1 , c, d2 ),

U (a, b, c2 , d1 )
U (a, b, c2 , d2 )
U (a, b2 , c, d1 )
U (a, b2 , c, d2 )

Table 4: Nested CUI decomposition
reverse direction mentioned done follows: run Algorithm 1 without
data, resulting table Table 4 (without actual utility values).
elicit data non deleted factors (all limited maximal cliques). Next,
recover convenient level 0 CUI representation using table, computing
deleted factor (going rightmost columns left) function factors
stored parents.

8. Conclusions
present graphical representation multiattribute utility functions, based conditional utility independence. CUI networks provide potentially compact representation
multiattribute utility function, via functional decomposition lower-dimensional functions depend node parents. CUI weaker independence condition
108

fiCUI networks

previously employed basis graphical utility representations, allowing common
patterns complementarity substitutivity relations disallowed additive models.
proposed techniques obtain verify structural information, use construct network elicit numeric data. addition, developed optimization
algorithm performs particularly well special case CUI trees. cases
leveraged efficient optimization CAI maps. Finally, show functions
decomposed set maximal cliques CUI network.
technique, CUI networks achieve dimensionality graphical models based
CAI GAI decompositions, yet broadly applicable independence conditions.

Acknowledgments
preliminary version paper published proceedings AAAI-06.
work supported part NSF grant IIS-0205435, STIET program NSF
IGERT grant 0114368. grateful thorough work anonymous reviewers,
whose suggestions provided valued help finalizing paper.

Appendix A. Proofs
A.1 Lemma 3
Proof. Let Z = \ (X ) C = \ (A B). simply apply two independence
conditions consequentially, define f, g that:
U (S) = U (XY Z) = f (Y Z) + g(Y Z)UY (S \ ) = f (Y Z) + g(Y Z)(f 0 ((BC) \ )
+ g 0 ((BC) \ )UY B (S \ (Y B))) = f(ZBY C) + g(ZBY C)U (S \ (Y B)).
Since Z B C = \(AX), last decomposition equivalent decomposition
(1) condition CUI(A X , B).
A.2 Proposition 6
Proof. CAI condition stronger CUI condition, CAI(x, y) CUI(x, y)
CUI(y, x). CUI network, node xi must case nodes
CUI given parents descendants. obvious since xi CAI
nodes given parents children.
A.3 Lemma 7
Proof. determine hxj (), Sc(xj ) needs determined. {xi } Sc(xi )
done, scope covered therefore recursively determined
assignment {xi } Sc(xi ).
A.4 Lemma 8
first introduce two additional lemmas.
Lemma 12. Ch({xi } Co(xi )) ({xi } Sc(xi ) Co(xi ))
109

fiEngel & Wellman

Proof. Let xj {xi } Co(xi ), Ch(xj ). xj = xi proof immediate
Ch(xi ) Sc(xi ). Assume xj Co(xi ). know Definition 10 Ch(xj )
Sc(xj ) ({xi } Sc(xi ) Co(xi )), proves lemma.
Lemma 13. An({xi } Co(xi )) Co(xi )
Proof. Let xj An(xi ) (clearly j < i, therefore xj
/ Sc(xi )). Let xj1 Sc(xj ).
j1 > j undirected path xj1 xj , blocked Sc(xj ). j1 i,
xj1 Sc(xi ){xi } unblocked path xj (and xi ). Otherwise,
let xj2 Sc(xj1 ), apply argument xj2 . continue xjk
xy Sc(xjk ), > point xy Sc(xi ) {xi } path xy , xjk , . . . , xj1 , xj , xi
recursion halts (note includes empty scopes), proving xj Co(xi ).
left prove An(Co(xi )) Co(xi ). Let xj Co(xi ), An(xj ). Applying
first part proof xj , get Co(xj ). Definition Co(xj ), get
< j w Sc(y), either w = xj , w Sc(xj ) w Co(xj ). show Co(xi ),
need prove cases w {xi } Sc(xi ) Co(xi ).
1. w = xj immediately w Co(xi ).
2. w Sc(xj ), xj Co(xi ) get either w = xi , w Sc(xi ) w Co(xi ).
3. w Co(xj ), repeat argument recursively z Sc(w). Note z precedes
w therefore recursion halt point.

Lemma 8. Let X = {xi } Co(xi ). Lemma 13, X external ancestors.
Lemma 12, external children X Sc(xi ). Therefore \ (X An(X) Ch(X)) =
\ (X Sc(xi )) result immediate Proposition 5.
A.5 Proposition 9
Proof. Let x node G0 . Let = \ (x In(x)) G = \ (x P a(x) Dn(x))
G0 . definition G, know CAI(Y, x), CUI(Y, x). Let
/ Y, 6= x (so
0
In(x) G). ,
/ P a(x) = In(x) G . arc (y, x) removed,
meaning Dn(x). therefore must case
/ . Therefore hence
CUI(Y , x).
A.6 Proposition 10
Proof. G become CUI tree, cycle least one weak link must removed.
Since (yi , yi+1 ) weak link c, must removed. Proposition 9,
variable ordering must ensure yi+1 ancestor yi . done
path according order p, might another path yi+1 yi . Let p1
path. combination p1 p another cycle c1 , therefore must
broken. Since p comprises strong links, must least one weak link (u, v)
p1 . (u, v) removed, v must ancestor u. done
path cycle c1 , path includes p, another path exists,
repeat argument. stage get larger cycle ci , larger path
110

fiCUI networks

pi pi1 . Therefore point one path pi must guaranteed
variable ordering, path includes p.
A.7 Proposition 11
Proof. show Algorithm 1 leads functional decomposition cliques.
outer loop algorithm maintains following iteration properties:
1. Aij , Qij P a(a)
2. Uij defined Kji
3. Aij xj clique
properties hold trivially initialization. Assume valid factors
stored network outer iteration j inner iteration i, next show
remain valid factor Urd created iteration j, i:
1. definition Adr = Aij xj . previous iteration definition Qdr ,
Aij , Qdr Qij P a(a). definitions Qij Qdr get P a(xj ) Qij Qdr ,
together yields result.
2. Urd factor CUI decomposition Uji (Kji ) Gij . scope contains: (i)
nodes affected last CUI decomposition, i.e. Kji \ Qij =
Aij xj = Adr , (ii) node xr , (iii) parents P a(xr ) fixed
Uji (i.e. P a(xr Kji )). know Kji = Aij xj Qij , xj
/ P a(xr ) (because

xr P a(xj )), P a(xr ) Aj = (using similar argument property 1).
Therefore (P a(xr ) Kji ) Qij , (i),(ii),(iii) get Krd = Adr xr Qdr .
3. Adr clique definition property previous iteration. xr Qij ,
therefore property 1 previous iteration xr P a(a) Aij . xr
Qij P a(xj ) (the last containment immediate definition Qij ). Therefore xr
parent members Adr , result Adr xr clique.
iteration properties, either Krd clique, Qdr non empty decomposition
applied reach node r outer loop. end process factors
defined cliques removed. factors remained defined
cliques. U (S) still represented new set factors since applied
valid decompositions factors.

References
Abbas, A. (2005). Attribute dominance utility. Decision Analysis, 2, 185206.
Bacchus, F., & Grove, A. (1995). Graphical models preference utility. Eleventh
Conference Uncertainty Artificial Intelligence, pp. 310, Montreal.
Boutilier, C., Bacchus, F., & Brafman, R. I. (2001). UCP-networks: directed graphical
representation conditional utilities. Seventeenth Conference Uncertainty
Artificial Intelligence, pp. 5664, Seattle.
111

fiEngel & Wellman

Boutilier, C., Brafman, R. I., Hoos, H. H., & Poole, D. (1999). Reasoning conditional ceteris paribus preference statements. Fifteenth Conference Uncertainty
Artificial Intelligence, pp. 7180, Stockholm.
Debreu, G. (1959). Topological methods cardinal utility theory. Arrow, K., Karlin, S.,
& Suppes, P. (Eds.), Mathematical Methods Social Sciences. Stanford University
Press.
Dyer, J. S., & Sarin, R. K. (1979). Measurable multiattribute value functions. Operations
Research, 27, 810822.
Fishburn, P. C. (1965). Independence utility theory whole product sets. Operations
Research, 13, 2845.
Fishburn, P. C. (1967). Interdependence additivity multivariate, unidimensional
expected utility theory. International Economic Review, 8, 335342.
Fishburn, P. C. (1975). Nondecomposable conjoint measurement bisymmetric structures.
Journal Mathematical Psychology, 12, 7589.
Fuhrken, G., & Richter, M. K. (1991). Polynomial utility. Economic Theory, 1 (3), 231249.
Gonzales, C., & Perny, P. (2004). GAI networks utility elicitation. Ninth International
Conference Principles Knowledge Representation Reasoning, pp. 224234,
Whistler, BC, Canada.
Gorman, W. M. (1968). structure utility functions. Review Economic Studies,
35, 367390.
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences
Value Tradeoffs. Wiley.
Krantz, D. H., Luce, R. D., Suppes, P., & Tversky, A. (1971). Foundations Measurement,
Vol. 1. Academic Press, New York.
La Mura, P., & Shoham, Y. (1999). Expected utility networks. Fifteenth Conference
Uncertainty Artificial Intelligence, pp. 366373, Stockholm.
Nilsson, D. (1998). efficient algorithm finding probable configurations
probabilistic expert systems. Statistics Computing, 8 (2), 159173.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Pearl, J., & Paz, A. (1989). Graphoids: graph based logic reasoning relevance
relations. Du Boulay, B. (Ed.), Advances Artificial Intelligence II. North-Holland,
New York.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming influence diagrams..
20, 365379.
Von Stengel, B. (1988). Decomposition multiattribute expected utility functions. Annals
Operations Research, 16, 161184.
Wellman, M. P., & Doyle, J. (1992). Modular utility representation decision-theoretic
planning. First International Conference Artificial Intelligence Planning Systems, pp. 236242, College Park, MD.

112


