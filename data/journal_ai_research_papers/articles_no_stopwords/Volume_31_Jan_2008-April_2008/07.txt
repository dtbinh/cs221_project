Journal Artificial Intelligence Research 31 (2008) 399-429

Submitted 09/07; published 03/08

Global Inference Sentence Compression
Integer Linear Programming Approach
James Clarke

jclarke@ed.ac.uk

Mirella Lapata

mlap@inf.ed.ac.uk

School Informatics
University Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK

Abstract
Sentence compression holds promise many applications ranging summarization
subtitle generation. work views sentence compression optimization problem
uses integer linear programming (ILP) infer globally optimal compressions
presence linguistically motivated constraints. show previous formulations
sentence compression recast ILPs extend models novel global
constraints. Experimental results written spoken texts demonstrate improvements
state-of-the-art models.

1. Introduction
computational treatment sentence compression recently attracted much attention
literature. task viewed producing summary single sentence
retains important information remains grammatical (Jing, 2000). sentence
compression mechanism would greatly benefit wide range applications. example,
summarization, could improve conciseness generated summaries (Jing, 2000;
Lin, 2003; Zajic, Door, Lin, & Schwartz, 2007). examples include compressing text
displayed small screens mobile phones PDAs (Corston-Oliver, 2001),
subtitle generation spoken transcripts (Vandeghinste & Pan, 2004), producing
audio scanning devices blind (Grefenstette, 1998).
Sentence compression commonly expressed word deletion problem: given input source sentence words x = x1 , x2 , . . . , xn , aim produce target compression
removing subset words (Knight & Marcu, 2002). compression problem extensively studied across different modeling paradigms, supervised
unsupervised. Supervised models typically trained parallel corpus source sentences target compressions come many flavors. Generative models aim model
probability target compression given source sentence either directly (Galley
& McKeown, 2007) indirectly using noisy-channel model (Knight & Marcu, 2002;
Turner & Charniak, 2005), whereas discriminative formulations attempt minimize error
rate training set. include decision-tree learning (Knight & Marcu, 2002), maximum entropy (Riezler, King, Crouch, & Zaenen, 2003), support vector machines (Nguyen,
Shimazu, Horiguchi, Ho, & Fukushi, 2004), large-margin learning (McDonald, 2006).
c
2008
AI Access Foundation. rights reserved.

fiClarke & Lapata

Unsupervised methods dispense parallel corpus generate compressions either
using rules (Turner & Charniak, 2005) language model (Hori & Furui, 2004).
Despite differences formulation, approaches model compression process
using local information. instance, order decide words drop, exploit
information adjacent words constituents. Local models good job
producing grammatical compressions, however somewhat limited scope since
cannot incorporate global constraints compression output. constraints
consider sentence whole instead isolated linguistic units (words constituents).
give concrete example may want ensure target compression verb,
provided source one first place. verbal arguments present
compression. pronouns retained. constraints fairly intuitive
used instill linguistic task specific information model.
instance, application compresses text displayed small screens would
presumably higher compression rate system generating subtitles spoken
text. global constraint could force former system generate compressions
fixed rate fixed number words.
Existing approaches model global properties compression problem
good reason. Finding best compression source sentence given space
possible compressions1 (this search process often referred decoding inference)
become intractable many constraints overly long sentences. Typically,
decoding problem solved efficiently using dynamic programming often conjunction
heuristics reduce search space (e.g., Turner & Charniak, 2005). Dynamic
programming guarantees find global optimum provided principle optimality holds. principle states given current state, optimal decision
remaining stages depend previously reached stages previously made
decisions (Winston & Venkataramanan, 2003). However, know false
case sentence compression. example, included modifiers left
noun compression probably include noun include verb
include arguments. dynamic programming approach cannot
easily guarantee constraints hold.
paper propose novel framework sentence compression incorporates
constraints compression output allows us find optimal solution.
formulation uses integer linear programming (ILP), general-purpose exact framework
NP-hard problems. Specifically, show previously proposed models recast
integer linear programs. extend models constraints express
linear inequalities. Decoding framework amounts finding best solution given
linear (scoring) function set linear constraints either global local.
Although ILP previously used sequence labeling tasks (Roth & Yih, 2004;
Punyakanok, Roth, Yih, & Zimak, 2004), application natural language generation
less widespread. present three compression models within ILP framework,
representative unsupervised (Knight & Marcu, 2002), semi-supervised (Hori & Furui,
2004), fully supervised modeling approach (McDonald, 2006). propose small
number constraints ensuring compressions structurally semantically
1. 2n possible compressions n number words sentence.

400

fiGlobal Inference Sentence Compression

valid experimentally evaluate impact compression task. cases,
show added constraints yield performance improvements.
remainder paper organized follows. Section 2 provides overview
related work. Section 3 present ILP framework compression models
employ experiments. constraints introduced Section 3.5. Section 4.3
discusses experimental set-up Section 5 presents results. Discussion future
work concludes paper.

2. Related Work
paper develop several ILP-based compression models. presenting
models, briefly summarize previous work addressing sentence compression emphasis data-driven approaches. Next, describe ILP techniques used
past solve inference problems natural language processing (NLP).
2.1 Sentence Compression
Jing (2000) perhaps first tackle sentence compression problem. approach
uses multiple knowledge sources determine phrases sentence remove. Central
system grammar checking module specifies sentential constituents
grammatically obligatory therefore present compression.
achieved using simple rules large-scale lexicon. knowledge sources include
WordNet corpus evidence gathered parallel corpus source-target sentence
pairs. phrase removed grammatically obligatory, focus
local context reasonable deletion probability (estimated parallel corpus).
contrast Jing (2000), bulk research sentence compression relies exclusively corpus data modeling compression process without recourse extensive knowledge sources (e.g., WordNet). large number approaches based
noisy-channel model (Knight & Marcu, 2002). approaches consist language
model P (y) (whose role guarantee compression output grammatical), channel
model P (x|y) (capturing probability source sentence x expansion
target compression y), decoder (which searches compression maximizes
P (y)P (x|y)). channel model acquired parsed version parallel corpus;
essentially stochastic synchronous context-free grammar (Aho & Ullman, 1969) whose
rule probabilities estimated using maximum likelihood. Modifications model
presented Turner Charniak (2005) Galley McKeown (2007) improved
results.
discriminative models (Knight & Marcu, 2002; Riezler et al., 2003; McDonald, 2006;
Nguyen et al., 2004) sentences represented rich feature space (also induced
parse trees) goal learn words word spans deleted given
context. instance, Knight Marcus (2002) decision-tree model, compression
performed deterministically tree rewriting process inspired shift-reduce
parsing paradigm. Nguyen et al. (2004) render model probabilistic use
support vector machines. McDonald (2006) formalizes sentence compression largemargin learning framework without making reference shift-reduce parsing. model
compression classification task: pairs words source sentence classified
401

fiClarke & Lapata

adjacent target compression. large number features defined
words, parts-of-speech, phrase structure trees dependencies. features
gathered adjacent words compression words in-between
dropped (see Section 3.4.3 detailed account).
compression models developed written text mind, Hori
Furui (2004) propose model automatically transcribed spoken text. model
generates compressions word deletion without using parallel data syntactic information way. Assuming fixed compression rate, searches compression
highest score using dynamic programming algorithm. scoring function consists language model responsible producing grammatical output, significance score
indicating whether word topical not, score representing speech recognizers
confidence transcribing given word correctly.
2.2 Integer Linear Programming NLP
ILPs constrained optimization problems objective function
constraints linear equations integer variables (see Section 3.1 details). ILP
techniques recently applied several NLP tasks, including relation extraction
(Roth & Yih, 2004), semantic role labeling (Punyakanok et al., 2004), generation
route directions (Marciniak & Strube, 2005), temporal link analysis (Bramsen, Deshpande,
Lee, & Barzilay, 2006), set partitioning (Barzilay & Lapata, 2006), syntactic parsing (Riedel
& Clarke, 2006), coreference resolution (Denis & Baldridge, 2007).
approaches combine local classifier inference procedure based
ILP. classifier proposes possible answers assessed presence global
constraints. ILP used make final decision consistent constraints
likely according classifier. example, semantic role labeling task involves
identifying verb-argument structure given sentence. Punyakanok et al. (2004) first
use SNOW, multi-class classifier2 (Roth, 1998), identify label candidate arguments.
observe labels assigned arguments sentence often contradict other.
resolve conflicts propose global constraints (e.g., argument
instantiated given verb, every verb least one argument) use
ILP reclassify output SNOW.
Dras (1999) develops document paraphrasing model using ILP. key premise
work cases one may want rewrite document conform
global constraints length, readability, style. proposed model three
ingredients: set sentence-level paraphrases rewriting text, set global constraints, objective function quantifies effect incurred paraphrases.
formulation, ILP used select paraphrases apply
global constraints satisfied. Paraphrase generation falls outside scope ILP
model sentence rewrite operations mainly syntactic provided module based
synchronous tree adjoining grammar (S-TAG, Shieber & Schabes, 1990). Unfortunately,
proof-of-concept presented; implementation evaluation module left
future work.
2. SNOWs learning algorithm variation Winnow update rule.

402

fiGlobal Inference Sentence Compression

work models sentence compression optimization problem. show previously proposed models reformulated context integer linear programming
allows us easily incorporate constraints decoding process. constraints linguistically semantically motivated designed bring less local
syntactic knowledge model help preserve meaning source sentence.
Previous work identified several important features compression task (Knight
& Marcu, 2002; McDonald, 2006); however, use global constraints novel
knowledge. Although sentence compression explicitly formulated terms
optimization, previous approaches rely optimization procedure generating
best compression. decoding process noisy-channel model searches best
compression given source channel models. However, compression found usually sub-optimal heuristics used reduce search space locally optimal
due search method employed. example, work Turner Charniak
(2005) decoder first searches best combination rules apply. traverses
list compression rules, removes sentences outside 100 best compressions (according channel model). list eventually truncated 25 compressions.
models (Hori & Furui, 2004; McDonald, 2006) compression score maximized
using dynamic programming however yield suboptimal results (see discussion
Section 1).
Contrary NLP work using ILP (a notable exception Roth & Yih, 2005),
view compression generation two stage process learning inference
carried sequentially (i.e., first local classifier hypothesizes list possible answers best answer selected using global constraints). models integrate
learning inference unified framework decoding takes place presence
available constraints, local global. Moreover, investigate influence
constraint set across models learning paradigms. Previous work typically formulates constraints single model (e.g., SNOW classifier) learning paradigm
(e.g., supervised). therefore assess constraint-based framework advocated
article influences performance expressive models (which require large amounts
parallel data) non-expressive ones (which use little parallel data none all).
words, able pose answer following question: kinds models
benefit constraint-based inference?
work close spirit rather different content Dras (1999). concentrate
compression, specific paraphrase type, apply models sentence-level.
constraints thus affect document whole individual sentences. Furthermore, compression generation integral part ILP models, whereas Dras assumes
paraphrases generated separate process.

3. Framework
section present details proposed framework sentence compression.
mentioned earlier, work models sentence compression directly optimization
problem. 2n possible compressions source sentence many
unreasonable, unlikely one compression satisfactory (Knight & Marcu, 2002). Ideally, require function captures operations
403

fiClarke & Lapata

(or rules) performed sentence create compression
time factoring desirable operation makes resulting compression.
perform search possible compressions select best one, determined
desirable is. wide range models expressed framework.
prerequisites implementing fairly low, require decoding process expressed linear function set linear constraints. practice, many
models rely Markov assumption factorization usually solved dynamic programming-based decoding process. algorithms formulated integer
linear programs little effort.
first give brief introduction integer linear programming, extension linear
programming readers unfamiliar mathematical programming. compression
models next described Section 3.4 constraints Section 3.5.
3.1 Linear Programming
Linear programming (LP) problems optimization problems constraints.
consist three parts:
Decision variables. variables control wish assign
optimal values to.
linear function (the objective function). function wish minimize
maximize. function influences values assigned decision variables.
Constraints. problems allow decision variables take certain
values. restrictions constraints.
terms best demonstrated simple example taken Winston
Venkataramanan (2003). Imagine manufacturer tables chairs shall call
Telfa Corporation. produce table, 1 hour labor 9 square board feet wood
required. Chairs require 1 hour labor 5 square board feet wood. Telfa
6 hours labor 45 square board feet wood available. profit made
table 8 GBP 5 GBP chairs. wish determine number tables
chairs manufactured maximize Telfas profit.
First, must determine decision variables. case define:
x1 = number tables manufactured
x2 = number chairs manufactured
objective function value wish maximize, namely profit.
Profit = 8x1 + 5x2
two constraints problem: must exceed 6 hours labor
45 square board feet wood must used. Also, cannot create negative
amount chairs tables:
404

fiGlobal Inference Sentence Compression

Labor constraint
x 1 + x2
Wood constraint
9x1 + 5x2
Variable constraints
x1
x2

6
45
0
0

decision variables, objective function constraints determined
express LP model:
max z = 8x1 + 5x2 (Objective function)
subject (s.t.)
x1 + x2
9x1 + 5x2
x1
x2

6 (Labor constraint)
45 (Wood constraint)
0
0

Two basic concepts involved solving LP problems feasibility region
optimal solution. optimal solution one constraints satisfied
objective function minimized maximized. specification value
decision variable referred point. feasibility region LP region
consisting set points satisfy LPs constraints. optimal solution
lies within feasibility region, point minimum maximum objective
function value.
set points satisfying single linear inequality half-space. feasibility region
defined intersection half-spaces (for linear inequalities) forms
polyhedron. Telfa example forms polyhedral set (a polyhedral convex set)
intersection four constraints. Figure 1a shows feasible region Telfa
example. find optimal solution graph line (or hyperplane) points
objective function value. maximization problems called isoprofit
line minimization problems isocost line. One isoprofit line represented
dashed black line Figure 1a. one isoprofit line find isoprofit
lines moving parallel original isoprofit line.
extreme points polyhedral set defined intersections lines
form boundaries polyhedral set (points B C Figure 1a).
shown LP optimal solution, extreme point globally
optimal. reduces search space optimization problem finding extreme
point highest lowest value. simplex algorithm (Dantzig, 1963) solves LPs
exploring extreme points polyhedral set. Specifically, moves one extreme
point adjacent extreme point (extreme points lie line segment)
optimal extreme point found. Although simplex algorithm exponential
worst-case complexity, practice algorithm efficient.
15
9
optimal solution Telfa example z = 165
4 , x1 = 4 , x2 = 4 . Thus,
achieve maximum profit 41.25 GBP must build 3.75 tables 2.25 chairs.
obviously impossible would expect people buy fractions tables chairs.
Here, want able constrain problem decision variables
take integer values. done Integer Linear Programming.
405

fiClarke & Lapata

a.

b.

10

10

9

9

= LPs feasible region

9x1 + 5x2 = 45

9x1+ 5x2 = 45

8

8

7

7

6 B

6

x2 5

x2 5

4

4

= IP feasible point
= IP relaxations feasible region

3

3

Optimal LP solution

Optimal LP solution
2

C

2

x 1 + x2 = 6

1
0



0

1

2

3

x1

4


5

6

x 1 + x2 = 6

11

7

0

0

1

2

3

x1

4

5

6

7

Figure 1: Feasible region Telfa example using linear (graph (a)) integer linear
(graph (b)) programming

3.2 Integer Linear Programming
Integer linear programming (ILP) problems LP problems
variables required non-negative integers. formulated similar manner
LP problems added constraint decision variables must take non-negative
integer values.
formulate Telfa problem ILP model merely add constraints x1
x2 must integer. gives:
max z = 8x1 + 5x2 (Objective function)
subject (s.t.)
x1 + x2
9x1 + 5x2
x1
x2


6 (Labor constraint)

45 (Wood constraint)
0; x1 integer
0; x2 integer

LP models, proved optimal solution lies extreme point
feasible region. case integer linear programs, wish consider points
integer values. illustrated Figure 1b Telfa problem. contrast
linear programming, solved efficiently worst case, integer programming
problems many practical situations NP-hard (Cormen, Leiserson, & Rivest, 1992).
406

fiGlobal Inference Sentence Compression

Fortunately, ILPs well studied optimization problem number techniques
developed find optimal solution. Two techniques cutting planes
method (Gomory, 1960) branch-and-bound method (Land & Doig, 1960).
briefly discuss methods here. detailed treatment refer interested
reader Winston Venkataramanan (2003) Nemhauser Wolsey (1988).
cutting planes method adds extra constraints slice parts feasible region
contains integer extreme points. However, process difficult
impossible (Nemhauser & Wolsey, 1988). branch-and-bound method enumerates
points ILPs feasible region prunes sections region known
sub-optimal. relaxing integer constraints solving resulting
LP problem (known LP relaxation). solution LP relaxation integral,
optimal solution. Otherwise, resulting solution provides upper bound
solution ILP. algorithm proceeds creating two new sub-problems based
non-integer solution one variable time. solved process
repeats optimal integer solution found.
Using branch-and-bound method, find optimal solution Telfa
problem z = 40, x1 = 5, x2 = 0; thus, achieve maximum profit 40 GBP, Telfa
must manufacture 5 tables 0 chairs. relatively simple problem, could
solved merely inspection. ILP problems involve many variables constraints
resulting feasible region large number integer points. branch-and-bound
procedure efficiently solve ILPs matter seconds forms part many
commercial ILP solvers. experiments use lp solve 3 , free optimization package
relies simplex algorithm brand-and-bound methods solving ILPs.
Note special circumstances solving methods may applicable.
example, implicit enumeration used solve ILPs variables binary
(also known pure 01 problems). Implicit enumeration similar branch-andbound method, systematically evaluates possible solutions, without however explicitly
solving (potentially) large number LPs derived relaxation. removes
much computational complexity involved determining sub-problem infeasible. Furthermore, class ILP problems known minimum cost network flow
problems (MCNFP), LP relaxation always yields integral solution. problems
therefore treated LP problems.
general, model yield optimal solution variables integers
constraint matrix property known total unimodularity. matrix totally
unimodular every square sub-matrix determinant equal 0, +1 1.
case constraint matrix looks totally unimodular, easier
problem solve branch-and-bound methods. practice good
formulate ILPs many variables possible coefficients 0, +1 1
constraints (Winston & Venkataramanan, 2003).
3.3 Constraints Logical Conditions
Although integer variables ILP problems may take arbitrary values, frequently
restricted 0 1. Binary variables (01 variables) particularly useful rep3. software available http://lpsolve.sourceforge.net/.

407

fiClarke & Lapata

Condition
Implication
Iff

Xor



Statement
b
b
b c
xor b xor c
b


Constraint
ba0
ab=0
a+b+c1
a+b+c=1
= 1; b = 1
1a=1

Table 1: represent logical conditions using binary variables constraints ILP.

resenting variety logical conditions within ILP framework use constraints. Table 1 lists several logical conditions equivalent constraints.
express transitivity, i.e., c b. Although often thought transitivity expressed polynomial expression binary
variables (i.e., ab = c), possible replace latter following linear inequalities (Williams, 1999):

(1 c) + 1
(1 c) + b 1
c + (1 a) + (1 b) 1
easily extended model indicator variables representing whether set binary
variables take certain values.
3.4 Compression Models
section describe three compression models reformulate integer linear
programs. first model simple language model used baseline
previous research (Knight & Marcu, 2002). second model based work Hori
Furui (2004); combines language model corpus-based significance scoring
function (we omit confidence score derived speech recognizer since
models applied text only). model requires small amount parallel data
learn weights language model significance score.
third model fully supervised, uses discriminative large-margin framework
(McDonald, 2006), trained trained larger parallel corpus. chose model
instead popular noisy-channel decision-tree models, two reasons, practical one theoretical one. First, McDonalds (2006) model delivers performance superior
decision-tree model (which turn performs comparably noisy-channel). Second, noisy channel entirely appropriate model sentence compression.
uses language model trained uncompressed sentences even though represents
probability compressed sentences. result, model consider compressed sentences less likely uncompressed ones (a discussion provided Turner &
Charniak, 2005).
408

fiGlobal Inference Sentence Compression

3.4.1 Language Model
language model perhaps simplest model springs mind. require
parallel corpus (although relatively large monolingual corpus necessary training),
naturally prefer short sentences longer ones. Furthermore, language model
used drop words either infrequent unseen training corpus. Knight
Marcu (2002) use bigram language model baseline noisy-channel
decision-tree models.
Let x = x1 , x2 , . . . , xn denote source sentence wish generate target
compression. introduce decision variable word source constrain
binary; value 0 represents word dropped, whereas value 1 includes
word target compression. Let:
=

(

1 xi compression
[1 . . . n]
0 otherwise

using unigram language model, objective function would maximize
overall sum decision variables (i.e., words) multiplied unigram probabilities
(all probabilities throughout paper log-transformed):
max

n
X

P (xi )

(1)

i=1

Thus, word selected, corresponding given value 1, probability
P (xi ) according language model counted total score.
unigram language model probably generate many ungrammatical compressions.
therefore use context-aware model objective function, namely trigram
model. Dynamic programming would typically used decode language model
traversing sentence left-to-right manner. algorithm efficient provides
context required conventional language model. However, difficult
impossible incorporate global constraints model decisions word
inclusion cannot extend beyond three word window. formulating decoding process
trigram language model integer linear program able take account
constraints affect compressed sentence globally. process much
involved task unigram case context, instead must
make decisions based word sequences rather isolated words. first create
additional decision variables:
=

(

ij =



1

ijk =

1 xi starts compression
[1 . . . n]
0 otherwise

sequence xi , xj ends
compression
[0 . . . n 1]

0 otherwise
j [i + 1 . . . n]



1

sequence xi , xj , xk [0 . . . n 2]
compression j [i + 1 . . . n 1]

0 otherwise
k [j + 1 . . . n]
409

fiClarke & Lapata

objective function given Equation (2). sum possible trigrams
occur compressions source sentence x0 represents start
token xi ith word sentence x. Equation (3) constrains decision variables
binary.
max z =

n
X

P (xi |start)
i=1
n2
n
X n1
X X

+

ijk P (xk |xi , xj )

i=1 j=i+1 k=j+1

+

n1
X

n
X

ij P (end|xi , xj )

(2)

i=0 j=i+1

subject to:

, , ij , ijk = 0 1

(3)

objective function (2) allows combination trigrams selected.
means invalid trigram sequences (e.g., two trigrams containing end token)
could appear target compression. avoid situation introducing sequential
constraints (on decision variables , ijk , , ij ) restrict set allowable
trigram combinations.
Constraint 1

Exactly one word begin sentence.
n
X

= 1

(4)

i=1

Constraint 2 word included sentence must either start sentence
preceded two words one word start token x0 .
k k

k2
X k1
X

ijk = 0

(5)

i=0 j=1

k : k [1 . . . n]
Constraint 3 word included sentence must either preceded one
word followed another must preceded one word end sentence.
j

j1
X

n
X

ijk

i=0 k=j+1

j1
X

ij = 0

(6)

i=0

j : j [1 . . . n]

Constraint 4 word sentence must followed two words followed
one word end sentence must preceded one word end
sentence.


n1
X

n
X

j=i+1 k=j+1

ijk

n
X

j=i+1

410

ij

i1
X

hi = 0

h=0

: [1 . . . n]

(7)

fiGlobal Inference Sentence Compression

Constraint 5

Exactly one word pair end sentence.
n1
X

n
X

ij = 1

(8)

i=0 j=i+1

sequential constraints described ensure second order factorization (for
trigrams) holds different compression-specific constraints presented Section 3.5.
Unless normalized sentence length, language model naturally prefer one-word
output. normalization however non-linear cannot incorporated ILP
formulation. Instead, impose constraint length compressed sentence.
Equation (9) forces compression contain least b tokens.
n
X

b

(9)

i=1

Alternatively, could force compression exactly b tokens (by substituting
inequality equality (9)) less b tokens (by replacing ).4
constraint (9) language model-specific used elsewhere.
3.4.2 Significance Model
language model described notion content words include
compression thus prefers words seen before. words constituents
different relative importance different documents even sentences.
Inspired Hori Furui (2004), add objective function (see Equation (2))
significance score designed highlight important content words. Hori Furuis
original formulation word weighted score similar un-normalized tf idf .
significance score applied indiscriminately words sentence solely
topic-related words, namely nouns verbs. score differs one respect. combines
document-level sentence-level significance. addition tf idf , word
weighted level embedding syntactic tree.
Intuitively, sentence multiply nested clauses, deeply embedded clauses
tend carry semantic content. illustrated Figure 2 depicts
clause embedding sentence Mr Field said resign reselected,
move could divide party nationally. Here, important information
conveyed clauses S3 (he resign) S4 (if reselected) embedded.
Accordingly, give weight words found clauses main
clause (S1 Figure 2). simple way enforce give clauses weight proportional
level embedding. modified significance score becomes:
I(xi ) =

Fa
l
log
N


(10)

xi topic word, frequency xi document corpus
respectively, Fa sum topic words corpus, l number clause
4. Compression rate limited range including two inequality constraints.

411

fiClarke & Lapata

S1
S2
Mr Field said
S3
resign
S4
reselected
, move
SBAR
could divide party nationally

Figure 2: clause embedding sentence Mr Field said resign
reselected, move could divide party nationally; nested boxes
correspond nested clauses.

constituents xi , N deepest level clause embedding. Fa
estimated large document collection, document-specific, whereas Nl sentencespecific. So, Figure 2 term Nl 1.0 (4/4) clause S4 , 0.75 (3/4) clause S3 ,
on. Individual words inherit weight clauses.
modified objective function significance score given below:
max z =

n
X

I(xi ) +

i=1
n2
X n1
X

+

n
X

P (xi |start)

i=1

n
X

ijk P (xk |xi , xj )

i=1 j=i+1 k=j+1

+

n1
X

n
X

ij P (end|xi , xj )

(11)

i=0 j=i+1

add weighting factor () objective, order counterbalance importance language model significance score. weight tuned small
parallel corpus. sequential constraints Equations (4)(8) used ensure
trigrams combined valid way.
3.4.3 Discriminative Model
fully supervised model, used discriminative model presented McDonald
(2006). model uses large-margin learning framework coupled feature set
defined compression bigrams syntactic structure.
Let x = x1 , . . . , xn denote source sentence target compression = y1 , . . . , ym
yj occurs x. function L(yi ) {1 . . . n} maps word yi target com412

fiGlobal Inference Sentence Compression

pression index word source sentence, x. include constraint
L(yi ) < L(yi+1 ) forces word x occur compression
y. Let score compression sentence x be:
(12)

s(x, y)

score factored using first-order Markov assumption words target
compression give:
s(x, y) =

|y|
X

s(x, L(yj1 ), L(yj ))

(13)

j=2

score function defined dot product high dimensional feature
representation corresponding weight vector:
s(x, y) =

|y|
X

w f (x, L(yj1 ), L(yj ))

(14)

j=2

Decoding model amounts finding combination bigrams maximizes
scoring function (14). McDonald (2006) uses dynamic programming approach
maximum score found left-to-right manner. algorithm extension
Viterbi case scores factor dynamic sub-strings (Sarawagi & Cohen,
2004; McDonald, Crammer, & Pereira, 2005a). allows back-pointers used
reconstruct highest scoring compression well k-best compressions.
similar trigram language model decoding process (see Section 3.4.1),
except bigram model used. Consequently, ILP formulation slightly
simpler trigram language model. Let:
=

(

1 xi compression
(1 n)
0 otherwise

introduce decision variables:
=
=
ij =

(

(

(

1 xi starts compression
[1 . . . n]
0 otherwise

1 word xi ends compression
0 otherwise
[1 . . . n]

1 sequence xi , xj compression [1 . . . n 1]
0 otherwise
j [i + 1 . . . n]

discriminative model expressed as:
max z =

n
X


i=1
n1
X

s(x, 0, i)

+

s(x, i, n + 1)

+

n
X

i=1 j=i+1
n
X
i=1

413

ij s(x, i, j)
(15)

fiClarke & Lapata

Constraint 1

Exactly one word begin sentence.
n
X

= 1

(16)

i=1

Constraint 2 word included sentence must either start compression
follow another word.

j j

j
X

ij = 0

(17)

i=1

j : j [1 . . . n]
Constraint 3 word included sentence must either followed another
word end sentence.



n
X

ij = 0

(18)

j=i+1

: [1 . . . n]

Constraint 4

Exactly one word end sentence.
n
X

= 1

(19)

i=1

Again, sequential constraints Equations (16)(19) necessary ensure
resulting combination bigrams valid.
current formulation provides single optimal compression given model. However, McDonalds (2006) dynamic programming algorithm capable returning k-best
compressions; useful learning algorithm described later. order produce
k-best compressions, must rerun ILP extra constraints forbid previous
solutions. words, first formulate ILP above, solve it, add solution
k-best list, create set constraints forbid configuration decision
variables form current solution. procedure repeated k compressions
found.
computation compression score crucially relies dot product
high dimensional feature representation corresponding weight vector (see Equation (14)). McDonald (2006) employs rich feature set defined adjacent words
individual parts-of-speech, dropped words phrases source sentence, dependency structures (also source sentence). features designed mimic
information presented previous noisy-channel decision-tree models Knight
Marcu (2002). Features adjacent words used proxy language model
noisy channel. Unlike models, treat parses gold standard, McDonald
uses dependency information another form evidence. Faced parses
noisy learning algorithm reduce weighting given features prove
414

fiGlobal Inference Sentence Compression

poor discriminators training data. Thus, model much robust
portable across different domains training corpora.
weight vector, w learned using Margin Infused Relaxed Algorithm (MIRA,
Crammer & Singer, 2003) discriminative large-margin online learning technique (McDonald, Crammer, & Pereira, 2005b). algorithm learns compressing sentence
comparing result gold standard. weights updated score
correct compression (the gold standard) greater score compressions margin proportional loss. loss function number words falsely
retained dropped incorrect compression relative gold standard. source
sentence exponentially many compressions thus exponentially many margin
constraints. render learning computationally tractable, McDonald et al. (2005b) create
constraints k compressions currently highest score, bestk (x; w).
3.5 Constraints
ready describe compression-specific constraints. models presented
previous sections contain sequential constraints thus equivalent
original formulation. constraints linguistically semantically motivated
similar fashion grammar checking component Jing (2000). However,
rely additional knowledge sources (such grammar lexicon WordNet)
beyond parse grammatical relations source sentence. obtain
RASP (Briscoe & Carroll, 2002), domain-independent, robust parsing system English.
However, parser broadly similar output (e.g., Lin, 2001) could serve
purposes. constraints revolve around modification, argument structure, discourse
related factors.
Modifier Constraints Modifier constraints ensure relationships head words
modifiers remain grammatical compression:
j 0

(20)

i, j : xj xi ncmods
j 0

(21)

i, j : xj xi detmods
Equation (20) guarantees include non-clausal modifier5 (ncmod) compression (such adjective noun) head modifier must included;
repeated determiners (detmod) (21). Table 2 illustrate constraints disallow deletion certain words (starred sentences denote compressions
would possible given constraints). example, modifier word Pasok
sentence (1a) compression, head Party included (see (1b)).
want ensure meaning source sentence preserved
compression, particularly face negation. Equation (22) implements forcing
compression head included (see sentence (2b) Table 2). similar
constraint added possessive modifiers (e.g., his, our), including genitives (e.g., Johns
5. Clausal modifiers (cmod) adjuncts modifying entire clauses. example ate cake
hungry, because-clause modifier sentence ate cake.

415

fiClarke & Lapata

1a.
1b.
2a.
2b.
2c.
3a.
3b.
3c.
3d.
3e.
3f.

became power player Greek Politics 1974, founded
socialist Pasok Party.
*He became power player Greek Politics 1974, founded
Pasok.
took troubled youth dont fathers, brought
room Dads dont children.
*We took troubled youth fathers, brought
room Dads children.
*We took troubled youth dont fathers, brought
room Dads dont children.
chain stretched Uganda Grenada Nicaragua, since 1970s.
*Stretched Uganda Grenada Nicaragua, since 1970s.
*The chain Uganda Grenada Nicaragua, since 1970s.
*The chain stretched Uganda Grenada Nicaragua, since 1970s.
*The chain stretched Grenada Nicaragua, since 1970s.
*The chain stretched Uganda Grenada Nicaragua, since 1970s.
Table 2: Examples compressions disallowed set constraints.

gift), shown Equation (23). example possessive constraint given
sentence (2c) Table 2.
j = 0

(22)

i, j : xj xi ncmods xj =
j = 0

(23)

i, j : xj xi possessive mods
Argument Structure Constraints define intuitive constraints take
overall sentence structure account. first constraint (Equation (24)) ensures
verb present compression arguments,
arguments included compression verb must included. thus
force program make decision verb, subject, object (see
sentence (3b) Table 2).
j = 0

(24)

i, j : xj subject/object verb xi
second constraint forces compression contain least one verb provided
source sentence contains one well:
X

1

(25)

i:xi verbs

constraint entails possible drop main verb stretched sentence (3a) (see sentence (3c) Table 2).
416

fiGlobal Inference Sentence Compression

sentential constraints include Equations (26) (27) apply prepositional phrases subordinate clauses. constraints force introducing term
(i.e., preposition, subordinator) included compression word
within syntactic constituent included. subordinator mean wh-words
(e.g., who, which, how, where), word that, subordinating conjunctions (e.g., after,
although, because). reverse true, i.e., introducing term included,
least one word syntactic constituent included.
j 0

(26)

i, j : xj PP/SUB
xi starts PP/SUB
X

j 0

(27)

i:xi PP/SUB

j : xj starts PP/SUB
example consider sentence (3d) Table 2. Here, cannot drop preposition
Uganda compression. Conversely, must include Uganda
compression (see sentence (3e)).
wish handle coordination. two head words conjoined source
sentence, included compression coordinating conjunction must
included:
(1 ) + j 1

(28)

(1 ) + k 1

(29)

+ (1 j ) + (1 k ) 1

(30)

i, j, k : xj xk conjoined xi
Consider sentence (3f) Table 2. Uganda Nicaragua present
compression, must include conjunction and.
Finally, Equation (31) disallows anything within brackets source sentence
included compression. somewhat superficial attempt excluding
parenthetical potentially unimportant material compression.
= 0

(31)

: xi bracketed words (inc parentheses)
Discourse Constraints discourse constraint concerns personal pronouns. Specifically, Equation (32) forces personal pronouns included compression.
constraint admittedly important generating coherent documents (as opposed
individual sentences). nevertheless impact sentence-level compressions,
particular verbal arguments missed parser. pronominal,
constraint (32) result grammatical output since argument structure
source sentence preserved compression.
= 1
: xi personal pronouns
417

(32)

fiClarke & Lapata

note constraints described would captured
models learn synchronous deletion rules corpus. example, noisy-channel
model Knight Marcu (2002) learns drop head latter modified
adjective noun, since transformations DT NN DT AJD NN ADJ
almost never seen data. Similarly, coordination constraint (Equations (28)(30))
would enforced using Turner Charniaks (2005) special rules enhance
parallel grammar rules modeling structurally complicated deletions
attested corpus. designing constraints aimed capturing appropriate
deletions many possible models, including rely training corpus
explicit notion parallel grammar (e.g., McDonald, 2006).
modification constraints would presumably redundant noisy-channel model,
could otherwise benefit specialized constraints, e.g., targeting sparse rules
noisy parse trees, however leave future work.
Another feature modeling framework presented deletions (or nondeletions) treated unconditional decisions. example, require drop
noun adjective-noun sequences adjective deleted well. require
always include verb compression source sentence one. hardwired decisions could cases prevent valid compressions considered. instance,
possible compress sentence appropriate behavior
appropriate orBob loves Mary John loves Susan Bob loves Mary John
Susan. Admittedly lose expressive power, yet ensure compressions
broadly grammatically, even unsupervised semi-supervised models. Furthermore, practice find models consistently outperform non-constraint-based
alternatives, without extensive constraint engineering.
3.6 Solving ILP
mentioned earlier (Section 3.1), solving ILPs NP-hard. cases coefficient matrix unimodular, shown optimal solution linear
program integral. Although coefficient matrix problems unimodular,
obtained integral solutions sentences experimented (approximately 3,000,
see Section 4.1 details). conjecture due fact variables 0, +1 1 coefficients constraints therefore constraint matrix
shares many properties unimodular matrix. generate solve ILP every
sentence wish compress. Solve times less second per sentence (including
input-output overheads) models presented here.

4. Experimental Set-up
evaluation experiments motivated three questions: (1) constraintbased compression models deliver performance gains non-constraint-based ones?
expect better compressions model variants incorporate compression-specific
constraints. (2) differences among constraint-based models? Here, would
investigate much modeling power gained addition constraints.
example, may case state-of-the-art model McDonalds (2006)
benefit much addition constraints. effect much bigger less
418

fiGlobal Inference Sentence Compression

sophisticated models. (3) models reported paper port across domains?
particular, interested assessing whether models proposed constraints
general robust enough produce good compressions written spoken
texts.
next describe data sets models trained tested (Section 4.1),
explain model parameters estimated (Section 4.2) present evaluation setup
(Section 4.3). discuss results Section 5.
4.1 Corpora
intent assess performance models described written spoken
text. appeal written text understandable since summarization work today
focuses domain. Speech data provides natural test-bed compression
applications (e.g., subtitle generation) poses additional challenges. Spoken utterances ungrammatical, incomplete, often contain artefacts false starts,
interjections, hesitations, disfluencies. Rather focusing spontaneous speech
abundant artefacts, conduct study less ambitious domain
broadcast news transcripts. lies in-between extremes written text spontaneous speech scripted beforehand usually read autocue.
Previous work sentence compression almost exclusively used Ziff-Davis corpus
training testing purposes. corpus originates collection news articles
computer products. created automatically matching sentences occur
article sentences occur abstract (Knight & Marcu, 2002). abstract
sentences contain subset source sentences words word order
remain same. earlier work (Clarke & Lapata, 2006) argued
Ziff-Davis corpus ideal studying compression several reasons. First, showed
human-authored compressions differ substantially Ziff-Davis tends
aggressively compressed. Second, humans likely drop individual words
lengthy constituents. Third, test portion Ziff-Davis contains solely 32 sentences. extremely small data set reveal statistically significant differences
among systems. fact, previous studies relied almost exclusively human judgments
assessing well-formedness compressed output, significance tests reported
by-subjects analyses only.
thus focused present study manually created corpora. Specifically,
asked annotators perform sentence compression removing tokens sentence-bysentence basis. Annotators free remove words deemed superfluous provided
deletions: (a) preserved important information source sentence,
(b) ensured compressed sentence remained grammatical. wished, could leave
sentence uncompressed marking inappropriate compression.
allowed delete whole sentences even believed contained information content
respect story would blur task abstracting. Following
guidelines, annotators produced compressions 82 newspaper articles (1,433 sentences)
British National Corpus (BNC) American News Text corpus (henceforth
written corpus) 50 stories (1,370 sentences) HUB-4 1996 English Broadcast
News corpus (henceforth spoken corpus). written corpus contains articles LA
419

fiClarke & Lapata

Times, Washington Post, Independent, Guardian Daily Telegraph. spoken
corpus contains broadcast news variety networks (CNN, ABC, CSPAN NPR)
manually transcribed segmented story sentence level.
corpora split training, development testing sets6 randomly article
boundaries (with set containing full stories) publicly available http:
//homepages.inf.ed.ac.uk/s0460084/data/.
4.2 Parameter Estimation
work present three compression models ranging unsupervised semisupervised, fully supervised. unsupervised model simply relies trigram language model driving compression (see Section 3.4.1). estimated 25 million tokens North American corpus using CMU-Cambridge Language Modeling
Toolkit (Clarkson & Rosenfeld, 1997) vocabulary size 50,000 tokens GoodTuring discounting. discourage one-word output force ILP generate compressions whose length less 40% source sentence (see constraint (9)).
semi-supervised model weighted combination word-based significance score
language model (see Section 3.4.2). significance score calculated using
25 million tokens American News Text corpus. optimized weight (see
Equation (11)) small subset training data (three documents case) using Powells method (Press, Teukolsky, Vetterling, & Flannery, 1992) loss function
based F-score grammatical relations found gold standard compression
systems best compression (see Section 4.3 details). optimal weight
approximately 1.8 written corpus 2.2 spoken corpus.
McDonalds (2006) supervised model trained written spoken training
sets. implementation used feature sets McDonald, difference
phrase structure dependency features extracted output
Roarks (2001) parser. McDonald uses Charniaks (2000) parser performs comparably.
model learnt using k-best compressions. development data, found
k = 10 provided best performance.
4.3 Evaluation
Previous studies relied almost exclusively human judgments assessing wellformedness automatically derived compressions. typically rated naive subjects two dimensions, grammaticality importance (Knight & Marcu, 2002). Although
automatic evaluation measures proposed (Riezler et al., 2003; Bangalore, Rambow, & Whittaker, 2000) use less widespread, suspect due small size
test portion Ziff-Davis corpus commonly used compression work.
evaluate output models two ways. First, present results using
automatic evaluation measure put forward Riezler et al. (2003). compare
grammatical relations found system compressions found gold
standard. allows us measure semantic aspects summarization quality terms
grammatical-functional information quantified using F-score. Furthermore,
6. splits 908/63/462 sentences written corpus 882/78/410 sentences spoken
corpus.

420

fiGlobal Inference Sentence Compression

Clarke Lapata (2006) show relations-based F-score correlates reliably
human judgments compression output. Since test corpora larger ZiffDavis (by factor ten), differences among systems highlighted using
significance testing.
implementation F-score measure used grammatical relations annotations
provided RASP (Briscoe & Carroll, 2002). parser particularly appropriate
compression task since provides parses full sentences sentence fragments
generally robust enough analyze semi-grammatical sentences. calculated F-score
relations provided RASP (e.g., subject, direct/indirect object, modifier; 15
total).
line previous work evaluate models eliciting human judgments.
Following work Knight Marcu (2002), conducted two separate experiments.
first experiment participants presented source sentence target
compression asked rate well compression preserved important
information source sentence. second experiment, asked rate
grammaticality compressed outputs. cases used five point rating
scale high number indicates better performance. randomly selected 21 sentences
test portion corpus. sentences compressed automatically
three models presented paper without constraints. included
gold standard compressions. materials thus consisted 294 (21 2 7) sourcetarget sentences. Latin square design ensured subjects see two different
compressions sentence. collected ratings 42 unpaid volunteers, self
reported native English speakers. studies conducted Internet using
custom build web interface. Examples experimental items given Table 3.

5. Results
Let us first discuss results compression output evaluated terms F-score.
Tables 4 5 illustrate performance models written spoken corpora,
respectively. present compression rate7 system. cases
constraint-based models (+Constr) yield better F-scores non-constrained ones.
difference starker semi-supervised model (Sig). constraints bring
improvement 17.2% written corpus 18.3% spoken corpus.
examined whether performance differences among models statistically significant, using
Wilcoxon test. written corpus constraint models significantly outperform
models without constraints. tendency observed spoken corpus except
model McDonald (2006) performs comparably without constraints.
wanted establish best constraint model. corpora
find language model performs worst, whereas significance model McDonald
perform comparably (i.e., F-score differences statistically significant). get
feeling difficulty task, calculated much annotators agreed
compression output. inter-annotator agreement (F-score) written corpus
65.8% spoken corpus 73.4%. agreement higher spoken texts since
consists many short utterances (e.g., Okay, Thats now, Good night)
7. term refers percentage words retained source sentence compression.

421

fiClarke & Lapata

Source

aim give councils control future growth second
homes.
Gold
aim give councils control growth homes.
LM
aim future.
LM+Constr aim give councils control.
Sig
aim give councils control future growth homes.
Sig+Constr aim give councils control future growth homes.
McD
aim give councils.
McD+Constr aim give councils control growth homes.
Source
Clinton administration recently unveiled new means encourage
brownfields redevelopment form tax incentive proposal.
Gold
Clinton administration unveiled new means encourage brownfields redevelopment tax incentive proposal.
LM
Clinton administration form tax.
LM+Constr Clinton administration unveiled means encourage redevelopment form.
Sig
Clinton administration unveiled encourage brownfields redevelopment form tax proposal.
Sig+Constr Clinton administration unveiled means encourage brownfields
redevelopment form tax proposal.
McD
Clinton unveiled means encourage brownfields redevelopment
tax incentive proposal.
McD+Constr Clinton administration unveiled means encourage brownfields
redevelopment form incentive proposal.
Table 3: Example compressions produced systems (Source: source sentence, Gold:
gold-standard compression, LM: language model compression, LM+Constr: language model compression constraints, Sig: significance model, Sig+Constr:
significance model constraints, McD: McDonalds (2006) compression model,
McD+Constr: McDonalds (2006) compression model constraints).

compressed little all. Note marked difference
automatic human compressions. best performing systems inferior human
output 20 F-score percentage points.
Differences automatic systems human output observed
respect compression rate. seen language model compresses
aggressively, whereas significance model McDonald tend conservative
closer gold standard. Interestingly, constraints necessarily increase
compression rate. latter increases significance model decreases
language model remains relatively constant McDonald. straightforward
impose compression rate constraint-based models (e.g., forcing model
P
retain b tokens ni=1 = b). However, refrained since wanted
422

fiGlobal Inference Sentence Compression

Models
LM
Sig
McD
LM+Constr
Sig+Constr
McD+Constr
Gold

CompR
46.2
60.6
60.1
41.2
72.0
63.7
70.3

F-score
18.4
23.3
36.0
28.2
40.5
40.8


Table 4: Results written corpus; compression rate (CompR) grammatical relation F-score (F-score); : +Constr model significantly different model
without constraints; : significantly different LM+Constr.
Models
LM
Sig
McD
LM+Constr
Sig+Constr
McD+Constr
Gold

CompR
52.0
60.9
68.6
49.5
78.4
68.5
76.1

F-score
25.4
30.4
47.6
34.8
48.7
50.1


Table 5: Results spoken corpus; compression rate (CompR) grammatical relation F-score (F-score); : +Constr model significantly different without
constraints; : significantly different LM+Constr.

models regulate compression rate sentence individually according
specific information content structure.
next consider results human study assesses detail quality
generated compressions two dimensions, namely grammaticality information
content. F-score conflates two dimensions therefore theory could unduly reward
system produces perfectly grammatical output without information loss. Tables 6
7 show mean ratings8 system (and gold standard) written
spoken corpora, respectively. first performed Analysis Variance (Anova)
examine effect different system compressions. Anova revealed reliable effect
grammaticality importance corpus (the effect significant
subjects items (p < 0.01)).
next examine impact constraints (+Constr tables). cases
observe increase ratings grammaticality importance model
supplemented constraints. Post-hoc Tukey tests reveal grammaticality
importance ratings language model significance model significantly improve
8. statistical tests reported subsequently done using mean ratings.

423

fiClarke & Lapata

Models

Grammar
2.25$

Importance

LM
Sig
McD

3.05

1.82$
2.99$
2.84

LM+Constr
Sig+Constr
McD+Constr
Gold

3.47
3.76
3.50
4.25

2.37$
3.53
3.17
3.98

2.26$

Table 6: Results written text corpus; average grammaticality score (Grammar)
average importance score (Importance) human judgments; : +Constr model
significantly different model without constraints; : significantly different
gold standard; $ : significantly different McD+Constr.

Models

Grammar
2.20$

Importance

LM
Sig
McD

2.29$
3.33

1.56
2.64
3.32

LM+Constr
Sig+Constr
McD+Constr
Gold

3.18
3.80
3.60
4.45

2.49$
3.69
3.31
4.25

Table 7: Results spoken text corpus; average grammaticality score (Grammar)
average importance score (Importance) human judgments; : +Constr model
significantly different model without constraints; : significantly different
gold standard; $ : significantly different McD+Constr.

constraints ( < 0.01). contrast, McDonalds system sees numerical improvement
additional constraints, difference statistically significant.
tendencies observed spoken written corpus.
Upon closer inspection, see constraints influence considerably
grammaticality unsupervised semi-supervised systems. Tukey tests reveal
LM+Constr Sig+Constr grammatical McD+Constr. terms importance,
Sig+Constr McD+Constr significantly better LM+Constr ( < 0.01).
surprising given LM+Constr simple model without mechanism
highlighting important words sentence. Interestingly, Sig+Constr performs well
McD+Constr retaining important words, despite fact requires
minimal supervision. Although constraint-based models overall perform better models without constraints, receive lower ratings (for grammaticality importance)
comparison gold standard. differences significant cases.
424

fiGlobal Inference Sentence Compression

summary, observe constraints boost performance. pronounced compression models either unsupervised use small amounts
parallel data. example, simple model Sig yields performance comparable
McDonald (2006) constraints taken account. encouraging result
suggesting ILP used create good compression models relatively little
effort (i.e., without extensive feature engineering elaborate knowledge sources). Performance gains obtained competitive models McDonalds fully
supervised. gains smaller, presumably initial model contains
rich feature representation consisting syntactic information generally good job
producing grammatical output. Finally, improvements consistent across corpora
evaluation paradigms.

6. Conclusions
paper presented novel method automatic sentence compression. key
aspect approach use integer linear programming inferring globally optimal
compressions presence linguistically motivated constraints. shown
previous formulations sentence compression recast ILPs extended
models local global constraints ensuring compressed output structurally
semantic well-formed. Contrary previous work employed ILP solely
decoding, models integrate learning inference unified framework.
experiments demonstrated advantages approach. Constraint-based
models consistently bring performance gains models without constraints. improvements impressive models require little supervision. case
point significance model discussed above. no-constraints incarnation
model performs poorly considerably worse McDonalds (2006) state-of-the-art
model. addition constraints improves output model performance indistinguishable McDonald. Note significance model requires
small amount training data (50 parallel sentences), whereas McDonald trained hundreds sentences. presupposes little feature engineering, whereas McDonald utilizes
thousands features. effort associated framing constraints, however
created applied across models corpora. observed small
performance gains McDonalds system latter supplemented constraints.
Larger improvements possible sophisticated constraints, however intent
devise set general constraints tuned mistakes specific
system particular.
Future improvements many varied. obvious extension concerns constraint set. Currently constraints mostly syntactic consider sentence
isolation. incorporating discourse constraints could highlight words important document-level. Presumably words topical document retained
compression. constraints could manipulate compression rate. example,
could encourage higher compression rate longer sentences. Another interesting
direction includes development better objective functions compression task.
objective functions presented far rely first second-order Markov assumptions.
Alternative objectives could take account structural similarity source
425

fiClarke & Lapata

sentence target compression; whether share content could
operationalized terms entropy.
Beyond task systems presented paper, believe approach holds
promise generation applications using decoding algorithms searching space
possible outcomes. Examples include sentence-level paraphrasing, headline generation,
summarization.

Acknowledgments
grateful annotators Vasilis Karaiskos, Beata Kouchnir, Sarah Luger.
Thanks Jean Carletta, Frank Keller, Steve Renals, Sebastian Riedel helpful
comments suggestions anonymous referees whose feedback helped substantially improve present paper. Lapata acknowledges support EPSRC (grant
GR/T04540/01). preliminary version work published proceedings
ACL 2006.

References
Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3, 3756.
Bangalore, S., Rambow, O., & Whittaker, S. (2000). Evaluation metrics generation.
Proceedings first International Conference Natural Language Generation,
pp. 18, Mitzpe Ramon, Israel.
Barzilay, R., & Lapata, M. (2006). Aggregation via set partitioning natural language
generation. Proceedings Human Language Technology Conference
North American Chapter Association Computational Linguistics, pp. 359
366, New York, NY, USA.
Bramsen, P., Deshpande, P., Lee, Y. K., & Barzilay, R. (2006). Inducing temporal graphs.
Proceedings 2006 Conference Empirical Methods Natural Language
Processing, pp. 189198, Sydney, Australia.
Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation general text.
Proceedings Third International Conference Language Resources Evaluation, pp. 14991504, Las Palmas, Gran Canaria.
Charniak, E. (2000). maximum-entropy-inspired parser. Proceedings 1st North
American Annual Meeting Association Computational Linguistics, pp. 132
139, Seattle, WA, USA.
Clarke, J., & Lapata, M. (2006). Models sentence compression: comparison across
domains, training requirements evaluation measures. Proceedings 21st
International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 377384, Sydney, Australia.
Clarkson, P., & Rosenfeld, R. (1997). Statistical language modeling using CMU
Cambridge toolkit. Proceedings Eurospeech97, pp. 27072710, Rhodes, Greece.
426

fiGlobal Inference Sentence Compression

Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1992). Intoduction Algorithms.
MIT Press.
Corston-Oliver, S. (2001). Text Compaction Display Small Screens. Proceedings Workshop Automatic Summarization 2nd Meeting North
American Chapter Association Computational Linguistics, pp. 8998, Pittsburgh, PA, USA.
Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms multiclass problems. Journal Machine Learning Research, 3, 951991.
Dantzig, G. B. (1963). Linear Programming Extensions. Princeton University Press,
Princeton, NJ, USA.
Denis, P., & Baldridge, J. (2007). Joint determination anaphoricity coreference
resolution using integer programming. Human Language Technologies 2007:
Conference North American Chapter Association Computational Linguistics; Proceedings Main Conference, pp. 236243, Rochester, NY.
Dras, M. (1999). Tree Adjoining Grammar Reluctant Paraphrasing Text. Ph.D.
thesis, Macquarie University.
Galley, M., & McKeown, K. (2007). Lexicalized markov grammars sentence compression.
Proceedings North American Chapter Association Computational
Linguistics, pp. 180187, Rochester, NY, USA.
Gomory, R. E. (1960). Solving linear programming problems integers. Bellman,
R., & Hall, M. (Eds.), Combinatorial analysis, Proceedings Symposia Applied
Mathematics, Vol. 10, Providence, RI, USA.
Grefenstette, G. (1998). Producing Intelligent Telegraphic Text Reduction Provide
Audio Scanning Service Blind. Hovy, E., & Radev, D. R. (Eds.), Proceedings
AAAI Symposium Intelligent Text Summarization, pp. 111117, Stanford,
CA, USA.
Hori, C., & Furui, S. (2004). Speech summarization: approach word extraction
method evaluation. IEICE Transactions Information Systems, E87D (1), 1525.
Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings
6th Applied Natural Language Processing Conference, pp. 310315, Seattle,WA,
USA.
Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: probabilistic
approach sentence compression. Artificial Intelligence, 139 (1), 91107.
Land, A. H., & Doig, A. G. (1960). automatic method solving discrete programming
problems. Econometrica, 28, 497520.
Lin, C.-Y. (2003). Improving summarization performance sentence compression pilot
study. Proceedings 6th International Workshop Information Retrieval
Asian Languages, pp. 18, Sapporo, Japan.
Lin, D. (2001). LaTaT: Language text analysis tools. Proceedings first Human
Language Technology Conference, pp. 222227, San Francisco, CA, USA.
427

fiClarke & Lapata

Marciniak, T., & Strube, M. (2005). Beyond pipeline: Discrete optimization NLP.
Proceedings Ninth Conference Computational Natural Language Learning,
pp. 136143, Ann Arbor, MI, USA.
McDonald, R. (2006). Discriminative sentence compression soft syntactic constraints.
Proceedings 11th Conference European Chapter Association
Computational Linguistics, Trento, Italy.
McDonald, R., Crammer, K., & Pereira, F. (2005a). Flexible text segmentation structured multilabel classification. Proceedings Human Language Technology Conference Conference Empirical Methods Natural Language Processing, pp.
987994, Vancouver, BC, Canada.
McDonald, R., Crammer, K., & Pereira, F. (2005b). Online large-margin training dependency parsers. 43rd Annual Meeting Association Computational
Linguistics, pp. 9198, Ann Arbor, MI, USA.
Nemhauser, G. L., & Wolsey, L. A. (1988). Integer Combinatorial Optimization. WileyInterscience series discrete mathematicals opitmization. Wiley, New York, NY,
USA.
Nguyen, M. L., Shimazu, A., Horiguchi, S., Ho, T. B., & Fukushi, M. (2004). Probabilistic
sentence reduction using support vector machines. Proceedings 20th international conference Computational Linguistics, pp. 743749, Geneva, Switzerland.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical
Recipes C: Art Scientific Computing. Cambridge University Press, New
York, NY, USA.
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. Proceedings International Conference
Computational Linguistics, pp. 13461352, Geneva, Switzerland.
Riedel, S., & Clarke, J. (2006). Incremental integer linear programming non-projective
dependency parsing. Proceedings 2006 Conference Empirical Methods
Natural Language Processing, pp. 129137, Sydney, Australia.
Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation
using ambiguity packing stochastic disambiguation methods lexical-functional
grammar. Human Language Technology Conference 3rd Meeting
North American Chapter Association Computational Linguistics, pp. 118
125, Edmonton, Canada.
Roark, B. (2001). Probabilistic top-down parsing language modeling. Computational
Linguistics, 27 (2), 249276.
Roth, D. (1998). Learning resolve natural language ambiguities: unified approach.
Proceedings 15th American Association Artificial Intelligence, pp.
806813, Madison, WI, USA.
Roth, D., & Yih, W. (2004). linear programming formulation global inference
natural language tasks. Proceedings Annual Conference Computational
Natural Language Learning, pp. 18, Boston, MA, USA.
428

fiGlobal Inference Sentence Compression

Roth, D., & Yih, W. (2005). Integer linear programming inference conditional random
fields. Proceedings International Conference Machine Learning, pp. 737
744, Bonn.
Sarawagi, S., & Cohen, W. W. (2004). Semi-markov conditional random fields information extraction. Advances Neural Information Processing Systems, Vancouver,
BC, Canada.
Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. Proceedings 13th International Conference Computational Linguistics, pp. 253258,
Helsinki, Finland.
Turner, J., & Charniak, E. (2005). Supervised unsupervised learning sentence
compression. Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 290297, Ann Arbor, MI, USA.
Vandeghinste, V., & Pan, Y. (2004). Sentence compression automated subtitling:
hybrid approach. Marie-Francine Moens, S. S. (Ed.), Text Summarization Branches
Out: Proceedings ACL-04 Workshop, pp. 8995, Barcelona, Spain.
Williams, H. P. (1999). Model Building Mathematical Programming (4th edition). Wiley.
Winston, W. L., & Venkataramanan, M. (2003). Introduction Mathematical Programming: Applications Algorithms (4th edition). Duxbury.
Zajic, D., Door, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence
compression tool document summarization tasks. Information Processing
Management Special Issue Summarization, 43 (6), 15491570.

429


