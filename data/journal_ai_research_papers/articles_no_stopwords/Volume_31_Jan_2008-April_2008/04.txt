Journal Artificial Intelligence Research 31 (2008) 543-590

Submitted 08/07; published 03/08

Creating Relational Data Unstructured
Ungrammatical Data Sources
Matthew Michelson
Craig A. Knoblock

michelso@isi.edu
knoblock@isi.edu

University Southern California
Information Sciences Instistute
4676 Admiralty Way
Marina del Rey, CA 90292 USA

Abstract
order agents act behalf users, retrieve integrate
vast amounts textual data World Wide Web. However, much useful data
Web neither grammatical formally structured, making querying difficult.
Examples types data sources online classifieds Craigslist1 auction
item listings eBay.2 call unstructured, ungrammatical data posts.
unstructured nature posts makes query integration difficult attributes
embedded within text. Also, attributes conform standardized values,
prevents queries based common attribute value. schema unknown
values may vary dramatically making accurate search difficult. Creating relational
data easy querying requires define schema embedded attributes
extract values posts standardizing values. Traditional information
extraction (IE) inadequate perform task relies clues data,
structure natural language, neither found posts. Furthermore,
traditional information extraction incorporate data cleaning, necessary
accurately query integrate source. two-step approach described paper
creates relational data sets unstructured ungrammatical text addressing
issues. this, require set known entities called reference set. first step
aligns post member reference set. allows algorithm define
schema post include standard values attributes defined schema.
second step performs information extraction attributes, including attributes
easily represented reference sets, price. manner create relational
structure previously unstructured data, supporting deep accurate queries
data well standard values integration. experimental results show
technique matches posts reference set accurately efficiently outperforms
state-of-the-art extraction systems extraction task posts.

1. Introduction
future vision Web includes computer agents searching information, making
decisions taking actions behalf human users. instance, agent could query
number data sources find lowest price given car email user
car listing, along directions seller available appointments see car.
1. www.craigslist.org
2. www.ebay.com
c
2008
AI Access Foundation. rights reserved.

fiMichelson & Knoblock

requires agent contain two data gathering mechanisms: ability query
sources ability integrate relevant sources information.
However, data gathering mechanisms assume sources designed support relational queries, well defined schema standard values
attributes. Yet always case. many data sources
World Wide Web would useful query, textual data within unstructured designed support querying. call text data sources
posts. Examples posts include text eBay auction listings, Internet classifieds
Craigslist, bulletin boards Bidding Travel3 , even summary text
hyperlinks returned querying Google. running example, consider three
posts used car classifieds shown Table 1.

Table 1: Three posts Honda Civics Craigslist
Craigslist Post
93 civic 5speed runs great obo (ri) $1800
93- 4dr Honda Civc LX Stick Shift $1800
94 DEL SOL Si Vtec (Glendale) $3000

current method query posts, whether agent person, keyword search.
However, keyword search inaccurate cannot support relational queries. example,
difference spelling keyword attribute within post would
limit post returned search. would case user searched
example listings Civic since second post would returned. Another factor
limits keyword accuracy exclusion redundant attributes. example,
classified posts cars include car model, make, since make
implied model. shown first third post Table 1. cases,
user keyword search using make Honda, posts returned.
Moreover, keyword search rich query framework. instance, consider
query, average price Hondas 1999 later?
keyword search requires user search Honda retrieve 1999
later. user must traverse returned set, keeping track prices
removing incorrectly returned posts.
However, schema standardized attribute values defined entities
posts, user could run example query using simple SQL statement
accurately, addressing problems created keyword search. standardized
attribute values ensure invariance issues spelling differences. Also, post
associated full schema values, even though post might contain car
make, instance, schema correct value it, returned
query car makes. Furthermore, standardized values allow integration
source outside sources. Integrating sources usually entails joining two sources
directly attributes translations attributes. Without standardized values
3. www.biddingfortravel.com

544

fiRelational Data Unstructured Data Sources

schema, would possible link ungrammatical unstructured data
sources outside sources. paper addresses problem adding schema
standardized attributes set posts, creating relational data set support
deep accurate queries.
One way create relational data set posts define schema
fill values schema elements using techniques information extraction. sometimes called semantic annotation. example, taking second
post Table 1 semantically annotating might yield 93- 4dr Honda Civc LX Stick
Shift $1800 <make>Honda< \make> <model>Civc< \model> <trim>4dr LX< \trim>
<year>1993< \year> <price>1800< \price>. However, traditional information extraction, relies grammatical structural characteristics text identify attributes
extract. Yet posts definition structured grammatical. Therefore, wrapper
extraction technologies Stalker (Muslea, Minton, & Knoblock, 2001) RoadRunner
(Crescenzi, Mecca, & Merialdo, 2001) cannot exploit structure posts.
posts grammatical enough exploit Natural Language Processing (NLP) based extraction
techniques used Whisk (Soderland, 1999) Rapier (Califf & Mooney,
1999).
Beyond difficulties extracting attributes within post using traditional extraction methods, require values attributes standardized,
process known data cleaning. Otherwise, querying newly relational data would
inaccurate boil keyword search. instance, using annotation above,
would still need query model Civc return record. Traditional
extraction address this.
However, data cleaning algorithms assume tuple-to-tuple transformations (Lee, Ling, Lu, & Ko, 1999; Chaudhuri, Ganjam, Ganti, & Motwani, 2003).
is, function maps attributes one tuple attributes another. approach would work ungrammatical unstructured data,
attributes embedded within post, maps set attributes
reference set. Therefore need take different approach problems figuring
attributes within post cleaning them.
approach creating relational data sets unstructured ungrammatical
posts exploits reference sets. reference set consists collections known entities
associated, common attributes. reference set online (or offline) set
reference documents, CIA World Fact Book.4 online (or
offline) database, Comics Price Guide.5 Semantic Web one envision
building reference sets numerous ontologies already exist. Using standardized
ontologies build reference sets allows consensus agreement upon reference set values,
implies higher reliability reference sets others might exist one
experts opinion. Using car example, reference set might Edmunds car buying
guide6 , defines schema cars well standard values attributes
model trim. order construct reference sets Web sources,
4. http://www.cia.gov/cia/publications/factbook/
5. www.comicspriceguide.com
6. www.edmunds.com

545

fiMichelson & Knoblock

Edmunds car buying guide, use wrapper technologies (Agent Builder7 case)
scrape data Web source, using schema source defines car.
use reference set build relational data set exploit attributes
reference set determine attributes post extracted. first step
algorithm finds best matching member reference set post.
called record linkage step. matching post member reference set
define schema elements post using schema reference set,
provide standard attributes attributes using attributes reference
set user queries posts.
Next, perform information extraction extract actual values post
match schema elements defined reference set. step information
extraction step. information extraction step, parts post extracted
best match attribute values reference set member chosen
record linkage step. step extract attributes easily represented
reference sets, prices dates. Although already schema
standardized attributes required create relational data set posts, still
extract actual attributes embedded within post accurately
learn extract attributes represented reference set, prices dates.
attributes extracted using regular expressions, extract actual
attributes within post might able accurately. example, consider
Ford 500 car. Without actually extracting attributes within post, might
extract 500 price, actually car name. overall approach outlined
Figure 1.
Although previously describe similar approach semantically annotating posts
(Michelson & Knoblock, 2005), paper extends research combining annotation work scalable record matching (Michelson & Knoblock, 2006).
make matching step annotation scalable, demonstrates
work efficient record matching extends unique problem matching posts,
embedded attributes, structured, relational data. paper presents
detailed description past work, including thorough evaluation procedure previously, using larger experimental data sets including reference set
includes tens thousands records.
article organized follows. first describe algorithm aligning
posts best matching members reference set Section 2. particular,
show matching takes place, efficiently generate candidate matches
make matching procedure scalable. Section 3, demonstrate
exploit matches extract attributes embedded within post. present
experiments Section 4, validating approaches blocking, matching information
extraction unstructured ungrammatical text. follow discussion
results Section 5 present related work Section 6. finish final
thoughts conclusions Section 7.

7. product Fetch Technologies http://www.fetch.com/products.asp

546

fiRelational Data Unstructured Data Sources

Figure 1: Creating relational data unstructured sources

2. Aligning Posts Reference Set
exploit reference set attributes create relational data posts, algorithm needs first decide member reference set best matches post.
matching, known record linkage (Fellegi & Sunter, 1969), provides schema attribute values necessary query integrate unstructured ungrammatical data
source. Record linkage broken two steps: generating candidate matches, called
blocking; separating true matches candidates matching
step.
approach, blocking generates candidate matches based similarity methods
certain attributes reference set compare posts. cars
example, algorithm may determine generate candidates finding common
tokens posts make attribute reference set. step detailed
Section 2.1 crucial limiting number candidates matches later examine
matching step. generating candidates, algorithm generates large set
features post candidate matches reference set. Using
features, algorithm employs machine learning methods separate true matches
false positives generated blocking. matching detailed Section 2.2.
547

fiMichelson & Knoblock

2.1 Generating Candidates Learning Blocking Schemes Record Linkage
infeasible compare post members reference set. Therefore
preprocessing step generates candidate matches comparing records
sets using fast, approximate methods. called blocking thought
partitioning full cross product record comparisons mutually exclusive blocks
(Newcombe, 1967). is, block attribute, first sort cluster data sets
attribute. apply comparison method single member block.
blocking, candidate matches examined detail discover true matches.
two main goals blocking. First, blocking limit number candidate matches, limits number expensive, detailed comparisons needed
record linkage. Second, blocking exclude true matches set candidate matches. means trade-off finding matching records
limiting size candidate matches. So, overall goal blocking make
matching step scalable, limiting number comparisons must make,
hindering accuracy passing many true matches possible.
blocking done using multi-pass approach (Hernandez & Stolfo, 1998),
combines candidates generated independent runs. example, cars
data, might make one pass data blocking tokens car model,
another run might block using tokens make along common tokens trim
values. One view multi-pass approach rule disjunctive normal form,
conjunction rule defines run, union rules combines
candidates generated run. Using example, rule might become ({tokenmatch, model} ({token-match, year}) ({token-match, make})). effectiveness
multi-pass approach hinges upon methods attributes chosen conjunctions.
Note conjunction set {method, attribute} pairs, make
restrictions methods used. set methods could include full string
metrics cosine similarity, simple common token matching outlined above, even
state-of-the-art n-gram methods shown experiments. key methods
necessarily choosing fastest (though show account method speed
below), rather choosing methods generate smallest set candidate
matches still cover true positives, since matching step consume
time.
Therefore, blocking scheme include enough conjunctions cover many true
matches can. example, first conjunct might cover true matches
datasets compared overlap years, second conjunct
cover rest true matches. adding independent runs
multi-pass approach.
However, since blocking scheme includes many conjunctions needs,
conjunctions limit number candidates generate. example, second
conjunct going generate lot unnecessary candidates since return records
share make. adding {method, attribute} pairs conjunction,
limit number candidates generates. example, change ({token-match,
548

fiRelational Data Unstructured Data Sources

make}) ({token-match, make} {token-match, trim}) still cover new true matches,
generate fewer additional candidates.
Therefore effective blocking schemes learn conjunctions minimize false
positives, learn enough conjunctions cover many true matches possible. two goals blocking clearly defined Reduction Ratio Pairs
Completeness (Elfeky, Verykios, & Elmagarmid, 2002).
Reduction Ratio (RR) quantifies well current blocking scheme minimizes
number candidates. Let C number candidate matches N size
cross product data sets.
RR = 1 C/N
clear adding {method,attribute} pairs conjunction increases
RR, changed ({token-match, zip}) ({token-match, zip} {token-match,
first name}).
Pairs Completeness (PC) measures coverage true positives, i.e., many
true matches candidate set versus entire set. Sm number
true matches candidate set, Nm number matches entire dataset,
then:
P C = Sm /Nm
Adding disjuncts increase PC. example, added second conjunction example blocking scheme first cover matches.
blocking approach paper, Blocking Scheme Learner (BSL), learns effective
blocking schemes disjunctive normal form maximizing reduction ratio pairs
completeness. way, BSL tries maximize two goals blocking. Previously
showed BSL aided scalability record linkage (Michelson & Knoblock, 2006),
paper extends idea showing work case matching posts
reference set records.
BSL algorithm uses modified version Sequential Covering Algorithm (SCA),
used discover disjunctive sets rules labeled training data (Mitchell, 1997).
case, SCA learn disjunctive sets conjunctions consisting {method, attribute}
pairs. Basically, call LEARN-ONE-RULE generates conjunction, BSL keeps
iterating call, covering true matches left iteration. way
SCA learns full blocking scheme. BSL algorithm shown Table 2.
two modifications classic SCA algorithm, shown bold.
First, BSL runs examples left cover, rather stopping
threshold. ensures maximize number true matches generated
candidates final blocking rule (Pairs Completeness). Note might, turn,
yield large number candidates, hurting Reduction Ratio. However, omitting true
matches directly affects accuracy record linkage, blocking preprocessing step
record linkage, important cover many true matches possible.
way BSL fulfills one blocking goals: eliminating true matches possible. Second,
learn new conjunction (in LEARN-ONE-RULE step) current blocking
scheme rule already contains newly learned rule, remove
rule containing newly learned rule. optimization allows us check rule
containment go, rather end.
549

fiMichelson & Knoblock

Table 2: Modified Sequential Covering Algorithm
SEQUENTIAL-COVERING(class, attributes, examples)
LearnedRules {}
Rule LEARN-ONE-RULE (class, attributes, examples)
examples left cover,
LearnedRules LearnedRules Rule
Examples Examples - {Examples covered Rule}
Rule LEARN-ONE-RULE (class, attributes, examples)
Rule contains previously learned rules, remove
contained rules.
Return LearnedRules

rule containment possible guarantee learn less restrictive
rules go. prove guarantee follows. proof done contradiction.
Assume two attributes B, method X. Also, assume previously
learned rules contain following conjunction, ({X, A}) currently learned rule
({X, A} {X, B}). is, assume learned rules contains rule less
specific currently learned rule. case, must least
one training example covered ({X, A} {X, B}) covered ({X, A}), since
SCA dictates remove examples covered ({X, A}) learn it. Clearly,
cannot happen, since examples covered specific ({X, A} {X, B})
would covered ({X, A}) already removed, means could
learned rule ({X, A} {X, B}). Thus, contradiction.
stated before, two main goals blocking minimize size candidate set, removing true matches set. already mentioned
BSL maximizes number true positives candidate set describe
BSL minimizes overall size candidate set, yields scalable record
linkage. minimize candidate sets size, learn restrictive conjunction
call LEARN-ONE-RULE SCA. define restrictive minimizing number candidates generated, long certain number true matches
still covered. (Without restriction, could learn conjunctions perfectly minimize
number candidates: simply return none.)
this, LEARN-ONE-RULE step performs general-to-specific beam search.
starts empty conjunction step adds {method, attribute} pair
yields smallest set candidates still cover least set number true matches.
is, learn conjunction maximizes Reduction Ratio,
time covering minimum value Pairs Completeness. use beam search allow
backtracking, since search greedy. However, since beam search goes
general-to-specific, ensure final rule restrictive possible. full
LEARN-ONE-RULE given Table 3.
constraint conjunction minimum PC ensures learned conjunction over-fit data. Without restriction, would possible
LEARN-ONE-RULE learn conjunction returns candidates, uselessly producing
optimal RR.
550

fiRelational Data Unstructured Data Sources

algorithms behavior well defined minimum PC threshold. Consider,
case algorithm learning restrictive rule minimum
coverage. case, parameter ends partitioning space cross product
example records threshold amount. is, set threshold amount 50%
examples covered, restrictive first rule covers 50% examples.
next rule covers 50% remaining, 25% examples. next
cover 12.5% examples, etc. sense, parameter well defined. set
threshold high, learn fewer, less restrictive conjunctions, possibly limiting RR,
although may increase PC slightly. set lower, cover examples,
need learn conjuncts. newer conjuncts, turn, may subsumed later
conjuncts, waste time learn. So, long parameter small
enough, affect coverage final blocking scheme, smaller
slows learning. set parameter 50% experiments8 .
analyze running time BSL show BSL take account
running time different blocking methods, need be. Assume x (method,
attribute) pairs (token, f irst name). Now, assume beam size b, since
use general-to-specific beam-search Learn-One-Rule procedure. Also, time
being, assume (method, attribute) pair generate blocking candidates O(1)
time. (We relax assumption later.) time hit Learn-One-Rule within BSL,
try rules beam (attribute, method) pairs current
beam rules. So, worst case, takes O(bx) time, since (method,
attribute) pair beam, try (method, attribute) pairs. Now,
worst case, learned disjunct would cover 1 training example, rule
disjunction pairs x. Therefore, run Learn-One-Rule x times, resulting
learning time O(bx2 ). e training examples, full training time O(ebx2 ),
BSL learn blocking scheme.
Now, assumed (method, attribute) runs O(1) time,
clearly case, since substantial amount literature blocking methods
8. Setting parameter lower 50% insignificant effect results, setting much
higher, 90%, increased PC small amount (if all), decreasing RR.

Table 3: Learning conjunction {method, attribute} pairs
LEARN-ONE-RULE (attributes, examples, min thresh, k)
Best-Conjunction {}
Candidate-conjunctions {method, attribute} pairs
Candidate-conjunctions empty,
ch Candidate-conjunctions
first iteration
ch ch {method,attribute}
Remove ch duplicates, inconsistent max. specific
REDUCTION-RATIO(ch) > REDUCTION-RATIO(Best-Conjunction)
PAIRS-COMPLETENESS(ch) min thresh
Best-Conjunction ch
Candidate-conjunctions best k members Candidate-conjunctions
return Best-conjunction

551

fiMichelson & Knoblock

blocking times vary significantly (Bilenko, Kamath, & Mooney, 2006). Let
us define function tx (e) represents long takes single (method, attribute)
pair x generate e candidates training example. Using notation,
Learn-One-Rule time becomes O(b(xtx (e))) (we run tx (e) time pair x)
full training time becomes O(eb(xtx (e))2 ). Clearly running time dominated
expensive blocking methodology. rule learned, bounded
time takes run rule (method, attribute) pairs involved, takes O(xtx (n)),
n number records classifying.
practical standpoint, easily modify BSL account time takes
certain blocking methods generate candidates. Learn-One-Rule step,
change performance metric reflect Reduction Ratio blocking time
weighted average. is, given Wrr weight Reduction Ratio Wb
weight blocking time, modify Learn-One-Rule maximize performance
disjunct based weighted average. Table 4 shows modified version LearnOne-Rule, changes shown bold.

Table 4: Learning conjunction {method, attribute} pairs using weights
LEARN-ONE-RULE (attributes, examples, min thresh, k)
Best-Conj {}
Candidate-conjunctions {method, attribute} pairs
Candidate-conjunctions empty,
ch Candidate-conjunctions
first iteration
ch ch {method,attribute}
Remove ch duplicates, inconsistent max. specific
SCORE(ch) = Wrr REDUCTION-RATIO(ch)+Wb BLOCK-TIME(ch)
SCORE(Best-Conj) = Wrr REDUCTION-RATIO(Best-conj)+Wb BLOCK-TIME(Best-conj)
SCORE(ch) > SCORE(Best-conj)
PAIRS-COMPLETENESS(ch) min thresh
Best-conj ch
Candidate-conjunctions best k members Candidate-conjunctions
return Best-conj

Note set Wb 0, using version Learn-One-Rule
used throughout paper, consider Reduction Ratio. Since
methods (token n-gram match) simple compute, requiring time build
initial index candidate generation, safely set Wb 0. Also,
making trade-off time versus reduction might always appropriate decision.
Although method may fast, sufficiently reduce reduction ratio,
time takes record linkage step might increase time would
taken run blocking using method provides larger increase reduction ratio.
Since classification often takes much longer candidate generation, goal
minimize candidates (maximize reduction ratio), turn minimizes classification
time. Further, key insight BSL choose blocking method,
importantly choose appropriate attributes block on. sense,
BSL feature selection algorithm blocking method. show
552

fiRelational Data Unstructured Data Sources

experiments, blocking important pick right attribute combinations,
BSL does, even using simple methods, blocking using sophisticated
methods.
easily extend BSL algorithm handle case matching posts members
reference set. special case posts attributes embedded
within reference set data relational structured schema elements.
handle special case, rather matching attribute method pairs across
data sources LEARN-ONE-RULE, instead compare attribute method
pairs relational data entire post. small change, showing
algorithm works well even special case.
learn good blocking scheme, efficiently generate candidates
post set align reference set. blocking step essential mapping large
amounts unstructured ungrammatical data sources larger larger reference
sets.
2.2 Matching Step
set candidates generated blocking one find member
reference set best matches current post. is, one data sources record (the
post) must align record data source (the reference set candidates).
whole alignment procedure referred record linkage (Fellegi & Sunter,
1969), refer finding particular matches blocking matching step.

Figure 2: traditional record linkage problem
However, record linkage problem presented article differs traditional
record linkage problem well studied. Traditional record linkage matches record
one data source record another data source relating respective,
decomposed attributes. instance, using second post Table 1, assuming
decomposed attributes, make post compared make reference
553

fiMichelson & Knoblock

Figure 3: problem matching post reference set
set. done models, trims, etc. record reference set
best matches post based similarities attributes would considered
match. represented Figure 2. Yet, attributes posts embedded
within single piece text yet identified. text compared reference
set, already decomposed attributes extraneous
tokens present post. Figure 3 depicts problem. type matching
traditional record linkage approaches apply.
Instead, matching step compares post attributes reference set
concatenated together. Since post compared whole record reference set
(in sense attributes), comparison record level
approximately reflects similar embedded attributes post
attributes candidate match. mimics idea traditional record linkage,
comparing fields determines similarity record level.
However, using record level similarity possible two candidates
generate record level similarity differing individual attributes. one
attributes discriminative other, needs way reflect
that. example, consider Figure 4. figure, two candidates share make
model. However, first candidate shares year second candidate shares
trim. Since candidates share make model, another
attribute common, possible generate record level comparison. Yet,
trim car, especially rare thing Hatchback discriminative
sharing year, since lots cars make, model year,
differ trim. difference individual attributes needs reflected.
discriminate attributes, matching step borrows idea traditional
record linkage incorporating individual comparisons attribute
554

fiRelational Data Unstructured Data Sources

Figure 4: Two records equal record level different field level similarities

data source best way determine match. is, record level
information enough discriminate matches, field level comparisons must exploited
well. field level comparisons matching step compares post
individual attribute reference set.
record field level comparisons represented vector different similarity functions called RL scores. incorporating different similarity functions, RL scores
reflects different types similarity exist text. Hence, record level
comparison, matching step generates RL scores vector post
attributes concatenated. generate field level comparisons, matching step calculates RL scores post individual attributes reference
set. RL scores vectors stored vector called VRL . populated,
VRL represents record field level similarities post member
reference set.
example reference set Figure 3, schema 4 attributes <make, model,
trim, year >. Assuming current candidate <Honda, Civic, 4D LX, 1993>,
VRL looks like:

VRL =<RL
RL
RL
RL
RL

scores(post,
scores(post,
scores(post,
scores(post,
scores(post,

Honda),
Civic),
4D LX),
1993),
Honda Civic 4D LX 1993)>

generally:
555

fiMichelson & Knoblock

VRL =<RL scores(post,
RL scores(post,
...,
RL scores(post,
RL scores(post,

attribute1 ),
attribute2 ),
attributen ),
attribute1 attribute2 . . . attributen )>

RL scores vector meant include notions many ways exist define
similarity textual values data sources. might case
one attribute differs another misplaced, missing changed letters. sort
similarity identifies two attributes similar, misspelled, called edit
distance. Another type textual similarity looks tokens attributes
defines similarity based upon number tokens shared attributes.
token level similarity robust spelling mistakes, puts emphasis
order tokens, whereas edit distance requires order tokens match
order attributes similar. Lastly, cases one attribute may sound
another, even spelled differently, one attribute may share common
root word another attribute, implies stemmed similarity. last two
examples neither token edit distance based similarities.
capture different similarity types, RL scores vector built three vectors reflect different similarity types discussed above. Hence, RL scores
is:
RL scores(post, attribute)=<token scores(post, attribute),
edit scores(post, attribute),
scores(post, attribute)>
vector token scores comprises three token level similarity scores. Two similarity
scores included vector based Jensen-Shannon distance, defines
similarities probability distributions tokens. One uses Dirichlet prior (Cohen,
Ravikumar, & Feinberg, 2003) smooths token probabilities using JelenikMercer mixture model (Zhai & Lafferty, 2001). last metric token scores vector
Jaccard similarity.
scores included, token scores vector takes form:
token scores(post, attribute)=<Jensen-Shannon-Dirichlet(post, attribute),
Jensen-Shannon-JM-Mixture(post, attribute),
Jaccard(post, attribute)>
vector edit scores consists edit distance scores comparisons
strings character level defined operations turn one string another.
instance, edit scores vector includes Levenshtein distance (Levenshtein, 1966),
returns minimum number operations turn string string T, SmithWaterman distance (Smith & Waterman, 1981) extension Levenshtein
distance. last score vector edit scores Jaro-Winkler similarity (Winkler
& Thibaudeau, 1991), extension Jaro metric (Jaro, 1989) used find
similar proper nouns. strict edit-distance, regard operations
transformations, Jaro-Winkler metric useful determinant string similarity.
character level metrics, edit scores vector defined as:
556

fiRelational Data Unstructured Data Sources

edit scores(post, attribute)=<Levenshtein(post, attribute),
Smith-Waterman(post, attribute),
Jaro-Winkler(post, attribute)>
similarities edit scores token scores vector defined SecondString package (Cohen et al., 2003) used experimental implementation
described Section 4.
Lastly, vector scores captures two types similarity fit
either token level edit distance similarity vector. vector includes two types
string similarities. first Soundex score post attribute.
Soundex uses phonetics token basis determining similarity.
is, misspelled words sound receive high Soundex score similarity.
similarity based upon Porter stemming algorithm (Porter, 1980),
removes suffixes strings root words compared similarity.
helps alleviate possible errors introduced prefix assumption introduced
Jaro-Winkler metric, since stems scored rather prefixes. Including
scores, scores vector becomes:
scores(post, attribute)=<Porter-Stemmer(post, attribute),
Soundex(post, attribute)>

Figure 5: full vector similarity scores used record linkage
Figure 5 shows full composition VRL , constituent similarity scores.
VRL constructed candidates, matching step performs
binary rescoring VRL help determine best match amongst candidates. rescoring helps determine best possible match post separating
557

fiMichelson & Knoblock

best candidate much possible. might candidates
similarly close values, one best match, rescoring emphasizes
best match downgrading close matches element values obvious non-matches, boosting difference score best
candidates elements.
rescore vectors candidate set C, rescoring method iterates
elements xi VRL C, VRL (s) contain maximum value xi map
xi 1, VRL (s) map xi 0. Mathematically, rescoring method is:
VRLj C, j = 0... |C|




xi VRLj , = 0... fiVRLj
(

f (xi , VRLj ) =

1, xi = max(xt VRLs , VRLs C, = i, = 0... |C|)
0, otherwise

example, suppose C contains 2 candidates, VRL1 VRL2 :
VRL1 = <{.999,...,1.2},...,{0.45,...,0.22}>
VRL2 = <{.888,...,0.0},...,{0.65,...,0.22}>
rescoring become:
VRL1 = <{1,...,1},...,{0,...,1}>
VRL2 = <{0,...,0},...,{1,...,1}>
rescoring, matching step passes VRL Support Vector Machine (SVM)
(Joachims, 1999) trained label matches non-matches. best match
candidate SVM classifies match, maximally positive score
decision function. one candidate share maximum score
decision function, thrown matches. enforces strict 1-1 mapping
posts members reference set. However, 1-n relationship captured
relaxing restriction. algorithm keeps either first candidate
maximal decision score, chooses one randomly set candidates
maximum decision score.
Although use SVMs paper differentiate matches non-matches,
algorithm strictly tied method. main characteristics learning
problem feature vectors sparse (because binary rescoring)
concepts dense (since many useful features may needed thus none
pruned feature selection). tried use Nave Bayes classifier matching
task, monumentally overwhelmed number features number
training examples. Yet say methods deal sparse
feature vectors dense concepts, online logistic regression boosting, could
used place SVM.
match post found, attributes matching reference set member
added annotation post including values reference set attributes
tags reflect schema reference set. overall matching algorithm
shown Figure 6.
558

fiRelational Data Unstructured Data Sources

Figure 6: approach matching posts records reference set
addition providing standardized set values query posts, standardized values allow integration outside sources values standardized
canonical values. instance, want integrate car classifieds safety
ratings website, easily join sources across attribute values.
manner, approaching annotation record linkage problem, create relational
data unstructured ungrammatical data sources. However, aid extraction
attributes easily represented reference sets, perform information extraction
posts well.

3. Extracting Data Posts
Although record linkage step creates relational data posts,
still attributes would extract post easily represented
reference sets, means record linkage step used attributes.
Examples attributes dates prices. Although many attributes
extracted using simple techniques, regular expressions, make
extraction annotation ever accurate using sophisticated information extraction.
motivate idea, consider Ford car model called 500. used regular
expressions, might extract 500 price car, would case.
However, try extract attributes, including model, would
extract 500 model correctly. Furthermore, might want extract actual
attributes post, are, extraction algorithm allows this.
perform extraction, algorithm infuses information extraction extra knowledge, rather relying possibly inconsistent characteristics. garner extra
559

fiMichelson & Knoblock

knowledge, approach exploits idea reference sets using attributes
matching reference set member basis identifying similar attributes post.
Then, algorithm label extracted values post schema
reference set, thus adding annotation based extracted values.
broad sense, algorithm two parts. First label token possible
attribute label junk ignored. tokens post labeled,
clean extracted labels. Figure 7 shows whole procedure graphically,
detail, using second post Table 1. steps shown figure
described detail below.

Figure 7: Extraction process attributes
begin extraction process, post broken tokens. Using first post
Table 1 example, set tokens becomes, {93, civic, 5speed,...}.
tokens scored attribute record reference set
deemed match.
score tokens, extraction process builds vector scores, VIE . VRL
vector matching step, VIE composed vectors represent similarities
token attributes reference set. However, composition
VIE slightly different VRL . contains comparison concatenation
attributes, vectors compose VIE different compose
VRL . Specifically, vectors form VIE called IE scores, similar
560

fiRelational Data Unstructured Data Sources

RL scores compose VRL , except contain token scores component, since
IE scores uses one token post time.
RL scores vector:
RL scores(post, attribute)=<token scores(post, attribute),
edit scores(post, attribute),
scores(post, attribute)>
becomes:
IE scores(token, attribute)=<edit scores(token, attribute),
scores(token, attribute)>
main difference VIE VRL VIE contains unique vector
contains user defined functions, regular expressions, capture attributes
easily represented reference sets, prices dates. attribute types
generally exhibit consistent characteristics allow extracted,
usually infeasible represent reference sets. makes traditional extraction methods
good choice attributes. vector called common scores types
characteristics used extract attributes common enough used
extraction.
Using first post Table 1, assume reference set match make Honda,
model Civic year 1993. means matching tuple would {Honda,
Civic, 1993}. match generates following VIE token civic post:
VIE =<common scores(civic),
IE scores(civic,Honda),
IE scores(civic,Civic),
IE scores(civic,1993)>
generally, given token, VIE looks like:
VIE =<common scores(token),
IE scores(token, attribute1 ),
IE scores(token, attribute2 )
...,
IE scores(token, attributen )>
VIE passed structured SVM (Tsochantaridis, Joachims, Hofmann,
& Altun, 2005; Tsochantaridis, Hofmann, Joachims, & Altun, 2004) trained give
attribute type label, make, model, price. Intuitively, similar attribute types
similar VIE vectors. makes generally high scores
make attribute reference set, small scores attributes. Further,
structured SVMs able infer extraction labels collectively, helps deciding
possible token labels. makes use structured SVMs ideal machine
learning method task. Note since VIE member cluster
winner takes all, binary rescoring.
Since many irrelevant tokens post annotated, SVM
learns VIE associate learned attribute type labeled
561

fiMichelson & Knoblock

junk, ignored. Without benefits reference set, recognizing junk
difficult characteristics text posts unreliable. example,
extraction relies solely capitalization token location, junk phrase Great Deal
might annotated attribute. Many traditional extraction systems work
domain ungrammatical unstructured text, addresses bibliographies,
assume token text must classified something, assumption
cannot made posts.
Nonetheless, possible junk token receive incorrect class label.
example, junk token enough matching letters, might labeled trim (since
trims may single letter two). leads noisy tokens within whole
extracted trim attribute. Therefore, labeling tokens individually gives approximation
data extracted.
extraction approach overcome problems generating noisy, labeled tokens
comparing whole extracted field analogue reference set attribute.
tokens post processed, whole attributes built compared corresponding attributes reference set. allows removal tokens introduce
noise extracted attribute.
removal noisy tokens extracted attribute starts generating two
baseline scores extracted attribute reference set attribute. One
Jaccard similarity, reflect token level similarity two attributes. However,
since many misspellings such, edit-distance based similarity metric,
Jaro-Winkler metric, used. baselines demonstrate accurately system
extracted/classified tokens isolation.
Using first post Table 1 ongoing example, assume phrase civic (ri)
extracted model. might occur car model Civic Rx,
instance. isolation, token (ri) could Rx model. Comparing
extracted car model reference attribute Civic generates Jaccard similarity 0.5
Jaro-Winkler score 0.83. shown top Figure 8.
Next, cleaning method goes extracted attribute, removing one token
time calculating new Jaccard Jaro-Winkler similarities. new scores
higher baselines, token becomes removal candidate. tokens
processed way, removal candidate highest scores removed,
whole process repeated. scores derived using removed token become
new baseline compare against. process ends tokens
yield improved scores baselines.
Shown Iteration 1 Figure 8, cleaning method finds (ri) removal
candidate since removing token extracted car model yields Jaccard score
1.0 Jaro-Winkler score 1.0, higher baseline scores. Since
highest scores trying token iteration, removed
baseline scores update. Then, since none remaining tokens provide improved scores
(since none), process terminates, yielding accurate attribute value.
shown Iteration 2 Figure 8. Note process would keep iterating,
tokens removed improve scores baseline. pseudocode
algorithm shown Figure 9.
562

fiRelational Data Unstructured Data Sources

Figure 8: Improving extraction accuracy reference set attributes
Note, however, limit machine learning component extraction
algorithm SVMs. Instead, claim cases, reference sets aid extraction
general, test this, architecture replace SVM component
methods. example, extraction experiments replace SVM extractor
Conditional Random Field (CRF) (Lafferty, McCallum, & Pereira, 2001) extractor
uses VIE features.
Therefore, whole extraction process takes token text, creates VIE
passes machine-learning extractor generates label token.
field cleaned extracted attribute saved.

4. Results
Phoebus system built experimentally validate approach building relational
data unstructured ungrammatical data sources. Specifically, Phoebus tests
techniques accuracy record linkage extraction, incorporates
BSL algorithm learning using blocking schemes. experimental data, comes
three domains posts: hotels, comic books, cars.
data hotel domain contains attributes hotel name, hotel area, star
rating, price dates, extracted test extraction algorithm. data
comes Bidding Travel website9 forum users share successful
bids Priceline items airline tickets hotel rates. experimental data
limited postings hotel rates Sacramento, San Diego Pittsburgh,
compose data set 1125 posts, 1028 posts match reference
set. reference set comes Bidding Travel hotel guides, special
9. www.biddingfortravel.com

563

fiMichelson & Knoblock

Algorithm 3.1: CleanAttribute(E, R)
comment: Clean extracted attribute E using reference set attribute R
RemovalCandidates C null
JaroW inklerBaseline JaroWinkler(E, R)
JaccardBaseline Jaccard(E, R)
token E

X RemoveToken(t, E)




JaroW inklerXt JaroWinkler(X , R)






Xt Jaccard(X , R)

Jaccard

JaroW
inklerXt >JaroW inklerBaseline









Jaccard >Jaccard


Xt
Baseline

n



C C
(



C = null
return (E)
(
E RemoveMaxCandidate(C,E)
else
CleanAttribute(E, R)

Figure 9: Algorithm clean extracted attribute
posts listing hotels ever posted given area. special posts provide
hotel names, hotel areas star ratings, reference set attributes. Therefore,
3 attributes standardized values used, allowing us treat
posts relational data set. reference set contains 132 records.
experimental data comic domain comes posts items sale
eBay. generate data set, eBay searched keywords Incredible Hulk
Fantastic Four comic books section website. (This returned items
comics, tshirts sets comics limited searched
for, makes problem difficult.) returned records contain attributes
comic title, issue number, price, publisher, publication year description,
extracted. (Note: description word description commonly associated
comic book, 1st appearance Rhino.) total number posts data
set 776, 697 matches. comic domain reference set uses data
Comics Price Guide10 , lists Incredible Hulk Fantastic Four comics.
reference set attributes title, issue number, description, publisher contains
918 records.
cars data consists posts made Craigslist regarding cars sale. dataset
consists classifieds cars Los Angeles, San Francisco, Boston, New York, New
10. http://www.comicspriceguide.com/

564

fiRelational Data Unstructured Data Sources

Jersey Chicago. total 2,568 posts data set, post
contains make, model, year, trim price. reference set Cars domain comes
Edmunds11 car buying guide. data set extracted make, model,
year trim cars 1990 2005, resulting 20,076 records. 15,338
matches posts Craigslist cars Edmunds.
Unlike hotels comics domains, strict 1-1 relationship post
reference set enforced cars domain. described previously, Phoebus relaxed 1-1 relationship form 1-n relationship posts reference
set. Sometimes records contain enough attributes discriminate single best
reference member. instance, posts contain model year might match
couple reference set records would differ trim attribute,
make, model, year. Yet, still use make, model year accurately
extraction. So, case, mentioned previously, pick one matches. way,
exploit attributes reference set, since confidence
those.
experiments, posts domain split two folds, one training
one testing. usually called two-fold cross validation. However, many cases twofold cross validation results using 50% data training 50% testing.
believe much data label, especially data sets become large,
experiments instead focus using less training data. One set experiments uses 30%
posts training tests remaining 70%, second set experiments
uses 10% posts train, testing remaining 90%. believe training
small amounts data, 10%, important empirical procedure since real
world data sets large labeling 50% large data sets time consuming
unrealistic. fact, size Cars domain prevented us using 30% data
training, since machine learning algorithms could scale number training
tuples would generate. Cars domain run experiments training
10% data. experiments performed 10 times, average results
10 trials reported.
4.1 Record Linkage Results
subsection report record linkage results, broken separate discussions
blocking results matching results.
4.1.1 Blocking Results
order BSL algorithm learn blocking scheme, must provided methods
use compare attributes. domains experiments use two common
methods. first, call token, compares matching token
attributes. second method, ngram3, considers matching 3-grams
attributes.
important note comparison BSL blocking methods,
Canopies method (McCallum, Nigam, & Ungar, 2000) Bigram indexing (Baxter,
Christen, & Churches, 2003), slightly misaligned algorithms solve different
11. www.edmunds.com

565

fiMichelson & Knoblock

problems. Methods Bigram indexing techniques make process
blocking pass attribute efficient. goal BSL, however, select
attribute combinations used blocking whole, trying different attribute
method pairs. Nonetheless, contend important select right attribute
combinations, even using simple methods, use sophisticated methods,
without insight attributes might useful. test hypothesis, compare
BSL using token 3-gram methods Bigram indexing attributes.
equivalent forming disjunction attributes using Bigram indexing
method. chose Bigram indexing particular designed perform fuzzy
blocking seems necessary case noisy post data. stated previously (Baxter
et al., 2003), use threshold 0.3 Bigram indexing, since works best.
compare BSL running disjunction attributes using simple token method
only. results, call blocking rule Disjunction. disjunction mirrors
idea picking simplest possible blocking method: namely using attributes
simple method.
stated previously, two goals blocking quantified Reduction Ratio
(RR) Pairs Completeness (PC). Table 5 shows values
many candidates generated average entire test set, comparing three
different approaches. Table 5 shows long took method learn rule
run rule. Lastly, column Time match shows long classifier needs
run given number candidates generated blocking scheme.
Table 6 shows example blocking schemes algorithm generated.
comparison attributes BSL selected attributes picked manually different
domains data structured reader pointed previous work
topic (Michelson & Knoblock, 2006).
results Table 5 validate idea important pick correct
attributes block (using simple methods) use sophisticated methods without
attention attributes. Comparing BSL rule Bigram results, combination
PC RR always better using BSL. Note although Cars domain Bigram
took significantly less time classifier due large RR,
PC 4%. case, Bigrams even covering 5% true matches.
Further, BSL results better using simplest method possible (the Disjuction), especially cases many records test upon. number
records scales up, becomes increasingly important gain good RR, maintaining
good PC value well. savings dramatically demonstrated Cars domain,
BSL outperformed Disjunction PC RR.
One surprising aspect results prevalent token method within
domains. expect ngram method would used almost exclusively since
many spelling mistakes within posts. However, case. hypothesize
learning algorithm uses token methods occur regularity
across posts common ngrams would since spelling mistakes might vary quite
differently across posts. suggests might regularity, terms
learn data, across posts initially surmised.
Another interesting result poor reduction ratio Comic domain. happens
rules contain disjunct finds common token within comic
566

fiRelational Data Unstructured Data Sources

Hotels (30%)
BSL
Disjunction
Bigrams
Hotels (10%)
BSL
Disjunction
Bigrams
Comics (30%)
BSL
Disjunction
Bigrams
Comics (10%)
BSL
Disjunction
Bigrams
Cars (10%)
BSL
Disjunction
Bigrams

RR

PC

# Cands

Time Learn (s)

Time Run (s)

Time match (s)

81.56
67.02
61.35

99.79
99.82
72.77

19,153
34,262
40,151

69.25
0
0

24.05
12.49
1.2

60.93
109.00
127.74

84.47
66.91
60.71

99.07
99.82
90.39

20,742
44,202
52,492

37.67
0
0

31.87
15.676
1.57

65.99
140.62
167.00

42.97
37.39
36.72

99.75
100.00
69.20

284,283
312,078
315,453

85.59
0
0

36.66
45.77
102.23

834.94
916.57
926.48

42.97
37.33
36.75

99.74
100.00
88.41

365,454
401,541
405,283

34.26
0
0

35.65
52.183
131.34

1,073.34
1,179.32
1,190.31

88.48
87.92
97.11

92.23
89.90
4.31

5,343,424
5,603,146
1,805,275

465.85
0
0

805.36
343.22
996.45

25,114.09
26.334.79
8,484.79

Table 5: Blocking results using BSL algorithm (amount data used training shown
parentheses).

Hotels Domain (30%)
({hotel area,token} {hotel name,token} {star rating, token}) ({hotel name, ngram3})
Hotels Domain (10%)
({hotel area,token} {hotel name,token}) ({hotel name,ngram3})
Comic Domain (30%)
({title, token})
Comic Domain (10%)
({title, token}) ({issue number,token} {publisher,token} {title,ngram3})
Cars Domain (10%)
({make,token}) ({model,ngram3}) ({year,token} {make,ngram3})
Table 6: example blocking schemes learned domains.

567

fiMichelson & Knoblock

title. rule produces poor reduction ratio value attribute
across almost reference set records. say,
unique values BSL algorithm use blocking, reduction ratio small.
domain, two values comic title attribute, Fantastic Four
Incredible Hulk. makes sense blocking done using title attribute only,
reduction half, since blocking value Fantastic Four gets rid
Incredible Hulk comics. points interesting limitation BSL algorithm.
many distinct values different attribute method pairs BSL
use learn from, lack values cripples performance reduction
ratio. Intuitively though, makes sense, since hard distinguish good candidate
matches bad candidate matches share attribute values.
Another result worth mentioning Hotels domain get lower RR
PC use less training data. happens BSL algorithm
runs examples cover, last examples introduce new
disjunct produces lot candidates, covering true positives,
would cause RR decrease, keeping PC high rate.
fact happens case. One way curb behavior would set
sort stopping threshold BSL, said, maximizing PC
important thing, choose this. want BSL cover many true positives
can, even means losing bit reduction.
fact, next test notion explicitly. set threshold SCA
95% training examples covered, algorithm stops returns learned
blocking scheme. helps avoid situation BSL learns general conjunction, solely cover last remaining training examples. happens, BSL
might end lowering RR, expense covering last training examples,
rule learned cover last examples overly general returns many
candidate matches.
Domain
Hotels Domain
Thresh (30%)
95% Thresh (30%)
Comic Domain
Thresh (30%)
95% Thresh (30%)
Cars Domain
Thresh (10%)
95% Thresh (10%)

Record Linkage
F-Measure

RR

PC

90.63
90.63

81.56
87.63

99.79
97.66

91.30
91.47

42.97
42.97

99.75
99.69

77.04
67.14

88.48
92.67

92.23
83.95

Table 7: comparison BSL covering training examples, covering 95%
training examples

568

fiRelational Data Unstructured Data Sources

Table 7 shows use threshold Hotels Cars domain see
statistically significant drop Pairs Completeness statistically significant increase
Reduction Ratio.12 expected behavior since threshold causes BSL kick
SCA cover last training examples, turn allows BSL
retain rule high RR, lower PC. However, look record linkage
results, see threshold fact large effect.13 Although
statistically significant difference F-measure record linkage Hotels domain,
difference Cars domain dramatic. use threshold, candidates
discovered rule generated using threshold effect 10% final
F-measure match results.14 Therefore, since F-measure results differ much,
conclude worthwhile maximize PC learning rules BSL, even
RR may decrease. say, even presence noise, turn may lead
overly generic blocking schemes, BSL try maximize true matches covers,
avoiding even difficult cases cover may affect matching results.
see Table 7, especially true Cars domain matching much
difficult Hotels domain.
Interestingly, Comic domain see statistically significant difference
RR PC. across trials almost always learn rule
whether use threshold not, rule covers enough training examples
threshold hit. Further, statistically significant change F-measure
record linkage results domain. expected since BSL would generate
candidate matches, whether uses threshold not, since cases almost always
learns blocking rules.
results using BSL encouraging show algorithm works
blocking matching unstructured ungrammatical text relational data
source. means algorithm works special case too, case
traditional record linkage matching one structured source another.
means overall algorithm semantic annotation much scalable
using fewer candidate matches previous work (Michelson & Knoblock, 2005).
4.1.2 Matching Results
Since alignment approach hinges leveraging reference sets, becomes necessary
show matching step performs well. measure accuracy, experiments employ
usual record linkage statistics:
P recision =
Recall =

#CorrectM atches
#T otalM atchesM ade
#CorrectM atches
#P ossibleM atches

12. Bold means statistically significant using two-tailed t-test set 0.05
13. Please see subsection 4.1.2 description record linkage experiments results.
14. Much difference attributed non-threshold version algorithm learning final
predicate includes make attribute itself, version threshold learn.
Since make attribute value covers many records, generates many candidates results
increasing PC reducing RR.

569

fiMichelson & Knoblock

F easure =

2 P recision Recall
P recison + Recall

record linkage approach article compared WHIRL (Cohen, 2000).
WHIRL performs record linkage performing soft-joins using vector-based cosine similarities attributes. record linkage systems require decomposed attributes
matching, case posts. WHIRL serves benchmark
requirement. mirror alignment task Phoebus, experiment
supplies WHIRL two tables: test set posts (either 70% 90% posts)
reference set attributes concatenated approximate record level match.
concatenation used matching individual attribute,
obvious combine matching attributes construct whole matching reference
set member.
perform record linkage, WHIRL soft-joins across tables, produces
list matches, ordered descending similarity score. post matches
join, reference set member(s) highest similarity score(s) called match.
Cars domain matches 1-N, means 1 match reference
set exploited later information extraction step. mirror idea, number
possible matches 1-N domain counted number posts match
reference set, rather reference set members match. Also,
means add single match total number correct matches given
post, rather correct matches, since one matters. done
WHIRL Phoebus, accurately reflects well algorithm would perform
processing step information extraction step.
record linkage results Phoebus WHIRL shown Table 8. Note
amount training data domain shown parentheses. results
statistically significant using two-tailed paired t-test =0.05, except
precision WHIRL Phoebus Cars domain, precision
Phoebus trained 10% 30% training data Comic domain.
Phoebus outperforms WHIRL uses many similarity types distinguish
matches. Also, since Phoebus uses record level attribute level similarities,
able distinguish records differ discriminative attributes.
especially apparent Cars domain. First, results indicate difficulty
matching car posts large reference set. largest experimental domain yet
used problem, encouraging well approach outperforms baseline. interesting results suggest techniques equally accurate
terms precision (in fact, statistically significant difference
sense) Phoebus able retrieve many relevant matches. means
Phoebus capture rich features predict matches WHIRLs cosine similarity alone. expect behavior Phoebus notion field token
level similarity, using many different similarity measures. justifies use many
similarity types field record level information, since goal find many
matches can.
encouraging using 10% data labeling, Phoebus able
perform almost well using 30% data training. Since amount data
Web vast, label 10% data get comparative results preferable
570

fiRelational Data Unstructured Data Sources

Hotel
Phoebus (30%)
Phoebus (10%)
WHIRL
Comic
Phoebus (30%)
Phoebus (10%)
WHIRL
Cars
Phoebus (10%)
WHIRL

Precision

Recall

F-measure

87.70
87.85
83.53

93.78
92.46
83.61

90.63
90.09
83.13

87.49
85.35
73.89

95.46
93.18
81.63

91.30
89.09
77.57

69.98
70.43

85.68
63.36

77.04
66.71

Table 8: Record linkage results

cost labeling data great. Especially since clean annotation, hence
relational data, comes correctly matching posts reference set,
label much data important want technique widely applicable.
fact, faced practical issue Cars domain unable
use 30% training since machine learning method would scale number
candidates generated much training data. So, fact report good
results 10% training data allows us extend work much larger Cars
domain.
method performs well outperforms WHIRL, results above,
clear whether use many string metrics, inclusion attributes
concatenation SVM provides advantage. test advantages
piece, ran several experiments isolating ideas.
First, ran Phoebus matching concatenation attributes
reference set, rather concatenation attributes individually. Earlier,
stated use concatenation mirror idea record level similarity
use attribute mirror field level similarity. hypothesis cases,
post match different reference set records record level score (using
concatenation), matching different attributes. removing
individual attributes leaving concatenation matching, test
concatenation influences matching isolation. Table 9 shows results
different domains.
Cars Comic domains see improvement F-measure, indicating
using attributes concatenation much better matching using
concatenation alone. supports notion need method capture
significance matching individual attributes since attributes better indicators
matching others. interesting note domains, WHIRL
better job machine learning using concatenation, even though WHIRL
571

fiMichelson & Knoblock

Hotels
Phoebus (30%)
Concatenation
WHIRL
Comic
Phoebus (30%)
Concatenation
WHIRL
Cars
Phoebus (10%)
Concatenation
WHIRL

Precision

Recall

F-Measure

87.70
88.49
83.61

93.78
93.19
83.53

90.63
90.78
83.13

87.49
61.81
73.89

95.46
46.55
81.63

91.30
51.31
77.57

69.98
47.94
70.43

85.68
58.73
63.36

77.04
52.79
66.71

Table 9: Matching using concatenation

uses concatenation attributes. WHIRL uses informationretrieval-style matching find best match, machine learning technique tries
learn characteristics best match. Clearly, difficult learn
characteristics are.
Hotels domain, find statistically significant difference F-measure
using concatenation alone. means concatenation sufficient determine
matches, need individual fields play role. specifically,
hotel name area seem important attributes matching
including part concatenation, concatenation still distinguishable enough
records determine matches. Since two three domains see
huge improvement, never lose F-measure, using concatenation
individual attributes valid matching. Also, since two domains concatenation
alone worse WHIRL, conclude part reason Phoebus outperform
WHIRL use individual attributes matching.
next experiment tests important include string metrics
feature vector matching. test idea, compare using metrics using
one, Jensen-Shannon distance. choose Jensen-Shannon distance
outperformed TF/IDF even soft TF/IDF (one accounts fuzzy token
matches) task selecting right reference sets given set posts (Michelson
& Knoblock, 2007). results shown Table 10.
Table 10 shows, using metrics yielded statistically significant, large improvement F-measure Comic Cars domains. means
string metrics, edit distances, capturing similarities JensenShannon distance alone not. Interestingly, domains, using Phoebus
Jensen-Shannon distance dominate WHIRLs performance. Therefore,
results Table 10 Table 9 demonstrate Phoebus benefits combination
572

fiRelational Data Unstructured Data Sources

Hotels
Phoebus (30%)
Jensen-Shannon
WHIRL
Comic
Phoebus (30%)
Jensen-Shannon
WHIRL
Cars
Phoebus (10%)
Jensen-Shannon
WHIRL

Precision

Recall

F-Measure

87.70
89.65
83.61

93.78
92.28
83.53

90.63
90.94
83.13

87.49
65.36
73.89

95.46
69.96
81.63

91.30
67.58
77.57

69.98
72.87
70.43

85.68
59.43
63.36

77.04
67.94
66.71

Table 10: Using string metrics versus using Jensen-Shannon distance

many, varied similarity metrics along use individual attributes field level
similarities, aspects contribute Phoebus outperforming WHIRL.
case Hotels data, statistically significant difference
matching results, case metrics provide relevant information
matching. Therefore, matches missed Jensen-Shannon method
missed include metrics. Hence, either missed matches
difficult discover, string metric method yet capture
similarity. example, post token DT reference set record
match hotel area Downtown, abbreviation metric could capture
relationship. However, Phoebus include abbreviation similarity measure.
Since none techniques isolation consistently outperforms WHIRL, conclude
Phoebus outperforms WHIRL combines multiple string metrics, uses
individual attributes concatenation, and, stated Section 2.2, SVM classifier
well suited record linkage task. results justify inclusion many
metrics individual attributes, along use SVM classifier.
last matching experiment justifies binary rescoring mechanism. Table 11 shows
results performing binary rescoring record linkage versus performing
binary recoring. hypothesize earlier paper binary rescoring allow
classifier accurately make match decisions rescoring separates
best candidate much possible. Table 11 shows case, across
domains perform binary rescoring gain statistically significant amount
F-measure. shows record linkage easily able identify
true matches possible candidates difference record linkage
algorithm use binary rescoring.
573

fiMichelson & Knoblock

Hotels
Phoebus (30%)
Binary Rescoring
Phoebus (10%)
Binary Rescoring
Comic
Phoebus (30%)
Binary Rescoring
Phoebus (10%)
Binary Rescoring
Cars
Phoebus (10%)
Binary Rescoring

Precision

Recall

F-Measure

87.70
75.44
87.85
73.49

93.78
81.82
92.46
78.40

90.63
78.50
90.09
75.86

87.49
84.87
85.35
81.52

95.46
89.91
93.18
88.26

91.30
87.31
89.09
84.75

69.98
39.78

85.68
48.77

77.04
43.82

Table 11: Record linkage results without binary rescoring

4.2 Extraction Results
section presents results experimentally validate approach extracting
actual attributes embedded within post. compare approach two
information extraction methods rely structure grammar posts.
First, experiments compare Phoebus baseline Conditional Random Field
(CRF) (Lafferty et al., 2001) extractor. Conditional Random Field probabilistic
model label segment data. labeling tasks, Part-of-Speech tagging, CRFs outperform Hidden Markov Models Maximum-Entropy Markov Models.
Therefore, representing state-of-the-art probabilistic graphical model, present
strong comparison approach extraction. CRFs used effectively
information extraction. instance, CRFs used combine information extraction coreference resolution good results (Wellner, McCallum, Peng, & Hay, 2004).
experiments use Simple Tagger implementation CRFs MALLET
(McCallum, 2002) suite text processing tools.
Further, stated Section 3 Extraction, created version Phoebus
uses CRFs, call PhoebusCRF. PhoebusCRF uses extraction features
(VIE ) Phoebus using SVM, common score regular expressions
string similarity metrics. include PhoebusCRF show extraction general
benefit reference set matching.
Second, experiments compare Phoebus Natural Language Processing (NLP) based
extraction techniques. Since posts ungrammatical unreliable lexical characteristics, NLP based systems expected well type data.
Amilcare system (Ciravegna, 2001), uses shallow NLP extraction,
shown outperform symbolic systems extraction tasks, use Amilcare
system compare against. Since Amilcare exploit gazetteers extra
574

fiRelational Data Unstructured Data Sources

information, experiments Amilcare receives reference data gazetteer aid
extraction. Simple Tagger Amilcare used default settings.
Lastly, compare Phoebus trained using 30% data training Phoebus
trained using 10% data. (We PhoebusCRF well.) experimental
results, amount training data put parentheses.
One component extraction vector VIE vector common scores, includes
user defined functions, regular expressions. Since domain specific
functions used algorithm, common scores domain must specified.
Hotels domain, common scores includes functions matchPriceRegex
matchDateRegex. functions gives positive score token matches price
date regular expression, 0 otherwise. Comic domain, common scores contains
functions matchPriceRegex matchYearRegex, give positive scores
token matches regular expression. Cars domain, common scores uses function
matchPriceRegex (since year attribute reference set, use common
score capture form).
cars data set, posts labeled training testing
extraction. domain, labeled 702 posts extraction, use
training testing extraction algorithm. Note, however, Phoebus perform
extraction posts, able report results those. fact,
running demo Phoebus, Cars domain live.15
extraction results presented using Precision, Recall F-Measure. Note
extraction results field level results. means extraction counted
correct tokens compromise field post correctly labeled.
Although much stricter rubric correctness, accurately models useful
extraction system would be. Tables 12, 13 14 show results correctly labeling
tokens within posts correct attribute label Hotel, Comic Cars
domains, respectively. Attributes italics attributes exist reference set.
column Freq shows average number fields test set associated
label. Also, observe * means results highest Phoebus score (Phoebus
PhoebusCRF) highest baseline (Amilcare Simple Tagger CRF) F-Measure
statistically significant using two-tailed paired t-test =0.05.
Phoebus PhoebusCRF outperform systems almost attributes (13
16), shown Table 15. fact, one attribute baseline system
best: using Amilcare extract Date attribute Hotels domain.
attribute, Phoebus PhoebusCRF use common-score regular-expression
main identifying feature. Since regular expression user supplied, propose
better regular expression could make Phoebus/PhoebusCRF extract dates even
accurately, overcoming baseline. Since systems perform well using reference
set data aid extraction, results show using reference sets greatly aid
extraction. especially evident compare PhoebusCRF Simple Tagger
CRF, since difference two extraction methods reference set attribute
similarity scores common scores.
15. http://www.isi.edu/integration/Phoebus/demos.html demo uses extraction model trained
702 labeled extraction examples, running live months writing
article.

575

fiMichelson & Knoblock

Area

Date

Name

Price

Star

Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF(30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)

Hotel
Recall
83.73
77.80
85.13
80.71
78.62
64.78
85.41
82.13
87.20
84.39
63.60
86.18
77.27
75.59
85.70
81.46
74.43
58.96
93.06
93.12
92.56
90.34
71.68
88.04
97.39
96.94
96.83
96.17
97.16
95.58

Precision
84.76
83.58
86.93
83.38
79.38
71.59
87.02
83.06
87.11
84.48
63.25
94.10
75.18
74.25
85.07
81.69
84.86
67.44
98.38
98.46
94.90
92.60
73.45
91.10
97.01
96.90
98.06
96.74
96.55
97.35

F-Measure
84.23
80.52
86.02
82.01
79.00
68.01
86.21
82.59
87.15
84.43
63.42
89.97
76.21
74.92
85.38
81.57
79.29
62.91
95.65
95.72
93.71
91.46
72.55
89.54
97.20
96.92
97.44
96.45
96.85
96.46

Frequency
~580

~700

~750

~720

~730

Table 12: Field level extraction results: Hotels domain

576

fiRelational Data Unstructured Data Sources

Descript.

Issue

Price

Publisher

Title

Year

Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)
Phoebus (30%)
Phoebus (10%)
PhoebusCRF (30%)
PhoebusCRF (10%)
Simple Tagger CRF (30%)
Amilcare (30%)

Comic
Recall
32.43
30.16
26.02
15.45
32.30
8.00
83.39
80.90
87.77
83.01
78.31
77.66
68.09
39.84
51.06
29.09
44.24
41.21
100.00
99.85
77.91
53.22
78.13
63.75
89.34
89.37
92.93
90.64
93.57
89.88
78.44
77.50
76.24
54.63
39.93
77.05

Precision
30.71
27.15
33.03
26.83
34.75
52.55
83.65
82.17
88.70
84.68
77.81
89.11
90.00
60.00
85.34
55.40
84.44
66.67
85.38
83.89
88.30
87.29
88.52
90.48
89.34
89.37
93.70
92.13
92.79
95.65
97.69
97.35
93.46
85.07
72.89
85.67

F-Measure
31.51
28.52
28.95
18.54
33.43*
13.78
83.52
81.52
88.23
83.84
78.05
82.98
77.39*
46.91
61.16
35.71
55.77
50.93
92.09
91.18
82.50
64.26
82.72
74.75
89.34
89.37
93.31*
91.37
93.18
92.65
86.99
86.28
83.80
66.14
51.54
81.04

Frequency
~90

~510

~15

~60

~540

~100

Table 13: Field level extraction results: Comic domain.

577

fiMichelson & Knoblock

Make

Model

Price

Trim

Year

Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)
Phoebus (10%)
PhoebusCRF (10%)
Simple Tagger CRF (10%)
Amilcare (10%)

Cars
Recall
98.21
90.73
85.68
97.58
92.61
84.58
78.76
78.44
97.17
93.59
83.66
90.06
63.11
55.61
55.94
27.21
88.48
85.54
91.12
86.32

Precision
99.93
96.71
95.69
91.76
96.67
94.10
91.21
84.31
95.91
92.59
98.16
91.27
70.15
64.95
66.49
53.99
98.23
96.44
76.78
91.92

F-Measure
99.06
93.36
90.39
94.57
94.59
88.79
84.52
81.24
96.53
93.09
90.33
90.28
66.43
59.28
60.57
35.94
93.08
90.59
83.31
88.97

Frequency
~580

~620

~580

~375

~600

Table 14: Field level extraction results: Cars domain.

Domain
Hotel
Comic
Cars


Phoebus
1
2
5
8

Num. Max. F-Measures
PhoebusCRF Amilcare Simple Tagger
3
1
0
1
0
0
0
0
0
4
1
0

Total Attributes
5
6
5
16

Table 15: Summary results extraction showing number times system
statistically significant highest F-Measure attribute.

578

fiRelational Data Unstructured Data Sources

Phoebus performs especially well Cars domain, best system
attributes. One interesting thing note result record
linkage results spectacular Cars domain, good enough yield
high extraction results. times system picking best
match reference set, still picking one close enough
reference set attributes useful extraction. trim extraction
results lowest, often attribute determines match
non-match. record linkage step likely selects car close, differs trim,
match incorrect trim likely extracted correctly,
rest attributes extracted using reference set member.
couple interesting notes come results. One intriguing
aspects results allow us estimate level structure different
attributes within domain. Since CRFs rely structure tokens within
post structured SVM method, hypothesize domains
structure, PhoebusCRF perform best domains least structure,
Phoebus perform best. Table 15 shows case. PhoebusCRF dominates
Hotels domain, where, example, many posts structure star rating
comes hotel name. using structure allow extractor get
hotel name accurately using information. Therefore see
overall structure within Hotels domain PhoebusCRF method
performs best, Phoebus. Contrast Cars domain, highly
unstructured, Phoebus performs best across attributes. domain
many missing tokens order attributes varied. Comic domain
varied attributes exhibit structure not, Table
15 shows, cases Phoebus PhoebusCRF dominates. However, although
Hotels data exhibits structure, important aspect research using
Phoebus allows one perform extraction without assuming structure data.
Also, result worth noting price attribute Comic domain bit
misleading. fact, none systems statistically significant respect
prices extract F-Measures
systems.
Another aspect came light statistical significance generalization
algorithm. Hotels Comic domains, able use 30% 10%
data training, many cases statistically significant difference
F-Measures extracted attributes using Phoebus. Hotels domain
name, area date statistically significant F-Measures training 30%
10% data, Comic domain difference F-Measure
issue description attributes significant (though description borderline).
means 11 attributes domains, roughly half insignificant.
Therefore little difference extraction whether use 10% data training
30%, extraction algorithm generalizes well. important since labeling
data extraction time consuming expensive.
One interesting result note except comic price (which insignificant
systems) hotel date (which close), Phoebus, using either 10% 30%
training data, outperformed systems attributes included
579

fiMichelson & Knoblock

reference set. lends credibility claim earlier section training
system extract attributes, even reference set,
accurately extract attributes reference set training system
identify something not.
overall performance Phoebus validates approach semantic annotation.
infusing information extraction outside knowledge reference sets, Phoebus
able perform well across three different domains, representative different type
source posts: auction sites, Internet classifieds forum/bulletin boards.

5. Discussion
goal research produce relational data unstructured ungrammatical
data sources accurately queried integrated sources.
representing attributes embedded within post standardized values
reference set, support structural queries integration. instance,
perform aggregate queries treat data source relational database
now. Furthermore, standardized values performing joins across data sources,
key integration multiple sources. standardized values aid cases
post actually contain attribute. instance, Table 1, two
listings include make Honda. However, matched reference set,
contain standardized value attribute used querying
integrating posts. especially powerful since posts never explicitly stated
attribute values. reference set attributes provide solution cases
extraction extremely difficult. example, none systems extracted
description attribute Comic domain well. However, one instead considers
description attribute reference set, quantified record linkage
results Comic domain, yields improvement 50% F-Measure
identifying description post.
may seem using reference set attributes annotation enough since
values already cleaned, extraction unnecessary. However, case.
one thing, one may want see actual values entered different attributes.
instance, user might want discover common spelling mistake abbreviation
attribute. Also, cases extraction results outperform record
linkage results. happens even post matched incorrect member
reference set, incorrect member likely close correct match,
used correctly extract much information. strong example this,
consider Cars domain. F-measure record linkage results good
extraction results domain. means matches chosen
probably incorrect differ correct match something small.
example, true match could trim 2 Door incorrectly chosen
match might trim 4 Door, would still enough information,
rest trim tokens, year, make model correctly extract
different attributes post itself. performing extraction values
post itself, overcome mistakes record linkage step still
exploit information incorrectly chosen reference set member.
580

fiRelational Data Unstructured Data Sources

Extraction attributes helps system classify (and ignore) junk
tokens. Labeling something junk much descriptive labeled junk
many possible class labels could share lexical characteristics. helps improve
extraction results items reference set, prices dates.
topic reference sets, important note algorithm tied
single reference set. algorithm extends include multiple reference sets iterating
process reference set used.
Consider following two cases. First, suppose user wants extract conference
names cities individual lists each. approach confined using
one reference set, would require constructing reference set contains power set
cities crossed conference names. approach would scale many attributes
distinct sources. However, lists used two reference sets, one
attribute, algorithm run conference name data,
reference set cities. iterative exploitation reference sets allows n reference
set attributes added without combinatorial explosion.
next interesting case post contains one attribute.
example, user needs extract two cities post. one reference set used,
includes cross product cities. However, using single reference set city
names done slightly modifying algorithm. new algorithm makes first
pass city reference set. pass, record linkage match either
one cities matches best, tie them. case tie, choose
first match. Using reference city, system extract city post,
remove post. system simply runs process again,
catch second city, using same, single reference set. could repeated many
times needed.
One issue arises reference sets discrepancy users knowledge
domain experts generally create reference sets. Cars domain,
instance, users interchangeably use attribute values hatchback, liftback,
wagon. reference set never includes term liftback suggests synonym
hatchback used common speech, Edmunds automobile jargon. term
wagon used Edmunds, used cars users describe
hatchbacks. implies slight difference meaning two, according
reference set authors.
Two issues arise discrepancies. first users interchanging words
cause problems extraction record linkage,
overcome incorporating sort thesaurus algorithm. record linkage,
thesaurus could expand certain attribute values used matching, example including
hatchback liftback reference set attribute includes term wagon.
However, subtle issues here. mostly case hatchback
called wagon happen wagon called hatchback. frequency
replacement must taken consideration errant matches created.
automate line future research. issue arises trusting
correctness Edmunds source. assume Edmunds right define one car
wagon different meaning classifying hatchback. fact,
581

fiMichelson & Knoblock

Edmunds classifies Mazda Protege5 wagon, Kelly Blue Book16 classifies
hatchback. seems invalidate idea wagon different meaning
hatchback. appear simple synonyms, would remain unknown
without outside knowledge Kelly Blue Book. generally, one assumes
reference set correct set standardized values, absolute truth.
meaningful reference sets constructed agreed-upon
ontologies Semantic Web. instance, reference set derived ontology
cars created biggest automotive businesses alleviate many
issues meaning, thesaurus scheme could work discrepancies introduced
users, rather reference sets.

6. Related Work
research driven principal cost annotating documents
Semantic Web free, is, automatic invisible users (Hendler, 2001).
Many researchers followed path, attempting automatically mark documents
Semantic Web, proposed (Cimiano, Handschuh, & Staab, 2004; Dingli,
Ciravegna, & Wilks, 2003; Handschuh, Staab, & Ciravegna, 2002; Vargas-Vera, Motta,
Domingue, Lanzoni, Stutt, & Ciravegna, 2002). However, systems rely lexical
information, part-of-speech tagging shallow Natural Language Processing
extraction/annotation (e.g., Amilcare, Ciravegna, 2001). option
data ungrammatical, post data. similar vein, systems
ADEL (Lerman, Gazen, Minton, & Knoblock, 2004) rely structure identify
annotate records Web pages. Again, failure posts exhibit structure
makes approach inappropriate. So, fair amount work automatic
labeling, little emphasis techniques could label text unstructured
ungrammatical.
Although idea record linkage new (Fellegi & Sunter, 1969) well studied
even (Bilenko & Mooney, 2003) current research focuses matching one set
records another set records based decomposed attributes. little work
matching data sets one record single string composed data sets
attributes match on, case posts reference sets. WHIRL system
(Cohen, 2000) allows record linkage without decomposed attributes, shown
Section 4.1 Phoebus outperforms WHIRL, since WHIRL relies solely vector-based
cosine similarity attributes, Phoebus exploits larger set features
represent field record level similarity. note interest EROCS system
(Chakaravarthy, Gupta, Roy, & Mohania, 2006) authors tackle problem
linking full text documents relational databases. technique involves filtering
non-nouns text, finding matches database.
intriguing approach; interesting future work would involve performing similar filtering
larger documents applying Phoebus algorithm match remaining nouns
reference sets.
Using reference sets attributes normalized values similar idea data
cleaning. However, data cleaning algorithms assume tuple-to-tuple transformations
16. www.kbb.com

582

fiRelational Data Unstructured Data Sources

(Lee et al., 1999; Chaudhuri et al., 2003). is, function maps attributes
one tuple attributes another. approach would work ungrammatical
unstructured data, attributes embedded within post, maps
set attributes reference set.
Although work describes technique information extraction, many methods,
Conditional Random Fields (CRF), assume least structure extracted
attributes extraction. extraction experiments show, Phoebus outperforms methods, Simple Tagger implementation Conditional Random
Fields (McCallum, 2002). IE approaches, Datamold (Borkar, Deshmukh, &
Sarawagi, 2001) CRAM (Agichtein & Ganti, 2004), segment whole records (like bibliographies) attributes, little structural assumption. fact, CRAM even uses
reference sets aid extraction. However, systems require every token
record receive label, possible posts filled irrelevant, junk
tokens. Along lines CRAM Datamold, work Bellare McCallum (2007)
uses reference set train CRF extract data, similar PhoebusCRF
implementation. However, two differences PhoebusCRF work
(Bellare & McCallum, 2007). First, work Bellare McCallum (2007) mentions
reference set records matched using simple heuristics, unclear
done. work, matching done explicitly accurately record linkage. Second, work uses records reference set label tokens training
extraction module, PhoebusCRF uses actual values matching reference
set record produce useful features extraction annotation.
Another IE approach similar performs named entity recognition using SemiCRFs dictionary component (Cohen & Sarawagi, 2004), functions
reference set. However, work dictionaries defined lists single attribute
entities, finding entity dictionary look-up task. reference sets
relational data, finding match becomes record linkage task. Further, work
Semi-CRFs (Cohen & Sarawagi, 2004) focuses task labeling segments tokens
uniform label, especially useful named entity recognition. case
posts, however, Phoebus needs relax restriction cases
segments interrupted, case hotel name area middle
hotel name segment. So, unlike work, Phoebus makes assumptions
structure posts. Recently, Semi-CRFs extended use database records
task integrating unstructured data relational databases (Mansuri & Sarawagi, 2006).
work similar links unstructured data, paper citations,
relational databases, reference sets authors venues. difference
view record linkage task, namely finding right reference set tuple match.
paper, even though use matches database aid extraction, view
linkage task extraction procedure followed matching task. Lastly,
first consider structured SVMs information extraction. Previous work used
structured SVMs perform Named Entity Recognition (Tsochantaridis et al., 2005)
extraction task use reference sets.
method aiding information extraction outside information (in form
reference sets) similar work ontology-based information extraction (Embley,
Campbell, Jiang, Liddle, Ng, Quass, & Smith, 1999). Later versions work even talk
583

fiMichelson & Knoblock

using ontology-based information extraction means semantically annotate unstructured data car classifieds (Ding, Embley, & Liddle, 2006). However, contrast
work, information extraction performed keyword-lookup ontology
along structural contextual rules aid labeling. ontology contains
keyword misspellings abbreviations, look-up performed presence noisy data. believe ontology-based extraction approach less scalable
record linkage type matching task creating maintaining ontology requires
extensive data engineering order encompass possible common spelling mistakes
abbreviations. Further, new data added ontology, additional data engineering
must performed. work, simply add new tuples reference set. Lastly,
contrast work, ontology based work assumes contextual structural rules
apply, making assumption data extract from. work, make
assumptions structure text extracting from.
Yet another interesting approach information extraction using ontologies Textpresso system extracts data biological text (Muller & Sternberg, 2004).
system uses regular expression based keyword look-up label tokens text based
ontology. tokens labeled, Textpresso perform fact extraction
extracting sequences labeled tokens fit particular pattern, gene-allele
reference associations. Although system uses reference set extraction,
differs keyword look-up lexicon.
recent work learning efficient blocking schemes Bilenko et al., (2006) developed
system learning disjunctive normal form blocking schemes. However, learn
schemes using graphical set covering algorithm, use version Sequential
Covering Algorithm (SCA). similarities BSL algorithm work
mining association rules transaction data (Agrawal, Imielinski, & Swami, 1993).
algorithms discover propositional rules. Further, algorithms use multiple passes
data set discover rules. However despite similarities, techniques
really solve different problems. BSL generates set candidate matches minimal
number false positives. this, BSL learns conjunctions maximally specific
(eliminating many false positives) unions together single disjunctive rule (to
cover different true positives). Since conjunctions maximally specific, BSL uses
SCA underneath, learns rules depth-first, general specific manner (Mitchell,
1997). hand, work mining association rules (Agrawal et al., 1993) looks
actual patterns data represent internal relationships. may
many relationships data could discovered, approach covers
data breadth-first fashion, selecting set rules iteration extending
appending new possible item.

7. Conclusion
article presents algorithm semantically annotating text ungrammatical
unstructured. Unstructured, ungrammatical sources contain much information,
cannot support structured queries. technique allows informative use
sources. Using approach, eBay agents could monitor auctions looking best
deals, user could find average price four-star hotel San Diego. semantic
584

fiRelational Data Unstructured Data Sources

annotation necessary society transitions Semantic Web, information
requires annotation useful agents, users unwilling extra work
provide required annotation.
future, technique could link mediator framework (Thakkar, Ambite, &
Knoblock, 2004) automatically acquiring reference sets. similar automatically
incorporating secondary sources record linkage (Michalowski, Thakkar, & Knoblock,
2005). automatic formulation queries retrieve correct domain reference set
direction future research. mediator framework place, Phoebus could
incorporate many reference sets needed full coverage possible attribute values
attribute types.
Unsupervised approaches record linkage extraction topics future research. including unsupervised record linkage extraction mediator component, approach would entirely self-contained, making semantic annotation posts
automatic process. Also, current implementation gives one class label per
token. Ideally Phoebus would give token possible labels, remove extraneous tokens systems cleans attributes, described Section 3.
disambiguation lead much higher accuracy extraction.
Future work could investigate inclusion thesauri terms attributes,
frequency replacement terms taken consideration. Also, exploring technologies automatically construct reference sets (and eventually thesauri)
numerous ontologies Semantic Web intriguing research path.
long term goal annotation extraction unstructured, ungrammatical
sources involves automating entire process. record linkage extraction methods
could become unsupervised, approach could automatically generate incorporate reference sets, apply automatically annotate data source.
would ideal approach making Semantic Web useful user
involvement.

Acknowledgments
research based upon work supported part National Science Foundation award number IIS-0324955, part Air Force Office Scientific Research
grant number FA9550-07-1-0416, part Defense Advanced Research Projects
Agency (DARPA), Department Interior, NBC, Acquisition Services Division, Contract No. NBCHD030010.
U.S.Government authorized reproduce distribute reports Governmental purposes notwithstanding copyright annotation thereon. views conclusions
contained herein authors interpreted necessarily representing official policies endorsements, either expressed implied,
organizations person connected them.
585

fiMichelson & Knoblock

References
Agichtein, E., & Ganti, V. (2004). Mining reference tables automatic text segmentation.
Proceedings 10th ACM Conference Knowledge Discovery Data
Mining, pp. 20 29. ACM Press.
Agrawal, R., Imielinski, T., & Swami, A. (1993). Mining association rules sets
items large databases. Proceedings ACM SIGMOD International Conference Management Data, pp. 207216. ACM Press.
Baxter, R., Christen, P., & Churches, T. (2003). comparison fast blocking methods
record linkage. Proceedings 9th ACM SIGKDD Workshop Data Cleaning,
Record Linkage, Object Identification, pp. 2527.
Bellare, K., & McCallum, A. (2007). Learning extractors unlabeled text using relevant
databases. Proceedings AAAI Workshop Information Integration
Web, pp. 1016.
Bilenko, M., Kamath, B., & Mooney, R. J. (2006). Adaptive blocking: Learning scale
record linkage clustering. Proceedings 6th IEEE International Conference
Data Mining, pp. 8796.
Bilenko, M., & Mooney, R. J. (2003). Adaptive duplicate detection using learnable string
similarity measures. Proceedings 9th ACM International Conference
Knowledge Discovery Data Mining, pp. 3948. ACM Press.
Borkar, V., Deshmukh, K., & Sarawagi, S. (2001). Automatic segmentation text
structured records. Proceedings ACM SIGMOD International Conference
Management Data, pp. 175186. ACM Press.
Califf, M. E., & Mooney, R. J. (1999). Relational learning pattern-match rules
information extraction. Proceedings 16th National Conference Artificial
Intelligence, pp. 328334.
Chakaravarthy, V. T., Gupta, H., Roy, P., & Mohania, M. (2006). Efficiently linking text
documents relevant structured information. Proceedings International
Conference Large Data Bases, pp. 667678. VLDB Endowment.
Chaudhuri, S., Ganjam, K., Ganti, V., & Motwani, R. (2003). Robust efficient fuzzy
match online data cleaning. Proceedings ACM SIGMOD International Conference Management Data, pp. 313324. ACM Press.
Cimiano, P., Handschuh, S., & Staab, S. (2004). Towards self-annotating web.
Proceedings 13th International Conference World Wide Web, pp. 462471.
ACM Press.
Ciravegna, F. (2001). Adaptive information extraction text rule induction
generalisation.. Proceedings 17th International Joint Conference Artificial
Intelligence, pp. 12511256.
586

fiRelational Data Unstructured Data Sources

Cohen, W., & Sarawagi, S. (2004). Exploiting dictionaries named entity extraction: combining semi-markov extraction processes data integration methods. Proceedings
10th ACM International Conference Knowledge Discovery Data Mining,
pp. 8998, Seattle, Washington. ACM Press.
Cohen, W. W. (2000). Data integration using similarity joins word-based information
representation language. ACM Transactions Information Systems, 18 (3), 288321.
Cohen, W. W., Ravikumar, P., & Feinberg, S. E. (2003). comparison string metrics
matching names records. Proceedings ACM SIGKDD Workshop
Data Cleaning, Record Linkage, Object Consoliation, pp. 1318.
Crescenzi, V., Mecca, G., & Merialdo, P. (2001). Roadrunner: Towards automatic data
extraction large web sites. Proceedings 27th International Conference
Large Data Bases, pp. 109118. VLDB Endowment.
Ding, Y., Embley, D. W., & Liddle, S. W. (2006). Automatic creation simplified querying semantic web content: approach based information-extraction ontologies.
Proceedings Asian Semantic Web Conference, pp. 400414.
Dingli, A., Ciravegna, F., & Wilks, Y. (2003). Automatic semantic annotation using unsupervised information extraction integration. Proceedings K-CAP Workshop Knowledge Markup Semantic Annotation.
Elfeky, M. G., Verykios, V. S., & Elmagarmid, A. K. (2002). TAILOR: record linkage
toolbox. Proceedings 18th International Conference Data Engineering, pp.
1728.
Embley, D. W., Campbell, D. M., Jiang, Y. S., Liddle, S. W., Ng, Y.-K., Quass, D., &
Smith, R. D. (1999). Conceptual-model-based data extraction multiple-record
web pages. Data Knowledge Engineering, 31 (3), 227251.
Fellegi, I. P., & Sunter, A. B. (1969). theory record linkage. Journal American
Statistical Association, 64, 11831210.
Handschuh, S., Staab, S., & Ciravegna, F. (2002). S-cream - semi-automatic creation
metadata. Proceedings 13th International Conference Knowledge Engineering Knowledge Management, pp. 165184. Springer Verlag.
Hendler, J. (2001). Agents semantic web. IEEE Intelligent Systems, 16 (2), 3037.
Hernandez, M. A., & Stolfo, S. J. (1998). Real-world data dirty: Data cleansing
merge/purge problem. Data Mining Knowledge Discovery, 2 (1), 937.
Jaro, M. A. (1989). Advances record-linkage methodology applied matching
1985 census tampa, florida. Journal American Statistical Association, 89,
414420.
Joachims, T. (1999). Advances Kernel Methods - Support Vector Learning, chap. 11:
Making large-Scale SVM Learning Practical. MIT-Press.
587

fiMichelson & Knoblock

Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models segmenting labeling sequence data. Proceedings 18th
International Conference Machine Learning, pp. 282289. Morgan Kaufmann.
Lee, M.-L., Ling, T. W., Lu, H., & Ko, Y. T. (1999). Cleansing data mining
warehousing. Proceedings 10th International Conference Database
Expert Systems Applications, pp. 751760. Springer-Verlag.
Lerman, K., Gazen, C., Minton, S., & Knoblock, C. A. (2004). Populating semantic web.
Proceedings AAAI Workshop Advances Text Extraction Mining.
Levenshtein, V. I. (1966). Binary codes capable correcting deletions, insertions,
reversals. English translation Soviet Physics Doklady, 10 (8), 707710.
Mansuri, I. R., & Sarawagi, S. (2006). Integrating unstructured data relational
databases. Proceedings International Conference Data Engineering, p. 29.
IEEE Computer Society.
McCallum, A. (2002).
Mallet:
http://mallet.cs.umass.edu.



machine

learning



language

toolkit.

McCallum, A., Nigam, K., & Ungar, L. H. (2000). Efficient clustering high-dimensional
data sets application reference matching. Proceedings 6th ACM
SIGKDD, pp. 169178.
Michalowski, M., Thakkar, S., & Knoblock, C. A. (2005). Automatically utilizing secondary
sources align information across sources. AI Magazine, Special Issue Semantic
Integration, Vol. 26, pp. 3345.
Michelson, M., & Knoblock, C. A. (2005). Semantic annotation unstructured ungrammatical text. Proceedings 19th International Joint Conference Artificial
Intelligence, pp. 10911098.
Michelson, M., & Knoblock, C. A. (2006). Learning blocking schemes record linkage.
Proceedings 21st National Conference Artificial Intelligence.
Michelson, M., & Knoblock, C. A. (2007). Unsupervised information extraction unstructured, ungrammatical data sources world wide web. International Journal
Document Analysis Recognition (IJDAR), Special Issue Noisy Text Analytics.
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill, New York.
Muller, H.-M., & Sternberg, E. E. K. P. W. (2004). Textpresso: ontology-based information retrieval extraction system biological literature. PLoS Biology, 2 (11).
Muslea, I., Minton, S., & Knoblock, C. A. (2001). Hierarchical wrapper induction
semistructured information sources. Autonomous Agents Multi-Agent Systems,
4 (1/2), 93114.
588

fiRelational Data Unstructured Data Sources

Newcombe, H. B. (1967). Record linkage: design efficient systems linking records
individual family histories. American Journal Human Genetics, 19 (3),
335359.
Porter, M. F. (1980). algorithm suffix stripping. Program, 14 (3), 130137.
Smith, T. F., & Waterman, M. S. (1981). Identification common molecular subsequences.
Journal Molecular Biology, 147, 195197.
Soderland, S. (1999). Learning information extraction rules semi-structured free
text. Machine Learning, 34 (1-3), 233272.
Thakkar, S., Ambite, J. L., & Knoblock, C. A. (2004). data integration approach
automatically composing optimizing web services. Proceedings ICAPS
Workshop Planning Scheduling Web Grid Services.
Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector machine
learning interdependent structured output spaces. Proceedings 21st
International Conference Machine Learning, p. 104. ACM Press.
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods
structured interdependent output variables. Journal Machine Learning
Research, 6, 14531484.
Vargas-Vera, M., Motta, E., Domingue, J., Lanzoni, M., Stutt, A., & Ciravegna, F. (2002).
MnM: Ontology driven semi-automatic automatic support semantic markup.
Proceedings 13th International Conference Knowledge Engineering
Management, pp. 213221.
Wellner, B., McCallum, A., Peng, F., & Hay, M. (2004). integrated, conditional model
information extraction coreference application citation matching.
Proceedings 20th Conference Uncertainty Artificial Intelligence, pp. 593
601.
Winkler, W. E., & Thibaudeau, Y. (1991). application fellegi-sunter model
record linkage 1990 U.S. Decennial Census. Tech. rep., Statistical Research
Report Series RR91/09 U.S. Bureau Census.
Zhai, C., & Lafferty, J. (2001). study smoothing methods language models applied
ad hoc information retrieval. Proceedings 24th ACM SIGIR Conference
Research Development Information Retrieval, pp. 334342. ACM Press.

589


