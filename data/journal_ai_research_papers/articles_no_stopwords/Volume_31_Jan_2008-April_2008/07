journal artificial intelligence

submitted published

global inference sentence compression
integer linear programming
james clarke

jclarke ed ac uk

mirella lapata

mlap inf ed ac uk

school informatics
university edinburgh
buccleuch place
edinburgh eh lw uk

abstract
sentence compression holds promise many applications ranging summarization
subtitle generation work views sentence compression optimization
uses integer linear programming ilp infer globally optimal compressions
presence linguistically motivated constraints previous formulations
sentence compression recast ilps extend novel global
constraints experimental written spoken texts demonstrate improvements
state art

introduction
computational treatment sentence compression recently attracted much attention
literature task viewed producing summary single sentence
retains important information remains grammatical jing sentence
compression mechanism would greatly benefit wide range applications example
summarization could improve conciseness generated summaries jing
lin zajic door lin schwartz examples include compressing text
displayed small screens mobile phones pdas corston oliver
subtitle generation spoken transcripts vandeghinste pan producing
audio scanning devices blind grefenstette
sentence compression commonly expressed word deletion given input source sentence words x x x xn aim produce target compression
removing subset words knight marcu compression extensively studied across different modeling paradigms supervised
unsupervised supervised typically trained parallel corpus source sentences target compressions come many flavors generative aim model
probability target compression given source sentence directly galley
mckeown indirectly noisy channel model knight marcu
turner charniak whereas discriminative formulations attempt minimize error
rate training set include decision tree learning knight marcu maximum entropy riezler king crouch zaenen support vector machines nguyen
shimazu horiguchi ho fukushi large margin learning mcdonald
c

ai access foundation rights reserved

ficlarke lapata

unsupervised methods dispense parallel corpus generate compressions
rules turner charniak language model hori furui
despite differences formulation approaches model compression process
local information instance order decide words drop exploit
information adjacent words constituents local good job
producing grammatical compressions however somewhat limited scope since
cannot incorporate global constraints compression output constraints
consider sentence whole instead isolated linguistic units words constituents
give concrete example may want ensure target compression verb
provided source one first place verbal arguments present
compression pronouns retained constraints fairly intuitive
used instill linguistic task specific information model
instance application compresses text displayed small screens would
presumably higher compression rate system generating subtitles spoken
text global constraint could force former system generate compressions
fixed rate fixed number words
existing approaches model global properties compression
good reason finding best compression source sentence given space
possible compressions search process often referred decoding inference
become intractable many constraints overly long sentences typically
decoding solved efficiently dynamic programming often conjunction
heuristics reduce search space e g turner charniak dynamic
programming guarantees global optimum provided principle optimality holds principle states given current state optimal decision
remaining stages depend previously reached stages previously made
decisions winston venkataramanan however know false
case sentence compression example included modifiers left
noun compression probably include noun include verb
include arguments dynamic programming cannot
easily guarantee constraints hold
propose novel framework sentence compression incorporates
constraints compression output allows us optimal solution
formulation uses integer linear programming ilp general purpose exact framework
np hard specifically previously proposed recast
integer linear programs extend constraints express
linear inequalities decoding framework amounts finding best solution given
linear scoring function set linear constraints global local
although ilp previously used sequence labeling tasks roth yih
punyakanok roth yih zimak application natural language generation
less widespread present three compression within ilp framework
representative unsupervised knight marcu semi supervised hori furui
fully supervised modeling mcdonald propose small
number constraints ensuring compressions structurally semantically
n possible compressions n number words sentence



figlobal inference sentence compression

valid experimentally evaluate impact compression task cases
added constraints yield performance improvements
remainder organized follows section provides overview
related work section present ilp framework compression
employ experiments constraints introduced section section
discusses experimental set section presents discussion future
work concludes

related work
develop several ilp compression presenting
briefly summarize previous work addressing sentence compression emphasis data driven approaches next describe ilp techniques used
past solve inference natural language processing nlp
sentence compression
jing perhaps first tackle sentence compression
uses multiple knowledge sources determine phrases sentence remove central
system grammar checking module specifies sentential constituents
grammatically obligatory therefore present compression
achieved simple rules large scale lexicon knowledge sources include
wordnet corpus evidence gathered parallel corpus source target sentence
pairs phrase removed grammatically obligatory focus
local context reasonable deletion probability estimated parallel corpus
contrast jing bulk sentence compression relies exclusively corpus data modeling compression process without recourse extensive knowledge sources e g wordnet large number approaches
noisy channel model knight marcu approaches consist language
model p whose role guarantee compression output grammatical channel
model p x capturing probability source sentence x expansion
target compression decoder searches compression maximizes
p p x channel model acquired parsed version parallel corpus
essentially stochastic synchronous context free grammar aho ullman whose
rule probabilities estimated maximum likelihood modifications model
presented turner charniak galley mckeown improved

discriminative knight marcu riezler et al mcdonald
nguyen et al sentences represented rich feature space induced
parse trees goal learn words word spans deleted given
context instance knight marcus decision tree model compression
performed deterministically tree rewriting process inspired shift reduce
parsing paradigm nguyen et al render model probabilistic use
support vector machines mcdonald formalizes sentence compression largemargin learning framework without making reference shift reduce parsing model
compression classification task pairs words source sentence classified


ficlarke lapata

adjacent target compression large number features defined
words parts speech phrase structure trees dependencies features
gathered adjacent words compression words
dropped see section detailed account
compression developed written text mind hori
furui propose model automatically transcribed spoken text model
generates compressions word deletion without parallel data syntactic information way assuming fixed compression rate searches compression
highest score dynamic programming scoring function consists language model responsible producing grammatical output significance score
indicating whether word topical score representing speech recognizers
confidence transcribing given word correctly
integer linear programming nlp
ilps constrained optimization objective function
constraints linear equations integer variables see section details ilp
techniques recently applied several nlp tasks including relation extraction
roth yih semantic role labeling punyakanok et al generation
route directions marciniak strube temporal link analysis bramsen deshpande
lee barzilay set partitioning barzilay lapata syntactic parsing riedel
clarke coreference resolution denis baldridge
approaches combine local classifier inference procedure
ilp classifier proposes possible answers assessed presence global
constraints ilp used make final decision consistent constraints
likely according classifier example semantic role labeling task involves
identifying verb argument structure given sentence punyakanok et al first
use snow multi class classifier roth identify label candidate arguments
observe labels assigned arguments sentence often contradict
resolve conflicts propose global constraints e g argument
instantiated given verb every verb least one argument use
ilp reclassify output snow
dras develops document paraphrasing model ilp key premise
work cases one may want rewrite document conform
global constraints length readability style proposed model three
ingredients set sentence level paraphrases rewriting text set global constraints objective function quantifies effect incurred paraphrases
formulation ilp used select paraphrases apply
global constraints satisfied paraphrase generation falls outside scope ilp
model sentence rewrite operations mainly syntactic provided module
synchronous tree adjoining grammar tag shieber schabes unfortunately
proof concept presented implementation evaluation module left
future work
snows learning variation winnow update rule



figlobal inference sentence compression

work sentence compression optimization previously proposed reformulated context integer linear programming
allows us easily incorporate constraints decoding process constraints linguistically semantically motivated designed bring less local
syntactic knowledge model help preserve meaning source sentence
previous work identified several important features compression task knight
marcu mcdonald however use global constraints novel
knowledge although sentence compression explicitly formulated terms
optimization previous approaches rely optimization procedure generating
best compression decoding process noisy channel model searches best
compression given source channel however compression found usually sub optimal heuristics used reduce search space locally optimal
due search method employed example work turner charniak
decoder first searches best combination rules apply traverses
list compression rules removes sentences outside best compressions according channel model list eventually truncated compressions
hori furui mcdonald compression score maximized
dynamic programming however yield suboptimal see discussion
section
contrary nlp work ilp notable exception roth yih
view compression generation two stage process learning inference
carried sequentially e first local classifier hypothesizes list possible answers best answer selected global constraints integrate
learning inference unified framework decoding takes place presence
available constraints local global moreover investigate influence
constraint set across learning paradigms previous work typically formulates constraints single model e g snow classifier learning paradigm
e g supervised therefore assess constraint framework advocated
article influences performance expressive require large amounts
parallel data non expressive ones use little parallel data none
words able pose answer following question kinds
benefit constraint inference
work close spirit rather different content dras concentrate
compression specific paraphrase type apply sentence level
constraints thus affect document whole individual sentences furthermore compression generation integral part ilp whereas dras assumes
paraphrases generated separate process

framework
section present details proposed framework sentence compression
mentioned earlier work sentence compression directly optimization
n possible compressions source sentence many
unreasonable unlikely one compression satisfactory knight marcu ideally require function captures operations


ficlarke lapata

rules performed sentence create compression
time factoring desirable operation makes resulting compression
perform search possible compressions select best one determined
desirable wide range expressed framework
prerequisites implementing fairly low require decoding process expressed linear function set linear constraints practice many
rely markov assumption factorization usually solved dynamic programming decoding process formulated integer
linear programs little effort
first give brief introduction integer linear programming extension linear
programming readers unfamiliar mathematical programming compression
next described section constraints section
linear programming
linear programming lp optimization constraints
consist three parts
decision variables variables control wish assign
optimal values
linear function objective function function wish minimize
maximize function influences values assigned decision variables
constraints allow decision variables take certain
values restrictions constraints
terms best demonstrated simple example taken winston
venkataramanan imagine manufacturer tables chairs shall call
telfa corporation produce table hour labor square board feet wood
required chairs require hour labor square board feet wood telfa
hours labor square board feet wood available profit made
table gbp gbp chairs wish determine number tables
chairs manufactured maximize telfas profit
first must determine decision variables case define
x number tables manufactured
x number chairs manufactured
objective function value wish maximize namely profit
profit x x
two constraints must exceed hours labor
square board feet wood must used cannot create negative
amount chairs tables


figlobal inference sentence compression

labor constraint
x x
wood constraint
x x
variable constraints
x
x






decision variables objective function constraints determined
express lp model
max z x x objective function
subject
x x
x x
x
x

labor constraint
wood constraint



two basic concepts involved solving lp feasibility region
optimal solution optimal solution one constraints satisfied
objective function minimized maximized specification value
decision variable referred point feasibility region lp region
consisting set points satisfy lps constraints optimal solution
lies within feasibility region point minimum maximum objective
function value
set points satisfying single linear inequality half space feasibility region
defined intersection half spaces linear inequalities forms
polyhedron telfa example forms polyhedral set polyhedral convex set
intersection four constraints figure shows feasible region telfa
example optimal solution graph line hyperplane points
objective function value maximization called isoprofit
line minimization isocost line one isoprofit line represented
dashed black line figure one isoprofit line isoprofit
lines moving parallel original isoprofit line
extreme points polyhedral set defined intersections lines
form boundaries polyhedral set points b c figure
shown lp optimal solution extreme point globally
optimal reduces search space optimization finding extreme
point highest lowest value simplex dantzig solves lps
exploring extreme points polyhedral set specifically moves one extreme
point adjacent extreme point extreme points lie line segment
optimal extreme point found although simplex exponential
worst case complexity practice efficient


optimal solution telfa example z
x x thus
achieve maximum profit gbp must build tables chairs
obviously impossible would expect people buy fractions tables chairs
want able constrain decision variables
take integer values done integer linear programming


ficlarke lapata



b









lps feasible region

x x

x x









b



x

x





ip feasible point
ip relaxations feasible region





optimal lp solution

optimal lp solution


c



x x














x








x x















x









figure feasible region telfa example linear graph integer linear
graph b programming

integer linear programming
integer linear programming ilp lp
variables required non negative integers formulated similar manner
lp added constraint decision variables must take non negative
integer values
formulate telfa ilp model merely add constraints x
x must integer gives
max z x x objective function
subject
x x
x x
x
x


labor constraint

wood constraint
x integer
x integer

lp proved optimal solution lies extreme point
feasible region case integer linear programs wish consider points
integer values illustrated figure b telfa contrast
linear programming solved efficiently worst case integer programming
many practical situations np hard cormen leiserson rivest


figlobal inference sentence compression

fortunately ilps well studied optimization number techniques
developed optimal solution two techniques cutting planes
method gomory branch bound method land doig
briefly discuss methods detailed treatment refer interested
reader winston venkataramanan nemhauser wolsey
cutting planes method adds extra constraints slice parts feasible region
contains integer extreme points however process difficult
impossible nemhauser wolsey branch bound method enumerates
points ilps feasible region prunes sections region known
sub optimal relaxing integer constraints solving resulting
lp known lp relaxation solution lp relaxation integral
optimal solution otherwise resulting solution provides upper bound
solution ilp proceeds creating two sub
non integer solution one variable time solved process
repeats optimal integer solution found
branch bound method optimal solution telfa
z x x thus achieve maximum profit gbp telfa
must manufacture tables chairs relatively simple could
solved merely inspection ilp involve many variables constraints
resulting feasible region large number integer points branch bound
procedure efficiently solve ilps matter seconds forms part many
commercial ilp solvers experiments use lp solve free optimization package
relies simplex brand bound methods solving ilps
note special circumstances solving methods may applicable
example implicit enumeration used solve ilps variables binary
known pure implicit enumeration similar branch andbound method systematically evaluates possible solutions without however explicitly
solving potentially large number lps derived relaxation removes
much computational complexity involved determining sub infeasible furthermore class ilp known minimum cost network flow
mcnfp lp relaxation yields integral solution
therefore treated lp
general model yield optimal solution variables integers
constraint matrix property known total unimodularity matrix totally
unimodular every square sub matrix determinant equal
case constraint matrix looks totally unimodular easier
solve branch bound methods practice good
formulate ilps many variables possible coefficients
constraints winston venkataramanan
constraints logical conditions
although integer variables ilp may take arbitrary values frequently
restricted binary variables variables particularly useful rep software available http lpsolve sourceforge net



ficlarke lapata

condition
implication
iff

xor



statement
b
b
b c
xor b xor c
b


constraint
ba
ab
b c
b c
b


table represent logical conditions binary variables constraints ilp

resenting variety logical conditions within ilp framework use constraints table lists several logical conditions equivalent constraints
express transitivity e c b although often thought transitivity expressed polynomial expression binary
variables e ab c possible replace latter following linear inequalities williams

c
c b
c b
easily extended model indicator variables representing whether set binary
variables take certain values
compression
section describe three compression reformulate integer linear
programs first model simple language model used baseline
previous knight marcu second model work hori
furui combines language model corpus significance scoring
function omit confidence score derived speech recognizer since
applied text model requires small amount parallel data
learn weights language model significance score
third model fully supervised uses discriminative large margin framework
mcdonald trained trained larger parallel corpus chose model
instead popular noisy channel decision tree two reasons practical one theoretical one first mcdonalds model delivers performance superior
decision tree model turn performs comparably noisy channel second noisy channel entirely appropriate model sentence compression
uses language model trained uncompressed sentences even though represents
probability compressed sentences model consider compressed sentences less likely uncompressed ones discussion provided turner
charniak


figlobal inference sentence compression

language model
language model perhaps simplest model springs mind require
parallel corpus although relatively large monolingual corpus necessary training
naturally prefer short sentences longer ones furthermore language model
used drop words infrequent unseen training corpus knight
marcu use bigram language model baseline noisy channel
decision tree
let x x x xn denote source sentence wish generate target
compression introduce decision variable word source constrain
binary value represents word dropped whereas value includes
word target compression let




xi compression
n
otherwise

unigram language model objective function would maximize
overall sum decision variables e words multiplied unigram probabilities
probabilities throughout log transformed
max

n
x

p xi





thus word selected corresponding given value probability
p xi according language model counted total score
unigram language model probably generate many ungrammatical compressions
therefore use context aware model objective function namely trigram
model dynamic programming would typically used decode language model
traversing sentence left right manner efficient provides
context required conventional language model however difficult
impossible incorporate global constraints model decisions word
inclusion cannot extend beyond three word window formulating decoding process
trigram language model integer linear program able take account
constraints affect compressed sentence globally process much
involved task unigram case context instead must
make decisions word sequences rather isolated words first create
additional decision variables




ij





ijk

xi starts compression
n
otherwise

sequence xi xj ends
compression
n

otherwise
j n





sequence xi xj xk n
compression j n

otherwise
k j n


ficlarke lapata

objective function given equation sum possible trigrams
occur compressions source sentence x represents start
token xi ith word sentence x equation constrains decision variables
binary
max z

n
x

p xi start

n
n
x n
x x



ijk p xk xi xj

j k j



n
x

n
x

ij p end xi xj



j

subject

ij ijk



objective function allows combination trigrams selected
means invalid trigram sequences e g two trigrams containing end token
could appear target compression avoid situation introducing sequential
constraints decision variables ijk ij restrict set allowable
trigram combinations
constraint

exactly one word begin sentence
n
x







constraint word included sentence must start sentence
preceded two words one word start token x
k k

k
x k
x

ijk



j

k k n
constraint word included sentence must preceded one
word followed another must preceded one word end sentence
j

j
x

n
x

ijk

k j

j
x

ij





j j n

constraint word sentence must followed two words followed
one word end sentence must preceded one word end
sentence


n
x

n
x

j k j

ijk

n
x

j



ij


x

hi

h

n



figlobal inference sentence compression

constraint

exactly one word pair end sentence
n
x

n
x

ij



j

sequential constraints described ensure second order factorization
trigrams holds different compression specific constraints presented section
unless normalized sentence length language model naturally prefer one word
output normalization however non linear cannot incorporated ilp
formulation instead impose constraint length compressed sentence
equation forces compression contain least b tokens
n
x

b





alternatively could force compression exactly b tokens substituting
inequality equality less b tokens replacing
constraint language model specific used elsewhere
significance model
language model described notion content words include
compression thus prefers words seen words constituents
different relative importance different documents even sentences
inspired hori furui add objective function see equation
significance score designed highlight important content words hori furuis
original formulation word weighted score similar un normalized tf idf
significance score applied indiscriminately words sentence solely
topic related words namely nouns verbs score differs one respect combines
document level sentence level significance addition tf idf word
weighted level embedding syntactic tree
intuitively sentence multiply nested clauses deeply embedded clauses
tend carry semantic content illustrated figure depicts
clause embedding sentence mr field said resign reselected
move could divide party nationally important information
conveyed clauses resign reselected embedded
accordingly give weight words found clauses main
clause figure simple way enforce give clauses weight proportional
level embedding modified significance score becomes
xi

fa
l
log
n




xi topic word frequency xi document corpus
respectively fa sum topic words corpus l number clause
compression rate limited range including two inequality constraints



ficlarke lapata



mr field said

resign

reselected
move
sbar
could divide party nationally

figure clause embedding sentence mr field said resign
reselected move could divide party nationally nested boxes
correspond nested clauses

constituents xi n deepest level clause embedding fa
estimated large document collection document specific whereas nl sentencespecific figure term nl clause clause
individual words inherit weight clauses
modified objective function significance score given
max z

n
x

xi


n
x n
x



n
x

p xi start



n
x

ijk p xk xi xj

j k j



n
x

n
x

ij p end xi xj



j

add weighting factor objective order counterbalance importance language model significance score weight tuned small
parallel corpus sequential constraints equations used ensure
trigrams combined valid way
discriminative model
fully supervised model used discriminative model presented mcdonald
model uses large margin learning framework coupled feature set
defined compression bigrams syntactic structure
let x x xn denote source sentence target compression ym
yj occurs x function l yi n maps word yi target com

figlobal inference sentence compression

pression index word source sentence x include constraint
l yi l yi forces word x occur compression
let score compression sentence x


x

score factored first order markov assumption words target
compression give
x


x

x l yj l yj



j

score function defined dot product high dimensional feature
representation corresponding weight vector
x


x

w f x l yj l yj



j

decoding model amounts finding combination bigrams maximizes
scoring function mcdonald uses dynamic programming
maximum score found left right manner extension
viterbi case scores factor dynamic sub strings sarawagi cohen
mcdonald crammer pereira allows back pointers used
reconstruct highest scoring compression well k best compressions
similar trigram language model decoding process see section
except bigram model used consequently ilp formulation slightly
simpler trigram language model let




xi compression
n
otherwise

introduce decision variables


ij







xi starts compression
n
otherwise

word xi ends compression
otherwise
n

sequence xi xj compression n
otherwise
j n

discriminative model expressed
max z

n
x



n
x

x



x n



n
x

j
n
x




ij x j


ficlarke lapata

constraint

exactly one word begin sentence
n
x







constraint word included sentence must start compression
follow another word

j j

j
x

ij





j j n
constraint word included sentence must followed another
word end sentence



n
x

ij



j

n

constraint

exactly one word end sentence
n
x







sequential constraints equations necessary ensure
resulting combination bigrams valid
current formulation provides single optimal compression given model however mcdonalds dynamic programming capable returning k best
compressions useful learning described later order produce
k best compressions must rerun ilp extra constraints forbid previous
solutions words first formulate ilp solve add solution
k best list create set constraints forbid configuration decision
variables form current solution procedure repeated k compressions
found
computation compression score crucially relies dot product
high dimensional feature representation corresponding weight vector see equation mcdonald employs rich feature set defined adjacent words
individual parts speech dropped words phrases source sentence dependency structures source sentence features designed mimic
information presented previous noisy channel decision tree knight
marcu features adjacent words used proxy language model
noisy channel unlike treat parses gold standard mcdonald
uses dependency information another form evidence faced parses
noisy learning reduce weighting given features prove


figlobal inference sentence compression

poor discriminators training data thus model much robust
portable across different domains training corpora
weight vector w learned margin infused relaxed mira
crammer singer discriminative large margin online learning technique mcdonald crammer pereira b learns compressing sentence
comparing gold standard weights updated score
correct compression gold standard greater score compressions margin proportional loss loss function number words falsely
retained dropped incorrect compression relative gold standard source
sentence exponentially many compressions thus exponentially many margin
constraints render learning computationally tractable mcdonald et al b create
constraints k compressions currently highest score bestk x w
constraints
ready describe compression specific constraints presented
previous sections contain sequential constraints thus equivalent
original formulation constraints linguistically semantically motivated
similar fashion grammar checking component jing however
rely additional knowledge sources grammar lexicon wordnet
beyond parse grammatical relations source sentence obtain
rasp briscoe carroll domain independent robust parsing system english
however parser broadly similar output e g lin could serve
purposes constraints revolve around modification argument structure discourse
related factors
modifier constraints modifier constraints ensure relationships head words
modifiers remain grammatical compression
j



j xj xi ncmods
j



j xj xi detmods
equation guarantees include non clausal modifier ncmod compression adjective noun head modifier must included
repeated determiners detmod table illustrate constraints disallow deletion certain words starred sentences denote compressions
would possible given constraints example modifier word pasok
sentence compression head party included see b
want ensure meaning source sentence preserved
compression particularly face negation equation implements forcing
compression head included see sentence b table similar
constraint added possessive modifiers e g including genitives e g johns
clausal modifiers cmod adjuncts modifying entire clauses example ate cake
hungry clause modifier sentence ate cake



ficlarke lapata


b

b
c

b
c

e
f

became power player greek politics founded
socialist pasok party
became power player greek politics founded
pasok
took troubled youth dont fathers brought
room dads dont children
took troubled youth fathers brought
room dads children
took troubled youth dont fathers brought
room dads dont children
chain stretched uganda grenada nicaragua since
stretched uganda grenada nicaragua since
chain uganda grenada nicaragua since
chain stretched uganda grenada nicaragua since
chain stretched grenada nicaragua since
chain stretched uganda grenada nicaragua since
table examples compressions disallowed set constraints

gift shown equation example possessive constraint given
sentence c table
j



j xj xi ncmods xj
j



j xj xi possessive mods
argument structure constraints define intuitive constraints take
overall sentence structure account first constraint equation ensures
verb present compression arguments
arguments included compression verb must included thus
force program make decision verb subject object see
sentence b table
j



j xj subject object verb xi
second constraint forces compression contain least one verb provided
source sentence contains one well
x





xi verbs

constraint entails possible drop main verb stretched sentence see sentence c table


figlobal inference sentence compression

sentential constraints include equations apply prepositional phrases subordinate clauses constraints force introducing term
e preposition subordinator included compression word
within syntactic constituent included subordinator mean wh words
e g word subordinating conjunctions e g
although reverse true e introducing term included
least one word syntactic constituent included
j



j xj pp sub
xi starts pp sub
x

j



xi pp sub

j xj starts pp sub
example consider sentence table cannot drop preposition
uganda compression conversely must include uganda
compression see sentence e
wish handle coordination two head words conjoined source
sentence included compression coordinating conjunction must
included
j



k



j k



j k xj xk conjoined xi
consider sentence f table uganda nicaragua present
compression must include conjunction
finally equation disallows anything within brackets source sentence
included compression somewhat superficial attempt excluding
parenthetical potentially unimportant material compression




xi bracketed words inc parentheses
discourse constraints discourse constraint concerns personal pronouns specifically equation forces personal pronouns included compression
constraint admittedly important generating coherent documents opposed
individual sentences nevertheless impact sentence level compressions
particular verbal arguments missed parser pronominal
constraint grammatical output since argument structure
source sentence preserved compression

xi personal pronouns




ficlarke lapata

note constraints described would captured
learn synchronous deletion rules corpus example noisy channel
model knight marcu learns drop head latter modified
adjective noun since transformations dt nn dt ajd nn adj
almost never seen data similarly coordination constraint equations
would enforced turner charniaks special rules enhance
parallel grammar rules modeling structurally complicated deletions
attested corpus designing constraints aimed capturing appropriate
deletions many possible including rely training corpus
explicit notion parallel grammar e g mcdonald
modification constraints would presumably redundant noisy channel model
could otherwise benefit specialized constraints e g targeting sparse rules
noisy parse trees however leave future work
another feature modeling framework presented deletions nondeletions treated unconditional decisions example require drop
noun adjective noun sequences adjective deleted well require
include verb compression source sentence one hardwired decisions could cases prevent valid compressions considered instance
possible compress sentence appropriate behavior
appropriate orbob loves mary john loves susan bob loves mary john
susan admittedly lose expressive power yet ensure compressions
broadly grammatically even unsupervised semi supervised furthermore practice consistently outperform non constraint
alternatives without extensive constraint engineering
solving ilp
mentioned earlier section solving ilps np hard cases coefficient matrix unimodular shown optimal solution linear
program integral although coefficient matrix unimodular
obtained integral solutions sentences experimented approximately
see section details conjecture due fact variables coefficients constraints therefore constraint matrix
shares many properties unimodular matrix generate solve ilp every
sentence wish compress solve times less second per sentence including
input output overheads presented

experimental set
evaluation experiments motivated three questions constraintbased compression deliver performance gains non constraint ones
expect better compressions model variants incorporate compression specific
constraints differences among constraint would
investigate much modeling power gained addition constraints
example may case state art model mcdonalds
benefit much addition constraints effect much bigger less


figlobal inference sentence compression

sophisticated reported port across domains
particular interested assessing whether proposed constraints
general robust enough produce good compressions written spoken
texts
next describe data sets trained tested section
explain model parameters estimated section present evaluation setup
section discuss section
corpora
intent assess performance described written spoken
text appeal written text understandable since summarization work today
focuses domain speech data provides natural test bed compression
applications e g subtitle generation poses additional challenges spoken utterances ungrammatical incomplete often contain artefacts false starts
interjections hesitations disfluencies rather focusing spontaneous speech
abundant artefacts conduct study less ambitious domain
broadcast news transcripts lies extremes written text spontaneous speech scripted beforehand usually read autocue
previous work sentence compression almost exclusively used ziff davis corpus
training testing purposes corpus originates collection news articles
computer products created automatically matching sentences occur
article sentences occur abstract knight marcu abstract
sentences contain subset source sentences words word order
remain earlier work clarke lapata argued
ziff davis corpus ideal studying compression several reasons first showed
human authored compressions differ substantially ziff davis tends
aggressively compressed second humans likely drop individual words
lengthy constituents third test portion ziff davis contains solely sentences extremely small data set reveal statistically significant differences
among systems fact previous studies relied almost exclusively human judgments
assessing well formedness compressed output significance tests reported
subjects analyses
thus focused present study manually created corpora specifically
asked annotators perform sentence compression removing tokens sentence bysentence basis annotators free remove words deemed superfluous provided
deletions preserved important information source sentence
b ensured compressed sentence remained grammatical wished could leave
sentence uncompressed marking inappropriate compression
allowed delete whole sentences even believed contained information content
respect story would blur task abstracting following
guidelines annotators produced compressions newspaper articles sentences
british national corpus bnc american news text corpus henceforth
written corpus stories sentences hub english broadcast
news corpus henceforth spoken corpus written corpus contains articles la


ficlarke lapata

times washington post independent guardian daily telegraph spoken
corpus contains broadcast news variety networks cnn abc cspan npr
manually transcribed segmented story sentence level
corpora split training development testing sets randomly article
boundaries set containing full stories publicly available http
homepages inf ed ac uk data
parameter estimation
work present three compression ranging unsupervised semisupervised fully supervised unsupervised model simply relies trigram language model driving compression see section estimated million tokens north american corpus cmu cambridge language modeling
toolkit clarkson rosenfeld vocabulary size tokens goodturing discounting discourage one word output force ilp generate compressions whose length less source sentence see constraint
semi supervised model weighted combination word significance score
language model see section significance score calculated
million tokens american news text corpus optimized weight see
equation small subset training data three documents case powells method press teukolsky vetterling flannery loss function
f score grammatical relations found gold standard compression
systems best compression see section details optimal weight
approximately written corpus spoken corpus
mcdonalds supervised model trained written spoken training
sets implementation used feature sets mcdonald difference
phrase structure dependency features extracted output
roarks parser mcdonald uses charniaks parser performs comparably
model learnt k best compressions development data found
k provided best performance
evaluation
previous studies relied almost exclusively human judgments assessing wellformedness automatically derived compressions typically rated naive subjects two dimensions grammaticality importance knight marcu although
automatic evaluation measures proposed riezler et al bangalore rambow whittaker use less widespread suspect due small size
test portion ziff davis corpus commonly used compression work
evaluate output two ways first present
automatic evaluation measure put forward riezler et al compare
grammatical relations found system compressions found gold
standard allows us measure semantic aspects summarization quality terms
grammatical functional information quantified f score furthermore
splits sentences written corpus sentences spoken
corpus



figlobal inference sentence compression

clarke lapata relations f score correlates reliably
human judgments compression output since test corpora larger ziffdavis factor ten differences among systems highlighted
significance testing
implementation f score measure used grammatical relations annotations
provided rasp briscoe carroll parser particularly appropriate
compression task since provides parses full sentences sentence fragments
generally robust enough analyze semi grammatical sentences calculated f score
relations provided rasp e g subject direct indirect object modifier
total
line previous work evaluate eliciting human judgments
following work knight marcu conducted two separate experiments
first experiment participants presented source sentence target
compression asked rate well compression preserved important
information source sentence second experiment asked rate
grammaticality compressed outputs cases used five point rating
scale high number indicates better performance randomly selected sentences
test portion corpus sentences compressed automatically
three presented without constraints included
gold standard compressions materials thus consisted sourcetarget sentences latin square design ensured subjects see two different
compressions sentence collected ratings unpaid volunteers self
reported native english speakers studies conducted internet
custom build web interface examples experimental items given table


let us first discuss compression output evaluated terms f score
tables illustrate performance written spoken corpora
respectively present compression rate system cases
constraint constr yield better f scores non constrained ones
difference starker semi supervised model sig constraints bring
improvement written corpus spoken corpus
examined whether performance differences among statistically significant
wilcoxon test written corpus constraint significantly outperform
without constraints tendency observed spoken corpus except
model mcdonald performs comparably without constraints
wanted establish best constraint model corpora
language model performs worst whereas significance model mcdonald
perform comparably e f score differences statistically significant get
feeling difficulty task calculated much annotators agreed
compression output inter annotator agreement f score written corpus
spoken corpus agreement higher spoken texts since
consists many short utterances e g okay thats good night
term refers percentage words retained source sentence compression



ficlarke lapata

source

aim give councils control future growth second
homes
gold
aim give councils control growth homes
lm
aim future
lm constr aim give councils control
sig
aim give councils control future growth homes
sig constr aim give councils control future growth homes
mcd
aim give councils
mcd constr aim give councils control growth homes
source
clinton administration recently unveiled means encourage
brownfields redevelopment form tax incentive proposal
gold
clinton administration unveiled means encourage brownfields redevelopment tax incentive proposal
lm
clinton administration form tax
lm constr clinton administration unveiled means encourage redevelopment form
sig
clinton administration unveiled encourage brownfields redevelopment form tax proposal
sig constr clinton administration unveiled means encourage brownfields
redevelopment form tax proposal
mcd
clinton unveiled means encourage brownfields redevelopment
tax incentive proposal
mcd constr clinton administration unveiled means encourage brownfields
redevelopment form incentive proposal
table example compressions produced systems source source sentence gold
gold standard compression lm language model compression lm constr language model compression constraints sig significance model sig constr
significance model constraints mcd mcdonalds compression model
mcd constr mcdonalds compression model constraints

compressed little note marked difference
automatic human compressions best performing systems inferior human
output f score percentage points
differences automatic systems human output observed
respect compression rate seen language model compresses
aggressively whereas significance model mcdonald tend conservative
closer gold standard interestingly constraints necessarily increase
compression rate latter increases significance model decreases
language model remains relatively constant mcdonald straightforward
impose compression rate constraint e g forcing model
p
retain b tokens ni b however refrained since wanted


figlobal inference sentence compression


lm
sig
mcd
lm constr
sig constr
mcd constr
gold

compr








f score








table written corpus compression rate compr grammatical relation f score f score constr model significantly different model
without constraints significantly different lm constr

lm
sig
mcd
lm constr
sig constr
mcd constr
gold

compr








f score








table spoken corpus compression rate compr grammatical relation f score f score constr model significantly different without
constraints significantly different lm constr

regulate compression rate sentence individually according
specific information content structure
next consider human study assesses detail quality
generated compressions two dimensions namely grammaticality information
content f score conflates two dimensions therefore theory could unduly reward
system produces perfectly grammatical output without information loss tables
mean ratings system gold standard written
spoken corpora respectively first performed analysis variance anova
examine effect different system compressions anova revealed reliable effect
grammaticality importance corpus effect significant
subjects items p
next examine impact constraints constr tables cases
observe increase ratings grammaticality importance model
supplemented constraints post hoc tukey tests reveal grammaticality
importance ratings language model significance model significantly improve
statistical tests reported subsequently done mean ratings



ficlarke lapata



grammar


importance

lm
sig
mcd







lm constr
sig constr
mcd constr
gold













table written text corpus average grammaticality score grammar
average importance score importance human judgments constr model
significantly different model without constraints significantly different
gold standard significantly different mcd constr



grammar


importance

lm
sig
mcd








lm constr
sig constr
mcd constr
gold











table spoken text corpus average grammaticality score grammar
average importance score importance human judgments constr model
significantly different model without constraints significantly different
gold standard significantly different mcd constr

constraints contrast mcdonalds system sees numerical improvement
additional constraints difference statistically significant
tendencies observed spoken written corpus
upon closer inspection see constraints influence considerably
grammaticality unsupervised semi supervised systems tukey tests reveal
lm constr sig constr grammatical mcd constr terms importance
sig constr mcd constr significantly better lm constr
surprising given lm constr simple model without mechanism
highlighting important words sentence interestingly sig constr performs well
mcd constr retaining important words despite fact requires
minimal supervision although constraint overall perform better without constraints receive lower ratings grammaticality importance
comparison gold standard differences significant cases


figlobal inference sentence compression

summary observe constraints boost performance pronounced compression unsupervised use small amounts
parallel data example simple model sig yields performance comparable
mcdonald constraints taken account encouraging
suggesting ilp used create good compression relatively little
effort e without extensive feature engineering elaborate knowledge sources performance gains obtained competitive mcdonalds fully
supervised gains smaller presumably initial model contains
rich feature representation consisting syntactic information generally good job
producing grammatical output finally improvements consistent across corpora
evaluation paradigms

conclusions
presented novel method automatic sentence compression key
aspect use integer linear programming inferring globally optimal
compressions presence linguistically motivated constraints shown
previous formulations sentence compression recast ilps extended
local global constraints ensuring compressed output structurally
semantic well formed contrary previous work employed ilp solely
decoding integrate learning inference unified framework
experiments demonstrated advantages constraint
consistently bring performance gains without constraints improvements impressive require little supervision case
point significance model discussed constraints incarnation
model performs poorly considerably worse mcdonalds state art
model addition constraints improves output model performance indistinguishable mcdonald note significance model requires
small amount training data parallel sentences whereas mcdonald trained hundreds sentences presupposes little feature engineering whereas mcdonald utilizes
thousands features effort associated framing constraints however
created applied across corpora observed small
performance gains mcdonalds system latter supplemented constraints
larger improvements possible sophisticated constraints however intent
devise set general constraints tuned mistakes specific
system particular
future improvements many varied obvious extension concerns constraint set currently constraints mostly syntactic consider sentence
isolation incorporating discourse constraints could highlight words important document level presumably words topical document retained
compression constraints could manipulate compression rate example
could encourage higher compression rate longer sentences another interesting
direction includes development better objective functions compression task
objective functions presented far rely first second order markov assumptions
alternative objectives could take account structural similarity source


ficlarke lapata

sentence target compression whether share content could
operationalized terms entropy
beyond task systems presented believe holds
promise generation applications decoding searching space
possible outcomes examples include sentence level paraphrasing headline generation
summarization

acknowledgments
grateful annotators vasilis karaiskos beata kouchnir sarah luger
thanks jean carletta frank keller steve renals sebastian riedel helpful
comments suggestions anonymous referees whose feedback helped substantially improve present lapata acknowledges support epsrc grant
gr preliminary version work published proceedings
acl

references
aho v ullman j syntax directed translations pushdown assembler journal computer system sciences
bangalore rambow whittaker evaluation metrics generation
proceedings first international conference natural language generation
pp mitzpe ramon israel
barzilay r lapata aggregation via set partitioning natural language
generation proceedings human language technology conference
north american chapter association computational linguistics pp
york ny usa
bramsen p deshpande p lee k barzilay r inducing temporal graphs
proceedings conference empirical methods natural language
processing pp sydney australia
briscoe e j carroll j robust accurate statistical annotation general text
proceedings third international conference language resources evaluation pp las palmas gran canaria
charniak e maximum entropy inspired parser proceedings st north
american annual meeting association computational linguistics pp
seattle wa usa
clarke j lapata sentence compression comparison across
domains training requirements evaluation measures proceedings st
international conference computational linguistics th annual meeting
association computational linguistics pp sydney australia
clarkson p rosenfeld r statistical language modeling cmu
cambridge toolkit proceedings eurospeech pp rhodes greece


figlobal inference sentence compression

cormen h leiserson c e rivest r l intoduction
mit press
corston oliver text compaction display small screens proceedings workshop automatic summarization nd meeting north
american chapter association computational linguistics pp pittsburgh pa usa
crammer k singer ultraconservative online multiclass journal machine learning
dantzig g b linear programming extensions princeton university press
princeton nj usa
denis p baldridge j joint determination anaphoricity coreference
resolution integer programming human language technologies
conference north american chapter association computational linguistics proceedings main conference pp rochester ny
dras tree adjoining grammar reluctant paraphrasing text ph
thesis macquarie university
galley mckeown k lexicalized markov grammars sentence compression
proceedings north american chapter association computational
linguistics pp rochester ny usa
gomory r e solving linear programming integers bellman
r hall eds combinatorial analysis proceedings symposia applied
mathematics vol providence ri usa
grefenstette g producing intelligent telegraphic text reduction provide
audio scanning service blind hovy e radev r eds proceedings
aaai symposium intelligent text summarization pp stanford
ca usa
hori c furui speech summarization word extraction
method evaluation ieice transactions information systems e
jing h sentence reduction automatic text summarization proceedings
th applied natural language processing conference pp seattle wa
usa
knight k marcu summarization beyond sentence extraction probabilistic
sentence compression artificial intelligence
land h doig g automatic method solving discrete programming
econometrica
lin c improving summarization performance sentence compression pilot
study proceedings th international workshop information retrieval
asian languages pp sapporo japan
lin latat language text analysis tools proceedings first human
language technology conference pp san francisco ca usa


ficlarke lapata

marciniak strube beyond pipeline discrete optimization nlp
proceedings ninth conference computational natural language learning
pp ann arbor mi usa
mcdonald r discriminative sentence compression soft syntactic constraints
proceedings th conference european chapter association
computational linguistics trento italy
mcdonald r crammer k pereira f flexible text segmentation structured multilabel classification proceedings human language technology conference conference empirical methods natural language processing pp
vancouver bc canada
mcdonald r crammer k pereira f b online large margin training dependency parsers rd annual meeting association computational
linguistics pp ann arbor mi usa
nemhauser g l wolsey l integer combinatorial optimization wileyinterscience series discrete mathematicals opitmization wiley york ny
usa
nguyen l shimazu horiguchi ho b fukushi probabilistic
sentence reduction support vector machines proceedings th international conference computational linguistics pp geneva switzerland
press w h teukolsky vetterling w flannery b p numerical
recipes c art scientific computing cambridge university press
york ny usa
punyakanok v roth yih w zimak semantic role labeling via integer linear programming inference proceedings international conference
computational linguistics pp geneva switzerland
riedel clarke j incremental integer linear programming non projective
dependency parsing proceedings conference empirical methods
natural language processing pp sydney australia
riezler king h crouch r zaenen statistical sentence condensation
ambiguity packing stochastic disambiguation methods lexical functional
grammar human language technology conference rd meeting
north american chapter association computational linguistics pp
edmonton canada
roark b probabilistic top parsing language modeling computational
linguistics
roth learning resolve natural language ambiguities unified
proceedings th american association artificial intelligence pp
madison wi usa
roth yih w linear programming formulation global inference
natural language tasks proceedings annual conference computational
natural language learning pp boston usa


figlobal inference sentence compression

roth yih w integer linear programming inference conditional random
fields proceedings international conference machine learning pp
bonn
sarawagi cohen w w semi markov conditional random fields information extraction advances neural information processing systems vancouver
bc canada
shieber schabes synchronous tree adjoining grammars proceedings th international conference computational linguistics pp
helsinki finland
turner j charniak e supervised unsupervised learning sentence
compression proceedings rd annual meeting association computational linguistics pp ann arbor mi usa
vandeghinste v pan sentence compression automated subtitling
hybrid marie francine moens ed text summarization branches
proceedings acl workshop pp barcelona spain
williams h p model building mathematical programming th edition wiley
winston w l venkataramanan introduction mathematical programming applications th edition duxbury
zajic door b j lin j schwartz r multi candidate reduction sentence
compression tool document summarization tasks information processing
management special issue summarization




