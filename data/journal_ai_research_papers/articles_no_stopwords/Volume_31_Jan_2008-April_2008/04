journal artificial intelligence

submitted published

creating relational data unstructured
ungrammatical data sources
matthew michelson
craig knoblock

michelso isi edu
knoblock isi edu

university southern california
information sciences instistute
admiralty way
marina del rey ca usa

abstract
order agents act behalf users retrieve integrate
vast amounts textual data world wide web however much useful data
web neither grammatical formally structured making querying difficult
examples types data sources online classifieds craigslist auction
item listings ebay call unstructured ungrammatical data posts
unstructured nature posts makes query integration difficult attributes
embedded within text attributes conform standardized values
prevents queries common attribute value schema unknown
values may vary dramatically making accurate search difficult creating relational
data easy querying requires define schema embedded attributes
extract values posts standardizing values traditional information
extraction ie inadequate perform task relies clues data
structure natural language neither found posts furthermore
traditional information extraction incorporate data cleaning necessary
accurately query integrate source two step described
creates relational data sets unstructured ungrammatical text addressing
issues require set known entities called reference set first step
aligns post member reference set allows define
schema post include standard values attributes defined schema
second step performs information extraction attributes including attributes
easily represented reference sets price manner create relational
structure previously unstructured data supporting deep accurate queries
data well standard values integration experimental
technique matches posts reference set accurately efficiently outperforms
state art extraction systems extraction task posts

introduction
future vision web includes computer agents searching information making
decisions taking actions behalf human users instance agent could query
number data sources lowest price given car email user
car listing along directions seller available appointments see car
www craigslist org
www ebay com
c

ai access foundation rights reserved

fimichelson knoblock

requires agent contain two data gathering mechanisms ability query
sources ability integrate relevant sources information
however data gathering mechanisms assume sources designed support relational queries well defined schema standard values
attributes yet case many data sources
world wide web would useful query textual data within unstructured designed support querying call text data sources
posts examples posts include text ebay auction listings internet classifieds
craigslist bulletin boards bidding travel even summary text
hyperlinks returned querying google running example consider three
posts used car classifieds shown table

table three posts honda civics craigslist
craigslist post
civic speed runs great obo ri
dr honda civc lx stick shift
del sol si vtec glendale

current method query posts whether agent person keyword search
however keyword search inaccurate cannot support relational queries example
difference spelling keyword attribute within post would
limit post returned search would case user searched
example listings civic since second post would returned another factor
limits keyword accuracy exclusion redundant attributes example
classified posts cars include car model make since make
implied model shown first third post table cases
user keyword search make honda posts returned
moreover keyword search rich query framework instance consider
query average price hondas later
keyword search requires user search honda retrieve
later user must traverse returned set keeping track prices
removing incorrectly returned posts
however schema standardized attribute values defined entities
posts user could run example query simple sql statement
accurately addressing created keyword search standardized
attribute values ensure invariance issues spelling differences post
associated full schema values even though post might contain car
make instance schema correct value returned
query car makes furthermore standardized values allow integration
source outside sources integrating sources usually entails joining two sources
directly attributes translations attributes without standardized values
www biddingfortravel com



firelational data unstructured data sources

schema would possible link ungrammatical unstructured data
sources outside sources addresses adding schema
standardized attributes set posts creating relational data set support
deep accurate queries
one way create relational data set posts define schema
fill values schema elements techniques information extraction sometimes called semantic annotation example taking second
post table semantically annotating might yield dr honda civc lx stick
shift make honda make model civc model trim dr lx trim
year year price price however traditional information extraction relies grammatical structural characteristics text identify attributes
extract yet posts definition structured grammatical therefore wrapper
extraction technologies stalker muslea minton knoblock roadrunner
crescenzi mecca merialdo cannot exploit structure posts
posts grammatical enough exploit natural language processing nlp extraction
techniques used whisk soderland rapier califf mooney

beyond difficulties extracting attributes within post traditional extraction methods require values attributes standardized
process known data cleaning otherwise querying newly relational data would
inaccurate boil keyword search instance annotation
would still need query model civc return record traditional
extraction address
however data cleaning assume tuple tuple transformations lee ling lu ko chaudhuri ganjam ganti motwani
function maps attributes one tuple attributes another would work ungrammatical unstructured data
attributes embedded within post maps set attributes
reference set therefore need take different figuring
attributes within post cleaning
creating relational data sets unstructured ungrammatical
posts exploits reference sets reference set consists collections known entities
associated common attributes reference set online offline set
reference documents cia world fact book online
offline database comics price guide semantic web one envision
building reference sets numerous ontologies already exist standardized
ontologies build reference sets allows consensus agreement upon reference set values
implies higher reliability reference sets others might exist one
experts opinion car example reference set might edmunds car buying
guide defines schema cars well standard values attributes
model trim order construct reference sets web sources
http www cia gov cia publications factbook
www comicspriceguide com
www edmunds com



fimichelson knoblock

edmunds car buying guide use wrapper technologies agent builder case
scrape data web source schema source defines car
use reference set build relational data set exploit attributes
reference set determine attributes post extracted first step
finds best matching member reference set post
called record linkage step matching post member reference set
define schema elements post schema reference set
provide standard attributes attributes attributes reference
set user queries posts
next perform information extraction extract actual values post
match schema elements defined reference set step information
extraction step information extraction step parts post extracted
best match attribute values reference set member chosen
record linkage step step extract attributes easily represented
reference sets prices dates although already schema
standardized attributes required create relational data set posts still
extract actual attributes embedded within post accurately
learn extract attributes represented reference set prices dates
attributes extracted regular expressions extract actual
attributes within post might able accurately example consider
ford car without actually extracting attributes within post might
extract price actually car name overall outlined
figure
although previously describe similar semantically annotating posts
michelson knoblock extends combining annotation work scalable record matching michelson knoblock
make matching step annotation scalable demonstrates
work efficient record matching extends unique matching posts
embedded attributes structured relational data presents
detailed description past work including thorough evaluation procedure previously larger experimental data sets including reference set
includes tens thousands records
article organized follows first describe aligning
posts best matching members reference set section particular
matching takes place efficiently generate candidate matches
make matching procedure scalable section demonstrate
exploit matches extract attributes embedded within post present
experiments section validating approaches blocking matching information
extraction unstructured ungrammatical text follow discussion
section present related work section finish final
thoughts conclusions section

product fetch technologies http www fetch com products asp



firelational data unstructured data sources

figure creating relational data unstructured sources

aligning posts reference set
exploit reference set attributes create relational data posts needs first decide member reference set best matches post
matching known record linkage fellegi sunter provides schema attribute values necessary query integrate unstructured ungrammatical data
source record linkage broken two steps generating candidate matches called
blocking separating true matches candidates matching
step
blocking generates candidate matches similarity methods
certain attributes reference set compare posts cars
example may determine generate candidates finding common
tokens posts make attribute reference set step detailed
section crucial limiting number candidates matches later examine
matching step generating candidates generates large set
features post candidate matches reference set
features employs machine learning methods separate true matches
false positives generated blocking matching detailed section


fimichelson knoblock

generating candidates learning blocking schemes record linkage
infeasible compare post members reference set therefore
preprocessing step generates candidate matches comparing records
sets fast approximate methods called blocking thought
partitioning full cross product record comparisons mutually exclusive blocks
newcombe block attribute first sort cluster data sets
attribute apply comparison method single member block
blocking candidate matches examined detail discover true matches
two main goals blocking first blocking limit number candidate matches limits number expensive detailed comparisons needed
record linkage second blocking exclude true matches set candidate matches means trade finding matching records
limiting size candidate matches overall goal blocking make
matching step scalable limiting number comparisons must make
hindering accuracy passing many true matches possible
blocking done multi pass hernandez stolfo
combines candidates generated independent runs example cars
data might make one pass data blocking tokens car model
another run might block tokens make along common tokens trim
values one view multi pass rule disjunctive normal form
conjunction rule defines run union rules combines
candidates generated run example rule might become tokenmatch model token match year token match make effectiveness
multi pass hinges upon methods attributes chosen conjunctions
note conjunction set method attribute pairs make
restrictions methods used set methods could include full string
metrics cosine similarity simple common token matching outlined even
state art n gram methods shown experiments key methods
necessarily choosing fastest though account method speed
rather choosing methods generate smallest set candidate
matches still cover true positives since matching step consume
time
therefore blocking scheme include enough conjunctions cover many true
matches example first conjunct might cover true matches
datasets compared overlap years second conjunct
cover rest true matches adding independent runs
multi pass
however since blocking scheme includes many conjunctions needs
conjunctions limit number candidates generate example second
conjunct going generate lot unnecessary candidates since return records
share make adding method attribute pairs conjunction
limit number candidates generates example change token match


firelational data unstructured data sources

make token match make token match trim still cover true matches
generate fewer additional candidates
therefore effective blocking schemes learn conjunctions minimize false
positives learn enough conjunctions cover many true matches possible two goals blocking clearly defined reduction ratio pairs
completeness elfeky verykios elmagarmid
reduction ratio rr quantifies well current blocking scheme minimizes
number candidates let c number candidate matches n size
cross product data sets
rr c n
clear adding method attribute pairs conjunction increases
rr changed token match zip token match zip token match
first name
pairs completeness pc measures coverage true positives e many
true matches candidate set versus entire set sm number
true matches candidate set nm number matches entire dataset

p c sm nm
adding disjuncts increase pc example added second conjunction example blocking scheme first cover matches
blocking blocking scheme learner bsl learns effective
blocking schemes disjunctive normal form maximizing reduction ratio pairs
completeness way bsl tries maximize two goals blocking previously
showed bsl aided scalability record linkage michelson knoblock
extends idea showing work case matching posts
reference set records
bsl uses modified version sequential covering sca
used discover disjunctive sets rules labeled training data mitchell
case sca learn disjunctive sets conjunctions consisting method attribute
pairs basically call learn one rule generates conjunction bsl keeps
iterating call covering true matches left iteration way
sca learns full blocking scheme bsl shown table
two modifications classic sca shown bold
first bsl runs examples left cover rather stopping
threshold ensures maximize number true matches generated
candidates final blocking rule pairs completeness note might turn
yield large number candidates hurting reduction ratio however omitting true
matches directly affects accuracy record linkage blocking preprocessing step
record linkage important cover many true matches possible
way bsl fulfills one blocking goals eliminating true matches possible second
learn conjunction learn one rule step current blocking
scheme rule already contains newly learned rule remove
rule containing newly learned rule optimization allows us check rule
containment go rather end


fimichelson knoblock

table modified sequential covering
sequential covering class attributes examples
learnedrules
rule learn one rule class attributes examples
examples left cover
learnedrules learnedrules rule
examples examples examples covered rule
rule learn one rule class attributes examples
rule contains previously learned rules remove
contained rules
return learnedrules

rule containment possible guarantee learn less restrictive
rules go prove guarantee follows proof done contradiction
assume two attributes b method x assume previously
learned rules contain following conjunction x currently learned rule
x x b assume learned rules contains rule less
specific currently learned rule case must least
one training example covered x x b covered x since
sca dictates remove examples covered x learn clearly
cannot happen since examples covered specific x x b
would covered x already removed means could
learned rule x x b thus contradiction
stated two main goals blocking minimize size candidate set removing true matches set already mentioned
bsl maximizes number true positives candidate set describe
bsl minimizes overall size candidate set yields scalable record
linkage minimize candidate sets size learn restrictive conjunction
call learn one rule sca define restrictive minimizing number candidates generated long certain number true matches
still covered without restriction could learn conjunctions perfectly minimize
number candidates simply return none
learn one rule step performs general specific beam search
starts empty conjunction step adds method attribute pair
yields smallest set candidates still cover least set number true matches
learn conjunction maximizes reduction ratio
time covering minimum value pairs completeness use beam search allow
backtracking since search greedy however since beam search goes
general specific ensure final rule restrictive possible full
learn one rule given table
constraint conjunction minimum pc ensures learned conjunction fit data without restriction would possible
learn one rule learn conjunction returns candidates uselessly producing
optimal rr


firelational data unstructured data sources

behavior well defined minimum pc threshold consider
case learning restrictive rule minimum
coverage case parameter ends partitioning space cross product
example records threshold amount set threshold amount
examples covered restrictive first rule covers examples
next rule covers remaining examples next
cover examples etc sense parameter well defined set
threshold high learn fewer less restrictive conjunctions possibly limiting rr
although may increase pc slightly set lower cover examples
need learn conjuncts newer conjuncts turn may subsumed later
conjuncts waste time learn long parameter small
enough affect coverage final blocking scheme smaller
slows learning set parameter experiments
analyze running time bsl bsl take account
running time different blocking methods need assume x method
attribute pairs token f irst name assume beam size b since
use general specific beam search learn one rule procedure time
assume method attribute pair generate blocking candidates
time relax assumption later time hit learn one rule within bsl
try rules beam attribute method pairs current
beam rules worst case takes bx time since method
attribute pair beam try method attribute pairs
worst case learned disjunct would cover training example rule
disjunction pairs x therefore run learn one rule x times resulting
learning time bx e training examples full training time ebx
bsl learn blocking scheme
assumed method attribute runs time
clearly case since substantial amount literature blocking methods
setting parameter lower insignificant effect setting much
higher increased pc small amount decreasing rr

table learning conjunction method attribute pairs
learn one rule attributes examples min thresh k
best conjunction
candidate conjunctions method attribute pairs
candidate conjunctions empty
ch candidate conjunctions
first iteration
ch ch method attribute
remove ch duplicates inconsistent max specific
reduction ratio ch reduction ratio best conjunction
pairs completeness ch min thresh
best conjunction ch
candidate conjunctions best k members candidate conjunctions
return best conjunction



fimichelson knoblock

blocking times vary significantly bilenko kamath mooney let
us define function tx e represents long takes single method attribute
pair x generate e candidates training example notation
learn one rule time becomes b xtx e run tx e time pair x
full training time becomes eb xtx e clearly running time dominated
expensive blocking methodology rule learned bounded
time takes run rule method attribute pairs involved takes xtx n
n number records classifying
practical standpoint easily modify bsl account time takes
certain blocking methods generate candidates learn one rule step
change performance metric reflect reduction ratio blocking time
weighted average given wrr weight reduction ratio wb
weight blocking time modify learn one rule maximize performance
disjunct weighted average table shows modified version learnone rule changes shown bold

table learning conjunction method attribute pairs weights
learn one rule attributes examples min thresh k
best conj
candidate conjunctions method attribute pairs
candidate conjunctions empty
ch candidate conjunctions
first iteration
ch ch method attribute
remove ch duplicates inconsistent max specific
score ch wrr reduction ratio ch wb block time ch
score best conj wrr reduction ratio best conj wb block time best conj
score ch score best conj
pairs completeness ch min thresh
best conj ch
candidate conjunctions best k members candidate conjunctions
return best conj

note set wb version learn one rule
used throughout consider reduction ratio since
methods token n gram match simple compute requiring time build
initial index candidate generation safely set wb
making trade time versus reduction might appropriate decision
although method may fast sufficiently reduce reduction ratio
time takes record linkage step might increase time would
taken run blocking method provides larger increase reduction ratio
since classification often takes much longer candidate generation goal
minimize candidates maximize reduction ratio turn minimizes classification
time key insight bsl choose blocking method
importantly choose appropriate attributes block sense
bsl feature selection blocking method


firelational data unstructured data sources

experiments blocking important pick right attribute combinations
bsl even simple methods blocking sophisticated
methods
easily extend bsl handle case matching posts members
reference set special case posts attributes embedded
within reference set data relational structured schema elements
handle special case rather matching attribute method pairs across
data sources learn one rule instead compare attribute method
pairs relational data entire post small change showing
works well even special case
learn good blocking scheme efficiently generate candidates
post set align reference set blocking step essential mapping large
amounts unstructured ungrammatical data sources larger larger reference
sets
matching step
set candidates generated blocking one member
reference set best matches current post one data sources record
post must align record data source reference set candidates
whole alignment procedure referred record linkage fellegi sunter
refer finding particular matches blocking matching step

figure traditional record linkage
however record linkage presented article differs traditional
record linkage well studied traditional record linkage matches record
one data source record another data source relating respective
decomposed attributes instance second post table assuming
decomposed attributes make post compared make reference


fimichelson knoblock

figure matching post reference set
set done trims etc record reference set
best matches post similarities attributes would considered
match represented figure yet attributes posts embedded
within single piece text yet identified text compared reference
set already decomposed attributes extraneous
tokens present post figure depicts type matching
traditional record linkage approaches apply
instead matching step compares post attributes reference set
concatenated together since post compared whole record reference set
sense attributes comparison record level
approximately reflects similar embedded attributes post
attributes candidate match mimics idea traditional record linkage
comparing fields determines similarity record level
however record level similarity possible two candidates
generate record level similarity differing individual attributes one
attributes discriminative needs way reflect
example consider figure figure two candidates share make
model however first candidate shares year second candidate shares
trim since candidates share make model another
attribute common possible generate record level comparison yet
trim car especially rare thing hatchback discriminative
sharing year since lots cars make model year
differ trim difference individual attributes needs reflected
discriminate attributes matching step borrows idea traditional
record linkage incorporating individual comparisons attribute


firelational data unstructured data sources

figure two records equal record level different field level similarities

data source best way determine match record level
information enough discriminate matches field level comparisons must exploited
well field level comparisons matching step compares post
individual attribute reference set
record field level comparisons represented vector different similarity functions called rl scores incorporating different similarity functions rl scores
reflects different types similarity exist text hence record level
comparison matching step generates rl scores vector post
attributes concatenated generate field level comparisons matching step calculates rl scores post individual attributes reference
set rl scores vectors stored vector called vrl populated
vrl represents record field level similarities post member
reference set
example reference set figure schema attributes make model
trim year assuming current candidate honda civic lx
vrl looks

vrl rl
rl
rl
rl
rl

scores post
scores post
scores post
scores post
scores post

honda
civic
lx

honda civic lx

generally


fimichelson knoblock

vrl rl scores post
rl scores post

rl scores post
rl scores post

attribute
attribute
attributen
attribute attribute attributen

rl scores vector meant include notions many ways exist define
similarity textual values data sources might case
one attribute differs another misplaced missing changed letters sort
similarity identifies two attributes similar misspelled called edit
distance another type textual similarity looks tokens attributes
defines similarity upon number tokens shared attributes
token level similarity robust spelling mistakes puts emphasis
order tokens whereas edit distance requires order tokens match
order attributes similar lastly cases one attribute may sound
another even spelled differently one attribute may share common
root word another attribute implies stemmed similarity last two
examples neither token edit distance similarities
capture different similarity types rl scores vector built three vectors reflect different similarity types discussed hence rl scores

rl scores post attribute token scores post attribute
edit scores post attribute
scores post attribute
vector token scores comprises three token level similarity scores two similarity
scores included vector jensen shannon distance defines
similarities probability distributions tokens one uses dirichlet prior cohen
ravikumar feinberg smooths token probabilities jelenikmercer mixture model zhai lafferty last metric token scores vector
jaccard similarity
scores included token scores vector takes form
token scores post attribute jensen shannon dirichlet post attribute
jensen shannon jm mixture post attribute
jaccard post attribute
vector edit scores consists edit distance scores comparisons
strings character level defined operations turn one string another
instance edit scores vector includes levenshtein distance levenshtein
returns minimum number operations turn string string smithwaterman distance smith waterman extension levenshtein
distance last score vector edit scores jaro winkler similarity winkler
thibaudeau extension jaro metric jaro used
similar proper nouns strict edit distance regard operations
transformations jaro winkler metric useful determinant string similarity
character level metrics edit scores vector defined


firelational data unstructured data sources

edit scores post attribute levenshtein post attribute
smith waterman post attribute
jaro winkler post attribute
similarities edit scores token scores vector defined secondstring package cohen et al used experimental implementation
described section
lastly vector scores captures two types similarity fit
token level edit distance similarity vector vector includes two types
string similarities first soundex score post attribute
soundex uses phonetics token basis determining similarity
misspelled words sound receive high soundex score similarity
similarity upon porter stemming porter
removes suffixes strings root words compared similarity
helps alleviate possible errors introduced prefix assumption introduced
jaro winkler metric since stems scored rather prefixes including
scores scores vector becomes
scores post attribute porter stemmer post attribute
soundex post attribute

figure full vector similarity scores used record linkage
figure shows full composition vrl constituent similarity scores
vrl constructed candidates matching step performs
binary rescoring vrl help determine best match amongst candidates rescoring helps determine best possible match post separating


fimichelson knoblock

best candidate much possible might candidates
similarly close values one best match rescoring emphasizes
best match downgrading close matches element values obvious non matches boosting difference score best
candidates elements
rescore vectors candidate set c rescoring method iterates
elements xi vrl c vrl contain maximum value xi map
xi vrl map xi mathematically rescoring method
vrlj c j c




xi vrlj fivrlj


f xi vrlj

xi max xt vrls vrls c c
otherwise

example suppose c contains candidates vrl vrl
vrl
vrl
rescoring become
vrl
vrl
rescoring matching step passes vrl support vector machine svm
joachims trained label matches non matches best match
candidate svm classifies match maximally positive score
decision function one candidate share maximum score
decision function thrown matches enforces strict mapping
posts members reference set however n relationship captured
relaxing restriction keeps first candidate
maximal decision score chooses one randomly set candidates
maximum decision score
although use svms differentiate matches non matches
strictly tied method main characteristics learning
feature vectors sparse binary rescoring
concepts dense since many useful features may needed thus none
pruned feature selection tried use nave bayes classifier matching
task monumentally overwhelmed number features number
training examples yet say methods deal sparse
feature vectors dense concepts online logistic regression boosting could
used place svm
match post found attributes matching reference set member
added annotation post including values reference set attributes
tags reflect schema reference set overall matching
shown figure


firelational data unstructured data sources

figure matching posts records reference set
addition providing standardized set values query posts standardized values allow integration outside sources values standardized
canonical values instance want integrate car classifieds safety
ratings website easily join sources across attribute values
manner approaching annotation record linkage create relational
data unstructured ungrammatical data sources however aid extraction
attributes easily represented reference sets perform information extraction
posts well

extracting data posts
although record linkage step creates relational data posts
still attributes would extract post easily represented
reference sets means record linkage step used attributes
examples attributes dates prices although many attributes
extracted simple techniques regular expressions make
extraction annotation ever accurate sophisticated information extraction
motivate idea consider ford car model called used regular
expressions might extract price car would case
however try extract attributes including model would
extract model correctly furthermore might want extract actual
attributes post extraction allows
perform extraction infuses information extraction extra knowledge rather relying possibly inconsistent characteristics garner extra


fimichelson knoblock

knowledge exploits idea reference sets attributes
matching reference set member basis identifying similar attributes post
label extracted values post schema
reference set thus adding annotation extracted values
broad sense two parts first label token possible
attribute label junk ignored tokens post labeled
clean extracted labels figure shows whole procedure graphically
detail second post table steps shown figure
described detail

figure extraction process attributes
begin extraction process post broken tokens first post
table example set tokens becomes civic speed
tokens scored attribute record reference set
deemed match
score tokens extraction process builds vector scores vie vrl
vector matching step vie composed vectors represent similarities
token attributes reference set however composition
vie slightly different vrl contains comparison concatenation
attributes vectors compose vie different compose
vrl specifically vectors form vie called ie scores similar


firelational data unstructured data sources

rl scores compose vrl except contain token scores component since
ie scores uses one token post time
rl scores vector
rl scores post attribute token scores post attribute
edit scores post attribute
scores post attribute
becomes
ie scores token attribute edit scores token attribute
scores token attribute
main difference vie vrl vie contains unique vector
contains user defined functions regular expressions capture attributes
easily represented reference sets prices dates attribute types
generally exhibit consistent characteristics allow extracted
usually infeasible represent reference sets makes traditional extraction methods
good choice attributes vector called common scores types
characteristics used extract attributes common enough used
extraction
first post table assume reference set match make honda
model civic year means matching tuple would honda
civic match generates following vie token civic post
vie common scores civic
ie scores civic honda
ie scores civic civic
ie scores civic
generally given token vie looks
vie common scores token
ie scores token attribute
ie scores token attribute

ie scores token attributen
vie passed structured svm tsochantaridis joachims hofmann
altun tsochantaridis hofmann joachims altun trained give
attribute type label make model price intuitively similar attribute types
similar vie vectors makes generally high scores
make attribute reference set small scores attributes
structured svms able infer extraction labels collectively helps deciding
possible token labels makes use structured svms ideal machine
learning method task note since vie member cluster
winner takes binary rescoring
since many irrelevant tokens post annotated svm
learns vie associate learned attribute type labeled


fimichelson knoblock

junk ignored without benefits reference set recognizing junk
difficult characteristics text posts unreliable example
extraction relies solely capitalization token location junk phrase great deal
might annotated attribute many traditional extraction systems work
domain ungrammatical unstructured text addresses bibliographies
assume token text must classified something assumption
cannot made posts
nonetheless possible junk token receive incorrect class label
example junk token enough matching letters might labeled trim since
trims may single letter two leads noisy tokens within whole
extracted trim attribute therefore labeling tokens individually gives approximation
data extracted
extraction overcome generating noisy labeled tokens
comparing whole extracted field analogue reference set attribute
tokens post processed whole attributes built compared corresponding attributes reference set allows removal tokens introduce
noise extracted attribute
removal noisy tokens extracted attribute starts generating two
baseline scores extracted attribute reference set attribute one
jaccard similarity reflect token level similarity two attributes however
since many misspellings edit distance similarity metric
jaro winkler metric used baselines demonstrate accurately system
extracted classified tokens isolation
first post table ongoing example assume phrase civic ri
extracted model might occur car model civic rx
instance isolation token ri could rx model comparing
extracted car model reference attribute civic generates jaccard similarity
jaro winkler score shown top figure
next cleaning method goes extracted attribute removing one token
time calculating jaccard jaro winkler similarities scores
higher baselines token becomes removal candidate tokens
processed way removal candidate highest scores removed
whole process repeated scores derived removed token become
baseline compare process ends tokens
yield improved scores baselines
shown iteration figure cleaning method finds ri removal
candidate since removing token extracted car model yields jaccard score
jaro winkler score higher baseline scores since
highest scores trying token iteration removed
baseline scores update since none remaining tokens provide improved scores
since none process terminates yielding accurate attribute value
shown iteration figure note process would keep iterating
tokens removed improve scores baseline pseudocode
shown figure


firelational data unstructured data sources

figure improving extraction accuracy reference set attributes
note however limit machine learning component extraction
svms instead claim cases reference sets aid extraction
general test architecture replace svm component
methods example extraction experiments replace svm extractor
conditional random field crf lafferty mccallum pereira extractor
uses vie features
therefore whole extraction process takes token text creates vie
passes machine learning extractor generates label token
field cleaned extracted attribute saved


phoebus system built experimentally validate building relational
data unstructured ungrammatical data sources specifically phoebus tests
techniques accuracy record linkage extraction incorporates
bsl learning blocking schemes experimental data comes
three domains posts hotels comic books cars
data hotel domain contains attributes hotel name hotel area star
rating price dates extracted test extraction data
comes bidding travel website forum users share successful
bids priceline items airline tickets hotel rates experimental data
limited postings hotel rates sacramento san diego pittsburgh
compose data set posts posts match reference
set reference set comes bidding travel hotel guides special
www biddingfortravel com



fimichelson knoblock

cleanattribute e r
comment clean extracted attribute e reference set attribute r
removalcandidates c null
jarow inklerbaseline jarowinkler e r
jaccardbaseline jaccard e r
token e

x removetoken e




jarow inklerxt jarowinkler x r






xt jaccard x r

jaccard

jarow
inklerxt jarow inklerbaseline









jaccard jaccard


xt
baseline

n



c c




c null
return e

e removemaxcandidate c e
else
cleanattribute e r

figure clean extracted attribute
posts listing hotels ever posted given area special posts provide
hotel names hotel areas star ratings reference set attributes therefore
attributes standardized values used allowing us treat
posts relational data set reference set contains records
experimental data comic domain comes posts items sale
ebay generate data set ebay searched keywords incredible hulk
fantastic four comic books section website returned items
comics tshirts sets comics limited searched
makes difficult returned records contain attributes
comic title issue number price publisher publication year description
extracted note description word description commonly associated
comic book st appearance rhino total number posts data
set matches comic domain reference set uses data
comics price guide lists incredible hulk fantastic four comics
reference set attributes title issue number description publisher contains
records
cars data consists posts made craigslist regarding cars sale dataset
consists classifieds cars los angeles san francisco boston york
http www comicspriceguide com



firelational data unstructured data sources

jersey chicago total posts data set post
contains make model year trim price reference set cars domain comes
edmunds car buying guide data set extracted make model
year trim cars resulting records
matches posts craigslist cars edmunds
unlike hotels comics domains strict relationship post
reference set enforced cars domain described previously phoebus relaxed relationship form n relationship posts reference
set sometimes records contain enough attributes discriminate single best
reference member instance posts contain model year might match
couple reference set records would differ trim attribute
make model year yet still use make model year accurately
extraction case mentioned previously pick one matches way
exploit attributes reference set since confidence

experiments posts domain split two folds one training
one testing usually called two fold cross validation however many cases twofold cross validation data training testing
believe much data label especially data sets become large
experiments instead focus less training data one set experiments uses
posts training tests remaining second set experiments
uses posts train testing remaining believe training
small amounts data important empirical procedure since real
world data sets large labeling large data sets time consuming
unrealistic fact size cars domain prevented us data
training since machine learning could scale number training
tuples would generate cars domain run experiments training
data experiments performed times average
trials reported
record linkage
subsection report record linkage broken separate discussions
blocking matching
blocking
order bsl learn blocking scheme must provided methods
use compare attributes domains experiments use two common
methods first call token compares matching token
attributes second method ngram considers matching grams
attributes
important note comparison bsl blocking methods
canopies method mccallum nigam ungar bigram indexing baxter
christen churches slightly misaligned solve different
www edmunds com



fimichelson knoblock

methods bigram indexing techniques make process
blocking pass attribute efficient goal bsl however select
attribute combinations used blocking whole trying different attribute
method pairs nonetheless contend important select right attribute
combinations even simple methods use sophisticated methods
without insight attributes might useful test hypothesis compare
bsl token gram methods bigram indexing attributes
equivalent forming disjunction attributes bigram indexing
method chose bigram indexing particular designed perform fuzzy
blocking seems necessary case noisy post data stated previously baxter
et al use threshold bigram indexing since works best
compare bsl running disjunction attributes simple token method
call blocking rule disjunction disjunction mirrors
idea picking simplest possible blocking method namely attributes
simple method
stated previously two goals blocking quantified reduction ratio
rr pairs completeness pc table shows values
many candidates generated average entire test set comparing three
different approaches table shows long took method learn rule
run rule lastly column time match shows long classifier needs
run given number candidates generated blocking scheme
table shows example blocking schemes generated
comparison attributes bsl selected attributes picked manually different
domains data structured reader pointed previous work
topic michelson knoblock
table validate idea important pick correct
attributes block simple methods use sophisticated methods without
attention attributes comparing bsl rule bigram combination
pc rr better bsl note although cars domain bigram
took significantly less time classifier due large rr
pc case bigrams even covering true matches
bsl better simplest method possible disjuction especially cases many records test upon number
records scales becomes increasingly important gain good rr maintaining
good pc value well savings dramatically demonstrated cars domain
bsl outperformed disjunction pc rr
one surprising aspect prevalent token method within
domains expect ngram method would used almost exclusively since
many spelling mistakes within posts however case hypothesize
learning uses token methods occur regularity
across posts common ngrams would since spelling mistakes might vary quite
differently across posts suggests might regularity terms
learn data across posts initially surmised
another interesting poor reduction ratio comic domain happens
rules contain disjunct finds common token within comic


firelational data unstructured data sources

hotels
bsl
disjunction
bigrams
hotels
bsl
disjunction
bigrams
comics
bsl
disjunction
bigrams
comics
bsl
disjunction
bigrams
cars
bsl
disjunction
bigrams

rr

pc

cands

time learn

time run

time match

























































































































table blocking bsl amount data used training shown
parentheses

hotels domain
hotel area token hotel name token star rating token hotel name ngram
hotels domain
hotel area token hotel name token hotel name ngram
comic domain
title token
comic domain
title token issue number token publisher token title ngram
cars domain
make token model ngram year token make ngram
table example blocking schemes learned domains



fimichelson knoblock

title rule produces poor reduction ratio value attribute
across almost reference set records say
unique values bsl use blocking reduction ratio small
domain two values comic title attribute fantastic four
incredible hulk makes sense blocking done title attribute
reduction half since blocking value fantastic four gets rid
incredible hulk comics points interesting limitation bsl
many distinct values different attribute method pairs bsl
use learn lack values cripples performance reduction
ratio intuitively though makes sense since hard distinguish good candidate
matches bad candidate matches share attribute values
another worth mentioning hotels domain get lower rr
pc use less training data happens bsl
runs examples cover last examples introduce
disjunct produces lot candidates covering true positives
would cause rr decrease keeping pc high rate
fact happens case one way curb behavior would set
sort stopping threshold bsl said maximizing pc
important thing choose want bsl cover many true positives
even means losing bit reduction
fact next test notion explicitly set threshold sca
training examples covered stops returns learned
blocking scheme helps avoid situation bsl learns general conjunction solely cover last remaining training examples happens bsl
might end lowering rr expense covering last training examples
rule learned cover last examples overly general returns many
candidate matches
domain
hotels domain
thresh
thresh
comic domain
thresh
thresh
cars domain
thresh
thresh

record linkage
f measure

rr

pc




























table comparison bsl covering training examples covering
training examples



firelational data unstructured data sources

table shows use threshold hotels cars domain see
statistically significant drop pairs completeness statistically significant increase
reduction ratio expected behavior since threshold causes bsl kick
sca cover last training examples turn allows bsl
retain rule high rr lower pc however look record linkage
see threshold fact large effect although
statistically significant difference f measure record linkage hotels domain
difference cars domain dramatic use threshold candidates
discovered rule generated threshold effect final
f measure match therefore since f measure differ much
conclude worthwhile maximize pc learning rules bsl even
rr may decrease say even presence noise turn may lead
overly generic blocking schemes bsl try maximize true matches covers
avoiding even difficult cases cover may affect matching
see table especially true cars domain matching much
difficult hotels domain
interestingly comic domain see statistically significant difference
rr pc across trials almost learn rule
whether use threshold rule covers enough training examples
threshold hit statistically significant change f measure
record linkage domain expected since bsl would generate
candidate matches whether uses threshold since cases almost
learns blocking rules
bsl encouraging works
blocking matching unstructured ungrammatical text relational data
source means works special case case
traditional record linkage matching one structured source another
means overall semantic annotation much scalable
fewer candidate matches previous work michelson knoblock
matching
since alignment hinges leveraging reference sets becomes necessary
matching step performs well measure accuracy experiments employ
usual record linkage statistics
p recision
recall

correctm atches
otalm atchesm ade
correctm atches
p ossiblem atches

bold means statistically significant two tailed test set
please see subsection description record linkage experiments
much difference attributed non threshold version learning final
predicate includes make attribute version threshold learn
since make attribute value covers many records generates many candidates
increasing pc reducing rr



fimichelson knoblock

f easure

p recision recall
p recison recall

record linkage article compared whirl cohen
whirl performs record linkage performing soft joins vector cosine similarities attributes record linkage systems require decomposed attributes
matching case posts whirl serves benchmark
requirement mirror alignment task phoebus experiment
supplies whirl two tables test set posts posts
reference set attributes concatenated approximate record level match
concatenation used matching individual attribute
obvious combine matching attributes construct whole matching reference
set member
perform record linkage whirl soft joins across tables produces
list matches ordered descending similarity score post matches
join reference set member highest similarity score called match
cars domain matches n means match reference
set exploited later information extraction step mirror idea number
possible matches n domain counted number posts match
reference set rather reference set members match
means add single match total number correct matches given
post rather correct matches since one matters done
whirl phoebus accurately reflects well would perform
processing step information extraction step
record linkage phoebus whirl shown table note
amount training data domain shown parentheses
statistically significant two tailed paired test except
precision whirl phoebus cars domain precision
phoebus trained training data comic domain
phoebus outperforms whirl uses many similarity types distinguish
matches since phoebus uses record level attribute level similarities
able distinguish records differ discriminative attributes
especially apparent cars domain first indicate difficulty
matching car posts large reference set largest experimental domain yet
used encouraging well outperforms baseline interesting suggest techniques equally accurate
terms precision fact statistically significant difference
sense phoebus able retrieve many relevant matches means
phoebus capture rich features predict matches whirls cosine similarity alone expect behavior phoebus notion field token
level similarity many different similarity measures justifies use many
similarity types field record level information since goal many
matches
encouraging data labeling phoebus able
perform almost well data training since amount data
web vast label data get comparative preferable


firelational data unstructured data sources

hotel
phoebus
phoebus
whirl
comic
phoebus
phoebus
whirl
cars
phoebus
whirl

precision

recall

f measure


































table record linkage

cost labeling data great especially since clean annotation hence
relational data comes correctly matching posts reference set
label much data important want technique widely applicable
fact faced practical issue cars domain unable
use training since machine learning method would scale number
candidates generated much training data fact report good
training data allows us extend work much larger cars
domain
method performs well outperforms whirl
clear whether use many string metrics inclusion attributes
concatenation svm provides advantage test advantages
piece ran several experiments isolating ideas
first ran phoebus matching concatenation attributes
reference set rather concatenation attributes individually earlier
stated use concatenation mirror idea record level similarity
use attribute mirror field level similarity hypothesis cases
post match different reference set records record level score
concatenation matching different attributes removing
individual attributes leaving concatenation matching test
concatenation influences matching isolation table shows
different domains
cars comic domains see improvement f measure indicating
attributes concatenation much better matching
concatenation alone supports notion need method capture
significance matching individual attributes since attributes better indicators
matching others interesting note domains whirl
better job machine learning concatenation even though whirl


fimichelson knoblock

hotels
phoebus
concatenation
whirl
comic
phoebus
concatenation
whirl
cars
phoebus
concatenation
whirl

precision

recall

f measure





































table matching concatenation

uses concatenation attributes whirl uses informationretrieval style matching best match machine learning technique tries
learn characteristics best match clearly difficult learn
characteristics
hotels domain statistically significant difference f measure
concatenation alone means concatenation sufficient determine
matches need individual fields play role specifically
hotel name area seem important attributes matching
including part concatenation concatenation still distinguishable enough
records determine matches since two three domains see
huge improvement never lose f measure concatenation
individual attributes valid matching since two domains concatenation
alone worse whirl conclude part reason phoebus outperform
whirl use individual attributes matching
next experiment tests important include string metrics
feature vector matching test idea compare metrics
one jensen shannon distance choose jensen shannon distance
outperformed tf idf even soft tf idf one accounts fuzzy token
matches task selecting right reference sets given set posts michelson
knoblock shown table
table shows metrics yielded statistically significant large improvement f measure comic cars domains means
string metrics edit distances capturing similarities jensenshannon distance alone interestingly domains phoebus
jensen shannon distance dominate whirls performance therefore
table table demonstrate phoebus benefits combination


firelational data unstructured data sources

hotels
phoebus
jensen shannon
whirl
comic
phoebus
jensen shannon
whirl
cars
phoebus
jensen shannon
whirl

precision

recall

f measure





































table string metrics versus jensen shannon distance

many varied similarity metrics along use individual attributes field level
similarities aspects contribute phoebus outperforming whirl
case hotels data statistically significant difference
matching case metrics provide relevant information
matching therefore matches missed jensen shannon method
missed include metrics hence missed matches
difficult discover string metric method yet capture
similarity example post token dt reference set record
match hotel area downtown abbreviation metric could capture
relationship however phoebus include abbreviation similarity measure
since none techniques isolation consistently outperforms whirl conclude
phoebus outperforms whirl combines multiple string metrics uses
individual attributes concatenation stated section svm classifier
well suited record linkage task justify inclusion many
metrics individual attributes along use svm classifier
last matching experiment justifies binary rescoring mechanism table shows
performing binary rescoring record linkage versus performing
binary recoring hypothesize earlier binary rescoring allow
classifier accurately make match decisions rescoring separates
best candidate much possible table shows case across
domains perform binary rescoring gain statistically significant amount
f measure shows record linkage easily able identify
true matches possible candidates difference record linkage
use binary rescoring


fimichelson knoblock

hotels
phoebus
binary rescoring
phoebus
binary rescoring
comic
phoebus
binary rescoring
phoebus
binary rescoring
cars
phoebus
binary rescoring

precision

recall

f measure








































table record linkage without binary rescoring

extraction
section presents experimentally validate extracting
actual attributes embedded within post compare two
information extraction methods rely structure grammar posts
first experiments compare phoebus baseline conditional random field
crf lafferty et al extractor conditional random field probabilistic
model label segment data labeling tasks part speech tagging crfs outperform hidden markov maximum entropy markov
therefore representing state art probabilistic graphical model present
strong comparison extraction crfs used effectively
information extraction instance crfs used combine information extraction coreference resolution good wellner mccallum peng hay
experiments use simple tagger implementation crfs mallet
mccallum suite text processing tools
stated section extraction created version phoebus
uses crfs call phoebuscrf phoebuscrf uses extraction features
vie phoebus svm common score regular expressions
string similarity metrics include phoebuscrf extraction general
benefit reference set matching
second experiments compare phoebus natural language processing nlp
extraction techniques since posts ungrammatical unreliable lexical characteristics nlp systems expected well type data
amilcare system ciravegna uses shallow nlp extraction
shown outperform symbolic systems extraction tasks use amilcare
system compare since amilcare exploit gazetteers extra


firelational data unstructured data sources

information experiments amilcare receives reference data gazetteer aid
extraction simple tagger amilcare used default settings
lastly compare phoebus trained data training phoebus
trained data phoebuscrf well experimental
amount training data put parentheses
one component extraction vector vie vector common scores includes
user defined functions regular expressions since domain specific
functions used common scores domain must specified
hotels domain common scores includes functions matchpriceregex
matchdateregex functions gives positive score token matches price
date regular expression otherwise comic domain common scores contains
functions matchpriceregex matchyearregex give positive scores
token matches regular expression cars domain common scores uses function
matchpriceregex since year attribute reference set use common
score capture form
cars data set posts labeled training testing
extraction domain labeled posts extraction use
training testing extraction note however phoebus perform
extraction posts able report fact
running demo phoebus cars domain live
extraction presented precision recall f measure note
extraction field level means extraction counted
correct tokens compromise field post correctly labeled
although much stricter rubric correctness accurately useful
extraction system would tables correctly labeling
tokens within posts correct attribute label hotel comic cars
domains respectively attributes italics attributes exist reference set
column freq shows average number fields test set associated
label observe means highest phoebus score phoebus
phoebuscrf highest baseline amilcare simple tagger crf f measure
statistically significant two tailed paired test
phoebus phoebuscrf outperform systems almost attributes
shown table fact one attribute baseline system
best amilcare extract date attribute hotels domain
attribute phoebus phoebuscrf use common score regular expression
main identifying feature since regular expression user supplied propose
better regular expression could make phoebus phoebuscrf extract dates even
accurately overcoming baseline since systems perform well reference
set data aid extraction reference sets greatly aid
extraction especially evident compare phoebuscrf simple tagger
crf since difference two extraction methods reference set attribute
similarity scores common scores
http www isi edu integration phoebus demos html demo uses extraction model trained
labeled extraction examples running live months writing
article



fimichelson knoblock

area

date

name

price

star

phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare

hotel
recall































precision































f measure































frequency










table field level extraction hotels domain



firelational data unstructured data sources

descript

issue

price

publisher

title

year

phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebus
phoebuscrf
phoebuscrf
simple tagger crf
amilcare

comic
recall





































precision





































f measure





































frequency












table field level extraction comic domain



fimichelson knoblock

make

model

price

trim

year

phoebus
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebuscrf
simple tagger crf
amilcare
phoebus
phoebuscrf
simple tagger crf
amilcare

cars
recall





















precision





















f measure





















frequency










table field level extraction cars domain

domain
hotel
comic
cars


phoebus





num max f measures
phoebuscrf amilcare simple tagger













total attributes





table summary extraction showing number times system
statistically significant highest f measure attribute



firelational data unstructured data sources

phoebus performs especially well cars domain best system
attributes one interesting thing note record
linkage spectacular cars domain good enough yield
high extraction times system picking best
match reference set still picking one close enough
reference set attributes useful extraction trim extraction
lowest often attribute determines match
non match record linkage step likely selects car close differs trim
match incorrect trim likely extracted correctly
rest attributes extracted reference set member
couple interesting notes come one intriguing
aspects allow us estimate level structure different
attributes within domain since crfs rely structure tokens within
post structured svm method hypothesize domains
structure phoebuscrf perform best domains least structure
phoebus perform best table shows case phoebuscrf dominates
hotels domain example many posts structure star rating
comes hotel name structure allow extractor get
hotel name accurately information therefore see
overall structure within hotels domain phoebuscrf method
performs best phoebus contrast cars domain highly
unstructured phoebus performs best across attributes domain
many missing tokens order attributes varied comic domain
varied attributes exhibit structure table
shows cases phoebus phoebuscrf dominates however although
hotels data exhibits structure important aspect
phoebus allows one perform extraction without assuming structure data
worth noting price attribute comic domain bit
misleading fact none systems statistically significant respect
prices extract f measures
systems
another aspect came light statistical significance generalization
hotels comic domains able use
data training many cases statistically significant difference
f measures extracted attributes phoebus hotels domain
name area date statistically significant f measures training
data comic domain difference f measure
issue description attributes significant though description borderline
means attributes domains roughly half insignificant
therefore little difference extraction whether use data training
extraction generalizes well important since labeling
data extraction time consuming expensive
one interesting note except comic price insignificant
systems hotel date close phoebus
training data outperformed systems attributes included


fimichelson knoblock

reference set lends credibility claim earlier section training
system extract attributes even reference set
accurately extract attributes reference set training system
identify something
overall performance phoebus validates semantic annotation
infusing information extraction outside knowledge reference sets phoebus
able perform well across three different domains representative different type
source posts auction sites internet classifieds forum bulletin boards

discussion
goal produce relational data unstructured ungrammatical
data sources accurately queried integrated sources
representing attributes embedded within post standardized values
reference set support structural queries integration instance
perform aggregate queries treat data source relational database
furthermore standardized values performing joins across data sources
key integration multiple sources standardized values aid cases
post actually contain attribute instance table two
listings include make honda however matched reference set
contain standardized value attribute used querying
integrating posts especially powerful since posts never explicitly stated
attribute values reference set attributes provide solution cases
extraction extremely difficult example none systems extracted
description attribute comic domain well however one instead considers
description attribute reference set quantified record linkage
comic domain yields improvement f measure
identifying description post
may seem reference set attributes annotation enough since
values already cleaned extraction unnecessary however case
one thing one may want see actual values entered different attributes
instance user might want discover common spelling mistake abbreviation
attribute cases extraction outperform record
linkage happens even post matched incorrect member
reference set incorrect member likely close correct match
used correctly extract much information strong example
consider cars domain f measure record linkage good
extraction domain means matches chosen
probably incorrect differ correct match something small
example true match could trim door incorrectly chosen
match might trim door would still enough information
rest trim tokens year make model correctly extract
different attributes post performing extraction values
post overcome mistakes record linkage step still
exploit information incorrectly chosen reference set member


firelational data unstructured data sources

extraction attributes helps system classify ignore junk
tokens labeling something junk much descriptive labeled junk
many possible class labels could share lexical characteristics helps improve
extraction items reference set prices dates
topic reference sets important note tied
single reference set extends include multiple reference sets iterating
process reference set used
consider following two cases first suppose user wants extract conference
names cities individual lists confined
one reference set would require constructing reference set contains power set
cities crossed conference names would scale many attributes
distinct sources however lists used two reference sets one
attribute run conference name data
reference set cities iterative exploitation reference sets allows n reference
set attributes added without combinatorial explosion
next interesting case post contains one attribute
example user needs extract two cities post one reference set used
includes cross product cities however single reference set city
names done slightly modifying makes first
pass city reference set pass record linkage match
one cities matches best tie case tie choose
first match reference city system extract city post
remove post system simply runs process
catch second city single reference set could repeated many
times needed
one issue arises reference sets discrepancy users knowledge
domain experts generally create reference sets cars domain
instance users interchangeably use attribute values hatchback liftback
wagon reference set never includes term liftback suggests synonym
hatchback used common speech edmunds automobile jargon term
wagon used edmunds used cars users describe
hatchbacks implies slight difference meaning two according
reference set authors
two issues arise discrepancies first users interchanging words
cause extraction record linkage
overcome incorporating sort thesaurus record linkage
thesaurus could expand certain attribute values used matching example including
hatchback liftback reference set attribute includes term wagon
however subtle issues mostly case hatchback
called wagon happen wagon called hatchback frequency
replacement must taken consideration errant matches created
automate line future issue arises trusting
correctness edmunds source assume edmunds right define one car
wagon different meaning classifying hatchback fact


fimichelson knoblock

edmunds classifies mazda protege wagon kelly blue book classifies
hatchback seems invalidate idea wagon different meaning
hatchback appear simple synonyms would remain unknown
without outside knowledge kelly blue book generally one assumes
reference set correct set standardized values absolute truth
meaningful reference sets constructed agreed upon
ontologies semantic web instance reference set derived ontology
cars created biggest automotive businesses alleviate many
issues meaning thesaurus scheme could work discrepancies introduced
users rather reference sets

related work
driven principal cost annotating documents
semantic web free automatic invisible users hendler
many researchers followed path attempting automatically mark documents
semantic web proposed cimiano handschuh staab dingli
ciravegna wilks handschuh staab ciravegna vargas vera motta
domingue lanzoni stutt ciravegna however systems rely lexical
information part speech tagging shallow natural language processing
extraction annotation e g amilcare ciravegna option
data ungrammatical post data similar vein systems
adel lerman gazen minton knoblock rely structure identify
annotate records web failure posts exhibit structure
makes inappropriate fair amount work automatic
labeling little emphasis techniques could label text unstructured
ungrammatical
although idea record linkage fellegi sunter well studied
even bilenko mooney current focuses matching one set
records another set records decomposed attributes little work
matching data sets one record single string composed data sets
attributes match case posts reference sets whirl system
cohen allows record linkage without decomposed attributes shown
section phoebus outperforms whirl since whirl relies solely vector
cosine similarity attributes phoebus exploits larger set features
represent field record level similarity note interest erocs system
chakaravarthy gupta roy mohania authors tackle
linking full text documents relational databases technique involves filtering
non nouns text finding matches database
intriguing interesting future work would involve performing similar filtering
larger documents applying phoebus match remaining nouns
reference sets
reference sets attributes normalized values similar idea data
cleaning however data cleaning assume tuple tuple transformations
www kbb com



firelational data unstructured data sources

lee et al chaudhuri et al function maps attributes
one tuple attributes another would work ungrammatical
unstructured data attributes embedded within post maps
set attributes reference set
although work describes technique information extraction many methods
conditional random fields crf assume least structure extracted
attributes extraction extraction experiments phoebus outperforms methods simple tagger implementation conditional random
fields mccallum ie approaches datamold borkar deshmukh
sarawagi cram agichtein ganti segment whole records bibliographies attributes little structural assumption fact cram even uses
reference sets aid extraction however systems require every token
record receive label possible posts filled irrelevant junk
tokens along lines cram datamold work bellare mccallum
uses reference set train crf extract data similar phoebuscrf
implementation however two differences phoebuscrf work
bellare mccallum first work bellare mccallum mentions
reference set records matched simple heuristics unclear
done work matching done explicitly accurately record linkage second work uses records reference set label tokens training
extraction module phoebuscrf uses actual values matching reference
set record produce useful features extraction annotation
another ie similar performs named entity recognition semicrfs dictionary component cohen sarawagi functions
reference set however work dictionaries defined lists single attribute
entities finding entity dictionary look task reference sets
relational data finding match becomes record linkage task work
semi crfs cohen sarawagi focuses task labeling segments tokens
uniform label especially useful named entity recognition case
posts however phoebus needs relax restriction cases
segments interrupted case hotel name area middle
hotel name segment unlike work phoebus makes assumptions
structure posts recently semi crfs extended use database records
task integrating unstructured data relational databases mansuri sarawagi
work similar links unstructured data citations
relational databases reference sets authors venues difference
view record linkage task namely finding right reference set tuple match
even though use matches database aid extraction view
linkage task extraction procedure followed matching task lastly
first consider structured svms information extraction previous work used
structured svms perform named entity recognition tsochantaridis et al
extraction task use reference sets
method aiding information extraction outside information form
reference sets similar work ontology information extraction embley
campbell jiang liddle ng quass smith later versions work even talk


fimichelson knoblock

ontology information extraction means semantically annotate unstructured data car classifieds ding embley liddle however contrast
work information extraction performed keyword lookup ontology
along structural contextual rules aid labeling ontology contains
keyword misspellings abbreviations look performed presence noisy data believe ontology extraction less scalable
record linkage type matching task creating maintaining ontology requires
extensive data engineering order encompass possible common spelling mistakes
abbreviations data added ontology additional data engineering
must performed work simply add tuples reference set lastly
contrast work ontology work assumes contextual structural rules
apply making assumption data extract work make
assumptions structure text extracting
yet another interesting information extraction ontologies textpresso system extracts data biological text muller sternberg
system uses regular expression keyword look label tokens text
ontology tokens labeled textpresso perform fact extraction
extracting sequences labeled tokens fit particular pattern gene allele
reference associations although system uses reference set extraction
differs keyword look lexicon
recent work learning efficient blocking schemes bilenko et al developed
system learning disjunctive normal form blocking schemes however learn
schemes graphical set covering use version sequential
covering sca similarities bsl work
mining association rules transaction data agrawal imielinski swami
discover propositional rules use multiple passes
data set discover rules however despite similarities techniques
really solve different bsl generates set candidate matches minimal
number false positives bsl learns conjunctions maximally specific
eliminating many false positives unions together single disjunctive rule
cover different true positives since conjunctions maximally specific bsl uses
sca underneath learns rules depth first general specific manner mitchell
hand work mining association rules agrawal et al looks
actual patterns data represent internal relationships may
many relationships data could discovered covers
data breadth first fashion selecting set rules iteration extending
appending possible item

conclusion
article presents semantically annotating text ungrammatical
unstructured unstructured ungrammatical sources contain much information
cannot support structured queries technique allows informative use
sources ebay agents could monitor auctions looking best
deals user could average price four star hotel san diego semantic


firelational data unstructured data sources

annotation necessary society transitions semantic web information
requires annotation useful agents users unwilling extra work
provide required annotation
future technique could link mediator framework thakkar ambite
knoblock automatically acquiring reference sets similar automatically
incorporating secondary sources record linkage michalowski thakkar knoblock
automatic formulation queries retrieve correct domain reference set
direction future mediator framework place phoebus could
incorporate many reference sets needed full coverage possible attribute values
attribute types
unsupervised approaches record linkage extraction topics future including unsupervised record linkage extraction mediator component would entirely self contained making semantic annotation posts
automatic process current implementation gives one class label per
token ideally phoebus would give token possible labels remove extraneous tokens systems cleans attributes described section
disambiguation lead much higher accuracy extraction
future work could investigate inclusion thesauri terms attributes
frequency replacement terms taken consideration exploring technologies automatically construct reference sets eventually thesauri
numerous ontologies semantic web intriguing path
long term goal annotation extraction unstructured ungrammatical
sources involves automating entire process record linkage extraction methods
could become unsupervised could automatically generate incorporate reference sets apply automatically annotate data source
would ideal making semantic web useful user
involvement

acknowledgments
upon work supported part national science foundation award number iis part air force office scientific
grant number fa part defense advanced projects
agency darpa department interior nbc acquisition services division contract nbchd
u government authorized reproduce distribute reports governmental purposes notwithstanding copyright annotation thereon views conclusions
contained herein authors interpreted necessarily representing official policies endorsements expressed implied
organizations person connected


fimichelson knoblock

references
agichtein e ganti v mining reference tables automatic text segmentation
proceedings th acm conference knowledge discovery data
mining pp acm press
agrawal r imielinski swami mining association rules sets
items large databases proceedings acm sigmod international conference management data pp acm press
baxter r christen p churches comparison fast blocking methods
record linkage proceedings th acm sigkdd workshop data cleaning
record linkage object identification pp
bellare k mccallum learning extractors unlabeled text relevant
databases proceedings aaai workshop information integration
web pp
bilenko kamath b mooney r j adaptive blocking learning scale
record linkage clustering proceedings th ieee international conference
data mining pp
bilenko mooney r j adaptive duplicate detection learnable string
similarity measures proceedings th acm international conference
knowledge discovery data mining pp acm press
borkar v deshmukh k sarawagi automatic segmentation text
structured records proceedings acm sigmod international conference
management data pp acm press
califf e mooney r j relational learning pattern match rules
information extraction proceedings th national conference artificial
intelligence pp
chakaravarthy v gupta h roy p mohania efficiently linking text
documents relevant structured information proceedings international
conference large data bases pp vldb endowment
chaudhuri ganjam k ganti v motwani r robust efficient fuzzy
match online data cleaning proceedings acm sigmod international conference management data pp acm press
cimiano p handschuh staab towards self annotating web
proceedings th international conference world wide web pp
acm press
ciravegna f adaptive information extraction text rule induction
generalisation proceedings th international joint conference artificial
intelligence pp


firelational data unstructured data sources

cohen w sarawagi exploiting dictionaries named entity extraction combining semi markov extraction processes data integration methods proceedings
th acm international conference knowledge discovery data mining
pp seattle washington acm press
cohen w w data integration similarity joins word information
representation language acm transactions information systems
cohen w w ravikumar p feinberg e comparison string metrics
matching names records proceedings acm sigkdd workshop
data cleaning record linkage object consoliation pp
crescenzi v mecca g merialdo p roadrunner towards automatic data
extraction large web sites proceedings th international conference
large data bases pp vldb endowment
ding embley w liddle w automatic creation simplified querying semantic web content information extraction ontologies
proceedings asian semantic web conference pp
dingli ciravegna f wilks automatic semantic annotation unsupervised information extraction integration proceedings k cap workshop knowledge markup semantic annotation
elfeky g verykios v elmagarmid k tailor record linkage
toolbox proceedings th international conference data engineering pp

embley w campbell jiang liddle w ng k quass
smith r conceptual model data extraction multiple record
web data knowledge engineering
fellegi p sunter b theory record linkage journal american
statistical association
handschuh staab ciravegna f cream semi automatic creation
metadata proceedings th international conference knowledge engineering knowledge management pp springer verlag
hendler j agents semantic web ieee intelligent systems
hernandez stolfo j real world data dirty data cleansing
merge purge data mining knowledge discovery
jaro advances record linkage methodology applied matching
census tampa florida journal american statistical association

joachims advances kernel methods support vector learning chap
making large scale svm learning practical mit press


fimichelson knoblock

lafferty j mccallum pereira f conditional random fields probabilistic segmenting labeling sequence data proceedings th
international conference machine learning pp morgan kaufmann
lee l ling w lu h ko cleansing data mining
warehousing proceedings th international conference database
expert systems applications pp springer verlag
lerman k gazen c minton knoblock c populating semantic web
proceedings aaai workshop advances text extraction mining
levenshtein v binary codes capable correcting deletions insertions
reversals english translation soviet physics doklady
mansuri r sarawagi integrating unstructured data relational
databases proceedings international conference data engineering p
ieee computer society
mccallum
mallet
http mallet cs umass edu



machine

learning



language

toolkit

mccallum nigam k ungar l h efficient clustering high dimensional
data sets application reference matching proceedings th acm
sigkdd pp
michalowski thakkar knoblock c automatically utilizing secondary
sources align information across sources ai magazine special issue semantic
integration vol pp
michelson knoblock c semantic annotation unstructured ungrammatical text proceedings th international joint conference artificial
intelligence pp
michelson knoblock c learning blocking schemes record linkage
proceedings st national conference artificial intelligence
michelson knoblock c unsupervised information extraction unstructured ungrammatical data sources world wide web international journal
document analysis recognition ijdar special issue noisy text analytics
mitchell machine learning mcgraw hill york
muller h sternberg e e k p w textpresso ontology information retrieval extraction system biological literature plos biology
muslea minton knoblock c hierarchical wrapper induction
semistructured information sources autonomous agents multi agent systems



firelational data unstructured data sources

newcombe h b record linkage design efficient systems linking records
individual family histories american journal human genetics

porter f suffix stripping program
smith f waterman identification common molecular subsequences
journal molecular biology
soderland learning information extraction rules semi structured free
text machine learning
thakkar ambite j l knoblock c data integration
automatically composing optimizing web services proceedings icaps
workshop scheduling web grid services
tsochantaridis hofmann joachims altun support vector machine
learning interdependent structured output spaces proceedings st
international conference machine learning p acm press
tsochantaridis joachims hofmann altun large margin methods
structured interdependent output variables journal machine learning

vargas vera motta e domingue j lanzoni stutt ciravegna f
mnm ontology driven semi automatic automatic support semantic markup
proceedings th international conference knowledge engineering
management pp
wellner b mccallum peng f hay integrated conditional model
information extraction coreference application citation matching
proceedings th conference uncertainty artificial intelligence pp

winkler w e thibaudeau application fellegi sunter model
record linkage u decennial census tech rep statistical
report series rr u bureau census
zhai c lafferty j study smoothing methods language applied
ad hoc information retrieval proceedings th acm sigir conference
development information retrieval pp acm press




