journal artificial intelligence

submitted published

durative actions stochastic domains
mausam
daniel weld

mausam cs washington edu
weld cs washington edu

dept computer science engineering
box university washington
seattle wa usa

abstract
probabilistic typically modeled markov decision process mdp
mdps otherwise expressive model allow sequential non durative actions
poses severe restrictions modeling solving real world extend
mdp model incorporate simultaneous action execution durative actions stochastic durations develop several combat computational explosion introduced
features key theoretical ideas used building modeling complex mdp extended state action space pruning irrelevant actions sampling
relevant actions informed heuristics guide search hybridizing different planners
achieve benefits approximating replanning empirical evaluation
illuminates different merits viz optimality empirical closeness
optimality theoretical error bounds speed

introduction
recent progress achieved researchers yielded relax individually many classical assumptions example successful temporal planners sgplan
sapa etc chen wah hsu kambhampati able model actions take
time probabilistic planners gpt lao spudd etc bonet geffner hansen
zilberstein hoey st aubin hu boutilier deal actions probabilistic
outcomes etc however order apply automated many real world domains must
eliminate larger groups assumptions concert example nasa researchers note
optimal control nasa mars rover requires reasoning uncertain concurrent durative
actions mixture discrete metric fluents bresina dearden meuleau smith washington todays planners handle large deterministic concurrent
durative actions mdps provide clear framework non concurrent durative actions
face uncertainty researchers considered concurrent uncertain durative actions
focus
example consider nasa mars rovers spirit oppurtunity goal
gathering data different locations instruments color infrared cameras microscopic imager mossbauer spectrometers etc transmitting data back earth concurrent
actions essential since instruments turned warmed calibrated rover
moving instruments transmitting data similarly uncertainty must explicitly
confronted rovers movement arm control actions cannot accurately predicted
furthermore actions e g moving locations setting experiments take
time fact temporal durations uncertain rover might lose way
c

ai access foundation rights reserved

fim ausam w eld

take long time reach another location etc able solve encountered rover framework needs explicitly model domain constructs
concurrency actions uncertain outcomes uncertain durations
present unified formalism domain features together
concurrent markov decision processes comdps extend mdps allowing multiple actions per
decision epoch use comdps base model involving concurrency
durative actions concurrent probabilistic temporal cptp formulated
comdps extended state space formulation able incorporate uncertainty
durations form probabilistic distributions
solving poses several computational challenges concurrency extended durations uncertainty durations lead explosive growth state space
action space branching factor develop two techniques pruned rtdp sampled rtdp
address blowup concurrency develop dur family handle
stochastic durations explore different points running time vs solutionquality tradeoff different propose several speedup mechanisms pruning provably sub optimal actions bellman backup intelligent sampling action
space admissible inadmissible heuristics computed solving non concurrent
hybridizing two planners obtain hybridized planner finds good quality solution intermediate running times approximating stochastic durations mean values replanning
exploiting structure multi modal duration distributions achieve higher quality approximations
rest organized follows section discuss fundamentals
mdps real time dynamic programming rtdp solution method section describe
model concurrent mdps section investigates theoretical properties temporal
section explains formulation cptp deterministic durations
extended case stochastic durations section section supported
empirical evaluation techniques presented section section survey
related work area conclude future directions sections

background
probabilistic uncertainty often modeled markov decision processes mdps different communities looked slightly different formulations
mdps versions typically differ objective functions maximizing reward vs minimizing
cost horizons finite infinite indefinite action representations dbn vs parametrized action
schemata formulations similar nature solve
though methods proposed applicable variants
clarity explanation assume particular formulation known stochastic shortest path
bertsekas
define markov decision process tuple hs ap pr c g
finite set discrete states use factored mdps e compactly represented
terms set state variables
finite set actions


fip lanning urative actions tochastic omains

state variables x x x x p
action
precondition
effect
toggle x
p
x x
toggle x
p
x x
toggle x
true
x x
change
toggle x
true
x x
change
toggle p
true
p p
goal x x x x

probability








figure probabilistic strips definition simple mdp potential parallelism
ap defines applicability function ap p denotes set actions
applied given state p represents power set
pr transition function write pr denote
probability arriving state executing action state
c cost model write c denote cost incurred
state reached executing action state
g set absorbing goal states e process ends one states
reached
start state
assume full observability e execution system complete access state
action performed seek optimal stationary policy e function
minimizes expected cost indefinite horizon incurred reach goal
state note cost function j mapping states expected cost reaching goal
state defines policy follows
j argmin

x

pr c j






aap

optimal policy derives optimal cost function j satisfies following pair
bellman equations
j g else
j min

aap

x

pr c j








example figure defines simple mdp four state variables x x need
set toggle actions actions e g toggle x probabilistic
developed solve mdps value iteration dynamic programming optimal cost function solution equations calculated
limit series approximations considering increasingly long action sequences jn


fim ausam w eld

cost state iteration n cost state next iteration calculated
process called bellman backup follows
jn min

aap

x

pr c jn








value iteration terminates jn jn termination guaranteed furthermore limit sequence ji guaranteed converge
optimal cost function j regardless initial values long goal reached every reachable state non zero probability unfortunately value iteration tends quite slow
since explicitly updates every state exponential number domain features one
optimization restricts search part state space reachable initial state two exploiting reachability analysis lao hansen zilberstein focus
rtdp barto bradtke singh
rtdp conceptually lazy version value iteration states get updated proportion frequency visited repeated executions greedy policy
rtdp trial path starting following greedy policy updating costs
states visited bellman backups trial ends goal reached number
updates exceeds threshold rtdp repeats trials convergence note common states
updated frequently rtdp wastes time states unreachable given current
policy rtdps strength ability quickly produce relatively good policy however complete
convergence every relevant state slow less likely potentially important states get
updated infrequently furthermore rtdp guaranteed terminate labeled rtdp lrtdp
fixes clever labeling scheme focuses attention states value
function yet converged bonet geffner labeled rtdp guaranteed terminate
guaranteed converge approximation optimal cost function states reachable optimal policy initial cost function admissible costs c positive
goal reachable reachable states non zero probability
mdps powerful framework model stochastic domains however mdps make
two unrealistic assumptions actions need executed sequentially actions
instantaneous unfortunately many real world domains assumptions
unrealistic example concurrent actions essential mars rover since instruments
turned warmed calibrated rover moving instruments
transmitting data moreover action durations non zero stochastic rover might
lose way navigating may take long time reach destination may make multiple
attempts finding accurate arm placement successively relax two
assumptions build scale spite additional complexities
imposed general

concurrent markov decision processes
define model concurrent mdp comdp allows multiple actions executed
parallel model different semi mdps generalized state semi mdps younes
simmons b incorporate action durations explicitly comdps focus
adding concurrency mdp framework input comdp slightly different
mdp hs apk prk ck g applicability function probability model cost


fip lanning urative actions tochastic omains

apk prk ck respectively encode distinction allowing sequential executions
single actions versus simultaneous executions sets actions
model
set states set actions goals g start state follow input mdp
difference lies fact instead executing one action time may execute
multiple let us define action combination set one actions
executed parallel action combination unit operator available agent
comdp takes following inputs
apk defines applicability function apk p p denotes set action
combinations applied given state
prk p transition function write prk denote
probability arriving state executing action combination state
ck p cost model write ck denote cost incurred
state reached executing action combination state
essence comdp takes action combination unit operator instead single action
convert comdp equivalent mdp mk specified
tuple hs p apk prk ck g solve known mdp
case study comdp probabilistic strips
general comdp could require exponentially larger input mdp since transition model cost model applicability function defined terms action combinations
opposed actions compact input representation general comdp interesting open
question future work consider special class compact comdp
one defined naturally via domain description similar probabilistic strips
representation mdps boutilier dean hanks
given domain encoded probabilistic strips compute safe set co executable
actions safe semantics probabilistic dynamics gets defined consistent way
describe
pplicability f unction
first discuss compute sets actions executed parallel since
actions may conflict adopt classical notion mutual exclusion blum furst apply factored action representation probabilistic strips
two distinct actions mutex may executed concurrently state one following occurs
inconsistent preconditions
outcome one action conflicts outcome
precondition one action conflicts possibly probabilistic effect


fim ausam w eld

effect one action possibly modifies feature upon another actions transition
function conditioned upon
additionally action never mutex essence non mutex actions interact effects executing sequence equals semantics
parallel executions clear
example continuing figure toggle x toggle x toggle x execute parallel
toggle x toggle x mutex conflicting preconditions similarly toggle x
toggle p mutex effect toggle p interferes precondition toggle x
toggle x outcomes depended toggle x would mutex due point
example toggle x toggle x mutex effect toggle x follows togglex probability x x else
applicability function defined set action combinations action
independently applicable actions pairwise non mutex
note pairwise concurrency sufficient ensure free concurrency multiple
actions formally apk defined terms original definition ap follows
apk ap mutex



ransition f unction
let ak action combination applicable since none actions
mutex transition function may calculated choosing arbitrary order apply
follows
prk

x



x

pr pr pr sk ak



sk

define applicability function transition function allowing consistent set actions executable concurrently alternative definitions possible
instance one might willing allow executing two actions together probability
conflict small conflict may defined two actions asserting contradictory effects
one negating precondition case state called failure could created system transitions state case conflict transition may
computed reflect low probability transition failure state
although impose model conflict free techniques dont actually depend assumption explicitly extend general comdps
c ost model
make small change probabilistic strips representation instead defining single
cost c action define additively sum resource time components follows
let durative cost e cost due time taken complete action
let r resource cost e cost resources used action


fip lanning urative actions tochastic omains

assuming additivity think cost action c r
sum time resource usage hence cost model combination actions terms
components may defined
ck ak

k
x

r ai max ai
k





example mars rover might incur lower cost preheats instrument changing
locations executes actions sequentially total time reduced
energy consumed change
solving comdp mdp
taken concurrent mdp allowed concurrency actions formulated equivalent mdp mk extended action space rest use term comdp
refer equivalent mdp mk
b ellman equations
extend equations set equations representing solution comdp
jk g else
jk min

aapk

x

n



prk ck jk





equations traditional mdp except instead considering single
actions backup state need consider applicable action combinations thus
small change must made traditional e g value iteration lao labeled rtdp
however since number action combinations worst case exponential efficiently
solving comdp requires techniques unfortunately structure exploit easily
since optimal action state classical mdp solution may even appear optimal
action combination associated concurrent mdp
theorem actions optimal combination comdp mk may individually suboptimal mdp
proof domain figure let us additional action toggle x toggles x
x probability toggles exactly one x x probability let
actions take one time unit therefore cost action combination one well
let start state x x x x p mdp optimal
action start state toggle x however comdp mk optimal combination
toggle x toggle x
pruned bellman backups
recall trial labeled rtdp performs bellman backups order calculate costs
applicable actions case action combinations chooses best action combination describe two pruning techniques reduce number backups computed


fim ausam w eld

let qk expected cost incurred executing action combination state
following greedy policy e
qkn

x

n



prk ck jkn





bellman update thus rewritten
jkn

min

aapk

qkn



c ombo kipping
since number applicable action combinations exponential would prune
suboptimal combinations following theorem imposes lower bound qk terms
costs qk values single actions theorem costs actions may depend
action starting ending state e states c c
theorem let ak action combination applicable state
comdp probabilistic strips costs dependent actions qkn values
monotonically non decreasing
qk max qk ai ck
k

k
x



ck ai



proof
qkn ck

x

prk jkn

eqn





x

prk jkn qkn ck





qkn ck

x

pr jkn





ck

x





pr ck



x







pr jkn



eqns
ck ck

x





prk jkn






k
x

k
x

ck ai

x

prk jknk

repeating actions



ck ai qknk ck



replacing n n k


eqn

fip lanning urative actions tochastic omains

qkn qkn k ck

k
x



ck ai



qkn ck

k
x



ck ai

monotonicity qkn





max qkn ai ck

k

k
x



ck ai





proof assumes equation probabilistic strips following corollary
used prune suboptimal action combinations
corollary let djkn e upper bound jkn
djkn e max qkn ai ck
k

k
x



ck ai



cannot optimal state iteration
proof let ak optimal combination state iteration n
djkn e jkn
jkn qkn
combining theorem
djkn e maxi k qkn ai

ck



k
x



ck ai



corollary justifies pruning rule combo skipping preserves optimality iteration
maintains cost function monotonicity powerful bellman backup
preserve monotonicity started admissible cost function apply
combo skipping one must compute qk values single actions applicable
calculate djkn e one may use optimal combination state previous iteration
aopt compute qkn aopt value gives upper bound value jkn
example consider figure let single action incur unit cost let cost action combination ck let state represent ordered values x x
x x p suppose nth iteration cost function assigns values
jkn jkn jkn jkn let aopt
state toggle x toggle x qkn toggle x ck toggle x jkn
qkn aopt ck aopt jkn jkn jkn
apply corollary skip combination toggle x toggle x iteration since
toggle x djkn e qkn aopt
experiments combo skipping yields considerable savings unfortunately comboskipping weakness prunes combination single iteration contrast
second rule combo elimination prunes irrelevant combinations altogether


fim ausam w eld

c ombo e limination
adapt action elimination theorem traditional mdps bertsekas prove similar
theorem comdps
theorem let action combination applicable state let bqk c denote
lower bound qk bqk c djk e never optimal combination
state
proof comdp mdp action space original proof mdps bertsekas
holds replacing action action combination
order apply theorem pruning one must able evaluate upper lower
bounds admissible cost function starting rtdp search value iteration
lao etc current cost jkn guaranteed lower bound optimal cost thus
qkn lower bound qk thus easy compute left hand side
inequality calculate upper bound optimal jk one may solve mdp
e traditional mdp forbids concurrency much faster solving comdp
yields upper bound cost forbidding concurrency restricts policy use
strict subset legal action combinations notice combo elimination used general
mdps restricted comdps probabilistic strips
example continuing previous example let toggle x qkn ck
jkn djk e solving mdp eliminated
state remaining iterations
used fashion combo elimination requires additional overhead optimally solving
single action mdp since rtdp exploit state space reachability limit
computation relevant states computation incrementally states visited

combo elimination requires computation current value qk lower
bound qk differs combo skipping avoids computation however
combo elimination prunes combination never needs reconsidered thus
tradeoff one perform expensive computation hoping long term pruning try
cheaper pruning rule fewer benefits since q value computation costly step adopt
following heuristic first try combo skipping fails prune combination attempt
combo elimination succeeds never consider tried implementing
heuristics combination skipped repeatedly try prune altogether combo elimination every state try combo elimination probability p neither
alternative performed significantly better kept original lower overhead heuristic
since combo skipping change step labeled rtdp combo elimination removes provably sub optimal combinations pruned labeled rtdp maintains convergence termination optimality efficiency used admissible heuristic
sampled bellman backups
since fundamental challenge posed comdps explosion action combinations sampling promising method reduce number bellman backups required per state
describe variant rtdp called sampled rtdp performs backups random set


fip lanning urative actions tochastic omains

action combinations choosing distribution favors combinations likely
optimal generate distribution
combinations previously discovered low qk values recorded memoizing best combinations per state iteration
calculating qk values applicable single actions current cost function
biasing sampling combinations choose ones contain actions low
qk values

sampled bellman backup state












function samplecomb state l
















returns best combination found

list l list applicable actions values
action
compute qk state action
insert ha qk state action l

newcomb samplecomb state l
compute qk state newcomb
clear memoizedlist state
compute qmin minimum qk values computed line
store combinations qk state qmin memoizedlist state
return first entry memoizedlist state

returns ith combination sampled backup

size memoizedlist state
return ith entry memoizedlist state return combination memoized previous iteration
newcomb
repeat
randomly sample action l proportional value
insert newcomb
remove actions mutex l
l empty
done true
else newcomb
done false sample least actions per combination
else
newcomb
done true prob newcomb
done
return newcomb

exposes exploration exploitation trade exploration refers testing wide range action combinations improve understanding relative merit exploitation hand advocates performing backups combinations previously
shown best manage tradeoff carefully maintaining distribution
combinations first memoize best combinations per state backed
similar action sampling used context space shuttle scheduling reduce number
actions considered value function computation zhang dietterich



fim ausam w eld

bellman update combinations constructed incremental probabilistic process
builds combination first randomly choosing initial action weighted individual qk value deciding whether add non mutex action stop growing combination
many implementations possible high level idea tried several
found similar describes implementation used
experiments takes state total number combinations input
returns best combination obtained far memoizes best combinations
state memoizedlist function helper function returns ith combination
one best combinations memoized previous iteration sampled combination
notice line function forces sampled combinations least size since
individual actions already backed line algo
ermination ptimality
since system consider every possible action combination sampled rtdp guaranteed choose best combination execute state even started
admissible heuristic may assign jkn cost greater optimal jk
e jkn values longer admissible better combination chosen subsequent
iteration jkn might set lower value jkn thus sampled rtdp monotonic
unfortunate since admissibility monotonicity important properties required termination optimality labeled rtdp indeed sampled rtdp loses important theoretical
properties good news extremely useful practice experiments sampled
rtdp usually terminates quickly returns costs extremely close optimal
mproving olution q uality
investigated several heuristics order improve quality solutions found
sampled rtdp heuristics compensate errors due partial search lack admissibility
heuristic whenever sampled rtdp asserts convergence state immediately
label converged would preclude exploration bonet geffner
instead first run complete backup phase admissible combinations rule
easy detect inconsistencies
heuristic run sampled rtdp completion use cost function produces j
initial heuristic estimate j subsequent run pruned rtdp usually
heuristic though inadmissible highly informative hence pruned rtdp terminates quite
quickly
heuristic run sampled rtdp pruned rtdp heuristic except instead
j cost function directly initial estimate scale linearly downward e
use j cj constant c guarantees hope
lies admissible side optimal experience often case
c run pruned rtdp yields optimal policy quickly
ensure termination implemented policy number trials exceeds threshold force monotonicity
cost function achieve termination reduce quality solution



fip lanning urative actions tochastic omains

experiments showed heuristic returns cost function close optimal adding
heuristic improves value moderately combination heuristics returns
optimal solution experiments
experiments concurrent mdp
concurrent mdp fundamental formulation modeling concurrent actions general
domain first compare techniques solve comdps viz pruned sampled rtdp
following sections use techniques model durative actions
tested three domains first domain probabilistic
variant nasa rover domain aips competition long fox
multiple objects photographed rocks tested resulting
data communicated back base station cameras need focused arms need
positioned usage since rover multiple arms multiple cameras domain
highly parallel cost function includes resource time components executing multiple actions parallel cheaper executing sequentially generated
state variables reachable states average number applicable
combinations per state avg ap measures amount concurrency

tested probabilistic version machineshop domain multiple subtasks e g
roll shape paint polish etc need performed different objects different
machines machines perform parallel capable every task tested
state variables around reachable states avg ap ranged

finally tested artificial domain similar one shown figure much
complex domain boolean variables need toggled however toggling probabilistic nature moreover certain pairs actions conflicting preconditions thus
varying number mutex actions may control domains degree parallelism
domain state variables reachable states avg ap

used labeled rtdp implemented gpt bonet geffner base mdp
solver implemented c implemented unpruned rtdp u rtdp pruned rtdp combo skipping ps rtdp pruned rtdp combo
skipping combo elimination pse rtdp sampled rtdp heuristic rtdp sampled rtdp heuristics value functions scaled rtdp tested
number instantiations three domains generated
varying number objects degrees parallelism distances goal experiments
performed ghz pentium processor gb ram
observe figure b pruning significantly speeds comparison pse rtdp rtdp rtdp figure b shows sampling dramatic
speedup respect pruned versions fact pure sampling rtdp converges extremely
quickly rtdp slightly slower however rtdp still much faster pse rtdp
comparison qualities solutions produced rtdp rtdp w r optimal shown
table observe solutions produced rtdp nearly optimal since
code may downloaded http www cs washington edu ai comdp comdp tgz



fim ausam w eld

comparison pruned unpruned rtdp rover domain

comparison pruned unpruned rtdp factory domain


x
ps rtdp
pse rtdp



times pruned rtdp sec

times pruned rtdp sec








x
ps rtdp
pse rtdp














times unpruned rtdp sec










times unpruned rtdp sec



figure b pruned vs unpruned rtdp rover machineshop domains respectively pruning
non optimal combinations achieves significant speedups larger

comparison pruned sampled rtdp rover domain

comparison pruned sampled rtdp factory domain


x
rtdp
rtdp



times sampled rtdp sec

times sampled rtdp sec








x
rtdp
rtdp













times pruned rtdp pse rtdp sec




times pruned rtdp pse rtdp sec

figure b sampled vs pruned rtdp rover machineshop domains respectively random
sampling action combinations yields dramatic improvements running times



fip lanning urative actions tochastic omains

comparison size rover domain


rtdp
rtdp
pse rtdp
u rtdp



rtdp
rtdp
pse rtdp
u rtdp





times sec

times sec

comparison different artificial domain














e

e
e
reach avg ap

e

e








avg ap



figure b comparison different size rover artificial domains size increases gap sampled pruned approaches widens
considerably

varying number samples rover









running times
values start state








concurrency avg ap

























j






value start state



rtdp pse rtdp

times sampled rtdp sec

speedup sampled rtdp pruned rtdp

speedup vs concurrency artificial domain









number samples




figure relative speed vs concurrency artificial domain b variation quality solution
efficiency confidence intervals number samples sampled rtdp one particular rover domain number samples increase
quality solution approaches optimal time still remains better pse rtdp
takes sec



fim ausam w eld


rover
rover
rover
rover
rover
rover
rover
artificial
artificial
artificial
machineshop
machineshop
machineshop
machineshop
machineshop

j rtdp
















j optimal
















error
















table quality solutions produced sampled rtdp
error rtdp small scaling makes admissible initial cost function pruned
rtdp indeed experiments rtdp produced optimal solution
figure b demonstrates running times vary size use product
number reachable states average number applicable action combinations per state
estimate size number reachable states artificial domains
hence x axis figure b avg ap figures verify
number applicable combinations plays major role running times concurrent mdp
figure fix factors vary degree parallelism observe
speedups obtained rtdp increase concurrency increases encouraging
expect rtdp perform well large inolving high concurrency even
approaches fail
figure b present another experiment vary number action combinations sampled backup solution quality inferior sampling
combinations quickly approaches optimal increasing number samples
experiments sample combinations per state

challenges temporal
comdp model powerful enough model concurrency actions still assumes
action instantaneous incorporate actual action durations modeling
essential increase scope current real world domains
present model discuss several theoretical challenges
imposed explicit action durations note section apply wide range

regardless whether durations uncertain fixed
regardless whether effects stochastic deterministic


fip lanning urative actions tochastic omains

actions uncertain duration modeled associating distribution possibly conditioned
outcome stochastic effects execution times focus whose objective
achieve goal state minimizing total expected time make span extend
cost functions combine make span resource usage raises question
goal counts achieved require
assumption executing actions terminate goal considered achieved
assumption action started cannot terminated prematurely
start asking question restricted set time points optimality
preserved even actions started points
definition time point action allowed start execution called decision
epoch time point pivot time effect might occur e g
end actions execution precondition may needed existing precondition may
longer needed happening time effect actually occurs
precondition definitely needed existing precondition longer needed
intuitively happening point change world state action constraints actually
happens e g effect precondition execution crosses pivot possible
happening information gained agents execution system e g didnt effect
occur may change direction future action choices clearly action durations
deterministic set pivots set happenings
example consider action whose durations follow uniform integer duration
started time timepoints pivots certain execution
finishes time happening execution
definition action pddl action fox long following hold
effects realized instantaneously start end e beginning
completion action respectively
preconditions may need hold instaneously start start end
end complete execution action

durative action
duration duration
condition p end q
effect end goal
durative action b
duration duration
effect start q end p

figure domain illustrate expressive action model may require arbitrary decision epochs
solution example b needs start units execution reach goal



fim ausam w eld

theorem pddl domain restricting decision epochs pivots causes incompleteness
e may incorrectly deemed unsolvable
proof consider deterministic temporal domain figure uses pddl notation
fox long initial state p true q false way reach goal
start time e g b timepoint open interval clearly
information gained time points interval none pivot still
required solving
intuitively instantaneous start end effects two pddl actions may require certain
relative alignment within achieve goal alignment may force one action start
somewhere possibly non pivot point midst others execution thus requiring
intermediate decision epochs considered
temporal planners may classified one two architectures constraint posting
approaches times action execution gradually constrained e g
zeno lpg see penberthy weld gerevini serina extended statespace methods e g tp sapa see haslum geffner kambhampati
theorem holds architectures strong computational implications state space
planners limiting attention subset decision epochs speed planners
theorem shows planners sapa prottle little aberdeen thiebaux
incomplete fortunately assumption restricts set decision epochs considerably
definition action tgp style action following hold
effects realized unknown point action execution thus used
action completed
preconditions must hold beginning action
preconditions features transition function conditioned must
changed actions execution except effect action
thus two tgp style actions may execute concurrently clobber others preconditions effects case tgp style actions set happenings nothing set
time points action terminates tgp pivots set points action might
terminate course sets additionally include zero
theorem actions tgp style set decision epochs may restricted pivots
without sacrificing completeness optimality
proof sketch contradiction suppose optimal policy satisfies theorem
must exist path optimal policy one must start action time even
though action could terminated since planner hasnt gained
information case analysis requires actions tgp style shows one could
started earlier execution path without increasing make span detailed proof
discussed appendix
case deterministic durations set happenings set pivots hence
following corollary holds
original tgp smith weld considered deterministic actions fixed duration use
phrase tgp style general way without restrictions



fip lanning urative actions tochastic omains

probabillity






g


makespan
probability





g

b
makespan











time

figure pivot decision epochs necessary optimal face nonmonotonic continuation domain goal achieved h hb duration
b mutex optimal policy starts finish
time starts b otherwise starts

corollary actions tgp style deterministic durations set decision
epochs may restricted happenings without sacrificing completeness optimality
uncertain durations may huge number pivots useful
constrain range decision epochs
definition action independent duration correlation probabilistic
effects duration
definition action monotonic continuation expected time action termination
nonincreasing execution
actions without probabilistic effects nature independent duration actions monotonic continuations common e g uniform exponential gaussian many duration distributions however actions bimodal multi modal distributions dont monotonic continuations example consider action uniform distribution
action doesnt terminate expected time completion calculated
times respectively monotonically decreasing example
non monotonic continuation see figure
conjecture actions tgp style independent duration monotonic continuation
set decision epochs may restricted happenings without sacrificing completeness
optimality
actions continuation nonmonotonic failure terminate increase expected
time remaining cause another sub plan preferred see figure similarly actions
duration isnt independent failure terminate changes probability eventual effects
may prompt actions started
exploiting theorems conjecture may significantly speed since
able limit number decision epochs needed decision making use theoretical
understanding first simplicity consider case tgp style actions
deterministic durations section relax restriction allowing stochastic durations
unimodal well multimodal


fim ausam w eld

togglep
p effect
conflict
p precondition
togglex












figure sample execution demonstrating conflict due interfering preconditions effects
actions shaded disambiguate preconditions effects

temporal deterministic durations
use abbreviation cptp short concurrent probabilistic temporal refer
probabilistic durative actions cptp input model
similar comdps except action costs c replaced deterministic
durations e input form hs pr g study objective minimizing expected time make span reaching goal rest make
following assumptions
assumption action durations integer valued
assumption negligible effect expressiveness one convert
rational durations one satisfies assumption scaling durations g c
denominators case irrational durations one arbitrarily close approximation original approximating irrational durations rational numbers
reasons discussed previous section adopt tgp temporal action model smith
weld rather complex pddl fox long specifically
assumption actions follow tgp model
restrictions consistent previous definition concurrency specifically
mutex definitions comdps probabilistic strips hold required assumptions illustration consider figure describes situation two actions
interfering preconditions effects executed concurrently see suppose
initially p false two actions toggle x toggle p started time respectively p precondition toggle x whose duration needs remain false
time toggle p may produce effects anytime may conflict
preconditions executing action hence forbid concurrent execution
toggle x toggle p ensure completely predictable outcome distribution
definition concurrency dynamics model remains consistent
equation thus techniques developed comdps derived probabilistic strips actions
may used


fip lanning urative actions tochastic omains

aligned epoch policy execution
takes units
togglex


f

f

f

f










time

togglex
f

f

f

f





interwoven epoch policy execution
takes units

figure comparison times taken sample execution interwoven epoch policy alignedepoch policy trajectories toggle x action fails four times succeeding
aligned policy must wait actions complete starting takes
time interwoven policy start actions middle

formulation comdp
model cptp comdp thus mdp one way list
two prominent formulations first formulation aligned epoch comdp
approximately solves quickly second formulation interleaved epochs
exactly larger state space hence takes longer solve existing
techniques subsequent subsections explore ways speed policy construction
interleaved epoch formulation
ligned e poch earch pace
simple way formulate cptp model standard comdp probabilistic strips
action costs set durations cost combination maximum
duration constituent actions equation formulation introduces substantial
approximation cptp true deterministic domains illustrate
example involving stochastic effects figure compares trajectories
toggle x actions fails four consecutive times succeeding figure f
denote failure success uncertain actions respectively vertical dashed lines represent
time points action started
consider actual executions resulting policies aligned epoch case figure
top combination actions started state next decision taken
effects actions observed hence name aligned epochs contrast figure
bottom shows decision epoch optimal execution cptp many actions
may midway execution explicitly take account actions
remaining execution times making subsequent decision thus actual state space
cptp decision making substantially different simple aligned epoch model
note due corollary sufficient consider decision epoch happening
e time point one actions complete thus assumption infer
decision epochs discrete integer course optimal policies property


fim ausam w eld

state variables x x x x p
action
precondition
toggle x

p
toggle x

p
toggle x

true

effect
x x
x x
x x
change
toggle x

true
x x
change
toggle p

true
p p
goal x x x x

probability








figure domain example extended action durations
easy see exists least one optimal policy action begins
happening hence search space reduces considerably
nterwoven e poch earch pace
adapt search space representation haslum geffner similar
bacchus ady kambhampati original state space
section augmented including set actions currently executing times passed
since started formally let interwoven state ordered pair hx

xs

x represents values state variables e x state original state space
denotes set ongoing actions times
passed since start thus
n
overall interwoven epoch search space aa z z represents
n
set
denotes cartesian product multiple sets
define set actions already execution words projection
ignoring execution times progress
hx
example continuing example domain figure suppose state state
variables false suppose action toggle x started units ago current time
state would represented hx x f f f f f toggle x five
state variables listed order x x x x p set would toggle x
allow possibility simply waiting action complete execution deciding decision epoch start additional action augment set op action
applicable states hx e states action still
executed state op action mutex non executing actions e
words decision epoch op started combination
use subscript denote interwoven state space value function j etc



fip lanning urative actions tochastic omains

involving op define op variable duration equal time another
already executing action completes next defined
interwoven applicability set defined


ap

apk x else
noop aas apk x aas

transition function need define probability transition function pr
interwoven state space decision epoch let agent state x suppose
agent decides execute action combination define ynew set similar
consisting actions starting formally ynew system
next decision epoch next time executing action terminates let us call time
next notice next depends executing newly started actions formally
next

min

ynew



moreover multiple actions may complete simultaneously define anext
set actions complete exactly next timesteps component state
decision epoch next time
ynext next ynew next
let hx let hx transition function cptp defined
prk x x anext ynext

otherwise




pr

words executing action combination state hx takes agent
decision epoch next ahead time specifically first time combination
anext completes lets us calculate ynext set actions still executing
times elapsed tgp style actions probability distribution different
state variables modified independently thus probability transition function due comdp
probabilistic strips used decide distribution state variables
combination anext taken state x
example continuing previous example let agent state execute action combination toggle x next since toggle x finish first thus
anext toggle x ynext toggle x hence probability distribution
states executing combination state
f f f f ynext probability
f f f f f ynext probability
precise definition model create multiple opt actions different constant durations opt
applicable interwoven state one next



fim ausam w eld

start goal states interwoven space start state hs set goal
states g hx x g
redefining start goal states applicability function probability transition
function finished modeling cptp comdp interwoven state space
use techniques comdps mdps well solve particular
use bellman equations described
bellman equations set equations solution cptp written
j g else












next pr j
j min



aap






x

use dursamp refer sampled rtdp search space main
bottleneck naively inheriting dursamp huge size interwoven state
space worst case actions executed concurrently size state space
q
aa get bound observing action number
possibilities executing remaining times
thus need reduce abstract aggregate state space order make
tractable present several heuristics used speed search
heuristics
present admissible inadmissible heuristics used initial cost
function dursamp first heuristic maximum concurrency solves underlying mdp thus quite efficient compute second heuristic average concurrency
inadmissible tends informed maximum concurrency heuristic
aximum c oncurrency h euristic
prove optimal expected cost traditional serial mdp divided maximum
number actions executed parallel lower bound expected make span
reaching goal cptp let j x denote value state x traditional
mdp costs action equal duration let q x denote expected cost reach
goal initially actions combination executed greedy serial policy followed
p
thereafter formally q x x prk x x j x let j value equivalent
cptp interwoven epoch state space let concurrency state
maximum number actions could executed state concurrently define maximum
concurrency domain c maximum number actions concurrently executed
world state domain following theorem used provide admissible
heuristic cptp
theorem let hx
j

j


j x

c
q x

c




fip lanning urative actions tochastic omains

proof sketch consider trajectory make span l state hx goal state
cptp optimal policy make concurrent actions sequential executing
chronological order started concurrent actions non interacting
outcomes stage similar probabilities maximum make span sequential
trajectory cl assuming c actions executing points semi mdp trajectory hence
j x possibly non stationary policy would cj thus j x cj


second inequality proven similar way
cases bounds tight example consider deterministic
optimal plan concurrently executing c actions unit duration makespan sequential version actions would taken sequentially make span
c
following theorem maximum concurrency mc heuristic state hx
defined follows
q x
j x
else hm c
hm c
c
c
maximum concurrency c calculated static analysis domain onetime expense complete heuristic function evaluated solving mdp states
however many states may never visited implementation calculation
demand states visited starting mdp current state rtdp run
seeded previous value function thus computation thrown away
relevant part state space explored refer dursamp initiated mc heuristic
durmc
samp
average c oncurrency h euristic
instead maximum concurrency c heuristic use average concurrency
domain ca get average concurrency ac heuristic call resulting
durac
samp ac heuristic admissible experiments typically informed
heuristic moreover case actions duration ac heuristic equals
mc heuristic
hybridized
present approximate method solve cptp many kinds
possible approximation methods technique exploits intuition best focus computation probable branches current policys reachable space danger
chance execution agent might end unlikely branch
poorly explored indeed might blunder dead end case undesirable apparently attractive policy might true expected make span infinity
since wish avoid dead ends explore desirable notion propriety
definition propriety policy proper state guaranteed lead eventually goal
state e avoids dead ends cycles barto et al define
proper produces proper policy one exists initial state
describe anytime approximation quickly generates proper policy
uses additional available computation time improve policy focusing
likely trajectories


fim ausam w eld

h ybridized p lanner
durhyb created hybridizing two policy creation indeed
novel notion hybridization general powerful applying many mdp however focus use hybridization cptp hybridization uses
anytime rtdp create policy frequently visited states uses faster
presumably suboptimal infrequent states
case cptp hybridizes rtdp interwoven epoch
aligned epoch aligned epochs rtdp converges relatively quickly state
space smaller resulting policy suboptimal cptp policy
waits currently executing actions terminate starting actions contrast
rtdp interwoven epochs generates optimal policy takes much longer converge
insight run rtdp interwoven space long enough generate policy
good common states stop well converges every state ensure
rarely explored states proper policy substitute aligned policy returning hybridized
policy
hybridized durhyb r k



initialize j admissible heuristic
repeat

perform rtdp trials

compute hybridized policy hyb interwoven epoch policy k familiar states aligned

epoch policy otherwise
clean hyb removing dead ends cycles
j hs evaluation hyb start state


j hs j hs



r
j hs

return hybridized policy hyb




thus key question decide states well explored
define familiarity state number times visited previous rtdp
trials reachable state whose familiarity less constant k aligned policy created
furthermore dead end state reached greedy interwoven policy create
aligned policy immediate precursors state cycle detected compute
aligned policy states part cycle
yet said hybridized terminates use rtdp helps us defining
simple termination condition parameter varied achieve desired
closeness optimality well intuition simple consider first optimal labeled rtdp
starts admissible heuristic guarantees value start state j hs
remains admissible thus less equal optimal contrast hybridized policys makespan longer equal optimal thus time progresses values
optimal make span opposite sides whenever two values within optimality ratio r
know found solution close optimal
implementation cycles detected simulation



fip lanning urative actions tochastic omains

finally evaluation hybridized policy done simulation perform
fixed number rtdp trials summarizes details one see
combined policy proper two reasons policy state aligned
policy proper rtdp aligned epoch model run convergence
rest states explicitly ensured cycles dead ends
experiments deterministic durations
continuing section set experiments evaluate techniques
solving involving explicit deterministic durations compare computation time
solution quality five methods interwoven sampled rtdp heuristic dursamp
ac
maximum concurrency durmc
samp average concurrency dursamp heuristics hybridized
durhyb sampled rtdp aligned epoch model durae test
rover machineshop aritificial domains use artificial domain see relative
performance techniques varies amount concurrency domain
e xperimental etup
modify domains used section additionally including action durations nasa
rover machineshop domains generate state variables actions whose duration range reachable states interwoven epoch state space
use artificial domain control experiments study effect degree parallelism
domain state variables reachable states
durations actions
use implementation sampled rtdp implement heuristics maximum concurrency hm c average concurrency hac initialization value function calculate
heuristics demand states visited instead computing complete heuristic
whole state space implement hybridized initial value
function set hm c heuristic parameters r k kept
respectively test number instances three
domains generate varying number objects degrees parallelism durations
actions distances goal
c omparison running imes
figures b variations running times different
rover machineshop artificial domains respectively first three bars represent
base sampled rtdp without heuristic hm c hac respectively fourth
bar represents hybridized hm c heuristic fifth bar computation
aligned epoch sampled rtdp costs set maximum action duration white
region fourth bar represents time taken aligned epoch rtdp computations
hybridized error bars represent confidence intervals running times note
plots log scale
note policies returned dursamp guaranteed optimal thus implemented
approximate replace dursamp pruned rtdp durprun optimality desired



fim ausam w eld

rover

mach







mach

mach

mach

mach

mach



ac
h
ae

rover



ac
h
ae

rover

time sec log scale









ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae







ac
h
ae

time sec log scale

rover



ac
h
ae

rover

rover



ac
h
ae





figure b running times log scale rover machineshop domain respectively
five bars represent times taken dursamp durmc
samp
ae durac
ac
dur
h

dur
ae
respectively

white
bar

dur
hyb
ae
hyb
samp
denotes portion time taken aligned epoch rtdp

algos
durmc
samp
durac
samp
durhyb
durae

speedup compared dursamp
rover
machineshop artificial average













table ratio time taken rtdp heuristics
heuristics produce times speedups hybridized algo produces x speedup aligned
epoch search produces x speedup sacrifices solution quality

notice durae solves extremely quickly natural since alignedepoch space much smaller use hm c hac speeds search model
comparing heuristics amongst average concurrency heuristic mostly
performs faster maximum concurrency presumably hac informed heuristic practice although cost inadmissible couple cases hac
doesnt perform better could focusing search incorrect region given
inadmissible nature
rover domain hybridized performs fastest fact speedups
dramatic compared methods domains comparable small
however large two domains hybridized outperforms others
huge margin fact largest artificial domain none heuristics able
converge within day durhyb durae converge solution


fip lanning urative actions tochastic omains



art

art

art

art



ac
h
ae

art



ac
h
ae


art art art art art art art

art

art

ratio make span optimal

time sec log scale

















ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae



ac
h
ae





figure b comparison different running times solution quality respectively
artificial domain degree parallelism increases become harder
largest solved durhyb durae

table shows speedups obtained compared basic dursamp
rover artificial domains speedups obtained durhyb durae much
prominent machineshop domain averaging domains h produces x speedup
ae produces x speedup
c omparison olution q uality
figures b b quality policies obtained five methods
domains measure quality simulating generated policy across multiple trials
reporting average time taken reach goal plot ratio measured expected
make span optimal expected make span table presents solution qualities method
averaged domain note aligned epoch policies usually yield
significantly longer make spans e g longer thus one must make quality sacrifice
speedy policy construction contrast hybridized extorts small sacrifice
quality exchange speed
variation c oncurrency
figure represents attempt see relative performance changed
increasing concurrency along top figure names numbers brackets
list average number applicable combinations mdp state avgss ap

range concurrent actions note difficult lot parallelism dursamp slows dramatically regardless heuristic contrast durhyb still able
quickly produce policy almost loss quality figure b
large optimal converge take optimal best policy found
runs



fim ausam w eld

rover

mach
ratio make span optimal







mach







ac
h
ae



ac
h
ae

mach






ac
h
ae

mach






ac
h
ae

mach







ac
h
ae

mach







ac
h
ae

ratio make span optimal





ac
h
ae

rover



ac
h
ae

rover



ac
h
ae

rover



ac
h
ae

rover



ac
h
ae

rover



ac
h
ae





figure b comparison make spans solution found optimal plotted yaxes rover machineshop domains respectively except durae produce
solutions quite close optimal

algos
dursamp
durmc
samp
durac
samp
durhyb
durae

rover






average quality
machineshop artificial











average






table overall solution quality produced note except durae produce policies whose quality quite close optimal average durae produces make spans
optimal

optimal uncertain durations
extend techniques previous section case action durations deterministic consider tgp style actions discrete temporal model assume
independent durations monotonic continuations section relaxes latter extending
handle multimodal duration distributions aim minimize
expected time required reach goal
formulating comdp
formulate comdp similar section
parameters comdp used directly work deterministic durations need
recompute transition function


fip lanning urative actions tochastic omains

state space aligned epoch state space well interwoven epoch space defined
section adequate model determine size interwoven
space replace duration action max duration let denote maximum
time within action complete overall interwoven epoch search space




zm zm represents set
denotes
cartesian product multiple sets
action space state may apply combination actions applicability function
reflecting fact combination actions safe w r w r already executing
actions case interwoven space previous sections previous state space
action space work well transition function definition needs change since
need take account uncertainty durations
transition function uncertain durations require significant changes probability transition
function pr interwoven space definitions section since assumptions justify conjecture need consider happenings choosing decision epochs
n

n

aa

computetransitionfunc hx














mintime min minimum remaining time
maxtime min maximum remaining time
integer mintime maxtime
set actions could possibly terminate
non empty subsets asubt
pc prob exactly asubt terminates see equation
w xt pw xt world state pw probability asubt terminates yielding xt
xt pw w
yt
asubt
insert hxt yt pw pc output
return output

computation transition function described although next decision
epoch determined happening still need consider pivots next state calculations
potential happenings mintime minimum time executing action could
terminate maxtime minimum time guaranteed least one action
terminate times mintime maxtime compute possible combinations
could terminate resulting next interwoven state probability pc line may
computed following formula
pc



prob terminates terminated till

aasubt



prob b terminate b b terminated till b



b b basub



considering pivots makes computationally intensive may
many pivots many action combinations could end one many outcomes
implementation cache transition function recompute
information state


fim ausam w eld

start goal states start state goal set developed deterministic durations
work unchanged durations stochastic start state hs goal set
g hx x g
thus modeled comdp interwoven state space redefined start goal states probability transition function use techniques
comdps solve particular use bellman equations
bellman equations interwoven epoch space define el time elapsed two interwoven states combination executed set equations
solution written

j g else

n

x
pr el j
j min


aap




compare equations equation one difference besides transition
function time elapsed within summation sign time elapsed depends
next interwoven state
modeled comdp use section use
dur denote family cptp involving stochastic durations
main bottleneck solving besides size interwoven state space
high branching factor
p olicy c onstruction rtdp h ybridized p lanning
since modeled comdp interwoven space may use pruned
rtdp durprun sampled rtdp dursamp policy construction since cost function el depends current next state combo skipping
apply thus durprun refers rtdp combo elimination
furthermore small adaptations necessary incrementally compute admissible
maximum concurrency c informed inadmissible average concurrency ac
heuristics example serial mdp rhs equation need compute
average duration action use actions cost
likewise speed hybridizing durhyb rtdp interwoven aligned epoch comdps produce near optimal policy significantly less time
dynamics aligned epoch space section one exception cost
combination case deterministic durations simply max duration constituent
actions novel twist stems fact uncertain durations require computation cost
action combination expected time last action combination terminate
example suppose two actions uniform duration distributions started
concurrently probabilities actions finished times earlier respectively thus expected duration completion combination
let us call ae


fip lanning urative actions tochastic omains

expected duration planner
modeled comdp full blown interwoven space stochastic durations cause
exlposive growth branching factor general n actions started possible
durations r probabilistic effects r n rn rn
potential successors number may computed follows duration
subset actions could complete action could r outcomes hence total
p
number successors per duration n n ci ri r n rn moreover none
actions finish time last step actions terminate leading rn outcomes
total number successors r n rn rn thus branching factor
multiplicative duration uncertainty exponential concurrency
manage extravagant computation must curb branching factor one method
ignore duration distributions assign action constant duration equal mean
distribution apply deterministic duration planner dursamp however
executing deterministic duration policy setting durations actually stochastic
action likely terminate time different mean expected duration durexp
planner addresses augmenting deterministic duration policy created account
unexpected outcomes
nline v ersion
procedure easiest understand online version wait unexpected
happens pause execution plan original estimate actions duration implausible
compute revised deterministic estimate terms ea min expected value
duration given terminated time min thus ea compute expected
duration
online durexp
build deterministic duration policy start state
repeat

execute action combination specified policy

wait interrupt

case action terminated expected nothing

case action terminates early

extend policy current state

case action didnt terminate expected

extend policy current state revising

duration follows

time elapsed since started executing

nextexp dea e

nextexp

nextexp dea nextexp e

endwhile

revised duration nextexp

endwait
goal reached



fim ausam w eld

example let duration action follow uniform distribution
expected value gets assigned first run dea e running
suppose action didnt terminate reach state running
say time units case revised expected duration would dea e
similarly doesnt terminate next expected duration would
finally words states executing times expected
terminate times expected completion
doesnt terminate
ffline v ersion
offline version contingencies done ahead
time fairness used version experiments although offline plans
possible action durations still much faster reason
solved significantly smaller less branching factor smaller
reachable state space previous computation succinctly stored form
hinterwoven state valuei pairs thus reused describes offline planner
subsequent example illustrates savings
offline durexp
build deterministic duration policy start state get current j values


insert queue open
repeat

state open pop

currstate pr currstate state state



currstate goal currstate set visited
visited insert currstate
j currstate converged
required change expected durations actions currently executing
currstate

solve deterministic duration start state currstate

insert currstate queue open
open empty





line assigns expected duration actions currently running
current state completd time previous termination point
reassignment follows similar case online version line
example consider domain two state variables x x two actions set x
set x task set variables initially false assume set x
succeeds whereas set x succeeds probability moreover let actions
uniform duration distribution case complete interwoven epoch search
could touch interwoven states state variable could true false action could
running running unit running units instead build deterministic
duration policy actions deterministic duration total number states
touched interwoven states action could running
running unit


fip lanning urative actions tochastic omains




b

g

c



g

optimal solution trajectory pr makespan


b

g

c

optimal solution trajectory pr makespan

c



g

dur exp solution makespan


time



b


g




figure example domain durexp compute optimal solution
suppose deterministic planner decides execute actions start state
committed combination easy see certain states never reached
example state h x x setx never visited since set x completes
guaranteed x set fact example states initiate offline
replanning line algo viz h x x setx h x x setx
h x x setx
p roperties
unfortunately durexp guaranteed produce optimal policy bad
policies generated expected duration planner experiments durexp
typically generates policies extremely close optimal even worst case pathological
domain able construct leads expected make span longer
optimal limit example illustrated
example consider domain actions n b n c n ai bi
takes time ci probabilistic duration probability ci takes unit time
remaining probability takes time thus expected duration ci
takes units sub spi goal may reached executing ai followed
bi alternatively goal may reached first executing ci recursively solving
sub spi domain durexp compute hai bi
best solution however optimal policy starts ai ci ci terminates
policy executes solution spi otherwise waits ai terminates executes bi
figure illustrates sub sp optimal policy expected make span
vs durexp make span general expected make span optimal policy

spn n n n thus limn exp
opt
multi modal duration distributions
planners previous two sections benefited considering small set happenings
instead pivots licensed conjecture unfortunately simplification


fim ausam w eld

warranted case actions multi modal duration distributions common
complex domains factors cant modeled explicitly example amount
time mars rover transmit data might bimodal distribution normally would
take little time dust storm progress unmodeled could take much longer
handle cases model durations mixture gaussians parameterized triple
hamplitude mean variancei
c mdp f ormulation
although cannot restrict decision epochs happenings need consider pivots
required actions multi modal distributions fact suffices consider pivots
regions distribution expected time completion increases cases
need consider happenings
two changes required transition function line maxtime
computation involves time next pivot increasing remaining time region
actions multi modal distributions thus forcing us take decision points even
action terminates another change line allows non empty subset asub
maxtime next state computed even without action termination making
changes transition function reformulate comdp interwoven space
thus solve previous methods pruned sampled rtdp hybrid expectedduration
rchetypal uration p lanner
develop multi modal variation expected duration planner called durarch instead assigning action single deterministic duration equal expected value planner
assigns probabilistic duration outcomes means different modes
distribution probabilities probability mass mode enhancement
reflects intuitive understanding multi modal distributions experiments confirm
durarch produces solutions shorter make spans durexp
experiments stochastic durations
evaluate techniques solving involving stochastic durations
compare computation time solution quality make span five planners domains
without multi modal duration distributions evaluate effectiveness
maximum mc average concurrency ac heuristics domains
e xperimental etup
modify rover machineshop artificial domains additionally including uncertainty
action durations set experiments largest million world states
reachable explored distinct states
interwoven state space domains contained many actions
actions many possible durations details domains please refer
longer version mausam


fip lanning urative actions tochastic omains

time sec





rover

machine shop


pruned
durprun
dursamp
sampled
durhyb
hybrid




durexp
exp dur






















figure time comparisons rover machineshop domains variation along
initialized average concurrency ac heuristic durexp performs best

algos
dursamp
durhyb
durexp

average quality make span
rover machineshop artificial










table three planners produce near optimal policies shown table ratios
optimal make span

c omparing running imes
compare without heuristics reaffirm heuristics significantly
speed computation indeed large solved without
heuristics comparing amongst ac beats c regardless
isnt surprising since ac sacrifices admissibility
figure reports running times initialized ac heuristic
rover machine shop domains durations unimodal durexp performs
planners substantial margins solving comparatively simpler
fewer states expanded thus approximation scales better others solving
example two machine shop large planners
cases hybridization speeds significant amounts performs better durexp
artificial domain
c omparing olution q uality
measure quality simulating generated policy across multiple trials report ratio
average expected make span optimal expected make span domains unimodal
distributions table make spans inadmissible heuristic ac par
optimal doesnt converge use best solution found across runs optimal



fim ausam w eld






durprun
pruned
dursamp
sampled



j

time log scale



durhyb
hybrid
durarch
arch dur
durexp
exp dur




durprun
dur prun
dursamp
dur samp




durhyb
dur hyb
durarch
dur arch



durexp
dur exp



























figure comparisons machine shop domain multi modal distributions computation
time comparisons durexp durarch perform much better algos b makespans returned different algos solutions returned dursamp almost optimal overall
durarch finds good balance running time solution quality

admissible heuristic c hybridized planner approximate userdefined bound experiments set bound make spans returned
quite close optimal differ durexp
quality guarantees still solutions returned tested upon nearly good
thus believe approximation quite useful scaling larger
without losing solution quality
ultimodal omains
develop multi modal variants domains e g machine shop domain time fetching paint bimodal stock paint fetched fast else needs ordered
alternative costly paint action doesnt require fetching paint solutions produced
dursamp made use pivots decision epochs starting costly paint action case
fetch action didnt terminate within first mode bimodal distribution e paint
stock
running time comparisons shown figure log scale durexp
terminates extremely quickly durarch far behind however make span comparisons figure b clearly illustrate approximations made methods order achieve
time durarch exhibits good balance time solution quality

related work
extends prior work originally reported several conference publications mausam
weld b
temporal planners may classified constraint posting extended state space methods discussed earlier section constraint promising probabilistic planners implemented architecture one exception buridan kush

fip lanning urative actions tochastic omains

stochastic

deterministic

concurrent
durative
non durative
dur tempastic
concurrent mdp
gsmdp prottle
factorial mdp
fpg aberdeen et al
paragraph
temporal
step optimal
tp sapa mips
graphplan satplan
tlplan etc

non concurrent
durative
non durative
time dependent mdp
mdp
ixtet circa
rtdp lao etc
foss onder

classical
numerical resources
hsp etc
sapa metric cpt

figure table listing planners implement different subsets concurrent stochastic durative actions

merick hanks weld performed poorly contrast mdp community
proven state space successful since powerful deterministic temporal planners
competitions use state space adopt
combine temporal mdps may interesting incorporate
constraint approaches probabilistic paradigm compare techniques

comparison semi mdps
semi markov decision process extension mdps allows durative actions take variable time discrete time semi mdp solved solving set equations direct
extension equations techniques solving discrete time semi mdps natural generalizations mdps main distinction semi mdp formulation
concurrent probabilistic temporal stochastic durations concerns presence concurrently executing actions model semi mdp allow concurrent actions
assumes one executing action time allowing concurrency actions intermediate decision epochs need deal large state action spaces encountered
semi mdps
furthermore younes simmons shown general case semi mdps incapable modeling concurrency concurrent actions stochastic continuous
durations needs another model known generalized semi markov decision process gsmdp
precise mathematical formulation younes simmons b
concurrency stochastic durative actions
tempastic younes simmons uses rich formalism e g continuous time exogenous
events expressive goal language generate concurrent plans stochastic durative actions tempastic uses completely non probabilistic planner generate plan treated
candidate policy repaired failure points identified method guarantee
completeness proximity optimal moreover attention paid towards heuristics
search control making implementation impractical
gsmdps younes simmons b extend continuous time mdps semi markov mdps
modeling asynchronous events processes younes simmonss approaches handle


fim ausam w eld

strictly expressive model due modeling continuous time solve
gsmdps approximation standard mdp phase type distributions
elegant scalability realistic yet demonstrated particular approximate discrete mdp model require many states yet still behave differently
continuous original
prottle little et al solves action language expressive
effects occur middle action execution dependent durations supported
prottle uses rtdp type search guided heuristics computed probabilistic
graph however plans finite horizon thus acyclic state space difficult
compare prottle prottle optimizes different objective function probability reaching goal outputs finite length conditional plan opposed cyclic plan
policy guaranteed reach goal
fpg aberdeen buffet learns separate neural network action individually
current state execution phase decision e whether action needs
executed taken independently decisions regarding actions way fpg able
effectively sidestep blowup caused exponential combinations actions practice
able quickly compute high quality solutions
rohanimanesh mahadevan investigate concurrency hierarchical reinforcement
learning framework abstract actions represented markov options propose
value iteration focus calculating joint termination conditions rewards received rather speeding policy construction hence consider possible markov
option combinations backup
aberdeen et al plan concurrent durative actions deterministic durations
specific military operations domain apply domain dependent heuristics speed
search extended state space
concurrency stochastic non durative actions
meuleau et al singh cohn deal special type mdp called factorial mdp
represented set smaller weakly coupled mdps separate mdps completely
independent except common resource constraints reward cost
purely additive meuleau hauskrecht kim peshkin kaelbling dean boutilier singh
cohn describe solutions sub mdps independently solved
sub policies merged create global policy thus concurrency actions different
sub mdps product work singh cohn present optimal similar
combo elimination used durprun whereas domain specific heuristics meuleau et al
guarantees work factorial mdps assumes weak coupling exists
identified factoring mdp hard
paragraph little thiebaux formulates concurrency regression
search probabilistic graph uses techniques nogood learning mutex
reasoning speed policy construction
guestrin et al solve multi agent mdp linear programming lp formulation expressing value function linear combination basis functions assuming
basis functions depend agents able reduce size lp
guestrin koller parr


fip lanning urative actions tochastic omains

stochastic non concurrent durative actions
many researchers studied stochastic durative actions absence concurrency
example foss onder use simple temporal networks generate plans
objective function time component simple temporal networks allow effective temporal
constraint reasoning methods generate temporally contingent plans
boyan littman propose time dependent mdps model actions
concurrent time dependent stochastic durations solution generates piecewise linear value functions
nasa researchers developed techniques generating non concurrent plans uncertain continuous durations greedy incrementally adds branches straightline plan bresina et al dearden meuleau ramakrishnan smith washington
handle continuous variables uncertain continuous effects solution heuristic
quality policies unknown since consider limited contingencies
solutions guaranteed reach goal
ixtet temporal planner uses constraint reasoning within partial order
laborie ghallab embeds temporal properties actions constraints
optimize make span circa example system plans uncertain durations
action associated unweighted set durations musliner murphy shin
deterministic concurrent durative actions
deterministic actions comparitively simpler much work
uncertainty previous deterministic instance
interwoven state representation transition function extensions extended state representations tp sapa tlplan haslum geffner kambhampati
bacchus ady
planners mips altaltp investigated fast generation parallel plans
deterministic settings edelkamp nigenda kambhampati jensen veloso
extend disjunctive uncertainty

future work
presented comprehensive set techniques handle probabilistic outcomes concurrent
durative actions single formalism direct attention towards different relaxations
extensions proposed model particular explore objective functions infinite
horizon continuous valued duration distributions temporally expressive action
degrees goal satisfaction interruptibility actions
extension cost functions
durative actions sections beyond focused make span
minimization however techniques quite general applicable directly
minor variations variety cost metrics illustration consider mixed cost
optimization addition duration action given
amount resource consumed per action wish minimize sum make span
total resource usage assuming resource consumption unaffected concurrent


fim ausam w eld

execution easily compute max concurrency heuristic mixed cost counterpart
equations
jt x
jr x

c
qt x
qr x
c

j

j




jt single action mdp assignng costs durations jr single
action mdp assigning costs resource consumptions informed average concurrency
heuristic similarly computed replacing maximum concurrency average concurrency
hybridized follows fashion fast comdp
solved techniques section
lines objective function minimize make span given certain maximum
resource usage total amount resource remaining included state space
comdps underlying single action mdps etc techniques may used
infinite horizon
defined techniques case indefinite horizon
absorbing state defined reachable alternative formulation
preferred allows infinite execution discounts future costs multiplying
discount factor step techniques suitably extended scenario
example theorem gets modified following
qk

k

qk ck

k
x





ik

ck ai



recall theorem provides us pruning rule combo skipping thus use
pruned rtdp pruning rule
extensions continuous duration distributions
confined actions discrete durations refer assumption
investigate effects dealing directly continuous uncertainty duration distributions let fit dt probability action ai completing times dt
conditioned action ai finishing time similarly define fit probability
action finishing time
let us consider extended state hx denotes action started units
ago world state x let applicable action started extended state define
min denotes maximum possible duration execution
action intuitively time least one action complete

q n hx

z


z


h



f f j n hx dt
h



f f j n hx dt










time











expected time reach goal

expected remaining time action

duration distribution

p lanning urative actions tochastic omains












time
























time

figure durations continuous real valued rather discrete may infinite number
potentially important decision epochs domain crucial decision epoch could required
time depending length possible alternate plans

x x world states obtained applying deterministic actions
respectively x recall j n mina q n fixed point computation

form desire jn jn functional form going equation
seems difficult achieve except perhaps specific action distributions
special example distributions constant
concurrency domain equations easily solvable interesting cases
solving equations challenging open question
furthermore dealing continuous multi modal distributions worsens decision epochs
explosion illustrate help example
example consider domain figure except let action bimodal distribution
two modes uniform respectively shown figure
let small duration figure b shows expected remaining termination times
terminates time notice due bimodality expected remaining execution time
increases expected time reach goal plan h shown
third graph suppose started need choose next decision
epoch easy see optimal decision epoch could point
would depend alternative routes goal example duration b
optimal time point start alternative route right expected time reach goal
first plan exceeds
thus choice decision epochs depends expected durations alternative routes
values known advance fact ones calculated
phase therefore choosing decision epochs ahead time seem possible makes
optimal continuous multi modal distribution mostly intractable reasonable
sized
generalizing tgp action model
assumption tgp style actions enables us compute optimal policies since prune
number decision epochs case complex action pddl fox long
old deterministic state space planners incomplete reasons
idea exploited order plan continuous resources feng dearden meuleau washington




fim ausam w eld

incomplete ppddl recently cushing et al introduced tempo statespace planner uses lifting time achieve completeness cushing kambhampati
mausam weld pursuit finding complete state space probabilistic planner
complex action natural step consider tempo representation probabilistic
setting working details seems relatively straightforward important
challenge right heuristics streamline search scale
extensions
several extensions basic framework suggested different
construct introduces additional structure need exploit knowledge order design
fast many times basic proposed may easily adapted
situations sometimes may list two important extensions
notion goal satisfaction different may require slightly different notions
goal reached example assumed thus far goal officially
achieved executed actions terminated alternatively one might consider goal
achieved satisfactory world state reached even though actions may
midst execution intermediate possibilities goal requires specific
actions necessarily end changing definition goal set
modeled comdp hybridized heuristics easily adapted
case
interruptible actions assumed started action cannot terminated
however richer model may allow preemptions well continuation interrupted
action actions could interrupted significantly
different flavor interrupting action kind decision requires full study
might action termination useful large extent similar
finding different concurrent paths goal starting together since one
interrupt executing paths soon goal reached instance example
figure longer holds since b started time later terminated needed
shorten make span
effect large durations
weakness extended state space approaches deterministic well probabilistic
settings dependence absolute durations accurate greatest common
divisor action durations instance domain action large duration
say another concurrently executable action duration world states
explored tuples general many states
behave similarly certain decision boundaries important start b
executing units c otherwise one example decision boundary
instead representing flat discrete states individually aggregate space
state represents several extended states help alleviate inefficiency
however obvious achieve aggregation automatically since adapting
well known methods aggregation hold case instance spudd hoey


fip lanning urative actions tochastic omains

et al uses algebraic decision diagrams represent abstract states jvalue aggregating valued states may enough us since expected time
completion depends linearly amount time left longest executing action
states differ amount time action executing able
aggregate together similar way feng et al use piecewise constant piecewise
linear representations adaptively discretize continuous variables case
variables executing active given time modeling
sparse high dimensional value function easy able exploit structure due
action durations essential future direction order scale complex real
world domains

conclusions
although concurrent durative actions stochastic effects characterize many real world domains planners handle challenges concert proposes unified statespace framework model solve state space formulations popular
deterministic temporal well probabilistic however
features bring additional complexities formulation afford solution techniques
develop dur family alleviates complexities evaluate techniques running times qualities solutions produced moreover study theoretical
properties domains identify key conditions fast optimal
possible make following contributions
define concurrent mdps comdp extension mdp model formulate
stochastic concurrent actions comdp cast back
mdp extended action space action space possibly exponential
number actions solving mdp naively may take huge performance hit
develop general notions pruning sampling speed pruning
refers pruning provably sub optimal action combinations state thus performing less computation still guaranteeing optimal solutions sampling solutions
rely intelligent sampling action combinations avoid dealing exponential
number method converges orders magnitude faster methods produces
near optimal solutions
formulate concurrent durative actions comdp two modified
state spaces aligned epoch interwoven epoch aligned epoch solutions
run fast interwoven epoch yield much higher quality solutions define two heuristic functions maximum concurrency mc average concurrency ac
guide search mc admissible heuristic whereas ac inadmissible typically informed leading better computational gains call dur
family subscripts samp prun refer sampling pruning respectively
optional superscripts ac mc refer heuristic employed optional
dur notifies stochastic durations example labeled rtdp
deterministic duration employing sampling started ac heuristic
abbreviated durac
samp


fim ausam w eld

develop general technique hybridizing two planners hybridizing interwovenepoch aligned epoch comdps yields much efficient durhyb
parameter varied trade speed optimality
experiments durhyb quickly produces near optimal solutions larger
speedups quite significant hybridized
used anytime fashion thus producing good quality proper policies policies
guaranteed reach goal within desired time moreover idea hybridizing two
planners general notion recently applied solving general stochastic
mausam bertoli weld
uncertainty durations leads complexities addition state action
spaces blowup branching factor number decision epochs
bound space decision epochs terms pivots times actions may potentially terminate conjecture restrictions thus making tractable
propose two expected duration planner durexp archetypal
duration planner durarch successively solve small
limited duration uncertainty respectively durarch able make use
additional structure offered multi modal duration distributions perform
much faster techniques moreover durarch offers good balance
time vs solution quality tradeoff
besides focus stochastic actions expose important theoretical issues related
durative actions repercussions deterministic temporal planners well
particular prove common state space temporal planners incomplete face
expressive action e g pddl may strong impact
future temporal cushing et al
overall proposes large set techniques useful modeling solving
employing stochastic effects concurrent executions durative actions
duration uncertainties range fast suboptimal solutions relatively slow
optimal explore different intermediate points spectrum
presented hope techniques useful scaling techniques real
world future

acknowledgments
thank blai bonet providing source code gpt well comments course
work thankful sumit sanghai theorem proving skills advice
stages grateful derek long anonymous reviewers
gave several thoughtful suggestions generalizing theory improving clarity
text thank subbarao kambhampati daniel lowd parag david smith others
provided useful comments drafts parts work performed university washington supported generous grants national
aeronautics space administration award nag national science foundation award
iis office naval awards n n
wrf tj cable professorship


fip lanning urative actions tochastic omains

references
aberdeen thiebaux zhang l decision theoretic military operations
icaps
aberdeen buffet
gradients icaps

concurrent probabilistic temporal policy

bacchus f ady resources concurrency forward chaining
ijcai pp
barto bradtke singh learning act real time dynamic programming
artificial intelligence
bertsekas dynamic programming optimal control athena scientific
blum furst fast graph analysis artificial intelligence

bonet b geffner h labeled rtdp improving convergence real time dynamic
programming icaps pp
bonet b geffner h mgpt probabilistic planner heuristic search jair

boutilier c dean hanks decision theoretic structural assumptions
computational leverage j artificial intelligence
boyan j littman l exact solutions time dependent mdps nips p

bresina j dearden r meuleau n smith washington r continuous time resource uncertainty challenge ai uai
chen wah b w hsu c temporal subgoal partitioning resolution sgplan jair
cushing w kambhampati mausam weld temporal really
temporal ijcai
dearden r meuleau n ramakrishnan smith e washington r incremental
contingency icaps workshop uncertainty incomplete information
b kambhampati sapa domain independent heuristic metric temporal
planner ecp
b kambhampati sapa scalable multi objective metric temporal planner
jair
edelkamp taming numbers duration model checking integrated
system journal artificial intelligence


fim ausam w eld

feng z dearden r meuleau n washington r dynamic programming structured continuous markov decision processes uai p
foss j onder n generating temporally contingent plans ijcai workshop
learning apriori unknown dynamic domains
fox long pddl extension pddl expressing temporal
domains jair special issue rd international competition
gerevini serina lpg planner local search graphs
action graphs aips p
guestrin c koller parr r max norm projections factored mdps ijcai
pp
hansen e zilberstein lao heuristic search finds solutions
loops artificial intelligence
haslum p geffner h heuristic time resources ecp
hoey j st aubin r hu boutilier c spudd stochastic decision
diagrams uai pp
jensen r veloso obdd universal synchronized agents
non deterministic domains journal artificial intelligence
kushmerick n hanks weld probabilistic artificial
intelligence
laborie p ghallab sharable resource constraints ijcai p

little aberdeen thiebaux prottle probabilistic temporal planner
aaai
little thiebaux concurrent probabilistic graphplan framework
icaps
long fox rd international competition analysis
jair
mausam stochastic concurrent durative actions ph dissertation university washington
mausam bertoli p weld hybridized planner stochastic domains ijcai
mausam weld solving concurrent markov decision processes aaai
mausam weld concurrent probabilistic temporal icaps pp



fip lanning urative actions tochastic omains

mausam weld challenges temporal uncertain durations
icaps
mausam weld b probabilistic temporal uncertain durations
aaai
meuleau n hauskrecht kim k e peshkin l kaelbling l dean boutilier c
solving large weakly coupled markov decision processes aaai pp

musliner murphy shin k world modeling dynamic construction
real time control plans artificial intelligence
nigenda r kambhampati altalt p online parallelization plans heuristic
state search journal artificial intelligence
penberthy j weld temporal continuous change aaai p
rohanimanesh k mahadevan decision theoretic concurrent temporally extended actions uai pp
singh cohn dynamically merge markov decision processes nips
mit press
smith weld temporal graphplan mutual exclusion reasoning ijcai
pp stockholm sweden san francisco ca morgan kaufmann
vidal v geffner h branching pruning optimal temporal pocl planner
constraint programming aij
younes h l simmons r g policy generation continuous time stochastic
domains concurrency icaps p
younes h l simmons r g b solving generalized semi markov decision processes
continuous phase type distributions aaai p
zhang w dietterich g reinforcement learning job shop scheduling
ijcai pp

appendix
proof theorem
prove statement theorem e actions tgp style set pivots
suffices optimal proof make use fact actions tgp style
consistent execution concurrent plan requires two executing actions non mutex
refer section explanation particular none effects conflict
precondition one conflict effects another
prove theorem contradition let us assume optimal solution
requires least one action start non pivot let us consider one optimal plans


fim ausam w eld

first non pivot point action needs start non pivot minimized let
us name time point let action starts point prove case
analysis may well start time without changing nature plan
non pivot contradict hypothesis minimum first non pivot point
pivot hypothesis contradicted need start non pivot
prove left shifted unit take one trajectory time recall
actions could several durations consider actions playing role
refers duration trajectory considering points
suffice since system state change points trajectory prove
execution none actions affected left shift following twelve
cases
actions b start b cant end non pivot thus b execute
concurrently implies b non mutex thus b may well start together
actions b continue execution use argument similar case
actions b end b tgp style effects realized open interval
ending therefore start conflict end b
actions b start b start together hence dependent
preconditions non mutex starting times shifted
direction
actions b continue execution b started refer case
similar points b
actions b end case possible due assumption non pivot
actions b start since continued execution point b
non mutex thus effects clobber bs preconditions hence b still executed
realizing effects
actions b continue execution b non mutex may end
earlier without effect b
actions b end b executing concurrently thus
non mutex may end together
actions b start b may still start since state
doesnt change
actions b continue execution b started refer case
else state change cause effect b
actions b end b non mutex executing concurrently thus effects dont clobber bs preconditions hence may end earlier
since left shifted trajectories therefore left shift legal
multiple actions start may shifted one one argument
hence proved



