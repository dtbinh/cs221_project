Journal Artificial Intelligence Research 31 (2008) 33-82

Submitted 02/07; published 01/08

Planning Durative Actions Stochastic Domains
Mausam
Daniel S. Weld

MAUSAM @ CS . WASHINGTON . EDU
WELD @ CS . WASHINGTON . EDU

Dept Computer Science Engineering
Box 352350, University Washington
Seattle, WA 98195 USA

Abstract
Probabilistic planning problems typically modeled Markov Decision Process (MDP).
MDPs, otherwise expressive model, allow sequential, non-durative actions.
poses severe restrictions modeling solving real world planning problem. extend
MDP model incorporate 1) simultaneous action execution, 2) durative actions, 3) stochastic durations. develop several algorithms combat computational explosion introduced
features. key theoretical ideas used building algorithms modeling complex problem MDP extended state/action space, pruning irrelevant actions, sampling
relevant actions, using informed heuristics guide search, hybridizing different planners
achieve benefits both, approximating problem replanning. empirical evaluation
illuminates different merits using various algorithms, viz., optimality, empirical closeness
optimality, theoretical error bounds, speed.

1. Introduction
Recent progress achieved planning researchers yielded new algorithms relax, individually, many classical assumptions. example, successful temporal planners SGPlan,
SAPA, etc. (Chen, Wah, & Hsu, 2006; & Kambhampati, 2003) able model actions take
time, probabilistic planners GPT, LAO*, SPUDD, etc. (Bonet & Geffner, 2005; Hansen &
Zilberstein, 2001; Hoey, St-Aubin, Hu, & Boutilier, 1999) deal actions probabilistic
outcomes, etc. However, order apply automated planning many real-world domains must
eliminate larger groups assumptions concert. example, NASA researchers note
optimal control NASA Mars rover requires reasoning uncertain, concurrent, durative
actions mixture discrete metric fluents (Bresina, Dearden, Meuleau, Smith, & Washington, 2002). todays planners handle large problems deterministic concurrent
durative actions, MDPs provide clear framework non-concurrent durative actions
face uncertainty, researchers considered concurrent, uncertain, durative actions
focus paper.
example consider NASA Mars rovers, Spirit Oppurtunity. goal
gathering data different locations various instruments (color infrared cameras, microscopic imager, Mossbauer spectrometers etc.) transmitting data back Earth. Concurrent
actions essential since instruments turned on, warmed calibrated, rover
moving, using instruments transmitting data. Similarly, uncertainty must explicitly
confronted rovers movement, arm control actions cannot accurately predicted.
Furthermore, actions, e.g., moving locations setting experiments, take
time. fact, temporal durations uncertain rover might lose way
c
2008
AI Access Foundation. rights reserved.

fiM AUSAM & W ELD

take long time reach another location, etc. able solve planning problems encountered rover, planning framework needs explicitly model domain constructs
concurrency, actions uncertain outcomes uncertain durations.
paper present unified formalism models domain features together.
Concurrent Markov Decision Processes (CoMDPs) extend MDPs allowing multiple actions per
decision epoch. use CoMDPs base model planning problems involving concurrency.
Problems durative actions, concurrent probabilistic temporal planning (CPTP), formulated
CoMDPs extended state space. formulation able incorporate uncertainty
durations form probabilistic distributions.
Solving planning problems poses several computational challenges: concurrency, extended durations, uncertainty durations lead explosive growth state space,
action space branching factor. develop two techniques, Pruned RTDP Sampled RTDP
address blowup concurrency. develop DUR family algorithms handle
stochastic durations. algorithms explore different points running time vs. solutionquality tradeoff. different algorithms propose several speedup mechanisms 1) pruning provably sub-optimal actions Bellman backup, 2) intelligent sampling action
space, 3) admissible inadmissible heuristics computed solving non-concurrent problems, 4)
hybridizing two planners obtain hybridized planner finds good quality solution intermediate running times, 5) approximating stochastic durations mean values replanning, 6)
exploiting structure multi-modal duration distributions achieve higher quality approximations.
rest paper organized follows: section 2 discuss fundamentals
MDPs real-time dynamic programming (RTDP) solution method. Section 3 describe
model Concurrent MDPs. Section 4 investigates theoretical properties temporal
problems. Section 5 explains formulation CPTP problem deterministic durations.
algorithms extended case stochastic durations Section 6. section supported
empirical evaluation techniques presented section. Section 7 survey
related work area. conclude future directions research Sections 8 9.

2. Background
Planning problems probabilistic uncertainty often modeled using Markov Decision Processes (MDPs). Different research communities looked slightly different formulations
MDPs. versions typically differ objective functions (maximizing reward vs. minimizing
cost), horizons (finite, infinite, indefinite) action representations (DBN vs. parametrized action
schemata). formulations similar nature, algorithms solve
them. Though, methods proposed paper applicable variants models,
clarity explanation assume particular formulation, known stochastic shortest path
problem (Bertsekas, 1995).
define Markov decision process (M) tuple hS, A, Ap, Pr, C, G, s0
finite set discrete states. use factored MDPs, i.e., compactly represented
terms set state variables.
finite set actions.
34

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

State variables : x1 , x2 , x3 , x4 , p12
Action
Precondition
Effect
toggle-x1
p12
x1 x1
toggle-x2
p12
x2 x2
toggle-x3
true
x3 x3
change
toggle-x4
true
x4 x4
change
toggle-p12
true
p12 p12
Goal : x1 = 1, x2 = 1, x3 = 1, x4 = 1

Probability
1
1
0.9
0.1
0.9
0.1
1

Figure 1: Probabilistic STRIPS definition simple MDP potential parallelism
Ap defines applicability function. Ap : P(A), denotes set actions
applied given state (P represents power set).
Pr : [0, 1] transition function. write Pr(s0 |s, a) denote
probability arriving state s0 executing action state s.
C : <+ cost model. write C(s, a, s0 ) denote cost incurred
state s0 reached executing action state s.
G set absorbing goal states, i.e., process ends one states
reached.
s0 start state.
assume full observability, i.e., execution system complete access new state
action performed. seek find optimal, stationary policy i.e., function
: minimizes expected cost (over indefinite horizon) incurred reach goal
state. Note cost function, J: <, mapping states expected cost reaching goal
state defines policy follows:
J (s) = argmin

X

Pr(s0 |s, a) C(s, a, s0 ) + J(s0 )




(1)

aAp(s) s0

optimal policy derives optimal cost function, J , satisfies following pair
Bellman equations.
J (s) = 0, G else
J (s) = min

aAp(s)

X

Pr(s0 |s, a) C(s, a, s0 ) + J (s0 )




(2)

s0

example, Figure 1 defines simple MDP four state variables (x1 , . . . , x4 ) need
set using toggle actions. actions, e.g., toggle-x3 probabilistic.
Various algorithms developed solve MDPs. Value iteration dynamic programming approach optimal cost function (the solution equations 2) calculated
limit series approximations, considering increasingly long action sequences. Jn (s)
35

fiM AUSAM & W ELD

cost state iteration n, cost state next iteration calculated
process called Bellman backup follows:
Jn+1 (s) = min

aAp(s)

X

Pr(s0 |s, a) C(s, a, s0 ) + Jn (s0 )




(3)

s0

Value iteration terminates S, |Jn (s) Jn1 (s)| , termination guaranteed > 0. Furthermore, limit, sequence {Ji } guaranteed converge
optimal cost function, J , regardless initial values long goal reached every reachable state non-zero probability. Unfortunately, value iteration tends quite slow,
since explicitly updates every state, |S| exponential number domain features. One
optimization restricts search part state space reachable initial state s0 . Two algorithms exploiting reachability analysis LAO* (Hansen & Zilberstein, 2001) focus:
RTDP (Barto, Bradtke, & Singh, 1995).
RTDP, conceptually, lazy version value iteration states get updated proportion frequency visited repeated executions greedy policy.
RTDP trial path starting s0 , following greedy policy updating costs
states visited using Bellman backups; trial ends goal reached number
updates exceeds threshold. RTDP repeats trials convergence. Note common states
updated frequently, RTDP wastes time states unreachable, given current
policy. RTDPs strength ability quickly produce relatively good policy; however, complete
convergence (at every relevant state) slow less likely (but potentially important) states get
updated infrequently. Furthermore, RTDP guaranteed terminate. Labeled RTDP (LRTDP)
fixes problems clever labeling scheme focuses attention states value
function yet converged (Bonet & Geffner, 2003). Labeled RTDP guaranteed terminate,
guaranteed converge -approximation optimal cost function (for states reachable using optimal policy) initial cost function admissible, costs (C) positive
goal reachable reachable states non-zero probability.
MDPs powerful framework model stochastic planning domains. However, MDPs make
two unrealistic assumptions 1) actions need executed sequentially, 2) actions
instantaneous. Unfortunately, many real-world domains assumptions
unrealistic. example, concurrent actions essential Mars rover, since instruments
turned on, warmed calibrated rover moving, using instruments
transmitting data. Moreover, action durations non-zero stochastic rover might
lose way navigating may take long time reach destination; may make multiple
attempts finding accurate arm placement. paper successively relax two
assumptions build models algorithms scale spite additional complexities
imposed general models.

3. Concurrent Markov Decision Processes
define new model, Concurrent MDP (CoMDP), allows multiple actions executed
parallel. model different semi-MDPs generalized state semi-MDPs (Younes
& Simmons, 2004b) incorporate action durations explicitly. CoMDPs focus
adding concurrency MDP framework. input CoMDP slightly different
MDP hS, A, Apk , Prk , Ck , G, s0 i. new applicability function, probability model cost
36

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

(Apk , Prk Ck respectively) encode distinction allowing sequential executions
single actions versus simultaneous executions sets actions.
3.1 Model
set states (S), set actions (A), goals (G) start state (s0 ) follow input MDP.
difference lies fact instead executing one action time, may execute
multiple them. Let us define action combination, A, set one actions
executed parallel. action combination new unit operator available agent,
CoMDP takes following new inputs
Apk defines new applicability function. Apk : P(P(A)), denotes set action
combinations applied given state.
Prk : P(A) [0, 1] transition function. write Prk (s0 |s, A) denote
probability arriving state s0 executing action combination state s.
Ck : P(A) <+ cost model. write Ck (s, A, s0 ) denote cost incurred
state s0 reached executing action combination state s.
essence, CoMDP takes action combination unit operator instead single action.
approach convert CoMDP equivalent MDP (Mk ) specified
tuple hS, P(A), Apk , Prk , Ck , G, s0 solve using known MDP algorithms.
3.2 Case Study: CoMDP Probabilistic STRIPS
general CoMDP could require exponentially larger input MDP, since transition model, cost model applicability function defined terms action combinations
opposed actions. compact input representation general CoMDP interesting, open
research question future. work, consider special class compact CoMDP
one defined naturally via domain description similar probabilistic STRIPS
representation MDPs (Boutilier, Dean, & Hanks, 1999).
Given domain encoded probabilistic STRIPS compute safe set co-executable
actions. safe semantics, probabilistic dynamics gets defined consistent way
describe below.
3.2.1 PPLICABILITY F UNCTION
first discuss compute sets actions executed parallel since
actions may conflict other. adopt classical planning notion mutual exclusion (Blum & Furst, 1997) apply factored action representation probabilistic STRIPS.
Two distinct actions mutex (may executed concurrently) state one following occurs:
1. inconsistent preconditions
2. outcome one action conflicts outcome
3. precondition one action conflicts (possibly probabilistic) effect other.
37

fiM AUSAM & W ELD

4. effect one action possibly modifies feature upon another actions transition
function conditioned upon.
Additionally, action never mutex itself. essence, non-mutex actions interact effects executing sequence a1 ; a2 equals a2 ; a1 semantics
parallel executions clear.
Example: Continuing Figure 1, toggle-x1 , toggle-x3 toggle-x4 execute parallel
toggle-x1 toggle-x2 mutex conflicting preconditions. Similarly, toggle-x1
toggle-p12 mutex effect toggle-p12 interferes precondition toggle-x1 .
toggle-x4 outcomes depended toggle-x1 would mutex too, due point 4 above.
example, toggle-x4 toggle-x1 mutex effect toggle-x4 follows: togglex1 probability x4 x4 0.9 else 0.1. 2
applicability function defined set action-combinations, A, action
independently applicable actions pairwise non-mutex other.
Note pairwise concurrency sufficient ensure problem-free concurrency multiple
actions A. Formally Apk defined terms original definition Ap follows:
Apk (s) = {A A|a, a0 A, a, a0 Ap(s) mutex(a, a0 )}

(4)

3.2.2 RANSITION F UNCTION
Let = {a1 , a2 , . . . , ak } action combination applicable s. Since none actions
mutex, transition function may calculated choosing arbitrary order apply
follows:
Prk (s0 |s, A) =

X

...

X

Pr(s1 |s, a1 )Pr(s2 |s1 , a2 ) . . . Pr(s0 |sk1 , ak )

(5)

s1 ,s2 ,...sk

define applicability function transition function allowing consistent set actions executable concurrently, alternative definitions possible.
instance, one might willing allow executing two actions together probability
conflict small. conflict may defined two actions asserting contradictory effects
one negating precondition other. case, new state called failure could created system transitions state case conflict. transition may
computed reflect low probability transition failure state.
Although impose model conflict-free, techniques dont actually depend assumption explicitly extend general CoMDPs.
3.2.3 C OST MODEL
make small change probabilistic STRIPS representation. Instead defining single
cost (C) action, define additively sum resource time components follows:
Let durative cost, i.e., cost due time taken complete action.
Let r resource cost, i.e., cost resources used action.
38

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Assuming additivity think cost action C(s, a, s0 ) = t(s, a, s0 ) + r(s, a, s0 ),
sum time resource usage. Hence, cost model combination actions terms
components may defined as:
Ck (s, {a1 , a2 , ..., ak }, s0 ) =

k
X

r(s, ai , s0 ) + max {t(s, ai , s0 )}
i=1..k

i=1

(6)

example, Mars rover might incur lower cost preheats instrument changing
locations executes actions sequentially, total time reduced
energy consumed change.
3.3 Solving CoMDP MDP Algorithms
taken concurrent MDP allowed concurrency actions formulated equivalent MDP, Mk , extended action space. rest paper use term CoMDP
refer equivalent MDP Mk .
3.3.1 B ELLMAN EQUATIONS
extend Equations 2 set equations representing solution CoMDP:
Jk (s) = 0, G else
Jk (s) = min

AApk (s)

X

n



Prk (s0 |s, A) Ck (s, A, s0 ) + Jk (s0 )

(7)

s0

equations traditional MDP, except instead considering single
actions backup state, need consider applicable action combinations. Thus,
small change must made traditional algorithms (e.g., value iteration, LAO*, Labeled RTDP).
However, since number action combinations worst-case exponential |A|, efficiently
solving CoMDP requires new techniques. Unfortunately, structure exploit easily,
since optimal action state classical MDP solution may even appear optimal
action combination associated concurrent MDP.
Theorem 1 actions optimal combination CoMDP (Mk ) may individually suboptimal MDP M.
Proof: domain Figure 1 let us additional action toggle-x34 toggles x3
x4 probability 0.5 toggles exactly one x3 x4 probability 0.25 each. Let
actions take one time unit each, therefore cost action combination one well.
Let start state x1 = 1, x2 = 1, x3 = 0, x4 = 0 p12 = 1. MDP optimal
action start state toggle-x34 . However, CoMDP Mk optimal combination
{toggle-x3 , toggle-x4 }. 2
3.4 Pruned Bellman Backups
Recall trial, Labeled RTDP performs Bellman backups order calculate costs
applicable actions (or case, action combinations) chooses best action (combination); describe two pruning techniques reduce number backups computed.
39

fiM AUSAM & W ELD

Let Qk (s, A) expected cost incurred executing action combination state
following greedy policy, i.e.
Qkn (s, A) =

X

n



Prk (s0 |s, A) Ck (s, A, s0 ) + Jkn1 (s0 )

(8)

s0

Bellman update thus rewritten as:
Jkn (s) =

min

AApk (s)

Qkn (s, A)

(9)

3.4.1 C OMBO -S KIPPING
Since number applicable action combinations exponential, would prune
suboptimal combinations. following theorem imposes lower bound Qk (s, A) terms
costs Qk -values single actions. theorem costs actions may depend
action starting ending state, i.e., states s, s0 C(s, a, s0 ) = C(a).
Theorem 2 Let = {a1 , a2 , . . . , ak } action combination applicable state s.
CoMDP probabilistic STRIPS, costs dependent actions Qkn values
monotonically non-decreasing
Qk (s, A) max Qk (s, {ai }) + Ck (A)
i=1..k

k
X

!

Ck ({ai })

i=1

Proof:
Qkn (s, A) = Ck (A) +

X

Prk (s0 |s, A)Jkn1 (s0 )

(using Eqn. 8)

s0



X

Prk (s0 |s, A)Jkn1 (s0 ) = Qkn (s, A) Ck (A)

(10)

s0

Qkn (s, {a1 }) = Ck ({a1 }) +

X

Pr(s00 |s, a1 )Jkn1 (s00 )

s00

"

Ck ({a1 }) +

X

#

00

Pr(s |s, a1 ) Ck ({a2 }) +

s00

X

000

00

000

Pr(s |s , a2 )Jkn2 (s )

s000

(using Eqns. 8 9)
= Ck ({a1 }) + Ck ({a2 }) +

X

000

000

Prk (s |s, {a1 , a2 })Jkn2 (s )

s000


=

k
X
i=1
k
X

Ck ({ai }) +

X

Prk (s0 |s, A)Jknk (s0 )

(repeating actions A)

s0

Ck ({ai }) + [Qknk+1 (s, A) Ck (A)]

i=1

Replacing n n + k 1
40

(using Eqn. 10)

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Qkn (s, A) Qkn+k1 (s, {a1 }) + Ck (A)

k
X

!

Ck ({ai })

i=1

Qkn (s, {a1 }) + Ck (A)

k
X

!

Ck ({ai })

(monotonicity Qkn )

i=1



max Qkn (s, {ai }) + Ck (A)

i=1..k

k
X

!

Ck ({ai })

i=1

2

proof assumes equation 5 probabilistic STRIPS. following corollary
used prune suboptimal action combinations:
Corollary 3 Let dJkn (s)e upper bound Jkn (s).
dJkn (s)e < max Qkn (s, {ai }) + Ck (A)
i=1..k

k
X

!

Ck ({ai })

i=1

cannot optimal state iteration.
Proof: Let = {a1 , a2 , . . . , ak } optimal combination state iteration n. Then,
dJkn (s)e Jkn (s)
Jkn (s) = Qkn (s, )
Combining Theorem 2
dJkn (s)e maxi=1..k Qkn (s, {ai }) +

Ck (An )



k
X

!

Ck ({ai }) 2

i=1

Corollary 3 justifies pruning rule, combo-skipping, preserves optimality iteration
algorithm maintains cost function monotonicity. powerful Bellman-backup
based algorithms preserve monotonicity started admissible cost function. apply
combo-skipping, one must compute Qk (s, {a}) values single actions applicable
s. calculate dJkn (s)e one may use optimal combination state previous iteration
(Aopt ) compute Qkn (s, Aopt ). value gives upper bound value Jkn (s).
Example: Consider Figure 1. Let single action incur unit cost, let cost action combination be: Ck (A) = 0.5 + 0.5|A|. Let state = (1,1,0,0,1) represent ordered values x1 = 1, x2 =
1, x3 = 0, x4 = 0, p12 = 1. Suppose, nth iteration, cost function assigns values:
Jkn (s) = 1, Jkn (s1 =(1,0,0,0,1)) = 2, Jkn (s2 =(1,1,1,0,1)) = 1, Jkn (s3 =(1,1,0,1,1)) = 1. Let Aopt
state {toggle-x3 , toggle-x4 }. Now, Qkn+1 (s, {toggle-x2 }) = Ck ({toggle-x2 }) + Jkn (s1 ) = 3
Qkn+1 (s, Aopt ) = Ck (Aopt ) + 0.810 + 0.09Jkn (s2 ) + 0.09Jkn (s3 ) + 0.01Jkn (s) = 1.69.
apply Corollary 3 skip combination {toggle-x2 , toggle-x3 } iteration, since
using toggle-x2 a1 , dJkn+1 (s)e = Qkn+1 (s, Aopt ) = 1.69 3 + 1.5 - 2 = 2.5. 2
Experiments show combo-skipping yields considerable savings. Unfortunately, comboskipping weakness prunes combination single iteration. contrast,
second rule, combo-elimination, prunes irrelevant combinations altogether.
41

fiM AUSAM & W ELD

3.4.2 C OMBO -E LIMINATION
adapt action elimination theorem traditional MDPs (Bertsekas, 1995) prove similar
theorem CoMDPs.
Theorem 4 Let action combination applicable state s. Let bQk (s, A)c denote
lower bound Qk (s, A). bQk (s, A)c > dJk (s)e never optimal combination
state s.
Proof: CoMDP MDP new action space, original proof MDPs (Bertsekas,
1995) holds replacing action action combination. 2
order apply theorem pruning, one must able evaluate upper lower
bounds. using admissible cost function starting RTDP search (or value iteration,
LAO* etc.), current cost Jkn (s) guaranteed lower bound optimal cost; thus,
Qkn (s, A) lower bound Qk (s, A). Thus, easy compute left hand side
inequality. calculate upper bound optimal Jk (s), one may solve MDP M,
i.e., traditional MDP forbids concurrency. much faster solving CoMDP,
yields upper bound cost, forbidding concurrency restricts policy use
strict subset legal action combinations. Notice combo-elimination used general
MDPs restricted CoMDPs probabilistic STRIPS.
Example: Continuing previous example, let A={toggle-x2 } Qkn+1 (s, A) = Ck (A) +
Jkn (s1 ) = 3 dJk (s)e = 2.222 (from solving MDP M). 3 > 2.222, eliminated
state remaining iterations. 2
Used fashion, combo-elimination requires additional overhead optimally solving
single-action MDP M. Since algorithms RTDP exploit state-space reachability limit
computation relevant states, computation incrementally, new states visited
algorithm.
Combo-elimination requires computation current value Qk (s, A) (for lower
bound Qk (s, A)); differs combo-skipping avoids computation. However,
combo-elimination prunes combination, never needs reconsidered. Thus,
tradeoff: one perform expensive computation, hoping long-term pruning, try
cheaper pruning rule fewer benefits? Since Q-value computation costly step, adopt
following heuristic: First, try combo-skipping; fails prune combination, attempt
combo-elimination; succeeds, never consider again. tried implementing
heuristics, as: 1) combination skipped repeatedly, try prune altogether combo-elimination. 2) every state, try combo-elimination probability p. Neither
alternative performed significantly better, kept original (lower overhead) heuristic.
Since combo-skipping change step labeled RTDP combo-elimination removes provably sub-optimal combinations, pruned labeled RTDP maintains convergence, termination, optimality efficiency, used admissible heuristic.
3.5 Sampled Bellman Backups
Since fundamental challenge posed CoMDPs explosion action combinations, sampling promising method reduce number Bellman backups required per state.
describe variant RTDP, called sampled RTDP, performs backups random set
42

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

action combinations1 , choosing distribution favors combinations likely
optimal. generate distribution by:
1. using combinations previously discovered low Qk -values (recorded memoizing best combinations per state, iteration)
2. calculating Qk -values applicable single actions (using current cost function)
biasing sampling combinations choose ones contain actions low
Qk -values.

Algorithm 1 Sampled Bellman Backup(state, m)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

Function 2 SampleComb(state, i, l)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

//returns best combination found

list l = //a list applicable actions values
action
compute Qk (state, {action})
insert ha, 1/Qk (state, {action})i l
[1..m]
newcomb = SampleComb(state, i, l);
compute Qk (state, newcomb)
clear memoizedlist[state]
compute Qmin minimum Qk values computed line 7
store combinations Qk (state, A) = Qmin memoizedlist[state]
return first entry memoizedlist[state]

//returns ith combination sampled backup

size(memoizedlist[state])
return ith entry memoizedlist[state] //return combination memoized previous iteration
newcomb =
repeat
randomly sample action l proportional value
insert newcomb
remove actions mutex l
l empty
done = true
else |newcomb| == 1
done = false //sample least 2 actions per combination
else
|newcomb|
done = true prob. |newcomb|+1
done
return newcomb

approach exposes exploration / exploitation trade-off. Exploration, here, refers testing wide range action combinations improve understanding relative merit. Exploitation, hand, advocates performing backups combinations previously
shown best. manage tradeoff carefully maintaining distribution
combinations. First, memoize best combinations per state; always backed-up
1. similar action sampling approach used context space shuttle scheduling reduce number
actions considered value function computation (Zhang & Dietterich, 1995).

43

fiM AUSAM & W ELD

Bellman update. combinations constructed incremental probabilistic process,
builds combination first randomly choosing initial action (weighted individual Qk -value), deciding whether add non-mutex action stop growing combination.
many implementations possible high level idea. tried several
found results similar them. Algorithm 1 describes implementation used
experiments. algorithm takes state total number combinations input
returns best combination obtained far. memoizes best combinations
state memoizedlist. Function 2 helper function returns ith combination either
one best combinations memoized previous iteration new sampled combination.
notice line 10 Function 2. forces sampled combinations least size 2, since
individual actions already backed (line 3 Algo 1).
3.5.1 ERMINATION PTIMALITY
Since system consider every possible action combination, sampled RTDP guaranteed choose best combination execute state. result, even started
admissible heuristic, algorithm may assign Jkn (s) cost greater optimal Jk (s)
i.e., Jkn (s) values longer admissible. better combination chosen subsequent
iteration, Jkn+1 (s) might set lower value Jkn (s), thus sampled RTDP monotonic.
unfortunate, since admissibility monotonicity important properties required termination2 optimality labeled RTDP; indeed, sampled RTDP loses important theoretical
properties. good news extremely useful practice. experiments, sampled
RTDP usually terminates quickly, returns costs extremely close optimal.
3.5.2 MPROVING OLUTION Q UALITY
investigated several heuristics order improve quality solutions found
sampled RTDP. heuristics compensate errors due partial search lack admissibility.
Heuristic 1: Whenever sampled RTDP asserts convergence state, immediately
label converged (which would preclude exploration (Bonet & Geffner, 2003));
instead first run complete backup phase, using admissible combinations, rule
easy-to-detect inconsistencies.
Heuristic 2: Run sampled RTDP completion, use cost function produces, J (),
initial heuristic estimate, J0 (), subsequent run pruned RTDP. Usually,
heuristic, though inadmissible, highly informative. Hence, pruned RTDP terminates quite
quickly.
Heuristic 3: Run sampled RTDP pruned RTDP, Heuristic 2, except instead
using J () cost function directly initial estimate, scale linearly downward i.e.,
use J0 () := cJ () constant c (0, 1). guarantees hope
lies admissible side optimal. experience often case
c = 0.9, run pruned RTDP yields optimal policy quickly.
2. ensure termination implemented policy: number trials exceeds threshold, force monotonicity
cost function. achieve termination reduce quality solution.

44

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Experiments showed Heuristic 1 returns cost function close optimal. Adding
Heuristic 2 improves value moderately, combination Heuristics 1 3 returns
optimal solution experiments.
3.6 Experiments: Concurrent MDP
Concurrent MDP fundamental formulation, modeling concurrent actions general planning
domain. first compare various techniques solve CoMDPs, viz., pruned sampled RTDP.
following sections use techniques model problems durative actions.
tested algorithms problems three domains. first domain probabilistic
variant NASA Rover domain 2002 AIPS Planning Competition (Long & Fox, 2003),
multiple objects photographed various rocks tested resulting
data communicated back base station. Cameras need focused, arms need
positioned usage. Since rover multiple arms multiple cameras, domain
highly parallel. cost function includes resource time components, executing multiple actions parallel cheaper executing sequentially. generated problems
20-30 state variables 81,000 reachable states average number applicable
combinations per state, Avg(Ap(s)), measures amount concurrency problem,
2735.
tested probabilistic version machineshop domain multiple subtasks (e.g.,
roll, shape, paint, polish etc.), need performed different objects using different
machines. Machines perform parallel, capable every task. tested
problems 26-28 state variables around 32,000 reachable states. Avg(Ap(s)) ranged
170 2640 various problems.
Finally, tested artificial domain similar one shown Figure 1 much
complex. domain, Boolean variables need toggled; however, toggling probabilistic nature. Moreover, certain pairs actions conflicting preconditions thus,
varying number mutex actions may control domains degree parallelism.
problems domain 19 state variables 32,000 reachable states, Avg(Ap(s))
1024 12287.
used Labeled RTDP, implemented GPT (Bonet & Geffner, 2005), base MDP
solver. implemented C++. implemented3 various algorithms, unpruned RTDP (U RTDP), pruned RTDP using combo skipping (Ps -RTDP), pruned RTDP using combo
skipping combo elimination (Pse -RTDP), sampled RTDP using Heuristic 1 (S-RTDP) sampled RTDP using Heuristics 1 3, value functions scaled 0.9 (S3 -RTDP). tested
algorithms number problem instantiations three domains, generated
varying number objects, degrees parallelism, distances goal. experiments
performed 2.8 GHz Pentium processor 2 GB RAM.
observe (Figure 2(a,b)) pruning significantly speeds algorithm. comparison Pse -RTDP S-RTDP S3 -RTDP (Figure 3(a,b)) shows sampling dramatic
speedup respect pruned versions. fact, pure sampling, S-RTDP, converges extremely
quickly, S3 -RTDP slightly slower. However, S3 -RTDP still much faster Pse -RTDP.
comparison qualities solutions produced S-RTDP S3 -RTDP w.r.t. optimal shown
Table 1. observe solutions produced S-RTDP always nearly optimal. Since
3. code may downloaded http://www.cs.washington.edu/ai/comdp/comdp.tgz

45

fiM AUSAM & W ELD

Comparison Pruned Unpruned RTDP Rover domain

Comparison Pruned Unpruned RTDP Factory domain
12000

y=x
Ps-RTDP
Pse-RTDP

25000

Times Pruned RTDP (in sec)

Times Pruned RTDP (in sec)

30000

20000
15000
10000
5000

y=x
Ps-RTDP
Pse-RTDP

10000
8000
6000
4000
2000

0

0
0

5000
10000 15000 20000 25000
Times Unpruned RTDP (in sec)

30000

0

2000
4000
6000
8000
10000
Times Unpruned RTDP (in sec)

12000

Figure 2: (a,b): Pruned vs. Unpruned RTDP Rover MachineShop domains respectively. Pruning
non-optimal combinations achieves significant speedups larger problems.

Comparison Pruned Sampled RTDP Rover domain

Comparison Pruned Sampled RTDP Factory domain
8000

y=x
S-RTDP
S3-RTDP

8000

Times Sampled RTDP (in sec)

Times Sampled RTDP (in sec)

10000

6000
4000
2000
0

y=x
S-RTDP
S3-RTDP

7000
6000
5000
4000
3000
2000
1000
0

0

2000 4000 6000 8000 1000012000140001600018000
Times Pruned RTDP (Pse-RTDP) - (in sec)

0

1000 2000 3000 4000 5000 6000 7000 8000
Times Pruned RTDP (Pse-RTDP) - (in sec)

Figure 3: (a,b): Sampled vs Pruned RTDP Rover MachineShop domains respectively. Random
sampling action combinations yields dramatic improvements running times.

46

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Comparison algorithms size problem Rover domain
30000

S-RTDP
S3-RTDP
Pse-RTDP
U-RTDP

25000

S-RTDP
S3-RTDP
Pse-RTDP
U-RTDP

15000

20000

Times (in sec)

Times (in sec)

Comparison different algorithms Artificial domain
20000

15000
10000

10000

5000
5000
0

0
0

5e+07

1e+08
1.5e+08
Reach(|S|)*Avg(Ap(s))

2e+08

2.5e+08

0

2000

4000

6000 8000
Avg(Ap(s))

10000 12000 14000

Figure 4: (a,b): Comparison different algorithms size problems Rover Artificial domains. problem size increases, gap sampled pruned approaches widens
considerably.

Results varying Number samples Rover Problem#4

300
250
200
150
100
50
0

Running times
Values start state

300

100

200

300 400 500 600 700
Concurrency : Avg(Ap(s))/|A|

800

900

12.8

250

12.79

200

12.78

150

12.77

100

12.76

50

J*(s0)

0
0

12.81

Value start state

350

S-RTDP/Pse-RTDP

Times Sampled RTDP (in sec)

Speedup : Sampled RTDP/Pruned RTDP

Speedup vs. Concurrency Artificial domain
350

10

20

30

40 50 60 70 80
Number samples

12.74
90 100

Figure 5: (a): Relative Speed vs. Concurrency Artificial domain. (b) : Variation quality solution
efficiency algorithm (with 95% confidence intervals) number samples Sampled RTDP one particular problem Rover domain. number samples increase,
quality solution approaches optimal time still remains better Pse -RTDP (which
takes 259 sec. problem).

47

fiM AUSAM & W ELD

Problem
Rover1
Rover2
Rover3
Rover4
Rover5
Rover6
Rover7
Artificial1
Artificial2
Artificial3
MachineShop1
MachineShop2
MachineShop3
MachineShop4
MachineShop5

J(s0 ) (S-RTDP)
10.7538
10.7535
11.0016
12.7490
7.3163
10.5063
12.9343
4.5137
6.3847
6.5583
15.0859
14.1414
16.3771
15.8588
9.0314

J (s0 ) (Optimal)
10.7535
10.7535
11.0016
12.7461
7.3163
10.5063
12.9246
4.5137
6.3847
6.5583
15.0338
14.0329
16.3412
15.8588
8.9844

Error
<0.01%
0
0
0.02%
0
0
0.08%
0
0
0
0.35%
0.77%
0.22%
0
0.56%

Table 1: Quality solutions produced Sampled RTDP
error S-RTDP small, scaling 0.9 makes admissible initial cost function pruned
RTDP; indeed, experiments, S3 -RTDP produced optimal solution.
Figure 4(a,b) demonstrates running times vary problem size. use product
number reachable states average number applicable action combinations per state
estimate size problem (the number reachable states artificial domains
same, hence x-axis Figure 4(b) Avg(Ap(s))). figures, verify
number applicable combinations plays major role running times concurrent MDP
algorithms. Figure 5(a), fix factors vary degree parallelism. observe
speedups obtained S-RTDP increase concurrency increases. encouraging result,
expect S-RTDP perform well large problems inolving high concurrency, even
approaches fail.
Figure 5(b), present another experiment vary number action combinations sampled backup. solution quality inferior sampling
combinations, quickly approaches optimal increasing number samples.
experiments sample 40 combinations per state.

4. Challenges Temporal Planning
CoMDP model powerful enough model concurrency actions, still assumes
action instantaneous. incorporate actual action durations modeling problem.
essential increase scope current models real world domains.
present model algorithms discuss several new theoretical challenges
imposed explicit action durations. Note results section apply wide range
planning problems:
regardless whether durations uncertain fixed
regardless whether effects stochastic deterministic.
48

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Actions uncertain duration modeled associating distribution (possibly conditioned
outcome stochastic effects) execution times. focus problems whose objective
achieve goal state minimizing total expected time (make-span), results extend
cost functions combine make-span resource usage. raises question
goal counts achieved. require that:
Assumption 1 executing actions terminate goal considered achieved.
Assumption 2 action, started, cannot terminated prematurely.
start asking question restricted set time points optimality
preserved even actions started points?
Definition 1 time point new action allowed start execution called decision
epoch. time point pivot either 0 time new effect might occur (e.g.,
end actions execution) new precondition may needed existing precondition may
longer needed. happening either 0 time effect actually occurs new
precondition definitely needed existing precondition longer needed.
Intuitively, happening point change world state action constraints actually
happens (e.g., new effect new precondition). execution crosses pivot (a possible
happening), information gained agents execution system (e.g., didnt effect
occur) may change direction future action choices. Clearly, action durations
deterministic, set pivots set happenings.
Example: Consider action whose durations follow uniform integer duration 1
10. started time 0 timepoints 0, 1, 2,. . ., 10 pivots. certain execution
finishes time 4 4 (and 0) happening (for execution). 2
Definition 2 action PDDL2.1 action (Fox & Long, 2003) following hold:
effects realized instantaneously either (at start) (at end), i.e., beginning
completion action (respectively).
preconditions may need hold instaneously start (at start), end (at
end) complete execution action (over all).

(:durative-action
:duration (= ?duration 4)
:condition (and (over P ) (at end Q))
:effect (at end Goal))
(:durative-action b
:duration (= ?duration 2)
:effect (and (at start Q) (at end (not P ))))

Figure 6: domain illustrate expressive action model may require arbitrary decision epochs
solution. example, b needs start 3 units execution reach Goal.

49

fiM AUSAM & W ELD

Theorem 5 PDDL2.1 domain restricting decision epochs pivots causes incompleteness
(i.e., problem may incorrectly deemed unsolvable).
Proof: Consider deterministic temporal planning domain Figure 6 uses PDDL2.1 notation
(Fox & Long, 2003). initial state P =true Q=false, way reach Goal
start time (e.g., 0), b timepoint open interval (t + 2, + 4). Clearly,
new information gained time points interval none pivot. Still,
required solving problem. 2
Intuitively, instantaneous start end effects two PDDL2.1 actions may require certain
relative alignment within achieve goal. alignment may force one action start
somewhere (possibly non-pivot point) midst others execution, thus requiring
intermediate decision epochs considered.
Temporal planners may classified one two architectures: constraint-posting
approaches times action execution gradually constrained planning (e.g.,
Zeno LPG, see Penberthy Weld, 1994; Gerevini Serina, 2002) extended statespace methods (e.g., TP4 SAPA, see Haslum Geffner, 2001; Kambhampati, 2001).
Theorem 5 holds architectures strong computational implications state-space
planners limiting attention subset decision epochs speed planners.
theorem shows planners SAPA Prottle (Little, Aberdeen, & Thiebaux, 2005)
incomplete. Fortunately, assumption restricts set decision epochs considerably.
Definition 3 action TGP-style action4 following hold:
effects realized unknown point action execution, thus used
action completed.
preconditions must hold beginning action.
preconditions (and features transition function conditioned) must
changed actions execution, except effect action itself.
Thus, two TGP-style actions may execute concurrently clobber others preconditions effects. case TGP-style actions set happenings nothing set
time points action terminates. TGP pivots set points action might
terminate. (Of course sets additionally include zero).
Theorem 6 actions TGP-style, set decision epochs may restricted pivots
without sacrificing completeness optimality.
Proof Sketch: contradiction. Suppose optimal policy satisfies theorem;
must exist path optimal policy one must start action, a, time even
though action could terminated t. Since planner hasnt gained
information t, case analysis (which requires actions TGP-style) shows one could
started earlier execution path without increasing make-span. detailed proof
discussed Appendix. 2
case deterministic durations, set happenings set pivots; hence
following corollary holds:
4. original TGP (Smith & Weld, 1999) considered deterministic actions fixed duration, use
phrase TGP-style general way, without restrictions.

50

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Probabillity: 0.5
a0

s0

a2

G

a1
Makespan: 3
Probability 0.5
a0

s0
a1

G

b0
Makespan: 9

0

2

4

6

8

Time

Figure 7: Pivot decision epochs necessary optimal planning face nonmonotonic continuation. domain, Goal achieved h{a0 , a1 }; a2 hb0 i; a0 duration 2
9; b0 mutex a1 . optimal policy starts a0 then, a0 finish
time 2, starts b0 (otherwise starts a1 ).

Corollary 7 actions TGP-style deterministic durations, set decision
epochs may restricted happenings without sacrificing completeness optimality.
planning uncertain durations may huge number pivots; useful
constrain range decision epochs.
Definition 4 action independent duration correlation probabilistic
effects duration.
Definition 5 action monotonic continuation expected time action termination
nonincreasing execution.
Actions without probabilistic effects, nature, independent duration. Actions monotonic continuations common, e.g. uniform, exponential, Gaussian, many duration distributions. However, actions bimodal multi-modal distributions dont monotonic continuations. example consider action uniform distribution [1,3].
action doesnt terminate 2, expected time completion calculated 2, 1.5,
1 times 0, 1, 2 respectively, monotonically decreasing. example
non-monotonic continuation see Figure 18.
Conjecture 8 actions TGP-style, independent duration monotonic continuation,
set decision epochs may restricted happenings without sacrificing completeness
optimality.
actions continuation nonmonotonic failure terminate increase expected
time remaining cause another sub-plan preferred (see Figure 7). Similarly, actions
duration isnt independent failure terminate changes probability eventual effects
may prompt new actions started.
exploiting theorems conjecture may significantly speed planning since
able limit number decision epochs needed decision-making. use theoretical
understanding models. First, simplicity, consider case TGP-style actions
deterministic durations. Section 6, relax restriction allowing stochastic durations,
unimodal well multimodal.
51

fiM AUSAM & W ELD

togglep12
p12 (effect)
conflict
p12 (Precondition)
togglex1
0

2

4

6

8

10

Figure 8: sample execution demonstrating conflict due interfering preconditions effects. (The
actions shaded disambiguate preconditions effects)

5. Temporal Planning Deterministic Durations
use abbreviation CPTP (short Concurrent Probabilistic Temporal Planning) refer
probabilistic planning problem durative actions. CPTP problem input model
similar CoMDPs except action costs, C(s, a, s0 ), replaced deterministic
durations, (a), i.e., input form hS, A, Pr, , G, s0 i. study objective minimizing expected time (make-span) reaching goal. rest paper make
following assumptions:
Assumption 3 action durations integer-valued.
assumption negligible effect expressiveness one convert problem
rational durations one satisfies Assumption 3 scaling durations g.c.d.
denominators. case irrational durations, one always find arbitrarily close approximation original problem approximating irrational durations rational numbers.
reasons discussed previous section adopt TGP temporal action model Smith
Weld (1999), rather complex PDDL2.1 (Fox & Long, 2003). Specifically:
Assumption 4 actions follow TGP model.
restrictions consistent previous definition concurrency. Specifically,
mutex definitions (of CoMDPs probabilistic STRIPS) hold required assumptions. illustration, consider Figure 8. describes situation two actions
interfering preconditions effects executed concurrently. see not, suppose
initially p12 false two actions toggle-x1 toggle-p12 started time 2 4, respectively. p12 precondition toggle-x1 , whose duration 5, needs remain false
time 7. toggle-p12 may produce effects anytime 4 9, may conflict
preconditions executing action. Hence, forbid concurrent execution
toggle-x1 toggle-p12 ensure completely predictable outcome distribution.
definition concurrency, dynamics model remains consistent
Equation 5. Thus techniques developed CoMDPs derived probabilistic STRIPS actions
may used.
52

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Aligned Epoch policy execution
(takes 9 units)
togglex1
t3

f

f

f

f

t3 t3 t3 t3

0

5



10
time

togglex1
f

f

f

f

t3 t3 t3 t3 t3



Interwoven Epoch policy execution
(takes 5 units)

Figure 9: Comparison times taken sample execution interwoven-epoch policy alignedepoch policy. trajectories toggle-x3 (t3) action fails four times succeeding.
aligned policy must wait actions complete starting more, takes
time interwoven policy, start actions middle.

5.1 Formulation CoMDP
model CPTP problem CoMDP, thus MDP, one way. list
two prominent formulations below. first formulation, aligned epoch CoMDP models
problem approximately solves quickly. second formulation, interleaved epochs models
problem exactly results larger state space hence takes longer solve using existing
techniques. subsequent subsections explore ways speed policy construction
interleaved epoch formulation.
5.1.1 LIGNED E POCH EARCH PACE
simple way formulate CPTP model standard CoMDP probabilistic STRIPS,
action costs set durations cost combination maximum
duration constituent actions (as Equation 6). formulation introduces substantial
approximation CPTP problem. true deterministic domains too, illustrate
using example involving stochastic effects. Figure 9 compares trajectories
toggle-x3 (t3) actions fails four consecutive times succeeding. figure, f
denote failure success uncertain actions, respectively. vertical dashed lines represent
time-points action started.
Consider actual executions resulting policies. aligned-epoch case (Figure 9
top), combination actions started state, next decision taken
effects actions observed (hence name aligned-epochs). contrast, Figure 9
bottom shows decision epoch optimal execution CPTP problem, many actions
may midway execution. explicitly take account actions
remaining execution times making subsequent decision. Thus, actual state space
CPTP decision making substantially different simple aligned-epoch model.
Note due Corollary 7 sufficient consider new decision epoch happening,
i.e., time-point one actions complete. Thus, using Assumption 3 infer
decision epochs discrete (integer). course, optimal policies property.
53

fiM AUSAM & W ELD

State variables : x1 , x2 , x3 , x4 , p12
Action
(a) Precondition
toggle-x1
5
p12
toggle-x2
5
p12
toggle-x3
1
true

Effect
x1 x1
x2 x2
x3 x3
change
toggle-x4
1
true
x4 x4
change
toggle-p12
5
true
p12 p12
Goal : x1 = 1, x2 = 1, x3 = 1, x4 = 1

Probability
1
1
0.9
0.1
0.9
0.1
1

Figure 10: domain Example 1 extended action durations.
easy see exists least one optimal policy action begins
happening. Hence search space reduces considerably.
5.1.2 NTERWOVEN E POCH EARCH PACE
adapt search space representation Haslum Geffner (2001), similar
research (Bacchus & Ady, 2001; & Kambhampati, 2001). original state space
Section 2 augmented including set actions currently executing times passed
since started. Formally, let new interwoven state5 - ordered pair hX,
where:
XS
= {(a, )|a A, 0 < (a)}
X represents values state variables (i.e. X state original state space)
denotes set ongoing actions times
passed since start . Thus
N
overall interwoven-epoch search space - = aA {a} Z(a) , Z(a) represents
N
set {0, 1, . . . , (a) 1}
denotes Cartesian product multiple sets.
define set actions already execution. words, projection
ignoring execution times progress:
= {a|(a, ) = hX, i}
Example: Continuing example domain Figure 10, suppose state s1 state
variables false, suppose action toggle-x1 started 3 units ago current time.
state would represented hX1 , Y1 X1 =(F, F, F, F, F ) Y1 ={(toggle-x1 ,3)} (the five
state variables listed order: x1 , x2 , x3 , x4 p12 ). set As1 would {toggle-x1 }.
allow possibility simply waiting action complete execution, is, deciding decision epoch start additional action, augment set no-op action,
applicable states = hX, 6= (i.e. states action still
executed). state s, no-op action mutex non-executing actions, i.e.,
\ . words, decision epoch either no-op started combination
5. use subscript - denote interwoven state space (S - ), value function (J - ), etc..

54

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

involving no-op. define no-op variable duration6 equal time another
already executing action completes (next (s, A) defined below).
interwoven applicability set defined as:
(

Ap - (s) =

Apk (X) = else
{noop}{A|AAs Apk (X) AAs = }

Transition Function: need define probability transition function, Pr - ,
interwoven state space. decision epoch let agent state = (X, ). Suppose
agent decides execute action combination A. Define Ynew set similar
consisting actions starting; formally Ynew = {(a, (a))|a A}. system,
next decision epoch next time executing action terminates. Let us call time
next (s, A). Notice next (s, A) depends executing newly started actions. Formally,
next (s, A) =

min

(a,)Y Ynew

(a)

Moreover, multiple actions may complete simultaneously. Define Anext (s, A)
set actions complete exactly next (s, A) timesteps. -component state
decision epoch next (s, A) time
Ynext (s, A) = {(a, + next (s, A))|(a, ) Ynew , (a) > next (s, A)}
Let s=hX, let s0 =hX 0 , 0 i. transition function CPTP defined as:
Prk (X 0 |X, Anext (s, A)) 0= Ynext (s, A)
0
otherwise

(
0

Pr - (s |s, A)=

words, executing action combination state = hX, takes agent
decision epoch next (s, A) ahead time, specifically first time combination
Anext (s, A) completes. lets us calculate Ynext (s, A): new set actions still executing
times elapsed. Also, TGP-style actions, probability distribution different
state variables modified independently. Thus probability transition function due CoMDP
probabilistic STRIPS used decide new distribution state variables,
combination Anext (s, A) taken state X.
Example: Continuing previous example, let agent state s1 execute action combination = {toggle-x4 }. next (s1 , A) = 1, since toggle-x4 finish first. Thus,
Anext (s1 , A)= {toggle-x4 }. Ynext (s1 , A) = {(toggle-x1 ,4)}. Hence, probability distribution
states executing combination state s1
((F, F, F, T, F ), Ynext (s1 , A)) probability = 0.9
((F, F, F, F, F ), Ynext (s1 , A)) probability = 0.1
6. precise definition model create multiple no-opt actions different constant durations no-opt
applicable interwoven state one = next (s, A).

55

fiM AUSAM & W ELD

Start Goal States: interwoven space, start state hs0 , new set goal
states G - = {hX, i|X G}.
redefining start goal states, applicability function, probability transition
function, finished modeling CPTP problem CoMDP interwoven state space.
use techniques CoMDPs (and MDPs well) solve problem. particular,
use Bellman equations described below.
Bellman Equations: set equations solution CPTP problem written as:
J - (s) = 0, G - else



(11)








next (s, A) + Pr - (s0 |s, A)J - (s0 )
J - (s) = min



AAp - (s)


s0



X

use DURsamp refer sampled RTDP algorithm search space. main
bottleneck naively inheriting algorithms DURsamp huge size interwoven state
space. worst case (when actions executed concurrently) size state space
Q
|S| ( aA (a)). get bound observing action a, (a) number
possibilities: either executing remaining times 1, 2, . . . , (a) 1.
Thus need reduce abstract/aggregate state space order make problem
tractable. present several heuristics used speed search.
5.2 Heuristics
present admissible inadmissible heuristics used initial cost
function DURsamp algorithm. first heuristic (maximum concurrency) solves underlying MDP thus quite efficient compute. second heuristic (average concurrency)
inadmissible, tends informed maximum concurrency heuristic.
5.2.1 AXIMUM C ONCURRENCY H EURISTIC
prove optimal expected cost traditional (serial) MDP divided maximum
number actions executed parallel lower bound expected make-span
reaching goal CPTP problem. Let J(X) denote value state X traditional
MDP costs action equal duration. Let Q(X, A) denote expected cost reach
goal initially actions combination executed greedy serial policy followed
P
thereafter. Formally, Q(X, A) = X 0 Prk (X 0 |X, A)J(X 0 ). Let J - (s) value equivalent
CPTP problem interwoven-epoch state space. Let concurrency state
maximum number actions could executed state concurrently. define maximum
concurrency domain (c) maximum number actions concurrently executed
world state domain. following theorem used provide admissible
heuristic CPTP problems.
Theorem 9 Let = hX, i,
J - (s)

J - (s)


J (X)
=
c
Q (X, )
6=
c
56

(12)

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Proof Sketch: Consider trajectory make-span L (from state = hX, goal state)
CPTP problem using optimal policy. make concurrent actions sequential executing
chronological order started. concurrent actions non-interacting,
outcomes stage similar probabilities. maximum make-span sequential
trajectory cL (assuming c actions executing points semi-MDP trajectory). Hence
J(X) using (possibly non-stationary) policy would cJ - (s). Thus J (X) cJ - (s).


second inequality proven similar way. 2
cases bounds tight. example, consider deterministic planning
problem optimal plan concurrently executing c actions unit duration (makespan = 1). sequential version, actions would taken sequentially (make-span =
c).
Following theorem, maximum concurrency (MC) heuristic state = hX,
defined follows:
Q (X, )
J (X)
else HM C (s) =
= HM C (s) =
c
c
maximum concurrency c calculated static analysis domain onetime expense. complete heuristic function evaluated solving MDP states.
However, many states may never visited. implementation, calculation
demand, states visited, starting MDP current state. RTDP run
seeded previous value function, thus computation thrown away
relevant part state space explored. refer DURsamp initiated MC heuristic
DURMC
samp .
5.2.2 AVERAGE C ONCURRENCY H EURISTIC
Instead using maximum concurrency c heuristic use average concurrency
domain (ca ) get average concurrency (AC) heuristic. call resulting algorithm
DURAC
samp . AC heuristic admissible, experiments typically informed
heuristic. Moreover, case actions duration, AC heuristic equals
MC heuristic.
5.3 Hybridized Algorithm
present approximate method solve CPTP problems. many kinds
possible approximation methods, technique exploits intuition best focus computation probable branches current policys reachable space. danger
approach chance that, execution, agent might end unlikely branch,
poorly explored; indeed might blunder dead-end case. undesirable, apparently attractive policy might true expected make-span infinity.
Since, wish avoid dead-ends, explore desirable notion propriety.
Definition 6 Propriety: policy proper state guaranteed lead, eventually, goal
state (i.e., avoids dead-ends cycles) (Barto et al., 1995). define planning algorithm
proper always produces proper policy (when one exists) initial state.
describe anytime approximation algorithm, quickly generates proper policy
uses additional available computation time improve policy, focusing
likely trajectories.
57

fiM AUSAM & W ELD

5.3.1 H YBRIDIZED P LANNER
algorithm, DURhyb , created hybridizing two policy creation algorithms. Indeed,
novel notion hybridization general powerful, applying many MDP-like problems; however, paper focus use hybridization CPTP. Hybridization uses
anytime algorithm RTDP create policy frequently visited states, uses faster (and
presumably suboptimal) algorithm infrequent states.
case CPTP, algorithm hybridizes RTDP algorithms interwoven-epoch
aligned-epoch models. aligned-epochs, RTDP converges relatively quickly, state
space smaller, resulting policy suboptimal CPTP problem, policy
waits currently executing actions terminate starting new actions. contrast,
RTDP interwoven-epochs generates optimal policy, takes much longer converge.
insight run RTDP interwoven space long enough generate policy
good common states, stop well converges every state. Then, ensure
rarely explored states proper policy, substitute aligned policy, returning hybridized
policy.
Algorithm 3 Hybridized Algorithm DURhyb (r, k, m)
1: -

2:
initialize J - (s) admissible heuristic
3: repeat
4:
perform RTDP trials
5:
compute hybridized policy (hyb ) using interwoven-epoch policy k-familiar states aligned-

epoch policy otherwise
clean hyb removing dead-ends cycles
J - hs0 , evaluation hyb start state


J - (hs0 ,i)J - (hs0 ,i)


8:
<r
J - (hs0 ,i)

9: return hybridized policy hyb

6:
7:

Thus key question decide states well explored not.
define familiarity state number times visited previous RTDP
trials. reachable state whose familiarity less constant, k, aligned policy created
it. Furthermore, dead-end state reached using greedy interwoven policy, create
aligned policy immediate precursors state. cycle detected7 , compute
aligned policy states part cycle.
yet said hybridized algorithm terminates. Use RTDP helps us defining
simple termination condition parameter varied achieve desired
closeness optimality well. intuition simple. Consider first, optimal labeled RTDP.
starts admissible heuristic guarantees value start state, J - (hs0 , i),
remains admissible (thus less equal optimal). contrast, hybridized policys makespan always longer equal optimal. Thus time progresses, values approach
optimal make-span opposite sides. Whenever two values within optimality ratio (r),
know algorithm found solution, close optimal.
7. implementation cycles detected using simulation.

58

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Finally, evaluation hybridized policy done using simulation, perform
fixed number RTDP trials. Algorithm 3 summarizes details algorithm. One see
combined policy proper two reasons: 1) policy state aligned
policy, proper RTDP aligned-epoch model run convergence,
2) rest states explicitly ensured cycles dead-ends.
5.4 Experiments: Planning Deterministic Durations
Continuing Section 3.6, set experiments evaluate various techniques
solving problems involving explicit deterministic durations. compare computation time
solution quality five methods: interwoven Sampled RTDP heuristic (DURsamp ),
AC
maximum concurrency (DURMC
samp ), average concurrency (DURsamp ) heuristics, hybridized
algorithm (DURhyb ) Sampled RTDP aligned-epoch model (DURAE ). test
Rover, MachineShop Aritificial domains. use Artificial domain see relative
performance techniques varies amount concurrency domain.
5.4.1 E XPERIMENTAL ETUP
modify domains used Section 3.6 additionally including action durations. NASA
Rover MachineShop domains, generate problems 17-26 state variables 12-18 actions, whose duration range 1 20. problems 15,000-700,000 reachable states interwoven-epoch state space, - .
use Artificial domain control experiments study effect degree parallelism.
problems domain 14 state variables 17,000-40,000 reachable states
durations actions 1 3.
use implementation Sampled RTDP8 implement heuristics: maximum concurrency (HM C ), average concurrency (HAC ), initialization value function. calculate
heuristics demand states visited, instead computing complete heuristic
whole state space once. implement hybridized algorithm initial value
function set HM C heuristic. parameters r, k, kept 0.05, 100 500,
respectively. test algorithms number problem instances three
domains, generate varying number objects, degrees parallelism, durations
actions distances goal.
5.4.2 C OMPARISON RUNNING IMES
Figures 11(a, b) 12(a) show variations running times algorithms different
problems Rover, Machineshop Artificial domains, respectively. first three bars represent
base Sampled RTDP without heuristic, HM C , HAC , respectively. fourth
bar represents hybridized algorithm (using HM C heuristic) fifth bar computation
aligned-epoch Sampled RTDP costs set maximum action duration. white
region fourth bar represents time taken aligned-epoch RTDP computations
hybridized algorithm. error bars represent 95% confidence intervals running times. Note
plots log scale.
8. Note policies returned DURsamp guaranteed optimal. Thus implemented algorithms
approximate. replace DURsamp pruned RTDP (DURprun ) optimality desired.

59

fiM AUSAM & W ELD

Rover16

Mach11

10^3

10^2

10^1

Mach12

Mach13

Mach14

Mach15

Mach16

0

AC
H
AE

Rover15

0

AC
H
AE

Rover14

Time sec (on log scale)

10^3

10^2

10^1

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

10^0

10^0

0

AC
H
AE

Time sec (on log scale)

Rover13

0

AC
H
AE

Rover12

Rover11

0

AC
H
AE

10^4

10^4

Figure 11: (a,b): Running times (on log scale) Rover Machineshop domain, respectively.
problem five bars represent times taken algorithms: DURsamp (0), DURMC
samp
(AE), DURAC
(AC),
DUR
(H),

DUR
(AE),
respectively.

white
bar

DUR
hyb
AE
hyb
samp
denotes portion time taken aligned-epoch RTDP.

Algos
DURMC
samp
DURAC
samp
DURhyb
DURAE

Speedup compared DURsamp
Rover
Machineshop Artificial Average
3.016764
1.545418
1.071645 1.877942
3.585993
2.173809
1.950643 2.570148
10.53418
2.154863
16.53159 9.74021
135.2841
16.42708
241.8623 131.1911

Table 2: ratio time taken - S-RTDP heuristics algorithm.
heuristics produce 2-3 times speedups. hybridized algo produces 10x speedup. Aligned
epoch search produces 100x speedup, sacrifices solution quality.

notice DURAE solves problems extremely quickly; natural since alignedepoch space much smaller. Use HM C HAC always speeds search - model.
Comparing heuristics amongst themselves, find average concurrency heuristic mostly
performs faster maximum concurrency presumably HAC informed heuristic practice, although cost inadmissible. find couple cases HAC
doesnt perform better; could focusing search incorrect region, given
inadmissible nature.
Rover domain, hybridized algorithm performs fastest. fact, speedups
dramatic compared methods. domains, results comparable small
problems. However, large problems two domains, hybridized outperforms others
huge margin. fact largest problem Artificial domain, none heuristics able
converge (within day) DURhyb DURAE converge solution.
60

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

10^4

Art12

Art13

Art14

Art15

0

AC
H
AE

Art11

0

AC
H
AE

1.6
Art11(68) Art12(77) Art13(81) Art14(107) Art15(224) Art16(383) Art17(1023)

Art16

Art17

Ratio make-span optimal

Time sec (on log scale)

1.5
10^3

10^2

10^1

1.4
1.3
1.2
1.1
1
0.9

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0

AC
H
AE

0.8

10^0

Figure 12: (a,b): Comparison different algorithms (running times solution quality respectively)
Artificial domain. degree parallelism increases problems become harder;
largest problem solved DURhyb DURAE .

Table 2 shows speedups obtained various algorithms compared basic DURsamp .
Rover Artificial domains speedups obtained DURhyb DURAE much
prominent Machineshop domain. Averaging domains, H produces 10x speedup
AE produces 100x speedup.
5.4.3 C OMPARISON OLUTION Q UALITY
Figures 13(a, b) 12(b) show quality policies obtained five methods
domains. measure quality simulating generated policy across multiple trials,
reporting average time taken reach goal. plot ratio so-measured expected
make-span optimal expected make-span9 . Table 3 presents solution qualities method,
averaged problems domain. note aligned-epoch policies usually yield
significantly longer make-spans (e.g., 25% longer); thus one must make quality sacrifice
speedy policy construction. contrast, hybridized algorithm extorts small sacrifice
quality exchange speed.
5.4.4 VARIATION C ONCURRENCY
Figure 12(a) represents attempt see relative performance algorithms changed
increasing concurrency. Along top figure, problem names, numbers brackets;
list average number applicable combinations MDP state, AvgsS - |Ap(s)|,

range 68 1023 concurrent actions. Note difficult problems lot parallelism, DURsamp slows dramatically, regardless heuristic. contrast, DURhyb still able
quickly produce policy, almost loss quality (Figure 12(b)).
9. large problems optimal algorithm converge. those, take optimal, best policy found
runs.

61

fiM AUSAM & W ELD

Rover16

1.7 Mach11
Ratio make-span optimal

1.4
1.3
1.2
1.1
1

Mach16

1.2
1.1
1

0

AC
H
AE

0

AC
H
AE

Mach15

1.3

0.8
0

AC
H
AE

Mach14

1.4

0.8
0

AC
H
AE

Mach13

1.5

0.9

0

AC
H
AE

Mach12

1.6

0.9

0

AC
H
AE

Ratio make-span optimal

1.5

0

AC
H
AE

Rover15

0

AC
H
AE

Rover14

0

AC
H
AE

Rover13

0

AC
H
AE

Rover12

0

AC
H
AE

Rover11

0

AC
H
AE

1.8

1.6

Figure 13: (a,b): Comparison make-spans solution found optimal(plotted 1 yaxes) Rover Machineshop domains, respectively. algorithms except DURAE produce
solutions quite close optimal.

Algos
DURsamp
DURMC
samp
DURAC
samp
DURhyb
DURAE

Rover
1.059625
1.018405
1.017141
1.059349
1.257205

Average Quality
Machineshop Artificial
1.065078
1.042561
1.062564
1.013465
1.046391
1.020523
1.075534
1.059201
1.244862
1.254407

Average
1.055704
1.031478
1.028019
1.064691
1.252158

Table 3: Overall solution quality produced algorithms. Note algorithms except DURAE produce policies whose quality quite close optimal. average DURAE produces make-spans
125% optimal.

6. Optimal Planning Uncertain Durations
extend techniques previous section case action durations deterministic. before, consider TGP-style actions discrete temporal model. assume
independent durations, monotonic continuations, Section 6.3 relaxes latter, extending
algorithms handle multimodal duration distributions. aim minimize
expected time required reach goal.
6.1 Formulating CoMDP
formulate planning problem CoMDP similar Section 5.1.
parameters CoMDP used directly work deterministic durations, need
recompute transition function.
62

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

State Space: aligned epoch state space well interwoven epoch space, defined
Section 5.1 adequate model planning problem. determine size interwoven
space, replace duration action max duration. Let (a) denote maximum
time within action complete. overall interwoven-epoch search space - =




{a} ZM (a) , ZM (a) represents set {0, 1, . . . , (a) 1}
denotes
Cartesian product multiple sets.
Action Space: state may apply combination actions applicability function
reflecting fact combination actions safe w.r.t (and w.r.t. already executing
actions case interwoven space) previous sections. previous state space
action space work well problem, transition function definition needs change, since
need take account uncertainty durations.
Transition Function: Uncertain durations require significant changes probability transition
function (Pr - ) interwoven space definitions Section 5.1.2. Since assumptions justify Conjecture 8, need consider happenings choosing decision epochs.
N

N

aA

Algorithm 4 ComputeTransitionFunc(s=hX, i,A)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

{(a, 0)}
mintime min(a,)Y minimum remaining time
maxtime min(a,)Y maximum remaining time
integer [mintime, maxtime]
set actions could possibly terminate
non-empty subsets Asubt
pc (prob. exactly Asubt terminates (see Equation 13).
W {(Xt , pw ) | Xt world state; pw probability Asubt terminates yielding Xt }.
(Xt , pw ) W
Yt {(a, + t) | (a, ) Y,
/ Asubt }
insert (hXt , Yt i, pw pc ) output
return output

computation transition function described Algorithm 4. Although next decision
epoch determined happening, still need consider pivots next state calculations
potential happenings. mintime minimum time executing action could
terminate, maxtime minimum time guaranteed least one action
terminate. times mintime maxtime compute possible combinations
could terminate resulting next interwoven state. probability, pc , (line 7) may
computed using following formula:
pc =



(prob. terminates + t|a hasn0 terminated till )

(a,a )Y,aAsubt



(prob. b doesn0 terminate b + t|b hasn0 terminated till b )

(13)

(b,b )Y,bAsub
/


Considering pivots makes algorithm computationally intensive may
many pivots many action combinations could end one, many outcomes each.
implementation, cache transition function recompute
information state.
63

fiM AUSAM & W ELD

Start Goal States: start state goal set developed deterministic durations
work unchanged durations stochastic. So, start state hs0 , goal set
G - = {hX, i|X G}.
Thus modeled problem CoMDP interwoven state space. redefined start goal states, probability transition function. use techniques
CoMDPs solve problem. particular, use Bellman equations below.
Bellman Equations Interwoven-Epoch Space: Define el (s, A, s0 ) time elapsed two interwoven states s0 combination executed s. set equations
solution problem written as:

J - (s) = 0, G - else

n

X
Pr - (s0 |s, A) el (s, A, s0 ) + J - (s0 )
J - (s) = min


AAp - (s) 0
-

(14)

Compare equations Equation 11. one difference besides new transition
function time elapsed within summation sign. time elapsed depends
next interwoven state.
modeled problem CoMDP use algorithms Section 5. use
DUR denote family algorithms CPTP problems involving stochastic durations.
main bottleneck solving problem, besides size interwoven state space,
high branching factor.
6.1.1 P OLICY C ONSTRUCTION : RTDP & H YBRIDIZED P LANNING
Since modeled problem CoMDP new interwoven space, may use pruned
RTDP (DURprun ) sampled RTDP (DURsamp ) policy construction. Since cost function problem (el ) depends current next state, combo-skipping
apply problem. Thus DURprun refers RTDP combo-elimination.
Furthermore, small adaptations necessary incrementally compute (admissible)
maximum concurrency (M C) (more informed, inadmissible) average concurrency (AC)
heuristics. example, serial MDP (in RHS Equation 12) need compute
average duration action use actions cost.
Likewise, speed planning hybridizing (DURhyb ) RTDP algorithms interwoven aligned-epoch CoMDPs produce near-optimal policy significantly less time.
dynamics aligned epoch space Section 5 one exception. cost
combination, case deterministic durations, simply max duration constituent
actions. novel twist stems fact uncertain durations require computation cost
action combination expected time last action combination terminate.
example, suppose two actions, uniform duration distributions [1,3], started
concurrently. probabilities actions finished times 1, 2 3 (and earlier) 1/9, 3/9, 5/9 respectively. Thus expected duration completion combination
(let us call AE ) 11/9 + 23/9 + 35/9 = 2.44.
64

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

6.2 Expected-Duration Planner
modeled CoMDP full-blown interwoven space, stochastic durations cause
exlposive growth branching factor. general, n actions started possible
durations r probabilistic effects, (m 1)[(r + 1)n rn 1] + rn
potential successors. number may computed follows: duration 1
1 subset actions could complete action could result r outcomes. Hence, total
P
number successors per duration i[1..n] n Ci ri = (r + 1)n rn 1. Moreover, none
actions finish time 1 last step actions terminate leading rn outcomes.
So, total number successors (m 1)[(r + 1)n rn 1] + rn . Thus, branching factor
multiplicative duration uncertainty exponential concurrency.
manage extravagant computation must curb branching factor. One method
ignore duration distributions. assign action constant duration equal mean
distribution, apply deterministic-duration planner DURsamp . However,
executing deterministic-duration policy setting durations actually stochastic,
action likely terminate time different mean, expected duration. DURexp
planner addresses problem augmenting deterministic-duration policy created account
unexpected outcomes.
6.2.1 NLINE V ERSION
procedure easiest understand online version (Algorithm 5): wait unexpected
happens, pause execution, re-plan. original estimate actions duration implausible,
compute revised deterministic estimate terms Ea (min) expected value
duration given terminated time min. Thus, Ea (0) compute expected
duration a.
Algorithm 5 Online DURexp
1: build deterministic-duration policy start state s0
2: repeat
3:
execute action combination specified policy
4:
wait interrupt
5:
case: action terminated expected {//do nothing}
6:
case: action terminates early
7:
extend policy current state
8:
case: action didnt terminate expected
9:
extend policy current state revising

duration follows:
10:
time elapsed since started executing
11:
nextexp dEa (0)e
12:
nextexp <
13:
nextexp dEa (nextexp)e
14:
endwhile
15:
revised duration nextexp
16:
endwait
17: goal reached

65

fiM AUSAM & W ELD

Example: Let duration action follow uniform distribution 1 15.
expected value gets assigned first run algorithm (dEa (0)e) 8. running
algorithm, suppose action didnt terminate 8 reach state running
for, say, 9 time units. case, revised expected duration would (dEa (8)e) = 12.
Similarly, doesnt terminate 12 either next expected duration would 14,
finally 15. words states executing times 0 8, expected
terminate 8. times 8 12 expected completion 12, 12 14 14
doesnt terminate 14 15. 2
6.2.2 FFLINE V ERSION
algorithm offline version re-planning contingencies done ahead
time fairness used version experiments. Although offline algorithm plans
possible action durations, still much faster algorithms. reason
planning problems solved significantly smaller (less branching factor, smaller
reachable state space), previous computation succinctly stored form
hinterwoven state, valuei pairs thus reused. Algorithm 6 describes offline planner
subsequent example illustrates savings.
Algorithm 6 Offline DURexp
1: build deterministic-duration policy start state s0 ; get current J - - values


2: insert s0 queue open
3: repeat
4:
state = open.pop()
5:
currstate s.t. Pr - (currstate|state, - (state)) > 0



currstate goal currstate set visited
visited.insert(currstate)
J - (currstate) converged
required, change expected durations actions currently executing
currstate.
10:
solve deterministic-duration planning problem start state currstate
11:
insert currstate queue open
12: open empty
6:
7:
8:
9:

Line 9 Algorithm 6 assigns new expected duration actions currently running
current state completd time previous termination point.
reassignment follows similar case online version (line 13).
Example: Consider domain two state-variables, x1 x2 , two actions set-x1
set-x2 . task set variables (initially false). Assume set-x2 always
succeeds whereas set-x1 succeeds 0.5 probability. Moreover, let actions
uniform duration distribution 1, 2, 3. case complete interwoven epoch search
could touch 36 interwoven states (each state variable could true false, action could
running, running 1 unit, running 2 units). Instead, build deterministic
duration policy actions deterministic duration 2, total number states
touched 16 interwoven states (each action could running
running 1 unit).
66

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Problem
A2

B2

G

C2



G

Optimal Solution (Trajectory 1, pr =0.5, makespan 9)
A2

B2

G

C2

Optimal Solution (Trajectory 2, pr =0.5, makespan 5)
A2
C2



G

DUR exp Solution (makespan 8)
A2

Time

0

B2
4

G
8

12

Figure 14: example domain DURexp algorithm compute optimal solution.
Now, suppose deterministic planner decides execute actions start state.
committed combination, easy see certain states never reached.
example, state h(x1 , x2 ), {(setx1 , 2)}i never visited, since set-x2 completes
guaranteed x2 set. fact, example, 3 new states initiate offline
replanning (line 10 Algo 6), viz., h(x1 , x2 ), {(setx2 , 2)}i, h(x1 , x2 ), {(setx2 , 2)}i,
h(x1 , x2 ), {(setx1 , 2)}i 2
6.2.3 P ROPERTIES
Unfortunately, DURexp algorithm guaranteed produce optimal policy. bad
policies generated expected-duration planner? experiments show DURexp
typically generates policies extremely close optimal. Even worst-case pathological
domain able construct leads expected make-span 50% longer
optimal (in limit). example illustrated below.
Example: consider domain actions A2:n , B2:n , C2:n D. Ai Bi
takes time 2i . Ci probabilistic duration: probability 0.5, Ci takes 1 unit time,
remaining probability, takes 2i+1 + 1 time. Thus, expected duration Ci
2i + 1. takes 4 units. sub-problem SPi , goal may reached executing Ai followed
Bi . Alternatively, goal may reached first executing Ci recursively solving
sub-problem SPi1 . domain, DURexp algorithm always compute hAi ; Bi
best solution. However, optimal policy starts {Ai , Ci }. Ci terminates 1,
policy executes solution SPi1 ; otherwise, waits Ai terminates executes Bi .
Figure 14 illustrates sub-problem SP2 optimal policy expected make-span
7 (vs. DURexp make-span 8). general, expected make-span optimal policy
3
SPn 13 [2n+2 + 24n ] + 22n + 2. Thus, limn exp
opt = 2 .2
6.3 Multi-Modal Duration Distributions
planners previous two sections benefited considering small set happenings
instead pivots, approach licensed Conjecture 8. Unfortunately, simplification
67

fiM AUSAM & W ELD

warranted case actions multi-modal duration distributions, common
complex domains factors cant modeled explicitly. example, amount
time Mars rover transmit data might bimodal distribution normally would
take little time, dust storm progress (unmodeled) could take much longer.
handle cases model durations mixture Gaussians parameterized triple
hamplitude, mean, variancei.
6.3.1 C MDP F ORMULATION
Although cannot restrict decision epochs happenings, need consider pivots;
required actions multi-modal distributions. fact, suffices consider pivots
regions distribution expected-time-to-completion increases. cases
need consider happenings.
Two changes required transition function Algorithm 4. line 3, maxtime
computation involves time next pivot increasing remaining time region
actions multi-modal distributions (thus forcing us take decision points, even
action terminates). Another change (in line 6) allows non-empty subset Asub =
maxtime. is, next state computed even without action termination. making
changes transition function reformulate problem CoMDP interwoven space
thus solve, using previous methods pruned/sampled RTDP, hybrid algorithm expectedduration algorithm.
6.3.2 RCHETYPAL -D URATION P LANNER
develop multi-modal variation expected-duration planner, called DURarch . Instead assigning action single deterministic duration equal expected value, planner
assigns probabilistic duration various outcomes means different modes
distribution probabilities probability mass mode. enhancement
reflects intuitive understanding multi-modal distributions experiments confirm
DURarch produces solutions shorter make-spans DURexp .
6.4 Experiments: Planning Stochastic Durations
evaluate techniques solving planning problems involving stochastic durations.
compare computation time solution quality (make-span) five planners domains
without multi-modal duration distributions. re-evaluate effectiveness
maximum- (MC) average-concurrency (AC) heuristics domains.
6.4.1 E XPERIMENTAL ETUP
modify Rover, MachineShop, Artificial domains additionally including uncertainty
action durations. set experiments, largest problem 4 million world states
65536 reachable. algorithms explored 1,000,000 distinct states
interwoven state space planning. domains contained many 18 actions,
actions many 13 possible durations. details domains please refer
longer version (Mausam, 2007).
68

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Planning Time (in sec)

6000
5000
4000

Rover

Machine-Shop

3000
Pruned
DURprun
DURsamp
Sampled
DURhyb
Hybrid

2000
1000

DURexp
Exp-Dur

0
21

22

23

24

25

26

27

28

29

30 Problems

Figure 15: Planning time comparisons Rover MachineShop domains: Variation along algorithms
initialized average concurrency (AC) heuristic; DURexp performs best.

Algos
DURsamp
DURhyb
DURexp

Average Quality Make-Span
Rover MachineShop Artificial
1.001
1.000
1.001
1.022
1.011
1.019
1.008
1.015
1.046

Table 4: three planners produce near-optimal policies shown table ratios
optimal make-span.11

6.4.2 C OMPARING RUNNING IMES
compare algorithms without heuristics reaffirm heuristics significantly
speed computation problems; indeed, problems large solved without
heuristics. Comparing amongst find AC beats C regardless
planning algorithm; isnt surprising since AC sacrifices admissibility.
Figure 15 reports running times various algorithms (initialized AC heuristic)
Rover Machine-Shop domains durations unimodal. DURexp out-performs
planners substantial margins. algorithm solving comparatively simpler
problem, fewer states expanded thus approximation scales better others solving,
example, two Machine-Shop problems, large planners.
cases hybridization speeds planning significant amounts, performs better DURexp
artificial domain.
6.4.3 C OMPARING OLUTION Q UALITY
measure quality simulating generated policy across multiple trials. report ratio
average expected make-span optimal expected make-span domains unimodal
distributions Table 4. find make-spans inadmissible heuristic AC par
11. optimal algorithm doesnt converge, use best solution found across runs optimal.

69

fiM AUSAM & W ELD

28
26
24

1000
DURprun
Pruned
DURsamp
Sampled

100

J*(s0)

Planning time (log scale)

10000

DURhyb
Hybrid
DURarch
Arch-Dur
DURexp
Exp-Dur

10

22
DURprun
DUR-prun
DURsamp
DUR-samp

20
18

DURhyb
DUR-hyb
DURarch
DUR-arch

16

DURexp
DUR-exp

14

31

32

33

34

35

36 Problems

31

32

33

34

35

36 Problems

Figure 16: Comparisons Machine-Shop domain multi-modal distributions. (a) Computation
Time comparisons: DURexp DURarch perform much better algos. (b) Makespans returned different algos: Solutions returned DURsamp almost optimal. Overall
DURarch finds good balance running time solution quality.

admissible heuristic C. hybridized planner approximate userdefined bound. experiments, set bound 5% find make-spans returned
algorithm quite close optimal always differ 5%. DURexp
quality guarantees, still solutions returned problems tested upon nearly good
algorithms. Thus, believe approximation quite useful scaling larger
problems without losing solution quality.
6.4.4 ULTIMODAL OMAINS
develop multi-modal variants domains; e.g., Machine-Shop domain, time fetching paint bimodal (if stock, paint fetched fast, else needs ordered).
alternative costly paint action doesnt require fetching paint. Solutions produced
DURsamp made use pivots decision epochs starting costly paint action case
fetch action didnt terminate within first mode bimodal distribution (i.e. paint
stock).
running time comparisons shown Figure 16(a) log-scale. find DURexp
terminates extremely quickly DURarch far behind. However, make-span comparisons Figure 16(b) clearly illustrate approximations made methods order achieve
planning time. DURarch exhibits good balance planning time solution quality.

7. Related Work
paper extends prior work, originally reported several conference publications (Mausam
& Weld, 2004, 2005, 2006a, 2006b).
Temporal planners may classified using constraint-posting extended state-space methods (discussed earlier Section 4). constraint approach promising, (if any) probabilistic planners implemented using architecture; one exception Buridan (Kush70

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

stochastic

deterministic

concurrent
durative
non-durative
DUR, Tempastic,
Concurrent MDP,
GSMDP, Prottle,
Factorial MDP,
FPG, Aberdeen et al.
Paragraph
Temporal Planning
Step-optimal planning
(TP4, Sapa, MIPS
(GraphPlan, SATPlan)
TLPlan, etc.)

non-concurrent
durative
non-durative
Time Dependent MDP,
MDP
IxTeT, CIRCA,
(RTDP, LAO*, etc.)
Foss & Onder
Planning
Classical Planning
Numerical Resources
(HSP, FF, etc.)
(Sapa, Metric-FF, CPT)

Figure 17: table listing various planners implement different subsets concurrent, stochastic, durative actions.

merick, Hanks, & Weld, 1995), performed poorly. contrast, MDP community
proven state-space approach successful. Since powerful deterministic temporal planners,
various planning competitions, use state-space approach, adopt
algorithms combine temporal planning MDPs. may interesting incorporate
constraint-based approaches probabilistic paradigm compare techniques
paper.
7.1 Comparison Semi-MDPs
Semi-Markov Decision Process extension MDPs allows durative actions take variable time. discrete time semi-MDP solved solving set equations direct
extension Equations 2. techniques solving discrete time semi-MDPs natural generalizations MDPs. main distinction semi-MDP formulation
concurrent probabilistic temporal planning stochastic durations concerns presence concurrently executing actions model. semi-MDP allow concurrent actions
assumes one executing action time. allowing concurrency actions intermediate decision epochs, algorithms need deal large state action spaces, encountered
semi-MDPs.
Furthermore, Younes Simmons shown general case, semi-MDPs incapable modeling concurrency. problem concurrent actions stochastic continuous
durations needs another model known Generalized Semi-Markov Decision Process (GSMDP)
precise mathematical formulation (Younes & Simmons, 2004b).
7.2 Concurrency Stochastic, Durative Actions
Tempastic (Younes & Simmons, 2004a) uses rich formalism (e.g. continuous time, exogenous
events, expressive goal language) generate concurrent plans stochastic durative actions. Tempastic uses completely non-probabilistic planner generate plan treated
candidate policy repaired failure points identified. method guarantee
completeness proximity optimal. Moreover, attention paid towards heuristics
search control making implementation impractical.
GSMDPs (Younes & Simmons, 2004b) extend continuous-time MDPs semi-Markov MDPs,
modeling asynchronous events processes. Younes Simmonss approaches handle
71

fiM AUSAM & W ELD

strictly expressive model due modeling continuous time. solve
GSMDPs approximation standard MDP using phase-type distributions. approach
elegant, scalability realistic problems yet demonstrated. particular, approximate, discrete MDP model require many states yet still behave differently
continuous original.
Prottle (Little et al., 2005) solves problems action language expressive
ours: effects occur middle action execution dependent durations supported.
Prottle uses RTDP-type search guided heuristics computed probabilistic planning
graph; however, plans finite horizon thus acyclic state space. difficult
compare Prottle approach Prottle optimizes different objective function (probability reaching goal), outputs finite-length conditional plan opposed cyclic plan
policy, guaranteed reach goal.
FPG (Aberdeen & Buffet, 2007) learns separate neural network action individually
based current state. execution phase decision, i.e., whether action needs
executed not, taken independently decisions regarding actions. way FPG able
effectively sidestep blowup caused exponential combinations actions. practice
able quickly compute high quality solutions.
Rohanimanesh Mahadevan (2001) investigate concurrency hierarchical reinforcement
learning framework, abstract actions represented Markov options. propose
algorithm based value-iteration, focus calculating joint termination conditions rewards received, rather speeding policy construction. Hence, consider possible Markov
option combinations backup.
Aberdeen et al. (2004) plan concurrent, durative actions deterministic durations
specific military operations domain. apply various domain-dependent heuristics speed
search extended state space.
7.3 Concurrency Stochastic, Non-durative Actions
Meuleau et al. Singh & Cohn deal special type MDP (called factorial MDP)
represented set smaller weakly coupled MDPs separate MDPs completely
independent except common resource constraints, reward cost models
purely additive (Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean, & Boutilier, 1998; Singh
& Cohn, 1998). describe solutions sub-MDPs independently solved
sub-policies merged create global policy. Thus, concurrency actions different
sub-MDPs by-product work. Singh & Cohn present optimal algorithm (similar
combo-elimination used DURprun ), whereas domain specific heuristics Meuleau et al.
guarantees. work Factorial MDPs assumes weak coupling exists
identified, factoring MDP hard problem itself.
Paragraph (Little & Thiebaux, 2006) formulates planning concurrency regression
search probabilistic planning graph. uses techniques nogood learning mutex
reasoning speed policy construction.
Guestrin et al. solve multi-agent MDP problem using linear programming (LP) formulation expressing value function linear combination basis functions. assuming
basis functions depend agents, able reduce size LP
(Guestrin, Koller, & Parr, 2001).
72

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

7.4 Stochastic, Non-concurrent, Durative Actions
Many researchers studied planning stochastic, durative actions absence concurrency.
example, Foss Onder (2005) use simple temporal networks generate plans
objective function time component. Simple Temporal Networks allow effective temporal
constraint reasoning methods generate temporally contingent plans.
Boyan Littman (2000) propose Time-dependent MDPs model problems actions
concurrent time-dependent, stochastic durations; solution generates piecewise linear value functions.
NASA researchers developed techniques generating non-concurrent plans uncertain continuous durations using greedy algorithm incrementally adds branches straightline plan (Bresina et al., 2002; Dearden, Meuleau, Ramakrishnan, Smith, & Washington, 2003).
handle continuous variables uncertain continuous effects, solution heuristic
quality policies unknown. Also, since consider limited contingencies,
solutions guaranteed reach goal.
IxTeT temporal planner uses constraint based reasoning within partial order planning
(Laborie & Ghallab, 1995). embeds temporal properties actions constraints
optimize make-span. CIRCA example system plans uncertain durations
action associated unweighted set durations (Musliner, Murphy, & Shin, 1991).
7.5 Deterministic, Concurrent, Durative Actions
Planning deterministic actions comparitively simpler problem much work
planning uncertainty based previous, deterministic planning research. instance,
interwoven state representation transition function extensions extended state representations TP4, SAPA, TLPlan (Haslum & Geffner, 2001; & Kambhampati, 2003;
Bacchus & Ady, 2001).
planners, MIPS AltAltp , investigated fast generation parallel plans
deterministic settings (Edelkamp, 2003; Nigenda & Kambhampati, 2003) Jensen Veloso
(2000) extend problems disjunctive uncertainty.

8. Future Work
presented comprehensive set techniques handle probabilistic outcomes, concurrent
durative actions single formalism, direct attention towards different relaxations
extensions proposed model. particular, explore objective functions, infinite
horizon problems, continuous-valued duration distributions, temporally expressive action models,
degrees goal satisfaction interruptibility actions.
8.1 Extension Cost Functions
planning problems durative actions (sections 4 beyond) focused make-span
minimization problems. However, techniques quite general applicable (directly
minor variations) variety cost metrics. illustration, consider mixed cost
optimization problem addition duration action, given
amount resource consumed per action, wish minimize sum make-span
total resource usage. Assuming resource consumption unaffected concurrent
73

fiM AUSAM & W ELD

execution, easily compute new max-concurrency heuristic. mixed-cost counterpart
Equations 12 is:
Jt (X)
+ Jr (X)
=
c
Qt (X, )
+ Qr (X, ) 6=
c

J - (s)

J - (s)


(15)

Here, Jt single-action MDP assignng costs durations Jr single
action MDP assigning costs resource consumptions. informed average concurrency
heuristic similarly computed replacing maximum concurrency average concurrency.
hybridized algorithm follows fashion, fast algorithm CoMDP
solved using techniques Section 3.
lines, objective function minimize make-span given certain maximum
resource usage, total amount resource remaining included state-space
CoMDPs underlying single-action MDPs etc. techniques may used.
8.2 Infinite Horizon Problems
paper defined techniques case indefinite horizon problems,
absorbing state defined reachable. problems alternative formulation
preferred allows infinite execution discounts future costs multiplying
discount factor step. Again, techniques suitably extended scenario.
example, Theorem 2 gets modified following:
Qk (s, A)

1k

Qk (s, {a1 }) + Ck (A)

k
X

!



ik

Ck ({ai })

i=1

Recall theorem provides us pruning rule, combo-skipping. Thus, use
Pruned RTDP new pruning rule.
8.3 Extensions Continuous Duration Distributions
confined actions discrete durations (refer Assumption 3).
investigate effects dealing directly continuous uncertainty duration distributions. Let fiT (t)dt probability action ai completing times + + + dt,
conditioned action ai finishing time . Similarly, define FiT (t) probability
action finishing time + .
Let us consider extended state hX, {(a1 , )}i, denotes action a1 started units
ago world state X. Let a2 applicable action started extended state. Define
= min(M (a1 )T, (a2 )), denotes maximum possible duration execution
action. Intuitively, time least one action complete.

Q - n+1 (hX, {(a1 , )}i, a2 ) =

Z
0

Z
0

h



f1T (t)F20 (t) + J - n (hX1 , {a2 , t}i) dt +
h



F1T (t)f20 (t) + J - n (hX2 , {a1 , + }i) dt
74

(16)

fi0

2

4

Time

6

8

10

10

10

Expected time reach goal

Expected Remaining Time action a0

Duration Distribution a0

P LANNING URATIVE ACTIONS TOCHASTIC OMAINS

8
6
4
2

0

2

4

Time

6

8

10

8
6
4
2

0

2

4

6

8

10

Time

Figure 18: durations continuous (real-valued) rather discrete, may infinite number
potentially important decision epochs. domain, crucial decision epoch could required
time (0, 1] depending length possible alternate plans.

X1 X2 world states obtained applying deterministic actions a1 a2
respectively X. Recall J - n+1 (s) = mina Q - n+1 (s, a). fixed point computation

form, desire Jn+1 Jn functional form12 . Going equation
seems difficult achieve, except perhaps specific action distributions
special planning problems. example, distributions constant
concurrency domain, equations easily solvable. interesting cases,
solving equations challenging open question.
Furthermore, dealing continuous multi-modal distributions worsens decision epochs
explosion. illustrate help example.
Example: Consider domain Figure 7 except let action a0 bimodal distribution,
two modes uniform 0-1 9-10 respectively shown Figure 18(a).
let a1 small duration. Figure 18(b) shows expected remaining termination times
a0 terminates time 10. Notice due bimodality, expected remaining execution time
increases 0 1. expected time reach goal using plan h{a0 , a1 }; a2 shown
third graph. suppose, started {a0 , a1 }, need choose next decision
epoch. easy see optimal decision epoch could point 0 1
would depend alternative routes goal. example, duration b0 7.75,
optimal time-point start alternative route 0.5 (right expected time reach goal
using first plan exceeds 7.75).
Thus, choice decision epochs depends expected durations alternative routes.
values known advance, fact ones calculated planning
phase. Therefore, choosing decision epochs ahead time seem possible. makes
optimal continuous multi-modal distribution planning problem mostly intractable reasonable
sized problem.
8.4 Generalizing TGP Action Model
assumption TGP style actions enables us compute optimal policies, since prune
number decision epochs. case complex action models PDDL2.1 (Fox & Long, 2003),
old, deterministic state-space planners incomplete. reasons, algorithms
12. idea exploited order plan continuous resources (Feng, Dearden, Meuleau, & Washington,
2004).

75

fiM AUSAM & W ELD

incomplete problems PPDDL2.1 . Recently, Cushing et al. introduced Tempo, statespace planner, uses lifting time achieve completeness (Cushing, Kambhampati,
Mausam, & Weld, 2007). pursuit finding complete, state-space, probabilistic planner
complex action models, natural step consider Tempo-like representation probabilistic
setting. working details seems relatively straightforward, important research
challenge find right heuristics streamline search algorithm scale.
8.5 Extensions
several extensions basic framework suggested. different
construct introduces additional structure need exploit knowledge order design
fast algorithms. Many times, basic algorithms proposed paper may easily adapted
situations, sometimes may not. list two important extensions below.
Notion Goal Satisfaction: Different problems may require slightly different notions
goal reached. example, assumed thus far goal officially
achieved executed actions terminated. Alternatively, one might consider goal
achieved satisfactory world state reached, even though actions may
midst execution. intermediate possibilities goal requires specific
actions necessarily end. changing definition goal set, problems
modeled CoMDP. hybridized algorithm heuristics easily adapted
case.
Interruptible Actions: assumed that, started, action cannot terminated.
However, richer model may allow preemptions, well continuation interrupted
action. problems, actions could interrupted will, significantly
different flavor. Interrupting action new kind decision requires full study
might action termination useful. large extent, planning similar
finding different concurrent paths goal starting together, since one
always interrupt executing paths soon goal reached. instance, example
Figure 7 longer holds since b0 started time 1, later terminated needed
shorten make-span.
8.6 Effect Large Durations
weakness extended-state space approaches, deterministic well probabilistic
settings, dependence absolute durations (or accurate, greatest common
divisor action durations). instance, domain action large duration,
say 100 another concurrently executable action duration 1, world states
explored tuples (a, 1), (a, 2), . . ., (a, 98), (a, 99). general, many states
behave similarly certain decision boundaries important. Start b
executing 50 units c otherwise one example decision boundary.
Instead representing flat discrete states individually, planning aggregate space
state represents several extended states help alleviate inefficiency.
However, obvious achieve aggregation automatically, since adapting
well-known methods aggregation hold case. instance, SPUDD (Hoey
76

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

et al., 1999) uses algebraic decision diagrams represent abstract states Jvalue. Aggregating valued states may enough us, since expected time
completion depends linearly amount time left longest executing action. So,
states differ amount time action executing able
aggregate together. similar way, Feng et al. (2004) use piecewise constant piecewise
linear representations adaptively discretize continuous variables. case, |A|
variables. executing active given time, modeling
sparse high-dimensional value function easy either. able exploit structure due
action durations essential future direction order scale algorithms complex real
world domains.

9. Conclusions
Although concurrent durative actions stochastic effects characterize many real-world domains, planners handle challenges concert. paper proposes unified statespace based framework model solve problems. State space formulations popular
deterministic temporal planning well probabilistic planning. However,
features bring additional complexities formulation afford new solution techniques.
develop DUR family algorithms alleviates complexities. evaluate techniques running times qualities solutions produced. Moreover, study theoretical
properties domains identify key conditions fast, optimal algorithms
possible. make following contributions:
1. define Concurrent MDPs (CoMDP) extension MDP model formulate
stochastic planning problem concurrent actions. CoMDP cast back new
MDP extended action space. action space possibly exponential
number actions, solving new MDP naively may take huge performance hit.
develop general notions pruning sampling speed algorithms. Pruning
refers pruning provably sub-optimal action-combinations state, thus performing less computation still guaranteeing optimal solutions. Sampling-based solutions
rely intelligent sampling action-combinations avoid dealing exponential
number. method converges orders magnitude faster methods produces
near-optimal solutions.
2. formulate planning concurrent, durative actions CoMDP two modified
state spaces aligned epoch, interwoven epoch. aligned epoch based solutions
run fast, interwoven epoch algorithms yield much higher quality solutions. define two heuristic functions maximum concurrency (MC), average concurrency (AC)
guide search. MC admissible heuristic, whereas AC, inadmissible, typically more-informed leading better computational gains. call algorithms DUR
family algorithms. subscripts samp prun refer sampling pruning respectively,
optional superscripts AC MC refer heuristic employed, optional ""
DUR notifies problem stochastic durations. example, Labeled RTDP
deterministic duration problem employing sampling started AC heuristic
abbreviated DURAC
samp .
77

fiM AUSAM & W ELD

3. develop general technique hybridizing two planners. Hybridizing interwovenepoch aligned-epoch CoMDPs yields much efficient algorithm, DURhyb .
algorithm parameter, varied trade-off speed optimality.
experiments, DURhyb quickly produces near-optimal solutions. larger problems,
speedups algorithms quite significant. hybridized algorithm
used anytime fashion thus producing good-quality proper policies (policies
guaranteed reach goal) within desired time. Moreover, idea hybridizing two
planners general notion; recently applied solving general stochastic planning
problems (Mausam, Bertoli, & Weld, 2007).
4. Uncertainty durations leads complexities addition state action
spaces, blowup branching factor number decision epochs.
bound space decision epochs terms pivots (times actions may potentially terminate) conjecture restrictions, thus making problem tractable.
propose two algorithms, expected duration planner (DURexp ) archetypal
duration planner (DURarch ), successively solve small planning problems
limited duration uncertainty, respectively. DURarch able make use
additional structure offered multi-modal duration distributions. algorithms perform
much faster techniques. Moreover, DURarch offers good balance
planning time vs. solution quality tradeoff.
5. Besides focus stochastic actions, expose important theoretical issues related
durative actions repercussions deterministic temporal planners well.
particular, prove common state-space temporal planners incomplete face
expressive action models, e.g., PDDL2.1 , result may strong impact
future temporal planning research (Cushing et al., 2007).
Overall, paper proposes large set techniques useful modeling solving
planning problems employing stochastic effects, concurrent executions durative actions
duration uncertainties. algorithms range fast suboptimal solutions, relatively slow
optimal. Various algorithms explore different intermediate points spectrum
presented. hope techniques useful scaling planning techniques real
world problems future.

Acknowledgments
thank Blai Bonet providing source code GPT well comments course
work. thankful Sumit Sanghai theorem proving skills advice various
stages research. grateful Derek Long anonymous reviewers paper
gave several thoughtful suggestions generalizing theory improving clarity
text. thank Subbarao Kambhampati, Daniel Lowd, Parag, David Smith others
provided useful comments drafts parts research. work performed University Washington 2003 2007 supported generous grants National
Aeronautics Space Administration (Award NAG 2-1538), National Science Foundation (Award
IIS-0307906), Office Naval Research (Awards N00014-02-1-0932, N00014-06-1-0147)
WRF / TJ Cable Professorship.
78

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

References
Aberdeen, D., Thiebaux, S., & Zhang, L. (2004). Decision-theoretic military operations planning.
ICAPS04.
Aberdeen, D., & Buffet, O. (2007).
gradients. ICAPS07.

Concurrent probabilistic temporal planning policy-

Bacchus, F., & Ady, M. (2001). Planning resources concurrency: forward chaining
approach. IJCAI01, pp. 417424.
Barto, A., Bradtke, S., & Singh, S. (1995). Learning act using real-time dynamic programming.
Artificial Intelligence, 72, 81138.
Bertsekas, D. (1995). Dynamic Programming Optimal Control. Athena Scientific.
Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial Intelligence,
90(12), 281300.
Bonet, B., & Geffner, H. (2003). Labeled RTDP: Improving convergence real-time dynamic
programming. ICAPS03, pp. 1221.
Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search. JAIR,
24, 933.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions
computational leverage. J. Artificial Intelligence Research, 11, 194.
Boyan, J. A., & Littman, M. L. (2000). Exact solutions time-dependent MDPs. NIPS00, p.
1026.
Bresina, J., Dearden, R., Meuleau, N., Smith, D., & Washington, R. (2002). Planning continuous time resource uncertainty : challenge AI. UAI02.
Chen, Y., Wah, B. W., & Hsu, C. (2006). Temporal planning using subgoal partitioning resolution sgplan. JAIR, 26, 323.
Cushing, W., Kambhampati, S., Mausam, & Weld, D. S. (2007). temporal planning really
temporal?. IJCAI07.
Dearden, R., Meuleau, N., Ramakrishnan, S., Smith, D. E., & Washington, R. (2003). Incremental
Contingency Planning. ICAPS03 Workshop Planning Uncertainty Incomplete Information.
Do, M. B., & Kambhampati, S. (2001). Sapa: domain-independent heuristic metric temporal
planner. ECP01.
Do, M. B., & Kambhampati, S. (2003). Sapa: scalable multi-objective metric temporal planner.
JAIR, 20, 155194.
Edelkamp, S. (2003). Taming numbers duration model checking integrated planning
system. Journal Artificial Intelligence Research, 20, 195238.
79

fiM AUSAM & W ELD

Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming structured continuous Markov decision processes. UAI04, p. 154.
Foss, J., & Onder, N. (2005). Generating temporally contingent plans. IJCAI05 Workshop
Planning Learning Apriori Unknown Dynamic Domains.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains.. JAIR Special Issue 3rd International Planning Competition, 20, 61124.
Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphs
action graphs. AIPS02, p. 281.
Guestrin, C., Koller, D., & Parr, R. (2001). Max-norm projections factored MDPs. IJCAI01,
pp. 673682.
Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. ECP01.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision
diagrams. UAI99, pp. 279288.
Jensen, R. M., & Veloso, M. (2000). OBDD=based universal planning synchronized agents
non-deterministic domains. Journal Artificial Intelligence Research, 13, 189.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning. Artificial
Intelligence, 76(1-2), 239286.
Laborie, P., & Ghallab, M. (1995). Planning sharable resource constraints. IJCAI95, p.
1643.
Little, I., Aberdeen, D., & Thiebaux, S. (2005). Prottle: probabilistic temporal planner.
AAAI05.
Little, I., & Thiebaux, S. (2006). Concurrent probabilistic planning graphplan framework.
ICAPS06.
Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.
JAIR, 20, 159.
Mausam (2007). Stochastic planning concurrent, durative actions. Ph.d. dissertation, University Washington.
Mausam, Bertoli, P., & Weld, D. (2007). hybridized planner stochastic domains. IJCAI07.
Mausam, & Weld, D. (2004). Solving concurrent Markov decision processes. AAAI04.
Mausam, & Weld, D. (2005). Concurrent probabilistic temporal planning. ICAPS05, pp. 120
129.
80

fiP LANNING URATIVE ACTIONS TOCHASTIC OMAINS

Mausam, & Weld, D. (2006a). Challenges temporal planning uncertain durations.
ICAPS06.
Mausam, & Weld, D. (2006b). Probabilistic temporal planning uncertain durations.
AAAI06.
Meuleau, N., Hauskrecht, M., Kim, K.-E., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving large weakly coupled Markov Decision Processes. AAAI98, pp.
165172.
Musliner, D., Murphy, D., & Shin, K. (1991). World modeling dynamic construction
real-time control plans. Artificial Intelligence, 74, 83127.
Nigenda, R. S., & Kambhampati, S. (2003). Altalt-p: Online parallelization plans heuristic
state search. Journal Artificial Intelligence Research, 19, 631657.
Penberthy, J., & Weld, D. (1994). Temporal planning continuous change. AAAI94, p. 1010.
Rohanimanesh, K., & Mahadevan, S. (2001). Decision-Theoretic planning concurrent temporally extended actions. UAI01, pp. 472479.
Singh, S., & Cohn, D. (1998). dynamically merge markov decision processes. NIPS98.
MIT Press.
Smith, D., & Weld, D. (1999). Temporal graphplan mutual exclusion reasoning. IJCAI99,
pp. 326333 Stockholm, Sweden. San Francisco, CA: Morgan Kaufmann.
Vidal, V., & Geffner, H. (2006). Branching pruning: optimal temporal pocl planner based
constraint programming. AIJ, 170(3), 298335.
Younes, H. L. S., & Simmons, R. G. (2004a). Policy generation continuous-time stochastic
domains concurrency. ICAPS04, p. 325.
Younes, H. L. S., & Simmons, R. G. (2004b). Solving generalized semi-markov decision processes
using continuous phase-type distributions. AAAI04, p. 742.
Zhang, W., & Dietterich, T. G. (1995). reinforcement learning approach job-shop scheduling.
IJCAI95, pp. 11141120.

Appendix
Proof Theorem 6
prove statement Theorem 6, i.e., actions TGP-style set pivots
suffices optimal planning. proof make use fact actions TGP-style
consistent execution concurrent plan requires two executing actions non-mutex
(refer Section 5 explanation that). particular, none effects conflict
precondition one conflict effects another.
prove theorem contradition. Let us assume problem optimal solution
requires least one action start non-pivot. Let us consider one optimal plans,
81

fiM AUSAM & W ELD

first non-pivot point action needs start non-pivot minimized. Let
us name time point let action starts point a. prove case
analysis may, well, start time 1 without changing nature plan. 1
non-pivot contradict hypothesis minimum first non-pivot point.
1 pivot hypothesis contradicted "need to" start non-pivot.
prove left-shifted 1 unit, take one trajectory time (recall
actions could several durations) consider actions playing role 1, t, + (a) 1,
+ (a), (a) refers duration trajectory. Considering points
suffice, since system state change points trajectory. prove
execution none actions affected left shift. following twelve
cases:
1. actions b start 1: b cant end (t non-pivot). Thus b execute
concurrently t, implies b non-mutex. Thus b may well start together.
2. actions b continue execution 1: Use argument similar case 1 above.
3. actions b end 1: b TGP-style, effects realized open interval
ending 1. Therefore, start conflict end b.
4. actions b start t: b start together hence dependent
preconditions. Also, non-mutex, starting times shifted
direction.
5. actions b continue execution t: b started 1 refer case 1 above. not,
1 similar points b.
6. actions b end t: Case possible due assumption non-pivot.
7. actions b start + (a) 1: Since continued execution point, b
non-mutex. Thus effects clobber bs preconditions. Hence, b still executed
realizing effects.
8. actions b continue execution + (a) 1: b non-mutex, may end
earlier without effect b.
9. actions b end + (a) 1: b executing concurrently. Thus
non-mutex. may end together.
10. actions b start + (a): b may still start + (a), since state + (a)
doesnt change.
11. actions b continue execution + (a): b started + (a) 1 refer case
7 above, else state change + (a) cause effect b.
12. actions b end + (a): b non-mutex executing concurrently. Thus, effects dont clobber bs preconditions. Hence, may end earlier.
Since left shifted trajectories, therefore left-shift legal. Also,
multiple actions start may shifted one one using argument.
Hence Proved. 2
82


