Journal Artificial Intelligence Research 19 (2003) 399-468

Submitted 1/02; published 10/03

Efficient Solution Algorithms Factored MDPs
Carlos Guestrin

guestrin@cs.stanford.edu

Computer Science Dept., Stanford University

Daphne Koller

koller@cs.stanford.edu

Computer Science Dept., Stanford University

Ronald Parr

parr@cs.duke.edu

Computer Science Dept., Duke University

Shobha Venkataraman

shobha@cs.cmu.edu

Computer Science Dept., Carnegie Mellon University

Abstract
paper addresses problem planning uncertainty large Markov Decision
Processes (MDPs). Factored MDPs represent complex state space using state variables
transition model using dynamic Bayesian network. representation often allows
exponential reduction representation size structured MDPs, complexity exact
solution algorithms MDPs grow exponentially representation size. paper,
present two approximate solution algorithms exploit structure factored MDPs.
use approximate value function represented linear combination basis functions,
basis function involves small subset domain variables. key contribution
paper shows basic operations algorithms performed efficiently
closed form, exploiting additive context-specific structure factored MDP.
central element algorithms novel linear program decomposition technique, analogous
variable elimination Bayesian networks, reduces exponentially large LP provably
equivalent, polynomial-sized one. One algorithm uses approximate linear programming,
second approximate dynamic programming. dynamic programming algorithm novel
uses approximation based max-norm, technique directly minimizes terms
appear error bounds approximate MDP algorithms. provide experimental results
problems 1040 states, demonstrating promising indication scalability
approach, compare algorithm existing state-of-the-art approach, showing,
problems, exponential gains computation time.

1. Introduction
last years, Markov Decision Processes (MDPs) used basic
semantics optimal planning decision theoretic agents stochastic environments.
MDP framework, system modeled via set states evolve stochastically.
main problem representation that, virtually real-life domain,
state space quite large. However, many large MDPs significant internal structure,
modeled compactly structure exploited representation.
Factored MDPs (Boutilier, Dearden, & Goldszmidt, 2000) one approach representing large, structured MDPs compactly. framework, state implicitly described
assignment set state variables. dynamic Bayesian network (DBN) (Dean
& Kanazawa, 1989) allow compact representation transition model,
exploiting fact transition variable often depends small number
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiGuestrin, Koller, Parr & Venkataraman

variables. Furthermore, momentary rewards often decomposed
sum rewards related individual variables small clusters variables.
two main types structure simultaneously exploited factored
MDPs: additive context-specific structure. Additive structure captures fact
typical large-scale systems often decomposed combination locally interacting components. example, consider management large factory many
production cells. course, long run, cell positioned early production line
generates faulty parts, whole factory may affected. However, quality
parts cell generates depends directly state cell quality
parts receives neighboring cells. additive structure present
reward function. example, cost running factory depends, among things,
sum costs maintaining local cell.
Context-specific structure encodes different type locality influence: Although
part large system may, general, influenced state every part
system, given point time small number parts may influence directly.
factory example, cell responsible anodization may receive parts directly
cell factory. However, work order cylindrical part may restrict
dependency cells lathe. Thus, context producing cylindrical
parts, quality anodized parts depends directly state cells
lathe.
Even large MDP represented compactly, example, using factored
representation, solving exactly may still intractable: Typical exact MDP solution algorithms require manipulation value function, whose representation linear
number states, exponential number state variables. One approach
approximate solution using approximate value function compact representation. common choice use linear value functions approximation value
functions linear combination potentially non-linear basis functions (Bellman,
Kalaba, & Kotkin, 1963; Sutton, 1988; Tsitsiklis & Van Roy, 1996b). work builds
ideas Koller Parr (1999, 2000), using factored (linear) value functions,
basis function restricted small subset domain variables.
paper presents two new algorithms computing linear value function approximations factored MDPs: one uses approximate dynamic programming another
uses approximate linear programming. algorithms based use factored linear value functions, highly expressive function approximation method.
representation allows algorithms take advantage additive context-specific
structure, order produce high-quality approximate solutions efficiently. capability exploit types structure distinguishes algorithms differ earlier
approaches (Boutilier et al., 2000), exploit context-specific structure. provide
detailed discussion differences Section 10.
show that, factored MDP factored value functions, various critical operations planning algorithms implemented closed form without necessarily
enumerating entire state space. particular, new algorithms build upon
novel linear programming decomposition technique. technique reduces structured LPs
exponentially many constraints equivalent, polynomially-sized ones. decomposition follows procedure analogous variable elimination applies additively
400

fiEfficient Solution Algorithms Factored MDPs

structured value functions (Bertele & Brioschi, 1972) value functions exploit context-specific structure (Zhang & Poole, 1999). Using basic operations,
planning algorithms implemented efficiently, even though size state space
grows exponentially number variables.
first method based approximate linear programming algorithm (Schweitzer
& Seidmann, 1985). algorithm generates linear, approximate value function
solving single linear program. Unfortunately, number constraints LP proposed
Schweitzer Seidmann grows exponentially number variables. Using LP
decomposition technique, exploit structure factored MDPs represent exactly
optimization problem exponentially fewer constraints.
terms approximate dynamic programming, paper makes twofold contribution.
First, provide new approach approximately solving MDPs using linear value
function. Previous approaches linear function approximation typically utilized
least squares (L2 -norm) approximation value function. Least squares approximations
incompatible convergence analyses MDPs, based max-norm.
provide first MDP solution algorithms value iteration policy iteration
use linear max-norm projection approximate value function, thereby directly
optimizing quantity appears provided error bounds. Second, show
exploit structure problem apply technique factored MDPs,
leveraging LP decomposition technique.
Although approximate dynamic programming currently possesses stronger theoretical
guarantees, experimental results suggest approximate linear programming
good alternative. Whereas former tends generate better policies set
basis functions, due simplicity computational advantages approximate linear
programming, add basis functions, obtaining better policy still requiring
less computation approximate dynamic programming approach.
Finally, present experimental results comparing approach work Boutilier
et al. (2000), illustrating tradeoffs two methods. particular,
problems significant context-specific structure value function, approach
faster due efficient handling value function representation. However,
cases significant context-specific structure problem, rather
value function, algorithm requires exponentially large value function
representation. classes problems, demonstrate using value function exploits additive context-specific structure, algorithm obtain
polynomial-time near-optimal approximation true value function.
paper starts presentation factored MDPs approximate solution algorithms MDPs. Section 4, describe basic operations used algorithms,
including LP decomposition technique. Section 5, present first two
algorithms: approximate linear programming algorithm factored MDPs. second
algorithm, approximate policy iteration max-norm projection, presented Section 6.
Section 7 describes approach efficiently computing bounds policy quality based
Bellman error. Section 8 shows extend methods deal context-specific
structure. paper concludes empirical evaluation Section 9 discussion
related work Section 10.
401

fiGuestrin, Koller, Parr & Venkataraman

paper greatly expanded version work published Guestrin
et al. (2001a), work presented Guestrin et al. (2001b, 2002).

2. Factored Markov Decision Processes
Markov decision process (MDP) mathematical framework sequential decision
problems stochastic domains. thus provides underlying semantics task
planning uncertainty. begin concise overview MDP framework,
describe representation factored MDPs.
2.1 Markov Decision Processes
briefly review MDP framework, referring reader books Bertsekas
Tsitsiklis (1996) Puterman (1994) in-depth review. Markov Decision Process
(MDP) defined 4-tuple (X, A, R, P ) where: X finite set |X| = N states;
finite set actions; R reward function R : X 7 R, R(x, a) represents
reward obtained agent state x taking action a; P Markovian
transition model P (x0 | x, a) represents probability going state x state
x0 action a. assume rewards bounded, is, exists Rmax
Rmax |R(x, a)| , x, a.
Example 2.1 Consider problem optimizing behavior system administrator
(SysAdmin) maintaining network computers. network, machine
connected subset machines. Various possible network topologies
defined manner (see Figure 1 examples). one simple network, might
connect machines ring, machine connected machines + 1 1. (In
example, assume addition subtraction performed modulo m.)
machine associated binary random variable Xi , representing whether
working failed. every time step, SysAdmin receives certain amount
money (reward) working machine. job SysAdmin decide
machine reboot; thus, + 1 possible actions time step: reboot one
machines nothing (only one machine rebooted per time step). machine
rebooted, working high probability next time step. Every machine
small probability failing time step. However, neighboring machine fails,
probability increases dramatically. failure probabilities define transition model
P (x0 | x, a), x particular assignment describing machines working
failed current time step, SysAdmins choice machine reboot x0
resulting state next time step.
assume MDP infinite horizon future rewards discounted
exponentially discount factor [0, 1). stationary policy MDP
mapping : X 7 A, (x) action agent takes state x. computer
network problem, possible configuration working failing machines, policy
would tell SysAdmin machine reboot. policy associated value
function V RN , V (x) discounted cumulative value agent gets
starts state x follows policy . precisely, value V state x
402

fiEfficient Solution Algorithms Factored MDPs

Server

Server

Star

Bidirectional Ring

Ring Star

Server

3 Legs

Ring Rings

Figure 1: Network topologies tested; status machine influence status
parent network.

policy given by:
V (x) = E

"
X





(t)

(t)

R X , (X

t=0


#

(0)
) X = x ,


X(t) random variable representing state system steps.
running example, value function represents much money SysAdmin expects
collect starts acting according network state x. value function
fixed policy fixed point set linear equations define value
state terms value possible successor states. formally, define:
Definition 2.2 DP operator, , stationary policy is:
V(x) = R(x, (x)) +

X

P (x0 | x, (x))V(x0 ).

x0

value function policy , V , fixed point operator: V = V .
optimal value function V describes optimal value agent achieve
starting state. V defined set non-linear equations. case,
value state must maximal expected value achievable policy starting
state. precisely, define:
Definition 2.3 Bellman operator, , is:
V(x) = max[R(x, a) +


X

P (x0 | x, a)V(x0 )].

x0

optimal value function V fixed point : V = V .
value function V, define policy obtained acting greedily relative
V. words, state, agent takes action maximizes one-step
403

fiGuestrin, Koller, Parr & Venkataraman

utility, assuming V represents long-term utility achieved next state.
precisely, define:
Greedy(V)(x) = arg max[R(x, a) +


X

P (x0 | x, a)V(x0 )].

(1)

x0

greedy policy relative optimal value function V optimal policy =
Greedy(V ).
2.2 Factored MDPs
Factored MDPs representation language allows us exploit problem structure
represent exponentially large MDPs compactly. idea representing large
MDP using factored model first proposed Boutilier et al. (1995).
factored MDP, set states described via set random variables X =
{X1 , . . . , Xn }, Xi takes values finite domain Dom(Xi ). state x
defines value xi Dom(Xi ) variable Xi . general, use upper case letters
(e.g., X) denote random variables, lower case (e.g., x) denote values.
use boldface denote vectors variables (e.g., X) values (x). instantiation
Dom(Y) subset variables Z Y, use y[Z] denote value
variables Z instantiation y.
factored MDP, define state transition model using dynamic Bayesian
network (DBN) (Dean & Kanazawa, 1989). Let Xi denote variable Xi current
time Xi0 , variable next step. transition graph DBN
two-layer directed acyclic graph G whose nodes {X1 , . . . , Xn , X10 , . . . , Xn0 }. denote
parents Xi0 graph Parents (Xi0 ). simplicity exposition, assume
Parents (Xi0 ) X; thus, arcs DBN variables consecutive
time slices. (This assumption used expository purposes only; intra-time slice arcs
handled small modification presented Section 4.1.) node Xi0 associated
conditional probability distribution (CPD) P (Xi0 | Parents (Xi0 )). transition
probability P (x0 | x) defined be:
P (x0 | x) =



P (x0i | ui ) ,



ui value x variables Parents (Xi0 ).
Example 2.4 Consider instance SysAdmin problem four computers, labelled
M1 , . . . , M4 , unidirectional ring topology shown Figure 2(a). first task
modeling problem factored MDP define state space X. machine
associated binary random variable Xi , representing whether working
failed. Thus, state space represented four random variables: {X1 , X2 , X3 , X4 }.
next task define transition model, represented DBN. parents
next time step variables Xi0 depend network topology. Specifically, probability
machine fail next time step depends whether working current
time step status direct neighbors (parents topology) network
current time step. shown Figure 2(b), parents Xi0 example Xi
Xi1 . CPD Xi0 Xi = false, Xi0 = false high probability;
404

fiEfficient Solution Algorithms Factored MDPs

X1

X1
R

X2

M1

R

M4

M2

M3

(a)

2

X3
R

X4
R

1

3

P (Xi0 = | Xi , Xi1 , A):
h
1

h

2

X2

X3
h

3

X4
h

4

(b)

Action reboot:
machine machine

4

Xi1
Xi
Xi1
Xi
Xi1
Xi
Xi1
Xi

=f
=f
=f
=t
=t
=f
=t
=t






1

0.0238

1

0.475

1

0.0475

1

0.95

(c)

Figure 2: Factored MDP example: network topology (a) obtain factored
MDP representation (b) CPDs described (c).

is, failures tend persist. Xi = true, Xi0 noisy parents (in
unidirectional ring topology Xi0 one parent Xi1 ); is, failure
neighbors independently cause machine fail.
described represent factored Markovian transition dynamics arising
MDP DBN, directly addressed representation actions.
Generally, define transition dynamics MDP defining separate DBN
model = hGa , Pa action a.
Example 2.5 system administrator example, action ai rebooting
one machines, default action nothing. transition model
described corresponds nothing action. transition model ai
different transition model variable Xi0 , Xi0 = true
probability one, regardless status neighboring machines. Figure 2(c) shows
actual CPD P (Xi0 = W orking | Xi , Xi1 , A), one entry assignment
state variables Xi Xi1 , action A.
fully specify MDP, need provide compact representation reward
function. assume reward function factored additively set localized
reward functions, depends small set variables. example,
might reward function associated machine i, depends Xi .
is, SysAdmin paid per-machine basis: every time step, receives money
machine working. formalize concept localized functions:
Definition 2.6 function f scope Scope[f ] = C X f : Dom(C) 7 R.
f scope Z, use f (z) shorthand f (y) part
instantiation z corresponds variables Y.
405

fiGuestrin, Koller, Parr & Venkataraman

characterize concept local rewards. Let R1a , . . . , Rra set
functions, scope Ria restricted variable cluster Uai {X1 , . . . , Xn }.
P
reward taking action state x defined Ra (x) = ri=1 Ria (Uai ) R.
example, reward function Ri associated machine i, depends
Xi , depend action choice. local rewards represented
diamonds Figure 2(b), usual notation influence diagrams (Howard &
Matheson, 1984).

3. Approximate Solution Algorithms
several algorithms compute optimal policy MDP. three
commonly used value iteration, policy iteration, linear programming. key component three algorithms computation value functions, defined Section 2.1.
Recall value function defines value state x state space.
explicit representation value function vector values different states,
solution algorithms implemented series simple algebraic steps. Thus,
case, three implemented efficiently.
Unfortunately, case factored MDPs, state space exponential number
variables domain. SysAdmin problem, example, state x system
assignment describing machines working failed; is, state x
assignment random variable Xi . Thus, number states exponential
number machines network (|X| = N = 2m ). Hence, even representing
explicit value function problems ten machines infeasible. One
might tempted believe factored transition dynamics rewards would result
factored value function, thereby represented compactly. Unfortunately, even
trivial factored MDPs, guarantee structure model preserved
value function (Koller & Parr, 1999).
section, discuss use approximate value function, admits
compact representation. describe approximate versions exact algorithms,
use approximate value functions. description section somewhat abstract,
specify basic operations required algorithms performed
explicitly. later sections, elaborate issues, describe algorithms
detail. brevity, choose focus policy iteration linear programming;
techniques easily extend value iteration.
3.1 Linear Value Functions
popular choice approximating value functions using linear regression, first
proposed Bellman et al. (1963). Here, define space allowable value functions
V H RN via set basis functions:
Definition 3.1 linear value function set basis functions H = {h1 , . . . , hk }
P
function V written V(x) = kj=1 wj hj (x) coefficients w =
(w1 , . . . , wk )0 .
define H linear subspace RN spanned basis functions H.
useful define N k matrix H whose columns k basis functions viewed
406

fiEfficient Solution Algorithms Factored MDPs

vectors. compact notation, approximate value function represented
Hw.
expressive power linear representation equivalent, example,
single layer neural network features corresponding basis functions defining
H. features defined, must optimize coefficients w order obtain
good approximation true value function. view approach separating
problem defining reasonable space features induced space H,
problem searching within space. former problem typically purview
domain experts, latter focus analysis algorithmic design. Clearly,
feature selection important issue essentially areas learning approximation.
offer simple methods selecting good features MDPs Section 11,
goal address large important topic paper.
chosen linear value function representation set basis functions,
problem becomes one finding values weights w Hw yield
good approximation true value function. paper, consider two
approaches: approximate dynamic programming using policy iteration approximate
linear programming. section, present two approaches. Section 4,
show exploit problem structure transform approaches practical
algorithms deal exponentially large state spaces.
3.2 Policy Iteration
3.2.1 Exact Algorithm
exact policy iteration algorithm iterates policies, producing improved policy
iteration. Starting initial policy (0) , iteration consists two phases.
Value determination computes, policy (t) , value function V(t) , finding
fixed point equation T(t) V(t) = V(t) , is, unique solution set linear
equations:
X
P (x0 | x, (t) (x))V(t) (x0 ), x.
V(t) (x) = R(x, (t) (x)) +
x0

policy improvement step defines next policy
(t+1) = Greedy(V(t) ).
shown process converges optimal policy (Bertsekas & Tsitsiklis,
1996). Furthermore, practice, convergence optimal policy often quick.
3.2.2 Approximate Policy Iteration
steps policy iteration algorithm require manipulation value functions
policies, often cannot represented explicitly large MDPs. define
version policy iteration algorithm uses approximate value functions, use
following basic idea: restrict algorithm using value functions within
provided H; whenever algorithm takes step results value function V
outside space, project result back space finding value function
within space closest V. precisely:
407

fiGuestrin, Koller, Parr & Venkataraman

Definition 3.2 projection operator mapping : RN H. said
projection w.r.t. norm kk V = Hw w arg minw kHw Vk.
is, V linear combination basis functions, closest V respect
chosen norm.
approximate policy iteration algorithm performs policy improvement step exactly. value determination step, value function value acting according
current policy (t) approximated linear combination basis functions.
consider problem value determination policy (t) . point,
useful introduce notation: Although rewards function state
action choice, policy fixed, rewards become function state
only, denote R(t) , R(t) (x) = R(x, (t) (x)). Similarly, transition
model: P(t) (x0 | x) = P (x0 | x, (t) (x)). rewrite value determination step
terms matrices vectors. view V(t) R(t) N -vectors, P(t)
N N matrix, equations:
V(t) = R(t) + P(t) V(t) .
system linear equations one equation state,
solved exactly relatively small N . goal provide approximate solution, within
H. precisely, want find:
w(t) = arg min kHw (R(t) + P(t) Hw)k ;
w







= arg min (H P(t) H) w(t) R(t) .
w

Thus, approximate policy iteration alternates two steps:
w(t) = arg min kHw (R(t) + P(t) Hw)k ;
w

(t+1) = Greedy(Hw(t) ).

(2)
(3)

3.2.3 Max-norm Projection
approach along lines described used various papers, several
recent theoretical algorithmic results (Schweitzer & Seidmann, 1985; Tsitsiklis & Van
Roy, 1996b; Van Roy, 1998; Koller & Parr, 1999, 2000). However, approaches suffer
problem might call norm incompatibility. computing projection,
utilize standard Euclidean projection operator respect L2 norm
weighted L2 norm.1 hand, convergence error analyses MDP
algorithms utilize max-norm (L ). incompatibility made difficult provide
error guarantees.
tie projection operator closely error bounds use
projection operator L norm. problem minimizing L norm
studied optimization literature problem finding Chebyshev solution2
1. Weighted L2 norm projections stable meaningful error bounds weights correspond
stationary distribution fixed policy evaluation (value determination) (Van Roy, 1998),
stable combined . Averagers (Gordon, 1995) stable non-expansive
L , require mixture weights determined priori. Thus, not, general,
minimize L error.
2. Chebyshev norm referred max, supremum L norms minimax solution.

408

fiEfficient Solution Algorithms Factored MDPs

overdetermined linear system equations (Cheney, 1982). problem defined
finding w that:
w arg min kCw bk .
(4)
w

use algorithm due Stiefel (1960), solves problem linear programming:
Variables: w1 , . . . , wk , ;
Minimize: ;
P
(5)
Subject to: kj=1 cij wj bi
Pk
bi j=1 cij wj , = 1...N.



P


constraints linear program imply kj=1 cij wj bi i,
equivalently, kCw bk . objective LP minimize . Thus,
solution (w , ) linear program, w solution Equation (4) L
projection error.
use L projection context approximate policy iteration
obvious way. implementing projection operation Equation (2), use
L projection (as Equation (4)), C = (H P(t) H) b = R(t) .
minimization solved using linear program (5).
key point LP k + 1 variables. However, 2N constraints,
makes impractical large state spaces. SysAdmin problem, example,
number constraints LP exponential number machines network
(a total 2 2m constraints machines). Section 4, show that, factored MDPs
linear value functions, 2N constraints represented efficiently, leading
tractable algorithm.
3.2.4 Error Analysis
motivated use max-norm projection within approximate policy iteration
algorithm via compatibility standard error analysis techniques MDP algorithms.
provide careful analysis impact L error introduced projection step. analysis provides motivation use projection step directly
minimizes quantity. acknowledge, however, main impact analysis
motivational. practice, cannot provide priori guarantees L projection
outperform methods.
goal analyze approximate policy iteration terms amount error
introduced step projection operation. error zero,
performing exact value determination, error accrue. error small,
get approximation accurate. result follows analysis below.
precisely, define projection error error resulting approximate
value determination step:








(t) = Hw(t) R(t) + P(t) Hw(t) .


Note that, using max-norm projection, finding set weights w(t)
exactly minimizes one-step projection error (t) . is, choosing best
409

fiGuestrin, Koller, Parr & Venkataraman

possible weights respect error measure. Furthermore, exactly error
measure going appear bounds theorem. Thus, make
bounds step tight possible.
first show projection error accrued step bounded:
Lemma 3.3 value determination error bounded: exists constant P Rmax
P (t) iterations algorithm.
Proof: See Appendix A.1.
Due contraction property Bellman operator, overall accumulated error
decaying average projection error incurred throughout iterations:
Definition 3.4 discounted value determination error iteration defined as:
(t1)
(0)
(t) +
; = 0.

(t)

=

Lemma 3.3 implies accumulated error remains bounded approximate policy
(t)
(1 )
iteration: P 1
. bound loss incurred acting according
policy generated approximate policy iteration algorithm, opposed
optimal policy:
Theorem 3.5 approximate policy iteration algorithm, let (t) policy generated
iteration t. Furthermore, let V(t) actual value acting according policy.
loss incurred using policy (t) opposed optimal policy value V
bounded by:
(t)
2



kV V(t) k kV V(0) k +
.
(6)
(1 )2
Proof: See Appendix A.2.
words, Equation (6) shows difference approximation iteration
optimal value function bounded sum two terms. first term
present standard policy iteration goes zero exponentially fast. second
discounted accumulated projection error and, Lemma 3.3 shows, bounded. second
term minimized choosing w(t) one minimizes:





Hw(t) R(t) + P(t) Hw(t)



,

exactly computation performed max-norm projection. Therefore,
theorem motivates use max-norm projections minimize error term appears
bound.
bounds provided far may seem fairly trivial, provided
strong priori bound (t) . Fortunately, several factors make bounds interesting despite lack priori guarantees. approximate policy iteration converges,
b policy
occurred experiments, obtain much tighter bound:
convergence, then:



V V 2b
,
b

(1 )
b one-step max-norm projection error associated estimating value
b . Since max-norm projection operation provides b

, easily obtain
410

fiEfficient Solution Algorithms Factored MDPs

posteriori bound part policy iteration procedure. details provided
Section 7.
One could rewrite bound Theorem 3.5 terms worst case projection error P , worst projection error cycle policies, approximate policy iteration
gets stuck cycle. formulations would closer analysis Bertsekas
Tsitsiklis (1996, Proposition 6.2, p.276). However, consider case policies
(or policies final cycle) low projection error, policies
cannot approximated well using projection operation, large
one-step projection error. worst-case bound would loose, would
dictated error difficult policy approximate. hand, using
discounted accumulated error formulation, errors introduced policies hard
approximate decay rapidly. Thus, error bound represents average case
analysis: decaying average projection errors policies encountered successive iterations algorithm. convergent case, bound computed
easily part policy iteration procedure max-norm projection used.
practical benefit posteriori bounds give meaningful feedback
impact choice value function approximation architecture.
explicitly addressing difficult general problem feature selection paper,
error bounds motivate algorithms aim minimize error given approximation
architecture provide feedback could useful future efforts automatically
discover improve approximation architectures.
3.3 Approximate Linear Programming
3.3.1 Exact Algorithm
Linear programming provides alternative method solving MDPs. formulates
problem finding value function linear program (LP). LP variables
V1 , . . . , VN , Vi represents V(xi ): value starting ith state system.
LP given by:
Variables: V1 , . . . , VN ;
P
Minimize:
xi (xi ) Vi ;
P
Subject to: Vi [R(xi , a) + j P (xj | xi , a)Vj ] xi X, A,

(7)

state relevance weights positive. Note that, exact case, solution
obtained positive weight vector. interesting note steps
simplex algorithm correspond policy changes single states, steps policy
iteration involve policy changes multiple states. practice, policy iteration tends
faster linear programming approach (Puterman, 1994).
3.3.2 Approximate Linear Program
approximate formulation LP approach, first proposed Schweitzer Seidmann (1985), restricts space allowable value functions linear space spanned
basis functions. approximate formulation, variables w1 , . . . , wk :
weights basis functions. LP given by:
411

fiGuestrin, Koller, Parr & Venkataraman

Variables: w1 , . . . , wk ;
P
P
Minimize:
(x) wi hi (x) ;
Px
P
P
0
0
Subject to:
wi hi (x) [R(x, a) +
x0 P (x | x, a)
wi hi (x )] x X, A.
(8)
words, formulation takes LP (7) substitutes explicit state
P
value function linear value function representation wi hi (x), or, compact
notation, V replaced Hw. linear program guaranteed feasible constant
function function constant value states included set
basis functions.
approximate linear programming formulation, choice state relevance weights,
, becomes important. Intuitively, constraints LP binding; is,
constraints tighter states others. state x, relevance
weight (x) indicates relative importance tight constraint. Therefore, unlike
exact case, solution obtained may differ different choices positive weight vector
. Furthermore, is, general, guarantee quality greedy policy
generated approximation Hw. However, recent work de Farias Van
Roy (2001a) provides analysis error relative best possible approximation subspace, guidance selecting improve quality
approximation. particular, analysis shows LP provides best
approximation Hw optimal value function V weighted L1 sense subject
constraint Hw Hw , weights L1 norm state relevance
weights .
transformation exact approximate problem formulation effect reducing number free variables LP k (one basis function
coefficient), number constraints remains N |A|. SysAdmin problem,
example, number constraints LP (8) (m + 1) 2m , number
machines network. Thus, process generating constraints solving
LP still seems unmanageable machines. next section, discuss
use structure factored MDP provide compact representation
efficient solution LP.

4. Factored Value Functions
linear value function approach, algorithms described Section 3, apply
choice basis functions. context factored MDPs, Koller Parr (1999) suggest
particular type basis function, particularly compatible structure
factored MDP. suggest that, although value function typically structured,
many cases might close structured. is, might wellapproximated using linear combination functions refers small
number variables. precisely, define:
Definition 4.1 factored (linear) value function linear function basis set
h1 , . . . , hk , scope hi restricted subset variables Ci .
Value functions type long history area multi-attribute utility theory (Keeney & Raiffa, 1976). example, might basis function hi
412

fiEfficient Solution Algorithms Factored MDPs

machine, indicating whether working not. basis function scope restricted
Xi . represented diamonds next time step Figure 2(b).
Factored value functions provide key performing efficient computations
exponential-sized state spaces factored MDPs. main insight restricted scope functions (including basis functions) allow certain basic operations
implemented efficiently. remainder section, show structure
factored MDPs exploited perform two crucial operations efficiently: one-step
lookahead (backprojection), representation exponentially many constraints
LPs. Then, use basic building blocks formulate efficient approximation algorithms factored MDPs, presented self-contained section:
approximate linear programming factored MDPs Section 5, approximate policy
iteration max-norm projection Section 6.
4.1 One-step Lookahead
key step algorithms computation one-step lookahead value
action a. necessary, example, computing greedy policy
Equation (1). Lets consider computation Q function, Qa (x), represents
expected value agent obtains taking action current time step receiving
long-term value V thereafter. Q function computed by:
Qa (x) = R(x, a) +

X

P (x0 | x, a)V(x).

(9)

x0

is, Qa (x) given current reward plus discounted expected future value.
Using notation, express greedy policy as: Greedy(V)(x) = maxa Qa (x).
Recall estimating long-term value policy using set basis
P
functions: V(x) = wi hi (x). Thus, rewrite Equation (9) as:
Qa (x) = R(x, a) +

X

P (x0 | x, a)

X

x0

wi hi (x).

(10)



P

size state space exponential, computing expectation x0 P (x0 |
P
x, a) wi hi (x) seems infeasible. Fortunately, discussed Koller Parr (1999),
expectation operation, backprojection, performed efficiently transition
model value function factored appropriately. linearity value
function permits linear decomposition, summand expectation
viewed independent value function updated manner similar value
iteration procedure used Boutilier et al. (2000). recap construction briefly,
first defining:
Ga (x) =

X
x0

P (x0 | x, a)

X

wi hi (x0 ) =



X


wi

X

P (x0 | x, a)hi (x0 ).

x0

Thus, compute expectation basis function separately:
gia (x) =

X

P (x0 | x, a)hi (x0 ),

x0

413

fiGuestrin, Koller, Parr & Venkataraman

P

weight wi obtain total expectation Ga (x) = wi gia (x).
intermediate function gia called backprojection basis function hi
transition model Pa , denote gia = Pa hi . Note that, factored MDPs,
transition model Pa factored (represented DBN) basis functions hi
scope restricted small set variables. two important properties allow us
compute backprojections efficiently.
show restricted scope function h (such basis functions)
backprojected transition model P represented DBN .
h scope restricted Y; goal compute g = P h. define backprojected scope set parents Y0 transition graph G ;
(Y0 ) = Yi0 Y0 Parents (Yi0 ). intra-time slice arcs included, Parents (Xi0 )
{X1 , . . . , Xn , X10 , . . . , Xn0 }, change algorithm definition backprojected scope . definition includes direct parents 0 ,
variables {X1 , . . . , Xn } ancestors 0 :
(Y0 ) = {Xj | exist directed path Xj Xi0 Y0 }.
Thus, backprojected scope may become larger, functions still factored.
show that, h scope restricted Y, backprojection g
scope restricted parents Y0 , i.e., (Y0 ). Furthermore, backprojection
computed enumerating settings variables (Y0 ), rather settings
variables X:
g(x) = (P h)(x);
=

X

P (x0 | x)h(x0 );

x0

=

X

P (x0 | x)h(y0 );

x0

=

X
y0

=

X

X

P (y0 | x)h(y0 )

P (u0 | x);

u0 (x0 y0 )

P (y0 | z)h(y0 );

y0

= g(z);
P

z value (Y0 ) x term u0 (x0 y0 ) P (u0 | x) = 1
sum probability distribution complete domain. Therefore, see (P h)
function whose scope restricted (Y0 ). Note cost computation depends
linearly |Dom( (Y0 ))|, depends (the scope h) complexity
process dynamics. backprojection procedure summarized Figure 3.
Returning example, consider basis function hi indicator variable Xi :
takes value 1 ith machine working 0 otherwise. hi scope restricted
Xi0 , thus, backprojection gi scope restricted Parents (Xi0 ): (Xi0 ) = {Xi1 , Xi }.
4.2 Representing Exponentially Many Constraints
seen Section 3, approximation algorithms require solution linear programs: LP (5) approximate policy iteration, LP (8) approximate
414

fiEfficient Solution Algorithms Factored MDPs

Backproja (h)

basis function h scope C.

Define scope backprojection: (C0 ) = Xi0 C0 Parentsa (Xi0 ).
0
assignment
P
Q (C ):0 0

g (y) = c0 C0 i|X 0 C0 Pa (c [Xi ] | y)h(c0 ).


Return g .

Figure 3: Backprojection basis function h.
linear programming algorithm. LPs common characteristics:
small number free variables (for k basis functions k + 1 free variables approximate policy iteration k approximate linear programming), number
constraints still exponential number state variables. However, factored MDPs,
LP constraints another useful property: functionals constraints
restricted scope. key observation allows us represent constraints
compactly.
First, observe constraints linear programs form:


X

wi ci (x) b(x), x,

(11)



w1 , . . . , wk free variables LP x ranges states.
general form represents type constraint max-norm projection LP (5)
approximate linear programming formulation (8).3
first insight construction replace entire set constraints
Equation (11) one equivalent non-linear constraint:
max
x

X

wi ci (x) b(x).

(12)



second insight new non-linear constraint implemented set
linear constraints using construction follows structure variable elimination
cost networks. insight allows us exploit structure factored MDPs represent
constraint compactly.
tackle problem representing constraint Equation (12) two steps:
first, computing maximum assignment fixed set weights; then, representing
non-linear constraint small set linear constraints, using construction call
factored LP.
4.2.1 Maximizing State Space
key computation algorithms represent non-linear constraint form
Equation (12) efficiently small set linear constraints. presenting construction, lets first consider simpler problem: Given fixed weights wi , would
P
compute maximization: = maxx wi ci (x) b(x), is, state x,
P

3. complementary constraints (5), b(x) wi ci (x), formulated using analogous
construction one present section changing sign ci (x) b(x). approximate
linear programming constraints (8) formulated form, show Section 5.

415

fiGuestrin, Koller, Parr & Venkataraman

P

difference wi ci (x) b(x) maximal. However, cannot explicitly enumerate exponential number states compute difference. Fortunately,
structure factored MDPs allows us compute maximum efficiently.
case factored MDPs, state space set vectors x assignments state variables X = {X1 , . . . , Xn }. view Cw b functions
state variables, hence difference. Thus, define function
P
F w (X1 , . . . , Xn ) F w (x) = wi ci (x) b(x). Note executed
representation shift; viewing F w function variables X, parameterized w. Recall size state space exponential number
variables. Hence, goal section compute maxx F w (x) without explicitly
considering exponentially many states. solution use fact F w
P
factored representation. precisely, Cw form wi ci (Zi ), Zi
subset X. example, might c1 (X1 , X2 ) takes value 1 states
X1 = true X2 = false 0 otherwise. Similarly, vector b case sum
P
restricted scope functions. Thus, express F w sum j fjw (Zj ), fjw may
may depend w. future, sometimes drop superscript w
clear context.
P
Using compact notation, goal simply compute maxx wi ci (x)
b(x) = maxx F w (x), is, find state x F w maximized. Recall
P
w
Fw =
j=1 fj (Zj ). maximize function, F , without enumerating every state
using non-serial dynamic programming (Bertele & Brioschi, 1972). idea virtually
identical variable elimination Bayesian network. review construction here,
central component solution LP.
goal compute
X
max
fj (x[Zj ]).
x1 ,...,xn

j

main idea that, rather summing functions maximization,
maximize variables one time. maximizing xl , summands
involving xl participate maximization.
Example 4.2 Assume
F = f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).
therefore wish compute:
max

x1 ,x2 ,x3 ,x4

f1 (x1 , x2 ) + f2 (x1 , x3 ) + f3 (x2 , x4 ) + f4 (x3 , x4 ).

first compute maximum x4 ; functions f1 f2 irrelevant,
push out. get
max f1 (x1 , x2 ) + f2 (x1 , x3 ) + max[f3 (x2 , x4 ) + f4 (x3 , x4 )].

x1 ,x2 ,x3

x4

result internal maximization depends values x2 , x3 ; thus, introduce new function e1 (X2 , X3 ) whose value point x2 , x3 value internal
max expression. problem reduces computing
max f1 (x1 , x2 ) + f2 (x1 , x3 ) + e1 (x2 , x3 ),

x1 ,x2 ,x3

416

fiEfficient Solution Algorithms Factored MDPs

VariableElimination (F, O)
//F = {f1 , . . . , fm } set functions maximized;
//O stores elimination order.

= 1 number variables:
//Select next variable eliminated.

Let l = O(i) ;
//Select relevant functions.

Let e1 , . . . , eL functions F whose scope contains Xl .
//Maximize current variable Xl .

Define new function e = maxxl
L
j=1 Scope[ej ] {Xl }.

PL

j=1 ej

; note Scope[e] =

//Update set functions.

Update set functions F = F {e} \ {e1 , . . . , eL }.
//Now, functions empty scope
P sum maximum value f1 + + fm .

Return maximum value

ei F

ei .

Figure 4: Variable elimination procedure computing maximum value f1 + + fm ,
restricted scope function.

one fewer variable. Next, eliminate another variable, say X3 , resulting
expression reducing to:
max f1 (x1 , x2 ) + e2 (x1 , x2 ),
x1 ,x2



e2 (x1 , x2 ) = max[f2 (x1 , x3 ) + e1 (x2 , x3 )].
x3

Finally, define
e3 = max f1 (x1 , x2 ) + e2 (x1 , x2 ).
x1 ,x2

result point number, desired maximum x1 , . . . , x4 .
naive approach enumerating states requires 63 arithmetic operations variables
binary, using variable elimination need perform 23 operations.
general variable elimination algorithm described Figure 4. inputs
algorithm functions maximized F = {f1 , . . . , fm } elimination
ordering variables, O(i) returns ith variable eliminated.
example above, variable Xl eliminated, select relevant functions
e1 , . . . , eL , whose scope contains Xl . functions removed set F
P
introduce new function e = maxxl L
j=1 ej . point, scope functions
F longer depends Xl , is, Xl eliminated. procedure repeated
variables eliminated. remaining functions F thus empty
scope. desired maximum therefore given sum remaining functions.
computational cost algorithm linear number new function
values introduced elimination process. precisely, consider computation
new function e whose scope Z. compute function, need compute |Dom[Z]|
different values. cost algorithm linear overall number values,
introduced throughout execution. shown Dechter (1999), cost exponential
417

fiGuestrin, Koller, Parr & Venkataraman

induced width cost network, undirected graph defined variables
X1 , . . . , Xn , edge Xl Xm appear together one original
functions fj . complexity algorithm is, course, dependent variable
elimination order problem structure. Computing optimal elimination order
NP-hard problem (Arnborg, Corneil, & Proskurowski, 1987) elimination orders
yielding low induced tree width exist problems. issues
confronted successfully large variety practical problems Bayesian network
community, benefited large variety good heuristics
developed variable elimination ordering problem (Bertele & Brioschi, 1972; Kjaerulff,
1990; Reed, 1992; Becker & Geiger, 2001).
4.2.2 Factored LP
section, present centerpiece planning algorithms: new, general
approach compactly representing exponentially large sets LP constraints problems
factored structure functions constraints decomposed
sum restricted scope functions. Consider original problem representing
non-linear constraint Equation (12) compactly. Recall wish represent
P
non-linear constraint maxx wi ci (x) b(x), equivalently, maxx F w (x),
without generating one constraint state Equation (11). new, key insight
non-linear constraint implemented using construction follows
structure variable elimination cost networks.
Consider function e used within F (including original s), let Z scope.
assignment z Z, introduce variable uez , whose value represents ez ,
linear program. initial functions fiw , include constraint ufzi = fiw (z).
fiw linear w, constraint linear LP variables. Now, consider new function
e introduced F eliminating variable Xl . Let e1 , . . . , eL functions extracted
F, let Z scope resulting e. introduce set constraints:
uez



L
X
ej
j=1

u(z,xl )[Zj ]

xl .

(13)

Let en last function generated elimination, recall scope empty.
Hence, single variable uen . introduce additional constraint uen .
complete algorithm, presented Figure 5, divided three parts: First,
generate equality constraints functions depend weights wi (basis functions).
second part, add equality constraints functions depend
weights (target functions). equality constraints let us abstract away differences
two types functions manage unified fashion third
part algorithm. third part follows procedure similar variable elimination
described Figure 4. However, unlike standard variable elimination would inP
troduce new function e, e = maxxl L
j=1 ej , factored LP procedure
introduce new LP variables uez . enforce definition e maximum Xl
PL
j=1 ej , introduce new LP constraints Equation (13).
Example 4.3 understand construction, consider simple example above,
assume want express fact maxx F w (x). first introduce set
418

fiEfficient Solution Algorithms Factored MDPs

FactoredLP (C, b,O)
// C = {c1 , . . . , ck } set basis functions.
// b = {b1 , . . . , bm } set target functions.
//O stores elimination order.
P
P
//Return (polynomial) set constraints equivalent wi ci (x) + j bj (x), x .
//Data structure constraints factored LP.

Let = {} .
//Data structure intermediate functions generated variable elimination.

Let F = {} .
//Generate equality constraint abstract away basis functions.

ci C:
Let Z = Scope[ci ].
assignment z Z, create new LP variable ufzi add
constraint :
ufzi = wi ci (z).
Store new function use variable elimination step: F = F {fi }.
//Generate equality constraint abstract away target functions.

bj b:
Let Z = Scope[bj ].
f
assignment z Z, create new LP variable uzj add
constraint :
f
uzj = bj (z).
Store new function fj use variable elimination step: F = F {fj }.
//Now, F contains functions involved LP, constraints become:
P
e (x), x , represent compactly using variable elimination procedure.
e F


= 1 number variables:

//Select next variable eliminated.

Let l = O(i) ;
//Select relevant functions.

Let e1 , . . . , eL functions F whose scope contains Xl , let
Zj = Scope[ej ].
//Introduce linear constraints maximum current variable Xl .

Define new function e scope Z = L
j=1 Zj {Xl } represent
PL
maxxl j=1 ej .
Add constraints enforce maximum: assignment z Z:
uez

L
X

e

j
u(z,x
l )[Zj ]

xl .

j=1

//Update set functions.

Update set functions F = F {e} \ {e1 , . . . , eL }.
//Now, variables eliminated functions empty scope.

Add last constraint :


X

ei .

ei F

Return .

Figure 5: Factored LP algorithm compact representation exponential set
P
P
constraints wi ci (x) + j bj (x), x.
419

fiGuestrin, Koller, Parr & Venkataraman

variables ufx11 ,x2 every instantiation values x1 , x2 variables X1 , X2 . Thus,
X1 X2 binary, four variables. introduce constraint
defining value ufx11 ,x2 appropriately. example, f1 above, uft,t1 = 0
uft,f1 = w1 . similar variables constraints fj value z
Zj . Note constraints simple equality constraint involving numerical
constants perhaps weight variables w.
Next, introduce variables intermediate expressions generated variable elimination. example, eliminating X4 , introduce set LP variables
uex12 ,x3 ; them, set constraints
uex12 ,x3 ufx32 ,x4 + ufx43 ,x4
one value x4 X4 . similar set constraint uex21 ,x2 terms
ufx21 ,x3 uex12 ,x3 . Note constraint simple linear inequality.
prove factored LP construction represents constraint
non-linear constraint Equation (12):
Theorem 4.4 constraints generated factored LP construction equivalent
non-linear constraint Equation (12). is, assignment (, w) satisfies
factored LP constraints satisfies constraint Equation (12).
Proof: See Appendix A.3.
P
Returning original formulation, j fjw Cw b original
set constraints. Hence new set constraints equivalent original set:
P
maxx wi ci (x) b(x) Equation (12), turn equivalent exponential
P
set constraints wi ci (x) b(x), x Equation (11). Thus, represent
exponential set constraints new set constraints LP variables. size
new set, variable elimination, exponential induced width cost
network, rather total number variables.
section, presented new, general approach compactly representing exponentially-large sets LP constraints problems factored structure. remainder
paper, exploit construction design efficient planning algorithms factored
MDPs.
4.2.3 Factored Max-norm Projection
use procedure representing exponential number constraints
Equation (11) compactly compute efficient max-norm projections, Equation (4):
w arg min kCw bk .
w

max-norm projection computed linear program (5). two sets
P
P
constraints LP: kj=1 cij wj bi , bi kj=1 cij wj , i.
sets instance constraints Equation (11), addressed
previous section. Thus, k basis functions C restricted scope
function target function b sum restricted scope functions,
use factored LP technique represent constraints max-norm projection LP
compactly. correctness algorithm corollary Theorem 4.4:
420

fiEfficient Solution Algorithms Factored MDPs

Corollary 4.5 solution ( , w ) linear program minimizes subject
constraints FactoredLP(C, b,O) FactoredLP(C, b,O), elimination
order satisfies:
w arg min kCw bk ,
w



= min kCw bk .
w

original max-norm projection LP k + 1 variables two constraints
state x; thus, number constraints exponential number state variables.
hand, new factored max-norm projection LP variables,
exponentially fewer constraints. number variables constraints new factored
LP exponential number state variables largest factor cost
network, rather exponential total number state variables. show
Section 9, exponential gain allows us compute max-norm projections efficiently
solving large factored MDPs.

5. Approximate Linear Programming
begin simplest approximate MDP solution algorithms, based
approximate linear programming formulation Section 3.3. Using basic operations
described Section 4, formulate algorithm simple efficient.
5.1 Algorithm
discussed Section 3.3, approximate linear program formulation based linear
programming approach solving MDPs presented Section 3.3. However, approximate version, restrict space value functions linear space defined
basis functions. precisely, approximate LP formulation, variables
w1 , . . . , wk weights basis functions. LP given by:
Variables: w1 , . . . , wk ;
P
P
Minimize:
(x) wi hi (x) ;
x
P
P
P
0
0
Subject to:
wi hi (x) [R(x, a) +
x0 P (x | x, a)
wi hi (x )] x X, A.
(14)
words, formulation takes LP (7) substitutes explicit state value
P
function linear value function representation wi hi (x). transformation
exact approximate problem formulation effect reducing number
free variables LP k (one basis function coefficient), number
constraints remains |X| |A|. SysAdmin problem, example, number
constraints LP (14) (m + 1) 2m , number machines
network. However, using algorithm representing exponentially large constraint sets
compactly able compute solution approximate linear programming
algorithm closed form exponentially smaller LP, Section 4.2.
P
P
First, consider objective function x (x) wi hi (x) LP (14). Naively
representing objective function requires summation exponentially large state
space. However, rewrite objective obtain compact representation.
first reorder terms:
421

fiGuestrin, Koller, Parr & Venkataraman

FactoredALP (P , R, , H, O, )
//P factored transition model.
//R set factored reward functions.
// discount factor.
//H set basis functions H = {h1 , . . . , hk }.
//O stores elimination order.
// state relevance weights.
//Return basis function weights w computed approximate linear programming.
//Cache backprojections basis functions.

basis function hi H; action a:
Let gia = Backproja (hi ).
//Compute factored state relevance weights.

basis function hi , compute factored state relevance weights
Equation (15) .
//Generate approximate linear programming constraints

Let = {}.
action a:
}, Ra , O).
Let = FactoredLP({g1a h1 , . . . , gka hkP

P

//So far, constraints guarantee R(x, a) + x0 P (x0 | x, a) wi hi (x0 )
P
w hi (x); satisfy approximate linear programming solution (14) must add

final constraint.

Let = { = 0}.
//We obtain solution weights solving LP.

Let w solution linear program: minimize
constraints .
Return w.

P


wi , subject

Figure 6: Factored approximate linear programming algorithm.

422

fiEfficient Solution Algorithms Factored MDPs

X

(x)

X

x

wi hi (x) =

X



X

wi

(x) hi (x).

x



Now, consider state relevance weights (x) distribution states, (x) > 0
P
x (x) = 1. backprojections, write:
=

X

X

(x) hi (x) =

x

(ci ) hi (ci );

(15)

ci Ci

(ci ) represents marginal state relevance weights domain
Dom[Ci ] basis function hi . example, use uniform state relevance weights
1
experiments (x) = |X|
marginals become (ci ) = |C1i | . Thus,
P
rewrite objective function wi , basis weight computed shown
Equation (15). state relevance weights represented marginals, cost
computing depends exponentially size scope Ci only, rather
exponentially number state variables. hand, state relevance
weights represented arbitrary distributions, need obtain marginals
Ci s, may efficient computation. Thus, greatest efficiency achieved
using compact representation, Bayesian network, state relevance weights.
Second, note right side constraints LP (14) correspond Qa
functions:
X
X
Qa (x) = Ra (x) +
P (x0 | x, a)
wi hi (x0 ).
x0



Using efficient backprojection operation factored MDPs described Section 4.1
rewrite Qa functions as:
Qa (x) = Ra (x) +

X

wi gia (x);



gia


backprojection basis function hi transition model Pa .
discussed, hi scope restricted Ci , gia restricted scope function (C0i ).
precompute backprojections gia basis relevance weights .
approximate linear programming LP (14) written as:
Variables: w1 , . . . , wk ;
P
Minimize:
w ;
Pi
P


Subject to:
w
hi (x) [R (x) +
wi gi (x)] x X, A.

(16)

Finally, rewrite LP use constraints form one Equation (12):
Variables: w1 , . . . , wk ;
P
Minimize:
wi ;
P
Subject to: 0 maxx {Ra (x) + wi [gia (x) hi (x)]} A.

(17)

use factored LP construction Section 4.2 represent non-linear
constraints compactly. Basically, one set factored LP constraints action
a. Specifically, write non-linear constraint form Equation (12) expressing functions C as: ci (x) = hi (x)gia (x). ci (x) restricted
423

fiGuestrin, Koller, Parr & Venkataraman

scope function; is, hi (x) scope restricted Ci , gia (x) scope restricted
(C0i ), means ci (x) scope restricted Ci (C0i ). Next, target
function b becomes reward function Ra (x) which, assumption, factored. Finally,
constraint Equation (12), free variable. hand, LP (17)
maximum right hand side must less zero. final condition
achieved adding constraint = 0. Thus, algorithm generates set factored
LP constraints, one action. total number constraints variables
new LP linear number actions |A| exponential induced width
cost network, rather total number variables. complete factored
approximate linear programming algorithm outlined Figure 6.
5.2 Example
present complete example operations required approximate LP algorithm solve factored MDP shown Figure 2(a). presentation follows four steps:
problem representation, basis function selection, backprojections LP construction.
Problem Representation: First, must fully specify factored MDP model
problem. structure DBN shown Figure 2(b). structure maintained
action choices. Next, must define transition probabilities action.
5 actions problem: nothing, reboot one 4 machines
network. CPDs actions shown Figure 2(c). Finally, must define
reward function. decompose global reward sum 4 local reward functions,
one machine, reward machine working. Specifically,
Ri (Xi = true) = 1 Ri (Xi = false) = 0, breaking symmetry setting R4 (X4 = true) =
2. use discount factor = 0.9.
simple example, use five simple basis functions.
Basis Function Selection:
First, include constant function h0 = 1. Next, add indicators machine
take value 1 machine working: hi (Xi = true) = 1 hi (Xi = false) = 0.
Backprojections:
first algorithmic step computing backprojection
basis functions, defined Section 4.1. backprojection constant basis
simple:
g0a =

X

Pa (x0 | x)h0 ;

x0

=

X

Pa (x0 | x) 1 ;

x0

= 1.
Next, must backproject indicator basis functions hi :
gia =

X
x0

=

Pa (x0 | x)hi (x0i ) ;

X



Pa (x0j | xj1 , xj )hi (x0i ) ;

x01 ,x02 ,x03 ,x04 j

424

fiEfficient Solution Algorithms Factored MDPs

=

X
x0i

=

X

X

Pa (x0i | xi1 , xi )hi (x0i )



Pa (x0j | xj1 , xj ) ;

x0 [X0 {Xi0 }] j6=i

Pa (x0i | xi1 , xi )hi (x0i ) ;

x0i

= Pa (Xi0 = true | xi1 , xi ) 1 + Pa (Xi0 = false | xi1 , xi ) 0 ;
= Pa (Xi0 = true | xi1 , xi ) .
Thus, gia restricted scope function {Xi1 , Xi }. use CPDs Figure 2(c) specify gia :
reboot =

(Xi1 , Xi ) =

reboot 6=

(Xi1 , Xi ) =

gi

gi

Xi = true Xi = false
Xi1 = true
1
1
;
Xi1 = false
1
1
Xi1 = true
Xi1 = false

Xi = true Xi = false
0.9
0.09
.
0.5
0.05

LP Construction:
illustrate factored LPs constructed algorithms,
define constraints approximate linear programming approach presented above.
First, define functions cai = gia hi , shown Equation (17). example,
functions ca0 = 1 = 0.1 constant basis, indicator bases:
reboot =

(Xi1 , Xi ) =

reboot 6=

(Xi1 , Xi ) =

ci

ci

Xi = true Xi = false
Xi1 = true
0.1
0.9
;
Xi1 = false
0.1
0.9
Xi1 = true
Xi1 = false

Xi = true Xi = false
0.19
0.081
.
0.55
0.045

Using definition cai , approximate linear programming constraints given by:
0 max
x

X

Ri +

X



wj caj , .

(18)

j

present LP construction one 5 actions: reboot = 1. Analogous constructions
made actions.
first set constraints, abstract away difference rewards basis
functions introducing LP variables u equality constraints. begin reward
functions:
R1
1
uR
x1 = 1 , ux1 = 0 ;

R2
2
uR
x2 = 1 , ux2 = 0 ;

R3
3
uR
x3 = 1 , ux3 = 0 ;

R4
4
uR
x4 = 2 , ux4 = 0 .

represent equality constraints caj functions reboot = 1 action. Note
appropriate basis function weight Equation (18) appears constraints:

425

fiGuestrin, Koller, Parr & Venkataraman

uc0 = 0.1 w0 ;
ucx11 ,x4 = 0.9 w1 ,
ucx11 ,x4 = 0.1 w1 ,
ucx11 ,x4 = 0.9 w1 ;
ucx11 ,x4 = 0.1 w1 ,
c
c
c
ux21 ,x2 = 0.19 w2 , ux21 ,x2 = 0.55 w2 , ux21 ,x2 = 0.081 w2 , ucx21 ,x2 = 0.045 w2 ;
ucx32 ,x3 = 0.19 w3 , ucx32 ,x3 = 0.55 w3 , ucx32 ,x3 = 0.081 w3 , ucx32 ,x3 = 0.045 w3 ;
ucx43 ,x4 = 0.19 w4 , ucx43 ,x4 = 0.55 w4 , ucx43 ,x4 = 0.081 w4 , ucx43 ,x4 = 0.045 w4 .
Using new LP variables, LP constraint Equation (18) reboot = 1 action
becomes:
0

max

X1 ,X2 ,X3 ,X4

4
4
X
X
c
c0

uR
+
u
+
uXjj1 ,Xj .
Xi
i=1

j=1

ready variable elimination process. illustrate elimination
variable X4 :
0

max

X1 ,X2 ,X3

3
3
h

X
X
c
Ri
c1
c4
c0
4
uXi + u +
uXjj1 ,Xj + max uR
X4 + uX1 ,X4 + uX3 ,X4 .
i=1

X4

j=2

h



c1
c4
4
represent term maxX4 uR
X4 + uX1 ,X4 + uX3 ,X4 set linear constraints,
one assignment X1 X3 , using new LP variables ueX11 ,X3 represent
maximum:

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c4
c1
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 ;

uex11 ,x3

c1
c4
4
uR
x4 + ux1 ,x4 + ux3 ,x4 .

eliminated variable X4 global non-linear constraint becomes:
0

3
3
X
X
c
c0

uR
+
u
+
uXjj1 ,Xj + ueX11 ,X3 .
Xi
X1 ,X2 ,X3

max

j=2

i=1

Next, eliminate variable X3 . new LP constraints variables form:
c3
e1
3
ueX21 ,X2 uR
X3 + uX2 ,X3 + uX1 ,X3 , X1 , X2 , X3 ;

thus, removing X3 global non-linear constraint:
2
X
c2
e2
c0

uR
Xi + u + uX1 ,X2 + uX1 ,X2 .
X1 ,X2

0 max

i=1

426

fiEfficient Solution Algorithms Factored MDPs

250000

Number LP constraints

200000

# explicit constraints =
(n+1) 2 n

Explicit LP
Factored LP
150000

100000

50000

# factored constraints =
12n2 + 5n - 8
0
0

2

4

6
8
10
Number machines ring

12

14

16

Figure 7: Number constraints LP generated explicit state representation
versus factored LP construction solution ring problem
basis functions single variables approximate linear programming
solution algorithm.

eliminate X2 , generating linear constraints:
c2
e2
2
ueX31 uR
X2 + uX1 ,X2 + uX1 ,X2 , X1 , X2 .

Now, global non-linear constraint involves X1 :
e3
c0
1
0 max uR
X1 + u + uX1 .
X1

X1 last variable eliminated, scope new LP variable empty
linear constraints given by:
e3
1
u e4 u R
X1 + uX1 , X1 .

state variables eliminated, turning global non-linear constraint
simple linear constraint:
0 uc0 + ue4 ,
completes LP description approximate linear programming solution
problem Figure 2.
small example four state variables, factored LP technique generates
total 89 equality constraints, 115 inequality constraints 149 LP variables,
explicit state representation Equation (8) generates 80 inequality constraints
5 LP variables. However, problem size increases, number constraints
LP variables factored LP approach grow O(n2 ), explicit state approach
grows exponentially, O(n2n ). scaling effect illustrated Figure 7.

6. Approximate Policy Iteration Max-norm Projection
factored approximate linear programming approach described previous section
elegant easy implement. However, cannot, general, provide strong
427

fiGuestrin, Koller, Parr & Venkataraman

guarantees error achieves. alternative use approximate policy
iteration described Section 3.2, offer certain bounds error. However,
shall see, algorithm significantly complicated, requires place
additional restrictions factored MDP.
particular, approximate policy iteration requires representation policy
iteration. order obtain compact policy representation, must make additional
assumption: action affects small number state variables. first state
assumption formally. Then, show obtain compact representation greedy
policy respect factored value function, assumption. Finally, describe
factored approximate policy iteration algorithm using max-norm projections.
6.1 Default Action Model
Section 2.2, presented factored MDP model, action associated
factored transition model represented DBN factored reward
function. However, different actions often similar transition dynamics, differing effect small set variables. particular, many cases variable
default evolution model, changes action affects directly (Boutilier
et al., 2000).
type structure turns useful compactly representing policies, property important approximate policy iteration algorithm. Thus, section
paper, restrict attention factored MDPs defined using default transition model = hGd , Pd (Koller & Parr, 2000). action a, define Effects[a] X0
variables next state whose local probability model different , i.e.,
variables Xi0 Pa (Xi0 | Parentsa (Xi0 )) 6= Pd (Xi0 | Parentsd (Xi0 )).
Example 6.1 system administrator example, action ai rebooting
one machines, default action nothing. transition model
described corresponds nothing action, default transition
model. transition model ai different transition model
variable Xi0 , Xi0 = true probability one, regardless status
neighboring machines. Thus, example, Effects[ai ] = Xi0 .
transition dynamics, define notion default reward model.
P
case, set reward functions ri=1 Ri (Ui ) associated default action
d. addition, action reward function Ra (Ua ). Here, extra reward
action scope restricted Rewards[a] = Uai {X1 , . . . , Xn }. Thus, total reward
P
associated action given Ra + ri=1 Ri . Note Ra factored
linear combination smaller terms even compact representation.
build additional assumption define complete algorithm.
Recall approximate policy iteration algorithm iterates two steps: policy
improvement approximate value determination. discuss steps.
6.2 Computing Greedy Policies
policy improvement step computes greedy policy relative value function V (t1) :
(t) = Greedy(V (t1) ).
428

fiEfficient Solution Algorithms Factored MDPs

Recall value function estimates linear form Hw. described
Section 4.1, greedy policy type value function given by:
Greedy(Hw)(x) = arg max Qa (x),


P

Qa represented by: Qa (x) = R(x, a) + wi gia (x).
attempt represent policy naively, faced problem
exponentially large state spaces. Fortunately, shown Koller Parr (2000),
greedy policy relative factored value function form decision list.
precisely, policy written form ht1 , a1 i, ht2 , a2 i, . . . , htL , aL i, ti
assignment values small subset Ti variables, ai action.
greedy action take state x action aj corresponding first event tj
list x consistent. completeness, review construction
decision-list policy.
critical assumption allows us represent policy compact decision list
default action assumption described Section 6.1. assumption, Qa
functions written as:


Qa (x) = R (x) +

r
X

Ri (x) +

i=1

X

wi gia (x),



Ra scope restricted Ua . Q function default action just:
P
P
Qd (x) = ri=1 Ri (x) + wi gid (x).
set linear Q-functions implicitly describes policy .
immediately obvious Q functions result compactly expressible policy.
important insight components weighted combination
identical, gia equal gid i. Intuitively, component gia corresponding
backprojection basis function hi (Ci ) different action influences
one variables Ci . formally, assume Effects[a] Ci = . case,
variables Ci transition model . Thus,
gia (x) = gid (x); words, ith component Qa function irrelevant
deciding whether action better default action d. define
components actually relevant: let Ia set indices Effects[a] Ci 6= .
indices basis functions whose backprojection differs Pa Pd .
example DBN Figure 2, actions basis functions involve single variables,
Iai = i.
Let us consider impact taking action default action d.
define impact difference value as:
(x) = Qa (x) Qd (x);
= Ra (x) +

X

h



wi gia (x) gid (x) .

(19)

iIa

analysis shows (x) function whose scope restricted




Ta = Ua iIa (C0i ) .
429

(20)

fiGuestrin, Koller, Parr & Venkataraman

DecisionListPolicy (Qa )
//Qa set Q-functions, one action;
//Return decision list policy .
//Initialize decision list.

Let = {}.
//Compute bonus functions.

action a, default action d:
Compute bonus taking action a,
(x) = Qa (x) Qd (x);
Equation (19). Note scope restricted Ta ,
Equation (20).
//Add states positive bonuses (unsorted) decision list.

assignment Ta :
(t) > 0, add branch decision list:
= {ht, a, (t)i}.
//Add default action (unsorted) decision list.

Let = {h, d, 0i}.
//Sort decision list obtain final policy.

Sort decision list decreasing order element ht, a, i.
Return .

Figure 8: Method computing decision list policy factored representation
Qa functions.

example DBN, Ta2 = {X1 , X2 }.
Intuitively, situation baseline value function Qd (x)
defines value state x. action changes baseline adding
subtracting amount state. point amount depends Ta ,
states variables Ta take values.
define greedy policy relative Q functions. action a, define
set conditionals ht, a, i, assignment values variables Ta ,
(t). Now, sort conditionals actions order decreasing :
ht1 , a1 , 1 i, ht2 , a2 , 2 i, . . . , htL , aL , L i.
Consider optimal action state x. would get largest possible bonus
default value. x consistent t1 , clearly take action a1 ,
gives us bonus 1 . not, try get 2 ; thus, check x
consistent t2 , so, take a2 . Using procedure, compute decisionlist policy associated linear estimate value function. complete algorithm
computing decision list policy summarized Figure 8.
P
Note number conditionals list |Dom(Ta )|; Ta , turn, depends
set basis function clusters intersect effects a. Thus, size
policy depends natural way interaction structure
430

fiEfficient Solution Algorithms Factored MDPs

process description structure basis functions. problems actions
modify large number variables, policy representation could become unwieldy.
approximate linear programming approach Section 5 appropriate cases,
require explicit representation policy.
6.3 Value Determination
approximate value determination step algorithm computes:
w(t) = arg min kHw (R(t) + P(t) Hw)k .
w

rearranging expression, get:
w(t) = arg min k(H P(t) H) w R(t) k .
w

equation instance optimization Equation (4). P(t) factored,
conclude C = (H P(t) H) matrix whose columns correspond restrictedscope functions. specifically:
(t)

ci (x) = hi (x) gi (x),
(t)

gi backprojection basis function hi transition model P(t) ,
described Section 4.1. target b = R(t) corresponds reward function,
moment assumed factored. Thus, apply factored LP
Section 4.2.3 estimate value policy (t) .
Unfortunately, transition model P(t) factored, decision list representation policy (t) will, general, induce transition model P(t) cannot
represented compact DBN. Nonetheless, still generate compact LP exploiting decision list structure policy. basic idea introduce cost networks
corresponding branch decision list, ensuring, additionally, states
consistent branch considered cost network maximization. Specifically,
factored LP construction branch hti , ai i. ith cost network
considers subset states consistent ith branch decision list.
Let Si set states x ti first event decision list x
consistent. is, state x Si , x consistent ti , consistent
tj j < i.
Recall that, Equation (11), LP construction defines set constraints
P
imply wi ci (x) b(x) state x. Instead, separate set
constraints states subset Si . state Si , know action ai
taken. Hence, apply construction using Pai transition model
factored assumption place non-factored P(t) . Similarly, reward function
P
becomes Rai (x) + ri=1 Ri (x) subset states.
issue guarantee cost network constraints derived transition model applied states Si . Specifically, must guarantee
applied states consistent ti , states consistent
tj j < i. guarantee first condition, simply instantiate variables Ti
take values specified ti . is, cost network considers variables
431

fiGuestrin, Koller, Parr & Venkataraman

FactoredAPI (P , R, , H, O, , tmax )
//P factored transition model.
//R set factored reward functions.
// discount factor.
//H set basis functions H = {h1 , . . . , hk }.
//O stores elimination order.
// Bellman error precision.
//tmax maximum number iterations.
//Return basis function weights w computed approximate policy iteration.
//Initialize weights

Let w(0) = 0.
//Cache backprojections basis functions.

basis function hi H; action a:
Let gia = Backproja (hi ).
//Main approximate policy iteration loop.

Let = 0.
Repeat
//Policy improvement part loop.
//Compute decision list policy iteration weights.

Let (t) = DecisionListPolicy(Ra +

P



(t)

wi gia ).

//Value determination part loop.
//Initialize constraints max-norm projection LP.

Let + = {} = {}.
//Initialize indicators.

Let = {}.
//For every branch decision list policy, generate relevant set constraints,
update indicators constraint state space future branches.

branch htj , aj decision list policy (t) :
//Instantiate variables Tj assignment given tj .




Instantiate set functions {h1 g1 j , . . . , hk gk j }
partial state assignment tj store C.
Instantiate target functions Raj partial state assignment tj store b.
Instantiate indicator functions partial state assignment tj store 0 .
//Generate factored LP constraints current decision list branch.

Let + = + FactoredLP(C, b + 0 , O).
Let = FactoredLP(C, b + 0 , O).
//Update indicator functions.

Let Ij (x) = 1(x = tj ) update indicators = Ij .
//We obtain new set weights solving LP, corresponds
max-norm projection.

Let w(t+1) solution linear program: minimize , subject
constraints {+ , }.
Let = + 1.
BellmanErr(Hw(t) ) tmax w(t1) = w(t) .
Return w(t) .

Figure 9: Factored approximate policy iteration max-norm projection algorithm.

432

fiEfficient Solution Algorithms Factored MDPs

{X1 , . . . , Xn }Ti , computes maximum states consistent Ti = ti .
guarantee second condition, ensure impose constraints
states associated previous decisions. achieved adding indicators Ij
previous decision tj , weight . specifically, Ij function takes value
states consistent tj zero assignments Tj . constraints
ith branch form:
R(x, ai ) +

X

wl (gl (x, ai ) h(x)) +

X

1(x = tj ),

x [ti ],

(21)

j<i

l

x [ti ] defines assignments X consistent ti . introduction
indicators causes constraints associated ti trivially satisfied states Sj
j < i. Note indicators restricted-scope function Tj
handled fashion terms factored LP. Thus, decision
list size L, factored LP contains constraints 2L cost networks. complete
approximate policy iteration max-norm projection algorithm outlined Figure 9.
6.4 Comparisons
instructive compare max-norm policy iteration algorithm L2 -projection
policy iteration algorithm Koller Parr (2000) terms computational costs per
iteration implementation complexity. Computing L2 projection requires (among
things) series dot product operations basis functions backprojected
basis functions hhi gj i. expressions easy compute P refers transition
model particular action a. However, policy represented decision list,
result policy improvement step, step becomes much complicated.
particular, every branch decision list, every pair basis functions j,
assignment variables Scope[hi ] Scope[gja ], requires solution
counting problem ]P -complete general. Although Koller Parr show
computation performed using Bayesian network (BN) inference, algorithm
still requires BN inference one assignments branch decision
list. makes algorithm difficult implement efficiently practice.
max-norm projection, hand, relies solving linear program every
iteration. size linear program depends cost networks generated.
discuss, two cost networks needed point decision list. complexity
cost networks approximately one BN inferences
counting problem L2 projection. Overall, branch decision
list, total two inferences, opposed one assignment
Scope[hi ] Scope[gja ] every pair basis functions j. Thus, max-norm policy
iteration algorithm substantially less complex computationally approach based
L2 -projection. Furthermore, use linear programming allows us rely existing
LP packages (such CPLEX), highly optimized.
interesting compare approximate policy iteration algorithm approximate linear programming algorithm presented Section 5. approximate
linear programming algorithm, never need compute decision list policy.
policy always represented implicitly Qa functions. Thus, algorithm
433

fiGuestrin, Koller, Parr & Venkataraman

require explicit computation manipulation greedy policy. difference two
important consequences: one computational terms generality.
First, compute consider decision lists makes approximate linear
programming faster easier implement. algorithm, generate single LP
one cost network action never need compute decision list policy.
hand, iteration, approximate policy iteration needs generate two LPs
every branch decision list size L, usually significantly longer |A|,
total 2L cost networks. terms representation, require policies
compact; thus, need make default action assumption. Therefore,
approximate linear programming algorithm deal general class problems,
action independent DBN transition model. hand,
described Section 3.2, approximate policy iteration stronger guarantees terms
error bounds. differences highlighted experimental results
presented Section 9.

7. Computing Bounds Policy Quality
presented two algorithms computing approximate solutions factored MDPs.
b w
b
algorithms generate linear value functions denoted Hw,
resulting basis function weights. practice, agent define behavior
b One issue remains
b = Greedy(Hw).
acting according greedy policy

b compares true optimal policy ; is, actual value Vb
policy
policy
b compares V .

Section 3, showed priori bounds quality policy. Another
possible procedure compute posteriori bound. is, given resulting weights
b compute bound loss acting according greedy policy
b rather
w,
optimal policy. achieved using Bellman error analysis Williams
Baird (1993).
Bellman error defined BellmanErr(V) = kT V Vk . Given greedy
b = Greedy(V), analysis provides bound:
policy


V V 2BellmanErr(V) .
b

1

(22)

b evaluate quality resulting
Thus, use Bellman error BellmanErr(Hw)
greedy policy.
Note computing Bellman error involves maximization state space.
Thus, complexity computation grows exponentially number state
variables. Koller Parr (2000) suggested structure factored MDP
exploited compute Bellman error efficiently. Here, show error bound
computed set cost networks using similar construction one maxb represented
norm projection algorithms. technique used
decision list depend algorithm used determine policy. Thus,
apply technique solutions determined approximate linear programming
action descriptions permit decision list representation policy.
b Bellman error given by:
set weights w,
434

fiEfficient Solution Algorithms Factored MDPs

b
FactoredBellmanErr (P , R, , H, O, w)
//P factored transition model.
//R set factored reward functions.
// discount factor.
//H set basis functions H = {h1 , . . . , hk }.
//O stores elimination order.
//w
b weights linear value function.
//Return Bellman error value function Hw.
b
//Cache backprojections basis functions.

basis function hi H; action a:
Let gia = Backproja (hi ).
//Compute decision list policy value function
Hw.
b

b = DecisionListPolicy(Ra + P w
Let
bi gi ).
//Initialize indicators.

Let = {}.
//Initialize Bellman error.

Let = 0.
//For every branch decision list policy, generate relevant cost networks, solve
variable elimination, update indicators constraint state space future branches.

b
branch htj , aj decision list policy :

//Instantiate variables Tj assignment given tj .




Instantiate set functions {w
b1 (h1 g1 j ), . . . , w
bk (hk gk j )}
partial state assignment tj store C.
Instantiate target functions Raj partial state assignment
tj store b.
Instantiate indicator functions partial state assignment
tj store 0 .
//Use variable elimination solve first cost network, update Bellman error, error
branch larger.

Let = max (, VariableElimination(C b + 0 , O)).
//Use variable elimination solve second cost network, update Bellman error, error
branch larger.

Let = max (, VariableElimination(C + b + 0 , O)).
//Update indicator functions.

Let Ij (x) = 1(x = tj ) update indicators = Ij .
Return .
b
Figure 10: Algorithm computing Bellman error factored value function Hw.

435

fiGuestrin, Koller, Parr & Venkataraman

b
b Hwk
b ;
BellmanErr(Hw)
= kT Hw


= max

P

P

P

maxx wi hi (x) Rb (x) x0 Pb (x0 | x) j wj hj (x0 ) ,
P
P
P
maxx Rb (x) + x0 Pb (x0 | x) j wj hj (x0 ) wi hi (x)

!

rewards Rb transition model Pb factored appropriately,
compute one two maximizations (maxx ) using variable elimination cost
b decision list policy
network described Section 4.2.1. However,
induce factored transition model. Fortunately, approximate policy iteration
algorithm Section 6, exploit structure decision list perform
maximization efficiently. particular, approximate policy iteration, generate
two cost networks branch decision list. guarantee maximization
performed states branch relevant, include type
indicator functions, force irrelevant states value , thus guaranteeing point decision list policy obtain corresponding state
maximum error. state overall largest Bellman error maximum
ones generated point decision list policy. complete factored
algorithm computing Bellman error outlined Figure 10.
One last interesting note concerns approximate policy iteration algorithm maxnorm projection Section 6. experiments, algorithm converged,
w(t) = w(t+1) iterations. convergence occurs, objective function
(t+1) linear program last iteration equal Bellman error final
policy:
Lemma 7.1 approximate policy iteration max-norm projection converges,
w(t) = w(t+1) iteration t, max-norm projection error (t+1) last
b = Hw(t) :
iteration equal Bellman error final value function estimate Hw
b = (t+1) .
BellmanErr(Hw)

Proof: See Appendix A.4.
Thus, bound loss acting according final policy (t+1) substituting
(t+1)

Bellman error bound:
Corollary 7.2 approximate policy iteration max-norm projection converges
b associated greedy policy
b =
iterations final value function estimate Hw
b
b instead optimal policy
Greedy(Hw),
loss acting according
bounded by:
(t+1)


V V 2
,
b

1

b.
Vb actual value policy

Therefore, approximate policy iteration converges obtain bound
quality resulting policy without needing compute Bellman error explicitly.
436

.

fiEfficient Solution Algorithms Factored MDPs

8. Exploiting Context-specific Structure
Thus far, presented suite algorithms exploit additive structure
reward basis functions sparse connectivity DBN representing transition
model. However, exists another important type structure
exploited efficient decision making: context-specific independence (CSI). example,
consider agent responsible building maintaining house, painting task
completed plumbing electrical wiring installed,
probability painting done 0, contexts plumbing electricity
done, independently agents action. representation used far
paper would use table represent type function. table exponentially
large number variables scope function, ignores context-specific
structure inherent problem definition.
Boutilier et al. (Boutilier et al., 1995; Dearden & Boutilier, 1997; Boutilier, Dean, &
Hanks, 1999; Boutilier et al., 2000) developed set algorithms exploit CSI
transition reward models perform efficient (approximate) planning. Although
approach often successful problems value function contains sufficient
context-specific structure, approach able exploit additive structure
often present real-world problems.
section, extend factored MDP model include context-specific structure.
present simple, yet effective extension algorithms exploit CSI
additive structure obtain efficient approximations factored MDPs. first extend
factored MDP representation include context-specific structure show
basic operations Section 4 required algorithms performed efficiently
new representation.
8.1 Factored MDPs Context-specific Additive Structure
several representations context-specific functions. common
decision trees (Boutilier et al., 1995), algebraic decision diagrams (ADDs) (Hoey, St-Aubin,
Hu, & Boutilier, 1999), rules (Zhang & Poole, 1999). choose use rules
basic representation, two main reasons. First, rule-based representation allows
fairly simple algorithm variable elimination, key operation framework.
Second, rules required mutually exclusive exhaustive, requirement
restrictive want exploit additive independence, functions
represented linear combination set non-mutually exclusive functions.
begin describing rule-based representation (along lines Zhang
Pooles presentation (1999)) probabilistic transition model, particular, CPDs
DBN model. Roughly speaking, rule corresponds set CPD entries
associated particular probability value. entries
value referred consistent contexts:
Definition 8.1 Let C {X, X0 } c Dom(C). say c consistent
b Dom(B), B {X, X0 }, c b assignment variables
C B.
probability consistent contexts represented probability rules:
437

fiGuestrin, Koller, Parr & Venkataraman

Electrical

Electrical

Done

done

Done

done

Plumbing

P(Painting) = 0

Plumbing

P(Painting) = 0
done

Done

done

Painting

P(Painting) = 0

Done

done

P(Painting) = 0

P(Painting) = 0.95

P(Painting) = 0

(a)

Done
P(Painting) = 0.9

(b)

4 = hElectrical : 0i
5 = hElectrical Plumbing : 0i
6 = hElectrical Plumbing Painting : 0i
7 = hElectrical Plumbing Painting : 0.9i
(d)

1 = hElectrical : 0i
2 = hElectrical Plumbing : 0i
3 = hElectrical Plumbing : 0.95i
(c)

Figure 11: Example CPDs variable Painting = true represented decision trees:
(a) action paint; (b) action paint. CPDs
represented probability rules shown (c) (d), respectively.

Definition 8.2 probability rule = hc : pi function : {X, X0 } 7 [0, 1],
context c Dom(C) C {X, X0 } p [0, 1], (x, x0 ) = p (x, x0 )
consistent c equal 1 otherwise.
case, convenient require rules mutually exclusive exhaustive, CPD entry uniquely defined association single rule.
Definition 8.3 rule-based conditional probability distribution (rule CPD) Pa function Pa : ({Xi0 } X) 7 [0, 1], composed set probability rules {1 , 2 , . . . , } whose
contexts mutually exclusive exhaustive. define:
Pa (x0i | x) = j (x, x0 ),
j unique rule Pa cj consistent (x0i , x). require that,
x,
X
Pa (x0i | x) = 1.
x0i

define Parentsa (Xi0 ) union contexts rules Pa (Xi0 | X).
example CPD represented set probability rules shown Figure 11.
Rules used represent additive functions, reward basis functions.
represent context specific value dependencies using value rules:
438

fiEfficient Solution Algorithms Factored MDPs

Definition 8.4 value rule = hc : vi function : X 7 R (x) = v
x consistent c 0 otherwise.
Note value rule hc : vi scope C.
important note value rules required mutually exclusive
exhaustive. value rule represents (weighted) indicator function, takes
value v states consistent context c, 0 states. given state,
values zero rules consistent state simply added together.
Example 8.5 construction example, might set rules:
1 = hPlumbing = done : 100i;
2 = hElectricity = done : 100i;
3 = hPainting = done : 100i;
4 = hAction = plumb : 10i;
..
.
which, summed together, define reward function R = 1 + 2 + 3 + 4 + .
general, reward function Ra represented rule-based function:
Definition 8.6 rule-based function f : X 7 R composed set rules {1 , . . . , n }
P
f (x) = ni=1 (x).
manner, one basis functions hj represented rule-based
function.
notion rule-based function related tree-structure functions used
Boutilier et al. (2000), substantially general. tree-structure value functions, rules corresponding different leaves mutually exclusive exhaustive.
Thus, total number different values represented tree equal number
leaves (or rules). rule-based function representation, rules mutually
exclusive, values added form overall function value different settings
variables. Different rules added different settings, and, fact, k rules,
one easily generate 2k different possible values, demonstrated Section 9. Thus,
rule-based functions provide compact representation much richer class
value functions.
Using rule-based representation, exploit CSI additive independence
representation factored MDP basis functions. show basic
operations Section 4 adapted exploit rule-based representation.
8.2 Adding, Multiplying Maximizing Consistent Rules
table-based algorithms, relied standard sum product operators applied
tables. order exploit CSI using rule-based representation, must redefine
standard operations. particular, algorithms need add multiply rules
ascribe values overlapping sets states.
start defining operations rules context:
439

fiGuestrin, Koller, Parr & Venkataraman

Definition 8.7 Let 1 = hc : v1 2 = hc : v2 two rules context c. Define
rule product 1 2 = hc : v1 v2 i, rule sum 1 + 2 = hc : v1 + v2 i.
Note definition restricted rules context. address
issue moment. First, introduce additional operation maximizes
variable set rules, otherwise share common context:
Definition 8.8 Let variable Dom[Y ] = {y1 , . . . , yk }, let , =
1, . . . , k, rule form = hc = yi : vi i. rule-based function
f = 1 + + k , define rule maximization maxY f = hc : maxi vi .
operation, maximized scope function f .
three operations described applied sets rules
satisfy stringent conditions. make set rules amenable application
operations, might need refine rules. therefore define
following operation:
Definition 8.9 Let = hc : vi rule, variable. Define rule split
Split(6 ) variable follows: Scope[C], Split(6 ) = {};
otherwise,
Split(6 ) = {hc = yi : vi | yi Dom[Y ]} .
Thus, split rule variable scope context ,
generate new set rules, one assignment domain .
general, purpose rule splitting extend context c one rule coincide
context c0 another consistent rule 0 . Naively, might take variables
Scope[C0 ] Scope[C] split recursively one them. However, process
creates unnecessarily many rules: variable Scope[C0 ] Scope[C] split
, one |Dom[Y ]| new rules generated remain consistent 0 :
one assignment one c0 . Thus, consistent rule
needs split further. define recursive splitting procedure achieves
parsimonious representation:
Definition 8.10 Let = hc : vi rule, b context b Dom[B].
Define recursive rule split Split(6 b) context b follows:
1. {}, c consistent b; else,
2. {}, Scope[B] Scope[C]; else,
3. {Split(i 6 b) | Split(6 )}, variable Scope[B] Scope[C] .

definition, variable Scope[B] Scope[C] leads generation k =
|Dom(Y )| rules step split. However, one k rules used
next recursive step one consistent b. Therefore, size
P
split set simply 1 + Scope[B]Scope[C] (|Dom(Y )| 1). size independent
order variables split within operation.
440

fiEfficient Solution Algorithms Factored MDPs

Note one rules Split(6 b) consistent b: one context
c b. Thus, want add two consistent rules 1 = hc1 : v1 2 = hc2 : v2 i,
need replace rules set:
Split(1 6 c2 ) Split(2 6 c1 ),
simply replace resulting rules hc1 c2 : v1 hc2 c1 : v2 sum
hc1 c2 : v1 + v2 i. Multiplication performed analogous manner.
Example 8.11 Consider adding following set consistent rules:
1 = ha b : 5i,
2 = ha c : 3i.
rules, context c1 1 b, context c2 2 c d.
Rules 1 2 consistent, therefore, must split perform addition
operation:


ha b c : 5i,
ha b c : 5i,
Split(1 6 c2 ) =

ha b c : 5i.
Likewise,

(

Split(2 6

c1 ) =

ha b c : 3i,
ha b c : 3i.

result adding rules 1 2
ha b c : 5i,
ha b c : 5i,
ha b c : 8i,
ha b c : 3i.

8.3 Rule-based One-step Lookahead
Using compact rule-based representation, able compute one-step lookahead
plan efficiently models significant context-specific additive independence.
Section 4.1 table-based case, rule-based Qa function represented
sum reward function discounted expected value next state.
Due linear approximation value function, expectation term is, turn,
represented linear combination backprojections basis functions.
exploit CSI, representing rewards basis functions rule-based functions.
represent Qa rule-based function, sufficient us show represent
backprojection gj basis function hj rule-based function.
P (h )
hj rule-based
function,
written hj (x) = j (x),

E
(h )
(h )
(h )
j form ci j : vi j . rule restricted scope function; thus,
simplify backprojection as:
441

fiGuestrin, Koller, Parr & Venkataraman

RuleBackproja () ,

given hc : vi, c Dom[C].

Let g = {}.
Select set P relevant probability rules:
P = {j P (Xi0 | Parents(Xi0 )) | Xi0 C c consistent cj }.
Remove X0 assignments context rules P.
// Multiply consistent rules:
two consistent rules 1 = hc1 : p1 2 = hc2 : p2 i:
c1 = c2 , replace two rules hc1 : p1 p2 i;
Else replace two rules set: Split(1 6 c2 ) Split(2 6 c1 ).
// Generate value rules:
rule P:
Update backprojection g = g {hci : pi vi}.
Return g.

Figure 12: Rule-based backprojection.
gja (x) =

X

Pa (x0 | x)hj (x0 ) ;

x0

=

X

Pa (x0 | x)

x0

=

XX


=

X (hj )



(x0 );


(hj )

0

Pa (x | x)i

(x0 );

x0

X (hj )

vi

(hj )

Pa (ci

| x);


(h )

(h )

term vi j Pa (ci j | x) written rule function. denote back(h )
projection operation RuleBackproja (i j ).
backprojection procedure, described Figure 12, follows three steps. First,
relevant rules selected: CPDs variables appear context ,
select rules consistent context, rules play role
backprojection computation. Second, multiply consistent probability rules
form local set mutually-exclusive rules. procedure analogous addition
procedure described Section 8.2. represented probabilities
affect mutually-exclusive set, simply represent backprojection
product probabilities value . is, backprojection
rule-based function one rule one mutually-exclusive probability rules
. context new value rule , value product
probability value .
Example 8.12 example, consider backprojection simple rule,
= h Painting = done : 100i,
CPD Figure 11(c) paint action:
RuleBackprojpaint () =

X

Ppaint (x0 | x)(x0 );

x0

442

fiEfficient Solution Algorithms Factored MDPs

X

=

Ppaint (Painting0 | x)(Painting0 );

Painting0

= 100

3


(Painting = done, x) .

i=1

Note product simple rules equivalent decision tree CPD shown
Figure 11(a). Hence, product equal 0 contexts, example, electricity
done time t. product non-zero one context: context associated
rule 3 . Thus, express result backprojection operation rule-based
function single rule:
RuleBackprojpaint () = hPlumbing Electrical : 95i.
Similarly, backprojection action paint represented
single rule:
RuleBackprojpaint () = hPlumbing Electrical Painting : 90i.
Using algorithm, write backprojection rule-based basis function hj as:
gja (x) =

X

(hj )

RuleBackproja (i

),

(23)



gja sum rule-based functions, therefore rule-based function.
simplicity notation, use gja = RuleBackproja (hj ) refer definition backproP
jection. Using notation, write Qa (x) = Ra (x) + j wj gja (x),
rule-based function.
8.4 Rule-based Maximization State Space
second key operation required extend planning algorithms exploit CSI
modify variable elimination algorithm Section 4.2.1 handle rule-based representation. Section 4.2.1, showed maximization linear combination
table-based functions restricted scope performed efficiently using non-serial
dynamic programming (Bertele & Brioschi, 1972), variable elimination. exploit structure rules, use algorithm similar variable elimination Bayesian network
context-specific independence (Zhang & Poole, 1999).
Intuitively, algorithm operates selecting value rules relevant variable
maximized current iteration. Then, local maximization performed
subset rules, generating new set rules without current variable.
procedure repeated recursively variables eliminated.
precisely, algorithm eliminates variables one one, elimination process performs maximization step variables domain. Suppose
eliminating Xi , whose collected value rules lead rule function f , f involves
additional variables set B, f scope B {Xi }. need compute
maximum value Xi choice b Dom[B]. use MaxOut (f, Xi ) denote procedure takes rule function f (B, Xi ) returns rule function g(B)
443

fiGuestrin, Koller, Parr & Venkataraman

MaxOut (f, B)
Let g = {}.
Add completing rules f : hB = bi : 0i, = 1, . . . , k.
// Summing consistent rules:
two consistent rules 1 = hc1 : v1 2 = hc2 : v2 i:
c1 = c2 , replace two rules hc1 : v1 + v2 i;
Else replace two rules set: Split(1 6 c2 ) Split(2 6 c1 ).
// Maximizing variable B:
Repeat f empty:
rules hc B = bi : vi i, bi Dom(B) :
remove rules f add rule hc : maxi vi g;
Else select two rules: = hci B = bi : vi j = hcj B = bj : vj
ci consistent cj , identical, replace
Split(i 6 cj ) Split(j 6 ci ) .
Return g.

Figure 13: Maximizing variable B rule function f .
that: g(b) = maxxi f (b, xi ). procedure extension variable elimination
algorithm Zhang Poole (Zhang & Poole, 1999).
rule-based variable elimination algorithm maintains set F value rules, initially
containing set rules maximized. algorithm repeats following steps
variable Xi variables eliminated:
1. Collect rules depend Xi = {hc : vi F | Xi C}
remove rules F.
2. Perform local maximization step Xi : gi = MaxOut (fi , Xi );
3. Add rules gi F; now, Xi eliminated.
cost algorithm polynomial number new rules generated
maximization operation MaxOut (fi , Xi ). number rules never larger many
cases exponentially smaller complexity bounds table-based maximization
Section 4.2.1, which, turn, exponential induced width cost network
graph (Dechter, 1999). However, computational costs involved managing sets rules
usually imply computational advantage rule-based approach tablebased one significant problems possess fair amount context-specific
structure.
remainder section, present algorithm computing local
maximization MaxOut (fi , Xi ). next section, show ideas applied
extending algorithm Section 4.2.2 exploit CSI LP representation
planning factored MDPs.
procedure, presented Figure 13, divided two parts: first, consistent
rules added together described Section 8.2; then, variable B maximized.
maximization performed generating set rules, one assignment B, whose
contexts assignment variables except B, Definition 8.8.
set substituted single rule without B assignment context value
equal maximum values rules original set. Note that, simplify
444

fiEfficient Solution Algorithms Factored MDPs

algorithm, initially need add set value rules 0 value, guarantee
rule function f complete (i.e., least one rule consistent every
context).
correctness procedure follows directly correctness rule-based
variable elimination procedure described Zhang Poole, merely replacing summations product max, products products sums. conclude
section small example illustrate algorithm:
Example 8.13 Suppose maximizing following set rules:
1
2
3
4

= ha : 1i,
= ha b : 2i,
= ha b c : 3i,
= ha b : 1i.

add completing rules, get:
5 = ha : 0i,
6 = ha : 0i.
first part algorithm, need add consistent rules: add 5 1 (which
remains unchanged), combine 1 4 , 6 2 , split 6 context
3 , get following inconsistent set rules:
2
3
7
8
9

= ha b : 2i,
= ha b c : 3i,
= ha b : 2i,
(from adding 4 consistent rule Split(1 6 b))
= ha b : 1i,
(from Split(1 6 b))
= ha b c : 0i,
(from Split(6 6 b c)).

Note several rules value 0 generated, shown
added rules consistent contexts. move second stage (repeat loop)
MaxOut. remove 2 , 8 , maximize them, give:
10 = hb : 2i.
select rules 3 7 split 7 c (3 split empty set
changed),
11 = ha b c : 2i,
12 = ha b c : 2i.
Maximizing rules 12 3 , get:
13 = hb c : 3i.
left 11 , maximized counterpart 9 gives
12 = hb c : 2i.
Notice that, throughout maximization, split variable C b ci ,
giving us 6 distinct rules final result. possible table-based
representation, since functions would 3 variables a,b,c, therefore
must 8 entries.
445

fiGuestrin, Koller, Parr & Venkataraman

8.5 Rule-based Factored LP
Section 4.2.2, showed LPs used algorithms exponentially many
P
constraints form: wi ci (x) b(x), x, substituted single,
P
equivalent, non-linear constraint: maxx wi ci (x) b(x). showed that, using
variable elimination, represent non-linear constraint equivalent set
linear constraints construction called factored LP. number constraints
factored LP linear size largest table generated variable elimination
procedure. table-based algorithm exploit additive independence.
extend algorithm Section 4.2.2 exploit additive context-specific structure,
using rule-based variable elimination described previous section.
Suppose wish enforce general constraint 0 maxy F w (y), F w (y) =
P w
j fj (y) fj rule. table-based version, superscript w means
fj might depend w. Specifically, fj comes basis function hi , multiplied
weight wi ; fj rule reward function, not.
rule-based factored linear program, generate LP variables associated
contexts; call LP rules. LP rule form hc : ui; associated
context c variable u linear program. begin transforming original
rules fjw LP rules follows: rule fj form hcj : vj comes basis
function hi , introduce LP rule ej = hcj : uj equality constraint uj = wi vj .
fj form comes reward function, introduce LP rule
form, equality constraint becomes uj = vj .
P
Now, LP rules need represent constraint: 0 maxy j ej (y).
represent constraint, follow algorithm similar variable elimination procedure Section 8.4. main difference occurs MaxOut (f, B) operation
Figure 13. Instead generating new value rules, generate new LP rules, associated
new variables new constraints. simplest case occurs computing split
adding two LP rules. example, add two value rules original algorithm,
instead perform following operation associated LP rules: LP rules
hc : ui hc : uj i, replace new rule hc : uk i, associated new LP
variable uk context c, whose value ui + uj . enforce value constraint,
simply add additional constraint LP: uk = ui + uj . similar procedure
followed computing split.
interesting constraints generated perform maximization.
rule-based variable elimination algorithm Figure 13, maximization occurs
replace set rules:
hc B = bi : vi i, bi Dom(B),
new rule





c : max vi .


Following process LP rule summation above, maximizing
ei = hc B = bi : ui i, bi Dom(B),
generate new LP variable uk associated rule ek = hc : uk i. However,
cannot add nonlinear constraint uk = maxi ui , add set equivalent linear
446

fiEfficient Solution Algorithms Factored MDPs

constraints
uk ui , i.
Therefore, using simple operations, exploit structure rule functions
P
represent nonlinear constraint en maxy j ej (y), en last LP
rule generate. final constraint un = implies representing exactly
constraints Equation (12), without enumerate every state.
correctness rule-based factored LP construction corollary Theorem 4.4
correctness rule-based variable elimination algorithm (Zhang & Poole,
1999) .
Corollary 8.14 constraints generated rule-based factored LP construction
equivalent non-linear constraint Equation (12). is, assignment (, w)
satisfies rule-based factored LP constraints satisfies constraint
Equation (12).
number variables constraints rule-based factored LP linear
number rules generated variable elimination process. turn, number rules
larger, often exponentially smaller, number entries table-based
approach.
illustrate generation LP constraints described, present small
example:
Example 8.15 Let e1 , e2 , e3 , e4 set LP rules depend variable
b maximized. Here, rule ei associated LP variable ui :
e1
e2
e3
e4

= ha b : u1 i,
= ha b c : u2 i,
= ha b : u3 i,
= ha b c : u4 i.

set, note rules e1 e2 consistent. combine generate
following rules:
e5 = ha b c : u5 i,
e6 = ha b c : u1 i.
constraint u1 + u2 = u5 . Similarly, e6 e4 may combined, resulting in:
e7 = ha b c : u6 i.
constraint u6 = u1 + u4 . Now, following three inconsistent rules
maximization:
e3 = ha b : u3 i,
e5 = ha b c : u5 i,
e7 = ha b c : u6 i.
Following maximization procedure, since pair rules eliminated right away,
split e3 e5 generate following rules:
e8 = ha b c : u3 i,
e9 = ha b c : u3 i,
e5 = ha b c : u5 i.
447

fiGuestrin, Koller, Parr & Venkataraman

maximize b e8 e5 , resulting following rule constraints
respectively:
e10 = ha c : u7 i,
u7 u5 ,
u7 u3 .
Likewise, maximizing b e9 e6 , get:
e11 = ha c : u8 i,
u8 u3 ,
u8 u6 ;
completes elimination variable b rule-based factored LP.
presented algorithm exploiting additive context-specific structure LP construction steps planning algorithms. rule-based factored LP
approach applied directly approximate linear programming approximate policy iteration algorithms, presented Sections 5 6.
additional modification required concerns manipulation decision
list policies presented Section 6.2. Although approximate linear programming
require explicit policy representation (or default action model), approximate policy iteration require us represent policy. Fortunately, major modifications
required rule-based case. particular, conditionals hti , ai , decision
list policies already context-specific rules. Thus, policy representation algorithm
Section 6.2 applied directly new rule-based representation. Therefore,
complete framework exploiting additive context-specific structure
efficient planning factored MDPs.

9. Experimental Results
factored representation value function appropriate certain types
systems: Systems involve many variables, strong interactions
variables fairly sparse, decoupling influence variables
induce unacceptable loss accuracy. argued Herbert Simon (1981)
Architecture Complexity, many complex systems nearly decomposable,
hierarchical structure, subsystems interacting weakly themselves.
evaluate algorithm, selected problems believe exhibit type structure.
section, perform various experiments intended explore performance
algorithms. First, compare factored approximate linear programming (LP)
approximate policy iteration (PI) algorithms. compare L2 -projection
algorithm Koller Parr (2000). second evaluation compares table-based implementation rule-based implementation exploit CSI. Finally, present
comparisons approach algorithms Boutilier et al. (2000).
9.1 Approximate LP Approximate PI
order compare approximate LP approximate PI algorithms, tested
SysAdmin problem described detail Section 2.1. problem relates system
448

fiEfficient Solution Algorithms Factored MDPs

administrator maintain network computers; experimented various
network architectures, shown Figure 1. Machines fail randomly, faulty machine
increases probability neighboring machines fail. every time step,
SysAdmin go one machine reboot it, causing working next time
step high probability. Recall state space problem grows exponentially
number machines network, is, problem machines 2m states.
machine receives reward 1 working (except ring, one machine
receives reward 2, introduce asymmetry), zero reward given faulty
machines, discount factor = 0.95. optimal strategy rebooting machines
depend upon topology, discount factor, status machines
network. machine machine j faulty, benefit rebooting must
weighed expected discounted impact delaying rebooting j js successors.
topologies rings, policy may function status every single
machine network.
basis functions used included independent indicators machine, value
1 working zero otherwise (i.e., one restricted scope function single
variable), constant basis, whose value 1 states. selected straightforward
variable elimination orders: Star Three Legs topologies, first eliminated
variables corresponding computers legs, center computer (server)
eliminated last; Ring, started arbitrary computer followed ring
order; Ring Star, ring machines eliminated first center one;
finally, Ring Rings topology, eliminated computers outer rings
first ones inner ring.
implemented factored policy iteration linear programming algorithms
Matlab, using CPLEX LP solver. Experiments performed Sun UltraSPARCII, 359 MHz 256MB RAM. evaluate complexity approximate policy
iteration max-norm projection algorithm, tests performed increasing
number states, is, increasing number machines network. Figure 14 shows
running time increasing problem sizes, various architectures. simplest one
Star, backprojection basis function scope restricted two
variables largest factor cost network scope restricted two variables.
difficult one Bidirectional Ring, factors contain five variables.
Note number states growing exponentially (indicated log scale
Figure 14), running times increase logarithmically number states,
polynomially number variables. illustrate behavior Figure 14(d),
fit 3rd order polynomial running times unidirectional ring. Note
size problem description grows quadratically number variables: adding
machine network adds possible action fixing machine.
problem,
computation
cost factored algorithm empirically grows approximately


(n |A|)1.5 , problem n variables, opposed exponential complexity
poly (2n , |A|) explicit algorithm.
evaluation, measured error approximate value function relative
true optimal value function V . Note possible compute V small
problems; case, able go 10 machines. comparison,
evaluated error approximate value function produced L2 -projection
449

fiGuestrin, Koller, Parr & Venkataraman

500

400

Ring
3 Legs

300

Ring Rings

300
Total Time (minutes)

Total Time (minutes)

400

Star

200

Ring Star
200

100

100

0

0
1E+00

1E+02

1E+04

1E+06 1E+08 1E+10
number states

1E+12

1

1E+14

100

10000
1000000
number states

(a)
1200

500

1000

Fitting polynomial:

800

time = 0.0184|X| - 0.6655|X| +
9.2499|X| - 31.922

3

Ring:

Total Time (minutes)

Total Time (minutes)

1E+10

(b)

600

400

100000000

Unidirectional
Bidirectional

300
200

2

2

Quality fit: R = 0.999
600

400

100

200
0
1E+00

1E+02

1E+04

1E+06

1E+08

1E+10

1E+12

1E+14

0
0

number state

(c)

10

20
30
40
number variables |X|

50

60

(d)

Figure 14: (a)(c) Running times policy iteration max-norm projection variants
SysAdmin problem; (d) Fitting polynomial running time
Ring topology.

algorithm Koller Parr (2000). discussed Section 6.4, L2 projections
factored MDPs Koller Parr difficult time consuming; hence,
able compare two algorithms smaller problems, equivalent L2 -projection
implemented using explicit state space formulation. Results algorithms
presented Figure 15(a), showing relative error approximate solutions
true value function increasing problem sizes. results indicate that, larger
problems, max-norm formulation generates better approximation true optimal
value function V L2 -projection. Here, used two types basis functions:
single variable functions, pairwise basis functions. pairwise basis functions
contain indicators neighboring pairs machines (i.e., functions two variables).
expected, use pairwise basis functions resulted better approximations.
450

fiEfficient Solution Algorithms Factored MDPs

0.4

Max norm, single basis
L2, single basis

0.3

Bellman Error / Rmax

Max norm, pair basis
L2, pair basis

Relative error:

0.2

0.1

0
3

4

5

6

7

8

9

10

number variables

0.3

0.2

Ring
3 Legs

0.1

0
1E+00

Star

1E+02

1E+04

1E+06

1E+08

1E+10

1E+12

1E+14

numbe r sta tes

(a)

(b)

Figure 15: (a) Relative error optimal value function V comparison L2 projection
Ring; (b) large models, measuring Bellman error convergence.

small problems, compare actual value policy generated
algorithm value optimal policy. Here, value policy generated
algorithm much closer value optimal policy error implied
difference approximate value function V . example, Star
architecture one server 6 clients, approximation single variable
basis functions relative error 12%, policy generated value
optimal policy. case, true policy generated L2
projection. Unidirectional Ring 8 machines pairwise basis, relative
error approximation V 10%, resulting policy
6% loss optimal policy. problem, L2 approximation value
function error 12%, true policy loss 9%. words, methods induce
policies lower errors errors approximate value function (at least
small problems). However, algorithm continues outperform L2 algorithm,
even respect actual policy loss.
large models, longer compute correct value function, cannot
evaluate results computing kV Hwk . Fortunately, discussed Section 7,
Bellman error used provide bound approximation error
computed efficiently exploiting problem-specific structure. Figure 15(b) shows
Bellman error increases slowly number states.
valuable look actual decision-list policies generated experiments.
First, noted lists tended short, length final decision list policy
grew approximately linearly number machines. Furthermore, policy
often fairly intuitive. Ring Star architecture, example, decision list
says: server faulty, fix server; else, another machine faulty, fix it.
Thus far, presented scaling results running times approximation error
approximate PI approach. compare algorithm simpler approximate
451

fiGuestrin, Koller, Parr & Venkataraman

400

200

PI single basis
PI single basis

160

LP single basis

140

LP pair basis

120

LP triple basis

Discounted reward final policy
(averaged 50 trials 100 steps)

Total running time (minutes)

180

100
80
60
40
20
0
0

5

10

15

20

25

30

35

numbe r machine

LP single basis
LP pair basis

300

LP triple basis

200

100

0
0

10

20

30

40

numbe r machine

(a)

(b)

Figure 16: Approximate LP versus approximate PI SysAdmin problem Ring
topology: (a) running time; (b) estimated value policy.

LP approach Section 5. shown Figure 16(a), approximate LP algorithm
factored MDPs significantly faster approximate PI algorithm. fact, approximate PI single-variable basis functions variables costly computationally
LP approach using basis functions consecutive triples variables. shown
Figure 16(b), singleton basis functions, approximate PI policy obtains slightly better
performance problem sizes. However, increase number basis functions
approximate LP formulation, value resulting policy much better. Thus,
problem, factored approximate linear programming formulation allows us use
basis functions obtain resulting policy higher value, still maintaining
faster running time. results, along simpler implementation, suggest
practice one may first try apply approximate linear programming algorithm
deciding move elaborate approximate policy iteration approach.
9.2 Comparing Table-based Rule-based Implementations
next evaluation compares table-based representation, exploits additive
independence, rule-based representation presented Section 8, exploit
additive context-specific independence. experiments, implemented
factored approximate linear programming algorithm table-based rule-based
representations C++, using CPLEX LP solver. Experiments performed
Sun UltraSPARC-II, 400 MHz 1GB RAM.
evaluate compare algorithms, utilized complex extension
SysAdmin problem. problem, dubbed Process-SysAdmin problem, contains three
state variables machine network: Loadi , Statusi Selectori . computer runs processes receives rewards processes terminate. processes
represented Loadi variable, takes values {Idle, Loaded, Success},
computer receives reward assignment Loadi Success. Statusi variable,
452

fitotal running time (minutes)

Efficient Solution Algorithms Factored MDPs

200
Table-based, single+ basis
Rule-based, single+ basis

150

Table-based, pair basis
100

Rule-based, pair basis

50
0
1E+00

1E+07

1E+14

1E+21

1E+28

1E+35

1E+42

number states

total running time (minutes)

(a)
250
200

Table-based, single+ basis
Rule-based, single+ basis

150

Table-based, pair basis
100
Rule-based, pair basis
50
0
1E+00 1E+04 1E+08 1E+12 1E+16 1E+20 1E+24 1E+28
number states

(b)

total running time (minutes)

600
2
(x-1)

(x-1)

= 7E- 17 x * 18 + 2E- 06 x * 18
+ 0.1124
2
R = 0.995

500

Table-based, single+ basis
Rule-based, single+ basis

400
300

3

2

= 0.2294x - 4.5415x +
30.974x - 67.851
R 2= 0.9995

200
100
0
0

5

10
number machines

15

20

(c)
Figure 17: Running time Process-SysAdmin problem various topologies: (a) Star;
(b) Ring; (c) Reverse star (with fit function).

453

fiGuestrin, Koller, Parr & Venkataraman

CPLEX time / Total time

1
0.8
Table-based, single+ basis
0.6
Rule-based, single+ basis
0.4
0.2
0
0

5

10

15

20

number machines

Figure 18: Fraction total running time spent CPLEX Process-SysAdmin problem Ring topology.

representing status machine i, takes values {Good, Faulty, Dead}; value
Faulty, processes smaller probability terminating value Dead,
running process lost Loadi becomes Idle. status machine become Faulty eventually Dead random; however, machine receives packet
dead machine, probability Statusi becomes Faulty Dead increases.
Selectori variable represents communication selecting one neighbors
uniformly random every time step. SysAdmin select one computer
reboot every time step. computer rebooted, status becomes Good
probability 1, running process lost, i.e., Loadi variable becomes Idle.
Thus, problem, SysAdmin must balance several conflicting goals: rebooting
machine kills processes, rebooting machine may cause cascading faults network.
Furthermore, SysAdmin choose one machine reboot, imposes additional tradeoff selecting one (potentially many) faulty dead machines
network reboot.
experimented two types basis functions: single+ includes indicators
joint assignments Loadi , Statusi Selectori , pair which, addition,
includes set indicators Statusi , Statusj , Selectori = j, neighbor j
machine network. discount factor = 0.95. variable elimination
order eliminated Loadi variables first, followed patterns
simple SysAdmin problem, eliminating first Statusi Selectori machine
eliminated.
Figure 17 compares running times table-based implementation ones
rule-based representation three topologies: Star, Ring, Reverse star.
Reverse star topology reverses direction influences Star: rather
central machine influencing machines topology, machines influence
central one. three topologies demonstrate three different levels CSI:
454

fiEfficient Solution Algorithms Factored MDPs

Star topology, factors generated variable elimination small. Thus, although
running times polynomial number state variables methods, tablebased representation significantly faster rule-based one, due overhead
managing rules. Ring topology illustrates intermediate behavior: single+
basis functions induce relatively small variable elimination factors, thus table-based
approach faster. However, pair basis factors larger rule-based
approach starts demonstrate faster running times larger problems. Finally, Reverse star topology represents worst-case scenario table-based approach. Here,
scope backprojection basis function central machine involve
computers network, machines potentially influence central one
next time step. Thus, size factors table-based variable elimination approach exponential number machines network, illustrated
exponential growth Figure 17(c). rule-based approach exploit CSI
problem; example, status central machine Status0 depends machine
j value selector j, i.e., Selector0 = j. exploiting CSI, solve
problem polynomial time number state variables, seen second curve
Figure 17(c).
instructive compare portion total running time spent CPLEX
table-based compared rule-based approach. Figure 18 illustrates
comparison. Note amount time spent CPLEX significantly higher
table-based approach. two reasons difference: first, due CSI, LPs
generated rule-based approach smaller table-based ones; second, rulebased variable elimination complex table-based one, due overhead
introduced rule management. Interestingly, proportion CPLEX time increases
problem size increases, indicating asymptotic complexity LP solution
higher variable elimination, thus suggesting that, larger problems, additional
large-scale LP optimization procedures, constraint generation, may helpful.
9.3 Comparison Apricodd
closely related work line research began work
Boutilier et al. (1995). particular, approximate Apricodd algorithm Hoey et
al. (1999), uses analytic decision diagrams (ADDs) represent value function
strong alternative approach solving factored MDPs. discussed detail Section 10, Apricodd algorithm successfully exploit context-specific structure
value function, representing set mutually-exclusive exhaustive branches
ADD. hand, approach exploit additive context-specific
structure problem, using linear combination non-mutually-exclusive rules.
better understand difference, evaluated rule-based approximate linear
programming algorithm Apricodd two problems, Linear Expon, designed
Boutilier et al. (2000) illustrate respectively best-case worst-case behavior
algorithm. experiments, used web-distributed version Apricodd (Hoey, St-Aubin, Hu, & Boutilier, 2002), running locally Linux Pentium III
700MHz 1GB RAM.
455

fiGuestrin, Koller, Parr & Venkataraman

500

Rule-based

40
30

3

2

= 0.1473x - 0.8595x + 2.5006x - 1.5964
2

R = 0.9997

20

Apricodd
2

= 0.0254x + 0.0363x
+ 0.0725

10

Apricodd

400
Time (in seconds)

Time (in seconds)

50

x

2

x

= 3E-05 * 2 - 0.0026 * 2 + 5.6737
R2 = 0.9999

300
200

Rule-based
= 5.275x3 - 29.95x2 +
53.915x - 28.83
R2 = 1

100

2

R = 0.9983

0

0

6

8

10
12
14
16
Number variables

18

6

20

8

10

12

Number variables

(a)

(b)

Figure 19: Comparing Apricodd rule-based approximate linear programming (a)
Linear (b) Expon problems.

two problems involve n binary variables X1 , . . . , Xn n deterministic actions
a1 , . . . , . reward 1 variables Xk true, 0 otherwise. problem
discounted factor = 0.99. difference Linear Expon
problems transition probabilities. Linear problem, action ak sets
variable Xk true makes succeeding variables, Xi > k, false. state space
Linear problem seen binary number, optimal policy set repeatedly
largest bit (Xk variable) preceding bits set true. Using ADD, optimal
value function problem represented linear space, n+1 leaves (Boutilier
et al., 2000). best-case Apricodd, algorithm compute value
function quite efficiently. Figure 19(a) compares running time Apricodd
one algorithms indicator basis functions pairs consecutive variables.
Note algorithms obtain policy polynomial time number
variables. However, structured problems, efficient implementation ADD
package used Apricodd makes faster problem.
hand, Expon problem illustrates worst-case Apricodd.
problem, action ak sets variable Xk true, preceding variables, Xi < k,
true, makes preceding variables false. state space seen binary number,
optimal policy goes binary numbers sequence, repeatedly setting
largest bit (Xk variable) preceding bits set true. Due discounting,
n
optimal value function assigns value 2 j1 jth binary number,
value function contains exponentially many different values. Using ADD, optimal
value function problem requires exponential number leaves (Boutilier et al.,
2000), illustrated exponential running time Figure 19(b). However,
value function approximated compactly factored linear value
function using n + 1 basis functions: indicator variable Xk constant
base. shown Figure 19(b), using representation, factored approximate linear
programming algorithm computes value function polynomial time. Furthermore,
456

fiEfficient Solution Algorithms Factored MDPs

30

60

Running time (minutes)

Discounted value policy
(avg. 50 runs 100 steps)

Rule-based LP

50

Apricodd
40
30
20
10
0

Rule-based LP

25

Apricodd
20
15
10
5
0

0

2

4
6
8
Number machines

10

12

0

2

4
6
8
Number machines

(a)

12

10

12

(b)

50

30

45

Rule-based LP

Rule-based LP

40

Discounted value policy
(avg. 50 runs 100 steps)

Running time (minutes)

10

Apricodd

35
30
25
20
15
10

25

Apricodd

20
15
10
5

5
0

0
0

2

4
6
8
Number machines

10

12

(c)

0

2

4
6
8
Number machines

(d)

Figure 20: Comparing Apricodd rule-based approximate linear programming single+ basis functions Process-SysAdmin problem Ring topology
(a) running time (b) value resulting policy; Star topology
(c) running time (d) value resulting policy.

policy obtained approach optimal problem. Thus, problem,
ability exploit additive independence allows efficient polynomial time solution.
compared Apricodd rule-based approximate linear programming
algorithm Process-SysAdmin problem. problem significant additive structure reward function factorization transition model. Although type
structure exploited directly Apricodd, ADD approximation steps performed
algorithm can, principle, allow Apricodd find approximate solutions problem. spent significant amount time attempting find best set parameters
Apricodd problems.4 settled sift method variable reordering
round approximation method size (maximum ADD size) criteria.
4. grateful Jesse Hoey Robert St-Aubin assistance selecting parameters.

457

fiGuestrin, Koller, Parr & Venkataraman

allow value function representation scale problem size, set maximum
ADD size 4000 + 400n network n machines. (We experimented variety
different growth rates maximum ADD size; here, parameters,
selected choice gave best results Apricodd.) compared Apricodd
parameters rule-based approximate linear programming algorithm
single+ basis functions Pentium III 700MHz 1GB RAM. results
summarized Figure 20.
small problems (up 45 machines), performance two algorithms
fairly similar terms running time quality policies generated.
However, problem size grows, running time Apricodd increases rapidly,
becomes significantly higher algorithm . Furthermore, problem size
increases, quality policies generated Apricodd deteriorates. difference
policy quality caused different value function representation used two
algorithms. ADDs used Apricodd represent k different values k leaves; thus,
forced agglomerate many different states represent using single value.
smaller problems, agglomeration still represent good policies. Unfortunately,
problem size increases state space grows exponentially, Apricodds policy
representation becomes inadequate, quality policies decreases.
hand, linear value functions represent exponentially many values k basis
functions, allows approach scale significantly larger problems.

10. Related Work
closely related work line research began work
Boutilier et al. (1995). address comparison separately below, begin
section broader background references.
10.1 Approximate MDP Solutions
field MDPs, popularly known, formalized Bellman (1957)
1950s. importance value function approximation recognized early stage
Bellman (1963). early 1990s MDP framework recognized AI
researchers formal framework could used address problem planning
uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993).
Within AI community, value function approximation developed concomitantly
notion value function representations Markov chains. Suttons seminal paper
temporal difference learning (1988), addressed use value functions prediction
planning, assumed general representation value function noted
connection general function approximators neural networks. However,
stability combination directly addressed time.
Several important developments gave AI community deeper insight relationship function approximation dynamic programming. Tsitsiklis Van
Roy (1996a) and, independently, Gordon (1995) popularized analysis approximate
MDP methods via contraction properties dynamic programming operator
function approximator. Tsitsiklis Van Roy (1996b) later established general convergence result linear value function approximators D(), Bertsekas
458

fiEfficient Solution Algorithms Factored MDPs

Tsitsiklis (1996) unified large body work approximate dynamic programming
name Neuro-dynamic Programming, providing many novel general error
analyses.
Approximate linear programming MDPs using linear value function approximation
introduced Schweitzer Seidmann (1985), although approach somewhat
deprecated fairly recently due lack compelling error analyses lack
effective method handling large number constraints. Recent work de Farias
Van Roy (2001a, 2001b) started address concerns new error bounds
constraint sampling methods. approach, rather sampling constraints, utilizes
structure model value function represent constraints compactly.
10.2 Factored Approaches
Tatman Shachter (1990) considered additive decomposition value nodes influence diagrams. number approaches factoring general MDPs explored
literature. Techniques exploiting reward functions decompose additively
studied Meuleau et al. (1998), Singh Cohn (1998).
use factored representations dynamic Bayesian networks pioneered
Boutilier et al. (1995) developed steadily recent years. methods rely
use context-specific structures decision trees analytic decision diagrams
(ADDs) (Hoey et al., 1999) represent transition dynamics DBN
value function. algorithms use dynamic programming partition state space,
representing partition using tree-like structure branches state variables
assigns values leaves. tree grown dynamically part dynamic programming process algorithm creates new leaves needed: leaf split
application DP operator two states associated leaf turn
different values backprojected value function. process interpreted
form model minimization (Dean & Givan, 1997).
number leaves tree used represent value function determines computational complexity algorithm. limits number distinct values
assigned states: since leaves represent partitioning state space, every state
maps exactly one leaf. However, recognized early on, trivial MDPs
require exponentially large value functions. observation led line approximation
algorithms aimed limiting tree size (Boutilier & Dearden, 1996) and, later, limiting
ADD size (St-Aubin, Hoey, & Boutilier, 2001). Kim Dean (2001) explored
techniques discovering tree-structured value functions factored MDPs.
methods permit good approximate solutions large MDPs, complexity still
determined number leaves representation number distinct values
assigned states still limited well.
Tadepalli Ok (1996) first apply linear value function approximation
Factored MDPs. Linear value function approximation potentially expressive
approximation method assign unique values every state MDP without
requiring storage space exponential number state variables. expressive
power tree k leaves captured linear function approximator k basis
functions basis function hi indicator function tests state belongs
459

fiGuestrin, Koller, Parr & Venkataraman

partition leaf i. Thus, set value functions represented
tree k leaves subset set value functions represented
value function k basis functions. experimental results Section 9.3 highlight
difference showing example problem requires exponentially many leaves
value function, approximated well using linear value function.
main advantage tree-based value functions structure determined
dynamically solution MDP. principle, value function representation derived automatically model description, approach requires less insight
user. problems value function well approximated relatively small number values, approach provides excellent solution problem.
method linear value function approximation aims address believe
common case, large range distinct values required achieve good
approximation.
Finally, note Schuurmans Patrascu (2001), based earlier work
max-norm projection using cost networks linear programs, independently developed
alternative approach approximate linear programming using cost network.
method embeds cost network inside single linear program. contrast, method
based constraint generation approach, using cost network detect constraint
violations. constraint violations found, new constraint added, repeatedly
generating attempting solve LPs feasible solution found. Interestingly,
approach Schuurmans Patrascu uses multiple calls variable elimination
order speed LP solution step, successful time spent
solving LP significantly larger time required variable elimination.
suggested Section 9.2, LP solution time larger table-based approach. Thus,
Schuurmans Patrascus constraint generation method probably successful
table-based problems rule-based ones.

11. Conclusions
paper, presented new algorithms approximate linear programming approximate dynamic programming (value policy iteration) factored MDPs.
algorithms leverage novel LP decomposition technique, analogous variable elimination cost networks, reduces exponentially large LP provably
equivalent, polynomial-sized one.
approximate dynamic programming algorithms motivated error analyses
showing importance minimizing L error. algorithms efficient
substantially easier implement previous algorithms based L2 -projection.
experimental results suggest perform better practice.
approximate linear programming algorithm factored MDPs simpler, easier
implement general dynamic programming approaches. Unlike policy
iteration algorithm, rely default action assumption, states
actions affect small number state variables. Although algorithm
theoretical guarantees max-norm projection approaches, empirically seems
favorable option. experiments suggest approximate policy iteration tends
generate better policies set basis functions. However, due computa460

fiEfficient Solution Algorithms Factored MDPs

tional advantages, add basis functions approximate linear programming
algorithm, obtaining better policy still maintaining much faster running time
approximate policy iteration.
Unlike previous approaches, algorithms exploit additive contextspecific structure factored MDP model. Typical real-world systems possess
types structure. thus, feature algorithms increase applicability factored MDPs practical problems. demonstrated exploiting
context-specific independence, using rule-based representation instead standard
table-based one, yield exponential improvements computational time problem significant amounts CSI. However, overhead managing sets rules make
less well-suited simpler problems. compared approach work
Boutilier et al. (2000), exploits context-specific structure. problems
significant context-specific structure value function, approach faster due
efficient handling ADD representation. However, problems
significant context-specific structure problem representation, rather value
function, require exponentially large ADDs. problems, demonstrated using linear value function algorithm obtain polynomial-time
near-optimal approximation true value function.
success algorithm depends ability capture important
structure value function using linear, factored approximation. ability, turn,
depends choice basis functions properties domain.
algorithms currently require designer specify factored basis functions.
limitation compared algorithms Boutilier et al. (2000), fully automated.
However, experiments suggest simple rules quite successful designing basis. First, ensure reward function representable basis.
simple basis that, addition, contained separate set indicators variable often
quite well. add indicators pairs variables; simply, choose
according DBN transition model, indicator added variables
Xi one variables Parents(Xi ), thus representing one-step influences.
procedure extended, adding basis functions represent influences
required. Thus, structure DBN gives us indications choose basis
functions. sources prior knowledge included specifying
basis.
Nonetheless, general algorithm choosing good factored basis functions still
exist. However, potential approaches: First, problems CSI, one
could apply algorithms Boutilier et al. iterations generate partial treestructured solutions. Indicators defined variables backprojection leaves
could, turn, used generate basis set problems. Second, Bellman
error computation, performed efficiently shown Section 7,
provide bound quality policy, actual state error
largest. knowledge used create mechanism incrementally increase
basis set, adding new basis functions tackle states high Bellman error.
many possible extensions work. already pursued extensions collaborative multiagent systems, multiple agents act simultaneously
maximize global reward (Guestrin et al., 2001b), factored POMDPs,
461

fiGuestrin, Koller, Parr & Venkataraman

full state observed directly, indirectly observation variables (Guestrin,
Koller, & Parr, 2001c). However, settings remain explored.
particular, hope address problem learning factored MDP planning
competitive multiagent system.
Additionally, paper tackled problems induced width cost
network sufficiently low possess sufficient context-specific structure allow
exact solution factored LPs. Unfortunately, practical problems may
prohibitively large induced width. plan leverage ideas loopy belief propagation algorithms approximate inference Bayesian networks (Pearl, 1988; Yedidia,
Freeman, & Weiss, 2001) address issue.
believe methods described herein significantly extend efficiency,
applicability general usability factored models value functions control
practical dynamic systems.

Acknowledgements
grateful Craig Boutilier, Dirk Ormoneit Uri Lerner many useful
discussions, anonymous reviewers detailed thorough comments.
would thank Jesse Hoey, Robert St-Aubin, Alan Hu, Craig Boutilier
distributing algorithm useful assistance using Apricodd
selecting parameters. work supported DoD MURI program, administered Office Naval Research Grant N00014-00-1-0637, Air Force contract
F30602-00-2-0598 DARPAs TASK program, Sloan Foundation. first
author supported Siebel Scholarship.

Appendix A. Proofs
A.1 Proof Lemma 3.3
exists setting weights zero setting yields bounded maxnorm projection error P policy (P Rmax ). max-norm projection operator
chooses set weights minimizes projection error (t) policy (t) . Thus,
projection error (t) must least low one given zero weights P
(which bounded). Thus, error remains bounded iterations.
A.2 Proof Theorem 3.5
First, need bound approximation V(t) :




V(t) Hw(t)












T(t) Hw(t) Hw(t)




+ V(t) T(t) Hw(t)








(t)
(t)
T(t) Hw Hw + V(t) Hw(t)






; (triangle inequality;)

; (T(t) contraction.)

Moving second term right hand side dividing 1 , obtain:




V(t) Hw(t)






1
(t)


.
T(t) Hw(t) Hw(t) =

1
1
462

(24)

fiEfficient Solution Algorithms Factored MDPs

next part proof, adapt lemma Bertsekas Tsitsiklis (1996, Lemma
6.2, p.277) fit framework. manipulation, lemma reformulated as:
kV V(t+1) k kV V(t) k +


2


V(t) Hw(t) .

1

(25)

proof concluded substituting Equation (24) Equation (25) and, finally, induction t.
A.3 Proof Theorem 4.4
First, note equality constraints represent simple change variable. Thus,
rewrite Equation (12) terms new LP variables ufzii as:
max

X

x

ufzii ,

(26)



assignment weights w implies assignment ufzii . stage,
LP variables.
remains show factored LP construction equivalent constraint
Equation (26). system n variables {X1 , . . . , Xn }, assume, without loss
generality, variables eliminated starting Xn X1 . prove
equivalence induction number variables.
base case n = 0, functions ci (x) b(x) Equation (12)
empty scope. case, Equation (26) written as:


X

uei .

(27)



case, transformation done constraint, equivalence immediate.
Now, assume result holds systems i1 variables prove equivalence
system variables. system, maximization decomposed
two terms: one factors depend Xi , irrelevant
maximization Xi , another term factors depend Xi . Using
decomposition, write Equation (26) as:




max

X ej

x1 ,...,xi

uzj ;

j

max

x1 ,...,xi1




X

X

uezll + max
xi

l : xi 6zl

e
uzjj .

(28)

j : xi zj

point define new LP variables uez corresponding second term
right hand side constraint. new LP variables must satisfy following
constraint:
uez max
xi

X ej
j=1

463

u(z,xi )[Zj ] .

(29)

fiGuestrin, Koller, Parr & Venkataraman

new non-linear constraint represented factored LP construction
set equivalent linear constraints:
uez

X ej
j=1

u(z,xi )[Zj ] , z, xi .

(30)

equivalence non-linear constraint Equation (29) set linear constraints Equation (30) shown considering binding constraints. new
LP variable created uez , |Xi | new constraints created, one value xi Xi .
assignment LP variables right hand side constraint EquaP
ej
tion (30), one |Xi | constraints relevant. is, one `j=1 u(z,x
)[Zj ]
maximal, corresponds maximum Xi . Again, value z
one assignment Xi achieves maximum, (and only) constraints
corresponding maximizing assignments could binding. Thus, Equation (29)
Equation (30) equivalent.
Substituting new LP variables uez Equation (28), get:


max

x1 ,...,xi1

X

uezll + uez ,

l : xi 6zl

depend Xi anymore. Thus, equivalent system i1 variables,
concluding induction step proof.
A.4 Proof Lemma 7.1
First note iteration + 1 objective function (t+1) max-norm projection
LP given by:








(t+1) = Hw(t+1) R(t+1) + P(t+1) Hw(t+1) .


However, convergence value function estimates equal iterations:
w(t+1) = w(t) .
that:








(t+1) = Hw(t) R(t+1) + P(t+1) Hw(t) .


operator notation, term equivalent to:






(t+1) = Hw(t) T(t+1) Hw(t) .


Note that, (t+1) = Greedy(Hw(t) ) definition. Thus, that:
T(t+1) Hw(t) = Hw(t) .
Finally, substituting previous expression, obtain result:






(t+1) = Hw(t) Hw(t) .


464

fiEfficient Solution Algorithms Factored MDPs

References
Arnborg, S., Corneil, D. G., & Proskurowski, A. (1987). Complexity finding embeddings
K-tree. SIAM Journal Algebraic Discrete Methods, 8 (2), 277 284.
Becker, A., & Geiger, D. (2001). sufficiently fast algorithm finding close optimal
clique trees. Artificial Intelligence, 125 (1-2), 317.
Bellman, R., Kalaba, R., & Kotkin, B. (1963). Polynomial approximation new computational technique dynamic programming. Math. Comp., 17 (8), 155161.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton, New
Jersey.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press, New
York.
Bertsekas, D., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, Massachusetts.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research, 11, 1
94.
Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamic
programming. Proc. ICML, pp. 5462.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proc. IJCAI, pp. 11041111.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121 (1-2), 49107.
Cheney, E. W. (1982). Approximation Theory (2nd edition). Chelsea Publishing Co., New
York, NY.
de Farias, D., & Van Roy, B. (2001a). linear programming approach approximate
dynamic programming. Submitted Operations Research.
de Farias, D., & Van Roy, B. (2001b). constraint sampling linear programming approach approximate dynamic programming. appear Mathematics
Operations Research.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. (1993). Planning deadlines
stochastic domains. Proceedings Eleventh National Conference Artificial
Intelligence (AAAI-93), pp. 574579, Washington, D.C. AAAI Press.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5 (3), 142150.
Dean, T., & Givan, R. (1997). Model minimization Markov decision processes.
Proceedings Fourteenth National Conference Artificial Intelligence (AAAI97), pp. 106111, Providence, Rhode Island, Oregon. AAAI Press.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.
465

fiGuestrin, Koller, Parr & Venkataraman

Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (12), 4185.
Gordon, G. (1995). Stable function approximation dynamic programming. Proceedings
Twelfth International Conference Machine Learning, pp. 261268, Tahoe
City, CA. Morgan Kaufmann.
Guestrin, C. E., Koller, D., & Parr, R. (2001a). Max-norm projections factored MDPs.
Proceedings Seventeenth International Joint Conference Artificial Intelligence (IJCAI-01), pp. 673 680, Seattle, Washington. Morgan Kaufmann.
Guestrin, C. E., Koller, D., & Parr, R. (2001b). Multiagent planning factored MDPs.
14th Neural Information Processing Systems (NIPS-14), pp. 15231530, Vancouver,
Canada.
Guestrin, C. E., Koller, D., & Parr, R. (2001c). Solving factored POMDPs linear value
functions. Seventeenth International Joint Conference Artificial Intelligence
(IJCAI-01) workshop Planning Uncertainty Incomplete Information,
pp. 67 75, Seattle, Washington.
Guestrin, C. E., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination planning factored MDPs. Eighteenth National Conference
Artificial Intelligence (AAAI-2002), pp. 253259, Edmonton, Canada.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using decision diagrams. Proceedings Fifteenth Conference Uncertainty
Artificial Intelligence (UAI-99), pp. 279288, Stockholm, Sweden. Morgan Kaufmann.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (2002). Stochastic planning using decision
diagrams C implementation. http://www.cs.ubc.ca/spider/staubin/Spudd/.
Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Howard, R. A., & Matheson, J. E. (Eds.), Readings Principles Applications Decision Analysis,
pp. 721762. Strategic Decisions Group, Menlo Park, California.
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences
Value Tradeoffs. Wiley, New York.
Kim, K.-E., & Dean, T. (2001). Solving factored Mdps using non-homogeneous partitioning. Proceedings Seventeenth International Joint Conference Artificial
Intelligence (IJCAI-01), pp. 683 689, Seattle, Washington. Morgan Kaufmann.
Kjaerulff, U. (1990). Triangulation graphs algorithms giving small total state space.
Tech. rep. TR R 90-09, Department Mathematics Computer Science, Strandvejen, Aalborg, Denmark.
Koller, D., & Parr, R. (1999). Computing factored value functions policies structured
MDPs. Proceedings Sixteenth International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 1332 1339. Morgan Kaufmann.
Koller, D., & Parr, R. (2000). Policy iteration factored MDPs. Proceedings
Sixteenth Conference Uncertainty Artificial Intelligence (UAI-00), pp. 326
334, Stanford, California. Morgan Kaufmann.
466

fiEfficient Solution Algorithms Factored MDPs

Meuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving large weakly-coupled Markov decision processes. Proceedings
15th National Conference Artificial Intelligence, pp. 165172, Madison, WI.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann, San Mateo, California.
Puterman, M. L. (1994). Markov decision processes: Discrete stochastic dynamic programming. Wiley, New York.
Reed, B. (1992). Finding approximate separators computing tree-width quickly.
24th Annual Symposium Theory Computing, pp. 221228. ACM.
Schuurmans, D., & Patrascu, R. (2001). Direct value-approximation factored MDPs.
Advances Neural Information Processing Systems (NIPS-14), pp. 15791586,
Vancouver, Canada.
Schweitzer, P., & Seidmann, A. (1985). Generalized polynomial approximations Markovian decision processes. Journal Mathematical Analysis Applications, 110, 568
582.
Simon, H. A. (1981). Sciences Artificial (second edition). MIT Press, Cambridge,
Massachusetts.
Singh, S., & Cohn, D. (1998). dynamically merge Markov decision processes.
Jordan, M. I., Kearns, M. J., & Solla, S. A. (Eds.), Advances Neural Information
Processing Systems, Vol. 10. MIT Press.
St-Aubin, R., Hoey, J., & Boutilier, C. (2001). APRICODD: Approximate policy construction using decision diagrams. Advances Neural Information Processing Systems
13: Proceedings 2000 Conference, pp. 10891095, Denver, Colorado. MIT Press.
Stiefel, E. (1960). Note Jordan elimination, linear programming Tchebycheff approximation. Numerische Mathematik, 2, 1 17.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine
Learning, 3, 944.
Tadepalli, P., & Ok, D. (1996). Scaling average reward reinforcmeent learning approximating domain models value function. Proceedings Thirteenth
International Conference Machine Learning, Bari, Italy. Morgan Kaufmann.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming influence diagrams.
IEEE Transactions Systems, Man Cybernetics, 20 (2), 365379.
Tsitsiklis, J. N., & Van Roy, B. (1996a). Feature-based methods large scale dynamic
programming. Machine Learning, 22, 5994.
Tsitsiklis, J. N., & Van Roy, B. (1996b). analysis temporal-difference learning
function approximation. Technical report LIDS-P-2322, Laboratory Information
Decision Systems, Massachusetts Institute Technology.
Van Roy, B. (1998). Learning Value Function Approximation Complex Decision
Processes. Ph.D. thesis, Massachusetts Institute Technology.
467

fiGuestrin, Koller, Parr & Venkataraman

Williams, R. J., & Baird, L. C. I. (1993). Tight performance bounds greedy policies based
imperfect value functions. Tech. rep., College Computer Science, Northeastern
University, Boston, Massachusetts.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. Advances
Neural Information Processing Systems 13: Proceedings 2000 Conference,
pp. 689695, Denver, Colorado. MIT Press.
Zhang, N., & Poole, D. (1999). role context-specific independence probabilistic
reasoning. Proceedings Sixteenth International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 12881293. Morgan Kaufmann.

468


