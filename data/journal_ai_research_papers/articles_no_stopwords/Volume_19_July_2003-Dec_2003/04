Journal Artificial Intelligence Research 19 (2003) 569-629

Submitted 05/01; published 12/03

Accelerating Reinforcement Learning
Implicit Imitation
Bob Price

price@cs.ubc.ca

Department Computer Science
University British Columbia
Vancouver, B.C., Canada V6T 1Z4

Craig Boutilier

cebly@cs.toronto.edu

Department Computer Science
University Toronto
Toronto, ON, Canada M5S 3H5

Abstract
Imitation viewed means enhancing learning multiagent environments.
augments agents ability learn useful behaviors making intelligent use
knowledge implicit behaviors demonstrated cooperative teachers experienced agents. propose study formal model implicit imitation
accelerate reinforcement learning dramatically certain cases. Roughly, observing
mentor, reinforcement-learning agent extract information capabilities
in, relative value of, unvisited parts state space. study two specific
instantiations model, one learning agent mentor identical
abilities, one designed deal agents mentors different action sets.
illustrate benefits implicit imitation integrating prioritized sweeping,
demonstrating improved performance convergence observation single
multiple mentors. Though make stringent assumptions regarding observability
possible interactions, briefly comment extensions model relax
restricitions.

1. Introduction
application reinforcement learning multiagent systems offers unique opportunities
challenges. agents viewed independently trying achieve ends,
interesting issues interaction agent policies (Littman, 1994) must resolved (e.g.,
appeal equilibrium concepts). However, fact agents may share information
mutual gain (Tan, 1993) distribute search optimal policies communicate reinforcement signals one another (Mataric, 1998) offers intriguing possibilities
accelerating reinforcement learning enhancing agent performance.
Another way individual agent performance improved
novice agent learn reasonable behavior expert mentor. type learning
brought explicit teaching demonstration (Atkeson & Schaal, 1997;
Lin, 1992; Whitehead, 1991a), sharing privileged information (Mataric, 1998),
explicit cognitive representation imitation (Bakker & Kuniyoshi, 1996).
imitation, agents exploration used ground observations agents
c
2003
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiPrice & Boutilier

behaviors capabilities resolve ambiguities observations arising
partial observability noise. common thread work use mentor
guide exploration observer. Typically, guidance achieved form
explicit communication mentor observer. less direct form teaching
involves observer extracting information mentor without mentor making
explicit attempt demonstrate specific behavior interest (Mitchell, Mahadevan, &
Steinberg, 1985).
paper develop imitation model call implicit imitation allows
agent accelerate reinforcement learning process observation expert
mentor (or mentors). agent observes state transitions induced mentors
actions uses information gleaned observations update estimated
value states actions. distinguish two settings implicit
imitation occur: homogeneous settings, learning agent mentor
identical actions; heterogeneous settings, capabilities may differ.
homogeneous setting, learner use observed mentor transitions directly
update estimated model actions, update value function. addition,
mentor provide hints observer parts state space
may worth focusing attention. observers attention area might take form
additional exploration area additional computation brought bear agents
prior beliefs area. heterogeneous setting, similar benefits accrue,
potential agent misled mentor possesses abilities different
own. case, learner needs mechanism detect situations
make efforts temper influence observations.
derive several new techniques support implicit imitation largely independent specific reinforcement learning algorithm, though best suited use
model-based methods. include model extraction, augmented backups, feasibility
testing, k-step repair. first describe implicit imitation homogeneous domains,
describe extension heterogeneous settings. illustrate effectiveness
empirically incorporating Moore Atkesons (1993) prioritized sweeping algorithm.
implicit imitation model several advantages direct forms imitation
teaching. require agent explicitly play role mentor teacher.
Observers learn simply watching behavior agents; observed mentor
shares certain subtasks observer, observed behavior incorporated (indirectly) observer improve estimate value function. important
many situations observer learn mentor
unwilling unable alter behavior accommodate observer, even communicate
information it. example, common communication protocols may unavailable
agents designed different developers (e.g., Internet agents); agents may find
competitive situation disincentive share information skills;
may simply incentive one agent provide information another.1
Another key advantage approachwhich arises formalizing imitation
reinforcement learning contextis fact observer constrained directly
1. reasons consistency, use term mentor describe agent observer
learn, even mentor unwilling unwitting participant.

570

fiImplicit Imitation

imitate (i.e., duplicate actions of) mentor. learner decide whether
explicit imitation worthwhile. Implicit imitation thus seen blending
advantages explicit teaching explicit knowledge transfer independent
learning. addition, agent learns observation, exploit existence
multiple mentors, essentially distributing search. Finally, assume
observer knows actual actions taken mentor, mentor shares
reward function (or goals) mentor. Again, stands sharp contrast many
existing models teaching, imitation, behavior learning observation. make
strict assumptions paper respect observability, complete knowledge
reward functions, existence mappings agent state spaces, model
generalized interesting ways. elaborate generalizations near
end paper.
remainder paper structured follows. provide necessary background Markov decision processes reinforcement learning development
implicit imitation model Section 2. Section 3, describe general formal framework
study implicit imitation reinforcement learning. Two specific instantiations
framework developed. Section 4, model homogeneous agents
developed. model extraction technique explained augmented Bellman backup
proposed mechanism incorporating observations model-based reinforcement
learning algorithms. Model confidence testing introduced ensure misleading
information undue influence learners exploration policy. use
mentor observations focus attention interesting parts state space
introduced. Section 5 develops model heterogeneous agents. model extends
homogeneous model feasibility testing, device learner detect
whether mentors abilities similar own, k-step repair, whereby learner
attempt mimic trajectory mentor cannot duplicated exactly.
techniques prove crucial heterogeneous settings. effectiveness models
demonstrated number carefully chosen navigation problems. Section 6 examines
conditions implicit imitation work well. Section 7 describes
several promising extensions model. Section 8 examines implicit imitation model
context related work Section 9 considers future work drawing
general conclusions implicit imitation field computational imitation
broadly.

2. Reinforcement Learning
aim provide formal model implicit imitation, whereby agent learn
act optimally combining experience observations behavior
expert mentor. so, describe section standard model
reinforcement learning used artificial intelligence. model build singleagent view learning act. begin reviewing Markov decision processes,
provide model sequential decision making uncertainty, move
describe reinforcement learning, emphasis model-based methods.
571

fiPrice & Boutilier

2.1 Markov Decision Processes
Markov decision processes (MDPs) proven useful modeling stochastic sequential decision problems, widely used decision-theoretic planning model
domains agents actions uncertain effects, agents knowledge environment uncertain, agent multiple, possibly conflicting objectives.
section, describe basic MDP model consider one classical solution procedure.
consider action costs formulation MDPs, though pose special
complications. Finally, make assumption full observability. Partially observable
MDPs (POMDPs) (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Smallwood &
Sondik, 1973) much computationally demanding fully observable MDPs.
imitation model based fully observable model, though generalizations model mentioned concluding section build POMDPs. refer
reader Bertsekas (1987); Boutilier, Dean Hanks (1999); Puterman (1994)
material MDPs.
MDP viewed stochastic automaton actions induce transitions
states, rewards obtained depending states visited agent.
Formally, MDP defined tuple hS, A, T, Ri, finite set states
possible worlds, finite set actions, state transition function, R reward
function. agent control state system extent performing actions
cause state transitions, movement current state new state.
Actions stochastic actual transition caused cannot generally predicted
certainty. transition function : (S) describes effects
action state. (si , a) probability distribution S; specifically, (si , a)(sj )
probability ending state sj action performed state si .
denote quantity Pr(si , a, sj ). require 0 Pr(si , a, sj ) 1
P
si , sj , si , sj Pr(si , a, sj ) = 1. components S, determine
dynamics system controlled. assumption system fully
observable means agent knows true state time (once stage
reached), decisions based solely knowledge. Thus, uncertainty lies
prediction actions effects, determining actual effect
execution.
(deterministic, stationary, Markovian) policy : describes course action
adopted agent controlling system. agent adopting policy performs
action (s) whenever finds state s. Policies form Markovian since
action choice state depend system history, stationary since
action choice depend stage decision problem. problems
consider, optimal stationary Markovian policies always exist.
assume bounded, real-valued reward function R : <. R(s) instantaneous reward agent receives occupying state s. number optimality criteria
adopted measure value policy , measuring way reward
accumulated agent traverses state space execution .
work, focus discounted infinite-horizon problems: current value reward received stages future discounted factor (0 < 1). allows simpler
572

fiImplicit Imitation

computational methods used, discounted total reward finite. Discounting
justified (e.g., economic) grounds many situations well.
value function V : < reflects value policy state s;
simply expected sum discounted future rewards obtained executing beginning
s. policy optimal if, policies , V (s) V (s).
guaranteed optimal (stationary) policies exist setting (Puterman,
1994). (optimal) value state V (s) value V (s) optimal policy .
solving MDP, refer problem constructing optimal policy. Value
iteration (Bellman, 1957) simple iterative approximation algorithm optimal policy
construction. Given arbitrary estimate V 0 true value function V , iteratively
improve estimate follows:
V n (si ) = R(si ) + max{
aA

X

Pr(si , a, sj )V n1 (sj )}

(1)

sj

computation V n (s) given V n1 known Bellman backup. sequence value
functions V n produced value iteration converges linearly V . iteration value
iteration requires O(|S|2 |A|) computation time, number iterations polynomial
|S|.
finite n, actions maximize right-hand side Equation 1 form
optimal policy, V n approximates value. Various termination criteria applied;
example, one might terminate algorithm
kV i+1 V k

(1 )
2

(2)

(where kXk = max{|x| : x X} denotes supremum norm). ensures resulting
value function V i+1 within 2 optimal function V state, induced
policy -optimal (i.e., value within V ) (Puterman, 1994).
concept useful later Q-function. Given arbitrary value
function V , define QVa (si )
QVa (si ) = R(si ) +

X

Pr(si , a, sj )V (sj )

(3)

sj

Intuitively, QVa (s) denotes value performing action state acting
manner value V (Watkins & Dayan, 1992). particular, define Qa
Q-function defined respect V , Qna Q-function defined respect
V n1 . manner, rewrite Equation 1 as:
V n (s) = max{Qna (s)}
aA

(4)

define ergodic MDP MDP every state reachable
state finite number steps non-zero probability.
573

fiPrice & Boutilier

2.2 Model-based Reinforcement Learning
One difficulty use MDPs construction optimal policy requires
agent know exact transition probabilities Pr reward model R. specification decision problem, requirements, especially detailed specification
domains dynamics, impose undue burden agents designer. Reinforcement
learning viewed solving MDP full details model, particular Pr R, known agent. Instead, agent learns act optimally
experience environment. provide brief overview reinforcement
learning section (with emphasis model-based approaches). details,
please refer texts Sutton Barto (1998) Bertsekas Tsitsiklis (1996),
survey Kaelbling, Littman Moore (1996).
general model, assume agent controlling MDP hS, A, T, Ri
initially knows state action spaces, A, transition model
reward function R. agent acts environment, stage process
makes transition hs, a, r, ti; is, takes action state s, receives reward r
moves state t. Based repeated experiences type determine optimal
policy one two ways: (a) model-based reinforcement learning, experiences
used learn true nature R, MDP solved using standard
methods (e.g., value iteration); (b) model-free reinforcement learning, experiences
used directly update estimate optimal value function Q-function.
Probably simplest model-based reinforcement learning scheme certainty equivalence approach. Intuitively, learning agent assumed current estimated
c
transition model Tb environment consisting estimated probabilities Pr(s,
a, t)
b
estimated rewards model R(s). experience hs, a, r, ti agent updates esc obtain policy
b would optimal
timated models, solves estimated MDP
estimated models correct, acts according policy.
make certainty equivalence approach precise, specific form estimated model
update procedure must adopted. common approach used empirical distribution observed state transitions rewards estimated model. instance,
action attempted C(s, a) times state s, C(s, a, t) occasions
c
state reached, estimate Pr(s,
a, t) = C(s, a, t)/C(s, a). C(s, a) = 0,
prior estimate used (e.g., one might assume state transitions equiprobable).
Bayesian approach (Dearden, Friedman, & Andre, 1999) uses explicit prior distribution
parameters transition distribution Pr(s, a, ), updates
experienced transition. instance, might assume Dirichlet (Generalized Beta)
distribution (DeGroot, 1975) parameters n(s, a, t) associated possible successor state t. Dirichlet parameters equal experience-based counts C(s, a, t)
plus prior count P (s, a, t) representing agents prior beliefs distribution
(i.e., n(s, a, t) = C(s, a, t) + P (s, a, t)). expected transition probability Pr(s, a, t)
P
c solved
n(s, a, t)/ t0 n(s, a, t0 ). Assuming parameter independence, MDP
using expected values. Furthermore, model updated ease, simply
increasing n(s, a, t) one observation hs, a, r, ti. model advantage
counter-based approach allowing flexible prior model generally
574

fiImplicit Imitation

assign probability zero unobserved transitions. adopt Bayesian perspective
imitation model.
One difficulty certainty equivalence approach computational burden rec update models Tb R
b (i.e., experience). One
solving MDP
could circumvent extent batching experiences updating (and re-solving)
model periodically. Alternatively, one could use computational effort judiciously
apply Bellman backups states whose values (or Q-values) likely change
given change model. Moore Atkesons (1993) prioritized sweeping
c
algorithm this. Tb updated changing Pr(s,
a, t), Bellman backup
b
b a). Suppose
applied update estimated value V , well Q-value Q(s,
b
b
magnitude change V (s) given V (s). predecessor w, Q-values
b
c
Q(w,
a0 )hence values Vb (w)can change Pr(w,
a0 , s) > 0. magnitude change
c
bounded Pr(w,
a0 , s)Vb (s). predecessors w placed priority
0
c
queue Pr(w, , s)Vb (s) serving priority. fixed number Bellman backups
applied states order appear queue. backup,
change value cause new predecessors inserted queue. way,
computational effort focused states Bellman backup greatest
impact due model change. Furthermore, backups applied subset
states, generally applied fixed number times. way contrast,
certainty equivalence approach, backups applied convergence. Thus prioritized
sweeping viewed specific form asynchronous value iteration, appealing
computational properties (Moore & Atkeson, 1993).
certainty equivalence, agent acts current approximation model
correct, even though model likely inaccurate early learning process.
optimal policy inaccurate model prevents agent exploring transitions form part optimal policy true model, agent fail
find optimal policy. reason, explicit exploration policies invariably used
ensure action tried state sufficiently often. acting randomly (assuming ergodic MDP), agent assured sampling action state infinitely
often limit. Unfortunately, actions agent fail exploit (in fact,
completely uninfluenced by) knowledge optimal policy. explorationexploitation tradeoff refers tension trying new actions order find
environment executing actions believed optimal basis
current estimated model.
common method exploration greedy method agent
chooses random action fraction time, 0 < < 1. Typically, decayed
time increase agents exploitation knowledge. Boltzmann approach,
action selected probability proportional value:
Prs (a) = P

eQ(s,a)/
e
a0

Q(s,a0 )/

(5)

proportionality adjusted nonlinearly temperature parameter .
0 probability selecting action highest value tends 1. Typically,
started high actions randomly explored early stages learning.
agent gains knowledge effects actions value effects,
575

fiPrice & Boutilier

parameter decayed agent spends time exploiting actions known
valuable less time randomly exploring actions.
sophisticated methods attempt use information model confidence
value magnitudes plan utility-maximizing exploration plan. early approximation
scheme found interval estimation method (Kaelbling, 1993). Bayesian
methods used calculate expected value information gained
exploration (Meuleau & Bourgine, 1999; Dearden et al., 1999).
concentrate paper model-based approaches reinforcement learning.
However, point model-free methodsthose estimate
optimal value function Q-function learned directly, without recourse domain
modelhave attracted much attention. example, TD-methods (Sutton, 1988)
Q-learning (Watkins & Dayan, 1992) proven among popular
methods reinforcement learning. methods modified deal model-free
approaches, discuss concluding section. focus so-called tablebased (or explicit) representations models value functions. state action
spaces large, table-based approaches become unwieldy, associated algorithms
generally intractable. situations, approximators often used estimate
values states. discuss ways techniques extended allow
function approximation concluding section.

3. Formal Framework Implicit Imitation
model influence mentor agent decision process learning
behavior observer, must extend single-agent decision model MDPs account
actions objectives multiple agents. section, introduce formal
framework studying implicit imitation. begin introducing general model
stochastic games (Shapley, 1953; Myerson, 1991), impose various assumptions
restrictions general model allow us focus key aspects implicit
imitation. note framework proposed useful study forms
knowledge transfer multiagent systems, briefly point various extensions
framework would permit implicit imitation, forms knowledge transfer,
general settings.
3.1 Non-Interacting Stochastic Games
Stochastic games viewed multiagent extension Markov decision processes.
Though Shapleys (1953) original formulation stochastic games involved zero-sum (fully
competitive) assumption, various generalizations model proposed allowing
arbitrary relationships agents utility functions (Myerson, 1991).2 Formally,
n-agent stochastic game hS, {Ai : n}, T, {Ri : n}i comprises set n agents
(1 n), set states S, set actions Ai agent i, state transition function
, reward function Ri agent i. Unlike MDP, individual agent actions
determine state transitions; rather joint action taken collection agents
determines system evolves point time. Let = A1
2. example, see fully cooperative multiagent MDP model proposed Boutilier (1999).

576

fiImplicit Imitation

set joint actions; : (S), (si , a)(sj ) = Pr(si , a, sj ) denoting
probability ending state sj joint action performed state si .
convenience, introduce notation Ai denote set joint actions A1
Ai1 Ai+1 involving agents except i. use ai ai denote
(full) joint action obtained conjoining ai Ai ai Ai .
interests individual agents may odds, strategic reasoning
notions equilibrium generally involved solution stochastic games.
aim study reinforcement agent might learn observing behavior
expert mentor, wish restrict model way strategic interactions need
considered: want focus settings actions observer
mentor interact. Furthermore, want assume reward functions
agents conflict way requires strategic reasoning.
define noninteracting stochastic games appealing notion agent projection function used extract agents local state underlying game.
games, agents local state determines aspects global state relevant
decision making process, projection function determines global states
identical agents local perspective. Formally, agent i, assume local
state space Si , projection function Li : Si . s, S, write
iff Li (s) = Li (t). equivalence relation partitions set equivalence classes
elements within specific class (i.e., L1
(s) Si ) need
distinguished agent purposes individual decision making. say stochastic
game noninteracting exists local state space Si projection function Li
agent that:
1. t, ai Ai , ai Ai , wi Si
X

{Pr(s, ai ai , w) : w L1
(wi )} =

X

{Pr(t, ai ai , w) : w L1
(wi )}

2. Ri (s) = Ri (t)
Intuitively, condition 1 imposes two distinct requirements game
perspective agent i. First, ignore existence agents, provides notion
state space abstraction suitable agent i. Specifically, Li clusters together states
state equivalence class identical dynamics respect
abstraction induced Li . type abstraction form bisimulation
type studied automaton minimization (Hartmanis & Stearns, 1966; Lee & Yannakakis,
1992) automatic abstraction methods developed MDPs (Dearden & Boutilier, 1997;
Dean & Givan, 1997). hard showignoring presence agentsthat
underlying system Markovian respect abstraction (or equivalently, w.r.t.
Si ) condition 1 met. quantification ai imposes strong noninteraction
requirement, namely, dynamics game perspective agent
independent strategies agents. Condition 2 simply requires
states within given equivalence class agent reward agent i.
means states within class need distinguishedeach local state viewed
atomic.
577

fiPrice & Boutilier

noninteracting game induces MDP Mi agent Mi = hSi , Ai , Pri , Ri
Pri given condition (1) above. Specifically, si , ti Si :
Pri (si , ai , ti ) =

P

{Pr(s, ai .ai , t) : L1
(ti )}

state L1
(si ) ai element Ai . Let : Sa Ai
optimal policy Mi . extend strategy iG : Ai underlying
stochastic game simply applying (si ) every state Li (s) = si .
following proposition shows term noninteracting indeed provides appropriate
description game.
Proposition 1 Let G noninteracting stochastic game, Mi induced MDP agent
i, optimal policy Mi . strategy iG extending G dominant
agent i.
Thus agent solve noninteracting game abstracting away irrelevant aspects state space, ignoring agent actions, solving personal MDP
Mi .
Given arbitrary stochastic game, generally quite difficult discover whether
noninteracting, requiring construction appropriate projection functions.
follows, simply assume underlying multiagent system noninteracting
game. Rather specifying game projection functions, specify individual MDPs Mi themselves. noninteracting game induced set individual
MDPs simply cross product individual MDPs. view often quite
natural. Consider example three robots moving two-dimensional office domain. able neglect possibility interactionfor example, robots
occupy 2-D position (at suitable level granularity) require
resources achieve tasksthen might specify individual MDP
robot. local state might determined robots x, y-position, orientation,
status tasks. global state space would cross product S1 S2 S3
local spaces. individual components joint action would affect
local state, agent would care (through reward function Ri ) local
state.
note projection function Li viewed equivalent observation function. assume agent distinguish elements Si
fact, observations agents states crucial imitation. Rather existence
Li simply means that, point view decision making known model,
agent need worry distinctions made Li . Assuming
computational limitations, agent need solve Mi , may use observations
agents order improve knowledge Mi dynamics.3
3.2 Implicit Imitation
Despite independent nature agent subprocesses noninteracting multiagent system, circumstances behavior one agent may relevant
3. elaborate condition computational limitations below.

578

fiImplicit Imitation

another. keep discussion simple, assume existence expert mentor agent
m, implementing stationary (and presumably optimal) policy
local MDP Mm = hSm , , Prm , Rm i. assume second agent o, observer,
local MDP Mo = hSo , Ao , Pro , Ro i. nothing mentors behavior relevant
observer knows MDP (and solve without computational difficulty),
situation quite different reinforcement learner without complete knowledge model Mo . may well observed behavior mentor provides
valuable information observer quest learn act optimally within Mo .
take extreme case, mentors MDP identical observers, mentor
expert (in sense acting optimally), behavior mentor indicates
exactly observer do. Even mentor acting optimally,
mentor observer different reward functions, mentor state transitions observed
learner provide valuable information dynamics domain.
Thus see one agent learning act, behavior another
potentially relevant learner, even underlying multiagent system noninteracting. Similar remarks, course, apply case observer knows MDP
Mo , computational restrictions make solving difficultobserved mentor transitions
might provide valuable information focus computational effort.4 main
motivation underlying model implicit imitation behavior expert
mentor provide hints appropriate courses action reinforcement learning
agent.
Intuitively, implicit imitation mechanism learning agent attempts
incorporate observed experience expert mentor agent learning process.
classical forms learning imitation, learner considers effects
mentors action (or action sequence) context. Unlike direct imitation, however,
assume learner must physically attempt duplicate mentors
behavior, assume mentors behavior necessarily appropriate
observer. Instead, influence mentor agents transition model
estimate value various states actions. elaborate points below.
follows, assume mentor associated MDP Mm , learner
observer associated MDP Mo , described above. MDPs fully observable.
focus reinforcement learning problem faced agent o. extension multiple
mentors straightforward discussed below, clarity assume one
mentor description abstract framework. clear certain conditions must
met observer extract useful information mentor. list number
assumptions make different points development model.
Observability: must assume learner observe certain aspects mentors behavior. work, assume state mentors MDP fully
observable learner. Equivalently, interpret full observability
underlying noninteracting game, together knowledge mentors projection
4. instance, algorithms asynchronous dynamic programming prioritized sweeping benefit
guidance. Indeed, distinction reinforcement learning solving MDPs viewed
rather blurry (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996). focus
case unknown model (i.e., classical reinforcement learning problem) opposed one
computational issues key.

579

fiPrice & Boutilier

function Lm . general partially observable model would require specification observation signal set Z observation function : Sm (Z),
O(so , sm )(z) denotes probability observer obtains signal z
local states observer mentor sm , respectively.
pursue model here. important note assume
observer access action taken point time. Since actions
stochastic, state (even fully observable) results mentor invoking
specific control signal generally insufficient determine signal. Thus seems
much reasonable assume states (and transitions) observable
actions gave rise them.
Analogy: observer mentor acting different local state spaces, clear
observations made mentors state transitions offer useful information
observer unless relationship two state spaces.
several ways relationship specified. Dautenhahn Nehaniv
(1998) use homomorphism define relationship mentor observer
specific family trajectories (see Section 8 discussion).
slightly different notion might involve use analogical mapping h : Sm
observed state transition provides information
observer dynamics value state h(s) . certain circumstances,
might require mapping h homomorphic respect Pr(, a, ) (for some,
all, a), perhaps even respect R. discuss issues detail
below. order simplify model avoid undue attention (admittedly
important) topic constructing suitable analogical mappings, simply assume
mentor observer identical state spaces; is, Sm
sense isomorphic. precise sense spaces isomorphicor
cases, presumed isomorphic proven otherwiseis elaborated
discuss relationship agent abilities. Thus point
simply refer state without distinguishing mentors local space Sm
observers .
Abilities: Even mapping states, observations mentors state transitions
tell observer something mentors abilities, own. must
assume observer way duplicate actions taken mentor
induce analogous transitions local state space. words, must
presumption mentor observer similar abilities.
sense analogical mapping state spaces taken
homomorphism. Specifically, might assume mentor observer
actions available (i.e., = Ao = A) h : Sm
homomorphic respect Pr(, a, ) A. requirement
weakened substantially, without diminishing utility, requiring
observer able implement actions actually taken mentor given
state s. Finally, might observer assumes duplicate
actions taken mentor finds evidence contrary. case,
presumed homomorphism state spaces. follows,
distinguish implicit imitation homogeneous action settingsdomains
580

fiImplicit Imitation

analogical mapping indeed homomorphicand heterogeneous action
settingswhere mapping may homomorphism.
general ways defining similarity ability, example, assuming
observer may able move state space similar fashion
mentor without following trajectories (Nehaniv & Dautenhahn, 1998).
instance, mentor may way moving directly key locations state
space, observer may able move analogous locations less
direct fashion. case, analogy states may determined
single actions, rather sequences actions local policies. suggest
ways dealing restricted forms analogy type Section 5.
Objectives: Even observer mentor similar identical abilities,
value observer information gleaned mentor may depend
actual policy implemented mentor. might suppose
closely related mentors policy optimal policy observer,
useful information be. Thus, extent, expect
closely aligned objectives mentor observer are, valuable
guidance provided mentor. Unlike existing teaching models,
suppose mentor making explicit efforts instruct observer.
objectives may identical, force observer
(attempt to) explicitly imitate behavior mentor. general, make
explicit assumptions relationship objectives mentor
observer. However, see that, extent, closer are,
utility derived implicit imitation.
Finally, remark important assumption make throughout remainder
paper: observer knows reward function Ro ; is, state s,
observer evaluate Ro (s) without visited state s. view consistent
view reinforcement learning automatic programming. user may easily
specify reward function (e.g., form set predicates evaluated
state) prior learning. may difficult specify domain model
optimal policy. setting, unknown component MDP Mo
transition function Pro . believe approach reinforcement learning is,
fact, common practice approach reward function must
sampled.
reiterate, aim describe mechanism observer accelerate
learning; emphasize position implicit imitationin contrast explicit
imitationis merely replicating behaviors (or state trajectories) observed another
agent, even attempting reach similar states. believe agent must learn
capabilities adapt information contained observed behavior
these. Agents must explore appropriate application (if any) observed behaviors,
integrating own, appropriate, achieve ends. therefore
see imitation interactive process behavior one agent used guide
learning another.
581

fiPrice & Boutilier

Given setting, list possible ways observer mentor (and
cannot) interact, contrasting along way perspective assumptions
existing models literature.5 First, observer could attempt directly infer
policy observations mentor state-action pairs. model conceptual
simplicity intuitive appeal, forms basis behavioral cloning paradigm
(Sammut, Hurst, Kedzier, & Michie, 1992; Urbancic & Bratko, 1994). However, assumes
observer mentor share reward function action capabilities.
assumes complete unambiguous trajectories (including action choices)
observed. related approach attempts deduce constraints value function
inferred action preferences mentor agent (Utgoff & Clouse, 1991; Suc & Bratko,
1997). Again, however, approach assumes congruity objectives. model
distinct models explicit teaching (Lin, 1992; Whitehead, 1991b): assume
mentor incentive move environment way explicitly
guides learner explore environment action space effectively.
Instead trying directly learn policy, observer could attempt use observed
state transitions agents improve environment model Pro (s, a, t).
accurate model reward function, observer could calculate
accurate values states. state values could used guide agent towards
distant rewards reduce need random exploration. insight forms core
implicit imitation model. approach developed literature,
appropriate conditions listed above, specifically, conditions
mentors actions unobservable, mentor observer different reward
functions objectives. Thus, approach applicable general conditions
many existing models imitation learning teaching.
addition model information, mentors may communicate information
relevance irrelevance regions state space certain classes reward functions.
observer use set states visited mentor heuristic guidance
perform backup computations state space.
next two sections, develop specific algorithms insights
agents use observations others improve models assess
relevance regions within state spaces. first focus homogeneous action
case, extend model deal heterogeneous actions.

4. Implicit Imitation Homogeneous Settings
begin describing implicit imitation homogeneous action settingsthe extension
heterogeneous settings build insights developed section. develop
technique called implicit imitation observations mentor used
accelerate reinforcement learning. First, define homogeneous setting.
develop implicit imitation algorithm. Finally, demonstrate implicit imitation
works number simple problems designed illustrate role various mechanisms describe.
5. describe models detail Section 8.

582

fiImplicit Imitation

4.1 Homogeneous Actions
homogeneous action setting defined follows. assume single mentor
observer o, individual MDPs Mm = hS, , Prm , Rm Mo = hS, Ao , Pro , Ro i,
respectively. Note agents share state space (more precisely, assume
trivial isomorphic mapping allows us identify local states). assume
mentor executing stationary policy . often treat policy
deterministic, remarks apply stochastic policies well. Let support
set Supp(m , s) state set actions accorded nonzero probability
state s. assume observer abilities mentor
following sense: s, S, Supp(m , s), exists action ao Ao
Pro (s, ao , t) = Prm (s, , t). words, observer able duplicate (in sense
inducing distribution successor states) actual behavior mentor;
equivalently, agents local state spaces isomorphic respect actions
actually taken mentor subset states actions might taken.
much weaker requiring full homomorphism Sm . course,
existence full homomorphism sufficient perspective; results
require this.
4.2 Implicit Imitation Algorithm
implicit imitation algorithm understood terms component processes.
First, extract action models mentor. integrate information
observers value estimates augmenting usual Bellman backup mentor
action models. confidence testing procedure ensures use augmented
model observers model mentor reliable observers model
behavior. extract occupancy information observations mentor
trajectories order focus observers computational effort (to extent) specific
parts state space. Finally, augment action selection process choose actions
explore high-value regions revealed mentor. remainder section
expands upon processes fit together.
4.2.1 Model Extraction
information available observer quest learn act optimally
divided two categories. First, action takes, receives experience tuple
hs, a, r, ti; fact, often ignore sampled reward r, since assume reward
function R known advance. standard model-based learning, experience
used update transition model Pro (s, a, ).
Second, mentor transition, observer obtains experience tuple hs, ti.
Note observer direct access action taken mentor,
induced state transition. Assume mentor implementing deterministic,
stationary policy , (s) denoting mentors choice action state s.
policy induces Markov chain Prm (, ) S, Prm (s, t) = Pr(s, (s), t) denoting
583

fiPrice & Boutilier

probability transition t.6 Since learner observes mentors state
c chain: Pr
c (s, t) simply estimated
transitions, construct estimate Pr
relative observed frequency mentor transitions (w.r.t. transitions taken
s). observer prior possible mentor transitions, standard Bayesian
update techniques used instead. use term model extraction process
estimating mentors Markov chain.
4.2.2 Augmented Bellman Backups
c mentors Markov chain.
Suppose observer constructed estimate Pr
homogeneity assumption, action (s) replicated exactly observer
state s. Thus, policy can, principle, duplicated observer (were able
identify actual actions used). such, define value mentors policy
observers perspective:

Vm (s) = Ro (s) +

X

Prm (s, t)Vm (t)

(6)

tS

Notice Equation 6 uses mentors dynamics observers reward function.
Letting V denote optimal (observers) value function, clearly V (s) Vm (s), Vm
provides lower bound observers value function.
importantly, terms making Vm (s) integrated directly Bellman equation observers MDP, forming augmented Bellman equation:
(

(

V (s) = Ro (s) + max max
aAo

X

)

Pro (s, a, t)V (t) ,

tS

X

)

Prm (s, t)V (t)

(7)

tS

usual Bellman equation extra term added, namely, second
P
summation, tS Prm (s, t)V (t) denoting expected value duplicating mentors
action . Since (unknown) action identical one observers actions,
term redundant augmented value equation valid. course, observer using
augmented backup operation must rely estimates quantities. observer
exploration policy ensures state visited infinitely often, estimates Pro
terms converge true values. mentors policy ergodic state space S,
Prm converge true value. mentors policy restricted subset
states 0 (those forming basis Markov chain), estimates Prm
subset converge correctly respect 0 chain ergodic. states
0 remain unvisited estimates remain uninformed data. Since
mentors policy control observer, way observer
influence distribution samples attained Prm . observer must therefore
able reason accuracy estimated model Prm restrict
application augmented equation states Prm known sufficient
accuracy.
6. somewhat imprecise, since initial distribution Markov chain unknown.
purposes, dynamics relevant observer, transition probabilities
used.

584

fiImplicit Imitation

Prm cannot used indiscriminately, argue highly informative
early learning process. Assuming mentor pursuing optimal policy (or
least behaving way tends visit certain states frequently),
many states observer much accurate estimates Prm (s, t)
Pro (s, a, t) specific a. Since observer learning, must explore
state spacecausing less frequent visits sand action spacethus spreading
experience actions a. generally ensures sample size upon
Prm based greater Pro action forms part mentors
policy. Apart accurate, use Prm (s, t) often give informed
value estimates state s, since prior action models often flat uniform,
become distinguishable given state observer sufficient experience state
s.
note reasoning holds even mentor implementing (stationary) stochastic policy (since expected value stochastic policy fully-observable
MDP cannot greater optimal deterministic policy). direction offered mentor implementing deterministic policy tends focused,
empirically found mentors offer broader guidance moderately stochastic
environments implement stochastic policies, since tend visit
state space. note extension multiple mentors straightforwardeach
mentor model incorporated augmented Bellman equation without difficulty.
4.2.3 Model Confidence
mentors Markov chain ergodic, mixing rate7 sufficiently low,
mentor may visit certain state relatively infrequently. estimated mentor transition
model corresponding state rarely (or never) visited mentor may provide
misleading estimatebased small sample prior mentors chainof
value mentors (unknown) action s; since mentors policy
control observer, misleading value may persist extended period. Since
augmented Bellman equation consider relative reliability mentor
observer models, value state may overestimated;8 is, observer
tricked overvaluing mentors (unknown) action, consequently overestimating
value state s.
overcome this, incorporate estimate model confidence augmented
backups. mentors Markov chain observers action transitions,
assume Dirichlet prior parameters multinomial distributions
(DeGroot, 1975). reflect observers initial uncertainty possible transition probabilities. sample counts mentor observer transitions, update
distributions. information, could attempt perform optimal Bayesian
estimation value function; sample counts small (and normal approximations appropriate), simple, closed form expression resultant
distributions values. could attempt employ sampling methods, in7. mixing rate refers quickly Markov chain approaches stationary distribution.
8. Note underestimates based considerations problematic, since augmented Bellman
equation reduces usual Bellman equation.

585

fiPrice & Boutilier

V


VO-

VO VM

Figure 1: Lower bounds action values incorporate uncertainty penalty
terest simplicity employed approximate method combining information
sources inspired Kaelblings (1993) interval estimation method.
Let V denote current estimated augmented value function, Pro Prm denote
2 denote variance
estimated observer mentor transition models. let o2
model parameters.
augmented Bellman backup respect V using confidence testing proceeds
follows. first compute observers optimal action ao based estimated
augmented values observers actions. Let Q(ao , s) = Vo (s) denote value.
best action, use model uncertainty encoded Dirichlet distribution
construct lower bound Vo (s) value state observer using model
(at state s) derived behavior (i.e., ignoring observations mentor).
employ transition counts (s, a, t) nm (s, t) denote number times
observer made transition state state action performed,
number times mentor observed making transition state
t, respectively. counts, estimate uncertainty model using
P
variance Dirichlet distribution. Let = (s, a, t) = t0 St (s, a, t0 ).
model variance is:
2
model
(s, a, t) =

( +

)2

+
+ ( + + 1)

(8)

variance Q-value action due uncertainty local model
found simple application rule combining linear combinations variances, V ar(cX + dY ) = c2 V ar(X) + d2 V ar(Y ) expression Bellman backup,
P
V ar(R(s) + P r(t|s, a)V (t). result is:
2 (s, a) = 2

X


2
model
(s, a, t)v(t)2

(9)

Using Chebychevs inequality,9 obtain confidence level even though Dirichlet
distributions small sample counts highly non-normal. lower bound
Vo (s) = Vo (s)co (s, ao ) suitable constant c. One may interpret penalizing
9. Chebychevs inequality states 1 k12 probability mass arbitrary distribution
within k standard deviations mean.

586

fiImplicit Imitation

2
2
FUNCTION augmentedBackup( V ,Pro ,omodel
,Prm ,mmodel
,s,c)

= arg maxaAo

P
tS

Pr(s, a, t)V (t)

P

Vo (s) = Ro (s) + PtS Pro (s, , t)V (t)
Vm (s) = Ro (s) + tS Prm (s, t)V (t)

P

2
2

o2 (s, ) =
(t)2
P tS2 omodel (s, , t)V
2
2
2
(s) =

(s, t)V (t)
tS mmodel

Vo (s) = Vo (s) c (s, )

Vm
(s) = Vm (s) c (s)
Vo (s) > Vm (s)
V (s) = Vo (s)
ELSE
V (s) = Vm (s)
END

Table 1: Implicit Backup
value state subtracting uncertainty (see Figure 1).10 value
Vm (s) mentors action (s) estimated similarly analogous lower bound
Vm (s) constructed. Vo (s) > Vm (s), say Vo (s) supersedes
Vm (s) write Vo (s) Vm (s). Vo (s) Vm (s) either mentor-inspired
model has, fact, lower expected value (within specified degree confidence)
uses nonoptimal action (from observers perspective), mentor-inspired model
lower confidence. either case, reject information provided mentor
use standard Bellman backup using action model derived solely observers
experience (thus suppressing augmented backup)the backed value Vo (s)
case.
algorithm computing augmented backup using confidence test shown
Table 1. algorithm parameters include current estimate augmented value
2
function V , current estimated model Pro associated local variance omodel
,
2
model mentors Markov chain Prm associated variance mmodel .
calculates lower bounds returns mean value, Vo Vm , greatest lower
bound. parameter c determines width confidence interval used mentor
rejection test.
4.2.4 Focusing
augmented Bellman backups improves accuracy observers model. second
way observer exploit observations mentor focus attention
states visited mentor. model-based approach, specific focusing mecha10. Ideally, would take uncertainty model current state account,
uncertainty future states well (Meuleau & Bourgine, 1999).

587

fiPrice & Boutilier

nism adopt require observer perform (possibly augmented) Bellman backup
state whenever mentor makes transition s. three effects. First,
mentor tends visit interesting regions space (e.g., shares certain reward structure observer), significant values backed mentor-visited states
bias observers exploration towards regions. Second, computational effort
c (s, t) changes,
concentrated toward parts state space estimated model Pr
hence estimated value one observers actions may change. Third,
computation focused model likely accurate (as discussed above).
4.2.5 Action Selection
integration exploration techniques action selection policy important
reinforcement learning algorithm guarantee convergence. implicit imitation, plays
second, crucial role helping agent exploit information extracted mentor.
improved convergence results rely greedy quality exploration strategy
bias observer towards higher-valued trajectories revealed mentor.
expediency, adopted -greedy action selection method, using exploration rate decays time. could easily employed semi-greedy
methods Boltzmann exploration. presence mentor, greedy action selection becomes complex. observer examines actions state usual
way obtains best action ao corresponding value Vo (s). value
calculated mentors action Vm (s). Vo (s) Vm (s), observers action
model used greedy action defined exactly mentor present.
If, however, Vm (s) Vo (s) would define greedy action action
dictated mentors policy state s. Unfortunately, observer know
action is, define greedy action observers action closest
mentors action according observers current model estimates s. precisely,
action similar mentors state s, denoted (s), whose outcome
distribution minimum Kullback-Leibler divergence mentors action outcome
distribution:
(

(s) = argmina

X

)

Pro (s, a, t) log Prm (s, t)

(10)



observers experience-based action models poor early training,
chance closest action computation select wrong action. rely
exploration policy ensure observers actions sampled appropriately
long run.11
present work assumed state space large agent
therefore able completely update Q-function whole space. (The
intractability updating entire state space one motivations using imitation
techniques). absence information states true values, would
bias value states along mentors trajectories look worthwhile
explore. assuming bounds reward function setting initial Qvalues entire space bound. simple examples, rewards strictly
11. mentor executing stochastic policy, test based KL-divergence mislead learner.

588

fiImplicit Imitation

positive set bounds zero. mentor trajectories intersect states valued
observing agent, backups cause states trajectories higher
value surrounding states. causes greedy step exploration method
prefer actions lead mentor-visited states actions agent
information.
4.2.6 Model Extraction Specific Reinforcement Learning Algorithms
Model extraction, augmented backups, focusing mechanism, extended notion
greedy action selection, integrated model-based reinforcement learning algorithms relative ease. Generically, implicit imitation algorithm requires that: (a)
c (s, t) Markov chain induced mentors
observer maintain estimate Pr
policythis estimate updated every observed transition; (b) backups
performed estimate value function use augmented backup (Equation 7) confic (s, a, t)
dence testing. course, backups implemented using estimated models Pr
c
Prm (s, t). addition, focusing mechanism requires augmented backup
performed state visited mentor.
demonstrate generality mechanisms combining wellknown efficient prioritized sweeping algorithm (Moore & Atkeson, 1993). outlined
c
Section 2.2, prioritized sweeping works maintaining estimated transition model Pr
b Whenever experience tuple hs, a, r, ti sampled, estimated
reward model R.
model state change; Bellman backup performed incorporate revised
model (usually fixed) number additional backups performed selected
states. States selected using priority estimates potential change values
based changes precipitated earlier backups. Effectively, computational resources
(backups) focused states benefit backups.
Incorporating ideas prioritized sweeping simply requires following changes:
c (s, a, t) transition hs, a, ti observer takes, estimated model Pr
dated augmented backup performed state s. Augmented backups
performed fixed number states using usual priority queue implementation.
c (s, t) updated
observed mentor transition hs, ti, estimated model Pr
augmented backup performed s. Augmented backups performed
fixed number states using usual priority queue implementation.

Keeping samples mentor behavior implements model extraction. Augmented backups
integrate information observers value function, performing augmented
backups observed transitions (in addition experienced transitions) incorporates
focusing mechanism. observer forced follow otherwise mimic actions
mentor directly. back value information along mentors trajectory
had. Ultimately, observer must move states discover actions
used; meantime, important value information propagated
guide exploration.
Implicit imitation alter long run theoretical convergence properties
underlying reinforcement learning algorithm. implicit imitation framework orthogonal -greedy exploration, alters definition greedy action,
589

fiPrice & Boutilier

greedy action taken. Given theoretically appropriate decay factor, -greedy
strategy thus ensure distributions action models state
sampled infinitely often limit converge true values. Since extracted
model mentor corresponds one observers actions, effect
value function calculations different effect observers sampled
action models. confidence mechanism ensures model samples
eventually come dominate is, fact, better. therefore sure convergence properties reinforcement learning implicit imitation identical
underlying reinforcement learning algorithm.
benefit implicit imitation lies way models extracted
mentor allow observer calculate lower bound value function use
lower bound choose greedy actions move agent towards higher-valued regions
state space. result quicker convergence optimal policies better short-term
practical performance respect accumulated discounted reward learning.
4.2.7 Extensions
implicit imitation model easily extended extract model information
multiple mentors, mixing matching pieces extracted mentor achieve good
results. searching, state, set mentors knows find
mentor highest value estimate. value estimate best mentor
compared using confidence test described observers value estimate.
formal expression algorithm given multi-augmented Bellman equation:
(

(

V (s) = Ro (s) + max max
aAo

max

mM

X

X

)

Pro (s, a, t)V (t)

,

tS

)

Prm (s, t)V (t)

(11)

tS

set candidate mentors. Ideally, confidence estimates taken
account comparing mentor estimates other, may get mentor
high mean value estimate large variance. observer experience
state all, mentor likely rejected poorer quality information
observer already experience. observer might
better picking mentor lower mean confident estimate would
succeeded test observers model. interests simplicity,
however, investigate multiple mentor combination without confidence testing.
now, assumed action costs (i.e., agents rewards depend
state action selected state); however, use general
reward functions (e.g., reward form R(s, a)). difficulty lies backing
action costs mentors chosen action unknown. Section 4.2.5 defined
closest action function . function used choose appropriate reward.
augmented Bellman equation generalized rewards takes following form:
(

(

V (s) = max max Ro (s, a) +
aAo

X
tS

590

)

Pro (s, a, t)V (t)

,

fiImplicit Imitation

Ro (s, (s)) +

X

)

Prm (s, t)V (t)

tS

note Bayesian methods could used could used estimate action costs
mentors chain well. case, generalized reward augmented equation
readily amended use confidence estimates similar fashion transition model.
4.3 Empirical Demonstrations
following empirical tests incorporate model extraction focusing mechanism
prioritized sweeping. results illustrate types problems scenarios
implicit imitation provide advantages reinforcement learning agent.
experiments, expert mentor introduced experiment serve model
observer. case, mentor following -greedy policy small (on
order 0.01). tends cause mentors trajectories lie within cluster
surrounding optimal trajectories (and reflect good optimal policies). Even
small amount exploration environment stochasticity, mentors generally
cover entire state space, confidence testing important.
experiments, prioritized sweeping used fixed number backups per observed experienced sample.12 -greedy exploration used decaying .
Observer agents given uniform Dirichlet priors Q-values initialized zero. Observer agents compared control agents benefit mentors experience,
otherwise identical (implementing prioritized sweeping similar parameters
exploration policies). tests performed stochastic grid world domains, since
make clear extent observers mentors optimal policies overlap (or
fail to). Figure 2, simple 10 10 example shows start end state grid.
typical optimal mentor trajectory illustrated solid line start
end states. dotted line shows typical mentor-influenced trajectory quite
similar observed mentor trajectory. assume eight-connectivity cells
state grid nine neighbors including itself, agents four
possible actions. experiments, four actions move agent compass
directions (North, South, East West), although agent initially know
action which. focus primarily whether imitation improves performance
learning, since learner converge optimal policy whether uses imitation
not.
4.3.1 Experiment 1: Imitation Effect
first experiment compare performance observer using model extraction
expert mentor performance control agent using independent reinforcement learning. Given uniform nature grid world lack intermediate
rewards, confidence testing required. agents attempt learn policy
maximizes discounted return 10 10 grid world. start upper-left corner
seek goal value 1.0 lower-right corner. Upon reaching goal, agents
12. Generally, number backups set roughly equal length optimal noise-free
path.

591

fiPrice & Boutilier



X

Figure 2: simple grid world start state goal state X

50
Obs
FA Series
Ctrl

Average Reward per 1000 Steps

40

30

20

10

Delta

0

10

0

500

1000

1500

2000

2500

3000

3500

4000

4500

Simulation Steps

Figure 3: Basic observer control agent comparisons

restarted upper-left corner. Generally mentor follow similar identical trajectory run, mentors trained using greedy strategy leaves
one path slightly highly valued rest. Action dynamics noisy,
intended direction realized 90% time, one directions taken
otherwise (uniformly). discount factor 0.9. Figure 3, plot cumulative
number goals obtained previous 1000 time steps observer Obs
control Ctrl agents (results averaged ten runs). observer able quickly
incorporate policy learned mentor value estimates. results
steeper learning curve. contrast, control agent slowly explores space build
model first. Delta curve shows difference performance agents.
agents converge optimal value function.
592

fiImplicit Imitation

30

25

Average Reward per 1000 Steps

20

Basic
Scale

15

10
Stoch
5

0

5

0

1000

2000

3000

4000

5000

6000

Simulation Steps

Figure 4: Delta curves showing influence domain size noise

4.3.2 Experiment 2: Scaling Noise
next experiment illustrates sensitivity imitation size state space
action noise level. Again, observer uses model-extraction confidence testing.
Figure 4, plot Delta curves (i.e., difference performance observer
control agents) Basic scenario described, Scale scenario
state space size increased 69 percent (to 13 13 grid), Stoch scenario
noise level increased 40 percent (results averaged ten runs).
total gain represented area curves observer non-imitating
prioritized sweeping agent increases state space size. reflects Whiteheads
(1991a) observation grid worlds, exploration requirements increase quickly
state space size, optimal path length increases linearly. see
guidance mentor help larger state spaces.
Increasing noise level reduces observers ability act upon information
received mentor therefore erodes advantage control agent.
note, however, benefit imitation degrades gracefully increased noise
present even relatively extreme noise level.
4.3.3 Experiment 3: Confidence Testing
Sometimes observers prior beliefs transition probabilities mentor
mislead observer cause generate inappropriate values. confidence mechanism proposed previous section prevent observer fooled
misleading priors mentors transition probabilities. demonstrate role
confidence mechanism implicit imitation, designed experiment based scenario illustrated Figure 5. Again, agents task navigate top-left corner
bottom-right corner 10 10 grid order attain reward +1. cre593

fiPrice & Boutilier

+5

+5

+5

+5

Figure 5: environment misleading priors
ated pathological scenario islands high reward (+5) enclosed obstacles.
Since observers priors reflect eight-connectivity uniform, high-valued cells
middle island believed reachable states diagonally adjacent
small prior probability. reality, however, agents action set precludes
agent therefore never able realize value. four islands
scenario thus create fairly large region center space high estimated
value, could potentially trap observer persisted prior beliefs.
Notice standard reinforcement learner quickly learn none actions
take rewarding islands; contrast, implicit imitator using augmented backups
could fooled prior mentor model. mentor visit states neighboring
island, observer evidence upon change prior belief
mentor actions equally likely take one eight possible directions.
imitator may falsely conclude basis mentor action model action
exist would allow access islands value. observer therefore needs
confidence mechanism detect mentor model less reliable model.
test confidence mechanism, mentor follows path around outside
obstacles path cannot lead observer trap (i.e., provides
evidence observer diagonal moves islands feasible).
combination high initial exploration rate ability prioritized sweeping
spread value across large distances virtually guarantees observer led
trap. Given scenario, ran two observer agents control. first observer
used confidence interval width given 5, which, according Chebychev rule,
cover approximately 96 percent arbitrary distribution. second observer
given 0 interval, effectively disables confidence testing. observer
confidence testing consistently became stuck. Examination value function revealed
consistent peaks within trap region, inspection agent state trajectories showed
stuck trap. observer confidence testing consistently escaped
trap. Observation value function time shows trap formed, faded
away observer gained enough experience actions allow ignore
594

fiImplicit Imitation

45
CR Series

40

Ctrl

35

Average Reward per 1000 Steps

Obs
30

25

20

15

10

5

0

5

Delta

0

2000

4000

6000

8000

10000

12000

Simulation Steps

Figure 6: Misleading priors may degrade performance
overcome erroneous priors mentor actions. Figure 6, performance
observer confidence testing shown performance control agent (results
averaged 10 runs). see observers performance slightly degraded
unaugmented control agent even pathological case.
4.3.4 Experiment 4: Qualitative Difficulty
next experiment demonstrates potential gains imitation increase
(qualitative) difficulty problem. observer employs model extraction
confidence testing, though confidence testing play significant role here.13
maze scenario, introduce obstacles order increase difficulty learning
problem. maze set 25 25 grid (Figure 7) 286 obstacles complicating
agents journey top-left bottom-right corner. optimal solution takes
form snaking 133-step path, distracting paths (up length 22) branching
solution path necessitating frequent backtracking. discount factor 0.98.
10 percent noise, optimal goal-attainment rate six goals per 1000 steps.
graph Figure 8 (with results averaged ten runs), see control
agent takes order 200,000 steps build decent value function reliably leads
goal. point, achieving four goals per 1000 steps average,
exploration rate still reasonably high (unfortunately, decreasing exploration quickly
leads slower value function formation). imitation agent able take advantage
mentors expertise build reliable value function 20,000 steps. Since
control agent unable reach goal first 20,000 steps, Delta
control imitator simply equal imitators performance.
13. mentor provide evidence path choices problem,
intermediate rewards would cause observer make use misleading mentor priors
states.

595

fiPrice & Boutilier

Figure 7: complex maze
7

CMB Series
6

Average Reward per 1000 Steps

5

4

3

Obs

2

Delta

1

0

0

Ctrl

0.5

1

1.5

2

2.5
5

x 10

Simulation Steps

Figure 8: Imitation complex space
imitator quickly achieve optimal goal attainment rate six goals per 1000 steps,
exploration rate decays much quickly.
4.3.5 Experiment 5: Improving Suboptimal Policies Imitation
augmented backup rule require reward structure mentor
observer identical. many useful scenarios rewards dissimilar
value functions policies induced share structure. experiment,
demonstrate one interesting scenario relatively easy find suboptimal
solution, difficult find optimal solution. observer finds suboptimal
path, however, able exploit observations mentor see
596

fiImplicit Imitation

1

4

*
*
*
*
*
*
*
*
*

*
*
*
*
*
*
*
*
*

2

3
5

Figure 9: maze perilous shortcut

shortcut significantly shortens path goal. structure scenario
shown Figure 9. suboptimal solution lies path location 1 around
scenic route location 2 goal location 3. mentor takes
vertical path location 4 location 5 shortcut.14 discourage
use shortcut novice agents, lined cells (marked *)
agent immediately jumps back start state. therefore difficult novice agent
executing random exploratory moves make way end shortcut
obtain value would reinforce future use. observer control
therefore generally find scenic route first.
Figure 10, performance (measured using goals reached previous 1000
steps) control observer compared (averaged ten runs), indicating
value observations. see observer control agent find longer
scenic route, though control agent takes longer find it. observer goes find
shortcut increases return almost double goal rate. experiment shows
mentors improve observer policies even observers goals
mentors path.
4.3.6 Experiment 6: Multiple Mentors
final experiment illustrates model extraction readily extended
observer extract models multiple mentors exploit valuable parts
each. Again, observer employs model extraction confidence testing. Figure 11,
learner must move start location 1 goal location 4. Two expert agents
different start goal states serve potential mentors. One mentor repeatedly moves
location 3 location 5 along dotted line, second mentor departs location
2 ends location 4 along dashed line. experiment, observer must
14. mentor proceeding 5 4 would provide guidance without prior knowledge actions
reversible.

597

fiPrice & Boutilier

35

CSB Series
30

Average Reward per 1000 Steps

25

Obs

20
Delta
15

10
Ctrl
5

0

0

0.5

1

1.5

2

2.5

3
4

x 10

Simulation Steps

Figure 10: Transfer non-identical rewards

1

3

2

5

4

Figure 11: Multiple mentors scenario

combine information examples provided two mentors independent
exploration order solve problem.
Figure 12, see observer successfully pulls together information
sources order learn much quickly control agent (results averaged
10 runs). see use value-based technique allows observer choose
mentors influence use state-by-state basis order get best solution
problem.
598

fiImplicit Imitation

60

Obs
CMM Series

Average Reward per 1000 Steps

50

40

Ctrl
30

20
Delta

10

0

0

1000

2000

3000

4000

5000

6000

Simulation Steps

Figure 12: Learning multiple mentors

5. Implicit Imitation Heterogeneous Settings
homogeneity assumption violated, implicit imitation framework described
cause learners convergence rate slow dramatically and, cases,
cause learner become stuck small neighborhood state space. particular,
learner unable make state transition (or transition
probability) mentor given state, may drastically overestimate value
state. inflated value estimate causes learner return repeatedly state
even though exploration never produce feasible action attains inflated
estimated value. mechanism removing influence mentors Markov
chain value estimatesthe observer extremely (and correctly) confident
observations mentors model. problem lies fact augmented
Bellman backup justified assumption observer duplicate every mentor
action. is, state s, Pro (s, a, t) = Prm (s, t)
t. equivalent action exist, guarantee value
calculated using mentor action model can, fact, achieved.
5.1 Feasibility Testing
heterogeneous settings, prevent lock-up poor convergence
use explicit action feasibility test: augmented backup performed s,
observer tests whether mentors action differs actions s, given
current estimated models. so, augmented backup suppressed standard
Bellman backup used update value function. 15 default, mentor actions
15. decision binary; could envision smoother decision criterion measures extent
mentors action duplicated.

599

fiPrice & Boutilier

assumed feasible observer; however, observer reasonably confident
infeasible state s, augmented backups suppressed s.
Recall uncertainty agents true transition probabilities captured
Dirichlet distribution derived sampled transitions. Comparing ao effected
difference means test respect corresponding Dirichlets. complicated
fact Dirichlets highly non-normal small parameter values transition
distributions multinomial. deal non-normality requiring minimum
number samples using robust Chebychev bounds pooled variance
distributions compared. Conceptually, evaluate Equation 12:
r

| Pro (s, ao , t) Prm (s, t)|
2
2
(s,ao ,t)omodel
(s,ao ,t)+nm (s,t)mmodel
(s,t)
(s,ao ,t)+nm (s,t)

> Z/2

(12)

Z/2 critical value test. parameter significance test,
probability falsely reject two actions different
actually same. Given highly non-normal distributions early training process,
appropriate Z value given computed Chebychevs bound solving
2 = 1 Z12 Z/2 .
samples accurate test, persist augmented
backups (embodying default assumption homogeneity). value estimate
inflated backups, agent biased obtain additional samples,
allow agent perform required feasibility test. assumption therefore
self-correcting. deal multivariate complications performing Bonferroni
test (Seber, 1984), shown give good results practice (Mi & Sampson,
1993), efficient compute, known robust dependence variables.
Bonferroni hypothesis test obtained conjoining several single variable tests. Suppose
actions ao result r possible successor states, s1 , , sr (i.e., r transition
probabilities compare). si , hypothesis Ei denotes ao
transition probability successor state si ; Pr(s, , si ) = Pr(s, ao , si ). let
Ei denote complementary hypothesis (i.e., transition probabilities differ).
Bonferroni inequality states:
"

Pr

r
\

#

Ei 1

i=1

r
X



Pr Ei



i=1



Thus test joint hypothesis ri=1 Ei two action models sameby
testing r complementary hypotheses Ei confidence level /r. reject
hypotheses reject notion two actions equal confidence
least . mentor action deemed infeasible every observer action ao ,
multivariate Bonferroni test described rejects hypothesis action
mentors.
Pseudo-code Bonferroni component feasibility test appears Table 2.
assumes sufficient number samples. efficiency reasons, cache results
feasibility testing. duplication mentors action state first determined
infeasible, set flag state effect.
600

fiImplicit Imitation

FUNCTION feasible(m,s) : Boolean
Ao
allSuccessorProbsSimilar = true
successors(s)
= |P roq
(s, a, t) P rm (s, t)|
(s,a,t) 2

(s,a,t)+nm (s,t) 2

omodel
z = /
(s,a,t)+nm (s,t)
z > z/2r
allSuccessorProbsSimilar = false
END
allSuccessorProbsSimilar
return true
END
RETURN false

mmodel

(s,t)

Table 2: Action Feasibility Testing
5.2 k-step Similarity Repair
Action feasibility testing essentially makes strict decision whether agent
duplicate mentors action specific state: decided mentors action
infeasible, augmented backups suppressed potential guidance offered eliminated
state. Unfortunately, strictness test results somewhat impoverished
notion similarity mentor observer. This, turn, unnecessarily limits
transfer mentor observer. propose mechanism whereby mentors
influence may persist even specific action chooses feasible mentor;
instead rely possibility observer may approximately duplicate mentors
trajectory instead exactly duplicating it.
Suppose observer previously constructed estimated value function using augmented backups. Using mentor action model (i.e., mentors chain Prm (s, t)), high
value calculated state s. Subsequently, suppose mentors action state
judged infeasible. illustrated Figure 13, estimated value state
originally due mentors action (s), sake illustration moves
high probability state t, lead highly-rewarding region
state space. number experiences state s, however, learner concludes
action (s)and associated high probability transition tis feasible.
point, one two things must occur: either (a) value calculated state
predecessors collapse exploration towards highly-valued regions
beyond state ceases; (b) estimated value drops slightly exploration continues
towards highly-valued regions. latter case may arise follows. observer
previously explored vicinity state s, observers action model may
sufficiently developed still connect higher value-regions beyond state state
Bellman backups. example, learner sufficient experience
learned highly-valued region reached alternative trajectory
u v w, newly discovered infeasibility mentors transition
deleterious effect value estimate s. highly-valued, likely states
close mentors trajectory explored degree. case, state
601

fiPrice & Boutilier

Infeasible Transition



High-value
State



w
"Bridge"

u

v

Figure 13: alternative path bridge value backups around infeasible paths
highly-valued using mentors action model, still
valued highly enough likely guide exploration toward area. call
alternative (in case u v w) mentors action bridge, allows
value higher value regions flow infeasible mentor transition.
bridge formed without intention agent, call process spontaneous
bridging.
spontaneous bridge exist, observers action models generally undeveloped (e.g., close uniform prior distributions). Typically,
undeveloped models assign small probability every possible outcome therefore diffuse value higher valued regions lead poor value estimate state s.
result often dramatic drop value state predecessors;
exploration towards highly-valued region neighborhood state ceases.
example, could occur observers transition models state assign low
probability (e.g., close prior probability) moving state u due lack experience
(or similarly surrounding states, u v, insufficiently explored).
spontaneous bridging effect motivates broader notion similarity.
observer find short sequence actions bridges infeasible action
mentors trajectory, mentors example still provide extremely useful guidance.
moment, assume short path path length greater given
integer k. say observer k-step similar mentor state observer
duplicate k fewer steps mentors nominal transition state sufficiently
high probability.
Given notion similarity, observer test whether spontaneous bridge
exists determine whether observer danger value function collapse
concomitant loss guidance decides suppress augmented backup state s.
this, observer initiates reachability analysis starting state using action
model Pro (s, a, t) determine sequence actions leads sufficiently
high probability state state mentors trajectory downstream
infeasible action.16 k-step bridge already exists, augmented backups safely
suppressed state s. efficiency, maintain flag state mark bridged.
state known bridged, k-step reachability analysis need repeated.
spontaneous bridge cannot found, might still possible intentionally set
build one. build bridge, observer must explore state k-steps away,
hoping make contact mentors trajectory downstream infeasible mentor
16. general state space ergodicity lacking, agent must consider predecessors state
k steps guarantee k-step paths checked.

602

fiImplicit Imitation

action. implement single search attempt k2 -step random walk, result
trajectory average k steps away long ergodicity local connectivity
assumptions satisfied. order search occur, must motivate observer
return state engage repeated exploration. could provide motivation
observer asking observer assume infeasible action repairable.
observer therefore continue augmented backups support high-value estimates
state observer repeatedly engage exploration point.
danger, course, may fact bridge, case observer
repeat search bridge indefinitely. therefore need mechanism terminate
repair process k-step repair infeasible. could attempt explicitly keep
track possible paths open observer paths explicitly tried
observer determine repair possibilities exhausted. Instead, elect
follow probabilistic search eliminates need bookkeeping: bridge cannot
constructed within n attempts k-step random walk, repairability assumption
judged falsified, augmented backup state suppressed observers bias
explore vicinity state eliminated. bridge found state s, flag used
mark state irreparable.
approach is, course, nave heuristic strategy; illustrates basic
import bridging. systematic strategies could used, involving explicit planning
find bridge using, say, local search (Alissandrakis, Nehaniv, & Dautenhahn, 2000).
Another aspect problem address persistence search
bridges. specific domain, number unsuccessful attempts find bridges,
learner may conclude unable reconstruct mentors behavior, case
search bridges may abandoned. involves simple, higher-level inference,
notion (or prior beliefs about) similarity capabilities. notions could
used automatically determine parameter settings (discussed below).
parameters k n must tuned empirically, estimated given knowledge connectivity domain prior beliefs similar (in terms
length average repair) trajectories mentor observer be. instance,
n > 8k 4 seems suitable 8-connected grid world low noise, based number
trajectories required cover perimeter states k-step rectangle around state.
note large values n reduce performance non-imitating
agents results temporary lock up.
Feasibility k-step repair easily integrated homogeneous implicit imitation framework. Essentially, simply elaborate conditions augmented
backup employed. course, additional representation introduced
keep track whether state feasible, bridged, repairable, many repair attempts made. action selection mechanism overridden
bridge-building algorithm required order search bridge. Bridge building
always terminates n attempts, however, cannot affect long run convergence.
aspects algorithm, however, exploration policy, unchanged.
complete elaborated decision procedure used determine augmented backups
employed state respect mentor appears Table 3. uses
internal state make decisions. original model, first check see
observers experience-based calculation value state supersedes mentor603

fiPrice & Boutilier

FUNCTION use augmented?(s,m) : Boolean
Vo (s) Vm (s) RETURN false
ELSE f easible(s, m) RETURN true
ELSE bridged(s, m) RETURN false
ELSE reachable(s, m)
bridged(s,m) := true
RETURN false
ELSE repairable(s, m) return false
ELSE % searching
0 < search steps(s, m) < k % search progress
return true
search steps(s, m) > k % search failed
attempts(s) > n
repairable(s) = false
RETURN false
ELSE
reset search(s,m)
attempts(s) := attempts(s) + 1
RETURN true
attempts(s) :=1 % initiate first attempt search
initiate-search(s)
RETURN true

Table 3: Elaborated augmented backup test

based calculation; so, observer uses experience-based calculation.
mentors action feasible, accept value calculated using observationbased value function. action infeasible check see state bridged.
first time test requested, reachability analysis performed, results
drawn cache subsequent requests. state bridged, suppress
augmented backups, confident cause value function collapse. state
bridged, ask repairable. first n requests, agent attempt
k-step repair. repair succeeds, state marked bridged. cannot repair
infeasible transition, mark not-repairable suppress augmented backups.
may wish employ implicit imitation feasibility testing multiple-mentor
scenario. key change implicit imitation without feasibility testing
observer imitate feasible actions. observer searches set
mentors one action results highest value estimate, observer
must consider mentors whose actions still considered feasible (or assumed
repairable).
5.3 Empirical Demonstrations
section, empirically demonstrate utility feasibility testing k-step repair
show techniques used surmount differences actions
agents small local differences state-space topology. problems
604

fiImplicit Imitation

chosen specifically demonstrate necessity utility feasibility testing
k-step repair.
5.3.1 Experiment 1: Necessity Feasibility Testing
first experiment shows importance feasibility testing implicit imitation
agents heterogeneous actions. scenario, agents must navigate across
obstacle-free, 10 10 grid world upper-left corner goal location lowerright. agent reset upper-left corner. first agent mentor
NEWS action set (North, South, East, West movement actions). mentor
given optimal stationary policy problem. study performance three
learners, Skew action set (N, S, NE, SW) unable duplicate mentor
exactly (e.g., duplicating mentors E-move requires learner move NE followed
S, move SE N). Due nature grid world, control imitation
agents actually execute actions get goal mentor
optimal goal rate control imitator therefore lower
mentor. first learner employs implicit imitation feasibility testing, second uses
imitation without feasibility testing, third control agent uses imitation (i.e.,
standard reinforcement learning agent). agents experience limited stochasticity
form 5% chance action randomly perturbed. last section,
agents use model-based reinforcement learning prioritized sweeping. set k = 3
n = 20.
effectiveness feasibility testing implicit imitation seen Figure 14.
horizontal axis represents time simulation steps vertical axis represents
average number goals achieved per 1000 time steps (averaged 10 runs). see
imitation agent feasibility testing converges much quickly optimal
goal-attainment rate agents. agent without feasibility testing achieves
sporadic success early on, frequently locks due repeated attempts duplicate
infeasible mentor actions. agent still manages reach goal time time,
stochastic actions permit agent become permanently stuck obstaclefree scenario. control agent without form imitation demonstrates significant
delay convergence relative imitation agents due lack form guidance,
easily surpasses agent without feasibility testing long run. gradual
slope control agent due higher variance control agents discovery time
optimal path, feasibility-testing imitator control agent converge
optimal solutions. shown comparison two imitation agents, feasibility
testing necessary adapt implicit imitation contexts involving heterogeneous actions.
5.3.2 Experiment 2: Changes State Space
developed feasibility testing bridging primarily deal problem adapting
agents heterogeneous actions. techniques, however, applied
agents differences state-space connectivity (ultimately, equivalent
notions). test this, constructed domain agents NEWS
action set, alter environment learners introducing obstacles arent
present mentor. Figure 15, learners find mentors path obstructed
605

fiPrice & Boutilier

40

Feas

35
FS Series

Average Reward per 1000 Steps

30

25

Ctrl

20

15
NoFeas
10

5

0

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 14: Utility feasibility testing


X

Figure 15: Obstacle map mentor path
obstacles. Movement toward obstacle causes learner remain current state.
sense, action different effect mentors.
Figure 16, see results qualitatively similar previous experiment.
contrast previous experiment, imitator control use NEWS action set
therefore shortest path length mentor. Consequently,
optimal goal rate imitators control higher previous experiment.
observer without feasibility testing difficulty maze, value function
augmented mentor observations consistently leads observer states whose path
goal directly blocked. agent feasibility testing quickly discovers
mentors influence inappropriate states. conclude local differences
state well handled feasibility testing.
Next, demonstrate feasibility testing completely generalize mentors
trajectory. Here, mentor follows path completely infeasible imitating
agent. fix mentors path runs give imitating agent maze shown
606

fiImplicit Imitation

50
Feas
FO Series

45

Average Reward per 1000 Steps

40

35

Ctrl

30

25

20

15

10

5

0

NoFeas

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 16: Interpolating around obstacles



Observer
Mentor
X

Figure 17: Parallel generalization
Figure 17 two states mentor visits blocked obstacle.
imitating agent able use mentors trajectory guidance builds
parallel trajectory completely disjoint mentors.
results Figure 18 show gain imitator feasibility testing
control agent diminishes, still exists marginally imitator forced generalize
completely infeasible mentor trajectory. agent without feasibility testing
poorly, even compared control agent. gets stuck around
doorway. high value gradient backed along mentors path becomes accessible
agents doorway. imitation agent feasibility conclude cannot
proceed south doorway (into wall) try different strategy.
imitator without feasibility testing never explores far enough away doorway
setup independent value gradient guide goal. slower decay
schedule exploration, imitator without feasibility testing would find goal,
607

fiPrice & Boutilier

60

Feas
50

FP Series

Average Reward per 1000 Steps

Ctrl

40

30

20

10

NoFeas
0

0

500

1000

1500

2000
2500
Simulation Steps

3000

3500

4000

4500

Figure 18: Parallel generalization results

would still reduce performance imitator feasibility testing.
imitator feasibility testing makes use prior beliefs follow mentor
backup value perpendicular mentors path. value gradient therefore form
parallel infeasible mentor path imitator follow along side infeasible
path towards doorway makes necessary feasibility test proceeds
goal.
explained earlier, simple problems good chance informal effects
prior value leakage stochastic exploration may form bridges feasibility testing
cuts value propagation guides exploration. difficult problems
agent spends lot time exploring, accumulate sufficient samples conclude
mentors actions infeasible long agent constructed bridge.
imitators performance would drop unaugmented reinforcement
learner.
demonstrate bridging, devised domain agents must navigate
upper-left corner bottom-right corner, across river three steps wide
exacts penalty 0.2 per step (see Figure 19). goal state worth 1.0. figure,
path mentor shown starting top corner, proceeding along edge
river crossing river goal. mentor employs NEWS action
set. observer uses Skew action set (N, NE, S, SW) attempts reproduce
mentor trajectory. fail reproduce critical transition border
river (because East action infeasible Skew agent). mentor action
longer used backup value rewarding state alternative
paths river blocks greedy exploration region. Without bridging
optimistic lengthly exploration phase, observer agents quickly discover negative
states river curtail exploration direction actually making across.
608

fiImplicit Imitation

Figure 19: River scenario
examine value function estimate (after 1000 steps) imitator feasibility
testing repair capabilities, see that, due suppression feasibility testing,
darkly shaded high-value states Figure 19 (backed goal) terminate abruptly
infeasible transition without making across river. fact, dominated
lighter grey circles showing negative values. experiment, show bridging
prolong exploration phase right way. employ k-step repair
procedure k = 3.
Examining graph Figure 20, see imitation agents experience early
negative dip guided deep river mentors influence. agent
without repair eventually decides mentors action infeasible, thereafter avoids
river (and possibility finding goal). imitator repair discovers
mentors action infeasible, immediately dispense mentors
guidance. keeps exploring area mentors trajectory using random walk,
accumulating negative reward suddenly finds bridge rapidly
converges optimal solution.17 control agent discovers goal
ten runs.

6. Applicability
simple experiments presented demonstrate major qualitative issues confronting implicit imitation agent specific mechanisms implicit imitation
address issues. section, examine assumptions mechanisms presented previous sections determine types problems suitable
implicit imitation. present several dimensions prove useful predicting
performance implicit imitation types problems.
17. repair steps take place area negative reward scenario, need case.
Repair doesnt imply short-term negative return.

609

fiPrice & Boutilier

15

Average Reward per 1000 Steps

10

FB Series

5
Ctrl
Ctrl
0
NoRepair

Repair
5

NoRepair
10

15

20

Repair

0

1000

2000

3000
Simulation Steps

4000

5000

6000

Figure 20: Utility bridging
already identified number assumptions implicit imitation
applicablesome assumptions models imitation teaching cannot
applied, assumptions restrict applicability model. include:
lack explicit communication mentors observer; independent objectives
mentors observer; full observability mentors observer; unobservability mentors
actions; (bounded) heterogeneity. Assumptions full observability necessary
modelas formulatedto work (though discuss extension partially observable case Section 7). Assumptions lack communication unobservable actions
extend applicability implicit imitation beyond models literature;
conditions hold, simpler form explicit communication may preferable. Finally,
assumptions bounded heterogeneity independent objectives ensure implicit
imitation applied widely. However, degree rewards
actions homogeneous impact utility (i.e., acceleration learning offered by) implicit imitation. turn attention predicting performance
implicit imitation function certain domain characteristics.
6.1 Predicting Performance
section examine two questions: first, given implicit imitation applicable,
implicit imitation bias agent suboptimal solution; second,
performance implicit imitation vary structural characteristics domains
one might want apply to? show analysis internal structure state space
used motivate metric (roughly) predicts implicit imitation performance.
conclude analysis problem space understood terms
distinct regions playing different roles within imitation context.
610

fiImplicit Imitation

implicit imitation model, use observations agents improve observers knowledge environment rely sensible exploration policy
exploit additional knowledge. clear understanding knowledge environment affects exploration therefore central understanding implicit imitation
perform domain.
Within implicit imitation framework, agents know reward functions, knowledge environment consists solely knowledge agents action models.
general, models take form. simplicity, restricted
models decomposed local models possible combination system
state agent action.
local models state-action pairs allow prediction j-step successor state
distribution given initial state sequence actions local policy. quality
j-step state predictions function every action model encountered
initial state states time j 1. Unfortunately, quality j-step
estimate drastically altered quality even single intermediate state-action
model. suggests connected regions state space, states
fairly accurate models, allow reasonably accurate future state predictions.
Since estimated value state based immediate reward
reward expected received subsequent states, quality value estimate
depend quality action models states connected s. Now,
since greedy exploration methods bias exploration according estimated value
actions, exploratory choices agent state dependent
connectivity reliable action models states reachable s. analysis
implicit imitation performance respect domain characteristics therefore organized
around idea state space connectivity regions connectivity defines.
6.1.1 Imitation Regions Framework
Since connected regions play important role implicit imitation, introduce classification different regions within state space shown graphically Figure 21.
follows, describe regions affect imitation performance model.
first observe many tasks carried agent small subset
states within state space defined problem. precisely, many MDPs,
optimal policy ensure agent remains small subspace state space.
leads us definition first regional distinction: relevant vs. irrelevant regions.
relevant region set states non-zero probability occupancy optimal
policy.18 -relevant region natural generalization optimal policy keeps
system within region fraction 1 time.
Within relevant region, distinguish three additional subregions. explored
region contains states observer formulated reliable action models
basis experience. augmented region contains states observer
lacks reliable action models improved value estimates due mentor observations.
18. One often assumes system starts one small set states. Markov chain induced
optimal policy ergodic, irrelevant region nonempty. Otherwise
empty.

611

fiPrice & Boutilier

Observer
Explored
Region
Blind
Region

Mentor
Augmented
Region

Irrelevant
Region

Reward
Figure 21: Classification regions state space
Note explored augmented regions created result observations
made learner (of either transitions mentor). regions
therefore significant connected components; is, contiguous regions state space
reliable action mentor models available. Finally, blind region designates
states observer neither (significant) personal experience benefit
mentor observations. information states within blind region come
(largely) agents prior beliefs.19
ask regions interact imitation agent. First consider
impact relevance. Implicit imitation makes assumption accurate dynamics
models allow observer make better decisions will, turn, result higher returns
sooner learning process. However, model information equally helpful:
imitator needs enough information irrelevant region able avoid it.
Since action choices influenced relative value actions, irrelevant region
avoided looks worse relevant region. Given diffuse priors action
models, none actions open agent initially appear particularly attractive.
However, mentor provides observations within relevant region quickly make
relevant region look much promising method achieving higher returns
therefore constrain exploration significantly. Therefore, considering problems
point view relevance, problem small relevant region relative entire space
combined mentor operates within relevant region result maximum
advantage imitation agent non-imitating agent.
explored region, observer sufficiently accurate models compute good
policy respect rewards within explored region. Additional observations
19. partitioning states explored, blind augmented regions bears resemblance Kearns
Singhs (1998) partitioning state space known unknown regions. Unlike Kearns Singh,
however, use partitions analysis. implicit imitation algorithm explicitly
maintain partitions use way compute policy.

612

fiImplicit Imitation

states within explored region provided mentor still improve performance
somewhat significant evidence required accurately discriminate expected
value two actions. Hence, mentor observations explored region help,
result dramatic speedups convergence.
Now, consider augmented region observers Q-values
augmented observations mentor. experiments previous sections,
seen observer entering augmented region experience significant speedups
convergence due information inherent augmented value function
location rewards region. Characteristics augmented zone, however,
affect degree augmentation improves convergence speed.
Since observer receives observations mentors state, actions,
observer improved value estimates states augmented region, policy.
observer must therefore infer actions taken duplicate mentors
behavior. observer prior beliefs effects actions, may able
perform immediate inference mentors actual choice action (perhaps using
KL-divergence maximum likelihood). observers prior model uninformative,
observer explore local action space. exploring local action space,
however, agent must take action action effect. Since
guarantee agent took action duplicates mentors action, may end
somewhere different mentor. action causes observer fall outside
augmented region, observer lose guidance augmented value function
provides fall back performance level non-imitating agent.
important consideration, then, probability observer remain
augmented regions continue receive guidance. One quality augmented region
affects observers probability staying within boundaries relative coverage
state space. policy mentor may sparse complete. relatively
deterministic domain defined begin end states, sparse policy covering states
may adequate. highly stochastic domain many start end states, agent
may need complete policy (i.e., covering every state). Implicit imitation provide
guidance agent domains stochastic require complete
policies, since policy cover larger part state space.
important completeness policy predicting guidance, must
take account probability transitions augmented region.
actions domain largely invertible (directly, effectively so), agent
chance re-entering augmented region. ergodicity lacking, however,
agent may wait process undergoes form reset
opportunity gather additional evidence regarding identity mentors actions
augmented region. reset places agent back explored region,
make way frontier last explored. lack ergodicity
would reduce agents ability make progress towards high-value regions resets,
agent still guided attempt augmented region. Effectively,
agent concentrate exploration boundary explored region
mentor augmented region.
utility mentor observations depend probability augmented
explored regions overlapping course agents exploration. explored
613

fiPrice & Boutilier

regions, accurate action models allow agent move quickly possible high
value regions. augmented regions, augmented Q-values inform agents states
lead highly-valued outcomes. augmented region abuts explored region,
improved value estimates augmented region rapidly communicated across
explored region accurate action models. observer use resultant improved
value estimates explored region, together accurate action models
explored region, rapidly move towards promising states frontier
explored region. states, observer explore outward thereby eventually
expand explored region encompass augmented region.
case explored region augmented region overlap,
blind region. Since observer information beyond priors blind region,
observer reduced random exploration. non-imitation context, states
explored blind. However, imitation context, blind area reduced
effective size augmented area. Hence, implicit imitation effectively shrinks size
search space problem even overlap explored
augmented spaces.
challenging case implicit imitation transfer occurs region augmented mentor observations fails connect observer explored region
regions significant reward values. case, augmented region initially
provide guidance. observer independently located rewarding states,
augmented regions used highlight shortcuts. shortcuts represent improvements agents policy. domains feasible solution easy find,
optimal solutions difficult, implicit imitation used convert feasible solution
increasingly optimal solution.
6.1.2 Cross regional textures
seen distinctive regions used provide certain level insight
imitation perform various domains. analyze imitation performance
terms properties cut across state space. analysis model information
impacts imitation performance, saw regions connected accurate action models
allowed observer use mentor observations learn promising direction
exploration. see, then, set mentor observations useful
concentrated connected region less useful dispersed state
space unconnected components. fortunate completely observable environments
observations mentors tend capture continuous trajectories, thereby providing
continuous regions augmented states. partially observable environments, occlusion
noise could lessen value mentor observations absence model predict
mentors state.
effects heterogeneity, whether due differences action capabilities
mentor observer due differences environment two agents,
understood terms connectivity action models. Value propagate along
chains action models hit state mentor observer different
action capabilities. state, may possible achieve mentors value
therefore, value propagation blocked. Again, sequential decision making aspect
614

fiImplicit Imitation

reinforcement learning leads conclusion many scattered differences
mentor observer create discontinuity throughout problem space, whereas
contiguous region differences mentor observer cause discontinuity
region, leave large regions fully connected. Hence, distribution pattern
differences mentor observer capabilities important prevalence
difference. explore pattern next section.
6.2 Fracture Metric
try characterize connectivity form metric. Since differences reward structure, environment dynamics action models affect connectivity would
manifest differences policies mentor observer, designed
metric based differences agents optimal policies. call metric fracture.
Essentially, computes average minimum distance state mentor
observer disagree policy state mentor observer agree policy. measure roughly captures difficulty observer faces profitably exploiting
mentor observations reduce exploration demands.
formally, let mentors optimal policy observers. Let
state space Sm 6=o set disputed states mentor observer
different optimal actions. set neighboring disputed states constitutes disputed
region. set Sm 6=o called undisputed states. Let distance
metric space S. metric corresponds number transitions along
minimal length path states (i.e., shortest path using nonzero probability
observer transitions).20 standard grid world, correspond Manhattan
distance. define fracture (S) state space average minimal distance
disputed state closest undisputed state:
(S) =

1

X

|Sm 6=o | sS

6=o

min

tSSm 6=o

(s, t).

(13)

things equal, lower fracture value tend increase propagation
value information across state space, potentially resulting less exploration
required. test metric, applied number scenarios varying fracture
coefficients. difficult construct scenarios vary fracture coefficient yet
expected value. scenarios Figure 22 constructed
length possible paths start state goal state x
scenario. scenario, however, upper path lower path. mentor
trained scenario penalizes lower path mentor learns take
upper path. imitator trained scenario upper path penalized
therefore take lower path. equalized difficulty problems
follows: using generic -greedy learning agent fixed exploration schedule (i.e.,
fixed initial rate decay) one scenario, tuned magnitude penalties
exact placement along loops scenarios learner using
exploration policy would converge optimal policy roughly number
steps each.
20. expected distance would give accurate estimate fracture, difficult calculate.

615

fiPrice & Boutilier

X

X





(a) = 0.5

X



(b) = 1.7



(c) = 3.5

X

(d) = 6.0

Figure 22: Fracture metric scenarios


0.5
1.7
3.5
6.0

5 102
60%

1 102
70%

Observer Initial Exploration Rate
5 103 1 103 5 104 1 104
90%
0%
80%
90%
90 %
30%
100 %
30 %
70 %

5 105

1 105

100 %

100 %

Figure 23: Percentage runs (of ten) converging optimal policy given fracture
initial exploration rate

Figure 22(a), mentor takes top loop optimal run, imitator
would take bottom loop. Since loops short length common
path long, average fracture low. compare Figure 22(d), see
loops longthe majority states scenario loops.
states loop distance nearest state observer mentor
policies agree, namely, state loop. scenario therefore high average
fracture coefficient.
Since loops various scenarios differ length, penalties inserted loops
vary respect distance goal state therefore affect total discounted expected reward different ways. penalties may cause agent
become stuck local minimum order avoid penalties exploration rate
low. set experiments, therefore compare observer agents basis
likely converge optimal solution given mentor example.
Figure 23 presents percentage runs (out ten) imitator converged
optimal solution (i.e., taking lower loops) function exploration rate
scenario fracture.21 see distinct diagonal trend table illustrating
increasing fracture requires imitator increase levels exploration order find
21. reasons computational expediency, entries near diagonal computed. Sampling entries confirms trend.

616

fiImplicit Imitation

optimal policy. suggests fracture reflects feature RL domains may
important predicting efficacy implicit imitation.
6.3 Suboptimality Bias
Implicit imitation fundamentally biasing exploration observer. such,
worthwhile ask positive effect observer performance. short
answer mentor following optimal policy observer cause observer
explore neighborhood optimal policy generally bias observer
towards finding optimal policy.
detailed answer requires looking explicitly exploration reinforcement learning. theory, -greedy exploration policy suitable rate decay cause
implicit imitators eventually converge optimal solution unassisted
counterparts. However, practice, exploration rate typically decayed quickly
order improve early exploitation mentor input. Given practical, theoretically
unsound exploration rates, observer may settle mentor strategy feasible,
non-optimal. easily imagine examples: consider situation agent
observing mentor following policy. Early learning process, value
policy followed mentor may look better estimated value alternative
policies available observer. could case mentors policy actually
optimal policy. hand, may case one alternative
policies, observer neither personal experience, observations
mentor, actually superior. Given lack information, aggressive exploitation policy might lead observer falsely conclude mentors policy optimal.
implicit imitation bias agent suboptimal policy, reason expect
agent learning domain sufficiently challenging warrant use imitation
would discovered better alternative. emphasize even mentors policy
suboptimal, still provides feasible solution preferable solution
many practical problems.
regard, see classic exploration/exploitation tradeoff additional
interpretation implicit imitation setting. component exploration rate
correspond observers belief sufficiency mentors policy.
paradigm, then, seems somewhat misleading think terms decision whether
follow specific mentor not. question much exploration
perform addition required reconstruct mentors policy.
6.4 Specific Applications
see applications implicit imitation variety contexts. emerging electronic
commerce information infrastructure driving development vast networks
multi-agent systems. networks used competitive purposes trade, implicit
imitation used RL agent learn buying strategies information
filtering policies agents order improve behavior.
control, implicit imitation could used transfer knowledge existing
learned controller already adapted clients new learning controller
completely different architecture. Many modern products elevator controllers
617

fiPrice & Boutilier

(Crites & Barto, 1998), cell traffic routers (Singh & Bertsekas, 1997) automotive fuel
injection systems use adaptive controllers optimize performance system
specific user profiles. upgrading technology underlying system, quite
possible sensors, actuators internal representation new system
incompatible old system. Implicit imitation provides method transferring
valuable user information systems without explicit communication.
traditional application imitation-like technologies lies area bootstrapping
intelligent artifacts using traces human behavior. Research within behavioral cloning
paradigm investigated transfer applications piloting aircraft (Sammut et al.,
1992) controlling loading cranes (Suc & Bratko, 1997). researchers investigated use imitation simplify programming robots (Kuniyoshi, Inaba, &
Inoue, 1994). ability imitation transfer complex, nonlinear dynamic behaviors
existing human agents makes particularly attractive control problems.

7. Extensions
model implicit imitation presented makes certain restrictive assumptions regarding structure decision problem solved (e.g., full observability, knowledge
reward function, discrete state action space). simplifying assumptions
aided detailed development model, believe basic intuitions much
technical development extended richer problem classes. suggest several
possible extensions section, provides interesting avenue
future research.
7.1 Unknown Reward Functions
current paradigm assumes observer knows reward function.
assumption consistent view RL form automatic programming.
can, however, relax constraint assuming ability generalize observed rewards.
Suppose expected reward expressed terms probability distribution
features observers state, Pr(r|f (so )). model-based RL, distribution
learned agent experience. features
applied mentors state sm , observer use learned
reward distribution estimate expected reward mentor states well. extends
paradigm domains rewards unknown, preserves ability
observer evaluate mentor experiences terms.
Imitation techniques designed around assumption observer mentor
share identical rewards, Utgoffs (1991), would course work absence
reward function. notion inverse reinforcement learning (Ng & Russell, 2000) could
adapted case well. challenge future research would explore synthesis
implicit imitation reward inversion approaches handle observers prior
beliefs intermediate level correlation reward function observer
mentor.
618

fiImplicit Imitation

7.2 Interaction agents
cast general imitation model framework stochastic games, restriction model presented thus far noninteracting games essentially means
standard issues associated multiagent interaction arise. are, course,
many tasks require interactions agents; cases, implicit imitation offers
potential accelerate learning. general solution requires integration imitation
general models multiagent RL based stochastic Markov games (Littman,
1994; Hu & Wellman, 1998; Bowling & Veloso, 2001). would doubt rather
challenging, yet rewarding endeavor.
take simple example, simple coordination problems (e.g., two mobile agents
trying avoid carrying related tasks) might imagine imitator
learning mentor reversing roles roles considering
observed state transition influenced joint action. general
settings, learning typically requires great care, since agents learning nonstationary
environment may converge (say, equilibrium). Again, imitation techniques offer
certain advantages: instance, mentor expertise suggest means coordinating
agents (e.g., providing focal point equilibrium selection, making clear
specific convention always passing right avoid collision).
challenges opportunities present imitation used multiagent settings. example, competitive educational domains, agents
choose actions maximize information exploration returns exploitation;
must reason actions communicate information agents.
competitive setting, one agent may wish disguise intentions, context
teaching, mentor may wish choose actions whose purpose abundantly clear.
considerations must become part action selection process.
7.3 Partially Observable Domains
extension model partially observable domains critical, since unrealistic
many settings suppose learner constantly monitor activities mentor.
central idea implicit imitation extract model information observations
mentor, rather duplicating mentor behavior. means mentors internal
belief state policy (directly) relevant learner. take somewhat
behaviorist stance concern mentors observed behaviors
tell us possibilities inherent environment. observer keep
belief state mentors current state, done using estimated
world model observer uses update belief state.
Preliminary investigation model suggests dealing partial observability
viable. derived update rules augmented partially observable updates.
updates based Bayesian formulation implicit imitation is, turn, based
Bayesian RL (Dearden et al., 1999). fully observable contexts, seen
effective exploration using mentor observations possible fully observable domains
Bayesian model imitation used (Price & Boutilier, 2003). extension
model cases mentors state partially observable reasonably straightforward.
anticipate updates performed using belief state mentors state
619

fiPrice & Boutilier

action help alleviate fracture could caused incomplete observation
behavior.
interesting dealing additional factor usual exploration-exploitation
tradeoff: determining whether worthwhile take actions render mentor
visible (e.g., ensuring mentor remains view source information remains
available learning).
7.4 Continuous Model-Free Learning
many realistic domains, continuous attributes large state action spaces prohibit
use explicit table-based representations. Reinforcement learning domains
typically modified make use function approximators estimate Q-function
points direct evidence received. Two important approaches
parameter-based models (e.g., neural networks) (Bertsekas & Tsitsiklis, 1996)
memory-based approaches (Atkeson, Moore, & Schaal, 1997). approaches,
model-free learning generally employed. is, agent keeps value function uses
environment implicit model perform backups using sampling distribution
provided environment observations.
One straightforward approach casting implicit imitation continuous setting would
employ model-free learning paradigm (Watkins & Dayan, 1992). First, recall augmented Bellman backup function used implicit imitation:
(

(

V (s) = Ro (s) + max max
aAo

X

)

Pro (s, a, t)V (t) ,

tS

)

X

Prm (s, t)V (t)

(14)

tS

examine augmented backup equation, see converted
model-free form much way ordinary Bellman backup. use standard
Q-function observer actions, add one additional action corresponds
action taken mentor.22 imagine observer state ,
took action ao ended state s0o . time, mentor made transition
state sm s0m . write:


Q(so , ao ) = (1 )Q(so , ao ) + (Ro (so , ao ) + max

max

a0 Ao






Q(s0o , a0 ) ,

Q(sm , ) = (1 )Q(sm , ) + (Ro (sm , ) + max max
0

Ao





Q(s0o , )


Q(s0m , a0 ) ,

(15)


Q(s0m , )

discussed earlier, relative quality mentor observer estimates Qfunction specific states may vary. Again, order avoid inaccurate prior beliefs
mentors action models bias exploration, need employ confidence measure
decide apply augmented equations. feel natural setting
kind tests memory-based approaches function approximation. Memorybased approaches, locally-weighted regression (Atkeson et al., 1997), provide estimates functions points previously unvisited, maintain evidence
22. doesnt imply observer knows actions corresponds .

620

fiImplicit Imitation

set used generate estimates. note implicit bias memory-based approaches assumes smoothness points unless additional data proves otherwise.
basis bias, propose compare average squared distance query
exemplars used estimate mentors Q-value average squared distance
query exemplars used observer-based estimate heuristically decide
agent reliable Q-value.
approach suggested benefit prioritized sweeping. Prioritizedsweeping, however, adapted continuous settings (Forbes & Andre, 2000).
feel reasonably efficient technique could made work.

8. Related Work
Research imitation spans broad range dimensions, ethological studies,
abstract algebraic formulations, industrial control algorithms. fields crossfertilized informed other, come stronger conceptual definitions
better understanding limits capabilities imitation. Many computational
models proposed exploit specialized niches variety control paradigms,
imitation techniques applied variety real-world control problems.
conceptual foundations imitation clarified work natural imitation. work apes (Russon & Galdikas, 1993), octopi (Fiorito & Scotto, 1992),
animals, know socially facilitated learning widespread throughout animal kingdom. number researchers pointed out, however, social facilitation
take many forms (Conte, 2000; Noble & Todd, 1999). instance, mentors attention
object draw observers attention thereby lead observer manipulate object independently model provided mentor. True imitation
therefore typically defined restrictive fashion. Visalberghi Fragazy (1990)
cite Mitchells definition:
1. something C (the copy behavior) produced organism
2. C similar something else (the Model behavior)
3. observation necessary production C (above baseline levels C
occurring spontaneously)
4. C designed similar
5. behavior C must novel behavior already organized precise way
organisms repertoire.
definition perhaps presupposes cognitive stance towards imitation
agent explicitly reasons behaviors agents behaviors relate
action capabilities goals.
Imitation analyzed terms type correspondence demonstrated
mentors behavior observers acquired behavior (Nehaniv & Dautenhahn,
1998; Byrne & Russon, 1998). Correspondence types distinguished level.
action level, correspondence actions. program level, actions
621

fiPrice & Boutilier

may completely different correspondence may found subgoals.
effect level, agent plans set actions achieve effect demonstrated
behavior direct correspondence subcomponents observers
actions mentors actions. term abstract imitation proposed
case agents imitate behaviors come imitating mental state
agents (Demiris & Hayes, 1997).
study specific computational models imitation yielded insights
nature observer-mentor relationship affects acquisition behaviors
observers. instance, related field behavioral cloning, observed
mentors implement conservative policies generally yield reliable clones (Urbancic
& Bratko, 1994). Highly-trained mentors following optimal policy small coverage
state space yield less reliable clones make mistakes (Sammut et al.,
1992). partially observable problems, learning perfect oracles disastrous,
may choose policies based perceptions available observer. observer
therefore incorrectly biased away less risky policies require additional
perceptual capabilities (Scheffer, Greiner, & Darken, 1997). Finally, observed
successful clones would often outperform original mentor due cleanup effect
(Sammut et al., 1992).
One original goals behavioral cloning (Michie, 1993) extract knowledge
humans speed design controllers. extracted knowledge
useful, argued rule-based systems offer best chance intelligibility
(van Lent & Laird, 1999). become clear, however, symbolic representations
complete answer. Representational capacity issue. Humans often organize
control tasks time, typically lacking state perception-based approaches
control. Humans naturally break tasks independent components
subgoals (Urbancic & Bratko, 1994). Studies demonstrated humans
give verbal descriptions control policies match actual actions
(Urbancic & Bratko, 1994). potential saving time acquisition borne
one study explicitly compared time extract rules time required
program controller (van Lent & Laird, 1999).
addition traditionally considered imitation, agent may face
problem learning imitate finding correspondence actions
states observer mentor (Nehaniv & Dautenhahn, 1998). fully credible approach
learning observation absence communication protocols deal
issue.
theoretical developments imitation research accompanied number
practical implementations. implementations take advantage properties different control paradigms demonstrate various aspects imitation. Early behavioral cloning
research took advantage supervised learning techniques decision trees (Sammut
et al., 1992). decision tree used learn human operator mapped perceptions actions. Perceptions encoded discrete values. time delay inserted
order synchronize perceptions actions trigger. Learning apprentice systems
(Mitchell et al., 1985) attempted extract useful knowledge watching users,
goal apprentices independently solve problems. Learning apprentices closely
related programming demonstration systems (Lieberman, 1993). Later efforts used
622

fiImplicit Imitation

sophisticated techniques extract actions visual perceptions abstract
actions future use (Kuniyoshi et al., 1994). Work associative recurrent learning models allowed work area extended learning temporal sequences
(Billard & Hayes, 1999). Associative learning used together innate following
behaviors acquire navigation expertise agents (Billard & Hayes, 1997).
related slightly different form imitation studied multi-agent
reinforcement learning community. early precursor imitation found work
sharing perceptions agents (Tan, 1993). Closer imitation idea
replaying perceptions actions one agent second agent (Lin, 1991; Whitehead,
1991a). Here, transfer one agent another, contrast behavioral clonings
transfer human agent. representation different. Reinforcement learning
provides agents ability reason effects current actions expected
future utility agents integrate knowledge knowledge extracted
agents comparing relative utility actions suggested knowledge
source. seeding approaches closely related. Trajectories recorded human
subjects used initialize planner subsequently optimizes plan order
account differences human effector robotic effector (Atkeson &
Schaal, 1997). technique extended handle notion subgoals within
task (Atkeson & Schaal, 1997). Subgoals addressed others (Suc & Bratko,
1997). work based idea agent extracting model mentor
using model information place bounds value actions using
reward function. Agents therefore learn mentors reward functions different
own.
Another approach family based assumption mentor rational
(i.e., follows optimal policy), reward function observer chooses
set actions. Given assumptions, conclude action
chosen mentor particular state must higher value mentor
alternatives open mentor (Utgoff & Clouse, 1991) therefore higher value
observer alternative. system Utgoff Clouse therefore iteratively adjusts
values actions constraint satisfied model. related approach
uses methodology linear-quadratic control ( Suc & Bratko, 1997). First model
system constructed. inverse control problem solved find cost matrix
would result observed controller behavior given environment model. Recent
work inverse reinforcement learning takes related approach reconstructing reward
functions observed behavior (Ng & Russell, 2000). similar inversion
quadratic control approach, formulated discrete domains.
Several researchers picked idea common representations perceptual functions action planning. One approach using representation
perception control based PID controller model. PID controller represents
behavior. output compared observed behaviors order select action
closest observed behavior (Demiris & Hayes, 1999). Explicit motor action
schema investigated dual role perceptual motor representations
(Mataric, Williamson, Demiris, & Mohan, 1998).
Imitation techniques applied diverse collection applications. Classical control applications include control systems robot arms (Kuniyoshi et al., 1994;
623

fiPrice & Boutilier

Friedrich, Munch, Dillmann, Bocionek, & Sassin, 1996), aeration plants (Scheffer et al.,
1997), container loading cranes (Suc & Bratko, 1997; Urbancic & Bratko, 1994). Imitation learning applied acceleration generic reinforcement learning (Lin,
1991; Whitehead, 1991a). Less traditional applications include transfer musical style
(Canamero, Arcos, & de Mantaras, 1999) support social atmosphere (Billard, Hayes, & Dautenhahn, 1999; Breazeal, 1999; Scassellati, 1999). Imitation
investigated route language acquisition transmission (Billard et al., 1999;
Oliphant, 1999).

9. Concluding Remarks
described formal principled approach imitation called implicit imitation.
stochastic problems explicit forms communication possible,
underlying model-based framework combined model extraction provides alternative
imitation learning-by-observation systems. new approach makes use
model compute actions imitator take without requiring observer
duplicate mentors actions exactly. shown implicit imitation offer significant
transfer capability several test problems, proves robust face
noise, capable integrating subskills multiple mentors, able provide benefits
increase difficulty problem.
seen feasibility testing extends implicit imitation principled manner
deal situations homogeneous action assumption invalid. Adding
bridging capabilities preserves extends mentors guidance presence infeasible actions, whether due differences action capabilities local differences state
spaces. approach relates idea following sense imitator
uses local search model repair discontinuities augmented value function acting world. process applying imitation various domains,
learned properties. particular developed fracture metric
characterize effectiveness mentor given observer specific domain.
made considerable progress extending imitation new problem classes. model
developed rather flexible extended several ways: example,
Bayesian approach imitation building work shows great potential (2003);
initial formulations promising approaches extending implicit imitation multiagent problems, partially observable domains domains reward function
specified priori.
number challenges remain field imitation. Bakker Kuniyoshi (1996)
describe number these. Among intriguing problems unique imitation are:
evaluation expected payoff observing mentor; inferring useful state
reward mappings domains mentors observers; repairing
locally searching order fit observed behaviors observers capabilities
goals. raised possibility agents attempting reason
information revealed actions addition whatever concrete value actions
agent.
Model-based reinforcement applied numerous problems. Since implicit imitation added model-based reinforcement learning relatively little effort,
624

fiImplicit Imitation

expect applied many problems. basis simple
elegant theory Markov decision processes makes easy describe analyze. Though
focused simple examples designed illustrate different mechanisms
required implicit imitation, expect variations approach provide interesting directions future research.

Acknowledgments
Thanks anonymous referees suggestions comments earlier versions
work Michael Littman editorial suggestions. Price supported NCE IRISIII Project BAC. Boutilier supported NSERC Research Grant OGP0121843,
NCE IRIS-III Project BAC. parts paper presented Implicit Imitation Reinforcement Learning, Proceedings Sixteenth International Conference
Machine Learning (ICML-99), Bled, Slovenia, pp.325334 (1999) Imitation
Reinforcement Learning Agents Heterogeneous Actions, Proceedings Fourteenth
Biennial Conference Canadian Society Computational Studies Intelligence (AI
2001), Ottawa, pp.111120 (2001).

References
Alissandrakis, A., Nehaniv, C. L., & Dautenhahn, K. (2000). Learning things
imitation. Bauer, M., & Rich, C. (Eds.), AAAI Fall Symposium Learning
Things, pp. 16 Cape Cod, MA.
Atkeson, C. G., & Schaal, S. (1997). Robot learning demonstration. Proceedings
Fourteenth International Conference Machine Learning, pp. 1220 Nashville, TN.
Atkeson, C. G., Moore, A. W., & Schaal, S. (1997). Locally weighted learning control. Artificial
Intelligence Review, 11 (1-5), 75113.
Bakker, P., & Kuniyoshi, Y. (1996). Robot see, robot do: overview robot imitation. AISB96
Workshop Learning Robots Animals, pp. 311 Brighton,UK.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton.
Bertsekas, D. P. (1987). Dynamic Programming: Deterministic Stochastic Models. Prentice-Hall,
Englewood Cliffs.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena, Belmont, MA.
Billard, A., & Hayes, G. (1997). Learning communicate imitation autonomous robots.
Proceedings Seventh International Conference Artificial Neural Networks, pp.
76368 Lausanne, Switzerland.
Billard, A., & Hayes, G. (1999). Drama, connectionist architecturefor control learning
autonomous robots. Adaptive Behavior Journal, 7, 3564.
Billard, A., Hayes, G., & Dautenhahn, K. (1999). Imitation skills means enhance learning
synthetic proto-language autonomous robot. Proceedings AISB99 Symposium
Imitation Animals Artifacts, pp. 8895 Edinburgh.
Boutilier, C. (1999). Sequential optimality coordination multiagent systems. Proceedings
Sixteenth International Joint Conference Artificial Intelligence, pp. 478485 Stockholm.
625

fiPrice & Boutilier

Boutilier, C., Dean, T., & Hanks, S. (1999). Decision theoretic planning: Structural assumptions
computational leverage. Journal Artificial Intelligence Research, 11, 194.
Bowling, M., & Veloso, M. (2001). Rational convergent learning stochastic games. Proceedings Seventeenth International Joint Conference Artificial Intelligence, pp. 10211026
Seattle.
Breazeal, C. (1999). Imitation social exchange humans robot. Proceedings
AISB99 Symposium Imitation Animals Artifacts, pp. 96104 Edinburgh.
Byrne, R. W., & Russon, A. E. (1998). Learning imitation: hierarchical approach. Behavioral
Brain Sciences, 21, 667721.
Canamero, D., Arcos, J. L., & de Mantaras, R. L. (1999). Imitating human performances automatically generate expressive jazz ballads. Proceedings AISB99 Symposium
Imitation Animals Artifacts, pp. 11520 Edinburgh.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally partially observable stochastic domains. Proceedings Twelfth National Conference Artificial
Intelligence, pp. 10231028 Seattle.
Conte, R. (2000). Intelligent social learning. Proceedings AISB00 Symposium Starting
Society: Applications Social Analogies Computational Systems Birmingham.
Crites, R., & Barto, A. G. (1998). Elevator group control using multiple reinforcement learning
agents. Machine-Learning, 33 (23), 23562.
Dean, T., & Givan, R. (1997). Model minimization Markov decision processes. Proceedings
Fourteenth National Conference Artificial Intelligence, pp. 106111 Providence.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning.
Artificial Intelligence, 89, 219283.
Dearden, R., Friedman, N., & Andre, D. (1999). Model-based bayesian exploration. Proceedings
Fifteenth Conference Uncertainty Artificial Intelligence, pp. 150159 Stockholm.
DeGroot, M. H. (1975). Probability statistics. Addison-Wesley, Reading, MA.
Demiris, J., & Hayes, G. (1997). robots ape?. Proceedings AAAI Fall Symposium
Socially Intelligent Agents, pp. 2831 Cambridge, MA.
Demiris, J., & Hayes, G. (1999). Active passive routes imitation. Proceedings
AISB99 Symposium Imitation Animals Artifacts, pp. 8187 Edinburgh.
Fiorito, G., & Scotto, P. (1992). Observational learning octopus vulgaris. Science, 256, 54547.
Forbes, J., & Andre, D. (2000). Practical reinforcement learning continuous domains. Tech. rep.
UCB/CSD-00-1109, Computer Science Division, University California, Berkeley.
Friedrich, H., Munch, S., Dillmann, R., Bocionek, S., & Sassin, M. (1996). Robot programming
demonstration (RPD): Support induction human interaction. Machine Learning, 23,
163189.
Hartmanis, J., & Stearns, R. E. (1966). Algebraic Structure Theory Sequential Machines. PrenticeHall, Englewood Cliffs.
Hu, J., & Wellman, M. P. (1998). Multiagent reinforcement learning: Theoretical framework
algorithm. Proceedings Fifthteenth International Conference Machine Learning,
pp. 242250 Madison, WI.
Kaelbling, L. P. (1993). Learning Embedded Systems. MIT Press, Cambridge,MA.
626

fiImplicit Imitation

Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research, 4, 237285.
Kearns, M., & Singh, S. (1998). Near-optimal reinforcement learning polynomial time. Proceedings Fifthteenth International Conference Machine Learning, pp. 260268 Madison,
WI.
Kuniyoshi, Y., Inaba, M., & Inoue, H. (1994). Learning watching: Extracting reusable task
knowledge visual observation human performance. IEEE Transactions Robotics
Automation, 10 (6), 799822.
Lee, D., & Yannakakis, M. (1992). Online miminization transition systems. Proceedings
24th Annual ACM Symposium Theory Computing (STOC-92), pp. 264274 Victoria,
BC.
Lieberman, H. (1993). Mondrian: teachable graphical editor. Cypher, A. (Ed.), Watch
Do: Programming Demonstration, pp. 340358. MIT Press, Cambridge, MA.
Lin, L.-J. (1991). Self-improvement based reinforcement learning, planning teaching. Machine
Learning: Proceedings Eighth International Workshop (ML91), 8, 32327.
Lin, L.-J. (1992). Self-improving reactive agents based reinforcement learning, planning
teaching. Machine Learning, 8, 293321.
Littman, M. L. (1994). Markov games framework multi-agent reinforcement learning.
Proceedings Eleventh International Conference Machine Learning, pp. 157163 New
Brunswick, NJ.
Lovejoy, W. S. (1991). survey algorithmic methods partially observed Markov decision
processes. Annals Operations Research, 28, 4766.
Mataric, M. J. (1998). Using communication reduce locality distributed multi-agent learning.
Journal Experimental Theoretical Artificial Intelligence, 10 (3), 357369.
Mataric, M. J., Williamson, M., Demiris, J., & Mohan, A. (1998). Behaviour-based primitives
articulated control. R. Pfiefer, B. Blumberg, J.-A. M. . S. W. W. (Ed.), Fifth International
conference simulation adaptive behavior SAB98, pp. 165170 Zurich. MIT Press.
Meuleau, N., & Bourgine, P. (1999). Exploration multi-state environments: Local mesures
back-propagation uncertainty. Machine Learning, 32 (2), 117154.
Mi, J., & Sampson, A. R. (1993). comparison Bonferroni Scheffe bounds. Journal
Statistical Planning Inference, 36, 101105.
Michie, D. (1993). Knowledge, learning machine intelligence. Sterling, L. (Ed.), Intelligent
Systems. Plenum Press, New York.
Mitchell, T. M., Mahadevan, S., & Steinberg, L. (1985). LEAP: learning apprentice VLSI
design. Proceedings Ninth International Joint Conference Artificial Intelligence,
pp. 573580 Los Altos, California. Morgan Kaufmann Publishers, Inc.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning less
data less real time. Machine Learning, 13 (1), 10330.
Myerson, R. B. (1991). Game Theory: Analysis Conflict. Harvard University Press, Cambridge.
Nehaniv, C., & Dautenhahn, K. (1998). Mapping dissimilar bodies: Affordances
algebraic foundations imitation. Proceedings Seventh European Workshop
Learning Robots, pp. 6472 Edinburgh.
627

fiPrice & Boutilier

Ng, A. Y., & Russell, S. (2000). Algorithms inverse reinforcement learning. Proceedings
Seventeenth International Conference Machine Learning, pp. 663670 Stanford.
Noble, J., & Todd, P. M. (1999). really imitation? review simple mechanisms social
information gathering. Proceedings AISB99 Symposium Imitation Animals
Artifacts, pp. 6573 Edinburgh.
Oliphant, M. (1999). Cultural transmission communications systems: Comparing observational
reinforcement learning models. Proceedings AISB99 Symposium Imitation
Animals Artifacts, pp. 4754 Edinburgh.
Price, B., & Boutilier, C. (2003). Bayesian approach imitation reinforcement learning. Proceedings Eighteenth International Joint Conference Artificial Intelligence Acapulco.
appear.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
John Wiley Sons, Inc., New York.
Russon, A., & Galdikas, B. (1993). Imitation free-ranging rehabilitant orangutans (pongopygmaeus). Journal Comparative Psychology, 107 (2), 147161.
Sammut, C., Hurst, S., Kedzier, D., & Michie, D. (1992). Learning fly. Proceedings
Ninth International Conference Machine Learning, pp. 385393 Aberdeen, UK.
Scassellati, B. (1999). Knowing imitate knowing succeed. Proceedings
AISB99 Symposium Imitation Animals Artifacts, pp. 105113 Edinburgh.
Scheffer, T., Greiner, R., & Darken, C. (1997). experimentation better perfect
guidance. Proceedings Fourteenth International Conference Machine Learning,
pp. 331339 Nashville.
Seber, G. A. F. (1984). Multivariate Observations. Wiley, New York.
Shapley, L. S. (1953). Stochastic games. Proceedings National Academy Sciences, 39,
327332.
Singh, S. P., & Bertsekas, D. (1997). Reinforcement learning dynamic channel allocation
cellular telephone systems. Advances Neural information processing systems, pp. 974
980 Cambridge, MA. MIT Press.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable Markov
processes finite horizon. Operations Research, 21, 10711088.
Suc, D., & Bratko, I. (1997). Skill reconstruction induction LQ controllers subgoals.
Proceedings Fifteenth International Joint Conference Artificial Intelligence, pp.
914919 Nagoya.
Sutton, R. S. (1988). Learning predict method temporal differences. Machine Learning,
3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. ICML93, pp. 33037.
Urbancic, T., & Bratko, I. (1994). Reconstruction human skill machine learning. Eleventh
European Conference Artificial Intelligence, pp. 498502 Amsterdam.
628

fiImplicit Imitation

Utgoff, P. E., & Clouse, J. A. (1991). Two kinds training information evaluation function
learning. Proceedings Ninth National Conference Artificial Intelligence, pp. 596
600 Anaheim, CA.
van Lent, M., & Laird, J. (1999). Learning hierarchical performance knowledge observation.
Proceedings Sixteenth International Conference Machine Learning, pp. 229238
Bled, Slovenia.
Visalberghi, E., & Fragazy, D. (1990). monkeys ape?. Parker, S., & Gibson, K. (Eds.),
Language Intelligence Monkeys Apes, pp. 247273. Cambridge University Press,
Cambridge.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Whitehead, S. D. (1991a). Complexity analysis cooperative mechanisms reinforcement learning. Proceedings Ninth National Conference Artificial Intelligence, pp. 607613
Anaheim.
Whitehead, S. D. (1991b). Complexity cooperation q-learning. Machine Learning. Proceedings Eighth International Workshop (ML91), pp. 363367.

629


