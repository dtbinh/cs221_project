journal artificial intelligence

submitted published

accelerating reinforcement learning
implicit imitation
bob price

price cs ubc ca

department computer science
university british columbia
vancouver b c canada v z

craig boutilier

cebly cs toronto edu

department computer science
university toronto
toronto canada h

abstract
imitation viewed means enhancing learning multiagent environments
augments agents ability learn useful behaviors making intelligent use
knowledge implicit behaviors demonstrated cooperative teachers experienced agents propose study formal model implicit imitation
accelerate reinforcement learning dramatically certain cases roughly observing
mentor reinforcement learning agent extract information capabilities
relative value unvisited parts state space study two specific
instantiations model one learning agent mentor identical
abilities one designed deal agents mentors different action sets
illustrate benefits implicit imitation integrating prioritized sweeping
demonstrating improved performance convergence observation single
multiple mentors though make stringent assumptions regarding observability
possible interactions briefly comment extensions model relax
restricitions

introduction
application reinforcement learning multiagent systems offers unique opportunities
challenges agents viewed independently trying achieve ends
interesting issues interaction agent policies littman must resolved e g
appeal equilibrium concepts however fact agents may share information
mutual gain tan distribute search optimal policies communicate reinforcement signals one another mataric offers intriguing possibilities
accelerating reinforcement learning enhancing agent performance
another way individual agent performance improved
novice agent learn reasonable behavior expert mentor type learning
brought explicit teaching demonstration atkeson schaal
lin whitehead sharing privileged information mataric
explicit cognitive representation imitation bakker kuniyoshi
imitation agents exploration used ground observations agents
c

ai access foundation morgan kaufmann publishers rights reserved

fiprice boutilier

behaviors capabilities resolve ambiguities observations arising
partial observability noise common thread work use mentor
guide exploration observer typically guidance achieved form
explicit communication mentor observer less direct form teaching
involves observer extracting information mentor without mentor making
explicit attempt demonstrate specific behavior interest mitchell mahadevan
steinberg
develop imitation model call implicit imitation allows
agent accelerate reinforcement learning process observation expert
mentor mentors agent observes state transitions induced mentors
actions uses information gleaned observations update estimated
value states actions distinguish two settings implicit
imitation occur homogeneous settings learning agent mentor
identical actions heterogeneous settings capabilities may differ
homogeneous setting learner use observed mentor transitions directly
update estimated model actions update value function addition
mentor provide hints observer parts state space
may worth focusing attention observers attention area might take form
additional exploration area additional computation brought bear agents
prior beliefs area heterogeneous setting similar benefits accrue
potential agent misled mentor possesses abilities different
case learner needs mechanism detect situations
make efforts temper influence observations
derive several techniques support implicit imitation largely independent specific reinforcement learning though best suited use
model methods include model extraction augmented backups feasibility
testing k step repair first describe implicit imitation homogeneous domains
describe extension heterogeneous settings illustrate effectiveness
empirically incorporating moore atkesons prioritized sweeping
implicit imitation model several advantages direct forms imitation
teaching require agent explicitly play role mentor teacher
observers learn simply watching behavior agents observed mentor
shares certain subtasks observer observed behavior incorporated indirectly observer improve estimate value function important
many situations observer learn mentor
unwilling unable alter behavior accommodate observer even communicate
information example common communication protocols may unavailable
agents designed different developers e g internet agents agents may
competitive situation disincentive share information skills
may simply incentive one agent provide information another
another key advantage approachwhich arises formalizing imitation
reinforcement learning contextis fact observer constrained directly
reasons consistency use term mentor describe agent observer
learn even mentor unwilling unwitting participant



fiimplicit imitation

imitate e duplicate actions mentor learner decide whether
explicit imitation worthwhile implicit imitation thus seen blending
advantages explicit teaching explicit knowledge transfer independent
learning addition agent learns observation exploit existence
multiple mentors essentially distributing search finally assume
observer knows actual actions taken mentor mentor shares
reward function goals mentor stands sharp contrast many
existing teaching imitation behavior learning observation make
strict assumptions respect observability complete knowledge
reward functions existence mappings agent state spaces model
generalized interesting ways elaborate generalizations near
end
remainder structured follows provide necessary background markov decision processes reinforcement learning development
implicit imitation model section section describe general formal framework
study implicit imitation reinforcement learning two specific instantiations
framework developed section model homogeneous agents
developed model extraction technique explained augmented bellman backup
proposed mechanism incorporating observations model reinforcement
learning model confidence testing introduced ensure misleading
information undue influence learners exploration policy use
mentor observations focus attention interesting parts state space
introduced section develops model heterogeneous agents model extends
homogeneous model feasibility testing device learner detect
whether mentors abilities similar k step repair whereby learner
attempt mimic trajectory mentor cannot duplicated exactly
techniques prove crucial heterogeneous settings effectiveness
demonstrated number carefully chosen navigation section examines
conditions implicit imitation work well section describes
several promising extensions model section examines implicit imitation model
context related work section considers future work drawing
general conclusions implicit imitation field computational imitation
broadly

reinforcement learning
aim provide formal model implicit imitation whereby agent learn
act optimally combining experience observations behavior
expert mentor describe section standard model
reinforcement learning used artificial intelligence model build singleagent view learning act begin reviewing markov decision processes
provide model sequential decision making uncertainty move
describe reinforcement learning emphasis model methods


fiprice boutilier

markov decision processes
markov decision processes mdps proven useful modeling stochastic sequential decision widely used decision theoretic model
domains agents actions uncertain effects agents knowledge environment uncertain agent multiple possibly conflicting objectives
section describe basic mdp model consider one classical solution procedure
consider action costs formulation mdps though pose special
complications finally make assumption full observability partially observable
mdps pomdps cassandra kaelbling littman lovejoy smallwood
sondik much computationally demanding fully observable mdps
imitation model fully observable model though generalizations model mentioned concluding section build pomdps refer
reader bertsekas boutilier dean hanks puterman
material mdps
mdp viewed stochastic automaton actions induce transitions
states rewards obtained depending states visited agent
formally mdp defined tuple hs ri finite set states
possible worlds finite set actions state transition function r reward
function agent control state system extent performing actions
cause state transitions movement current state state
actions stochastic actual transition caused cannot generally predicted
certainty transition function describes effects
action state si probability distribution specifically si sj
probability ending state sj action performed state si
denote quantity pr si sj require pr si sj
p
si sj si sj pr si sj components determine
dynamics system controlled assumption system fully
observable means agent knows true state time stage
reached decisions solely knowledge thus uncertainty lies
prediction actions effects determining actual effect
execution
deterministic stationary markovian policy describes course action
adopted agent controlling system agent adopting policy performs
action whenever finds state policies form markovian since
action choice state depend system history stationary since
action choice depend stage decision
consider optimal stationary markovian policies exist
assume bounded real valued reward function r r instantaneous reward agent receives occupying state number optimality criteria
adopted measure value policy measuring way reward
accumulated agent traverses state space execution
work focus discounted infinite horizon current value reward received stages future discounted factor allows simpler


fiimplicit imitation

computational methods used discounted total reward finite discounting
justified e g economic grounds many situations well
value function v reflects value policy state
simply expected sum discounted future rewards obtained executing beginning
policy optimal policies v v
guaranteed optimal stationary policies exist setting puterman
optimal value state v value v optimal policy
solving mdp refer constructing optimal policy value
iteration bellman simple iterative approximation optimal policy
construction given arbitrary estimate v true value function v iteratively
improve estimate follows
v n si r si max
aa

x

pr si sj v n sj



sj

computation v n given v n known bellman backup sequence value
functions v n produced value iteration converges linearly v iteration value
iteration requires computation time number iterations polynomial

finite n actions maximize right hand side equation form
optimal policy v n approximates value termination criteria applied
example one might terminate
kv v k






kxk max x x x denotes supremum norm ensures resulting
value function v within optimal function v state induced
policy optimal e value within v puterman
concept useful later q function given arbitrary value
function v define qva si
qva si r si

x

pr si sj v sj



sj

intuitively qva denotes value performing action state acting
manner value v watkins dayan particular define qa
q function defined respect v qna q function defined respect
v n manner rewrite equation
v n max qna
aa



define ergodic mdp mdp every state reachable
state finite number steps non zero probability


fiprice boutilier

model reinforcement learning
one difficulty use mdps construction optimal policy requires
agent know exact transition probabilities pr reward model r specification decision requirements especially detailed specification
domains dynamics impose undue burden agents designer reinforcement
learning viewed solving mdp full details model particular pr r known agent instead agent learns act optimally
experience environment provide brief overview reinforcement
learning section emphasis model approaches details
please refer texts sutton barto bertsekas tsitsiklis
survey kaelbling littman moore
general model assume agent controlling mdp hs ri
initially knows state action spaces transition model
reward function r agent acts environment stage process
makes transition hs r ti takes action state receives reward r
moves state repeated experiences type determine optimal
policy one two ways model reinforcement learning experiences
used learn true nature r mdp solved standard
methods e g value iteration b model free reinforcement learning experiences
used directly update estimate optimal value function q function
probably simplest model reinforcement learning scheme certainty equivalence intuitively learning agent assumed current estimated
c
transition model tb environment consisting estimated probabilities pr

b
estimated rewards model r experience hs r ti agent updates esc obtain policy
b would optimal
timated solves estimated mdp
estimated correct acts according policy
make certainty equivalence precise specific form estimated model
update procedure must adopted common used empirical distribution observed state transitions rewards estimated model instance
action attempted c times state c occasions
c
state reached estimate pr
c c c
prior estimate used e g one might assume state transitions equiprobable
bayesian dearden friedman andre uses explicit prior distribution
parameters transition distribution pr updates
experienced transition instance might assume dirichlet generalized beta
distribution degroot parameters n associated possible successor state dirichlet parameters equal experience counts c
plus prior count p representing agents prior beliefs distribution
e n c p expected transition probability pr
p
c solved
n n assuming parameter independence mdp
expected values furthermore model updated ease simply
increasing n one observation hs r ti model advantage
counter allowing flexible prior model generally


fiimplicit imitation

assign probability zero unobserved transitions adopt bayesian perspective
imitation model
one difficulty certainty equivalence computational burden rec update tb r
b e experience one
solving mdp
could circumvent extent batching experiences updating solving
model periodically alternatively one could use computational effort judiciously
apply bellman backups states whose values q values likely change
given change model moore atkesons prioritized sweeping
c
tb updated changing pr
bellman backup
b
b suppose
applied update estimated value v well q value q
b
b
magnitude change v given v predecessor w q values
b
c
q w
hence values vb w change pr w
magnitude change
c
bounded pr w
vb predecessors w placed priority

c
queue pr w vb serving priority fixed number bellman backups
applied states order appear queue backup
change value cause predecessors inserted queue way
computational effort focused states bellman backup greatest
impact due model change furthermore backups applied subset
states generally applied fixed number times way contrast
certainty equivalence backups applied convergence thus prioritized
sweeping viewed specific form asynchronous value iteration appealing
computational properties moore atkeson
certainty equivalence agent acts current approximation model
correct even though model likely inaccurate early learning process
optimal policy inaccurate model prevents agent exploring transitions form part optimal policy true model agent fail
optimal policy reason explicit exploration policies invariably used
ensure action tried state sufficiently often acting randomly assuming ergodic mdp agent assured sampling action state infinitely
often limit unfortunately actions agent fail exploit fact
completely uninfluenced knowledge optimal policy explorationexploitation tradeoff refers tension trying actions order
environment executing actions believed optimal basis
current estimated model
common method exploration greedy method agent
chooses random action fraction time typically decayed
time increase agents exploitation knowledge boltzmann
action selected probability proportional value
prs p

eq
e


q



proportionality adjusted nonlinearly temperature parameter
probability selecting action highest value tends typically
started high actions randomly explored early stages learning
agent gains knowledge effects actions value effects


fiprice boutilier

parameter decayed agent spends time exploiting actions known
valuable less time randomly exploring actions
sophisticated methods attempt use information model confidence
value magnitudes plan utility maximizing exploration plan early approximation
scheme found interval estimation method kaelbling bayesian
methods used calculate expected value information gained
exploration meuleau bourgine dearden et al
concentrate model approaches reinforcement learning
however point model free methodsthose estimate
optimal value function q function learned directly without recourse domain
modelhave attracted much attention example td methods sutton
q learning watkins dayan proven among popular
methods reinforcement learning methods modified deal model free
approaches discuss concluding section focus called tablebased explicit representations value functions state action
spaces large table approaches become unwieldy associated
generally intractable situations approximators often used estimate
values states discuss ways techniques extended allow
function approximation concluding section

formal framework implicit imitation
model influence mentor agent decision process learning
behavior observer must extend single agent decision model mdps account
actions objectives multiple agents section introduce formal
framework studying implicit imitation begin introducing general model
stochastic games shapley myerson impose assumptions
restrictions general model allow us focus key aspects implicit
imitation note framework proposed useful study forms
knowledge transfer multiagent systems briefly point extensions
framework would permit implicit imitation forms knowledge transfer
general settings
non interacting stochastic games
stochastic games viewed multiagent extension markov decision processes
though shapleys original formulation stochastic games involved zero sum fully
competitive assumption generalizations model proposed allowing
arbitrary relationships agents utility functions myerson formally
n agent stochastic game hs ai n ri n comprises set n agents
n set states set actions ai agent state transition function
reward function ri agent unlike mdp individual agent actions
determine state transitions rather joint action taken collection agents
determines system evolves point time let
example see fully cooperative multiagent mdp model proposed boutilier



fiimplicit imitation

set joint actions si sj pr si sj denoting
probability ending state sj joint action performed state si
convenience introduce notation ai denote set joint actions
ai ai involving agents except use ai ai denote
full joint action obtained conjoining ai ai ai ai
interests individual agents may odds strategic reasoning
notions equilibrium generally involved solution stochastic games
aim study reinforcement agent might learn observing behavior
expert mentor wish restrict model way strategic interactions need
considered want focus settings actions observer
mentor interact furthermore want assume reward functions
agents conflict way requires strategic reasoning
define noninteracting stochastic games appealing notion agent projection function used extract agents local state underlying game
games agents local state determines aspects global state relevant
decision making process projection function determines global states
identical agents local perspective formally agent assume local
state space si projection function li si write
iff li li equivalence relation partitions set equivalence classes
elements within specific class e l
si need
distinguished agent purposes individual decision making say stochastic
game noninteracting exists local state space si projection function li
agent
ai ai ai ai wi si
x

pr ai ai w w l
wi

x

pr ai ai w w l
wi

ri ri
intuitively condition imposes two distinct requirements game
perspective agent first ignore existence agents provides notion
state space abstraction suitable agent specifically li clusters together states
state equivalence class identical dynamics respect
abstraction induced li type abstraction form bisimulation
type studied automaton minimization hartmanis stearns lee yannakakis
automatic abstraction methods developed mdps dearden boutilier
dean givan hard showignoring presence agentsthat
underlying system markovian respect abstraction equivalently w r
si condition met quantification ai imposes strong noninteraction
requirement namely dynamics game perspective agent
independent strategies agents condition simply requires
states within given equivalence class agent reward agent
means states within class need distinguishedeach local state viewed
atomic


fiprice boutilier

noninteracting game induces mdp mi agent mi hsi ai pri ri
pri given condition specifically si ti si
pri si ai ti

p

pr ai ai l
ti

state l
si ai element ai let sa ai
optimal policy mi extend strategy ig ai underlying
stochastic game simply applying si every state li si
following proposition shows term noninteracting indeed provides appropriate
description game
proposition let g noninteracting stochastic game mi induced mdp agent
optimal policy mi strategy ig extending g dominant
agent
thus agent solve noninteracting game abstracting away irrelevant aspects state space ignoring agent actions solving personal mdp
mi
given arbitrary stochastic game generally quite difficult discover whether
noninteracting requiring construction appropriate projection functions
follows simply assume underlying multiagent system noninteracting
game rather specifying game projection functions specify individual mdps mi noninteracting game induced set individual
mdps simply cross product individual mdps view often quite
natural consider example three robots moving two dimensional office domain able neglect possibility interactionfor example robots
occupy position suitable level granularity require
resources achieve tasksthen might specify individual mdp
robot local state might determined robots x position orientation
status tasks global state space would cross product
local spaces individual components joint action would affect
local state agent would care reward function ri local
state
note projection function li viewed equivalent observation function assume agent distinguish elements si
fact observations agents states crucial imitation rather existence
li simply means point view decision making known model
agent need worry distinctions made li assuming
computational limitations agent need solve mi may use observations
agents order improve knowledge mi dynamics
implicit imitation
despite independent nature agent subprocesses noninteracting multiagent system circumstances behavior one agent may relevant
elaborate condition computational limitations



fiimplicit imitation

another keep discussion simple assume existence expert mentor agent
implementing stationary presumably optimal policy
local mdp mm hsm prm rm assume second agent observer
local mdp mo hso ao pro ro nothing mentors behavior relevant
observer knows mdp solve without computational difficulty
situation quite different reinforcement learner without complete knowledge model mo may well observed behavior mentor provides
valuable information observer quest learn act optimally within mo
take extreme case mentors mdp identical observers mentor
expert sense acting optimally behavior mentor indicates
exactly observer even mentor acting optimally
mentor observer different reward functions mentor state transitions observed
learner provide valuable information dynamics domain
thus see one agent learning act behavior another
potentially relevant learner even underlying multiagent system noninteracting similar remarks course apply case observer knows mdp
mo computational restrictions make solving difficultobserved mentor transitions
might provide valuable information focus computational effort main
motivation underlying model implicit imitation behavior expert
mentor provide hints appropriate courses action reinforcement learning
agent
intuitively implicit imitation mechanism learning agent attempts
incorporate observed experience expert mentor agent learning process
classical forms learning imitation learner considers effects
mentors action action sequence context unlike direct imitation however
assume learner must physically attempt duplicate mentors
behavior assume mentors behavior necessarily appropriate
observer instead influence mentor agents transition model
estimate value states actions elaborate points
follows assume mentor associated mdp mm learner
observer associated mdp mo described mdps fully observable
focus reinforcement learning faced agent extension multiple
mentors straightforward discussed clarity assume one
mentor description abstract framework clear certain conditions must
met observer extract useful information mentor list number
assumptions make different points development model
observability must assume learner observe certain aspects mentors behavior work assume state mentors mdp fully
observable learner equivalently interpret full observability
underlying noninteracting game together knowledge mentors projection
instance asynchronous dynamic programming prioritized sweeping benefit
guidance indeed distinction reinforcement learning solving mdps viewed
rather blurry sutton barto bertsekas tsitsiklis focus
case unknown model e classical reinforcement learning opposed one
computational issues key



fiprice boutilier

function lm general partially observable model would require specification observation signal set z observation function sm z
sm z denotes probability observer obtains signal z
local states observer mentor sm respectively
pursue model important note assume
observer access action taken point time since actions
stochastic state even fully observable mentor invoking
specific control signal generally insufficient determine signal thus seems
much reasonable assume states transitions observable
actions gave rise
analogy observer mentor acting different local state spaces clear
observations made mentors state transitions offer useful information
observer unless relationship two state spaces
several ways relationship specified dautenhahn nehaniv
use homomorphism define relationship mentor observer
specific family trajectories see section discussion
slightly different notion might involve use analogical mapping h sm
observed state transition provides information
observer dynamics value state h certain circumstances
might require mapping h homomorphic respect pr
perhaps even respect r discuss issues detail
order simplify model avoid undue attention admittedly
important topic constructing suitable analogical mappings simply assume
mentor observer identical state spaces sm
sense isomorphic precise sense spaces isomorphicor
cases presumed isomorphic proven otherwiseis elaborated
discuss relationship agent abilities thus point
simply refer state without distinguishing mentors local space sm
observers
abilities even mapping states observations mentors state transitions
tell observer something mentors abilities must
assume observer way duplicate actions taken mentor
induce analogous transitions local state space words must
presumption mentor observer similar abilities
sense analogical mapping state spaces taken
homomorphism specifically might assume mentor observer
actions available e ao h sm
homomorphic respect pr requirement
weakened substantially without diminishing utility requiring
observer able implement actions actually taken mentor given
state finally might observer assumes duplicate
actions taken mentor finds evidence contrary case
presumed homomorphism state spaces follows
distinguish implicit imitation homogeneous action settingsdomains


fiimplicit imitation

analogical mapping indeed homomorphicand heterogeneous action
settingswhere mapping may homomorphism
general ways defining similarity ability example assuming
observer may able move state space similar fashion
mentor without following trajectories nehaniv dautenhahn
instance mentor may way moving directly key locations state
space observer may able move analogous locations less
direct fashion case analogy states may determined
single actions rather sequences actions local policies suggest
ways dealing restricted forms analogy type section
objectives even observer mentor similar identical abilities
value observer information gleaned mentor may depend
actual policy implemented mentor might suppose
closely related mentors policy optimal policy observer
useful information thus extent expect
closely aligned objectives mentor observer valuable
guidance provided mentor unlike existing teaching
suppose mentor making explicit efforts instruct observer
objectives may identical force observer
attempt explicitly imitate behavior mentor general make
explicit assumptions relationship objectives mentor
observer however see extent closer
utility derived implicit imitation
finally remark important assumption make throughout remainder
observer knows reward function ro state
observer evaluate ro without visited state view consistent
view reinforcement learning automatic programming user may easily
specify reward function e g form set predicates evaluated
state prior learning may difficult specify domain model
optimal policy setting unknown component mdp mo
transition function pro believe reinforcement learning
fact common practice reward function must
sampled
reiterate aim describe mechanism observer accelerate
learning emphasize position implicit imitationin contrast explicit
imitationis merely replicating behaviors state trajectories observed another
agent even attempting reach similar states believe agent must learn
capabilities adapt information contained observed behavior
agents must explore appropriate application observed behaviors
integrating appropriate achieve ends therefore
see imitation interactive process behavior one agent used guide
learning another


fiprice boutilier

given setting list possible ways observer mentor
cannot interact contrasting along way perspective assumptions
existing literature first observer could attempt directly infer
policy observations mentor state action pairs model conceptual
simplicity intuitive appeal forms basis behavioral cloning paradigm
sammut hurst kedzier michie urbancic bratko however assumes
observer mentor share reward function action capabilities
assumes complete unambiguous trajectories including action choices
observed related attempts deduce constraints value function
inferred action preferences mentor agent utgoff clouse suc bratko
however assumes congruity objectives model
distinct explicit teaching lin whitehead b assume
mentor incentive move environment way explicitly
guides learner explore environment action space effectively
instead trying directly learn policy observer could attempt use observed
state transitions agents improve environment model pro
accurate model reward function observer could calculate
accurate values states state values could used guide agent towards
distant rewards reduce need random exploration insight forms core
implicit imitation model developed literature
appropriate conditions listed specifically conditions
mentors actions unobservable mentor observer different reward
functions objectives thus applicable general conditions
many existing imitation learning teaching
addition model information mentors may communicate information
relevance irrelevance regions state space certain classes reward functions
observer use set states visited mentor heuristic guidance
perform backup computations state space
next two sections develop specific insights
agents use observations others improve assess
relevance regions within state spaces first focus homogeneous action
case extend model deal heterogeneous actions

implicit imitation homogeneous settings
begin describing implicit imitation homogeneous action settingsthe extension
heterogeneous settings build insights developed section develop
technique called implicit imitation observations mentor used
accelerate reinforcement learning first define homogeneous setting
develop implicit imitation finally demonstrate implicit imitation
works number simple designed illustrate role mechanisms describe
describe detail section



fiimplicit imitation

homogeneous actions
homogeneous action setting defined follows assume single mentor
observer individual mdps mm hs prm rm mo hs ao pro ro
respectively note agents share state space precisely assume
trivial isomorphic mapping allows us identify local states assume
mentor executing stationary policy often treat policy
deterministic remarks apply stochastic policies well let support
set supp state set actions accorded nonzero probability
state assume observer abilities mentor
following sense supp exists action ao ao
pro ao prm words observer able duplicate sense
inducing distribution successor states actual behavior mentor
equivalently agents local state spaces isomorphic respect actions
actually taken mentor subset states actions might taken
much weaker requiring full homomorphism sm course
existence full homomorphism sufficient perspective
require
implicit imitation
implicit imitation understood terms component processes
first extract action mentor integrate information
observers value estimates augmenting usual bellman backup mentor
action confidence testing procedure ensures use augmented
model observers model mentor reliable observers model
behavior extract occupancy information observations mentor
trajectories order focus observers computational effort extent specific
parts state space finally augment action selection process choose actions
explore high value regions revealed mentor remainder section
expands upon processes fit together
model extraction
information available observer quest learn act optimally
divided two categories first action takes receives experience tuple
hs r ti fact often ignore sampled reward r since assume reward
function r known advance standard model learning experience
used update transition model pro
second mentor transition observer obtains experience tuple hs ti
note observer direct access action taken mentor
induced state transition assume mentor implementing deterministic
stationary policy denoting mentors choice action state
policy induces markov chain prm prm pr denoting


fiprice boutilier

probability transition since learner observes mentors state
c chain pr
c simply estimated
transitions construct estimate pr
relative observed frequency mentor transitions w r transitions taken
observer prior possible mentor transitions standard bayesian
update techniques used instead use term model extraction process
estimating mentors markov chain
augmented bellman backups
c mentors markov chain
suppose observer constructed estimate pr
homogeneity assumption action replicated exactly observer
state thus policy principle duplicated observer able
identify actual actions used define value mentors policy
observers perspective

vm ro

x

prm vm



ts

notice equation uses mentors dynamics observers reward function
letting v denote optimal observers value function clearly v vm vm
provides lower bound observers value function
importantly terms making vm integrated directly bellman equation observers mdp forming augmented bellman equation




v ro max max
aao

x



pro v

ts

x



prm v



ts

usual bellman equation extra term added namely second
p
summation ts prm v denoting expected value duplicating mentors
action since unknown action identical one observers actions
term redundant augmented value equation valid course observer
augmented backup operation must rely estimates quantities observer
exploration policy ensures state visited infinitely often estimates pro
terms converge true values mentors policy ergodic state space
prm converge true value mentors policy restricted subset
states forming basis markov chain estimates prm
subset converge correctly respect chain ergodic states
remain unvisited estimates remain uninformed data since
mentors policy control observer way observer
influence distribution samples attained prm observer must therefore
able reason accuracy estimated model prm restrict
application augmented equation states prm known sufficient
accuracy
somewhat imprecise since initial distribution markov chain unknown
purposes dynamics relevant observer transition probabilities
used



fiimplicit imitation

prm cannot used indiscriminately argue highly informative
early learning process assuming mentor pursuing optimal policy
least behaving way tends visit certain states frequently
many states observer much accurate estimates prm
pro specific since observer learning must explore
state spacecausing less frequent visits sand action spacethus spreading
experience actions generally ensures sample size upon
prm greater pro action forms part mentors
policy apart accurate use prm often give informed
value estimates state since prior action often flat uniform
become distinguishable given state observer sufficient experience state

note reasoning holds even mentor implementing stationary stochastic policy since expected value stochastic policy fully observable
mdp cannot greater optimal deterministic policy direction offered mentor implementing deterministic policy tends focused
empirically found mentors offer broader guidance moderately stochastic
environments implement stochastic policies since tend visit
state space note extension multiple mentors straightforwardeach
mentor model incorporated augmented bellman equation without difficulty
model confidence
mentors markov chain ergodic mixing rate sufficiently low
mentor may visit certain state relatively infrequently estimated mentor transition
model corresponding state rarely never visited mentor may provide
misleading estimatebased small sample prior mentors chainof
value mentors unknown action since mentors policy
control observer misleading value may persist extended period since
augmented bellman equation consider relative reliability mentor
observer value state may overestimated observer
tricked overvaluing mentors unknown action consequently overestimating
value state
overcome incorporate estimate model confidence augmented
backups mentors markov chain observers action transitions
assume dirichlet prior parameters multinomial distributions
degroot reflect observers initial uncertainty possible transition probabilities sample counts mentor observer transitions update
distributions information could attempt perform optimal bayesian
estimation value function sample counts small normal approximations appropriate simple closed form expression resultant
distributions values could attempt employ sampling methods mixing rate refers quickly markov chain approaches stationary distribution
note underestimates considerations problematic since augmented bellman
equation reduces usual bellman equation



fiprice boutilier

v


vo

vo vm

figure lower bounds action values incorporate uncertainty penalty
terest simplicity employed approximate method combining information
sources inspired kaelblings interval estimation method
let v denote current estimated augmented value function pro prm denote
denote variance
estimated observer mentor transition let
model parameters
augmented bellman backup respect v confidence testing proceeds
follows first compute observers optimal action ao estimated
augmented values observers actions let q ao vo denote value
best action use model uncertainty encoded dirichlet distribution
construct lower bound vo value state observer model
state derived behavior e ignoring observations mentor
employ transition counts nm denote number times
observer made transition state state action performed
number times mentor observed making transition state
respectively counts estimate uncertainty model
p
variance dirichlet distribution let st
model variance

model











variance q value action due uncertainty local model
found simple application rule combining linear combinations variances v ar cx dy c v ar x v ar expression bellman backup
p
v ar r p r v


x



model
v



chebychevs inequality obtain confidence level even though dirichlet
distributions small sample counts highly non normal lower bound
vo vo co ao suitable constant c one may interpret penalizing
chebychevs inequality states k probability mass arbitrary distribution
within k standard deviations mean



fiimplicit imitation



function augmentedbackup v pro omodel
prm mmodel
c

arg maxaao

p
ts

pr v

p

vo ro pts pro v
vm ro ts prm v

p






p ts omodel v





v
ts mmodel

vo vo c

vm
vm c
vo vm
v vo
else
v vm
end

table implicit backup
value state subtracting uncertainty see figure value
vm mentors action estimated similarly analogous lower bound
vm constructed vo vm say vo supersedes
vm write vo vm vo vm mentor inspired
model fact lower expected value within specified degree confidence
uses nonoptimal action observers perspective mentor inspired model
lower confidence case reject information provided mentor
use standard bellman backup action model derived solely observers
experience thus suppressing augmented backup backed value vo
case
computing augmented backup confidence test shown
table parameters include current estimate augmented value

function v current estimated model pro associated local variance omodel


model mentors markov chain prm associated variance mmodel
calculates lower bounds returns mean value vo vm greatest lower
bound parameter c determines width confidence interval used mentor
rejection test
focusing
augmented bellman backups improves accuracy observers model second
way observer exploit observations mentor focus attention
states visited mentor model specific focusing mecha ideally would take uncertainty model current state account
uncertainty future states well meuleau bourgine



fiprice boutilier

nism adopt require observer perform possibly augmented bellman backup
state whenever mentor makes transition three effects first
mentor tends visit interesting regions space e g shares certain reward structure observer significant values backed mentor visited states
bias observers exploration towards regions second computational effort
c changes
concentrated toward parts state space estimated model pr
hence estimated value one observers actions may change third
computation focused model likely accurate discussed
action selection
integration exploration techniques action selection policy important
reinforcement learning guarantee convergence implicit imitation plays
second crucial role helping agent exploit information extracted mentor
improved convergence rely greedy quality exploration strategy
bias observer towards higher valued trajectories revealed mentor
expediency adopted greedy action selection method exploration rate decays time could easily employed semi greedy
methods boltzmann exploration presence mentor greedy action selection becomes complex observer examines actions state usual
way obtains best action ao corresponding value vo value
calculated mentors action vm vo vm observers action
model used greedy action defined exactly mentor present
however vm vo would define greedy action action
dictated mentors policy state unfortunately observer know
action define greedy action observers action closest
mentors action according observers current model estimates precisely
action similar mentors state denoted whose outcome
distribution minimum kullback leibler divergence mentors action outcome
distribution


argmina

x



pro log prm





observers experience action poor early training
chance closest action computation select wrong action rely
exploration policy ensure observers actions sampled appropriately
long run
present work assumed state space large agent
therefore able completely update q function whole space
intractability updating entire state space one motivations imitation
techniques absence information states true values would
bias value states along mentors trajectories look worthwhile
explore assuming bounds reward function setting initial qvalues entire space bound simple examples rewards strictly
mentor executing stochastic policy test kl divergence mislead learner



fiimplicit imitation

positive set bounds zero mentor trajectories intersect states valued
observing agent backups cause states trajectories higher
value surrounding states causes greedy step exploration method
prefer actions lead mentor visited states actions agent
information
model extraction specific reinforcement learning
model extraction augmented backups focusing mechanism extended notion
greedy action selection integrated model reinforcement learning relative ease generically implicit imitation requires
c markov chain induced mentors
observer maintain estimate pr
policythis estimate updated every observed transition b backups
performed estimate value function use augmented backup equation confic
dence testing course backups implemented estimated pr
c
prm addition focusing mechanism requires augmented backup
performed state visited mentor
demonstrate generality mechanisms combining efficient prioritized sweeping moore atkeson outlined
c
section prioritized sweeping works maintaining estimated transition model pr
b whenever experience tuple hs r ti sampled estimated
reward model r
model state change bellman backup performed incorporate revised
model usually fixed number additional backups performed selected
states states selected priority estimates potential change values
changes precipitated earlier backups effectively computational resources
backups focused states benefit backups
incorporating ideas prioritized sweeping simply requires following changes
c transition hs ti observer takes estimated model pr
dated augmented backup performed state augmented backups
performed fixed number states usual priority queue implementation
c updated
observed mentor transition hs ti estimated model pr
augmented backup performed augmented backups performed
fixed number states usual priority queue implementation

keeping samples mentor behavior implements model extraction augmented backups
integrate information observers value function performing augmented
backups observed transitions addition experienced transitions incorporates
focusing mechanism observer forced follow otherwise mimic actions
mentor directly back value information along mentors trajectory
ultimately observer must move states discover actions
used meantime important value information propagated
guide exploration
implicit imitation alter long run theoretical convergence properties
underlying reinforcement learning implicit imitation framework orthogonal greedy exploration alters definition greedy action


fiprice boutilier

greedy action taken given theoretically appropriate decay factor greedy
strategy thus ensure distributions action state
sampled infinitely often limit converge true values since extracted
model mentor corresponds one observers actions effect
value function calculations different effect observers sampled
action confidence mechanism ensures model samples
eventually come dominate fact better therefore sure convergence properties reinforcement learning implicit imitation identical
underlying reinforcement learning
benefit implicit imitation lies way extracted
mentor allow observer calculate lower bound value function use
lower bound choose greedy actions move agent towards higher valued regions
state space quicker convergence optimal policies better short term
practical performance respect accumulated discounted reward learning
extensions
implicit imitation model easily extended extract model information
multiple mentors mixing matching pieces extracted mentor achieve good
searching state set mentors knows
mentor highest value estimate value estimate best mentor
compared confidence test described observers value estimate
formal expression given multi augmented bellman equation




v ro max max
aao

max

mm

x

x



pro v



ts



prm v



ts

set candidate mentors ideally confidence estimates taken
account comparing mentor estimates may get mentor
high mean value estimate large variance observer experience
state mentor likely rejected poorer quality information
observer already experience observer might
better picking mentor lower mean confident estimate would
succeeded test observers model interests simplicity
however investigate multiple mentor combination without confidence testing
assumed action costs e agents rewards depend
state action selected state however use general
reward functions e g reward form r difficulty lies backing
action costs mentors chosen action unknown section defined
closest action function function used choose appropriate reward
augmented bellman equation generalized rewards takes following form




v max max ro
aao

x
ts





pro v



fiimplicit imitation

ro

x



prm v

ts

note bayesian methods could used could used estimate action costs
mentors chain well case generalized reward augmented equation
readily amended use confidence estimates similar fashion transition model
empirical demonstrations
following empirical tests incorporate model extraction focusing mechanism
prioritized sweeping illustrate types scenarios
implicit imitation provide advantages reinforcement learning agent
experiments expert mentor introduced experiment serve model
observer case mentor following greedy policy small
order tends cause mentors trajectories lie within cluster
surrounding optimal trajectories reflect good optimal policies even
small amount exploration environment stochasticity mentors generally
cover entire state space confidence testing important
experiments prioritized sweeping used fixed number backups per observed experienced sample greedy exploration used decaying
observer agents given uniform dirichlet priors q values initialized zero observer agents compared control agents benefit mentors experience
otherwise identical implementing prioritized sweeping similar parameters
exploration policies tests performed stochastic grid world domains since
make clear extent observers mentors optimal policies overlap
fail figure simple example shows start end state grid
typical optimal mentor trajectory illustrated solid line start
end states dotted line shows typical mentor influenced trajectory quite
similar observed mentor trajectory assume eight connectivity cells
state grid nine neighbors including agents four
possible actions experiments four actions move agent compass
directions north south east west although agent initially know
action focus primarily whether imitation improves performance
learning since learner converge optimal policy whether uses imitation

experiment imitation effect
first experiment compare performance observer model extraction
expert mentor performance control agent independent reinforcement learning given uniform nature grid world lack intermediate
rewards confidence testing required agents attempt learn policy
maximizes discounted return grid world start upper left corner
seek goal value lower right corner upon reaching goal agents
generally number backups set roughly equal length optimal noise free
path



fiprice boutilier



x

figure simple grid world start state goal state x


obs
fa series
ctrl

average reward per steps









delta

























simulation steps

figure basic observer control agent comparisons

restarted upper left corner generally mentor follow similar identical trajectory run mentors trained greedy strategy leaves
one path slightly highly valued rest action dynamics noisy
intended direction realized time one directions taken
otherwise uniformly discount factor figure plot cumulative
number goals obtained previous time steps observer obs
control ctrl agents averaged ten runs observer able quickly
incorporate policy learned mentor value estimates
steeper learning curve contrast control agent slowly explores space build
model first delta curve shows difference performance agents
agents converge optimal value function


fiimplicit imitation





average reward per steps



basic
scale




stoch




















simulation steps

figure delta curves showing influence domain size noise

experiment scaling noise
next experiment illustrates sensitivity imitation size state space
action noise level observer uses model extraction confidence testing
figure plot delta curves e difference performance observer
control agents basic scenario described scale scenario
state space size increased percent grid stoch scenario
noise level increased percent averaged ten runs
total gain represented area curves observer non imitating
prioritized sweeping agent increases state space size reflects whiteheads
observation grid worlds exploration requirements increase quickly
state space size optimal path length increases linearly see
guidance mentor help larger state spaces
increasing noise level reduces observers ability act upon information
received mentor therefore erodes advantage control agent
note however benefit imitation degrades gracefully increased noise
present even relatively extreme noise level
experiment confidence testing
sometimes observers prior beliefs transition probabilities mentor
mislead observer cause generate inappropriate values confidence mechanism proposed previous section prevent observer fooled
misleading priors mentors transition probabilities demonstrate role
confidence mechanism implicit imitation designed experiment scenario illustrated figure agents task navigate top left corner
bottom right corner grid order attain reward cre

fiprice boutilier









figure environment misleading priors
ated pathological scenario islands high reward enclosed obstacles
since observers priors reflect eight connectivity uniform high valued cells
middle island believed reachable states diagonally adjacent
small prior probability reality however agents action set precludes
agent therefore never able realize value four islands
scenario thus create fairly large region center space high estimated
value could potentially trap observer persisted prior beliefs
notice standard reinforcement learner quickly learn none actions
take rewarding islands contrast implicit imitator augmented backups
could fooled prior mentor model mentor visit states neighboring
island observer evidence upon change prior belief
mentor actions equally likely take one eight possible directions
imitator may falsely conclude basis mentor action model action
exist would allow access islands value observer therefore needs
confidence mechanism detect mentor model less reliable model
test confidence mechanism mentor follows path around outside
obstacles path cannot lead observer trap e provides
evidence observer diagonal moves islands feasible
combination high initial exploration rate ability prioritized sweeping
spread value across large distances virtually guarantees observer led
trap given scenario ran two observer agents control first observer
used confidence interval width given according chebychev rule
cover approximately percent arbitrary distribution second observer
given interval effectively disables confidence testing observer
confidence testing consistently became stuck examination value function revealed
consistent peaks within trap region inspection agent state trajectories showed
stuck trap observer confidence testing consistently escaped
trap observation value function time shows trap formed faded
away observer gained enough experience actions allow ignore


fiimplicit imitation


cr series



ctrl



average reward per steps

obs
















delta















simulation steps

figure misleading priors may degrade performance
overcome erroneous priors mentor actions figure performance
observer confidence testing shown performance control agent
averaged runs see observers performance slightly degraded
unaugmented control agent even pathological case
experiment qualitative difficulty
next experiment demonstrates potential gains imitation increase
qualitative difficulty observer employs model extraction
confidence testing though confidence testing play significant role
maze scenario introduce obstacles order increase difficulty learning
maze set grid figure obstacles complicating
agents journey top left bottom right corner optimal solution takes
form snaking step path distracting paths length branching
solution path necessitating frequent backtracking discount factor
percent noise optimal goal attainment rate six goals per steps
graph figure averaged ten runs see control
agent takes order steps build decent value function reliably leads
goal point achieving four goals per steps average
exploration rate still reasonably high unfortunately decreasing exploration quickly
leads slower value function formation imitation agent able take advantage
mentors expertise build reliable value function steps since
control agent unable reach goal first steps delta
control imitator simply equal imitators performance
mentor provide evidence path choices
intermediate rewards would cause observer make use misleading mentor priors
states



fiprice boutilier

figure complex maze


cmb series


average reward per steps







obs



delta







ctrl












x

simulation steps

figure imitation complex space
imitator quickly achieve optimal goal attainment rate six goals per steps
exploration rate decays much quickly
experiment improving suboptimal policies imitation
augmented backup rule require reward structure mentor
observer identical many useful scenarios rewards dissimilar
value functions policies induced share structure experiment
demonstrate one interesting scenario relatively easy suboptimal
solution difficult optimal solution observer finds suboptimal
path however able exploit observations mentor see


fiimplicit imitation






























figure maze perilous shortcut

shortcut significantly shortens path goal structure scenario
shown figure suboptimal solution lies path location around
scenic route location goal location mentor takes
vertical path location location shortcut discourage
use shortcut novice agents lined cells marked
agent immediately jumps back start state therefore difficult novice agent
executing random exploratory moves make way end shortcut
obtain value would reinforce future use observer control
therefore generally scenic route first
figure performance measured goals reached previous
steps control observer compared averaged ten runs indicating
value observations see observer control agent longer
scenic route though control agent takes longer observer goes
shortcut increases return almost double goal rate experiment shows
mentors improve observer policies even observers goals
mentors path
experiment multiple mentors
final experiment illustrates model extraction readily extended
observer extract multiple mentors exploit valuable parts
observer employs model extraction confidence testing figure
learner must move start location goal location two expert agents
different start goal states serve potential mentors one mentor repeatedly moves
location location along dotted line second mentor departs location
ends location along dashed line experiment observer must
mentor proceeding would provide guidance without prior knowledge actions
reversible



fiprice boutilier



csb series


average reward per steps



obs


delta



ctrl



















x

simulation steps

figure transfer non identical rewards











figure multiple mentors scenario

combine information examples provided two mentors independent
exploration order solve
figure see observer successfully pulls together information
sources order learn much quickly control agent averaged
runs see use value technique allows observer choose
mentors influence use state state basis order get best solution



fiimplicit imitation



obs
cmm series

average reward per steps





ctrl



delta



















simulation steps

figure learning multiple mentors

implicit imitation heterogeneous settings
homogeneity assumption violated implicit imitation framework described
cause learners convergence rate slow dramatically cases
cause learner become stuck small neighborhood state space particular
learner unable make state transition transition
probability mentor given state may drastically overestimate value
state inflated value estimate causes learner return repeatedly state
even though exploration never produce feasible action attains inflated
estimated value mechanism removing influence mentors markov
chain value estimatesthe observer extremely correctly confident
observations mentors model lies fact augmented
bellman backup justified assumption observer duplicate every mentor
action state pro prm
equivalent action exist guarantee value
calculated mentor action model fact achieved
feasibility testing
heterogeneous settings prevent lock poor convergence
use explicit action feasibility test augmented backup performed
observer tests whether mentors action differs actions given
current estimated augmented backup suppressed standard
bellman backup used update value function default mentor actions
decision binary could envision smoother decision criterion measures extent
mentors action duplicated



fiprice boutilier

assumed feasible observer however observer reasonably confident
infeasible state augmented backups suppressed
recall uncertainty agents true transition probabilities captured
dirichlet distribution derived sampled transitions comparing ao effected
difference means test respect corresponding dirichlets complicated
fact dirichlets highly non normal small parameter values transition
distributions multinomial deal non normality requiring minimum
number samples robust chebychev bounds pooled variance
distributions compared conceptually evaluate equation
r

pro ao prm


ao omodel
ao nm mmodel

ao nm

z



z critical value test parameter significance test
probability falsely reject two actions different
actually given highly non normal distributions early training process
appropriate z value given computed chebychevs bound solving
z z
samples accurate test persist augmented
backups embodying default assumption homogeneity value estimate
inflated backups agent biased obtain additional samples
allow agent perform required feasibility test assumption therefore
self correcting deal multivariate complications performing bonferroni
test seber shown give good practice mi sampson
efficient compute known robust dependence variables
bonferroni hypothesis test obtained conjoining several single variable tests suppose
actions ao r possible successor states sr e r transition
probabilities compare si hypothesis ei denotes ao
transition probability successor state si pr si pr ao si let
ei denote complementary hypothesis e transition probabilities differ
bonferroni inequality states


pr

r




ei



r
x



pr ei







thus test joint hypothesis ri ei two action sameby
testing r complementary hypotheses ei confidence level r reject
hypotheses reject notion two actions equal confidence
least mentor action deemed infeasible every observer action ao
multivariate bonferroni test described rejects hypothesis action
mentors
pseudo code bonferroni component feasibility test appears table
assumes sufficient number samples efficiency reasons cache
feasibility testing duplication mentors action state first determined
infeasible set flag state effect


fiimplicit imitation

function feasible boolean
ao
allsuccessorprobssimilar true
successors
p roq
p rm


nm

omodel
z
nm
z z r
allsuccessorprobssimilar false
end
allsuccessorprobssimilar
return true
end
return false

mmodel



table action feasibility testing
k step similarity repair
action feasibility testing essentially makes strict decision whether agent
duplicate mentors action specific state decided mentors action
infeasible augmented backups suppressed potential guidance offered eliminated
state unfortunately strictness test somewhat impoverished
notion similarity mentor observer turn unnecessarily limits
transfer mentor observer propose mechanism whereby mentors
influence may persist even specific action chooses feasible mentor
instead rely possibility observer may approximately duplicate mentors
trajectory instead exactly duplicating
suppose observer previously constructed estimated value function augmented backups mentor action model e mentors chain prm high
value calculated state subsequently suppose mentors action state
judged infeasible illustrated figure estimated value state
originally due mentors action sake illustration moves
high probability state lead highly rewarding region
state space number experiences state however learner concludes
action associated high probability transition tis feasible
point one two things must occur value calculated state
predecessors collapse exploration towards highly valued regions
beyond state ceases b estimated value drops slightly exploration continues
towards highly valued regions latter case may arise follows observer
previously explored vicinity state observers action model may
sufficiently developed still connect higher value regions beyond state state
bellman backups example learner sufficient experience
learned highly valued region reached alternative trajectory
u v w newly discovered infeasibility mentors transition
deleterious effect value estimate highly valued likely states
close mentors trajectory explored degree case state


fiprice boutilier

infeasible transition



high value
state



w
bridge

u

v

figure alternative path bridge value backups around infeasible paths
highly valued mentors action model still
valued highly enough likely guide exploration toward area call
alternative case u v w mentors action bridge allows
value higher value regions flow infeasible mentor transition
bridge formed without intention agent call process spontaneous
bridging
spontaneous bridge exist observers action generally undeveloped e g close uniform prior distributions typically
undeveloped assign small probability every possible outcome therefore diffuse value higher valued regions lead poor value estimate state
often dramatic drop value state predecessors
exploration towards highly valued region neighborhood state ceases
example could occur observers transition state assign low
probability e g close prior probability moving state u due lack experience
similarly surrounding states u v insufficiently explored
spontaneous bridging effect motivates broader notion similarity
observer short sequence actions bridges infeasible action
mentors trajectory mentors example still provide extremely useful guidance
moment assume short path path length greater given
integer k say observer k step similar mentor state observer
duplicate k fewer steps mentors nominal transition state sufficiently
high probability
given notion similarity observer test whether spontaneous bridge
exists determine whether observer danger value function collapse
concomitant loss guidance decides suppress augmented backup state
observer initiates reachability analysis starting state action
model pro determine sequence actions leads sufficiently
high probability state state mentors trajectory downstream
infeasible action k step bridge already exists augmented backups safely
suppressed state efficiency maintain flag state mark bridged
state known bridged k step reachability analysis need repeated
spontaneous bridge cannot found might still possible intentionally set
build one build bridge observer must explore state k steps away
hoping make contact mentors trajectory downstream infeasible mentor
general state space ergodicity lacking agent must consider predecessors state
k steps guarantee k step paths checked



fiimplicit imitation

action implement single search attempt k step random walk
trajectory average k steps away long ergodicity local connectivity
assumptions satisfied order search occur must motivate observer
return state engage repeated exploration could provide motivation
observer asking observer assume infeasible action repairable
observer therefore continue augmented backups support high value estimates
state observer repeatedly engage exploration point
danger course may fact bridge case observer
repeat search bridge indefinitely therefore need mechanism terminate
repair process k step repair infeasible could attempt explicitly keep
track possible paths open observer paths explicitly tried
observer determine repair possibilities exhausted instead elect
follow probabilistic search eliminates need bookkeeping bridge cannot
constructed within n attempts k step random walk repairability assumption
judged falsified augmented backup state suppressed observers bias
explore vicinity state eliminated bridge found state flag used
mark state irreparable
course nave heuristic strategy illustrates basic
import bridging systematic strategies could used involving explicit
bridge say local search alissandrakis nehaniv dautenhahn
another aspect address persistence search
bridges specific domain number unsuccessful attempts bridges
learner may conclude unable reconstruct mentors behavior case
search bridges may abandoned involves simple higher level inference
notion prior beliefs similarity capabilities notions could
used automatically determine parameter settings discussed
parameters k n must tuned empirically estimated given knowledge connectivity domain prior beliefs similar terms
length average repair trajectories mentor observer instance
n k seems suitable connected grid world low noise number
trajectories required cover perimeter states k step rectangle around state
note large values n reduce performance non imitating
agents temporary lock
feasibility k step repair easily integrated homogeneous implicit imitation framework essentially simply elaborate conditions augmented
backup employed course additional representation introduced
keep track whether state feasible bridged repairable many repair attempts made action selection mechanism overridden
bridge building required order search bridge bridge building
terminates n attempts however cannot affect long run convergence
aspects however exploration policy unchanged
complete elaborated decision procedure used determine augmented backups
employed state respect mentor appears table uses
internal state make decisions original model first check see
observers experience calculation value state supersedes mentor

fiprice boutilier

function use augmented boolean
vo vm return false
else f easible return true
else bridged return false
else reachable
bridged true
return false
else repairable return false
else searching
search steps k search progress
return true
search steps k search failed
attempts n
repairable false
return false
else
reset search
attempts attempts
return true
attempts initiate first attempt search
initiate search
return true

table elaborated augmented backup test

calculation observer uses experience calculation
mentors action feasible accept value calculated observationbased value function action infeasible check see state bridged
first time test requested reachability analysis performed
drawn cache subsequent requests state bridged suppress
augmented backups confident cause value function collapse state
bridged ask repairable first n requests agent attempt
k step repair repair succeeds state marked bridged cannot repair
infeasible transition mark repairable suppress augmented backups
may wish employ implicit imitation feasibility testing multiple mentor
scenario key change implicit imitation without feasibility testing
observer imitate feasible actions observer searches set
mentors one action highest value estimate observer
must consider mentors whose actions still considered feasible assumed
repairable
empirical demonstrations
section empirically demonstrate utility feasibility testing k step repair
techniques used surmount differences actions
agents small local differences state space topology


fiimplicit imitation

chosen specifically demonstrate necessity utility feasibility testing
k step repair
experiment necessity feasibility testing
first experiment shows importance feasibility testing implicit imitation
agents heterogeneous actions scenario agents must navigate across
obstacle free grid world upper left corner goal location lowerright agent reset upper left corner first agent mentor
news action set north south east west movement actions mentor
given optimal stationary policy study performance three
learners skew action set n ne sw unable duplicate mentor
exactly e g duplicating mentors e move requires learner move ne followed
move se n due nature grid world control imitation
agents actually execute actions get goal mentor
optimal goal rate control imitator therefore lower
mentor first learner employs implicit imitation feasibility testing second uses
imitation without feasibility testing third control agent uses imitation e
standard reinforcement learning agent agents experience limited stochasticity
form chance action randomly perturbed last section
agents use model reinforcement learning prioritized sweeping set k
n
effectiveness feasibility testing implicit imitation seen figure
horizontal axis represents time simulation steps vertical axis represents
average number goals achieved per time steps averaged runs see
imitation agent feasibility testing converges much quickly optimal
goal attainment rate agents agent without feasibility testing achieves
sporadic success early frequently locks due repeated attempts duplicate
infeasible mentor actions agent still manages reach goal time time
stochastic actions permit agent become permanently stuck obstaclefree scenario control agent without form imitation demonstrates significant
delay convergence relative imitation agents due lack form guidance
easily surpasses agent without feasibility testing long run gradual
slope control agent due higher variance control agents discovery time
optimal path feasibility testing imitator control agent converge
optimal solutions shown comparison two imitation agents feasibility
testing necessary adapt implicit imitation contexts involving heterogeneous actions
experiment changes state space
developed feasibility testing bridging primarily deal adapting
agents heterogeneous actions techniques however applied
agents differences state space connectivity ultimately equivalent
notions test constructed domain agents news
action set alter environment learners introducing obstacles arent
present mentor figure learners mentors path obstructed


fiprice boutilier



feas


fs series

average reward per steps





ctrl




nofeas
















simulation steps









figure utility feasibility testing


x

figure obstacle map mentor path
obstacles movement toward obstacle causes learner remain current state
sense action different effect mentors
figure see qualitatively similar previous experiment
contrast previous experiment imitator control use news action set
therefore shortest path length mentor consequently
optimal goal rate imitators control higher previous experiment
observer without feasibility testing difficulty maze value function
augmented mentor observations consistently leads observer states whose path
goal directly blocked agent feasibility testing quickly discovers
mentors influence inappropriate states conclude local differences
state well handled feasibility testing
next demonstrate feasibility testing completely generalize mentors
trajectory mentor follows path completely infeasible imitating
agent fix mentors path runs give imitating agent maze shown


fiimplicit imitation


feas
fo series



average reward per steps





ctrl















nofeas











simulation steps









figure interpolating around obstacles



observer
mentor
x

figure parallel generalization
figure two states mentor visits blocked obstacle
imitating agent able use mentors trajectory guidance builds
parallel trajectory completely disjoint mentors
figure gain imitator feasibility testing
control agent diminishes still exists marginally imitator forced generalize
completely infeasible mentor trajectory agent without feasibility testing
poorly even compared control agent gets stuck around
doorway high value gradient backed along mentors path becomes accessible
agents doorway imitation agent feasibility conclude cannot
proceed south doorway wall try different strategy
imitator without feasibility testing never explores far enough away doorway
setup independent value gradient guide goal slower decay
schedule exploration imitator without feasibility testing would goal


fiprice boutilier



feas


fp series

average reward per steps

ctrl









nofeas












simulation steps









figure parallel generalization

would still reduce performance imitator feasibility testing
imitator feasibility testing makes use prior beliefs follow mentor
backup value perpendicular mentors path value gradient therefore form
parallel infeasible mentor path imitator follow along side infeasible
path towards doorway makes necessary feasibility test proceeds
goal
explained earlier simple good chance informal effects
prior value leakage stochastic exploration may form bridges feasibility testing
cuts value propagation guides exploration difficult
agent spends lot time exploring accumulate sufficient samples conclude
mentors actions infeasible long agent constructed bridge
imitators performance would drop unaugmented reinforcement
learner
demonstrate bridging devised domain agents must navigate
upper left corner bottom right corner across river three steps wide
exacts penalty per step see figure goal state worth figure
path mentor shown starting top corner proceeding along edge
river crossing river goal mentor employs news action
set observer uses skew action set n ne sw attempts reproduce
mentor trajectory fail reproduce critical transition border
river east action infeasible skew agent mentor action
longer used backup value rewarding state alternative
paths river blocks greedy exploration region without bridging
optimistic lengthly exploration phase observer agents quickly discover negative
states river curtail exploration direction actually making across


fiimplicit imitation

figure river scenario
examine value function estimate steps imitator feasibility
testing repair capabilities see due suppression feasibility testing
darkly shaded high value states figure backed goal terminate abruptly
infeasible transition without making across river fact dominated
lighter grey circles showing negative values experiment bridging
prolong exploration phase right way employ k step repair
procedure k
examining graph figure see imitation agents experience early
negative dip guided deep river mentors influence agent
without repair eventually decides mentors action infeasible thereafter avoids
river possibility finding goal imitator repair discovers
mentors action infeasible immediately dispense mentors
guidance keeps exploring area mentors trajectory random walk
accumulating negative reward suddenly finds bridge rapidly
converges optimal solution control agent discovers goal
ten runs

applicability
simple experiments presented demonstrate major qualitative issues confronting implicit imitation agent specific mechanisms implicit imitation
address issues section examine assumptions mechanisms presented previous sections determine types suitable
implicit imitation present several dimensions prove useful predicting
performance implicit imitation types
repair steps take place area negative reward scenario need case
repair doesnt imply short term negative return



fiprice boutilier



average reward per steps



fb series


ctrl
ctrl

norepair

repair


norepair






repair








simulation steps







figure utility bridging
already identified number assumptions implicit imitation
applicablesome assumptions imitation teaching cannot
applied assumptions restrict applicability model include
lack explicit communication mentors observer independent objectives
mentors observer full observability mentors observer unobservability mentors
actions bounded heterogeneity assumptions full observability necessary
modelas formulatedto work though discuss extension partially observable case section assumptions lack communication unobservable actions
extend applicability implicit imitation beyond literature
conditions hold simpler form explicit communication may preferable finally
assumptions bounded heterogeneity independent objectives ensure implicit
imitation applied widely however degree rewards
actions homogeneous impact utility e acceleration learning offered implicit imitation turn attention predicting performance
implicit imitation function certain domain characteristics
predicting performance
section examine two questions first given implicit imitation applicable
implicit imitation bias agent suboptimal solution second
performance implicit imitation vary structural characteristics domains
one might want apply analysis internal structure state space
used motivate metric roughly predicts implicit imitation performance
conclude analysis space understood terms
distinct regions playing different roles within imitation context


fiimplicit imitation

implicit imitation model use observations agents improve observers knowledge environment rely sensible exploration policy
exploit additional knowledge clear understanding knowledge environment affects exploration therefore central understanding implicit imitation
perform domain
within implicit imitation framework agents know reward functions knowledge environment consists solely knowledge agents action
general take form simplicity restricted
decomposed local possible combination system
state agent action
local state action pairs allow prediction j step successor state
distribution given initial state sequence actions local policy quality
j step state predictions function every action model encountered
initial state states time j unfortunately quality j step
estimate drastically altered quality even single intermediate state action
model suggests connected regions state space states
fairly accurate allow reasonably accurate future state predictions
since estimated value state immediate reward
reward expected received subsequent states quality value estimate
depend quality action states connected
since greedy exploration methods bias exploration according estimated value
actions exploratory choices agent state dependent
connectivity reliable action states reachable analysis
implicit imitation performance respect domain characteristics therefore organized
around idea state space connectivity regions connectivity defines
imitation regions framework
since connected regions play important role implicit imitation introduce classification different regions within state space shown graphically figure
follows describe regions affect imitation performance model
first observe many tasks carried agent small subset
states within state space defined precisely many mdps
optimal policy ensure agent remains small subspace state space
leads us definition first regional distinction relevant vs irrelevant regions
relevant region set states non zero probability occupancy optimal
policy relevant region natural generalization optimal policy keeps
system within region fraction time
within relevant region distinguish three additional subregions explored
region contains states observer formulated reliable action
basis experience augmented region contains states observer
lacks reliable action improved value estimates due mentor observations
one often assumes system starts one small set states markov chain induced
optimal policy ergodic irrelevant region nonempty otherwise
empty



fiprice boutilier

observer
explored
region
blind
region

mentor
augmented
region

irrelevant
region

reward
figure classification regions state space
note explored augmented regions created observations
made learner transitions mentor regions
therefore significant connected components contiguous regions state space
reliable action mentor available finally blind region designates
states observer neither significant personal experience benefit
mentor observations information states within blind region come
largely agents prior beliefs
ask regions interact imitation agent first consider
impact relevance implicit imitation makes assumption accurate dynamics
allow observer make better decisions turn higher returns
sooner learning process however model information equally helpful
imitator needs enough information irrelevant region able avoid
since action choices influenced relative value actions irrelevant region
avoided looks worse relevant region given diffuse priors action
none actions open agent initially appear particularly attractive
however mentor provides observations within relevant region quickly make
relevant region look much promising method achieving higher returns
therefore constrain exploration significantly therefore considering
point view relevance small relevant region relative entire space
combined mentor operates within relevant region maximum
advantage imitation agent non imitating agent
explored region observer sufficiently accurate compute good
policy respect rewards within explored region additional observations
partitioning states explored blind augmented regions bears resemblance kearns
singhs partitioning state space known unknown regions unlike kearns singh
however use partitions analysis implicit imitation explicitly
maintain partitions use way compute policy



fiimplicit imitation

states within explored region provided mentor still improve performance
somewhat significant evidence required accurately discriminate expected
value two actions hence mentor observations explored region help
dramatic speedups convergence
consider augmented region observers q values
augmented observations mentor experiments previous sections
seen observer entering augmented region experience significant speedups
convergence due information inherent augmented value function
location rewards region characteristics augmented zone however
affect degree augmentation improves convergence speed
since observer receives observations mentors state actions
observer improved value estimates states augmented region policy
observer must therefore infer actions taken duplicate mentors
behavior observer prior beliefs effects actions may able
perform immediate inference mentors actual choice action perhaps
kl divergence maximum likelihood observers prior model uninformative
observer explore local action space exploring local action space
however agent must take action action effect since
guarantee agent took action duplicates mentors action may end
somewhere different mentor action causes observer fall outside
augmented region observer lose guidance augmented value function
provides fall back performance level non imitating agent
important consideration probability observer remain
augmented regions continue receive guidance one quality augmented region
affects observers probability staying within boundaries relative coverage
state space policy mentor may sparse complete relatively
deterministic domain defined begin end states sparse policy covering states
may adequate highly stochastic domain many start end states agent
may need complete policy e covering every state implicit imitation provide
guidance agent domains stochastic require complete
policies since policy cover larger part state space
important completeness policy predicting guidance must
take account probability transitions augmented region
actions domain largely invertible directly effectively agent
chance entering augmented region ergodicity lacking however
agent may wait process undergoes form reset
opportunity gather additional evidence regarding identity mentors actions
augmented region reset places agent back explored region
make way frontier last explored lack ergodicity
would reduce agents ability make progress towards high value regions resets
agent still guided attempt augmented region effectively
agent concentrate exploration boundary explored region
mentor augmented region
utility mentor observations depend probability augmented
explored regions overlapping course agents exploration explored


fiprice boutilier

regions accurate action allow agent move quickly possible high
value regions augmented regions augmented q values inform agents states
lead highly valued outcomes augmented region abuts explored region
improved value estimates augmented region rapidly communicated across
explored region accurate action observer use resultant improved
value estimates explored region together accurate action
explored region rapidly move towards promising states frontier
explored region states observer explore outward thereby eventually
expand explored region encompass augmented region
case explored region augmented region overlap
blind region since observer information beyond priors blind region
observer reduced random exploration non imitation context states
explored blind however imitation context blind area reduced
effective size augmented area hence implicit imitation effectively shrinks size
search space even overlap explored
augmented spaces
challenging case implicit imitation transfer occurs region augmented mentor observations fails connect observer explored region
regions significant reward values case augmented region initially
provide guidance observer independently located rewarding states
augmented regions used highlight shortcuts shortcuts represent improvements agents policy domains feasible solution easy
optimal solutions difficult implicit imitation used convert feasible solution
increasingly optimal solution
cross regional textures
seen distinctive regions used provide certain level insight
imitation perform domains analyze imitation performance
terms properties cut across state space analysis model information
impacts imitation performance saw regions connected accurate action
allowed observer use mentor observations learn promising direction
exploration see set mentor observations useful
concentrated connected region less useful dispersed state
space unconnected components fortunate completely observable environments
observations mentors tend capture continuous trajectories thereby providing
continuous regions augmented states partially observable environments occlusion
noise could lessen value mentor observations absence model predict
mentors state
effects heterogeneity whether due differences action capabilities
mentor observer due differences environment two agents
understood terms connectivity action value propagate along
chains action hit state mentor observer different
action capabilities state may possible achieve mentors value
therefore value propagation blocked sequential decision making aspect


fiimplicit imitation

reinforcement learning leads conclusion many scattered differences
mentor observer create discontinuity throughout space whereas
contiguous region differences mentor observer cause discontinuity
region leave large regions fully connected hence distribution pattern
differences mentor observer capabilities important prevalence
difference explore pattern next section
fracture metric
try characterize connectivity form metric since differences reward structure environment dynamics action affect connectivity would
manifest differences policies mentor observer designed
metric differences agents optimal policies call metric fracture
essentially computes average minimum distance state mentor
observer disagree policy state mentor observer agree policy measure roughly captures difficulty observer faces profitably exploiting
mentor observations reduce exploration demands
formally let mentors optimal policy observers let
state space sm set disputed states mentor observer
different optimal actions set neighboring disputed states constitutes disputed
region set sm called undisputed states let distance
metric space metric corresponds number transitions along
minimal length path states e shortest path nonzero probability
observer transitions standard grid world correspond manhattan
distance define fracture state space average minimal distance
disputed state closest undisputed state




x

sm ss



min

tssm





things equal lower fracture value tend increase propagation
value information across state space potentially resulting less exploration
required test metric applied number scenarios varying fracture
coefficients difficult construct scenarios vary fracture coefficient yet
expected value scenarios figure constructed
length possible paths start state goal state x
scenario scenario however upper path lower path mentor
trained scenario penalizes lower path mentor learns take
upper path imitator trained scenario upper path penalized
therefore take lower path equalized difficulty
follows generic greedy learning agent fixed exploration schedule e
fixed initial rate decay one scenario tuned magnitude penalties
exact placement along loops scenarios learner
exploration policy would converge optimal policy roughly number
steps
expected distance would give accurate estimate fracture difficult calculate



fiprice boutilier

x

x







x



b



c

x



figure fracture metric scenarios













observer initial exploration rate



















figure percentage runs ten converging optimal policy given fracture
initial exploration rate

figure mentor takes top loop optimal run imitator
would take bottom loop since loops short length common
path long average fracture low compare figure see
loops longthe majority states scenario loops
states loop distance nearest state observer mentor
policies agree namely state loop scenario therefore high average
fracture coefficient
since loops scenarios differ length penalties inserted loops
vary respect distance goal state therefore affect total discounted expected reward different ways penalties may cause agent
become stuck local minimum order avoid penalties exploration rate
low set experiments therefore compare observer agents basis
likely converge optimal solution given mentor example
figure presents percentage runs ten imitator converged
optimal solution e taking lower loops function exploration rate
scenario fracture see distinct diagonal trend table illustrating
increasing fracture requires imitator increase levels exploration order
reasons computational expediency entries near diagonal computed sampling entries confirms trend



fiimplicit imitation

optimal policy suggests fracture reflects feature rl domains may
important predicting efficacy implicit imitation
suboptimality bias
implicit imitation fundamentally biasing exploration observer
worthwhile ask positive effect observer performance short
answer mentor following optimal policy observer cause observer
explore neighborhood optimal policy generally bias observer
towards finding optimal policy
detailed answer requires looking explicitly exploration reinforcement learning theory greedy exploration policy suitable rate decay cause
implicit imitators eventually converge optimal solution unassisted
counterparts however practice exploration rate typically decayed quickly
order improve early exploitation mentor input given practical theoretically
unsound exploration rates observer may settle mentor strategy feasible
non optimal easily imagine examples consider situation agent
observing mentor following policy early learning process value
policy followed mentor may look better estimated value alternative
policies available observer could case mentors policy actually
optimal policy hand may case one alternative
policies observer neither personal experience observations
mentor actually superior given lack information aggressive exploitation policy might lead observer falsely conclude mentors policy optimal
implicit imitation bias agent suboptimal policy reason expect
agent learning domain sufficiently challenging warrant use imitation
would discovered better alternative emphasize even mentors policy
suboptimal still provides feasible solution preferable solution
many practical
regard see classic exploration exploitation tradeoff additional
interpretation implicit imitation setting component exploration rate
correspond observers belief sufficiency mentors policy
paradigm seems somewhat misleading think terms decision whether
follow specific mentor question much exploration
perform addition required reconstruct mentors policy
specific applications
see applications implicit imitation variety contexts emerging electronic
commerce information infrastructure driving development vast networks
multi agent systems networks used competitive purposes trade implicit
imitation used rl agent learn buying strategies information
filtering policies agents order improve behavior
control implicit imitation could used transfer knowledge existing
learned controller already adapted clients learning controller
completely different architecture many modern products elevator controllers


fiprice boutilier

crites barto cell traffic routers singh bertsekas automotive fuel
injection systems use adaptive controllers optimize performance system
specific user profiles upgrading technology underlying system quite
possible sensors actuators internal representation system
incompatible old system implicit imitation provides method transferring
valuable user information systems without explicit communication
traditional application imitation technologies lies area bootstrapping
intelligent artifacts traces human behavior within behavioral cloning
paradigm investigated transfer applications piloting aircraft sammut et al
controlling loading cranes suc bratko researchers investigated use imitation simplify programming robots kuniyoshi inaba
inoue ability imitation transfer complex nonlinear dynamic behaviors
existing human agents makes particularly attractive control

extensions
model implicit imitation presented makes certain restrictive assumptions regarding structure decision solved e g full observability knowledge
reward function discrete state action space simplifying assumptions
aided detailed development model believe basic intuitions much
technical development extended richer classes suggest several
possible extensions section provides interesting avenue
future
unknown reward functions
current paradigm assumes observer knows reward function
assumption consistent view rl form automatic programming
however relax constraint assuming ability generalize observed rewards
suppose expected reward expressed terms probability distribution
features observers state pr r f model rl distribution
learned agent experience features
applied mentors state sm observer use learned
reward distribution estimate expected reward mentor states well extends
paradigm domains rewards unknown preserves ability
observer evaluate mentor experiences terms
imitation techniques designed around assumption observer mentor
share identical rewards utgoffs would course work absence
reward function notion inverse reinforcement learning ng russell could
adapted case well challenge future would explore synthesis
implicit imitation reward inversion approaches handle observers prior
beliefs intermediate level correlation reward function observer
mentor


fiimplicit imitation

interaction agents
cast general imitation model framework stochastic games restriction model presented thus far noninteracting games essentially means
standard issues associated multiagent interaction arise course
many tasks require interactions agents cases implicit imitation offers
potential accelerate learning general solution requires integration imitation
general multiagent rl stochastic markov games littman
hu wellman bowling veloso would doubt rather
challenging yet rewarding endeavor
take simple example simple coordination e g two mobile agents
trying avoid carrying related tasks might imagine imitator
learning mentor reversing roles roles considering
observed state transition influenced joint action general
settings learning typically requires great care since agents learning nonstationary
environment may converge say equilibrium imitation techniques offer
certain advantages instance mentor expertise suggest means coordinating
agents e g providing focal point equilibrium selection making clear
specific convention passing right avoid collision
challenges opportunities present imitation used multiagent settings example competitive educational domains agents
choose actions maximize information exploration returns exploitation
must reason actions communicate information agents
competitive setting one agent may wish disguise intentions context
teaching mentor may wish choose actions whose purpose abundantly clear
considerations must become part action selection process
partially observable domains
extension model partially observable domains critical since unrealistic
many settings suppose learner constantly monitor activities mentor
central idea implicit imitation extract model information observations
mentor rather duplicating mentor behavior means mentors internal
belief state policy directly relevant learner take somewhat
behaviorist stance concern mentors observed behaviors
tell us possibilities inherent environment observer keep
belief state mentors current state done estimated
world model observer uses update belief state
preliminary investigation model suggests dealing partial observability
viable derived update rules augmented partially observable updates
updates bayesian formulation implicit imitation turn
bayesian rl dearden et al fully observable contexts seen
effective exploration mentor observations possible fully observable domains
bayesian model imitation used price boutilier extension
model cases mentors state partially observable reasonably straightforward
anticipate updates performed belief state mentors state


fiprice boutilier

action help alleviate fracture could caused incomplete observation
behavior
interesting dealing additional factor usual exploration exploitation
tradeoff determining whether worthwhile take actions render mentor
visible e g ensuring mentor remains view source information remains
available learning
continuous model free learning
many realistic domains continuous attributes large state action spaces prohibit
use explicit table representations reinforcement learning domains
typically modified make use function approximators estimate q function
points direct evidence received two important approaches
parameter e g neural networks bertsekas tsitsiklis
memory approaches atkeson moore schaal approaches
model free learning generally employed agent keeps value function uses
environment implicit model perform backups sampling distribution
provided environment observations
one straightforward casting implicit imitation continuous setting would
employ model free learning paradigm watkins dayan first recall augmented bellman backup function used implicit imitation




v ro max max
aao

x



pro v

ts



x

prm v



ts

examine augmented backup equation see converted
model free form much way ordinary bellman backup use standard
q function observer actions add one additional action corresponds
action taken mentor imagine observer state
took action ao ended state time mentor made transition
state sm write


q ao q ao ro ao max

max

ao






q

q sm q sm ro sm max max


ao





q


q




q

discussed earlier relative quality mentor observer estimates qfunction specific states may vary order avoid inaccurate prior beliefs
mentors action bias exploration need employ confidence measure
decide apply augmented equations feel natural setting
kind tests memory approaches function approximation memorybased approaches locally weighted regression atkeson et al provide estimates functions points previously unvisited maintain evidence
doesnt imply observer knows actions corresponds



fiimplicit imitation

set used generate estimates note implicit bias memory approaches assumes smoothness points unless additional data proves otherwise
basis bias propose compare average squared distance query
exemplars used estimate mentors q value average squared distance
query exemplars used observer estimate heuristically decide
agent reliable q value
suggested benefit prioritized sweeping prioritizedsweeping however adapted continuous settings forbes andre
feel reasonably efficient technique could made work

related work
imitation spans broad range dimensions ethological studies
abstract algebraic formulations industrial control fields crossfertilized informed come stronger conceptual definitions
better understanding limits capabilities imitation many computational
proposed exploit specialized niches variety control paradigms
imitation techniques applied variety real world control
conceptual foundations imitation clarified work natural imitation work apes russon galdikas octopi fiorito scotto
animals know socially facilitated learning widespread throughout animal kingdom number researchers pointed however social facilitation
take many forms conte noble todd instance mentors attention
object draw observers attention thereby lead observer manipulate object independently model provided mentor true imitation
therefore typically defined restrictive fashion visalberghi fragazy
cite mitchells definition
something c copy behavior produced organism
c similar something else model behavior
observation necessary production c baseline levels c
occurring spontaneously
c designed similar
behavior c must novel behavior already organized precise way
organisms repertoire
definition perhaps presupposes cognitive stance towards imitation
agent explicitly reasons behaviors agents behaviors relate
action capabilities goals
imitation analyzed terms type correspondence demonstrated
mentors behavior observers acquired behavior nehaniv dautenhahn
byrne russon correspondence types distinguished level
action level correspondence actions program level actions


fiprice boutilier

may completely different correspondence may found subgoals
effect level agent plans set actions achieve effect demonstrated
behavior direct correspondence subcomponents observers
actions mentors actions term abstract imitation proposed
case agents imitate behaviors come imitating mental state
agents demiris hayes
study specific computational imitation yielded insights
nature observer mentor relationship affects acquisition behaviors
observers instance related field behavioral cloning observed
mentors implement conservative policies generally yield reliable clones urbancic
bratko highly trained mentors following optimal policy small coverage
state space yield less reliable clones make mistakes sammut et al
partially observable learning perfect oracles disastrous
may choose policies perceptions available observer observer
therefore incorrectly biased away less risky policies require additional
perceptual capabilities scheffer greiner darken finally observed
successful clones would often outperform original mentor due cleanup effect
sammut et al
one original goals behavioral cloning michie extract knowledge
humans speed design controllers extracted knowledge
useful argued rule systems offer best chance intelligibility
van lent laird become clear however symbolic representations
complete answer representational capacity issue humans often organize
control tasks time typically lacking state perception approaches
control humans naturally break tasks independent components
subgoals urbancic bratko studies demonstrated humans
give verbal descriptions control policies match actual actions
urbancic bratko potential saving time acquisition borne
one study explicitly compared time extract rules time required
program controller van lent laird
addition traditionally considered imitation agent may face
learning imitate finding correspondence actions
states observer mentor nehaniv dautenhahn fully credible
learning observation absence communication protocols deal
issue
theoretical developments imitation accompanied number
practical implementations implementations take advantage properties different control paradigms demonstrate aspects imitation early behavioral cloning
took advantage supervised learning techniques decision trees sammut
et al decision tree used learn human operator mapped perceptions actions perceptions encoded discrete values time delay inserted
order synchronize perceptions actions trigger learning apprentice systems
mitchell et al attempted extract useful knowledge watching users
goal apprentices independently solve learning apprentices closely
related programming demonstration systems lieberman later efforts used


fiimplicit imitation

sophisticated techniques extract actions visual perceptions abstract
actions future use kuniyoshi et al work associative recurrent learning allowed work area extended learning temporal sequences
billard hayes associative learning used together innate following
behaviors acquire navigation expertise agents billard hayes
related slightly different form imitation studied multi agent
reinforcement learning community early precursor imitation found work
sharing perceptions agents tan closer imitation idea
replaying perceptions actions one agent second agent lin whitehead
transfer one agent another contrast behavioral clonings
transfer human agent representation different reinforcement learning
provides agents ability reason effects current actions expected
future utility agents integrate knowledge knowledge extracted
agents comparing relative utility actions suggested knowledge
source seeding approaches closely related trajectories recorded human
subjects used initialize planner subsequently optimizes plan order
account differences human effector robotic effector atkeson
schaal technique extended handle notion subgoals within
task atkeson schaal subgoals addressed others suc bratko
work idea agent extracting model mentor
model information place bounds value actions
reward function agents therefore learn mentors reward functions different

another family assumption mentor rational
e follows optimal policy reward function observer chooses
set actions given assumptions conclude action
chosen mentor particular state must higher value mentor
alternatives open mentor utgoff clouse therefore higher value
observer alternative system utgoff clouse therefore iteratively adjusts
values actions constraint satisfied model related
uses methodology linear quadratic control suc bratko first model
system constructed inverse control solved cost matrix
would observed controller behavior given environment model recent
work inverse reinforcement learning takes related reconstructing reward
functions observed behavior ng russell similar inversion
quadratic control formulated discrete domains
several researchers picked idea common representations perceptual functions action one representation
perception control pid controller model pid controller represents
behavior output compared observed behaviors order select action
closest observed behavior demiris hayes explicit motor action
schema investigated dual role perceptual motor representations
mataric williamson demiris mohan
imitation techniques applied diverse collection applications classical control applications include control systems robot arms kuniyoshi et al


fiprice boutilier

friedrich munch dillmann bocionek sassin aeration plants scheffer et al
container loading cranes suc bratko urbancic bratko imitation learning applied acceleration generic reinforcement learning lin
whitehead less traditional applications include transfer musical style
canamero arcos de mantaras support social atmosphere billard hayes dautenhahn breazeal scassellati imitation
investigated route language acquisition transmission billard et al
oliphant

concluding remarks
described formal principled imitation called implicit imitation
stochastic explicit forms communication possible
underlying model framework combined model extraction provides alternative
imitation learning observation systems makes use
model compute actions imitator take without requiring observer
duplicate mentors actions exactly shown implicit imitation offer significant
transfer capability several test proves robust face
noise capable integrating subskills multiple mentors able provide benefits
increase difficulty
seen feasibility testing extends implicit imitation principled manner
deal situations homogeneous action assumption invalid adding
bridging capabilities preserves extends mentors guidance presence infeasible actions whether due differences action capabilities local differences state
spaces relates idea following sense imitator
uses local search model repair discontinuities augmented value function acting world process applying imitation domains
learned properties particular developed fracture metric
characterize effectiveness mentor given observer specific domain
made considerable progress extending imitation classes model
developed rather flexible extended several ways example
bayesian imitation building work shows great potential
initial formulations promising approaches extending implicit imitation multiagent partially observable domains domains reward function
specified priori
number challenges remain field imitation bakker kuniyoshi
describe number among intriguing unique imitation
evaluation expected payoff observing mentor inferring useful state
reward mappings domains mentors observers repairing
locally searching order fit observed behaviors observers capabilities
goals raised possibility agents attempting reason
information revealed actions addition whatever concrete value actions
agent
model reinforcement applied numerous since implicit imitation added model reinforcement learning relatively little effort


fiimplicit imitation

expect applied many basis simple
elegant theory markov decision processes makes easy describe analyze though
focused simple examples designed illustrate different mechanisms
required implicit imitation expect variations provide interesting directions future

acknowledgments
thanks anonymous referees suggestions comments earlier versions
work michael littman editorial suggestions price supported nce irisiii project bac boutilier supported nserc grant ogp
nce iris iii project bac parts presented implicit imitation reinforcement learning proceedings sixteenth international conference
machine learning icml bled slovenia pp imitation
reinforcement learning agents heterogeneous actions proceedings fourteenth
biennial conference canadian society computational studies intelligence ai
ottawa pp

references
alissandrakis nehaniv c l dautenhahn k learning things
imitation bauer rich c eds aaai fall symposium learning
things pp cape cod
atkeson c g schaal robot learning demonstration proceedings
fourteenth international conference machine learning pp nashville tn
atkeson c g moore w schaal locally weighted learning control artificial
intelligence review
bakker p kuniyoshi robot see robot overview robot imitation aisb
workshop learning robots animals pp brighton uk
bellman r e dynamic programming princeton university press princeton
bertsekas p dynamic programming deterministic stochastic prentice hall
englewood cliffs
bertsekas p tsitsiklis j n neuro dynamic programming athena belmont
billard hayes g learning communicate imitation autonomous robots
proceedings seventh international conference artificial neural networks pp
lausanne switzerland
billard hayes g drama connectionist architecturefor control learning
autonomous robots adaptive behavior journal
billard hayes g dautenhahn k imitation skills means enhance learning
synthetic proto language autonomous robot proceedings aisb symposium
imitation animals artifacts pp edinburgh
boutilier c sequential optimality coordination multiagent systems proceedings
sixteenth international joint conference artificial intelligence pp stockholm


fiprice boutilier

boutilier c dean hanks decision theoretic structural assumptions
computational leverage journal artificial intelligence
bowling veloso rational convergent learning stochastic games proceedings seventeenth international joint conference artificial intelligence pp
seattle
breazeal c imitation social exchange humans robot proceedings
aisb symposium imitation animals artifacts pp edinburgh
byrne r w russon e learning imitation hierarchical behavioral
brain sciences
canamero arcos j l de mantaras r l imitating human performances automatically generate expressive jazz ballads proceedings aisb symposium
imitation animals artifacts pp edinburgh
cassandra r kaelbling l p littman l acting optimally partially observable stochastic domains proceedings twelfth national conference artificial
intelligence pp seattle
conte r intelligent social learning proceedings aisb symposium starting
society applications social analogies computational systems birmingham
crites r barto g elevator group control multiple reinforcement learning
agents machine learning
dean givan r model minimization markov decision processes proceedings
fourteenth national conference artificial intelligence pp providence
dearden r boutilier c abstraction approximate decision theoretic
artificial intelligence
dearden r friedman n andre model bayesian exploration proceedings
fifteenth conference uncertainty artificial intelligence pp stockholm
degroot h probability statistics addison wesley reading
demiris j hayes g robots ape proceedings aaai fall symposium
socially intelligent agents pp cambridge
demiris j hayes g active passive routes imitation proceedings
aisb symposium imitation animals artifacts pp edinburgh
fiorito g scotto p observational learning octopus vulgaris science
forbes j andre practical reinforcement learning continuous domains tech rep
ucb csd computer science division university california berkeley
friedrich h munch dillmann r bocionek sassin robot programming
demonstration rpd support induction human interaction machine learning

hartmanis j stearns r e algebraic structure theory sequential machines prenticehall englewood cliffs
hu j wellman p multiagent reinforcement learning theoretical framework
proceedings fifthteenth international conference machine learning
pp madison wi
kaelbling l p learning embedded systems mit press cambridge


fiimplicit imitation

kaelbling l p littman l moore w reinforcement learning survey journal
artificial intelligence
kearns singh near optimal reinforcement learning polynomial time proceedings fifthteenth international conference machine learning pp madison
wi
kuniyoshi inaba inoue h learning watching extracting reusable task
knowledge visual observation human performance ieee transactions robotics
automation
lee yannakakis online miminization transition systems proceedings
th annual acm symposium theory computing stoc pp victoria
bc
lieberman h mondrian teachable graphical editor cypher ed watch
programming demonstration pp mit press cambridge
lin l j self improvement reinforcement learning teaching machine
learning proceedings eighth international workshop ml
lin l j self improving reactive agents reinforcement learning
teaching machine learning
littman l markov games framework multi agent reinforcement learning
proceedings eleventh international conference machine learning pp
brunswick nj
lovejoy w survey algorithmic methods partially observed markov decision
processes annals operations
mataric j communication reduce locality distributed multi agent learning
journal experimental theoretical artificial intelligence
mataric j williamson demiris j mohan behaviour primitives
articulated control r pfiefer b blumberg j w w ed fifth international
conference simulation adaptive behavior sab pp zurich mit press
meuleau n bourgine p exploration multi state environments local mesures
back propagation uncertainty machine learning
mi j sampson r comparison bonferroni scheffe bounds journal
statistical inference
michie knowledge learning machine intelligence sterling l ed intelligent
systems plenum press york
mitchell mahadevan steinberg l leap learning apprentice vlsi
design proceedings ninth international joint conference artificial intelligence
pp los altos california morgan kaufmann publishers inc
moore w atkeson c g prioritized sweeping reinforcement learning less
data less real time machine learning
myerson r b game theory analysis conflict harvard university press cambridge
nehaniv c dautenhahn k mapping dissimilar bodies affordances
algebraic foundations imitation proceedings seventh european workshop
learning robots pp edinburgh


fiprice boutilier

ng russell inverse reinforcement learning proceedings
seventeenth international conference machine learning pp stanford
noble j todd p really imitation review simple mechanisms social
information gathering proceedings aisb symposium imitation animals
artifacts pp edinburgh
oliphant cultural transmission communications systems comparing observational
reinforcement learning proceedings aisb symposium imitation
animals artifacts pp edinburgh
price b boutilier c bayesian imitation reinforcement learning proceedings eighteenth international joint conference artificial intelligence acapulco
appear
puterman l markov decision processes discrete stochastic dynamic programming
john wiley sons inc york
russon galdikas b imitation free ranging rehabilitant orangutans pongopygmaeus journal comparative psychology
sammut c hurst kedzier michie learning fly proceedings
ninth international conference machine learning pp aberdeen uk
scassellati b knowing imitate knowing succeed proceedings
aisb symposium imitation animals artifacts pp edinburgh
scheffer greiner r darken c experimentation better perfect
guidance proceedings fourteenth international conference machine learning
pp nashville
seber g f multivariate observations wiley york
shapley l stochastic games proceedings national academy sciences

singh p bertsekas reinforcement learning dynamic channel allocation
cellular telephone systems advances neural information processing systems pp
cambridge mit press
smallwood r sondik e j optimal control partially observable markov
processes finite horizon operations
suc bratko skill reconstruction induction lq controllers subgoals
proceedings fifteenth international joint conference artificial intelligence pp
nagoya
sutton r learning predict method temporal differences machine learning

sutton r barto g reinforcement learning introduction mit press
cambridge
tan multi agent reinforcement learning independent vs cooperative agents icml pp
urbancic bratko reconstruction human skill machine learning eleventh
european conference artificial intelligence pp amsterdam


fiimplicit imitation

utgoff p e clouse j two kinds training information evaluation function
learning proceedings ninth national conference artificial intelligence pp
anaheim ca
van lent laird j learning hierarchical performance knowledge observation
proceedings sixteenth international conference machine learning pp
bled slovenia
visalberghi e fragazy monkeys ape parker gibson k eds
language intelligence monkeys apes pp cambridge university press
cambridge
watkins c j c h dayan p q learning machine learning
whitehead complexity analysis cooperative mechanisms reinforcement learning proceedings ninth national conference artificial intelligence pp
anaheim
whitehead b complexity cooperation q learning machine learning proceedings eighth international workshop ml pp




