Journal Artificial Intelligence Research 37 (2010) 141-188

Submitted 10/09; published 02/10

Frequency Meaning:
Vector Space Models Semantics
Peter D. Turney

peter.turney@nrc-cnrc.gc.ca

National Research Council Canada
Ottawa, Ontario, Canada, K1A 0R6

Patrick Pantel

me@patrickpantel.com

Yahoo! Labs
Sunnyvale, CA, 94089, USA

Abstract
Computers understand little meaning human language. profoundly
limits ability give instructions computers, ability computers explain
actions us, ability computers analyse process text. Vector space
models (VSMs) semantics beginning address limits. paper surveys
use VSMs semantic processing text. organize literature VSMs according
structure matrix VSM. currently three broad classes VSMs,
based termdocument, wordcontext, pairpattern matrices, yielding three classes
applications. survey broad range applications three categories
take detailed look specific open source project category. goal
survey show breadth applications VSMs semantics, provide new
perspective VSMs already familiar area, provide
pointers literature less familiar field.

1. Introduction
One biggest obstacles making full use power computers
currently understand little meaning human language. Recent progress
search engine technology scratching surface human language, yet
impact society economy already immense. hints transformative
impact deeper semantic technologies have. Vector space models (VSMs), surveyed
paper, likely part new semantic technologies.
paper, use term semantics general sense, meaning word,
phrase, sentence, text human language, study meaning.
concerned narrower senses semantics, semantic web approaches
semantics based formal logic. present survey VSMs relation
distributional hypothesis approach representing aspects natural language
semantics.
VSM developed SMART information retrieval system (Salton, 1971)
Gerard Salton colleagues (Salton, Wong, & Yang, 1975). SMART pioneered
many concepts used modern search engines (Manning, Raghavan, &
Schutze, 2008). idea VSM represent document collection
point space (a vector vector space). Points close together space
semantically similar points far apart semantically distant. users
c
2010
AI Access Foundation National Research Council Canada. Reprinted permission.

fiTurney & Pantel

query represented point space documents (the query pseudodocument). documents sorted order increasing distance (decreasing semantic
similarity) query presented user.
success VSM information retrieval inspired researchers extend
VSM semantic tasks natural language processing, impressive results.
instance, Rapp (2003) used vector-based representation word meaning achieve
score 92.5% multiple-choice synonym questions Test English Foreign
Language (TOEFL), whereas average human score 64.5%.1 Turney (2006) used
vector-based representation semantic relations attain score 56% multiple-choice
analogy questions SAT college entrance test, compared average human score
57%.2
survey, organized past work VSMs according type matrix
involved: termdocument, wordcontext, pairpattern. believe choice
particular matrix type fundamental choices, particular
linguistic processing mathematical processing. Although three matrix types cover
work, reason believe three types exhaust possibilities.
expect future work introduce new types matrices higher-order tensors.3
1.1 Motivation Vector Space Models Semantics
VSMs several attractive properties. VSMs extract knowledge automatically
given corpus, thus require much less labour approaches semantics,
hand-coded knowledge bases ontologies. example, main resource used
Rapps (2003) VSM system measuring word similarity British National Corpus
(BNC),4 whereas main resource used non-VSM systems measuring word similarity
(Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Jarmasz & Szpakowicz, 2003)
lexicon, WordNet5 Rogets Thesaurus. Gathering corpus new language
generally much easier building lexicon, building lexicon often involves
gathering corpus, SemCor WordNet (Miller, Leacock, Tengi, & Bunker, 1993).
VSMs perform well tasks involve measuring similarity meaning
words, phrases, documents. search engines use VSMs measure similarity
query document (Manning et al., 2008). leading algorithms measuring semantic relatedness use VSMs (Pantel & Lin, 2002a; Rapp, 2003; Turney, Littman,
Bigham, & Shnayder, 2003). leading algorithms measuring similarity semantic relations use VSMs (Lin & Pantel, 2001; Turney, 2006; Nakov & Hearst, 2008).
(Section 2.4 discusses differences types similarity.)
find VSMs especially interesting due relation distributional hypothesis related hypotheses (see Section 2.7). distributional hypothesis
1. Regarding average score 64.5% TOEFL questions, Landauer Dumais (1997) note
that, Although know performance would compare, example, U.S. school
children particular age, told average score adequate admission many
universities.
2. average score highschool students senior year, applying US universities.
discussion score, see Section 6.3 Turneys (2006) paper.
3. vector first-order tensor matrix second-order tensor. See Section 2.5.
4. See http://www.natcorp.ox.ac.uk/.
5. See http://wordnet.princeton.edu/.

142

fiFrom Frequency Meaning

words occur similar contexts tend similar meanings (Wittgenstein, 1953;
Harris, 1954; Weaver, 1955; Firth, 1957; Deerwester, Dumais, Landauer, Furnas, & Harshman, 1990). Efforts apply abstract hypothesis concrete algorithms measuring
similarity meaning often lead vectors, matrices, higher-order tensors.
intimate connection distributional hypothesis VSMs strong motivation
taking close look VSMs.
uses vectors matrices count vector space models. purposes
survey, take defining property VSMs values elements
VSM must derived event frequencies, number times given word
appears given context (see Section 2.6). example, often lexicon knowledge
base may viewed graph, graph may represented using adjacency matrix,
imply lexicon VSM, because, general, values
elements adjacency matrix derived event frequencies. emphasis
event frequencies brings unity variety VSMs explicitly connects
distributional hypothesis; furthermore, avoids triviality excluding many possible
matrix representations.
1.2 Vectors AI Cognitive Science
Vectors common AI cognitive science; common VSM
introduced Salton et al. (1975). novelty VSM use frequencies
corpus text clue discovering semantic information.
machine learning, typical problem learn classify cluster set items
(i.e., examples, cases, individuals, entities) represented feature vectors (Mitchell, 1997;
Witten & Frank, 2005). general, features derived event frequencies,
although possible (see Section 4.6). example, machine learning algorithm
applied classifying clustering documents (Sebastiani, 2002).
Collaborative filtering recommender systems use vectors (Resnick, Iacovou,
Suchak, Bergstrom, & Riedl, 1994; Breese, Heckerman, & Kadie, 1998; Linden, Smith, &
York, 2003). typical recommender system, person-item matrix,
rows correspond people (customers, consumers), columns correspond items
(products, purchases), value element rating (poor, fair, excellent)
person given item. Many mathematical techniques work well
termdocument matrices (see Section 4) work well person-item matrices,
ratings derived event frequencies.
cognitive science, prototype theory often makes use vectors. basic idea
prototype theory members category central others (Rosch
& Lloyd, 1978; Lakoff, 1987). example, robin central (prototypical) member
category bird, whereas penguin peripheral. Concepts varying degrees
membership categories (graded categorization). natural way formalize
represent concepts vectors categories sets vectors (Nosofsky, 1986; Smith, Osherson, Rips, & Keane, 1988). However, vectors usually based numerical scores
elicited questioning human subjects; based event frequencies.
Another area psychology makes extensive use vectors psychometrics,
studies measurement psychological abilities traits. usual instrument
143

fiTurney & Pantel

measurement test questionnaire, personality test. results test
typically represented subject-item matrix, rows represent subjects
(people) experiment columns represent items (questions) test
(questionnaire). value element matrix answer corresponding
subject gave corresponding item. Many techniques vector analysis, factor
analysis (Spearman, 1904), pioneered psychometrics.
cognitive science, Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer & Dumais, 1997), Hyperspace Analogue Language (HAL) (Lund, Burgess, &
Atchley, 1995; Lund & Burgess, 1996), related research (Landauer, McNamara, Dennis, & Kintsch, 2007) entirely within scope VSMs, defined above, since
research uses vector space models values elements derived
event frequencies, number times given word appears given context. Cognitive scientists argued empirical theoretical reasons
believing VSMs, LSA HAL, plausible models aspects human cognition (Landauer et al., 2007). AI, computational linguistics, information
retrieval, plausibility essential, may seen sign VSMs
promising area research.
1.3 Motivation Survey
paper survey vector space models semantics. currently comprehensive, up-to-date survey field. show survey, vector space models
highly successful approach semantics, wide range potential actual
applications. much recent growth research area.
paper interest AI researchers work natural language,
especially researchers interested semantics. survey serve general
introduction area provide framework unified perspective
organizing diverse literature topic. encourage new research area,
pointing open problems areas exploration.
survey makes following contributions:
New framework: provide new framework organizing literature: term
document, wordcontext, pairpattern matrices (see Section 2). framework shows
importance structure matrix (the choice rows columns) determining potential applications may inspire researchers explore new structures
(different kinds rows columns, higher-order tensors instead matrices).
New developments: draw attention pairpattern matrices. use pair
pattern matrices relatively new deserves study. matrices address
criticisms directed wordcontext matrices, regarding lack sensitivity
word order.
Breadth approaches applications: existing survey shows
breadth potential actual applications VSMs semantics. Existing summaries
omit pairpattern matrices (Landauer et al., 2007).
Focus NLP CL: focus survey systems perform practical
tasks natural language processing computational linguistics. Existing overviews focus
cognitive psychology (Landauer et al., 2007).
144

fiFrom Frequency Meaning

Success stories: draw attention fact VSMs arguably
successful approach semantics, far.
1.4 Intended Readership
goal writing paper survey state art vector space models
semantics, introduce topic new area, give new
perspective already familiar area.
assume reader basic understanding vectors, matrices, linear algebra,
one might acquire introductory undergraduate course linear algebra,
text book (Golub & Van Loan, 1996). basic concepts vectors matrices
important mathematical details. Widdows (2004) gives gentle
introduction vectors perspective semantics.
assume reader familiarity computational linguistics information retrieval. Manning et al. (2008) provide good introduction information retrieval.
computational linguistics, recommend Manning Schutzes (1999) text.
reader familiar linear algebra computational linguistics, survey
present barriers understanding. Beyond background, necessary
familiar VSMs used information retrieval, natural language processing, computational linguistics. However, reader would
background reading, recommend Landauer et al.s (2007) collection.
1.5 Highlights Outline
article structured follows. Section 2 explains framework organizing
literature VSMs according type matrix involved: termdocument, wordcontext,
pairpattern. section, present overview VSMs, without getting
details matrix generated corpus raw text.
high-level framework place, Sections 3 4 examine steps involved
generating matrix. Section 3 discusses linguistic processing Section 4 reviews
mathematical processing. order corpus would processed
VSM systems (first linguistic processing, mathematical processing).
VSMs used semantics, input model usually plain text.
VSMs work directly raw text, first apply linguistic processing
text, stemming, part-of-speech tagging, word sense tagging, parsing. Section 3
looks linguistic tools semantic VSMs.
simple VSM, simple termdocument VSM, value element
document vector number times corresponding word occurs given
document, VSMs apply mathematical processing raw frequency values.
Section 4 presents main mathematical operations: weighting elements, smoothing
matrix, comparing vectors. section describes optimization strategies
comparing vectors, distributed sparse matrix multiplication randomized
techniques.
end Section 4, reader general view concepts involved
vector space models semantics. take detailed look three VSM systems
Section 5. representative termdocument VSMs, present Lucene information
145

fiTurney & Pantel

retrieval library.6 wordcontext VSMs, explore Semantic Vectors package,
builds Lucene.7 representative pairpattern VSMs, review Latent
Relational Analysis module S-Space package, builds Lucene.8
source code three systems available open source licensing.
turn broad survey applications semantic VSMs Section 6. section serves short historical view research semantic VSMs, beginning
information retrieval Section 6.1. purpose give reader idea
breadth applications VSMs provide pointers literature,
reader wishes examine applications detail.
termdocument matrix, rows correspond terms columns correspond documents (Section 6.1). document provides context understanding term.
generalize idea documents chunks text arbitrary size (phrases, sentences,
paragraphs, chapters, books, collections), result wordcontext matrix, includes termdocument matrix special case. Section 6.2 discusses applications
wordcontext matrices. Section 6.3 considers pairpattern matrices, rows correspond pairs terms columns correspond patterns pairs
occur.
Section 7, discuss alternatives VSMs semantics. Section 8 considers
future VSMs, raising questions power limitations. conclude
Section 9.

2. Vector Space Models Semantics
theme unites various forms VSMs discuss paper
stated statistical semantics hypothesis: statistical patterns human word usage
used figure people mean.9 general hypothesis underlies several
specific hypotheses, bag words hypothesis, distributional hypothesis,
extended distributional hypothesis, latent relation hypothesis, discussed below.
2.1 Similarity Documents: TermDocument Matrix
paper, use following notational conventions: Matrices denoted bold
capital letters, A. Vectors denoted bold lowercase letters, b. Scalars represented
lowercase italic letters, c.
large collection documents, hence large number document
vectors, convenient organize vectors matrix. row vectors matrix
correspond terms (usually terms words, discuss possibilities)
6.
7.
8.
9.

See http://lucene.apache.org/java/docs/.
See http://code.google.com/p/semanticvectors/.
See http://code.google.com/p/airhead-research/wiki/LatentRelationalAnalysis.
phrase taken Faculty Profile George Furnas University Michigan,
http://www.si.umich.edu/people/faculty-detail.htm?sid=41. full quote is, Statistical Semantics
Studies statistical patterns human word usage used figure people
mean, least level sufficient information access. term statistical semantics appeared
work Furnas, Landauer, Gomez, Dumais (1983), defined there.

146

fiFrom Frequency Meaning

column vectors correspond documents (web pages, example). kind
matrix called termdocument matrix.
mathematics, bag (also called multiset) set, except duplicates
allowed. example, {a, a, b, c, c, c} bag containing a, b, c. Order matter
bags sets; bags {a, a, b, c, c, c} {c, a, c, b, a, c} equivalent. represent
bag {a, a, b, c, c, c} vector x = h2, 1, 3i, stipulating first element
x frequency bag, second element frequency b bag,
third element frequency c. set bags represented matrix X,
column x:j corresponds bag, row xi: corresponds unique member,
element xij frequency i-th member j-th bag.
termdocument matrix, document vector represents corresponding document
bag words. information retrieval, bag words hypothesis
estimate relevance documents query representing documents
query bags words. is, frequencies words document tend indicate
relevance document query. bag words hypothesis basis
applying VSM information retrieval (Salton et al., 1975). hypothesis expresses
belief column vector termdocument matrix captures (to degree)
aspect meaning corresponding document; document about.
Let X termdocument matrix. Suppose document collection contains n documents unique terms. matrix X rows (one row unique
term vocabulary) n columns (one column document). Let wi i-th
term vocabulary let dj j-th document collection. i-th row
X row vector xi: j-th column X column vector x:j . row vector
xi: contains n elements, one element document, column vector x:j contains
elements, one element term. Suppose X simple matrix frequencies.
element xij X frequency i-th term wi j-th document dj .
general, value elements X zero (the matrix sparse),
since documents use small fraction whole vocabulary. randomly
choose term wi document dj , likely wi occur anywhere dj ,
therefore xij equals 0.
pattern numbers xi: kind signature i-th term wi ; likewise,
pattern numbers x:j signature j-th document dj . is, pattern
numbers tells us, degree, term document about.
vector x:j may seem rather crude representation document dj . tells
us frequently words appear document, sequential order words
lost. vector attempt capture structure phrases, sentences,
paragraphs, chapters document. However, spite crudeness, search
engines work surprisingly well; vectors seem capture important aspect semantics.
VSM Salton et al. (1975) arguably first practical, useful algorithm
extracting semantic information word usage. intuitive justification term
document matrix topic document probabilistically influence authors
choice words writing document.10 two documents similar topics,
two corresponding column vectors tend similar patterns numbers.
10. Newer generative models, Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003), directly
model intuition. See Sections 4.3 7.

147

fiTurney & Pantel

2.2 Similarity Words: WordContext Matrix
Salton et al. (1975) focused measuring document similarity, treating query search
engine pseudo-document. relevance document query given
similarity vectors. Deerwester et al. (1990) observed shift focus
measuring word similarity, instead document similarity, looking row vectors
termdocument matrix, instead column vectors.
Deerwester et al. (1990) inspired termdocument matrix Salton et al. (1975),
document necessarily optimal length text measuring word similarity.
general, may wordcontext matrix, context given words,
phrases, sentences, paragraphs, chapters, documents, exotic possibilities,
sequences characters patterns.
distributional hypothesis linguistics words occur similar contexts
tend similar meanings (Harris, 1954). hypothesis justification applying VSM measuring word similarity. word may represented vector
elements derived occurrences word various contexts,
windows words (Lund & Burgess, 1996), grammatical dependencies (Lin, 1998;
Pado & Lapata, 2007), richer contexts consisting dependency links selectional
preferences argument positions (Erk & Pado, 2008); see Sahlgrens (2006) thesis
comprehensive study various contexts. Similar row vectors wordcontext matrix
indicate similar word meanings.
idea word usage reveal semantics implicit things
Wittgenstein (1953) said language-games family resemblance. Wittgenstein
primarily interested physical activities form context word usage (e.g.,
word brick, spoken context physical activity building house), main
context word often words.11
Weaver (1955) argued word sense disambiguation machine translation
based co-occurrence frequency context words near given target word (the
word want disambiguate). Firth (1957, p. 11) said, shall know word
company keeps. Deerwester et al. (1990) showed intuitions Wittgenstein (1953), Harris (1954), Weaver, Firth could used practical algorithm.
2.3 Similarity Relations: PairPattern Matrix
pairpattern matrix, row vectors correspond pairs words, mason : stone
carpenter : wood, column vectors correspond patterns pairs cooccur, X cuts X works . Lin Pantel (2001) introduced
pairpattern matrix purpose measuring semantic similarity patterns;
is, similarity column vectors. Given pattern X solves , algorithm
able find similar patterns, solved X, resolved X,
X resolves .
Lin Pantel (2001) proposed extended distributional hypothesis, patterns
co-occur similar pairs tend similar meanings. patterns X solves
11. Wittgensteins intuition might better captured matrix combines words modalities,
images (Monay & Gatica-Perez, 2003). values elements derived event
frequencies, would include VSM approach semantics.

148

fiFrom Frequency Meaning

solved X tend co-occur similar X : pairs, suggests
patterns similar meanings. Pattern similarity used infer one sentence
paraphrase another (Lin & Pantel, 2001).
Turney et al. (2003) introduced use pairpattern matrix measuring
semantic similarity relations word pairs; is, similarity row vectors.
example, pairs mason : stone, carpenter : wood, potter : clay, glassblower : glass
share semantic relation artisan : material. case, first member pair
artisan makes artifacts material second member pair.
pairs tend co-occur similar patterns, X used X
shaped into.
latent relation hypothesis pairs words co-occur similar patterns
tend similar semantic relations (Turney, 2008a). Word pairs similar row
vectors pairpattern matrix tend similar semantic relations. inverse
extended distributional hypothesis, patterns similar column vectors
pairpattern matrix tend similar meanings.
2.4 Similarities
Pairpattern matrices suited measuring similarity semantic relations
pairs words; is, relational similarity. contrast, wordcontext matrices suited
measuring attributional similarity. distinction attributional relational
similarity explored depth Gentner (1983).
attributional similarity two words b, sima (a, b) <, depends
degree correspondence properties b. correspondence
is, greater attributional similarity. relational similarity two pairs
words : b c : d, simr (a : b, c : d) <, depends degree correspondence
relations : b c : d. correspondence is, greater relational
similarity. example, dog wolf relatively high degree attributional similarity, whereas dog : bark cat : meow relatively high degree relational similarity
(Turney, 2006).
tempting suppose relational similarity reduced attributional
similarity. example, mason carpenter similar words stone wood
similar words; therefore, perhaps follows mason : stone carpenter : wood
similar relations. Perhaps simr (a : b, c : d) reduced sima (a, c) + sima (b, d). However,
mason, carpenter, potter, glassblower similar words (they artisans),
wood, clay, stone, glass (they materials used artisans), cannot infer
mason : glass carpenter : clay similar relations. Turney (2006, 2008a)
presented experimental evidence relational similarity reduce attributional
similarity.
term semantic relatedness computational linguistics (Budanitsky & Hirst, 2001)
corresponds attributional similarity cognitive science (Gentner, 1983). Two words
semantically related kind semantic relation (Budanitsky & Hirst,
2001); semantically related degree share attributes (Turney, 2006).
Examples synonyms (bank trust company), meronyms (car wheel), antonyms
(hot cold), words functionally related frequently associated (pencil
149

fiTurney & Pantel

paper). might usually think antonyms similar, antonyms high
degree attributional similarity (hot cold kinds temperature, black white
kinds colour, loud quiet kinds sound). prefer term attributional
similarity term semantic relatedness, attributional similarity emphasizes
contrast relational similarity, whereas semantic relatedness could confused
relational similarity.
computational linguistics, term semantic similarity applied words share
hypernym (car bicycle semantically similar, share hypernym
vehicle) (Resnik, 1995). Semantic similarity specific type attributional similarity.
prefer term taxonomical similarity term semantic similarity, term
semantic similarity misleading. Intuitively, attributional relational similarity
involve meaning, deserve called semantic similarity.
Words semantically associated tend co-occur frequently (e.g., bee
honey) (Chiarello, Burgess, Richards, & Pollock, 1990). Words may taxonomically similar semantically associated (doctor nurse), taxonomically similar semantically associated (horse platypus), semantically associated taxonomically similar
(cradle baby), neither semantically associated taxonomically similar (calculus
candy).
Schutze Pedersen (1993) defined two ways words distributed corpus text: two words tend neighbours other, syntagmatic
associates. two words similar neighbours, paradigmatic parallels. Syntagmatic associates often different parts speech, whereas paradigmatic parallels
usually part speech. Syntagmatic associates tend semantically associated (bee honey often neighbours); paradigmatic parallels tend taxonomically
similar (doctor nurse similar neighbours).
2.5 Semantic VSMs
possibilities exhausted termdocument, wordcontext, pairpattern
matrices. might want consider triplepattern matrices, measuring semantic
similarity word triples. Whereas pairpattern matrix might row mason :
stone column X works , triplepattern matrix could row mason :
stone : masonry column X uses build Z. However, n-tuples words grow
increasingly rare n increases. example, phrases contain mason, stone,
masonry together less frequent phrases contain mason stone together.
triplepattern matrix much sparse pairpattern matrix (ceteris paribus).
quantity text need, order enough numbers make matrices
useful, grows rapidly n increases. may better break n-tuples pairs.
example, : b : c could decomposed : b, : c, b : c (Turney, 2008a). similarity
two triples, : b : c : e : f , could estimated similarity corresponding
pairs. relatively dense pairpattern matrix could serve surrogate relatively
sparse triplepattern matrix.
may go beyond matrices. generalization matrix tensor (Kolda
& Bader, 2009; Acar & Yener, 2009). scalar (a single number) zeroth-order tensor,
vector first-order tensor, matrix second-order tensor. tensor order three
150

fiFrom Frequency Meaning

higher called higher-order tensor. Chew, Bader, Kolda, Abdelali (2007) use term
documentlanguage third-order tensor multilingual information retrieval. Turney (2007)
uses wordwordpattern tensor measure similarity words. Van de Cruys (2009) uses
verbsubjectobject tensor learn selectional preferences verbs.
Turneys (2007) tensor, example, rows correspond words TOEFL
multiple-choice synonym questions, columns correspond words Basic English (Ogden, 1930),12 tubes correspond patterns join rows columns (hence
wordwordpattern third-order tensor). given word TOEFL questions represented corresponding wordpattern matrix slice tensor. elements
slice correspond patterns relate given TOEFL word word
Basic English. similarity two TOEFL words calculated comparing two
corresponding matrix slices. algorithm achieves 83.8% TOEFL questions.
2.6 Types Tokens
token single instance symbol, whereas type general class tokens (Manning
et al., 2008). Consider following example (from Samuel Beckett):

Ever tried. Ever failed.
matter. Try again.
Fail again. Fail better.

two tokens type Ever, two tokens type again, two tokens
type Fail. Lets say line example document, three
documents two sentences each. represent example tokendocument
matrix typedocument matrix. tokendocument matrix twelve rows, one
token, three columns, one line (Figure 1). typedocument matrix
nine rows, one type, three columns (Figure 2).
row vector token binary values: element 1 given token appears
given document 0 otherwise. row vector type integer values: element
frequency given type given document. vectors related,
type vector sum corresponding token vectors. example, row vector
type Ever sum two token vectors two tokens Ever.
applications dealing polysemy, one approach uses vectors represent word
tokens (Schutze, 1998; Agirre & Edmonds, 2006) another uses vectors represent
word types (Pantel & Lin, 2002a). Typical word sense disambiguation (WSD) algorithms
deal word tokens (instances words specific contexts) rather word types.
mention approaches polysemy Section 6, due similarity close
relationship, although defining characteristic VSM concerned
frequencies (see Section 1.1), frequency property types, tokens.
12. Basic English highly reduced subset English, designed easy people learn. words
Basic English listed http://ogden.basic-english.org/.

151

fiTurney & Pantel

Ever tried.
Ever failed.
Ever
tried
Ever
failed

matter
Try

Fail

Fail
better

matter.
Try again.

1
1
1
1
0
0
0
0
0
0
0
0

0
0
0
0
1
1
1
1
0
0
0
0

Fail again.
Fail better.
0
0
0
0
0
0
0
0
1
1
1
1

Figure 1: tokendocument matrix. Rows tokens columns documents.

Ever tried.
Ever failed.
Ever
tried
failed

matter
Try

Fail
better

matter.
Try again.

2
1
1
0
0
0
0
0
0

0
0
0
1
1
1
1
0
0

Fail again.
Fail better.
0
0
0
0
0
0
1
2
1

Figure 2: typedocument matrix. Rows types columns documents.

152

fiFrom Frequency Meaning

2.7 Hypotheses
mentioned five hypotheses section. repeat hypotheses
interpret terms vectors. hypothesis, cite work explicitly
states something hypothesis implicitly assumes something hypothesis.
Statistical semantics hypothesis: Statistical patterns human word usage
used figure people mean (Weaver, 1955; Furnas et al., 1983). units text
similar vectors text frequency matrix,13 tend similar meanings.
(We take general hypothesis subsumes four specific hypotheses
follow.)
Bag words hypothesis: frequencies words document tend indicate
relevance document query (Salton et al., 1975). documents pseudodocuments (queries) similar column vectors termdocument matrix,
tend similar meanings.
Distributional hypothesis: Words occur similar contexts tend similar
meanings (Harris, 1954; Firth, 1957; Deerwester et al., 1990). words similar row
vectors wordcontext matrix, tend similar meanings.
Extended distributional hypothesis: Patterns co-occur similar pairs tend
similar meanings (Lin & Pantel, 2001). patterns similar column vectors
pairpattern matrix, tend express similar semantic relations.
Latent relation hypothesis: Pairs words co-occur similar patterns tend
similar semantic relations (Turney et al., 2003). word pairs similar row
vectors pairpattern matrix, tend similar semantic relations.
yet explained means say vectors similar. discuss
Section 4.4.

3. Linguistic Processing Vector Space Models
assume raw data large corpus natural language text.
generate termdocument, wordcontext, pairpattern matrix, useful apply
linguistic processing raw text. types processing used
grouped three classes. First, need tokenize raw text; is, need decide
constitutes term extract terms raw text. Second, may want
normalize raw text, convert superficially different strings characters
form (e.g., car, Car, cars, Cars could normalized car). Third, may want
annotate raw text, mark identical strings characters different (e.g., fly
verb could annotated fly/VB fly noun could annotated fly/NN).
Grefenstette (1994) presents good study linguistic processing wordcontext
VSMs. uses similar three-step decomposition linguistic processing: tokenization,
surface syntactic analysis, syntactic attribute extraction.
13. text frequency matrix, mean matrix higher-order tensor values elements
derived frequencies pieces text context pieces text collection
text. text frequency matrix intended general structure, includes termdocument,
wordcontext, pairpattern matrices special cases.

153

fiTurney & Pantel

3.1 Tokenization
Tokenization English seems simple first glance: words separated spaces.
assumption approximately true English, may work sufficiently well basic
VSM, advanced VSM requires sophisticated approach tokenization.
accurate English tokenizer must know handle punctuation (e.g., dont, Janes,
and/or), hyphenation (e.g., state-of-the-art versus state art), recognize multi-word
terms (e.g., Barack Obama ice hockey) (Manning et al., 2008). may wish
ignore stop words, high-frequency words relatively low information content,
function words (e.g., of, the, and) pronouns (e.g., them, who, that). popular list
stop words set 571 common words included source code SMART
system (Salton, 1971).14
languages (e.g., Chinese), words separated spaces. basic VSM
break text character unigrams bigrams. sophisticated approach
match input text entries lexicon, matching often
determine unique tokenization (Sproat & Emerson, 2003). Furthermore, native speakers
often disagree correct segmentation. Highly accurate tokenization challenging
task human languages.
3.2 Normalization
motivation normalization observation many different strings characters often convey essentially identical meanings. Given want get meaning
underlies words, seems reasonable normalize superficial variations converting form. common types normalization case folding
(converting words lower case) stemming (reducing inflected words stem
root form).
Case folding easy English, problematic languages. French,
accents optional uppercase, may difficult restore missing accents
converting words lowercase. words cannot distinguished without accents;
example, PECHE could either peche (meaning fishing peach) peche (meaning sin).
Even English, case folding cause problems, case sometimes semantic
significance. example, SMART information retrieval system, whereas smart
common adjective; Bush may surname, whereas bush kind plant.
Morphology study internal structure words. Often word composed
stem (root) added affixes (inflections), plural forms past tenses (e.g.,
trapped composed stem trap affix -ed). Stemming, kind morphological
analysis, process reducing inflected words stems. English, affixes
simpler regular many languages, stemming algorithms based
heuristics (rules thumb) work relatively well (Lovins, 1968; Porter, 1980; Minnen,
Carroll, & Pearce, 2001). agglutinative language (e.g., Inuktitut), many concepts
combined single word, using various prefixes, infixes, suffixes, morphological
analysis complicated. single word agglutinative language may correspond
sentence half dozen words English (Johnson & Martin, 2003).
14. source code available ftp://ftp.cs.cornell.edu/pub/smart/.

154

fiFrom Frequency Meaning

performance information retrieval system often measured precision
recall (Manning et al., 2008). precision system estimate conditional
probability document truly relevant query, system says relevant.
recall system estimate conditional probability system say
document relevant query, truly relevant.
general, normalization increases recall reduces precision (Kraaij & Pohlmann,
1996). natural, given nature normalization. remove superficial
variations believe irrelevant meaning, make easier recognize similarities; find similar things, recall increases. sometimes superficial
variations semantic significance; ignoring variations causes errors, precision
decreases. Normalization positive effect precision cases variant
tokens infrequent smoothing variations gives reliable statistics.
small corpus, may able afford overly selective, may
best aggressively normalize text, increase recall. large corpus,
precision may important, might want normalization. Hull (1996)
gives good analysis normalization information retrieval.
3.3 Annotation
Annotation inverse normalization. different strings characters may
meaning, happens identical strings characters may different
meanings, depending context. Common forms annotation include part-of-speech
tagging (marking words according parts speech), word sense tagging (marking
ambiguous words according intended meanings), parsing (analyzing grammatical structure sentences marking words sentences according
grammatical roles) (Manning & Schutze, 1999).
Since annotation inverse normalization, expect decrease recall
increase precision. example, tagging program noun verb, may
able selectively search documents act computer programming
(verb) instead documents discuss particular computer programs (noun); hence
increase precision. However, document computer programs (noun) may
something useful say act computer programming (verb), even document
never uses verb form program; hence may decrease recall.
Large gains IR performance recently reported result query annotation syntactic semantic information. Syntactic annotation includes query
segmentation (Tan & Peng, 2008) part speech tagging (Barr, Jones, & Regelson,
2008). Examples semantic annotation disambiguating abbreviations queries (Wei,
Peng, & Dumoulin, 2008) finding query keyword associations (Lavrenko & Croft, 2001;
Cao, Nie, & Bai, 2005).
Annotation useful measuring semantic similarity words concepts
(wordcontext matrices). example, Pantel Lin (2002a) presented algorithm
discover word senses clustering row vectors wordcontext matrix, using
contextual information derived parsing.
155

fiTurney & Pantel

4. Mathematical Processing Vector Space Models
text tokenized (optionally) normalized annotated, first step
generate matrix frequencies. Second, may want adjust weights
elements matrix, common words high frequencies, yet less
informative rare words. Third, may want smooth matrix, reduce
amount random noise fill zero elements sparse matrix. Fourth,
many different ways measure similarity two vectors.
Lowe (2001) gives good summary mathematical processing wordcontext VSMs.
decomposes VSM construction similar four-step process: calculate frequencies,
transform raw frequency counts, smooth space (dimensionality reduction),
calculate similarities.
4.1 Building Frequency Matrix
element frequency matrix corresponds event: certain item (term, word,
word pair) occurred certain situation (document, context, pattern) certain number
times (frequency). abstract level, building frequency matrix simple matter
counting events. practice, complicated corpus large.
typical approach building frequency matrix involves two steps. First, scan sequentially corpus, recording events frequencies hash table,
database, search engine index. Second, use resulting data structure generate
frequency matrix, sparse matrix representation (Gilbert, Moler, & Schreiber, 1992).
4.2 Weighting Elements
idea weighting give weight surprising events less weight expected
events. hypothesis surprising events, shared two vectors, discriminative similarity vectors less surprising events. example,
measuring semantic similarity words mouse rat, contexts dissect
exterminate discriminative similarity contexts like.
information theory, surprising event higher information content expected
event (Shannon, 1948). popular way formalize idea termdocument
matrices tf-idf (term frequency inverse document frequency) family weighting
functions (Sparck Jones, 1972). element gets high weight corresponding term
frequent corresponding document (i.e., tf high), term rare
documents corpus (i.e., df low, thus idf high). Salton Buckley (1988)
defined large family tf-idf weighting functions evaluated information retrieval tasks, demonstrating tf-idf weighting yield significant improvements
raw frequency.
Another kind weighting, often combined tf-idf weighting, length normalization
(Singhal, Salton, Mitra, & Buckley, 1996). information retrieval, document length
ignored, search engines tend bias favour longer documents. Length
normalization corrects bias.
Term weighting may used correct correlated terms. example,
terms hostage hostages tend correlated, yet may want normalize
156

fiFrom Frequency Meaning

term (as Section 3.2), slightly different meanings.
alternative normalizing them, may reduce weights co-occur
document (Church, 1995).
Feature selection may viewed form weighting, terms get
weight zero hence removed matrix. Forman (2003) provides good
study feature selection methods text classification.
alternative tf-idf Pointwise Mutual Information (PMI) (Church & Hanks, 1989;
Turney, 2001), works well wordcontext matrices (Pantel & Lin, 2002a)
termdocument matrices (Pantel & Lin, 2002b). variation PMI Positive PMI
(PPMI), PMI values less zero replaced zero (Niwa &
Nitta, 1994). Bullinaria Levy (2007) demonstrated PPMI performs better
wide variety weighting approaches measuring semantic similarity word
context matrices. Turney (2008a) applied PPMI pairpattern matrices. give
formal definition PPMI here, example effective weighting function.
Let F wordcontext frequency matrix nr rows nc columns. i-th row
F row vector fi: j-th column F column vector f:j . row fi:
corresponds word wi column f:j corresponds context cj . value
element fij number times wi occurs context cj . Let X matrix
results PPMI applied F. new matrix X number rows
columns raw frequency matrix F. value element xij X defined
follows:
fij
pij = Pnr Pnc

j=1 fij

i=1

(1)

Pnc

j=1 fij
pi = Pnr Pnc

(2)

Pnr
f
Pncij
= Pnr i=1

(3)

i=1

pj

i=1

j=1 fij
j=1 fij



pij
pmiij = log
pi pj

pmiij pmiij > 0
xij =
0 otherwise

(4)
(5)

definition, pij estimated probability word wi occurs context
cj , pi estimated probability word wi , pj estimated probability
context cj . wi cj statistically independent, pi pj = pij (by definition
independence), thus pmiij zero (since log(1) = 0). product pi pj
would expect pij wi occurs cj pure random chance. hand,
interesting semantic relation wi cj , expect pij larger
would wi cj indepedent; hence find pij > pi pj ,
thus pmiij positive. follows distributional hypothesis (see Section 2).
word wi unrelated context cj , may find pmiij negative. PPMI
designed give high value xij interesting semantic relation
157

fiTurney & Pantel

wi cj ; otherwise, xij value zero, indicating occurrence wi
cj uninformative.
well-known problem PMI biased towards infrequent events. Consider
case wi cj statistically dependent (i.e., maximum association).
pij = pi = pj . Hence (4) becomes log (1/pi ) PMI increases probability
word wi decreases. Several discounting factors proposed alleviate problem.
example follows (Pantel & Lin, 2002a):
P c
P r
fik )
fkj , nk=1
min ( nk=1
fij
Pnc
Pnr
ij =

fij + 1 min ( k=1 fkj , k=1 fik ) + 1
newpmiij = ij pmiij

(6)
(7)

Another way deal infrequent events Laplace smoothing probability
estimates, pij , pi , pj (Turney & Littman, 2003). constant positive value added
raw frequencies calculating probabilities; fij replaced fij + k,
k > 0. larger constant, greater smoothing effect. Laplace smoothing
pushes pmiij values towards zero. magnitude push (the difference
pmiij without Laplace smoothing) depends raw frequency fij .
frequency large, push small; frequency small, push large. Thus
Laplace smoothing reduces bias PMI towards infrequent events.
4.3 Smoothing Matrix
simplest way improve information retrieval performance limit number
vector components. Keeping components representing frequently occurring
content words way; however, common words, have, carry little
semantic discrimination power. Simple component smoothing heuristics, based properties weighting schemes presented Section 4.2, shown maintain
semantic discrimination power improve performance similarity computations.
Computing similarity pairs vectors, described Section 4.4,
computationally intensive task. However, vectors share non-zero coordinate
must compared (i.e., two vectors share coordinate dissimilar).
frequent context words, word the, unfortunately result vectors matching
non-zero coordinate. words precisely contexts little semantic
discrimination power. Consider pointwise mutual information weighting described
Section 4.2. Highly weighted dimensions co-occur frequently words
definition highly discriminating contexts (i.e., high association
words co-occur). keeping context-word dimensions
PMI conservative threshold setting others zero, Lin (1998) showed
number comparisons needed compare vectors greatly decreases losing
little precision similarity score top-200 similar words every word.
smoothing matrix, one computes reverse index non-zero coordinates.
Then, compare similarity words context vector words context
vectors, vectors found match non-zero component reverse index must
compared. Section 4.5 proposes optimizations along lines.
158

fiFrom Frequency Meaning

Deerwester et al. (1990) found elegant way improve similarity measurements
mathematical operation termdocument matrix, X, based linear algebra. operation truncated Singular Value Decomposition (SVD), called thin SVD. Deerwester
et al. briefly mentioned truncated SVD applied document similarity
word similarity, focus document similarity. Landauer Dumais (1997)
applied truncated SVD word similarity, achieving human-level scores multiple-choice
synonym questions Test English Foreign Language (TOEFL). Truncated
SVD applied document similarity called Latent Semantic Indexing (LSI),
called Latent Semantic Analysis (LSA) applied word similarity.
several ways thinking truncated SVD works. first
present math behind truncated SVD describe four ways looking it:
latent meaning, noise reduction, high-order co-occurrence, sparsity reduction.
SVD decomposes X product three matrices UVT , U V
column orthonormal form (i.e., columns orthogonal unit length, UT U =
VT V = I) diagonal matrix singular values (Golub & Van Loan, 1996). X
rank r, rank r. Let k , k < r, diagonal matrix formed
top k singular values, let Uk Vk matrices produced selecting
corresponding columns U V. matrix Uk k VkT matrix rank k
best approximates original matrix X, sense minimizes approximation
errors. is, X = Uk k VkT minimizes kX XkF matrices X rank k,
k . . . kF denotes Frobenius norm (Golub & Van Loan, 1996).
Latent meaning: Deerwester et al. (1990) Landauer Dumais (1997) describe
truncated SVD method discovering latent meaning. Suppose word
context matrix X. truncated SVD, X = Uk k VkT , creates low-dimensional linear
mapping row space (words) column space (contexts). low-dimensional
mapping captures latent (hidden) meaning words contexts. Limiting
number latent dimensions (k < r) forces greater correspondence words
contexts. forced correspondence words contexts improves similarity
measurement.
Noise reduction: Rapp (2003) describes truncated SVD noise reduction technique.
may think matrix X = Uk k VkT smoothed version original matrix X.
matrix Uk maps row space (the space spanned rows X) smaller
k-dimensional space matrix Vk maps column space (the space spanned
columns X) k-dimensional space. diagonal matrix k specifies
weights reduced k-dimensional space. singular values ranked
descending order amount variation X fit. think matrix
X composed mixture signal noise, signal noise,
Uk k VkT mostly captures variation X due signal, whereas remaining
vectors UVT mostly fitting variation X due noise.
High-order co-occurrence: Landauer Dumais (1997) describe truncated
SVD method discovering high-order co-occurrence. Direct co-occurrence (firstorder co-occurrence) two words appear identical contexts. Indirect co-occurrence
(high-order co-occurrence) two words appear similar contexts. Similarity
contexts may defined recursively terms lower-order co-occurrence. Lemaire
Denhiere (2006) demonstrate truncated SVD discover high-order co-occurrence.
159

fiTurney & Pantel

Sparsity reduction: general, matrix X sparse (mostly zeroes),
truncated SVD, X = Uk k VkT , dense. Sparsity may viewed problem insufficient
data: text, matrix X would fewer zeroes, VSM would perform
better chosen task. perspective, truncated SVD way simulating
missing text, compensating lack data (Vozalis & Margaritis, 2003).
different ways viewing truncated SVD compatible other;
possible perspectives correct. Future work likely provide
views SVD perhaps unifying view.
good C implementation SVD large sparse matrices Rohdes SVDLIBC.15
Another approach Brands (2006) incremental truncated SVD algorithm.16 Yet another
approach Gorrells (2006) Hebbian algorithm incremental truncated SVD. Brands
Gorrells algorithms introduce interesting new ways handling missing values,
instead treating zero values.
higher-order tensors, operations analogous truncated SVD,
parallel factor analysis (PARAFAC) (Harshman, 1970), canonical decomposition
(CANDECOMP) (Carroll & Chang, 1970) (equivalent PARAFAC discovered independently), Tucker decomposition (Tucker, 1966). overview tensor decompositions, see surveys Kolda Bader (2009) Acar Yener (2009). Turney (2007)
gives empirical evaluation well four different Tucker decomposition algorithms
scale large sparse third-order tensors. low-RAM algorithm, Multislice Projection,
large sparse tensors presented evaluated.17
Since work Deerwester et al. (1990), subsequent research discovered many
alternative matrix smoothing processes, Nonnegative Matrix Factorization (NMF)
(Lee & Seung, 1999), Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), Kernel Principal Components Analysis (KPCA) (Scholkopf,
Smola, & Muller, 1997), Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Discrete
Component Analysis (DCA) (Buntine & Jakulin, 2006).
four perspectives truncated SVD, presented above, apply equally well
recent matrix smoothing algorithms. newer smoothing algorithms tend
computationally intensive truncated SVD, attempt model
word frequencies better SVD. Truncated SVD implicitly assumes elements
X Gaussian distribution. Minimizing Frobenius norm kX XkF
minimize noise, noise Gaussian distribution. However, known
word frequencies Gaussian distributions. recent algorithms based
realistic models distribution word frequencies.18
4.4 Comparing Vectors
popular way measure similarity two frequency vectors (raw weighted)
take cosine. Let x two vectors, n elements.
15.
16.
17.
18.

SVDLIBC available http://tedlab.mit.edu/dr/svdlibc/.
MATLAB source code available http://web.mit.edu/wingated/www/resources.html.
MATLAB source code available http://www.apperceptual.com/multislice/.
experience, pmiij appears approximately Gaussian, may explain PMI works well
truncated SVD, PPMI puzzling, less Gaussian PMI, yet apparently
yields better semantic models PMI.

160

fiFrom Frequency Meaning

x = hx1 , x2 , . . . , xn

(8)

= hy1 , y2 , . . . , yn

(9)

cosine angle x calculated follows:
Pn
cos(x, y) = qP
n

yi
Pn

i=1 xi

2
2
i=1 xi
i=1 yi
xy
=

xx yy
x

=

kxk kyk

(10)
(11)
(12)

words, cosine angle two vectors inner product
vectors, normalized unit length. x frequency vectors
words, frequent word long vector rare word short vector,
yet words might synonyms. Cosine captures idea length vectors
irrelevant; important thing angle vectors.
cosine ranges 1 vectors point opposite directions ( 180
degrees) +1 point direction ( 0 degrees). vectors
orthogonal ( 90 degrees), cosine zero. raw frequency vectors,
necessarily cannot negative elements, cosine cannot negative, weighting
smoothing often introduce negative elements. PPMI weighting yield negative
elements, truncated SVD generate negative elements, even input matrix
negative values.
measure distance vectors easily converted measure similarity
inversion (13) subtraction (14).

sim(x, y) = 1/dist(x, y)

(13)

sim(x, y) = 1 dist(x, y)

(14)

Many similarity measures proposed IR (Jones & Furnas, 1987)
lexical semantics circles (Lin, 1998; Dagan, Lee, & Pereira, 1999; Lee, 1999; Weeds, Weir,
& McCarthy, 2004). commonly said IR that, properly normalized, difference
retrieval performance using different measures insignificant (van Rijsbergen, 1979).
Often vectors normalized way (e.g., unit length unit probability)
applying similarity measure.
Popular geometric measures vector distance include Euclidean distance Manhattan distance. Distance measures information theory include Hellinger, Bhattacharya,
Kullback-Leibler. Bullinaria Levy (2007) compared five distance measures
cosine similarity measure four different tasks involving word similarity. Overall,
best measure cosine. popular measures Dice Jaccard coefficients
(Manning et al., 2008).
161

fiTurney & Pantel

Lee (1999) proposed that, finding word similarities, measures focused
overlapping coordinates less importance negative features (i.e., coordinates
one word nonzero value zero value) appear perform
better. Lees experiments, Jaccard, Jensen-Shannon, L1 measures seemed
perform best. Weeds et al. (2004) studied linguistic statistical properties
similar words returned various similarity measures found measures
grouped three classes:
1. high-frequency sensitive measures (cosine, Jensen-Shannon, -skew, recall),
2. low-frequency sensitive measures (precision),
3. similar-frequency sensitive methods (Jaccard, Jaccard+MI, Lin, harmonic mean).
Given word w0 , use high-frequency sensitive measure score words wi
according similarity w0 , higher frequency words tend get higher scores
lower frequency words. use low-frequency sensitive measure,
bias towards lower frequency words. Similar-frequency sensitive methods prefer word wi
approximately frequency w0 . one experiment determining
compositionality collocations, high-frequency sensitive measures outperformed
classes (Weeds et al., 2004). believe determining appropriate similarity
measure inherently dependent similarity task, sparsity statistics,
frequency distribution elements compared, smoothing method applied
matrix.
4.5 Efficient Comparisons
Computing similarity rows (or columns) large matrix non-trivial
problem, worst case cubic running time O(n2r nc ), nr number rows
nc number columns (i.e., dimensionality feature space). Optimizations
parallelization often necessary.
4.5.1 Sparse-Matrix Multiplication
One optimization strategy generalized sparse-matrix multiplication approach (Sarawagi
& Kirpal, 2004), based observation scalar product two vectors
depends coordinates vectors nonzero values. Further,
observe commonly used similarity measures vectors x y, cosine,
overlap, Dice, decomposed three values: one depending nonzero
values x, another depending nonzero values y, third depending
nonzero coordinates shared x y. formally, commonly used similarity
scores, sim(x, y), expressed follows:
sim(x, y) = f0 (

Pn

i=1 f1 (xi , yi ), f2 (x), f3 (y))

(15)

example, cosine measure, cos(x, y), defined (10), expressed model
follows:
162

fiFrom Frequency Meaning

P
cos(x, y) = f0 ( ni=1 f1 (xi , yi ), f2 (x), f3 (y))

f0 (a, b, c) =
bc
f1 (a, b) = b
qP
n
2
f2 (a) = f3 (a) =
i=1 ai

(16)
(17)
(18)
(19)

Let X matrix want compute pairwise similarity, sim(x, y),
rows columns x y. Efficient computation similarity matrix
achieved leveraging fact sim(x, y) determined solely nonzero
coordinates shared x (i.e., f1 (0, xi ) = f1 (xi , 0) = 0 xi )
vectors sparse. case, calculating f1 (xi , yi ) required
vectors shared nonzero coordinate, significantly reducing cost computation.
Determining vectors share nonzero coodinate easily achieved first building
inverted index coordinates. indexing, precompute f2 (x)
f3 (y) without changing algorithm complexity. Then, vector x retrieve
constant time, index, vector shares nonzero coordinate x
weP
apply f1 (xi , yi ) shared coordinates i. computational cost algorithm
Ni2 Ni number vectors nonzero i-th coordinate. worst
case time complexity O(ncv) n number vectors compared, c
maximum number nonzero coordinates vector, v number vectors
nonzero i-th coordinate coordinate nonzero
vectors. words, algorithm efficient density coordinates
low. experiments computing semantic similarity pairs
words large web crawl, observed near linear average running time complexity n.
computational cost reduced leveraging element weighting
techniques described Section 4.2. setting zero coordinates low
PPMI, PMI tf-idf score, coordinate density dramatically reduced cost
losing little discriminative power. vein, Bayardo, Ma, Srikant (2007) described
strategy omits coordinates highest number nonzero values.
algorithm gives significant advantage interested finding solely
similarity highly similar vectors.
4.5.2 Distributed Implementation using MapReduce
algorithm described Section 4.5.1 assumes matrix X fit memory,
large X may impossible. Also, element X processed independently, running parallel processes non-intersecting subsets X makes processing
faster. Elsayed, Lin, Oard (2008) proposed MapReduce implementation deployed using Hadoop, open-source software package implementing MapReduce framework
distributed file system.19 Hadoop shown scale several thousands machines,
allowing users write simple code, seamlessly manage sophisticated parallel execution code. Dean Ghemawat (2008) provide good primer MapReduce
programming.
19. Hadoop available download http://lucene.apache.org/hadoop/.

163

fiTurney & Pantel

MapReduce models Map step used start n Map tasks parallel,
caching one m-th part X inverted index streaming one n-th part X
it. actual inputs read tasks directly HDFS (Hadoop Distributed
File System). value determined amount memory dedicated
inverted index, n determined trading fact that, n increases,
parallelism obtained increased cost building inverted index
n times.
similarity algorithm Section 4.5.1 runs task Map step
MapReduce job. Reduce step groups output rows (or columns) X.
4.5.3 Randomized Algorithms
optimization strategies use randomized techniques approximate various similarity measures. aim randomized algorithms improve computational efficiency
(memory time) projecting high-dimensional vectors low-dimensional subspace.
Truncated SVD performs projection, SVD computationally intensive.20
insight randomized techniques high-dimensional vectors randomly projected low-dimensional subspace relatively little impact final similarity
scores. Significant reductions computational cost reported little average error computing true similarity scores, especially applications word
similarity interested top-k similar vectors vector
(Ravichandran, Pantel, & Hovy, 2005; Gorman & Curran, 2006).
Random Indexing, approximation technique based Sparse Distributed Memory
(Kanerva, 1993), computes pairwise similarity rows (or vectors) matrix
complexity O(nr nc 1 ), 1 fixed constant representing length index
vectors assigned column. value 1 controls tradeoff accuracy versus
efficiency. elements index vector mostly zeros small number
randomly assigned +1s 1s. cosine measure two rows r1 r2
approximated computing cosine two fingerprint vectors, fingerprint(r1 )
fingerprint(r2 ), fingerprint(r) computed summing index vectors
non-unique coordinate r. Random Indexing shown perform well LSA
word synonym selection task (Karlgren & Sahlgren, 2001).
Locality sensitive hashing (LSH) (Broder, 1997) another technique approximates
similarity matrix complexity O(n2r 2 ), 2 constant number random
projections, controls accuracy versus efficiency tradeoff.21 LSH general class
techniques defining functions map vectors (rows columns) short signatures
fingerprints, two similar vectors likely similar fingerprints. Definitions
LSH functions include Min-wise independent function, preserves Jaccard
similarity vectors (Broder, 1997), functions preserve cosine similarity
vectors (Charikar, 2002). word similarity task, Ravichandran et al. (2005)
showed that, average, 80% top-10 similar words random words found
top-10 results using Charikars functions, average cosine error 0.016
20. However, efficient forms SVD (Brand, 2006; Gorrell, 2006).
21. LSH stems work Rabin (1981), proposed use hash functions random irreducible
polynomials create short fingerprints collections documents. techniques useful many
tasks, removing duplicate documents (deduping) web crawl.

164

fiFrom Frequency Meaning

(using 2 = 10,000 random projections). Gorman Curran (2006) provide detailed
comparison Random Indexing LSH distributional similarity task. BNC
corpus, LSH outperformed Random Indexing; however, larger corpora combining BNC,
Reuters Corpus, English news holdings LDC 2003, Random
Indexing outperformed LSH efficiency accuracy.
4.6 Machine Learning
intended application VSM clustering classification, similarity measure
cosine (Section 4.4) may used. classification, nearest-neighbour algorithm
use cosine measure nearness (Dasarathy, 1991). clustering, similaritybased clustering algorithm use cosine measure similarity (Jain, Murty, & Flynn,
1999). However, many machine learning algorithms work directly
vectors VSM, without requiring external similarity measure, cosine.
effect, machine learning algorithms implicitly use internal approaches
measuring similarity.
machine learning algorithm works real-valued vectors use vectors
VSM (Witten & Frank, 2005). Linguistic processing (Section 3) mathematical
processing (Section 4) may still necessary, machine learning algorithm handle
vector comparison (Sections 4.4 4.5).
addition unsupervised (clustering) supervised (classification) machine learning, vectors VSM may used semi-supervised learning (Ando & Zhang,
2005; Collobert & Weston, 2008). general, nothing unique VSMs would
compel choice one machine learning algorithm another, aside algorithms
performance given task. Therefore refer readers machine learning
literature (Witten & Frank, 2005), since advice specific VSMs.

5. Three Open Source VSM Systems
illustrate three types VSMs discussed Section 2, section presents three open
source systems, one VSM type. chosen present open source systems
interested readers obtain source code find systems
apply systems projects. three systems written Java
designed portability ease use.
5.1 TermDocument Matrix: Lucene
Lucene22 open source full-featured text search engine library supported Apache
Software Foundation. arguably ubiquitous implementation termdocument
matrix, powering many search engines CNET, SourceForge, Wikipedia, Disney,
AOL Comcast. Lucene offers efficient storage, indexing, well retrieval ranking
functionalities. Although primarily used termdocument matrix, generalizes
VSMs.
22. Apache Lucene available download http://lucene.apache.org/.

165

fiTurney & Pantel

Content, webpages, PDF documents, images, video, programmatically
decomposed fields stored database. database implements term
document matrix, content corresponds documents fields correspond terms.
Fields stored database indices computed field values. Lucene
uses fields generalization content terms, allowing string literal index
documents. example, webpage could indexed terms contains,
anchor texts pointing it, host name, semantic classes
classified (e.g., spam, product review, news, etc.). webpage retrieved
search terms matching fields.
Columns termdocument matrix consist fields particular instance
content (e.g., webpage). rows consist instances content index.
Various statistics frequency tf-idf stored matrix. developer
defines fields schema identifies indexed Lucene. developer
optionally defines content ranking function indexed field.
index built, Lucene offers functionalities retrieving content. Users
issue many query types phrase queries, wildcard queries, proximity queries, range
queries (e.g., date range queries), field-restricted queries. Results sorted
field index updates occur simultaneously searching. Lucenes index
directly loaded Tomcat webserver offers APIs common programming languages. Solr,23 separate Apache Software Foundation project, open source enterprise
webserver searching Lucene index presenting search results. full-featured
webserver providing functionalities XML/HTTP JSON APIs, hit highlighting,
faceted search, caching, replication.
simple recipe creating web search service, using Nutch, Lucene Solr, consists
crawling set URLs (using Nutch), creating termdocument matrix index (using
Lucene), serving search results (using Solr). Nutch,24 Apache Software Foundation
open source web search software, offers functionality crawling web seed set
URLs, building link-graph web crawl, parsing web documents
HTML pages. good set seed URLs Nutch downloaded freely
Open Directory Project.25 Crawled pages HTML-parsed, indexed
Lucene. resulting indexed collection queried served Solr
installation Tomcat.
information Lucene, recommend Gospodnetic Hatchers (2004)
book. Konchady (2008) explains integrate Lucene LingPipe GATE
sophisticated semantic processing.26

23.
24.
25.
26.

Apache Solr available download http://lucene.apache.org/solr/.
Apache Nutch available download http://lucene.apache.org/nutch/.
See http://www.dmoz.org/.
Information LingPipe available http://alias-i.com/lingpipe/. GATE (General Architecture Text Engineering) home page http://gate.ac.uk/.

166

fiFrom Frequency Meaning

5.2 WordContext Matrix: Semantic Vectors
Semantic Vectors27 open source project implementing random projection approach
measuring word similarity (see Section 4.5.3). package uses Lucene create term
document matrix, creates vectors Lucenes termdocument matrix, using
random projection dimensionality reduction. random projection vectors
used, example, measure semantic similarity two words find words
similar given word.
idea random projection take high-dimensional vectors randomly project
relatively low-dimensional space (Sahlgren, 2005). viewed
kind smoothing operation (Section 4.3), developers Semantic Vectors
package emphasize simplicity efficiency random projection (Section 4.5), rather
smoothing ability. argue matrix smoothing algorithms might
smooth better, none perform well random indexing, terms
computational complexity building smooth matrix incrementally updating
matrix new data arrives (Widdows & Ferraro, 2008). aim encourage
research development semantic vectors creating simple efficient open
source package.
Semantic Vectors package designed convenient use, portable, easy
extend modify. design software incorporates lessons learned
earlier Stanford Infomap project.28 Although default generate random projection
vectors, system modular design allows kinds wordcontext matrices
used instead random projection matrices.
package supports two basic functions: building wordcontext matrix searching
vectors matrix. addition generating word vectors, building
operation generate document vectors calculating weighted sums word vectors
words document. searching operation used search similar
words search documents similar query. query single word
several words combined, using various mathematical operations corresponding
vectors. mathematical operations include vector negation disjunction, based
quantum logic (Widdows, 2004). Widdows Ferraro (2008) provide good summary
Semantic Vectors software.
5.3 PairPattern Matrix: Latent Relational Analysis S-Space
Latent Relational Analysis29 (LRA) open source project implementing pairpattern
matrix. component S-Space package, library tools building
comparing different semantic spaces.
LRA takes input textual corpus set word pairs. pairpattern matrix
built deriving lexical patterns link together word pairs corpus. example, consider word pair hKorea, Japani following retrieved matching sentences:
27. Semantic Vectors software package measuring word similarity, available Simplified BSD
License http://code.google.com/p/semanticvectors/.
28. See http://infomap-nlp.sourceforge.net/.
29. Latent Relational Analysis part S-Space package distributed GNU General
Public License version 2. available http://code.google.com/p/airhead-research/. time
writing, LRA module development.

167

fiTurney & Pantel

Korea looks new Japan prime ministers effect Korea-Japan relations.
channel Korea vs. Japan football game?
two sentences, LRA extracts two patterns: X looks new X vs. .
patterns become two columns pairpattern matrix, word pair hKorea,
Japani becomes row. Pattern frequencies counted smoothed using SVD (see
Section 4.3).
order mitigate sparseness occurrences word pairs, thesaurus
WordNet used expand seed word pairs alternatives. example pair
hKorea, Japani may expanded include hSouth Korea, Japani, hRepublic Korea,
Japani, hKorea, Nipponi, hSouth Korea, Nipponi, hRepublic Korea, Nipponi.
LRA uses Lucene (see Section 5.1) backend store matrix, index it, serve
contents. detailed description LRA algorithm, suggest Turneys (2006)
paper.

6. Applications
section, survey semantic applications VSMs. aim
breadth, rather depth; readers want depth consult references.
goal give reader impression scope flexibility VSMs semantics.
following applications grouped according type matrix involved: term
document, wordcontext, pairpattern. Note section exhaustive;
many references applications space list here.
6.1 TermDocument Matrices
Termdocument matrices suited measuring semantic similarity documents
queries (see Section 2.1). usual measure similarity cosine column vectors
weighted termdocument matrix. variety applications measures
document similarity.
Document retrieval: termdocument matrix first developed document
retrieval (Salton et al., 1975), large body literature VSM
document retrieval (Manning et al., 2008), including several journals conferences
devoted topic. core idea is, given query, rank documents order
decreasing cosine angles query vector document vectors (Salton
et al., 1975). One variation theme cross-lingual document retrieval,
query one language used retrieve document another language (Landauer &
Littman, 1990). important technical advance discovery smoothing
termdocument matrix truncated SVD improve precision recall (Deerwester
et al., 1990), although commercial systems use smoothing, due computational
expense document collection large dynamic. Random indexing (Sahlgren,
2005) incremental SVD (Brand, 2006) may help address scaling issues. Another
important development document retrieval addition collaborative filtering,
form PageRank (Brin & Page, 1998).
Document clustering: Given measure document similarity, cluster
documents groups, similarity tends high within group, low across
168

fiFrom Frequency Meaning

groups (Manning et al., 2008). clusters may partitional (flat) (Cutting, Karger,
Pedersen, & Tukey, 1992; Pantel & Lin, 2002b) may hierarchical structure
(groups groups) (Zhao & Karypis, 2002); may non-overlapping (hard) (Croft,
1977) overlapping (soft) (Zamir & Etzioni, 1999). Clustering algorithms differ
clusters compared abstracted. single-link clustering, similarity
two clusters maximum similarities members. Complete-link
clustering uses minimum similarities average-link clustering uses average
similarities (Manning et al., 2008).
Document classification: Given training set documents class labels
testing set unlabeled documents, task document classification learn
training set assign labels testing set (Manning et al., 2008). labels may
topics documents (Sebastiani, 2002), sentiment documents (e.g.,
positive versus negative product reviews) (Pang, Lee, & Vaithyanathan, 2002; Kim, Pantel,
Chklovski, & Pennacchiotti, 2006), spam versus non-spam (Sahami, Dumais, Heckerman, &
Horvitz, 1998; Pantel & Lin, 1998), labels might inferred words
documents. classify documents, implying documents
class similar way; thus document classification implies notion document
similarity, machine learning approaches document classification involve term
document matrix (Sebastiani, 2002). measure document similarity, cosine,
directly applied document classification using nearest-neighbour algorithm (Yang,
1999).
Essay grading: Student essays may automatically graded comparing
one high-quality reference essays given essay topic (Wolfe, Schreiner, Rehder,
Laham, Foltz, Kintsch, & Landauer, 1998; Foltz, Laham, & Landauer, 1999). student
essays reference essays compared cosines termdocument matrix.
grade assigned student essay proportional similarity one
reference essays; student essay highly similar reference essay gets high grade.
Document segmentation: task document segmentation partition document sections, section focuses different subtopic document
(Hearst, 1997; Choi, 2000). may treat document series blocks, block
sentence paragraph. problem detect topic shift one block
next. Hearst (1997) Choi (2000) use cosine columns wordblock
frequency matrix measure semantic similarity blocks. topic shift signaled
drop cosine consecutive blocks. wordblock matrix viewed
small termdocument matrix, corpus single document documents
blocks.
Question answering: Given simple question, task Question Answering (QA)
find short answer question searching large corpus. typical question is, many calories Big Mac? algorithms QA four
components, question analysis, document retrieval, passage retrieval, answer extraction
(Tellex, Katz, Lin, Fern, & Marton, 2003; Dang, Lin, & Kelly, 2006). Vector-based similarity measurements often used document retrieval passage retrieval (Tellex
et al., 2003).
Call routing: Chu-carroll Carpenter (1999) present vector-based system
automatically routing telephone calls, based callers spoken answer question,
169

fiTurney & Pantel

may direct call? callers answer ambiguous, system automatically
generates question caller, derived VSM, prompts caller
information.
6.2 WordContext Matrices
Wordcontext matrices suited measuring semantic similarity words (see
Section 2.2). example, measure similarity two words cosine
angle corresponding row vectors wordcontext matrix. many
applications measures word similarity.
Word similarity: Deerwester et al. (1990) discovered measure word similarity comparing row vectors termdocument matrix. Landauer Dumais (1997)
evaluated approach 80 multiple-choice synonym questions Test English Foreign Language (TOEFL), achieving human-level performance (64.4% correct
wordcontext matrix 64.5% average non-English US college applicant).
documents used Landauer Dumais average length 151 words,
seems short document, long context word. researchers soon
switched much shorter lengths, prefer call wordcontext matrices, instead termdocument matrices. Lund Burgess (1996) used context window
ten words. Schutze (1998) used fifty-word window (25 words, centered target
word). Rapp (2003) achieved 92.5% correct 80 TOEFL questions, using four-word
context window (2 words, centered target word, removing stop words).
TOEFL results suggest performance improves context window shrinks. seems
immediate context word much important distant context
determining meaning word.
Word clustering: Pereira, Tishby, Lee (1993) applied soft hierarchical clustering
row-vectors wordcontext matrix. one experiment, words nouns
contexts verbs given nouns direct objects. another experiment,
words verbs contexts nouns direct objects given
verbs. Schutzes (1998) seminal word sense discrimination model used hard flat clustering
row-vectors wordcontext matrix, context given window 25
words, centered target word. Pantel Lin (2002a) applied soft flat clustering
wordcontext matrix, context based parsed text. algorithms
able discover different senses polysemous words, generating different clusters
sense. effect, different clusters correspond different concepts underlie
words.
Word classification: Turney Littman (2003) used wordcontext matrix classify words positive (honest, intrepid) negative (disturbing, superfluous). used
General Inquirer (GI) lexicon (Stone, Dunphy, Smith, & Ogilvie, 1966) evaluate
algorithms. GI lexicon includes 11,788 words, labeled 182 categories related
opinion, affect, attitude.30 Turney Littman hypothesize 182 categories
discriminated wordcontext matrix.
Automatic thesaurus generation: WordNet popular tool research natural
language processing (Fellbaum, 1998), creating maintaing lexical resources
30. GI lexicon available http://www.wjh.harvard.edu/inquirer/spreadsheet guide.htm.

170

fiFrom Frequency Meaning

labour intensive, natural wonder whether process automated
degree.31 task seen instance word clustering (when thesaurus
generated scratch) classification (when existing thesaurus automatically
extended), worthwhile consider task automatic thesaurus generation
separately clustering classification, due specific requirements thesaurus,
particular kind similarity appropriate thesaurus (see Section 2.4).
Several researchers used wordcontext matrices specifically task assisting
automating thesaurus generation (Crouch, 1988; Grefenstette, 1994; Ruge, 1997; Pantel &
Lin, 2002a; Curran & Moens, 2002).
Word sense disambiguation: typical Word Sense Disambiguation (WSD) system
(Agirre & Edmonds, 2006; Pedersen, 2006) uses feature vector representation
vector corresponds token word, type (see Section 2.6). However, Leacock,
Towell, Voorhees (1993) used wordcontext frequency matrix WSD,
vector corresponds type annotated sense tag. Yuret Yatbaz (2009) applied
wordcontext frequency matrix unsupervised WSD, achieving results comparable
performance supervised WSD systems.
Context-sensitive spelling correction: People frequently confuse certain sets
words, there, theyre, their. confusions cannot detected simple dictionary-based spelling checker; require context-sensitive spelling correction.
wordcontext frequency matrix may used correct kinds spelling errors (Jones
& Martin, 1997).
Semantic role labeling: task semantic role labeling label parts sentence according roles play sentence, usually terms connection
main verb sentence. Erk (2007) presented system wordcontext
frequency matrix used improve performance semantic role labeling. Pennacchiotti, Cao, Basili, Croce, Roth (2008) show wordcontext matrices reliably
predict semantic frame unknown lexical unit refers, good levels
accuracy. lexical unit induction important semantic role labeling, narrow
candidate set roles observed lexical unit.
Query expansion: Queries submitted search engines Google Yahoo!
often directly match terms relevant documents. alleviate
problem, process query expansion used generating new search terms
consistent intent original query. VSMs form basis query semantics
models (Cao, Jiang, Pei, He, Liao, Chen, & Li, 2008). methods represent queries
using session contexts, query cooccurrences user sessions (Huang, Chien, &
Oyang, 2003; Jones, Rey, Madani, & Greiner, 2006), others use click contexts,
urls clicked result query (Wen, Nie, & Zhang, 2001).
Textual advertising: pay-per-click advertising models, prevalent search engines
Google Yahoo!, users pay keywords, called bidterms, used
display ads relevant queries issued users. scarcity data makes ad
matching difficult and, response, several techniques bidterm expansion using VSMs
proposed. wordcontext matrix consists rows bidterms columns
31. WordNet available http://wordnet.princeton.edu/.

171

fiTurney & Pantel

(contexts) consist advertiser identifiers (Gleich & Zhukov, 2004) co-bidded bidterms
(second order co-occurrences) (Chang, Pantel, Popescu, & Gabrilovich, 2009).
Information extraction: field information extraction (IE) includes named
entity recognition (NER: recognizing chunk text name entity,
person place), relation extraction, event extraction, fact extraction. Pasca et
al. (2006) demonstrate wordcontext frequency matrix facilitate fact extraction.
Vyas Pantel (2009) propose semi-supervised model using wordcontext matrix
building iteratively refining arbitrary classes named entities.
6.3 PairPattern Matrices
Pairpattern matrices suited measuring semantic similarity word pairs
patterns (see Section 2.3). example, measure similarity two word
pairs cosine angle corresponding row vectors pairpattern
matrix. many applications measures relational similarity.
Relational similarity: measure attributional similarity cosine
angle row vectors wordcontext matrix, measure relational
similarity cosine angle rows pairpattern matrix. approach
measuring relational similarity introduced Turney et al. (2003) examined
detail Turney Littman (2005). Turney (2006) evaluated approach
relational similarity 374 multiple-choice analogy questions SAT college
entrance test, achieving human-level performance (56% correct pairpattern matrix
57% correct average US college applicant). highest performance
far algorithm. best algorithm based attributional similarity accuracy
35% (Turney, 2006). best non-VSM algorithm achieves 43% (Veale, 2004).
Pattern similarity: Instead measuring similarity row vectors pair
pattern matrix, measure similarity columns; is, measure
pattern similarity. Lin Pantel (2001) constructed pairpattern matrix
patterns derived parsed text. Pattern similarity used infer one
phrase paraphrase another phrase, useful natural language generation,
text summarization, information retrieval, question answering.
Relational clustering: Bicici Yuret (2006) clustered word pairs representing
row vectors pairpattern matrix. Davidov Rappoport (2008) first clustered
contexts (patterns) identified representative pairs context cluster.
used representative pairs automatically generate multiple-choice analogy questions,
style SAT analogy questions.
Relational classification: Chklovski Pantel (2004) used pairpattern matrix
classify pairs verbs semantic classes. example, taint : poison classified
strength (poisoning stronger tainting) assess : review classified enablement
(assessing enabled reviewing). Turney (2005) used pairpattern matrix classify
noun compounds semantic classes. example, flu virus classified cause (the
virus causes flu), home town classified location (the home located town),
weather report classified topic (the topic report weather).
Relational search: Cafarella, Banko, Etzioni (2006) described relational search
task searching entities satisfy given semantic relations. example
172

fiFrom Frequency Meaning

query relational search engine list X X causes cancer.
example, relation, cause, one terms relation, cancer, given
user, task search engine find terms satisfy users query.
organizers Task 4 SemEval 2007 (Girju, Nakov, Nastase, Szpakowicz, Turney, & Yuret,
2007) envisioned two-step approach relational search: first conventional search engine
would look candidate answers, relational classification system would filter
incorrect answers. first step manually simulated Task 4 organizers
goal Task 4 design systems second step. task attracted 14 teams
submitted 15 systems. Nakov Hearst (2007) achieved good results using pairpattern
matrix.
Automatic thesaurus generation: discussed automatic thesaurus generation
Section 6.2, wordcontext matrices, arguably relational similarity relevant
attributional similarity thesaurus generation. example, information WordNet relations words rather words individually.
Snow, Jurafsky, Ng (2006) used pairpattern matrix build hypernym-hyponym
taxonomy, whereas Pennacchiotti Pantel (2006) built meronymy causation taxonomy. Turney (2008b) showed pairpattern matrix distinguish synonyms
antonyms, synonyms non-synonyms, taxonomically similar words (hair fur)
words merely semantically associated (cradle baby).
Analogical mapping: Proportional analogies form : b :: c : d, means
b c d. example, mason : stone :: carpenter : wood means mason stone
carpenter wood. 374 multiple-choice analogy questions SAT college
entrance test (mentioned above) involve proportional analogies. pairpattern
matrix, solve proportional analogies selecting choice maximizes relational
similarity (e.g., simr (mason : stone, carpenter : wood) high value). However, often
encounter analogies involve four terms. well-known analogy
solar system Rutherford-Bohr model atom contains least fourteen
terms. solar system, planet, attracts, revolves, sun, gravity, solar system,
mass. atom, revolves, atom, attracts, electromagnetism, nucleus,
charge, electron. Turney (2008a) demonstrated handle complex,
systematic analogies decomposing sets proportional analogies.

7. Alternative Approaches Semantics
applications list Section 6 necessarily require VSM approach.
application, many possible approaches. section, briefly
consider main alternatives.
Underlying applications termdocument matrices (Section 6.1) task
measuring semantic similarity documents queries. main alternatives
VSMs task probabilistic models, traditional probabilistic retrieval
models information retrieval (van Rijsbergen, 1979; Baeza-Yates & Ribeiro-Neto, 1999)
recent statistical language models inspired information theory (Liu &
Croft, 2005). idea statistical language models information retrieval measure
similarity query document creating probabilistic language model
173

fiTurney & Pantel

given document measuring probability given query according
language model.
progress information retrieval, distinction VSM approach
probabilistic approach becoming blurred, approach borrows ideas
other. Language models typically involve multiplying probabilities, view
adding logs probabilities, makes language models look similar VSMs.
applications wordcontext matrices (Section 6.2) share task measuring
semantic similarity words. main alternatives VSMs measuring word similarity
approaches use lexicons, WordNet (Resnik, 1995; Jiang & Conrath, 1997;
Hirst & St-Onge, 1998; Leacock & Chodrow, 1998; Budanitsky & Hirst, 2001). idea
view lexicon graph, nodes correspond word senses edges represent
relations words, hypernymy hyponymy. similarity two
words proportional length path graph joins two words.
Several approaches measuring semantic similarity words combine VSM
lexicon (Turney et al., 2003; Pantel, 2005; Patwardhan & Pedersen, 2006; Mohammad &
Hirst, 2006). Humans use dictionary definitions observations word usage,
natural expect best performance algorithms use distributional
lexical information.
Pairpattern matrices (Section 6.3) common task measuring semantic
similarity relations. wordcontext matrices, main alternatives approaches
use lexicons (Rosario & Hearst, 2001; Rosario, Hearst, & Fillmore, 2002; Nastase
& Szpakowicz, 2003; Veale, 2003, 2004). idea reduce relational similarity
attributional similarity, simr (a : b, c : d) sima (a, c) + sima (b, d), use lexicon
measure attributional similarity. discuss Section 2.4, reduction
work general. However, reduction often good approximation,
evidence hybrid approach, combining VSM lexicon, beneficial (Turney
et al., 2003; Nastase, Sayyad-Shirabad, Sokolova, & Szpakowicz, 2006).

8. Future Vector Space Models Semantics
Several authors criticized VSMs (French & Labiouse, 2002; Pado & Lapata, 2003;
Morris & Hirst, 2004; Budanitsky & Hirst, 2006). criticism stems
fact termdocument wordcontext matrices typically ignore word order. LSA,
instance, phrase commonly represented sum vectors individual
words phrase; hence phrases house boat boat house represented
vector, although different meanings. English, word order expresses
relational information. house boat boat house Tool-Purpose relation,
house boat means Tool-Purpose(boat, house) (a boat serves house), whereas boat
house means Tool-Purpose(house, boat) (a house sheltering storing boats).
Landauer (2002) estimates 80% meaning English text comes word
choice remaining 20% comes word order. However, VSMs inherently
limited 80% meaning text. Mitchell Lapata (2008) propose composition
models sensitive word order. example, make simple additive model become
syntax-aware, allow different weightings contributions vector components. Constituents important composition therefore participate
174

fiFrom Frequency Meaning

actively. Clark Pulman (2007) assigned distributional meaning sentences using Hilbert space tensor product. Widdows Ferraro (2008), inspired quantum
mechanics, explores several operators modeling composition meaning. Pairpattern
matrices sensitive order words pair (Turney, 2006). Thus
several ways handle word order VSMs.
raises question, limits VSMs semantics? semantics
represented VSMs? much yet know represent
VSMs. example, Widdows (2004) van Rijsbergen (2004) show disjunction,
conjunction, negation represented vectors, yet know
represent arbitrary statements first-order predicate calculus. However, seems possible
future work may discover answers limitations.
survey, assumed VSMs composed elements values
derived event frequencies. ties VSMs form distributional hypothesis
(see Sections 1.1 2.7); therefore limits VSMs depend limits family
distributional hypotheses. statistical patterns word usage sufficient figure
people mean? arguably major open question VSMs, answer
determine future VSMs. strong argument one way other,
believe continuing progress VSMs suggests far reaching
limits.

9. Conclusions
want information help person, use words make request
describe problem, person replies words. Unfortunately, computers
understand human language, forced use artificial languages unnatural user
interfaces. science fiction, dream computers understand human language,
listen us talk us. achieve full potential computers, must enable
understand semantics natural language. VSMs likely part
solution problem computing semantics.
Many researchers struggled problem semantics come
conclusion meaning words closely connected statistics word usage
(Section 2.7). try make intuition precise, soon find working
vectors values derived event frequencies; is, dealing VSMs.
survey, organized past work VSMs according structure
matrix (termdocument, wordcontext, pairpattern). believe structure
matrix important factor determining types applications
possible. linguistic processing (Section 3) mathematical processing (Section 4)
play smaller (but important) roles.
goal survey show breadth power VSMs, introduce
VSMs less familiar them, provide new perspective VSMs
already familiar them. hope emphasis structure
matrix inspire new research. reason believe three matrix
types present exhaust possibilities. expect new matrix types new tensors
open applications VSMs. seems possible us semantics
human language might one day captured kind VSM.
175

fiTurney & Pantel

Acknowledgments
Thanks Annie Zaenen prompting paper. Thanks Saif Mohammad Mariana
Soffer comments. Thanks Arkady Borkovsky Eric Crestan developing
distributed sparse-matrix multiplication algorithm, Marco Pennacchiotti
invaluable comments. Thanks anonymous reviewers JAIR helpful
comments suggestions.

References
Acar, E., & Yener, B. (2009). Unsupervised multiway data analysis: literature survey.
IEEE Transactions Knowledge Data Engineering, 21 (1), 620.
Agirre, E., & Edmonds, P. G. (2006). Word Sense Disambiguation: Algorithms Applications. Springer.
Ando, R. K. (2000). Latent semantic space: Iterative scaling improves precision interdocument similarity measurement. Proceedings 23rd Annual ACM SIGIR
Conference Research Development Information Retrieval (SIGIR-2000), pp.
216223.
Ando, R. K., & Zhang, T. (2005). framework learning predictive structures
multiple tasks unlabeled data. Journal Machine Learning Research, 6, 1817
1853.
Baeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison Wesley.
Barr, C., Jones, R., & Regelson, M. (2008). linguistic structure English websearch queries. Conference Empirical Methods Natural Language Processing
(EMNLP).
Bayardo, R. J., Ma, Y., & Srikant, R. (2007). Scaling pairs similarity search.
Proceedings 16th international conference World Wide Web (WWW 07),
pp. 131140, New York, NY. ACM.
Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions.
Proceedings Fifteenth Turkish Symposium Artificial Intelligence Neural
Networks (TAINN 2006), Akyaka, Mugla, Turkey.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal
Machine Learning Research, 3, 9931022.
Brand, M. (2006). Fast low-rank modifications thin singular value decomposition.
Linear Algebra Applications, 415 (1), 2030.
Breese, J., Heckerman, D., & Kadie, C. (1998). Empirical analysis predictive algorithms
collaborative filtering. Proceedings 14th Conference Uncertainty
Artificial Intelligence, pp. 4352. Morgan Kaufmann.
Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine.
Proceedings Seventh World Wide Web Conference (WWW7), pp. 107117.
Broder, A. (1997). resemblance containment documents. Compression
Complexity Sequences (SEQUENCES97), pp. 2129. IEEE Computer Society.
176

fiFrom Frequency Meaning

Budanitsky, A., & Hirst, G. (2001). Semantic distance WordNet: experimental,
application-oriented evaluation five measures. Proceedings Workshop
WordNet Lexical Resources, Second Meeting North American
Chapter Association Computational Linguistics (NAACL-2001), pp. 2924,
Pittsburgh, PA.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures semantic distance. Computational Linguistics, 32 (1), 1347.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),
510526.
Buntine, W., & Jakulin, A. (2006). Discrete component analysis. Subspace, Latent
Structure Feature Selection: Statistical Optimization Perspectives Workshop
SLSFS 2005, pp. 133, Bohinj, Slovenia. Springer.
Cafarella, M. J., Banko, M., & Etzioni, O. (2006). Relational web search. Tech. rep., University Washington, Department Computer Science Engineering. Technical
Report 2006-04-02.
Cao, G., Nie, J.-Y., & Bai, J. (2005). Integrating word relationships language models.
Proceedings 28th Annual International ACM SIGIR Conference Research
Development Information Retrieval (SIGIR 05), pp. 298305, New York, NY.
ACM.
Cao, H., Jiang, D., Pei, J., He, Q., Liao, Z., Chen, E., & Li, H. (2008). Context-aware query
suggestion mining click-through session data. Proceeding 14th ACM
SIGKDD International Conference Knowledge Discovery Data Mining (KDD
08), pp. 875883. ACM.
Carroll, J. D., & Chang, J.-J. (1970). Analysis individual differences multidimensional
scaling via n-way generalization Eckart-Young decomposition. Psychometrika,
35 (3), 283319.
Chang, W., Pantel, P., Popescu, A.-M., & Gabrilovich, E. (2009). Towards intent-driven
bidterm suggestion. Proceedings WWW-09 (Short Paper), Madrid, Spain.
Charikar, M. S. (2002). Similarity estimation techniques rounding algorithms. Proceedings thiry-fourth annual ACM symposium Theory computing (STOC
02), pp. 380388. ACM.
Chew, P., Bader, B., Kolda, T., & Abdelali, A. (2007). Cross-language information retrieval using PARAFAC2. Proceedings 13th ACM SIGKDD International
Conference Knowledge Discovery Data Mining (KDD07), pp. 143152. ACM
Press.
Chiarello, C., Burgess, C., Richards, L., & Pollock, A. (1990). Semantic associative
priming cerebral hemispheres: words do, words dont . . . sometimes,
places. Brain Language, 38, 75104.
Chklovski, T., & Pantel, P. (2004). VerbOcean: Mining web fine-grained semantic
verb relations. Proceedings Experimental Methods Natural Language Processing 2004 (EMNLP-04), pp. 3340, Barcelona, Spain.
177

fiTurney & Pantel

Choi, F. Y. Y. (2000). Advances domain independent linear text segmentation.
Proceedings 1st Meeting North American Chapter Association
Computational Linguistics, pp. 2633.
Chu-carroll, J., & Carpenter, B. (1999). Vector-based natural language call routing. Computational Linguistics, 25 (3), 361388.
Church, K. (1995). One term two?. Proceedings 18th Annual International
ACM SIGIR Conference Research Development Information Retrieval, pp.
310318.
Church, K., & Hanks, P. (1989). Word association norms, mutual information, lexicography. Proceedings 27th Annual Conference Association Computational Linguistics, pp. 7683, Vancouver, British Columbia.
Clark, S., & Pulman, S. (2007). Combining symbolic distributional models meaning.
Proceedings AAAI Spring Symposium Quantum Interaction, pp. 5255.
Collobert, R., & Weston, J. (2008). unified architecture natural language processing:
Deep neural networks multitask learning. Proceedings 25th International
Conference Machine Learning (ICML-08), pp. 160167.
Croft, W. B. (1977). Clustering large files documents using single-link method.
Journal American Society Information Science, 28 (6), 341344.
Crouch, C. J. (1988). cluster-based approach thesaurus construction. Proceedings
11th Annual International ACM SIGIR Conference, pp. 309320, Grenoble,
France.
Curran, J. R., & Moens, M. (2002). Improvements automatic thesaurus extraction.
Unsupervised Lexical Acquisition: Proceedings Workshop ACL Special
Interest Group Lexicon (SIGLEX), pp. 5966, Philadelphia, PA.
Cutting, D. R., Karger, D. R., Pedersen, J. O., & Tukey, J. W. (1992). Scatter/gather:
cluster-based approach browsing large document collections. Proceedings
15th Annual International ACM SIGIR Conference, pp. 318329.
Dagan, I., Lee, L., & Pereira, F. C. N. (1999). Similarity-based models word cooccurrence
probabilities. Machine Learning, 34 (13), 4369.
Dang, H. T., Lin, J., & Kelly, D. (2006). Overview TREC 2006 question answering
track. Proceedings Fifteenth Text REtrieval Conference (TREC 2006).
Dasarathy, B. (1991). Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques.
IEEE Computer Society Press.
Davidov, D., & Rappoport, A. (2008). Unsupervised discovery generic relationships using
pattern clusters evaluation automatically generated SAT analogy questions.
Proceedings 46th Annual Meeting ACL HLT (ACL-HLT-08), pp.
692700, Columbus, Ohio.
Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified data processing large clusters.
Communications ACM, 51 (1), 107113.
178

fiFrom Frequency Meaning

Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing latent semantic analysis. Journal American Society
Information Science (JASIS), 41 (6), 391407.
Elsayed, T., Lin, J., & Oard, D. (2008). Pairwise document similarity large collections
mapreduce. Proceedings Association Computational Linguistics
Human Language Technology Conference 2008 (ACL-08: HLT), Short Papers, pp.
265268, Columbus, Ohio. Association Computational Linguistics.
Erk, K. (2007). simple, similarity-based model selectional preferences. Proceedings
45th Annual Meeting Association Computational Linguistics, pp. 216
223,, Prague, Czech Republic.
Erk, K., & Pado, S. (2008). structured vector space model word meaning context.
Proceedings 2008 Conference Empirical Methods Natural Language
Processing (EMNLP-08), pp. 897906, Honolulu, HI.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies Linguistic
Analysis, pp. 132. Blackwell, Oxford.
Foltz, P. W., Laham, D., & Landauer, T. K. (1999). intelligent essay assessor: Applications educational technology. Interactive Multimedia Electronic Journal
Computer-Enhanced Learning, 1 (2).
Forman, G. (2003). extensive empirical study feature selection metrics text classification. Journal Machine Learning Research, 3, 12891305.
French, R. M., & Labiouse, C. (2002). Four problems extracting human semantics
large text corpora. Proceedings 24th Annual Conference Cognitive
Science Society.
Furnas, G. W., Landauer, T. K., Gomez, L. M., & Dumais, S. T. (1983). Statistical semantics: Analysis potential performance keyword information systems. Bell
System Technical Journal, 62 (6), 17531806.
Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive
Science, 7 (2), 155170.
Gilbert, J. R., Moler, C., & Schreiber, R. (1992). Sparse matrices MATLAB: Design
implementation. SIAM Journal Matrix Analysis Applications, 13 (1), 333356.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification semantic relations nominals. Proceedings
Fourth International Workshop Semantic Evaluations (SemEval 2007), pp.
1318, Prague, Czech Republic.
Gleich, D., & Zhukov, L. (2004). SVD based term suggestion ranking system.
Proceedings Fourth IEEE International Conference Data Mining (ICDM
04), pp. 391394. IEEE Computer Society.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). Johns
Hopkins University Press, Baltimore, MD.
179

fiTurney & Pantel

Gorman, J., & Curran, J. R. (2006). Scaling distributional similarity large corpora.
Proceedings 21st International Conference Computational Linguistics
44th annual meeting Association Computational Linguistics (ACL 2006),
pp. 361368. Association Computational Linguistics.
Gorrell, G. (2006). Generalized Hebbian algorithm incremental singular value decomposition natural language processing. Proceedings 11th Conference
European Chapter Association Computational Linguistics (EACL-06), pp.
97104.
Gospodnetic, O., & Hatcher, E. (2004). Lucene Action. Manning Publications.
Grefenstette, G. (1994). Explorations Automatic Thesaurus Discovery. Kluwer.
Harris, Z. (1954). Distributional structure. Word, 10 (23), 146162.
Harshman, R. (1970). Foundations parafac procedure: Models conditions
explanatory multi-modal factor analysis. UCLA Working Papers Phonetics, 16.
Hearst, M. (1997). Texttiling: Segmenting text multi-paragraph subtopic passages.
Computational Linguistics, 23 (1), 3364.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. Fellbaum, C. (Ed.), WordNet: Electronic
Lexical Database, pp. 305332. MIT Press.
Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. Proceedings 22nd
Annual ACM Conference Research Development Information Retrieval (SIGIR 99), pp. 5057, Berkeley, California.
Huang, C.-K., Chien, L.-F., & Oyang, Y.-J. (2003). Relevant term suggestion interactive
web search based contextual information query session logs. Journal
American Society Information Science Technology, 54 (7), 638649.
Hull, D. (1996). Stemming algorithms: case study detailed evaluation. Journal
American Society Information Science, 47 (1), 7084.
Jain, A., Murty, N., & Flynn, P. (1999). Data clustering: review. ACM Computing
Surveys, 31 (3), 264323.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity.
Proceedings International Conference Recent Advances Natural Language
Processing (RANLP-03), pp. 212219, Borovets, Bulgaria.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings International Conference Research
Computational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.
Johnson, H., & Martin, J. (2003). Unsupervised learning morphology English
Inuktitut. Proceedings HLT-NAACL 2003, pp. 4345.
Jones, M. P., & Martin, J. H. (1997). Contextual spelling correction using latent semantic analysis. Proceedings Fifth Conference Applied Natural Language
Processing, pp. 166173, Washington, DC.
180

fiFrom Frequency Meaning

Jones, R., Rey, B., Madani, O., & Greiner, W. (2006). Generating query substitutions.
Proceedings 15th international conference World Wide Web (WWW 06),
pp. 387396, New York, NY. ACM.
Jones, W. P., & Furnas, G. W. (1987). Pictures relevance: geometric analysis
similarity measures. Journal American Society Information Science, 38 (6),
420442.
Kanerva, P. (1993). Sparse distributed memory related models. Hassoun, M. H.
(Ed.), Associative neural memories, pp. 5076. Oxford University Press, New York,
NY.
Karlgren, J., & Sahlgren, M. (2001). words understanding. Uesaka, Y., Kanerva,
P., & Asoh, H. (Eds.), Foundations Real-World Intelligence, pp. 294308. CSLI
Publications.
Kim, S.-M., Pantel, P., Chklovski, T., & Pennacchiotti, M. (2006). Automatically assessing
review helpfulness. Proceedings 2006 Conference Empirical Methods
Natural Language Processing, pp. 423430.
Kolda, T., & Bader, B. (2009). Tensor decompositions applications. SIAM Review,
51 (3), 455500.
Konchady, M. (2008). Building Search Applications: Lucene, LingPipe, Gate. Mustru
Publishing.
Kraaij, W., & Pohlmann, R. (1996). Viewing stemming recall enhancement. Proceedings 19th Annual International ACM SIGIR Conference, pp. 4048.
Lakoff, G. (1987). Women, Fire, Dangerous Things. University Chicago Press,
Chicago, IL.
Landauer, T. K. (2002). computational basis learning cognition: Arguments
LSA. Ross, B. H. (Ed.), Psychology Learning Motivation: Advances
Research Theory, Vol. 41, pp. 4384. Academic Press.
Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.
Psychological Review, 104 (2), 211240.
Landauer, T. K., & Littman, M. L. (1990). Fully automatic cross-language document
retrieval using latent semantic indexing. Proceedings Sixth Annual Conference
UW Centre New Oxford English Dictionary Text Research, pp. 31
38, Waterloo, Ontario.
Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W. (2007). Handbook Latent
Semantic Analysis. Lawrence Erlbaum, Mahwah, NJ.
Lavrenko, V., & Croft, W. B. (2001). Relevance based language models. Proceedings
24th Annual International ACM SIGIR Conference Research Development
Information Retrieval (SIGIR 01), pp. 120127, New York, NY. ACM.
Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarity
word sense identification. Fellbaum, C. (Ed.), WordNet: Electronic Lexical
Database. MIT Press.
181

fiTurney & Pantel

Leacock, C., Towell, G., & Voorhees, E. (1993). Corpus-based statistical sense resolution.
Proceedings ARPA Workshop Human Language Technology, pp. 260265.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrix
factorization. Nature, 401, 788791.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th Annual
Meeting Association Computational Linguistics, pp. 2532.
Lemaire, B., & Denhiere, G. (2006). Effects high-order co-occurrences word semantic
similarity. Current Psychology Letters: Behaviour, Brain & Cognition, 18 (1).
Lin, D. (1998). Automatic retrieval clustering similar words. roceedings
17th international conference Computational linguistics, pp. 768774. Association
Computational Linguistics.
Lin, D., & Pantel, P. (2001). DIRT discovery inference rules text. Proceedings
ACM SIGKDD Conference Knowledge Discovery Data Mining 2001, pp.
323328.
Linden, G., Smith, B., & York, J. (2003). Amazon.com recommendations: Item-to-item
collaborative filtering. IEEE Internet Computing, 7680.
Liu, X., & Croft, W. B. (2005). Statistical language modeling information retrieval.
Annual Review Information Science Technology, 39, 328.
Lovins, J. B. (1968). Development stemming algorithm. Mechanical Translation
Computational Linguistics, 11, 2231.
Lowe, W. (2001). Towards theory semantic space. Proceedings Twenty-first
Annual Conference Cognitive Science Society, pp. 576581.
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces lexical
co-occurrence. Behavior Research Methods, Instruments, Computers, 28 (2), 203
208.
Lund, K., Burgess, C., & Atchley, R. A. (1995). Semantic associative priming highdimensional semantic space. Proceedings 17th Annual Conference
Cognitive Science Society, pp. 660665.
Manning, C., & Schutze, H. (1999). Foundations Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
Manning, C. D., Raghavan, P., & Schutze, H. (2008). Introduction Information Retrieval.
Cambridge University Press, Cambridge, UK.
Miller, G., Leacock, C., Tengi, R., & Bunker, R. (1993). semantic concordance.
Proceedings 3rd DARPA Workshop Human Language Technology, pp. 303
308.
Minnen, G., Carroll, J., & Pearce, D. (2001). Applied morphological processing English.
Natural Language Engineering, 7 (3), 207223.
Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings ACL-08: HLT, pp. 236244, Columbus, Ohio. Association Computational
Linguistics.
182

fiFrom Frequency Meaning

Mitchell, T. (1997). Machine Learning. McGraw-Hill, Columbus, OH.
Mohammad, S., & Hirst, G. (2006). Distributional measures concept-distance: taskoriented evaluation. Proceedings Conference Empirical Methods Natural
Language Processing (EMNLP-2006), pp. 3543.
Monay, F., & Gatica-Perez, D. (2003). image auto-annotation latent space models.
Proceedings Eleventh ACM International Conference Multimedia, pp.
275278.
Morris, J., & Hirst, G. (2004). Non-classical lexical semantic relations. Workshop
Computational Lexical Semantics, HLT-NAACL-04, Boston, MA.
Nakov, P., & Hearst, M. (2007). UCB: System description SemEval Task 4. Proceedings Fourth International Workshop Semantic Evaluations (SemEval 2007),
pp. 366369, Prague, Czech Republic.
Nakov, P., & Hearst, M. (2008). Solving relational similarity problems using theweb
corpus. Proceedings ACL-08: HLT, pp. 452460, Columbus, Ohio.
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., & Szpakowicz, S. (2006). Learning nounmodifier semantic relations corpus-based WordNet-based features. Proceedings 21st National Conference Artificial Intelligence (AAAI-06), pp.
781786.
Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.
Fifth International Workshop Computational Semantics (IWCS-5), pp. 285301,
Tilburg, Netherlands.
Niwa, Y., & Nitta, Y. (1994). Co-occurrence vectors corpora vs. distance vectors
dictionaries. Proceedings 15th International Conference Computational
Linguistics, pp. 304309, Kyoto, Japan.
Nosofsky, R. (1986). Attention, similarity, identification-categorization relationship.
Journal Experimental Psychology: General, 115 (1), 3957.
Ogden, C. K. (1930). Basic English: General Introduction Rules Grammar.
Kegan Paul, Trench, Trubner Co.
Pado, S., & Lapata, M. (2003). Constructing semantic space models parsed corpora.
Proceedings 41st Annual Meeting Association Computational Linguistics, pp. 128135, Sapporo, Japan.
Pado, S., & Lapata, M. (2007). Dependency-based construction semantic space models.
Computational Linguistics, 33 (2), 161199.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? sentiment classification using
machine learning techniques. Proceedings Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 7986, Philadelphia, PA.
Pantel, P. (2005). Inducing ontological co-occurrence vectors. Proceedings Association
Computational Linguistics (ACL-05), pp. 125132.
Pantel, P., & Lin, D. (1998). Spamcop: spam classification organization program.
Learning Text Categorization: Papers AAAI 1998 Workshop, pp. 9598.
183

fiTurney & Pantel

Pantel, P., & Lin, D. (2002a). Discovering word senses text. Proceedings
Eighth ACM SIGKDD International Conference Knowledge Discovery Data
Mining, pp. 613619, Edmonton, Canada.
Pantel, P., & Lin, D. (2002b). Document clustering committees. Proceedings
25th Annual International ACM SIGIR Conference, pp. 199206.
Pasca, M., Lin, D., Bigham, J., Lifchits, A., & Jain, A. (2006). Names similarities
Web: Fact extraction fast lane. Proceedings 21st International
Conference Computational Linguistics 44th Annual Meeting ACL, pp.
809816, Sydney, Australia.
Patwardhan, S., & Pedersen, T. (2006). Using wordnet-based context vectors estimate
semantic relatedness concepts. Proceedings Workshop Making
Sense Sense 11th Conference European Chapter Association
Computational Linguistics (EACL-2006), pp. 18.
Pedersen, T. (2006). Unsupervised corpus-based methods WSD. Word Sense Disambiguation: Algorithms Applications, pp. 133166. Springer.
Pennacchiotti, M., Cao, D. D., Basili, R., Croce, D., & Roth, M. (2008). Automatic induction
FrameNet lexical units. Proceedings 2008 Conference Empirical Methods
Natural Language Processing (EMNLP-08), pp. 457465, Honolulu, Hawaii.
Pennacchiotti, M., & Pantel, P. (2006). Ontologizing semantic relations. Proceedings
21st International Conference Computational Linguistics 44th annual
meeting Association Computational Linguistics, pp. 793800. Association
Computational Linguistics.
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering English words.
Proceedings 31st Annual Meeting Association Computational Linguistics,
pp. 183190.
Porter, M. (1980). algorithm suffix stripping. Program, 14 (3), 130137.
Rabin, M. O. (1981). Fingerprinting random polynomials. Tech. rep., Center research
Computing technology, Harvard University. Technical Report TR-15-81.
Rapp, R. (2003). Word sense discovery based sense descriptor dissimilarity. Proceedings Ninth Machine Translation Summit, pp. 315322.
Ravichandran, D., Pantel, P., & Hovy, E. (2005). Randomized algorithms nlp: using
locality sensitive hash function high speed noun clustering. Proceedings
43rd Annual Meeting Association Computational Linguistics (ACL 05), pp.
622629, Morristown, NJ. Association Computational Linguistics.
Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., & Riedl, J. (1994). Grouplens: open
architecture collaborative filtering netnews. Proceedings ACM 1994
Conference Computer Supported Cooperative Work, pp. 175186. ACM Press.
Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.
Proceedings 14th International Joint Conference Artificial Intelligence
(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.
184

fiFrom Frequency Meaning

Rosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compounds
via domain-specific lexical hierarchy. Proceedings 2001 Conference
Empirical Methods Natural Language Processing (EMNLP-01), pp. 8290.
Rosario, B., Hearst, M., & Fillmore, C. (2002). descent hierarchy, selection
relational semantics. Proceedings 40th Annual Meeting Association
Computational Linguistics (ACL-02), pp. 247254.
Rosch, E., & Lloyd, B. (1978). Cognition Categorization. Lawrence Erlbaum, Hillsdale,
NJ.
Ruge, G. (1997). Automatic detection thesaurus relations information retrieval applications. Freksa, C., Jantzen, M., & Valk, R. (Eds.), Foundations Computer
Science, pp. 499506. Springer.
Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. (1998). Bayesian approach
filtering junk e-mail. Proceedings AAAI-98 Workshop Learning Text
Categorization.
Sahlgren, M. (2005). introduction random indexing. Proceedings Methods
Applications Semantic Indexing Workshop 7th International Conference
Terminology Knowledge Engineering (TKE), Copenhagen, Denmark.
Sahlgren, M. (2006). Word-Space Model: Using distributional analysis represent syntagmatic paradigmatic relations words high-dimensional vector spaces.
Ph.D. thesis, Department Linguistics, Stockholm University.
Salton, G. (1971). SMART retrieval system: Experiments automatic document processing. Prentice-Hall, Upper Saddle River, NJ.
Salton, G., & Buckley, C. (1988). Term-weighting approaches automatic text retrieval.
Information Processing Management, 24 (5), 513523.
Salton, G., Wong, A., & Yang, C.-S. (1975). vector space model automatic indexing.
Communications ACM, 18 (11), 613620.
Sarawagi, S., & Kirpal, A. (2004). Efficient set joins similarity predicates. Proceedings 2004 ACM SIGMOD International Conference Management Data
(SIGMOD 04), pp. 743754, New York, NY. ACM.
Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis.
Proceedings International Conference Artificial Neural Networks (ICANN1997), pp. 583588, Berlin.
Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),
97124.
Schutze, H., & Pedersen, J. (1993). vector model syntagmatic paradigmatic
relatedness. Making Sense Words: Proceedings Conference, pp. 104113,
Oxford, England.
Sebastiani, F. (2002). Machine learning automated text categorization. ACM Computing
Surveys (CSUR), 34 (1), 147.
Shannon, C. (1948). mathematical theory communication. Bell System Technical
Journal, 27, 379423, 623656.
185

fiTurney & Pantel

Singhal, A., Salton, G., Mitra, M., & Buckley, C. (1996). Document length normalization.
Information Processing Management, 32 (5), 619633.
Smith, E., Osherson, D., Rips, L., & Keane, M. (1988). Combining prototypes: selective
modification model. Cognitive Science, 12 (4), 485527.
Snow, R., Jurafsky, D., & Ng, A. Y. (2006). Semantic taxonomy induction heterogenous evidence. Proceedings 21st International Conference Computational
Linguistics 44th annual meeting ACL, pp. 801808.
Sparck Jones, K. (1972). statistical interpretation term specificity application
retrieval. Journal Documentation, 28 (1), 1121.
Spearman, C. (1904). General intelligence, objectively determined measured. American Journal Psychology, 15, 201293.
Sproat, R., & Emerson, T. (2003). first international Chinese word segmentation bakeoff. Proceedings Second SIGHAN Workshop Chinese Language Processing,
pp. 133143, Sapporo, Japan.
Stone, P. J., Dunphy, D. C., Smith, M. S., & Ogilvie, D. M. (1966). General Inquirer:
Computer Approach Content Analysis. MIT Press, Cambridge, MA.
Tan, B., & Peng, F. (2008). Unsupervised query segmentation using generative language
models Wikipedia. Proceeding 17th international conference World
Wide Web (WWW 08), pp. 347356, New York, NY. ACM.
Tellex, S., Katz, B., Lin, J., Fern, A., & Marton, G. (2003). Quantitative evaluation
passage retrieval algorithms question answering. Proceedings 26th Annual
International ACM SIGIR Conference Research Development Information
Retrieval (SIGIR), pp. 4147.
Tucker, L. R. (1966). mathematical notes three-mode factor analysis. Psychometrika, 31 (3), 279311.
Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.
Proceedings Twelfth European Conference Machine Learning (ECML-01),
pp. 491502, Freiburg, Germany.
Turney, P. D. (2005). Measuring semantic similarity latent relational analysis. Proceedings Nineteenth International Joint Conference Artificial Intelligence
(IJCAI-05), pp. 11361141, Edinburgh, Scotland.
Turney, P. D. (2006). Similarity semantic relations. Computational Linguistics, 32 (3),
379416.
Turney, P. D. (2007). Empirical evaluation four tensor decomposition algorithms. Tech.
rep., Institute Information Technology, National Research Council Canada.
Technical Report ERB-1152.
Turney, P. D. (2008a). latent relation mapping engine: Algorithm experiments.
Journal Artificial Intelligence Research, 33, 615655.
Turney, P. D. (2008b). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference Computational
Linguistics (Coling 2008), pp. 905912, Manchester, UK.
186

fiFrom Frequency Meaning

Turney, P. D., & Littman, M. L. (2003). Measuring praise criticism: Inference
semantic orientation association. ACM Transactions Information Systems,
21 (4), 315346.
Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semantic
relations. Machine Learning, 60 (13), 251278.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent
modules solve multiple-choice synonym analogy problems. Proceedings
International Conference Recent Advances Natural Language Processing
(RANLP-03), pp. 482489, Borovets, Bulgaria.
Van de Cruys, T. (2009). non-negative tensor factorization model selectional preference
induction. Proceedings Workshop Geometric Models Natural Language
Semantics (GEMS-09), pp. 8390, Athens, Greece.
van Rijsbergen, C. J. (2004). Geometry Information Retrieval. Cambridge University
Press, Cambridge, UK.
van Rijsbergen, C. J. (1979). Information Retrieval. Butterworths.
Veale, T. (2003). analogical thesaurus. Proceedings 15th Innovative Applications Artificial Intelligence Conference (IAAI 2003), pp. 137142, Acapulco,
Mexico.
Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.
Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),
pp. 606612, Valencia, Spain.
Vozalis, E., & Margaritis, K. (2003). Analysis recommender systems algorithms.
Proceedings 6th Hellenic European Conference Computer Mathematics
Applications (HERCMA-2003), Athens, Greece.
Vyas, V., & Pantel, P. (2009). Semi-automatic entity set refinement. Proceedings
NAACL-09, Boulder, CO.
Weaver, W. (1955). Translation. Locke, W., & Booth, D. (Eds.), Machine Translation
Languages: Fourteen Essays. MIT Press, Cambridge, MA.
Weeds, J., Weir, D., & McCarthy, D. (2004). Characterising measures lexical distributional similarity. Proceedings 20th International Conference Computational Linguistics (COLING 04), pp. 10151021. Association Computational
Linguistics.
Wei, X., Peng, F., & Dumoulin, B. (2008). Analyzing web text association disambiguate
abbreviation queries. Proceedings 31st Annual International ACM SIGIR
Conference Research Development Information Retrieval (SIGIR 08), pp.
751752, New York, NY. ACM.
Wen, J.-R., Nie, J.-Y., & Zhang, H.-J. (2001). Clustering user queries search engine.
Proceedings 10th International Conference World Wide Web (WWW 01),
pp. 162168, New York, NY. ACM.
Widdows, D. (2004). Geometry Meaning. Center Study Language
Information, Stanford, CA.
187

fiTurney & Pantel

Widdows, D., & Ferraro, K. (2008). Semantic vectors: scalable open source package
online technology management application. Proceedings Sixth International
Conference Language Resources Evaluation (LREC 2008), pp. 11831190.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools
Techniques Java Implementations. Morgan Kaufmann, San Francisco.
Wittgenstein, L. (1953). Philosophical Investigations. Blackwell. Translated G.E.M.
Anscombe.
Wolfe, M. B. W., Schreiner, M. E., Rehder, B., Laham, D., Foltz, P. W., Kintsch, W., &
Landauer, T. K. (1998). Learning text: Matching readers texts latent
semantic analysis. Discourse Processes, 25, 309336.
Yang, Y. (1999). evaluation statistical approaches text categorization. Information
Retrieval, 1 (1), 6990.
Yuret, D., & Yatbaz, M. A. (2009). noisy channel model unsupervised word sense
disambiguation. Computational Linguistics. review.
Zamir, O., & Etzioni, O. (1999). Grouper: dynamic clustering interface Web search
results. Computer Networks: International Journal Computer Telecommunications Networking, 31 (11), 13611374.
Zhao, Y., & Karypis, G. (2002). Evaluation hierarchical clustering algorithms document datasets. Proceedings Eleventh International Conference Information Knowledge Management, pp. 515524, McLean, Virginia.

188


