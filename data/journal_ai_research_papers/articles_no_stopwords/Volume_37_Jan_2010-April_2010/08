journal artificial intelligence

submitted published

investigation mathematical programming
finite horizon decentralized pomdps
raghav aras

raghav aras gmail com

ims suplec metz
rue edouard belin metz technopole
metz france

alain dutech

alain dutech loria fr

maia loria inria
campus scientifique bp
vandoeuvre les nancy france

abstract
decentralized uncertain environments complex task generally dealt
decision theoretic mainly framework decentralized partially observable markov decision processes dec pomdps although dec pomdps
general powerful modeling tool solving task overwhelming
complexity doubly exponential study alternate formulation dec pomdps relying sequence form representation policies
formulation derive mixed integer linear programming milp
solved give exact optimal solutions dec pomdps
milps derived combinatorial characteristics optimal
solutions dec pomdps concepts borrowed game theory
experimental validation classical test dec pomdp literature
compare existing mathematical programming outperforms dynamic programming less efficient forward search except
particular
main contributions work use mathematical programming decpomdps better understanding dec pomdps solutions besides
argue alternate representation dec pomdps could helpful designing
novel looking approximate solutions dec pomdps

introduction
framework decentralized partially observable markov decision processes decpomdps used model designing system made autonomous
agents need coordinate order achieve joint goal solving dec pomdps
untractable task belong class nexp complete see section
dec pomdps reformulated sequence form dec pomdps
derive mixed integer linear programs solved efficient solvers
order design exact optimal solutions finite horizon dec pomdps main
motivation investigate benefits limits novel get
better understanding dec pomdps see section practical level provide
heuristics solving dec pomdps evaluate classical
see section
c

ai access foundation rights reserved

fiaras dutech

context
one main goals artificial intelligence build artificial agents exhibit
intelligent behavior agent entity situated environment perceive
sensors act upon actuators concept e select
sequence actions order reach goal central field artificial
intelligence years notion intelligent behavior difficult assess
measure prefer refer concept rational behavior formulated russell
norvig consequence work presented uses decision theoretic
order build agents take optimal actions uncertain partially
unknown environment
particularly interested cooperative multi agent systems multiple
independent agents limited perception environment must interact coordinate order achieve joint task central process full knowledge state
system control agents contrary agent autonomous
entity must execute actions setting blessing agent
ideally deal small part curse coordination
cooperation harder develop enforce
decision theoretic rational behavior relies mostly framework
markov decision processes mdp puterman system seen sequence
discrete states stochastic dynamics particular states giving positive negative
reward process divided discrete decision periods number periods
called horizon mdp periods action chosen
influence transition process next state right actions
influence transition probabilities states objective controller
system maximize long term return often additive function reward
earned given horizon controller knows dynamics system
made transition function reward function derived field
dynamic programming see bellman allow controller compute optimal
deterministic policy e decision function associates optimal action every
state expected long term return optimal process called
mdp community
fact mdp framework quite straightforward model
one agent full complete knowledge state system agents
especially multi agent setting generally able determine complete
exact state system noisy faulty limited sensors
nature consequence different states system observed
similar agent different optimal actions taken
states one speaks perceptual aliasing extension mdps called partially
observable markov decisions processes pomdps deals explicitly phenomenon
allows single agent compute plans setting provided knows conditional
probabilities observations given state environment cassandra kaelbling
littman
pointed boutilier multi agent could solved mdps
considered centralized point view control although


fimathematical programming dec pomdps

centralized process interested decentralized settings every
agent executes policy even agents could instantly communicate observation consider joint observation resulting communications would still enough identify state system framework
decentralized partially observable markov decision processes dec pomdp proposed
bernstein givan immerman zilberstein takes account decentralization
control partial observability dec pomdp looking optimal joint
policies composed one policy agent individual policies
computed centralized way independently executed agents
main limitation dec pomdps provably untractable
belong class nexp complete bernstein et al concretely
complexity implies worst case finding optimal joint policy finite
horizon dec pomdp requires time exponential horizon one make
good choices complexity finding exact
optimal solutions dec pomdps doubly exponential complexity
look approximate solutions discussed detailed
work oliehoek spaan vlassis follow dynamic
programming forward search adapting concepts
designed pomdps
yet concept decentralized focus quite large body
previous work fields example team decision radner
later formulated markov system field control theory anderson
moore led markov team decision pynadath tambe
field mathematics abundant literature game theory brings way
looking multi agent particular dec pomdp finite horizon
thought game extensive form imperfect information identical interests
osborne rubinstein
taking inspiration field game theory mathematical programming design exact solving dec pomdps precisely subject contribution
field decentralized multi agent
motivations
main objective work investigate use mathematical programming
especially mixed integer linear programs milp diwekar solving decpomdps motivation relies fact field linear programming quite
mature great interest industry consequence exist many efficient
solvers mixed integer linear programs want see efficient solvers
perform framework dec pomdps
therefore reformulate dec pomdp solve mixed integer linear
program shown article two paths lead mathematical programs one
grounded work koller megiddo von stengel koller megiddo
von stengel another one grounded combinatorial considerations
methods rely special reformulation dec pomdps called


fiaras dutech

sequence form dec pomdps policy defined histories e sequences
observations actions generate applied dec pomdp
basic idea work select among histories dec pomdp
histories part optimal policy end optimal solution
milp presented article assign positive weight history decpomdp every history non negative weight part optimal policy
dec pomdp number possible histories exponential horizon
complexity naive search optimal set histories doubly
exponential therefore idea appears untractable useless
nevertheless combining efficiency milp solvers quite
simple heuristics leads exact compare quite well existing exact
fact sequence form dec pomdps need memory space exponential
size even solving milps exponential size
milp thus leads doubly exponential complexity sequence form
argue sequence form milps compare quite well dynamic programming thanks
optimized industrial milp solvers cplex
still investigations experiments mathematical programming decpomdps solely aim finding exact solutions dec pomdps main motivation better understanding dec pomdps limits benefits
mathematical programming hope knowledge help deciding
extent mathematical programming sequence form dec pomdps used
design novel look approximate solutions dec pomdps
contributions
develop order exact optimal joint policies
dec pomdps main inspiration comes work koller von stegel
megiddo shows solve games extensive form imperfect information
identical interests nash equilibrium kind game koller et al
koller megiddo von stengel caused breakthrough
memory space requirement linear size game whereas
canonical required space exponential size game
breakthrough mostly due use formulation policy call
sequence form
main contribution detailed section adapt sequence form
introduced koller von stegel megiddo framework dec pomdps koller
et al koller megiddo von stengel possible
formulate resolution dec pomdp special kind mathematical program
still solved quite efficiently mixed linear program variables
required adaptation resulting mixed integer linear
program straightforward fact koller von stegel megiddo could
one nash equilibrium agent game needed dec pomdps
set policies called joint policy corresponds nash equilibrium
highest value finding one nash equilibrium already complex task
enough besides whereas koller von stegel megiddo could applied


fimathematical programming dec pomdps

agent games extend solve dec pomdps arbitrary
number agents constitutes important contribution
order formulate dec pomdps milps analyze detail structure
optimal joint policy dec pomdp joint policy sequence form expressed
set individual policies described set possible trajectories
agents dec pomdp combinatorial considerations individual
histories well constraints ensure histories define valid joint policy
heart formulation dec pomdp mixed linear program developped
sections thus another contribution work better understanding
properties optimal solutions dec pomdps knowledge might lead
formulation approximate dec pomdps
another important contribution work introduce heuristics boosting performance mathematical programs propose see section
heuristics take advantage succinctness dec pomdp model knowledge acquired regarding structure optimal policies consequently able
reduce size mathematical programs resulting reducing time taken
solve heuristics constitute important pre processing step solving
programs present two types heuristics elimination extraneous histories
reduces size mixed integer linear programs introduction cuts
mixed integer linear programs reduces time taken solve program
practical level article presents three different mixed integer linear
programs two directly derived work koller von stegel megiddo
see table third one solely combinatorial considerations
individual policies histories see table theoretical validity formulations backed several theorems conducted experimental evaluations
heuristics several classical dec pomdp thus
able confirm quite comparable dynamic programming exact
outperformed forward search gmaa oliehoek et al
though milps indeed faster one order magnitude
two gmaa
overview article
remainder article organized follows section introduces formalism
dec pomdp background classical usually dynamic
programing expose reformulation dec pomdp sequence form
section define notions needed sequence form section
use combinatorial properties sequence form policies derive first
mixed integer linear program milp table solving dec pomdp game
theoretic concepts nash equilibrium take inspiration previous work games
extensive form design two milps solving dec pomdp tables
milps smaller size detailed derivation presented section
contributed heuristics speed practical resolutions milps make
core section section presents experimental validations milp
classical benchmarks dec pomdp literature well randomly


fiaras dutech

built finally section analyzes discusses work conclude
section

dec pomdp
section gives formal definition decentralized partially observed markov decision
processes introduced bernstein et al described solution decpomdp policy defined space information sets optimal value
sections ends quick overview classical methods developed
solve dec pomdps
formal definition
dec pomdp defined tuple h ai p oi g r
n set agents
finite set states set probability distributions shall denoted
members shall called belief states
agent ai set actions ii ai denotes set joint
actions
p state transition function
p probability state period
period state agents performed joint action thus
time period pair states joint action
holds
p pr st st
thus p defines discrete state discrete time controlled markov process
agent oi set observations ii oi denotes set joint
observations
g joint observation function
g probability agents receive
joint observation agent receives observation oi state
period previous period agents took joint
action thus time period joint action state
joint observation holds
g pr ot st
r r reward function r r
reward obtained agents take joint action state
process


fimathematical programming dec pomdps

horizon agents allowed joint actions
process halts
initial state dec pomdp denotes
probability state first period
said p define controlled markov process next state depends
previous state joint action chosen agents agents
access state process rely observations generally
partial noisy state specified observation function g time
time agents receive non zero reward according reward function r

n

n









nt

n







nt



st

figure dec pomdp every period process environment state
st every agent receives observations oti decides action ati joint
action hat atn alters state process
specifically illustrated figure control dec pomdp n
agents unfolds discrete time periods follows period
process state denoted st first period state chosen
according agents take actions period afterward agent
takes action denoted ati ai according agents policy
agents take joint action hat atn following events occur
agents obtain reward r st
state st determined according function p arguments st
agent receives observation ot
oi joint observation ot




hot
determined function g arguments
period changes
dec pomdp interested following properties


fiaras dutech

horizon finite known agents
agents cannot infer exact state system joint observations
general setting dec pomdps
agents observe actions observations agents
aware observations reward
agents perfect memory past base choice action
sequence past actions observations speak perfect recall setting
transition observation functions stationary meaning depend
period
solving dec pomdp means finding agents policies e decision functions
optimize given criterion rewards received criterion work
called cumulative reward defined


x




e
r ha



e mathematical expectation
example dec pomdp
known decentralized tiger hereby denoted tiger
introduced nair tambe yokoo pynadath marsella widely used
test dec pomdps variation previously introduced
pomdps e dec pomdps one agent kaelbling littman cassandra

given two agents confronted two closed doors behind one
door tiger behind escape route agents know door
leads agent independently open one two doors
listen carefully order detect tiger opens wrong door
lives imperiled open escape door free
agents limited time decide door open use time
gather information precise location tiger listening carefully detect
location tiger formalized dec pomdp
two states tiger behind left door sl right door sr
two agents must decide act
three actions agent open left door al open right door ar
listen ao
two observations thing agent observe hear tiger
left ol right


fimathematical programming dec pomdps

initial state chosen according uniform distribution long door
remains closed state change one door opened state reset
sl sr equal probability observations noisy reflecting difficulty
detecting tiger example tiger left action ao produces
observation ol time agents perform ao joint observation
ol ol occurs probability reward function encourages
agents coordinate actions example reward open escape
door bigger one listens opens good door
full state transition function joint observation function reward function described
work nair et al
information sets histories
information set agent sequence ot even length
elements odd positions actions agent members ai even
positions observations agent members oi information set length
shall called null information set denoted information set length
shall called terminal information set set information sets lengths less
equal shall denoted
define history agent sequence ot odd
length elements odd positions actions agent members ai
even positions observations agent members oi define length
history number actions history example history
length shall called terminal history histories lengths less shall called
non terminal histories history null length shall denoted information
set associated history h denoted h information set composed removing
h last action h history observation h information set
shall denote hit set possible histories length agent thus hi
set actions ai shall denote hi set histories agent lengths
less equal size ni hi thus
ni hi

pt



ai oi

ai

ai oi

ai oi



set hit terminal histories agent shall denoted ei set hi hit
non terminal histories agent shall denoted ni
tuple hh h hn made one history agent called joint history
tuple obtained removing history hi joint history h noted hi called
reduced joint history
example coming back tiger example set valid histories could ao
ao ol ao ao ao ao ol ao ol ao ao ol ao ar ao ao ol ao ao ao ar
incidently set histories corresponds support policy e histories
generated policy figure explained next section


fiaras dutech

policies
period time policy must tell agent action choose choice
whatever past present knowledge agent process
time one possibility define individual policy agent mapping
information sets actions formally
ai



among set policies three families usually distinguished
pure policies pure deterministic policy maps given information set one
unique action set pure policies agent denoted pure policies
could defined trajectories past observations since actions
chosen deterministically reconstructed observations
mixed policies mixed policy probability distribution set pure
policies thus agent mixed policy control dec pomdp
pure policy randomly chosen set pure policies
stochastic policies stochastic policy general formulation associates
probability distribution actions history
come back tiger section figure gives possible policy
horizon shown policy classically represented action observation tree
kind tree branch labelled observation given sequence past
observations one starts root node follows branches action
node node contains action executed agent seen
sequence observations
observation sequence
chosen action

ol
ao


ao


ao

ol ol
al

ol
ao

ol
ao


ar

ao
ol



ao

ao

ol



ol



al

ao

ao

ar

figure pure policy tiger pure policy maps sequences observations
actions represented action observation tree

joint policy h n n tuple policy agent
individual policies must horizon agent define
notion reduced joint policy h n composed
policies agents thus hi


fimathematical programming dec pomdps

value function
executed agents every horizon joint policy generates probability distribution possible sequences reward one compute value
policy according equation thus value joint policy formally defined
v e


x

r st







given state first period chosen according actions chosen
according
recursive definition value function policy way
compute horizon finite definition requires concepts
shall introduce
given belief state joint action joint observation
let denote probability agents receive joint observation take
joint action period state chosen according probability
defined
x
x


p g



ss

given belief state joint action joint observation
updated belief state ao respect defined

ao

p
g ss p






ao







p
given belief state joint action r denotes ss r
definitions notations value v defined follows
v v



v defined recursion equations given
equations straight reformulation classical bellman equations finite
horizon
histories null length
v r

x

v



oo

denotes joint action h n denotes
updated state given joint observation


fiaras dutech

non terminal histories



tuple sequences observations ho

oi
sequence observations agent
v r

x



v



oo


updated state given joint action joint observation
ho tuple sequences observations ho










n

n

terminal histories tuple sequences



observations ho


n


v r

x

r



ss

optimal policy policy best possible value verifying
v v





important fact dec pomdps following theorem
restrict set pure policies looking solution dec pomdp
theorem dec pomdp least one optimal pure joint policy
proof see proof work nair et al



overview dec pomdps solutions limitations
detailed work oliehoek et al existing methods solving decpomdps finite horizon belong several broad families brute force alternating
maximization search dynamic programming
brute force simplest solving dec pomdp enumerate
possible joint policies evaluate order optimal one however
method becomes quickly untractable number joint policies doubly exponential
horizon
alternating maximization following chades scherrer charpillet nair
et al one possible way solve dec pomdps agent small group
agents alternatively search better policy agents freeze
policy called alternating maximization oliehoek alternated co evolution
chades method guarantees nash equilibria locally optimal joint
policy


fimathematical programming dec pomdps

heuristic search concept introduced szer charpillet
zilberstein relies heuristic search looking optimal joint policy
admissible approximation value optimal joint policy search
progresses joint policies provably worse current admissible solution
pruned szer et al used underlying mdps pomdps compute admissible heuristic
oliehoek et al introduced better heuristic resolution bayesian
game carefully crafted cost function currently oliehoeks method called gmaa
generic multi agent quickest exact method large set benchmarks
every exact method limited quite simple
dynamic programming work hansen bernstein zilberstein
adapts solutions designed pomdps domain dec pomdps general
idea start policies time step used build time step policies
process clearly less efficient heuristic search
exponential number policies must constructed evaluated iteration
policies pruned pruning less efficient
exposed details oliehoek et al several others approaches developed subclasses dec pomdps example special settings agents allowed communicate exchange informations settings
transition function split independant transition functions agent
studied found easier solve generic dec pomdps

sequence form dec pomdps
section introduces fundamental concept policies sequence form
formulation dec pomdp thus possible leads non linear program
nlp solution defines optimal solution dec pomdp
policies sequence form
history function p agent mapping set histories interval
value p h weight history h history function p policy
defines probability function set histories agent saying
history hi hi p hi conditional probability hi given observation sequence
oti
every policy defines policy function every policy function associated
valid policy constraints must met fact history function p sequenceform policy agent following constraints met
x

p



aai

p h

x

p h

h ni oi



aai

h denotes history obtained concatenating h definition
appears slightly different form lemma work koller et al


fiaras dutech

variables x h h hi
x

x



aai

x h

x

x h

h ni oi



h hi



aai

x h

table policy constraints set linear inequalities solved provide valid
sequence form policy agent weights x h possible
define policy agent

sequence form policy stochastic probability choosing action
information set h p h p h support p sequence form policy made
set histories non negative weight e p h hi p h
sequence form policy p defines unique policy agent sequence form policy
called policy rest ambiguity present
set policies sequence form agent shall denoted xi set
pure policies sequence form shall denoted xi xi
way similar definitions section define sequence form joint
policy tuple sequence form policies one agent weight q
joint
history h hhi sequence form joint policy hp p pn product ii pi hi
set joint policies sequence form ii xi shall denoted x set
reduced sequence form joint policy called xi
policy constraints
policy agent sequence form found solving set linear inequalities
li found table li merely implement definition policy sequenceform li contains one variable x h history h hi represent weight
h policy solution x li constitutes policy sequence form
example section e appendices policy constraints decentralized
tiger given agents horizon
notice policy constraints agent variable constrained
non negative whereas definition policy sequence form weight history
must interval mean variable solution policy
constraints assume value higher actually policy constraints
prevent variable assuming value higher following lemma
shows
lemma every solution x h hi x h belongs
interval


fimathematical programming dec pomdps

proof shown forward induction
every x h non negative see eq case every action
ai x greater otherwise constraint would violated
h hi e ai x h belong
every h hit x h previous reasoning applied constraint
leads evidently fact x h every h hit
thereby induction holds

later article order simplify task looking joint policies policy
constraints li used pure policies looking pure policies limitation
finite horizon dec pomdps admit deterministic policies policies defined
information set fact pure policies needed two three milps build
order solve dec pomdps otherwise derivation would possible see
sections
looking pure policies obvious solution would impose every variable
x h belongs set solving mixed integer linear program
generally good idea limit number integer variables integer variable
possible node branch bound method used assign integer values
variables efficient implementation mixed integer linear program take
advantage following lemma impose weights terminal histories
take possible values
lemma replaced
x h

h ni

x h

h ei




every solution x resulting li h hi x h
speak li
proof prove backward induction let h history length
due oi holds
x
x h
x h

aai

since h history length history h terminal history due lemma
x h therefore sum right hand side equation
due x h hence sum right hand side
value ergo x h value
reasoning x h every non terminal
history h length

formulate linear inequalities table memory require
p space
exponential horizon agent size hi tt ai oi
exponential number variables
lp exponential
pt

number constraints li table ai oi meaning number
constraints li exponential


fiaras dutech

sequence form dec pomdp
able give formulation dec pomdp use sequence form
policies want stress formulation provide us
ways solving dec pomdps mathematical programming
given classical formulation dec pomdp see section equivalent
sequence form dec pomdp tuple hi hi ri
n set agents
agent hi set histories length less equal
agent defined previous section set hi derived sets
ai oi
joint history conditional probability function joint history j h
j probability j occurring conditional agents taking joint actions
according given initial state dec pomdp function
derived set states state transition function p joint
observation function g
r joint history value joint history j h r j value
expected reward agents obtain joint history j occurs function
derived set states state transition function p joint observation
function g reward function r alternatively r described function
r
formulation folds p g r relying set histories
give details computation r
j conditional probability sequence joint observations received
agents till period j j ot j sequence joint actions
taken till period j j j initial state
dec pomdp
j prob j j ot j j j j



probability product probabilities seeing observation ok j given
appropriate belief state action chosen time k
j




ok j jk ak j



k

jk probability distribution given agents followed joint
history j time k
jk prob j j ok j




fimathematical programming dec pomdps

variables xi h h hi
maximize

x

r j

je



xi ji



ii

subject
x

xi





h ni oi



h hi



aai

xi h

x

xi h

aai

xi h

table nlp non linear program expresses constraints finding sequenceform joint policy optimal solution dec pomdp

regarding value joint history defined
r j r j j




r j

x
x

jk r ak j



k ss

thus v p value sequence form joint policy p weighted sum
value histories support
x
v p
p j r j

jh

p j

q

ii

pi ji

non linear program solving dec pomdps
sequence form formulation dec pomdp able express joint
policies sets linear constraints assess value every policy solving decpomdp amounts finding policy maximal value done
non linear program nlp table xi variables weights
histories agent
example example formulation nlp found appendices
section e given decentralized tiger agents horizon

constraints program form convex set objective function
concave explained appendix general case solving non linear program


fiaras dutech

difficult generalized method guarantee finding global maximum
point however particular nlp fact multilinear mathematical program see
drenick kind programs still difficult solve two
agents considered one speaks bilinear programs solved easily
petrik zilberstein horst tuy
evident inefficient method global maximum point evaluate
extreme points set feasible solutions program since known every
global well local maximum point non concave function lies extreme point
set fletcher inefficient method test tells
extreme point local maximum point global maximum point hence unless
extreme points evaluated cannot sure obtained global maximum
point set feasible solutions nlp x set step joint policies
set extreme points set x set pure step joint policies whose number
doubly exponential exponential n enumerating extreme points
nlp untractable
developed next sections linearize objective function
nlp order deal linear programs describe two ways
one combinatorial consideration section
game theory concepts section cases shall mean adding variables
constraints nlp upon shall derive mixed integer linear programs
possible global maximum point hence optimal joint policy
dec pomdp

combinatorial considerations mathematical programming
section explains possible use combinatorial properties dec pomdps
transform previous nlp mixed integer linear program shown mathematical program belongs family mixed integer linear programs meaning
variables linear program must take integer values set
linearization objective function
borrowing ideas field quadratic assignment papadimitriou steiglitz turn non linear objective function previous nlp linear
objective function linear constraints involving variables z must take integer
values variable z j represents product xi ji variables
thus objective function
x

maximize
r j
xi ji

je

ii

rewritten
maximize

x

r j z j

je

j hj j jn




fimathematical programming dec pomdps

must ensure two way mapping value variables
z x variables solution mathematical program

z j
xi ji

ii

restrict ourself pure policies x variables
case previous constraint becomes
z j xi ji





take advantage fact support pure policy agent
composed oi terminal histories express constraints one hand
guarantee z j equal enough x variables equal
write
n
x

xi ji nz j

j e





hand limit number z j variables take value
enumerate number joint terminal histories end

x
oi

z j
ii

je

constraints would weight heavily mathematical program would
one constraint terminal joint history number exponential n
idea reduce number constraints reason joint histories
individual histories history h agent part support solution
ofpthe e xq
h number joint histories
q belongs
j ei z hh j ki ok suggest replace ei constraints

n
x

xi ji nz j

j e







p

ei constraints
x



ok
xi h
oi

xi h
ok

z hh j

j ei

q

ki

h ei



ki

fewer integer variables
linearization objective function rely fact dealing pure
policies meaning every x z variable supposed value solving
linear programs integer variables usually branch bound technique


fiaras dutech

variables
xi h h hi
z j j e
x

maximize

r j z j



je

subject
x

xi





h ni oi



aai

xi h

x

xi h

aai

x



z hh j xi h


j hi

ok

h ei



ki

x

z j

je



oi



ii

xi h

h ni

xi h



h ei

z j



j e



table milp mixed integer linear program finds sequence form joint policy
optimal solution dec pomdp

fletcher efficiency reasons important reduce number integer
variables mathematical programs
done section relax x variables allow take non negative
values provided x values terminal histories constrained integer values
furthermore proved following lemma constraints x guarantee
z variables take value
eventually end following linear program real integer variables
thus called mixed integer linear program milp milp shown table
example section e appendices example milp given
decentralized tiger agents horizon
lemma every solution x z milp table j e z j

proof let x z solution milp let
z j e z j
si xi h


ei xi h









si z j j e ji j z j





j

ei



fimathematical programming dec pomdps

q
q
showing z

due z

ii oi
q iit

shall establish z ii oi
due upper bound z

variable implication z j terminal joint history j thus
proving statement lemma
note lemma agent xi pure policy therefore
si x oi means set constraints reduced terminal
joint history j ei appear right hand side oi times
left hand side xi h thus j ei
si z j oi



h since
know agent history h hi xi q
xi pure policy given reduced terminal joint history j ki xk jk
secondly due following implication clearly holds terminal joint
history j
z j xi ji





therefore obtain
si z j oi






oi

xk jk



ki

consequence
x

x

si z j

j ei



oi

j ei

xk jk



xk jk



xk h



ki

oi

x



j ei ki

oi



x

ki h ek

oi



ok



ki







oj





ji

since



j ei

si z j z holds

p



z

j ei

si z j z hence

oj



ji

thus statement lemma proved




fiaras dutech

summary
combinatorial considerations possible design milp solving given
dec pomdp proved theorem solution milp defines optimal joint
policy
nevertheless milp quite large kt constraints
pfor dec pomdp
q
nt
hi ei k variables kt variables must take integer values
next section details another method linearization nlp leads
smaller mathematical program agent case
theorem given solution x z milp x hx x xn pure
period optimal joint policy sequence form
proof due policy constraints domain constraints agent xi

pure sequence form policy
q agent due constraints z values
product ii xi ji values maximizing objective function
effectively maximizing value sequence form policy hx x xn thus

hx x xn optimal joint policy original dec pomdp

game theoretical considerations mathematical
programming
section borrows concepts nash equilibrium regret game theory
order design yet another mixed integer linear program solving dec pomdps
fact two milps designed one applied agents
one number agents main objective part derive smaller
mathematical program agent case indeed milp agents see table
slightly less variables constraints milp see table thus might prove easier
solve hand agents considered derivation
leads milp given completeness bigger milp
links fields multiagent systems game theory numerous
literature see example sandholm parsons wooldridge elaborate fact optimal policy dec pomdp nash equilibrium
fact nash equilibrium highest utility agents share reward
agent case derivation make order build milp similar
first derivation sandholm gilpin conitzer give details
derivation adapt dec pomdp adding objective function
agents derivation still use nash equilibriae pure strategies
rest article make distinction policy sequence form
policy strategy agent context concepts equivalent borrowing
game theory joint policy denoted p q individual policy pi qi
reduced policy pi qi
nash equilibrium
nash equilibrium joint policy policy best response reduced
joint policy formed policies joint policy context sequence form


fimathematical programming dec pomdps

dec pomdp policy pi xi agent said best response reduced
joint policy qi xi holds
v hpi qi v hpi qi

pi xi



joint policy p x nash equilibrium holds
v p v hpi pi

x x

hei j ei

r hh j



ki

pi xi




pk jk pi h pi h

pi xi





derivation necessary conditions nash equilibrium consists deriving
necessary conditions policy best response reduced joint policy
following program finds policy agent best response reduced joint
policy qi xi constraints ensure policy defines valid joint policy
see section objective function traduction concept best response
variables xi h h hi



x x

maximize
r hh j
qk jk xi h


hei

j ei



ki

subject

x

xi



aai

xi h

x

xi h

h ni oi



h hi



aai

xi h

linear program lp must still refined solution best
response agent global best response e policy agent best
response agents mean introducing variables set variable
agent main point adapt objective function current
objective function applied global best response would lead non linear
objective function product weights policies would appear
make use dual program lp
linear program lp one variable xi h history h hi representing
weight h one constraint per information set agent words
constraint linear program lp uniquely labeled information set instance
constraint labeled null information set nonterminal
history h observation corresponding constraint labeled
information set h thus lp ni variables mi constraints
described appendix see appendix b dual lp expressed


fiaras dutech

variables yi
minimize

yi



subject
yi h

x

yi h

h ni



qk jk

h ei



ooi

yi h

x

r hh j

j ei



ki

yi





h denotes information set h belongs dual one free variable
yi every information set agent function h defined section appears mapping histories information sets dual program
one constraint per history agent thus dual mi variables ni constraints
note objective dual minimize yi primal lp
right hand side constraints except first one
theorem duality see appendix b applied primal lp
transformed dual says solutions value mathematically
means



x x

r hh j
qk jk xi h yi



hei

j ei

ki

thus value joint policy hxi qi expressed
x x


v hxi qi
r hh j
qk jk xi h
hei

j ei



ki



v hxi qi yi



due constraints primal lp holds
x


x x x
xi
yi h xi h
xi h
yi yi
aai

hni ooi



aai

constraint guarantees first term braces constraints
guarantee remaining terms inside braces right hand side
rewritten
x
x
x




p
xi yi
ooi yi

xi h yi h
yi h
aai

ooi

hni ai



x

xi h yi h

hei



p


hni xi h



yi h

x

ooi

x
yi h
xi h yi h
hei

h information set yi h shortcut writing yi h





fimathematical programming dec pomdps

combining equations get
x
x


xi h
yi h
yi h
ooi

hni



x

xi h



hei

yi h

r hh j

x



j ei



ki


qk jk



time introduce supplementary variables w information set variables usually called slack variables defined
x
yi h
yi h wi h h ni

ooi

yi h

x



r hh j

j ei



qk jk wi h

h ei



ki

shown section c appendix slack variables correspond concept
regret defined game theory regret history expresses loss accumulated
reward agent incurs acts according history rather according
history would belong optimal joint policy
thanks slack variables furthermore rewrite simply
x
x
xi h wi h
xi h wi h

hni

hei

x

xi h wi h



hhi

sum ni products ni size hi product sum
necessarily xi h wi h constrained nonnegative primal
dual respectively property strongly linked complementary slackness
optimality criterion linear programs see example vanderbei hence
equivalent
xi h wi h

h hi



back framework dec pomdps constraints written
pi h hh qi

h hi



sum solving following mathematical program would give optimal joint
policy dec pomdp constraints non linear thus prevent us
solving program directly linearization constraints called complementarity
constraints subject next section
variables
xi h wi h h hi
yi
maximize







fiaras dutech

subject
x

xi



aai

xi h

x

h ni oi



yi h wi h

h ni



xk jk wi h

h ei



xi h

aai

yi h

x

ooi

yi h

x

j ei

r hh j



ki

xi h wi h

h hi



xi h

h hi



wi h

h hi



yi





dealing complementarity constraints
section explains non linear constraints xi h wi h previous mathematical program turned sets linear constraints thus lead mixed
integer linear programming formulation solution dec pomdp
consider complementarity constraint ab variables b assume
lower bound values b let upper bounds values b
respectively ua ub let c variable complementarity constraint
ab separated following equivalent pair linear constraints
ua c



b ub c



words pair constraints satisfied surely case ab
easily verified c c set
constrained ua c less c b set
since b constrained ub c less case
ab
consider complementarity constraint xi h wi h non linear program wish separate constraint pair linear constraints
recall xi h represents weight h wi h represents regret h
first requirement convert constraint pair linear constraints lower
bound values two terms indeed case since xi h wi h
constrained non negative nlp next require upper bounds
weights histories regrets histories shown lemma upper
bound value xi h h upper bounds regrets histories
require calculus


fimathematical programming dec pomdps

policy pi agent holds
x
pi h oi



hei

therefore every reduced joint policy hq q qn xi holds

x
ok
qk jk
j ei ki



ki

since regret terminal history h agent given hq q qn defined
x



h q max
qk jk r hh j r hh j
h h

j ei ki

conclude upper bound ui h regret terminal history h ei
agent






ui h
ok
max max
r hh j min r hh j

ki

h h j ei

j ei

let us consider upper bounds regrets non terminal histories let
information set length agent let ei ei denote set terminal histories
agent first elements history set identical let h
history length agent let ei h ei denote set terminal histories
first elements history set identical h since policy
pi agent holds
x
pi h oi

h ei h

conclude upper bound ui h regret nonterminal history
h ni length agent




max
max
r
hh

j


min
min
r
hg
j


ui h li


h ei h j ei

gei h j ei


li oi



ok



ki

notice h terminal reduces
complementarity constraint xi h wi h separated pair linear
constraints variable bi h follows
xi h bi h
wi ui h bi h
bi h






fiaras dutech

variables
xi h wi h bi h h hi
yi
maximize





subject
x

xi



aai

xi h

x

h ni oi



h ni



r hh h x h w h

h e



r hh hi x h w h

h e



xi h

aai

yi h

x

yi h wi h

ooi

h

x

h e

h

x

h e

xi h bi h
wi h ui h bi h

h hi
h hi




xi h

h hi



wi h

h hi



bi h

h hi

yi





table milp agents mixed integer linear program derived game
theoretic considerations finds optimal stochastic joint policies dec pomdps
agents

program agents
combine policy constraints section constraints seen
policy best response sections maximization value
joint policy derive mixed integer linear program solution
optimal joint policy dec pomdp agents table details program
call milp agents
example formulation decentralized tiger agents
horizon found appendices section e
variables program vectors xi wi bi yi agent note
agent history h agent ui h denotes upper bound
regret history h


fimathematical programming dec pomdps

solution x w b milp agents consists following quantities
optimal joint policy x hx x may stochastic ii agent
history h hi wi h regret h given policy xi agent
iii agent information set yi value given
policy xi agent iv agent vector bi simply tells us
histories support xi history h agent bi h
support xi note replace objective function
without affecting program following
theorem given solution x w b milp agents x hx x
optimal joint policy sequence form
proof due policy constraints agent xi sequence form policy
agent due constraints yi contains values information sets
agent given xi due complementarity constraints xi best
response xi thus hx x nash equilibrium finally maximizing value
null information set agent effectively maximizing value hx x
thus hx x optimal joint policy

comparison milp presented table milp agents
constitutes particularly effective program term computation time finding agent optimal period joint policy much smaller program number
variables required milp exponential n number variables required
milp agents exponential represents major reduction size
lead improvement term computation time
program agents
number agents constraint non linear program
longer complementarity constraint
variables could linq
earized particular term ki xk jk constraint involves
many variables different agents linearize term restrict pure joint policies exploit combinatorial facts number
histories involved leads mixed linear program called milp n agents
depicted table
variables program milp n agents vectors xi wi bi yi
agent vector z following
theorem given solution x w b z milp n agents x hx x
xn pure period optimal joint policy sequence form
proof due policy constraints domain constraints agent
pure sequence form policy agent due constraints yi
contains values information sets agent given xi due complementarity
constraints xi best response xi thus x nash equilibrium
finally maximizing value null information set agent effectively
maximizing value x thus x optimal joint policy

xi



fiaras dutech

variables
xi h wi h bi h h hi
yi
z j j e
maximize





xi



xi h

h ni oi

subject
x

aai

xi h

x

aai

yi h

x

yi h wi h

h ni



ooi

yi h

x

r hh ji z j wi h h ei


oi
je
x

z hh j xi h
ok
j ei



ki

h ei
x

z j
oi
je




ii

xi h bi h

h hi

wi h ui h bi h

h hi

xi h

h ni

xi h
wi h

h ei

h hi

bi h

h hi

yi
z j

j e









table milp n agents mixed integer linear program derived game
theoretic considerations finds pure optimal joint policies dec pomdps
agents



fimathematical programming dec pomdps

compared milp table milp n agents roughly size
real valued variables variables precise p
milp
variable every terminal history every agent approximatively ii ai oi
integer variables milp n agents two variables
every terminal well
p
nonterminal history agent approximatively ii ai oi integer variables

summary

formulation solution dec pomdp application duality
theorem linear programs allow us formulate solution dec pomdp
solution kind milp agents milp kt variables
constraints thus smaller milp previous section still
milps quite large next section investigates heuristic ways speed
resolution

heuristics speeding mathematical programs
section focusses ways speed resolution milps presented
far two ideas exploited first prune set sequence form policies
removing histories provably part optimal joint policy
histories called locally extraneous give lower uppers bounds
objective function milps bounds sometimes used branch
bound method often used milp solvers finalize values integer variables
locally extraneous histories
locally extraneous history history required optimal joint policy
initial state dec pomdp could replaced co history
without affecting value joint policy co history history h agent
defined history agent identical h aspects except last
action ai b c co history c u b v b history c u b v c set
co histories history h shall denoted c h
formally history h hit length agent said locally extraneous
reduced joint histories length
every probability distribution set hi
exists history h c h
x


j r hh j r hh j


j hti

j denotes probability j
alternative definition follows history h hit length agent said
locally extraneous exists probability distribution set co histories
h reduced joint history j length holds
x
h r hh j r hh j

h c h



fiaras dutech

h denotes probability co history h
following theorem justifies incremental pruning locally extraneous histories
search optimal joint policies faster performed smaller
set possible support histories
theorem every optimal period joint policy p agent
terminal history h agent locally extraneous pi h exists
another period joint policy p optimal identical p respects
except pi h
proof let p period joint policy optimal assume
agent terminal history h agent locally extraneous pi h
exists least one co history h h
x


pi j r hh j r hh j


j ht


let q period policy agent identical pi respects except q h
pi h pi h q h shall q optimal holds
x

j ht


v hq pi v hpi pi


pi j r hh j q h r hh j pi h r hh j pi h

x

j ht




pi j r hh j q h pi h r hh j pi h

x

j ht




pi j r hh j pi h r hh j pi h

since q h pi h pi h therefore
x

j ht


v hq pi v hpi pi


pi j r hh j r hh j
due

hence p hq pi optimal period joint policy



one could wonder order extraneous histories pruned important
answer question following theorem shows many co histories
extraneous pruned order
value one pruned
pruning one change fact others still extraneous
theorem two co histories h h locally extraneous values
equal h locally extraneous
r hh j r hh j j hi

relatively c h h


fimathematical programming dec pomdps

proof let c denotes union c h c h immediately c h
c h c h c h h resp h locally extraneous means
exists probability distribution c h resp c h j

hi
x
h r hh j r hh j

h c h

x

h r hh j r hh j



h c h


eq expanded
x

h r hh j

h c h

h r hh j r hh j
h

gives
x
h
h r hh j
h c h

leading
x



x

h r hh j r hh j

h c h h



h h h r hh j h h r hh j

h c h h

two cases possible
h h case r hh j r hh j r hh j

r hh j r hh j r hh j j hi
h h case
x

h c h h

h h h
r hh j r hh j
h h



meaning even without h h still locally extraneous
h h h
probability distribution c h h
h h
x

h c h

h

h h h
h h h

h h
h h
h h
h h










fiaras dutech

order prune locally extraneous histories one must able identify histories
indeed two complementary ways
first method relies definition value history see section

r hh j hh j r hh j



therefore
hh j


j hi



true history h means every joint history length occurring
given history part priori probability thus h clearly
extraneous besides every co history h locally extraneous share
probabilities
second test needed locally extraneous histories verify
turn linear programing particular following linear program

variables j j hi
minimize





subject
x

j hti



j r hh j r hh j

x

h c h

j




j hti

following lemma

j


j hi



lemma exists solution linear program
h locally extraneous
proof let solution lp probability distribution
due constraints since minimizing due
hi
every co history h h
constraints every hi
x


j r hh j r hh j


j hti

therefore definition h locally extraneous



following procedure identifies locally extraneous terminal histories agents
proceed iterative pruning mainly motivated theorems
effectively removing extraneous histories procedure similar procedure
iterated elimination dominated strategies game osborne rubinstein
concept quite similar process policy elimination backward step
dynamic programming partially observable stochastic games hansen et al


fimathematical programming dec pomdps

step agent set hit ei let h denote set ii hit
joint history j h compute store value r j j joint
observation sequence probability j j
step agent history h hit reduced joint
hh j remove h h
history j hi

step agent history h hit follows c h hit
non empty check whether h locally extraneous setting solving
set h set c h
lp setting lp replace hi

set c h hit upon solving lp h found locally extraneous
remove h hit
step step history agent found locally extraneous go
step otherwise terminate procedure
procedure builds set hit agent set contains every terminal
history agent required finding optimal joint policy every
terminal history locally extraneous agent every history
hit hit locally extraneous reason reiterating step
history h agent found locally extraneous consequently removed
hit possible history agent previously locally
extraneous becomes due removal h hit hence order verify
case history reiterate step
besides step procedure prunes histories impossible given
model dec pomdp observation sequence observed
last pruning step taken order remove non terminal histories
lead extraneous terminal histories last step recursive starting histories
horizon remove histories hi non extraneous terminal histories
histories hi h extraneous ai oi
complexity pruning locally extraneous histories exponential
complexity joint history must examined compute value occurence
probability worst case linear program run every local history
order check extraneous experimentations needed see prunning
really interesting
cutting planes
previous heuristics aimed reducing search space linear programs
incidentally good impact time needed solve programs another option
directly aims reducing computation time use cutting planes cornuejols
cut dantzig special constraint identifies portion set
feasible solutions optimal solution provably lie cuts used
conjunction branch bounds mechanism reduce number possibles
combination integer variables examined solver
present two kinds cuts


fiaras dutech

variables j j h
maximize

x

r j j



je

subject
x





aa

j

x

j

j n



j h



aa

j

table pomdp linear program finds optimal policy pomdp
upper bound objective function
first cut propose upper bound pomdp cut value optimal
period joint policy given dec pomdp bounded value
vp optimal period policy pomdp derived dec pomdp
derived pomdp dec pomdp assuming centralized controller e
one agent joint actions
sequence form representation pomdp quite straightforward calling h
set tt ht joint histories lengths less equal n set h e nonterminal joint histories policy pomdp horizon sequence form function
q h
x
q

aa

q j

x

q j

j n



aa

value vp q sequence form policy q given
x
vp q
r j q j



je

thereby solution linear program table p
optimal policy
pomdp horizon optimal value pomdp je r j j
value v p optimal joint policy p hp p pn dec pomdp
bounded value vp q associated pomdp
complexity complexity finding upper bound linked complexity
solving pomdp showed papadimitriou tsitsiklis pspace
e require memory polynomial size leading possible
exponential complexity time experimentation help us decide
cases upper bound cut efficient


fimathematical programming dec pomdps

lower bound objective function
case dec pomdps non negative reward trivial value
period optimal policy bounded value horizon optimal
value general case take account lowest reward possible
compute lower bound say
x
r j z j v min min r

aa ss

je

v value optimal policy horizon reasoning leads
iterated computation dec pomdps longer longer horizon reminiscent
maa szer et al experiments tell worthwhile solve bigger
bigger dec pomdps take advantage lower bound better directly
tackle horizon without lower bound
complexity compute lower bound one required solve dec pomdp whith
horizon one step shorter current horizon complexity clearly
least exponential experiments value dec pomdp used
dec pomdp bigger horizon case computation time
augmented best time solve smaller dec pomdp
summary
pruning locally extraneous histories bounds objective function
practical use software solving milps presented pruning histories
means space policies used milp reduced formulation
milp depends combinatorial characteristics dec pomdp milp
must altered appendix
validity far cuts concerned alter solution found milps
solution milps still optimal solution dec pomdp extraneous histories pruned least one valid policy left solution step
history pruned co histories left besides
reduced set histories still used build optimal policy theroem
consequence milp build reduced set histories admit solution
solution one optimal joint policy
next section experimental allow us understand cases
heuristics introduced useful

experiments
mathematical programs heuristics designed tested four
classical found literature involving two agents
mainly compared computation time required solve dec pomdp mixed
integer linear programming methods computation time reported methods found
literature tested programs three agent randomly
designed


fiaras dutech


mabc
tiger
fire fighting
grid meeting
random pbs

ai






oi













n






table complexity used test beds

milp milp solved ilog cplex solver commercial set
java packages relies combination simplex branch bounds
methods fletcher software run intel p ghz gb
ram default configuration parameters mathematical programs different
combination heuristics evaluated pruning locally extraneous histories
lower bound cut upper bound cut respectively denoted loc low
tables come
non linear program nlp section evaluated solvers neos website http www neos mcs anl gov even thought
method guarantee optimal solution dec pomdp three solvers
used lancelot abbreviated lanc loqo snopt
tables report found literature following
dp stands dynamic programming hansen et al dp lpc improved
version dynamic programming policies compressed order fit
memory speed evaluation proposed boularias chaib draa
pbdp extension dynamic programming pruning guided knowledge
reachable belief states detailed work szer charpillet maa
heuristically guided forward search proposed szer et al generalized
improved version called gmaa developed oliehoek et al
selected evaluate detailed coming subsections
widely used evaluate dec pomdps literature
complexity term space size summarized table
multi access broadcast channel
several versions multi access broadcast channel mabc found
literature use description given hansen et al allows
formalized dec pomdp
mabc given two nodes computers required send messages
common channel given duration time time imagined
split discrete periods node buffer capacity one message
buffer empty period refilled certain probability next period
period one node send message nodes send message
period collision messages occurs neither message transmitted case
collision node intimated collision signal collision


fimathematical programming dec pomdps

signaling mechanism faulty case collision certain probability
send signal one nodes
interested pre allocating channel amongst two nodes given number
periods pre allocation consists giving channel one nodes period
function nodes information period nodes information period
consists sequence collision signals received till period
modeling dec pomdp obtain agent state actionsper agent observations per agent dec pomdp whose components follows
node agent
state described states buffers two nodes
state buffer empty full hence four states
empty empty empty full full empty full full
node two possible actions use channel dont use channel
period node may receive collision signal may node
two possible observations collision collision
initial state full full state transition function p
joint observation function g reward function r taken hansen et al
agents full buffers period use channel period
state unchanged next period agents full buffers
next period agent full buffer period uses channel
period buffer refilled certain probability next period
agent probability agent probability agents
empty buffers period irrespective actions take period buffers
get refilled probabilities agent agent
observation function g follows state period full full
joint action taken agents previous period use channel use channel
probability receive collision signal probability one
receives collision signal probability neither receives
collision signal state may period
joint action agents may taken previous period agents receive
collision signal
reward function r quite simple state period full empty
joint action taken use channel dont use channel state period
empty full joint action taken dont use channel use channel reward
combination state joint action reward
evaluated three different horizons
respective optimal policies value
detailed table horizon value computation
time best policy found given
milp compares favorably classical except
gmaa far better horizon horizon roughly within


fiaras dutech

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
low
milp
cplex

milp
cplex
loc
milp
cplex
loc low
milp
cplex
loc
milp
cplex
nlp
snopt
nlp
lanc
nlp
loqo

family
dp
dyn prog
dp lpc
dyn prog
pbdp
dyn prog
maa
fw search
gmaa
fw search

horizon
value
time















value
time











horizon
value
time



















value
time











horizon
value
time














value
time








table mabc value computation time seconds solution
computed several methods best highlighted
appropriate time shows first time used run heuristics global
time format heuristic total time means timeout
indicates fit memory indicates
tested

order magnitude milp pertinent heuristics expected apart
simplest setting horizon nlp resolution optimal policy
dec pomdp computation time lower methods among
milp methods milp better milp even best heuristics horizon
size increases heuristics way milps
able cope size table shows mabc
pruning extraneous histories loc heuristic good method
investigation revealed heuristics proved locally extraneous
far cutting bounds concerned dont seem useful first horizon
necessary milp solution horizon
one must mind one optimal policy horizon
multi agent tiger
explained section multi agent tiger tiger introduced
nair et al general description ob

fimathematical programming dec pomdps

joint action
listen listen
listen listen
listen listen
listen listen
listen listen
listen listen
listen listen
listen listen


state
left
left
left
left
right
right
right
right


joint observation
noise left noise left
noise left noise right
noise right noise left
noise right noise right
noise left noise left
noise left noise right
noise right noise left
noise right noise right


probability










table joint observation function g tiger
tain agent state actions per agent observations per agent dec pomdp whose
elements follows
person agent agent dec pomdp
state described location tiger thus consists
two states left tiger behind left door right tiger behind right
door
agents set actions consists three actions open left open left door
open right open right door listen listen
agents set observations consists two observations noise left noise coming
left door noise right noise coming right door
initial state equi probability distribution state transition function p
joint observation function g reward function r taken nair
et al p quite simple one agents opens door period state
next period set back agents listen period state
process unchanged next period g given table quite simple
nair et al describes two reward functions called b
report reward function given table behavior
similar reward functions optimal value
horizons respectively
horizon dynamic programming forward search methods generally better
mathematical programs contrary horizon computation time milp low heuristic significatively better even
gmaa unlike mabc pruning extraneous histories improve methods
milp quite understandable deeper investigations showed
extraneous histories lower cutting bounds proves efficient
seen kind heuristic search best policy directly set policies


fiaras dutech

joint action
open right open right
open left open left
open right open left
open left open right
listen listen
listen open right
open right listen
listen open left
open left listen

left










right










table reward function tiger

gmaa set combination histories may explain good behavior
milp low
must noted approximate methods nlp
depicted memory bound dynamic programming
seuken zilberstein able optimal solution
methods nlp quite fast sometimes accurate
fire fighters
fire fighters introduced benchmark oliehoek
et al team n fire fighters extinguish fires row nh
houses
state house given integer parameter called fire level f
takes discrete value fire nf fire maximum severity every time
step agent move one house two agents house
extinguish existing fire house agent alone fire level lowered
probability neighbor house burning probability otherwise
burning house fireman present increase fire level f one point
probability neighbor house burning probability otherwise
unattended non burning house catch fire probability neighbor house
burning action agents receive reward f house still
burning agent observe flames location probability
depends fire level f f otherwise start
agents outside houses fire level houses sampled
uniform distribution
model following characteristics
na agents nh actions nf possible informations

h
states nnf h possible states burning houses
nnf h na n
n


h
different ways distribute na fire fighters houses
na n
na
example agents houses levels fire lead states


fimathematical programming dec pomdps

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
low
milp
cplex

milp
cplex
loc
milp
cplex
loc low
milp
cplex
loc
milp
cplex
nlp
snopt
nlp
lanc
nlp
loqo

family
dp
dyn prog
dp lpc
dyn prog
pbdp
dyn prog
maa
fw search
gmaa
fw search

horizon
value
time















value
time











horizon
value
time














value
time










table tiger value computation time seconds solution
computed several methods best highlighted
appropriate time shows first time used run heuristics
global time format heuristic total time means timeout
indicates fit memory indicates tested



fiaras dutech

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
nlp
snopt
nlp
lanc
nlp
loqo

family
maa
fw search
gmaa
fw search

horizon
value
time









value
time




horizon
value
time








value
time



table fire fighting value computation time seconds solution computed several methods best highlighted
means timeout maa gmaa value parenthesis
taken work oliehoek et al optimal
different optimal values

possible use information joint action reduce number state
needed transition function simply nnf h meaning states agents
houses levels fire
transition observation reward functions easily derived description
dynamic programming methods tested
formulation quite horizon value optimal policy given oliehoek
et al differs value found milp whereas
methods supposed exact might come slight differences
respective formulation horizon oliehoek et al report
optimal value
milp methods clearly outperformed maa gmaa
nlp methods give optimal solution horizon better term
computation time might nlp able optimal policies horizon
setting differs work oliehoek et al able check
policy found really optimal main reason superiority forward
search method lies fact admits many many optimal policies
value fact horizon milp methods optimal policy quite
quickly around milp branch bound must evaluate
potential policies knowing indeed found optimal policy forward
search methods stop nearly soon hit one optimal solution
heuristics reported improve performance milp
take away computation time thus worse


fimathematical programming dec pomdps

meeting grid
called meeting grid deals two agents want meet stay
together grid world introduced work bernstein hansen
zilberstein
two robots navigating two two grid world
obstacles robot sense whether walls left right
goal robots spend much time possible square actions
move left right stay square robot attempts
move open square goes intended direction probability
otherwise randomly goes another direction stays square
move wall staying square robots interfere
cannot sense reward agents share square
otherwise initial state distribution deterministic placing robots
upper left corner grid
modelled dec pomdp
agents one actions observations wall left wall
right
states since robot squares time
transition observation reward functions easily derived description
dynamic programming methods tested
formulation quite intrinsically complex
solved horizon optimal value found method differ
value reported oliehoek et al whereas found optimal values
horizon report optimal values
roughly pattern
maa gmaa quicker milp time milp able
optimal solution horizon nlp methods give quite good slower
gmaa numerous optimal policies milp methods
able detect policy found quickly indeed optimal
heuristics reported improve performance
milp take away computation time thus worse
random agent
test agents used randomly generated decpomdps state transition function joint observation function reward
functions randomly generated dec pomdps actions observations
per agent states rewards randomly generated integers range
complexity family quite similar complexity mabc
see section


fiaras dutech

resolution method
program
solver
heuristics
milp
cplex
milp
cplex
nlp
snopt
nlp
lanc
nlp
loqo

family
maa
fw search
gmaa
fw search

horizon
value time










value time





horizon
value time









value time




table meeting grid value computation time seconds
solution computed several methods best
highlighted means timeout maa gmaa value
parenthesis taken work oliehoek et al
optimal different optimal values

program
milp
milp

least time secs



time secs



average



std deviation



table times taken milp milp agent random horizon

order assess real complexity random first tested
two agent version horizon averaged runs
programs given table compared mabc seemed
comparable complexity random proves easier solve vs
number variable relatively small weight much
resolution time milp thus faster
three agent horizon given table
averaged runs even though size search space smaller case
agents horizon policies whereas
agents horizon possible policies agent seems
difficult solve demonstrating one big issue policy coordination
heuristics bring significative improvement resolution time milp predicted
milp n efficient given completeness


fimathematical programming dec pomdps

program
milp
milp low
milp n

least time secs




time secs




average




std deviation




table times taken milp milp n agent random horizon

discussion
organized discussion two parts first part analyze
offer explanations behavior usefulness heuristics
second part explicitely address important questions
analysis
appears milp methods better alternative dynamic programming methods solving dec pomdps globally generally clearly outperformed forward search methods structure thus characteristics
big influence efficiency milp methods whereas seems
behavior gmaa terms computation time quite correlated
complexity size action observation spaces milp methods seem
sometimes less correlated complexity case mabc many
extraneous histories pruned tiger special structure
outperform gmaa contrary many optimal policies exists forward
search methods gmaa clearly better choice finally non linear programs
even though guarantee optimal solution generally good alternative
sometimes able good solution computation time often
better gmaa might prove useful approximate heuristic driven forward
searches
computational record two agent programs shows milp agents
slower milp horizon grows two reasons sluggishness
milp agents may attributed time taken branch bound bb
method solve milp inversely proportional number variables
milp milp agents many variables milp event hough
total number variables exponentially less milp first reason
secondly milp agents complicated program milp many
constraints milp milp simple program concerned finding subset
given set addition finding weights histories milp finds weights
terminal joint histories extra superfluous quantity forced
hand milp agents takes much circuitous route finding many
superfluous quantities milp addition weights histories milp agents
finds supports policies regrets histories values information sets thus


fiaras dutech



heuristic

mabc

loc
low

loc
low

loc

tiger

meeting

horizon
time pruned









horizon
time pruned










horizon
time pruned









horizon
time pruned





table computation time heuristics loc heuristics give computation time seconds number locally extraneous histories pruned
total number histories agent denotes cases one
additional history prunned second agent low heuristic
computation time given

relaxation milp agents takes longer solve relaxation milp
second reason slowness bb method solves milp agents
bigger namely fire fighters meeting grid horizon
stays small milp agents compete milp slightly lower size
complexity grows ai oi whereas grows ai oi milp
small difference hold long number integer variables quickly lessens
efficiency milp agents
far heuristic concerned proved invaluable mabc
tiger useless others case mabc heuristics helpful
prune large number extraneous heuristics ultimately combination
upper bound cut efficient horizon grows case
tiger although extraneous histories found lower bound cut heuristic
milp leads quickest solving horizon
heuristics burden greedy computation
time speed resolution example grid meeting time
taken prune extraneous histories bigger time saved solving
added value heuristics depends nature
depicted table right able predict usefulness without
trying
emphasize given lie limit possible solve
exact manner given memory computer used resolution especially
terms horizon furthermore number agent increases length
horizon must decreased still solvable


fimathematical programming dec pomdps

questions
mathematical programing presented raises different questions
explicitly addressed questions appears important us
q sequence form entirely doomed exponential
complexity
number sequence form joint policies grows doubly exponentially horizon number agents sequence form seems doomed even compared
dynamic programming doubly exponential worst cases indeed
arguments must taken consideration
exponential number individual histories need evaluated joint
part sequence form left milp solver every computation done
particular history computing value checking extraneous greater
reusability computations done entire policies history shared many
joint policies individual policy way sequence form allows us work
reusable part policies without work directly world distributions
set joint policies
milps derived sequence form dec pomdps need memory size
grows exponentially horizon number agents obviously
complexity quickly overwhelming case every exact method
far shown experiments milp derived sequence form
compares quite well dynamic programming even outperformed forward methods
gmaa
q milp sometimes take little time optimal joint
policy compared existing
despite complexity milp three factors contribute relative
efficiency milp
first efficiency linear programming tools solving milp
bb method solves sequence linear programs simplex
lps relaxation milp theory simplex requires
worst case exponential number steps size lp solving lp
well known practice usually solves lp polynomial number
steps size lp since size relaxation milp exponential
horizon means roughly speaking time taken solve relaxation
milp exponential horizon whereas doubly exponential
methods
second factor sparsity matrix coefficients constraints
milp sparsity matrix formed coefficients constraints
statement must qualified worst case time requirement demonstrated
variants simplex demonstrated basic version simplex




fiaras dutech

lp determines practice rate pivoting
simplex solves lp applies lemkes context
lcp sparser matrix lesser time required perform elementary
pivoting row operations involved simplex lesser space
required model lp
third factor fact supplement milp cuts computational
experience clearly shows speeds computations first two
factors related solving relaxation milp e lp third factor
impact bb method upper bound cut identifies additional
terminating condition bb method thus enabling terminate earlier
absence condition lower bound cut attempts shorten list
active subproblems lps bb method solves sequentially due
cut bb method potentially lesser number lps solve note
inserting lower bound cut emulating forward search properties

q know milp solver ilogs cplex experiments
reason speedup
clearly would slower even sometime slower classical dynamic
programming used another program solving milps experimented milps solvers neos website indeed
slow true cplex solver used experiments quite optimized
nevertheless exactly one points wanted experiment
one advantages formulating dec pomdp milp possibility use
fact mixed integer linear programs important industrial world
optimized solvers exist
formulate dec pomdp milp mostly

q main contribution
stated earlier current dec pomdps largely inspired pomdps main contribution pursue entirely different
e mixed integer linear programming learned lot
dec pomdps pro con mathematical programming
lead formulation
designing first drawn attention representation policy namely sequence form policy introduced koller megiddo
von stengel sequence form policy compact representation
policy agent afford compact representation set policies
agent
proposed finite horizon dec pomdps mathematical
programming precise milps mdp domain


fimathematical programming dec pomdps

mathematical programming long used solving infinite horizon case
instance infinite horizon mdp solved linear program depenoux
recently mathematical programming directed infinite horizon pomdps
dec pomdps thus infinite horizon dec mdp state transition independence solved milp petrik zilberstein infinite horizon
pomdp dec pomdp solved local optima nonlinear program amato bernstein zilberstein b finite horizon case much different
character infinite horizon case dealt dynamic programming
stated earlier whereas dynamic programming quite successful finite horizon
mdps pomdps less finite horizon dec pomdps
contrast game theory mathematical programming successfully directed
games finite horizon lemkes two player normal form games
govindan wilson n player normal form games koller megiddo
von stengel internally uses lemkes two player extensive
form games finite horizon games
remained way appropriate mathematical programming
solving finite horizon case pomdp dec pomdp domain work done
precisely incidently solving kind n player normal form games throughout shown mathematical programming
particular integer programming applied solving finite horizon decpomdps easy see presented yields linear program
solving finite horizon pomdp additionally computational experience
indicates finite horizon dec pomdps mathematical programming may
better faster dynamic programming shown well entrenched
dynamic programming heuristic pruning redundant extraneous objects
case histories integrated mathematical programming
hence main contribution presents first time alternative solving finite horizon pomdps dec pomdps milps
q mathematical programming presented something dead end
question bit controversial short answer question could
small yes true every looks exact optimal solutions
dec pomdps whether grounded dynamic programming forward search
mathematical programming complexity exact solution
untractable still improved
longer answer mitigated especially light recent advances made
dynamic programming forward search one crucial point sequenceform dec pomdps pruning extraneous histories recent work oliehoek
whiteson spaan shown clusters histories equivalent
way could reduce nomber constraints milps amato
dibangoye zilberstein improves speed dynamic programming
operator could help finding extraneous histories least work


fiaras dutech

still required stating every aspect sequence form dec pomdps
studied
turn even longer answer consider long horizon case given exact
including ones presented tackle horizons less
long horizon mean anything upwards time periods long horizon case
required conceive possibly sub optimal joint policy given horizon
determine upper bound loss value incurred joint policy instead
optimal joint policy
current trend long horizon case memory bounded memory
bounded dynamic programming mbdp seuken zilberstein
main exponent backward induction dp
hansen et al attempts run limited amount
space order unlike dp prunes even non extraneous e nondominated policy trees iteration thus iteration retains
pre determined number trees variants used
joint policy mabc tiger box pushing long
horizons order thousands time periods
mbdp provide upper bound loss value bounded dp bdp
presented amato carlin zilberstein c give
upper bound however interesting dec pomdp tiger
mbdp finds much better joint policy bdp
meaningful way introduce notion memory boundedness
fix priori upper bound size concerned mathematical program
presents sorts difficulties main difficulty seems need represent
policy long horizon limited space mbdp solves
may termed recursive representation recursive representation
causes mbdp take long time evaluate joint policy allow
represent long horizon joint policy limited space context
mathematical programming would change policy constraints
way long horizon policy represented system consisting limited
number linear equations linear inequalities besides policy constraints
constraints presented programs would accordingly transfigured
evident us transfiguration constraints possible
hand infinite horizon case seems promising candidate adapt
mathematical programming already applied success
solving infinite horizon dec pomdps amato et al computational experience mathematical programming shows better finds higher
quality solutions lesser time dynamic programming bernstein et al
szer charpillet
nevertheless two inter related shortcomings first
finds joint controller e infinite horizon joint policy fixed size
optimal size second much graver first fixed size finds locally optimal
joint controller guarantee finding optimal joint controller
program presented work amato et al non convex


fimathematical programming dec pomdps

nonlinear program nlp nlp finds fixed size joint controller canonical form
e form finite state machine believe shortcomings
removed conceiving mathematical program specifically mixed integer linear
program finds joint controller sequence form stated earlier main
challenge regard therefore identification sequence form infinite
horizon policy fact may sequence form characterization infinite
horizon policy obtained could used conceiving program long horizon
undiscounted reward case well
q help achieve designing artificial autonomous agents
first sight work direct immediate applied benefits
purpose building artificial intelligent agents understanding intelligence works
even limited field multi agent contributions theoretical
level practical one
real artificial multi agent systems indeed modeled dec pomdps even
make use communication common knowledge common social law real
systems would likely made large number states actions observations require
solutions large horizon mathematical programming practically
useless setting limited dec pomdps small size
simpler far trivial solve explicitly take account
characteristics real systems exist works take advantage communications
xuan lesser zilberstein ghavamzadeh mahadevan existing
independencies system wu durfee becker zilberstein lesser goldman
focus interaction agents thomas bourjot chevrier
said answering previous questions rely approximate solutions etc
intention facilitate use adaptation
concepts used work knowledge structure optimal solution
dec pomdp end decided describe milp programs
importantly derived programs making use properties
optimal dec pomdp solutions
truly autonomous agents require adapt unforeseen situations
work dedicated seems easy argue contribute
much end hand learning dec pomdps never
really addressed except fringe work particular settings scherrer charpillet ghavamzadeh mahadevan buffet dutech charpillet fact
even simple pomdps learning difficult task singh jaakkola jordan
currently promising deals learning predictive state
representation psr pomdp singh littman jong pardoe stone james
singh mccracken bowling making due allowance fundamental
differences functional role psr histories notice psr histories quite similar structure early say might trying
learn useful histories dec pomdp could take inspiration way
right psrs learned pomdps



fiaras dutech

conclusion
designed investigated exact solving decentralized partially observable markov decision processes finite horizon dec pomdps main contribution use sequence form policies sets histories
order reformulate dec pomdp non linear programming nlp
presented two different approaches linearize nlp order global
optimal solutions dec pomdps first combinatorial
properties optimal policies dec pomdps second one relies concepts
borrowed field game theory lead formulating dec pomdps
mixed integer linear programming milps several heuristics speeding
resolution milps make another important contribution work
experimental validation mathematical programming designed
work conducted classical dec pomdp found literature
experiments expected milp methods outperform classical dynamic
programming general less efficient costly
forward search methods gmaa especially case dec pomdp admits
many optimal policies nevertheless according nature milp methods
sometimes greatly outperform gmaa tiger
clear exact resolution dec pomdps scale size
length horizon designing exact methods useful order
develop improve approximate methods see least three directions
work contribute one direction could take advantage large
literature finding approximate solutions milps adapt
milps formulated dec pomdps another direction would use knowledge
gained work derive improved heuristics guiding existing approximate existing
methods dec pomdps example work seuken zilberstein
order limit memory resources used resolution prune space
policies consider work could help better estimation
policies important kept search space one direction
currently investigating adapt dec pomdps infinite length
looking yet another representation would allow seen milps
importantly work participates better understanding dec pomdps
analyzed understood key characteristics nature optimal policies order
design milps presented knowledge useful work
dealing dec pomdps even pomdps experimentations given
interesting insights nature tested term existence
extraneous histories number optimal policies insights might first
step toward taxonomy dec pomdps

appendix non convex non linear program
simplest example section aims showing non linear program
nlp expressed table non convex


fimathematical programming dec pomdps

let us consider example two agents one possible actions b
want solve horizon decision set possible joint histories
ha ai ha bi hb ai hb bi nlp solve
variables x x b x x
maximize

r ha ai x x r ha bi x x b



r hb ai x b x r hb bi x b x b
subject
x x b
x x b
x

x b

x

x b

matrix formulation objective
x following kind

c
e
c
c e
f

function eq would xt c x c


f







x
x b

x
x
x b



eigen value vector v v v v v straightforward
eigen value v v v v c v v v v
matrix c hessian objective function positive definite thus objective
function convex

appendix b linear program duality
every linear program lp converse linear program called dual first lp
called primal distinguish dual primal maximizes quantity
dual minimizes quantity n variables constraints primal
variables n constraints dual consider following primal lp
variables x n
maximize

n
x

c x



subject
n
x

j x b j

j



x

n



fiaras dutech

primal lp one variable x n data lp consists
numbers c n numbers b j j numbers
j n j lp thus n variables
constraints dual lp following lp
variables j j


minimize


x

b j j

j

subject



x

j j c

n

j

j

j

dual lp one variable j j j variable free
variable allowed take value r dual lp variables n
constraints
theorem linear programming duality follows
theorem b luenberger primal lp dual lp finite optimal
solution corresponding values objective functions
equal
applying theorem primal dual pair given holds
n
x

c x


x

b j j

j



x denotes optimal solution primal denotes optimal solution
dual
theorem complementary slackness follows
theorem b vanderbei suppose x feasible primal linear program
feasible dual let w wm denote corresponding primal slack variables
let z zn denote corresponding dual slack variables x optimal
respective
xj zj

j n

wi yi




fimathematical programming dec pomdps

appendix c regret dec pomdps
value information set ii agent reduced joint policy q
denoted q defined
x
q max
r hh j q j

h

j ei

terminal information set non terminal
x
h q
q max
h



ooi

regret history h agent reduced joint policy q
denoted h q defined
x
h q h q
r hh j q j


j hi

h terminal h non terminal
h q h q

x

h q



ooi

concept regret agent independant policy agent
useful looking optimal policy optimal value known
thus easier manipulate optimal value policy

appendix program changes due optimizations
pruning locally globally extraneous histories reduces size search space
mathematical programs constraints programs depend size
search space must alter constraints
let denote superscript sets actually used program example ei
actual set terminal histories agent pruned extraneous histories

programs milp table milp n agents table rely fact
number histories given length support pure policy agent fixed
equal oi may case pruned sets following changes
made
constraint milp milp n agents
x

z j
oi
je

ii

must replaced
x

z j


ii

je



oi



fiaras dutech

set constraints milp milp n agents
x

z hh j

j ei



ok xi h

h ei



ok xi h

h ei

ki

must replaced
x

z hh j



ki

j ei

set constraints milp n agents
yi h

x

r hh ji z j wi h
oi

h ei

je

must replaced
yi h

x

r hh ji z j wi h
oi

h ei



je

appendix e example tiger
example derived decentralized tiger tiger described
section two agents actions al ar ao observations ol
consider horizon
terminal histories agent ao ol ao ao ol al ao ol ar ao ao
ao al ao ar al ol ao al ol al al ol ar al ao al al al ar ar ol ao ar ol al
ar ol ar ar ao ar al ar ar
thus joint histories agents hao ol ao ao ol ao hao ol ao ao ol al
hao ol ao ao ol ar har ar ar ar
e policy constraints
policy constraints horizon one agent tiger would
variables x every history
x ao x al x ar
x ao x ao ol ao x ao ol al x ao ol ar
x ao x ao ao x ao al x ao ar
x al x al ol ao x al ol al x al ol ar
x al x al ao x al al x al ar
x ar x ar ol ao x ar ol al x ar ol ar
x ar x ar ao x ar al x ar ar


fimathematical programming dec pomdps

x ao

x al

x ar

x ao ol ao x ao ol al x ao ol ar
x ao ao x ao al x ao ar
x al ol ao

x al ol al

x al ol ar

x al ao x al al x al ar
x ar ol ao x ar ol al x ar ol ar
x ar ao x ar al x ar ar
e non linear program tiger
non linear program finding optimal sequence form policy tiger
horizon would
variables xi every history agent

maximize

r hao ol ao ao ol ao x ao ol ao x ao ol ao
r hao ol ao ao ol al x ao ol ao x ao ol al
r hao ol ao ao ol ar x ao ol ao x ao ol ar


subject
x ao x al x ar
x ao x ao ol ao x ao ol al x ao ol ar
x ao x ao ao x ao al x ao ar
x al x al ol ao x al ol al x al ol ar
x al x al ao x al al x al ar
x ar x ar ol ao x ar ol al x ar ol ar
x ar x ar ao x ar al x ar ar

x ao x al x ar
x ao x ao ol ao x ao ol al x ao ol ar
x ao x ao ao x ao al x ao ar
x al x al ol ao x al ol al x al ol ar
x al x al ao x al al x al ar
x ar x ar ol ao x ar ol al x ar ol ar
x ar x ar ao x ar al x ar ar


fiaras dutech

x ao

x al

x ar

x ao ol ao x ao ol al x ao ol ar
x ao ao x ao al x ao ar
x al ol ao

x al ol al

x al ol ar

x al ao x al al x al ar
x ar ol ao x ar ol al x ar ol ar
x ar ao x ar al x ar ar

x ao

x al

x ar

x ao ol ao x ao ol al x ao ol ar
x ao ao x ao al x ao ar
x al ol ao

x al ol al

x al ol ar

x al ao x al al x al ar
x ar ol ao x ar ol al x ar ol ar
x ar ao x ar al x ar ar

e milp tiger
milp horizon agents tiger would
variables
xi h every history agent
z j every terminal joint history

maximize

r hao ol ao ao ol ao z hao ol ao ao ol ao
r hao ol ao ao ol al z hao ol ao ao ol al
r hao ol ao ao ol ar z hao ol ao ao ol ar



fimathematical programming dec pomdps

subject
x ao x al x ar
x ao x ao ol ao x ao ol al x ao ol ar
x ao x ao ao x ao al x ao ar

x ao x al x ar
x ao x ao ol ao x ao ol al x ao ol ar
x ao x ao ao x ao al x ao ar

z hao ol ao ao ol ao z hao ol ao ao ol al z hao ol ao ao ol ar x ao ol ao
z hao ol ao ao ol ao z hao ol al ao ol ao z hao ol ar ao ol ao x ao ol ao
z hao ol al ao ol ao z hao ol al ao ol al z hao ol al ao ol ar x ao ol al
z hao ol ao ao ol al z hao ol al ao ol al z hao ol ar ao ol al x ao ol al


x ao

x al

x ar

x ao ol ao

x ao ol al

x ao ol ar

x ao ao

x ao al

x ao ar


x ao

x al

x ar

x ao ol ao

x ao ol al

x ao ol ar

x ao ao

x ao al

x ao ar


z hao ol ao ao ol ao z hao ol ao ao ol al z hao ol ao ao ol ar
z hao ol al ao ol ao z hao ol al ao ol al z hao ol al ao ol ar

e milp agents tiger
milp agents horizon agents tiger would
variables
xi h wi h bi h every history agent
yi agent every information set
maximize





fiaras dutech

subject

x ao x al x ar
x ao x ao ol ao x ao ol al x ao ol ar
x ao x ao ao x ao al x ao ar

x ao x al x ar
x ao x ao ol ao x ao ol al x ao ol ar
x ao x ao ao x ao al x ao ar

ao ol ao w ao
al ol al w al
ar ol ar w ar
ao ol ao w ao
al ol al w al
ar ol ar w ar

ao ol r hao ol ao ao ol ao x ao ol ao
r hao ol ao ao ol al x ao ol al
r hao ol ao ao ol ar x ao ol ar
r hao ol ao al ol ao x al ol ao
r hao ol ao al ol al x al ol al
r hao ol ao al ol ar x al ol ar
w ao ol ao

ao ol r hao ol al ao ol ao x ao ol ao
r hao ol al ao ol al x ao ol al
r hao ol al ao ol ar x ao ol ar
r hao ol al al ol ao x al ol ao
r hao ol al al ol al x al ol al
r hao ol al al ol ar x al ol ar
w ao ol al


fimathematical programming dec pomdps


ar r har ar ao ol ao x ao ol ao
r har ar ao ol al x ao ol al
r har ar ao ol ar x ao ol ar
r har ar al ol ao x al ol ao
r har ar al ol al x al ol al
r har ar al ol ar x al ol ar
w ar ar
ao ol r hao ol ao ao ol ao x ao ol ao
r hao ol al ao ol ao x ao ol al
r hao ol ar ao ol ao x ao ol ar
r hal ol ao ao ol ao x al ol ao
r hal ol al ao ol ao x al ol al
r hal ol ar ao ol ao x al ol ar
w ao ol ao
ao ol r hao ol ao ao ol al x ao ol ao
r hao ol al ao ol al x ao ol al
r hao ol ar ao ol al x ao ol ar
r hal ol ao ao ol al x al ol ao
r hal ol al ao ol al x al ol al
r hal ol ar ao ol al x al ol ar
w ao ol al


x ao b ao

x al b al

x ar b ar

x ao ol ao b ao ol ao

x ao ol al b ao ol al

x ao ol ar b ao ol ar



w ao u ao b ao

w al u al b al

w ar u ar b ar

w ao ol ao u ao ol ao b ao ol ao

w ao ol al u ao ol al b ao ol al

w ao ol ar u ao ol ar b ao ol ar




fiaras dutech

x ao
x ao ol ao

x al
x ao ol al

x ar
x ao ol ar


w ao
w ao ol ao

w al
w ao ol al

w ar
w ao ol ar


b ao
b ao ol ao

b al
b ao ol al



ao ol ao

agent



b ar
b ao ol ar

fimathematical programming dec pomdps

references
amato c bernstein zilberstein optimizing memory bounded controllers decentralized pomdps proc twenty third conf uncertainty
artificial intelligence uai
amato c bernstein zilberstein b solving pomdps quadratically
constrained linear programs proc twentieth int joint conf artificial
intelligence ijcai
amato c carlin zilberstein c bounded dynamic programming
decentralized pomdps proc workshop multi agent sequential decision
making uncertain domains msdm aamas
amato c dibangoye j zilberstein incremental policy generation finitehorizon dec pomdps proc nineteenth int conf automated
scheduling icaps
anderson b moore j time varying feedback laws decentralized control
nineteenth ieee conference decision control including symposium
adaptive processes
becker r zilberstein lesser v goldman c solving transition independent
decentralized markov decision processes journal artificial intelligence

bellman r dynamic programming princeton university press princeton newjersey
bernstein givan r immerman n zilberstein complexity decentralized control markov decision processes mathematics operations

bernstein hansen e zilberstein bounded policy iteration
decentralized pomdps proc nineteenth int joint conf artificial
intelligence ijcai pp
boularias chaib draa b exact dynamic programming decentralized
pomdps lossless policy compression proc int conf automated
scheduling icaps
boutilier c learning coordination multiagent decision processes
proceedings th conference theoretical aspects rationality knowledge tark de zeeuwse stromen nederlands
buffet dutech charpillet f shaping multi agent systems gradient reinforcement learning autonomous agent multi agent system journal
aamasj


fiaras dutech

cassandra kaelbling l littman acting optimally partially observable
stochastic domains proc th nat conf artificial intelligence aaai
chades scherrer b charpillet f heuristic solving
decentralized pomdp assessment pursuit proc acm
symposium applied computing pp
cornuejols g valid inequalities mixed integer linear programs mathematical
programming b
dantzig g b significance solving linear programming
integer variables econometrica
depenoux f probabilistic production inventory management
science
diwekar u introduction applied optimization edition springer
drenick r multilinear programming duality theories journal optimization
theory applications
fletcher r practical methods optimization john wiley sons york
ghavamzadeh mahadevan learning communicate act cooperative multiagent systems hierarchical reinforcement learning proc rd
int joint conf autonomous agents multi agent systems aamas
govindan wilson r global newton method compute nash equilibria
journal economic theory
hansen e bernstein zilberstein dynamic programming partially
observable stochastic games proc nineteenth national conference artificial intelligence aaai
horst r tuy h global optimization deterministic approaches rd edition
springer
james singh learning discovery predictive state representations
dynamical systems reset proc twenty first int conf machine
learning icml
kaelbling l littman cassandra acting partially
observable stochastic domains artificial intelligence
koller megiddo n von stengel b fast finding randomized
strategies game trees proceedings th acm symposium theory
computing stoc pp
koller megiddo n finding mixed strategies small supports extensive
form games international journal game theory


fimathematical programming dec pomdps

lemke c bimatrix equilibrium points mathematical programming management science
luenberger linear nonlinear programming addison wesley publishing
company reading massachussetts
mccracken p bowling h online discovery learning predictive state
representations advances neural information processing systems nips
nair r tambe yokoo pynadath marsella taming decentralized
pomdps towards efficient policy computation multiagent setting proc
int joint conference artificial intelligence ijcai
oliehoek f spaan vlassis n optimal approximate q value functions
decentralized pomdps journal artificial intelligence jair

oliehoek f whiteson spaan lossless clustering histories decentralized pomdps proc international joint conference autonomous
agents multi agent systems pp
osborne j rubinstein course game theory mit press
cambridge mass
papadimitriou c h steiglitz k combinatorial optimization
complexity dover publications
papadimitriou c h tsitsiklis j complexity markov decision processes mathematics operations
parsons wooldridge game theory decision theory multi agent
systems autonomous agents multi agent systems jaamas
petrik zilberstein average reward decentralized markov decision processes proc twentieth int joint conf artificial intelligence ijcai

petrik zilberstein bilinear programming multiagent
journal artificial intelligence jair
puterman markov decision processes discrete stochastic dynamic programming john wiley sons inc york ny
pynadath tambe communicative multiagent team decision analyzing teamwork theories journal artificial intelligence

radner r application linear programming team decision
management science
russell norvig p artificial intelligence modern prentice hall


fiaras dutech

sandholm multiagent systems chap distributed rational decision making pp
mit press ed g weiss
sandholm gilpin conitzer v mixed integer programming methods
finding nash equilibria proc national conference artificial intelligence
aaai
scherrer b charpillet f cooperative co learning model
solving multi agent reinforcement proc ieee int conf
tools artificial intelligence ictai
seuken zilberstein memory bounded dynamic programming decpomdps proc twentieth int joint conf artificial intelligence ijcai
singh jaakkola jordan learning without state estimation partially
observable markovian decision processes proceedings eleventh international
conference machine learning
singh littman jong n pardoe stone p learning predictive state
representations proc twentieth int conf machine learning icml
szer charpillet f point dynamic programming dec pomdps
proc twenty first national conf artificial intelligence aaai
szer charpillet f zilberstein maa heuristic search
solving decentralized pomdps proc twenty first conf uncertainty
artificial intelligence uai pp
thomas v bourjot c chevrier v interac dec mdp towards use
interactions dec mdp proc third int joint conf autonomous
agents multi agent systems aamas york usa pp
vanderbei r j linear programming foundations extensions rd edition
springer
von stengel b handbook game theory vol chap computing equilibria
two person games pp north holland amsterdam
wu j durfee e h mixed integer linear programming transitionindependent decentralized mdps proc fifth int joint conf autonomous
agents multiagent systems aamas pp york ny usa
acm
xuan p lesser v zilberstein communication multi agent markov
decision processes proc icmas workshop game theoretic decision
theoretics agents boston




