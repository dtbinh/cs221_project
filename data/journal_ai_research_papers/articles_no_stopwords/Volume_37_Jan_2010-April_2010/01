Journal Artificial Intelligence Research 37 (2010) 41-83

Submitted 07/09; published 02/10

Predicting Performance IDA* using
Conditional Distributions
Uzi Zahavi

zahaviu@cs.biu.ac.il

Computer Science Department
Bar-Ilan University, Israel

Ariel Felner

felner@bgu.ac.il

Department Information Systems Engineering
Deutsche Telekom Labs
Ben-Gurion University, Israel

Neil Burch

burch@cs.ualberta.ca

Computing Science Department
University Alberta, Canada

Robert C. Holte

holte@cs.ualberta.ca

Computing Science Department
University Alberta, Canada

Abstract
Korf, Reid, Edelkamp introduced formula predict number nodes IDA*
expand single iteration given consistent heuristic, experimentally demonstrated could make accurate predictions. paper show that, addition requiring heuristic consistent, formulas predictions accurate
levels brute-force search tree heuristic values obey unconditional distribution defined used formula. propose
new formula works well without requirements, i.e., make accurate predictions IDA*s performance inconsistent heuristics heuristic values
level obey unconditional distribution. order achieve introduce
conditional distribution heuristic values generalization unconditional
heuristic distribution. provide extensions formula handle individual
start states augmentation IDA* bidirectional pathmax (BPMX), technique propagating heuristic values inconsistent heuristics used. Experimental
results demonstrate accuracy new method variations.

1. Introduction Overview
Heuristic search algorithms A* (Hart, Nilsson, & Raphael, 1968) IDA* (Korf,
1985) guided cost function f (n) = g(n) + h(n), g(n) actual distance
start state state n h(n) heuristic function estimating cost n
nearest goal state. heuristic h admissible h(n) dist(n, goal) every state n
goal state goal, dist(n, m) cost least-cost path n m. h(n)
admissible, i.e. always returns lower bound estimate optimal cost, algorithms
guaranteed find optimal path start state goal state one exists.
c
2010
AI Access Foundation. rights reserved.

fiZahavi, Felner, Burch, & Holte

important question ask many nodes expanded algorithms
solve given problem. major advance answering question work done
Korf, Reid, Edelkamp introduced formula predict number nodes IDA*
expand (Korf & Reid, 1998; Korf, Reid, & Edelkamp, 2001). papers, formula
present, predictions makes, referred KRE paper. Prior
KRE, standard method comparing two heuristic functions compare
average values, preference given heuristic larger average (Korf,
1997; Korf & Felner, 2002; Felner, Korf, Meshulam, & Holte, 2007). KRE made substantial
improvement characterizing quality heuristic function distribution
values. developed KRE formula based heuristic distribution
predict number nodes expanded IDA* searching specific heuristic
cost threshold. Finally, compared predictions formula actual
number nodes expanded IDA* different thresholds several benchmark search
spaces showed gave virtually perfect predictions. major advance
analysis search algorithms heuristics.
Despite impressive results, KRE formula two main shortcomings. first
KRE assumes addition admissible given heuristic consistent.
heuristic h consistent every pair states, n, h(m) h(n) dist(m, n).1
heuristic consistent, heuristic values nodes children thus constrained similar heuristic value node. heuristic inconsistent
consistent, i.e. pair nodes n, h(m) h(n) > dist(m, n). Inconsistency
allows nodes children heuristic values arbitrarily larger smaller
nodes heuristic value. term inconsistency negative connotation
something avoided, recent studies shown inconsistent heuristics easy
define many search applications produce substantial performance improvements
(Felner, Zahavi, Schaeffer, & Holte, 2005; Zahavi, Felner, Schaeffer, & Sturtevant, 2007;
Zahavi, Felner, Holte, & Schaeffer, 2008). reason, important extend
KRE formula accurately predict IDA*s performance inconsistent heuristics,
heuristics likely become increasingly important future applications.
second shortcoming KRE formula works well levels
search tree heuristic distribution follows equilibrium distribution (defined
Section 3.1.2). always holds sufficiently deep levels search tree,
heuristic values converge equilibrium distribution. addition, hold
levels heuristic values set start states distributed according
equilibrium distribution. However, shown (in Section 3.2.2) KRE
formula inaccurate depths practical interest single start states
large sets start states whose values distributed according equilibrium
distribution. cases, heuristic values levels search tree
actually examined IDA* obey equilibrium distribution applying KRE
cases result inaccurate predictions.
main objective paper develop formula accurately predict number
nodes IDA* expand, given cost threshold, given heuristic set start
states, including currently covered KRE. first extend KREs idea
1. general definition graph. case undirected graphs write consistency
definition |h(m) h(n)| dist(m, n).

42

fiPredicting Performance IDA* using Conditional Distributions

heuristic distribution, unconditional, conditional distribution,
probability specific heuristic value constant, KRE, conditioned
certain local properties search space. conditional distribution provides
insights behavior heuristic values search informed
(in context search tree) specific heuristic value produced.
allows better study heuristic behavior.
Based conditional distribution develop new formula, CDP (Conditional Distribution Prediction), predicts IDA*s performance set start states (regardless
heuristic values distributed) desired depth (not necessarily large)
whether heuristic consistent not. CDP recursive structure information
number nodes propagated root leaves search tree.
experiments CDPs predictions least accurate KREs, CDP much
accurate inconsistent heuristics sets start states non-equilibrium
heuristic distributions. basic form, CDP particularly accurate single start
states. describe simple extension improves accuracy setting. Finally,
adapt CDP make predictions IDA* augmented bidirectional pathmax
method (BPMX) (Felner et al., 2005). inconsistent heuristics used, BPMX
useful addition IDA*. prunes many subtrees would otherwise explored,
thereby substantially reducing number nodes IDA* expands.
Throughout paper provide experimental results demonstrating accuracy
CDP scenarios using two benchmark domains used KRE
sliding-tile puzzle Rubiks Cube.
simplicity discussion, assume paper edges cost 1.
true many problem domains. generalization ideas case variable edge
costs straightforward, although practical implementation introduces additional
challenges (briefly described Section 11.2).
paper organized follows. Section 2 presents background material. Section 3
derives KRE formula first principles discusses limitations. Section 4,
notion conditional distribution heuristic values presented. new formula, CDP,
presented Section 4.2. Section 5 discusses subtle important way
experiments differ KREs. Experimental results presented Sections 6 7.
extension CDP formula better handle single start states presented Section 8.
Section 9 proposes technique, based CDP, estimating upper lower bounds
number nodes IDA* expand given unconditional distribution. Section 10
presents extension CDP predicting performance IDA* BPMX applied.
Related work discussed Section 11, conclusions suggestions future work
given Section 12. preliminary version paper appeared (Zahavi, Felner, Burch,
& Holte, 2008).

2. Background
Two application domains used KRE demonstrate accuracy formula.
experiments use exactly domains. section describe
well search algorithm different heuristic functions used
experiments.
43

fiZahavi, Felner, Burch, & Holte

2.1 Problem Domains
Two classic examples AI literature single-agent pathfinding problems
Rubiks Cube sliding-tile puzzle.
2.1.1 Rubiks Cube

Figure 1: 3 3 3 Rubiks Cube
Rubiks Cube invented 1974 Erno Rubik Hungary. standard version
consists 3 3 3 cube (Figure 1), different colored stickers exposed
squares sub-cubes, cubies. 20 movable cubies 6 stable cubies
center face. movable cubies divided eight corner cubies,
three faces each, twelve edge cubies, two faces each. Corner cubies move
among corner positions, edge cubies move among edge positions.
one 6 faces cube rotated 90, 180, 270 degrees relative
rest cube. results 18 possible moves state. Since twisting
face twice row redundant, branching factor first move reduced
15. addition, movements opposite faces independent. example, twisting
left face right face leads state performing moves
opposite order. Pruning redundant moves results search tree asymptotic
branching factor 13.34847 (Korf, 1997).
goal state, squares side cube color. puzzle
scrambled making number random moves, task restore cube
original unscrambled state. 4 1019 different reachable states.
2.1.2 Sliding-tile Puzzles
sliding-tile puzzle consists square frame containing set numbered square tiles,
empty position called blank. legal operators slide tile
horizontally vertically adjacent blank blank position. problem
rearrange tiles random initial configuration particular desired goal
configuration. state space grows exponentially size number tiles increases,
shown finding optimal solutions sliding-tile problem NPcomplete (Ratner & Warmuth, 1986). two common versions sliding-tile
puzzle 3 3 8-puzzle, 4 4 15-puzzle. 8-puzzle contains 9!/2 (181,440)
44

fiPredicting Performance IDA* using Conditional Distributions

1

2

3
7

1

2

4

5

6

3

4

5

8

9

10 11

6

7

8

12 13 14 15

Figure 2: 8-puzzle 15-puzzle goal states
reachable states, 15-puzzle contains 1013 reachable states. goal states
puzzles shown Figure 2.
classic heuristic function sliding-tile puzzles called Manhattan Distance. computed counting number grid units tile displaced
goal position, summing values tiles, excluding blank. Since
tile must move least Manhattan Distance goal position, move changes
location one tile one grid unit, Manhattan Distance lower bound
minimum number moves needed solve problem instance.
2.2 Iterative Deepening A*
Iterative deepening A* (IDA*) (Korf, 1985) performs series depth-first searches, increasing cost threshold time. depth-first search, nodes n f (n)
expanded. Threshold initially set h(s), start node. goal
found using current threshold, search ends successfully. Otherwise, IDA* proceeds
next iteration increasing minimum f value exceeded previous
iteration.
2.3 Pattern Databases (PDBs)
powerful approach obtaining admissible heuristics create simplified version,
abstraction, given state space use exact distances abstract space
estimates distances original state space. type abstractions use
paper sliding-tile puzzles illustrated Figure 3. left side
figure shows 15-puzzle state goal state. right side shows corresponding
abstract states, defined erasing numbers tiles except 2, 3, 6
7. estimate distance goal state 15-puzzle, calculate
exact distance abstract state corresponding abstract goal state.
pattern database (PDB) lookup table stores distance abstract goal
every abstract state (or pattern) (Culberson & Schaeffer, 1994, 1998). PDB built
running breadth-first search2 backwards abstract goal whole abstract
space spanned. compute h(s) state original space, mapped
corresponding abstract state p distance-to-goal p looked PDB.
2. description assumes operators cost. technique easily extended
cases operators different costs.

45

fiZahavi, Felner, Burch, & Holte

PDB lookup

State
11

9

5

10

1

15 12

2

4

14 13

2

3

6

7

8

3

1

2

3

2

3

4

5

6

7

6

7

8

9

10 11

13
6

7

12 13 14 15

Goal State

Goal Pattern

(a)

(b)

Figure 3: Example regular lookups

example, PDB 15-puzzle based tiles 2, 3, 6, 7 would contain entry
every possible way placing four tiles blank 16 puzzle positions.
PDB could implemented 5-dimensional array, P DB, array indexes
locations blank tiles 2, 3, 6, 7 respectively. lookup state
shown Figure 3 would P DB[0][8][12][13][14] (the blank position 0, tile 2
position 8, tile 3 position 12, etc.). paper, accessing PDB state
described referred regular lookup, heuristic value returned
regular lookup referred regular heuristic value.
Pattern databases proven useful finding lower bounds combinatorial
puzzles (Korf, 1997; Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner, Korf, &
Hanan, 2004; Felner et al., 2007). Furthermore, proven useful
search problems, e.g., multiple sequence alignment (McNaughton, Lu, Schaeffer, & Szafron,
2002; Zhou & Hansen, 2004) planning (Edelkamp, 2001a).
2.4 Geometric Symmetries
common practice exploit special properties state space enable additional
heuristic evaluations. particular, additional PDB lookups performed given
single PDB. example, consider Rubiks Cube suppose PDB based
positions cubies yellow face (the positions cubies dont
matter). Reflecting rotating puzzle enable similar lookups cubies
different color (e.g., green, red, etc.) since puzzle perfectly symmetric respect
color. Thus, 24 symmetric lookups PDB different heuristic
values obtained lookups PDB. heuristic values
admissible given state puzzle.
46

fiPredicting Performance IDA* using Conditional Distributions

another example, consider sliding-tile puzzle. line symmetry main
diagonal (assuming goal location blank upper left corner). configuration tiles reflected main diagonal reflected configuration
shares attributes original one. reflections usually used using
PDBs sliding-tile puzzle (Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner
et al., 2004, 2007) looked PDB.
2.5 Methods Creating Inconsistent Heuristics
consistent heuristics, difference heuristic value neighboring nodes
constrained less equal cost connecting edge. inconsistent
heuristics, constraint difference heuristic values neighboring
nodes much larger cost edge connecting them.
KRE formula designed work consistent heuristics therefore KRE
papers report experiments done consistent heuristics only. contrast, new
formula, CDP, works types heuristics including inconsistent heuristics. Therefore,
paper, addition usual consistent heuristics regular PDB lookups
Manhattan Distance experiment inconsistent heuristics. previously
described several methods producing inconsistent heuristics (Zahavi et al., 2007). Two
inconsistent heuristics used experiments Random selection
heuristics Dual evaluations.
Random selection heuristics: well-known method overcoming pitfalls
given heuristic employ several heuristics use maximum value (Holte,
Felner, Newton, Meshulam, & Furcy, 2006). example, multiple heuristics
based domain-specific geometric symmetries ones described above.
using geometric symmetries additional storage costs associated
extra evaluations, even evaluations based PDBs.
Although using multiple heuristics results improved heuristic value, therefore
likely reduce number nodes expanded finding solution, increases
time required calculate heuristic values nodes, might increase
overall running time search. Instead using available heuristics
every heuristic calculation, one could instead choose consult one them,
selection made either randomly systematically. one
heuristic consulted node, time-per-node virtually
one heuristic available. Even individual heuristics consistent,
heuristic values actually used inconsistent different heuristics
consulted different nodes. showed (Zahavi et al., 2007) inconsistency
generally reduces number expanded nodes compared using heuristic
nodes almost low maximum heuristics
computed every node. Rubiks Cube, randomly chose one 24
different lookups PDB arise 24 lines symmetry
cube.
Dual evaluation: permutation state spaces Rubiks Cube, state
exists dual state sd located distance goal (Felner
47

fiZahavi, Felner, Burch, & Holte

et al., 2005; Zahavi, Felner, Holte, & Schaeffer, 2006; Zahavi et al., 2008). Therefore,
admissible heuristic applied sd admissible s. puzzles studied
paper permutation state spaces, dual state puzzles
calculated reversing role locations objects: regular state uses
set objects indexed current location, dual state set
locations indexed objects contain. using PDBs, dual lookup
look sd PDB. Performing regular PDB lookups states
generated search produces consistent values. However, values produced
performing dual lookup inconsistent identity objects
queried change dramatically two consecutive lookups. Due
diversity, dual heuristic shown preferable regular heuristic (Zahavi
et al., 2007). exact definition explanations dual lookup provided
original papers (Felner et al., 2005; Zahavi et al., 2006, 2008).
important note three PDB lookups (regular, dual, random) consult
PDB. Thus, need amount memory share overall
distribution heuristic values (Zahavi et al., 2007).

3. KRE Formula Limitations
section begins short derivation KRE formula state spaces
state transitions cost 1. KRE describe generalized account
variable edge costs (Korf et al., 2001).
3.1 KRE formula
given state IDA* threshold d, KRE aims predict N (s, d), number nodes
IDA* expand uses start state complete search
IDA* threshold (i.e., searches depth terminate search goal
encountered). written
N (s, d) =


X

Ni (s, d)

(1)

i=0

Ni (s, d) number nodes expanded IDA* level threshold d.
One way decompose Ni (s, d) product two terms
Ni (s, d) = Ni (s) Pex (s, d, i)

(2)

Ni (s) number nodes level BF Ssd , brute-force search tree (i.e.,
tree created breadth first search without heuristic pruning) depth rooted
start state s, Pex (s, d, i) percentage nodes level BF Ssd
expanded IDA* threshold d.
KRE, Ni (s) written Ni , i.e., without dependence start state s.
perfectly correct state spaces uniform branching factor b, Ni (s)
cases simply bi . state spaces non-uniform regular branching structure,
48

fiPredicting Performance IDA* using Conditional Distributions

KRE showed Ni could computed exactly using recurrence equations independent s. However, base cases recurrences KRE depend
using Ni instead Ni (s) reasonable strictly correct.
3.1.1 Conditions Node Expansion IDA*
understand Pex (s, d, i) treated KRE, necessary reflect conditions
required node expansion. node n level BF Ssd expanded IDA*
satisfies two conditions:
1. f (n) = g(n) + h(n) must less equal d. edges unit cost,
g(n) = condition equivalent h(n) i. call nodes satisfy
condition potential nodes potential expanded.
2. n must generated IDA*, i.e., parent (at level 1) must expanded
IDA*.
KRE restricted analysis heuristics consistent proved case
second condition implied first condition. words, given
heuristic consistent, nodes expanded IDA* level BF Ssd threshold
exactly set potential nodes level i.3 observation allows Equation 2
rewritten
Ni (s, d) = Ni (s) PP OT EN IAL (s, i, i)

(3)

PP OT EN IAL (s, i, v) defined percentage nodes level BF Ssd whose
heuristic value less equal v.
Note although PP OT EN IAL(s, i, di) = Pex (s, d, i) given heuristic consistent, PP OT EN IAL (s, i, i) overestimates Pex (s, d, i) heuristic inconsistent,
sometimes large amount (see Section 3.2.1).
3.1.2 Approximating PP OT EN IAL (s, i, v)
KRE use three different approximations PP OT EN IAL(s, i, v). KREs theoretical analysis
PP OT EN IAL(s, i, v) approximated equilibrium distribution, denote
PEQ (v). defined probability node chosen randomly uniformly
among nodes given depth brute-force search tree heuristic value less
equal v, limit large depth (Korf et al. 2001, p. 208). KRE proved that,
limit large d,

X
Ni (s) PEQ (d i)
i=0

would converge N (s, d) given heuristic consistent. final formula (the KRE
formula) therefore:
3. See section 3.2.1 discussion KRE formula consistent heuristics.

49

fiZahavi, Felner, Burch, & Holte

N (s, d) =


X

Ni (s) PEQ (d i)

(4)

i=0

KRE contrasted equilibrium distribution overall distribution,
defined probability state chosen randomly uniformly states
problem heuristic value less equal v (p. 207). Unlike equilibrium
distribution, defined search tree, overall distribution property
state space. overall distribution directly computed pattern database,
one pattern database used entries corresponds number
states original state space, approximated, complex settings,
computing heuristic values large random sample states. KRE argued
Rubiks Cube overall distribution heuristic defined single pattern database
equilibrium distribution, sliding-tile puzzles, two
distributions different.
heuristic used KREs experiments Rubiks Cube defined maximum
three pattern databases. individual pattern database, overall distribution
computed exactly. KREs experiments distributions combined approximate PP OT EN IAL(s, i, v) assuming values three pattern databases
independent.
experiments sliding-tile puzzles, KRE defined three types states based
whether blank located corner position, edge position, interior
position, approximated PP OT EN IAL(s, i, v) weighted combination overall distributions states type. weights used level exact
percentages states different types level.
experiments followed KRE precisely use overall distribution individual Rubiks Cube pattern databases weighted overall distribution described
sliding-tile puzzles. simplicity, reminder paper use phrase
unconditional heuristic distribution4 notation P (v) refer probability
node heuristic less equal v. let exact context determine
distribution P (v) actually denotes, whether equilibrium distribution, overall
distribution, approximation PP OT EN IAL Pex . Likewise use
p(v) (lower case p) denote P (v) P (v 1) (with p(0) = P (0)). p(v) probability
state heuristic value exactly v according distribution P .
3.2 Limitations KRE Formula
KRE formula (Equation 4) two main shortcomings: (1) predictions
accurate given heuristic inconsistent, (2) even consistent heuristics
predictions inaccurate individual start states sets start states whose heuristic
values distributed according unconditional heuristic distribution, P (v).
turn examine detail.
50

fiPredicting Performance IDA* using Conditional Distributions

Consistent
4
3
3

3 3

expanded
generated
generated

5

Inconsistent
4 R
3

3

4 6

3

5
6

4 3 n

Figure 4: Consistent versus inconsistent heuristics
3.2.1 Inconsistent Heuristics
specifically mentioned KRE papers one property required KRE analysis
heuristic consistent. necessary KRE formula aims
count number potential nodes level BF Ssd . consistent heuristics,
heuristic value neighboring states never changes change g-value,
illustrated left side Figure 4 (where number inside node heuristic
value). implies f -value nodes ancestor always less equal
f -value node (i.e., f monotone non-decreasing along path search tree).
Therefore, easy prove consistent heuristics ancestors potential
node potential nodes (Korf et al., 2001). Consequently IDA* expand
potential nodes BF Ssd . Hence, formula KRE aims count
number potential nodes BF Ssd used predict number nodes IDA*
expand given consistent heuristic.
inconsistent heuristics reasoning apply. heuristic values neighboring states differ much cost edge connects them,
thus f -values along path search tree guaranteed monotonically
non-decreasing. Therefore, ancestors potential node guaranteed
potential nodes themselves, consequence potential node might never
generated. example, consider search tree right side Figure 4. numbers
inside node show nodes heuristic value. Assume start node R
IDA* threshold 5 (a node potential node f -value less equal
5). 3 potential nodes depth 2 (all heuristic value 3). Consider
potential node n. path node node potential node
(f (m) = 1 + 5 = 6 > 5), generated expanded. Therefore, node n
never generated, preventing IDA* expanding it. Since KRE formula counts
number potential nodes, count node n thus overestimate number
expanded nodes inconsistent heuristic used.
amount KRE overestimates number nodes expanded IDA*
inconsistent heuristic large. illustrate this, consider state space
Rubiks Cube PDB heuristic defined locations 6 (out 12) edge
cubies. regular method looking heuristic value PDB produces consistent
heuristic. discussed Section 2.5 two alternative PDB lookups produce inconsistent
4. unconditional distinguish conditional distribution introduce Section 4.1

51

fiZahavi, Felner, Burch, & Holte


8
9
10
11
12
13

KRE
257
3,431
45,801
611,385
8,161,064
108,937,712

Regular
277
3,624
47,546
626,792
8,298,262
110,087,215

Dual
36
518
6,809
92,094
1,225,538
16,333,931

Random Symmetry
26
346
4,608
61,617
823,003
10,907,276

Table 1: Rubiks Cube - Number nodes expanded IDA* using regular, dual,
random-symmetry PDB lookups different IDA* threshold corresponding KRE predictions.

heuristics dual evaluation random selection multiple heuristics.
Rubiks Cube 24 symmetries applied state create new
way perform PDB lookup it. Thus, 24 heuristics Rubiks Cube based
PDB random-symmetry lookup chooses one randomly.
three lookups (regular, dual, random-symmetry) consult PDB
distribution heuristic values, P (v), therefore KRE predict
IDA* expand number nodes regardless whether regular, dual,
random-symmetry lookup done. experimental results Table 1 show
substantially different number nodes actually expanded practice
methods.
row Table 1 presents results specific IDA* threshold (d). result
average 1, 000 random initial states, generated making 180 random
moves goal state. KRE column shows KRE prediction based
unconditional heuristic distribution. last three columns Table 1 show number
nodes IDA* expands performs either regular, dual, random-symmetry lookup
PDB. KRE prediction within 8% actual number nodes expanded
IDA* uses regular (consistent) PDB lookup (third column) substantially
overestimates number nodes expanded IDA* uses dual random-symmetry
inconsistent lookups PDB (fourth fifth columns).
3.2.2 Sets Start States Whose Heuristics Values Obey
unconditional heuristic distribution
explained above, KRE used unconditional heuristic distribution P (v) and,
theoretical analysis, proved use KRE formula would give accurate predictions limit large depth. fact, accurate predictions occur soon
heuristic distribution depth interest closely approximates P (v). happens
large depths definition happen even shallow levels certain
circumstances. reason KRE able produce extremely accurate predictions
experiments using unconditional heuristic distribution P (v) depths
start states experiments report average predictions performances
52

fiPredicting Performance IDA* using Conditional Distributions

large number randomly drawn start states. spaces used KREs experiments,
heuristic distribution large random set start states closely approximated
P (v) distribution used. caused heuristic distributions levels closely
approximate P (v).
However, set start states heuristic values distributed according
P (v), case non-random sets start states single start state,
KRE expected make good predictions small depths. words,
cases unconditional heuristic distribution P (v) expected good
approximation Pex (s, d, i).
Consider case single start state consistent heuristic. distribution
heuristic values search tree close start state highly correlated
heuristic value start state, therefore search trees
start states different heuristic values. example, great deal pruning likely
occur near top search tree start state large heuristic value, resulting
fewer nodes expanded start state small heuristic value. Applying KRE
two states produce prediction, therefore inaccurate
least one them, uses unconditional heuristic distribution P (v)
cases.
h
5
6
7
8
9

IDA*
30,363,829
18,533,503
10,065,838
6,002,025
3,538,964

KRE
8,161,064
8,161,064
8,161,064
8,161,064
8,161,064

Table 2: Results set 1,000 start states h-value shown first column
(regular PDB lookup, IDA* threshold = 12)

Table 2 demonstrates phenomenon Rubiks Cube one regular 6-edge PDB
lookup IDA* threshold = 12. IDA* column shows average number nodes
expanded 1, 000 start states, heuristic value h given row. KRE
ignores heuristic values start states predicts 8,161,064 nodes
expanded IDA* every start state. row = 12 Table 1 shows
accurate prediction performance averaged large random sample start
states, Table 2 see low start states small heuristic values
high ones large heuristic values.
3.2.3 Convergence Heuristic Distributions Large Depths
described above, KRE make accurate predictions level nodes level
actually obey unconditional heuristic distribution P (v). increases, distribution
heuristic values start converge P (v). rate convergence depends upon
state space. believed fairly slow sliding-tile puzzles, faster
53

fiZahavi, Felner, Burch, & Holte

Rubiks Cube. convergence occurs IDA* threshold reached KRE
provide accurate predictions set start states (including single start states).
order experimentally test repeated KRE Rubiks Cube experiment but,
addition using large set random start states, looked individual
performance two start states, s6 , low heuristic value (6), s11 ,
maximum value heuristic used experiment (11). KRE used
8-6-6 heuristic takes maximum 3 different PDBs (one based 8 corner
cubies two based 6 edge cubies each). heuristic admissible consistent.
billion random states sampled estimate P (v) maximum value 11
average value 8.898.

KRE

10
11
12
13
14
15
16
17

1,510
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Multiple start states
IDA*
Ratio
1,501
0.99
20,151
1.00
270,396
1.00
3,564,495
0.99
47,961,699
1.00
642,403,155
1.00
8,599,849,255
1.01
114,773,120,996
1.01

s6
53,262
422,256
3,413,547
29,114,115
259,577,913
2,451,954,240
24,484,797,237
258,031,139,364

Single start state
Ratio
s11
0.03
0.05
8,526
0.08
162,627
0.12
2,602,029
0.18
38,169,381
0.26
542,241,315
0.35
7,551,612,957
0.44 103,934,322,960

Ratio
2.37
1.66
1.38
1.26
1.18
1.13
1.10

Table 3: Rubiks Cube - Max (8,6,6) PDBs
Table 3 presents results. KRE column presents KRE prediction
Multiple start states columns presents actual number states generated (averaged
set random start states) IDA* threshold. columns copied KRE
journal paper (Korf et al., 2001). Ratio columns Table 3 shows value predicted
KRE formula divided actual number nodes generated. ratio found
close 1.0 multiple start states, indicating KREs predictions
accurate.
results two individual start states tested shown Single start
state part table. Note states optimally solved depth 17, but,
KRE, search depth run completion. cases KRE formula
accurate small thresholds accuracy prediction increased threshold
increased. threshold = 17 KRE prediction roughly factor 2 small s6
10% large s11 . large improvement smaller thresholds.
predictions become even accurate depth continues increase.
reason predictions improve larger values deeper depths
heuristic distribution within single level converges unconditional heuristic distribution. Using dashed dotted lines various types, Figure 5(a) shows distribution
heuristic values seen states 0, 1, 2 4 moves away s6 . solid line
Figure 5(a) unconditional heuristic distribution. x-axis corresponds different
heuristic values y-axis shows percentage states specified depth
heuristic values less equal x value. example depth 0 (which includes
54

fiPredicting Performance IDA* using Conditional Distributions

120

120
Unconditional heuristic distribution
Depth = 4
Depth = 2
Depth = 1
Depth = 0

80

100
cumulative percentage

cumulative percentage

100

Unconditional heuristic distribution
Depth = 4
Depth = 2
Depth = 1
Depth = 0

60
40
20

80
60
40
20

0

0
0

2

4

6

8

10

12

0

2

Heuristic value

4

6

8

10

Heuristic value

(a) Heuristic Distributions s6

(b) Heuristic Distributions s11

Figure 5: Convergence heuristic distributions

start state only) heuristic value 6 seen (leftmost curve). depth 1,
heuristic values 5, 6 7 seen (second curve left), on. figure
shows heuristic distribution successive depths converges unconditional
heuristic distribution (rightmost curve Figure 5(a)). depth 17 (not shown), heuristic distribution probably quite close unconditional heuristic distribution, making
KRE prediction quite accurate even single start state.
Figure 5(b) shows heuristic distributions nodes 0, 1, 2, 4 moves
away s11 . case unconditional heuristic distribution left
heuristic distributions shallow depths, heuristic distribution depth 0
rightmost curve figure. Comparing parts (a) (b) Figure 5 see
convergence unconditional heuristic distribution faster s11 s6 ,
explains KRE prediction Table 3 accurate s11 .

4. Conditional Distribution CDP Formula
present new formula CDP (Conditional Distribution Prediction), overcomes two shortcomings KRE described previous section. important feature
CDP extends unconditional heuristic distribution heuristic values P (v)
used KRE conditional distribution.
4.1 Conditional Distribution Heuristic Values
conditional distribution heuristic values denoted P (v|context), context
represents local properties search tree neighborhood node influence
distribution heuristic values nodes children. Specifically, Pn (v) percentage
node ns children heuristic value less equal v, define
P (v|context) average Pn (v) nodes n satisfy conditions defined
55

12

fiZahavi, Felner, Burch, & Holte

context. P (v|context) interpreted probability node heuristic
value less equal v produced node satisfying conditions specified
context expanded. context empty denoted P (v) Section 3.
use p(v|context) (lower case p) denote probability node heuristic value
equal v produced node
P satisfying conditions specified context
expanded. Obviously, P (v|context) = vi=0 p(i|context).
4.1.1 Basic 1-Step Model
conditioning context combination local properties search tree,
including properties node (e.g. heuristic value), operator applied
generate node, properties nodes ancestors search tree, etc. simplest
conditional distribution p(v|vp ), probability node heuristic value equal v
produced node value vp expanded. call 1-step model
value conditioned nodes one step away only. special circumstances,
p(v|vp ) determined exactly analysis state space heuristic,
general must approximated empirically sampling state space.
sampling method p(v|vp ) represented entry [v][vp ] two-dimensional
matrix [0..hmax ][0..hmax ], hmax maximum possible heuristic value. build
matrix first set values matrix 0. randomly generate state
calculate heuristic value vp . that, generate child state one
time, calculate childs heuristic value (v), increment [v][vp ]. repeat
process large number times order generate large sample. Finally, divide
value cell matrix sum column cell belongs to, entry
[v][vp ] represents percentage children generated value v state
value vp expanded.

vp

vp
6

v

7

8

9

10

6

7

8

9

10

6

0.17 0.11 0.06 0.03 0.02

7

0.36 0.38 0.33 0.25 0.19

0.00 0.44 0.70 0.67 0.00

8

0.37 0.44 0.53 0.60 0.62

9

0.00 0.00 0.09 0.32 0.89

9

0.02 0.03 0.05 0.08 0.14

10

0.00 0.00 0.00 0.01 0.11

10

0.00 0.00 0.00 0.01 0.01

6

0.30 0.11 0.00 0.00 0.00

7

0.60 0.45 0.21 0.00 0.00

8

v

(a) Consistent heuristic

(b) Inconsistent heuristic

Figure 6: portion Conditional Distribution matrix Rubiks Cube consistent
inconsistent heuristics

56

fiPredicting Performance IDA* using Conditional Distributions

Figure 6 shows bottom right corner two matrices 6-edge PDB
Rubiks Cube. left matrix (a) shows p(v|vp ) regular (consistent) lookup
PDB right matrix (b) shows p(v|vp ) inconsistent heuristic created
dual lookup PDB. matrix (a) tridiagonal neighboring values
cannot differ 1. example, states heuristic value 8
children heuristics 7, 8 9; occur probabilities 0.21, 0.70 0.09
respectively (see column 8). contrast, matrix (b) tridiagonal. column 8,
example, see 6% time states heuristic value 8 children
heuristic values 6.
4.1.2 Richer Models
IDA* expands node, eliminates children operator pruning.
example, state spaces undirected operators, using studies,
parent node would generated among nodes children IDA* would immediately
prune away. Distribution p(v|vp ) take account. order take
consideration necessary extend context conditional probability include
heuristic value parent node expanded (we refer parent node
gp). denote p(v|vp ,vgp ) call 2-step model conditions
information ancestors two steps away. p(v|vp ,vgp ) gives probability
node heuristic value equal v generated node expanded
heuristic value vp parent node expanded heuristic value
vgp . estimated sampling way done estimate p(v|vp ), except
sample generates random state, gp, neighbors,
neighbors except eliminated operator pruning. Naturally, results
sampling 2-step model stored three-dimensional array.
context conditional distribution extended ways well.
sliding-tile puzzles, KRE conditions overall distribution type state
expanded, type indicates blank corner, edge, interior
location. experiments sliding-tile puzzle below, extend p(v|vp ,vgp )
type information: p(v, t|vp , tp ,vgp , tgp ) gives probability node type
heuristic value equal v generated node expanded heuristic
value vp type tp expanded nodes parent heuristic value vgp type tgp .
4.2 New Prediction Formula, CDP (Conditional Distribution Prediction)
section use conditional distributions described develop CDP, alternative KRE formula predicting number nodes IDA* expand
given heuristic, IDA* threshold, set start states. shown experimentally, new formula CDP overcomes limitations KRE works well inconsistent
heuristics set start states arbitrary IDA* threshold.
overall approach follows. Define Ni (s, d, v) number nodes
IDA* generate level heuristic value equal v start state
IDA* threshold.
Pdi Given Ni (s, d, v), number nodes IDA* expand level
threshold v=0 Ni (s, d, v), and, N (s, d), total number nodes expanded
complete iteration IDA* threshold levels, quantity ultimately
57

fiZahavi, Felner, Burch, & Holte

P P
interested in, di=0 di
v=0 Ni (s, d, v). summations v runs
nodes heuristic values range [0 . . . i] expanded level i.
Ni (s, d, v) could calculated exactly, formula would calculate N (s, d) exactly
whether given heuristic consistent not. However, general method efficiently calculating Ni (s, d, v) exactly. Instead, Ni (s, d, v) estimated recursively
Ni1 (s, d, v) conditional distribution; exact details depend conditional
model used given subsections follow. use Ni (s, d, v)
denote approximation Ni (s, d, v). Section 4.5.1 describe conditions
calculation is, fact, exact, therefore produces perfect predictions N (s, d).
general case predictions may perfect estimates.
present time analytical tools estimating accuracy show
experimentally, estimates often accurate.
4.3 Prediction Using Basic 1-Step Model
basic 1-step conditional distribution p(v|vp ) used, Ni (s, d, v) estimated
recursively follows:
d(i1)

Ni (s, d, v) Ni (s, d, v) =

X

Ni1 (s, d, vp ) bvp p(v|vp )

(5)

vp =0

bvp average branching factor nodes heuristic value vp , estimated
sampling process estimates conditional distribution.5 reasoning
behind equation Ni1 (s, d, vp )bvp total number children IDA* generates
via nodes expands level 1 heuristic value equal vp . multiplied
p(v|vp ) get expected number children heuristic value v. Nodes
level i1 expanded heuristic value less equal (i 1),
hence summation includes vp values range [0 . . . (i 1)]. restricting
vp less equal (i 1) every recursive application formula,
ensure (even inconsistent heuristics) node counted level
ancestors expanded IDA*. base case recursion, N0 (s, d, v), 1
v = h(s) 0 values v.
Based this, number nodes expanded IDA* given start state s, threshold d,
particular heuristic predicted follows:
CDP1 (s, d) =

X
di
X

Ni (s, d, v)

(6)

i=0 v=0

set, S, start states given instead one start state, calculation
identical except base case recursion defined using start states
S. is, define N0 (S, d, v) equal k k states heuristic
value v. rest formula remains (with substituted everywhere).
5. general case equation branching factor depends context defines conditional distribution. Since 1-step model, context heuristic value v, formally
allow branching factor depend it. practice, branching factor usually
heuristic values.

58

fiPredicting Performance IDA* using Conditional Distributions

4.4 Prediction Using Richer Models
2-step conditional distribution p(v|vp , vgp ) used, define Ni (s, d, v, vp )
number nodes IDA* generate level heuristic value equal v
nodes level 1 heuristic value vp start state IDA*
threshold. Ni (s, d, v, vp ) estimated recursively follows:
d(i2)

Ni (s, d, v, vp ) Ni (s, d, v, vp ) =

X

Ni1 (s, d, vp , vgp ) bvp ,vgp p(v|vp , vgp )

(7)

vgp =0

bvp ,vgp average branching factor nodes heuristic value vp parent
heuristic value vgp . base case 2-step model level 1, level 0.
N1 (s, d, v, vp ) 0 vp 6= h(s), number children start state
heuristic value v vp = h(s). Based 2-step model number nodes expanded
IDA* given start state s, threshold d, particular heuristic predicted
follows:
CDP2 (s, d) =

X
di d(i1)
X
X
i=0 v=0

Ni (s, d, v, vp )

(8)

vp =0

set start states instead one, base case N1 (S, d, v, vp ),
number children heuristic value v states heuristic value vp .
Analogous definitions Ni CDP used definition context.
example, using 1-step model set state types, one would define Ni (s, d, v, t)
number nodes type IDA* generate level heuristic value
equal v, estimate recursively follows:
d(i1)

Ni (s, d, v, t) Ni (s, d, v, t) =

X

X

vp =0

tp

Ni1 (s, d, vp , tp ) bvp ,tp p(v, t|vp , tp )

(9)

Based model number nodes expanded IDA* given start state s, threshold
d, particular heuristic predicted follows:
CDP(s, d) =

X
di X
X

Ni (s, d, v, t)

(10)

i=0 v=0 tT

4.5 Prediction Accuracy
accuracy predictions arbitrarily good arbitrarily bad depending
accuracy conditional model used. following subsections examine
extreme cases.
principle, extending context never decrease accuracy predictions
additional information taken account. However, conditional model
estimated sampling, extended context result poorer predictions
fewer samples context. explanation 1-step model
accurate 2-step model rows h = 6 h = 9 Table 7 Section 6.2
below.
59

fiZahavi, Felner, Burch, & Holte

4.5.1 Perfect Predictions
Consider definition context includes heuristic value node expanded (vp contexts defined above) contains sufficient information allow
operator pruning correctly accounted for. use notation (v, x) refer
specific instance context, v heuristic value node expanded
x instantiation information context (e.g., state type
information last model above). general form predictive model
context
CDP(s, d) =

X
di
X
i=0 v=0

X

Ni (s, d, v, x)

(11)

x
(v, x)
instance
context


d(i1)

Ni (s, d, v, x) =

X

X

vp =0

xp
(vp , xp )
instance
context

Ni1 (s, d, vp , xp ) bvp ,xp p(v, x|vp , xp )

(12)

bvp ,xp average branching factor, operator pruning, nodes satisfying
conditions context (vp , xp ), p(v, x|vp , xp ) average nodes n satisfying
conditions context (vp , xp ) pn (v, x), percentage ns children, operator
pruning, satisfy conditions context (v, x).
If, every context (vp , xp ), nodes n satisfying conditions defined (vp , xp )
exactly branching factor bvp ,xp exactly value pn (v, x)
contexts (v, x), simple proof induction starting correctness base cases,
N1 (s, d, v, x), shows Ni (s, d, v, x) = Ni (s, d, v, x) i, i.e., prediction
method correctly calculates exactly many nodes satisfy conditions
context every level search tree. follows CDP(s, d) exactly
number nodes IDA* expand given start state IDA* threshold d.
practical setting predictions 2-step model guaranteed
perfect reasoning following conditions hold:
1. heuristic defined exact distance goal abstract state space,
case single pattern database used.
2. two states, s1 , s2 , map abstract state x set
operators {op1 , ..., opk } apply them,
3. states s1 s2 map abstract state x, operators op {op1 , ..., opk }
apply s1 s2 , s1 child op(s1 ) s2 child op(s2 ) map abstract
state, op(x).
60

fiPredicting Performance IDA* using Conditional Distributions

Define context node heuristic value abstract state maps.
Condition (2) guarantees every context (v, x), nodes satisfying conditions
(v, x) exactly branching factor bv,x . true nodes n1
n2 satisfy conditions context (v, x), map abstract state
x, condition (2) requires exactly set operators apply
both. Conditions (2) (3) together guarantee every context (vp , xp ), nodes
satisfying conditions (vp , xp ) exactly value pn (v, x) v x.
true nodes n1 n2 satisfy conditions context (vp , xp ),
map abstract state xp , set operators applies both, operator
op creates child that, cases, maps specific abstract state, op(xp ). Therefore
percentage children map particular abstract state
n1 n2 .
straightforward implementation prediction method setting associates
counter abstract state, initialized number start states map
abstract state. counter abstract state x updated value
(1 d) adding it, operator op, current value counter
abstract state op(y) = x. algorithm computational complexity
O(d |A| 2 ) |A| number abstract states effective branching
factor abstract space. complexity depends linearly d, contrast
typically exponential dependency number nodes IDA* expand,
sufficiently large prediction arbitrarily faster compute search
itself. example, PDB 15-puzzle based positions 8 tiles
blank (roughly 4 billion abstract states), prediction 1000 start states = 52
takes 6% time required execute search.
exact prediction setting two potential uses. first determine
searching single PDB feasible not. example, calculation might show
even first iteration IDA* (with threshold h(start)) take
year complete. second use prediction compare actual performance
alternative method executed set start states (e.g. taking maximum
set PDBs) performance using single PDB without actually execute
IDA* search single PDB.
4.5.2 Poor Predictions
predictions made conditional model extremely inaccurate distribution
heuristic values independent information supplied context. illustrate
example based 4x3 sliding-tile puzzle two heuristics, PDB based
locations tiles 1-7 blank, heuristic returns 0 every state.
given state blank goal position, position even number
moves goal position, heuristic value state taken PDB.
states heuristic value 0. search tree, heuristic used level
therefore opposite one used level 1.
1-step model situation clearly hopeless predicting heuristic
distribution levels PDB used sufficiently large
distribution level converges unconditional distribution.
61

fiZahavi, Felner, Burch, & Holte

hope 2-step model could make reasonably accurate predictions
PDB, considered itself, defines consistent heuristic therefore distribution heuristic values nodes children somewhat correlated heuristic
value nodes parent.
tested using 4x3 sliding-tile puzzle, small enough could
build 2-step model using states state space error introduced
sampling process. test prediction accuracy model generated
50,000 solvable states random and, explained detail next section, used
state start state combination IDA* threshold IDA* would actually
executed iteration threshold given state start state. means
different number start states might used value d. Num column
Table 4 indicates many start states used value (first column)
included table results 5,000 start states used.
IDA* column shows average number nodes expanded IDA* start
states used Prediction column shows number predicted
2-step model. Ratio column Prediction divided IDA*. One clearly see
improvement predictions increases. even deepest depth
sample provided 5,000 start states, prediction factor 6 smaller
true value. course, using constant heuristic value 0 alternate levels
something one would practice, obtained similar results, essentially
reason, 15-puzzle switching, one level next, pattern
database based tiles 1-7 pattern database based tiles 9-15 (see Section 7.1).


27
28
29
30
31
32
33
34
35
36
37
38
39

IDA*
1,212
1,529
2,340
3,072
4,818
6,607
10,748
15,184
24,613
36,726
60,779
96,077
152,079

CDP2
Prediction Ratio
48
0.04
63
0.04
90
0.04
131
0.04
208
0.04
338
0.05
585
0.05
1,027
0.07
1,896
0.08
3,513
0.10
6,737
0.11
12,941
0.13
25,119
0.17

Num
5,754
7,780
9,086
11,561
12,397
14,109
14,109
14,545
13,492
12,261
10,405
8,355
6,505

Table 4: 4x3 sliding-tile puzzle, alternating good heuristic 0.

5. Experimental Setup
next two sections describe experimental results obtained running IDA*
comparing number nodes expanded number predicted KRE
62

fiPredicting Performance IDA* using Conditional Distributions

CDP. experimented two application domains used KRE, namely, Rubiks
Cube (Section 6) sliding-tile puzzle (Section 7). domain evaluated
accuracy two formulas, consistent inconsistent heuristics, set
solvable start states generated random.
experiments reported here, start states used given IDA* threshold
subject special condition. State used start state combination
threshold IDA* actually performs iteration threshold start
state. example, would use start state = 17 distance 11
goal h(s) > 17. addition, sliding-tile puzzle, start state would
used IDA* threshold h(s) different parity. contrast,
experiments KRE paper restrict choice start states way,
start states used every IDA* threshold .
difference start states chosen large impact number
nodes IDA* expands. Table 5 illustrates 15-Puzzle using Manhattan
Distance heuristic IDA* threshold (first column) 43 50. nodes
column Unrestricted shows number nodes IDA* expanded average
50,000 randomly generated solvable start states. values column close
agreement corresponding results Table 5 KRE paper (Korf et al., 2001).
number column shows many start states satisfy additional condition.
remove start states violate condition, IDA* expands substantially fewer
nodes average, shown nodes column Restricted, difference
increases increases. = 50 almost order magnitude difference
number nodes expanded two settings. difference needs kept
mind making comparisons experimental results reported KRE
papers.


43
44
45
46
47
48
49
50

Unrestricted
nodes
439,942
1,014,941
1,985,565
4,542,249
8,963,747
20,355,110
40,479,725
91,329,281

Restricted
number
nodes
22,525
219,001
22,484
393,406
22,937
688,119
22,266 1,182,522
22,243 2,108,766
21,028 3,508,482
20,389 6,037,064
18,758 9,904,973

Table 5: 15-Puzzle Manhattan Distance. effect nodes expanded start states
randomly chosen subject condition.

63

fiZahavi, Felner, Burch, & Holte

6. Experimental Results Rubiks Cube
begin Rubiks Cube experiments. heuristic used 6-edge PDB
heuristic described (Section 3.2.1). experimented (consistent) regular
lookup (inconsistent) random-symmetry dual lookups PDB.
CDP formula, two models used, CDP1 CDP2 , denote 1-step 2-step
models, respectively.
outlined Section 4.1.1, conditional distribution tables built generating one billion states (each generated applying 180 random moves goal state),
computing neighbors, incorporating heuristic information matrix representing one-step model. two-step model generated
grandchildren used heuristic information.
addition, order get reliable samples added following two procedures:
generating children grandchildren sampling used pruning
techniques based operator ordering used main search (see
description Section 2.1.1). is, use sequence operators
would generated main search. done looking random
walk led initial state using last operator random walk
basis operator pruning.
order get reliable sample need entry table sufficiently
sampled. entries table low frequency. example, states
heuristic value 0 rare even sample billion states causing
table 0 row generated small sample. Therefore, enriched
entries artificially creating random states heuristic value 0.
under-sampled entries sampled similar way. One technique, example,
creating (with high probability) random state heuristic value x,
perform random walk length x random state heuristic value 0.
6.1 Rubiks Cube Consistent Heuristics
Table 6 compares KRE CDP1 CDP2 . accuracy three prediction methods
compared using regular lookups 6-edge PDB. Results row
averages set 1000 random states. row presents results IDA* iteration


8
9
10
11
12
13

IDA*
277
3,624
47,546
626,792
8,298,262
110,087,215

KRE
Prediction
257
3,431
45,801
611,385
8,161,064
108,937,712

Ratio
0.93
0.95
0.96
0.98
0.98
0.99

CDP1
Prediction Ratio
235
0.85
3,151
0.87
41,599
0.87
546,808
0.87
7,188,863
0.87
94,711,234
0.86

CDP2
Prediction Ratio
257
0.93
3,446
0.95
45,985
0.97
613,332
0.98
8,180,676
0.99
109,133,021
0.99

Table 6: Rubiks Cube consistent heuristic.
64

fiPredicting Performance IDA* using Conditional Distributions

different threshold (d), given first column. second column (IDA*) presents
actual number nodes expanded IDA* threshold. next columns report
predictions accuracy (Ratio) prediction defined ratio
predicted number actual number expanded nodes. reported
KRE paper, KRE formula found accurate consistent heuristic
averaged large set random start states. table shows CDP1 reasonably
accurate systematically underestimates one-step model consider
nodes parent included among children. elaborate below.
CDP2 predictions accurate, slightly accurate KREs.
6.2 Rubiks Cube Start States Specific Heuristic Values
Table 2, presented (Section 3.2.2), related discussion, show KRE might
make accurate predictions start states restricted specific heuristic
value h. particular example shown (IDA* threshold 12) KRE always predict
value 8, 161, 064, exact value depends specific set start states used
IDA* threshold 12 sufficiently large number nodes
independent start states. Table 7 extends Table 2 include predictions CDP.
shows versions CDP substantially outperform KRE particular set
start states.

h
5
6
7
8
9

IDA*
30,363,829
18,533,503
10,065,838
6,002,025
3,538,964

KRE
Prediction Ratio
8,161,064
0.27
8,161,064
0.44
8,161,064
0.81
8,161,064
1.36
8,161,064
2.31

CDP1
Prediction Ratio
48,972,619
1.61
17,300,476
0.93
7,918,821
0.79
5,094,018
0.85
3,946,146
1.12

CDP2
Prediction Ratio
20,771,895
0.68
13,525,425
0.73
9,131,303
0.91
6,743,686
1.12
5,240,425
1.48

Table 7: Results different start state heuristic values (h) regular PDB
IDA* threshold = 12.

6.3 Rubiks Cube Inconsistent Heuristics
experiments repeated inconsistent heuristics. dual randomsymmetry lookups performed 6-edge PDB instead regular lookup, thereby
creating inconsistent heuristic. discussed Section 3.2.1, KRE produces
prediction heuristics (consistent inconsistent) derived single PDB
overestimates inconsistent heuristics. Table 8 shows CDP2 extremely accurate.
prediction always within 2% actual number nodes expanded.
1-step model used CDP1 systematically underestimates actual number
nodes expanded regular dual lookups (see regular lookup Table 6
dual lookup Table 8). understand why, consider happens node
right side Figure 7 expanded. generates two children, node n (assuming
65

fiZahavi, Felner, Burch, & Holte

KRE
Prediction



IDA*

8
9
10
11
12
13

36
518
6,809
92,094
1,225,538
16,333,931

8
9
10
11
12
13

26
346
4,608
61,617
823,003
10,907,276

CDP1
Ratio Prediction Ratio
Dual
257
7.14
31
0.86
3,431
6.62
418
0.81
45,801
6.73
5,556
0.82
611,385
6.64
74,037
0.80
8,161,064
6.66
987,666
0.81
108,937,712
6.67 13,180,960
0.81
Random Symmetry
257
9.88
26
1.00
3,431
9.92
353
1.02
45,801
9.94
4,718
1.02
611,385
9.92
62,990
1.02
8,161,064
9.92
840,849
1.02
108,937,712
9.99 11,224,108
1.03

CDP2
Prediction Ratio
36
508
6,792
90,664
1,210,225
16,154,640

1.00
0.98
1.00
0.98
0.99
0.99

26
346
4,601
61,174
815,444
10,878,227

1.00
1.00
1.00
0.99
0.99
1.00

Table 8: Rubiks Cube dual, random-symmetry (inconsistent) heuristics
operators inverses case Rubiks Cube) copy parent R (shown ms
left child Figure 7). child 2 levels deeper R therefore f -value
2 greater Rs. IDA* threshold 5, child potential node
1-step model conclude generate potential child probability
0.5, whereas fact children remain operator pruning potential
nodes.

4 R
3
4

3 n

Figure 7: 1-step model may underestimate
reason 1-step model underestimate number nodes expanded
random-symmetry lookups done child copy R constrained
heuristic value R different symmetries could chosen
different occurrences R. childs f -value correlation f -value R
explanation CDP1 underestimates apply.
fact, different copies state uncorrelated h-values effect operator
pruning needs taken account reduces number children,
done well within 1-step model calculating branching factor.
may advantages using wider context 2-step model results
random-symmetry heuristic show minor case.
66

fiPredicting Performance IDA* using Conditional Distributions

7. Experimental Results - Sliding-Tile Puzzle
KRE experiments sliding-tile puzzle, three state types used, based
whether blank corner, edge, interior location. used state types
experiments used exact recurrence equations N (s, v, d, t) type-dependent
version KRE formula. heuristic used Manhattan Distance (MD). experimented 2-step CDP includes type system recurrence equations.
Results 1-step CDP included performed poorly early versions experiments.
8-puzzle conditional distribution P (v, t|vp , tp ,vgp , tgp ) needed CDP2
typed unconditional distribution P (v, t) needed type-dependent KRE formula
computed enumerating states 8-puzzle reachable goal.
15-puzzle, possible exhaustive enumeration entire state
space conditional distributions estimated generating ten billion reachable
states random. uniform random sample used estimate P (v, t) KRE,
state sample used gp sampling method described Section 4.1.2
P (v, t|vp , tp ,vgp , tgp ). latter, however, basic sampling method extended
even processing ten billion gp states entries 6-dimensional
matrix missing sampled sufficiently. correct this, generate
gp, children, grandchildren update matrix accordingly, check
matrix already contains data gps great-grandchildren. generate
gps great-grandchildren update corresponding entries matrix. continues
long encounter contexts never seen before. introduces small
statistical bias sample, guarantees sample contains required
data.

h

#States

12
14
16
18
20

11,454
19,426
18,528
10,099
2,719

34
36
38
40
42
44

1,331
2,330
2,999
3,028
2,454
1,507

KRE
IDA*
Prediction
Ratio
8-puzzle depth 22
1,499
1,391
0.93
1,042
1,404
1.35
660
1,419
2.15
377
1,447
3.84
168
1,503
8.95
15-puzzle depth 52
77,028,888 420,858,250
5.46
38,206,986 424,113,561
11.10
16,226,330 428,883,700
26.43
6,310,724 433,096,514
68.63
2,137,488 438,475,079 205.14
620,322 444,543,678 716.63

CDP2
Prediction Ratio
1,809
1,051
544
246
91

1.21
1.01
0.82
0.65
0.54

172,845,559
64,247,275
21,505,426
6,477,903
1,749,231
409,341

2.24
1.68
1.33
1.03
0.82
0.66

Table 9: sliding-tile puzzles consistent heuristic (MD).
Prediction results KRE CDP2 8- 15-puzzles shown Table 9
format above. 8-puzzle predictions made IDA* threshold
67

fiZahavi, Felner, Burch, & Holte

22 row corresponds group 8-puzzle states heuristic
value h (shown first column) IDA* would actually used threshold
22. second column gives number states group. Clearly, shown
IDA* column, states higher initial heuristic values IDA* expanded smaller
number nodes. trend reflected KRE predictions since KRE take
h account. KRE difference attributes different rows
different type distribution given group. Thus, predicted number expanded
nodes KRE similar rows (around 1,400). CDP formula takes heuristic
value start state account able predict number expanded nodes
much better KRE. bottom part Table 9 show results 15-puzzle
IDA* threshold 52. Similar tendencies observed.
7.1 Inconsistent Heuristics Sliding-tile Puzzle
next experiment inconsistent heuristic 8-puzzle. defined two PDBs,
one based location blank tiles 14, based location
blank tiles 58. create inconsistent heuristic, one PDBs consulted
regular lookup. choice PDB made systematically, randomly, based
position blank. Different occurrences state guaranteed
lookup neighboring states guaranteed consult different PDBs
causes inconsistency. results presented Table 10 variety IDA* thresholds.
threshold Num column indicates many start states used.
results show CDPs predictions reasonably accurate, much accurate
KREs overestimate factor 26.


18
19
20
21
22
23
24
25
26
27
28
29

Num
44,243
40,773
60,944
48,888
60,345
40,894
42,031
22,494
18,668
7,036
4,131
762

IDA*
14.5
22.2
27.4
43.3
58.5
95.4
135.7
226.7
327.8
562.0
818.4
1,431.7

KRE
Prediction
80.4
151.5
244.2
459.0
734.4
1,383.6
2,200.6
4,155.3
6,569.9
12,475.0
19,515.7
37,424.6

Ratio
5.56
6.82
8.91
10.59
12.55
14.50
16.21
18.33
20.04
22.20
23.85
26.14

CDP2
Prediction Ratio
10.4
0.72
16.1
0.73
20.2
0.74
32.1
0.74
44.0
0.75
72.5
0.76
103.4
0.76
174.2
0.77
251.0
0.77
432.2
0.77
618.8
0.76
1,074.8
0.75

Table 10: Inconsistent heuristic 8-puzzle.
Similar experiments conducted 15-puzzle. Here, first PDB based
location blank tiles 17, based location
blank tiles 915. Table 11 shows results IDA* thresholds 48 55 (recall
median solution length puzzle 52). numbers shown averages
68

fiPredicting Performance IDA* using Conditional Distributions

50,000 start states. CDP predictions 15-puzzle considerably worse
8-puzzle, KRE predictions degraded much more. reason
inaccuracy predictions discussed Section 4.5.2. Much accurate
predictions produced context extended include heuristic value
pattern databases, one search algorithm actually consults.


48
49
50
51
52
53
54
55

IDA*
231,939.6
388,201.1
644,350.1
1,062,597.5
1,746,025.1
2,773,611.6
4,539,767.0
7,546,286.9

KRE
Prediction
311,462,527.1
664,920,142.2
1,413,202,357.9
3,014,405,997.5
6,404,191,951.4
13,639,455,787.3
29,035,096,650.9
61,899,533,064.7

Ratio
1,342.9
1,712.8
2,193.2
2,836.8
3,667.9
4,917.6
6,395.7
8,202.6

CDP2
Prediction
71,550.2
149,257.5
313,132.4
663,004.4
1,402,898.2
2,985,321.1
6,361,011.5
13,627,941.8

Ratio
0.308
0.384
0.486
0.624
0.803
1.076
1.401
1.806

Table 11: Inconsistent heuristic 15-puzzle.

8. Accurate Predictions Single Start States
seen CDP works well base cases recursive calculation
Ni (s, d, v) seeded large set start states, matter heuristic values
distributed. However, actual number expanded nodes specific single start
state deviate number predicted CDP. conditional distribution reflects
expected values nodes share context, single start state
interest might behave differently average state context.
Consider Rubiks Cube state heuristic value 8. CDP2 predicts IDA*
expand 6, 743, 686 state IDA* threshold 12. Table 2 shows
average (over 1, 000 start states heuristic value 8) 6, 002, 025 states expanded.
Examining results individual start states showed actual number
expanded nodes ranged 2, 398, 072 15, 290, 697 nodes.
order predict number expanded nodes single start state propose
following enhancement CDP. Suppose want predict number expanded
nodes IDA* threshold start state s. First, perform small initial search
depth r. use states depth r seed base cases CDP formula
compute formula IDA* threshold r. cause larger set nodes
used calculating Ni (s, d, v), thereby improving accuracy CDPs predictions.
8.1 Rubiks Cube, 6-edge PDB Heuristic
Table 12 shows results four specific Rubiks Cube states heuristic value 8 (of
regular 6-edge PDB lookup) IDA* threshold set 12. chose
states least greatest number expanded nodes two states around
median. first column shows actual number nodes IDA* expands state.
69

fiZahavi, Felner, Burch, & Holte

next columns show number expanded nodes predicted enhanced CDP2
formula initial search performed depths (r) 0, 2, 5 6. Clearly,
initial searches give much better predictions original CDP2 (with r = 0),
predicts 6, 743, 686 states. initial search depth 6, predictions
accurate.
h
8
8
8
8

IDA*
2,398,072
4,826,154
9,892,376
15,290,697

CDP2 (r=0)
6,743,686
6,743,686
6,743,686
6,743,686

CDP2 (r=2)
4,854,485
7,072,952
8,555,170
9,432,008

CDP2 (r=5)
3,047,836
5,495,475
9,611,325
13,384,290

CDP2 (r=6)
2,696,532
5,184,453
9,763,455
14,482,001

Table 12: Single state (d = 12).

8.2 Rubiks Cube, 8-6-6 Heuristic
Section 3.2.3 presented KRE predictions two start states, s6 , heuristic value
6, s11 , heuristic value 11, Rubiks Cube 8-6-6 heuristic.
repeat experiments CDP1 . Tables 13 14 show results initial
search depth (r) 0 4. tables show CDP1 able achieve substantially
better predictions KRE cases, initial search depth 4 usually
improved CDP1 predictions.

10
11
12
13
14
15
16
17

IDA*
53,262
422,256
3,413,547
29,114,115
259,577,913
2,451,954,240
24,484,797,237
258,031,139,364

KRE
1,510
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Ratio
0.03
0.05
0.08
0.12
0.18
0.26
0.35
0.44

CDP1 (r=0)
32,207
246,158
1,979,417
16,690,055
149,319,061
1,435,177,445
14,925,206,678
167,181,670,892

Ratio
0.60
0.58
0.58
0.57
0.58
0.59
0.61
0.65

CDP1 (r=4)
69,770
690,556
5,422,001
42,650,077
345,370,148
2,934,134,125
26,380,507,927
254,622,231,216

Ratio
1.31
1.64
1.59
1.46
1.33
1.20
1.08
0.99

Table 13: 8-6-6 PDB, single start state s6
8.3 Experiments 8-Puzzle - Single Start States
performed experiments enhanced CDP2 formula states 8-puzzle
(consistent) MD heuristic. use term trial refer pair single
start state given IDA* threshold d. trials included possible values
start states IDA* would actually perform search IDA* threshold
d. Predictions made trial separately, relative error, predicted/actual,
trial calculated. results shown Figure 8. four curves
figure, KRE, CDP, enhanced CDP initial search depths (r) 5
70

fiPredicting Performance IDA* using Conditional Distributions


11
12
13
14
15
16
17

IDA*
8,526
162,627
2,602,029
38,169,381
542,241,315
7,551,612,957
103,934,322,960

KRE
20,169
269,229
3,593,800
47,971,732
640,349,193
8,547,681,506
114,098,463,567

Ratio
2.37
1.66
1.38
1.26
1.18
1.13
1.10

CDP1 (r=0)
8,246
191,077
3,188,470
47,281,091
665,292,864
9,125,863,883
123,571,401,411

Ratio
0.97
1.17
1.23
1.24
1.23
1.21
1.19

CDP1 (r=4)
8,904
139,422
2,834,542
45,690,554
614,042,865
8,544,807,943
120,978,148,822

Ratio
1.04
0.86
1.09
1.20
1.13
1.13
1.16

Table 14: 8-6-6 PDB, single start state s11

cumulative percentage

100
80
60
40
KRE
CDP2
CDP2 (radius=5)
CDP2 (radius=10)

20
0
0

1

2

3

4

5

6

7

8

9

10

predicted / actual

Figure 8: Relative error 8-puzzle
10. x-axis relative error. y-axis percentage trials
prediction relative error x less. example, y-value 20% KRE
curve x = 0.5 means KRE underestimated factor 2 20%
trials. rightmost point KRE plot (x = 10, = 94%) indicates 6%
trials KREs prediction 10 times actual number nodes expanded.
contrast CDP much larger percentage highly accurate predictions, 99%
predictions within factor two actual number nodes expanded. figure
clearly shows advantage using enhanced CDP. initial search depth
10, 90% trials predictions within 10% correct number.

9. Performance Range Given Unconditional Distribution
experiments paper used 6-edge PDB Rubiks Cube illustrated fact number nodes IDA* expands given PDB vary tremendously
depending PDB used (Zahavi et al., 2007). see clearly, middle
three columns Table 15 show data already seen Tables 6 8, namely,
number nodes IDA* expands 6-edge PDB used regular manner,
71

fiZahavi, Felner, Burch, & Holte

dual lookups, random-symmetry lookups. IDA* expands ten times fewer
nodes 6-edge PDB consulted random-symmetry lookups
consulted normal way.
raises intriguing question range performance achieved
varying conditional distribution unconditional distribution fixed.

8
9
10
11
12
13
Correlation

CDP
257
3,431
45,801
611,385
8,161,064
108,937,712

Regular
277
3,624
47,546
626,792
8,298,262
110,087,215
0.591

Dual
36
518
6,809
92,094
1,225,538
16,333,931
0.359

Random Symmetry
26
346
4,608
61,617
823,003
10,907,276
0.187

CDP
16
210
2,813
37,553
501,273
6,691,215

Table 15: Range IDA* Performace 6-edge Rubiks Cube PDB

9.1 Upper Limit
upper extreme, results nodes expanded, occurs consistent
heuristic used. IDA* expands potential nodes, maximum
number nodes expanded conditional distribution parent
every potential node level potential node level 1. exact calculation
number potential nodes brute-force tree therefore theoretical upper bound
number nodes IDA* expand given unconditional distribution.
already discussed, one way estimate number potential nodes use
KRE formula. estimate upper bound number nodes IDA* could
expand denoted CDP Table 15.
Alternatively, number potential nodes approximated CDP formula
given conditional distribution. Consider Equation 6. summation consider
possible vp values [0, d(i1)] nodes potential nodes level i1. Thus
nodes expanded IDA* level 1 nodes generate
children level i.6 Now, lets substitute vp [0, hmax ]. consider
nodes level 1, even ones potential nodes. Using
summation calculate number nodes heuristic v level even ones
actually generated IDA* (because parents potential nodes, i.e.
vp > (i 1). shown Equation 13.
Ni (s, v) =

hX
max

Ni1 (s, vp ) bvp p(v|vp )

(13)

vp =0

6. Note heuristic consistent vp values {v 1, v,v + 1} need considered
summation nodes values vp (smaller v 1 larger v + 1) cannot
generate children heuristic value v.

72

fiPredicting Performance IDA* using Conditional Distributions

Using general prediction equation get:

CDP =

X
di
X

Ni (s, v)

(14)

i=0 v=0

gives alternative method approximate number potential nodes.
methods approximate upper bound. practice, however, possible
number expanded nodes slightly exceed approximate bound due noise
small errors sampling calculations.
9.2 Lower Limit
consistent heuristics values neighboring states highly correlated.
extreme cases correlation heuristic values neighboring
nodes. is, heuristic value child node statistically independent heuristic
value parent. means regardless parents heuristic value vp , heuristic
values children distributed according unconditional heuristic distribution,
i.e., p(v|vp ) = p(v).
motivation using estimated lower bound number nodes IDA*
could expand given unconditional distribution empirical observation
number nodes IDA* expands decreases correlation parents heuristic
value childrens heuristic values decreases.
illustrated last row three middle columns Table 15, shows
correlation heuristic values neighboring states different types
lookups done 6-edge PDB. calculated using Pearsons correlation coefficient,
defined n pairs x, values according following equation

Correlationxy

Pn
Pn
xi yi i=1 xi i=1 yi
p Pn
= p Pn
Pn
Pn
n i=1 x2i ( i=1 xi )2 n i=1 yi2 ( i=1 yi )2
n

Pn

i=1

(15)

order calculate correlation, 60, 000 random pairs (xi ,yi ) neighboring states
generated. heuristic values computed used Equation 15. bottom
row Table 15 shows number nodes expanded decreases correlation
neighboring heuristic values decreases. leads us suggest number
nodes expanded reach minimum correlation zero.7
estimated lower bound calculated using CDP formula p(v|vp ) = p(v).
denote CDP. 1-step model would calculated using following
equations:
7. theory, possible heuristic negative correlation parents heuristic value
childrens heuristic values, i.e., parents low heuristic values could tend children
large heuristic values vice versa. believe unlikely occur practice.

73

fiZahavi, Felner, Burch, & Holte

d(i1)

Ni (s, d, v) =

X

Ni1 (s, d, vp ) bvp p(v)

(16)

vp =0

CDP =

di
X
X

Ni (s, d, v)

(17)

i=0 v=0

seen comparing rightmost two columns Table 15, randomsymmetry use 6-edge PDB within factor two estimated minimum
possible number nodes expanded PDB, suggests substantially
improve upon performance one would use different PDB.
Table 16 shows estimated upper lower bounds IDA*s performance, range
IDA* thresholds, three different PDBs Rubiks Cube. bounds calculated
using 1, 000 random start states. table shows that, according estimates, inconsistent heuristics based 5-edge PDB outperform consistent heuristics based
6-edge PDB probably cannot outperform consistent heuristics based 7-edge
PDB since estimated lower bound 5-edge PDB larger estimated upper
bound 7-edge PDB.


8
9
10
11
12
13

5-edge PDB
CDP
CDP
2,869
134
38,355
2,278
511,982
30,623
6,834,185
408,775
91,225,920
5,456,512
1,217,726,395 72,836,079

6-edge PDB
CDP
CDP
257
16
3,431
210
45,801
2,813
611,385
37,553
8,161,064
501,273
108,937,712 6,691,215

7-edge PDB
CDP
CDP
42
10
348
42
4,535
291
60,535
3,829
808,051
51,116
10,786,252 682,311

Table 16: Estimated Bounds Performance three Rubiks Cube PDBs.

10. Predicting Performance IDA* BPMX
inconsistent heuristic, heuristic value child much larger
parent. happens state space undirected edges, childs heuristic
value propagated back parent. causes parents f -value exceed
IDA* threshold entire search subtree rooted parent pruned without
generating remaining children. propagation technique called bidirectional
pathmax (BPMX) (Felner et al., 2005; Zahavi et al., 2007). shown effective
reducing search effort pruning subtrees would otherwise explored.
show modify CDP handle BPMX propagation. Since BPMX applies
state spaces undirected edges, discussion section limited spaces.
74

fiPredicting Performance IDA* using Conditional Distributions

10.1 Bidirectional Pathmax (BPMX)
Traditional pathmax (Mero, 1984) propagates heuristic values parent children,
applied state space. Admissibility preserved subtracting cost
connecting edge heuristic value. basic insight bidirectional pathmax
(BPMX) edges undirected heuristic values propagate neighbors,
includes child node parent. process continue distance
direction. BPMX illustrated Figure 9. left side figure shows
inconsistent heuristic values node two children. Consider left child
heuristic value 5. Since value admissible edges example cost
one, immediate neighbors least 4 moves away goal, neighbors
least 3 moves away, on. left child generated, heuristic value
(h = 5) propagate parent right child. preserve
admissibility, propagation along path reduces h cost traversing path.
results h = 4 root h = 3 right child. using IDA*,
bidirectional propagation may cause many nodes pruned would otherwise
expanded. example, suppose current IDA* threshold 2. Without propagation
h left child, root node (f = g + h = 0 + 2 = 2) right child
(f = g + h = 1 + 1 = 2) would expanded. Using propagation, left child
increase parents h value 4, resulting search node abandoned
without even generating right child.

4

2
5

5

1

3

Figure 9: Propagation values inconsistent heuristics

10.2 CDP Overestimates BPMX Applied
inconsistent heuristic used BPMX applied, CDP overestimate
number expanded nodes count nodes subtrees BPMX
prunes. Section 4.2, defined Ni (s, d, v) number nodes IDA*
generate level heuristic value exactly equal v start state
IDA* threshold. formula given estimating Ni (s, d, v) (Equation 5) was:
d(i1)

Ni (s, d, v) =

X

Ni1 (s, d, vp ) bvp p(v|vp )

vp =0

calculating Ni (s, d, v) Ni1 (s, d, vp ) formula assumes node
expanded children generated. Ni1 (s, d, vp ) multiplied
branching factor bvp . BPMX applied, child may prune parent
rest children generated. happens, assumption children
expanded nodes generated would wrong. example, without BPMX,
75

fiZahavi, Felner, Burch, & Holte

expanding root left tree Figure 9 children generated child
right expanded. Indeed CDP count two nodes case. BPMX
applied root expanded child right generated (and therefore
expanded). Thus, CDP, counts two nodes, overestimating number
nodes expanded. following section modify equation correct this.
10.3 New Formula Estimating Ni (s, d, v)
Let n node currently expanded. Assume n b children
consider order generated. call order generation order.
Note BPMX applied, probability child generated decreases
move generation order. Children appear late order
larger chance generated since previous children might
cause BPMX cutoff. Let pbx (l) probability child location l order
generated even BPMX applied. definition extend Equation 5
follows:
d(i1) bvp

Ni (s, d, v) =

X X
{Ni1 (s, d, vp ) pbx (l) p(v|vp )}
vp =0

(18)

l=1

Ni (s, d, v) calculated similar way Equation 5, except way count
total number children IDA* generates via nodes expands level 1
heuristic value equal vp . idea iterate possible locations
generation order calculate probability node location l generated.
practice, however, actual context pbx variables besides location l.
includes IDA* threshold (d), depth parent (i 1) heuristic
value parent (vp ), thus get final formula:
d(i1) bvp

Ni (s, d, v) =

X X
{Ni1 (s, d, vp ) pbx (l, d, 1, vp ) p(v|vp )}
vp =0

(19)

l=1

exactly equal Equation 5 special case pbx (l) = 1 l,
happens BPMX used used consistent heuristic.
10.4 Calculating pbx
simplicity, model assumes heuristic value propagated BPMX
one level tree. means state pruned immediate
children descendants deeper levels. make assumption another
reason besides simplicity description. experiments Rubiks Cube
domains showed indeed almost pruning BPMX caused 1-level BPMX
propagation. generalized formula deeper BPMX propagations similarly
developed include complicated recursive terms low practical value,
least state spaces heuristics studied.
Assume c child n location l generation order. Child c
generated n pruned l 1 children appear c
76

fiPredicting Performance IDA* using Conditional Distributions

generation order. Assume n level threshold d. Since n
expanded, h(n) i. BPMX h(n) increased (and cause BPMX pruning)
child k h(k) > + 1. case, h(k) 1 larger i,
used instead h(n) IDA* decide expand n additional children
generated. Therefore, order child c location l generation order
generated, l 1 predecessors generation order must heuristics less
equal + 1. Assuming heuristic value parent v probability

pbx (l, d, i, v) = {

di+1
X

p(h|v)}l1

(20)

h=0

sum probability relevant heuristic value raise sum
power l 1 since l 1 children appear c.
10.5 Experiments Rubiks Cube BPMX
repeated experiments Rubiks Cube 6-edge PDB BPMX
activated. Since BPMX affects inconsistent heuristics, Dual Random
Symmetry heuristics tested. heuristic tested IDA* thresholds 8
13. results, averaged set 1, 000 random states, presented
Table 17. BPMX columns repeated Table 8. additional columns
show results BPMX. column IDA* + BPMX presents actual number
expanded nodes using BPMX. BPMX reduces number nodes expanded
30% Dual 25% reduction Random Symmetry, making
unmodified CDP2 predictions high amount. CDPbx
2 column
shows modifications introduced section greatly improve accuracy.



IDA*

8
9
10
11
12
13

36
518
6,809
92,094
1,225,538
16,333,931

8
9
10
11
12
13

26
346
4,608
61,617
823,003
10,907,276

BPMX
CDP2

BPMX
Ratio IDA* + BPMX
CDPbx
2
Dual
36
1.00
26
24
508
0.98
353
328
6,792
1.00
4,700
4,387
90,664
0.98
62,405
58,562
1,210,225
0.99
831,362
781,704
16,154,640
0.99
11,091,676 10,434,547
Random Symmetry
26
1.00
19
18
346
1.00
256
240
4,601
1.00
3,432
3,207
61,174
0.99
45,881
42,818
815,444
0.99
608,816
571,556
10,878,227
1.00
8,125,962
7,629,396

Ratio
0.92
0.93
0.93
0.94
0.94
0.94
0.95
0.94
0.93
0.93
0.94
0.94

Table 17: BPMX Rubiks Cube - Dual & Random Symmetry

77

fiZahavi, Felner, Burch, & Holte

11. Related Work
Previous work predicting A* IDA*s performance properties heuristic falls
two main camps. first bases analysis accuracy heuristic,
second bases analysis, done, distribution heuristic values.
next two subsections survey approaches.
11.1 Analysis Based Heuristics Accuracy
One common approach characterize heuristic focusing error heuristic
value (deviation optimal cost). first analysis line, focusing effect
errors performance search algorithms, done Pohl (1970). Many
papers line appeared since (Pohl, 1977; Gaschnig, 1979; Huyn, Dechter, &
Pearl, 1980; Karp & Pearl, 1983; Pearl, 1984; Chenoweth & Davis, 1991; McDiarmid &
Provan, 1991; Sen, Bagchi, & Zhang, 2004; Dinh, Russell, & Su, 2007; Helmert & Roger,
2008).
works usually assume abstract model space tree every node
exactly b children aim provide asymptotic estimation number expanded
nodes. mainly differ model assumptions (e.g. binary non-binary trees)
case results derived (worst case average case). Worst case analysis
showed correlation
heuristic errors search complexity.
|h(n)h (n)|
found relative error,
, constant, search complexity
h (n)
exponential (in length solution path) absolute error, |h(n) h (n)|,
bounded constant search complexity linear (Pohl, 1977; Gaschnig, 1979). Three
main assumptions used Pohl (1977) branching factor assumed
constant across inputs, single goal state transpositions
search space. assumptions hold, case many standard
benchmark domains planning, general search algorithms A* explore exponential
number states even assumption almost perfect heuristic (i.e., heuristic
whose error bounded small additive constant) (Helmert & Roger, 2008).
Since difficult guarantee precise bounds magnitude errors produced
given heuristic, probabilistic characterization magnitudes suggested (Huyn
et al., 1980; Pearl, 1984). Heuristics modeled random variables (RVs), relative
errors assumed independent identically distributed (IID model). model,
attaining average polynomial A* complexity proved essentially equivalent
requiring values h(n) clustered near h (n) allowed deviation
logarithmic function h (n) itself.
Additional research line conducted Chenoweth Davis (1991). Instead
using IID model, suggested using NC model, places constraints
errors h. model heuristic defined according heuristic values grow respect distance goal, according error.
predicted A* complexity polynomial whenever values h(n)
logarithmicaly clustered near h (n) + (h (n)), arbitrary, non-negative,
non-decreasing function. Heuristics whose values grow slower distance
goal cause exponential complexity. Studies NC model showed replacing
78

fiPredicting Performance IDA* using Conditional Distributions

heuristic h wh w 0 often change A* complexity exponential
polynomial.
works focused tree searches. contrast, Sen et al. (2004) presented
general technique extending analysis average case performance A*
search spaces trees search spaces directed acyclic graphs. analytical results show expected complexity change exponential polynomial
heuristic estimates nodes become accurate restrictions placed
cost matrix. Recent research line, analyzing complexity A* algorithm
presented Dinh et al. (2007). research presented worst average case analysis performance A* approximately accurate heuristics8 search problems
multiple solutions. Bounds presented paper proved dependent
heuristic accuracy distribution solutions.
11.2 Analysis Based Heuristic Distribution
discussed outset paper, KRE suggested alternative approach calculating time complexity IDA* multiple-goal spaces (Korf & Reid, 1998; Korf
et al., 2001). Arguing heuristic accuracy difficult obtain, suggested
deriving analysis unconditional distribution heuristic values, easy
determine least approximately. came method deriving
closed-form formula Ni , number nodes level brute-force search tree.
method later formalized (Edelkamp, 2001b). Unlike work described
previous subsection, provides big-O complexity analysis, KREs aim (and ours)
exactly predict number nodes IDA* expand.
KRE correctly point that, operators cost, Ni must
defined number nodes reached path cost i, opposed
number nodes edges start state. calculation Ni
general setting studied detail Ruml, slightly different context (Ruml,
2002). solution involves using conditional distribution edge costs bears
strong resemblance conditional distribution heuristic values.
Based work KRE insight PDB heuristics correlation
size PDB heuristic value distribution, new analysis limited
PDB heuristics done (Korf, 2007; Breyer & Korf, 2008). prediction achieved
based branching factor problem size PDB without knowing
actual heuristic distribution. order derive heuristic distribution
size PDB assumed forward backward branching factors
abstract space equal abstract space negligible number cycles.
Since second assumption usually realistic model underestimates number
expanded nodes.
KRE formula developed predict performance IDA* algorithm.
general approach applied A* long appropriate modifications made
computations Ni P (v) (Korf et al., 2001; Holte & Hernadvolgyi, 2004; Breyer
& Korf, 2008). challenge accounting effect A*s pruning search
tree generates state previously reached path smaller equal
8. heuristic -approximation (1 )h (s) h(s) (1 + )h (s) states search space.

79

fiZahavi, Felner, Burch, & Holte

cost. particularly challenging heuristic inconsistent, case
first time A* generates state guaranteed reached via least-cost
path, state occur A*s search tree. Indeed, worst case,
every state A* enumerate paths state decreasing order cost,
thereby generating exactly search tree IDA* (Martelli, 1977). general,
A*s pruning reduce Ni , especially large i, ways may hard capture
small set recurrence equations. heuristic distribution A*s entire search
tree, taken maximum depth, is, consistent heuristics, overall distribution (Korf
et al., 2001) since state occurs exactly A*s search tree (as observed,
true inconsistent heuristics). imply overall distribution
used good effect level-by-level basis, use KRE formula result
accurate predictions A*s performance 15-puzzle two different consistent
heuristics used together exact calculation Ni A*s search tree (Breyer
& Korf, 2008).

12. Conclusions Future Work
Historically, heuristics characterized average. KRE introduced idea
characterizing heuristics unconditional heuristic distribution presented
formula predict number nodes expanded one iteration IDA* based
unconditional heuristic distribution. work presented paper takes another
step along line. conditional distribution introduced, prediction
formula CDP based it, advance understanding properties heuristic affect
performance IDA*.
CDP method advances KRE improving predictions shallow depths,
wider range sets start states, inconsistent heuristics. shown
use make accurate prediction single start state IDA* search
uses BPMX heuristic value propagation.
course, sophisticated methods, preprocessing needed
special care must taken gathering data order get reliable sample.
much easier calculate average heuristic calculate 3-dimensional
matrix. hand, latter approach better characterizes heuristic
enables generating accurate predictions larger variety circumstances.
Future work address number issues. yet clear attributes make
best context prediction, influenced choice heuristic
attributes specific domain. Larger contexts (more parameters) probably provide better prediction cost pre-processing. tradeoff needs
studied. Another direction aim extend analysis approach predict
performance search algorithms A*.

13. Acknowledgments
research supported grant number 728/06 305/09 Israeli Science
Foundation (ISF) Ariel Felner. Robert Holte Neil Burch gratefully acknowledge
ongoing support work Canadas Natural Sciences Engineering Research
80

fiPredicting Performance IDA* using Conditional Distributions

Council (NSERC) Albertas Informatics Circle Research Excellence (iCORE).
code Rubiks Cube paper based implementation Richard E. Korf
used seminal work domain(Korf, 1997). thank anonymous reviewer
encouraged us widen experimental results better explain results
KRE relation results. His/her comments clearly improved strength
paper. Thanks Sandra Zilles careful checking details Section 4.

References
Breyer, T., & Korf, R. (2008). Recent results analyzing performance heuristic
search. Proceedings First International Workshop Search Artificial
Intelligence Robotics (held conjunction AAAI), pp. 2431.
Chenoweth, S. V., & Davis, H. W. (1991). High-performance A* search using rapidly
growing heuristics. Proceedings Twelfth International Joint Conference
Artificial Intelligence (IJCAI-91), pp. 198203.
Culberson, J. C., & Schaeffer, J. (1994). Efficiently searching 15-puzzle. Tech. rep.
94-08, Department Computer Science, University Alberta.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14 (3), 318334.
Dinh, H. T., Russell, A., & Su, Y. (2007). value good advice: complexity
A* search accurate heuristics. Proceedings Twenty-Second Conference
Artificial Intelligence (AAAI-07), pp. 11401145.
Edelkamp, S. (2001a). Planning pattern databases. Proceedings 6th European
Conference Planning (ECP-01), pp. 1334.
Edelkamp, S. (2001b). Prediction regular search tree growth spectral analysis.
Advances Artificial Intelligence, Joint German/Austrian Conference AI,
(KI/OGAI-2001), pp. 154168.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal
Artificial Intelligence Research, 22, 279318.
Felner, A., Korf, R. E., Meshulam, R., & Holte, R. C. (2007). Compressed pattern databases.
Journal Artificial Intelligence Research, 30, 213247.
Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. C. (2005). Dual lookups pattern
databases. Proceedings Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05), pp. 103108.
Gaschnig, J. (1979). Performance Measurement Analysis Certain Search Algorithms.
Ph.D. thesis, Carnegie-Mellon University.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination minimum cost paths. IEEE Transactions Systems Science Cybernetics,
SCC-4(2), 100107.
Helmert, M., & Roger, G. (2008). good almost perfect?. Proceedings
Twenty-Third Conference Artificial Intelligence (AAAI-08), pp. 944949.
81

fiZahavi, Felner, Burch, & Holte

Holte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizing
multiple pattern databases speeds heuristic search. Artificial Intelligence, 170 (1617), 11231136.
Holte, R. C., & Hernadvolgyi, I. T. (2004). Steps towards automatic creation search
heuristics. Tech. rep. TR04-02, Computing Science Department, University Alberta.
Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis complexity A*.
Artificial Intelligence, 15 (3), 241254.
Karp, R. M., & Pearl, J. (1983). Searching optimal path tree random costs.
Artificial Intelligence, 21 (1-2), 99116.
Korf, R. E. (1985). Depth-first iterative-deepening: optimal admissible tree search.
Artificial Intelligence, 27 (1), 97109.
Korf, R. E. (1997). Finding optimal solutions Rubiks Cube using pattern databases.
Proceedings Fourteenth Conference Artificial Intelligence (AAAI-97), pp.
700705.
Korf, R. E. (2007). Analyzing performance pattern database heuristics. Proceedings
Twenty-Second Conference Artificial Intelligence (AAAI-07), pp. 11641170.
Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134 (1-2), 922.
Korf, R. E., & Reid, M. (1998). Complexity analysis admissible heuristic search.
Proceedings Fifteenth Conference Artificial Intelligence (AAAI-98), pp. 305
310.
Korf, R. E., Reid, M., & Edelkamp, S. (2001). Time complexity Iterative-Deepening-A* .
Artificial Intelligence, 129 (1-2), 199218.
Martelli, A. (1977). complexity admissible search algorithms. Artificial Intelligence, 8, 113.
McDiarmid, C. J. H., & Provan, G. M. (1991). expected-cost analysis backtracking
non-backtracking algorithms. Proceedings Twelfth International Joint
Conference Artificial Intelligence (IJCAI-91), pp. 172177.
McNaughton, M., Lu, P., Schaeffer, J., & Szafron, D. (2002). Memory-efficient A* heuristics
multiple sequence alignment. Proceedings Eighteenth Conference
Artificial Intelligence (AAAI-02), pp. 737743.
Mero, L. (1984). heuristic search algorithm modifiable estimate. Artificial Intelligence, 23 (1), 1327.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving.
Addison & Wesley.
Pohl, I. (1970). Heuristic search viewed path finding graph. Artificial Intelligence,
1 (3), 193204.
Pohl, I. (1977). Practical theoretical considerations heuristic search algorithms.
Machine Intelligence, 8, 5572.
82

fiPredicting Performance IDA* using Conditional Distributions

Ratner, D., & Warmuth, M. K. (1986). Finding shortest solution n n extension
15-puzzle intractable. Proceedings Fifth Conference Artificial
Intelligence (AAAI-86), pp. 168172.
Ruml, W. (2002). Adaptive Tree Search. Ph.D. thesis, Harvard University.
Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis best-first search
two representative directed acyclic graphs. Artificial Intelligence, 155 (1-2), 183206.
Zahavi, U., Felner, A., Burch, N., & Holte, R. C. (2008). Predicting performance
IDA* conditional distributions. Proceedings Twenty-Third Conference
Artificial Intelligence (AAAI-08), pp. 381386.
Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search permutation
state spaces. Proceedings Twenty-First Conference Artificial Intelligence
(AAAI-06), pp. 10761081.
Zahavi, U., Felner, A., Holte, R. C., & Schaeffer, J. (2008). Duality permutation state
spaces dual search algorithm. Artificial Intelligence, 172 (4-5), 514540.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. R. (2007). Inconsistent heuristics.
Proceedings Twenty-Second Conference Artificial Intelligence (AAAI-07),
pp. 12111216.
Zhou, R., & Hansen, E. A. (2004). Space-efficient memory-based heuristics. Proceedings
Nineteenth Conference Artificial Intelligence (AAAI-04), pp. 677682.

83


