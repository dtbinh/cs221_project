journal artificial intelligence

submitted published

frequency meaning
vector space semantics
peter turney

peter turney nrc cnrc gc ca

national council canada
ottawa ontario canada k r

patrick pantel

patrickpantel com

yahoo labs
sunnyvale ca usa

abstract
computers understand little meaning human language profoundly
limits ability give instructions computers ability computers explain
actions us ability computers analyse process text vector space
vsms semantics beginning address limits surveys
use vsms semantic processing text organize literature vsms according
structure matrix vsm currently three broad classes vsms
termdocument wordcontext pairpattern matrices yielding three classes
applications survey broad range applications three categories
take detailed look specific open source project category goal
survey breadth applications vsms semantics provide
perspective vsms already familiar area provide
pointers literature less familiar field

introduction
one biggest obstacles making full use power computers
currently understand little meaning human language recent progress
search engine technology scratching surface human language yet
impact society economy already immense hints transformative
impact deeper semantic technologies vector space vsms surveyed
likely part semantic technologies
use term semantics general sense meaning word
phrase sentence text human language study meaning
concerned narrower senses semantics semantic web approaches
semantics formal logic present survey vsms relation
distributional hypothesis representing aspects natural language
semantics
vsm developed smart information retrieval system salton
gerard salton colleagues salton wong yang smart pioneered
many concepts used modern search engines manning raghavan
schutze idea vsm represent document collection
point space vector vector space points close together space
semantically similar points far apart semantically distant users
c

ai access foundation national council canada reprinted permission

fiturney pantel

query represented point space documents query pseudodocument documents sorted order increasing distance decreasing semantic
similarity query presented user
success vsm information retrieval inspired researchers extend
vsm semantic tasks natural language processing impressive
instance rapp used vector representation word meaning achieve
score multiple choice synonym questions test english foreign
language toefl whereas average human score turney used
vector representation semantic relations attain score multiple choice
analogy questions sat college entrance test compared average human score

survey organized past work vsms according type matrix
involved termdocument wordcontext pairpattern believe choice
particular matrix type fundamental choices particular
linguistic processing mathematical processing although three matrix types cover
work reason believe three types exhaust possibilities
expect future work introduce types matrices higher order tensors
motivation vector space semantics
vsms several attractive properties vsms extract knowledge automatically
given corpus thus require much less labour approaches semantics
hand coded knowledge bases ontologies example main resource used
rapps vsm system measuring word similarity british national corpus
bnc whereas main resource used non vsm systems measuring word similarity
hirst st onge leacock chodrow jarmasz szpakowicz
lexicon wordnet rogets thesaurus gathering corpus language
generally much easier building lexicon building lexicon often involves
gathering corpus semcor wordnet miller leacock tengi bunker
vsms perform well tasks involve measuring similarity meaning
words phrases documents search engines use vsms measure similarity
query document manning et al leading measuring semantic relatedness use vsms pantel lin rapp turney littman
bigham shnayder leading measuring similarity semantic relations use vsms lin pantel turney nakov hearst
section discusses differences types similarity
vsms especially interesting due relation distributional hypothesis related hypotheses see section distributional hypothesis
regarding average score toefl questions landauer dumais note
although know performance would compare example u school
children particular age told average score adequate admission many
universities
average score highschool students senior year applying us universities
discussion score see section turneys
vector first order tensor matrix second order tensor see section
see http www natcorp ox ac uk
see http wordnet princeton edu



fifrom frequency meaning

words occur similar contexts tend similar meanings wittgenstein
harris weaver firth deerwester dumais landauer furnas harshman efforts apply abstract hypothesis concrete measuring
similarity meaning often lead vectors matrices higher order tensors
intimate connection distributional hypothesis vsms strong motivation
taking close look vsms
uses vectors matrices count vector space purposes
survey take defining property vsms values elements
vsm must derived event frequencies number times given word
appears given context see section example often lexicon knowledge
base may viewed graph graph may represented adjacency matrix
imply lexicon vsm general values
elements adjacency matrix derived event frequencies emphasis
event frequencies brings unity variety vsms explicitly connects
distributional hypothesis furthermore avoids triviality excluding many possible
matrix representations
vectors ai cognitive science
vectors common ai cognitive science common vsm
introduced salton et al novelty vsm use frequencies
corpus text clue discovering semantic information
machine learning typical learn classify cluster set items
e examples cases individuals entities represented feature vectors mitchell
witten frank general features derived event frequencies
although possible see section example machine learning
applied classifying clustering documents sebastiani
collaborative filtering recommender systems use vectors resnick iacovou
suchak bergstrom riedl breese heckerman kadie linden smith
york typical recommender system person item matrix
rows correspond people customers consumers columns correspond items
products purchases value element rating poor fair excellent
person given item many mathematical techniques work well
termdocument matrices see section work well person item matrices
ratings derived event frequencies
cognitive science prototype theory often makes use vectors basic idea
prototype theory members category central others rosch
lloyd lakoff example robin central prototypical member
category bird whereas penguin peripheral concepts varying degrees
membership categories graded categorization natural way formalize
represent concepts vectors categories sets vectors nosofsky smith osherson rips keane however vectors usually numerical scores
elicited questioning human subjects event frequencies
another area psychology makes extensive use vectors psychometrics
studies measurement psychological abilities traits usual instrument


fiturney pantel

measurement test questionnaire personality test test
typically represented subject item matrix rows represent subjects
people experiment columns represent items questions test
questionnaire value element matrix answer corresponding
subject gave corresponding item many techniques vector analysis factor
analysis spearman pioneered psychometrics
cognitive science latent semantic analysis lsa deerwester et al landauer dumais hyperspace analogue language hal lund burgess
atchley lund burgess related landauer mcnamara dennis kintsch entirely within scope vsms defined since
uses vector space values elements derived
event frequencies number times given word appears given context cognitive scientists argued empirical theoretical reasons
believing vsms lsa hal plausible aspects human cognition landauer et al ai computational linguistics information
retrieval plausibility essential may seen sign vsms
promising area
motivation survey
survey vector space semantics currently comprehensive date survey field survey vector space
highly successful semantics wide range potential actual
applications much recent growth area
interest ai researchers work natural language
especially researchers interested semantics survey serve general
introduction area provide framework unified perspective
organizing diverse literature topic encourage area
pointing open areas exploration
survey makes following contributions
framework provide framework organizing literature term
document wordcontext pairpattern matrices see section framework shows
importance structure matrix choice rows columns determining potential applications may inspire researchers explore structures
different kinds rows columns higher order tensors instead matrices
developments draw attention pairpattern matrices use pair
pattern matrices relatively deserves study matrices address
criticisms directed wordcontext matrices regarding lack sensitivity
word order
breadth approaches applications existing survey shows
breadth potential actual applications vsms semantics existing summaries
omit pairpattern matrices landauer et al
focus nlp cl focus survey systems perform practical
tasks natural language processing computational linguistics existing overviews focus
cognitive psychology landauer et al


fifrom frequency meaning

success stories draw attention fact vsms arguably
successful semantics far
intended readership
goal writing survey state art vector space
semantics introduce topic area give
perspective already familiar area
assume reader basic understanding vectors matrices linear algebra
one might acquire introductory undergraduate course linear algebra
text book golub van loan basic concepts vectors matrices
important mathematical details widdows gives gentle
introduction vectors perspective semantics
assume reader familiarity computational linguistics information retrieval manning et al provide good introduction information retrieval
computational linguistics recommend manning schutzes text
reader familiar linear algebra computational linguistics survey
present barriers understanding beyond background necessary
familiar vsms used information retrieval natural language processing computational linguistics however reader would
background reading recommend landauer et al collection
highlights outline
article structured follows section explains framework organizing
literature vsms according type matrix involved termdocument wordcontext
pairpattern section present overview vsms without getting
details matrix generated corpus raw text
high level framework place sections examine steps involved
generating matrix section discusses linguistic processing section reviews
mathematical processing order corpus would processed
vsm systems first linguistic processing mathematical processing
vsms used semantics input model usually plain text
vsms work directly raw text first apply linguistic processing
text stemming part speech tagging word sense tagging parsing section
looks linguistic tools semantic vsms
simple vsm simple termdocument vsm value element
document vector number times corresponding word occurs given
document vsms apply mathematical processing raw frequency values
section presents main mathematical operations weighting elements smoothing
matrix comparing vectors section describes optimization strategies
comparing vectors distributed sparse matrix multiplication randomized
techniques
end section reader general view concepts involved
vector space semantics take detailed look three vsm systems
section representative termdocument vsms present lucene information


fiturney pantel

retrieval library wordcontext vsms explore semantic vectors package
builds lucene representative pairpattern vsms review latent
relational analysis module space package builds lucene
source code three systems available open source licensing
turn broad survey applications semantic vsms section section serves short historical view semantic vsms beginning
information retrieval section purpose give reader idea
breadth applications vsms provide pointers literature
reader wishes examine applications detail
termdocument matrix rows correspond terms columns correspond documents section document provides context understanding term
generalize idea documents chunks text arbitrary size phrases sentences
paragraphs chapters books collections wordcontext matrix includes termdocument matrix special case section discusses applications
wordcontext matrices section considers pairpattern matrices rows correspond pairs terms columns correspond patterns pairs
occur
section discuss alternatives vsms semantics section considers
future vsms raising questions power limitations conclude
section

vector space semantics
theme unites forms vsms discuss
stated statistical semantics hypothesis statistical patterns human word usage
used figure people mean general hypothesis underlies several
specific hypotheses bag words hypothesis distributional hypothesis
extended distributional hypothesis latent relation hypothesis discussed
similarity documents termdocument matrix
use following notational conventions matrices denoted bold
capital letters vectors denoted bold lowercase letters b scalars represented
lowercase italic letters c
large collection documents hence large number document
vectors convenient organize vectors matrix row vectors matrix
correspond terms usually terms words discuss possibilities





see http lucene apache org java docs
see http code google com p semanticvectors
see http code google com p airhead wiki latentrelationalanalysis
phrase taken faculty profile george furnas university michigan
http www si umich edu people faculty detail htm sid full quote statistical semantics
studies statistical patterns human word usage used figure people
mean least level sufficient information access term statistical semantics appeared
work furnas landauer gomez dumais defined



fifrom frequency meaning

column vectors correspond documents web example kind
matrix called termdocument matrix
mathematics bag called multiset set except duplicates
allowed example b c c c bag containing b c order matter
bags sets bags b c c c c c b c equivalent represent
bag b c c c vector x h stipulating first element
x frequency bag second element frequency b bag
third element frequency c set bags represented matrix x
column x j corresponds bag row xi corresponds unique member
element xij frequency th member j th bag
termdocument matrix document vector represents corresponding document
bag words information retrieval bag words hypothesis
estimate relevance documents query representing documents
query bags words frequencies words document tend indicate
relevance document query bag words hypothesis basis
applying vsm information retrieval salton et al hypothesis expresses
belief column vector termdocument matrix captures degree
aspect meaning corresponding document document
let x termdocument matrix suppose document collection contains n documents unique terms matrix x rows one row unique
term vocabulary n columns one column document let wi th
term vocabulary let dj j th document collection th row
x row vector xi j th column x column vector x j row vector
xi contains n elements one element document column vector x j contains
elements one element term suppose x simple matrix frequencies
element xij x frequency th term wi j th document dj
general value elements x zero matrix sparse
since documents use small fraction whole vocabulary randomly
choose term wi document dj likely wi occur anywhere dj
therefore xij equals
pattern numbers xi kind signature th term wi likewise
pattern numbers x j signature j th document dj pattern
numbers tells us degree term document
vector x j may seem rather crude representation document dj tells
us frequently words appear document sequential order words
lost vector attempt capture structure phrases sentences
paragraphs chapters document however spite crudeness search
engines work surprisingly well vectors seem capture important aspect semantics
vsm salton et al arguably first practical useful
extracting semantic information word usage intuitive justification term
document matrix topic document probabilistically influence authors
choice words writing document two documents similar topics
two corresponding column vectors tend similar patterns numbers
newer generative latent dirichlet allocation lda blei ng jordan directly
model intuition see sections



fiturney pantel

similarity words wordcontext matrix
salton et al focused measuring document similarity treating query search
engine pseudo document relevance document query given
similarity vectors deerwester et al observed shift focus
measuring word similarity instead document similarity looking row vectors
termdocument matrix instead column vectors
deerwester et al inspired termdocument matrix salton et al
document necessarily optimal length text measuring word similarity
general may wordcontext matrix context given words
phrases sentences paragraphs chapters documents exotic possibilities
sequences characters patterns
distributional hypothesis linguistics words occur similar contexts
tend similar meanings harris hypothesis justification applying vsm measuring word similarity word may represented vector
elements derived occurrences word contexts
windows words lund burgess grammatical dependencies lin
pado lapata richer contexts consisting dependency links selectional
preferences argument positions erk pado see sahlgrens thesis
comprehensive study contexts similar row vectors wordcontext matrix
indicate similar word meanings
idea word usage reveal semantics implicit things
wittgenstein said language games family resemblance wittgenstein
primarily interested physical activities form context word usage e g
word brick spoken context physical activity building house main
context word often words
weaver argued word sense disambiguation machine translation
co occurrence frequency context words near given target word
word want disambiguate firth p said shall know word
company keeps deerwester et al showed intuitions wittgenstein harris weaver firth could used practical
similarity relations pairpattern matrix
pairpattern matrix row vectors correspond pairs words mason stone
carpenter wood column vectors correspond patterns pairs cooccur x cuts x works lin pantel introduced
pairpattern matrix purpose measuring semantic similarity patterns
similarity column vectors given pattern x solves
able similar patterns solved x resolved x
x resolves
lin pantel proposed extended distributional hypothesis patterns
co occur similar pairs tend similar meanings patterns x solves
wittgensteins intuition might better captured matrix combines words modalities
images monay gatica perez values elements derived event
frequencies would include vsm semantics



fifrom frequency meaning

solved x tend co occur similar x pairs suggests
patterns similar meanings pattern similarity used infer one sentence
paraphrase another lin pantel
turney et al introduced use pairpattern matrix measuring
semantic similarity relations word pairs similarity row vectors
example pairs mason stone carpenter wood potter clay glassblower glass
share semantic relation artisan material case first member pair
artisan makes artifacts material second member pair
pairs tend co occur similar patterns x used x
shaped
latent relation hypothesis pairs words co occur similar patterns
tend similar semantic relations turney word pairs similar row
vectors pairpattern matrix tend similar semantic relations inverse
extended distributional hypothesis patterns similar column vectors
pairpattern matrix tend similar meanings
similarities
pairpattern matrices suited measuring similarity semantic relations
pairs words relational similarity contrast wordcontext matrices suited
measuring attributional similarity distinction attributional relational
similarity explored depth gentner
attributional similarity two words b sima b depends
degree correspondence properties b correspondence
greater attributional similarity relational similarity two pairs
words b c simr b c depends degree correspondence
relations b c correspondence greater relational
similarity example dog wolf relatively high degree attributional similarity whereas dog bark cat meow relatively high degree relational similarity
turney
tempting suppose relational similarity reduced attributional
similarity example mason carpenter similar words stone wood
similar words therefore perhaps follows mason stone carpenter wood
similar relations perhaps simr b c reduced sima c sima b however
mason carpenter potter glassblower similar words artisans
wood clay stone glass materials used artisans cannot infer
mason glass carpenter clay similar relations turney
presented experimental evidence relational similarity reduce attributional
similarity
term semantic relatedness computational linguistics budanitsky hirst
corresponds attributional similarity cognitive science gentner two words
semantically related kind semantic relation budanitsky hirst
semantically related degree share attributes turney
examples synonyms bank trust company meronyms car wheel antonyms
hot cold words functionally related frequently associated pencil


fiturney pantel

might usually think antonyms similar antonyms high
degree attributional similarity hot cold kinds temperature black white
kinds colour loud quiet kinds sound prefer term attributional
similarity term semantic relatedness attributional similarity emphasizes
contrast relational similarity whereas semantic relatedness could confused
relational similarity
computational linguistics term semantic similarity applied words share
hypernym car bicycle semantically similar share hypernym
vehicle resnik semantic similarity specific type attributional similarity
prefer term taxonomical similarity term semantic similarity term
semantic similarity misleading intuitively attributional relational similarity
involve meaning deserve called semantic similarity
words semantically associated tend co occur frequently e g bee
honey chiarello burgess richards pollock words may taxonomically similar semantically associated doctor nurse taxonomically similar semantically associated horse platypus semantically associated taxonomically similar
cradle baby neither semantically associated taxonomically similar calculus
candy
schutze pedersen defined two ways words distributed corpus text two words tend neighbours syntagmatic
associates two words similar neighbours paradigmatic parallels syntagmatic associates often different parts speech whereas paradigmatic parallels
usually part speech syntagmatic associates tend semantically associated bee honey often neighbours paradigmatic parallels tend taxonomically
similar doctor nurse similar neighbours
semantic vsms
possibilities exhausted termdocument wordcontext pairpattern
matrices might want consider triplepattern matrices measuring semantic
similarity word triples whereas pairpattern matrix might row mason
stone column x works triplepattern matrix could row mason
stone masonry column x uses build z however n tuples words grow
increasingly rare n increases example phrases contain mason stone
masonry together less frequent phrases contain mason stone together
triplepattern matrix much sparse pairpattern matrix ceteris paribus
quantity text need order enough numbers make matrices
useful grows rapidly n increases may better break n tuples pairs
example b c could decomposed b c b c turney similarity
two triples b c e f could estimated similarity corresponding
pairs relatively dense pairpattern matrix could serve surrogate relatively
sparse triplepattern matrix
may go beyond matrices generalization matrix tensor kolda
bader acar yener scalar single number zeroth order tensor
vector first order tensor matrix second order tensor tensor order three


fifrom frequency meaning

higher called higher order tensor chew bader kolda abdelali use term
documentlanguage third order tensor multilingual information retrieval turney
uses wordwordpattern tensor measure similarity words van de cruys uses
verbsubjectobject tensor learn selectional preferences verbs
turneys tensor example rows correspond words toefl
multiple choice synonym questions columns correspond words basic english ogden tubes correspond patterns join rows columns hence
wordwordpattern third order tensor given word toefl questions represented corresponding wordpattern matrix slice tensor elements
slice correspond patterns relate given toefl word word
basic english similarity two toefl words calculated comparing two
corresponding matrix slices achieves toefl questions
types tokens
token single instance symbol whereas type general class tokens manning
et al consider following example samuel beckett

ever tried ever failed
matter try
fail fail better

two tokens type ever two tokens type two tokens
type fail lets say line example document three
documents two sentences represent example tokendocument
matrix typedocument matrix tokendocument matrix twelve rows one
token three columns one line figure typedocument matrix
nine rows one type three columns figure
row vector token binary values element given token appears
given document otherwise row vector type integer values element
frequency given type given document vectors related
type vector sum corresponding token vectors example row vector
type ever sum two token vectors two tokens ever
applications dealing polysemy one uses vectors represent word
tokens schutze agirre edmonds another uses vectors represent
word types pantel lin typical word sense disambiguation wsd
deal word tokens instances words specific contexts rather word types
mention approaches polysemy section due similarity close
relationship although defining characteristic vsm concerned
frequencies see section frequency property types tokens
basic english highly reduced subset english designed easy people learn words
basic english listed http ogden basic english org



fiturney pantel

ever tried
ever failed
ever
tried
ever
failed

matter
try

fail

fail
better

matter
try



























fail
fail better













figure tokendocument matrix rows tokens columns documents

ever tried
ever failed
ever
tried
failed

matter
try

fail
better

matter
try





















fail
fail better










figure typedocument matrix rows types columns documents



fifrom frequency meaning

hypotheses
mentioned five hypotheses section repeat hypotheses
interpret terms vectors hypothesis cite work explicitly
states something hypothesis implicitly assumes something hypothesis
statistical semantics hypothesis statistical patterns human word usage
used figure people mean weaver furnas et al units text
similar vectors text frequency matrix tend similar meanings
take general hypothesis subsumes four specific hypotheses
follow
bag words hypothesis frequencies words document tend indicate
relevance document query salton et al documents pseudodocuments queries similar column vectors termdocument matrix
tend similar meanings
distributional hypothesis words occur similar contexts tend similar
meanings harris firth deerwester et al words similar row
vectors wordcontext matrix tend similar meanings
extended distributional hypothesis patterns co occur similar pairs tend
similar meanings lin pantel patterns similar column vectors
pairpattern matrix tend express similar semantic relations
latent relation hypothesis pairs words co occur similar patterns tend
similar semantic relations turney et al word pairs similar row
vectors pairpattern matrix tend similar semantic relations
yet explained means say vectors similar discuss
section

linguistic processing vector space
assume raw data large corpus natural language text
generate termdocument wordcontext pairpattern matrix useful apply
linguistic processing raw text types processing used
grouped three classes first need tokenize raw text need decide
constitutes term extract terms raw text second may want
normalize raw text convert superficially different strings characters
form e g car car cars cars could normalized car third may want
annotate raw text mark identical strings characters different e g fly
verb could annotated fly vb fly noun could annotated fly nn
grefenstette presents good study linguistic processing wordcontext
vsms uses similar three step decomposition linguistic processing tokenization
surface syntactic analysis syntactic attribute extraction
text frequency matrix mean matrix higher order tensor values elements
derived frequencies pieces text context pieces text collection
text text frequency matrix intended general structure includes termdocument
wordcontext pairpattern matrices special cases



fiturney pantel

tokenization
tokenization english seems simple first glance words separated spaces
assumption approximately true english may work sufficiently well basic
vsm advanced vsm requires sophisticated tokenization
accurate english tokenizer must know handle punctuation e g dont janes
hyphenation e g state art versus state art recognize multi word
terms e g barack obama ice hockey manning et al may wish
ignore stop words high frequency words relatively low information content
function words e g pronouns e g popular list
stop words set common words included source code smart
system salton
languages e g chinese words separated spaces basic vsm
break text character unigrams bigrams sophisticated
match input text entries lexicon matching often
determine unique tokenization sproat emerson furthermore native speakers
often disagree correct segmentation highly accurate tokenization challenging
task human languages
normalization
motivation normalization observation many different strings characters often convey essentially identical meanings given want get meaning
underlies words seems reasonable normalize superficial variations converting form common types normalization case folding
converting words lower case stemming reducing inflected words stem
root form
case folding easy english problematic languages french
accents optional uppercase may difficult restore missing accents
converting words lowercase words cannot distinguished without accents
example peche could peche meaning fishing peach peche meaning sin
even english case folding cause case sometimes semantic
significance example smart information retrieval system whereas smart
common adjective bush may surname whereas bush kind plant
morphology study internal structure words often word composed
stem root added affixes inflections plural forms past tenses e g
trapped composed stem trap affix ed stemming kind morphological
analysis process reducing inflected words stems english affixes
simpler regular many languages stemming
heuristics rules thumb work relatively well lovins porter minnen
carroll pearce agglutinative language e g inuktitut many concepts
combined single word prefixes infixes suffixes morphological
analysis complicated single word agglutinative language may correspond
sentence half dozen words english johnson martin
source code available ftp ftp cs cornell edu pub smart



fifrom frequency meaning

performance information retrieval system often measured precision
recall manning et al precision system estimate conditional
probability document truly relevant query system says relevant
recall system estimate conditional probability system say
document relevant query truly relevant
general normalization increases recall reduces precision kraaij pohlmann
natural given nature normalization remove superficial
variations believe irrelevant meaning make easier recognize similarities similar things recall increases sometimes superficial
variations semantic significance ignoring variations causes errors precision
decreases normalization positive effect precision cases variant
tokens infrequent smoothing variations gives reliable statistics
small corpus may able afford overly selective may
best aggressively normalize text increase recall large corpus
precision may important might want normalization hull
gives good analysis normalization information retrieval
annotation
annotation inverse normalization different strings characters may
meaning happens identical strings characters may different
meanings depending context common forms annotation include part speech
tagging marking words according parts speech word sense tagging marking
ambiguous words according intended meanings parsing analyzing grammatical structure sentences marking words sentences according
grammatical roles manning schutze
since annotation inverse normalization expect decrease recall
increase precision example tagging program noun verb may
able selectively search documents act computer programming
verb instead documents discuss particular computer programs noun hence
increase precision however document computer programs noun may
something useful say act computer programming verb even document
never uses verb form program hence may decrease recall
large gains ir performance recently reported query annotation syntactic semantic information syntactic annotation includes query
segmentation tan peng part speech tagging barr jones regelson
examples semantic annotation disambiguating abbreviations queries wei
peng dumoulin finding query keyword associations lavrenko croft
cao nie bai
annotation useful measuring semantic similarity words concepts
wordcontext matrices example pantel lin presented
discover word senses clustering row vectors wordcontext matrix
contextual information derived parsing


fiturney pantel

mathematical processing vector space
text tokenized optionally normalized annotated first step
generate matrix frequencies second may want adjust weights
elements matrix common words high frequencies yet less
informative rare words third may want smooth matrix reduce
amount random noise fill zero elements sparse matrix fourth
many different ways measure similarity two vectors
lowe gives good summary mathematical processing wordcontext vsms
decomposes vsm construction similar four step process calculate frequencies
transform raw frequency counts smooth space dimensionality reduction
calculate similarities
building frequency matrix
element frequency matrix corresponds event certain item term word
word pair occurred certain situation document context pattern certain number
times frequency abstract level building frequency matrix simple matter
counting events practice complicated corpus large
typical building frequency matrix involves two steps first scan sequentially corpus recording events frequencies hash table
database search engine index second use resulting data structure generate
frequency matrix sparse matrix representation gilbert moler schreiber
weighting elements
idea weighting give weight surprising events less weight expected
events hypothesis surprising events shared two vectors discriminative similarity vectors less surprising events example
measuring semantic similarity words mouse rat contexts dissect
exterminate discriminative similarity contexts
information theory surprising event higher information content expected
event shannon popular way formalize idea termdocument
matrices tf idf term frequency inverse document frequency family weighting
functions sparck jones element gets high weight corresponding term
frequent corresponding document e tf high term rare
documents corpus e df low thus idf high salton buckley
defined large family tf idf weighting functions evaluated information retrieval tasks demonstrating tf idf weighting yield significant improvements
raw frequency
another kind weighting often combined tf idf weighting length normalization
singhal salton mitra buckley information retrieval document length
ignored search engines tend bias favour longer documents length
normalization corrects bias
term weighting may used correct correlated terms example
terms hostage hostages tend correlated yet may want normalize


fifrom frequency meaning

term section slightly different meanings
alternative normalizing may reduce weights co occur
document church
feature selection may viewed form weighting terms get
weight zero hence removed matrix forman provides good
study feature selection methods text classification
alternative tf idf pointwise mutual information pmi church hanks
turney works well wordcontext matrices pantel lin
termdocument matrices pantel lin b variation pmi positive pmi
ppmi pmi values less zero replaced zero niwa
nitta bullinaria levy demonstrated ppmi performs better
wide variety weighting approaches measuring semantic similarity word
context matrices turney applied ppmi pairpattern matrices give
formal definition ppmi example effective weighting function
let f wordcontext frequency matrix nr rows nc columns th row
f row vector j th column f column vector f j row
corresponds word wi column f j corresponds context cj value
element fij number times wi occurs context cj let x matrix
ppmi applied f matrix x number rows
columns raw frequency matrix f value element xij x defined
follows
fij
pij pnr pnc

j fij





pnc

j fij
pi pnr pnc



pnr
f
pncij
pnr





pj



j fij
j fij



pij
pmiij log
pi pj

pmiij pmiij
xij
otherwise




definition pij estimated probability word wi occurs context
cj pi estimated probability word wi pj estimated probability
context cj wi cj statistically independent pi pj pij definition
independence thus pmiij zero since log product pi pj
would expect pij wi occurs cj pure random chance hand
interesting semantic relation wi cj expect pij larger
would wi cj indepedent hence pij pi pj
thus pmiij positive follows distributional hypothesis see section
word wi unrelated context cj may pmiij negative ppmi
designed give high value xij interesting semantic relation


fiturney pantel

wi cj otherwise xij value zero indicating occurrence wi
cj uninformative
well known pmi biased towards infrequent events consider
case wi cj statistically dependent e maximum association
pij pi pj hence becomes log pi pmi increases probability
word wi decreases several discounting factors proposed alleviate
example follows pantel lin
p c
p r
fik
fkj nk
min nk
fij
pnc
pnr
ij

fij min k fkj k fik
newpmiij ij pmiij




another way deal infrequent events laplace smoothing probability
estimates pij pi pj turney littman constant positive value added
raw frequencies calculating probabilities fij replaced fij k
k larger constant greater smoothing effect laplace smoothing
pushes pmiij values towards zero magnitude push difference
pmiij without laplace smoothing depends raw frequency fij
frequency large push small frequency small push large thus
laplace smoothing reduces bias pmi towards infrequent events
smoothing matrix
simplest way improve information retrieval performance limit number
vector components keeping components representing frequently occurring
content words way however common words carry little
semantic discrimination power simple component smoothing heuristics properties weighting schemes presented section shown maintain
semantic discrimination power improve performance similarity computations
computing similarity pairs vectors described section
computationally intensive task however vectors share non zero coordinate
must compared e two vectors share coordinate dissimilar
frequent context words word unfortunately vectors matching
non zero coordinate words precisely contexts little semantic
discrimination power consider pointwise mutual information weighting described
section highly weighted dimensions co occur frequently words
definition highly discriminating contexts e high association
words co occur keeping context word dimensions
pmi conservative threshold setting others zero lin showed
number comparisons needed compare vectors greatly decreases losing
little precision similarity score top similar words every word
smoothing matrix one computes reverse index non zero coordinates
compare similarity words context vector words context
vectors vectors found match non zero component reverse index must
compared section proposes optimizations along lines


fifrom frequency meaning

deerwester et al found elegant way improve similarity measurements
mathematical operation termdocument matrix x linear algebra operation truncated singular value decomposition svd called thin svd deerwester
et al briefly mentioned truncated svd applied document similarity
word similarity focus document similarity landauer dumais
applied truncated svd word similarity achieving human level scores multiple choice
synonym questions test english foreign language toefl truncated
svd applied document similarity called latent semantic indexing lsi
called latent semantic analysis lsa applied word similarity
several ways thinking truncated svd works first
present math behind truncated svd describe four ways looking
latent meaning noise reduction high order co occurrence sparsity reduction
svd decomposes x product three matrices uvt u v
column orthonormal form e columns orthogonal unit length ut u
vt v diagonal matrix singular values golub van loan x
rank r rank r let k k r diagonal matrix formed
top k singular values let uk vk matrices produced selecting
corresponding columns u v matrix uk k vkt matrix rank k
best approximates original matrix x sense minimizes approximation
errors x uk k vkt minimizes kx xkf matrices x rank k
k kf denotes frobenius norm golub van loan
latent meaning deerwester et al landauer dumais describe
truncated svd method discovering latent meaning suppose word
context matrix x truncated svd x uk k vkt creates low dimensional linear
mapping row space words column space contexts low dimensional
mapping captures latent hidden meaning words contexts limiting
number latent dimensions k r forces greater correspondence words
contexts forced correspondence words contexts improves similarity
measurement
noise reduction rapp describes truncated svd noise reduction technique
may think matrix x uk k vkt smoothed version original matrix x
matrix uk maps row space space spanned rows x smaller
k dimensional space matrix vk maps column space space spanned
columns x k dimensional space diagonal matrix k specifies
weights reduced k dimensional space singular values ranked
descending order amount variation x fit think matrix
x composed mixture signal noise signal noise
uk k vkt mostly captures variation x due signal whereas remaining
vectors uvt mostly fitting variation x due noise
high order co occurrence landauer dumais describe truncated
svd method discovering high order co occurrence direct co occurrence firstorder co occurrence two words appear identical contexts indirect co occurrence
high order co occurrence two words appear similar contexts similarity
contexts may defined recursively terms lower order co occurrence lemaire
denhiere demonstrate truncated svd discover high order co occurrence


fiturney pantel

sparsity reduction general matrix x sparse mostly zeroes
truncated svd x uk k vkt dense sparsity may viewed insufficient
data text matrix x would fewer zeroes vsm would perform
better chosen task perspective truncated svd way simulating
missing text compensating lack data vozalis margaritis
different ways viewing truncated svd compatible
possible perspectives correct future work likely provide
views svd perhaps unifying view
good c implementation svd large sparse matrices rohdes svdlibc
another brands incremental truncated svd yet another
gorrells hebbian incremental truncated svd brands
gorrells introduce interesting ways handling missing values
instead treating zero values
higher order tensors operations analogous truncated svd
parallel factor analysis parafac harshman canonical decomposition
candecomp carroll chang equivalent parafac discovered independently tucker decomposition tucker overview tensor decompositions see surveys kolda bader acar yener turney
gives empirical evaluation well four different tucker decomposition
scale large sparse third order tensors low ram multislice projection
large sparse tensors presented evaluated
since work deerwester et al subsequent discovered many
alternative matrix smoothing processes nonnegative matrix factorization nmf
lee seung probabilistic latent semantic indexing plsi hofmann iterative scaling ando kernel principal components analysis kpca scholkopf
smola muller latent dirichlet allocation lda blei et al discrete
component analysis dca buntine jakulin
four perspectives truncated svd presented apply equally well
recent matrix smoothing newer smoothing tend
computationally intensive truncated svd attempt model
word frequencies better svd truncated svd implicitly assumes elements
x gaussian distribution minimizing frobenius norm kx xkf
minimize noise noise gaussian distribution however known
word frequencies gaussian distributions recent
realistic distribution word frequencies
comparing vectors
popular way measure similarity two frequency vectors raw weighted
take cosine let x two vectors n elements





svdlibc available http tedlab mit edu dr svdlibc
matlab source code available http web mit edu wingated www resources html
matlab source code available http www apperceptual com multislice
experience pmiij appears approximately gaussian may explain pmi works well
truncated svd ppmi puzzling less gaussian pmi yet apparently
yields better semantic pmi



fifrom frequency meaning

x hx x xn



hy yn



cosine angle x calculated follows
pn
cos x qp
n

yi
pn

xi



xi
yi
xy


xx yy
x



kxk kyk





words cosine angle two vectors inner product
vectors normalized unit length x frequency vectors
words frequent word long vector rare word short vector
yet words might synonyms cosine captures idea length vectors
irrelevant important thing angle vectors
cosine ranges vectors point opposite directions
degrees point direction degrees vectors
orthogonal degrees cosine zero raw frequency vectors
necessarily cannot negative elements cosine cannot negative weighting
smoothing often introduce negative elements ppmi weighting yield negative
elements truncated svd generate negative elements even input matrix
negative values
measure distance vectors easily converted measure similarity
inversion subtraction

sim x dist x



sim x dist x



many similarity measures proposed ir jones furnas
lexical semantics circles lin dagan lee pereira lee weeds weir
mccarthy commonly said ir properly normalized difference
retrieval performance different measures insignificant van rijsbergen
often vectors normalized way e g unit length unit probability
applying similarity measure
popular geometric measures vector distance include euclidean distance manhattan distance distance measures information theory include hellinger bhattacharya
kullback leibler bullinaria levy compared five distance measures
cosine similarity measure four different tasks involving word similarity overall
best measure cosine popular measures dice jaccard coefficients
manning et al


fiturney pantel

lee proposed finding word similarities measures focused
overlapping coordinates less importance negative features e coordinates
one word nonzero value zero value appear perform
better lees experiments jaccard jensen shannon l measures seemed
perform best weeds et al studied linguistic statistical properties
similar words returned similarity measures found measures
grouped three classes
high frequency sensitive measures cosine jensen shannon skew recall
low frequency sensitive measures precision
similar frequency sensitive methods jaccard jaccard mi lin harmonic mean
given word w use high frequency sensitive measure score words wi
according similarity w higher frequency words tend get higher scores
lower frequency words use low frequency sensitive measure
bias towards lower frequency words similar frequency sensitive methods prefer word wi
approximately frequency w one experiment determining
compositionality collocations high frequency sensitive measures outperformed
classes weeds et al believe determining appropriate similarity
measure inherently dependent similarity task sparsity statistics
frequency distribution elements compared smoothing method applied
matrix
efficient comparisons
computing similarity rows columns large matrix non trivial
worst case cubic running time n r nc nr number rows
nc number columns e dimensionality feature space optimizations
parallelization often necessary
sparse matrix multiplication
one optimization strategy generalized sparse matrix multiplication sarawagi
kirpal observation scalar product two vectors
depends coordinates vectors nonzero values
observe commonly used similarity measures vectors x cosine
overlap dice decomposed three values one depending nonzero
values x another depending nonzero values third depending
nonzero coordinates shared x formally commonly used similarity
scores sim x expressed follows
sim x f

pn

f xi yi f x f



example cosine measure cos x defined expressed model
follows


fifrom frequency meaning

p
cos x f ni f xi yi f x f

f b c
bc
f b b
qp
n

f f
ai






let x matrix want compute pairwise similarity sim x
rows columns x efficient computation similarity matrix
achieved leveraging fact sim x determined solely nonzero
coordinates shared x e f xi f xi xi
vectors sparse case calculating f xi yi required
vectors shared nonzero coordinate significantly reducing cost computation
determining vectors share nonzero coodinate easily achieved first building
inverted index coordinates indexing precompute f x
f without changing complexity vector x retrieve
constant time index vector shares nonzero coordinate x
wep
apply f xi yi shared coordinates computational cost
ni ni number vectors nonzero th coordinate worst
case time complexity ncv n number vectors compared c
maximum number nonzero coordinates vector v number vectors
nonzero th coordinate coordinate nonzero
vectors words efficient density coordinates
low experiments computing semantic similarity pairs
words large web crawl observed near linear average running time complexity n
computational cost reduced leveraging element weighting
techniques described section setting zero coordinates low
ppmi pmi tf idf score coordinate density dramatically reduced cost
losing little discriminative power vein bayardo srikant described
strategy omits coordinates highest number nonzero values
gives significant advantage interested finding solely
similarity highly similar vectors
distributed implementation mapreduce
described section assumes matrix x fit memory
large x may impossible element x processed independently running parallel processes non intersecting subsets x makes processing
faster elsayed lin oard proposed mapreduce implementation deployed hadoop open source software package implementing mapreduce framework
distributed file system hadoop shown scale several thousands machines
allowing users write simple code seamlessly manage sophisticated parallel execution code dean ghemawat provide good primer mapreduce
programming
hadoop available download http lucene apache org hadoop



fiturney pantel

mapreduce map step used start n map tasks parallel
caching one th part x inverted index streaming one n th part x
actual inputs read tasks directly hdfs hadoop distributed
file system value determined amount memory dedicated
inverted index n determined trading fact n increases
parallelism obtained increased cost building inverted index
n times
similarity section runs task map step
mapreduce job reduce step groups output rows columns x
randomized
optimization strategies use randomized techniques approximate similarity measures aim randomized improve computational efficiency
memory time projecting high dimensional vectors low dimensional subspace
truncated svd performs projection svd computationally intensive
insight randomized techniques high dimensional vectors randomly projected low dimensional subspace relatively little impact final similarity
scores significant reductions computational cost reported little average error computing true similarity scores especially applications word
similarity interested top k similar vectors vector
ravichandran pantel hovy gorman curran
random indexing approximation technique sparse distributed memory
kanerva computes pairwise similarity rows vectors matrix
complexity nr nc fixed constant representing length index
vectors assigned column value controls tradeoff accuracy versus
efficiency elements index vector mostly zeros small number
randomly assigned cosine measure two rows r r
approximated computing cosine two fingerprint vectors fingerprint r
fingerprint r fingerprint r computed summing index vectors
non unique coordinate r random indexing shown perform well lsa
word synonym selection task karlgren sahlgren
locality sensitive hashing lsh broder another technique approximates
similarity matrix complexity n r constant number random
projections controls accuracy versus efficiency tradeoff lsh general class
techniques defining functions map vectors rows columns short signatures
fingerprints two similar vectors likely similar fingerprints definitions
lsh functions include min wise independent function preserves jaccard
similarity vectors broder functions preserve cosine similarity
vectors charikar word similarity task ravichandran et al
showed average top similar words random words found
top charikars functions average cosine error
however efficient forms svd brand gorrell
lsh stems work rabin proposed use hash functions random irreducible
polynomials create short fingerprints collections documents techniques useful many
tasks removing duplicate documents deduping web crawl



fifrom frequency meaning

random projections gorman curran provide detailed
comparison random indexing lsh distributional similarity task bnc
corpus lsh outperformed random indexing however larger corpora combining bnc
reuters corpus english news holdings ldc random
indexing outperformed lsh efficiency accuracy
machine learning
intended application vsm clustering classification similarity measure
cosine section may used classification nearest neighbour
use cosine measure nearness dasarathy clustering similaritybased clustering use cosine measure similarity jain murty flynn
however many machine learning work directly
vectors vsm without requiring external similarity measure cosine
effect machine learning implicitly use internal approaches
measuring similarity
machine learning works real valued vectors use vectors
vsm witten frank linguistic processing section mathematical
processing section may still necessary machine learning handle
vector comparison sections
addition unsupervised clustering supervised classification machine learning vectors vsm may used semi supervised learning ando zhang
collobert weston general nothing unique vsms would
compel choice one machine learning another aside
performance given task therefore refer readers machine learning
literature witten frank since advice specific vsms

three open source vsm systems
illustrate three types vsms discussed section section presents three open
source systems one vsm type chosen present open source systems
interested readers obtain source code systems
apply systems projects three systems written java
designed portability ease use
termdocument matrix lucene
lucene open source full featured text search engine library supported apache
software foundation arguably ubiquitous implementation termdocument
matrix powering many search engines cnet sourceforge wikipedia disney
aol comcast lucene offers efficient storage indexing well retrieval ranking
functionalities although primarily used termdocument matrix generalizes
vsms
apache lucene available download http lucene apache org



fiturney pantel

content webpages pdf documents images video programmatically
decomposed fields stored database database implements term
document matrix content corresponds documents fields correspond terms
fields stored database indices computed field values lucene
uses fields generalization content terms allowing string literal index
documents example webpage could indexed terms contains
anchor texts pointing host name semantic classes
classified e g spam product review news etc webpage retrieved
search terms matching fields
columns termdocument matrix consist fields particular instance
content e g webpage rows consist instances content index
statistics frequency tf idf stored matrix developer
defines fields schema identifies indexed lucene developer
optionally defines content ranking function indexed field
index built lucene offers functionalities retrieving content users
issue many query types phrase queries wildcard queries proximity queries range
queries e g date range queries field restricted queries sorted
field index updates occur simultaneously searching lucenes index
directly loaded tomcat webserver offers apis common programming languages solr separate apache software foundation project open source enterprise
webserver searching lucene index presenting search full featured
webserver providing functionalities xml http json apis hit highlighting
faceted search caching replication
simple recipe creating web search service nutch lucene solr consists
crawling set urls nutch creating termdocument matrix index
lucene serving search solr nutch apache software foundation
open source web search software offers functionality crawling web seed set
urls building link graph web crawl parsing web documents
html good set seed urls nutch downloaded freely
open directory project crawled html parsed indexed
lucene resulting indexed collection queried served solr
installation tomcat
information lucene recommend gospodnetic hatchers
book konchady explains integrate lucene lingpipe gate
sophisticated semantic processing






apache solr available download http lucene apache org solr
apache nutch available download http lucene apache org nutch
see http www dmoz org
information lingpipe available http alias com lingpipe gate general architecture text engineering home page http gate ac uk



fifrom frequency meaning

wordcontext matrix semantic vectors
semantic vectors open source project implementing random projection
measuring word similarity see section package uses lucene create term
document matrix creates vectors lucenes termdocument matrix
random projection dimensionality reduction random projection vectors
used example measure semantic similarity two words words
similar given word
idea random projection take high dimensional vectors randomly project
relatively low dimensional space sahlgren viewed
kind smoothing operation section developers semantic vectors
package emphasize simplicity efficiency random projection section rather
smoothing ability argue matrix smoothing might
smooth better none perform well random indexing terms
computational complexity building smooth matrix incrementally updating
matrix data arrives widdows ferraro aim encourage
development semantic vectors creating simple efficient open
source package
semantic vectors package designed convenient use portable easy
extend modify design software incorporates lessons learned
earlier stanford infomap project although default generate random projection
vectors system modular design allows kinds wordcontext matrices
used instead random projection matrices
package supports two basic functions building wordcontext matrix searching
vectors matrix addition generating word vectors building
operation generate document vectors calculating weighted sums word vectors
words document searching operation used search similar
words search documents similar query query single word
several words combined mathematical operations corresponding
vectors mathematical operations include vector negation disjunction
quantum logic widdows widdows ferraro provide good summary
semantic vectors software
pairpattern matrix latent relational analysis space
latent relational analysis lra open source project implementing pairpattern
matrix component space package library tools building
comparing different semantic spaces
lra takes input textual corpus set word pairs pairpattern matrix
built deriving lexical patterns link together word pairs corpus example consider word pair hkorea japani following retrieved matching sentences
semantic vectors software package measuring word similarity available simplified bsd
license http code google com p semanticvectors
see http infomap nlp sourceforge net
latent relational analysis part space package distributed gnu general
public license version available http code google com p airhead time
writing lra module development



fiturney pantel

korea looks japan prime ministers effect korea japan relations
channel korea vs japan football game
two sentences lra extracts two patterns x looks x vs
patterns become two columns pairpattern matrix word pair hkorea
japani becomes row pattern frequencies counted smoothed svd see
section
order mitigate sparseness occurrences word pairs thesaurus
wordnet used expand seed word pairs alternatives example pair
hkorea japani may expanded include hsouth korea japani hrepublic korea
japani hkorea nipponi hsouth korea nipponi hrepublic korea nipponi
lra uses lucene see section backend store matrix index serve
contents detailed description lra suggest turneys


applications
section survey semantic applications vsms aim
breadth rather depth readers want depth consult references
goal give reader impression scope flexibility vsms semantics
following applications grouped according type matrix involved term
document wordcontext pairpattern note section exhaustive
many references applications space list
termdocument matrices
termdocument matrices suited measuring semantic similarity documents
queries see section usual measure similarity cosine column vectors
weighted termdocument matrix variety applications measures
document similarity
document retrieval termdocument matrix first developed document
retrieval salton et al large body literature vsm
document retrieval manning et al including several journals conferences
devoted topic core idea given query rank documents order
decreasing cosine angles query vector document vectors salton
et al one variation theme cross lingual document retrieval
query one language used retrieve document another language landauer
littman important technical advance discovery smoothing
termdocument matrix truncated svd improve precision recall deerwester
et al although commercial systems use smoothing due computational
expense document collection large dynamic random indexing sahlgren
incremental svd brand may help address scaling issues another
important development document retrieval addition collaborative filtering
form pagerank brin page
document clustering given measure document similarity cluster
documents groups similarity tends high within group low across


fifrom frequency meaning

groups manning et al clusters may partitional flat cutting karger
pedersen tukey pantel lin b may hierarchical structure
groups groups zhao karypis may non overlapping hard croft
overlapping soft zamir etzioni clustering differ
clusters compared abstracted single link clustering similarity
two clusters maximum similarities members complete link
clustering uses minimum similarities average link clustering uses average
similarities manning et al
document classification given training set documents class labels
testing set unlabeled documents task document classification learn
training set assign labels testing set manning et al labels may
topics documents sebastiani sentiment documents e g
positive versus negative product reviews pang lee vaithyanathan kim pantel
chklovski pennacchiotti spam versus non spam sahami dumais heckerman
horvitz pantel lin labels might inferred words
documents classify documents implying documents
class similar way thus document classification implies notion document
similarity machine learning approaches document classification involve term
document matrix sebastiani measure document similarity cosine
directly applied document classification nearest neighbour yang

essay grading student essays may automatically graded comparing
one high quality reference essays given essay topic wolfe schreiner rehder
laham foltz kintsch landauer foltz laham landauer student
essays reference essays compared cosines termdocument matrix
grade assigned student essay proportional similarity one
reference essays student essay highly similar reference essay gets high grade
document segmentation task document segmentation partition document sections section focuses different subtopic document
hearst choi may treat document series blocks block
sentence paragraph detect topic shift one block
next hearst choi use cosine columns wordblock
frequency matrix measure semantic similarity blocks topic shift signaled
drop cosine consecutive blocks wordblock matrix viewed
small termdocument matrix corpus single document documents
blocks
question answering given simple question task question answering qa
short answer question searching large corpus typical question many calories big mac qa four
components question analysis document retrieval passage retrieval answer extraction
tellex katz lin fern marton dang lin kelly vector similarity measurements often used document retrieval passage retrieval tellex
et al
call routing chu carroll carpenter present vector system
automatically routing telephone calls callers spoken answer question


fiturney pantel

may direct call callers answer ambiguous system automatically
generates question caller derived vsm prompts caller
information
wordcontext matrices
wordcontext matrices suited measuring semantic similarity words see
section example measure similarity two words cosine
angle corresponding row vectors wordcontext matrix many
applications measures word similarity
word similarity deerwester et al discovered measure word similarity comparing row vectors termdocument matrix landauer dumais
evaluated multiple choice synonym questions test english foreign language toefl achieving human level performance correct
wordcontext matrix average non english us college applicant
documents used landauer dumais average length words
seems short document long context word researchers soon
switched much shorter lengths prefer call wordcontext matrices instead termdocument matrices lund burgess used context window
ten words schutze used fifty word window words centered target
word rapp achieved correct toefl questions four word
context window words centered target word removing stop words
toefl suggest performance improves context window shrinks seems
immediate context word much important distant context
determining meaning word
word clustering pereira tishby lee applied soft hierarchical clustering
row vectors wordcontext matrix one experiment words nouns
contexts verbs given nouns direct objects another experiment
words verbs contexts nouns direct objects given
verbs schutzes seminal word sense discrimination model used hard flat clustering
row vectors wordcontext matrix context given window
words centered target word pantel lin applied soft flat clustering
wordcontext matrix context parsed text
able discover different senses polysemous words generating different clusters
sense effect different clusters correspond different concepts underlie
words
word classification turney littman used wordcontext matrix classify words positive honest intrepid negative disturbing superfluous used
general inquirer gi lexicon stone dunphy smith ogilvie evaluate
gi lexicon includes words labeled categories related
opinion affect attitude turney littman hypothesize categories
discriminated wordcontext matrix
automatic thesaurus generation wordnet popular tool natural
language processing fellbaum creating maintaing lexical resources
gi lexicon available http www wjh harvard edu inquirer spreadsheet guide htm



fifrom frequency meaning

labour intensive natural wonder whether process automated
degree task seen instance word clustering thesaurus
generated scratch classification existing thesaurus automatically
extended worthwhile consider task automatic thesaurus generation
separately clustering classification due specific requirements thesaurus
particular kind similarity appropriate thesaurus see section
several researchers used wordcontext matrices specifically task assisting
automating thesaurus generation crouch grefenstette ruge pantel
lin curran moens
word sense disambiguation typical word sense disambiguation wsd system
agirre edmonds pedersen uses feature vector representation
vector corresponds token word type see section however leacock
towell voorhees used wordcontext frequency matrix wsd
vector corresponds type annotated sense tag yuret yatbaz applied
wordcontext frequency matrix unsupervised wsd achieving comparable
performance supervised wsd systems
context sensitive spelling correction people frequently confuse certain sets
words theyre confusions cannot detected simple dictionary spelling checker require context sensitive spelling correction
wordcontext frequency matrix may used correct kinds spelling errors jones
martin
semantic role labeling task semantic role labeling label parts sentence according roles play sentence usually terms connection
main verb sentence erk presented system wordcontext
frequency matrix used improve performance semantic role labeling pennacchiotti cao basili croce roth wordcontext matrices reliably
predict semantic frame unknown lexical unit refers good levels
accuracy lexical unit induction important semantic role labeling narrow
candidate set roles observed lexical unit
query expansion queries submitted search engines google yahoo
often directly match terms relevant documents alleviate
process query expansion used generating search terms
consistent intent original query vsms form basis query semantics
cao jiang pei liao chen li methods represent queries
session contexts query cooccurrences user sessions huang chien
oyang jones rey madani greiner others use click contexts
urls clicked query wen nie zhang
textual advertising pay per click advertising prevalent search engines
google yahoo users pay keywords called bidterms used
display ads relevant queries issued users scarcity data makes ad
matching difficult response several techniques bidterm expansion vsms
proposed wordcontext matrix consists rows bidterms columns
wordnet available http wordnet princeton edu



fiturney pantel

contexts consist advertiser identifiers gleich zhukov co bidded bidterms
second order co occurrences chang pantel popescu gabrilovich
information extraction field information extraction ie includes named
entity recognition ner recognizing chunk text name entity
person place relation extraction event extraction fact extraction pasca et
al demonstrate wordcontext frequency matrix facilitate fact extraction
vyas pantel propose semi supervised model wordcontext matrix
building iteratively refining arbitrary classes named entities
pairpattern matrices
pairpattern matrices suited measuring semantic similarity word pairs
patterns see section example measure similarity two word
pairs cosine angle corresponding row vectors pairpattern
matrix many applications measures relational similarity
relational similarity measure attributional similarity cosine
angle row vectors wordcontext matrix measure relational
similarity cosine angle rows pairpattern matrix
measuring relational similarity introduced turney et al examined
detail turney littman turney evaluated
relational similarity multiple choice analogy questions sat college
entrance test achieving human level performance correct pairpattern matrix
correct average us college applicant highest performance
far best attributional similarity accuracy
turney best non vsm achieves veale
pattern similarity instead measuring similarity row vectors pair
pattern matrix measure similarity columns measure
pattern similarity lin pantel constructed pairpattern matrix
patterns derived parsed text pattern similarity used infer one
phrase paraphrase another phrase useful natural language generation
text summarization information retrieval question answering
relational clustering bicici yuret clustered word pairs representing
row vectors pairpattern matrix davidov rappoport first clustered
contexts patterns identified representative pairs context cluster
used representative pairs automatically generate multiple choice analogy questions
style sat analogy questions
relational classification chklovski pantel used pairpattern matrix
classify pairs verbs semantic classes example taint poison classified
strength poisoning stronger tainting assess review classified enablement
assessing enabled reviewing turney used pairpattern matrix classify
noun compounds semantic classes example flu virus classified cause
virus causes flu home town classified location home located town
weather report classified topic topic report weather
relational search cafarella banko etzioni described relational search
task searching entities satisfy given semantic relations example


fifrom frequency meaning

query relational search engine list x x causes cancer
example relation cause one terms relation cancer given
user task search engine terms satisfy users query
organizers task semeval girju nakov nastase szpakowicz turney yuret
envisioned two step relational search first conventional search engine
would look candidate answers relational classification system would filter
incorrect answers first step manually simulated task organizers
goal task design systems second step task attracted teams
submitted systems nakov hearst achieved good pairpattern
matrix
automatic thesaurus generation discussed automatic thesaurus generation
section wordcontext matrices arguably relational similarity relevant
attributional similarity thesaurus generation example information wordnet relations words rather words individually
snow jurafsky ng used pairpattern matrix build hypernym hyponym
taxonomy whereas pennacchiotti pantel built meronymy causation taxonomy turney b showed pairpattern matrix distinguish synonyms
antonyms synonyms non synonyms taxonomically similar words hair fur
words merely semantically associated cradle baby
analogical mapping proportional analogies form b c means
b c example mason stone carpenter wood means mason stone
carpenter wood multiple choice analogy questions sat college
entrance test mentioned involve proportional analogies pairpattern
matrix solve proportional analogies selecting choice maximizes relational
similarity e g simr mason stone carpenter wood high value however often
encounter analogies involve four terms well known analogy
solar system rutherford bohr model atom contains least fourteen
terms solar system planet attracts revolves sun gravity solar system
mass atom revolves atom attracts electromagnetism nucleus
charge electron turney demonstrated handle complex
systematic analogies decomposing sets proportional analogies

alternative approaches semantics
applications list section necessarily require vsm
application many possible approaches section briefly
consider main alternatives
underlying applications termdocument matrices section task
measuring semantic similarity documents queries main alternatives
vsms task probabilistic traditional probabilistic retrieval
information retrieval van rijsbergen baeza yates ribeiro neto
recent statistical language inspired information theory liu
croft idea statistical language information retrieval measure
similarity query document creating probabilistic language model


fiturney pantel

given document measuring probability given query according
language model
progress information retrieval distinction vsm
probabilistic becoming blurred borrows ideas
language typically involve multiplying probabilities view
adding logs probabilities makes language look similar vsms
applications wordcontext matrices section share task measuring
semantic similarity words main alternatives vsms measuring word similarity
approaches use lexicons wordnet resnik jiang conrath
hirst st onge leacock chodrow budanitsky hirst idea
view lexicon graph nodes correspond word senses edges represent
relations words hypernymy hyponymy similarity two
words proportional length path graph joins two words
several approaches measuring semantic similarity words combine vsm
lexicon turney et al pantel patwardhan pedersen mohammad
hirst humans use dictionary definitions observations word usage
natural expect best performance use distributional
lexical information
pairpattern matrices section common task measuring semantic
similarity relations wordcontext matrices main alternatives approaches
use lexicons rosario hearst rosario hearst fillmore nastase
szpakowicz veale idea reduce relational similarity
attributional similarity simr b c sima c sima b use lexicon
measure attributional similarity discuss section reduction
work general however reduction often good approximation
evidence hybrid combining vsm lexicon beneficial turney
et al nastase sayyad shirabad sokolova szpakowicz

future vector space semantics
several authors criticized vsms french labiouse pado lapata
morris hirst budanitsky hirst criticism stems
fact termdocument wordcontext matrices typically ignore word order lsa
instance phrase commonly represented sum vectors individual
words phrase hence phrases house boat boat house represented
vector although different meanings english word order expresses
relational information house boat boat house tool purpose relation
house boat means tool purpose boat house boat serves house whereas boat
house means tool purpose house boat house sheltering storing boats
landauer estimates meaning english text comes word
choice remaining comes word order however vsms inherently
limited meaning text mitchell lapata propose composition
sensitive word order example make simple additive model become
syntax aware allow different weightings contributions vector components constituents important composition therefore participate


fifrom frequency meaning

actively clark pulman assigned distributional meaning sentences hilbert space tensor product widdows ferraro inspired quantum
mechanics explores several operators modeling composition meaning pairpattern
matrices sensitive order words pair turney thus
several ways handle word order vsms
raises question limits vsms semantics semantics
represented vsms much yet know represent
vsms example widdows van rijsbergen disjunction
conjunction negation represented vectors yet know
represent arbitrary statements first order predicate calculus however seems possible
future work may discover answers limitations
survey assumed vsms composed elements values
derived event frequencies ties vsms form distributional hypothesis
see sections therefore limits vsms depend limits family
distributional hypotheses statistical patterns word usage sufficient figure
people mean arguably major open question vsms answer
determine future vsms strong argument one way
believe continuing progress vsms suggests far reaching
limits

conclusions
want information help person use words make request
describe person replies words unfortunately computers
understand human language forced use artificial languages unnatural user
interfaces science fiction dream computers understand human language
listen us talk us achieve full potential computers must enable
understand semantics natural language vsms likely part
solution computing semantics
many researchers struggled semantics come
conclusion meaning words closely connected statistics word usage
section try make intuition precise soon working
vectors values derived event frequencies dealing vsms
survey organized past work vsms according structure
matrix termdocument wordcontext pairpattern believe structure
matrix important factor determining types applications
possible linguistic processing section mathematical processing section
play smaller important roles
goal survey breadth power vsms introduce
vsms less familiar provide perspective vsms
already familiar hope emphasis structure
matrix inspire reason believe three matrix
types present exhaust possibilities expect matrix types tensors
open applications vsms seems possible us semantics
human language might one day captured kind vsm


fiturney pantel

acknowledgments
thanks annie zaenen prompting thanks saif mohammad mariana
soffer comments thanks arkady borkovsky eric crestan developing
distributed sparse matrix multiplication marco pennacchiotti
invaluable comments thanks anonymous reviewers jair helpful
comments suggestions

references
acar e yener b unsupervised multiway data analysis literature survey
ieee transactions knowledge data engineering
agirre e edmonds p g word sense disambiguation applications springer
ando r k latent semantic space iterative scaling improves precision interdocument similarity measurement proceedings rd annual acm sigir
conference development information retrieval sigir pp

ando r k zhang framework learning predictive structures
multiple tasks unlabeled data journal machine learning

baeza yates r ribeiro neto b modern information retrieval addison wesley
barr c jones r regelson linguistic structure english websearch queries conference empirical methods natural language processing
emnlp
bayardo r j srikant r scaling pairs similarity search
proceedings th international conference world wide web www
pp york ny acm
bicici e yuret clustering word pairs answer analogy questions
proceedings fifteenth turkish symposium artificial intelligence neural
networks tainn akyaka mugla turkey
blei ng jordan latent dirichlet allocation journal
machine learning
brand fast low rank modifications thin singular value decomposition
linear algebra applications
breese j heckerman kadie c empirical analysis predictive
collaborative filtering proceedings th conference uncertainty
artificial intelligence pp morgan kaufmann
brin page l anatomy large scale hypertextual web search engine
proceedings seventh world wide web conference www pp
broder resemblance containment documents compression
complexity sequences sequences pp ieee computer society


fifrom frequency meaning

budanitsky hirst g semantic distance wordnet experimental
application oriented evaluation five measures proceedings workshop
wordnet lexical resources second meeting north american
chapter association computational linguistics naacl pp
pittsburgh pa
budanitsky hirst g evaluating wordnet measures semantic distance computational linguistics
bullinaria j levy j extracting semantic representations word cooccurrence statistics computational study behavior methods

buntine w jakulin discrete component analysis subspace latent
structure feature selection statistical optimization perspectives workshop
slsfs pp bohinj slovenia springer
cafarella j banko etzioni relational web search tech rep university washington department computer science engineering technical
report
cao g nie j bai j integrating word relationships language
proceedings th annual international acm sigir conference
development information retrieval sigir pp york ny
acm
cao h jiang pei j q liao z chen e li h context aware query
suggestion mining click session data proceeding th acm
sigkdd international conference knowledge discovery data mining kdd
pp acm
carroll j chang j j analysis individual differences multidimensional
scaling via n way generalization eckart young decomposition psychometrika

chang w pantel p popescu gabrilovich e towards intent driven
bidterm suggestion proceedings www short madrid spain
charikar similarity estimation techniques rounding proceedings thiry fourth annual acm symposium theory computing stoc
pp acm
chew p bader b kolda abdelali cross language information retrieval parafac proceedings th acm sigkdd international
conference knowledge discovery data mining kdd pp acm
press
chiarello c burgess c richards l pollock semantic associative
priming cerebral hemispheres words words dont sometimes
places brain language
chklovski pantel p verbocean mining web fine grained semantic
verb relations proceedings experimental methods natural language processing emnlp pp barcelona spain


fiturney pantel

choi f advances domain independent linear text segmentation
proceedings st meeting north american chapter association
computational linguistics pp
chu carroll j carpenter b vector natural language call routing computational linguistics
church k one term two proceedings th annual international
acm sigir conference development information retrieval pp

church k hanks p word association norms mutual information lexicography proceedings th annual conference association computational linguistics pp vancouver british columbia
clark pulman combining symbolic distributional meaning
proceedings aaai spring symposium quantum interaction pp
collobert r weston j unified architecture natural language processing
deep neural networks multitask learning proceedings th international
conference machine learning icml pp
croft w b clustering large files documents single link method
journal american society information science
crouch c j cluster thesaurus construction proceedings
th annual international acm sigir conference pp grenoble
france
curran j r moens improvements automatic thesaurus extraction
unsupervised lexical acquisition proceedings workshop acl special
interest group lexicon siglex pp philadelphia pa
cutting r karger r pedersen j tukey j w scatter gather
cluster browsing large document collections proceedings
th annual international acm sigir conference pp
dagan lee l pereira f c n similarity word cooccurrence
probabilities machine learning
dang h lin j kelly overview trec question answering
track proceedings fifteenth text retrieval conference trec
dasarathy b nearest neighbor nn norms nn pattern classification techniques
ieee computer society press
davidov rappoport unsupervised discovery generic relationships
pattern clusters evaluation automatically generated sat analogy questions
proceedings th annual meeting acl hlt acl hlt pp
columbus ohio
dean j ghemawat mapreduce simplified data processing large clusters
communications acm


fifrom frequency meaning

deerwester c dumais landauer k furnas g w harshman r
indexing latent semantic analysis journal american society
information science jasis
elsayed lin j oard pairwise document similarity large collections
mapreduce proceedings association computational linguistics
human language technology conference acl hlt short papers pp
columbus ohio association computational linguistics
erk k simple similarity model selectional preferences proceedings
th annual meeting association computational linguistics pp
prague czech republic
erk k pado structured vector space model word meaning context
proceedings conference empirical methods natural language
processing emnlp pp honolulu hi
fellbaum c ed wordnet electronic lexical database mit press
firth j r synopsis linguistic theory studies linguistic
analysis pp blackwell oxford
foltz p w laham landauer k intelligent essay assessor applications educational technology interactive multimedia electronic journal
computer enhanced learning
forman g extensive empirical study feature selection metrics text classification journal machine learning
french r labiouse c four extracting human semantics
large text corpora proceedings th annual conference cognitive
science society
furnas g w landauer k gomez l dumais statistical semantics analysis potential performance keyword information systems bell
system technical journal
gentner structure mapping theoretical framework analogy cognitive
science
gilbert j r moler c schreiber r sparse matrices matlab design
implementation siam journal matrix analysis applications
girju r nakov p nastase v szpakowicz turney p yuret semeval task classification semantic relations nominals proceedings
fourth international workshop semantic evaluations semeval pp
prague czech republic
gleich zhukov l svd term suggestion ranking system
proceedings fourth ieee international conference data mining icdm
pp ieee computer society
golub g h van loan c f matrix computations third edition johns
hopkins university press baltimore md


fiturney pantel

gorman j curran j r scaling distributional similarity large corpora
proceedings st international conference computational linguistics
th annual meeting association computational linguistics acl
pp association computational linguistics
gorrell g generalized hebbian incremental singular value decomposition natural language processing proceedings th conference
european chapter association computational linguistics eacl pp

gospodnetic hatcher e lucene action manning publications
grefenstette g explorations automatic thesaurus discovery kluwer
harris z distributional structure word
harshman r foundations parafac procedure conditions
explanatory multi modal factor analysis ucla working papers phonetics
hearst texttiling segmenting text multi paragraph subtopic passages
computational linguistics
hirst g st onge lexical chains representations context detection
correction malapropisms fellbaum c ed wordnet electronic
lexical database pp mit press
hofmann probabilistic latent semantic indexing proceedings nd
annual acm conference development information retrieval sigir pp berkeley california
huang c k chien l f oyang j relevant term suggestion interactive
web search contextual information query session logs journal
american society information science technology
hull stemming case study detailed evaluation journal
american society information science
jain murty n flynn p data clustering review acm computing
surveys
jarmasz szpakowicz rogets thesaurus semantic similarity
proceedings international conference recent advances natural language
processing ranlp pp borovets bulgaria
jiang j j conrath w semantic similarity corpus statistics
lexical taxonomy proceedings international conference
computational linguistics rocling x pp tapei taiwan
johnson h martin j unsupervised learning morphology english
inuktitut proceedings hlt naacl pp
jones p martin j h contextual spelling correction latent semantic analysis proceedings fifth conference applied natural language
processing pp washington dc


fifrom frequency meaning

jones r rey b madani greiner w generating query substitutions
proceedings th international conference world wide web www
pp york ny acm
jones w p furnas g w pictures relevance geometric analysis
similarity measures journal american society information science

kanerva p sparse distributed memory related hassoun h
ed associative neural memories pp oxford university press york
ny
karlgren j sahlgren words understanding uesaka kanerva
p asoh h eds foundations real world intelligence pp csli
publications
kim pantel p chklovski pennacchiotti automatically assessing
review helpfulness proceedings conference empirical methods
natural language processing pp
kolda bader b tensor decompositions applications siam review

konchady building search applications lucene lingpipe gate mustru
publishing
kraaij w pohlmann r viewing stemming recall enhancement proceedings th annual international acm sigir conference pp
lakoff g women fire dangerous things university chicago press
chicago il
landauer k computational basis learning cognition arguments
lsa ross b h ed psychology learning motivation advances
theory vol pp academic press
landauer k dumais solution platos latent semantic analysis theory acquisition induction representation knowledge
psychological review
landauer k littman l fully automatic cross language document
retrieval latent semantic indexing proceedings sixth annual conference
uw centre oxford english dictionary text pp
waterloo ontario
landauer k mcnamara dennis kintsch w handbook latent
semantic analysis lawrence erlbaum mahwah nj
lavrenko v croft w b relevance language proceedings
th annual international acm sigir conference development
information retrieval sigir pp york ny acm
leacock c chodrow combining local context wordnet similarity
word sense identification fellbaum c ed wordnet electronic lexical
database mit press


fiturney pantel

leacock c towell g voorhees e corpus statistical sense resolution
proceedings arpa workshop human language technology pp
lee seung h learning parts objects nonnegative matrix
factorization nature
lee l measures distributional similarity proceedings th annual
meeting association computational linguistics pp
lemaire b denhiere g effects high order co occurrences word semantic
similarity current psychology letters behaviour brain cognition
lin automatic retrieval clustering similar words roceedings
th international conference computational linguistics pp association
computational linguistics
lin pantel p dirt discovery inference rules text proceedings
acm sigkdd conference knowledge discovery data mining pp

linden g smith b york j amazon com recommendations item item
collaborative filtering ieee internet computing
liu x croft w b statistical language modeling information retrieval
annual review information science technology
lovins j b development stemming mechanical translation
computational linguistics
lowe w towards theory semantic space proceedings twenty first
annual conference cognitive science society pp
lund k burgess c producing high dimensional semantic spaces lexical
co occurrence behavior methods instruments computers

lund k burgess c atchley r semantic associative priming highdimensional semantic space proceedings th annual conference
cognitive science society pp
manning c schutze h foundations statistical natural language processing
mit press cambridge
manning c raghavan p schutze h introduction information retrieval
cambridge university press cambridge uk
miller g leacock c tengi r bunker r semantic concordance
proceedings rd darpa workshop human language technology pp

minnen g carroll j pearce applied morphological processing english
natural language engineering
mitchell j lapata vector semantic composition proceedings acl hlt pp columbus ohio association computational
linguistics


fifrom frequency meaning

mitchell machine learning mcgraw hill columbus oh
mohammad hirst g distributional measures concept distance taskoriented evaluation proceedings conference empirical methods natural
language processing emnlp pp
monay f gatica perez image auto annotation latent space
proceedings eleventh acm international conference multimedia pp

morris j hirst g non classical lexical semantic relations workshop
computational lexical semantics hlt naacl boston
nakov p hearst ucb system description semeval task proceedings fourth international workshop semantic evaluations semeval
pp prague czech republic
nakov p hearst solving relational similarity theweb
corpus proceedings acl hlt pp columbus ohio
nastase v sayyad shirabad j sokolova szpakowicz learning nounmodifier semantic relations corpus wordnet features proceedings st national conference artificial intelligence aaai pp

nastase v szpakowicz exploring noun modifier semantic relations
fifth international workshop computational semantics iwcs pp
tilburg netherlands
niwa nitta co occurrence vectors corpora vs distance vectors
dictionaries proceedings th international conference computational
linguistics pp kyoto japan
nosofsky r attention similarity identification categorization relationship
journal experimental psychology general
ogden c k basic english general introduction rules grammar
kegan paul trench trubner co
pado lapata constructing semantic space parsed corpora
proceedings st annual meeting association computational linguistics pp sapporo japan
pado lapata dependency construction semantic space
computational linguistics
pang b lee l vaithyanathan thumbs sentiment classification
machine learning techniques proceedings conference empirical methods
natural language processing emnlp pp philadelphia pa
pantel p inducing ontological co occurrence vectors proceedings association
computational linguistics acl pp
pantel p lin spamcop spam classification organization program
learning text categorization papers aaai workshop pp


fiturney pantel

pantel p lin discovering word senses text proceedings
eighth acm sigkdd international conference knowledge discovery data
mining pp edmonton canada
pantel p lin b document clustering committees proceedings
th annual international acm sigir conference pp
pasca lin bigham j lifchits jain names similarities
web fact extraction fast lane proceedings st international
conference computational linguistics th annual meeting acl pp
sydney australia
patwardhan pedersen wordnet context vectors estimate
semantic relatedness concepts proceedings workshop making
sense sense th conference european chapter association
computational linguistics eacl pp
pedersen unsupervised corpus methods wsd word sense disambiguation applications pp springer
pennacchiotti cao basili r croce roth automatic induction
framenet lexical units proceedings conference empirical methods
natural language processing emnlp pp honolulu hawaii
pennacchiotti pantel p ontologizing semantic relations proceedings
st international conference computational linguistics th annual
meeting association computational linguistics pp association
computational linguistics
pereira f tishby n lee l distributional clustering english words
proceedings st annual meeting association computational linguistics
pp
porter suffix stripping program
rabin fingerprinting random polynomials tech rep center
computing technology harvard university technical report tr
rapp r word sense discovery sense descriptor dissimilarity proceedings ninth machine translation summit pp
ravichandran pantel p hovy e randomized nlp
locality sensitive hash function high speed noun clustering proceedings
rd annual meeting association computational linguistics acl pp
morristown nj association computational linguistics
resnick p iacovou n suchak bergstrom p riedl j grouplens open
architecture collaborative filtering netnews proceedings acm
conference computer supported cooperative work pp acm press
resnik p information content evaluate semantic similarity taxonomy
proceedings th international joint conference artificial intelligence
ijcai pp san mateo ca morgan kaufmann


fifrom frequency meaning

rosario b hearst classifying semantic relations noun compounds
via domain specific lexical hierarchy proceedings conference
empirical methods natural language processing emnlp pp
rosario b hearst fillmore c descent hierarchy selection
relational semantics proceedings th annual meeting association
computational linguistics acl pp
rosch e lloyd b cognition categorization lawrence erlbaum hillsdale
nj
ruge g automatic detection thesaurus relations information retrieval applications freksa c jantzen valk r eds foundations computer
science pp springer
sahami dumais heckerman horvitz e bayesian
filtering junk e mail proceedings aaai workshop learning text
categorization
sahlgren introduction random indexing proceedings methods
applications semantic indexing workshop th international conference
terminology knowledge engineering tke copenhagen denmark
sahlgren word space model distributional analysis represent syntagmatic paradigmatic relations words high dimensional vector spaces
ph thesis department linguistics stockholm university
salton g smart retrieval system experiments automatic document processing prentice hall upper saddle river nj
salton g buckley c term weighting approaches automatic text retrieval
information processing management
salton g wong yang c vector space model automatic indexing
communications acm
sarawagi kirpal efficient set joins similarity predicates proceedings acm sigmod international conference management data
sigmod pp york ny acm
scholkopf b smola j muller k r kernel principal component analysis
proceedings international conference artificial neural networks icann pp berlin
schutze h automatic word sense discrimination computational linguistics

schutze h pedersen j vector model syntagmatic paradigmatic
relatedness making sense words proceedings conference pp
oxford england
sebastiani f machine learning automated text categorization acm computing
surveys csur
shannon c mathematical theory communication bell system technical
journal


fiturney pantel

singhal salton g mitra buckley c document length normalization
information processing management
smith e osherson rips l keane combining prototypes selective
modification model cognitive science
snow r jurafsky ng semantic taxonomy induction heterogenous evidence proceedings st international conference computational
linguistics th annual meeting acl pp
sparck jones k statistical interpretation term specificity application
retrieval journal documentation
spearman c general intelligence objectively determined measured american journal psychology
sproat r emerson first international chinese word segmentation bakeoff proceedings second sighan workshop chinese language processing
pp sapporo japan
stone p j dunphy c smith ogilvie general inquirer
computer content analysis mit press cambridge
tan b peng f unsupervised query segmentation generative language
wikipedia proceeding th international conference world
wide web www pp york ny acm
tellex katz b lin j fern marton g quantitative evaluation
passage retrieval question answering proceedings th annual
international acm sigir conference development information
retrieval sigir pp
tucker l r mathematical notes three mode factor analysis psychometrika
turney p mining web synonyms pmi ir versus lsa toefl
proceedings twelfth european conference machine learning ecml
pp freiburg germany
turney p measuring semantic similarity latent relational analysis proceedings nineteenth international joint conference artificial intelligence
ijcai pp edinburgh scotland
turney p similarity semantic relations computational linguistics

turney p empirical evaluation four tensor decomposition tech
rep institute information technology national council canada
technical report erb
turney p latent relation mapping engine experiments
journal artificial intelligence
turney p b uniform analogies synonyms antonyms associations proceedings nd international conference computational
linguistics coling pp manchester uk


fifrom frequency meaning

turney p littman l measuring praise criticism inference
semantic orientation association acm transactions information systems

turney p littman l corpus learning analogies semantic
relations machine learning
turney p littman l bigham j shnayder v combining independent
modules solve multiple choice synonym analogy proceedings
international conference recent advances natural language processing
ranlp pp borovets bulgaria
van de cruys non negative tensor factorization model selectional preference
induction proceedings workshop geometric natural language
semantics gems pp athens greece
van rijsbergen c j geometry information retrieval cambridge university
press cambridge uk
van rijsbergen c j information retrieval butterworths
veale analogical thesaurus proceedings th innovative applications artificial intelligence conference iaai pp acapulco
mexico
veale wordnet sits sat knowledge lexical analogy
proceedings th european conference artificial intelligence ecai
pp valencia spain
vozalis e margaritis k analysis recommender systems
proceedings th hellenic european conference computer mathematics
applications hercma athens greece
vyas v pantel p semi automatic entity set refinement proceedings
naacl boulder co
weaver w translation locke w booth eds machine translation
languages fourteen essays mit press cambridge
weeds j weir mccarthy characterising measures lexical distributional similarity proceedings th international conference computational linguistics coling pp association computational
linguistics
wei x peng f dumoulin b analyzing web text association disambiguate
abbreviation queries proceedings st annual international acm sigir
conference development information retrieval sigir pp
york ny acm
wen j r nie j zhang h j clustering user queries search engine
proceedings th international conference world wide web www
pp york ny acm
widdows geometry meaning center study language
information stanford ca


fiturney pantel

widdows ferraro k semantic vectors scalable open source package
online technology management application proceedings sixth international
conference language resources evaluation lrec pp
witten h frank e data mining practical machine learning tools
techniques java implementations morgan kaufmann san francisco
wittgenstein l philosophical investigations blackwell translated g e
anscombe
wolfe b w schreiner e rehder b laham foltz p w kintsch w
landauer k learning text matching readers texts latent
semantic analysis discourse processes
yang evaluation statistical approaches text categorization information
retrieval
yuret yatbaz noisy channel model unsupervised word sense
disambiguation computational linguistics review
zamir etzioni grouper dynamic clustering interface web search
computer networks international journal computer telecommunications networking
zhao karypis g evaluation hierarchical clustering document datasets proceedings eleventh international conference information knowledge management pp mclean virginia




