Journal Artificial Intelligence Research 37 (2010) 1-39

Submitted 07/09; published 01/10

Text Relatedness Based Word Thesaurus
George Tsatsaronis

GBT @ IDI . NTNU .

Department Computer Information Science
Norwegian University Science Technology, Norway

Iraklis Varlamis

VARLAMIS @ HUA . GR

Department Informatics Telematics
Harokopio University, Greece

Michalis Vazirgiannis

MVAZIRG @ AUEB . GR

Department Informatics
Athens University Economics Business, Greece

Abstract
computation relatedness two fragments text automated manner requires
taking account wide range factors pertaining meaning two fragments convey,
pairwise relations words. Without doubt, measure relatedness
text segments must take account lexical semantic relatedness words.
measure captures well aspects text relatedness may help many tasks,
text retrieval, classification clustering. paper present new approach measuring
semantic relatedness words based implicit semantic links. approach exploits word thesaurus order devise implicit semantic links words. Based
approach, introduce Omiotis, new measure semantic relatedness texts
capitalizes word-to-word semantic relatedness measure (SR) extends measure
relatedness texts. gradually validate method: first evaluate performance
semantic relatedness measure individual words, covering word-to-word similarity relatedness, synonym identification word analogy; then, proceed evaluating
performance method measuring text-to-text semantic relatedness two tasks, namely
sentence-to-sentence similarity paraphrase recognition. Experimental evaluation shows
proposed method outperforms every lexicon-based method semantic relatedness selected
tasks used data sets, competes well corpus-based hybrid approaches.

1. Introduction
Relatedness texts perceived several different ways. Primarily, one think
lexical relatedness similarity texts, easily captured vectorial representation texts (van Rijsbergen, 1979) standard similarity measure, Cosine, Dice (Salton
& McGill, 1983), Jaccard (1901). models high impact information retrieval
past decades. Several improvements proposed techniques towards inventing sophisticated weighting schemes text words, example TF-IDF
variations (Aizawa, 2003). directions explore need capture latent semantic relations dimensions (words) created vector space model, using techniques latent
semantic analysis (Landauer, Foltz, & Laham, 1998). Another aspect text relatedness, probably
equal importance, semantic relatedness two text segments. example, sentences shares company dropped 14 cents, business institutions stock slumped
14 cents obvious semantic relatedness, traditional measures text similarity fail
c
2010
AI Access Foundation. rights reserved.

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

recognize. motivation work show measure relatedness texts,
takes account lexical semantic relatedness word elements, performs
better traditional lexical matching models, handle cases one above.
paper propose Omiotis1 , new measure semantic relatedness texts,
extends SR, measure semantic relatedness words. word-to-word relatedness measure, turn, based construction semantic links individual words, according
word thesaurus, case WordNet (Fellbaum, 1998). pair words potentially connected via one semantic paths, one comprising one semantic
relations (edges) connect intermediate thesaurus concepts (nodes). weighting semantic
path consider three key factors: (a) semantic path length, (b) intermediate nodes specificity denoted node depth thesaurus hierarchy, (c) types semantic edges
compose path. triptych allows measure perform well complex linguistic tasks,
require simple similarity, SAT Analogy Test2 demonstrated
experiments. best knowledge, Omiotis first measure semantic relatedness
texts considers tandem three factors measuring pairwise word-to-word
semantic relatedness scores. Omiotis integrates semantic relatedness word level words
statistical information text level provide final semantic relatedness score texts.
contributions work summarized following: 1) new measure computing semantic relatedness words, namely SR, exploits semantic information thesaurus offer, including semantic relations crossing parts speech (POS), taking
account relation weights depth thesaurus nodes; 2) new measure computing semantic relatedness texts, namely Omiotis, require use external
corpora learning methods, supervised unsupervised, 3) thorough experimental evaluation
benchmark data sets measuring performance word-to-word similarity relatedness
tasks, well word analogy; addition, experimental evaluation two text related tasks
(sentence-to-sentence similarity paraphrase recognition) measuring performance
text-to-text relatedness measure. Additional contributions work are: a) use semantic relations offered WordNet, increases chances finding semantic path
two words, b) availability pre-computed semantic relatedness scores every pair
WordNet senses, accelerates computation semantic relatedness texts facilitates incorporation semantic relatedness several applications (Tsatsaronis, Varlamis,
Nrvag, & Vazirgiannis, 2009; Tsatsaronis & Panagiotopoulou, 2009).
rest paper organized follows: Section 2 discusses preliminary concepts regarding
word thesauri, semantic network construction, semantic relatedness similarity measures,
summarizes related work fields. Section 3 presents key contributions work.
Section 4 provides experimental evaluation analysis results. Finally, Section 5
presents conclusions next steps work.

2. Preliminaries Related Work
approach capitalizes word thesaurus order define measure semantic relatedness
words, expands measure compute text relatedness using semantic
1. Omiotis Greek word relatedness similarity.
2. http://www.aclweb.org/aclwiki/index.php?title=SAT_Analogy_Questions

2

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

lexical information. order facilitate understanding methodology elaborate
preliminary concepts section present related research approaches.
2.1 Word Thesauri use Text Applications
Word thesauri, WordNet (Fellbaum, 1998) Rogets International Thesaurus (Morris & Hirst,
1991), constitute knowledge base several text-related research tasks. WordNet
used successfully knowledge base construction Generalized Vector Space Models
(GVSM) semantic kernels document similarity application text classification,
works Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald Weikum (2005), Basili,
Cammisa Moschitti (2005), text retrieval, works Voorhees (1993), Stokoe,
Oakes Tait (2003), previous work regarding definition new GVSM uses
word-to-word semantic relatedness (Tsatsaronis & Panagiotopoulou, 2009). Furthermore, idea
using thesaurus knowledge base text retrieval proven successful case
cross language information retrieval, example case CLIR system introduced
Clough Stevenson (2004). Finally, exploitation word thesauri linguistic tasks,
Word Sense Disambiguation (WSD) (Ide & Veronis, 1998) yielded interesting results
(Mihalcea & Moldovan, 1999; Tsatsaronis, Vazirgiannis, & Androutsopoulos, 2007; Tsatsaronis,
Varlamis, & Vazirgiannis, 2008).
application text relatedness measure text classification retrieval tasks
first consider impact lexical ambiguity WSD overall performance tasks.
Sanderson (1994, 2008) concludes ambiguity words take many forms, new test collections needed realize true importance resolving ambiguity embedding semantic
relatedness sense disambiguation text retrieval task. analysis Barzilay Elhadad (1997), Barzilay, Elhadad McKeown (2002) impact WSD performance
text summarization tasks addressed considering possible interpretations lexical
chains created text. Similar methodology, tackle word ambiguity taking account every possible type semantic information thesaurus offer, given sense
text word.
aforementioned approaches, clear use word thesaurus offer much
potential design models capture semantic relatedness texts, consequently, may improve performance existing retrieval classification models certain
circumstances discussed respective research works (Mavroeidis et al., 2005; Basili
et al., 2005; Stokoe et al., 2003; Clough & Stevenson, 2004). word thesaurus employed
development Omiotis WordNet. lexical database contains English nouns, verbs, adjectives
adverbs, organized sets synonym senses (synsets). Hereafter, terms senses, synsets
concepts used interchangeably. Synsets connected various links represent semantic
relations (i.e., hypernymy / hyponymy, meronymy / holonymy, synonymy / antonymy,
entailment / causality, troponymy, domain / domain terms, derivationally related forms, coordinate
terms, attributes, stem adjectives, etc.). Several relations cross parts speech, domain
terms relation, connects senses pertaining domain (e.g., light, noun meaning electromagnetic radiation producing visual sensation, belongs domain physics).
best knowledge, proposed approach first utilizes aforementioned
semantic relations exist WordNet construction semantic relatedness measure.
3

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

2.2 Creating Semantic Networks Word Thesauri
Omiotis based creation semantic paths words text using thesaurus
concepts relations. Early approaches field, used gloss words respective word
definitions order build semantic networks text (Veronis & Ide, 1990). idea representing text semantic network initially introduced Quilian (1969). expansion
WordNet semantic relations cross parts speech added possibilities semantic network construction text. recent approaches semantic network construction
word thesauri, Mihalcea, Tarau Figa (2004) Navigli (2008), utilize wide range
WordNet semantic relations instead gloss words. methods outperformed previous
methods used semantic networks words WSD tasks Senseval 2 3 English language (Palmer, Fellbaum, & Cotton, 2001; Snyder & Palmer, 2004). work adopt
semantic network construction method introduced past (Tsatsaronis et al., 2007).
method utilizes available semantic relations WordNet. WSD task, respective method outperformed matched previous methods used semantic networks
words WSD tasks Senseval 2 3 English language, largely due rich
representation semantic networks offered. Section 3.1 introduces semantic relatedness
measure.
2.3 Measures Semantic Relatedness
Semantic relatedness words concepts exploited, past, text summarization (Barzilay et al., 2002), text retrieval (Stokoe et al., 2003; Smeaton, Kelledy, & ODonnell,
1995; Richardson & Smeaton, 1995) WSD (Patwardhan, Banerjee, & Pedersen, 2003) tasks.
Semantic relatedness measures widely classified dictionary-based3 , corpus-based hybrid.
Among dictionary-based measures, measure Agirre Rigau (1995) one first
measures developed compute semantic relatedness two concepts (i.e., set
concepts). measure based density depth concepts set
length shortest path connects them. However, assume edges path
equally important.
measure proposed Leacock, Miller Chodorow (1998) computing semantic
similarity pair concepts takes account length shortest path connecting
them, measured number nodes participating path, maximum depth
taxonomy. measure two concepts s1 s2 computed follows:
Sim(s1 , s2 ) = log

length
2D

(1)

length length shortest path connecting s1 s2 maximum depth
taxonomy used.
Regarding hybrid measures, Resniks (1995, 1999) measure pairs concepts based
Information Content (IC) deepest concept subsume (least common subsumer),
considered hybrid measure, since combines hierarchy used thesaurus, statistical information concepts measured large corpora. specifically,
3. found bibliography knowledge-based, thesaurus-based, lexicon-based.

4

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

semantic similarity given pair concepts s1 s2 , s0 least common
subsumer (i.e., least common ancestor), defined following equation:
Sim(s1 , s2 ) = IC(s0 )

(2)

Information Content (IC) concept (i.e., s0 ) defined as:
IC(s0 ) = logP (s0 )

(3)

P (s0 ) probability occurrence concept s0 large corpus.
measure proposed Jiang Conrath (1997), based concept IC. Given
two concepts s1 s2 , least common subsumer s0 , semantic similarity defined
follows:
1
Sim(s1 , s2 ) =
(4)
IC(s1 ) + IC(s2 ) 2 IC(s0 )
measure Lin (1998) based IC. Given, again, s1 , s2 , s0 , before,
similarity s1 s2 defined follows:
Sim(s1 , s2 ) =

2 IC(s0 )
IC(s1 ) + IC(s2 )

(5)

Hirst St-Onge (1998) reexamine idea constructing lexical chains words,
based synsets respective semantic edges connect WordNet. initial
idea lexical chains first introduced Morris Hirst (1991), defined lexical
cohesion passage, based cohesion lexical chains passages elements,
acted indicator continuity passages lexical meaning.
encourage reader consult analysis Budanitsky Hirst (2006) detailed
discussion aforementioned measures, well measures proposed prior
aforementioned. measures use noun hierarchy (except measure
Hirst St-Onge), implementation several measures provided Patwardhan,
Banerjee Pedersen (2003) publicly available WordNet::Similarity package utilize
verb hierarchy. Still, relations cross parts speech considered, well
factors discussed detail Section 3. contrast, measure defines semantic relatedness
two concepts, independently Part Speech (POS), utilizing available
semantic links offered WordNet.
recent works interest semantic relatedness, include: measures Jarmasz
Szpakowicz (2003), use Rogets thesaurus compute semantic similarity, replicating
number WordNet-based approaches, LSA-based measure Finkelstein et al. (2002),
perform Latent Semantic Analysis (Landauer et al., 1998) capture text relatedness
considered corpus-based method, measure Patwardhan Pedersen (2006), utilize gloss words found words definitions create WordNet-based context vectors,
methods Strube Ponzetto (2006, 2007a), Gabrilovich Markovitch (2007), Milne
Witten (2008) use Wikipedia compute semantic relatedness considered
corpus-based approaches, method Mihalcea, Corley Strappavara (2006),
hybrid method combines knowledge-based corpus-based measures text relatedness.
recent hybrid measures semantic similarity are: measure proposed Li et al. (2006),
use information WordNet corpus statistics collected Brown Corpus (Kucera,
5

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Francis, & Caroll, 1967) compute similarity short texts, measure text distance proposed Tsang (2008), uses distributional similarity ontological/knowledge
information compute distance text fragments. Distributional similarity used
supervised combination WordNet-based approaches (Agirre, Alfonseca, Hall, Kravalova,
Pasca, & Soroa, 2009), produce supervised measure semantic relatedness. Li et al. (2006)
created new data set experimental evaluation, use Section 4
evaluate Omiotis measure compare approach.
following section formally define Omiotis provide details, creation
semantic links computation relatedness words texts. give evidence
measures complexity justify design choices. Finally, discuss potential applications
measure text related tasks.

3. Measuring Word-to-Word Text-to-Text Semantic Relatedness
section presents details Omiotis, measure text semantic relatedness. measure
capitalizes idea semantic relatedness WordNet senses, extends compute
relatedness words finally texts. Since definition semantic relatedness
ranges pairs keyword senses pairs texts, Omiotis defined way captures
relatedness every granularity. result, applied wide range linguistic
text related tasks WSD, word similarity word analogy, text similarity, keyword
ranking. key points proposed measure are: (a) constructs semantic links
word senses WordNet pre-computes relatedness score every pair WordNet
senses, (b) computes semantic relatedness pair words taking account
relatedness corresponding WordNet senses, (c) computes semantic relatedness score
two given text segments extending word-to-word relatedness. Depending task,
computation semantic relatedness modified take account senses
word, words text, apply additional weights depending
word importance sense importance context. allows Omiotis adapted various
text related tasks, without modifying main process computing relatedness. Section 3.1
follows, formally define semantic relatedness measure Section 3.2 provide
detailed justification design decisions.
3.1 Construct Semantic Links Words
first step measuring semantic relatedness two text fragments, find
implicit semantic links words two fragments. Thus, present definition
semantic relatedness pair thesaurus concepts, takes account semantic path
connecting concepts, expands measure relatedness words. order solve
problem constructing semantic paths words, base approach previous
method construct semantic networks words (Tsatsaronis et al., 2007).
3.1.1 EMANTIC N ETWORK C ONSTRUCTION W ORD HESAURI
Figure 1 gives example construction semantic network two words ti tj .
simplicity reasons, assume construction semantic path senses S.i.2 S.j.1
(Initial Phase), though could every possible combination two words
6

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

ti

S.i.1

S.j.1

S.i.2

S.j.2

...

...

S.i.7

S.j.5

Synonym

...

Holonym

Meronym
S.i.2

tj

...

S.j.1

Hypernym
Antonym

...
Hyponym

Initial Phase
Index:

= Word Node

Network Expansion
= Sense Node

= Semantic Link

Figure 1: Constructing semantic networks word thesauri.
senses. Initially, two sense nodes expanded using semantic links offered WordNet.
semantic links senses, found thesaurus, become edges pointed senses
nodes network (Network Expansion). expansion process repeated recursively
shortest 4 path S.i.2 S.j.1 found. path found S.i.2 S.j.1
senses consequently words semantically related.
3.1.2 EMANTIC R ELATEDNESS



PAIR C ONCEPTS

semantic relatedness pair concepts measured constructed semantic network.
considers path length, captured compactness, path depth, captured semantic
path elaboration, defined following. measure WSD based idea
compactness initially proposed Mavroeidis et al. (2005). original measure used
nouns hypernym relation, extended current work support WordNets
relations noun, verb adjective parts speech. define new compactness
measure (Definition 1) core Omiotis measure.
Definition 1 Given word thesaurus O, weighting scheme edges assigns weight
w (0, 1) edge, pair senses = (s1 , s2 ), path P length l connecting
Qthe two
senses, semantic compactness (SCM (S, O, P )) defined as: SCM (S, O, P ) = li=1 wi ,
w1 , w2 , ..., wl paths edges weights. s1 = s2 SCM (S, O, P ) = 1.
path s1 s2 SCM (S, O, P ) = 0.
Note compactness takes path length account bound [0, 1]. Higher compactness
senses implies higher semantic relatedness. intuition behind edge types weighting
certain types provide stronger semantic connections others. Considering lexicographers WordNet tend use relation types often others (we assume
used relation types stronger types less used), straightforward solution define edge
types weights proportion frequency occurrence WordNet 2.0. weights assigned
type using solution shown Table 1 accordance found Song
et al. (2004). table shows probability occurrence WordNet 2.0 every possible edge
type thesaurus, descending order probability values. detailed analysis choices
made Definition 1 definitions follow performed Section 3.2.
depth nodes belong path affects term relatedness. standard means
measuring depth word thesaurus hypernym/hyponym hierarchical relation noun
adjective POS hypernym/troponym verb POS. adverb POS related stem
4. details presented Algorithm 1.

7

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

WordNet 2.0 Edge Type

Probability Occurrence

hypernym/hyponym
nominalization
category domain
part meronym/holonym
region domain
similar
usage domain
member meronym/holonym
antonym
verb group
see
attribute
entailment
cause
substance meronym/holonym
derived
participle

0.61
0.147
0.094
0.0367
0.0238
0.02
0.016
0.014
0.0105
0.01
0.0091
0.00414
0.00195
0.00158
0.00089
0.0003
3.4E 06

Table 1: Probability occurrence every edge type WordNet 2.0.
adjective sense used measure depth. path shallow sense nodes general
compared path deep nodes. parameter semantic relatedness terms
captured measure semantic path elaboration introduced following definition.
Definition 2 Given word thesaurus , pair senses = (s1 , s2 ), s1 ,s2
s1 6= s2, path P =< p1 , p2 , ..., pl > length l, either s1 = p1 s2 = pl
s1 = pl s2 = p1 , semantic path elaboration path (SP E(S, O, P )) defined as:
Q
1
di+1
SP E(S, O, P ) = li=1 d2di +d
dmax
, di depth sense pi according O, dmax
i+1
maximum depth O. s1 = s2 , d1 = d2 = SP E(S, O, P ) =
path s1 s2 SP E(S, O, P ) = 0.


dmax .



obvious Definition 2 path length l comprises l+1 nodes, thus = l, di+1
last node path. Essentially, SPE harmonic mean two depths normalized
maximum thesaurus depth. harmonic mean preferred average depths, since offers lower upper bound gives realistic estimation paths depth. Compactness
Semantic Path Elaboration measures capture two important parameters measuring semantic relatedness terms (Budanitsky & Hirst, 2006), namely path length senses depth
used thesaurus. combine two measures following definition Semantic
Relatedness two terms:
Definition 3 Given word thesaurus O, pair senses = (s1 , s2 ) semantic relatedness
(SR(S, O)) defined maxP {SCM (S, O, P ) SP E(S, O, P )}.
8

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Algorithm 1 Maximum-Semantic-Relatedness(G, u, v, w)
1:
2:

3:
4:
5:
6:
7:
8:
9:
10:
11:

12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:

INPUT: directed weighted graph G, two nodes u, v weighting scheme w : E (0..1).
OUTPUT: path u v maximum product edges weights.
Initialize-Single-Source(G, u)
vertices v VG
d[v] =
[v] = N U
end
d[u] = 1
Relax(u, v, w)
d[v] < d[u] w(u, v)
d[v] = d[u] w(u, v)
[v] = u
end
Maximum-Relatedness(G, u, v, w)
Initialize-Single-Source(G, u)
S=
Q = VG
v Q
= Extract Q vertex maximum
=Ss
vertices k Adjacency List
Relax(s, k, w)
end
end
return path following ancestors v back u

Given word thesaurus, one semantic path connecting two senses.
senses compactness take different values different paths. cases, use
path maximizes semantic relatedness. computation introduce Algorithm 1,
modification Dijkstras algorithm (Cormen, Leiserson, & Rivest, 1990) finding shortest path two nodes weighted directed graph. algorithm, G representation
directed weighted graph given input (e.g., using adjacency lists), VG set
vertices G. Also, two sets used; S, contains vertices
maximum semantic relatedness computed source vertex (i.e., u), Q,
contains vertices algorithm computed yet maximum relatedness source vertex. Furthermore, three tables used; d, which, vertex v stores
maximum semantic relatedness found given time algorithm execution source
vertex, i.e., u d[v]; , vertex v stores predecessor [v]; w, stores
edge weights graph (e.g., w[k, m] stores edge weight edge starts k
goes m).
9

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

algorithm comprises three functions: (a) Initialize-Single-Source(G, u), initializes
tables , every vertex v graph. precisely, sets d[v] = , since semantic relatedness source unknown beginning, algorithm seeks
maximum semantic relatedness initially set minimum value (i.e., ).
sets [v] = N U LL, since beginning algorithm execution aware
yet predecessor vertex v following path source vertex u v results maximum semantic relatedness; (b) Relax(u, v, w), given two vertices, u v
directly connected edge weight w[u, v], updates value d[v], case
follow edge (u, v) results higher semantic relatedness vertex v
source, compared value computed time algorithm execution;
(c) Maximum-Relatedness(G, u, v, w), uses aforementioned functions executes
Dijkstras algorithm. proof algorithms correctness follows next theorem.
Theorem 1 Given word thesaurus O, edges weighting function w : E (0, 1),
higher value declares stronger edge, pair senses S(ss , sf ) declaring source (ss ) destination (sf ) vertices, SCM (S, O, P ) SP E(S, O, P ) maximized path returned
2di dj
=w

Algorithm 1, using weighting scheme wij
ij dmax (di +dj ) , wij new weight
edge connecting senses si sj .
Proof 1 show vertex sf VG , d[sf ] maximum product edges weight
selected path, starting ss , time sf inserted S. on,
notation (ss , sf ) represent product. Path p connects vertex S, namely ss ,
vertex VG S, namely sf . Consider first vertex sy along p sy VG let
sx ys predecessor. Now, path p decomposed ss sx sy sf . claim
d[sy ] = (ss , sy ) sf inserted S. Observe sx S. Then, sf chosen
first vertex d[sf ] 6= (ss , sf ) inserted S, d[sx ] = (ss , sx )
sx inserted S.
sy occurs sf path ss sf edge weights nonnegative
(0, 1) (ss , sy ) (ss , sf ), thus d[sy ] = (ss , sy ) (ss , sf ) d[sf ].
sy sf V sf chosen, d[sf ] d[sy ]. Thus, d[sy ] =
(ss , sy ) = (ss , sf ) = d[sf ]. Consequently, d[sf ] = (ss , sf ) contradicts choice sf .
conclude time vertex sf inserted S, d[sf ] = (ss , sf ).
Next, prove returned maximum product SCM (S, O, P ) SP E(S, O, P ), let
path ss sf maximum edge weight product k edges. Then, Algorithm 1
Q
2dk df
2d2 d3
2ds d2

returns maximum ki=1 wi(i+1)
= ws2 dmax
(ds +d2 ) w23 dmax (d2 +d3 ) ...wkf dmax (dk +df ) =
Qk 2di di+1
Qk
1
i=1 di +di+1 dmax = SCM (S, O, P ) SP E(S, O, P ).
i=1 wi(i+1)
3.1.3 EMANTIC R ELATEDNESS



PAIR ERMS

Based Definition 3, measures semantic relatedness pair senses S,
define semantic relatedness pair terms (t1 , t2 ) follows.
Definition 4 Let word thesaurus O, let = (t1 , t2 ) pair terms entries
O, let X1 set senses t1 X2 set senses t2 O. Let S1 , S2 , ..., S|X1 ||X2 |
set pairs senses, Sk = (si , sj ), si X1 sj X2 . semantic relatedness
(SR(T, S, O)) defined as:
10

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

maxSk {maxP {SCM (Sk , O, P ) SP E(Sk , O, P )}} = maxSk {SR(Sk , O)}
k = 1..|X1 | |X2 |. Semantic relatedness two terms t1 , t2 t1 t2

/ defined 1. Semantic relatedness t1 , t2 t1 t2
/ O, vice versa,
considered 0.
remaining paper, SR(T, S, O) pair terms denoted SR(T ),
ease readability.
3.2 Analysis SR Measure
section present rationale behind Definitions 1, 2, 3, providing theoretical
and/or experimental evidence decisions made design measure. illustrate
advantages disadvantages different alternatives using simple examples argue
decisions. Finally, discuss advantages SR previous measures semantic
relatedness.
list decisions made design semantic relatedness measure comprises: a)
use senses POS, instead noun senses only, b) use semantic edge types found
WordNet, instead IS-A relation only, c) use edge weights, d) use senses depth
scaling factor. important mention measures semantic relatedness differ
measures semantic similarity, traditionally use hierarchical relations ignore
type semantic relations. addition, concepts differentiate semantic distance,
sense latter metric.
3.2.1 U SE POS NFORMATION
Firstly, shall argue fact use POS designing semantic relatedness measure important, increase coverage measure. rationale supporting
decision fairly simple. Current data sets evaluating semantic relatedness even semantic similarity measures restricted nouns, example Rubenstein Goodenough 65 word
pairs (1965), Miller Charles 30 word pairs (1991), Word-Similarity-353 collection
(Finkelstein et al., 2002). Thus, experimental evaluation data sets cannot pinpoint
caveat omitting remaining parts speech. However, text similarity tasks benchmark
data sets comprise nouns. Throughout following analysis, reader must consider
resulting measure semantic relatedness among words destined embedded
text-to-text semantic relatedness, shown next section.
following two sentences paraphrase example taken Microsoft Paraphrase
Corpus (Dolan, Quirk, & Brockett, 2004) show importance using POS well,
verbs:
charges espionage aiding enemy carry death penalty.
convicted spying charges could face death penalty.

Words appear WordNet written bold stopwords omitted simplicity5 .
two sentences many nouns common (charges, death, penalty), pairs
words two sentences contribute evidence two sentences
5. stopwords list used available http://www.db-net.aueb.gr/gbt/resources/stopwords.txt

11

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

paraphrase. example espionage spying obvious semantic relatedness, well
enemy spying. Also, convicted charges, well convicted penalty. type
evidence would disregarded measure semantic relatedness similarity
uses noun POS hierarchy WordNet. Examples measures are: measure
Sussna (1993), Wu Palmer (1994), Jiang Conrath (1997), Resnik (1995, 1999),
WordNet-based component measure proposed Finkelstein et al. (2002). point
view, decision use POS information expands potential matches found measure allows use measure complicated tasks, paraphrase recognition, text
retrieval, text classification.
3.2.2 U SE E YPE EMANTIC R ELATIONS
decision use parts speech construction semantic graphs, introduced previous work (Tsatsaronis et al., 2007), imposes involvement semantic
relations instead merely taxonomic (IS-A) ones. Moreover, decision based evidence
related literature. work Smeaton et al. (1995) provides experimental evidence measuring semantic similarity incorporating non-hierarchical link types (i.e. part meronym/holonym,
member meronym/holonym, substance meronym/holonym) improves much performance
measure. experimental evaluation conducted adopting small variation Resniks
measure (1995).
Hirst St-Onge (1998) reported discovered several limitations missing
connections set WordNet relations construction lexical chains sentences
detection correction malapropisms. provided following example using
pair words bold report caveat:
School administrators say taxpayers expect schools provide child care
school lunches, integrate immigrants community, offer special classes adult
students,.

intrinsic connection nouns child care school, exist WordNet,
cannot discovered considering hierarchical edge types. connection depicted
Figure 2, shows path WordNet. rich semantic representation able detect
connections address problems aforementioned type.
3.2.3 U SE W EIGHTS



E DGES

work Resnik (1999) reports simple edge counting, implicitly assumes links
taxonomy represent uniform distances, problematic best semantic distance
measure WordNet. similar direction lie findings Sussna (1993), performed
thorough experimental evaluation varying edge weights order measure semantic distance
concepts. Sussnas findings, revealed weights semantic edges non-negligible
factor application measure WSD, best results reported
edge weighting scheme used, instead assigning edge weight.
reasons, decided assign weight every edge type, chose simple probability
occurrence edge type WordNet, edge weighting scheme (see Table 1).
important factor absent several similarity measures proposed past,
measures Leacock et al. (1998), Jarmasz Szpakowicz (2003) Banerjee Pedersen
(2003), outperformed experimental evaluation measure.
12

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

activity
(Noun)
Hyponym
Hypernym

education
(Noun)

aid
(Noun)

Nominalization

educate
(Verb)

Hyponym
Hypernym

service
(Noun)

school
(Verb)

Nominalization

school
(Noun)

Hypernym

child care
(Noun)

Figure 2: Semantic path child care school following WordNet edges.
instrumentality
(Noun)

conveyance
(Noun)

Hyponym
Hypernym

Hypernym
implement
(Noun)
container
(Noun)

vehicle
(Noun)

Hyponym

public
transport
(Noun)

Hyponym
Hypernym
bar
(Noun)

Hypernym

Hyponym

wheeled
vehicle
(Noun)

autobus
(Noun)

Hyponym
wheeled
vehicle
(Noun)

Hypernym
lever
(Noun)

self-propelled
vehicle
(Noun)
Category Domain

Hyponym
Hypernym

car
(Noun)

Hypernym

pedal
(Noun)

motor vehicle
(Noun)
passenger
(Noun)

Part Meronym
Hyponym
Hypernym

accelerator
(Noun)

car
(Noun)

Category Domain

NWPL Path
PR Path

Figure 3: Product Relatedness (PR) Normalized Weighted Path Length (NWPL) paths pairs:
car accelerator (left), car autobus (right).

3.2.4 U SE EPTH CALING FACTOR
decision incorporate depth scaling factor (SPE Definition 2) edge weighting
mechanism inspired thorough experimental evaluation conducted Sussna (1993),
13

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

provided evidence importance edge weighting factor semantic network
based measures. According experiments Miller Charles data set Spearman
correlation human judgements much lower (7 percentage points) omitting depth
scaling factor adopting SPE factor (see Definition 3).
3.2.5 J USTIFICATION SR EFINITIONS
According Definition 1, semantic compactness pair concepts product depthscaled weights edges connecting two concepts. use product instead sum
normalized sum edges weights explained following.
Since might several paths connecting two concepts, Definition 3 clearly selects
path maximizes product semantic compactness (SC) semantic path elaboration (SPE). simplicity, ignore effect depth scaling
factor (SPE Definition 2)
Q
consequently, aim find path maximizes li=1 ei , e1 , e2 , ..., el
(non depth-scaled) weights edges path connecting two given concepts. Let us name
less elaborate version semantic relatedness measure product relatedness (PR),
P R(S, O) = maxP {SCM (S, O, P )}. alternative would beenPto define semantic coml

e


pactness normalized sum weights path, is: i=1
. case,
l
semantic relatedness would measured path maximizes latter formula, since
nature, semantic relatedness always seeks find path maximizes connectivity
two concepts. Let us name alternative normalized weighted path length (NWPL).
example Figure 3, show PR NWPL compute semantic relatedness
term pair car accelerator (left) car autobus (right). path maximizes
respective formulas PR NWPL using Algorithm 1 edge weights Table 1, illustrated
Figure 3 using black white arrows respectively. pair car accelerator sum-based
formula, normalized path length, selects large path example, final
computed relatedness 0.61, weight hypernym/hyponym edges. PR finds
path maximizing product immediate part meronym relation car accelerator,
computed relatedness 0.0367, weight part meronym edges. main
problem arising NWPL fact cannot distinguish among relatedness
pair concepts hypernym/hyponym hierarchy WordNet. example, NWPL
computes relatedness (0.61) every possible concept pair shown top figure.
contrast, PR able distinguish pairs terms relatedness. precisely,
behavior PR due fact embeds notion path length, since computed
relatedness decays factor range (0, 1) every hop made following type semantic
relation. Another example, shows importance considering WordNet relations,
one shown right part Figure 3, NWPL PR paths computed
term pair car autobus. Again, NWPL selects large path, incline
hypernym/hyponym tree.
Clearly, NWPL would rather traverse huge path hypernym/hyponym edges,
following less important edge type, would decrease average path importance.
behavior creates serious drawbacks: (a) lack ability distinguish relatedness among
pair concepts hierarchy, (b) large increase actual computational cost
Algorithm 1, due fact tend incline hypernym/hyponym hierarchy,
even direct semantic edge (other hypernym/hyponym) connecting two concepts,

14

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

shown Figure 3. Furthermore, conducting experiments NWPL 30 word pairs
Miller Charles, discovered almost 40% cases, NWPL produces
value semantic relatedness, equal 0.61, unable distinguish creating many
ties. Thus, PR better option use measure, semantic compactness factor.
Last, least, regarding overall design SR, mention proposed measure solely based use WordNet, contrast measures semantic relatedness use
large corpora, Wikipedia. Although, measures, ones proposed Gabrilovich
Markovitch (2007), Ponzetto Strube (2007a), provide larger coverage regarding concepts reside WordNet, require processing large corpora (Wikipedia),
changes fast frequently. Experimental evaluation Section 4 shows
measure competes well aforementioned word-to-word relatedness measures
used data sets. following section, introduce Omiotis, extension SR measuring
text-to-text relatedness.
3.3 Omiotis
quantify degree two text segments semantically relate other, build upon
SR measure, significantly extend order account terms semantic
relatedness lexical similarity. texts may contain overly-specialized
terms (e.g., algorithms name) represented WordNet. Therefore, relying entirely
term semantics identifying degree texts relate would hamper
performance approach. hand, semantics serve complement relevance
estimations given different text terms might refer (nearly-) identical concepts.
quantify lexical similarity two texts, e.g., text B, begin estimation terms importance weights determined standard TF-IDF weighting
scheme (Salton, Buckley, & Yu, 1982).
Thereafter, estimate lexical relevance, denoted a,b terms b B
based harmonic mean respective terms TF-IDF values, given by:
a,b =

2 F IDF (a, A) F IDF (b, B)
F IDF (a, A) + F IDF (b, B)

(6)

Harmonic mean preferred instead average, since provides tight upper bound (Li,
2008). decision based fact F IDF (a, A) F IDF (b, B) two different
quantities measuring qualitative strength b respective texts.
computed lexical relevance text terms b, estimate semantic
relatedness, i.e. SR(a, b) described previously. Based estimated lexical relevance
semantic relatedness pairs text terms, next step find every word text
corresponding word b text B maximizes product semantic relatedness lexical
similarity values given Equation 7.
b = arg max(a,b SR(a, b))

(7)

bB

b corresponds term text B, entails maximum lexical similarity
semantic relatedness term text A.6 similar manner, define , corresponds
6. function argmax selects case examined ones, maximizes input formula function.

15

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

term text A, entails maximum lexical similarity semantic relatedness
term b text B.
= arg max(a,b SR(a, b))
(8)
aA

Consequently, aggregate lexical semantic relevance scores terms text A,
reference best match text B denoted shown Equation 9.

!
X
1
(A, B) =
(9)
a,b SR(a, b )
|A|
aA

opposite direction (i.e. words B words A) cover
cases two texts equal number terms.
Finally, derive degree relevance texts B combining values
estimated terms entail maximum lexical semantic relevance one another,
given by:
[(A, B) + (B, A)]
(10)
2
Algorithm 2 summarizes computation Omiotis. computation entails series steps,
complexity discussed Section 3.5.
Omiotis(A, B) =

3.4 Applications Semantic Relatedness
section describe methodology incorporating semantic relatedness pairs
words pairs text segments, several applications.
3.4.1 W ORD - -W ORD IMILARITY
Rubenstein Goodenough (1965) obtained synonymy judgements 51 human subjects 65
pairs words, effort investigate relationship similarity context similarity meaning (synonymy). Since then, idea evaluating computational measures semantic
relatedness comparing human judgments given set word pairs, widely
used, even data sets developed. proposed measure semantic relatedness
words (SR), introduced Definition 4, used directly task, order
evaluate basis Omiotis measure, measurement word-to-word semantic relatedness. application straightforward: Let n pairs words used word similarity data
set. Then, semantic relatedness every pair computed, using SR(T, S, O) defined 4.
computed values sorted descending order, produced ranking similarities
compared gold standard ranking humans, using Spearman correlation. scores
used compute Pearsons product moment correlation. Additional measures semantic
relatedness compared examining respective correlation values
human judgements.
3.4.2 SAT NALOGY ESTS
problem identifying similarities word analogies among pairs words difficult problem
standardized test assessing human ability language understanding,
16

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Algorithm 2 Omiotis(A,B, Sem, Lex )
1: INPUT: Two texts B, comprising n terms (a b terms B
respectively),
semantic relatedness measure Sem : SR(a, b) (0..1),
weighting scheme term importance text Lex : F IDF (a, A) (0..1)
2: OUTPUT: Find pair terms maximizes product Sem Lex values.

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

Compute-Zeta(A,B)
sum(A) := 0
terms
b := N U
empZeta := 0
terms b B
a,b = 2Lex(a,A)Lex(b,B)
Lex(a,A)+Lex(b,B)
empZeta < a,b Sem(a, b)
empZeta = i,j Sem(a, b)
b = b
end
end
sum(A) := sum(A) + empZeta
end
Zeta(A, B) := sum(A)/|A|
Compute-Omiotis(A,B)

17:

Omiotis(A, B) :=

Zeta(A,B)+Zeta(B,A)
2

scope well known SAT analogy tests (Scholastic Aptitude Tests). SAT tests
used admission tests universities colleges United States. participants aim
locate five given word pairs one presents similar analogy target
pair.
Although difficult machines model human cognition word analogy, several
approaches exist bibliography attempt tackle problem. Previous approaches
widely categorized into: corpus-based, lexicon-based hybrid. examples corpus-based
approaches Turney (2008b) Bicici Yuret (2006). Examples lexicon-based
approaches, Veale (2004) application lexicon-based measure Hirst
St-Onge (1998) SAT, found work Turney (2006). Hybrid approaches
applied SAT, application measures Resnik (1995) Lin (1998)
found work Turney (2006).
order reader understand difficulty answering SAT questions, must point
average US college applicant scores 57% (Turney & Littman, 2005), top
corpus-based approach scores 56.1% (Turney, 2006), top lexicon-based scores 42% (Veale,
2004) top hybrid scores 33.2% (Resnik, 1995).
17

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Another way categorizing approaches measure semantic similarity analogy tasks
distinguish among attributional relational similarity measures (Gentner, 1983).7 Representative approaches first category lexicon-based approaches, paradigms relational
similarity measures found approaches based Latent Relational Analysis (LRA) (Turney,
2006). great interest point LRA-based approaches, LRME algorithm proposed recently Turney (2008a), superior attributional similarity approaches discovering
word analogies. fact supported experimental findings Turney (2006). Without
doubt, relational similarity approaches may perform better SAT analogy task, still,
shown later experiments conducted applications, paraphrase recognition,
lexicon-based measures outperform LRA-based approaches tasks.
Semantic relatedness (SR) words, applied Omiotis, exploited solve
word analogy task. aim word analogy is, given pair words w1 w2 , identify
series semantic relations lead w1 w2 (semantic path). SAT test, target pair
(w1 ,w2 ) candidate word pairs (w1k ,w2k ), k usually 1 5, processed order
find pairs analogy. aim locate pair k, exposes maximum similarity w1
w2 . straightforward method choose among 5 candidate pairs employ two criteria:
first, k analogies analogy target pair compared, candidate
shows far similar analogy selected. However, similar analogy
obvious, 6 pairs may examined together order slightest differences lead
correct answer discovered. attempt model human cognition task using SR
two fold manner: (a) measure SR capture horizontal analogy given pair
possible candidate pairs, (b) measure SR capture vertical analogy
given pair possible candidate pairs. two aspects covered following
Equations 11 13. capture horizontal analogy pair words candidate pair,
measure difference SR score two words respectively shown:
s1 (w1k , w2k ) = 1 |SR(w1 , w2 ) SR(w1k , w2k )|

(11)

Essentially, s1 expresses horizontal analogy candidate pair (w1k , w2k ) given
pair (w1 , w2 ). Similarly, capture notion vertical analogy two pairs
computing difference SR scores among two pairs words, follows:
s2 (w1k , w2k ) = 1 |SR(w1 , w1k ) SR(w2 , w2k )|

(12)

Finally, rank candidates depending combined vertical horizontal analogy
given pair, according following equation:
s(w1k , w2k ) =

s1 (w1k , w2k ) + s2 (w1k , w2k )
2

(13)

Eventually, select candidate pair maximum combined score, taking account
aspects (horizontal vertical) analogy given candidate pairs.
intuition behind selection two scores handling SAT test,
following. order words pairs (both target candidates) random. Usually,
given pair (w1 , w2 ), candidate pairs (w1k , w2k ) test solved one successfully
7. Two objects, X Y, attributionally similar attributes X similar attributes Y. Two pairs, A:B C:D,
relationally similar relations B similar relations C D.

18

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Stem: wallet : money

Choices:

(a)

safe : lock

(b)

suitcase : clothing

S1: 0.2605
S2: 6.75E-04
(c)

camera : film

S1: 0.4795
S2: 0.015
(d)

setting : jewel

S1: 0.1805
S2: 7.87E-05
(e)

car : engine

S1: 0.3764
S2: 8.99E-05

Winner based S1 (Horizontal Analogy): b
Winner based S2 (Vertical Analogy): b
Winner based combined S: b
Correct Answer: b

S1: 0.1506
S2: 0.0029

Figure 4: Example computing Semantic Relatedness measure (SR) given Scholastic Aptitude Test (SAT) question.

find analogy: w1k w2k w1 w2 . perspective, s1 s2 try find
candidate pair best aligns target pair. Figure 4 illustrates two types analogies
(horizontal vertical) example SAT question.
order motivate selection s1 s2 answering SAT questions,
discuss detail two quantities pertain concepts strength type
relations pair SAT words. Turney (2006) describes method comparing
relations candidate word pairs stem word pair, utilizes type
relation connecting words pair finally selects pair best matches
type relation connecting words stem pair. Though explicitly examine
label edges connecting words pair, implicitly computing SR
them. Since weighting WordNet edges fine grained, distinguishes every
type semantic relation WordNet, instead labels, using edge weights. SR definition
provide fine grained distinguishment two pairs words, depending types
edges connecting words respectively, expressed weights, taking
account factors, depth nodes comprising connecting path inside
thesaurus. Besides s1 , attempts capture aforementioned properties word pairs,
s2 attempts words order among two word pairs (i.e., first word
first pair, second word second pair). forms attempt capture
aligned two word pairs, according SR values words.
3.4.3 PARAPHRASE R ECOGNITION



ENTENCE - -S ENTENCE IMILARITY

Performance applications relying natural language processing may suffer fact
processed documents might contain lexically different, yet semantically related, text segments.
task recognizing synonym text segments, better known paraphrase recognition,
detection, challenging difficult solve, shown work Pasca (2005). task
important many text related applications, summarization (Hirao, Fukusima, Oku19

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

mura, Nobata, & Nanba, 2005), information extraction (Shinyama & Sekine, 2003) question
answering (Pasca, 2003). experimentally evaluate application Omiotis paraphrasing
detection task (Section 4.2), using Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
application Omiotis paraphrase detection straightforward: given pair text segments,
compute Omiotis score them, using Equation 10 Algorithm 2. Higher values
Omiotis given pair denote stronger semantic relation examined text segments.
task reduced define threshold, Omiotis value considered
determining sign paraphrasing pair. experimental evaluation Omiotis, explain
detail selected threshold paraphrase recognition task.
similar manner, using Equation 10 Algorithm 2, semantic relatedness scores
pairs sentences computed. task, using data set Li et al. (2006)
evaluate Omiotis, comprising 30 sentence pairs, human scores provided. Section 4
describe detail experimental set up.
3.5 Complexity Implementation Issues
computation Omiotis entails series steps, complexity strongly related
base measure Semantic Relatedness (SR). Primarily, given two words, w1 w2 construction
time semantic network used compute SR according Algorithm 1, proven
O(2 k l+1 ) (Tsatsaronis et al., 2007), k maximum branching factor used
thesaurus nodes l maximum semantic path length thesaurus. semantic
network constructed, complexity Algorithm 1 reduced standard time complexity
cost Dijkstras algorithm. Using Fibonacci heaps, possible alleviate computational
burden Dijkstra improve time complexity. semantic network, Dijkstra takes
O(nL + mD + nE), n number nodes network, number edges, L
time insert, time decrease-key E time extract-min. Fibonacci heaps
used L = = O(1) cost extract-min O(logn), thus significantly reducing
cost execution. whole procedure repeated 2 n1 n2 times computation
Omiotis two documents d1 d2 total n1 n2 distinct words respectively.
aforementioned, obvious computation Omiotis cheap general.
purpose, order improve systems scalability, pre-computed stored
SR values every possible pair synsets RDBMS. one-time computation
cost, dramatically decreases computational complexity Omiotis. database schema
three entities, namely Node, Edge Paths. Node contains WordNet synsets. Edge indexes
edges WordNet graph adding weight information edge computed using SR
measure. Finally, Paths contains pairs WordNet synsets directly indirectly connected WordNet graph computed relatedness. pairs found running
Breadth First Search (BFS) starting WordNet roots POS. Table 2 provides statistical
information RDBMS exceeds 220 Gbytes size. Column 1 indicates number
distinct synsets examined, column 2 shows total number edges, column 3 depicts
number connected synsets (by least one path following offered WordNet edges).
current implementation takes advantage database structures (indices, stored procedures etc)
order decrease computational complexity Omiotis. following example indicative
complexity SR computation. average number senses per term 5 7
2
(depending POS). pair terms known POS, perform n2 (n 6) combinations
20

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Distinct Synsets
115,424

Total Edges
324,268

Connected Synset Pairs
5,591,162,361

Table 2: Statistics WordNet graph implemented database.

pair synsets compute similarity presented Definition 3.
similarities pre-computed, time required processing 100 pairs terms 1 sec,
makes computation Omiotis feasible scalable. proof concept, developed
on-line version SR Omiotis measures8 , user test term-to-term
sentence-to-sentence semantic relatedness measures (Tsatsaronis et al., 2009).

4. Experimental Evaluation
experimental evaluation Omiotis two-fold. First, test performance wordto-word semantic relatedness measure (SR), Omiotis based, three types tasks: (a)
word-to-word similarity relatedness, (b) synonym identification, (c) Scholastic Aptitude
Test (SAT). Second, evaluate performance Omiotis two tasks: (a) sentence-to-sentence
similarity, (b) paraphrase recognition task.
4.1 Evaluation Semantic Relatedness (SR) Measure
evaluation proposed semantic relatedness measure two terms experimented three different categories tests. first category comprises data sets contain
word pairs, human subjects provided similarity scores relatedness scores.
provided scores create ranking word pairs, similar irrelevant.
evaluate performance measures, computing correlation list human
rankings list produced measures. task, evaluate performance SR
three benchmark data sets, namely Rubenstein Goodenough 65 word pairs (1965) (R&G),
Miller Charles 30 word pairs (1991) (M&C), humans provided similarity scores, and, also, Word-Similarity-353 collection (Finkelstein et al., 2002) (353-C),
comprises 353 word pairs, humans provided relatedness scores.
second category experiments comprises synonym identification tests. tests, given
initial word, appropriate synonym word must identified among given options.
task evaluate performance SR TOEFL data set, comprising 80 multiple
choice synonym questions, ESL data set, comprising 50 multiple choice synonym questions
questions.9
third category experiments based Scholastic Aptitude Test (SAT) questions.
SAT, given pair words, relevant pair among five given pairs must selected.
task based word analogy identification. evaluation data set comprises 374 test questions.
8. Publicly available http://omiotis.hua.gr
9. http://www.aclweb.org/aclwiki/index.php?title=TOEFL_Synonym_Questions
http://www.aclweb.org/aclwiki/index.php?title=ESL_Synonym_Questions_(State_of_the_art)

21

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-based

Corpus-based

Hybrid

Method
HS
LC
JS
GM
WLM
SP
IS-A SP
JC
L
R
HR
SR

R&G
Spearmans Pearsons r
0.745
0.786

0.785
0.838
N/A
0.818

0.816
N/A

0.64
N/A
N/A
0.52
N/A
0.70

0.709
0.781

0.77
0.818
0.7485
0.778
0.817
N/A
0.8614
0.876

M&C
Spearmans Pearsons r
0.653
0.744

0.748
0.816
N/A
0.878
0.723
N/A

0.70
N/A
N/A
0.47
N/A
0.69
0.805
0.85
0.767
0.829
0.737
0.774
0.904
N/A
0.856
0.864

Table 3: Spearmans Pearsons correlations Rubenstein Goodenough (R&G)
Miller Charles (M&C) data sets. Confidence levels: =0.90, =0.95, =0.99

4.1.1 E VALUATION



EMANTIC IMILARITY



R ELATEDNESS

first category experiments, compared measure ten known measures
semantic relatedness: Hirst St-Onge (1998) (HS), Jiang Conrath (1997) (JC), Leacock
et al. (1998) (LC), Lin (1998) (L), Resnik (1995, 1999) (R), Jarmasz Szpakowicz (2003) (JS),
Gabrilovich Markovitch (2007, 2009) (GM), Milne Witten (2008) (WLM), Finkelstein et al.
(2002) (LSA), Hughes Ramage (2007) (HR), Strube Ponzetto (2006, 2007a) (SP).
measure Strube Ponzetto included results version measure
based IS-A relations (Ponzetto & Strube, 2007b) (IS-A SP). measure, including
measure (SR), computed Spearman rank order correlation coefficient
() Pearson product-moment correlation coefficient (r), derived r, since
computation relatedness scores transformed rankings. correlation
coefficients computed based relatedness scores rankings provided humans
three data sets (the relatedness scores create ranking pairs words, based
similarity). measures HS, JC, LC, L R, rankings relatedness scores
word pairs R&G M&C data sets, given work Budanitsky Hirst
(2006). JS measure, r value given work Jarmasz Szpakowicz (2003)
R&G M&C data sets, value given work Gabrilovich
Markovitch (2007). GM measure values given work Gabrilovich
Markovitch (2007). WLM measure values given work Milne Witten
(2008). LSA method value given work Gabrilovich Markovitch (2007),
353-C data set. HR measure values given work Hughes
Ramage (2007). Finally, SP measure r values given work Ponzetto
Strube (2007a), IS-A SP given work Ponzetto Strube (2007b).
22

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Table 3 show values r R&G M&C data sets SR
compared measures. human scores pairs words two data sets
found analysis Budanitsky Hirst (2006). Note M&C data set subset
R&G data set. cases, computation r feasible, due missing
information regarding detailed rankings relatedness scores respective measures.
cases table entry N/A. LSA measure omitted table
r reported literature two data sets. conducted statistical
significance test difference SR correlations respective correlations
compared measures, using Fishers z-transformation (Fisher, 1915). reported number,
symbol indicates difference correlation produced SR respective
measure statistically significant 0.99 confidence level (p < 0.01). symbol indicates
0.95 confidence level (p < 0.05) and, finally, symbol indicates statistical
significance correlations difference 0.90 confidence level (p < 0.10). cases
difference statistically significant confidence levels, indicating
symbol.
Table 4 show values r 353-C data set. reason present results
experiments 353-C data set another table respective results R&B
M&C data sets collection focuses concept semantic relatedness, rather
concept semantic similarity (Gabrilovich & Markovitch, 2007). Relatedness general
concept similarity, argued analysis Budanitsky Hirst (2006). Thus,
argued humans 353-C thought differently scoring, compared case
R&B M&C data sets. detailed human scores 353-C data set made available
collection10 . measures L, JC HS omitted, information available
computing r values. remark regarding 353-C collection, need add fact
cases inter-judge correlations may fall 65%, R&B M&C
data sets inter-judge correlations 0.88 0.95. Again, statistical significance tests
conducted using Fishers z-transformation, regarding difference SR correlations
correlations compared measures. used symbols indicate level
statistical significance previously. regards reported correlations
R&G M&C data sets, shown SR performs well, since majority cases
SR higher correlation compared measures semantic relatedness similarity
category (knowledge-based, corpus-based hybrid). R&G data set SR reports
highest r correlations. M&C data set SR second highest correlation.
HR measure highest correlation, R&G 353-C SR outperforms HR.
differences SR HR statistically significant two examined data sets.
Also, M&C data set SR second r correlation JS reporting highest,
JS outperformed SR R&G 353-C data sets. case M&C data set,
difference SR JS statistically significant, SR outperforms JS R&G
353-C data sets, statistically significant difference reported correlations. Another
important conclusion results, fact IS-A SP measure performs better
SP measure. mainly due fact computation similarity values
data sets, inclusion IS-A relations much reasonable (Ponzetto & Strube, 2007b).
differences results (SP IS-A SP) motivate even SR measure, since
10. http://www.cs.technion.ac.il/gabr/resources/data/wordsim353/

23

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-based

Corpus-based

Hybrid

Method
LC
JS
GM
WLM
LSA
SP
R
HR
SR

353-C
Spearmans Pearsons r
N/A
0.34

0.55
N/A

0.75
N/A
0.69
N/A

0.56
N/A
N/A
0.49
N/A
0.34

0.552
N/A
0.61
0.628

Table 4: Spearmans Pearsons correlations 353 word pairs (353-C) data set. Confidence
levels: =0.90, =0.95, =0.99

take best worlds, i.e., weigh IS-A relations high, fall back relations
necessary.
Regarding 353-C data set, results Table 4 show SR performs well,
top performers Wikipedia-based approaches (Gabrilovich & Markovitch, 2009; Milne
& Witten, 2008). difference statistically significant, note
SR outperforms GM WLM R&G M&C data sets, statistically significant
difference well. Partly, difference performance SR compared GM WLM
explained follows: GM measure considers words context (Gabrilovich & Markovitch,
2009), thus inherently performs word sense disambiguation; contrast, SR takes input pair
words, lacks context, based information existing WordNet, which, especially
several cases 353-C data set, creates disadvantage (e.g., word pair Arafat
Jackson, 11 different entries second word WordNet). holds
WLM measure. Another reason difference performance coverage WordNet.
several cases, one two words 353-C data set comprising pair, exist
WordNet (e.g., football player Maradona). However, expected, shown
experimental analysis Omiotis follows, context considered, proposed semantic
relatedness measure performs better (the reader may wish consult Table 9, subset
R&G data set contains full definitions words, correlations Omiotis
human judgements top found among compared approaches).
visualize performance measure comprehensible manner, present
Figure 5 relatedness values given humans pairs R&G M&C data sets,
increasing order value (left side) respective values pairs produced using SR
(right side). Note x-axis charts begins least related pair terms, according
humans, continues related pair terms. y-axis left chart
respective humans rating pair terms. right figure shows SR pair. closer
look Figure 5 reveals values produced SR (right figure) follow pattern similar
human ratings (left figure).
24

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

HUMAN RATINGS HUMAN RANKINGS - R&G Data Set

SEMANTIC RELATEDNESS HUMAN RANKINGS - R&G Data Set

4
0.9
Semantic Relatedness

Human Rating

3.5
3
2.5
2
1.5
1
0.5

20

30

40

50

0.7
0.6
0.5
0.4
0.3
0.2

correlation human pairs ranking human ratings

correlation human pairs ranking semantic relatedness

0.1

0
10

0.8

60

65

10

20

30

40

50

60

Pair Number

Pair Number

HUMAN RATINGS HUMAN RANKINGS - M&C Data Set

SEMANTIC RELATEDNESS HUMAN RANKINGS - M&C Data Set

65

4
Semantic Relatedness

Human Rating

3.5
3
2.5
2
1.5
1
0.5

correlation human pairs ranking human ratings

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

correlation human pairs ranking semantic relatedness

0
5

10

15

20

25

30

5

Pair Number

10

15

20

25

30

Pair Number

Figure 5: Correlation human ratings Semantic Relatedness measure (SR) Rubenstein Goodenough (R&G) Miller Charles (M&C) data sets.

4.1.2 E VALUATION



YNONYM DENTIFICATION

synonym identification task using TOEFL 80 questions data set ESL 50
questions data set. TOEFL data set compare several methods. specifically, examine: lexicon-based measures Leacock et al. (1998) (LC), Hirst St-Onge
(1998) (HS), Jarmasz Szpakowicz (2003) (JS); corpus-based measures Landauer
Dumais (1997) (LD), Pado Lapata (2007) (PL), Turney (2008b) (T), Terra Clarke (2003)
(TC), Matveeva et al. (2005) (M); hybrid measures Resnik (1995) (R), Lin (1998) (L),
Jiang Conrath (1997) (JC), Turney et al. (2003) (PR); Web-based method RuizCasado et al. (2005) (RC). report results random guessing (RG) performance
average college applicant (H). Table 5 shows results 80 TOEFL questions.
table reports number correct respective percentage given measures. order
test statistical significance differences measures performance, conducted
Fishers Exact Test (Agresti, 1990). previous tables, symbol indicates statistically
significant difference 0.99 confidence level, 0.95 confidence level, 0.90
confidence level. results Table 5 show SR ranks second among reported methods,
best method hybrid PR (Turney et al., 2003). regards comparison
lexicon-based methods, SR reports better results, statistically significant confidence levels
indicated.
similar manner, conducted experiments ESL 50 questions data set,
compare results with: lexicon-based measures Leacock et al. (1998) (LC), Hirst StOnge (1998) (HS), Jarmasz Szpakowicz (2003) (JS); corpus-based measures Turney
(2001) (PMI-IR), Terra Clarke (2003) (TC); hybrid measures Resnik (1995) (R),
25

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Category
Lexicon-Based

Corpus-Based

Hybrid
Web-Based


Method
LC
HS
JS
LD
PL

TC

R
L
JC
PR
RC
RG
H
SR

#Correct Answers
17
62
63
52
58
61
65
69
16
19
20
78
66
20
52
70

Percentage Correct Answers
0.212
0.775
0.787
0.65
0.725
0.762
0.812
0.862
0.2
0.237
0.25
0.975
0.825
0.25
0.65
0.875

Table 5: Number percentage correct answers TOEFL 80 questions test. Confidence
levels: =0.90, =0.95, =0.99

Category
Lexicon-Based
Corpus-Based
Hybrid


Method
LC
HS
JS
PMI-IR
TC
R
L
JC
RG
SR

#Correct Answers
18
31
41
37
40
16
18
18
20
41

Percentage Correct Answers
0.36
0.62
0.82
0.74
0.8
0.32
0.36
0.36
0.25
0.82

Table 6: Number percentage correct answers ESL 50 questions test. Confidence levels:
=0.95, =0.99

Lin (1998) (L), Jiang Conrath (1997) (JC). report results, together random
guessing, Table 6. results Table 6 show SR ranks first, performance
JS data set, outperforming compared corpus-based methods.
26

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Category
Lexicon-Based
Corpus-Based
Hybrid
Web-Based


Method
LC
HS
V
LRA
PMI-IR
R
L
JC
B
RG
S1
S2

NB
UB

#Correct Answers
117
120
161
210
131
124
102
102
150
75
106
114
128
142
196

Percentage Correct Answers
0.313
0.321
0.43
0.561
0.35
0.332
0.273
0.273
0.4
0.2
0.283
0.304
0.342
0.381
0.524

Table 7: Number percentage correct answers 374 Scholastic Aptitude Test (SAT)
questions. Confidence levels: =0.90, =0.95, =0.99

results interesting, since indicate lexicon-based methods promising
synonym identification tasks.
4.1.3 E VALUATION



SAT NALOGY Q UESTIONS

approach choose evaluate SR analogy task use typical benchmark test
set employed related bibliography, namely Scholastic Aptitude Test (SAT).11 comprises
374 words pairs target pair 5 supplementary pairs words. average US college
applicant answered correctly 57 percent questions, machine-based approach
yet surpassed performance average college applicant.
Table 7, present number correct answers respective percentage (recall)
374 SAT questions, following methods: random guessing (RG), Jiang Conrath (1997)
(JC), Lin (1998) (L), Leacock et al. (1998) (LC), Hirst St-Onge (1998) (HS), Resnik (1995)
(R), Bollegala et al. (2008) (B), Veale (2004) (V), PMI-IR (Turney, 2001) LRA (Turney, 2006).
Furthermore, present results s1 (Equation 11), s2 (Equation 12) (Equation 13).
present, before, statistical significance differences performance, conducting
Fishers exact test.
Towards direction combining answers s1 s2 different manner
naive average, report upper bound performance attempt. computed
simply finding union correct answers s1 s2 may provide. reported
table (UB). effort design learning mechanism would learn select
11. Many thanks Peter Turney, providing us standard set experimentation, comprising 374 SAT questions.

27

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

s1 s2 answers SAT question, goal reach upper-bound, designed
implemented simple representation SAT questions training instances.
SAT question, created training instance 6 features: minimum s1 value found
question (among five computed values possible pairs), maximum s1 value,
difference. added features regarding s2 . trained tested
Naive Bayes classifier using ten-fold cross validation 374 SAT questions. results
experiment shown table NB (Naive Bayes). Finally, present top results ever
reported literature specific data set, LRA method Turney (2006).
reported table (LRA).
results presented Table 7 show ranks second among compared lexicon-based
measures first measure Veale (2004) (V). method Bollegala et al. (2008)
(B) achieves higher score SR, needs training SAT questions. point
note LRA method needs almost 8 days process 374 SAT questions (Turney, 2006),
(B) needs around 6 hours (Bollegala et al., 2008), needs less 3 minutes.
Furthermore, fact combining s1 s2 reach 52.4% shows produce
promising results, classifier learns successfully combine them. N B results,
simple attempt construct learner features, shows important boost
performance 4.1%. proper feature engineering task, training SAT questions
potentially yield promising results, gap 38.1% upper bound
52.4% still large. all, results prove lexicon-based relatedness measure
comparable performance state art measures SAT task, smaller
execution time majority methods outperform recall.
4.2 Evaluation Omiotis Measure
order evaluate performance Omiotis measure, performed two experiments
test ability measure capture similarity sentences. first experiment
based data set produced Li et al. (2006). second experiment based paraphrase
recognition task, using Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
4.2.1 E VALUATION



ENTENCE IMILARITY

data set produced Li et al. (2006) comprises 65 sentence pairs (each pair consists two
sentences respective dictionary word definitions R&G 65 word pairs data set).
used dictionary Collins Cobuild dictionary (Sinclair, 2001). sentence pair,
similarity scores provided 32 human participants, ranging 0.0 (the sentences
unrelated meaning), 4.0 (the sentences identical meaning).12 .
65 sentence pairs, Li et al. (2006) decided keep subset 30 sentence pairs,
similarly process applied Miller Charles (1991), order retain sentences whose
human ratings create even distribution across similarity range. Thus, apply Omiotis
subset 65 sentence pairs, described Li et al. (2006). data set,
compare Omiotis STASIS measure semantic similarity, proposed Li et al. (2006),
LSA-based approach described OShea et al. (2008), STS measure proposed Islam
Inkpen (2008). best knowledge, data set used three
12. data set publicly available http://www.docm.mmu.ac.uk/STAFF/J.Oshea/

28

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

previous works. Table 8 present sentence pairs used, respective scores humans,
STASIS, LSA, STS, Omiotis.
Sentence Pair
1.cord:smile
5.autograph:shore
9.asylum:fruit
13.boy:rooster
17.coast:forest
21.boy:sage
25.forest:graveyard
29.bird:woodland
33.hill:woodland
37.magician:oracle
41.oracle:sage
47.furnace:stove
48.magician:wizard
49.hill:mound
50.cord:string
51.glass:tumbler
52.grin:smile
53.serf:slave
54.journey:voyage
55.autograph:signature
56.coast:shore
57.forest:woodland
58.implement:tool
59.cock:rooster
60.boy:lad
61.cushion:pillow
62.cemetery:graveyard
63.automobile:car
64.midday:noon
65.gem: jewel

Human
0.01
0.005
0.005
0.108
0.063
0.043
0.065
0.013
0.145
0.13
0.283
0.348
0.355
0.293
0.47
0.138
0.485
0.483
0.36
0.405
0.588
0.628
0.59
0.863
0.58
0.523
0.773
0.558
0.955
0.653

STASIS
0.329
0.287
0.209
0.53
0.356
0.512
0.546
0.335
0.59
0.438
0.428
0.721
0.641
0.739
0.685
0.649
0.493
0.394
0.517
0.55
0.759
0.7
0.753
1
0.663
0.662
0.729
0.639
0.998
0.831

LSA
0.51
0.53
0.505
0.535
0.575
0.53
0.595
0.505
0.81
0.58
0.575
0.715
0.615
0.54
0.675
0.725
0.695
0.83
0.61
0.7
0.78
0.75
0.83
0.985
0.83
0.63
0.74
0.87
1
0.86

STS
0.06
0.11
0.07
0.16
0.26
0.16
0.33
0.12
0.29
0.20
0.09
0.30
0.34
0.15
0.49
0.28
0.32
0.44
0.41
0.19
0.47
0.26
0.51
0.94
0.60
0.29
0.51
0.52
0.93
0.65

Omiotis
0.1062
0.1048
0.1046
0.3028
0.2988
0.243
0.2995
0.1074
0.4946
0.1085
0.1082
0.2164
0.5295
0.5701
0.5502
0.5206
0.5987
0.4965
0.4255
0.4287
0.9308
0.612
0.7392
0.9982
0.9309
0.3466
0.7343
0.7889
0.9291
0.8194

Table 8: Human, STASIS, LSA, STS Omiotis scores 30 sentence pairs.
Table 9 present results comparison, comprising reporting Spearmans
rank order correlation coefficient Pearsons product moment correlation coefficient r
STASIS, LSA, STS, Omiotis. included results, version Omiotis
take account inter-POS relations (i.e., relations cross parts speech).
version Omiotis indicated table SimpleOmiotis. objective experiment
measure contribution relations cross parts speech computation text-to29

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

STASIS
LSA
STS
Simple Omiotis
Omiotis
Average Participant
Worst Participant
Best Participant

Spearmans
0.8126
0.8714
0.838
0.6889
0.8905
N/A
N/A
N/A

Pearsons r
0.8162
0.8384
0.853
0.7277
0.856
0.825
0.594
0.921

Table 9: Spearmans Pearsons correlations human similarity ratings. Confidence levels:
=0.95, =0.99

text semantic relatedness values, though types relations reported previous
bibliography advantageous (Jarmasz, 2003; Jarmasz & Szpakowicz, 2003), individual
contribution never measured.
show r correlation average participant (mean individuals group;
n = 32, leave-one-out resampling standard deviation 0.072), worst participant (worst participant group; n = 32, leave-one-out resampling) best participant (best participant
group; n = 32, leave-one-out resampling), taken work OShea et al. (2008).
addition, conducted z-test regarding difference Omiotis correlations
compared measures correlations. symbols used previous tables indicate confidence level statistical significance. Note, also, reported correlations (STASIS, LSA,
STS, Omiotis) individually constitute statistically significant positive correlations human scores (r) rankings (). results indicate, Omiotis best correlation, according
r values, compared STASIS, LSA, STS. Furthermore, contribution semantic relations cross parts speech obvious, since difference simple version
Omiotis omits defined Omiotis measure large statistically significant
0.99 confidence level. Overall, results indicate Omiotis applied successfully
computation similarities small text segments, sentences.
4.2.2 E VALUATION



PARAPHRASE R ECOGNITION

order evaluate performance Omiotis measuring semantic relatedness small text segments, conducted additional experiments paraphrase recognition task
using test pairs Microsoft Research Paraphrase Corpus (Dolan et al., 2004).
original data set, containing training test pairs, run experiments 1725 test
pairs text segments, collected news sources Web period
18 months. pair, human subjects determined whether two texts pair
consists paraphrase (direction issue). reported inter-judge agreement
annotators 83%. paraphrase recognition task widely studied past,
since important many natural language applications, question answering (Harabagiu
30

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Category
Baselines
Corpus-based

Lexicon-based

Machine learning-based

Method
Random
VSM Cosine
PMI-IR
LSA
STS
JC
LC
Lesk
L
WP
R
Comb.
Wan et al.
Zhang Patrick
Qiu et al.
Finch et al.
Omiotis

Accuracy
51.3
65.4
69.9
68.4
72.6
69.3
69.5
69.3
69.3
69
69
70.3
75
71.9
72
74.96
69.97

Precision
68.3
71.6
70.2
69.7
74.7
72.2
72.4
72.4
71.6
70.2
69
69.6
77
74.3
72.5
76.58
70.78

Recall
50
79.5
95.2
95.2
89.1
87.1
87
86.6
88.7
92.1
96.4
97.7
90
88.2
93.4
89.8
93.4

F-Measure
57.8
75.3
81
80.5
81.3
79
79
78.9
79.2
80
80.4
81.3
83
80.7
81.6
82.66
80.52

Table 10: Omiotis competitive methods performance Microsoft Research Paraphrase
Corpus (MSR).

& Hickl, 2006), text summarization (Madnani, Zajic, Dorr, Fazil Ayan, & Lin, 2007).
task computed Omiotis sentences every pair marked paraphrases
pairs Omiotis value greater threshold. threshold set 0.2, tuning
training set. used simple approach tuning, namely forward hill-climbing beam
search (Guyon, Gunn, Nikravesh, & Zadeh, 2006).
compare performance Omiotis several methods various categories;
precisely, against: (a) two baseline methods, random selection method marks randomly
pair paraphrase (Random), vector-based similarity measure, using
cosine similarity measure TF-IDF weighting features (VSM Cosine) 13 , (b) corpusbased methods; PMI-IR proposed Turney (2001), LSA-based approach introduced
Mihalcea et al. (2006), STS measure proposed Islam Inkpen (2008), (c) lexiconbased methods; Jiang Conrath (1997) (JC), Leacock et al. (1998) (LC), Lin (1998) (L), Resnik
(1995, 1999) (R), Lesk (1986) (Lesk), Wu Palmer (1994) (WP), metric combines
measures category, proposed Mihalcea et al. (2006) (Comb.), (d) machine-learning
based techniques, constitute state art paraphrase recognition, method
Wan et al. (2006), trains classifier lexical dependency similarity measures,
method Zhang Patrick (2005), build feature vector lexical similarities
sentence pairs (e.g., edit distance, number common words), method Qiu et al.
13. features words used data set.

31

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

(2006), use SVM classifier (Vapnik, 1995) decide whether set features
sentence created parsing semantic role labelling matches respective
set second sentence pair, importance, and, finally, method Finch
et al. (2005), train SVM classifier based machine translation evaluation metrics.
results evaluation shown Table 10. results indicate Omiotis surpasses
lexicon-based methods, matches combined method Mihalcea et al. (2006).
point must mention tuned Omiotis goal maximize F-Measure
test set, cost dropping precision favor recall. type tuning reported FMeasure 81.7, larger F-Measures lexicon-based, corpus-based
two machine learning-based approaches. Even though reported results used different
simpler tuning explained previously, still results indicate Omiotis manages well
paraphrase recognition task produces comparable results state art.
believe used part machine learning-based method, since one best
choices lexicon-based methods paraphrase recognition, constitutes part
plan future work application.

5. Conclusions Future Work
paper presented new measure text semantic relatedness. major strength
measure lies formulation semantic relatedness words. Experimental evaluation, proved measure approximates human understanding semantic relatedness
words better previous related measures. combination path length, nodes depth
edges type single formula allowed us apply semantic relatedness measure different
text-based tasks good performance. specifically, SR measure outperformed overall used data sets state art measures word-to-word tasks Omiotis measure
performed well sentence similarity paraphrase recognition tasks. Although,
results word analogy task satisfactory, since special tuning performed,
confident still place improvement. extensive evaluation SR Omiotis
several applications shows capabilities measures proves applied
several text related tasks. next plans apply relatedness measures applications, text classification clustering, keyword sentence extraction, query
expansion, compare state art techniques field. Finally, improving
supporting infrastructure order facilitate large scale tasks document clustering text
retrieval.

Acknowledgments
Part work done George Tsatsaronis Department Informatics Athens
University Economics Business. would thank Kjetil Nrvag constructive
comments, Ion Androutsopoulos feedback early stage work. would
thank anonymous reviewers detailed feedback.
32

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

References
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, M., & Soroa, A. (2009). study
similarity relatedness using distributional wordnet-based approaches.. Proceedings Human Language Technologies: 2009 Annual Conference North American
Chapter Association Computational Linguistics (NAACL), pp. 1927.
Agirre, E., & Rigau, G. (1995). proposal word sense disambiguation using conceptual distance. Proceedings International Conference Recent Advances Natural Language Processing (RANLP).
Agresti, A. (1990). Categorical Data Analysis. Wiley, Hoboken, NJ.
Aizawa, A. (2003). information-theoretic perspective TF-IDF measures. Information Processing Management, 39(1), 4565.
Banerjee, S., & Pedersen, T. (2003). Extended gloss overlaps measure semantic relatedness.
Proceedings Eighteenth International Joint Conference Artificial Intelligence
(IJCAI), pp. 805810.
Barzilay, R., & Elhadad, M. (1997). Using lexical chains text summarization. Proceedings
ACL 97/EACL 97 Workshop Intelligent Scalable Text Summarization, pp. 1017.
Barzilay, R., Elhadad, M., & McKeown, K. (2002). Inferring strategies sentence ordering
multidocument news summarization. Journal Artificial Intelligence Research, 17, 3555.
Basili, R., Cammisa, M., & Moschitti, A. (2005). semantic kernel exploit linguistic knowledge. Proceedings Advances Artificial Intelligence, Ninth Congress Italian
Association Artificial Intelligence (AI*IA), pp. 290302.
Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions. Proceedings
Fifteenth Turkish Symposium Artificial Intelligence Neural Networks.
Bollegala, D., Matsuo, Y., & Ishizuka, M. (2008). WWW sits sat: Measuring relational similarity web. Proceedings Eighteenth European Conference Artificial
Intelligence (ECAI), pp. 333337.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures lexical semantic relatedness. Computational Linguistics, 32(1), 1347.
Clough, P., & Stevenson, M. (2004). Cross-language information retrieval using EuroWordNet
word sense disambiguation. Proceedings Twenty Sixth European Conference
Information Retrieval (ECIR), pp. 327337.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.
Dolan, W., Quirk, C., & Brockett, C. (2004). Unsupervised construction large paraphrase corpora:
Exploiting massively parallel news sources. Proceedings Twentieth International
Conference Computational Linguistics (COLING).
Fellbaum, C. (1998). WordNet electronic lexical database. MIT Press.
33

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Finch, A., Hwang, Y., & Sumita, E. (2005). Using machine translation evaluation techniques determine sentence-level semantic equivalence. Proceedings 3rd International Workshop Paraphrasing,.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin, E. (2002).
Placing search context: concept revisited. ACM Transactions Information Systems,
20(1), 116131.
Fisher, R. (1915). Frequency distribution values correlation coefficient samples
indefinitely large population. Biometrika, 10, 507521.
Gabrilovich, E., & Markovitch, R. (2007). Computing semantic relatedness using Wikipedia-based
explicit semantic analysis. Proceedings Twentieth International Joint Conference
Artificial Intelligence (IJCAI), pp. 16061611.
Gabrilovich, E., & Markovitch, R. (2009). Wikipedia-based semantic interpretation natural
language processing. Journal Artificial Intelligence Research, 34, 443498.
Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive Science,
7(2), 155170.
Guyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. (2006). Feature Extraction, Foundations
Applications. Springer.
Harabagiu, S., & Hickl, A. (2006). Methods using textual entailment open-domain question
answering.. Proceedings Joint Conference International Committee Computational Linguistics Association Computational Linguistics (COLING-ACL),
pp. 905912.
Hirao, T., Fukusima, T., Okumura, M., Nobata, C., & Nanba, H. (2005). Corpus evaluation
measures multiple document summarization multiple sources. Proceedings
Twentieth International Conference Computational Linguistics (COLING), pp. 535541.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. WordNet: Electronic Lexical Database, chapter 13, pp.
305332 Cambridge. MIT Press.
Hughes, T., & Ramage, D. (2007). Lexical semantic relatedness random graph walks. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP),
pp. 581589.
Ide, N., & Veronis, J. (1998). Word Sense Disambiguation: State Art. Computational
Linguistics, 24(1), 140.
Islam, A., & Inkpen, D. (2008). Semantic text similarity using corpus-based word similarity
string similarity. ACM Transactions Knowledge Discovery Data, 2(2), 125.
Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des alpes et des
jura.. Bulletin del la Societe Vaudoise des Sciences Naturelles, 37, 547579.
34

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Jarmasz, M. (2003). Rogets thesaurus semantic similarity. Masters Thesis, University
Ottawa.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity. Proceedings
International Conference Recent Advances Natural Language Processing (RANLP),
pp. 212219.
Jiang, J., & Conrath, D. (1997). Semantic similarity based corpus statistics lexical taxonomy. Proceedings International Conference Research Computational Linguistics
(ROCLING X), pp. 1933.
Kucera, H., Francis, W., & Caroll, J. (1967). Computational Analysis Present Day American
English. Brown University Press.
Landauer, T., & Dumais, S. (1997). solution Platos problem: latent semantic analysis
theory acquisition, induction, representation knowledge. Psychological Review,
104(2), 211240.
Landauer, T., Foltz, P., & Laham, D. (1998). Introduction latent semantc analysis. Discourse
Processes, 25, 259284.
Leacock, C., Miller, G., & Chodorow, M. (1998). Using corpus statistics WordNet relations
sense identification. Computational Linguistics, 24(1), 147165.
Lesk, M. (1986). Automated sense disambiguation using machine-readable dictionaries:
tell pine cone ice cream cone. Proceedings Fifth Annual International
Conference Systems Documentation (SIGDOC), pp. 2426.
Li, P. (2008). Estimators tail bounds dimension reduction l (0 < 2) using stable
random projections. Proceedings Nineteenth Annual ACM-SIAM Symposium
Discrete Algorithms (SODA), pp. 1019.
Li, Y., McLean, D., Bandar, Z., OShea, J., & Crockett, K. (2006). Sentence similarity based
semantic nets corpus statistics. IEEE Transactions Knowledge Data Engineering,
18(8), 11381150.
Lin, D. (1998). information-theoretic definition similarity. Proceedings Fifteenth
International Conference Machine Learning (ICML), pp. 296304.
Madnani, N., Zajic, D., Dorr, B., Fazil Ayan, N., & Lin, J. (2007). Multiple alternative sentence
compressions automatic text summarization. Proceedings HLT/NAACL Document Understanding Conference (DUC).
Matveeva, I., Levow, G., Farahat, A., & Royer, C. (2005). Generalized latent semantic analysis
term representation. Proceedings International Conference Recent Advances
Natural Language Processing (RANLP).
Mavroeidis, D., Tsatsaronis, G., Vazirgiannis, M., Theobald, M., & Weikum, G. (2005). Word sense
disambiguation exploiting hierarchical thesauri text classification. Proceedings
Ninth European Conference Principles Practice Knowledge Discovery Databases
(PKDD), pp. 181192.
35

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Mihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based knowledge-based measures
text semantic similarity. Proceedings Twenty First Conference Artificial Intelligence (AAAI), pp. 775780.
Mihalcea, R., & Moldovan, D. (1999). method word sense disambiguation unrestricted text.
Proceedings 37th annual meeting Association Computational Linguistics
(ACL), pp. 152158.
Mihalcea, R., Tarau, P., & Figa, E. (2004). PageRank semantic networks application
word sense disambiguation. Proceedings Twentieth International Conference
Computational Linguistics (COLING).
Miller, G., & Charles, W. (1991). Contextual correlates semantic similarity. Language
Cognitive Processes, 6(1), 128.
Milne, D., & Witten, I. (2008). effective, low-cost measure semantic relatedness obtained
Wikipedia links. Proceedings first AAAI Workshop Wikipedia Artificial
Intelligence (WIKIAI).
Morris, J., & Hirst, G. (1991). Lexical cohesion computed thesaural relations indicator
structure text. Computational Linguistics, 17, 2148.
Navigli, R. (2008). structural approach automatic adjudication word sense disagreements. Natural Language Engineering, 14(4), 547573.
OShea, J., Bandar, Z., Crocket, K., & McLean, D. (2008). comparative study two short
text semantic similarity measures. Proceedings Agent Multi-Agent Systems:
Technologies Applications, Second KES International Symposium (KES-AMSTA), pp.
172181.
Pado, S., & Lapata, M. (2007). Dependency-based construction semantic space models. Computational Linguistics, 33(2), 161199.
Palmer, M., Fellbaum, C., & Cotton, S. (2001). English tasks: All-words verb lexical sample.
Proceedings Senseval-2, pp. 2124.
Pasca, M. (2003). Open-domain question answering large text collections. CSLI Studies
Computational Linguistics. CSLI Publications, Distributed University Chicago
Press.
Pasca, M. (2005). Mining paraphrases self-anchored web sentence fragments. Proceedings
Ninth European Conference Principles Practice Knowledge Discovery
Databases (PKDD), pp. 193204.
Patwardhan, S., Banerjee, S., & Pedersen, T. (2003). Using measures semantic relatedness
word sense disambiguation. Proceedings Fourth International Conference Intelligent Text Processing Computational Linguistics (CICLing), pp. 241257.
Patwardhan, S., & Pedersen, T. (2006). Using WordNet based context vectors estimate semantic relatedness concepts. Proceedings EACL 2006 Workshop Making Sense
Sense - Bringing Computational Linguistics Psycholinguistics Together, pp. 18.
36

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Ponzetto, S., & Strube, M. (2007a). Knowledge derived Wikipedia computing semantic
relatedness. Journal Artificial Intelligence Research, 30, 181212.
Ponzetto, S., & Strube, M. (2007b). Deriving large-scale taxonomy Wikipedia. Proceedings Twenty Second Conference Artificial Intelligence (AAAI), pp. 14401445.
Qiu, L., Kan, M., & Chua, T. (2006). Paraphrase recognition via dissimilarity significance classification. Proceedings Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 1826.
Quilian, R. (1969). teachable language comprehender: simulation program theory
language. Communications ACM, 12(8), 459476.
Resnik, P. (1995). Using information content evaluate semantic similarity. Proceedings
Fourteenth International Joint Conference Artificial Intelligence (IJCAI), pp. 448453.
Resnik, P. (1999). Semantic similarity taxonomy: information-based measure application problems ambiguity natural language. Journal Artificial Intelligence
Research, 11, 95130.
Richardson, R., & Smeaton, A. (1995). Using WordNet knowledge-based approach information retrieval. Proceedings BCS-IRSG Colloquium.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates synonymy. Communications
ACM, 8(10), 627633.
Ruiz-Casado, M., Alfonseca, E., & Castells, P. (2005). Using context-window overlapping synonym discovery ontology extension. Proceedings International Conference
Recent Advances Natural Language Processing (RANLP).
Salton, G., Buckley, C., & Yu, C. (1982). evaluation term dependence models information retrieval. Proceedings Fifth Annual International ACM SIGIR Conference
Research Development Information Retrieval, pp. 151173.
Salton, G., & McGill, M. (1983). Introduction Modern Information Retrieval. McGraw-Hill.
Sanderson, M. (1994). Word sense disambiguation information retrieval. Proceedings
Seventeenth Annual International ACM SIGIR Conference Research Development
Information Retrieval, pp. 142151.
Sanderson, M. (2008). Ambiguous queries: Test collections need sense. Proceedings
Thirty First Annual International ACM SIGIR Conference Research Development
Information Retrieval, pp. 499506.
Shinyama, Y., & Sekine, S. (2003). Paraphrase acquisition information extraction. Proceedings ACL 2nd Workshop Paraphrasing: Paraphrase Acquisition Applications,
pp. 6571.
Sinclair, J. (2001). Collins Cobuild English Dictionary Advanced Learners, 3rd edn. Harper
Collins, New York.
37

fiT SATSARONIS , VARLAMIS , & VAZIRGIANNIS

Smeaton, A., Kelledy, F., & ODonnell, R. (1995). TREC-4 experiments Dublin City University:
Thresholding posting lists, query expansion WordNet POS tagging Spanish.
Proceedings Fourth Text REtrieval Conference (TREC).
Snyder, B., & Palmer, M. (2004). English All-words task. Proceedings Senseval-3, pp.
4143.
Song, Y., Han, K., & Rim, H. (2004). term weighting method based lexical chain automatic summarization. Proceedings Fifth International Conference Intelligent Text
Processing Computational Linguistics (CICLing), pp. 636639.
Stokoe, C., Oakes, M., & Tait, J. (2003). Word sense disambiguation information retrieval revisited. Proceedings Twenty Sixth Annual International ACM SIGIR Conference
Research Development Information Retrieval, pp. 159166.
Strube, M., & Ponzetto, S. (2006). WikiRelate! Computing semantic relatedness using Wikipedia.
Proceedings Twenty First Conference Artificial Intelligence (AAAI), pp. 1419
1424.
Sussna, M. (1993). Word sense disambiguation free-text indexing using massive semantic network. Proceedings Second International Conference Information Knowledge
Management (CIKM), pp. 6774.
Terra, E., & Clarke, C. (2003). Frequency estimates statistical word similarity measures.
Proceedings North American Chapter Association Computational Linguistics
- Human Language Technologies Conference (HLT/NAACL)., pp. 244251.
Tsang, V. (2008). Graph Approach Measuring Text Distance. PhD Thesis, University
Toronto.
Tsatsaronis, G., & Panagiotopoulou, V. (2009). generalized vector space model text retrieval
based semantic relatedness. Proceedings 12th Conference European
Chapter Association Computational Linguistics (EACL - Student Research Workshop), pp. 7078.
Tsatsaronis, G., Varlamis, I., Nrvag, K., & Vazirgiannis, M. (2009). Omiotis: thesaurus-based
measure text relatedness. Proceedings European Conference Machine Learning
Principles Practice Knowledge Discovery Databases (ECML-PKDD), pp. 742
745.
Tsatsaronis, G., Varlamis, I., & Vazirgiannis, M. (2008). Word sense disambiguation semantic
networks. Proceedings 11th International Conference Text, Speech Dialogue
(TSD), pp. 219226.
Tsatsaronis, G., Vazirgiannis, M., & Androutsopoulos, I. (2007). Word sense disambiguation
spreading activation networks generated thesauri. Proceedings Twentieth International Joint Conference Artificial Intelligence (IJCAI), pp. 17251730.
Turney, P. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL. Proceedings
Twelfth European Conference Machine Learning (ECML), pp. 491502.
38

fiT EXT R ELATEDNESS BASED W ORD HESAURUS

Turney, P. (2006). Similarity semantic relations. Computational Linguistics, 32(3), 379416.
Turney, P. (2008a). latent relation mapping engine: Algorithm experiments. Journal
Artificial Intelligence Research, 33, 615655.
Turney, P. (2008b). uniform approach analogies, synonyms, antonyms, associations.
Proceedings Twenty Second International Conference Computational Linguistics
(COLING), pp. 905912.
Turney, P., & Littman, M. (2005). Corpus-based learning analogies semantic relations.
Machine Learning, 60(1-3), 251278.
Turney, P., Littman, M., Bigham, J., & Shnayder, V. (2003). Combining independent modules
solve multiple-choice synonym analogy problems. Proceedings International
Conference Recent Advances Natural Language Processing (RANLP), pp. 482489.
van Rijsbergen, C. (1979). Information Retrieval. Butterworth.
Vapnik, V. (1995). nature statistical learning theory. Springer.
Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy. Proceedings Sixteenth European Conference Artificial Intelligence (ECAI), pp. 606
612.
Veronis, J., & Ide, N. (1990). Word sense disambiguation large neural networks extracted
machine readable dictionaries. Proceedings Thirteenth International Conference Computational Linguistics (COLING), pp. 389394.
Voorhees, E. (1993). Using WordNet disambiguate word sense text retrieval. Proceedings
Sixteenth Annual International ACM SIGIR Conference Research Development
Information Retrieval, pp. 171180.
Wan, S., Dras, M., Dale, R., & Paris, C. (2006). Using dependency-based features take parafarce paraphrase. Proceedings Australasian Language Technology Workshop,
pp. 131138.
Wu, Z., & Palmer, M. (1994). Verb semantics lexical selection. Proceedings Thirty
Second Annual Meeting Association Computational Linguistics (ACL), pp. 133
138.
Zhang, Y., & Patrick, J. (2005). Paraphrase identification text canonicalization. Proceedings
Australasian Language Technology Workshop, pp. 160166.

39


