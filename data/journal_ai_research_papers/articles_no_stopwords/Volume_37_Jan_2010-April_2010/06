Journal Artificial Intelligence Research 37 (2010) 247-277

Submitted 08/09; published 03/10

Context-Based Word Acquisition Situated Dialogue
Virtual World

Shaolin Qu
Joyce Y. Chai

qushaoli@cse.msu.edu
jchai@cse.msu.edu

Department Computer Science Engineering
Michigan State University
East Lansing, MI 48824 USA

Abstract
tackle vocabulary problem conversational systems, previous work applied
unsupervised learning approaches co-occurring speech eye gaze interaction
automatically acquire new words. Although approaches shown promise, several
issues related human language behavior human-machine conversation
addressed. First, psycholinguistic studies shown certain temporal regularities
human eye movement language production. regularities potentially
guide acquisition process, incorporated previous unsupervised approaches. Second, conversational systems generally existing knowledge
base domain vocabulary. existing knowledge potentially
help bootstrap constrain acquired new words, incorporated
previous models. Third, eye gaze could serve different functions human-machine conversation. gaze streams may closely coupled speech stream, thus
potentially detrimental word acquisition. Automated recognition closely-coupled
speech-gaze streams based conversation context important. address issues,
developed new approaches incorporate user language behavior, domain knowledge,
conversation context word acquisition. evaluated approaches context situated dialogue virtual world. experimental results shown
incorporating three types contextual information significantly improves word
acquisition performance.

1. Introduction
One major bottleneck human machine conversation robust language interpretation.
encountered vocabulary outside systems knowledge, system tends
fail. conversational interfaces become increasingly important many applications remote interaction robots (Lemon, Gruenstein, & Peters, 2002; Fong
& Nourbakhsh, 2005) automated training education (Traum & Rickel, 2002),
ability automatically acquire new words online conversation becomes essential.
Different traditional telephony-based spoken dialogue systems, conversational interfaces users look graphic display virtual world interacting artificial
agents using natural speech. unique setting provides opportunity automated
vocabulary acquisition. interaction, users visual perception (e.g., indicated
eye gaze) provides potential channel system automatically learn new words.
c
2010
AI Access Foundation. rights reserved.

fiQu & Chai

idea, shown previous work (Yu & Ballard, 2004; Liu, Chai, & Jin, 2007),
parallel data visual perception spoken utterances used unsupervised
approaches automatically identify mappings words visual entities
thus acquire new words. previous approaches provide promising direction,
mainly rely co-occurrence words visual entities completely unsupervised manner. However, human machine conversation, different types
extra information related human language behaviors characteristics conversation
systems. Although extra information potentially provide supervision guide
word acquisition process improve performance, systematically explored
previous work.
First, large body psycholinguistic studies shown eye gaze tightly linked
human language processing. evident language comprehension (Tanenhaus, Spivey-Knowiton, Eberhard, & Sedivy, 1995; Eberhard, Spivey-Knowiton, Sedivy, &
Tanenhaus, 1995; Allopenna, Magnuson, & Tanenhaus, 1998; Dahan & Tanenhaus, 2005;
Spivey, Tanenhaus, Eberhard, & Sedivy, 2002) language production (Meyer, Sleiderink,
& Levelt, 1998; Rayner, 1998; Griffin & Bock, 2000; Bock, Irwin, Davidson, & Leveltb, 2003;
Brown-Schmidt & Tanenhaus, 2006; Griffin, 2001). Specifically human language production, directly relevant automated computer interpretation human language,
studies found significant temporal regularities mentioned objects
corresponding words (Meyer et al., 1998; Rayner, 1998; Griffin & Bock, 2000). object
naming tasks, onset word begins approximately one second speaker
looked corresponding visual referent (Griffin, 2004), gazes longer
difficult name referent retrieve (Meyer et al., 1998; Griffin, 2001).
100-300 ms articulation object name begins, eyes move next object relevant task (Meyer et al., 1998). findings suggest eyes move
mentioned objects corresponding words uttered. Although language
behavior used constrain mapping words visual objects,
incorporated previous approaches.
Second, practical conversational systems always existing knowledge application domains vocabularies. knowledge base acquired development
time: either authored domain experts automatically learned available data.
Although existing knowledge rather limited desired enhanced automatically online (e.g., automated vocabulary acquisition), provides important
information structure domain existing vocabularies,
bootstrap constrain new word acquisition. type domain knowledge
utilized previous approaches.
Third, although psycholinguistic studies provide us sound empirical basis
assuming eye movements predictive speech, gaze behavior interactive
setting much complex. different types eye movements (Kahneman,
1973). naturally occurring eye gaze speech production may serve different
functions, example, engage conversation manage turn taking (Nakano,
Reinstein, Stocky, & Cassell, 2003). Furthermore, interacting graphic display,
user could talking objects previously seen display something
completely unrelated object user looking at. Therefore using speechgaze pairs word acquisition detrimental. type gaze mostly useful
248

fiContext-Based Word Acquisition Situated Dialogue Virtual World

word acquisition kind reflects underlying attention tightly links
content co-occurring speech. Thus, automatically recognizing closely coupled speech
gaze streams online conversation word acquisition important. However,
examined previous work.
address three issues, developed new approaches automatic word
acquisition (1) incorporate findings user language behavior psycholinguistic
studies, particular temporal alignment spoken words eye gaze; (2) utilize
existing domain knowledge, (3) automatically identify closely-coupled speech
gaze streams based conversation context. evaluated approaches context
situated dialogue virtual world. experimental results shown incorporating
three types contextual information significantly improves word acquisition
performance. simulation studies demonstrate effect automatic online
word acquisition improving language understanding human-machine conversation.
following sections, first introduce domain data collection investigation. describe enhanced models word acquisition incorporate
additional contextual information (e.g., language behavior spoken words eye
gaze, domain knowledge, conversation context). Finally, present empirical evaluation
enhanced models demonstrate effect online word acquisition spoken
language understanding human-machine conversation.

2. Related Work
work motivated previous work grounded language acquisition eye gaze
multimodal human-computer interaction.
Grounded language acquisition learn meaning language connecting
language perception world. Language acquisition grounding words
visual perceptions objects studied various language learning systems.
example, given speech paired video images single objects, mutual information audio visual signals used learn words associating acoustic phoneme
sequences visual prototypes (e.g., color, size, shape) objects (Roy & Pentland,
2002; Roy, 2002). Generative models developed learn words associating words
image regions given parallel data pictures description text (Barnard, Duygulu,
de Freitas, Forsyth, Blei, & Jordan, 2003). Given pairs spoken instructions containing
object names corresponding objects, utterance-object joint probability model
used learn object names identifying object name phonemes associating
objects (Taguchi, Iwahashi, Nose, Funakoshi, & Nakano, 2009). Given sequences
utterances paired scene representations, incremental translation model developed learn word meaning associating words semantic representations
referents scene (Fazly, Alishahi, & Stevenson, 2008). addition grounding
individual words, previous work investigated grounding phrases (referring expressions) visual objects semantic decomposition, example using context free
grammar connects linguistic structures underlying visual properties (Gorniak &
Roy, 2004).
Besides visual objects, approaches developed ground words meaning representations events. example, event logic applied ground verbs
249

fiQu & Chai

motion events represented force dynamics encoding support, contact,
attachment relations objects video images (Siskind, 2001). video game
domain, translation model used ground words semantic roles user actions (Fleischman & Roy, 2005). simulated Robocup soccer domain, given textual game
commentaries paired symbolic descriptions game events, approaches based
statistical parsing learning developed ground commentary text game
events (Chen & Mooney, 2008). less restricted data setting, generative models
developed simultaneously segment text utterances map utterances
meaning representations event states (Liang, Jordan, & Klein, 2009). Different
previous work, work, visual perception indicated eye gaze. Eye
gaze, one hand, indicative human attention, provides opportunities link
language perception; hand, implicit subconscious input,
could bring additional challenge word acquisition.
Eye gaze long explored human-computer interaction direct manipulation interfaces pointing device (Jacob, 1991; Wang, 1995; Zhai, Morimoto, & Ihde,
1999). Eye gaze modality multimodal interaction goes beyond function
pointing. different speech eye gaze systems, eye gaze explored
purpose mutual disambiguation (Tanaka, 1999; Zhang, Imamiya, Go, & Mao, 2004),
complement speech channel reference resolution (Campana, Baldridge, Dowding,
Hockey, Remington, & Stone, 2001; Kaur, Termaine, Huang, Wilder, Gacovski, Flippo,
& Mantravadi, 2003; Prasov & Chai, 2008; Byron, Mampilly, Sharma, & Xu, 2005)
speech recognition (Cooke, 2006; Qu & Chai, 2007), managing human-computer
dialogue (Qvarfordt & Zhai, 2005).
Eye gaze explored recently word acquisition. example, Yu Ballard
(2004) proposed embodied multimodal learning interface word acquisition, especially
eye movement. work, given speech paired eye gaze information
video images, translation model applied acquire words associating acoustic
phone sequences visual representations objects actions. work inspired
research mostly related effort here. difference work
work Yu Ballard lies two aspects. First, learning environment
different. Yu Ballard focuses narrative descriptions actions (e.g.,
making sandwich, pouring drinks, etc.) human subjects, focus
interactive conversation. conversation, human participant take speaker
role addressee role. represents new scenario word acquisition based
eye movement may new implications. Second, work Yu Ballard,
IBM Translation Model 1 applied word acquisition. work,
incorporate types information user language behavior, domain knowledge,
conversation context translation models.
previous work, experimented application IBM Translation Model
1 vocabulary acquisition gaze modeling conversation setting (Liu et al.,
2007). reported initial investigation incorporating temporal information
domain knowledge translation models (Qu & Chai, 2008) well automatically
identifying closely coupled speech gaze streams (Qu & Chai, 2009). paper extends
previous work provides comprehensive evaluation incorporating knowledge
interactivity word acquisition much richer application domain. examine
250

fiContext-Based Word Acquisition Situated Dialogue Virtual World

word acquisition affected automated speech recognition effect
online word acquisition language understanding human-machine conversation.

3. Domain Data
facilitate work word acquisition, collected data based situated dialogue
virtual world. data set different data set used previous investigation (Qu & Chai, 2008). difference lies two aspects: 1) dataset
collected mixed initiative human-machine conversation whereas data previous investigation based question answering; 2) user studies work
conducted context situated dialogue, human users immersed
complex virtual world move around virtual environment.
3.1 Domain

Figure 1: Treasure hunting domain
application virtual world treasure hunting. human user immersed
virtual world (i.e., castle) 1 able move around look hidden treasures.
castle contains 115 3D objects. user needs consult remote expert (i.e.,
artificial agent) advice finding treasures. expert knowledge
environment treasures cannot see user castle.
user needs describe expert sees order get advice
go look for.
Figure 1 shows snapshot virtual world. Figure 2 shows excerpt dialog
user expert finding one treasures. focus investigation
word acquisition, conducted wizard-of-oz studies collect data 2 . specifically, set
response templates corresponding different types responses predefined.
1. virtual world developed based Irrlicht game engine http://irrlicht.sourceforge.net/.
2. WOZ studies allow observation user interacting fully functional system,
system functions supplemented hidden human wizard. Users led believe
interacting real system (Klemmer, Sinha, Chen, Landay, Aboobaker, & Wang, 2000).
technology, permits experimentally controlled study human computer interaction behavior
without exposing users unpredicted responses, used widely earlier research speech
interfaces multimodal systems.

251

fiQu & Chai

S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:
U:
S:

Tell see.
see golden tomb sarcophagus.
Describe it.
appears made gold Egyptian decorations it.
else see?
see carving hanging wall appears African.
Go back.
Im back sarcophagus. appears lava surrounding fire.
couldnt understand you.
room sarcophagus surrounded lava fire.
see statue?
try open sarcophagus?
No.
see statue.
Describe.
appears stone statue Egyptian.
else see?
see small vase pottery vase decorated flowers.
treasure near.
try move vase?
Yes.
cant move vase.
see another one?
Yes, theres wooden table pottery jug purple vase it. try move
purple vase?
see statue?
Yes, see stone statue.
see another one?
Yes, see gold black statue Egyptian sitting.
Compare objects.
stone statue bad shape, golden statue appears good shape.
golden crown. sitting wooden chair.
Try moving it.
move head statue see scroll. take scroll?
Describe.
white scroll red seal.
Take it.
took scroll.
Good.

Figure 2: dialog user system finding one treasures

system turn, system automatically determines kind responses (i.e., template)
provided; example, whether ask clarification, provide confirmation,
ask information particular object. decision made based
distribution simulate different types system acceptance rejection. distribution
modified according different needs experiments. Based chosen
template, human wizard serves language understanding component fills
template specific information related user input. filled template
252

fiContext-Based Word Acquisition Situated Dialogue Virtual World

used automatically generate natural language processed Microsoft
Text-to-Speech engine generate speech responses. experiments, users
speech recorded, users eye gaze captured Tobii eye tracker.
3.2 Data Preprocessing
20 users experiments, collected 3709 utterances accompanying gaze fixations.
transcribed collected speech. vocabulary size speech transcript 1082,
among 757 nouns/adjectives. users speech automatically recognized
online Microsoft speech recognizer word error rate (WER) 48.1%
1-best recognition. vocabulary size 1-best speech recognition 3041, among
1643 nouns/adjectives. nouns adjectives transcriptions
recognized 1-best hypotheses automatically identified Stanford Part-of-Speech
Tagger (Toutanova & Manning, 2000; Toutanova, Klein, Manning, & Singer, 2003).
Theres



purple

vase





orange

face
speech str eam

gaze fixation

ts
[table_vase] [vase_purple]

gaze str eam

te

[vase_greek3] [vase_greek3]

[vase_greek3]

[vase_greek3]

[fixated entity]

Figure 3: Accompanying gaze fixations 1-best recognition users utterance
Theres purple vase orange vase. (There two incorrectly recognized
words face 1-best recognition)

collected speech gaze streams automatically paired together system.
time system detected sentence boundary users speech, paired
recognized speech gaze fixations system accumulating since
previously detected sentence boundary. Figure 3 shows stream pair users speech
accompanying gaze fixations. speech stream, spoken word timestamped
speech recognizer. gaze stream, gaze fixation starting timestamp
ts ending timestamp te provided eye tracker. gaze fixation results
fixated entity (3D object). multiple entities fixated one gaze fixation due
overlapping entities, foremost one chosen. gaze stream, neighboring
gaze fixations fixate entity merged.
Given paired speech gaze streams, build set parallel word sequences
gaze fixated entity sequences {(w, e)} task word acquisition. word sequence
w consists nouns adjectives 1-best recognition spoken utterance.
entity sequence e contains entities fixated gaze fixations. parallel
speech gaze streams shown Figure 3, resulting word sequence w = [purple vase
orange face] resulting entity sequence e = [table vase vase purple vase greek3 ].
253

fiQu & Chai

4. Translation Models Word Acquisition
Since working conversational systems users interact visual scene,
consider task word acquisition associating words visual entities
domain. Given parallel speech gaze fixated entities {(w, e)}, formulate word
acquisition translation problem use translation models estimate word-entity
association probabilities p(w|e). words highest association probabilities
chosen acquired words entity e.
4.1 Base Model
Using translation model (Brown, Pietra, Pietra, & Mercer, 1993), word
equally likely aligned entity,
X
l

1
p(w|e) =
p(wj |ei )
(l + 1)m

(1)

j=1 i=0

l lengths entity word sequences respectively. refer
model Model-1.
4.2 Base Model II
Using translation model II (Brown et al., 1993), alignments dependent
word/entity positions word/entity sequence lengths,
p(w|e) =

X
l


p(aj = i|j, m, l)p(wj |ei )

(2)

j=1 i=0

aj = means wj aligned ei . aj = 0, wj aligned
entity (e0 represents null entity). refer model Model-2.
Compared Model-1, Model-2 considers ordering words entities word
acquisition. EM algorithms used estimate probabilities p(w|e) translation
models.

5. Incorporating User Language Behavior Word Acquisition
Model-2, word-entity alignments estimated co-occurring word entity sequences unsupervised way. estimated alignments dependent
words/entities appear word/entity sequences, words gaze
fixated entities actually occur. Motivated findings users move eyes
mentioned object directly speaking word (Griffin & Bock, 2000), make
word-entity alignments dependent temporal relation new model (referred
Model-2t):
X
l

p(w|e) =
pt (aj = i|j, e, w)p(wj |ei )
(3)
j=1 i=0

254

fiContext-Based Word Acquisition Situated Dialogue Virtual World

pt (aj = i|j, e, w) temporal alignment probability computed based temporal distance entity ei word wj .
define temporal distance ei wj

ts (ei ) ts (wj ) te (ei )
0
te (ei ) ts (wj ) ts (wj ) > te (ei )
d(ei , wj ) =
(4)

ts (ei ) ts (wj ) ts (wj ) < ts (ei )
ts (wj ) starting timestamp (ms) word wj , ts (ei ) te (ei ) starting
ending timestamps (ms) gaze fixation entity ei .
alignment word wj entity ei decided temporal distance d(ei , wj ).
Based psycholinguistic finding eye gaze happens spoken word, wj
allowed aligned ei wj happens earlier ei (i.e., d(ei , wj ) > 0). wj
happens earlier ei (i.e., d(ei , wj ) 0), closer are, likely
aligned. Specifically, temporal alignment probability wj ei co-occurring
instance (w, e) computed

0
d(ei , wj ) > 0



exp[ d(ei , wj )]
d(ei , wj ) 0
l
pt (aj = i|j, e, w) =
(5)
X


exp[ d(ei , wj )]


i=0

constant scaling d(ei , wj ).
EM algorithm used estimate probabilities p(w|e) Model-2t.
worthwhile mention that, findings psycholinguistic studies provided specific
offsets terms eye gaze corresponds speech production. example, shows
speakers look object second say it, 100-300 ms
articulation object name begins, eyes move next object relevant
task (Meyer et al., 1998). Since conversation setting study much
complex simple settings psycholinguistic research, found larger variations
offset (Liu et al., 2007) data. Therefore chose use offset
alignment model here.

6. Incorporating Domain Knowledge Word Acquisition
Speech-gaze temporal alignment occurrence statistics sometimes sufficient
associate words entities correctly. example, suppose user says lamp
dresser looking lamp object table object. Due co-occurrence
lamp object, words dresser lamp likely associated lamp
object translation models. result, word dresser likely incorrectly
acquired lamp object. reason, word lamp could acquired
incorrectly table object. solve type association problem, semantic
knowledge domain words helpful. example, knowledge
word lamp semantically related object lamp help system avoid
associating word dresser lamp object. Specifically, solve type wordentity association utilizing domain knowledge present system external
lexical semantic resources.
255

fiQu & Chai

one hand, conversational system domain model, knowledge
representation domain types objects properties relations, task structures, etc. domain model usually acquired development
stage deployment system. domain model provides important resource enable domain reasoning language interpretation (DeVault & Stone, 2003).
hand, available resources domain independent lexical knowledge (e.g., WordNet, see Fellbaum, 1998). idea domain model
linked external lexical resources either manually automatically development
state, external knowledge source used help constrain acquired words.
following sections, first describe domain modeling, define semantic
relatedness word entity based domain modeling WordNet semantic lexicon,
finally describe different ways using semantic relatedness word entity
help word acquisition.
6.1 Domain Modeling
model treasure hunting domain shown Figure 4. domain model contains
domain related semantic concepts. practical conversational systems, domain
modeling typically acquired development stage either manual authoring
domain experts automated learning based annotated data. current work,
properties domain entities represented domain concepts. entity properties include: semantic type, color, size, shape, material. use WordNet synsets
represent domain concepts (i.e., synsets format word#part-of-speech#senseid). sense-id represents specific WordNet sense associated word
representing concept. example, domain concepts SEM PLATE COLOR
entity plate represented synsets plate#n#4 color#n#1 WordNet.
Note link domain concepts WordNet synsets automatically acquired given existing vocabularies. Here, illustrate idea, simplify problem
directly connect domain concepts synsets.
Note domain model, domain concepts specific certain entity,
general concepts certain type entity. Multiple entities type
properties share set domain concepts. Therefore, properties
color size entity general concepts color#n#1 size#n#1
instead specific concepts yellow#a#1 big#a#1, concepts
shared entities type, different colors sizes.
6.2 Semantic Relatedness Word Entity
compute semantic relatedness word w entity e based semantic
similarity w properties e using domain model bridge. Specifically,
semantic relatedness SR(e, w) defined
SR(e, w) = max sim(s(cie ), sj (w))
i,j

(6)

cie i-th property entity e, s(cie ) synset property cie domain model,
sj (w) j-th synset word w defined WordNet, sim(, ) similarity
score two synsets.
256

fiContext-Based Word Acquisition Situated Dialogue Virtual World

n e l

Entities:

Domain
concepts:

pharaoh

plate

SEM_PLATE

COLOR

SEM_PHARAOH

COLOR

SIZE

color#n#1

WordNet
concepts:

pharaoh#n#1

plate#n#4

picture#n#2

size#n#1

Figure 4: Domain model domain concepts represented WordNet synsets

computed similarity score two synsets based path length them.
similarity score inversely proportional number nodes along shortest path
synsets defined WordNet. two synsets same,
maximal similarity score 1. WordNet-Similarity tool (Pedersen, Patwardhan, &
Michelizzi, 2004) used synset similarity computation.
6.3 Word Acquisition Word-Entity Semantic Relatedness
use semantic relatedness word entity help system acquire semantically compatible words entity, therefore improve word acquisition performance.
semantic relatedness applied word acquisition two ways: post-process
learned word-entity association probabilities rescoring semantic relatedness,
directly affect learning word-entity associations constraining alignment
word entity translation models.
6.3.1 Rescoring Semantic Relatedness
acquired word list entity ei , word wj association probability p(wj |ei )
learned translation model. use semantic relatedness SR(ei , wj )
redistribute probability mass wj . new association probability given by:

p(wj |ei )SR(ei , wj )
p0 (wj |ei ) = X
p(wj |ei )SR(ei , wj )
j

257

(7)

fiQu & Chai

6.3.2 Semantic Alignment Constraint Translation Model
used constrain word-entity alignment translation model, semantic relatedness used alone used together speech-gaze temporal information decide
alignment probability word entity (Qu & Chai, 2008).
Using semantic relatedness constrain word-entity alignments Model-2s,

X
l

p(w|e) =
ps (aj = i|j, e, w)p(wj |ei )
(8)
j=1 i=0

ps (aj = i|j, e, w) alignment probability based semantic relatedness,
SR(ei , wj )
ps (aj = i|j, e, w) = X
SR(ei , wj )

(9)



Using semantic relatedness temporal information constrain word-entity alignments Model-2ts,
p(w|e) =

X
l


pts (aj = i|j, e, w)p(wj |ei )

(10)

j=1 i=0

pts (aj = i|j, e, w) alignment probability decided temporal
relation semantic relatedness ei wj ,
ps (aj = i|j, e, w)pt (aj = i|j, e, w)
pts (aj = i|j, e, w) = X
ps (aj = i|j, e, w)pt (aj = i|j, e, w)

(11)



ps (aj = i|j, e, w) semantic alignment probability Equation (9),
pt (aj = i|j, e, w) temporal alignment probability given Equation (5).
EM algorithms used estimate p(w|e) Model-2s Model-2ts.

7. Incorporating Conversation Context Word Acquisition
mentioned earlier, speech-gaze pairs useful word acquisition. speechgaze pair, speech word relates gaze fixated
entities, instance adds noise word acquisition. Therefore, identify
closely coupled speech-gaze pairs use word acquisition.
section, first describe feature extraction based conversation interactivity, describe use logistic regression classifier predict whether speech-gaze
pair closely coupled speech-gaze instance instance least one noun
adjective speech stream referring gaze fixated entity gaze stream.
training classifier speech-gaze prediction, manually labeled instance whether closely coupled speech-gaze instance based speech transcript
gaze fixations.
258

fiContext-Based Word Acquisition Situated Dialogue Virtual World

7.1 Features Extraction
parallel speech-gaze instance, following sets features automatically extracted.
7.1.1 Speech Features (S-Feat)
Let cw count nouns adjectives utterance, ls temporal length
speech. following features extracted speech:
cw count nouns adjectives.
nouns adjectives expected users utterance describing entities.
cw /ls normalized noun/adjective count.
effect speech length ls cw considered.
7.1.2 Gaze Features (G-Feat)
fixated entity ei , let lei fixation temporal length. Note several gaze
fixations may fixated entity, lei total length gaze fixations
fixate entity ei . extract following features gaze stream:
ce count different gaze fixated entities.
Less fixated entities expected user describing entities looking
them.
ce /ls normalized entity count.
effect speech temporal length ls ce considered.
maxi (lei ) maximal fixation length.
least one fixated entitys fixation expected long enough user
describing entities looking them.
mean(lei ) average fixation length.
average gaze fixation length expected longer user describing
entities looking them.
var(lei ) variance fixation lengths.
variance fixation lengths expected smaller user describing entities looking them.
number gaze fixated entities decided users eye gaze,
affected visual scene. Let cse count entities visible
length gaze stream. extract following scene related feature:
ce /cse scene normalized fixated entity count.
effect visual scene ce considered.
259

fiQu & Chai

7.1.3 User Activity Features (UA-Feat)
interacting system, users activity helpful determining
whether users eye gaze tightly linked content speech. following
features extracted users activities:
maximal distance users movements maximal change user position (3D
coordinates) speech length.
user expected move within smaller range looking entities
describing them.
variance users positions
user expected move less frequently looking entities describing
them.
7.1.4 Conversation Context Features (CC-Feat)
talking system (i.e., expert), users language gaze behavior
influenced state conversation. speech-gaze instance, use
previous system response type nominal feature predict whether closely
coupled speech-gaze instance.
treasure hunting domain, eight types system responses two categories:
System-Initiative Responses:
specific-see system asks whether user sees certain entity, e.g., see
another couch?.
nonspecific-see system asks whether user sees anything, e.g., see
anything else?, Tell see.
previous-see system asks whether user previously sees something, e.g.,
previously seen similar object?.
describe system asks user describe detail user sees, e.g.,
Describe it, Tell it.
compare system asks user compare user sees, e.g., Compare
objects.
clarify system asks user make clarification, e.g., understand
that, Please repeat that.
action-request system asks user take action, e.g., Go back, Try moving
it.
User-Initiative Responses:
misc system hands initiative back user without specifying
requirements, e.g., dont know, Yes.
260

fiContext-Based Word Acquisition Situated Dialogue Virtual World

7.2 Logistic Regression Model
Given extracted feature x closely coupled label instance
training set, train ridge logistic regression model (Cessie & Houwelingen, 1992)
predict whether instance closely coupled instance (y = 1) (y = 0).
logistic regression model, probability yi = 1, given feature xi =
(xi1 , xi2 , . . . , xim ), modeled
P

exp(
j=1 j xj )
P
p(yi |xi ) =

1 + exp(
j=1 j xj )
j features weights learned.
log-likelihood l data (X, y)
X
l() =
[yi log p(yi |xi ) + (1 yi ) log(1 p(yi |xi ))]


ridge logistic regression, parameters j estimated maximizing regularized loglikelihood
l () = l() ||||2
ridge parameter introduced achieve stable parameter estimation.
7.3 Evaluation Speech-gaze Identification
Since goal identifying closely coupled speech-gaze instances improve word acquisition interested acquiring nouns adjectives, instances
recognized nouns/adjectives used training logistic regression classifier. Among
2969 instances recognized nouns/adjectives gaze fixations, 2002 (67.4%) instances labeled closely coupled. speech-gaze prediction evaluated 10-fold
cross validation.
Table 1 shows prediction precision recall different sets features used.
seen table, features used, prediction precision goes
recall goes down. important note prediction precision critical
recall word acquisition sufficient amount data available. Noisy instances
gaze link speech content hurt word acquisition since
guide translation models ground words wrong entities. Although
higher recall helpful, effect expected become less co-occurrences
already established.
results show speech features (S-Feat) conversation context features (CCFeat), used alone, improve prediction precision much compared baseline
predicting instances closely coupled precision 67.4%. used alone,
gaze features (G-Feat) user activity features (UA-Feat) two useful feature
sets increasing prediction precision. used together, prediction precision increased. Adding either speech features conversation context features
gaze user activity features (G-Feat + UA-Feat + S-Feat/CC-Feat) increases
prediction precision more. Using four sets features (G-Feat + UA-Feat + S-Feat +
261

fiQu & Chai

Feature sets
Null (baseline)
S-Feat
G-Feat
UA-Feat
CC-Feat
G-Feat + UA-Feat
G-Feat + UA-Feat + S-Feat
G-Feat + UA-Feat + CC-Feat
G-Feat + UA-Feat + S-Feat + CC-Feat

Precision
0.674
0.686
0.707
0.704
0.688
0.719
0.741
0.731
0.748

Recall
1
0.995
0.958
0.942
0.936
0.948
0.908
0.918
0.899

Table 1: Speech-gaze prediction performances different feature sets

CC-Feat) achieves highest prediction precision. McNemar tests shown
significant change compared using G-Feat + UA-Feat + S-Feat (2 = 8.3, p < 0.004)
feature configurations (2 = 45.4 442.7, p < 0.0001). Therefore, choose
use feature sets identify closely coupled speech-gaze instances word acquisition.

8. Evaluation Word Acquisition
practical conversational system starts initial knowledge base (vocabulary).
assume system already one default word entity default vocabulary.
default word entity indicates semantic type entity. example,
word barrel default word entity barrel. Among acquired words,
evaluate new words systems vocabulary. example, word
barrel would excluded candidate words acquired entity barrel.
8.1 Grounding Words Domain Concepts
Based translation models word acquisition (Sections 5 & 6), obtain
word-entity association probability p(w|e). probability provides means ground
words entities. conversational systems, one important goal word acquisition
make system understand semantic meaning new words. Word acquisition
grounding words objects always sufficient identifying semantic meanings.
Suppose word green grounded green chair object, word chair. Although
system aware green word describing green chair, know
word green refers chairs color word chair refers chairs
semantic type. Thus, learning word-entity associations p(w|e) translation
models, need ground words domain concepts entity properties.
Based domain model discussed earlier (Section 6.1), apply WordNet ground
words domain concepts. entity e, based association probabilities p(w|e),
choose n-best words acquired words e. n-best words n highest
association probabilities. word w acquired e, grounded concept ce w
262

fiContext-Based Word Acquisition Situated Dialogue Virtual World

chosen one highest semantic relatedness w:
ce = arg max[max sim(s(cie ), sj (w))]


j

(12)

sim(s(cie ), sj (w)) semantic similarity score defined Equation (6).
evaluate acquired words domain concepts, manually compile set
gold standard words users speech transcripts gaze fixations. gold
standard words words users used refer entities
properties (e.g., color, size, shape) interaction system. automatically acquired words evaluated gold standard words.
8.2 Evaluation Metrics
Following standard evaluation information retrieval, following metrics used
evaluate words acquired domain concepts (i.e., entity properties) {ce }.
Precision

P

ce

Recall

# words correctly acquired ce
P
ce # words acquired ce

P
ce # words correctly acquired ce
P
ce # gold standard words ce

Mean Average Precision (MAP)

MAP =

X PNw P (r) rel(r)
r=1
Ne
e
#e

Ne number gold standard words properties ce entity
e, Nw vocabulary size, P (r) acquisition precision given cut-off rank r,
rel(r) binary function indicating whether word rank r gold-standard
word property ce entity e.
8.3 Evaluation Results
investigate effects speech-gaze temporal information domain semantic knowledge word acquisition, compare word acquisition performances following
models:
Model-1 base model without word-entity alignment (Equation (1)).
Model-1-r Model-1 semantic relatedness rescoring word-entity association.
Model-2 base model II positional alignment (Equation (2)).
Model-2s enhanced model semantic alignment (Equation (8)).
Model-2t enhanced model temporal alignment (Equation (3)).
263

fiQu & Chai

Model-2ts enhanced model temporal semantic alignment (Equation (10)).
Model-2t-r Model-2t semantic relatedness rescoring word-entity association.
8.3.1 Results Using Speech-Gaze Temporal Information
1

0.4

Model-1
Model-2
Model-2t

0.9

0.38
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.34
0.32
0.3
0.28
0.26

0.1
0

0.36

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.24
M-1

M-2

M-2t

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 5: Word acquisition performance speech-gaze temporal information used
Figure 5 shows interpolated precision-recall curves Mean Average Precisions
(MAPs) Model-2t baseline models Model-1 Model-2. shown figure,
Model-2 improve word acquisition compared Model-1. result shows
helpful consider index-based positional alignment word entity
word acquisition. incorporating speech-gaze temporal alignment, Model-2t consistently
achieves higher precisions Model-1 different recalls. terms MAP, Model-2t
significantly increases MAP (t = 3.08, p < 0.002) compared Model-1. means
use speech-gaze temporal alignment improves word acquisition.
8.3.2 Results Using Domain Semantic Relatedness
Figure 6 shows results using domain semantic relatedness word acquisition.
shown figure, compared baseline using extra knowledge (Model-1),
using domain semantic relatedness improves word acquisition matter used
rescore word-entity association (Model-1-r) constrain word-entity alignment (Model2s). Compared Model-1, MAP significantly improved Model-1-r (t = 6.32, p <
0.001) Model-2s (t = 5.36, p < 0.001).
Domain semantic relatedness used together speech-gaze temporal information improve word acquisition. Compared Model-1, MAP significantly increased Model-2ts (t = 5.59, p < 0.001) uses semantic relatedness together temporal information constrain word-entity alignments Model-2t-r (t = 6.01, p < 0.001),
semantic relatedness used rescore word-entity associations learned Model2t.
264

fiContext-Based Word Acquisition Situated Dialogue Virtual World

1

0.4

Model-1
Model-1-r
Model-2s
Model-2ts
Model-2t-r

0.8

Precision

0.7

0.38
Mean Average Precision

0.9

0.6
0.5
0.4
0.3
0.2

0.34
0.32
0.3
0.28
0.26

0.1
0

0.36

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.24
M-1

M-1-r

M-2s

M-2ts

M-2t-r

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 6: Word acquisition performance domain semantic relatedness used
Comparing two ways using semantic relatedness word acquisition, found
rescoring word-entity association semantic relatedness works better. Model-2t-r
achieve higher MAP (t = 2.22, p < 0.015) Model-2ts.
verified using speech-gaze temporal alignment domain semantic
relatedness rescoring works better using either one alone. temporal alignment
semantic relatedness rescoring, Model-2t-r significantly increases MAP compared
Model-1-r (t = 2.75, p < 0.004) semantic relatedness rescoring used
Model-2t (t = 5.38, p < 0.001) temporal alignment used.
8.3.3 Results Based Identified Closely Coupled Speech-Gaze Streams
shown Model-2t-r, speech-gaze temporal alignment domain
semantic relatedness rescoring incorporated, achieves best word acquisition performance. Therefore, Model-2t-r used evaluate word acquisition based
identified closely coupled speech-gaze data. Since Model-2t-r requires linking domain models external knowledge source (e.g., WordNet) may available
applications, evaluate effect identification closely coupled speech-gaze
streams word acquisition Model-2t, speech-gaze temporal alignment
incorporated.
evaluate effect automatic identification closely coupled speech-gaze instances
word acquisition 10-fold cross validation. fold, 10% data set
used train logistic regression classifier predicting closely coupled speechgaze instances, instances, predicted closely coupled instances, true (manually
labeled) closely coupled instances 90% data set used word
acquisition respectively. Figures 7 & 8 show averaged interpolated precision-recall curves
MAPs achieved Model-2t Model-2t-r using instances (all ), predicted
closely coupled instances (predicted ), true closely coupled instances (true).
words acquired Model-2t, shown Figure 7, using predicted closely
coupled instances achieves better performance using instances. MAP significantly increased (t = 2.69, p < 0.005) acquiring words predicted closely coupled
265

fiQu & Chai

1

0.42


predicted
true

0.9

0.4
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.38

0.36

0.34

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.32

1



predicted

true

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 7: Word acquisition performance Model-2t different data set

1

0.42


predicted
true

0.9

0.4
Mean Average Precision

0.8

Precision

0.7
0.6
0.5
0.4
0.3
0.2

0.38

0.36

0.34

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.32


predicted

true

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 8: Word acquisition performance Model-2t-r different data set

instances. result shows identification closely coupled speech-gaze instances
helps word acquisition. true closely coupled speech-gaze instances used
word acquisition, acquisition performance improved. means better
identification closely coupled speech-gaze instances lead better word acquisition
performance.
words acquired Model-2t-r, shown Figure 8, using predicted closely
coupled instances improves acquisition performance compared using instances.
acquiring words predicted closely coupled speech-gaze instances, MAP increased
(t = 1.81, p < 0.037) although improvement less significant one Model2t.
266

fiContext-Based Word Acquisition Situated Dialogue Virtual World

1

0.56

1-best
transcript
predicted 1-best
predicted transcript

0.8

0.52
Mean Average Precision

0.9

Precision

0.7
0.6
0.5
0.4
0.3
0.2

1-best
transcript

0.48

0.44

0.4

0.36

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0.32

1



predicted

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 9: Word acquisition performance Model-2t speech recognition transcript
1

0.56

1-best
transcript
predicted 1-best
predicted transcript

0.8

0.52
Mean Average Precision

0.9

Precision

0.7
0.6
0.5
0.4
0.3
0.2

1-best
transcript

0.48

0.44

0.4

0.36

0.1
0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.32


predicted

Recall

(a) Precision vs Recall

(b) Mean Average Precision

Figure 10: Word acquisition performance Model-2t-r speech recognition transcript

8.3.4 Comparison Results Based Speech Recognition Transcript
show effect speech recognition quality word acquisition, compare
acquisition performances based speech transcript 1-best recognition. word
acquisition based speech transcript, word sequence parallel speech-gaze data
set contains nouns adjectives speech transcript. Accordingly, speech feature
used coupled speech-gaze identification extracted speech transcript.
Figures 9 & 10 show word acquisition performances Model-2t Model-2t-r using
instances using predicted coupled instances based speech transcript
1-best recognition respectively. shown figures, quality speech recognition
critical word acquisition performance. expected, word acquisition performance based
speech transcript much better recognized speech.
267

fiQu & Chai

9. Examples
Table 2 shows 10-best candidate words acquired entity couch Model-1, Model2t, Model-2t-r based speech-gaze instances Model-2t-r based predicted
closely coupled instances. probabilities candidate words given
table. Across models, although four words (shown bold font)
acquired model, ranking acquired words achieves best Model-2t-r
based predicted closely coupled instances.
Table 3 shows another example 10-best candidate words acquired entity
stool four different models. Model-1 acquires four correct words 10-best list.
Although Model-2t acquires four correct words 10-best list, rankings
words higher. speech-gaze temporal alignment domain semantic
relatedness rescoring, Model-2t-r acquires seven correct words 10-best list.
rankings correct words improved. Compared using instances
Model-2t-r, although using predicted coupled instances Model-2t-r results
seven correct words ranks 10-best list, probabilities
correctly acquired words higher. means results based predicted
coupled instances confident.
Model
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Rank 6
Rank 7
Rank 8
Rank 9
Rank 10

Model-1
couch(0.1105)
bedroom(0.1047)
chair(0.1004)
bad(0.0936)
room(0.0539)
wooden(0.0354)
bench(0.0319)
small(0.0289)
yellow(0.0274)
couple(0.0270)

Model-2t
couch(0.1224)
chair(0.0798)
bed(0.0593)
small(0.0536)
room(0.0528)
bad(0.0489)
yellow(0.0333)
bench(0.0332)
lot(0.0331)
wooden(0.0226)

Model-2t-r
couch(0.4743)
chair(0.1668)
bench(0.0949)
bed(0.0311)
small(0.0235)
bad(0.0226)
room(0.0174)
lot(0.0151)
yellow(0.0107)
couple(0.0085)

Model-2t-r(predicted)
couch(0.4667)
chair(0.1557)
bench(0.11129)
bed(0.0368)
small(0.0278)
bad(0.0265)
room(0.0137)
yellow(0.0127)
couple(0.0101)
lot(0.0090)

Table 2: N-best candidate words acquired entity couch different models

Model
Rank 1
Rank 2
Rank 3
Rank 4
Rank 5
Rank 6
Rank 7
Rank 8
Rank 9
Rank 10

Model-1
plant(0.0793)
room(0.0508)
little(0.0471)
flower(0.0424)
stairs(0.0320)
call(0.0319)
square(0.0302)
footstool(0.0301)
brown(0.0300)
short(0.0294)

Model-2t
plant(0.0592)
room(0.0440)
little(0.0410)
flower(0.0409)
square(0.0408)
small(0.0403)
next(0.0308)
stool(0.0307)
brown(0.0300)
stairs(0.0226)

Model-2t-r
stool(0.1457)
little(0.1435)
small(0.1412)
footstool(0.0573)
ottoman(0.0572)
ground(0.0275)
media(0.0263)
chair(0.0257)
plant(0.0253)
square(0.0234)

Model-2t-r(predicted)
stool(0.1532)
little(0.1509)
small(0.1490)
footstool(0.0602)
ottoman(0.0601)
ground(0.0289)
media(0.0276)
chair(0.0272)
plant(0.0270)
square(0.0247)

Table 3: N-best candidate words acquired entity stool different models

268

fiContext-Based Word Acquisition Situated Dialogue Virtual World

10. Effect Online Word Acquisition Language Understanding
One important goal word acquisition use acquired new words help language
understanding subsequent conversation. demonstrate effect online word acquisition language understanding, conduct simulation studies based collected
data. simulations, system starts initial knowledge base vocabulary
words associated domain concepts. system continuously enhances knowledge
base acquiring words users Model-2t-r incorporates speech-gaze
temporal information domain semantic relatedness. enhanced knowledge base
used understand language new users.
evaluate language understanding performance concept identification rate (CIR):
CIR =

#correctly identified concepts 1-best speech recognition
#concepts speech transcript

simulate process online word acquisition evaluate effect language
understanding two situations: 1) system starts training data
small initial vocabulary, 2) system starts training data.
10.1 Simulation 1: System Starts Training Data
build conversational systems, one approach domain experts provide domain vocabulary system design time. first simulation follows practice.
system provided default vocabulary start without training data. default
vocabulary contains one seed word domain concept.
Using collected data 20 users, simulation process goes following
steps:
user index = 1, 2, . . . , 20:
Evaluate CIR i-th users utterances (1-best speech recognition)
current system vocabulary.
Acquire words instances (with 1-best speech recognition) users
1 i.
Among 10-best acquired words, add verified new words system vocabulary.
process, language understanding performance individual user
depends users language well users position user sequence.
reduce effect user ordering language understanding performance,
simulation process repeated 1000 times randomly ordered users. average
CIRs simulations shown Figure 11.
Figure 11 shows CIRs system static knowledge base (vocabulary). curve drawn way curve dynamic knowledge
base, except without word acquisition random simulation processes. see
figure, system doest word acquisition capability, language understanding performance change users communicated system.
capability automatic word acquisition, systems language understanding
performance becomes better users talked system.
269

fiQu & Chai

0.7
static knowledge base
dynamic knowledge base

0.65
0.6

CIR

0.55
0.5
0.45
0.4
0.35
0.3
0.25

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20
User Index

Figure 11: CIR user language achieved system starting training data
10.2 Simulation 2: System Starts Training Data
Many conversational systems use real user data derive domain vocabulary. follow
practice, second simulation provides system training data. training
data serves two purposes: 1) build initial vocabulary system; 2) train classifier
predict closely coupled speech-gaze instances new users data.
Using collected data 20 users, simulation process goes following
steps:
Using first users data training data, acquire words training instances
(with speech transcript); add verified 10-best words systems vocabulary
seed words; build classifier training data prediction closely coupled
speech-gaze instances.
Evaluate effect incremental word acquisition CIR remaining (20-m)
users data. user index = 1, 2, . . . , (20-m):
Evaluate CIR i-th users utterances (1-best speech recognition).
Predict closely coupled speech-gaze instances i-th users data.
Acquire words training users true coupled instances (with speech
transcript) predicted coupled instances (with 1-best speech recognition)
users 1 i.
Among 10-best acquired words, add verified new words system vocabulary.
simulation process repeated 1000 times randomly ordered users
reduce effect user ordering language understanding performance. Figure 12
shows averaged language understanding performance random simulations.
language understanding performance system static knowledge base
shown Figure 12. curve drawn random simulations without
steps word acquisition. observe general trend figure that, word
acquisition, systems language understanding becomes better users
270

fiContext-Based Word Acquisition Situated Dialogue Virtual World

0.6
static knowledge base
dynamic knowledge base
0.59

CIR

0.58

0.57

0.56

0.55

1

2

3

4

5

6

7

8

9

10

User Index

Figure 12: CIR user language achieved system starting training data 10
users

communicated system. Without word acquisition capability, systems language
understanding performance increase users conversed
system.
simulations show automatic vocabulary acquisition beneficial systems
language understanding performance training data available. training data
available, vocabulary acquisition could important beneficial robust
language understanding.
10.3 Effect Speech Recognition Online Word Acquisition
Language Understanding
simulation results Figures 11 & 12 based 1-best recognized speech hypotheses relatively high WER (48.1%). better speech recognition, system
better concept identification performance. show effect speech recognition
quality online word acquisition language understanding, perform Simulation 1 Simulation 2 based speech transcript. simulation processes
ones based 1-best speech recognition except word acquisition based
speech transcript CIR evaluated speech transcript new simulations.
Figure 13 shows CIR curves based speech transcript online conversation.
word acquisition, systems language understanding becomes better
users communicated system. consistent CIR curves based
1-best speech recognition. However, CIRs based speech transcript much higher
CIRs based 1-best speech recognition, verifies speech recognition
quality critical language understanding performance.

11. Discussion Future Work
experimental results shown incorporating extra information improves word
acquisition compared completely unsupervised approaches. However, current ap271

fiQu & Chai

1

0.95
static knowledge base
dynamic knowledge base

static knowledge base
dynamic knowledge base

0.8

0.93
CIR

0.94

CIR

0.9

0.7

0.92

0.6

0.91

0.5

1

2

3

4

5

6

7

8

0.9

9 10 11 12 13 14 15 16 17 18 19 20
User Index

(a) Simulation 1: training data

1

2

3

4

5

6

7

8

9

10

User Index

(b) Simulation 2: training data 10 users

Figure 13: CIR user language (transcript) achieved system online conversation

proaches several limitations. First, incorporation domain knowledge
semantic relatedness based WordNet restrict acquired words appear
WordNet. certainly desirable. limitation readily addressed
changing way word probability distribution tailored semantic relatedness
(in Section 6.3.1 Section 6.3.2). example, one simple way keep probability
mass words WordNet tailor distribution words
occur WordNet based semantic relatedness object.
Second, current approach, acquired words limited words
recognized speech recognizer. shown Section 8.3.4, speech recognition
performance rather poor experiments. partly due lack language
models specifically trained domain. Approaches improve speech recognition,
example, based referential semantic language model described (Schuler, Wu, &
Schwartz, 2009) potentially improve acquisition performance. Furthermore, set
acquired words bounded vocabulary speech recognizer. new words
dictionary acquired. break barrier, inspired previous
work (Yu & Ballard, 2004; Taguchi et al., 2009), currently extending approach
incorporate grounding acoustic phoneme sequences domain concepts.
Another limitation current approaches incapable acquiring
multiword expressions. map single words domain concepts. However,
observe multiword expressions (e.g., Rubiks cube) data. examine
issue future work incorporating linguistic knowledge modeling
fertility entities, example, IBM Model 3 4.
simplicity current models limits word acquisition performance.
example, alignment model based temporal information directly incorporates findings
psycholinguistic studies. studies generally conducted much simpler
settings without interaction. recent work Fang, Chai, Ferreira (2009) shown
correlations intensity gaze fixations objects denoted linguistic centers
272

fiContext-Based Word Acquisition Situated Dialogue Virtual World

(e.g., forward-looking centers based centering theory, Grosz, Joshi, & Weinstein, 1995).
plan incorporate results improve alignment modeling future.
improve performance, another interesting direction take consideration interactive nature conversation, example, combining dialog management
solicit user feedback acquired words. However, important identify
strategies balance trade explicit feedback solicitation (and thus lengthening interaction) quality acquired words. Reinforcement learning
potential approach address problem.

12. Conclusions
Motivated psycholinguistic findings, investigate use eye gaze automatic word acquisition multimodal conversational systems. paper presents several
enhanced models incorporate user language behavior, domain knowledge, conversation context word acquisition. experiments shown enhanced
models significantly improve word acquisition performance.
Recent advancement eye tracking technology made available non-intrusive eye
tracking devices tolerate head motion provide high tracking quality. Integrating eye tracking conversational interfaces longer beyond reach. believe
incorporating eye gaze automatic word acquisition provides another potential approach
improve robustness human-machine conversation.

Acknowledgments
work supported IIS-0347548 IIS-0535112 National Science Foundation. would thank anonymous reviewers valuable comments
suggestions.

References
Allopenna, P. D., Magnuson, J. S., & Tanenhaus, M. K. (1998). Tracking time course
spoken word recognition using eye movements: Evidence continuous mapping
models. Journal Memory & Language, 38, 419439.
Barnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., & Jordan, M. (2003). Matching words pictures. Journal Machine Learning Research, 3, 11071135.
Bock, K., Irwin, D. E., Davidson, D. J., & Leveltb, W. (2003). Minding clock. Journal
Memory Language, 48, 653685.
Brown, P. F., Pietra, S. D., Pietra, V. J. D., & Mercer, R. L. (1993). mathematic
statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Brown-Schmidt, S., & Tanenhaus, M. K. (2006). Watching eyes talking size:
investigation message formulation utterance planning. Journal Memory
Language, 54, 592609.
273

fiQu & Chai

Byron, D., Mampilly, T., Sharma, V., & Xu, T. (2005). Utilizing visual attention
cross-modal coreference interpretation. Proceedings Fifth International
Interdisciplinary Conference Modeling Using Context (CONTEXT-05), pp.
8396.
Campana, E., Baldridge, J., Dowding, J., Hockey, B., Remington, R., & Stone, L. (2001).
Using eye movements determine referents spoken dialogue system. Proceedings Workshop Perceptive User Interface.
Cessie, S. L., & Houwelingen, J. V. (1992). Ridge estimators logistic regression. Applied
Statistics, 41 (1), 191201.
Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language
acquisition. Proceedings 25th International Conference Machine Learning
(ICML).
Cooke, N. J. (2006). Gaze-Contigent Automatic Speech Recognition. Ph.D. thesis, University
Birminham.
Dahan, D., & Tanenhaus, M. K. (2005). Looking rope looking snake:
Conceptually mediated eye movements spoken-word recognition. Psychonomic
Bulletin & Review, 12 (3), 453459.
DeVault, D., & Stone, M. (2003). Domain inference incremental interpretation.
Proceedings ICoS.
Eberhard, K., Spivey-Knowiton, M., Sedivy, J., & Tanenhaus, M. (1995). Eye movements
window real-time spoken language comprehension natural contexts. Journal
Psycholinguistic Research, 24, 409436.
Fang, R., Chai, J. Y., & Ferreira, F. (2009). linguistic attention gaze fixations
inmultimodal conversational interfaces. Proceedings International Conference
Multimodal Interfaces (ICMI), pp. 143150.
Fazly, A., Alishahi, A., & Stevenson, S. (2008). probabilistic incremental model word
learning presence referential uncertainty. Proceedings 30th Annual
Conference Cognitive Science Society.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Fleischman, M., & Roy, D. (2005). Intentional context situated language learning.
Proceedings 9th Conference Computational Natural Language Learning
(CoNLL).
Fong, T. W., & Nourbakhsh, I. (2005). Interaction challenges human-robot space exploration. Interactions, 12 (2), 4245.
Gorniak, P., & Roy, D. (2004). Grounded semantic composition visual scenes. Journal
Artificial Intelligence Research, 21, 429470.
Griffin, Z., & Bock, K. (2000). eyes say speaking. Psychological Science,
11, 274279.
Griffin, Z. M. (2001). Gaze durations speech reflect word selection phonological
encoding. Cognition, 82, B1B14.
274

fiContext-Based Word Acquisition Situated Dialogue Virtual World

Griffin, Z. M. (2004). look? Reasons eye movements related language production.
Henderson, J., & Ferreira, F. (Eds.), Interface Language, Vision, Action:
Eye Movements Visual World, pp. 213248. Taylor Francis.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: framework modeling
local coherence discourse. Computational Linguistics, 21 (2), 203226.
Jacob, R. J. K. (1991). use eye movements human-computer interaction techniques:
look get. ACM Transactions Information Systems, 9 (3),
152169.
Kahneman, D. (1973). Attention Effort. Prentice-Hall, Inc., Englewood Cliffs.
Kaur, M., Termaine, M., Huang, N., Wilder, J., Gacovski, Z., Flippo, F., & Mantravadi,
C. S. (2003). it? event synchronization gaze-speech input systems.
Proceedings International Conference Multimodal Interfaces (ICMI).
Klemmer, S., Sinha, A., Chen, J., Landay, J., Aboobaker, N., & Wang, A. (2000). SUEDE:
wizard oz prototyping tool speech user interfaces. Proceedings ACM
Symposium User Interface Software Technology, pp. 110.
Lemon, O., Gruenstein, A., & Peters, S. (2002). Collaborative activities multitasking
dialogue systems. Traitement Automatique des Langues, 43 (2), 131154.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences
less supervision. Proceedings 47th Annual Meeting Association
Computational Linguistics (ACL).
Liu, Y., Chai, J., & Jin, R. (2007). Automated vocabulary acquisition interpretation
multimodal conversational systems. Proceedings 45th Annual Meeting
Association Computational Linguistics (ACL).
Meyer, A., Sleiderink, A., & Levelt, W. (1998). Viewing naming objects: eye movements
noun phrase production. Cognition, 66 (22), 2533.
Nakano, Y., Reinstein, G., Stocky, T., & Cassell, J. (2003). Towards model face-to-face
grounding. Proceedings Annual Meeting Association Computational
Linguistics (ACL).
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity - measuring
relatedness concepts. Proceedings Nineteenth National Conference
Artificial Intelligence (AAAI).
Prasov, Z., & Chai, J. Y. (2008). Whats gaze? role eye-gaze reference resolution
multimodal conversational interfaces. Proceedings ACM 12th International
Conference Intelligent User interfaces (IUI).
Qu, S., & Chai, J. Y. (2007). exploration eye gaze spoken language processing
multimodal conversational interfaces. Proceedings Human Language Technology Conference North American Chapter Association Computational
Linguistics (HLT-NAACL), pp. 284291.
Qu, S., & Chai, J. Y. (2008). Incorporating temporal semantic information eye gaze
automatic word acquisition multimodal conversational systems. Proceedings
275

fiQu & Chai

Conference Empirical Methods Natural Language Processing (EMNLP),
pp. 244253.
Qu, S., & Chai, J. Y. (2009). role interactivity human-machine conversation
automatic word acquisition. Proceedings 10th Annual Meeting Special
Interest Group Discourse Dialogue (SIGDIAL), pp. 188195.
Qvarfordt, P., & Zhai, S. (2005). Conversing user based eye-gaze patterns.
Proceedings Conference Human Factors Computing Systems (CHI).
Rayner, K. (1998). Eye movements reading information processing - 20 years
research. Psychological Bulletin, 124 (3), 372422.
Roy, D. (2002). Learning visually-grounded words syntax scene description task.
Computer Speech Language, 16 (3), 353385.
Roy, D., & Pentland, A. (2002). Learning words sights sounds, computational
model. Cognitive Science, 26 (1), 113146.
Schuler, W., Wu, S., & Schwartz, L. (2009). framework fast incremental interpretation
speech decoding. Computational Linguistics, 35 (3), 313343.
Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception using
force dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.
Spivey, M. J., Tanenhaus, M. K., Eberhard, K. M., & Sedivy, J. C. (2002). Eye movements
spoken language comprehension: Effects visual context syntactic ambiguity
resolution. Cognitive Psychology, 45, 447481.
Taguchi, R., Iwahashi, N., Nose, T., Funakoshi, K., & Nakano, M. (2009). Learning lexicons spoken utterances based statistical model selection. Proceedings
Interspeech.
Tanaka, K. (1999). robust selection system using real-time multi-modal user-agent interactions. Proceedings International Conference Intelligent User Interfaces
(IUI).
Tanenhaus, M., Spivey-Knowiton, M., Eberhard, K., & Sedivy, J. (1995). Integration
visual linguistic information spoken language comprehension. Science, 268,
16321634.
Toutanova, K., Klein, D., Manning, C., & Singer, Y. (2003). Feature-Rich Part-of-Speech
tagging cyclic dependency network. Proceedings Human Language
Technology Conference North American Chapter Association Computational Linguistics (HLT-NAACL), pp. 252259.
Toutanova, K., & Manning, C. D. (2000). Enriching knowledge sources used maximum entropy part-of-speech tagger. Proceedings Joint SIGDAT Conference Empirical Methods Natural Language Processing Large Corpora
(EMNLP/VLC), pp. 6370.
Traum, D., & Rickel, J. (2002). Embodied agents multiparty dialogue immersive
virtual worlds. Proceedings 1st international joint conference Autonomous
Agents Multi-Agent Systems.
276

fiContext-Based Word Acquisition Situated Dialogue Virtual World

Wang, J. (1995). Integration eye-gaze, voice manual response multimodal user
interfaces. Proceedings IEEE International Conference Systems, Man
Cybernetics, pp. 39383942.
Yu, C., & Ballard, D. (2004). multimodal learning interface grounding spoken language
sensory perceptions. ACM Transactions Applied Perceptions, 1 (1), 5780.
Zhai, S., Morimoto, C., & Ihde, S. (1999). Manual gaze input cascaded (MAGIC)
pointing. Proceedings Conference Human Factors Computing Systems
(CHI), pp. 246253.
Zhang, Q., Imamiya, A., Go, K., & Mao, X. (2004). Overriding errors speech gaze
multimodal architecture. Proceedings International Conference Intelligent
User Interfaces (IUI).

277


