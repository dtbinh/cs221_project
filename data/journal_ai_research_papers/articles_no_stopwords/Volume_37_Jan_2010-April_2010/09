Journal Artificial Intelligence Research 37 (2010) 397-435

Submitted 11/09; published 03/10

Training Multilingual Sportscaster:
Using Perceptual Context Learn Language
David L. Chen
Joohyun Kim
Raymond J. Mooney

DLCC @ CS . UTEXAS . EDU
SCIMITAR @ CS . UTEXAS . EDU
MOONEY @ CS . UTEXAS . EDU

Department Computer Science
University Texas Austin
1 University Station C0500, Austin TX 78712, USA

Abstract
present novel framework learning interpret generate language using perceptual context supervision. demonstrate capabilities developing system learns
sportscast simulated robot soccer games English Korean without language-specific
prior knowledge. Training employs ambiguous supervision consisting stream descriptive textual comments sequence events extracted simulation trace. system
simultaneously establishes correspondences individual comments events
describe building translation model supports parsing generation.
present novel algorithm learning events worth describing. Human evaluations
generated commentaries indicate reasonable quality cases even par
produced humans limited domain.

1. Introduction
current natural language processing (NLP) systems built using statistical learning algorithms trained large annotated corpora. However, annotating sentences requisite parse
trees (Marcus, Santorini, & Marcinkiewicz, 1993), word senses (Ide & Jeronis, 1998) semantic
roles (Kingsbury, Palmer, & Marcus, 2002) difficult expensive undertaking. contrast,
children acquire language exposure linguistic input context rich, relevant,
perceptual environment. Also, connecting words phrases objects events world,
semantics language grounded perceptual experience (Harnad, 1990). Ideally, machine
learning system would able acquire language similar manner without explicit human supervision. step direction, present system describe events simulated
soccer game learning sample language commentaries paired traces simulated
activity without language-specific prior knowledge. screenshot system generated
commentary shown Figure 1.
fair amount research grounded language learning (Roy, 2002;
Bailey, Feldman, Narayanan, & Lakoff, 1997; Barnard, Duygulu, Forsyth, de Freitas, Blei, & Jordan, 2003; Yu & Ballard, 2004; Gold & Scassellati, 2007), focus dealing
raw perceptual data rather language issues. Many systems aimed learn meanings words phrases rather interpreting entire sentences. recent work dealt
fairly complex language data (Liang, Jordan, & Klein, 2009; Branavan, Chen, Zettlemoyer, &
c
2010
AI Access Foundation. rights reserved.

fiC HEN , K IM , & OONEY

Figure 1: Screenshot commentator system
Barzilay, 2009) address three problems alignment, semantic parsing, natural
language generation. contrast, work investigates build complete language learning
system using parallel data perceptual context. study problem simulated environment retains many important properties dynamic world multiple agents
actions avoiding many complexities robotics computer vision. Specifically,
use RoboCup simulator (Chen, Foroughi, Heintz, Kapetanakis, Kostiadis, Kummeneje, Noda,
Obst, Riley, Steffens, Wang, & Yin, 2003) provides fairly detailed physical simulation
robot soccer. several groups constructed RoboCup commentator systems (Andre,
Binsted, Tanaka-Ishii, Luke, Herzog, & Rist, 2000) provide textual natural-language (NL)
transcript simulated game, systems use manually-developed templates based
learning.
commentator system learns semantically interpret generate language RoboCup
soccer domain observing on-going commentary game paired evolving simulator state. exploiting existing techniques abstracting symbolic description activity
field detailed states physical simulator (Andre et al., 2000), obtain pairing
natural language symbolic description perceptual context uttered.
However, training data highly ambiguous comment usually co-occurs several events game. integrate enhance existing methods learning semantic parsers
NL generators (Kate & Mooney, 2007; Wong & Mooney, 2007) order learn understand
generate language ambiguous training data. develop system that,
ambiguous training data, learns events worth describing, perform
strategic generation, is, deciding say well say (tactical generation). 1
1. conciseness, use terminology early work generation (e.g., McKeown, 1985). Strategic tactical
generation commonly referred content selection surface realization, respectively

398

fiT RAINING ULTILINGUAL PORTSCASTER

evaluate system demonstrate language-independence training generate
commentaries English Korean. Experiments test data (annotated evaluation purposes only) demonstrate system learns accurately semantically parse sentences, generate
sentences, decide events describe. Finally, subjective human evaluation commentated game clips demonstrate limited domain, system generates sportscasts
cases similar quality produced humans.
three main contributions make paper. First, explore possibility
learning grounded language models perceptual context form ambiguous parallel
data. Second, investigate several different methods disambiguating data determined
using combined score includes tactical strategic generation scores performed
best overall. Finally, built complete system learns sportscast multiple languages.
carefully verified automatic human evaluations system able perform
several tasks including disambiguating training data, semantic parsing, tactical strategic
generation. language involved work restricted compared handcrafted commercial sportscasting systems, goal demonstrate feasibility learning grounded
language system language-specific prior knowledge.
remainder paper structured follows. Section 2 provides background previous work utilize extend build system. Section 3 describes sportscasting
data collected train test approach. Section 4 Section 5 present details
basic methods learning tactical strategic generation, respectively, initial experimental results. Section 6 discusses extensions basic system incorporate information
strategic generation process disambiguating training data. Section 7 presents experimental results initializing system data disambiguated recent method aligning
language facts may refer. Section 8 discusses additions try detect superfluous sentences refer extracted event. Section 9 presents human evaluation
automatically generated sportscasts. Section 10 reviews related work, Section 11 discusses future
work, Section 12 presents conclusions.

2. Background
Systems learning semantic parsers induce function maps natural-language (NL) sentences
meaning representations (MRs) formal logical language. Existing work focused
learning supervised corpus sentence manually annotated correct MR
(Mooney, 2007; Zettlemoyer & Collins, 2007; Lu, Ng, Lee, & Zettlemoyer, 2008; Jurcicek, Gasic,
Keizer, Mairesse, Thomson, & Young, 2009). human annotated corpora expensive
difficult produce, limiting utility approach. Kate Mooney (2007) introduced
extension one system, K RISP (Kate & Mooney, 2006), learn ambiguous
training data requires little human annotation effort. However, system unable
generate language required sportscasting task. Thus, enhanced another system
called WASP (Wong & Mooney, 2006) capable language generation well semantic
parsing similar manner allow learn ambiguous supervision. briefly describe
previous systems below. systems assume access formal deterministic
context-free grammar (CFG) defines formal meaning representation language (MRL). Since
MRLs formal computer-interpretable languages, grammar usually easily available.
399

fiC HEN , K IM , & OONEY

2.1 KRISP KRISPER
K RISP (Kernel-based Robust Interpretation Semantic Parsing) (Kate & Mooney, 2006) uses
support vector machines (SVMs) string kernels build semantic parsers. SVMs state-ofthe-art machine learning methods learn maximum-margin separators prevent over-fitting
high-dimensional data natural language text (Joachims, 1998). extended
non-linear separators non-vector data exploiting kernels implicitly create even higher
dimensional space complex data (nearly) linearly separable (Shawe-Taylor & Cristianini,
2004). Recently, kernels strings trees effectively applied variety problems
text learning NLP (Lodhi, Saunders, Shawe-Taylor, Cristianini, & Watkins, 2002; Zelenko,
Aone, & Richardella, 2003; Collins, 2002; Bunescu & Mooney, 2005). particular, K RISP uses
string kernel introduced Lodhi et al. (2002) classify substrings NL sentence.
First, K RISP learns classifiers recognize word phrase NL sentence indicates
particular concept MRL introduced MR. uses production rules
MRL grammar represent semantic concepts, learns classifiers production
classify NL substrings indicative production not. semantically parsing
sentence, classifier estimates probability production covering different substrings
sentence. information used compositionally build complete MR
sentence. Given partial matching provided string kernels over-fitting prevention
provided SVMs, K RISP experimentally shown particularly robust noisy training
data (Kate & Mooney, 2006).
K RISPER (Kate & Mooney, 2007) extension K RISP handles ambiguous training
data, sentence annotated set potential MRs, one correct.
Psuedocode method shown Algorithm 1. employs iterative approach analogous
expectation maximization (EM) (Dempster, Laird, & Rubin, 1977) improves upon selection
correct NLMR pairs iteration. first iteration (lines 3-9), assumes
MRs paired sentence correct trains K RISP resulting noisy supervision.
subsequent iterations (lines 11-27), K RISPER uses currently trained parser score
potential NLMR pair, selects likely MR sentence, retrains parser
resulting disambiguated supervised data. manner, K RISPER able learn type
weak supervision expected grounded language learner exposed sentences ambiguous
contexts. However, system previously tested artificially corrupted generated
data.
2.2 WASP WASP1
WASP (Word-Alignment-based Semantic Parsing) (Wong & Mooney, 2006) uses state-of-the-art
statistical machine translation (SMT) techniques (Brown, Cocke, Della Pietra, Della Pietra, Jelinek,
Lafferty, Mercer, & Roossin, 1990; Yamada & Knight, 2001; Chiang, 2005) learn semantic
parsers. SMT methods learn effective machine translators training parallel corpora consisting
human translations documents one alternative natural languages. resulting
translators typically significantly effective manually developed systems SMT
become dominant approach machine translation. Wong Mooney (2006) adapted
methods learn translate NL MRL rather one NL another.
First, SMT word alignment system, GIZA++ (Och & Ney, 2003; Brown, Della Pietra, Della
Pietra, & Mercer, 1993), used acquire bilingual lexicon consisting NL substrings coupled
400

fiT RAINING ULTILINGUAL PORTSCASTER

Algorithm 1 K RISPER
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , K RISP semantic parser
1:
2:
3:
4:
5:
6:
7:
8:
9:

main
//Initial training loop
sentence si
meaning representation mj R(si )
add (si , mj ) InitialTrainingSet
end
end
SemanticModel = Train(InitialTrainingSet)

10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:

//Iterative retraining
repeat
sentence si
meaning representation mj R(si )
mj .score = Evaluate(si , mj , SemanticModel )
end
end
BestExampleSet
set consistent examples = {(s, m)|s S, MR(s)}
P
m.score maximized
SemanticModel = rain(BestExamplesSet)
Convergence MAX ITER reached
end main

22:

function Train(TrainingExamples)
Train K RISP unambiguous TrainingExamples
25:
return trained K RISP semantic parser
26: end function
23:

24:

27:

function Evaluate(s, m, SemanticModel )
Use K RISP semantic parser SemanticModel find derivation meaning representation sentence
30:
return parsing score
31: end function

28:

29:

401

fiC HEN , K IM , & OONEY

translations target MRL. formal languages, MRLs frequently contain many
purely syntactic tokens parentheses brackets, difficult align words
NL. Consequently, found much effective align words NL productions
MRL grammar used parse corresponding MR. Therefore, GIZA++ used
produce N 1 alignment words NL sentence sequence MRL
productions corresponding top-down left-most derivation corresponding MR.
Complete MRs formed combining NL substrings translations using
grammatical framework called synchronous CFG (SCFG) (Aho & Ullman, 1972), forms
basis existing syntax-based SMT (Yamada & Knight, 2001; Chiang, 2005). SCFG,
right hand side production rule contains two strings, case one NL
MRL. Derivations SCFG simultaneously produce NL sentences corresponding MRs.
bilingual lexicon acquired word alignments training data used construct set
SCFG production rules. probabilistic parser produced training maximum-entropy
model using EM learn parameters SCFG productions, similar methods
used Riezler, Prescher, Kuhn, Johnson (2000), Zettlemoyer Collins (2005).
translate novel NL sentence MR, probabilistic chart parser (Stolcke, 1995) used find
probable synchronous derivation generates given NL, corresponding MR
generated derivation returned.
Since SCFGs symmetric, used generate NL MR well parse NL
MR (Wong & Mooney, 2007). allows learned grammar used parsing
generation, elegant property important advantages (Shieber, 1988). generation
system, WASP1 , uses noisy-channel model (Brown et al., 1990):
arg max Pr(e|f ) = arg max Pr(e) Pr(f |e)
e

(1)

e

e refers NL string generated given input MR, f . Pr(e) language model,
Pr(f |e) parsing model provided WASPs learned SCFG. generation task find
sentence e (1) e good sentence priori, (2) meaning input
MR. language model, use standard n-gram model, useful ranking candidate
generated sentences (Knight & Hatzivassiloglou, 1995).

3. Sportscasting Data
train test system, assembled human-commentated soccer games RoboCup
simulation league (www.robocup.org). Since focus language learning computer vision, chose use simulated games instead real game video simplify extraction
perceptual information. Based ROCCO RoboCup commentators incremental event recognition module (Andre et al., 2000) manually developed symbolic representations game events
rule-based system automatically extract simulator traces. extracted
events mainly involve actions ball, kicking passing, include
game information whether current playmode kickoff, offside, corner kick.
events represented atomic formulas predicate logic timestamps. logical facts
constitute requisite MRs, manually developed simple CFG formal semantic
language. Details events detected complete grammar found Appendix A.
NL portion data, humans commentate games watching
simulator. collected commentaries English Korean. English commentaries
402

fiT RAINING ULTILINGUAL PORTSCASTER

Total # comments
Total # words
Vocabulary size
Avg. words per comment

English dataset
2036
11742
454
5.77

Korean dataset
1999
7941
344
3.97

Table 1: Word statistics English Korean datasets

Number events

Total

2001 final
2002 final
2003 final
2004 final

4003
2223
2113
2318

722
514
410
390

2001 final
2002 final
2003 final
2004 final

4003
2223
2113
2318

673
454
412
460

Number comments
MRs Correct MR
English dataset
671
520
458
376
397
320
342
323
Korean dataset
650
600
444
419
396
369
423
375

Events per comment
Max Average Std. Dev.
9
10
12
9

2.235
2.403
2.849
2.729

1.641
1.653
2.051
1.697

10
12
10
9

2.138
2.489
2.551
2.601

2.076
3.083
3.672
2.593

Table 2: Alignment statistics English Korean datasets. comments correct meaning representations associated essentially noise training
data (18% English dataset 8% Korean dataset). Moreover, average
2 possible events linked comment half links
incorrect.

produced two different people Korean commentaries produced single person.
commentators typed comments text box, recorded timestamp.
construct final ambiguous training data, paired comment events
occurred five seconds less comment made. Examples ambiguous training
data shown Figure 2. edges connect sentences events might refer. English
translations Korean commentaries included figure readers benefit
part actual data. Note use English words predicates constants
MRs human readability only, system treats arbitrary conceptual tokens must
learn connection English Korean words.
annotated total four games, namely, finals RoboCup simulation league
year 2001 2004. Word statistics data shown Table 1.
sentences fairly short due nature sportscasts, data provides challenges form
synonyms (e.g. Pink1, PinkG pink goalie refer player) polysemes
(e.g. kick kicks toward goal refers kick event whereas kicks Pink3 refers
pass event.) Alignment statistics datasets shown Table 2. 2001 final almost
twice number events games went double overtime.
403

fiC HEN , K IM , & OONEY

Natural Language Commentary

Meaning Representation
badPass ( PurplePlayer1 ,
PinkPlayer8 )
turnover ( PurplePlayer1 ,
PinkPlayer8 )
kick ( PinkPlayer8 )
pass ( PinkPlayer8 , PinkPlayer11 )
kick ( PinkPlayer11 )

Purple goalie turns ball
Pink8
Purple team sloppy today
Pink8 passes Pink11
Pink11 looks around teammate

kick ( PinkPlayer11 )
ballstopped
kick ( PinkPlayer11 )
pass ( PinkPlayer11 , PinkPlayer8 )
kick ( PinkPlayer8 )
pass ( PinkPlayer8 , PinkPlayer11 )

Pink11 makes long pass Pink8

Pink8 passes back Pink11

(a) Sample trace ambiguous English training data

Natural Language Commentary

Meaning Representation

10 11 .
(purple10 passes purple 11)

kick ( PurplePlayer10 )

11 10 .
(purple11 passes purple 10)

kick ( PurplePlayer11 )

10 3 .
(pink3 steals ball purple 10)

steal ( PinkPlayer3 )

3 .
(pink3 passes pink goalie)

kick ( PinkPlayer3 )

pass ( PurplePlayer10 , PurplePlayer11 )

pass ( PurplePlayer11 , PurplePlayer10 )

turnover ( PurplePlayer10 , PinkPlayer3 )

playmode ( free_kick_r )

(b) Sample trace ambiguous Korean training data

Figure 2: Examples training data. outgoing edges comments indicate
possibly associated meaning representations considered system. bold links
indicate correct matches comments meaning representations.

404

fiT RAINING ULTILINGUAL PORTSCASTER

evaluation purposes only, gold-standard matching produced examining comment manually selecting correct MR exists. matching approximate
sometimes comments contain information present MRs. example, comment might describe location length pass MR captures participants
pass. bold lines Figure 2 indicate annotated correct matches sample data. Notice sentences correct matches (about one fifth English data one tenth
Korean data). example, sentence Purple team sloppy today Figure 2(a)
cannot represented MRL consequently corresponding correct MR.
another example, Korean sentence translation pink3 passes pink goalie Figure 2(b) represented MRL, correct match due incomplete
event detection. free kick called pink3 passing pink goalie pass event
retrieved. Finally, case sentence Pink11 makes long pass Pink8 Figure 2(a), correct MR falls outside 5-second window. game, Table 2 shows
total number NL sentences, number least one recent extracted event
could refer, number actually refer one recent extracted
events. maximum, average, standard deviation number recent events paired
comment given.

4. Learning Tactical Generation Ambiguous Supervision
existing systems capable solving parts sportscasting problem, none
able perform whole task. need system deal ambiguous supervision
K RISPER generate language WASP. introduce three systems
both. overview differences existing systems new systems present
shown Table 3.
three systems introduced based extensions WASP, underlying language
learner. main problem need solve disambiguate training data
train WASP create language generator. new system uses different
disambiguation criteria determine best matching NL sentences MRs.

4.1 WASPER
first system extension WASP manner similar K RISP extended create
K RISPER. uses EM-like retraining handle ambiguously annotated data, resulting system
call WASPER. general, system learns semantic parsers extended handle
ambiguous data long produce confidence levels given NLMR pairs. Given set
sentences set MRs associated sentence R(s), disambiguate
data finding pairs (s, m), R(s) = arg maxm P r(m|s). Although
probability used here, ranking relative potential parses would suffice. pseudocode
WASPER shown Algorithm 2. difference compared K RISPER pseudocode
use WASP semantic parser instead K RISP parser. Also, produce WASP
language generator well desired final output task.
405

fiC HEN , K IM , & OONEY

Algorithm

Underlying learner

K RISP
K RISPER
WASP

SVM
K RISP
GIZA align words,
MR tokens,
learn probalistic SCFG
WASP
First disambiguate
K RISPER,
train WASP
WASP

WASPER
K RISPER -WASP

WASPER -G EN

Generate?

Disambiguation criteria



Yes

Ambiguous
data?

Yes


Yes
Yes

Yes
Yes

WASPs parsing score
K RISPs parsing score

Yes

Yes

NIST score
best NL given MR

n/a
K RISPs parsing score
n/a

Table 3: Overview various learning systems presented. first three algorithms existing
systems. introduce last three systems able learn ambiguous
training data acquire language generator. differ disambiguate
training data.

Algorithm 2 WASPER
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , WASP semantic parser/language generator
1: main
2:
Algorithm 1
3: end main
4:

function Train(TrainingExamples)
Train WASP unambiguous TrainingExamples
7:
return trained WASP semantic parser/language generator
8: end function

5:

6:

9:

function Evaluate(s, m, SemanticModel )
11:
Use WASP semantic parser SemanticModel find derivation meaning representation sentence
12:
return parsing score
13: end function
10:

406

fiT RAINING ULTILINGUAL PORTSCASTER

4.2 KRISPER-WASP
K RISP shown quite robust handling noisy training data (Kate & Mooney, 2006).
important training noisy training data used initialize parser
K RISPERs first iteration. However, K RISPER cannot learn language generator, necessary sportscasting task. result, create new system called K RISPER-WASP
good disambiguating training data capable generation. first use K RISPER
train ambiguous data produce disambiguated training set using prediction
likely MR sentence. unambiguous training set used train WASP
produce parser generator.
4.3 WASPER-GEN
K RISPER WASPER, criterion selecting best NLMR pairs retraining based maximizing probability parsing sentence particular MR. However,
since WASPER capable parsing generation, could alternatively select best
NLMR pairs evaluating likely generate sentence particular MR. Thus,
built another version WASPER called WASPER-G EN disambiguates training data
order maximize performance generation rather parsing. pseudocode shown
Algorithm 3. algorithm WASPER except evaluation function. uses
generation-based score rather parsing-based score select best NLMR pairs.
Specifically, NLMR pair (s, m) scored computing NIST score, machine translation (MT) metric, sentence best generated sentence (lines 9-12).2
Formally, given set sentences set MRs associated sentence
R(s), disambiguate data finding pairs (s, m), R(s) =
arg maxm N IST (s, argmaxs P r(s |m)).
NIST measures precision translation terms proportion n-grams shares
human translation (Doddington, 2002). used evaluate NL generation. Another
popular MT metric BLEU score (Papineni, Roukos, Ward, & Zhu, 2002), inadequate
purpose since comparing one short sentence another instead comparing whole
documents. BLEU score computes geometric mean n-gram precision value n,
means score 0 matching n-gram found every value n. common
setting maximum n 4, two sentences matching 4-gram would
receive BLEU score 0. Consequently, BLEU score unable distinguish quality
generated sentences since fairly short. contrast, NIST uses additive score
avoids problem.
4.4 Experimental Evaluation
section presents experimental results RoboCup data four systems: K RISPER, WASPER,
K RISPER-WASP, WASPER-G EN. Since aware existing systems could
learn semantically parse generate language using ambiguous supervision based perceptual context, constructed lower upper baselines using unmodified WASP. Since
2. natural way use generation-based score would use probability NL given MR (P r(s|m)).
However, initial experiments using metric produce good results. tried changing WASP
maximize joint probability instead parsing probability. However, improve results.

407

fiC HEN , K IM , & OONEY

Algorithm 3 WASPER -G EN
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , WASP semantic parser/language generator
1: main
2:
Algorithm 1
3: end main
4:

function Train(TrainingExamples)
6:
Algorithm 2
7: end function

5:

8:

function Evaluate(s, m, SemanticModel )
GeneratedSentence Use WASP language generator SemanticModel produce
sentence meaning representation
11:
return NIST score GeneratedSentence
12: end function
9:

10:

WASP requires unambiguous training data, randomly pick meaning sentence
set potential MRs serve lower baseline. use WASP trained gold matching
consists correct NLMR pairs annotated human upper baseline. represents upper-bound systems could achieve disambiguated training data
perfectly.
evaluate system three tasks: matching, parsing, generation. matching task
measures well systems disambiguate training data. parsing generation tasks
measure well systems translate NL MR, MR NL, respectively.
Since four games total, trained using possible combinations one three
games. matching, measured performance training data since goal disambiguate data. parsing generation, tested games used training.
Results averaged train/test combinations. evaluated matching parsing using
F-measure, harmonic mean recall precision. Precision fraction systems
annotations correct. Recall fraction annotations gold-standard
system correctly produces. Generation evaluated using BLEU scores roughly estimates well produced sentences match target sentences. treat game
whole document avoid problem using BLEU score sentence-level comparisons mentioned earlier. Also, increase number reference sentences MR using
sentences test data corresponding equivalent MRs, e.g. pass(PinkPLayer7,
PinkPlayer8) occurs multiple times test data, sentences matched MR
gold matchings used reference sentences MR.
4.4.1 ATCHING NL



MR

Since handling ambiguous training data important aspect grounded language learning,
first evaluate well various systems pick correct NLMR pairs. Figure 3 shows Fmeasure identifying correct set pairs various systems. learning systems
408

fi0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

RAINING ULTILINGUAL PORTSCASTER

0.6
0.55

0.6
0.55

0.5

0.5

0.45

0.45

WASPER
WASPER-GEN
KRISPER
random matching

0.4

WASPER
WASPER-GEN
KRISPER
random matching

0.4

0.35

0.35
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.9

0.9

0.8

0.8

0.7

0.7

F-measure

F-measure

Figure 3: Matching results basic systems. WASPER -G EN performs best, outperforming
existing system K RISPER datasets.

0.6

0.5
WASP gold matching
KRISPER
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.4

0.3

0.6

0.5
WASP gold matching
KRISPER
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.4

0.3

0.2

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 4: Semantic parsing results basic systems. results largely mirrors
matching results WASPER -G EN performing best overall.

perform significantly better random F-measure 0.5. English
Korean data, WASPER -G EN best system. WASPER equals outperforms previous
system K RISPER well.
4.4.2 EMANTIC PARSING
Next, present results accuracy learned semantic parsers. trained system
used parse produce MR sentence test set correct MR
gold-standard matching. parse considered correct matches gold standard
exactly. Parsing fairly difficult task usually one way describe
event. example, Player1 passes player2 refer event Player1 kicks
ball player2. Thus, accurate parsing requires learning different ways people describe
409

fi0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

C HEN , K IM , & OONEY

0.3
0.25

0.4
0.35

WASP gold matching
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.2
0.15

WASP gold matching
KRISPER-WASP
WASPER
WASPER-GEN
WASP random matching

0.3
0.25

0.1

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 5: Tactical generation results basic systems. relative performances
various systems change, WASPER -G EN still best system.

event. Synonymy limited verbs. data, Pink1, PinkG pink goalie
refer player1 pink team. Since providing systems prior knowledge,
learn different ways referring entity.
parsing results shown Figure 4 generally correlate well matching results. Systems better disambiguating training data better parsing
supervised training data less noisy. WASPER-G EN best overall English Korean data. interesting note K RISPER relatively well English data
compared matching performance. K RISP robust noise WASP
(Kate & Mooney, 2006) even though trained noisier set data WASPER -G EN
still produced comparable parser.
4.4.3 G ENERATION
third evaluation task generation. WASP-based systems given MR test
set gold-standard matching NL sentence asked generate NL description.
quality generated sentence measured comparing gold-standard using BLEU
scoring.
task tolerant noise training data parsing system
needs learn one way accurately describe event. property reflected results,
shown Figure 5, even baseline system, WASP random matching, fairly well,
outperforming K RISPER-WASP datasets WASPER Korean data. number
event types fairly small, relatively small number correct matchings required
perform task well long event type associated correct sentence pattern
often sentence pattern.
two tasks, WASPER -G EN best system task. One possible explanation WASPER-G ENs superior performance stems disambiguation objective function.
Systems WASPER K RISPER-WASP use parsing scores attempt learn good translation model sentence pattern. hand, WASPER-G EN tries learn good
410

fiT RAINING ULTILINGUAL PORTSCASTER

translation model MR pattern. Thus, WASPER-G EN likely converge good
model fewer MR patterns sentence patterns. However, argued learning
good translation models sentence pattern help producing varied commentaries,
quality captured BLEU score. Another possible advantage WASPER -G EN
uses softer scoring function. probabilities parsing particular sentence
MR sensitive noise training data, WASPER -G EN looks top generated
sentences MR. Even noise data, top generated sentence remains relatively
constant. Moreover, minor variations sentence change results dramatically since
NIST score allows partial matching.

5. Learning Strategic Generation
language generator alone enough produce sportscast. addition tactical generation
deciding say something, sportscaster must preform strategic generation
choosing say (McKeown, 1985).
developed novel method learning events describe. event type (i.e.
predicate pass, goal), system uses training data estimate probability
mentioned sportscaster. Given gold-standard NLMR matches, probability
easy estimate; however, learner know correct matching. Instead, system
must estimate probabilities ambiguous training data. compare two basic methods
estimating probabilities.
first method uses inferred NLMR matching produced language-learning system.
probability commenting event type, ei , estimated percentage events
type ei matched NL sentence.
second method, call Iterative Generation Strategy Learning (IGSL), uses variant EM, treating matching assignments hidden variables, initializing match
prior probability, iterating improve probability estimates commenting event
type. Unlike first method, IGSL uses information MRs explicitly associated
sentence training. Algorithm 4 shows pseudocode. main loop alternates two
steps:
1. Calculating expected probability NLMR matching given current model
likely event commented (line 6)
2. Update prior probability event type mentioned human commentator based
matchings (line 9).
first iteration, NLMR match assigned
probability inversely proportional
P
amount ambiguity associated sentence ( eEvent(s) Pr (e) = |Event(s)|). example,
sentence associated five possible MRs assign match probability 51 . prior
probability mentioning event type estimated average probability assigned
instances event type. Notice process always guarantee proper probability since
MR associated multiple sentences. Thus, limit probability one.
subsequent iterations, probabilities NLMR matchings updated according
new priors. assign match prior probability event type normalized across
associated MRs NL sentence. update priors event type using
411

fiC HEN , K IM , & OONEY

Algorithm 4 Iterative Generation Strategy Learning
input event types E = {e1 , ..., en }, number occurrences event type otalCount(ei )
entire game trace, sentences event types associated meaning representations Event(s)
output probabilities commenting event type P r(ei )
1: Initialize Pr (ei ) = 1
2: repeat
3:
event type ei E
4:
MatchCount = 0
5:
sentence
P
Pr (e)

P
6:
ProbOfMatch = eEvent(s)e=e
Pr (e)
eEvent(s)

7:
8:
9:
10:
11:

MatchCount = MatchCount + ProbOfMatch
end
MatchCount
Pr (ei ) = min( TotalCount(e
,1) {Ensure proper probabilities}
i)
end
Convergence MAX ITER reached

!"#$%&&

'()*+*,-,%.&)/&*#,$0& 4)(2+-,5#3&6()*+*,-,%.&
1)22#$%#3&)$&

*+--7%)66#3&

89:;&<&8:=>?&

?9@:&<&8:=>A&

*+3'+77B6C(6-#DE6,$F?G&

:9;D:&

:9?8A&

%C($)"#(B6C(6-#DE6,$F?G&

:9;:;&

:9H@H&

%C($)"#(B6C(6-#DE6,$F?G&

:9;:;&
:9:;8&

IJ#&7#-#1%#3&#"#$%&,7&"#(*+-,5#3&&
K$&#"#$%&,7&7#-#1%#3&*+7#3&&
)$&%J#&$)(2+-,5#3&6()*+*,-,%.& (+$3)2-.&+11)(3,$0&%)&,%7&6()*+*,-,%.&&
)/&*#,$0&1)22#$%#3&)$&

Figure 6: example strategic generation component works. every timestep,
stochastically select event events occurring moment.
decide whether verbalize selected event based IGSLs estimated probability
commented upon.

new estimated probabilities matchings. process repeated probabilities
converge pre-specified number iterations occurred.
generate sportscast, use learned probabilities determine events describe.
time step, first determine events occurring time. select one
randomly based normalized probabilities. avoid overly verbose, want
make comment every time something happening, especially event rarely commented on.
Thus, stochastically decide whether comment selected event based probability.
example process shown Figure 6.
412

fi0.8

0.8

0.7

0.7

0.6

0.6

F-measure

F-measure

RAINING ULTILINGUAL PORTSCASTER

0.5
inferred gold matching
IGSL
inferred KRISPER
inferred WASPER
inferred WASPER-GEN
inferred random matching

0.4

0.5
inferred gold matching
IGSL
inferred KRISPER
inferred WASPER
inferred WASPER-GEN
inferred random matching

0.4

0.3

0.3
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 7: Strategic generation results various systems. novel algorithm IGSL performs
best, almost par upper bound uses gold-annotated matchings.

event
ballstopped
kick
pass
turnover
badPass

# occurrences
5817
2122
1069
566
371

% commented
1.72 104
0.0 33
0.999
0.214
0.429

IGSL
1.09 105
0.018
0.983
0.909
0.970

inferred WASPER -G EN
0.016
0.117
0.800
0.353
0.493

Table 4: Top 5 frequent events, % times commented on, probabilities
learned top algorithms English data

5.1 Experimental Evaluation
different methods learning strategic generation evaluated based often events
describe test data coincide human decided describe. first
approach, results using inferred matchings produced K RISPER, WASPER, WASPER-G EN
well gold random matching establishing baselines presented Figure 7.
graph, clear IGSL outperforms learning inferred matchings actually
performs level close using gold matching. However, important note
limiting potential learning gold matching using predicates decide
whether talk event.
English data, probabilities learned IGSL inferred matchings WASPER G EN five frequently occurring events shown Table 4. WASPER -G EN learns
fairly good probabilities general, well IGSL frequent events.
IGSL uses occurrences events associated possible comments
training iterations. Rarely commented events ballstopped kick often occur without
comments uttered. Consequently, IGSL assigns low prior probabilities
lowers chances matched sentences. hand, WASPER -G EN
use priors sometimes incorrectly matches comments them. Thus, using inferred
413

fiC HEN , K IM , & OONEY

matches WASPER -G EN results learning higher probabilities commenting rarely
commented events.
methods use predicates MRs decide whether comment not,
perform quite well data collected. particular, IGSL performs best, use
strategic generation rest paper.

6. Using Strategic Generation Improve Matching
section, explore knowledge learned strategic generation used improve
accuracy matching sentences MRs. previous section, described several ways
learn strategic generation, including IGSL learns directly ambiguous training data.
Knowing events people tend talk help resolve ambiguities training
data. Events likely discussed likely matched
NL sentence disambiguating training data. Therefore, section describes methods
integrate strategic generation scores (such Table 4) scoring NLMR pairs used
matching process.
6.1 WASPER-GEN-IGSL
WASPER -G EN -IGSL extension WASPER -G EN uses strategic generation scores
IGSL. WASPER -G EN uses NIST score pick best MR sentence finding MR
generates sentence closet actual NL sentence. WASPER -G EN -IGSL combines tactical
(NIST) strategic (IGSL) generation scores pick best NLMR pairs. simply multiplies
NIST score IGSL score together form composite score. new score biases
selection matching pairs include events IGSL determines are, priori, likely
discussed. helpful, especially beginning WASP produce
particularly good language generator. many instances, generated sentences
possible MRs equally bad overlap target sentence. Even generation
produces perfectly good sentence, generation score unreliable comparing
single sentence single reference often short well. Consequently, often
difficult WASPER-G EN distinguish among several MRs equal scores. hand,
event types different strategic generation scores, default choosing
MR higher prior probability mentioned. Algorithm 5 shows pseudocode
WASPER -G EN -IGSL.
6.2 Variant WASPER-GEN Systems
Although WASPER -G EN uses NIST score estimate goodness NLMR pairs, could easily
use MT evaluation metric. already discussed unsuitability BLEU comparing short individual sentences since assigns zero many pairs. However, NIST score
limitations. example, normalized, may affect performance WASPER -G EN IGSL combined IGSL score. Another limitation comes using higher-order
N-grams. Commentaries domain often short, frequently higher-order
N-gram matches generated sentences target NL sentences.
METEOR metric (Banerjee & Lavie, 2005) designed resolve various weaknesses
BLEU NIST metrics, focused word-to-word matches reference
414

fiT RAINING ULTILINGUAL PORTSCASTER

Algorithm 5 WASPER -G EN -IGSL
input sentences associated sets meaning representations R(s)
output BestExamplesSet, set NL-MR pairs,
SemanticModel , WASP semantic parser/language generator
1: main
2:
Algorithm 1
3: end main
4:

function Train(TrainingExamples)
6:
Algorithm 2
7: end function

5:

8:
9:
10:
11:
12:
13:
14:
15:

function Evaluate(s, m, SemanticModel )
Call Algorithm 4 collect IGSL scores
GeneratedSentence Use WASP language generator SemanticModel produce
sentence meaning representation
TacticalGenerationScore NIST score GeneratedSentence
StrategicGenerationScore Pr (event type m) result Algorithm 4
return TacticalGenerationScore StrategicGenerationScore
end function

sentence test sentence. METEOR first evaluates uni-gram matches reference
test sentence determines well words ordered. METEOR seems
appropriate domain good generated sentences missing adjectives adverbs critical meaning sentence prevent higher-order N-gram matches.
addition, METEOR normalized always 0 1, may combine effectively IGSL scores (which range 01).
6.3 Experimental Evaluation
evaluated new systems, WASPER-G EN-I GSL NIST METEOR scoring using
methodology Section 4.4. matching results shown Figure 8, including results
WASPER -G EN, best system previous section. WASPER-G EN-IGSL
either NIST METEOR scoring clearly outperforms WASPER-G EN. indicates strategicgeneration information help disambiguate data. Using different MT metrics produces
less noticeable effect. clear winner English data; however, METEOR seems
improve performance Korean data.
Parsing results shown Figure 9. previously noted, parsing results generally mirror
matching results. new systems outperform WASPER -G EN, previously best system.
again, English data show clear advantage using either NIST METEOR,
Korean data gives slight edge using METEOR metric.
Results tactical generation shown Figure 10. English Korean
data, new systems come close performance WASPER-G EN beat it. However,
new systems outperform K RISPER -WASP WASPER shown figure.
415

fi0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75
F-measure

F-measure

C HEN , K IM , & OONEY

0.7
0.65
0.6

0.7
0.65
0.6

0.55

0.55
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.5

WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.5

0.45

0.45
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75
F-measure

F-measure

Figure 8: Matching results. Integrating strategic information improves results previously best system WASPER -G EN. choice MT metric used, however, makes
less impact.

0.7
0.65

0.7
0.65

0.6

0.6
WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.55

WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.55

0.5

0.5
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 9: Semantic parsing results. results similar matching results integrating
strategic generation information improves performance.

416

fi0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

RAINING ULTILINGUAL PORTSCASTER

0.3
0.25

0.4
0.35

0.2

0.3
WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.15

WASP gold matching
WASPER-GEN
WASPER-GEN-IGSL
WASPER-GEN-IGSL using METEOR

0.25

0.1

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 10: Tactical generation results. two new systems come close performance
WASPER -G EN, beat it. However, outperform systems
presented earlier shown figure.

Overall, expected, using strategic information improves performance matching
semantic parsing tasks. English Korean datasets, WASPER-G EN-IGSL
variant using METEOR metric clearly outperform WASPER-G EN utilize
strategic information. However, strategic information improve tactical generation.
could due ceiling effect WASPER -G EN already performs level near upper
baseline. matching performance improved, generation performance little room
grow.

7. Using Generative Alignment Model
Recently, Liang et al. (2009) developed generative model used match naturallanguage sentences facts corresponding database may refer. one
evaluation domains, used English RoboCup sportscasting data. method solves
matching (alignment) problem data, address tasks semantic parsing
language generation. However, generative model elegantly integrates simple strategic
tactical language generation models order find overall probable alignment sentences
events. demonstrated improved matching performance English data, generating
accurate NLMR pairs best system. Thus, curious results could
used improve systems, perform semantic parsing generation.
ran code new Korean data resulted much worse matching results compared
best system seen Table 5.
simplest way utilizing results use NLMR pairs produced method
supervised data WASP. expected, improved NLMR pairs English data resulted
improved semantic parsers seen results Table 6. Even Korean dataset,
training matchings produced system ended fairly well even though matching performance poor. tactical generation, using matching produced marginal
improvement English dataset surprisingly large improvement Korean data
417

fiC HEN , K IM , & OONEY

Algorithm
Liang et al. (2009)
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
initialization Initialized
75.7
59.7
79.3
68.1
75.8
73.5
73.9
73.1
75.1

Korean dataset
initialization Initialized
69.4
72.8
76.6
75.3
80.0
81.9
81.6
83.8
84.1

Table 5: Matching results (F1 scores) 4-fold cross-validation English Korean
datasets. Systems run initialization initialized matchings produced
Liang et al.s (2009) system.

Algorithm
WASP
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
initialization Initialized
n/a
80.3
61.84
79.32
70.15
77.59
73.19
73.04
72.75
74.62

Korean dataset
initialization Initialized
n/a
74.01
69.12
75.69
72.02
77.49
78.75
75.27
80.65
81.21

Table 6: Semantic parsing results (F1 scores) 4-fold cross-validation English
Korean datasets. Systems run initialization initialized matchings
produced Liang et al.s (2009) system.

Algorithm
WASP
WASPER
WASPER-G EN
WASPER-G EN-I GSL
WASPER-G EN-I GSL -M ETEOR

English dataset
initialization Initialized
n/a
0.4580
0.3471
0.4599
0.4560
0.4414
0.4223
0.4585
0.4062
0.4353

Korean dataset
initialization Initialized
n/a
0.5828
0.4524
0.6118
0.5575
0.6796
0.5371
0.6710
0.5180
0.6591

Table 7: Tactical generation results (BLEU score) 4-fold cross-validation English
Korean datasets. Systems run initialization initialized matchings
produced Liang et al.s (2009) system.

shown Table 7. Overall, using alignments produced Liang et al.s system resulted good
semantic parsers tactical generators.
addition training WASP alignment, utilize output better
starting point systems. Instead initializing iterative alignment methods
model trained ambiguous NLMR pairs, initialized disambiguated
NLMR pairs produced Liang et al.s system.
418

fiT RAINING ULTILINGUAL PORTSCASTER

Initializing systems manner almost always improved performance three
tasks (Tables 5, 6, 7). Moreover, results best systems exceed simply training WASP alignment cases except semantic parsing English data. Thus,
combining Liang et al.s alignment disambiguation techniques seems produce best
overall results. English data, WASPER initialization performs best matching generation. slightly worse semantic parsing task compared WASP trained
Liang et al.s alignment. Korean data, systems better training WASP
alignment. WASPER -G EN -I GSL -M ETEOR initialization performs best matching
semantic parsing WASPER -G EN initialization performs best generation.
Overall, initializing systems alignment output Liang et al.s generative model
improved performance expected. Starting cleaner set data led better initial semantic
parsers language generators led better end results. Furthermore, incorporating
semantic parser tactical generator, able improve Liang et al.s alignments
achieve even better results cases.

8. Removing Superfluous Comments
far, discussed handle ambiguity multiple possible MRs
NL sentence. training, methods assume NL sentence matches
exactly one potential MRs. However, comments superfluous, sense
refer currently extracted event represented set potential MRs. previously
shown Tables 2, one fifth English sentences one tenth Korean sentences
superfluous sense.
many reasons superfluous sentences. occur naturally language
people always talk current environment. domain, sportscasters often mention
past events general information particular teams players. Moreover, depending
application, chosen MRL may represent things people talk about. example,
RoboCup MRL cannot represent information players actively engaged
ball. Finally, even sentence represented chosen MRL, errors perceptual
system incorrect estimation event occurred lead superfluous sentences.
perceptual errors alleviated degree increasing size window used
capture potential MRs (the previous 5 seconds experiments). However, comes
cost increased ambiguity associates MRs sentence.
deal problem superfluous sentences, eliminate lowest-scoring NLMR
pairs (e.g. lowest parsing scores WASPER lowest NIST scores WASPER-G EN). However,
order set pruning threshold, need automatically estimate amount superfluous
commentary absence supervised data. Notice problem looks similar
strategic generation problem (estimating likely MR participates correct matching
opposed likely NL sentence participates correct matching), approaches used
cannot applied. First, cannot use matches inferred existing systems estimate
fraction superfluous comments since current systems match every sentence MR.
difficult develop algorithm similar IGSL due imbalance NL sentences
MRs. Since many MRs, examples events occurring without
commentaries vice versa.
419

fiC HEN , K IM , & OONEY

8.1 Estimating Superfluous Rate Using Internal Cross Validation
propose using form internal (i.e. within training set) cross validation estimate rate
superfluous comments. algorithm used conjunction systems,
chose implement K RISPER trains much faster systems. makes
tractable train many different semantic parsers choose best one. basic idea
use part ambiguous training data estimate accuracy semantic parser even though
know correct matchings. Assuming reasonable superfluous sentence rate, know
time correct MR contained set MRs associated NL sentence.
Thus, assume semantic parser parses NL sentence one MRs associated
better one parses MR set. approach estimating
accuracy, evaluate semantic parsers learned using various pruning thresholds pick
best one. algorithm briefly summarized following steps:
1. Split training set internal training set internal validation set.
2. Train K RISPER N times internal training set using N different threshold values (eliminating lowest scoring NLMR pairs threshold retraining iteration
Algorithm 1).
3. Test N semantic parsers internal validation set determine parser able
parse largest number sentences one potential MRs.
4. Use threshold value produced best parser previous step train final parser
complete original training set.
8.2 Experiments
evaluated effect removing superfluous sentences three tasks: matching, parsing,
generation. present results K RISPER K RISPER -WASP. matching,
show results K RISPER responsible disambiguating training data
systems (so K RISPER -WASPs results same). generation, show results
K RISPER-WASP, since K RISPER cannot perform generation.
matching results shown Figure 11 demonstrate removing superfluous sentences
improve performance English Korean, although difference small absolute
terms. parsing results shown Figure 12 indicate removing superfluous sentences usually
improves accuracy K RISPER K RISPER -WASP marginally. observed
many times, parsing results consistent matching results. Finally, tactical generation results shown Figure 13 suggest removing superfluous comments actually decreases
performance somewhat. again, potential explanation generation less sensitive
noisy training data. removing superfluous comments improves purity training data,
removes potentially useful examples. Consequently, system learn generate sentences removed data. Overall, generation, advantage
cleaner disambiguated training data apparently outweighed loss data.
420

fi0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

RAINING ULTILINGUAL PORTSCASTER

0.6
0.55

0.6
0.55

0.5

0.5

0.45

0.45

0.4

0.4

KRISPER
KRISPER superfluous comment removal

0.35

KRISPER
KRISPER superfluous comment removal

0.35
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65
F-measure

F-measure

Figure 11: Matching results comparing effects removing superfluous comments.

0.6
0.55

0.6
0.55

0.5

0.5
KRISPER
KRISPER superfluous comment removal
KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.45

KRISPER
KRISPER superfluous comment removal
KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.45

0.4

0.4
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

0.5

0.6

0.45

0.55

0.4

0.5

0.35

0.45
BLEU

BLEU

Figure 12: Semantic parsing results improved marginally superfluous comment removal.

0.3

0.4

0.25

0.35

0.2

0.3

0.15

0.25

KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.1

KRISPER-WASP
KRISPER-WASP superfluous comment removal

0.2
1

2
Number Training Games

3

1

(a) English

2
Number Training Games

3

(b) Korean

Figure 13: Tactical generation performance decreases removing superfluous comments.

421

fiC HEN , K IM , & OONEY

9. Human Subjective Evaluation
best, automatic evaluation generation imperfect approximation human assessment.
Moreover, automatically evaluating quality entire generated sportscast even difficult. Consequently, used Amazons Mechanical Turk collect human judgements
produced sportscasts. human judge shown three clips simulated game video one sitting. 8 video clips total. 8 clips use 4 game segments 4 minutes each, one
four games (2001-2004 RoboCup finals). 4 game segments commentated
human system. use IGSL determine events comment
WASPER -G EN (our best performing system tactical generation) produce commentaries.
make commentaries varied, took top 5 outputs WASPER -G EN chose
one stochastically weighted scores. system always trained three games, leaving game test segment extracted. video clips accompanied
commentaries appear subtitles screen well audio produced automated text speech system 3 videos shown random counter-balanced order ensure
consistent bias toward segments shown earlier later. asked judges score
commentaries using following metrics:

Score
5
4
3
2
1

Fluency
Flawless
Good
Non-native
Disfluent
Gibberish

Semantic
Correctness
Always
Usually
Sometimes
Rarely
Never

Sportscasting
Ability
Excellent
Good
Average
Bad
Terrible

Fluency semantic correctness, adequacy, standard metrics human evaluations NL
translations generations. Fluency measures well commentaries structured, including
syntax grammar. Semantic correctness indicates whether commentaries accurately describe
happening game. Finally, sportscasting ability measures overall quality
sportscast. includes whether sportscasts interesting flow well. addition
metrics, asked whether thought sportscast composed human
computer (Human?).
Since Mechanical Turk recruits judges Internet, make sure judges
assigning ratings randomly. Thus, addition asking rate video,
asked count number goals video. Incorrect responses question caused
ratings discarded. ensure judges faithfully watched entire clip
assigning ratings. pruning, average 36 ratings (from 40 original ratings)
8 videos English data. Since difficult recruit Korean judges
Internet, recruited person collected 7 ratings average video
Korean data. Table 8 9 show results English Korean data, respectively.
Statistically significant results shown boldface.
Results surprisingly good English data across categories machine actually
scoring higher human average. However, differences statistically significant
3. Sample video clips sound available web http://www.cs.utexas.edu/users/ml/
clamp/sportscasting/.

422

fiT RAINING ULTILINGUAL PORTSCASTER

2001 final
2002 final
2003 final
2004 final
Average

Commentator
Human
Machine
Human
Machine
Human
Machine
Human
Machine
Human
Machine

Fluency
3.74
3.89
4.13
3.97
3.54
3.89
4.03
4.13
3.86
3.94

Semantic
Correctness
3.59
3.81
4.58
3.74
3.73
4.26
4.17
4.38
4.03
4.03

Sportscasting
Ability
3.15
3.61
4.03
3.29
2.61
3.37
3.54
4.00
3.34
3.48

Human?
20.59%
40.00%
42.11%
11.76%
13.51%
19.30%
20.00%
56.25%
24.31%
26.76%

Table 8: Human evaluation overall sportscasts English data. Bold numbers indicate statistical
significance.

2001 final
2002 final
2003 final
2004 final
Average

Commentator
Human
Machine
Human
Machine
Human
Machine
Human
Machine
Human
Machine

Fluency
3.75
3.50
4.17
3.25
3.86
2.38
3.00
2.71
3.66
2.93

Semantic
Correctness
4.13
3.67
4.33
3.38
4.29
3.25
3.75
3.43
4.10
3.41

Sportscasting
Ability
4.00
2.83
3.83
3.13
4.00
2.88
3.25
3.00
3.76
2.97

Human?
50.00%
33.33%
83.33%
50.00%
85.71%
25.00%
37.50%
14.29%
62.07%
31.03%

Table 9: Human evaluation overall sportscasts Korean data. Bold numbers indicate statistical
significance.

423

fiC HEN , K IM , & OONEY

based unpaired t-test (p > 0.05). Nevertheless, encouraging see machine
rated highly. variance humans performance since two different
commentators. notably, compared machine, humans performance 2002 final
quite good commentary included many details position players,
types passes, comments overall flow game. hand,
humans performance 2003 final quite bad human commentator
mechanical used sentence pattern repeatedly. machine performance
even throughout although sometimes gets lucky. example, machine serendipitously said
beginning exciting match. near start 2004 final clip simply
statement incorrectly learned correspond extracted MR actually unrelated.
results Korean impressive. human beats machine average
categories. However, largest difference scores category 0.8.
Moreover, absolute scores indicate generated Korean sportscast least acceptable
quality. judges even mistakenly thought produced humans one third time.
Part reason worse performance compared English data Korean commentaries fairly detailed included events extracted limited perceptual
system. Thus, machine simply way competing limited expressing
information present extracted MRs.
elicited comments human judges get qualitative evaluation. Overall,
judges thought generated commentaries good accurately described actions
field. Picking top 5 generated sentences added variability machine-generated
sportscasts improved results compared earlier experiments presented Chen
Mooney (2008). However, machine still sometimes misses significant plays scoring
corner kicks. plays happen much less frequently often coincide
many events (e.g. shooting ball kickoffs co-occur scoring). Thus, machine
harder time learning infrequent events. Another issue concerns representation.
Many people complain long gaps sportscasts lack details. event detector
concentrates ball possession positions elapsed time. Thus, player holding onto
ball dribbling long time produce events detected simulated perceptual
system. Also, short pass backfield treated exactly long pass across
field near goal. Finally, people desired colorful commentary (background information,
statistics, analysis game) fill voids. somewhat orthogonal issue since
goal build play-by-play commentator described events currently happening.

10. Related Work
section review related work semantic parsing, natural language generation
well grounded language learning.
10.1 Semantic Parsing
mentioned Section 2, existing work semantic parser learners focused supervised
learning sentence annotated semantic meaning. semantic-parser learners additionally require either syntactic annotations (Ge & Mooney, 2005) prior syntactic knowledge target language (Ge & Mooney, 2009; Zettlemoyer & Collins, 2005, 2007). Since
world never provides direct feedback syntactic structure, language-learning methods
424

fiT RAINING ULTILINGUAL PORTSCASTER

require syntactic annotation directly applicable grounded language learning. Therefore,
methods learn semantic annotation critical learning language perceptual
context.
use logic formulas MRs, particular MRL use contains atomic
formulas equivalently represented frames slots. systems use
transformation-based learning (Jurcicek et al., 2009), Markov logic (Meza-Ruiz, Riedel, &
Lemon, 2008) learn semantic parsers using frames slots. principle, framework
used semantic parser learner long provides confidence scores parse results.
10.2 Natural Language Generation
several existing systems sportscast RoboCup games (Andre et al., 2000). Given
game states provided RoboCup simulator, extract game events generate real-time
commentaries. consider many practical issues timeliness, coherence, variability,
emotion needed produce good sportscasts. However, systems hand-built
generate language using pre-determined templates rules. contrast, concentrate
learning problem induce generation components ambiguous training data. Nevertheless, augmenting system components systems could improve
final sportscasts produced.
prior work learning lexicon elementary semantic expressions corresponding natural language realizations (Barzilay & Lee, 2002). work uses multiple-sequence
alignment datasets supply several verbalizations corresponding semantics extract
dictionary.
Duboue McKeown (2003) first propose algorithm learning strategic generation automatically data. Using semantics associated texts, system learns classifier
determines whether particular piece information included presentation not.
recent work learning strategic generation using reinforcement learning
(Zaragoza & Li, 2005). work involves game setting speaker must aid listener
reaching given destination avoiding obstacles. game played repeatedly find
optimal strategy conveys pertinent information minimizing number
messages. consider different problem setting reinforcements available
strategic generation learner.
addition, work performing strategic generation collective task
(Barzilay & Lapata, 2005). considering strategic generation decisions jointly, captures
dependencies utterances. creates consistent overall output consistent
humans perform task. approach could potentially help system produce
better overall sportscasts.
10.3 Grounded Language Learning
One ambitious end-to-end visually-grounded scene-description system VITRA (Herzog & Wazinski, 1994) comments traffic scenes soccer matches. system first
transforms raw visual data geometrical representations. Next, set rules extract spatial relations interesting motion events representations. Presumed intentions, plans, plan
425

fiC HEN , K IM , & OONEY

interactions agents extracted based domain-specific knowledge. However,
since system hand-coded cannot adapted easily new domains.
Srihari Burhans (1994) used captions accompanying photos help identify people
objects. introduced idea visual semantics, theory extracting visual information
constraints accompanying text. example, using caption information, system
determine spatial relationship entities mentioned, likely size shape
object interest, whether entity natural artificial. However, system based
hand-coded knowledge.
Siskind (1996) performed earliest work learning grounded word meanings.
learning algorithm addresses problem ambiguous training referential uncertainty
semantic lexical acquisition, address larger problems learning complete semantic
parsers language generators.
Several robotics computer vision researchers worked inferring grounded meanings
individual words short referring expressions visual perceptual context (e.g., Roy, 2002;
Bailey et al., 1997; Barnard et al., 2003; Yu & Ballard, 2004). However, complexity
natural language used existing work restrictive, many systems use pre-coded
knowledge language, almost use static images learn language describing objects
relations, cannot learn language describing actions. sophisticated grammatical
formalism used learn syntax work finite-state hidden-Markov model. contrast,
work exploits latest techniques statistical context-free grammars syntax-based statistical
machine translation handle complexities natural language.
recently, Gold Scassellati (2007) built system called TWIG uses existing language knowledge help learn meaning new words. robot uses partial parses focus
attention possible meanings new words. playing game catch, robot able
learn meaning well identity relations.
variety work learning captions accompany pictures
videos (Satoh, Nakamura, & Kanade, 1997; Berg, Berg, Edwards, & Forsyth, 2004). area
particular interest given large amount captioned images video available web
television. Satoh et al. (1997) built system detect faces newscasts. However, use fairly
simple manually-written rules determine entity picture language refers.
Berg et al. (2004) used elaborate learning method cluster faces names. Using
data, estimate likelihood entity appearing picture given context.
recent work video retrieval focused learning recognize events sports videos
connecting English words appearing accompanying closed captions (Fleischman
& Roy, 2007; Gupta & Mooney, 2009). However, work learns connection
individual words video events learn describe events using full grammatical
sentences. avoid difficult problems computer vision, work uses simulated world
perception complex events participants much simpler.
addition observing events passively, work grounded language learning interactive environments computer video games (Gorniak & Roy, 2005).
work, players cooperate communicate order accomplish certain task.
system learns map spoken instructions specific actions; however, relies existing statistical
parsers learn syntax semantics language perceptual environment
alone. Kerr, Cohen, Chang (2008) developed system learns grounded word-meanings
nouns, adjectives, spatial prepositions human instructing perform tasks vir426

fiT RAINING ULTILINGUAL PORTSCASTER

tual world; however, system assumes existing syntactic parser prior knowledge verb
semantics unable learn experience.
Recently, interest learning interpret English instructions describing use particular website perform computer tasks (Branavan et al., 2009; Lau,
Drews, & Nichols, 2009). systems learn predict correct computer action (pressing
button, choosing menu item, typing text field, etc.) corresponding step instructions. Instead using parallel training data perceptual context, systems utilize
direct matches words natural language instructions English words explicitly occurring menu items computer instructions order establish connection
language environment.
One core subproblems work addresses matching sentences facts world
refer. recent projects attempt align text English summaries American
football games database records contain statistics events game (Snyder
& Barzilay, 2007; Liang et al., 2009). However, Snyder Barzilay (2007) use supervised
approach requires annotating correct correspondences text semantic
representations. hand, Liang et al. (2009) developed unsupervised approach
using generative model solve alignment problem. demonstrated improved results
matching sentences events RoboCup English sportscasting data. However, work
address semantic parsing language generation. Section 7 presents results showing
methods improve NLMR matches produced approach well use
learn parsers generators.

11. Future Work
previously discussed, limitations current system due inadequacies
perception events extracted RoboCup simulator. language commentary,
particularly Korean data, refers information events currently represented
extracted MRs. example, player dribbling ball captured perceptual system.
event extractor could extended include information output representations.
Commentaries always immediate actions happening field.
refer statistics game, background information, analysis game.
difficult obtain, would simple augment potential MRs include events
current score number turnovers, etc. may difficult learn correctly,
potentially would make commentaries much natural engaging.
statements commentaries specifically refer pattern activity across several
recent events rather single event. example, one English commentaries,
statement Purple team sloppy today. appears series turn-overs team.
simulated perception could extended extract patterns activity sloppiness;
however assumes concepts predefined, extracting many higher-level
predicates would greatly increase ambiguity training data. current system assumes
already concepts words needs learn perceive concepts represent
MRs. However, would interesting include Whorfian style language
learning (Whorf, 1964) unknown word sloppiness could actually cause
creation new concept. content words seem consistently correlate
perceived event, system could collect examples recent activity word used try
427

fiC HEN , K IM , & OONEY

learn new higher-level concept captures regularity situations. example, given
examples situations referred sloppy, inductive logic programming system (Lavrac &
Dzeroski, 1994) able detect pattern several recent turnovers.
Another shortcoming current system MR treated independently. fails
exploit fact many MRs related other. example, pass preceded kick,
bad pass followed turnover. natural way use graphical representation
represent entities events relationships them.
Currently tactical strategic generation system loosely coupled. However,
conceptually much closely related, solving one problem help solve
other. Initializing system output Liang et al. (2009), uses generative model
includes strategic tactical components, produced somewhat better results. However,
interaction components loose tighter integration different
pieces could yield stronger results tasks.
obvious extension current work apply real RoboCup games rather
simulated ones. Recent work Rozinat, Zickler, Veloso, van der Aalst, McMillen (2008)
analyzes games RoboCup Small Size League using video overhead camera.
using symbolic event trace extracted real perceptual system, methods could
applied real-world games. Using speech recognition accept spoken language input another
obvious extension.
currently exploring extending approach learn interpret generate NL instructions navigating virtual environment. system observe one person giving English
navigation instructions (e.g. Go hall turn left pass chair.) another person follows directions get chosen destination. collecting examples sentences
paired actions executed together information local environment,
system construct ambiguous supervised dataset language learning. approach
could eventually lead virtual agents games educational simulations automatically
learn interpret generate natural language instructions.

12. Conclusion
presented end-to-end system learns generate natural-language sportscasts
simulated RoboCup soccer games training sample human commentaries paired automatically extracted game events. learning semantically interpret generate language without
explicitly annotated training data, demonstrated system learn language simply
observing linguistic descriptions ongoing events. demonstrated systems language
independence successfully training produce sportscasts English Korean.
Dealing ambiguous supervision inherent training environment critical issue
learning language perceptual context. evaluated various methods disambiguating
training data order learn semantic parsers language generators. Using generation
evaluation metric criterion selecting best NLMR pairs produced better results
using semantic parsing scores initial training data noisy. system learns
model strategic generation ambiguous training data estimating probability
event type evokes human commentary. Moreover, using strategic generation information
help disambiguate training data shown improve results. demonstrated
system initialized alignments produced different system achieve better
428

fiT RAINING ULTILINGUAL PORTSCASTER

results either system alone. Finally, experimental evaluation verified overall system
learns accurately parse generate comments generate sportscasts competitive
produced humans.

Acknowledgments
thank Adam Bossy work simulating perception RoboCup games.
thank Percy Liang sharing software experimental results us. Finally, thank
anonymous reviewers JAIR editor, Lillian Lee, insightful comments
helped improve final presentation paper. work funded NSF grant IIS
0712907X. experiments run Mastodon Cluster, provided NSF Grant
EIA-0303609.

Appendix A. Details meaning representation language
Table 10 shows brief explanations different events detect simulated perception.
Event
Playmode
Ballstopped
Turnover
Kick
Pass
BadPass
Defense
Steal
Block

Description
Signifies current play mode defined game
ball speed minimum threshold
current possessor ball last possessor different teams
player possession ball one time interval next
player gains possession ball different player team
pass player gaining possession ball different team
transfer one player opposing player penalty area
player possession ball one time interval another player
different team next time interval
Transfer one player opposing goalie.
Table 10: Description different events detected

include context-free grammar developed meaning representation language. derivations start root symbol *S.

*S
*S
*S
*S
*S
*S
*S
*S
*S

->
->
->
->
->
->
->
->
->

playmode ( *PLAYMODE )
ballstopped
turnover ( *PLAYER , *PLAYER )
kick ( *PLAYER )
pass ( *PLAYER , *PLAYER )
badPass ( *PLAYER , *PLAYER )
defense ( *PLAYER , *PLAYER )
steal ( *PLAYER )
block ( *PLAYER )

429

fiC HEN , K IM , & OONEY

*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYMODE
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER
*PLAYER

->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->
->

->
->
->
->
->
->
->
->
->
->
->
->
->
->
->

kick_off_l
kick_off_r
kick_in_l
kick_in_r
play_on
offside_l
offside_r
free_kick_l
free_kick_r
corner_kick_l
corner_kick_r
goal_kick_l
goal_kick_r
goal_l
goal_r

pink1
pink2
pink3
pink4
pink5
pink6
pink7
pink8
pink9
pink10
pink11
purple1
purple2
purple3
purple4
purple5
purple6
purple7
purple8
purple9
purple10
purple11

430

fiT RAINING ULTILINGUAL PORTSCASTER

References
Aho, A. V., & Ullman, J. D. (1972). Theory Parsing, Translation, Compiling. Prentice
Hall, Englewood Cliffs, NJ.
Andre, E., Binsted, K., Tanaka-Ishii, K., Luke, S., Herzog, G., & Rist, T. (2000). Three RoboCup
simulation league commentator systems. AI Magazine, 21(1), 5766.
Bailey, D., Feldman, J., Narayanan, S., & Lakoff, G. (1997). Modeling embodied lexical development. Proceedings Nineteenth Annual Conference Cognitive Science Society.
Banerjee, S., & Lavie, A. (2005). METEOR: automatic metric MT evaluation improved
correlation human judgments. Proceedings ACL Workshop Intrinsic
Extrinsic Evaluation Measures Machine Translation and/or Summarization, pp. 6572
Ann Arbor, Michigan. Association Computational Linguistics.
Barnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei, D. M., & Jordan, M. I. (2003). Matching
words pictures. Journal Machine Learning Research, 3, 11071135.
Barzilay, R., & Lapata, M. (2005). Collective content selection concept-to-text generation.
Proceedings Human Language Technology Conference Conference Empirical
Methods Natural Language Processing (HLT/EMNLP-05).
Barzilay, R., & Lee, L. (2002). Bootstrapping lexical choice via multiple-sequence alignment.
Proceedings 2002 Conference Empirical Methods Natural Language Processing
(EMNLP-02).
Berg, T. L., Berg, A. C., Edwards, J., & Forsyth, D. A. (2004). Whos picture. Advances
Neural Information Processing Systems 17 (NIPS 2004).
Branavan, S., Chen, H., Zettlemoyer, L. S., & Barzilay, R. (2009). Reinforcement learning
mapping instructions actions. Proceedings Joint conference 47th Annual
Meeting Association Computational Linguistics 4th International Joint
Conference Natural Language Processing Asian Federation Natural Language
Processin (ACL-IJCNLP 2009).
Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J. D., Mercer,
R. L., & Roossin, P. S. (1990). statistical approach machine translation. Computational
Linguistics, 16(2), 7985.
Brown, P. F., Della Pietra, V. J., Della Pietra, S. A., & Mercer, R. L. (1993). mathematics
statistical machine translation: Parameter estimation. Computational Linguistics, 19(2),
263312.
Bunescu, R. C., & Mooney, R. J. (2005). Subsequence kernels relation extraction. Weiss,
Y., Scholkopf, B., & Platt, J. (Eds.), Advances Neural Information Processing Systems 19
(NIPS 2006) Vancouver, BC.
Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language acquisition. Proceedings 25th International Conference Machine Learning (ICML-2008)
Helsinki, Finland.
431

fiC HEN , K IM , & OONEY

Chen, M., Foroughi, E., Heintz, F., Kapetanakis, S., Kostiadis, K., Kummeneje, J., Noda, I., Obst,
O., Riley, P., Steffens, T., Wang, Y., & Yin, X. (2003). Users manual: RoboCup soccer server
manual soccer server version 7.07 later.. Available http://sourceforge.
net/projects/sserver/.
Chiang, D. (2005). hierarchical phrase-based model statistical machine translation. Proceedings 43nd Annual Meeting Association Computational Linguistics (ACL05), pp. 263270 Ann Arbor, MI.
Collins, M. (2002). New ranking algorithms parsing tagging: Kernels discrete structures, voted perceptron. Proceedings 40th Annual Meeting Association
Computational Linguistics (ACL-2002), pp. 263270 Philadelphia, PA.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society B, 39, 138.
Doddington, G. (2002). Automatic evaluation machine translation quality using n-gram cooccurrence statistics. Proceedings ARPA Workshop Human Language Technology,
pp. 128132 San Diego, CA.
Duboue, P. A., & McKeown, K. R. (2003). Statistical acquisition content selection rules
natural language generation. Proceedings 2003 Conference Empirical Methods
Natural Language Processing (EMNLP-03), pp. 121128.
Fleischman, M., & Roy, D. (2007). Situated models meaning sports video retrieval. Proceedings Human Language Technologies: Conference North American Chapter
Association Computational Linguistics (NAACL-HLT-07) Rochester, NY.
Ge, R., & Mooney, R. J. (2005). statistical semantic parser integrates syntax semantics.
Proceedings Ninth Conference Computational Natural Language Learning (CoNLL2005), pp. 916 Ann Arbor, MI.
Ge, R., & Mooney, R. J. (2009). Learning compositional semantic parser using existing syntactic parser. Proceedings Joint conference 47th Annual Meeting Association Computational Linguistics 4th International Joint Conference Natural
Language Processing Asian Federation Natural Language Processin (ACL-IJCNLP
2009).
Gold, K., & Scassellati, B. (2007). robot uses existing vocabulary infer non-visual word
meanings observation. Proceedings Twenty-Second Conference Artificial
Intelligence (AAAI-07).
Gorniak, P., & Roy, D. (2005). Speaking sidekick: Understanding situated speech
computer role playing games. Proceedings 4th Conference Artificial Intelligence
Interactive Digital Entertainment Stanford, CA.
Gupta, S., & Mooney, R. (2009). Using closed captions train activity recognizers improve
video retrieval. Proceedings CVPR-09 Workshop Visual Contextual Learning
Annotated Images Videos (VCL) Miami, FL.
432

fiT RAINING ULTILINGUAL PORTSCASTER

Harnad, S. (1990). symbol grounding problem. Physica D, 42, 335346.
Herzog, G., & Wazinski, P. (1994). VIsual TRAnslator: Linking perceptions natural language
descriptions. Artificial Intelligence Review, 8(2/3), 175187.
Ide, N. A., & Jeronis, J. (1998). Introduction special issue word sense disambiguation:
state art. Computational Linguistics, 24(1), 140.
Joachims, T. (1998). Text categorization support vector machines: Learning many relevant
features. Proceedings Tenth European Conference Machine Learning (ECML98), pp. 137142 Berlin. Springer-Verlag.
Jurcicek, J., Gasic, M., Keizer, S., Mairesse, F., Thomson, B., & Young, S. (2009). Transformationbased learning semantic parsing. Interspeech Brighton, UK.
Kate, R. J., & Mooney, R. J. (2006). Using string-kernels learning semantic parsers. Proceedings 21st International Conference Computational Linguistics 44th Annual
Meeting Association Computational Linguistics (COLING/ACL-06), pp. 913920
Sydney, Australia.
Kate, R. J., & Mooney, R. J. (2007). Learning language semantics ambiguous supervision.
Proceedings Twenty-Second Conference Artificial Intelligence (AAAI-07), pp.
895900 Vancouver, Canada.
Kerr, W., Cohen, P. R., & Chang, Y.-H. (2008). Learning playing wubble world. Proceedings Fourth Artificial Intelligence Interactive Digital Entertainment Conference
(AIIDE) Palo Alto, CA.
Kingsbury, P., Palmer, M., & Marcus, M. (2002). Adding semantic annotation Penn treebank.
Proceedings Human Language Technology Conference San Diego, CA.
Knight, K., & Hatzivassiloglou, V. (1995). Two-level, many-paths generation. Proceedings
33rd Annual Meeting Association Computational Linguistics (ACL-95), pp.
252260 Cambridge, MA.
Lau, T., Drews, C., & Nichols, J. (2009). Interpreting written how-to instructions. Proceedings
Twenty-first International Joint Conference Artificial Intelligence (IJCAI-2009).
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming: Techniques Applications.
Ellis Horwood.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences less supervision. Proceedings Joint conference 47th Annual Meeting Association
Computational Linguistics 4th International Joint Conference Natural Language
Processing Asian Federation Natural Language Processin (ACL-IJCNLP 2009).
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., & Watkins, C. (2002). Text classification
using string kernels. Journal Machine Learning Research, 2, 419444.
433

fiC HEN , K IM , & OONEY

Lu, W., Ng, H. T., Lee, W. S., & Zettlemoyer, L. S. (2008). generative model parsing natural
language meaning representations. Proceedings 2008 Conference Empirical
Methods Natural Language Processing (EMNLP-08) Honolulu, HI.
Marcus, M., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated corpus
English: Penn treebank. Computational Linguistics, 19(2), 313330.
McKeown, K. R. (1985). Discourse strategies generating natural-language text. Artificial Intelligence, 27(1), 141.
Meza-Ruiz, I. V., Riedel, S., & Lemon, O. (2008). Spoken language understanding dialogue
systems, using 2-layer Markov logic network: improving semantic accuracy. Proceedings
Londial.
Mooney, R. J. (2007). Learning semantic parsing. Gelbukh, A. (Ed.), Computational Linguistics Intelligent Text Processing: Proceedings 8th International Conference,
CICLing 2007, Mexico City, pp. 311324. Springer Verlag, Berlin.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment models.
Computational Linguistics, 29(1), 1951.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic evaluation
machine translation. Proceedings 40th Annual Meeting Association
Computational Linguistics (ACL-2002), pp. 311318 Philadelphia, PA.
Riezler, S., Prescher, D., Kuhn, J., & Johnson, M. (2000). Lexicalized stochastic modeling
constraint-based grammars using log-linear measures EM training. Proceedings
38th Annual Meeting Association Computational Linguistics (ACL-2000), pp.
480487 Hong Kong.
Roy, D. (2002). Learning visually grounded words syntax scene description task. Computer Speech Language, 16(3), 353385.
Rozinat, A., Zickler, S., Veloso, M., van der Aalst, W., & McMillen, C. (2008). Analyzing multiagent activity logs using process mining techniques. Proceedings 9th International
Symposium Distributed Autonomous Robotic Systems (DARS-08) Tsukuba, Japan.
Satoh, S., Nakamura, Y., & Kanade, T. (1997). Name-it: Naming detecting faces video
integration image natural language processing. Proceedings Fifteenth
International Joint Conference Artificial Intelligence (IJCAI-97).
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods Pattern Analysis. Cambridge University Press.
Shieber, S. M. (1988). uniform architecture parsing generation. Proceedings
12th International Conference Computational Linguistics (COLING-88), pp. 614619 Budapest, Hungary.
Siskind, J. M. (1996). computational study cross-situational techniques learning word-tomeaning mappings. Cognition, 61(1), 3991.
434

fiT RAINING ULTILINGUAL PORTSCASTER

Snyder, B., & Barzilay, R. (2007). Database-text alignment via structured multilabel classification. Proceedings Twentieth International Joint Conference Artificial Intelligence
(IJCAI-2007).
Srihari, R. K., & Burhans, D. T. (1994). Visual semantics: Extracting visual information
text accompanying pictures. Proceedings Twelfth National Conference Artificial
Intelligence (AAAI-94).
Stolcke, A. (1995). efficient probabilistic context-free parsing algorithm computes prefix
probabilities. Computational Linguistics, 21(2), 165201.
Whorf, B. L. (1964). Language, Thought, Reality: Selected Writings. MIT Press.
Wong, Y., & Mooney, R. J. (2006). Learning semantic parsing statistical machine translation. Proceedings Human Language Technology Conference / North American Chapter
Association Computational Linguistics Annual Meeting (HLT-NAACL-06), pp. 439
446 New York City, NY.
Wong, Y., & Mooney, R. J. (2007). Generation inverting semantic parser uses statistical
machine translation. Proceedings Human Language Technologies: Conference
North American Chapter Association Computational Linguistics (NAACL-HLT07), pp. 172179 Rochester, NY.
Yamada, K., & Knight, K. (2001). syntax-based statistical translation model. Proceedings
39th Annual Meeting Association Computational Linguistics (ACL-2001), pp.
523530 Toulouse, France.
Yu, C., & Ballard, D. H. (2004). integration grounding language learning objects.
Proceedings Nineteenth National Conference Artificial Intelligence (AAAI-04), pp.
488493.
Zaragoza, H., & Li, C.-H. (2005). Learning talk descriptive games. Proceedings
Human Language Technology Conference Conference Empirical Methods
Natural Language Processing (HLT/EMNLP-05), pp. 291298 Vancouver, Canada.
Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods relation extraction. Journal
Machine Learning Research, 3, 10831106.
Zettlemoyer, L. S., & Collins, M. (2005). Learning map sentences logical form: Structured
classification probabilistic categorial grammars. Proceedings 21st Conference
Uncertainty Artificial Intelligence (UAI-2005) Edinburgh, Scotland.
Zettlemoyer, L. S., & Collins, M. (2007). Online learning relaxed CCG grammars parsing
logical form. Proceedings 2007 Joint Conference Empirical Methods Natural
Language Processing Computational Natural Language Learning (EMNLP-CoNLL-07),
pp. 678687 Prague, Czech Republic.

435


