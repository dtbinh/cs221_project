Journal Artificial Intelligence Research 37 (2010) 329-396

Submitted 08/09; published 3/10

Investigation Mathematical Programming
Finite Horizon Decentralized POMDPs
Raghav Aras

raghav.aras@gmail.com

IMS, Suplec Metz
2 rue Edouard Belin, Metz Technopole
57070 Metz - France

Alain Dutech

alain.dutech@loria.fr

MAIA - LORIA/INRIA
Campus Scientifique - BP 239
54506 Vandoeuvre les Nancy - France

Abstract
Decentralized planning uncertain environments complex task generally dealt
using decision-theoretic approach, mainly framework Decentralized Partially Observable Markov Decision Processes (DEC-POMDPs). Although DEC-POMDPS
general powerful modeling tool, solving task overwhelming
complexity doubly exponential. paper, study alternate formulation DEC-POMDPs relying sequence-form representation policies.
formulation, show derive Mixed Integer Linear Programming (MILP) problems
that, solved, give exact optimal solutions DEC-POMDPs. show
MILPs derived either using combinatorial characteristics optimal
solutions DEC-POMDPs using concepts borrowed game theory.
experimental validation classical test problems DEC-POMDP literature,
compare approach existing algorithms. Results show mathematical programming outperforms dynamic programming less efficient forward search, except
particular problems.
main contributions work use mathematical programming DECPOMDPs better understanding DEC-POMDPs solutions. Besides,
argue alternate representation DEC-POMDPs could helpful designing
novel algorithms looking approximate solutions DEC-POMDPs.

1. Introduction
framework Decentralized Partially Observable Markov Decision Processes (DECPOMDPs) used model problem designing system made autonomous
agents need coordinate order achieve joint goal. Solving DEC-POMDPs
untractable task belong class NEXP-complete problems (see Section 1.1).
paper, DEC-POMDPs reformulated sequence-form DEC-POMDPs
derive Mixed Integer Linear Programs solved using efficient solvers
order design exact optimal solutions finite-horizon DEC-POMDPs. main
motivation investigate benefits limits novel approach get
better understanding DEC-POMDPs (see Section 1.2). practical level, provide
new algorithms heuristics solving DEC-POMDPs evaluate classical
problems (see Section 1.3).
c
2010
AI Access Foundation. rights reserved.

fiAras & Dutech

1.1 Context
One main goals Artificial Intelligence build artificial agents exhibit
intelligent behavior. agent entity situated environment perceive
sensors act upon using actuators. concept planning, i.e., select
sequence actions order reach goal, central field Artificial
Intelligence years. notion intelligent behavior difficult assess
measure, prefer refer concept rational behavior formulated Russell
Norvig (1995). consequence, work presented uses decision-theoretic
approach order build agents take optimal actions uncertain partially
unknown environment.
particularly interested cooperative multi-agent systems multiple
independent agents limited perception environment must interact coordinate order achieve joint task. central process full knowledge state
system control agents. contrary, agent autonomous
entity must execute actions itself. setting blessing, agent
ideally deal small part problem, curse, coordination
cooperation harder develop enforce.
decision-theoretic approach rational behavior relies mostly framework
Markov Decision Processes (MDP) (Puterman, 1994). system seen sequence
discrete states stochastic dynamics, particular states giving positive negative
reward. process divided discrete decision periods; number periods
called horizon MDP. periods, action chosen
influence transition process next state. using right actions
influence transition probabilities states, objective controller
system maximize long term return, often additive function reward
earned given horizon. controller knows dynamics system,
made transition function reward function, algorithms derived field
Dynamic Programming (see Bellman, 1957) allow controller compute optimal
deterministic policy, i.e., decision function associates optimal action every
state expected long term return optimal. process called planning
MDP community.
fact, using MDP framework, quite straightforward model problem
one agent full complete knowledge state system. agents,
especially multi-agent setting, generally able determine complete
exact state system noisy, faulty limited sensors
nature problem itself. consequence, different states system observed
similar agent problem different optimal actions taken
states; one speaks perceptual aliasing. extension MDPs called Partially
Observable Markov Decisions Processes (POMDPs) deals explicitly phenomenon
allows single agent compute plans setting provided knows conditional
probabilities observations given state environment (Cassandra, Kaelbling, &
Littman, 1994).
pointed Boutilier (1996), multi-agent problems could solved MDPs
considered centralized point view planning control. Here, although
330

fiMathematical Programming DEC-POMDPs

planning centralized process, interested decentralized settings every
agent executes policy. Even agents could instantly communicate observation, consider problems joint observation resulting communications would still enough identify state system. framework
Decentralized Partially Observable Markov Decision Processes (DEC-POMDP) proposed
Bernstein, Givan, Immerman, Zilberstein (2002) takes account decentralization
control partial observability. DEC-POMDP, looking optimal joint
policies composed one policy agent, individual policies
computed centralized way independently executed agents.
main limitation DEC-POMDPs provably untractable
belong class NEXP-complete problems (Bernstein et al., 2002). Concretely,
complexity result implies that, worst case, finding optimal joint policy finite
horizon DEC-POMDP requires time exponential horizon one always make
good choices. complexity, algorithms finding exact
optimal solutions DEC-POMDPs (they doubly exponential complexity)
look approximate solutions. discussed detailed
work Oliehoek, Spaan, Vlassis (2008), algorithms follow either dynamic
programming approach forward search approach adapting concepts algorithms
designed POMDPs.
Yet, concept decentralized planning focus quite large body
previous work fields research. example, Team Decision Problem (Radner,
1959), later formulated Markov system field control theory Anderson
Moore (1980), led Markov Team Decision Problem (Pynadath & Tambe, 2002).
field mathematics, abundant literature Game Theory brings new way
looking multi-agent planning. particular, DEC-POMDP finite horizon
thought game extensive form imperfect information identical interests
(Osborne & Rubinstein, 1994).
Taking inspiration field game theory mathematical programming design exact algorithms solving DEC-POMDPs precisely subject contribution
field decentralized multi-agent planning.
1.2 Motivations
main objective work investigate use mathematical programming,
especially mixed-integer linear programs (MILP) (Diwekar, 2008), solving DECPOMDPs. motivation relies fact field linear programming quite
mature great interest industry. consequence, exist many efficient
solvers mixed-integer linear programs want see efficient solvers
perform framework DEC-POMDPs.
Therefore, reformulate DEC-POMDP solve mixed-integer linear
program. shown article, two paths lead mathematical programs, one
grounded work Koller, Megiddo, von Stengel (1994), Koller Megiddo
(1996) von Stengel (2002), another one grounded combinatorial considerations.
methods rely special reformulation DEC-POMDPs called
331

fiAras & Dutech

sequence-form DEC-POMDPs policy defined histories (i.e., sequences
observations actions) generate applied DEC-POMDP.
basic idea work select, among histories DEC-POMDP,
histories part optimal policy. end, optimal solution
MILP presented article assign positive weight history DECPOMDP every history non-negative weight part optimal policy
DEC-POMDP. number possible histories exponential horizon
problem, complexity naive search optimal set histories doubly
exponential. Therefore, idea appears untractable useless.
Nevertheless, show combining efficiency MILP solvers quite
simple heuristics leads exact algorithms compare quite well existing exact
algorithms. fact, sequence-form DEC-POMDPs need memory space exponential
size problem. Even solving MILPs exponential size
MILP thus leads doubly exponential complexity sequence-form based algorithms,
argue sequence-form MILPs compare quite well dynamic programming thanks
optimized industrial MILP solvers Cplex.
Still, investigations experiments Mathematical Programming DECPOMDPs solely aim finding exact solutions DEC-POMDPs. main motivation better understanding DEC-POMDPs limits benefits
mathematical programming approach. hope knowledge help deciding
extent mathematical programming sequence-form DEC-POMDPs used
design novel algorithms look approximate solutions DEC-POMDPs.
1.3 Contributions
paper develop new algorithms order find exact optimal joint policies
DEC-POMDPs. main inspiration comes work Koller, von Stegel
Megiddo shows solve games extensive form imperfect information
identical interests, find Nash equilibrium kind game (Koller et al.,
1994; Koller & Megiddo, 1996; von Stengel, 2002). algorithms caused breakthrough
memory space requirement approach linear size game whereas
canonical algorithms required space exponential size game.
breakthrough mostly due use new formulation policy call
sequence-form.
main contribution, detailed Section 3.3, adapt sequence-form
introduced Koller, von Stegel Megiddo framework DEC-POMDPs (Koller
et al., 1994; Koller & Megiddo, 1996; von Stengel, 2002). result, possible
formulate resolution DEC-POMDP special kind mathematical program
still solved quite efficiently: mixed linear program variables
required either 0 1. adaptation resulting mixed-integer linear
program straightforward. fact, Koller, von Stegel Megiddo could find
one Nash equilibrium 2-agent game. needed DEC-POMDPs find
set policies, called joint policy, corresponds Nash equilibrium
highest value, finding one Nash equilibrium already complex task
enough. Besides, whereas Koller, von Stegel Megiddo algorithms could applied
332

fiMathematical Programming DEC-POMDPs

2-agent games, extend approach solve DEC-POMDPs arbitrary
number agents, constitutes important contribution.
order formulate DEC-POMDPs MILPs, analyze detail structure
optimal joint policy DEC-POMDP. joint policy sequence-form expressed
set individual policies described set possible trajectories
agents DEC-POMDP. Combinatorial considerations individual
histories, well constraints ensure histories define valid joint policy
heart formulation DEC-POMDP mixed linear program, developped
Sections 4 5. Thus, another contribution work better understanding
properties optimal solutions DEC-POMDPs, knowledge might lead
formulation new approximate algorithms DEC-POMDPs.
Another important contribution work introduce heuristics boosting performance mathematical programs propose (see Section 6).
heuristics take advantage succinctness DEC-POMDP model knowledge acquired regarding structure optimal policies. Consequently, able
reduce size mathematical programs (resulting reducing time taken
solve them). heuristics constitute important pre-processing step solving
programs. present two types heuristics: elimination extraneous histories
reduces size mixed integer linear programs introduction cuts
mixed integer linear programs reduces time taken solve program.
practical level, article presents three different mixed integer linear
programs, two directly derived work Koller, von Stegel Megiddo
(see Table 4 5) third one based solely combinatorial considerations
individual policies histories (see Table 3). theoretical validity formulations backed several theorems. conducted experimental evaluations
algorithms heuristics several classical DEC-POMDP problems. thus
able confirm algorithms quite comparable dynamic programming exact
algorithms outperformed forward search algorithms GMAA* (Oliehoek et al.,
2008). problems, though, MILPs indeed faster one order magnitude
two GMAA*.
1.4 Overview Article
remainder article organized follows. Section 2 introduces formalism
DEC-POMDP background classical algorithms, usually based dynamic
programing. expose reformulation DEC-POMDP sequence-form
Section 3 define various notions needed sequence-form. Section 4,
show use combinatorial properties sequence-form policies derive first
mixed integer linear program (MILP, Table 3) solving DEC-POMDP. using game
theoretic concepts Nash equilibrium, take inspiration previous work games
extensive form design two MILPs solving DEC-POMDP (Tables 4, 5).
MILPs smaller size detailed derivation presented Section 5.
contributed heuristics speed practical resolutions various MILPs make
core Section 6. Section 7 presents experimental validations MILP-based
algorithms classical benchmarks DEC-POMDP literature well randomly
333

fiAras & Dutech

built problems. Finally, Section 8 analyzes discusses work conclude
paper Section 9.

2. Dec-POMDP
section gives formal definition Decentralized Partially Observed Markov Decision
Processes introduced Bernstein et al. (2002). described, solution DECPOMDP policy defined space information sets optimal value.
sections ends quick overview classical methods developed
solve DEC-POMDPs.
2.1 Formal Definition
DEC-POMDP defined tuple = h I, S, {Ai }, P, {Oi }, G, R, , where:
= {1, 2, , n} set agents.
finite set states. set probability distributions shall denoted
(S). Members (S) shall called belief states.
agent I, Ai set actions. = iI Ai denotes set joint
actions.
P : [0, 1] state transition function. s,
A, P(s, a, ) probability state problem period if,
period 1, state agents performed joint action a. Thus,
time period 2, pair states s, joint action A,
holds:
P(s, a, ) = Pr(st = |st1 = s, = a).
Thus, (S, A, P) defines discrete-state, discrete-time controlled Markov process.
agent I, Oi set observations. = iI Oi denotes set joint
observations.
G : [0, 1] joint observation function. A,
S, G(a, s, o) probability agents receive
joint observation (that is, agent receives observation oi ) state
problem period previous period agents took joint
action a. Thus, time period 2, joint action A, state
joint observation O, holds:
G(a, s, o) = Pr(ot = o|st = s, at1 = a).
R : R reward function. A, R(s, a) R
reward obtained agents take joint action state
process s.
334

fiMathematical Programming DEC-POMDPs

horizon problem. agents allowed joint-actions
process halts.
(S) initial state DEC-POMDP. S, (s) denotes
probability state problem first period s.
said, S, P define controlled Markov Process next state depends
previous state joint action chosen agents. agents
access state process rely observations, generally
partial noisy, state, specified observation function G. time
time, agents receive non-zero reward according reward function R.

n0

n1

10

11

s0

s1

nt

n1

1t

11

s2

nt

1t

st

Figure 1: DEC-POMDP. every period process, environment state
st , every agent receives observations oti decides action ati . joint
action hat1 , at2 , , atn alters state process.
specifically, illustrated Figure 1, control DEC-POMDP n
agents unfolds discrete time periods, = 1, 2, ,T follows. period t,
process state denoted st S. first period = 1, state s1 chosen
according agents take actions a1i . period > 1 afterward, agent
takes action denoted ati Ai according agents policy.
agents take joint action = hat1 , at2 , , atn i, following events occur:
1. agents obtain reward R(st , ).
2. state st+1 determined according function P arguments st .
3. agent receives observation ot+1
Oi . joint observation ot+1 =

t+1
t+1
t+1 .
hot+1
1 , o2 , , determined function G arguments
4. period changes + 1.
paper, DEC-POMDP interested following properties:
335

fiAras & Dutech

horizon finite known agents;
agents cannot infer exact state system joint observations (this
general setting DEC-POMDPs);
agents observe actions observations agents.
aware observations reward;
agents perfect memory past; base choice action
sequence past actions observations. speak perfect recall setting;
transition observation functions stationary, meaning depend
period t.
Solving DEC-POMDP means finding agents policies (i.e., decision functions)
optimize given criterion based rewards received. criterion work
called cumulative reward defined by:
"
#
X




E
R(s , ha1 , a2 , . . . , i)
(1)
t=1

E mathematical expectation.
2.2 Example DEC-POMDP
problem known Decentralized Tiger Problem (hereby denoted MA-Tiger),
introduced Nair, Tambe, Yokoo, Pynadath, Marsella (2003), widely used
test DEC-POMDPs algorithms. variation problem previously introduced
POMDPs (i.e., DEC-POMDPs one agent) Kaelbling, Littman, Cassandra
(1998).
problem, given two agents confronted two closed doors. Behind one
door tiger, behind escape route. agents know door
leads what. agent, independently other, open one two doors
listen carefully order detect tiger. either opens wrong door,
lives imperiled. open escape door, free.
agents limited time decide door open. use time
gather information precise location tiger listening carefully detect
location tiger. problem formalized DEC-POMDP with:
two states tiger either behind left door (sl ) right door (sr );
two agents, must decide act;
three actions agent: open left door (al ), open right door (ar )
listen (ao );
two observations, thing agent observe hear tiger
left (ol ) right (or ).
336

fiMathematical Programming DEC-POMDPs

initial state chosen according uniform distribution S. long door
remains closed, state change but, one door opened, state reset
either sl sr equal probability. observations noisy, reflecting difficulty
detecting tiger. example, tiger left, action ao produces
observation ol 85% time. agents perform ao , joint observation
(ol ,ol ) occurs probability 0.85 0.85 = 0.72. reward function encourages
agents coordinate actions as, example, reward open escape
door (+20) bigger one listens opens good door (+9).
full state transition function, joint observation function reward function described
work Nair et al. (2003).
2.3 Information Sets Histories
information set agent sequence (a1 .o2 .a2 .o3 .ot ) even length
elements odd positions actions agent (members Ai ) even
positions observations agent (members Oi ). information set length 0
shall called null information set, denoted . information set length 1
shall called terminal information set. set information sets lengths less
equal 1 shall denoted .
define history agent sequence (a1 .o2 . a2 . o3 .ot .at ) odd
length elements odd positions actions agent (members Ai )
even positions observations agent (members Oi ). define length
history number actions history (t example). history
length shall called terminal history. Histories lengths less shall called
non-terminal histories. history null length shall denoted . information
set associated history h, denoted (h), information set composed removing
h last action. h history observation, h.o information set.
shall denote Hit set possible histories length agent i. Thus, Hi1
set actions Ai . shall denote Hi set histories agent lengths
less equal . size ni Hi thus:
ni = |Hi | =

PT


t1
t=1 |Ai | |Oi |

= |Ai |

(|Ai ||Oi |)T 1
.
|Ai ||Oi | 1

(2)

set HiT terminal histories agent shall denoted Ei . set Hi \HiT
non-terminal histories agent shall denoted Ni .
tuple hh1 , h2 , . . . , hn made one history agent called joint history.
tuple obtained removing history hi joint history h noted hi called
i-reduced joint history.
Example Coming back MA-Tiger example, set valid histories could be: , (ao ),
(ao .ol .ao ), (ao .or .ao ), (ao .ol .ao .ol .ao ), (ao .ol .ao .or .ar ), (ao .or .ao .ol .ao ) (ao .or .ao .or .ar ).
Incidently, set histories corresponds support policy (i.e., histories
generated using policy) Figure 2, explained next section.
337

fiAras & Dutech

2.4 Policies
period time, policy must tell agent action choose. choice
based whatever past present knowledge agent process
time t. One possibility define individual policy agent mapping
information sets actions. formally:
: (Ai )

(3)

Among set policies, three families usually distinguished:
Pure policies. pure deterministic policy maps given information set one
unique action. set pure policies agent denoted . Pure policies
could defined using trajectories past observations since actions,
chosen deterministically, reconstructed observations.
Mixed policies. mixed policy probability distribution set pure
policies. Thus, agent using mixed policy control DEC-POMDP using
pure policy randomly chosen set pure policies.
Stochastic policies. stochastic policy general formulation associates
probability distribution actions history.
come back MA-Tiger problem (Section 2.2), Figure 2 gives possible policy
horizon 2. shown, policy classically represented action-observation tree.
kind tree, branch labelled observation. given sequence past
observations, one starts root node follows branches action
node. node contains action executed agent seen
sequence observations.
Observation sequence
Chosen action

ol
ao


ao


ao

ol .ol
al

ol .or
ao

.ol
ao

.or
ar

ao
ol



ao

ao

ol



ol



al

ao

ao

ar

Figure 2: Pure policy MA-Tiger. pure policy maps sequences observations
actions. represented action-observation tree.

joint policy = h1 , 2 , , n n-tuple policy agent i.
individual policies must horizon. agent i, define
notion i-reduced joint policy = h1 , , i1 , i+1 , , n composed
policies agents. thus = hi , i.
338

fiMathematical Programming DEC-POMDPs

2.5 Value Function
executed agents, every -horizon joint policy generates probability distribution possible sequences reward one compute value
policy according Equation 1. Thus value joint policy formally defined as:
V (, ) = E

"
X

R(st , )|,

t=1

#

(4)

given state first period chosen according actions chosen
according .
recursive definition value function policy way
compute horizon finite. definition requires concepts
shall introduce.
Given belief state (S), joint action joint observation O,
let (o|, a) denote probability agents receive joint observation take
joint action period state chosen according . probability
defined
X
X
(o|, a) =
(s)
P(s, a, )G(a, , o)
(5)


sS

Given belief state (S), joint action joint observation ,
updated belief state ao (S) respect defined (for
S),
ao (s ) =

P
G(a,s ,o)[ sS (s)P(s,a,s )]
(o|,a)

(o|, a) > 0

(6)

ao (s ) =

0

(o|, a) = 0

(7)

P
Given belief state (S) joint action A, R(, a) denotes sS (s)R(s, a).
Using definitions notations, value V (, ) defined follows:
V (, ) = V (, , )

(8)

V (, , ) defined recursion using equations (9), (10) (11), given below.
equations straight reformulation classical Bellman equations finite
horizon problems.
histories null length
V (, , ) = R(, ()) +

X

(o|, ())V (()o , , o)

(9)

oO

() denotes joint action h1 (), 2 (), , n ()i ()o denotes
updated state given () joint observation o.
339

fiAras & Dutech

non-terminal histories. (S), {1, . . . , 2},
1:T
1:T
1:T
tuple sequences observations o1:T = ho1:T

1 , o2 , , oi
sequence observations agent I:
V ( , , o1:T ) = R( , (o1:T )) +

X

1:T )o

(o| , (o1:T ))V ((o

, , o1:T .o) (10)

oO
1:T

(o )o updated state given joint action (o1:T ) joint observation
= ho1 , o2 , , o1:T .o tuple sequences (t + 1) observations ho1:T
1 .o1 ,
1:T .o i.
o1:T
.o
,



,

n
2
n
2
terminal histories. (S), tuple sequences (T - 1)
1
1
1 i:
observations o1:T 1 = ho1:T
, o1:T
, , o1:T
n
1
2
V ( , , o1:T 1 ) = R(, (o1:T 1 )) =

X

(s)R(s, (o1:T 1 ))

(11)

sS

optimal policy policy best possible value, verifying:
V (, ) V (, )

.

(12)

important fact DEC-POMDPs, based following theorem,
restrict set pure policies looking solution DEC-POMDP.
Theorem 2.1. DEC-POMDP least one optimal pure joint policy.
Proof: See proof work Nair et al. (2003).



2.6 Overview DEC-POMDPs Solutions Limitations
detailed work Oliehoek et al. (2008), existing methods solving DECPOMDPs finite-horizon belong several broad families: brute force, alternating
maximization, search algorithms dynamic programming.
Brute Force simplest approach solving DEC-POMDP enumerate
possible joint policies evaluate order find optimal one. However,
method becomes quickly untractable number joint policies doubly exponential
horizon problem.
Alternating Maximization Following Chades, Scherrer, Charpillet (2002) Nair
et al. (2003), one possible way solve DEC-POMDPs agent (or small group
agents) alternatively search better policy agents freeze
policy. Called alternating maximization Oliehoek alternated co-evolution
Chades method guarantees find Nash equilibria, locally optimal joint
policy.
340

fiMathematical Programming DEC-POMDPs

Heuristic Search Algorithms concept introduced Szer, Charpillet,
Zilberstein (2005) relies heuristic search looking optimal joint policy,
using admissible approximation value optimal joint policy. search
progresses, joint policies provably worse current admissible solution
pruned. Szer et al. used underlying MDPs POMDPs compute admissible heuristic,
Oliehoek et al. (2008) introduced better heuristic based resolution Bayesian
Game carefully crafted cost function. Currently, Oliehoeks method called GMAA*
(for Generic Multi-Agent A*) quickest exact method large set benchmarks.
But, every exact method, limited quite simple problems.
Dynamic Programming work Hansen, Bernstein, Zilberstein (2004)
adapts solutions designed POMDPs domain DEC-POMDPs. general
idea start policies 1 time step used build 2 time step policies
on. process clearly less efficient heuristic search approach
exponential number policies must constructed evaluated iteration
algorithm. policies pruned but, again, pruning less efficient.
exposed details paper Oliehoek et al. (2008), several others approaches developed subclasses DEC-POMDPs. example, special settings agents allowed communicate exchange informations settings
transition function split independant transition functions agent
studied found easier solve generic DEC-POMDPs.

3. Sequence-Form DEC-POMDPs
section introduces fundamental concept policies sequence-form. new
formulation DEC-POMDP thus possible leads Non-Linear Program
(NLP) solution defines optimal solution DEC-POMDP.
3.1 Policies Sequence-Form
history function p agent mapping set histories interval
[0, 1]. value p(h) weight history h history function p. policy
defines probability function set histories agent saying that,
history hi Hi , p(hi ) conditional probability hi given observation sequence
(o0i .o1i . .oti ) .
every policy defines policy function, every policy function associated
valid policy. constraints must met. fact, history function p sequenceform policy agent following constraints met:
X

p(a) = 1,

(13)

aAi

p(h) +

X

p(h.o.a) = 0,

h Ni , Oi ,

(14)

aAi

h.o.a denotes history obtained concatenating h. definition
appears slightly different form Lemma 5.1 work Koller et al. (1994).
341

fiAras & Dutech

Variables: x(h), h Hi ,
X

x(a) = 1

(15)

aAi

x(h) +

X

x(h.o.a) = 0,

h Ni , Oi

(16)

h Hi

(17)

aAi

x(h) 0,

Table 1: Policy Constraints. set linear inequalities, solved, provide valid
sequence-form policy agent i. is, weights x(h), possible
define policy agent i.

sequence-form policy stochastic probability choosing action
information set h.o p(h.o.a)/p(h). support S(p) sequence-form policy made
set histories non-negative weight, i.e. S(p) = {h Hi | p(h) > 0}.
sequence-form policy p defines unique policy agent, sequence-form policy
called policy rest paper ambiguity present.
set policies sequence-form agent shall denoted Xi . set
pure policies sequence-form shall denoted Xi Xi .
way similar definitions Section 2.4, define sequence-form joint
policy tuple sequence-form policies, one agent. weight Q
joint
history h = hhi sequence-form joint policy hp1 , p2 , , pn product iI pi (hi ).
set joint policies sequence-form iI Xi shall denoted X set
i-reduced sequence-form joint policy called Xi .
3.2 Policy Constraints
policy agent sequence-form found solving set linear inequalities
(LI) found Table 1. LI merely implement definition policy sequenceform. LI contains one variable x(h) history h Hi represent weight
h policy. solution x LI constitutes policy sequence-form.
Example Section E.1 Appendices, policy constraints decentralized
Tiger problem given 2 agents horizon 2.
Notice policy constraints agent, variable constrained
non-negative whereas definition policy sequence-form, weight history
must interval [0, 1]. mean variable solution policy
constraints assume value higher 1? Actually, policy constraints
prevent variable assuming value higher 1 following lemma
shows.
Lemma 3.1. every solution x (15)-(17), h Hi , x (h) belongs [0, 1]
interval.
342

fiMathematical Programming DEC-POMDPs

Proof: shown forward induction.
Every x(h) non-negative (see Eq. (17)), case every action
Ai . Then, x(a) greater 1 otherwise constraint (15) would violated. So,
h Hi1 , (i.e. Ai ), x(h) belong [0, 1].
every h Hit x(h) [0, 1], previous reasoning applied using constraint
(16) leads evidently fact x(h) [0, 1] every h Hit+1 .
Thereby, induction holds t.

Later article, order simplify task looking joint policies, policy
constraints LI used find pure policies. Looking pure policies limitation
finite-horizon DEC-POMDPs admit deterministic policies policies defined
information set. fact, pure policies needed two three MILPs build
order solve DEC-POMDPs, otherwise derivation would possible (see
Sections 4 5.4).
Looking pure policies, obvious solution would impose every variable
x(h) belongs set {0, 1}. But, solving mixed integer linear program,
generally good idea limit number integer variables integer variable
possible node branch bound method used assign integer values
variables. efficient implementation mixed integer linear program take
advantage following lemma impose weights terminal histories
take 0 1 possible values.
Lemma 3.2. (15)-(17), (17) replaced by,
x(h) 0,

h Ni

x(h) {0, 1},

h Ei

(18)
(19)

every solution x resulting LI, h Hi , x (h) = 0 1.
speak 0-1 LI.
Proof: prove backward induction. Let h history length - 1.
Due (16), Oi , holds,
X
x (h) =
x (h.o.a).
(20)
aAi

Since h history length - 1, history h.o.a terminal history. Due Lemma
3.1, x (h) [0, 1]. Therefore, sum right hand side equation
[0, 1]. due (19), x (h.o.a) {0, 1}. Hence sum right hand side
either 0 1, value between. Ergo, x (h) {0, 1} value
between. reasoning, show x (h) {0, 1} every non-terminal
history h length - 2, - 3, , 1.

formulate linear inequalities Table 1 memory, require
P space
exponential horizon. agent I, size Hi Tt=1 |Ai |t |Oi |t1 .
exponential number variables
LP exponential .
PT 1in

number constraints LI Table 1 t=0 |Ai | |Oi |t , meaning number
constraints LI exponential .
343

fiAras & Dutech

3.3 Sequence-Form DEC-POMDP
able give formulation DEC-POMDP based use sequence-form
policies. want stress re-formulation, provide us
new ways solving DEC-POMDPs mathematical programming.
Given classical formulation DEC-POMDP (see Section 2.1), equivalent
sequence-form DEC-POMDP tuple hI, {Hi }, , Ri where:
= {1, 2, , n} set agents.
agent I, Hi set histories length less equal
agent i, defined previous section. set Hi derived using sets
Ai Oi .
joint history conditional probability function. joint history j H,
(, j) probability j occurring conditional agents taking joint actions
according given initial state DEC-POMDP . function
derived using set states S, state transition function P joint
observation function G.
R joint history value. joint history j H, R(, j) value
expected reward agents obtain joint history j occurs. function
derived using set states S, state transition function P, joint observation
function G reward function R. Alternatively, R described function
R.
formulation folds S, P G R relying set histories.
give details computation R.
(, j) conditional probability sequence joint observations received
agents till period (o1 (j).o2 (j). . ot1 (j)) sequence joint actions
taken till period - 1 (a1 (j). a2 (j). . at1 (j)) initial state
DEC-POMDP . is,
(, j) = Prob.(o1 (j).o2 (j). .ot1 (j)|, a1 (j).a2 (j). .at1 (j))

(21)

probability product probabilities seeing observation ok (j) given
appropriate belief state action chosen time k, is:
(, j) =

t1


(ok (j)|jk1 , ak (j))

(22)

k=1

jk probability distribution given agents followed joint
history j time k, is:
jk (s) = Prob.(s|o1 (j).a1 (j). .ok (j)).
344

(23)

fiMathematical Programming DEC-POMDPs

Variables: xi (h), I, h Hi
Maximize

X

R(, j)

jE



xi (ji )

(27)

iI

subject
X

xi (a) = 1,



(28)

I, h Ni , Oi

(29)

I, h Hi

(30)

aAi

xi (h) +

X

xi (h.o.a) = 0,

aAi

xi (h) 0,

Table 2: NLP. non-linear program expresses constraints finding sequenceform joint policy optimal solution DEC-POMDP.

Regarding value joint history, defined by:
R(, j) = R(, j)(, j)

(24)


R(, j) =

X
X

jk1 (s)R(s, ak (j)).

(25)

k=1 sS

Thus, V(, p), value sequence-form joint policy p, weighted sum
value histories support:
X
V(, p) =
p(j)R(, j)
(26)
jH

p(j) =

Q

iI

pi (ji ).

3.4 Non-Linear Program Solving DEC-POMDPs.
using sequence-form formulation DEC-POMDP, able express joint
policies sets linear constraints assess value every policy. Solving DECPOMDP amounts finding policy maximal value, done
non-linear program (NLP) Table 2 where, again, xi variables weights
histories agent i.
Example example formulation NLP found Appendices,
Section E.2. given decentralized Tiger problem 2 agents horizon
2.
constraints program form convex set, objective function
concave (as explained appendix A). general case, solving non-linear program
345

fiAras & Dutech

difficult generalized method guarantee finding global maximum
point. However, particular NLP fact Multilinear Mathematical Program (see
Drenick, 1992) kind programs still difficult solve. two
agents considered, one speaks bilinear programs, solved easily
(Petrik & Zilberstein, 2009; Horst & Tuy, 2003).
evident, inefficient, method find global maximum point evaluate
extreme points set feasible solutions program since known every
global well local maximum point non-concave function lies extreme point
set (Fletcher, 1987). inefficient method test tells
extreme point local maximum point global maximum point. Hence, unless
extreme points evaluated, cannot sure obtained global maximum
point. set feasible solutions NLP X, set -step joint policies.
set extreme points set X, set pure -step joint policies, whose number
doubly exponential exponential n. enumerating extreme points
NLP untractable.
approach, developed next sections, linearize objective function
NLP order deal linear programs. describe two ways
this: one based combinatorial consideration (Section 4) based
game theory concepts (Section 5). cases, shall mean adding variables
constraints NLP, upon so, shall derive mixed integer linear programs
possible find global maximum point hence optimal joint policy
DEC-POMDP.

4. Combinatorial Considerations Mathematical Programming
section explains possible use combinatorial properties DEC-POMDPs
transform previous NLP mixed integer linear program. shown, mathematical program belongs family 0-1 Mixed Integer Linear Programs, meaning
variables linear program must take integer values set {0, 1}.
4.1 Linearization Objective Function
Borrowing ideas field Quadratic Assignment Problems (Papadimitriou & Steiglitz, 1982), turn non-linear objective function previous NLP linear
objective function linear constraints involving new variables z must take integer
values. variable z(j) represents product xi (ji ) variables.
Thus, objective function was:
X

maximize
R(, j)
xi (ji )
(31)
jE

iI

rewritten
maximize

X

R(, j)z(j)

jE

j = hj1 , j2 , , jn i.
346

(32)

fiMathematical Programming DEC-POMDPs

must ensure two way mapping value new variables
z x variables solution mathematical program, is:

z (j) =
xi (ji ).
(33)
iI

this, restrict ourself pure policies x variables 0 1.
case, previous constraint (33) becomes:
z (j) = 1 xi (ji ) = 1,



(34)

There, take advantage fact support pure policy agent
composed |Oi |T 1 terminal histories express new constraints. one hand,
guarantee z(j) equal 1 enough x variables equal 1,
write:
n
X

xi (ji ) nz(j) 0,

j E.

(35)

i=1

hand, limit number z(j) variables take value 1,
enumerate number joint terminal histories end with:

X
|Oi |T 1 .
(36)
z(j) =
iI

jE

constraints (35) would weight heavily mathematical program would
one constraint terminal joint history, number exponential n
. idea reduce number constraints reason joint histories
individual histories. history h agent part support solution
ofPthe problem (i.e., xQ
(h) = 1) number joint histories
Q belongs
( j Ei z(hh, j i)) kI\{i} |Ok |T 1 . Then, suggest replace |Ei | constraints
(35)
n
X

xi (ji ) nz(j) 0,

j E.

(35)

i=1



P

|Ei | constraints
X



|Ok |T 1
xi (h)
|Oi |T 1

= xi (h)
|Ok |T 1 ,

z(hh, j i) =

j Ei

Q

kI

I, h Ei .

(37)

kI\{i}

4.2 Fewer Integer Variables
linearization objective function rely fact dealing pure
policies, meaning every x z variable supposed value either 0 1. solving
linear programs integer variables usually based branch bound technique
347

fiAras & Dutech

Variables:
xi (h), I, h Hi
z(j), j E
X

Maximize

R(, j)z(j)

(38)

jE

subject to:
X

xi (a) = 1,



(39)

I, h Ni , Oi

(40)

aAi

xi (h) +

X

xi (h.o.a) = 0,

aAi

X



z(hh, j i) = xi (h)


j Hi

|Ok |T 1 ,

I, h Ei

(41)

kI\{i}

X

z(j) =

jE



|Oi |T 1

(42)

iI

xi (h) 0,

I, h Ni

xi (h) {0, 1},

(43)

I, h Ei

z(j) [0, 1],

(44)

j E

(45)

Table 3: MILP. 0-1 mixed integer linear program finds sequence-form joint policy
optimal solution DEC-POMDP.

(Fletcher, 1987), efficiency reasons, important reduce number integer
variables mathematical programs.
done Section 3.2, relax x variables allow take non-negative
values provided x values terminal histories constrained integer values.
Furthermore, proved following lemma, constraints x guarantee
z variables take value {0, 1}.
eventually end following linear program real integer variables,
thus called 0-1 mixed integer linear program (MILP). MILP shown Table 3.
Example Section E.3 Appendices, example MILP given
problem decentralized Tiger 2 agents horizon 2.
Lemma 4.1. every solution (x , z ) MILP Table 3, j E, z (j)
either 0 1.
Proof: Let (x , z ) solution MILP. Let,
S(z) = {j E|z (j) > 0}
Si (xi ) = {h


Ei |xi (h)

= 1},




(46)


Si (z, j ) = {j E|ji = j , z (j) > 0},
348

(47)
I,

j

Ei

(48)

fiMathematical Programming DEC-POMDPs

Q
Q
1 . showing |S(z)|
1 ,
Now, due (42) (45), |S(z)|
i|
iI |Oi |
Q iIT|O
1
shall establish |S(z)| = iI |Oi |
. due upper bound 1 z

variable, implication z (j) 0 1 terminal joint history j thus
proving statement lemma.
Note Lemma (3.2), agent i, xi pure policy. Therefore,
|Si (x)| = |Oi |T 1 . means set constraints (41), i-reduced terminal
joint history j Ei appear right hand side |Oi |T 1 times
left hand side, xi (h) = 1. Thus, j Ei ,
|Si (z, j )| |Oi |T 1 .

(49)

(h) either 0 1 since
Now, know agent history h Hi , xi Q
xi pure policy. So, given i-reduced terminal joint history j , kI\{i} xk (jk ) either
0 1. Secondly, due (41), following implication clearly holds terminal joint
history j,
z (j) > 0 xi (ji ) = 1,

I.

(50)

Therefore, obtain
|Si (z, j )| |Oi |T 1

(51)


1

= |Oi |

xk (jk ).

(52)

kI\{i}

consequence,
X

X

|Si (z, j )|

j Ei



|Oi |T 1

j Ei

xk (jk )

(53)

xk (jk )

(54)

xk (h )

(55)

kI\{i}

= |Oi |T 1

X



j Ei kI\{i}

= |Oi |T 1



X

kI\{i} h Ek

= |Oi |T 1



|Ok |T 1

(56)

kI\{i}

=



1

|Oj |

.

(57)

jI

Since



j Ei

Si (z, j ) = S(z), holds

P



|S(z)|

j Ei

|Si (z, j )| = |S(z)|. Hence,

|Oj |T 1 .

(58)

jI

Thus statement lemma proved.
349



fiAras & Dutech

4.3 Summary
using combinatorial considerations, possible design 0-1 MILP solving given
DEC-POMDP. proved theorem 4.1, solution MILP defines optimal joint
policy
Nevertheless, MILP quite large, O(kT ) constraints
Pfor DEC-POMDP.
Q
nT
|Hi | + |Ei | = O(k ) variables, O(kT ) variables must take integer values.
next section details another method linearization NLP leads
smaller mathematical program 2-agent case.
Theorem 4.1. Given solution (x , z ) MILP, x = hx1 , x2 , , xn pure
-period optimal joint policy sequence-form.
Proof: Due policy constraints domain constraints agent, xi

pure sequence-form policy
Q agent i. Due constraints (41)-(42), z values 1
product iI xi (ji ) values 1. Then, maximizing objective function
effectively maximizing value sequence-form policy hx1 , x2 , , xn i. Thus,

hx1 , x2 , , xn optimal joint policy original DEC-POMDP.

5. Game-Theoretical Considerations Mathematical
Programming
section borrows concepts Nash equilibrium regret game theory
order design yet another 0-1 Mixed Integer Linear Program solving DEC-POMDPs.
fact, two MILPs designed, one applied 2 agents
one number agents. main objective part derive smaller
mathematical program 2 agent case. Indeed, MILP-2 agents (see Table 4)
slightly less variables constraints MILP (see Table 3) thus might prove easier
solve. hand, 2 agents considered, new derivation
leads MILP given completeness bigger MILP.
Links fields multiagent systems game theory numerous
literature (see, example, Sandholm, 1999; Parsons & Wooldridge, 2002). elaborate fact optimal policy DEC-POMDP Nash Equilibrium.
fact Nash Equilibrium highest utility agents share reward.
2-agent case, derivation make order build MILP similar
first derivation Sandholm, Gilpin, Conitzer (2005). give details
derivation adapt DEC-POMDP adding objective function it.
2 agents, derivation still use find Nash equilibriae pure strategies.
rest article, make distinction policy, sequence-form
policy strategy agent as, context, concepts equivalent. Borrowing
game theory, joint policy denoted p q, individual policy pi qi
i-reduced policy pi qi .
5.1 Nash Equilibrium
Nash Equilibrium joint policy policy best response reduced
joint policy formed policies joint policy. context sequence-form
350

fiMathematical Programming DEC-POMDPs

DEC-POMDP, policy pi Xi agent said best response i-reduced
joint policy qi Xi holds
V(, hpi , qi i) V(, hpi , qi i) 0,

pi Xi .

(59)

joint policy p X Nash Equilibrium holds
V(, p) V(, hpi , pi i) 0,
is,
X X

hEi j Ei

R(, hh, j i)



kI\{i}

I, pi Xi .



0,
pk (jk ) pi (h) pi (h)

I, pi Xi .

(60)

(61)

derivation necessary conditions Nash equilibrium consists deriving
necessary conditions policy best response reduced joint policy.
following program finds policy agent best response i-reduced joint
policy qi Xi . Constraints (63)-(64) ensure policy defines valid joint policy
(see Section 3.2) objective function traduction concept best response.
Variables: xi (h), I, h Hi



X X

Maximize
R(, hh, j i)
qk (jk ) xi (h)


hEi

j Ei

(62)

kI\{i}

subject to:

X

xi (a) = 1

(63)

aAi

xi (h) +

X

xi (h.o.a) = 0,

h Ni , Oi

(64)

h Hi .

(65)

aAi

xi (h) 0,

linear program (LP) must still refined solution best
response agent global best response, i.e., policy agent best
response agents. mean introducing new variables (a set variable
agent). main point adapt objective function current
objective function, applied find global best response, would lead non-linear
objective function product weights policies would appear. this,
make use dual program (LP).
linear program (LP) one variable xi (h) history h Hi representing
weight h. one constraint per information set agent i. words,
constraint linear program (LP) uniquely labeled information set. instance,
constraint (63) labeled null information set , nonterminal
history h observation o, corresponding constraint (64) labeled
information set h.o. Thus, (LP) ni variables mi constraints.
described appendix (see appendix B), dual (LP) expressed as:
351

fiAras & Dutech

Variables: yi (),
Minimize

yi ()

(66)

subject to:
yi ((h))

X

yi (h.o) 0,

h Ni

(67)

qk (jk ) 0,

h Ei

(68)

oOi

yi ((h))

X

R(, hh, j i)

j Ei



kI\{i}

yi () (, +),



(69)

(h) denotes information set h belongs. dual one free variable
yi () every information set agent i. function (h) (defined Section 2.3) appears mapping histories information sets1 . dual program
one constraint per history agent. Thus, dual mi variables ni constraints.
Note objective dual minimize yi () primal (LP),
right hand side constraints, except first one, 0.
theorem duality (see appendix B), applied primal (LP) (62)-(65)
transformed dual (66)-(69), says solutions value. Mathematically,
means that:



X X

R(, hh, j i)
qk (jk ) xi (h) = yi ().
(70)


hEi

j Ei

kI\{i}

Thus, value joint policy hxi , qi expressed either
X X


V(, hxi , qi i) =
R(, hh, j i)
qk (jk ) xi (h)
hEi

j Ei

(71)

kI\{i}



V(, hxi , qi i) = yi ().

(72)

Due constraints (63) (64) primal LP, holds
X


X X X
xi (a) +
yi (h.o) xi (h) +
xi (h.o.a)
yi () = yi ()
aAi

hNi oOi

(73)

aAi

constraint (63) guarantees first term braces 1 constraints (65)
guarantee remaining terms inside braces 0. right hand side
(73) rewritten
X
X
X




P
xi (a) yi ()
oOi yi (a.o)
+
xi (h) yi ((h))
yi (h.o)
aAi

oOi

hNi \Ai

+

X

xi (h)yi ((h))

hEi

=

P


hNi xi (h)



yi ((h))

X

oOi

X
yi (h.o) +
xi (h)yi ((h))
hEi

1. h.o information set, yi (h.o) shortcut writing yi ((h.o)).

352

(74)

fiMathematical Programming DEC-POMDPs

So, combining equations (70) (74), get
X
X


xi (h)
yi ((h))
yi (h.o)
oOi

hNi

+

X

xi (h)



hEi

yi ((h))

R(, hh, j i)

X



j Ei



kI\{i}


qk (jk ) = 0

(75)

time introduce supplementary variables w information set. variables, usually called slack variables, defined as:
X
yi ((h))
yi (h.o) = wi (h), h Ni
(76)
oOi

yi ((h))

X



R(, hh, j i)

j Ei



qk (jk ) = wi (h),

h Ei .

(77)

kI\{i}

shown Section C appendix, slack variables correspond concept
regret defined game theory. regret history expresses loss accumulated
reward agent incurs acts according history rather according
history would belong optimal joint policy.
Thanks slack variables, furthermore rewrite (75) simply
X
X
xi (h)wi (h) +
xi (h)wi (h) = 0
(78)
hNi

hEi

X

xi (h)wi (h) = 0.

(79)

hHi

Now, (79) sum ni products, ni size Hi . product sum
necessarily 0 xi (h) wi (h) constrained nonnegative primal
dual respectively. property strongly linked complementary slackness
optimality criterion linear programs (see, example, Vanderbei, 2008). Hence, (79)
equivalent
xi (h)wi (h) = 0,

h Hi .

(80)

Back framework DEC-POMDPs, constraints written:
pi (h)i (hh, qi i) = 0,

h Hi .

(81)

sum up, solving following mathematical program would give optimal joint
policy DEC-POMDP. constraints (87) non-linear thus prevent us
solving program directly. linearization constraints, called complementarity
constraints, subject next section.
Variables:
xi (h), wi (h) h Hi
yi ()
Maximize

y1 ()

353

(82)

fiAras & Dutech

subject to:
X

xi (a) = 1

(83)

aAi

xi (h) +

X

I, h Ni , Oi

(84)

yi (h.o) = wi (h),

I, h Ni

(85)

xk (jk ) = wi (h),

I, h Ei

(86)

xi (h.o.a) = 0,

aAi

yi ((h))

X

oOi

yi ((h))

X

j Ei

R(, hh, j i)



kI\{i}

xi (h)wi (h) = 0,

I, h Hi

(87)

xi (h) 0,

I, h Hi

(88)

wi (h) 0,

I, h Hi

(89)

yi () (, +),

I,

(90)

5.2 Dealing Complementarity Constraints
section explains non-linear constraints xi (h)wi (h) = 0 previous mathematical program turned sets linear constraints thus lead mixed
integer linear programming formulation solution DEC-POMDP.
Consider complementarity constraint ab = 0 variables b. Assume
lower bound values b 0. Let upper bounds values b
respectively ua ub . let c 0-1 variable. Then, complementarity constraint
ab = 0 separated following equivalent pair linear constraints,
ua c

(91)

b ub (1 c).

(92)

words, pair constraints satisfied, surely case ab = 0.
easily verified. c either 0 1. c = 0, set 0
constrained ua c (and less 0); c = 1, b set 0
since b constrained ub (1 c) (and less 0). either case,
ab = 0.
consider complementarity constraint xi (h)wi (h) = 0 non-linear program (82)-(90) above. wish separate constraint pair linear constraints.
recall xi (h) represents weight h wi (h) represents regret h.
first requirement convert constraint pair linear constraints lower
bound values two terms 0. indeed case since xi (h) wi (h)
constrained non-negative NLP. Next, require upper bounds
weights histories regrets histories. shown Lemma 3.1 upper
bound value xi (h) h 1. upper bounds regrets histories,
require calculus.
354

fiMathematical Programming DEC-POMDPs

policy pi agent holds
X
pi (h) = |Oi |T 1 .

(93)

hEi

Therefore, every i-reduced joint policy hq1 , q2 , , qn Xi , holds

X
|Ok |T 1
qk (jk ) =
j Ei kI\{i}

(94)

kI\{i}

Since regret terminal history h agent given hq1 , q2 , , qn defined
X


(95)
(h, q) = max
qk (jk ) R(, hh , j i) R(, hh, j i) ,
h (h)

j Ei kI\{i}

conclude upper bound Ui (h) regret terminal history h Ei
agent is,



1


Ui (h) =
|Ok |
max max
R(, hh , j i) min R(, hh, j i) . (96)

kI\{i}

h (h) j Ei

j Ei

let us consider upper bounds regrets non-terminal histories. Let
information set length agent i. Let Ei () Ei denote set terminal histories
agent first 2t elements history set identical . Let h
history length agent i. Let Ei (h) Ei denote set terminal histories
first 2t - 1 elements history set identical h. Since policy
pi agent i, holds
X
pi (h ) |Oi |T
(97)
h Ei (h)

conclude upper bound Ui (h) regret nonterminal history
h Ni length agent




max
max
R(,
hh
,
j
i)

min
min
R(,
hg,
j
i)
(98)
Ui (h) = Li


h Ei ((h)) j Ei

gEi (h) j Ei


Li = |Oi |T



|Ok |T 1 .

(99)

kI\{i}

Notice = (that is, h terminal) (98) reduces (96).
So, complementarity constraint xi (h)wi (h) = 0 separated pair linear
constraints using 0-1 variable bi (h) follows,
xi (h) 1 bi (h)
wi Ui (h)bi (h)
bi (h) {0, 1}
355

(100)
(101)
(102)

fiAras & Dutech

Variables:
xi (h), wi (h) bi (h) {1, 2} h Hi
yi () {1, 2}
Maximize

y1 ()

(103)

subject to:
X

xi (a) = 1

(104)

aAi

xi (h) +

X

= 1, 2, h Ni , Oi

(105)

= 1, 2, h Ni

(106)

R(, hh, h i)x2 (h ) = w1 (h),

h E1

(107)

R(, hh , hi)x1 (h ) = w2 (h),

h E2

(108)

xi (h.o.a) = 0,

aAi

yi ((h))

X

yi (h.o) = wi (h),

oOi

y1 ((h))

X

h E2

y2 ((h))

X

h E1

xi (h) 1 bi (h),
wi (h) Ui (h)bi (h),

= 1, 2, h Hi
= 1, 2, h Hi

(109)
(110)

xi (h) 0,

= 1, 2, h Hi

(111)

wi (h) 0,

= 1, 2, h Hi

(112)

bi (h) {0, 1},

= 1, 2, h Hi

yi () (, +),

(113)

= 1, 2, (114)

Table 4: MILP-2 agents. 0-1 mixed integer linear program, derived game
theoretic considerations, finds optimal stochastic joint policies DEC-POMDPs
2 agents.

5.3 Program 2 Agents
combine policy constraints (Section 3.2), constraints seen
policy best response (Sections 5.1, 5.2) maximization value
joint policy, derive 0-1 mixed integer linear program solution
optimal joint policy DEC-POMDP 2 agents. Table 4 details program
call MILP-2 agents.
Example formulation decentralized Tiger problem 2 agents
horizon 2 found appendices, Section E.4
variables program vectors xi , wi , bi yi agent i. Note
agent history h agent i, Ui (h) denotes upper bound
regret history h.
356

fiMathematical Programming DEC-POMDPs

solution (x , , w , b ) MILP-2 agents consists following quantities: (i)
optimal joint policy x = hx1 , x2 may stochastic; (ii) agent = 1,
2, history h Hi , wi (h), regret h given policy xi agent;
(iii) agent = 1, 2, information set , yi (), value given
policy xi agent; (iv) agent = 1, 2, vector bi simply tells us
histories support xi ; history h agent bi (h) = 1
support xi . Note replace y1 () y2 () objective function
without affecting program. following result.
Theorem 5.1. Given solution (x , w , , b ) MILP-2 agents, x = hx1 , x2
optimal joint policy sequence-form.
Proof: Due policy constraints agent, xi sequence-form policy
agent i. Due constraints (106)-(108), yi contains values information sets
agent given xi . Due complementarity constraints (109)-(110), xi best
response xi . Thus hx1 , x2 Nash equilibrium. Finally, maximizing value
null information set agent 1, effectively maximizing value hx1 , x2 i.
Thus hx1 , x2 optimal joint policy.

comparison MILP presented Table 3, MILP-2 agents
constitutes particularly effective program term computation time finding 2agent optimal -period joint policy much smaller program. number
variables required MILP exponential n, number variables required
MILP-2 agents exponential . represents major reduction size
lead improvement term computation time.
5.4 Program 3 Agents
number agents 2, constraint (86) non-linear program
(82)-(90) longer complementarity constraint
2 variables could linQ
earized before. particular, term kI\{i} xk (jk ) constraint (86) involves
many variables different agents. linearize term, restrict pure joint policies exploit combinatorial facts number
histories involved. leads 0-1 mixed linear program called MILP-n agents
depicted Table 5.
variables program MILP-n agents vectors xi , wi , bi yi
agent vector z. following result.
Theorem 5.2. Given solution (x , w , , b , z ) MILP-n agents, x = hx1 , x2 ,
, xn pure -period optimal joint policy sequence-form.
Proof: Due policy constraints domain constraints agent,
pure sequence-form policy agent i. Due constraints (118)-(119), yi
contains values information sets agent given xi . Due complementarity
constraints (122)-(123), xi best response xi . Thus x Nash equilibrium.
Finally, maximizing value null information set agent 1, effectively
maximizing value x . Thus x optimal joint policy.

xi

357

fiAras & Dutech

Variables:
xi (h), wi (h) bi (h) h Hi
yi () I,
z(j) j E
Maximize

y1 ()

(115)

xi (a) = 1

(116)

xi (h.o.a) = 0,

I, h Ni , Oi (117)

subject to:
X

aAi

xi (h) +

X

aAi

yi ((h))

X

yi (h.o) = wi (h),

I, h Ni

(118)

oOi

yi ((h))

X
1
R(, hh, ji i)z(j) = wi (h), I, h Ei

1
|Oi |
jE
X

z(hh, j i) = xi (h)
|Ok |T 1 ,
j Ei

(119)

kI\{i}

I, h Ei
X

z(j) =
|Oi |T 1
jE

(120)
(121)

iI

xi (h) 1 bi (h),

I, h Hi (122)

wi (h) Ui (h)bi (h),

I, h Hi (123)

xi (h) 0,

I, h Ni

xi (h) {0, 1}
wi (h) 0,

I, h Ei

I, h Hi

bi (h) {0, 1},

h Hi

yi () (, +),
z(j) [0, 1],

j E

(124)
(125)
(126)
(127)

I, i(128)
(129)

Table 5: MILP-n agents. 0-1 mixed integer linear program, derived game
theoretic considerations, finds pure optimal joint policies DEC-POMDPs
3 agents.

358

fiMathematical Programming DEC-POMDPs

Compared MILP Table 3, MILP-n agents roughly size
real valued variables 0-1 variables. precise, P
MILP 0-1
variable every terminal history every agent (that approximatively iI |Ai |T |Oi |T 1
integer variables) MILP-n agents two 0-1 variables
every terminal well
P
nonterminal history agent (approximatively 2 iI (|Ai ||Oi |)T integer variables).

5.5 Summary

formulation solution DEC-POMDP application Duality
Theorem Linear Programs allow us formulate solution DEC-POMDP
solution new kind 0-1 MILP. 2 agents, MILP O(kT ) variables
constraints thus smaller MILP previous section. Still,
MILPS quite large next section investigates heuristic ways speed
resolution.

6. Heuristics Speeding Mathematical Programs
section focusses ways speed resolution various MILPs presented
far. Two ideas exploited. First, show prune set sequence-form policies
removing histories provably part optimal joint policy.
histories called locally extraneous. Then, give lower uppers bounds
objective function MILPs, bounds sometimes used branch
bound method often used MILP solvers finalize values integer variables.
6.1 Locally Extraneous Histories
locally extraneous history history required find optimal joint policy
initial state DEC-POMDP could replaced co-history
without affecting value joint policy. co-history history h agent
defined history agent identical h aspects except last
action. Ai = {b, c}, co-history c.u.b.v.b history c.u.b.v.c. set
co-histories history h shall denoted C(h).
Formally, history h Hit length agent said locally extraneous if,
i-reduced joint histories length t,
every probability distribution set Hi
exists history h C(h)
X


(j ) R(, hh , j i) R(, hh, j i)
0
(130)
j Hti

(j ) denotes probability j .
alternative definition follows. history h Hit length agent said
locally extraneous exists probability distribution set co-histories
h i-reduced joint history j length t, holds
X
(h )R(, hh , j i) R(, hh, j i)
(131)
h C(h)

359

fiAras & Dutech

(h ) denotes probability co-history h .
following theorem justifies incremental pruning locally extraneous histories
search optimal joint policies faster performed smaller
set possible support histories.
Theorem 6.1. every optimal -period joint policy p agent
terminal history h agent locally extraneous , pi (h) > 0, exists
another -period joint policy p optimal identical p respects
except pi (h) = 0.
Proof: Let p -period joint policy optimal . Assume
agent terminal history h agent locally extraneous , pi (h) > 0.
(130), exists least one co-history h h that,
X


pi (j ) R(, hh , j i) R(, hh, j i)
0.
(132)
j HT


Let q -period policy agent identical pi respects except q(h )
= pi (h) + pi (h ) q(h) = 0. shall show q optimal . holds,
X

j HT


V(, hq, pi i) V(, hpi , pi i) =


pi (j ) R(, hh , j i)q(h ) R(, hh , j i)pi (h ) R(, hh, j i)pi (h)
=
X

j HT




pi (j ) R(, hh , j i)(q(h ) pi (h )) R(, hh, j i)pi (h)
=
X

j HT




pi (j ) R(, hh , j i)pi (h) R(, hh, j i)pi (h)

since q(h ) = pi (h) + pi (h ). Therefore,
X

j HT


V(, hq, pi i) V(, hpi , pi i) =


pi (j ) R(, hh , j i) R(, hh, j i)
0 (due (132)).

Hence, p = hq, pi optimal -period joint policy .



One could wonder order extraneous histories pruned important
not. answer question, following theorem shows many co-histories
extraneous, pruned order as:
either value, one pruned ;
pruning one change fact others still extraneous.
Theorem 6.2. two co-histories h1 h2 locally extraneous, either values
equal h locally extraneous
R(, hh1 , j i) R(, hh2 , j i)for j Hi
1
relatively C(h) \ {h2 }.
360

fiMathematical Programming DEC-POMDPs

Proof: Let C + denotes union C(h1 ) C(h2 ). immediately C(h1 ) =
C + \ {h1 } C(h2 ) = C + \ {h2 }. h1 (resp. h2 ) locally extraneous means
exists probability distribution 1 C(h1 ) (resp. 2 C(h2 )) that, j
:
Hi
X
1 (h )R(, hh , j i) R(, hh1 , j i)
(133)
h C + \{h1 }

X

2 (h )R(, hh , j i) R(, hh2 , j i)

(134)

h C + \{h2 }

(135)
Eq. (133) expanded in:
X

1 (h2 )R(, hh2 , j i) +

h C + \{h

1 (h )R(, hh , j i) R(, hh1 , j i).
1 ,h2 }

Using (134) (136) gives
X
1 (h2 )
2 (h )R(, hh , j i) +
h C + \{h2 }

leading
X

(136)

X

1 (h )R(, hh , j i) R(, hh1 , j i)

h C + \{h1 ,h2 }

(137)

(1 (h2 )2 (h ) + 1 (h ))R(, hh , j i) (1 1 (h2 )2 (h1 ))R(, hh1 , j i) (138)

h C + \{h1 ,h2 }

So, two cases possible:
1 (h2 ) = 2 (h1 ) = 1. case, R(, hh2 , j i) R(, hh1 , j i) R(, hh1 , j i)
.
R(, hh2 , j i), R(, hh1 , j i) = R(, hh2 , j i) j Hi
1 (h2 )2 (h1 ) < 1. case have:
X

h C + \{h1 ,h2 }

1 (h2 )2 (h ) + 1 (h )
R(, hh , j i) R(, hh1 , j i)
1 1 (h2 )2 (h1 )

(139)

meaning even without using h2 , h1 still locally extraneous
1 (h2 )2 (h )+1 (h )
probability distribution C + \ {h1 , h2 }
11 (h2 )2 (h1 )
X

h C + \{h

1 ,h2 }

1 (h2 )(1 2 (h1 )) + (1 1 (h2 ))
1 (h2 )2 (h ) + 1 (h )
=
1 1 (h2 )2 (h1 )
1 1 (h2 )2 (h1 )
1 1 (h2 )2 (h1 )
1 1 (h2 )2 (h1 )
= 1.
=

(140)
(141)
(142)


361

fiAras & Dutech

order prune locally extraneous histories, one must able identify histories.
indeed two complementary ways this.
first method relies definition value history (see Section 3.3),

R(, hh, j i) = (, hh, j i)R(, hh, j i).

(143)

Therefore,
(, hh, j i) = 0,


j Hi

(144)

true history h, means every joint history length occurring
given history part priori probability 0. thus, h clearly
extraneous. Besides, every co-history h locally extraneous share
probabilities.
second test needed locally extraneous histories verify (144).
again, turn linear programing particular following linear program

Variables: y(j), j Hi
Minimize



(145)

subject to:
X

j Hti



y(j ) R(, hh , j i) R(, hh, j i)
,
X

h C(h)

y(j ) = 1

(146)
(147)

j Hti

following Lemma.

y(j ) 0,


j Hi

(148)

Lemma 6.1. If, exists solution ( , ) linear program (145)-(148) 0,
h locally extraneous.
Proof : Let ( , ) solution LP (145)-(148). probability distribution
due constraints (147)-(148). 0, since minimizing , due
Hi
), every co-history h h
constraints (146), every (Hi
X


y(j ) R(, hh , j i) R(, hh, j i)
.
(149)
j Hti

Therefore, definition, h locally extraneous.



following procedure identifies locally extraneous terminal histories agents
proceed iterative pruning. mainly motivated Theorems 6.1 6.2
effectively removing extraneous histories. procedure similar procedure
iterated elimination dominated strategies game (Osborne & Rubinstein, 1994).
concept quite similar process policy elimination backward step
dynamic programming partially observable stochastic games (Hansen et al., 2004).
362

fiMathematical Programming DEC-POMDPs

Step 1: agent I, set HiT Ei . Let H denote set iI HiT .
joint history j H , compute store value R(, j) j joint
observation sequence probability (, j) j.
Step 2: agent I, history h HiT , i-reduced joint
, (, hh, j i) = 0, remove h H .
history j Hi

Step 3: agent I, history h HiT follows: C(h) HiT
non-empty, check whether h locally extraneous setting solving
set H set C(h)
LP (145)-(148). setting LP, replace Hi

set C(h) HiT . upon solving LP, h found locally extraneous
, remove h HiT .
Step 4: Step 3 history (of agent) found locally extraneous, go
Step 3. Otherwise, terminate procedure.
procedure builds set HiT agent i. set contains every terminal
history agent required finding optimal joint policy , every
terminal history locally extraneous . agent i, every history
HiT HiT locally extraneous. reason reiterating Step 3
history h agent found locally extraneous consequently removed
HiT , possible history agent previously locally
extraneous becomes so, due removal h HiT . Hence, order verify
case history not, reiterate Step 3.
Besides, Step 2 procedure prunes histories impossible given
model DEC-POMDP observation sequence observed.
last pruning step taken order remove non-terminal histories
lead extraneous terminal histories. last step recursive, starting histories
horizon 1, remove histories hi non-extraneous terminal histories,
is, histories hi h.o.a extraneous Ai Oi .
Complexity algorithm pruning locally extraneous histories exponential
complexity. joint history must examined compute value occurence
probability. Then, worst case, Linear Program run every local history
order check extraneous not. Experimentations needed see prunning
really interesting.
6.2 Cutting Planes
Previous heuristics aimed reducing search space linear programs,
incidentally good impact time needed solve programs. Another option
directly aims reducing computation time use cutting planes (Cornuejols,
2008). cut (Dantzig, 1960) special constraint identifies portion set
feasible solutions optimal solution provably lie. Cuts used
conjunction various branch bounds mechanism reduce number possibles
combination integer variables examined solver.
present two kinds cuts.
363

fiAras & Dutech

Variables: y(j), j H
Maximize

X

R(, j)y(j)

(153)

jE

subject to,
X

y(a) = 1

(154)

aA

y(j) +

X

y(j.o.a) = 0,

j N ,

(155)

j H

(156)

aA

y(j) 0,

Table 6: POMDP. linear program finds optimal policy POMDP.
6.2.1 Upper Bound Objective Function
first cut propose upper bound POMDP cut. value optimal
-period joint policy given DEC-POMDP bounded value
VP optimal -period policy POMDP derived DEC-POMDP.
derived POMDP DEC-POMDP assuming centralized controller (i.e.
one agent using joint-actions).
sequence-form representation POMDP quite straightforward. Calling H
set Tt=1 Ht joint histories lengths less equal N set H\E nonterminal joint histories, policy POMDP horizon sequence-form function
q H [0, 1] that:
X
q(a) = 1
(150)
aA

q(j) +

X

q(j.o.a) = 0,

j N ,

(151)

aA

value VP (, q) sequence-form policy q given by:
X
VP (, q) =
R(, j)q(j)

(152)

jE

Thereby, solution linear program Table 6 P
optimal policy
POMDP horizon optimal value POMDP jE R(, j)y (j). So,
value V(, p ) optimal joint policy p = hp1 , p2 , , pn DEC-POMDP
bounded value VP (, q ) associated POMDP.
Complexity complexity finding upper bound linked complexity
solving POMDP which, showed Papadimitriou Tsitsiklis (1987), PSPACE
(i.e. require memory polynomial size problem, leading possible
exponential complexity time). again, experimentation help us decide
cases upper bound cut efficient.
364

fiMathematical Programming DEC-POMDPs

6.2.2 Lower Bound Objective Function
case DEC-POMDPs non-negative reward, trivial show value
-period optimal policy bounded value 1 horizon optimal
value. So, general case, take account lowest reward possible
compute lower bound say that:
X
R(, j)z(j) V 1 () + min min R(s, a)
(157)
aA sS

jE

V 1 value optimal policy horizon 1. reasoning leads
iterated computation DEC-POMDPs longer longer horizon, reminiscent
MAA* algorithm (Szer et al., 2005). Experiments tell worthwhile solve bigger
bigger DEC-POMDPs take advantage lower bound better directly
tackle horizon problem without using lower bound.
Complexity compute lower bound, one required solve DEC-POMDP whith
horizon one step shorter current horizon. complexity clearly
least exponential. experiments, value DEC-POMDP used
DEC-POMDP bigger horizon. case, computation time
augmented best time solve smaller DEC-POMDP.
6.3 Summary
Pruning locally extraneous histories using bounds objective function
practical use software solving MILPs presented paper. Pruning histories
means space policies used MILP reduced and, formulation
MILP depends combinatorial characteristics DEC-POMDP, MILP
must altered show Appendix D.
Validity far cuts concerned, alter solution found MILPs,
solution MILPs still optimal solution DEC-POMDP. extraneous histories pruned, least one valid policy left solution because, step
3 algorithm, history pruned co-histories left. Besides,
reduced set histories still used build optimal policy Theroem 6.1.
consequence, MILP build reduced set histories admit solution
solution one optimal joint policy.
next section, experimental results allow us understand cases
heuristics introduced useful.

7. Experiments
mathematical programs heuristics designed paper tested four
classical problems found literature. problems, involving two agents,
mainly compared computation time required solve DEC-POMDP using Mixed
Integer Linear Programming methods computation time reported methods found
literature. tested programs three-agent problems randomly
designed.
365

fiAras & Dutech

Problem
MABC
MA-Tiger
Fire Fighting
Grid Meeting
Random Pbs

|Ai |
2
3
3
5
2

|Oi |
2
2
2
2
2

|S|
4
2
27
16
50

n
2
2
2
2
3

Table 7: Complexity various problems used test beds.

MILP MILP-2 solved using iLog Cplex 10 solver commercial set
Java packages relies combination Simplex Branch Bounds
methods (Fletcher, 1987). software run Intel P4 3.4 GHz 2Gb
RAM using default configuration parameters. mathematical programs, different
combination heuristics evaluated: pruning locally extraneous histories, using
lower bound cut using upper bound cut, respectively denoted LOC, Low
result tables come.
Non-Linear Program (NLP) Section 3.4 evaluated using various solvers NEOS website (http://www-neos.mcs.anl.gov ), even thought
method guarantee optimal solution DEC-POMDP. Three solvers
used: LANCELOT (abbreviated LANC.), LOQO SNOPT.
result tables report results found literature following algorithms:
DP stands Dynamic Programming Hansen et al. (2004); DP-LPC improved
version Dynamic Programming policies compressed order fit
memory speed evaluation proposed Boularias Chaib-draa (2008);
PBDP extension Dynamic Programming pruning guided knowledge
reachable belief-states detailed work Szer Charpillet (2006); MAA*
heuristically guided forward search proposed Szer et al. (2005) generalized
improved version algorithm called GMAA* developed Oliehoek et al. (2008).
problems selected evaluate algorithms detailed coming subsections.
widely used evaluate DEC-POMDPs algorithms literature
complexity, term space size, summarized Table 7.
7.1 Multi-Access Broadcast Channel Problem
Several versions Multi-Access Broadcast Channel (MABC) problem found
literature. use description given Hansen et al. (2004) allows
problem formalized DEC-POMDP.
MABC, given two nodes (computers) required send messages
common channel given duration time. Time imagined
split discrete periods. node buffer capacity one message.
buffer empty period refilled certain probability next period.
period, one node send message. nodes send message
period, collision messages occurs neither message transmitted. case
collision, node intimated collision signal. collision
366

fiMathematical Programming DEC-POMDPs

signaling mechanism faulty. case collision, certain probability,
send signal either one nodes.
interested pre-allocating channel amongst two nodes given number
periods. pre-allocation consists giving channel one nodes period
function nodes information period. nodes information period
consists sequence collision signals received till period.
modeling problem DEC-POMDP, obtain 2-agent, 4-state, 2-actionsper-agent, 2-observations-per-agent DEC-POMDP whose components follows.
node agent.
state problem described states buffers two nodes.
state buffer either Empty Full. Hence, problem four states:
(Empty, Empty), (Empty, Full), (Full, Empty) (Full, Full).
node two possible actions, Use Channel Dont Use Channel.
period, node may either receive collision signal may not. node
two possible observations, Collision Collision.
initial state problem (Full, Full). state transition function P,
joint observation function G reward function R taken Hansen et al.
(2004). agents full buffers period, use channel period,
state problem unchanged next period; agents full buffers
next period. agent full buffer period uses channel
period, buffer refilled certain probability next period.
agent 1, probability 0.9 agent 2, probability 0.1. agents
empty buffers period, irrespective actions take period, buffers
get refilled probabilities 0.9 (for agent 1) 0.1 (for agent 2).
observation function G follows. state period (Full, Full)
joint action taken agents previous period (Use Channel, Use Channel),
probability receive collision signal 0.81, probability one
receives collision signal 0.09 probability neither receives
collision signal 0.01. state problem may period
joint action agents may taken previous period, agents receive
collision signal.
reward function R quite simple. state period (Full, Empty)
joint action taken (Use Channel, Dont Use Channel) state period
(Empty, Full) joint action taken (Dont Use Channel, Use Channel), reward
1; combination state joint action, reward 0.
evaluated various algorithms problem three different horizons (3,
4 5) respective optimal policies value 2.99, 3.89 4.79. Results
detailed Table 8 where, horizon algorithm, value computation
time best policy found given.
results show MILP compares favorably classical algorithms except
GMAA* always far better horizon 4 and, horizon 5, roughly within
367

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP
Cplex
Low
MILP
Cplex

MILP
Cplex
LOC
MILP
Cplex
LOC, Low
MILP
Cplex
LOC,
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
DP
Dyn. Prog.
DP-LPC
Dyn. Prog.
PBDP
Dyn. Prog.
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
2.99
0.86
2.99 0.10 / 0.93
2.99 0.28 / 1.03
2.99 0.34 / 0.84
2.99 0.44 / 0.84
2.99 0.62 / 0.93
2.99
0.39
2.90
0.01
2.99
0.02
2.90
0.01
Value
Time
2.99
5
2.99
0.36
2.99
< 1s
2.99
< 1s
?
?

Horizon 4
Value
Time
3.89
900
3.89
0.39 / 900
3.89
0.56 / 907
3.89
1.05 / 80
3.89
1.44 / 120
3.89 1.61 / 10.2
3.89
3.53
3.17
0.01
3.79
0.95
3.79
0.05
Value
Time
3.89
17.59
3.89
4.59
3.89
2
3.89
5400
3.89
0.03

Horizon 5
Value
Time
-m
3.5 / -m
4.73 / -m
2.27 / -t
5.77 / -t
4.79
7.00 / 25
-m
4.70
0.21
4.69
20
4.69
0.18
Value
Time
-m
-m
4.79
105
-t
4.79
5.68

Table 8: MABC Problem. Value computation time (in seconds) solution
problem computed several methods, best results highlighted.
appropriate, time shows first time used run heuristics global
time, format heuristic/total time. -t means timeout 10,000s;
-m indicates problem fit memory ? indicates
algorithm tested problem.

order magnitude MILP pertinent heuristics. expected, apart
simplest setting (horizon 3), NLP based resolution find optimal policy
DEC-POMDP, computation time lower methods. Among
MILP methods, MILP-2 better MILP even best heuristics horizon 3
4. size problem increases, heuristics way MILPs
able cope size problem. table shows that, MABC
problem, pruning extraneous histories using LOC heuristic always good method
investigation revealed 62% heuristics proved locally extraneous.
far cutting bounds concerned, dont seem useful first (for horizon
3 4) necessary MILP find solution horizon 5. problem,
one must mind one optimal policy horizon.
7.2 Multi-Agent Tiger Problem
explained section 2.2, Multi-Agent Tiger problem (MA-Tiger) introduced
paper Nair et al. (2003). general description problem, ob368

fiMathematical Programming DEC-POMDPs

Joint Action
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(Listen, Listen)
(*, *)

State
Left
Left
Left
Left
Right
Right
Right
Right
*

Joint Observation
(Noise Left, Noise Left)
(Noise Left, Noise Right)
(Noise Right, Noise Left)
(Noise Right, Noise Right)
(Noise Left, Noise Left)
(Noise Left, Noise Right)
(Noise Right, Noise Left)
(Noise Right, Noise Right)
(*, *)

Probability
0.7225
0.1275
0.1275
0.0225
0.0225
0.1275
0.1275
0.7225
0.25

Table 9: Joint Observation Function G MA-Tiger Problem.
tain 2-agent, 2-state, 3-actions-per-agent, 2-observations-per agent DEC-POMDP whose
elements follows.
person agent. So, 2-agent DEC-POMDP.
state problem described location tiger. Thus, consists
two states Left (tiger behind left door) Right (tiger behind right
door).
agents set actions consists three actions: Open Left (open left door),
Open Right (open right door) Listen (listen).
agents set observations consists two observations: Noise Left (noise coming
left door) Noise Right (noise coming right door).
initial state equi-probability distribution S. state transition function P,
joint observation function G reward function R taken paper Nair
et al. (2003). P quite simple. one agents opens door period, state
problem next period set back . agents listen period, state
process unchanged next period. G, given Table (9), quite simple.
Nair et al. (2003) describes two reward functions called B problem,
report results reward function A, given Table 10, behavior
algorithm similar reward functions. optimal value problem
horizons 3 4 respectively 5.19 4.80.
horizon 3, dynamic programming forward search methods generally better
mathematical programs. contrary horizon 4 computation time MILP Low heuristic significatively better other, even
GMAA*. Unlike MABC, pruning extraneous histories improve methods
based MILP, quite understandable deeper investigations showed
extraneous histories. Using lower cutting bounds proves efficient
seen kind heuristic search best policy ; directly set policies (like
369

fiAras & Dutech

Joint Action
(Open Right, Open Right)
(Open Left, Open Left)
(Open Right, Open Left)
(Open Left, Open Right)
(Listen, Listen)
(Listen, Open Right)
(Open Right, Listen)
(Listen, Open Left)
(Open Left, Listen)

Left
20
-50
-100
-100
-2
9
9
-101
-101

Right
-50
20
-100
-100
-2
-101
-101
9
9

Table 10: Reward Function MA-Tiger Problem.

GMAA*) set combination histories, may explain good behavior
MILP+Low.
must noted problem, approximate methods NLP
algorithms depicted Memory Bound Dynamic Programming
Seuken Zilberstein (2007) able find optimal solution. And, again,
methods based NLP quite fast sometimes accurate.
7.3 Fire Fighters Problem
problem Fire Fighters (FF) introduced new benchmark Oliehoek
et al. (2008). models team n fire fighters extinguish fires row nh
houses.
state house given integer parameter, called fire level f ,
takes discrete value 0 (no fire) nf (fire maximum severity). every time
step, agent move one house. two agents house,
extinguish existing fire house. agent alone, fire level lowered
0.6 probability neighbor house burning 1 probability otherwise.
burning house fireman present increase fire level f one point 0.8
probability neighbor house burning probability 0.4 otherwise.
unattended non-burning house catch fire probability 0.8 neighbor house
burning. action, agents receive reward f house still
burning. agent observe flames location probability
depends fire level: 0.2 f = 0, 0.5 f = 1 0.8 otherwise. start,
agents outside houses fire level houses sampled
uniform distribution.
model following characteristics:
na agents, nh actions nf possible informations.

h 1
states nnf h possible states burning houses
nnf h . na +n
n


h 1
different ways distribute na fire fighters houses.
na +n
na
example, 2 agents 3 houses 3 levels fire lead 9 6 = 54 states. But,
370

fiMathematical Programming DEC-POMDPs

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP
Cplex
Low
MILP
Cplex

MILP
Cplex
LOC
MILP
Cplex
LOC, Low
MILP
Cplex
LOC,
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
DP
Dyn. Prog.
DP-LPC
Dyn. Prog.
PBDP
Dyn. Prog.
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
5.19
3.17
5.19 0.46 / 4.9
5.19 0.42 / 3.5
5.19 1.41 / 6.4
5.19 1.88 / 7.6
5.19 1.83 / 6.2
5.19
11.16
-45
0.03
5.19
0.47
5.19
0.01
Value
Time
5.19
2.29
5.19
1.79
?
?
5.19
0.02
5.19
0.04

Horizon 4
Value
Time
-t
4.80
3.5 / 72
0.75 / -t
16.0 / -t
4.80 19.5 / 175
16.75 / -t
-t
-9.80
4.62
4.80
514
4.78
91
Value
Time
-m
4.80
534
?
?
4.80
5961
4.80
3208

Table 11: MA-Tiger Problem. Value computation time (in seconds) solution
problem computed several methods, best results highlighted.
appropriate, time shows first time used run heuristics
global time, format heuristic/total time.-t means timeout
10.000s; -m indicates problem fit memory ? indicates algorithm tested problem.

371

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 3
Value
Time
-t
-5.98
38
-5.98
0.05
-5.98
2.49
-6.08
0.24
Value
Time
(-5.73) 0.29
(-5.73)
0.41

Horizon 4
Value
Time
-t
-t
-7.08
4.61
-7.13
1637
-7.14
83
Value
Time
(-6.57) 5737
(-6.57) 5510

Table 12: Fire Fighting Problem. Value computation time (in seconds) solution problem computed several methods, best results highlighted.
-t means timeout 10.000s. MAA* GMAA*, value parenthesis
taken work Oliehoek et al. (2008) optimal
different optimal values.

possible use information joint action reduce number state
needed transition function simply nnf h , meaning 27 states 2 agents
3 houses 3 levels fire.
Transition, observation reward functions easily derived description.
problem, dynamic programming based methods tested problem
formulation quite new. horizon 3, value optimal policy given Oliehoek
et al. (2008) (5.73) differs value found MILP algorithms (5.98) whereas
methods supposed exact. might come slight differences
respective formulation problems. horizon 4, Oliehoek et al. (2008) report
optimal value (6.57).
problem, MILP methods clearly outperformed MAA* GMAA*.
NLP methods, give optimal solution horizon 3, better term
computation time. might NLP able find optimal policies horizon 4
setting differs work Oliehoek et al. (2008), able check
policy found really optimal. main reason superiority forward
search method lies fact problem admits many many optimal policies
value. fact, horizon 4, MILP-based methods find optimal policy quite
quickly (around 82s MILP-2) then, using branch-and-bound, must evaluate
potential policies knowing indeed found optimal policy. Forward
search methods stop nearly soon hit one optimal solution.
Heuristics reported as, improve performance MILP
take away computation time thus results worse.
372

fiMathematical Programming DEC-POMDPs

7.4 Meeting Grid
problem called Meeting grid deals two agents want meet stay
together grid world. introduced work Bernstein, Hansen,
Zilberstein (2005).
problem, two robots navigating two-by-two grid world
obstacles. robot sense whether walls left right,
goal robots spend much time possible square. actions
move up, down, left right, stay square. robot attempts
move open square, goes intended direction probability 0.6,
otherwise randomly either goes another direction stays square.
move wall results staying square. robots interfere
cannot sense other. reward 1 agents share square,
0 otherwise. initial state distribution deterministic, placing robots
upper left corner grid.
problem modelled DEC-POMDP where:
2 agents, one 5 actions observations (wall left, wall
right).
16 states, since robot 4 squares time.
Transition, observation reward functions easily derived description.
problem, dynamic programming based methods tested problem
formulation quite new. problem intrinsically complex
solved horizon 2 3. Again, optimal value found method differ
value reported Oliehoek et al. (2008). Whereas found optimal values
1.12 1.87 horizon 2 3, report optimal values 0.91 1.55.
Results problem roughly pattern results
problem. MAA* GMAA* quicker MILP, time MILP able find
optimal solution horizon 3. NLP methods give quite good results slower
GMAA*. FF, numerous optimal policies MILP methods
able detect policy found quickly indeed optimal.
Again, heuristics reported as, improve performance
MILP take away computation time thus results worse.
7.5 Random 3-Agent Problems
test approach problems 3 agents, used randomly generated DECPOMDPs state transition function, joint observation function reward
functions randomly generated. DEC-POMDPs 2 actions 2 observations
per agent 50 states. Rewards randomly generated integers range 1 5.
complexity family problem quite similar complexity MABC
problem (see Section 7.1).
373

fiAras & Dutech

Resolution method
Program
Solver
Heuristics
MILP
Cplex
MILP-2
Cplex
NLP
SNOPT
NLP
LANC.
NLP
LOQO
Algorithm
Family
MAA*
Fw. Search
GMAA*
Fw. Search

Horizon 2
Value Time
1.12
0.65
1.12
0.61
0.91
0.01
1.12
0.06
1.12
0.07
Value Time
(0.91)
0s
(0.91)
0s

Horizon 3
Value Time
1.87
1624
-t
1.26
1.05
1.87
257
0.48
81
Value Time
(1.55)
10.8
(1.55) 5.81

Table 13: Meeting Grid Problem. Value computation time (in seconds)
solution problem computed several methods, best results
highlighted. -t means timeout 10.000s. MAA* GMAA*, value
parenthesis taken work Oliehoek et al. (2008)
optimal different optimal values...

Program
MILP
MILP-2

Least Time (secs)
2.45
6.85

Time (secs)
455
356

Average
120.6
86.88

Std. Deviation
183.48
111.56

Table 14: Times taken MILP MILP-2 2-agent Random Problem horizon 4.

order assess real complexity Random problem, first tested
two-agent version problem horizon 4. Results averaged 10 runs
programs given Table 14. compared MABC problem seemed
comparable complexity, Random problem proves easier solve (120s vs 900s).
problem, number 0-1 variable relatively small, weight much
resolution time MILP-2 thus faster.
Results three-agent problem horizon 3 given Table 15,
averaged 10 runs. Even though size search space smaller case
(for 3 agents horizon 3, 9 1021 policies whereas problem 2
agents horizon 4, 1.5 1051 possible policies), 3 agent problems seems
difficult solve, demonstrating one big issue policy coordination. Here,
heuristics bring significative improvement resolution time MILP. predicted,
MILP-n efficient given completeness.
374

fiMathematical Programming DEC-POMDPs

Program
MILP
MILP-Low
MILP-n

Least Time (secs)
21
26
754

Time (secs)
173
90
2013

Average
70.6
53.2
1173

Std. Deviation
64.02
24.2
715

Table 15: Times taken MILP MILP-n 3-agent Random problem horizon 3.

8. Discussion
organized discussion two parts. first part, analyze results
offer explanations behavior algorithms usefulness heuristics. Then,
second part, explicitely address important questions.
8.1 Analysis Results
results, appears MILP methods better alternative Dynamic Programming methods solving DEC-POMDPs globally generally clearly outperformed forward search methods. structure thus characteristics
problem big influence efficiency MILP methods. Whereas seems
behavior GMAA* terms computation time quite correlated
complexity problem (size action observation spaces), MILP methods seem
sometimes less correlated complexity. case MABC problem (many
extraneous histories pruned) MA-Tiger problem (special structure)
outperform GMAA*. contrary, many optimal policies exists, forward
search methods GMAA* clearly better choice. Finally, Non-Linear Programs,
even though guarantee optimal solution, generally good alternative
sometimes able find good solution computation time often
better GMAA*. might prove useful approximate heuristic-driven forward
searches.
computational record two 2-agent programs shows MILP-2 agents
slower MILP horizon grows. two reasons sluggishness
MILP-2 agents may attributed. time taken branch bound (BB)
method solve 0-1 MILP inversely proportional number 0-1 variables
MILP. MILP-2 agents many 0-1 variables MILP event hough
total number variables exponentially less MILP. first reason.
Secondly, MILP-2 agents complicated program MILP; many
constraints MILP. MILP simple program, concerned finding subset
given set. addition finding weights histories, MILP finds weights
terminal joint histories. extra superfluous quantity forced find.
hand, MILP-2 agents takes much circuitous route, finding many
superfluous quantities MILP. addition weights histories, MILP-2 agents
finds supports policies, regrets histories values information sets. Thus,
375

fiAras & Dutech

Problem

Heuristic

MABC

LOC
Low

LOC
Low

LOC

MA-Tiger

Meeting

Horizon 2
Time #pruned

0.41

0/18

1.36

15/50*

Horizon 3
Time #pruned
0.34
14/32
0.10
0.28
1.41
0/108
0.46
0.42
74.721 191/500*

Horizon 4
Time #pruned
1.047
74/128
0.39
3.89
16.0
0/648
3.5
0.75

Horizon 5
Time #pruned
2.27
350/512
3.5
4.73

Table 16: Computation time heuristics. LOC heuristics, give computation time seconds number locally extraneous histories pruned
total number histories (for agent). * denotes cases one
additional history prunned second agent. Low heuristic,
computation time given.

relaxation MILP-2 agents takes longer solve relaxation MILP.
second reason slowness BB method solves MILP-2 agents.
bigger problems, namely Fire-Fighters Meeting Grid, horizon
stays small, MILP-2 agents compete MILP slightly lower size.
complexity grows O((|Ai ||Oi |)T ) whereas grows O((|Ai ||Oi |)2T ) MILP.
small difference hold long number integer variables quickly lessens
efficiency MILP-2 agents.
far heuristic concerned, proved invaluable problems (MABC
MA-Tiger) useless others. case MABC, heuristics helpful
prune large number extraneous heuristics ultimately, combination
upper bound cut efficient horizon grows. case
MA-Tiger, although extraneous histories found, using lower bound cut heuristic
MILP leads quickest algorithm solving problem horizon 4.
problems, heuristics burden greedy computation
time speed resolution. example, Grid Meeting problem, time
taken prune extraneous histories bigger time saved solving problem.
result, added value using heuristics depends nature problem
(as depicted Table 16) but, right now, able predict usefulness without
trying them.
emphasize results given lie limit possible solve
exact manner given memory computer used resolution, especially
terms horizon. Furthermore, number agent increases, length
horizon must decreased problems still solvable.
376

fiMathematical Programming DEC-POMDPs

8.2 Questions
mathematical programing approach presented paper raises different questions.
explicitly addressed questions appears important us.
Q1: sequence-form approach entirely doomed exponential
complexity?
number sequence-form joint policies grows doubly exponentially horizon number agents, sequence-form approach seems doomed, even compared
dynamic programming doubly exponential worst cases only. But, indeed,
arguments must taken consideration.
exponential number individual histories need evaluated. joint
part sequence-form left MILP solver. every computation done
particular history, computing value checking extraneous, greater
reusability computations done entire policies. history shared many
joint policies individual policy. way, sequence-form allows us work
reusable part policies without work directly world distributions
set joint-policies.
Then, MILPs derived sequence-form DEC-POMDPs need memory size
grows exponentially horizon number agents. Obviously,
complexity quickly overwhelming case every exact method
far. shown experiments, MILP approach derived sequence-form
compares quite well dynamic programming, even outperformed forward methods
GMAA*.
Q2: MILP sometimes take little time find optimal joint
policy compared existing algorithms?
Despite complexity MILP approach, three factors contribute relative
efficiency MILP.
1. First, efficiency linear programming tools themselves. solving MILP,
BB method solves sequence linear programs using simplex algorithm.
LPs relaxation MILP. theory, simplex algorithm requires
worst case exponential number steps (in size LP) solving LP2 ,
well known that, practice, usually solves LP polynomial number
steps (in size LP). Since size relaxation MILP exponential
horizon, means that, roughly speaking, time taken solve relaxation
MILP exponential horizon whereas doubly exponential
methods.
2. second factor sparsity matrix coefficients constraints
MILP. sparsity matrix formed coefficients constraints
2. statement must qualified: worst case time requirement demonstrated
variants simplex algorithm. demonstrated basic version simplex
algorithm.

377

fiAras & Dutech

LP determines practice rate pivoting algorithm
simplex solves LP (this applies Lemkes algorithm context
LCP). sparser matrix, lesser time required perform elementary
pivoting (row) operations involved simplex algorithm lesser space
required model LP.
3. third factor fact supplement MILP cuts; computational
experience clearly shows speeds computations. first two
factors related solving relaxation MILP (i.e., LP), third factor
impact BB method itself. upper bound cut identifies additional
terminating condition BB method, thus enabling terminate earlier
absence condition. lower bound cut attempts shorten list
active subproblems (LPs) BB method solves sequentially. Due
cut, BB method potentially lesser number LPs solve. Note
inserting lower bound cut, emulating forward search properties
A* algorithm.
Q3: know MILP-solver (iLogs Cplex experiments)
reason speedup?
Clearly, approach would slower, even sometime slower classical dynamic
programming approach used another program solving MILPs experimented MILPs solvers NEOS website indeed
slow. true Cplex, solver used experiments, quite optimized.
Nevertheless, exactly one points wanted experiment paper:
one advantages formulating DEC-POMDP MILP possibility use
fact that, mixed integer linear programs important industrial world,
optimized solvers exist.
Then, formulate DEC-POMDP MILP mostly paper
about.
Q4: main contribution paper?
stated earlier paper, current algorithms DEC-POMDPs largely inspired POMDPs algorithms. main contribution pursue entirely different
approach, i.e., mixed integer linear programming. such, learned lot
DEC-POMDPs pro & con mathematical programming approach.
lead formulation new algorithms.
designing algorithms, have, first all, drawn attention new representation policy, namely sequence form policy, introduced Koller, Megiddo
von Stengel. sequence form policy compact representation
policy agent, afford compact representation set policies
agent.
algorithms proposed finite horizon DEC-POMDPs mathematical
programming algorithms. precise, 0-1 MILPs. MDP domain,
378

fiMathematical Programming DEC-POMDPs

mathematical programming long used solving infinite horizon case.
instance, infinite horizon MDP solved linear program (dEpenoux, 1963).
recently, mathematical programming directed infinite horizon POMDPs
DEC-POMDPs. Thus, infinite horizon DEC-MDP (with state transition independence) solved 0-1 MILP (Petrik & Zilberstein, 2007) infinite horizon
POMDP DEC-POMDP solved (for local optima) nonlinear program (Amato, Bernstein, & Zilberstein, 2007b, 2007a). finite horizon case much different
character infinite horizon case dealt using dynamic programming.
stated earlier, whereas dynamic programming quite successful finite horizon
MDPs POMDPs, less finite horizon DEC-POMDPs.
contrast, game theory, mathematical programming successfully directed
games finite horizon. Lemkes algorithm (1965) two-player normal form games,
Govindan-Wilson algorithm (2001) n-player normal form games Koller, Megiddo
von Stengel approach (which internally uses Lemkes algorithm) two-player extensive
form games finite-horizon games.
remained find way appropriate mathematical programming
solving finite horizon case POMDP/DEC-POMDP domain. work done
precisely (incidently, algorithm solving kind n-player normal form games). Throughout paper, shown mathematical programming
(in particular, 0-1 integer programming) applied solving finite horizon DECPOMDPs (it easy see approach presented yields linear program
solving finite horizon POMDP). Additionally, computational experience
approach indicates finite horizon DEC-POMDPs, mathematical programming may
better (faster) dynamic programming. shown well-entrenched
dynamic programming heuristic pruning redundant extraneous objects (in
case, histories) integrated mathematical programming approach.
Hence, main contribution paper presents, first time, alternative approach solving finite horizon POMDPs/DEC-POMDPs based MILPs.
Q5: mathematical programming approach presented paper something dead end?
question bit controversial short answer question could
small yes. true every approach looks exact optimal solutions
DEC-POMDPs, whether grounded dynamic programming forward search
mathematical programming. complexity problem, exact solution
always untractable algorithms still improved.
longer answer mitigated, especially light recent advances made
dynamic programming forward search algorithms. One crucial point sequenceform DEC-POMDPs pruning extraneous histories. recent work Oliehoek,
Whiteson, Spaan (2009) shown clusters histories equivalent
way could reduce nomber constraints MILPs. approach Amato,
Dibangoye, Zilberstein (2009) improves speed dynamic programming
operator could help finding extraneous histories. So, least, work
379

fiAras & Dutech

still required stating every aspect sequence-form DEC-POMDPs
studied.
turn even longer answer. Consider long horizon case. Given exact
algorithms (including ones presented paper) tackle horizons less 6,
long horizon, mean anything upwards 6 time periods. long horizon case,
required conceive possibly sub-optimal joint policy given horizon
determine upper bound loss value incurred using joint policy instead
using optimal joint policy.
current trend long horizon case memory-bounded approach. memory
bounded dynamic programming (MBDP) algorithm (Seuken & Zilberstein, 2007)
main exponent approach. algorithm based backward induction DP
algorithm (Hansen et al., 2004). algorithm attempts run limited amount
space. order so, unlike DP algorithm, prunes even non-extraneous (i.e., nondominated) policy trees iteration. Thus, iteration, algorithm retains
pre-determined number trees. algorithm variants used find
joint policy MABC, MA-tiger Box pushing problems long
horizons (of order thousands time periods).
MBDP provide upper bound loss value. bounded DP (BDP)
algorithm presented paper Amato, Carlin, Zilberstein (2007c) give
upper bound. However, interesting DEC-POMDP problems (such MA-tiger),
MBDP finds much better joint policy BDP.
meaningful way introduce notion memory boundedness approach
fix priori upper bound size concerned mathematical program.
presents sorts difficulties main difficulty seems need represent
policy long horizon limited space. MBDP algorithm solves problem
using may termed recursive representation. recursive representation
causes MBDP algorithm take long time evaluate joint policy, allow
algorithm represent long horizon joint policy limited space. context
mathematical programming approach, would change policy constraints
way long horizon policy represented system consisting limited
number linear equations linear inequalities. Besides policy constraints,
constraints presented programs would accordingly transfigured.
evident (to us) transfiguration constraints possible.
hand, infinite horizon case seems promising candidate adapt
approach to. Mathematical programming already applied, success,
solving infinite horizon DEC-POMDPs (Amato et al., 2007a). computational experience mathematical programming approach shows better (finds higher
quality solutions lesser time) dynamic programming approach (Bernstein et al.,
2005; Szer & Charpillet, 2006).
Nevertheless, approach two inter-related shortcomings. First, approach
finds joint controller (i.e., infinite horizon joint policy) fixed size
optimal size. Second, much graver first, fixed size, finds locally optimal
joint controller. approach guarantee finding optimal joint controller.
program presented work Amato et al. (2007a) (non-convex)
380

fiMathematical Programming DEC-POMDPs

nonlinear program (NLP). NLP finds fixed size joint controller canonical form
(i.e., form finite state machine). believe shortcomings
removed conceiving mathematical program (specifically, 0-1 mixed integer linear
program) finds joint controller sequence-form. stated earlier, main
challenge regard therefore identification sequence-form infinite
horizon policy. fact, may sequence-form characterization infinite
horizon policy obtained, could used conceiving program long horizon
(undiscounted reward) case well.
Q6: help achieve designing artificial autonomous agents ?
first sight, work direct immediate applied benefits
purpose building artificial intelligent agents understanding intelligence works.
Even limited field multi-agent planning, contributions theoretical
level practical one.
Real artificial multi-agent systems indeed modeled DEC-POMDPs, even
make use communication, common knowledge, common social law. Then, real
systems would likely made large number states, actions observations require
solutions large horizon. mathematical programming approach practically
useless setting limited DEC-POMDPs small size. models
simpler far trivial solve explicitly take account
characteristics real systems exist. works take advantage communications
(Xuan, Lesser, & Zilberstein, 2000; Ghavamzadeh & Mahadevan, 2004), existing
independencies system (Wu & Durfee, 2006; Becker, Zilberstein, Lesser, & Goldman,
2004), focus interaction agents (Thomas, Bourjot, & Chevrier, 2004),
some, said answering previous questions, rely approximate solutions, etc...
intention facilitate re-use adaptation models
concepts used work knowledge structure optimal solution
DEC-POMDP. end, decided describe MILP programs also,
importantly, derived programs making use properties
optimal DEC-POMDP solutions.
Truly autonomous agents require adapt new unforeseen situations.
work dedicated planning, seems easy argue contribute
much end either. hand, learning DEC-POMDPs never
really addressed except fringe work particular settings (Scherrer & Charpillet, 2002; Ghavamzadeh & Mahadevan, 2004; Buffet, Dutech, & Charpillet, 2007). fact,
even simple POMDPs, learning difficult task (Singh, Jaakkola, & Jordan,
1994). Currently, promising research deals learning Predictive State
Representation (PSR) POMDP (Singh, Littman, Jong, Pardoe, & Stone, 2003; James
& Singh, 2004; McCracken & Bowling, 2005). Making due allowance fundamental
differences functional role PSR histories, notice PSR histories quite similar structure. early say, might trying
learn useful histories DEC-POMDP could take inspiration way
right PSRs learned POMDPs.

381

fiAras & Dutech

9. Conclusion
designed investigated new exact algorithms solving Decentralized Partially Observable Markov Decision Processes finite horizon (DEC-POMDPs). main contribution paper use sequence-form policies, based sets histories,
order reformulate DEC-POMDP non-linear programming problem (NLP).
presented two different approaches linearize NLP order find global
optimal solutions DEC-POMDPs. first approach based combinatorial
properties optimal policies DEC-POMDPs second one relies concepts
borrowed field game theory. lead formulating DEC-POMDPs 0-1
Mixed Integer Linear Programming problems (MILPs). Several heuristics speeding
resolution MILPs make another important contribution work.
Experimental validation mathematical programming problems designed
work conducted classical DEC-POMDP problems found literature.
experiments show that, expected, MILP methods outperform classical Dynamic
Programming algorithms. But, general, less efficient costly
forward search methods GMAA*, especially case DEC-POMDP admits
many optimal policies. Nevertheless, according nature problem, MILP methods
sometimes greatly outperform GMAA* (as MA-Tiger problem).
clear exact resolution DEC-POMDPs scale size
problems length horizon, designing exact methods useful order
develop improve approximate methods. see least three research directions
work contribute. One direction could take advantage large
literature algorithms finding approximate solutions MILPs adapt
MILPs formulated DEC-POMDPs. Another direction would use knowledge
gained work derive improved heuristics guiding existing approximate existing
methods DEC-POMDPs. example, work Seuken Zilberstein (2007),
order limit memory resources used resolution algorithm, prune space
policies consider them; work could help using better estimation
policies important kept search space. Then, one direction
currently investigating adapt approach DEC-POMDPs infinite length
looking yet another representation would allow problems seen MILPs.
importantly, work participates better understanding DEC-POMDPs.
analyzed understood key characteristics nature optimal policies order
design MILPs presented paper. knowledge useful work
dealing DEC-POMDPs even POMDPs. experimentations given
interesting insights nature various problems tested, term existence
extraneous histories number optimal policies. insights might first
step toward taxonomy DEC-POMDPs.

Appendix A. Non-Convex Non-Linear Program
Using simplest example, section aims showing Non-Linear Program
(NLP) expressed Table 2 non-convex.
382

fiMathematical Programming DEC-POMDPs

Let us consider example two agents, one 2 possible actions (a b)
want solve horizon-1 decision problem. set possible joint-histories then:
ha, ai, ha, bi, hb, ai hb, bi. NLP solve is:
Variables: x1 (a), x1 (b), x2 (a), x2 (a)
Maximize

R(, ha, ai)x1 (a)x2 (a) + R(, ha, bi)x1 (a)x2 (b)

(158)

+R(, hb, ai)x1 (b)x2 (a) + R(, hb, bi)x1 (b)x2 (b)
subject
x1 (a) + x1 (b) = 1
x2 (a) + x2 (b) = 1
x1 (a) 0,

x1 (b) 0

x2 (a) 0,

x2 (b) 0

matrix formulation objective
x following kind:

0 0 c
0 0 e
C=
c e 0
f 0

function eq. (158) would xT .C.x C


f

0
0




x1 (a)
x1 (b)

x=
x2 (a) .
x2 (b)

(159)

eigen value vector v = [v1 v2 v3 v4 ]T straightforward show
eigen value: [v1 v2 v3 v4 ]T = C.[v1 v2 v3 v4 ]T . result,
matrix C, hessian objective function, positive-definite thus objective
function convex.

Appendix B. Linear Program Duality
Every linear program (LP) converse linear program called dual. first LP
called primal distinguish dual. primal maximizes quantity,
dual minimizes quantity. n variables constraints primal,
variables n constraints dual. Consider following (primal) LP.
Variables: x(i), {1, 2, , n}
Maximize

n
X

c(i)x(i)

i=1

subject to:
n
X

a(i, j)x(i) = b(j),

j = 1, 2, ,

i=1

x(i) 0,

= 1, 2, , n

383

fiAras & Dutech

primal LP one variable x(i) = 1 n. data LP consists
numbers c(i) = 1 n, numbers b(j) j = 1 numbers
a(i, j) = 1 n j = 1 m. LP thus n variables
constraints. dual LP following LP.
Variables: y(j), j {1, 2, , }


Minimize


X

b(j)y(j)

j=1

subject To:



X

a(i, j)y(j) c(i),

= 1, 2, , n

j=1

y(j) (, +),

j = 1, 2, ,

dual LP one variable y(j) j = 1 m. y(j) variable free
variable. is, allowed take value R. dual LP variables n
constraints.
theorem linear programming duality follows.
Theorem B.1. (Luenberger, 1984) either primal LP dual LP finite optimal
solution, other, corresponding values objective functions
equal.
Applying theorem primal-dual pair given above, holds,
n
X

c(i)x (i) =


X

b(j)y (j)

j=1

i=1

x denotes optimal solution primal denotes optimal solution
dual.
theorem complementary slackness follows.
Theorem B.2. (Vanderbei, 2008) Suppose x feasible primal linear program
feasible dual. Let (w1 , ,wm ) denote corresponding primal slack variables,
let (z1 , ,zn ) denote corresponding dual slack variables. x optimal
respective problems
xj zj = 0

j = 1, , n,

wi yi = 0

= 1, , m.
384

fiMathematical Programming DEC-POMDPs

Appendix C. Regret DEC-POMDPs
value information set Ii agent i-reduced joint policy q,
denoted (, q), defined by:
X
(, q) = max
R(, hh, j i)q(j )
(160)
h

j Ei

terminal information set and, non-terminal, by:
X
(h.o, q)
(, q) = max
h

(161)

oOi

Then, regret history h agent i-reduced joint policy q,
denoted (h, q), defined by:
X
(h, q) = ((h), q)
R(, hh, j i)q(j )
(162)

j Hi

h terminal and, h non-terminal, by:
(h, q) = ((h), q)

X

(h.o, q)

(163)

oOi

concept regret agent i, independant policy agent i,
useful looking optimal policy optimal value known: 0.
thus easier manipulate optimal value policy.

Appendix D. Program Changes Due Optimizations
Pruning locally globally extraneous histories reduces size search space
mathematical programs. Now, constraints programs depend size
search space, must alter constraints.
Let denote superscript sets actually used program. example, Ei
actual set terminal histories agent i, pruned extraneous histories
not.
Programs MILP (Table 3) MILP-n agents (Table 5) rely fact
number histories given length support pure policy agent fixed
equal |Oi |t1 . may case pruned sets, following changes
made:
constraint (42) MILP (121) MILP-n agents,
X

z(j) =
|Oi |T 1
jE

iI

must replaced
X

z(j)


iI

jE

385

|Oi |T 1 .

(164)

fiAras & Dutech

set constraints (41) MILP (120) MILP-n agents,
X

z(hh, j i) =

j Ei



|Ok |T 1 xi (h),

I, h Ei



|Ok |T 1 xi (h),

I, h Ei .

kI\{i}

must replaced
X

z(hh, j i)

(165)

kI\{i}

j Ei

set constraints (119) MILP-n agents,
yi ((h))

X
1
R(, hh, ji i)z(j) = wi (h),
|Oi |T 1

h Ei

jE

must replaced
yi ((h))

X
1
R(, hh, ji i)z(j) = wi (h),
|Oi |T 1

h Ei .

(166)

jE

Appendix E. Example using MA-Tiger
example derived using Decentralized Tiger Problem (MA-Tiger) described
Section 2.2. two agents, 3 actions (al , ar , ao ) 2 observations (ol , ).
consider problem horizon 2.
18 (32 2) terminal histories agent: ao .ol .ao , ao .ol .al , ao .ol .ar , ao .or .ao ,
ao .or .al , ao .or .ar , al .ol .ao , al .ol .al , al .ol .ar , al .or .ao , al .or .al , al .or .ar , ar .ol .ao , ar .ol .al ,
ar .ol .ar , ar .or .ao , ar .or .al , ar .or .ar .
thus 324 (182 = 322 22 ) joint histories agents: hao .ol .ao ,ao .ol .ao i,hao .ol .ao ,ao .ol .al i,
hao .ol .ao ,ao .ol .ar i, , har .or .ar ,ar .or .ar i.
E.1 Policy Constraints
policy constraints horizon 2 one agent MA-Tiger problem would be:
Variables: x every history
x(ao ) + x(al ) + x(ar ) = 0
x(ao ) + x(ao .ol .ao ) + x(ao .ol .al ) + x(ao .ol .ar ) = 0
x(ao ) + x(ao .or .ao ) + x(ao .or .al ) + x(ao .or .ar ) = 0
x(al ) + x(al .ol .ao ) + x(al .ol .al ) + x(al .ol .ar ) = 0
x(al ) + x(al .or .ao ) + x(al .or .al ) + x(al .or .ar ) = 0
x(ar ) + x(ar .ol .ao ) + x(ar .ol .al ) + x(ar .ol .ar ) = 0
x(ar ) + x(ar .or .ao ) + x(ar .or .al ) + x(ar .or .ar ) = 0
386

fiMathematical Programming DEC-POMDPs

x(ao ) 0

x(al ) 0

x(ar ) 0

x(ao .ol .ao ) 0 x(ao .ol .al ) 0 x(ao .ol .ar ) 0
x(ao .or .ao ) 0 x(ao .or .al ) 0 x(ao .or .ar ) 0
x(al .ol .ao ) 0

x(al .ol .al ) 0

x(al .ol .ar ) 0

x(al .or .ao ) 0 x(al .or .al ) 0 x(al .or .ar ) 0
x(ar .ol .ao ) 0 x(ar .ol .al ) 0 x(ar .ol .ar ) 0
x(ar .or .ao ) 0 x(ar .or .al ) 0 x(ar .or .ar ) 0
E.2 Non-Linear Program MA-Tiger
Non-Linear Program finding optimal sequence-form policy MA-Tiger
horizon 2 would be:
Variables: xi every history agent

Maximize

R(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )x2 (ao .ol .ao )
+ R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )x2 (ao .ol .al )
+ R(, hao .ol .ao , ao .ol .ar i)x1 (ao .ol .ao )x2 (ao .ol .ar )
+

subject to:
x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0
x1 (al ) + x1 (al .ol .ao ) + x1 (al .ol .al ) + x1 (al .ol .ar ) = 0
x1 (al ) + x1 (al .or .ao ) + x1 (al .or .al ) + x1 (al .or .ar ) = 0
x1 (ar ) + x1 (ar .ol .ao ) + x1 (ar .ol .al ) + x1 (ar .ol .ar ) = 0
x1 (ar ) + x1 (ar .or .ao ) + x1 (ar .or .al ) + x1 (ar .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0
x2 (al ) + x2 (al .ol .ao ) + x2 (al .ol .al ) + x2 (al .ol .ar ) = 0
x2 (al ) + x2 (al .or .ao ) + x2 (al .or .al ) + x2 (al .or .ar ) = 0
x2 (ar ) + x2 (ar .ol .ao ) + x2 (ar .ol .al ) + x2 (ar .ol .ar ) = 0
x2 (ar ) + x2 (ar .or .ao ) + x2 (ar .or .al ) + x2 (ar .or .ar ) = 0
387

fiAras & Dutech

x1 (ao ) 0

x1 (al ) 0

x1 (ar ) 0

x1 (ao .ol .ao ) 0 x1 (ao .ol .al ) 0 x1 (ao .ol .ar ) 0
x1 (ao .or .ao ) 0 x1 (ao .or .al ) 0 x1 (ao .or .ar ) 0
x1 (al .ol .ao ) 0

x1 (al .ol .al ) 0

x1 (al .ol .ar ) 0

x1 (al .or .ao ) 0 x1 (al .or .al ) 0 x1 (al .or .ar ) 0
x1 (ar .ol .ao ) 0 x1 (ar .ol .al ) 0 x1 (ar .ol .ar ) 0
x1 (ar .or .ao ) 0 x1 (ar .or .al ) 0 x1 (ar .or .ar ) 0

x2 (ao ) 0

x2 (al ) 0

x2 (ar ) 0

x2 (ao .ol .ao ) 0 x2 (ao .ol .al ) 0 x2 (ao .ol .ar ) 0
x2 (ao .or .ao ) 0 x2 (ao .or .al ) 0 x2 (ao .or .ar ) 0
x2 (al .ol .ao ) 0

x2 (al .ol .al ) 0

x2 (al .ol .ar ) 0

x2 (al .or .ao ) 0 x2 (al .or .al ) 0 x2 (al .or .ar ) 0
x2 (ar .ol .ao ) 0 x2 (ar .ol .al ) 0 x2 (ar .ol .ar ) 0
x2 (ar .or .ao ) 0 x2 (ar .or .al ) 0 x2 (ar .or .ar ) 0

E.3 MILP MA-Tiger
MILP horizon 2 agents MA-Tiger problem would be:
Variables:
xi (h) every history agent
z(j) every terminal joint history

Maximize

R(, hao .ol .ao , ao .ol .ao i)z(hao .ol .ao , ao .ol .ao i)
+ R(, hao .ol .ao , ao .ol .al i)z(hao .ol .ao , ao .ol .al i)
+ R(, hao .ol .ao , ao .ol .ar i)z(hao .ol .ao , ao .ol .ar i)
+
388

fiMathematical Programming DEC-POMDPs

subject to:
x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0

z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .ao , ao .ol .al i) + z(hao .ol .ao , ao .ol .ar i) = 2 x1 (ao .ol .ao )
z(hao .ol .ao , ao .ol .ao i) + z(hao .ol .al , ao .ol .ao i) + z(hao .ol .ar , ao .ol .ao i) = 2 x2 (ao .ol .ao )
z(hao .ol .al , ao .ol .ao i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .al , ao .ol .ar i) = 2 x1 (ao .ol .al )
z(hao .ol .ao , ao .ol .al i) + z(hao .ol .al , ao .ol .al i) + z(hao .ol .ar , ao .ol .al i) = 2 x2 (ao .ol .al )


x1 (ao ) 0

x1 (al ) 0

x1 (ar ) 0

x1 (ao .ol .ao ) {0, 1}

x1 (ao .ol .al ) {0, 1}

x1 (ao .ol .ar ) {0, 1}

x1 (ao .or .ao ) {0, 1}

x1 (ao .or .al ) {0, 1}

x1 (ao .or .ar ) {0, 1}


x2 (ao ) 0

x2 (al ) 0

x2 (ar ) 0

x2 (ao .ol .ao ) {0, 1}

x2 (ao .ol .al ) {0, 1}

x2 (ao .ol .ar ) {0, 1}

x2 (ao .or .ao ) {0, 1}

x2 (ao .or .al ) {0, 1}

x2 (ao .or .ar ) {0, 1}


z(hao .ol .ao , ao .ol .ao i) {0, 1} z(hao .ol .ao , ao .ol .al i) {0, 1} z(hao .ol .ao , ao .ol .ar i) {0, 1}
z(hao .ol .al , ao .ol .ao i) {0, 1} z(hao .ol .al , ao .ol .al i) {0, 1} z(hao .ol .al , ao .ol .ar i) {0, 1}

E.4 MILP-2 Agents MA-Tiger
MILP-2 agents horizon 2 agents MA-Tiger problem would be:
Variables:
xi (h), wi (h) bi (h) every history agent
yi ()) agent every information set
Maximize

y1 ()

389

fiAras & Dutech

subject to:

x1 (ao ) + x1 (al ) + x1 (ar ) = 0
x1 (ao ) + x1 (ao .ol .ao ) + x1 (ao .ol .al ) + x1 (ao .ol .ar ) = 0
x1 (ao ) + x1 (ao .or .ao ) + x1 (ao .or .al ) + x1 (ao .or .ar ) = 0

x2 (ao ) + x2 (al ) + x2 (ar ) = 0
x2 (ao ) + x2 (ao .ol .ao ) + x2 (ao .ol .al ) + x2 (ao .ol .ar ) = 0
x2 (ao ) + x2 (ao .or .ao ) + x2 (ao .or .al ) + x2 (ao .or .ar ) = 0

y1 () y1 (ao .ol ) y1 (ao .or ) = w1 (ao )
y1 () y1 (al .ol ) y1 (al .or ) = w1 (al )
y1 () y1 (ar .ol ) y1 (ar .or ) = w1 (ar )
y2 () y2 (ao .ol ) y2 (ao .or ) = w2 (ao )
y2 () y2 (al .ol ) y2 (al .or ) = w2 (al )
y2 () y2 (ar .ol ) y2 (ar .or ) = w2 (ar )

y1 (ao .ol ) R(, hao .ol .ao , ao .ol .ao i)x2 (ao .ol .ao )
R(, hao .ol .ao , ao .ol .al i)x2 (ao .ol .al )
R(, hao .ol .ao , ao .ol .ar i)x2 (ao .ol .ar )
R(, hao .ol .ao , al .ol .ao i)x2 (al .ol .ao )
R(, hao .ol .ao , al .ol .al i)x2 (al .ol .al )
R(, hao .ol .ao , al .ol .ar i)x2 (al .ol .ar )
= w1 (ao .ol .ao )

y1 (ao .ol ) R(, hao .ol .al , ao .ol .ao i)x2 (ao .ol .ao )
R(, hao .ol .al , ao .ol .al i)x2 (ao .ol .al )
R(, hao .ol .al , ao .ol .ar i)x2 (ao .ol .ar )
R(, hao .ol .al , al .ol .ao i)x2 (al .ol .ao )
R(, hao .ol .al , al .ol .al i)x2 (al .ol .al )
R(, hao .ol .al , al .ol .ar i)x2 (al .ol .ar )
= w1 (ao .ol .al )
390

fiMathematical Programming DEC-POMDPs


y1 (ar .or ) R(, har .or .ar , ao .ol .ao i)x2 (ao .ol .ao )
R(, har .or .ar , ao .ol .al i)x2 (ao .ol .al )
R(, har .or .ar , ao .ol .ar i)x2 (ao .ol .ar )
R(, har .or .ar , al .ol .ao i)x2 (al .ol .ao )
R(, har .or .ar , al .ol .al i)x2 (al .ol .al )
R(, har .or .ar , al .ol .ar i)x2 (al .ol .ar )
= w1 (ar .or .ar )
y2 (ao .ol ) R(, hao .ol .ao , ao .ol .ao i)x1 (ao .ol .ao )
R(, hao .ol .al , ao .ol .ao i)x1 (ao .ol .al )
R(, hao .ol .ar , ao .ol .ao i)x1 (ao .ol .ar )
R(, hal .ol .ao , ao .ol .ao i)x1 (al .ol .ao )
R(, hal .ol .al , ao .ol .ao i)x1 (al .ol .al )
R(, hal .ol .ar , ao .ol .ao i)x1 (al .ol .ar )
= w2 (ao .ol .ao )
y2 (ao .ol ) R(, hao .ol .ao , ao .ol .al i)x1 (ao .ol .ao )
R(, hao .ol .al , ao .ol .al i)x1 (ao .ol .al )
R(, hao .ol .ar , ao .ol .al i)x1 (ao .ol .ar )
R(, hal .ol .ao , ao .ol .al i)x1 (al .ol .ao )
R(, hal .ol .al , ao .ol .al i)x1 (al .ol .al )
R(, hal .ol .ar , ao .ol .al i)x1 (al .ol .ar )
= w2 (ao .ol .al )


x1 (ao ) 1 b1 (ao )

x1 (al ) 1 b1 (al )

x1 (ar ) 1 b1 (ar )

x1 (ao .ol .ao ) 1 b1 (ao .ol .ao )

x1 (ao .ol .al ) 1 b1 (ao .ol .al )

x1 (ao .ol .ar ) 1 b1 (ao .ol .ar )



w1 (ao ) U1 (ao )b1 (ao )

w1 (al ) U1 (al )b1 (al )

w1 (ar ) U1 (ar )b1 (ar )

w1 (ao .ol .ao ) U1 (ao .ol .ao )b1 (ao .ol .ao )

w1 (ao .ol .al ) U1 (ao .ol .al )b1 (ao .ol .al )

w1 (ao .ol .ar ) U1 (ao .ol .ar )b1 (ao .ol .ar )


391

fiAras & Dutech

x1 (ao ) 0
x1 (ao .ol .ao ) 0

x1 (al ) 0
x1 (ao .ol .al ) 0

x1 (ar ) 0
x1 (ao .ol .ar ) 0


w1 (ao ) 0
w1 (ao .ol .ao ) 0

w1 (al ) 0
w1 (ao .ol .al ) 0

w1 (ar ) 0
w1 (ao .ol .ar ) 0


b1 (ao ) {0, 1}
b1 (ao .ol .ao ) {0, 1}

b1 (al ) {0, 1}
b1 (ao .ol .al ) {0, 1}


y1 () (, +)
y1 (ao .ol ) (, +) y1 (ao .or ) (, +)

... agent 2

392

b1 (ar ) {0, 1}
b1 (ao .ol .ar ) {0, 1}

fiMathematical Programming DEC-POMDPs

References
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007a). Optimizing memory-bounded controllers decentralized POMDPs. Proc. Twenty-Third Conf. Uncertainty
Artificial Intelligence (UAI-07).
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007b). Solving POMDPs using quadratically
constrained linear programs. Proc. Twentieth Int. Joint Conf. Artificial
Intelligence (IJCAI07).
Amato, C., Carlin, A., & Zilberstein, S. (2007c). Bounded dynamic programming
decentralized POMDPs. Proc. Workshop Multi-Agent Sequential Decision
Making Uncertain Domains (MSDM) AAMAS07.
Amato, C., Dibangoye, J., & Zilberstein, S. (2009). Incremental policy generation finitehorizon DEC-POMDPs. Proc. Nineteenth Int. Conf. Automated Planning
Scheduling (ICAPS-09).
Anderson, B., & Moore, J. (1980). Time-varying feedback laws decentralized control.
Nineteenth IEEE Conference Decision Control including Symposium
Adaptive Processes, 19(1), 519524.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. (2004). Solving transition independent
decentralized Markov decision processes. Journal Artificial Intelligence Research,
22, 423455.
Bellman, R. (1957). Dynamic programming. Princeton University Press, Princeton, NewJersey.
Bernstein, D., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research,
27 (4), 819840.
Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iteration
decentralized POMDPs. Proc. Nineteenth Int. Joint Conf. Artificial
Intelligence (IJCAI), pp. 12871292.
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralized
pomdps lossless policy compression. Proc. Int. Conf. Automated
Planning Scheduling (ICAPS08).
Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.
Proceedings 6th Conference Theoretical Aspects Rationality Knowledge (TARK 96), De Zeeuwse Stromen, Nederlands.
Buffet, O., Dutech, A., & Charpillet, F. (2007). Shaping multi-agent systems gradient reinforcement learning. Autonomous Agent Multi-Agent System Journal
(AAMASJ), 15 (2), 197220.
393

fiAras & Dutech

Cassandra, A., Kaelbling, L., & Littman, M. (1994). Acting optimally partially observable
stochastic domains. Proc. 12th Nat. Conf. Artificial Intelligence (AAAI).
Chades, I., Scherrer, B., & Charpillet, F. (2002). heuristic approach solving
decentralized-POMDP: assessment pursuit problem. Proc. 2002 ACM
Symposium Applied Computing, pp. 5762.
Cornuejols, G. (2008). Valid inequalities mixed integer linear programs. Mathematical
Programming B, 112, 344.
Dantzig, G. B. (1960). significance solving linear programming problems
integer variables. Econometrica, 28(1), 3044.
dEpenoux, F. (1963). probabilistic production inventory problem. Management
Science, 10(1), 98108.
Diwekar, U. (2008). Introduction Applied Optimization (2 edition). Springer.
Drenick, R. (1992). Multilinear programming: Duality theories. Journal Optimization
Theory Applications, 72(3), 459486.
Fletcher, R. (1987). Practical Methods Optimization. John Wiley & Sons, New York.
Ghavamzadeh, M., & Mahadevan, S. (2004). Learning communicate act cooperative multiagent systems using hierarchical reinforcement learning. Proc. 3rd
Int. Joint Conf. Autonomous Agents Multi-Agent Systems (AAMAS04).
Govindan, S., & Wilson, R. (2001). global newton method compute Nash equilibria.
Journal Economic Theory, 110, 6586.
Hansen, E., Bernstein, D., & Zilberstein, S. (2004). Dynamic programming partially
observable stochastic games. Proc. Nineteenth National Conference Artificial Intelligence (AAAI-04).
Horst, R., & Tuy, H. (2003). Global Optimization: Deterministic Approaches (3rd edition).
Springer.
James, M., & Singh, S. (2004). Learning discovery predictive state representations
dynamical systems reset. Proc. Twenty-first Int. Conf. Machine
Learning (ICML04).
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101, 99134.
Koller, D., Megiddo, N., & von Stengel, B. (1994). Fast algorithms finding randomized
strategies game trees. Proceedings 26th ACM Symposium Theory
Computing (STOC 94), pp. 750759.
Koller, D., & Megiddo, N. (1996). Finding mixed strategies small supports extensive
form games. International Journal Game Theory, 25(1), 7392.
394

fiMathematical Programming DEC-POMDPs

Lemke, C. (1965). Bimatrix Equilibrium Points Mathematical Programming. Management Science, 11(7), 681689.
Luenberger, D. (1984). Linear Nonlinear Programming. Addison-Wesley Publishing
Company, Reading, Massachussetts.
McCracken, P., & Bowling, M. H. (2005). Online discovery learning predictive state
representations. Advances Neural Information Processing Systems 18 (NIPS05).
Nair, R., Tambe, M., Yokoo, M., Pynadath, D., & Marsella, S. (2003). Taming decentralized
POMDPs: towards efficient policy computation multiagent setting. Proc.
Int. Joint Conference Artificial Intelligence, IJCAI03.
Oliehoek, F., Spaan, M., & Vlassis, N. (2008). Optimal approximate Q-value functions
decentralized POMDPs. Journal Artificial Intelligence Research (JAIR), 32,
289353.
Oliehoek, F., Whiteson, S., & Spaan, M. (2009). Lossless clustering histories decentralized POMDPs. Proc. International Joint Conference Autonomous
Agents Multi Agent Systems, pp. 577584.
Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT Press,
Cambridge, Mass.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial Optimization: Algorithms
Complexity. Dover Publications.
Papadimitriou, C. H., & Tsitsiklis, J. (1987). Complexity Markov Decision Processes. Mathematics Operations Research, 12 (3), 441 450.
Parsons, S., & Wooldridge, M. (2002). Game theory decision theory multi-agent
systems. Autonomous Agents Multi-Agent Systems (JAAMAS), 5(3), 243254.
Petrik, M., & Zilberstein, S. (2007). Average-reward decentralized Markov decision processes. Proc. Twentieth Int. Joint Conf. Artificial Intelligence (IJCAI
2007).
Petrik, M., & Zilberstein, S. (2009). bilinear programming approach multiagent
planning. Journal Artificial Intelligence Research (JAIR), 35, 235274.
Puterman, M. (1994). Markov Decision Processes: discrete stochastic dynamic programming. John Wiley & Sons, Inc. New York, NY.
Pynadath, D., & Tambe, M. (2002). Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories Models. Journal Artificial Intelligence
Research, 16, 389423.
Radner, R. (1959). application linear programming team decision problems.
Management Science, 5, 143150.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: modern approach. Prentice Hall.
395

fiAras & Dutech

Sandholm, T. (1999). Multiagent systems, chap. Distributed rational decision making, pp.
201258. MIT Press. Ed. G. Weiss.
Sandholm, T., Gilpin, A., & Conitzer, V. (2005). Mixed-integer programming methods
finding nash equilibria. Proc. National Conference Artificial Intelligence
(AAAI).
Scherrer, B., & Charpillet, F. (2002). Cooperative co-learning: model based approach
solving multi agent reinforcement problems. Proc. IEEE Int. Conf.
Tools Artificial Intelligence (ICTAI02).
Seuken, S., & Zilberstein, S. (2007). Memory-bounded dynamic programming DECPOMDPs. Proc. Twentieth Int. Joint Conf. Artificial Intelligence (IJCAI07).
Singh, S., Jaakkola, T., & Jordan, M. (1994). Learning without state estimation partially
observable markovian decision processes.. Proceedings Eleventh International
Conference Machine Learning.
Singh, S., Littman, M., Jong, N., Pardoe, D., & Stone, P. (2003). Learning predictive state
representations. Proc. Twentieth Int. Conf. Machine Learning (ICML03).
Szer, D., & Charpillet, F. (2006). Point-based Dynamic Programming DEC-POMDPs.
Proc. Twenty-First National Conf. Artificial Intelligence (AAAI 2006).
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm
solving decentralized POMDPs. Proc. Twenty-First Conf. Uncertainty
Artificial Intelligence (UAI05), pp. 576 583.
Thomas, V., Bourjot, C., & Chevrier, V. (2004). Interac-DEC-MDP : Towards use
interactions DEC-MDP. Proc. Third Int. Joint Conf. Autonomous
Agents Multi-Agent Systems (AAMAS04), New York, USA, pp. 14501451.
Vanderbei, R. J. (2008). Linear Programming: Foundations Extensions (3rd edition).
Springer.
von Stengel, B. (2002). Handbook Game Theory, Vol. 3, chap. 45-Computing equilibria
two-person games, pp. 17231759. North-Holland, Amsterdam.
Wu, J., & Durfee, E. H. (2006). Mixed-integer linear programming transitionindependent decentralized MDPs. Proc. fifth Int. Joint Conf. Autonomous
Agents Multiagent Systems (AAMAS06), pp. 10581060 New York, NY, USA.
ACM.
Xuan, P., Lesser, V., & Zilberstein, S. (2000). Communication multi-agent Markov
decision processes. Proc. ICMAS Workshop Game Theoretic Decision
Theoretics Agents Boston, MA.

396


