Journal Artificial Intelligence Research 8 (1998) 67-91

Submitted 7/97; published 3/98

Cached Sucient Statistics Ecient Machine Learning
Large Datasets
Andrew Moore
Mary Soon Lee

School Computer Science Robotics Institute
Carnegie Mellon University, Pittsburgh PA 15213

awm@cs.cmu.edu
mslee@cs.cmu.edu

Abstract

paper introduces new algorithms data structures quick counting machine
learning datasets. focus counting task constructing contingency tables,
approach applicable counting number records dataset match
conjunctive queries. Subject certain assumptions, costs operations
shown independent number records dataset loglinear
number non-zero entries contingency table.
provide sparse data structure, ADtree, minimize memory use.
provide analytical worst-case bounds structure several models data distribution. empirically demonstrate tractably-sized data structures produced
large real-world datasets (a) using sparse tree structure never allocates memory
counts zero, (b) never allocating memory counts deduced
counts, (c) bothering expand tree fully near leaves.
show ADtree used accelerate Bayes net structure finding algorithms, rule learning algorithms, feature selection algorithms, provide
number empirical results comparing ADtree methods traditional direct counting
approaches. discuss possible uses ADtrees machine learning methods, discuss merits ADtrees comparison alternative representations
kd-trees, R-trees Frequent Sets.

1. Caching Sucient Statistics

Computational eciency important concern machine learning algorithms, especially
applied large datasets (Fayyad, Mannila, & Piatetsky-Shapiro, 1997; Fayyad &
Uthurusamy, 1996) real-time scenarios. earlier work showed kd-trees
multiresolution cached regression matrix statistics enable fast locally weighted
instance based regression (Moore, Schneider, & Deng, 1997). paper, attempt
accelerate predictions symbolic attributes using kind kd-tree splits
dimensions nodes.
Many machine learning algorithms operating datasets symbolic attributes need
frequent counting. work applicable Online Analytical Processing (OLAP)
applications data mining, operations large datasets multidimensional
database access, DataCube operations (Harinarayan, Rajaraman, & Ullman, 1996),
association rule learning (Agrawal, Mannila, Srikant, Toivonen, & Verkamo, 1996) could
accelerated fast counting.
Let us begin establishing notation. given data set R records
attributes. attributes called a1; a2; : : : . value attribute ai
c 1998 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiMoore & Lee

th record small integer lying range f1; 2; : : : ni g ni called arity
attribute i. Figure 1 gives example.
k

Attributes

a1

a2

a3

n 1= 2

n 2= 4

n 3= 2

Record 1

1

1

1

M=3

Record 2

2

3

1

R=6

Record 3

2

4

2

Record 4

1

3

1

Record 5

2

3

1

Record 6

1

3

1

Arity

Figure 1: simple dataset used example. R = 6 records
= 3 attributes.

1.1 Queries

query set (attribute = value) pairs left hand sides pairs form
subset fa1 : : : g arranged increasing order index. Four examples queries
dataset
(a1 = 1); (a2 = 3; a3 = 1); (); (a1 = 2; a2 = 4; a3 = 2)

(1)

Notice total number possible queries
i=1 (ni + 1).
attribute either appear query one ni values may take, may
omitted (which equivalent giving ai = * \don't care" value).

1.2 Counts

count query, denoted C (Query) simply number records dataset
matching (attribute = value) pairs Query. example dataset find:
( 1 = 1)
C (a2 = 3; a3 = 1)
C ()
C (a1 = 2; a2 = 4; a3 = 2)
C

1.3 Contingency Tables

=
=
=
=

3
4
6
1

subset attributes, ai(1) : : : ai(n) , associated contingency table denoted
ct(ai(1) : : : ai(n)). table row possible sets values
ai(1) : : : ai(n) . row corresponding ai(1) = v1 : : : ai(n) = vn records count C (ai(1) =
3
v1 : : : ai(n) = vn ). example dataset 3 attributes 2 = 8 contingency tables
exist, depicted Figure 2.
68

fiCached Sufficient Statistics Efficient Machine Learning

ct()

ct(a1 )

ct(a3 )

#

#

#

#
1

2

3

6

1 3
2 3

1 5
2 1

1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2

1
1
2
2
3
3
4
4
1
1
2
2
3
3
4
4

1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2

1

ct(a2 )

ct(a1,a2,a3 )

3

#
2

1
2
3
4

1
0
4
1

ct(a1,a2 )

ct(a1, a3 )
#
1

3

1
1
2
2

1
2
1
2

3
0
2
1

ct(a2,a3 )

#
1

2

#
2

3

1
1
1
1
2
2
2
2

1
2
3
4
1
2
3
4

1
1
2
2
3
3
4
4

1
2
1
2
1
2
1
2

1
0
2
0
0
0
2
1

1
0
0
0
4
0
0
1

1
0
0
0
2
0
0
0
0
0
0
0
2
0
0
1

Figure 2: eight possible contingency tables dataset Figure 1.
conditional contingency table, written
ct(ai(1) : : : ai(n) j aj(1) = u1; : : : aj(p) = up)

(2)

contingency table subset records dataset match query
right j symbol. example,
a1

1
ct(a1 ; a3 j a2 = 3) = 1
2
2

a3

1
2
1
2

#
2
0
2
0

Contingency tables used variety machine learning applications, including
building probability tables Bayes nets evaluating candidate conjunctive rules
rule learning algorithms (Quinlan, 1990; Clark & Niblett, 1989). would thus desirable
able perform counting eciently.
prepared pay one-time cost building caching data structure,
easy suggest mechanism counting constant time. possible query,
precompute contingency table. total amount numbers stored memory
data structure would
i=1 (ni + 1), even humble dataset Figure 1
45, revealed Figure 2. real dataset ten attributes medium
arity, fifteen binary attributes, far large fit main memory.
would retain speed precomputed contingency tables without incurring
intractable memory demand. subject paper.

2. Cache Reduction 1: Dense ADtree Caching Sucient Statistics

First describe ADtree, data structure use represent set
possible counts. initial simplified description obvious tree representation
yield immediate memory savings,1 later provide several opportunities
1. SE-tree (Rymon, 1993) similar data structure.

69

fiMoore & Lee

cutting zero counts redundant counts. structure shown Figure 3.
ADtree node (shown rectangle) child nodes called \Vary nodes" (shown ovals).
ADnode represents query stores number records match query (in
C = # field). Vary aj child ADnode one child nj values
attribute aj . kth child represents query Vary aj 's parent,
additional constraint aj = k.
=*
1

.
.
.

= *
c=#

Vary 1
=1

=n

=*

=*

=*

=1

..
.

..
.

1
2

..
.

1

...

2

...

Vary 2

1

=*

1
2

1

...

=n
2

..
.

2

= *

= *

= *

= *

c=#

c=#

c=#

c=#

...

...

...
Vary 2

Vary

Vary 3

Vary

..
.

..
.

...
Vary

Figure 3: top ADnodes ADtree, described text.
Notes regarding structure:

Although drawn diagram, description query (e.g.,

= 1; a2 =
*; ::aM = * leftmost ADnode second level) explicitly recorded
ADnode. contents ADnode simply count set pointers
Vary aj children.
contents Vary aj node set pointers ADnodes.
1



cost looking count proportional number instantiated variables

query. example, look C (a7 = 2; a13 = 1; a22 = 3) would follow
following path tree: Vary a7 ! a7 = 2 ! Vary a13 ! a13 = 1 ! Vary a22 !
a22 = 3. count obtained resulting node.

Notice node ADN Vary
Vary ai+1 Vary

ai

+2

ai

parent, ADN's children

... Vary .

necessary store Vary nodes indices i+1 information
obtained another path tree.
70

fiCached Sufficient Statistics Efficient Machine Learning

2.1 Cutting nodes counts zero

described, tree sparse contain exactly
i=1 (ni + 1) nodes. Sparseness
easily achieved storing NULL instead node query matches zero
records. specializations query count zero
appear anywhere tree. datasets reduce number
numbers need stored. example, dataset Figure 1, previously
needed 45 numbers represent contingency tables, need 22 numbers.

3. Cache Reduction II: Sparse ADtree

easy devise datasets benefit failing store counts zero.
Suppose binary attributes 2M records kth record bits
binary representation k. query count zero tree contains 3M
nodes. reduce tree size despite this, take advantage observation
many counts stored tree redundant.
Vary aj node ADtree stores nj subtrees|one subtree value
aj . Instead, find common values aj (call MCV) store
NULL place MCVth subtree. remaining nj , 1 subtrees represented
before. example simple dataset given Figure 4. Vary aj node
records values common MCV field. Appendix B describes
straightforward algorithm building ADtree.
see Section 4, still possible build full exact contingency tables (or
give counts specific queries) time slightly longer full ADtree
Section 2. first let us examine memory consequences representation.
Appendix shows binary attributes, given attributes R records,
number nodes needed store tree bounded 2M worst case (and
much less R < 2M ). contrast, amount memory needed dense tree
Section 2 3M worst case.
Notice Figure 4 MCV value context dependent. Depending constraints
parent nodes, a2's MCV sometimes 1 sometimes 2. context dependency
provide dramatic savings (as frequently case) correlations among
attributes. discussed Appendix A.

4. Computing Contingency Tables Sparse ADtree

Given ADtree, wish able quickly construct contingency tables arbitrary set attributes fai(1) : : : ai(n) g.
Notice conditional contingency table ct(ai(1) : : : ai(n) j Query) built recursively. first build

ct(
ct(

(2) : : : ai(n) j ai(1) = 1; Query)
ai(2) : : : ai(n) j ai(1) = 2; Query)

ai

ct(

ai

(2) : : : ai(n)

j

..
.

(1) = ni(1); Query)

ai

71

fiMoore & Lee

=*
1
=*
2

c =8
Vary 2
mcv = 2

Vary 1
mcv = 3

=1

=2

NULL

=*

=*

(mcv)

c =1

c =3

1
2

Vary 2
mcv = 2

1
2

=*
1
=1

NULL
(mcv)

2

c =3



Vary 2
mcv = 1

NULL

NULL

NULL

=2

Count 0

(mcv)

(mcv)

=2

1

1
2
2
2
3
3
3
3

1
2

c =1



2

2
1
1
2
1
2
2
2

Figure 4: sparse ADtree built dataset shown bottom
right. common value a1 3, a1 = 3 subtree
Vary a1 child root node NULL. Vary a2
nodes common child set NULL (which child
common depends context).
example, build ct(a1; a3) using dataset Figure 1, build ct(a3 j a1 = 1)
ct(a3 j a1 = 2) combine Figure 5.
ct(a3 | 1 = 1)

ct(a1 , a3 )

a3 #
1
2

3
0

ct(a3 | 1 = 2)
a3 #
1
2

a1

a3 #

1
1

1
2

3
0

2
2

1
2

2
1

2
1

Figure 5: example (using numbers Figure 1) contingency
tables combined recursively form larger contingency tables.
building conditional contingency table ADtree, need
explicitly specify query condition. Instead, supply ADnode ADtree,
implicitly equivalent information. algorithm is:

72

fiCached Sufficient Statistics Efficient Machine Learning

MakeContab( f

ai(1) : : : ai(n) g , ADN)
Let VN := Vary ai(1) subnode ADN.
Let MCV := VN:MCV.
k := 1; 2; : : : ; ni(1)
k 6= MCV
Let ADNk := ai(1) = k subnode VN.
CTk := MakeContab(fai(2) : : : ai(n) g; ADNk ).

CTMCV

:=

(calculated explained below)

Return concatenation CT1 : : : CTn .
i(1)

base case recursion occurs first argument empty, case
return one-element contingency table containing count associated current
ADnode, ADN.
omission algorithm. iteration k 2 f1; 2; : : : ni(1)g
unable compute conditional contingency table CTMCV ai(1) = MCV
subtree deliberately missing per Section 3. instead?
take advantage following property contingency tables:

ct(

ai

(2) : : : ai(n) j Query) =

X

ni(1)

ct(

(2) : : : ai(n)

ai

k=1

j

(1) = k; Query)

ai

(3)

value ct(ai(2) : : : ai(n) j Query) computed within algorithm calling

MakeContab(f

(2) : : : ai(n) g; ADN)

(4)

ai

missing conditional contingency table algorithm computed
following row-wise subtraction:
X
CTMCV := MakeContab(fai(2) : : : ai(n) g; ADN) ,
CTk
(5)
k6=M CV

Frequent Sets (Agrawal et al., 1996), traditionally used learning association
rules, used computing counts. recent paper (Mannila & Toivonen, 1996),
employs similar subtraction trick, calculates counts Frequent Sets.
Section 8 discuss strengths weaknesses Frequent Sets comparison
ADtrees.

4.1 Complexity building contingency table

cost computing contingency table? Let us consider theoretical worstcase cost computing contingency table n attributes arity k|note
cost unrealistically pessimistic (except k = 2), contingency tables
sparse, discussed later. assumption attributes arity, k,
made simplify calculation worst-case cost, needed code.
73

fiMoore & Lee

contingency table n attributes kn entries. Write C (n) = cost computing
contingency table. top-level call MakeContab k calls build
contingency tables n , 1 attributes: k , 1 calls build CT(ai(2) : : : ai(n) j
ai(1) = j; Query) every j f1; 2; : : : k g except MCV, final call build
CT(ai(2) : : : ai(n) j Query). k , 1 subtractions contingency tables,
require kn,1 numeric subtractions.
C (0)
= 1
(6)
n,1
C (n)
= kC (n , 1) + (k , 1)k
n > 0
(7)
solution recurrence relation C (n) = (1 + n(k , 1))kn,1 ; cost loglinear
size contingency table. comparison, used cached data structure,
simply counted dataset order build contingency table would
need O(nR + kn ) operations R number records dataset. thus
cheaper standard counting method kn R. interested large datasets
R may 100; 000. case method present several
order magnitude speedup for, say, contingency table eight binary attributes. Notice
cost independent M, total number attributes dataset,
depends upon (almost always much smaller) number attributes n requested
contingency table.

4.2 Sparse representation contingency tables

practice, represent contingency tables multidimensional arrays, rather
tree structures. gives slow counting approach ADtree approach
substantial computational advantage cases contingency table sparse, i.e.
many zero entries. Figure 6 shows sparse contingency table representation.
mean average-case behavior much faster worst case contingency tables
large numbers attributes high-arity attributes.
Indeed, experiments Section 7 show costs rising much slowly O(nkn,1 )
n increases. Note using sparse representation, worst-case
MakeContab O(min(nR; nkn,1)) R maximum possible number
non-zero contingency table entries.

5. Cache Reduction III: Leaf-Lists

introduce scheme reducing memory use. worth building
ADtree data structure small number records. example, suppose
15 records 40 binary attributes. analysis Appendix shows us
worst case ADtree might require 10701 nodes. computing contingency tables
using resulting ADtree would, records, faster conventional
counting approach, would merely require us retain dataset memory.
Aside concluding ADtrees useful small datasets,
leads final method saving memory large ADtrees. ADtree node fewer
Rmin records expand subtree. Instead maintains list pointers
original dataset, explicitly listing records match current ADnode.
list pointers called leaf-list. Figure 7 gives example.
74

fiCached Sufficient Statistics Efficient Machine Learning

v=1

ct(a1,a 2,a 3)

v=1

3

#
1

2

3

1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2

1
1
2
2
3
3
4
4
1
1
2
2
3
3
4
4

1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2

1
0
0
0
2
0
0
0
0
0
0
0
2
0
0
1

v=2
v=1

2

v=3

v=4

a1

v=1

v=2





v=2

NULL

NULL

NULL

v=1



3

v=4

2

NULL

2

v=3

NULL

NULL

3

v=2

v=2

v=1



1



v=2
v=1



2
NULL
NULL

3

v=2

1

Figure 6: right hand figure sparse representation contingency table left.
use leaf-lists one minor two major consequences. minor consequence need include straightforward change contingency table generating
algorithm handle leaf-list nodes. minor alteration described here. first
major consequence dataset must retained main memory
algorithms inspect leaf-lists access rows data pointed leaf-lists.
second major consequence ADtree may require much less memory.
documented Section 7 worst-case bounds provided Appendix A.

6. Using ADtrees Machine Learning
see Section 7, ADtree structure substantially speed computation
contingency tables large real datasets. machine learning statistical
algorithms take advantage this? provide three examples: Feature Selection,
Bayes net scoring rule learning. seems likely many algorithms
benefit, example stepwise logistic regression, GMDH (Madala & Ivakhnenko, 1994),
text classification. Even decision tree (Quinlan, 1983; Breiman, Friedman, Olshen, &
Stone, 1984) learning may benefit. 2 future work examine ways speed
nearest neighbor memory-based queries using ADtrees.
2. depends whether cost initially building ADtree amortized many runs
decision tree algorithm. Repeated runs decision tree building occur one using wrapper
model feature selection (John, Kohavi, & P eger, 1994), one using intensive search
tree structures traditional greedy search (Quinlan, 1983; Breiman et al., 1984)

75

fiMoore & Lee

a1 = *
a2 = *
a3 = *
c =9
Row 1 2 3

a1 = 1
a2 = *
a3 = *
c =3
See rows 1,2,3

Vary 1

Vary 2

Vary 3

1

1

1

1

mcv = 2

mcv = 1

mcv = 1

2

1

1

2

3

1

2

1

4

2

2

2

5

2

1

1

6

2

1

2

7

2

2

1

Vary 3

8

3

2

2

mcv = 1

9

3

1

1

NULL
(mcv)

a1 = 3
a2 = *
a3 = *

NULL
(mcv)

a1 = *
a2 = 2
a3 = *

c =2

NULL
(mcv)

a1 = *
a2 = *
a3 = 2

c =4

c =4

See rows 8,9

NULL
(mcv)

a1 = *
a2 = 2
a3 = 2
c =2
See rows 4,8

Figure 7: ADtree built using leaf-lists Rmin = 4. node
matching 3 fewer records expanded, simply records set
pointers dataset (shown right).

6.1 Datasets

experiments used datasets Table 1. dataset supplied us
continuous attributes already discretized ranges.

6.2 Using ADtrees Feature Selection

Given attributes, one output wish predict, often interesting
ask \which subset n attributes, (n < ), best predictor output
distribution datapoints ected dataset?" (Kohavi, 1995).
many ways scoring set features, particularly simple one information
gain (Cover & Thomas, 1991).
Let aout attribute wish predict let ai(1) : : : ai(n) set attributes
used inputs. Let X set possible assignments values ai(1) : : : ai(n) write
Assignk 2 X kth assignment.
X
jX j C (Assign ) nX C (a = v; Assign )
nX
C (aout = v )

k
k
InfoGain = f
,
f
(8)
R
R
C (Assignk )
v=1
v=1
k=1




R number records entire dataset
( ) = ,x log2 x
(9)
counts needed computation read directly ct(aout ; ai(1) : : : ai(n) ).
Searching best subset attributes simply question search among
attribute-sets size n (n specified user). simple example designed test
f x

76

fiCached Sufficient Statistics Efficient Machine Learning

Name

= Num. = Num.
Records Attributes
ADULT1
15,060 15
small \Adult Income" dataset placed UCI
repository Ron Kohavi (Kohavi, 1996). Contains
census data related job, wealth, nationality. Attribute arities range 2 41. UCI repository called Test Set. Rows missing
values removed.
ADULT2
30,162 15
kinds records different
data. Training Set.
ADULT3
45,222 15
ADULT1 ADULT2 concatenated.
CENSUS1
142,421 13
larger dataset based different census, provided Ron Kohavi.
CENSUS2
142,421 15
data CENSUS1, addition
two extra, high-arity attributes.
BIRTH
9,672 97
Records concerning wide number readings
factors recorded various stages pregnancy. attributes binary, 70 attributes sparse, 95% values
FALSE.
SYNTH
30K{500K 24
Synthetic datasets entirely binary attributes generated using Bayes net Figure 8.
R

Table 1: Datasets used experiments.

counting methods: practical feature selector would need penalize number
rows contingency table (else high arity attributes would tend win).

6.3 Using ADtrees Bayes Net Structure Discovery

many possible Bayes net learning tasks, entail counting, hence
might speeded ADtrees. paper present experimental results
particular example scoring structure Bayes net decide well matches
data.
use maximum likelihood scoring penalty number parameters.
first compute probability table associated node. Write Parents(j)
parent attributes node j write Xj set possible assignments values
Parents(j). maximum likelihood estimate
( = v j Xj )

P aj

estimated

(10)

( = v; Xj )
(11)
C (Xj )
estimates node j 's probability tables read ct(aj ; Parents(j)).
next step scoring structure decide likelihood data given
probability tables computed penalize number parameters network
(without penalty likelihood would increase every time link added
C aj

77

fiMoore & Lee

Figure 8: Bayes net generated SYNTH datasets.
three kinds nodes. nodes marked triangles generated
P (ai = 1) = 0:8; P (ai = 2) = 0:2. square nodes deterministic.
square node takes value 2 sum four parents even, else
takes value 1. circle nodes probabilistic functions single
parent, defined P (ai = 2 j Parent = 1) = 0 P (ai = 2 j Parent =
2) = 0:4. provides dataset fairly sparse values many
interdependencies.
network). penalized log-likelihood score (Friedman & Yakhini, 1996)

,

N

params log(R)=2 + R

n

X
X X
j

j =1

Asgn

2Xj v=1

( = v ^ Asgn) log P (aj = v j Asgn)

P aj

(12)

Nparams total number probability table entries network.
search among structures find best score. experiments use randomrestart stochastic hill climbing operations random addition removal
network link randomly swapping pair nodes. latter operation necessary
allow search algorithm choose best ordering nodes Bayes net. Stochastic
searches popular method finding Bayes net structures (Friedman &
Yakhini, 1996). probability tables affected nodes recomputed
step.
Figure 9 shows Bayes net structure returned Bayes net structure finder
30,000 iterations hill climbing.

6.4 Using ADtrees Rule Finding
Given output attribute aout distinguished value
conjunctive queries form

out,

v

Assign = (ai(1) = v1 : : : ai(n) = vn )
78

rule finders search among
(13)

fiCached Sufficient Statistics Efficient Machine Learning

a_
_
t_
t_
r_
i_
b_
u_
t_
e
relationship
class
sex
capital-gain
hours-per-week
marital-status
education-num
capital-loss
age
race
education
workclass
native-country
fnlwgt
occupation

_
s_
c_
o_
r_
e
_
n_
p
2.13834
2
0.643388
36
0.511666
24
0.0357936 30
0.851964
48
0.762479
72
1.0941
26
0.22767
10
0.788001
28
0.740212
18
1.71784
36
1.33278
108
0.647258
30
0.0410872 40
2.66097
448

pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars

=
=
=
=
=
=
=
=
=
=
=
=
=
=
=

<no parents>
relationship
relationship class
class
relationship class sex
relationship sex
class
class
marital-status
relationship education-num
relationship education-num
relationship hours-per-week education-num
education-num race
<no parents>
class sex education workclass

Score 435219
search took 226 seconds.

Figure 9: Output Bayes structure finder running
ADULT2 dataset. Score contribution sum Equation 12 due
specified attribute. np number entries probability
table specified attribute.
find query maximizes estimated value
(

P aout

vout ; Assign)
= vout j Assign) = C (aoutC=(Assign
)

(14)

avoid rules without significant support, insist C (Assign) (the number
records matching query) must threshold Smin.
experiments implement brute force search looks possible
queries involve user-specified number attributes, n. build ct(aout ; ai(1) : : : ai(n) )
turn (there choose n tables), look rows table
queries using ai(1) : : : ai(n) greater minimum support Smin.
return priority queue highest scoring rules. instance ADULT2 dataset,
best rule predicting \class" 4 attributes was:
score = 0.965 (218/226), workclass = Private, education-num = above12, maritalstatus = Married-civ-spouse, capital-loss = above1600 ) class 50k

7. Experimental Results
Let us first examine memory required ADtree datasets. Table 2 shows us,
example, ADULT2 dataset produced ADtree 95,000 nodes. tree
required almost 11 megabytes memory. Among three ADULT datasets, size
tree varied approximately linearly number records.
Unless otherwise specified, experiments section, ADULT datasets
used leaf-lists. BIRTH SYNTHETIC datasets used leaf-lists size Rmin = 16
default. BIRTH dataset, large number sparse attributes, required modest
8 megabytes store tree|many magnitudes worst-case bounds. Among
synthetic datasets, tree size increased sublinearly dataset size. indicates
79

fiMoore & Lee

Dataset
CENSUS1
CENSUS2
ADULT1
ADULT2
ADULT3
BIRTH
SYN30K
SYN60K
SYN125K
SYN250K
SYN500K


13
15
15
15
15
97
24
24
24
24
24

R
142521
142521
15060
30162
45222
9672
30000
60000
125000
250000
500000

Nodes Megabytes Build Time
24007
1.5
17
209577
13.2
32
58200
7.0
6
94900
10.9
10
162900
15.5
15
87400
7.9
14
34100
2.8
8
59300
4.9
17
95400
7.9
36
150000
12.4
73
219000
18.2
150

Table 2: size ADtrees various datasets. number
attributes. R number records. Nodes number nodes
ADtree. Megabytes amount memory needed store
tree. Build Time number seconds needed build tree (to
nearest second).

dataset gets larger, novel records (which may cause new nodes appear
tree) become less frequent.
Table 3 shows costs performing 30,000 iterations Bayes net structure searching.
experiments performed 200Mhz Pentium Pro machine 192 megabytes
main memory. Recall Bayes net iteration involves one random change
network requires recomputation one contingency table (the exception first
iteration, nodes must computed). means time run 30,000
iterations essentially time compute 30,000 contingency tables. Among ADULT
datasets, advantage ADtree conventional counting ranges factor
19 32. Unsurprisingly, computational costs ADULT increase sublinearly
dataset size ADtree linearly conventional counting. computational
advantages sublinear behavior much pronounced synthetic data.
Next, Table 4 examines effect leaf-lists ADULT2 BIRTH datasets.
ADULT dataset, byte size tree decreases factor 5 leaf-lists
increased 1 64. computational cost running Bayes search increases
25%, indicating worth-while tradeoff memory scarce.
Bayes net scoring results involved average cost computing contingency tables
many different sizes. following results Tables 5 6 make savings fixed
size attribute sets easier discern. tables give results feature selection
rule finding algorithms, respectively. biggest savings come small attribute sets.
Computational savings sets size one two are, however, particularly interesting
since counts could cached straightforward methods without needing tricks.
cases, however, see large savings, especially BIRTH data. Datasets
larger numbers rows would, course, reveal larger savings.
80

fiCached Sufficient Statistics Efficient Machine Learning

Dataset
CENSUS1
CENSUS2
ADULT1
ADULT2
ADULT3
BIRTH
SYN30K
SYN60K
SYN125K
SYN250K
SYN500K


13
15
15
15
15
97
24
24
24
24
24

R ADtree Time Regular Time Speedup Factor
142521
162
48300
298.1
142521
783
27000
34.5
15060
150
2850
19
30162
226
5160
22.8
45222
220
7140
32.5
9672
23
2820
122.6
30000
32
10410
325.3
60000
32
18360
573.8
125000
34
42840
1260.0
250000
35
88830
2538.0
500000
36
155158
4298.3

Table 3: time (in seconds) perform 30,000 hill-climbing iterations
searching best Bayes net structure. ADtree Time time
using ADtree Regular Time time taken using
conventional probability table scoring method counting
dataset. Speedup Factor number times ADtree
method faster conventional method. ADtree times
include time building ADtree first place (given
Table 2). typical use ADtrees build tree
able use many data analysis operations, building
cost amortized. case, even including tree building cost
would minor impact results.

min

R

1
2
4
8
16
32
64
128
256
512
1024
2048
4096

ADULT2
BIRTH
#Mb #nodes Build Search #Mb #nodes Build Search
Secs
Secs
Secs
Secs
10.87 94,872
13
225
8.46 86,680
11
223
6.30 75,011
9
223 27.60 245,722
32
25
4.62 62,095
7
224 14.90 152,409
21
22
3.37 49,000
6
232 7.95 87,415
15
23
2.55 37,790
5
245 4.30 46,777
11
26
1.98 27,726
5
274 2.42 23,474
8
31
1.59 18,903
3
331 1.48 11,554
7
38
1.38 12,539
3
420 0.95
5,150
5
61
1.21
7,336
3
586 0.65
2,237
4
100
1.05
3,928
3
881 0.45
887
3
170
0.95
1,890
1
1231 0.28
246
1
270
0.86
780
1
1827 0.28
201
2
303

Table 4: Investigating effect Rmin parameter ADULT2
dataset BIRTH dataset. #Mb memory used ADtree.
#nodes number nodes ADtree. Build Secs time
build ADtree. Search Secs time needed perform 30,000
iterations Bayes net structure search.
81

fiMoore & Lee

Number
Number
Attributes Attribute
Sets
1
14
2
91
3
364
4
1,001
5
2,002

ADULT2

ADtree Regular Speedup

Time

Time

Factor

.000071
.00054
.0025
.0083
.023

.048
.067
.088
.11
.14

675.0
124.0
34.9
13.4
6.0

Number
Attribute
Sets
96
4,560
142,880
3,321,960
61,124,064

BIRTH

ADtree Regular Speedup
Time

Time

.000018 .015
.000042 .021
.000093 .028
.00019
.00033

Factor
841
509
298

Table 5: time taken search among attribute sets given size
(Number Attributes) set gives best information gain
predicting output attribute. times, seconds, average
evaluation times per attribute-set.

Number Number
Attributes
Rules
1
116
2
4,251
3
56,775
4
378,984
5 1,505,763

ADULT2

ADtree Regular Speedup

Time
.000019
.000019
.000024
.000031
.000042

Time
.0056
.0014
.00058
.00030
.00019

Factor
295.0
75.3
23.8
9.8
4.7

Number
Rules
194
17,738
987,134
37,824,734
1,077,672,055

BIRTH

ADtree Regular Speedup

Time
.000025
.000021
.000022
.000024
.000026

Time
.0072
.0055
.0040
.0030

Table 6: time taken search among rules given size (Number Attributes) highest scoring rules predicting output
attribute. times, seconds, average evaluation time per
rule.

82

Factor
286
259
186
127

fiCached Sufficient Statistics Efficient Machine Learning

8. Alternative Data Structures
8.1 use kd-tree?
kd-trees used accelerating learning algorithms (Omohundro, 1987; Moore et al.,
1997). primary difference kd-tree node splits one attribute instead
attributes. results much less memory (linear number records).
counting expensive. Suppose, example, level one tree splits a1 , level
two splits a2, etc. Then, case binary variables, query involving
attributes a20 higher, explore paths tree level 20.
datasets fewer 220 records may cheaper performing linear search
records. Another possibility, R-trees (Guttman, 1984; Roussopoulos & Leifker,
1985), store databases -dimensional geometric objects. However, context,
offer advantages kd-trees.

8.2 use Frequent Set finder?
Frequent Set finders (Agrawal et al., 1996) typically used large databases
millions records containing sparse binary attributes. Ecient algorithms exist
finding subsets attributes co-occur value TRUE fixed
number (chosen user, called support) records. Recent research(Mannila &
Toivonen, 1996) suggests Frequent Sets used perform ecient counting.
case support = 1, Frequent Sets gathered and, counts
Frequent Set retained, equivalent producing ADtree instead
performing node cutoff common value, cutoff always occurs value
FALSE.
use Frequent Sets way would thus similar use ADtrees,
one advantage one disadvantage. advantage ecient algorithms
developed building Frequent Sets small number sequential passes
data. ADtree requires random access dataset built,
leaf-lists. impractical dataset large reside main memory
accessed database queries.
disadvantage Frequent Sets comparison ADtrees that,
circumstances, former may require much memory. Assume value 2 rarer
1 throughout attributes dataset assume reasonably thus choose
find Frequent Sets 2s. Unnecessarily many sets produced
correlations. extreme case, imagine dataset 30% values 2, 70%
1 attributes perfectly correlated|all values record identical. Then,
attributes would 2M Frequent Sets 2s. contrast, ADtree would
contain + 1 nodes. extreme example, datasets much weaker
inter-attribute correlations similarly benefit using ADtree.
Leaf-lists another technique reduce size ADtrees further. could
used Frequent Set representation.
83

fiMoore & Lee

8.3 use hash tables?

knew small set contingency tables would ever requested, instead
possible contingency tables, ADtree would unnecessary. would better
remember small set contingency tables explicitly. Then, kind tree structure
could used index contingency tables. hash table would equally time
ecient require less space. hash table coding individual counts contingency
tables would similarly allow us use space proportional number non-zero
entries stored tables. representing sucient statistics permit fast solution
contingency table request, ADtree structure remains memory ecient
hash-table approach (or method stores non-zero counts)
memory reductions exploit ignoring common values.

9. Discussion

9.1 numeric attributes?

ADtree representation designed entirely symbolic attributes. faced
numeric attributes, simplest solution discretize fixed finite set values
treated symbols, little help user requests counts
queries involving inequalities numeric attributes. future work evaluate use
structures combining elements multiresolution kd-trees real attributes (Moore
et al., 1997) ADtrees.

9.2 Algorithm-specific counting tricks

Many algorithms count using conventional \linear" method algorithm-specific
ways accelerating performance. example, Bayes net structure finder may try
remember contingency tables tried previously case needs re-evaluate
them. deletes link, deduce new contingency table old one
without needing linear count.
cases, appropriate use ADtree may lazy caching mechanism. birth, ADtree consists root node. Whenever structure finder
needs contingency table cannot deduced current ADtree structure,
appropriate nodes ADtree expanded. ADtree takes role
algorithm-specific caching methods, (in general) using much less memory
contingency tables remembered.

9.3 Hard update incrementally

Although tree built cheaply (see experimental results Section 7),
although built lazily, ADtree cannot updated cheaply new record.
one new record may match 2M nodes tree worst case.

9.4 Scaling

ADtree representation useful datasets rough size shape used
paper. first datasets looked at|the ones described paper|we
84

fiCached Sufficient Statistics Efficient Machine Learning

shown empirically sizes ADtrees tractable given real noisy data.
included one dataset 97 attributes. extent attributes skewed
values correlated enables ADtree avoid approaching
worse-case bounds. main technical contribution paper trick allows
us prune most-common-values. Without it, skewedness correlation would hardly
help all. 3 empirical contribution paper show actual
sizes ADtrees produced real data vastly smaller sizes would get
worst-case bounds Appendix A.
despite savings, ADtrees cannot yet represent sucient statistics
huge datasets many hundreds non-sparse poorly correlated attributes.
dataset ADtree cannot fit main memory? latter case,
could simply increase size leaf-lists, trading decreased memory increased
time build contingency tables. inadequate least three possibilities remain.
First, could build approximate ADtrees store information nodes
match fewer threshold number records. approximate contingency
tables (complete error bounds) produced (Mannila & Toivonen, 1996).
second possibility exploit secondary storage store deep, rarely visited nodes
ADtree disk. would doubtless best achieved integrating machine learning
algorithms current database management tools|a topic considerable interest
data mining community (Fayyad et al., 1997). third possibility, restricts size
contingency tables may ask for, refuse store counts queries
threshold number attributes.

9.5 cost building tree?
practice, ADtrees could used two ways:

One-off. traditional algorithm required build ADtree, run fast
version algorithm, discard ADtree, return results.

Amortized. new dataset becomes available, new ADtree built it.

tree shipped re-used anyone wishes real-time counting
queries, multivariate graphs charts, machine learning algorithms
subset attributes. cost initial tree building amortized
times used. database terminology, process known materializing (Harinarayan et al., 1996) suggested desirable datamining
several researchers (John & Lent, 1997; Mannila & Toivonen, 1996).

one-off option useful cost building ADtree plus cost running
ADtree-based algorithm less cost original counting-based algorithm.
intensive machine learning methods studied here, condition safely satisfied.
decided use less intensive, greedier Bayes net structure finder? Table 7
3. Without pruning, datasets ran memory 192 megabyte machine
built even 1% tree, easy show BIRTH dataset would needed store
1030 nodes.

85

fiMoore & Lee

Dataset

Speedup ignoring
Speedup allowing Speedup allowing
build-time, 30,000
build-time, 30,000
build-time, 300
iterations
iterations
iterations
CENSUS1
298.15
269.83
25.94
CENSUS2
34.48
33.13
6.78
ADULT1
19.00
18.27
3.80
ADULT2
22.83
21.86
4.21
ADULT3
32.45
30.38
4.15
BIRTH
122.61
76.22
1.98
SYN30K
325.31
260.25
12.51
SYN60K
573.75
374.69
10.60
SYN125K
1260.00
612.00
11.79
SYN250K
2538.00
822.50
12.11
SYN500K
4309.94
834.18
10.32

Table 7: Computational economics building ADtrees using
search Bayes net structures using experiments Section 7.
shows run 300 iterations instead 30,0004 account one-off
ADtree building cost, relative speedup using ADtrees declines greatly.
conclude: data analysis intense benefit using ADtrees even
used one-off fashion. ADtree used multiple purposes
build-time amortized resulting relative eciency gains traditional counting
exhaustive searches non-exhaustive searches. Algorithms
use non-exhaustive searches include hill-climbing Bayes net learners, greedy rule learners
CN2 (Clark & Niblett, 1989) decision tree learners (Quinlan, 1983; Breiman
et al., 1984).

Acknowledgements
work sponsored National Science Foundation Career Award Andrew Moore.
authors thank Justin Boyan, Scott Davies, Nir Friedman, Jeff Schneider
suggestions, Ron Kohavi providing census datasets.

Appendix A: Memory Costs

appendix examine size tree. simplicity, restrict attention
case binary attributes.

worst-case number nodes ADtree
Given dataset attributes R records, worst-case ADtree occur
2M possible records exist dataset. Then, every subset attributes exists
exactly one node ADtree. example consider attribute set fai(1) : : : ai(n) g,
i(1) < i(2) < ::: < i(n). Suppose node tree corresponding
4. Unsurprisingly, resulting Bayes nets highly inferior structure.

86

fiCached Sufficient Statistics Efficient Machine Learning

query fai(1) = v1 : : : ai(n) = vn g values v1 : : : vn . definition ADtree,
remembering considering case binary attributes, state:
v1 least common value ai(1).
v2 least common value ai(2) among records match (ai(1) = v1).
..
.



least common value ai(k+1) among records match (ai(1) =
1
(k) = vk ).
one node. Moreover, since worst-case assumption
possible records exist database, see ADtree indeed contain
node. Thus, worst-case number nodes number possible subsets
attributes: 2M .
vk

+1

v ; : : : ; ai

worst-case number nodes ADtree reasonable number rows
frequently case dataset 2M . fewer records, much
R

lower worst-case bound ADtree size. node kth level tree corresponds
query involving k attributes (counting root node level 0). node match
R2,k records node's ancestors tree pruned
least half records choosing expand least common value attribute
introduced ancestor. Thus, tree nodes level blog2 Rc + 1
tree, nodes would match fewer R2,blog Rc,1 < 1 records.
would thus match records, making NULL.
nodes ADtree must exist level blog2 Rc higher. number nodes
level k ( ), every node level k involves attribute set size
k (given binary attributes) every attribute set one node
ADtree. Thus total number nodes tree, summing levels less

!
blog
XRc
bounded O(M blog Rc =(blog2 Rc , 1)!)
(15)
k
2


k

2

2

k=0

number nodes assume skewed independent attribute values

Imagine values attributes dataset independent random binary
variables, taking value 2 probability p taking value 1 probability 1 , p.
p 0:5, smaller expect ADtree be. because,
average, less common value Vary node match fraction min(p; 1 , p)
parent's records. And, average, number records matched kth level
tree R(min(p; 1 , p))k . Thus, maximum level tree may
find node matching one records approximately b(log2 R)=(, log2 q )c,
q = min(p; 1 , p). total number nodes tree approximately
!
b(log R)X
=(, log q)c

bounded O(M b(log R)=(, log q)c =(b(log2 R)=(, log2 q )c,1)!)
k
2

2

2

k=0

2

(16)

87

fiMoore & Lee

Since exponent reduced factor log2 (1=q ), skewedness among attributes
thus brings enormous savings memory.

number nodes assume correlated attribute values

ADtree benefits correlations among attributes much way
benefits skewedness. example, suppose record generated
simple Bayes net Figure 10, random variable B hidden (not included
record). 6= j , P (ai 6= aj ) = 2p(1 , p). ADN node resulting ADtree
number records matching node two levels ADN tree
fraction 2p(1 , p) number records matching ADN. see
number nodes tree approximately
!
b(log R)X
=(, log q)c

bounded O(M b(log R)=(, log q)c =(b(log2 R)=(, log2 q )c,1)!)
k
2

2

2

2

k=0

(17)
p
q = 2p(1 , p). Correlation among attributes thus bring enormous
savings memory even (as case example) marginal distribution
individual attributes uniform.
P(B) = 0.5

B

P(a | B) = 1 - p
a1

a2

...



P(a | ~B) = p

Figure 10: Bayes net generates correlated boolean attributes
a1 ; a2 : : : .

number nodes dense ADtree Section 2

dense ADtrees cut tree common value Vary node.
worst case ADtree occur 2M possible records exist dataset. dense
ADtree require 3M nodes every possible query (with attribute taking
values 1, 2 *) count tree. number nodes kth level
dense ADtree 2k ( ) worst case.

k

number nodes using Leaf-lists

Leaf-lists described Section 5. tree built using maximum leaf-list size Rmin,
node ADtree matching fewer Rmin records leaf node. means
Formulae 15, 16 17 re-used, replacing R R=Rmin. important
remember, however, leaf nodes must contain room Rmin numbers instead
single count.
88

fiCached Sufficient Statistics Efficient Machine Learning

Appendix B: Building ADtree
define function MakeADTree( , RecordNums) RecordNums subset
f1 2
g ( total number records dataset) 1 .


;

; : : :; R

R





makes ADtree rows specified RecordNums ADnodes represent
queries attributes ai higher used.

MakeADTree( , RecordNums)


Make new ADnode called ADN.
ADN:COUNT := j RecordNums j.
j := i; + 1; : : : ;
j th Vary node ADN := MakeVaryNode(aj ; RecordNums).

MakeADTree uses function MakeVaryNode, define:
MakeVaryNode( , RecordNums)


Make new Vary node called VN.
k := 1; 2; : : : ni
Let Childnumsk := fg.
j 2 RecordNums
Let vij = Value attribute ai record j
Add j set Childnumsv
Let VN:MCV := argmaxk j Childnumsk j.
k := 1; 2; : : : ni
j Childnumsk j= 0 k = MCV
Set ai = k subtree VN NULL.
Else
Set ai = k subtree VN MakeADTree(ai+1 ; Childnumsk )
ij

build entire tree, must call MakeADTree(a1; f1 : : : Rg). Assuming binary attributes, cost building tree R records attributes bounded

blog
XRc R !
(18)
2k k
k=0
2

References

Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., & Verkamo, A. I. (1996). Fast discovery association rules. Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., &
Uthurusamy, R. (Eds.), Advances Knowledge Discovery Data Mining. AAAI
Press.
89

fiMoore & Lee

Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification
Regression Trees. Wadsworth.
Clark, P., & Niblett, R. (1989). CN2 induction algorithm. Machine Learning, 3,
261{284.
Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. John Wiley &
Sons.
Fayyad, U., Mannila, H., & Piatetsky-Shapiro, G. (1997). Data Mining Knowledge
Discovery. Kluwer Academic Publishers. new journal.
Fayyad, U., & Uthurusamy, R. (1996). Special issue Data Mining. Communications
ACM, 39 (11).
Friedman, N., & Yakhini, Z. (1996). sample complexity learning Bayesian networks. Proceedings 12th conference Uncertainty Artificial Intelligence.
Morgan Kaufmann.
Guttman, A. (1984). R-trees: dynamic index structure spatial searching. Proceedings Third ACM SIGACT-SIGMOD Symposium Principles Database
Systems. Assn Computing Machinery.
Harinarayan, V., Rajaraman, A., & Ullman, J. D. (1996). Implementing Data Cubes Eciently. Proceedings Fifteenth ACM SIGACT-SIGMOD-SIGART Symposium
Principles Database Systems : PODS 1996, pp. 205{216. Assn Computing
Machinery.
John, G. H., Kohavi, R., & P eger, K. (1994). Irrelevant features Subset Selection
Problem. Cohen, W. W., & Hirsh, H. (Eds.), Machine Learning: Proceedings
Eleventh International Conference. Morgan Kaufmann.
John, G. H., & Lent, B. (1997). SIPping data firehose. Proceedings Third
International Conference Knowledge Discovery Data Mining. AAAI Press.
Kohavi, R. (1995). Power Decision Tables. Lavrae, N., & Wrobel, S. (Eds.),
Machine Learning : ECML-95 : 8th European Conference Machine Learning,
Heraclion, Crete, Greece. Springer Verlag.
Kohavi, R. (1996). Scaling accuracy naive-Bayes classifiers: decision-tree hybrid. E. Simoudis J. Han U. Fayyad (Ed.), Proceedings Second
International Conference Knowledge Discovery Data Mining. AAAI Press.
Madala, H. R., & Ivakhnenko, A. G. (1994). Inductive Learning Algorithms Complex
Systems Modeling. CRC Press Inc., Boca Raton.
Mannila, H., & Toivonen, H. (1996). Multiple uses frequent sets condensed representations. E. Simoudis J. Han U. Fayyad (Ed.), Proceedings Second
International Conference Knowledge Discovery Data Mining. AAAI Press.
90

fiCached Sufficient Statistics Efficient Machine Learning

Moore, A. W., Schneider, J., & Deng, K. (1997). Ecient Locally Weighted Polynomial
Regression Predictions. D. Fisher (Ed.), Proceedings 1997 International
Machine Learning Conference. Morgan Kaufmann.
Omohundro, S. M. (1987). Ecient Algorithms Neural Network Behaviour. Journal
Complex Systems, 1 (2), 273{347.
Quinlan, J. R. (1983). Learning Ecient Classification Procedures Application
Chess End Games. Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning|An Artificial Intelligence Approach (I). Tioga Publishing Company,
Palo Alto.
Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5,
239{266.
Roussopoulos, N., & Leifker, D. (1985). Direct spatial search pictorial databases using
packed R-trees. Navathe, S. (Ed.), Proceedings ACM-SIGMOD 1985 International Conference Management Data. Assn Computing Machinery.
Rymon, R. (1993). SE-tree based Characterization Induction Problem. P.
Utgoff (Ed.), Proceedings 10th International Conference Machine Learning.
Morgan Kaufmann.

91


