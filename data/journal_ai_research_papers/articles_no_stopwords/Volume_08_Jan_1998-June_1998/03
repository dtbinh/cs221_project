journal artificial intelligence

submitted published

cached sucient statistics ecient machine learning
large datasets
andrew moore
mary soon lee

school computer science robotics institute
carnegie mellon university pittsburgh pa

awm cs cmu edu
mslee cs cmu edu

abstract

introduces data structures quick counting machine
learning datasets focus counting task constructing contingency tables
applicable counting number records dataset match
conjunctive queries subject certain assumptions costs operations
shown independent number records dataset loglinear
number non zero entries contingency table
provide sparse data structure adtree minimize memory use
provide analytical worst case bounds structure several data distribution empirically demonstrate tractably sized data structures produced
large real world datasets sparse tree structure never allocates memory
counts zero b never allocating memory counts deduced
counts c bothering expand tree fully near leaves
adtree used accelerate bayes net structure finding rule learning feature selection provide
number empirical comparing adtree methods traditional direct counting
approaches discuss possible uses adtrees machine learning methods discuss merits adtrees comparison alternative representations
kd trees r trees frequent sets

caching sucient statistics

computational eciency important concern machine learning especially
applied large datasets fayyad mannila piatetsky shapiro fayyad
uthurusamy real time scenarios earlier work showed kd trees
multiresolution cached regression matrix statistics enable fast locally weighted
instance regression moore schneider deng attempt
accelerate predictions symbolic attributes kind kd tree splits
dimensions nodes
many machine learning operating datasets symbolic attributes need
frequent counting work applicable online analytical processing olap
applications data mining operations large datasets multidimensional
database access datacube operations harinarayan rajaraman ullman
association rule learning agrawal mannila srikant toivonen verkamo could
accelerated fast counting
let us begin establishing notation given data set r records
attributes attributes called value attribute ai
c ai access foundation morgan kaufmann publishers rights reserved

fimoore lee

th record small integer lying range f ni g ni called arity
attribute figure gives example
k

attributes







n

n

n

record









record







r

record







record







record







record







arity

figure simple dataset used example r records
attributes

queries

query set attribute value pairs left hand sides pairs form
subset fa g arranged increasing order index four examples queries
dataset




notice total number possible queries
ni
attribute appear query one ni values may take may
omitted equivalent giving ai care value

counts

count query denoted c query simply number records dataset
matching attribute value pairs query example dataset

c
c
c
c

contingency tables











subset attributes ai ai n associated contingency table denoted
ct ai ai n table row possible sets values
ai ai n row corresponding ai v ai n vn records count c ai

v ai n vn example dataset attributes contingency tables
exist depicted figure


ficached sufficient statistics efficient machine learning

ct

ct

ct











































































ct

ct
















ct

ct




















ct


















































































figure eight possible contingency tables dataset figure
conditional contingency table written
ct ai ai n j aj u aj p



contingency table subset records dataset match query
right j symbol example



ct j
















contingency tables used variety machine learning applications including
building probability tables bayes nets evaluating candidate conjunctive rules
rule learning quinlan clark niblett would thus desirable
able perform counting eciently
prepared pay one time cost building caching data structure
easy suggest mechanism counting constant time possible query
precompute contingency table total amount numbers stored memory
data structure would
ni even humble dataset figure
revealed figure real dataset ten attributes medium
arity fifteen binary attributes far large fit main memory
would retain speed precomputed contingency tables without incurring
intractable memory demand subject

cache reduction dense adtree caching sucient statistics

first describe adtree data structure use represent set
possible counts initial simplified description obvious tree representation
yield immediate memory savings later provide several opportunities
se tree rymon similar data structure



fimoore lee

cutting zero counts redundant counts structure shown figure
adtree node shown rectangle child nodes called vary nodes shown ovals
adnode represents query stores number records match query
c field vary aj child adnode one child nj values
attribute aj kth child represents query vary aj parent
additional constraint aj k








c

vary


n





























vary












n















c

c

c

c






vary

vary

vary

vary








vary

figure top adnodes adtree described text
notes regarding structure

although drawn diagram description query e g


leftmost adnode second level explicitly recorded
adnode contents adnode simply count set pointers
vary aj children
contents vary aj node set pointers adnodes




cost looking count proportional number instantiated variables

query example look c would follow
following path tree vary vary vary
count obtained resulting node

notice node adn vary
vary ai vary

ai



ai

parent adn children

vary

necessary store vary nodes indices information
obtained another path tree


ficached sufficient statistics efficient machine learning

cutting nodes counts zero

described tree sparse contain exactly
ni nodes sparseness
easily achieved storing null instead node query matches zero
records specializations query count zero
appear anywhere tree datasets reduce number
numbers need stored example dataset figure previously
needed numbers represent contingency tables need numbers

cache reduction ii sparse adtree

easy devise datasets benefit failing store counts zero
suppose binary attributes records kth record bits
binary representation k query count zero tree contains
nodes reduce tree size despite take advantage observation
many counts stored tree redundant
vary aj node adtree stores nj subtrees one subtree value
aj instead common values aj call mcv store
null place mcvth subtree remaining nj subtrees represented
example simple dataset given figure vary aj node
records values common mcv field appendix b describes
straightforward building adtree
see section still possible build full exact contingency tables
give counts specific queries time slightly longer full adtree
section first let us examine memory consequences representation
appendix shows binary attributes given attributes r records
number nodes needed store tree bounded worst case
much less r contrast amount memory needed dense tree
section worst case
notice figure mcv value context dependent depending constraints
parent nodes mcv sometimes sometimes context dependency
provide dramatic savings frequently case correlations among
attributes discussed appendix

computing contingency tables sparse adtree

given adtree wish able quickly construct contingency tables arbitrary set attributes fai ai n g
notice conditional contingency table ct ai ai n j query built recursively first build

ct
ct

ai n j ai query
ai ai n j ai query

ai

ct

ai

ai n

j




ni query

ai



fimoore lee






c
vary
mcv

vary
mcv





null





mcv

c

c




vary
mcv








null
mcv



c



vary
mcv

null

null

null



count

mcv

mcv

















c














figure sparse adtree built dataset shown bottom
right common value subtree
vary child root node null vary
nodes common child set null child
common depends context
example build ct dataset figure build ct j
ct j combine figure
ct

ct








ct





























figure example numbers figure contingency
tables combined recursively form larger contingency tables
building conditional contingency table adtree need
explicitly specify query condition instead supply adnode adtree
implicitly equivalent information



ficached sufficient statistics efficient machine learning

makecontab f

ai ai n g adn
let vn vary ai subnode adn
let mcv vn mcv
k ni
k mcv
let adnk ai k subnode vn
ctk makecontab fai ai n g adnk

ctmcv



calculated explained

return concatenation ct ctn


base case recursion occurs first argument empty case
return one element contingency table containing count associated current
adnode adn
omission iteration k f ni g
unable compute conditional contingency table ctmcv ai mcv
subtree deliberately missing per section instead
take advantage following property contingency tables

ct

ai

ai n j query

x

ni

ct

ai n

ai

k

j

k query

ai



value ct ai ai n j query computed within calling

makecontab f

ai n g adn



ai

missing conditional contingency table computed
following row wise subtraction
x
ctmcv makecontab fai ai n g adn
ctk

k cv

frequent sets agrawal et al traditionally used learning association
rules used computing counts recent mannila toivonen
employs similar subtraction trick calculates counts frequent sets
section discuss strengths weaknesses frequent sets comparison
adtrees

complexity building contingency table

cost computing contingency table let us consider theoretical worstcase cost computing contingency table n attributes arity k note
cost unrealistically pessimistic except k contingency tables
sparse discussed later assumption attributes arity k
made simplify calculation worst case cost needed code


fimoore lee

contingency table n attributes kn entries write c n cost computing
contingency table top level call makecontab k calls build
contingency tables n attributes k calls build ct ai ai n j
ai j query every j f k g except mcv final call build
ct ai ai n j query k subtractions contingency tables
require kn numeric subtractions
c


n
c n
kc n k k
n

solution recurrence relation c n n k kn cost loglinear
size contingency table comparison used cached data structure
simply counted dataset order build contingency table would
need nr kn operations r number records dataset thus
cheaper standard counting method kn r interested large datasets
r may case method present several
order magnitude speedup say contingency table eight binary attributes notice
cost independent total number attributes dataset
depends upon almost much smaller number attributes n requested
contingency table

sparse representation contingency tables

practice represent contingency tables multidimensional arrays rather
tree structures gives slow counting adtree
substantial computational advantage cases contingency table sparse e
many zero entries figure shows sparse contingency table representation
mean average case behavior much faster worst case contingency tables
large numbers attributes high arity attributes
indeed experiments section costs rising much slowly nkn
n increases note sparse representation worst case
makecontab min nr nkn r maximum possible number
non zero contingency table entries

cache reduction iii leaf lists

introduce scheme reducing memory use worth building
adtree data structure small number records example suppose
records binary attributes analysis appendix shows us
worst case adtree might require nodes computing contingency tables
resulting adtree would records faster conventional
counting would merely require us retain dataset memory
aside concluding adtrees useful small datasets
leads final method saving memory large adtrees adtree node fewer
rmin records expand subtree instead maintains list pointers
original dataset explicitly listing records match current adnode
list pointers called leaf list figure gives example


ficached sufficient statistics efficient machine learning

v

ct

v














































































v
v



v

v



v

v





v

null

null

null

v





v



null



v

null

null



v

v

v







v
v




null
null



v



figure right hand figure sparse representation contingency table left
use leaf lists one minor two major consequences minor consequence need include straightforward change contingency table generating
handle leaf list nodes minor alteration described first
major consequence dataset must retained main memory
inspect leaf lists access rows data pointed leaf lists
second major consequence adtree may require much less memory
documented section worst case bounds provided appendix

adtrees machine learning
see section adtree structure substantially speed computation
contingency tables large real datasets machine learning statistical
take advantage provide three examples feature selection
bayes net scoring rule learning seems likely many
benefit example stepwise logistic regression gmdh madala ivakhnenko
text classification even decision tree quinlan breiman friedman olshen
stone learning may benefit future work examine ways speed
nearest neighbor memory queries adtrees
depends whether cost initially building adtree amortized many runs
decision tree repeated runs decision tree building occur one wrapper
model feature selection john kohavi p eger one intensive search
tree structures traditional greedy search quinlan breiman et al



fimoore lee




c
row




c
see rows

vary

vary

vary









mcv

mcv

mcv

















































vary









mcv









null
mcv





null
mcv





c

null
mcv





c

c

see rows

null
mcv




c
see rows

figure adtree built leaf lists rmin node
matching fewer records expanded simply records set
pointers dataset shown right

datasets

experiments used datasets table dataset supplied us
continuous attributes already discretized ranges

adtrees feature selection

given attributes one output wish predict often interesting
ask subset n attributes n best predictor output
distribution datapoints ected dataset kohavi
many ways scoring set features particularly simple one information
gain cover thomas
let aout attribute wish predict let ai ai n set attributes
used inputs let x set possible assignments values ai ai n write
assignk x kth assignment
x
jx j c assign nx c v assign
nx
c aout v

k
k
infogain f

f

r
r
c assignk
v
v
k




r number records entire dataset
x log x

counts needed computation read directly ct aout ai ai n
searching best subset attributes simply question search among
attribute sets size n n specified user simple example designed test
f x



ficached sufficient statistics efficient machine learning

name

num num
records attributes
adult

small adult income dataset placed uci
repository ron kohavi kohavi contains
census data related job wealth nationality attribute arities range uci repository called test set rows missing
values removed
adult

kinds records different
data training set
adult

adult adult concatenated
census

larger dataset different census provided ron kohavi
census

data census addition
two extra high arity attributes
birth

records concerning wide number readings
factors recorded stages pregnancy attributes binary attributes sparse values
false
synth
k k
synthetic datasets entirely binary attributes generated bayes net figure
r

table datasets used experiments

counting methods practical feature selector would need penalize number
rows contingency table else high arity attributes would tend win

adtrees bayes net structure discovery

many possible bayes net learning tasks entail counting hence
might speeded adtrees present experimental
particular example scoring structure bayes net decide well matches
data
use maximum likelihood scoring penalty number parameters
first compute probability table associated node write parents j
parent attributes node j write xj set possible assignments values
parents j maximum likelihood estimate
v j xj

p aj

estimated



v xj

c xj
estimates node j probability tables read ct aj parents j
next step scoring structure decide likelihood data given
probability tables computed penalize number parameters network
without penalty likelihood would increase every time link added
c aj



fimoore lee

figure bayes net generated synth datasets
three kinds nodes nodes marked triangles generated
p ai p ai square nodes deterministic
square node takes value sum four parents even else
takes value circle nodes probabilistic functions single
parent defined p ai j parent p ai j parent
provides dataset fairly sparse values many
interdependencies
network penalized log likelihood score friedman yakhini



n

params log r r

n

x
x x
j

j

asgn

xj v

v asgn log p aj v j asgn

p aj



nparams total number probability table entries network
search among structures best score experiments use randomrestart stochastic hill climbing operations random addition removal
network link randomly swapping pair nodes latter operation necessary
allow search choose best ordering nodes bayes net stochastic
searches popular method finding bayes net structures friedman
yakhini probability tables affected nodes recomputed
step
figure shows bayes net structure returned bayes net structure finder
iterations hill climbing

adtrees rule finding
given output attribute aout distinguished value
conjunctive queries form



v

assign ai v ai n vn


rule finders search among


ficached sufficient statistics efficient machine learning





r

b
u

e
relationship
class
sex
capital gain
hours per week
marital status
education num
capital loss
age
race
education
workclass
native country
fnlwgt
occupation



c

r
e

n
p





























pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars
pars

















parents
relationship
relationship class
class
relationship class sex
relationship sex
class
class
marital status
relationship education num
relationship education num
relationship hours per week education num
education num race
parents
class sex education workclass

score
search took seconds

figure output bayes structure finder running
adult dataset score contribution sum equation due
specified attribute np number entries probability
table specified attribute
query maximizes estimated value


p aout

vout assign
vout j assign c aoutc assign




avoid rules without significant support insist c assign number
records matching query must threshold smin
experiments implement brute force search looks possible
queries involve user specified number attributes n build ct aout ai ai n
turn choose n tables look rows table
queries ai ai n greater minimum support smin
return priority queue highest scoring rules instance adult dataset
best rule predicting class attributes
score workclass private education num maritalstatus married civ spouse capital loss class k

experimental
let us first examine memory required adtree datasets table shows us
example adult dataset produced adtree nodes tree
required almost megabytes memory among three adult datasets size
tree varied approximately linearly number records
unless otherwise specified experiments section adult datasets
used leaf lists birth synthetic datasets used leaf lists size rmin
default birth dataset large number sparse attributes required modest
megabytes store tree many magnitudes worst case bounds among
synthetic datasets tree size increased sublinearly dataset size indicates


fimoore lee

dataset
census
census
adult
adult
adult
birth
syn k
syn k
syn k
syn k
syn k














r












nodes megabytes build time


































table size adtrees datasets number
attributes r number records nodes number nodes
adtree megabytes amount memory needed store
tree build time number seconds needed build tree
nearest second

dataset gets larger novel records may cause nodes appear
tree become less frequent
table shows costs performing iterations bayes net structure searching
experiments performed mhz pentium pro machine megabytes
main memory recall bayes net iteration involves one random change
network requires recomputation one contingency table exception first
iteration nodes must computed means time run
iterations essentially time compute contingency tables among adult
datasets advantage adtree conventional counting ranges factor
unsurprisingly computational costs adult increase sublinearly
dataset size adtree linearly conventional counting computational
advantages sublinear behavior much pronounced synthetic data
next table examines effect leaf lists adult birth datasets
adult dataset byte size tree decreases factor leaf lists
increased computational cost running bayes search increases
indicating worth tradeoff memory scarce
bayes net scoring involved average cost computing contingency tables
many different sizes following tables make savings fixed
size attribute sets easier discern tables give feature selection
rule finding respectively biggest savings come small attribute sets
computational savings sets size one two however particularly interesting
since counts could cached straightforward methods without needing tricks
cases however see large savings especially birth data datasets
larger numbers rows would course reveal larger savings


ficached sufficient statistics efficient machine learning

dataset
census
census
adult
adult
adult
birth
syn k
syn k
syn k
syn k
syn k














r adtree time regular time speedup factor













































table time seconds perform hill climbing iterations
searching best bayes net structure adtree time time
adtree regular time time taken
conventional probability table scoring method counting
dataset speedup factor number times adtree
method faster conventional method adtree times
include time building adtree first place given
table typical use adtrees build tree
able use many data analysis operations building
cost amortized case even including tree building cost
would minor impact

min

r















adult
birth
mb nodes build search mb nodes build search
secs
secs
secs
secs







































































table investigating effect rmin parameter adult
dataset birth dataset mb memory used adtree
nodes number nodes adtree build secs time
build adtree search secs time needed perform
iterations bayes net structure search


fimoore lee

number
number
attributes attribute
sets











adult

adtree regular speedup

time

time

factor



















number
attribute
sets






birth

adtree regular speedup
time

time







factor




table time taken search among attribute sets given size
number attributes set gives best information gain
predicting output attribute times seconds average
evaluation times per attribute set

number number
attributes
rules










adult

adtree regular speedup

time






time






factor






number
rules






birth

adtree regular speedup

time






time





table time taken search among rules given size number attributes highest scoring rules predicting output
attribute times seconds average evaluation time per
rule



factor





ficached sufficient statistics efficient machine learning

alternative data structures
use kd tree
kd trees used accelerating learning omohundro moore et al
primary difference kd tree node splits one attribute instead
attributes much less memory linear number records
counting expensive suppose example level one tree splits level
two splits etc case binary variables query involving
attributes higher explore paths tree level
datasets fewer records may cheaper performing linear search
records another possibility r trees guttman roussopoulos leifker
store databases dimensional geometric objects however context
offer advantages kd trees

use frequent set finder
frequent set finders agrawal et al typically used large databases
millions records containing sparse binary attributes ecient exist
finding subsets attributes co occur value true fixed
number chosen user called support records recent mannila
toivonen suggests frequent sets used perform ecient counting
case support frequent sets gathered counts
frequent set retained equivalent producing adtree instead
performing node cutoff common value cutoff occurs value
false
use frequent sets way would thus similar use adtrees
one advantage one disadvantage advantage ecient
developed building frequent sets small number sequential passes
data adtree requires random access dataset built
leaf lists impractical dataset large reside main memory
accessed database queries
disadvantage frequent sets comparison adtrees
circumstances former may require much memory assume value rarer
throughout attributes dataset assume reasonably thus choose
frequent sets unnecessarily many sets produced
correlations extreme case imagine dataset values
attributes perfectly correlated values record identical
attributes would frequent sets contrast adtree would
contain nodes extreme example datasets much weaker
inter attribute correlations similarly benefit adtree
leaf lists another technique reduce size adtrees could
used frequent set representation


fimoore lee

use hash tables

knew small set contingency tables would ever requested instead
possible contingency tables adtree would unnecessary would better
remember small set contingency tables explicitly kind tree structure
could used index contingency tables hash table would equally time
ecient require less space hash table coding individual counts contingency
tables would similarly allow us use space proportional number non zero
entries stored tables representing sucient statistics permit fast solution
contingency table request adtree structure remains memory ecient
hash table method stores non zero counts
memory reductions exploit ignoring common values

discussion

numeric attributes

adtree representation designed entirely symbolic attributes faced
numeric attributes simplest solution discretize fixed finite set values
treated symbols little help user requests counts
queries involving inequalities numeric attributes future work evaluate use
structures combining elements multiresolution kd trees real attributes moore
et al adtrees

specific counting tricks

many count conventional linear method specific
ways accelerating performance example bayes net structure finder may try
remember contingency tables tried previously case needs evaluate
deletes link deduce contingency table old one
without needing linear count
cases appropriate use adtree may lazy caching mechanism birth adtree consists root node whenever structure finder
needs contingency table cannot deduced current adtree structure
appropriate nodes adtree expanded adtree takes role
specific caching methods general much less memory
contingency tables remembered

hard update incrementally

although tree built cheaply see experimental section
although built lazily adtree cannot updated cheaply record
one record may match nodes tree worst case

scaling

adtree representation useful datasets rough size shape used
first datasets looked ones described


ficached sufficient statistics efficient machine learning

shown empirically sizes adtrees tractable given real noisy data
included one dataset attributes extent attributes skewed
values correlated enables adtree avoid approaching
worse case bounds main technical contribution trick allows
us prune common values without skewedness correlation would hardly
help empirical contribution actual
sizes adtrees produced real data vastly smaller sizes would get
worst case bounds appendix
despite savings adtrees cannot yet represent sucient statistics
huge datasets many hundreds non sparse poorly correlated attributes
dataset adtree cannot fit main memory latter case
could simply increase size leaf lists trading decreased memory increased
time build contingency tables inadequate least three possibilities remain
first could build approximate adtrees store information nodes
match fewer threshold number records approximate contingency
tables complete error bounds produced mannila toivonen
second possibility exploit secondary storage store deep rarely visited nodes
adtree disk would doubtless best achieved integrating machine learning
current database management tools topic considerable interest
data mining community fayyad et al third possibility restricts size
contingency tables may ask refuse store counts queries
threshold number attributes

cost building tree
practice adtrees could used two ways

one traditional required build adtree run fast
version discard adtree return

amortized dataset becomes available adtree built

tree shipped used anyone wishes real time counting
queries multivariate graphs charts machine learning
subset attributes cost initial tree building amortized
times used database terminology process known materializing harinarayan et al suggested desirable datamining
several researchers john lent mannila toivonen

one option useful cost building adtree plus cost running
adtree less cost original counting
intensive machine learning methods studied condition safely satisfied
decided use less intensive greedier bayes net structure finder table
without pruning datasets ran memory megabyte machine
built even tree easy birth dataset would needed store
nodes



fimoore lee

dataset

speedup ignoring
speedup allowing speedup allowing
build time
build time
build time
iterations
iterations
iterations
census



census



adult



adult



adult



birth



syn k



syn k



syn k



syn k



syn k




table computational economics building adtrees
search bayes net structures experiments section
shows run iterations instead account one
adtree building cost relative speedup adtrees declines greatly
conclude data analysis intense benefit adtrees even
used one fashion adtree used multiple purposes
build time amortized resulting relative eciency gains traditional counting
exhaustive searches non exhaustive searches
use non exhaustive searches include hill climbing bayes net learners greedy rule learners
cn clark niblett decision tree learners quinlan breiman
et al

acknowledgements
work sponsored national science foundation career award andrew moore
authors thank justin boyan scott davies nir friedman jeff schneider
suggestions ron kohavi providing census datasets

appendix memory costs

appendix examine size tree simplicity restrict attention
case binary attributes

worst case number nodes adtree
given dataset attributes r records worst case adtree occur
possible records exist dataset every subset attributes exists
exactly one node adtree example consider attribute set fai ai n g
n suppose node tree corresponding
unsurprisingly resulting bayes nets highly inferior structure



ficached sufficient statistics efficient machine learning

query fai v ai n vn g values v vn definition adtree
remembering considering case binary attributes state
v least common value ai
v least common value ai among records match ai v





least common value ai k among records match ai

k vk
one node moreover since worst case assumption
possible records exist database see adtree indeed contain
node thus worst case number nodes number possible subsets
attributes
vk



v ai

worst case number nodes adtree reasonable number rows
frequently case dataset fewer records much
r

lower worst case bound adtree size node kth level tree corresponds
query involving k attributes counting root node level node match
r k records node ancestors tree pruned
least half records choosing expand least common value attribute
introduced ancestor thus tree nodes level blog rc
tree nodes would match fewer r blog rc records
would thus match records making null
nodes adtree must exist level blog rc higher number nodes
level k every node level k involves attribute set size
k given binary attributes every attribute set one node
adtree thus total number nodes tree summing levels less


blog
xrc
bounded blog rc blog rc

k



k





k

number nodes assume skewed independent attribute values

imagine values attributes dataset independent random binary
variables taking value probability p taking value probability p
p smaller expect adtree
average less common value vary node match fraction min p p
parent records average number records matched kth level
tree r min p p k thus maximum level tree may
node matching one records approximately b log r log q c
q min p p total number nodes tree approximately

b log r x
log q c

bounded b log r log q c b log r log q c
k






k







fimoore lee

since exponent reduced factor log q skewedness among attributes
thus brings enormous savings memory

number nodes assume correlated attribute values

adtree benefits correlations among attributes much way
benefits skewedness example suppose record generated
simple bayes net figure random variable b hidden included
record j p ai aj p p adn node resulting adtree
number records matching node two levels adn tree
fraction p p number records matching adn see
number nodes tree approximately

b log r x
log q c

bounded b log r log q c b log r log q c
k








k


p
q p p correlation among attributes thus bring enormous
savings memory even case example marginal distribution
individual attributes uniform
p b

b

p b p








p b p

figure bayes net generates correlated boolean attributes


number nodes dense adtree section

dense adtrees cut tree common value vary node
worst case adtree occur possible records exist dataset dense
adtree require nodes every possible query attribute taking
values count tree number nodes kth level
dense adtree k worst case

k

number nodes leaf lists

leaf lists described section tree built maximum leaf list size rmin
node adtree matching fewer rmin records leaf node means
formulae used replacing r r rmin important
remember however leaf nodes must contain room rmin numbers instead
single count


ficached sufficient statistics efficient machine learning

appendix b building adtree
define function makeadtree recordnums recordnums subset
f
g total number records dataset




r

r





makes adtree rows specified recordnums adnodes represent
queries attributes ai higher used

makeadtree recordnums


make adnode called adn
adn count j recordnums j
j
j th vary node adn makevarynode aj recordnums

makeadtree uses function makevarynode define
makevarynode recordnums


make vary node called vn
k ni
let childnumsk fg
j recordnums
let vij value attribute ai record j
add j set childnumsv
let vn mcv argmaxk j childnumsk j
k ni
j childnumsk j k mcv
set ai k subtree vn null
else
set ai k subtree vn makeadtree ai childnumsk
ij

build entire tree must call makeadtree f rg assuming binary attributes cost building tree r records attributes bounded

blog
xrc r

k k
k


references

agrawal r mannila h srikant r toivonen h verkamo fast discovery association rules fayyad u piatetsky shapiro g smyth p
uthurusamy r eds advances knowledge discovery data mining aaai
press


fimoore lee

breiman l friedman j h olshen r stone c j classification
regression trees wadsworth
clark p niblett r cn induction machine learning

cover thomas j elements information theory john wiley
sons
fayyad u mannila h piatetsky shapiro g data mining knowledge
discovery kluwer academic publishers journal
fayyad u uthurusamy r special issue data mining communications
acm
friedman n yakhini z sample complexity learning bayesian networks proceedings th conference uncertainty artificial intelligence
morgan kaufmann
guttman r trees dynamic index structure spatial searching proceedings third acm sigact sigmod symposium principles database
systems assn computing machinery
harinarayan v rajaraman ullman j implementing data cubes eciently proceedings fifteenth acm sigact sigmod sigart symposium
principles database systems pods pp assn computing
machinery
john g h kohavi r p eger k irrelevant features subset selection
cohen w w hirsh h eds machine learning proceedings
eleventh international conference morgan kaufmann
john g h lent b sipping data firehose proceedings third
international conference knowledge discovery data mining aaai press
kohavi r power decision tables lavrae n wrobel eds
machine learning ecml th european conference machine learning
heraclion crete greece springer verlag
kohavi r scaling accuracy naive bayes classifiers decision tree hybrid e simoudis j han u fayyad ed proceedings second
international conference knowledge discovery data mining aaai press
madala h r ivakhnenko g inductive learning complex
systems modeling crc press inc boca raton
mannila h toivonen h multiple uses frequent sets condensed representations e simoudis j han u fayyad ed proceedings second
international conference knowledge discovery data mining aaai press


ficached sufficient statistics efficient machine learning

moore w schneider j deng k ecient locally weighted polynomial
regression predictions fisher ed proceedings international
machine learning conference morgan kaufmann
omohundro ecient neural network behaviour journal
complex systems
quinlan j r learning ecient classification procedures application
chess end games michalski r carbonell j g mitchell eds machine learning artificial intelligence tioga publishing company
palo alto
quinlan j r learning logical definitions relations machine learning

roussopoulos n leifker direct spatial search pictorial databases
packed r trees navathe ed proceedings acm sigmod international conference management data assn computing machinery
rymon r se tree characterization induction p
utgoff ed proceedings th international conference machine learning
morgan kaufmann




