Journal Artificial Intelligence Research 43 (2012) 477522

Submitted 12/11; published 03/12

Proximity-Based Non-uniform Abstractions
Approximate Planning
Jir Baum
Ann E. Nicholson
Trevor I. Dix

Jiri@baum.com.au
Ann.Nicholson@monash.edu
Trevor.Dix@monash.edu

Faculty Information Technology
Monash University, Clayton, Victoria, Australia

Abstract
deterministic world, planning agent certain consequences
planned sequence actions. so, however, dynamic, stochastic domains
Markov decision processes commonly used. Unfortunately suffer curse
dimensionality: state space Cartesian product many small sets (dimensions),
planning exponential number dimensions.
new technique exploits intuitive strategy selectively ignoring various dimensions different parts state space. resulting non-uniformity strong
implications, since approximation longer Markovian, requiring use modified planner. use spatial temporal proximity measure, responds
continued planning well movement agent state space, dynamically adapt abstraction planning progresses.
present qualitative quantitative results across range experimental domains
showing agent exploiting novel approximation method successfully finds solutions planning problem using much less full state space. assess
analyse features domains method exploit.

1. Introduction
deterministic world planning agent certain consequences
actions, plan sequence actions, knowing execution necessarily
achieve goals. assumption appropriate flexible, multi-purpose robots
intelligent software agents need able plan dynamic, stochastic
domains operate, outcome taking action uncertain.
small medium-sized stochastic domains, theory Markov decision processes
provides algorithms generating optimal plan (Bellman, 1957; Howard, 1960; Puterman & Shin, 1978). plan takes account uncertainty outcome taking
action, specified distribution possible outcomes. flexibility,
reward function rather simple goal, relative desirability
otherwise situation specified.
However, domain becomes larger, algorithms become intractable approximate solutions become necessary (for instance Drummond & Bresina, 1990; Dean,
Kaelbling, Kirman, & Nicholson, 1995; Kim, 2001; Steinkraus, 2005). particular
state space expressed terms dimensions, Cartesian product sets,
size resulting computational cost exponential number dimensions.
c
2012
AI Access Foundation. rights reserved.

fiBaum, Nicholson & Dix

hand, fortunately, results fairly structured state-space effective
approximations often possible.
solution based selectively ignoring dimensions, parts
state space, time. words, obtain approximate solutions
dynamically varying level abstraction different parts state space.
two aspects approach. Firstly, varying level abstraction introduces
artefacts, planning algorithm must somewhat modified eliminate these.
Secondly, interestingly, appropriate abstraction must selected later modified
planning action progress.
work extension synthesis two existing approaches approximate planning: locality-based approximation envelope methods (Dean et al., 1995)
structure-based approximation uniform abstraction (Nicholson & Kaelbling, 1994; Dearden & Boutilier, 1997). work extends exploiting structure
locality, broadening scope problems contemplated. Baum Nicholson
(1998) introduced main concepts full details algorithms experimental
results presented Baums (2006) thesis. studies arbitrary
abstraction, instance Bertsekas Tsitsiklis (1996). However, generally
theoretical case tended treat approximation Markovian,
would resulted unacceptable performance practice. improve extending planning algorithm deal non-Markovian aspects approximation.
Finally, use measure locality, introduced Baum Nicholson (1998),
similar flexible influence measure Munos Moore (1999).
assume agent continues improve plan acting
planning failures generally fatal. deal control error exclusively. Sensor
error considered assumed agent accurately discern current
world state (fully observable), accurately knows state space, goal
reward function, distribution effect actions (no learning).
remainder paper organised follows. Section 2 reviews background,
introduces abstraction provides framework. Section 3 discusses planning
static non-uniform abstraction, Section 4 presents method initially selecting
non-uniform abstraction based problem description. Section 5 presents method
changing abstraction based policy planned, Sections 6 7 introduce
proximity measure method varying abstraction based measure,
respectively. Section 8 presents results based direct evaluation calculated
policy simulation. Finally, Section 9 discusses results Section 10 gives
conclusions outlines possible directions future work.

2. Planning Non-uniform Abstractions
non-deterministic world planning agent cannot certain consequences
actions except probabilities, cannot plan simple sequence actions achieve
goals. able plan dynamic, stochastic domains, must use
sophisticated approach. Markov decision processes appropriate commonly used
representation sort planning problem.
478

fiProximity-Based Non-uniform Abstractions Planning

2.1 Illustrative Problems
aid exposition, present two example problems here. full set experimental
domains presented Section 8.1.
two illustrative problems grid navigation domain, shown Figure 1.
integer x coordinates 0 9, three doors either
open closed damage indication either yes no. agent
move one four cardinal directions, open door next it, nothing.
doors fairly difficult open, probability success 10% per time step,
moving 80% chance success, effect case failure. Running
wall closed door causes damage, cannot repaired. transitions
shown Table 1. agent starts location marked s0 Figure 1 doors
closed damage, goal reach location marked damage.
x=0
y=0

1

2

3

4

5

6

7



s0

8

9

k3

1

x=0
y=0

2

3

4

5

6

7

s0

8

9

k3

1



2

d1

d2

k1

4

4

5

5

6

6



8





d1
3

7



2

3

9

1



d2



k1





7
8

k2

d3

9

(a)

k2

d3
(b)

Figure 1: layout grid navigation domain. blue arrows show optimal
path (a) suboptimal path (b) 3Keys problem. 3Doors
problem grid layout (walls doors) keys.

3Keys problem contains keys, required open doors. agent
may one time. additional action allows agent
pick key location shown figure, open action requires
corresponding key effective (there separate unlock action). 3Doors problem
contains keys doors unlocked closed therefore corresponding
keys pickup action.
optimal policy obtained exact planning 3Doors problem simply takes
shortest path door 2. 3Keys problem, optimal plan collect
keys 3 1, pass south door 1 east door 3, shown Figure 1(a).
suboptimal plan shown Figure 1(b).
479

fiBaum, Nicholson & Dix

x
Stay




pre-state
d1
d2
d3

post-state
d1
d2

dmg



x















3
3


y+1



















South
2
7




2
2
2
9


open






open






















80%
80%

North
2
7




3
3
0
3


open






open






















80%
80%

East
4
4
4
4
4
9
x

0
1
2
9























open




















80%
80%
80%
80%

West
5
5
5
5
5
0
x

0
1
2
9























open




















Open
2
2
7
7
4
5


2
3
2
3
9
9










































80%

80%

80%
80%
80%
80%
80%

80%
10%
10%
10%
10%
10%
10%

d3

dmg



























yes
yes


2
2


y1





















yes
yes


5
5
5
5


x+1





































yes
yes


4
4
4
4


x1





































yes
yes


















open
open








open
open








open
open








yes

Table 1: Transitions 3Doors problem, showing important changed dimensions
only. First matching transition used. percentage shown, given
post-state occur probability, otherwise state unchanged. Transitions without percentages deterministic.

480

fiProximity-Based Non-uniform Abstractions Planning

2.2 Exact Planning
One approach exact planning stochastic
domains
involves using Markov Decision



0
Processes (MDPs). MDP tuple S, A, T, R, , state space,
set available actions, transition function, R reward function s0
initial state. agent begins state s0 . time step, agent selects
action A, which, together current state, applies obtain distribution
S. current state next time-step random according distribution
write PrT (s, a, ) probability action taken state result state
next time-step. agent given reward time step, calculated
R current state (and possibly action selected). aim agent
maximise cumulative function rewards, typically expected discounted1
sum discounting factor . fully-observable MDP, agent full knowledge.
particular, agent aware , R current state selecting action.
well-known fully-observable MDP, optimal solution expressed
policy : mapping current state optimum action. planning
problem, then, calculation . side-effect calculation, standard
algorithms calculate value function V : R, expected discounted sum
rewards starting state. Table 2 summarises notation used paper.
well known iterative algorithms, Bellmans (1957) value iteration, Howards
(1960) policy iteration, modified policy iteration Puterman Shin (1978)
computing optimal policy . However, becomes larger, calculation
becomes computationally expensive. particularly state space structured
Cartesian product dimensions, = S1 S2 SD , |S| exponential
D. Since algorithms explicitly store V usually , functions S,
space complexity therefore exponential D. Since iterate arrays,
time complexity least exponential D, even consideration
fast iterative algorithms converge. Typically, grows, planning
quickly becomes intractable. Since practice amount computation allowed
agent limited, necessitates approximations process.
3Doors problem, six dimensions (two x coordinates, three
doors one damage), = {0 . . . 9} {0 . . . 9} {open, closed}
{open, closed} {open, closed} {damage, damage} |S| = 12 800. action space
set five actions, = {north, south, east, west, open}. transition function
specifies outcomes taking action state. reward function R 0
agent h7, 7i location (marked diagram) damage, 1
location damage, 2 damage. Finally, s0 state
agent h0, 0i location, doors closed, damage.
Exact planning listed results |S| V (s0 ) comparison.
approximation, planner must consider whole state space S. |S| therefore
measure cost planning directly terms space indirectly terms
time. hand, since planning exact, optimal value function V
1. illustrative problems simple goals achievement, use time discounting order
remain general mathematical convenience.

481

fiBaum, Nicholson & Dix

symbol
original

Sd

meaning

abstract
W P(S)


state space (specific state space / worldview, resp.)
dimension state space
N
number dimensions
sS
wW
state
0


initial state
scur

current state (in on-line planning)
sd Sd
wd Sd
dimension state w, resp.

set actions (action space)
a0
default action


transition function (formal)
PrT :SAS[0, 1] PrT :WAW[0, 1] transition function (in use)
R:S R
R:WR
reward function (one-step reward)
V :SR
V :W R
value function (expected discounted sum rewards)
[0, 1)
discount factor reward
:SA
:WA
policy
:

optimal policy
V :S R

optimal value function (exact value function )
, :
, : W
approximate policy, ith approximate policy

exact value function (note: may abstract)
V : R
V : R
V : W R
approximate value function (approx. V )
Table 2: Summary notation. first column notation original MDP,
second notation non-uniform abstraction applied.

obtained, along optimal policy ensuring agent expect obtain
value. figures approximations must measure.
2.3 Uniform Abstraction
One method approximation take advantage dimensions ignoring
irrelevant marginally relevant order obtain
approximate solution. uniform sense dimensions ignored
throughout state space. Since approach attacks curse dimensionality
originates, dimensions, effective counteracting it.
Dearden Boutilier use obtain exact solution (Boutilier, 1997) approximate one (Boutilier & Dearden, 1996; Dearden & Boutilier, 1997). However, abstractions fixed throughout execution, dimensions deleted problem
pre-determined sequence. makes approach somewhat inflexible. Similarly,
Nicholson Kaelbling (1994) propose technique approximate planning.
delete dimensions problem based sensitivity analysis, refine abstraction execution time permits, still uniform. Dietterich (2000) uses kind
abstraction combination hierarchical planning good effect: subtask,
Navigate location t, ignore irrelevant dimensions, location items
482

fiProximity-Based Non-uniform Abstractions Planning

picked even ultimate destination agent. Generally, time problem
description derived general source rather specified particular
problem, uniform abstraction help. Gardiol Kaelbling (2008) use dimensions relevant, marginally, ignoring results approximate
solution improved planning progresses.
Unfortunately, however, least human-specified problems, one would generally
expect mentioned dimensions way relevant. Irrelevant dimensions
eliminated human designer natural course specifying problem.
Depending domain situation marginally relevant dimensions might
included, often, nearly enough effective approximation.
list comparisons uniform abstraction results reason
sample domains, makes little sense. almost dimensions
important solving problem. case methods exist
effective uniform abstraction, integrated approach easily.
2.4 Non-uniform Abstraction
approximation, non-uniform abstraction, replaces state space W, particular type partition S, originally introduced Baum Nicholson (1998). call
W worldview, members W worldview states members specific
states.2 Non-uniform abstraction based intuitive idea ignoring dimensions
parts state space. example, door interest agent
walk it, ignored parts state space
distant door. particular member worldview wi W, dimension
either taken account
(concrete, refined in), ignored altogether (abstract,
Q completely
w singleton subset corresponding
w
coarsened out). wi =

d=1

concrete dimensions equal Sd abstract dimensions.3 worldview
selection modification methods ensure W remains partition times.
give example, 3Doors problem one possible worldview location
damage dimensions concrete every state, door dimensions concrete
states within two steps respective door.
Note domain still fully-observable. question lack knowledge
dimensions question, wilful conditional ignorance planning
matter computational expediency. approximation subsumes exact
planning uniform abstraction. exact planning, dimensions set uniformly
concrete, |W| = |S| worldview state corresponds one specific state.
uniform abstraction, combination abstract concrete dimensions fixed
entire worldview. treated special cases general approach.4
2. Previously, used word envelope concept (Baum & Nicholson, 1998), however,
worldview better describes approximation used envelope.
3. allow dimension partially considered, abstract level dimensions,
within them. dimension x coordinate either particular value, fully
abstract, never 59, instance.
4. modified calculation reduces standard algorithm uniform fully concrete worldviews,
planner obtains standard results cases.

483

fiBaum, Nicholson & Dix

hand, approximation longer Markovian. dimension
abstracted away indeterminate. notation Markov Decision Processes,
represented distribution concrete states, dimension
stochastic specific (but ignored) value. distinction important
truly stochastic outcome, quite valid plan retry action
succeeds (for instance, opening door 3Doors problem). dimension
merely ignored, agent obtain outcome (door closed) time moves
region dimension ignored, within worldview, previous
states appear matter. discuss Section 3.
2.5 Comparison Approaches
Non-uniform abstractions began appear literature first usually side-effect
structured method, state space represented decision tree based
individual dimensions, Boutilier, Dearden, Goldszmidt (1995, 2000). Note,
however, decision tree structure imposes restriction kinds non-uniform
abstraction represented: dimension root tree considered
throughout state space, on. significant restriction results
representation much limited representation. similar restriction affects
de Alfaro Roys (2007) magnifying-lens abstraction, refinement multivalued dimensions taken bit-by-bit bits interleaved, level
decision tree halves space along different dimension pre-determined order.
note, would work well dimensions correspond more-or-less connected
space, gridworld, would less well features doors
grid navigation domain. Magnifying-lens abstraction calculates upper lower bounds
value function, rather single approximation, advantage guiding
abstraction selection allows definite termination condition (which lack).
hand, always considers fully-concrete states part algorithm, limiting
space savings square root state space, whereas algorithm work
mixture variously abstract states necessarily including fully concrete
ones. Another related approach variable grids used discretisation,
indirectly used discrete domains, Boutilier, Goldszmidt, Sabata (1999) do,
dimensions reasonably approximated continuous (for instance money). Unlike
approach, variable grids completely inapplicable predicates binary
enumerated dimensions. Some, Reyes, Sucar, Morales (2009), use techniques
ways quite similar continuous MDPs, though quite different
ways: consider refinement only, coarsening; use sampling, rather
directly dealing domain model; use different refinement method,
refinement evaluated fact either committed rolled back.
Perhaps similar approach one modules Steinkraus (2005),
ignore-state-variables module. However, module appears completely manual,
requiring input variables (dimensions) ignored parts state
space. uses values dimensions current state scur , rather
distribution, obviously restricts situations may used (for instance,
3Doors problem, doors could ignored starting state). Finally, since
484

fiProximity-Based Non-uniform Abstractions Planning

Steinkraus (2005) analyse report relative contributions modules
solution, meta-planning problem selecting arranging modules,
difficult know extent particular module useful.
approaches take advantage different features different domains. instance,
factored MDP approach (used, instance, Boutilier et al., 2000, Guestrin,
Koller, Parr, & Venkataraman, 2003) suitable domains parts state
action spaces grouped together within group actions action
dimensions affect corresponding states state dimensions interaction
groups weak. St-Aubin, Hoey, Boutilier (2000) iterate symbolic representation
form algebraic decision diagrams produce approximate solutions, Sanner
Boutilier (2009) iterate symbolic representation whole class problems
domain, using symbolic dynamic programming, first-order algebraic decision diagrams
linear value approximation, pre-compute generic solution used
quickly solve specific problems class. focus state space, others approximate action space, typically grouping actions (possibly hierarchically)
macro actions, Korf (1985). instance Hauskrecht, Meuleau, Kaelbling, Dean,
Boutilier (1998) Botea, Enzenberger, Muller, Schaeffer (2005) take approach,
Parr (1998) uses finite state automata macro actions Srivastava, Immerman, Zilberstein (2009) take using algorithm-like plans branches
loops. Goldman, Musliner, Boddy, Durfee, Wu (2007) reduce state space generating (limited-horizon, undiscounted) MDP different, non-MDP representation
including reachable states, pruning detected clearly
immediately poor, inferior equivalent already-generated states. Naturally, many
approaches combined. instance, Gardiol Kaelbling (2004, 2008) combine state space abstraction envelope work Dean et al. (1995), Steinkraus
(2005) uses modular planner view combining many approaches may
appropriate given problem. details approaches variants
refer reader recent survey field Daoui, Abbad, Tkiouat (2010).
2.6 Dynamic Approximate Planning
top-level algorithm shown Algorithm 1. initialisation, consisting
selecting initial abstraction setting policy, value proximity a0 , 0
proportionally size worldview state, respectively,5 planner enters
infinite loop stochastically alternates among five possible calculations,
described following sections. elsewhere algorithm, use
stochastic choice default absence directed method.
agent assumed processing power available acting,
continually improve policy, modify approximation updates focus
planning based current state. means agent need plan
well unlikely possibilities, therefore expend planning effort
likely paths closer future, expecting reaches parts
state space, improve approximation appropriate.
5. Initialising approximate policy action a0 constitutes domain-specific heuristic namely,
known default action a0 reasonably safe states, nothing action.

485

fiBaum, Nicholson & Dix

Algorithm 1 High-level algorithm Approximate Planning Dynamic Non-uniform
Abstractions
select initial abstraction /* Algorithm 3 */
worldview states w
(w) a0 ; V (w) 0; P(w) |w|
|S|
policy value calculation /* Algorithm 2 */
loop
choose stochastically
policy value calculation /* Algorithm 2 */

policy-based refinement /* Algorithm 4 */

proximity calculation /* Algorithm 5 */

proximity-based refinement /* Algorithm 6 */

proximity-based coarsening /* Algorithm 7 */
input latest current state; output policy

Actual execution policy assumed separate thread (executive),
planner concern timeliness requirements
domain: whenever action needs taken, executive simply uses policy
recently received planner.
Dean et al. (1995) call recurrent deliberation, use locality-based
approximation. similar architecture used CIRCA system (Musliner, Durfee, &
Shin, 1995; Goldman, Musliner, Krebsbach, & Boddy, 1997) guarantee hard deadlines.
CIRCA terminology, planner AIS (AI subsystem), executive
RTS (real-time subsystem).
alternative recurrent deliberation pre-cursor deliberation, agent first
plans, finished planning begin act, making
adjustments plan policy. Effectively, planner, current state constant
equal initial state throughout planning. work pre-cursor mode used
measurements, involves fewer potentially confounding variables.
Conceptually, approach divided two broad parts: open-ended problem selecting good abstraction relatively closed problem planning within
abstraction. Since latter part closed, deal first, next
section, covering Algorithm 2. explore open-ended part Sections 57,
covering Algorithms 37.

3. Solving Non-uniformly Abstracted MDPs
Given non-uniform abstraction, simplest way use planning take one
standard MDP algorithms, modified policy iteration Puterman
Shin (1978), adapt non-uniform abstraction minimally. formulae translate
486

fiProximity-Based Non-uniform Abstractions Planning

directly obvious fashion. becomes function worldview states instead concrete
states, on, shown Algorithm 2 (using simple variant update policy
w procedure). Probabilities transition one worldview state another
approximated using uniform distribution concrete states (or possibly
distribution, information available).
Algorithm 2 policy value calculation
repeat n times
worldview states w
update value w
worldview states w
update policy w
update value w
procedure update value w
PrT (w, (w), w) = 1
/* optimisation V (w) calculated directly case */
V (w) R(w)
1
else
P
V (w) R(w) + w PrT (w, (w), w )V (w )

procedure update policy
P w variant simple
(w) min arg maxa w PrT (w, a, w )V (w )

procedure update policy w variant Locally Uniform Abstraction
/* see Section 3.1 discussion Locally Uniform Abstraction */
absdims {d :
w . PrT (w, a, w ) > 0 w abstract d}
w abstract
absdims
LUA w . w :
dimension w = dimension w
/ absdims
P
|w w |


V w . w W |w | V (w )
P
(w) min arg maxa w PrT (w, a, w )V (LUA(w ))

Note Algorithm 2, considered ordered set a0 smallest element
minimum used arg max gives one possibility.
two aspects: (a) domain-specific heuristic, instance, breaking ties favour
default action possible, (b) avoid policy-basedP
refinement (see Section 5)
based actions equal value. Secondly, efficiency, w calculated
states w PrT (w, a, w ) > 0, since states make contribution
sum. Finally, number n tuning parameter particularly critical (we use
n = 10).
course, replacing state space worldview W way not, general,
preserve Markov property, since actual dynamics may depend aspects state
space abstracted worldview. simple variant ignore assume
Markov property anyway, grounds is, all, approximation.
Unfortunately, resulting performance unacceptably large error, including
outright non-attainment goals.
487

fiBaum, Nicholson & Dix

instance, 3Doors problem, situation occur three
doors whenever abstract s0 concrete near door question.
doors relatively difficult open, 10% probability success per try.
hand, moving area abstract area
concrete, assumed probability door already open 50%.
calculations performed, turns preferable plan loop, repeatedly trying
illusory 50% chance success rather attempting open door
10% chance success. agent never reach goal. Worse still, ways,
estimate quality solution quite good, V (s0 ) 19.0,
fact better even optimal solutions V (s0 ) 27.5, true quality
solution poor, V (s0 ) = 100 000, corresponding never reaching goal (but
incurring damage, either; figures discounting factor = 0.999 99).
Regions take account particularly bad piece information may seem unattractive, described above, vice versa. call problem Ostrich effect,
agent refusing accept unpleasant fact, mythical ostrich buries
head sand. solution, Locally Uniform Abstraction, described next section.
abstracted approximation simply treated MDP agent
know state reach (near closed door near open door), correspond
underlying process, might reach particular state deterministically (as
here). problem especially obvious example, planner plans loop.
reminiscent problem noted Cassandra, Kaelbling, Kurien (1996),
plan derived POMDP failed actual robot got loop particular
situation sensor completely reliable contrary model.
3.1 Locally Uniform Abstraction
ostrich effect occurs states different abstraction considered, instance
one door abstract one door concrete closed.
solution make abstraction locally uniform, therefore locally Markovian
duration policy generation iterative step. making abstraction locally,
temporarily uniform, iterative step policy generation algorithm never work
across edge abstract region, and, since information available
states considered point, impetus favoured
avoided basis (for instance, avoiding state door concrete closed
favour one door abstract). action chosen chosen based
information presence absence.
modification update policy w procedure Algorithm 2:
states considered one one, region around state accessed
function returns locally uniform version. States concrete state
considered averaged ignore distinctions. different states
considered, sometimes states taken themselves, sometimes estimated
values V averaged adjacent states. means dimensions
partially considered states cases, mean
concrete region must extend one step beyond region dimension
488

fiProximity-Based Non-uniform Abstractions Planning

immediately relevant. dimension fully considered state, possible
outcomes actions state must concrete dimension.
modified procedure proceeds follows: first dimensions abstract
possible outcome state updated w collected variable absdims.
function LUA constructed takes worldview states w returns potential
worldview states w w abstract dimensions absdims.
core modification, named LUA Locally Uniform Abstraction. Since
potential states returned LUA not, general, members W, therefore
necessarily value stored V , function V constructed calculates
weighted
averages value function V potential states. sum,
P

calculated states w w w 6= efficiency. Finally,

w
update step carried using two functions LUA V .
Unfortunately, modification applied, algorithm may may converge depending worldview. Failure converge occurs concrete region
small cases, algorithm cycle two policies (or conceivably
more) instead converging. One must careful, therefore, worldview, avoid
situations, else detect modify worldview accordingly. policybased worldview refinement algorithm described Section 5 ensures convergence
practice.

4. Initial Abstraction
beginning planning, planner must select initial abstraction. Since
worldview never completely discarded planner, infelicity stage may
impair entire planning process, worldview-improvement algorithms
make amount weakness here.
different ways select initial abstraction. propose one heuristic
method selecting initial worldview based problem description,
variants. Consider example door 3Doors problem associated
two locations, is, immediately either side. makes sense, then, consider
status door two locations. association read problem
specification. Intuitively, structure solution likely resemble structure
problem. incorporates structure transition function initial
worldview. reward function incorporated, reflecting assumption
dimensions reward based important.
use two-step method derive initial worldview, shown Algorithm 3.
Firstly, reward function specified based particular dimensions. make
dimensions concrete throughout worldview, leave dimensions abstract.
3Doors problem, x dmg dimensions, step
10 10 2 = 200 states worldview.
Secondly, transition function specified decision trees, one per action. use
find nexuses dimensions, is, linking points, points
dimensions interact. nexus corresponds one path root
tree leaf. example, 3Doors problem, decision tree open action
contains leaf whose ancestors x, y, d1 stochastic node, choices leading
489

fiBaum, Nicholson & Dix

Algorithm 3 select initial abstraction
/* set worldview completely abstract */
W {S}
/* reward step */
reward step enabled
dimensions mentioned reward tree
refine whole worldview dimension
/* nexus step */
nexus step enabled
leaf nodes action trees
worldview states w matching pre-state
refine w dimensions mentioned pre-state

leaf labelled respectively 4, 2, closed 10%. corresponds nexus
sx = 4, sy = 2 sd1 = closed (the stochastic node ignored determining nexus).
total, four nexuses side door, two locations immediately
adjacent, shown Figure 2(a), connecting relevant door dimension x
coordinates. initial worldview shown Figure 2(b), x, dmg concrete
everywhere doors abstract except concrete one location directly
side door, corresponding location nexuses Figure 2(a).
steps, |W| = 212, compared |S| = 1 600 specific states.

x=0

1

2

3

4

5

6

7

8

9

x=0

y=0

y=0

1

1

1

2

3

4

5

6

7

2





2

d1

d2

3





3

d1

d2

4

4

5

5

6

6

7

7

8

8

9



9

(a)

8

9

d3 d3
(b)

Figure 2: Nexus step initial abstraction, showing (a) location nexuses
3Doors problem (there four nexuses ) (b) locations
door dimensions concrete initial worldview.

490

fiProximity-Based Non-uniform Abstractions Planning

3Keys problem, location nexuses Figure 2(a), except
nexuses location involve corresponding
key dimensions. Thus, initial worldview, locations shown Figure 2(b)
concrete corresponding door dimension, also, closed,
corresponding key dimension. states doors open, key dimension
remains abstract. initial worldview size 3Keys |W| = 224.
Due locally-uniform abstraction, concrete door dimensions taken
account minimal degree. worldview used without
refinement, expected resulting policies would poor.
results6 bear expectation. worldview initialization methods therefore
intended used own, rather basis refinement. Thus,
real test methods well work coupled worldview
modification methods, described below.

5. Policy-Based Refinement
section presents first worldview modification methods, policy-based refinement. method modifies worldview based directly current approximate
policy . particular, refines states based differences actions planned
adjacent, differently-abstract states. differences indicate dimension may
important, adjacent states abstract dimension refined (i.e.
dimension made concrete states).
method previously introduced Baum Nicholson (1998), showed,
using small navigation domain example (the 3Doors problem paper),
refinement method resulted good policy, though optimal. present quantitative results consider complex domains.
5.1 Motivation
motivation method twofold. Firstly, already indicated, method detects
areas particular dimension important, affects action planned,
ensures concrete adjacent states. Thus regions dimension taken
account expand long dimension matters, stop. Secondly,
method fulfils requirements choosing worldview avoid non-convergence
policy calculation, mentioned Section 3.1 above.
Dimensions important affect policy, since policy planners
output. less important parts state space affect
policy. Thus, dimensions need concrete remain abstract
gleaned part state space comparing optimal actions
various states. optimal actions equal, states abstract,
differ, states concrete. However, optimal policy .
approximate policy worldview, difficult. However, planner compare
policies areas dimension concrete, found important there,
expand area concrete. policy-based refinement policy calculation
6. Omitted uninteresting, presented Baum (2006).

491

fiBaum, Nicholson & Dix

alternate, refinement continue area dimension concrete covers
whole region important.
Section 3.1 noted planning algorithm requires worldview chosen
care. algorithm described section detects situations potentially problematic locally-uniform abstraction modifies worldview preclude them.
Intuitively, incorrect behaviour occurs edge concrete region intersects
place two fairly-similarly valued courses action, corresponding
two different paths goal.
5.2 Method
method uses transition function definition adjacent states, worldview states w w considered adjacent . PrT (w, a, w ) > 0. definition
symmetrical general, since transition function not, problem
method, seen below. algorithm shown Algorithm 4.
Algorithm 4 policy-based refinement
candidates
worldview states w
actions
w : P r(w, a, w ) > 0

dimensions
: w abstract w concrete
w

abstract


construct w :
dimension w = dimension w 6=

b

w , w . (w ) 6= (wb ) wa w 6= wb w 6=
/* policy throughout w */
candidates candidates {(w, d)}
(w, d) candidates
w W
/* replace w
group states concrete */
anew
w
concrete

wnew :
dimension wnew = dimension w 6=
W W {wnew }
new
(wnew ) (w); V (wnew ) V (w); P(wnew ) |w|w| | P(w)
W W \ {w} /* discarding stored (w), V (w) P(w) */

Example 3Doors problem, instance, applying method planning
increases number worldview states initial 212 220231, depending
stochastic choices (recall |S| = 1 600 comparison). produces concrete regions
nice tight around doors, shown Figure 3, allowing algorithm
converge reasonable solution. solution fact optimal given initial
state s0 , though simply coincidence, since s0 taken account
algorithm states somewhat suboptimal actions (the agent would reach
goal states, shortest route).
492

fiProximity-Based Non-uniform Abstractions Planning

x=0

1

2

3

4

5

6

7

8

9

x=0

y=0

1

2

3

4

5

6

7

8

9

y=0


1

d1

d2

1

k2

2

d1 d1 d1

d2 d2 d2

2

3

d1 d1 d1 d1

d2

3

d1 k1 d1 d1

4

d1 d1

4

d1 d1 d1 d1 d1

5

5

d1 d1 d1

6

6

d1

7

7





k1






k2



8

d3

8

k3

9

d3 d3 d3

9

k3 k3 k3

(a)



k2 k2 k2







(b)

Figure 3: Example non-uniform abstraction (a) 3Doors (b) 3Keys problems
policy-based refinement. x, dmg dimensions concrete everyd
where; d1, d2 d3 indicate corresponding door concrete; k1,


k2 k3 indicate corresponding door concrete corresponding
key concrete door closed.

worldview obtained method often quite compact. instance, rather
refining simple 2 3 rectangular region side door 3Doors, human
might, algorithm makes 4 locations concrete approach side door,
enough obtain good solution. seen north sides doors
d1 d2, well west side door d3 (3 concrete locations, due edge).
departure side doors d2 d3, even better makes refinement all:
south door d2 east door d3, action move toward goal, regardless
status door actions equal, refinement takes place.
south side door d1 seems rather less compact. concrete area fact
big 6 locations 3Doors seems excessive compared compact
concrete areas elsewhere. occur nexus close region
best action take genuinely depends status dimension nexus,
difference small. somehow agent found h4, 3i policy-based
refinement independent scur optimal path genuinely would depend whether
door d1 open, path slightly suboptimal case. theory
region could arbitrarily large extent, seems relatively minor effect
practice. Here, instance, adds couple states, 1% |W|,
found real problem domains (or domains used Baum,
2006).
493

fiBaum, Nicholson & Dix

5.3 Limitations
Policy-based refinement deal cases single dimension makes difference. two dimensions needed combination, often miss them.
instance, 3Keys problem key quite distant corresponding door
policy-based refinement therefore never find relationship two.
key, appears reason pick up, door appears
means unlocking it.
Obviously, fixed ad hoc rewarding picking keys sake.
Indeed, domain formulations literature exactly that, rewarding agent
partial achievement goal. However, clean solution. effect,
domain specifications cheat providing hints.
Another problem policy-based refinement provide coarsening
worldview, modifying ways, instance execution progresses
planner needs update plan. Indeed, policy-based refinement ignores initial state
s0 altogether, current state scur recurrent planning. Thus produces
solution regardless part problem agent actually asked solve.
waste computation solving parts agent unlikely actually
visit, perhaps importantly carries penalty corresponding loss
quality relevant parts.
following sections describe proximity-based worldview modification, needed
solve domains combinations dimensions important makes
use s0 scur , appropriate.

6. Proximity Measure
general, worldview mostly concrete near agent planned path
goal, allow detailed planning, mostly abstract elsewhere, conserve computational resources. section describe measure (originally Baum & Nicholson,
1998) realises concept, proximity P, decreases state
future less probable.7 section extends brief description Baum
Nicholson (1998). following section present new worldview modification
methods based directly measure.
6.1 Motivation
proximity P realisation intuitive concept states near agent
likely visited, opposed distant agent unlikely. naturally
takes account current state scur recurrent planning, initial state s0
pre-cursor planning, unlike policy-based refinement ignores altogether. Thus
planner selecting worldviews based proximity measure produce solutions tailored
particular scur s0 ignore parts MDP irrelevant nearirrelevant performance state. Thus saves computation would otherwise
7. Baum Nicholson (1998) used word likelihood measure. prefer proximity
avoid confusion meanings word likelihood. Munos Moore (1999) use word
influence somewhat similar measure continuous domains.

494

fiProximity-Based Non-uniform Abstractions Planning

wasted solving parts agent unlikely actually visit, perhaps
importantly carries advantage corresponding gain quality
relevant parts. allows agent deal problems 3Keys beyond
reach policy-based refinement.
Implicitly, agent plans reaches mostly-abstract parts
state space, improve approximation appropriate. planner thus continually
improves policy, modifies approximation updates focus planning based
current state scur . means refining regions agent finds
likely visit, coarsening away details regions longer likely
visit already traversed.
three aspects proximity: temporal, spatial probabilistic. Firstly,
temporal aspect indicates states may encountered near future, exponentially decaying scale. second aspect spatial nearness states (in terms
state space) agent planned path. spatial aspect somewhat indirect,
spatial structure domain represented implicitly transition
matrix, proximity measure reflect it. two aspects combined
proximity give single real number 0 1 state, denoted P P
proximity, spatial aspect temporal aspect. number
interpreted probability namely probability encountering state P
interpreted probability distribution states, giving final, probabilistic
aspect proximity.
6.2 Calculation
formula proximity P similar formula value function.
three differences. Firstly, instead beginning reward function based
current state function, cur. Secondly, transition probabilities time-reversed
(that is, matrix transposed). value calculation based
reward function, occurs future (after taking actions), current state
function based present, taking actions. Since order taking actions
function upon formula based reversed time, similar reversal must
b used
applied transition probabilities. Thirdly, estimated future policy
b
b
instead . estimate, stochastic policy defined making (s) distribution
actions assigns constant probability current (s) distributes
remaining probability mass among actions equally. distributed probability
mass corresponds probability policy change sometime future,
or, alternately, probability currently-selected action yet correct.
formula therefore:
X

b ), s)P(s )
Pr(s , (s
(1)
P(s) cur(s) + P







P proximity discounting factor (0 P < 1)

1 P scur =
cur(s) =
0
otherwise
495

(2)

fiBaum, Nicholson & Dix

P
constant 1P chosen current-state function P(s) converges
1, words P probability distribution. checked near future,
agent probability P(s) state s, assuming follow policy
near future defined probability checking time proportional
Pt (that is, P interpreted stopping probability). value calculation,
one instead solve set linear equations
X

b ), s)P(s )
Pr(s , (s
(3)
P(s) = cur(s) + P




or, matrix notation,
(I P TbT )P = cur

(4)

b identity
Tb transition matrix induced stochastic policy
matrix. implementation uses matrix form, shown Algorithm 5. proximity
measure needs little adjustment work non-uniformly abstract worldview:
simply replaced w (1) (2), scur = becoming scur w.
Algorithm 5 proximity calculation
solve matrix equation P linear system:
(I P TbT )P = cur
measure two tuning parameters, replanning probability discounting
factor P . replanning probability controls spatial aspect: trades focus
likely path planning less likely eventualities nearby. Similarly, P controls
temporal aspect: smaller P is, short sighted greedy planning
be. Conversely, P close 1, planner spend time planning future
might better spent planning here-and-now. set depending
reward discounting factor , mode planner. use P = 0.95,
replanning probability 10%.
Example Proximities 3Doors problem shown Figure 4 initial situation (agent h0, 0i, doors closed) possible situation later execution
(agent h4, 2i, doors closed). Larger symbols correspond higher proximity. One
immediately see agents planned path goal, large symbols correspond
states agent expects visit. Conversely small proximities show locations
agents planned path goal. example, agent expect visit
states south-western room, especially already passed door
1. Similarly, proximities around initial state much lower agent
h4, 2i, expect need return.
6.3 Discussion
One interesting feature resulting numbers emphasise absorbing nearabsorbing states somewhat might intuitively expected. However, considering
496

fiProximity-Based Non-uniform Abstractions Planning

x=0

1

2

3

4

5

6

7

8

9

x=0

y=0

y=0

1

1

2

2

3

3

4

4

5

5

6

6

7

7

8

8

9

9

hx = 0, = 0i

1

2

3

4

5

6

7

8

9

hx = 4, = 2i

Figure 4: Proximities 3Doors problem s0 possible later scur ; symbol size
logarithmic, proximities range 237 21.4 ; P = 0.95, replanning probability 10%.

absorbing states general important, good feature, especially since
normally planner try minimise probability entering absorbing state
(unless goal). feature help ensure absorbing states kept
mind long chance falling them. Dean et al. (1995), instance,
note algorithm undesirable absorbing states along path goal
tend come candidates removal consideration (due low probability
reaching current policy), make special accommodation
removed consideration. proximity measure emphasising
states, special handling necessary.
contrast approach, Kirman (1994) uses probabilities Es steps,
Es (an estimate of) number steps agent take switching
previous policy policy currently calculated. assumes Es
estimated well, current policy policy executive, oneplanning-cycle probability appropriate measure. fact one would prefer least
two-planning-cycle look-ahead, agent begins within area focus
new policy, remains throughout validity policy, probably
longer, since planners foresight extend beyond next thinking cycle.
philosophically, reliance planning cycle length desirable,
artefact planner rather intrinsic domain.
somewhat related approach prioritised sweeping (see instance Barto, Bradtke,
& Singh, 1995). present approach, defines measure states
way interesting. Unlike approach, applies measure determine
497

fiBaum, Nicholson & Dix

order formulae V calculation applied,
applied preferentially interesting states less frequently uninteresting
unimportant states. well-known order calculation MDP planning
algorithms varied greatly without forfeiting convergence optimal policy,
prioritised sweeping takes advantage this. Often done measure change
V previous calculations, approaches use look-ahead current state,
ways simple version proximity (in fact, corresponds
threshold P replanning probability set 1). proximity measure P might well
good candidate approach: apply V calculation states chosen directly
according P distribution.8
Munos Moore (1999) use influence measure deterministic continuous
domains, similar P. fact, main difference measure
two parameters re-uses replanning probability
(effectively zero). means cannot take account replanning, neither
difference horizon entails, possibility policy may change
acted upon. Absorbing states, instance, would emphasised
proximities.

7. Proximity-Based Dynamic Abstraction
proximity measure described previous section used focus planners attention areas likely useful near future. Firstly, means worldview
made match proximities, refining coarsening appropriate. Secondly, since proximity measure takes account current state, method
automatically update worldview agents circumstances change recurrent
mode, is, planning execution concurrent.
7.1 Refinement
High proximity indicates states agent likely visit near future.
planner therefore plan states carefully. abstract, reason
refine allow detailed planning. states high proximity
therefore considered candidates refinement.
High proximity defined simple threshold, shown Algorithm 6.
refinement occurs, anomaly sometimes appears. anomaly led
policy-based refinement method, arises different levels abstraction, here,
adjacent abstract state causes problem, rather recentlyrefined one. state refined, values V new states initially estimated
states previous value V . However, typically, means
overestimated others underestimated. policy re-calculated,
state overestimated value attractive.
Since problem directly follows moment refinement, self-correcting.
iterations, planner converges correct policy values. However,
8. retaining theoretical guarantee convergence desired, care would taken since

P zero states reachable current state. practice, course, optimality
otherwise unreachable states immaterial.

498

fiProximity-Based Non-uniform Abstractions Planning

Algorithm 6 proximity-based refinement
stochastically choose dimension
worldview states w
P(w) > threshold w abstract
/* replace w
group states concrete */
anew
w
concrete
new
w
:

dimension wnew = dimension w 6=
W W {wnew }
new
(wnew ) (w); V (wnew ) V (w); P(wnew ) |w|w| | P(w)
W W \ {w} /* discarding stored (w), V (w) P(w) */

so, transient anomalies appear policy, worst case,
planner may replan path, refine states re-trigger
anomaly. Rather large parts state space spuriously refined way.
occurs combined V calculation phase, may update
V chance converge. solution create variant phase, V
calculation only, replaces V calculation phase values stabilise.
two iterations, appears sufficient. alternative solution would
copy difference values adjacent, concrete states
possible, thus obtaining better estimated values newly-refined states. However, since
simpler solution V -only calculation works satisfactorily, complex possibility
explored.
7.2 Coarsening
Low proximity indicates states agent unlikely visit near future.
planner therefore need plan states carefully. Usually, already
abstract, never refined first place. However, concrete
previously refined reason coarsen free memory
CPU time detailed planning elsewhere. states low proximity
therefore considered candidates coarsening.
Proximity-based coarsening useful primarily on-line planning scenario recurrent planning. agent moves state space current state
scur changes, states likely visited near future. especially useful agent finds unexpected part state space, instance
due low-probability outcomes, agent planned path leading part way
goal (perhaps partial reward). case, however, parts state
space already traversed coarsened favour refinement front agent.9
One might imagine planning progresses, planner may wish concentrate
different parts state space coarsening might useful cull abandoned
explorations switch focus. However, observed domains
9. States already traversed cannot discarded, even agent never visit again, since
worldview partition since agent necessarily know whether need revisit (or
end revisiting) states.

499

fiBaum, Nicholson & Dix

found pre-cursor mode, coarsening generally worsens quality policies
positive contribution.
Coarsening proceeds three steps, shown Algorithm 7. first step
similar proximity-based refinement: time proximity-based coarsening phase
invoked, worldview scanned states low proximity (below threshold),
put list candidates. second step tricky. Coarsening needs join
several states one. However, representation allow arbitrary partitions
worldviews therefore allow coarsening-together arbitrary set
worldview states. planner must therefore find group states among lowproximity candidates coarsened valid worldview state. groups
detected fact differ one dimension size
dimension, therefore covering completely. Finally, groups found
replaced single abstract state.
Algorithm 7 proximity-based coarsening
/* collect candidates coarsening */
candidates {w : P < threshold}
/* find groups candidates coarsened together */
/* partition candidates according pattern abstract concrete dimensions */
patterns candidates / {(wa , wb ) : . wa concrete wb concrete d}
groups
p patterns
dimensions
states p concrete
/* partition p dimensions except d, giving potential groups */
potgroups p / {(wa , wb ) : 6= . dimension wa = dimension wb }
/* add potential groups size dimension groups */
groups groups {g potgroups : |g| = |Sd |}
/* replace group states single, abstract state */
g groups
g W
stochastically choose
wa g
new
w
abstract
construct wnew :
dimension wnew = dimension wa 6=
W W {wnew }
P
1 P
new )


(wnew ) (wa ); V (wnew ) |g|
wg V (w); P(w
wg P(w)

W W \ g /* discarding stored (w), V (w) P(w) w g */

cases, may impossible coarsen section worldview despite low
proximity, due situation somewhat akin grid-lock. Probably simplest example
one Figure 5, shows worldview five states three-dimensional binary
specific state space, three states ignoring different dimension each,
remaining two take account three. situation, group states
500

fiProximity-Based Non-uniform Abstractions Planning

S1

0

0

0

0

1

1

1

1

S2

0

0

1

1

1

1

0

0

S3

0

1

1

0

0

1

1

0

w1

w2

w3

w4

w5

Figure 5: non-uniform worldview cannot immediately coarsened. state space
three binary dimensions (eight states). worldview two concrete states,
w1 w4 , three abstract states, w2 , w3 w5 , abstract different
dimension.

coarsened single dimension. coarsening possible, one states must
first refined, low P candidates proximity-based
refinement. this, integration uniform abstraction method coarsening
would straightforward selecting initial worldview refinement,
unless worldview kept uniform. However, even non-uniform worldview would
difficult. instance, dimension could simply removed possible
rather everywhere.

8. Results
run algorithm range different domains demonstrate approach.
domains divide two broad groups. first group consists grid navigation
domain only. domain intuition gathered preliminary runs
done, however, problems domain show well approach performs,
cannot show generality. second group consists domains literature,
demonstrating well approach generalises.
8.1 Experimental Domains
introduce domains section. first five problems grid navigation
domain, two already described Section 2.1, shown Figure 1, three additional
problems. remaining domains based domains literature, particular
used Kim (2001) Barry (2009).
described Section 2.1, problems grid navigation domain shown Figure 1
x dimensions size 10, three door dimensions (binary: open/closed)
damage dimension (also binary). far 3Doors problem, 3Keys
problem, keys agent must pick keys open corresponding doors.
three remaining problems 1Key, shuttlebot 1010. 1Key problem
similar 3Keys, except agent capable holding one key time,
instead three binary dimensions keys, one four-valued dimension
indicating key agent holds (or none). shuttlebot problem introduces
cyclic goal (with extra loaded dimension damage dimension tri-valued)
501

fiBaum, Nicholson & Dix

(a) grid navigation domain
keys world
Problem
3Doors
0
1Key
3
3
3Keys
0
shuttlebot
1010
0

keys held time

1
1, 2 3



note

cyclic
tiled

dimensions
6
7
9
7
8

|S|
1 600
6 400
12 800
4 800
160 000

(b) robot4 -k domain
Problem
robot4 -10
robot4 -15
robot4 -20
robot4 -25

dimensions
11
16
21
26

|S|
10 240
491 520
20 971 520
838 860 800

(c) factory domain
Problem
s-factory
s-factory1
s-factory3
(d) tireworld domain
locations
Problem
tire-small
5
8
tire-medium
tire-large
19
19
tire-large-n0

initial
n1
n0
n12
n0

dimensions
17
21
25

goal
n4
n3
n3
n3

|S|
131 072
2 097 152
33 554 432

route length
3
4
1
3

dimensions
12
18
40
40

|S|
4 096
262 144
1 099 511 627 776
1 099 511 627 776

Table 3: Experimental domains problems, dimensionality state
space size.

1010 variant increases size problem tiling grid 10
direction (by two extra dimensions, xx yy, size 10). Table 3(a) summarises
problems domain.
next two domains based Kim (2001). Firstly, robot4 -k domain,
based Kims (2001) ROBOT-k domain reducing number actions
four. robot4 -k domain problems consist cycle k rooms shown Figure 6,
room light, analogous doors 3Doors problem
enable agent move. four actions variant go forward, turn light
current room off, nothing. original formulation allowed agent
combination toggling lights going forward, total 2k+1 actions,
reduced approach intended approximate action space.
goal move first room last. k + 1 dimensions state space
k2k states, listed Table 3(b).
502

fiProximity-Based Non-uniform Abstractions Planning

4
k-1

3

0

2
1

Figure 6: robot4 -k domain.
drill B
part B:

shape B

drilled

polish B

polish B

shaped

dip B
polished

spray B
handpaint B

painted
glue

connected

bolt
drill
part A:

shape

shaped

drilled
polish

polish

dip
polished

spray

painted

handpaint

Figure 7: factory domain.
Kims (2001) factory domain10 series variants simple manufacturing problem, represented purely predicates (that is, dimensions size 2). agent make
product two parts must drilled, painted finally joined together.
Figure 7 shows simplified diagram, omitting interactions options
instance, achieve painted predicate, agent may spray, dip handpaint
object; connect two objects, may use glue bolt (and latter requires
drilled); on. Unlike domains, partial rewards available
agent achieving certain subgoals. problems used listed Table 3(c).
final domain tireworld domain 2006 ICAPS IPC competition
(Littman, Weissman, & Bonet, 2006) used Barry (2009). domain, robotic
car trying drive point point B. car room carry one spare tire
locations additional spare tires them. locations, car
carrying spare, pick one up. n locations car,
2n + 2 binary dimensions problem, follows: n dimensions used represent
location car. valid states states one location dimension
true, explicitly stated anywhere domain.11 Another n dimensions
used represent locations spare tire not. final two
dimensions represent whether car carrying spare whether flat tire.
10. domain previously used Hoey, St.-Aubin, Hu, Boutilier (1999) based
builder domain Dearden Boutilier (1997) adapted standard job-shop scheduling
problems used test partial-order planners.
11. touch aspect discussion Section 9, case include domain without
change order facilitate comparison literature.

503

fiBaum, Nicholson & Dix

n0

n15
n10
n8

n4

n0
n12

n17

n6

n3

n9

n8
n18
n2

n6

n14

n4

n10

n16
n2

n1

n1
n1

n13

n3

n12
n3
n7

n0

tire-small

n7

n11

tire-medium

n5

tire-large

goal ( )
locations.
Figure 8: tireworld domain problems, indicating initial ( )

Barry (2009) uses two tireworld problems, labelled small large.
small tireworld problem 5 locations 12 variables 14 actions, large
one 19 locations 40 variables 100 actions. Curiously, large problem,
direct road initial goal locations, takes single action
solve problem. makes difficult assess whether Barrys method has, fact,
scaled up. addition two, created medium-sized tireworld, 8 locations
18 variables, removing locations large tireworld moving initial
location n0, goal. variants listed Table 3(d) shown
Figure 8. final variant, tire-large-n0, shown, identical large tireworld
except initial location moved n0.
8.2 Direct Evaluation Policies Pre-cursor Deliberation
smaller problems 3Doors, 1Key, 3Keys, directly evaluate
approximate policies produced planner running pre-cursor deliberation.
problems small enough use exact algorithm calculate actual value
function corresponding approximate policies. noted Section 2.6,
useful involves fewer potentially confounding variables, exploit full
potential approach.12
Table 4(a) shows results policy-based refinement is,
proximity-based methods (Algorithms 5, 6 7) disabled. problem, table
lists size problem |S| value optimal solution initial state
12. Proximity-based coarsening (Algorithm 7) primarily aimed regions state space agent
already traversed, pre-cursor deliberation traversal. Coarsening would therefore
expected bring limited benefit pre-cursor deliberation direct evaluation would
meaningful evaluate performance. therefore evaluated recurrent deliberation.

504

fiProximity-Based Non-uniform Abstractions Planning

olu
tio
nv
alu
wo
e
rld
vie
w
siz
e
rel
ati

wo
rld
vie
w
siz
pla
e
nn
er

esti
sol
uti te

val
ue
act
ua
ls
olu
tio
nv
alu
e

ize

op
tim
al


sta
te
sp
ce


dis

cou
nti
ng

fac
tor

V (s0 ), representing costs results exact planning. followed
size worldview |W| absolute number percentage |S|, planners
estimate value solution initial state V (s0 ), actual value
solution initial state V (s0 ). first half part table discounting
factor = 0.999 99, second half = 0.95. Averages 10 runs
shown planner run 1000 . 1000 chosen approximation
assumed 1 000 phases sufficient planner converge. practice,
convergence generally took place much earlier. detected, however,
overall assumption planner continues plan forever, responding changing
inputs, makes convergence somewhat irrelevant.


problem
|S|
V (s0 )
|W|
(a) policy-based refinement
226.4
0.999 99 3Doors
1 600 27.50
326.8
1Key
6 400 79.47
3Keys
12 800 61.98
262.0
222.6
0.95
3Doors
1 600 14.63
1Key
6 400 19.59
296.8
245.7
3Keys
12 800 18.99
(b) proximity-based refinement
0.999 99 3Doors
1 600 27.50 1 381.7
1Key
6 400 79.47 4 166.7
3Keys
12 800 61.98 4 262.8
0.95
3Doors
1 600 14.63 1 363.2
1Key
6 400 19.59 2 828.2
3Keys
12 800 18.99 4 230.6
(c) policy- proximity-based refinement
0.999 99 3Doors
1 600 27.50 1 361.7
1Key
6 400 79.47 4 635.5
3Keys
12 800 61.98 5 948.2
0.95
3Doors
1 600 14.63 1 359.5
1Key
6 400 19.59 4 036.1
3Keys
12 800 18.99 3 748.8

|W|
|S|

V (s0 )

V (s0 )

14%
5.1%
2.0%
14%
5.6%
1.9%

22.50
37 512.20
25 015.60
13.22
15.23
14.56

27.50
100 000.00
100 000.00
14.63
20.00
20.00

86%
65%
33%
85%
44%
33%

27.50
60 031.79
70 018.59
14.63
19.92
19.70

27.50
60 031.79
70 018.59
14.63
19.92
19.70

85%
72%
46%
85%
63%
29%

27.50
20 063.57
30 043.39
14.63
19.67
20.00

27.50
20 063.57
30 043.39
14.63
19.67
20.00

Table 4: Results direct evaluation policies pre-cursor deliberation three
different refinement methods, evaluated 1 000 phases.

505

fiBaum, Nicholson & Dix

results part (a) table divide neatly two types: without keys (3Doors
problem), planner succeeds ten runs, getting perfect policies given starting
state. two problems, 1Key 3Keys, planning invariably fails. two
problems, agent must pick key far door opens, version
planner simply cannot think ahead extent. three these, planner
somewhat optimistic, estimating better value obtains cases even
better optimum. instance, 3Doors problem = 0.999 99, planners
estimate value V (s0 ) 22.50, better true value
optimum, V (s0 ) = V (s0 ) = 27.50. fractional |W| table due
averaged ten runs. final size worldview sometimes depends extent
order dimensions states refined, order randomised
runs. instance, 3Doors problem = 0.999 99, W various
sizes ranging 220 231 states end ten runs, average
226.4.
results part (a) similar two values . main difference
smaller leads smaller numbers. instance, value indicating failure 100 000
1
,
= 0.999 99 20 = 0.95. values tend multiples 1
1
smaller value here, 1 20 rather 100 000. cases,
smaller range make differences less obvious: instance, estimated value
1
column (V (s0 )), clear whether numbers approximations 1 1
1
(and failure reach goal) 0 1
minus small number (representing success).
1
represent rewards costs
units represent once-off rewards costs, multiples 1
obtained perpetuity. However, expected desired behaviour. smaller
represents disinterest distant future, reward cost perpetuity
much important once-off reward cost.
Table 4(b) shows results ten runs pre-cursor mode 1000 proximitybased refinement (no policy-based refinement coarsening) problem
. seen, 3Doors problem solved optimally cases.
surprising, complex problem.
1Key 3Keys problems interesting. figures Table 4(b) arise
average 31 successful runs, values close equal optimal values
V (s0 ), 23 unsuccessful runs values 100 000. = 0.999 99,
planner found successful policy 4 10 runs 1Key problem 3 times
10 3Keys problem. Similarly = 0.95 case (2 times 3 times,
1
respectively), since optimal path quite long compared 1
, success
means reward 19.59 (or 18.99) failure punished 20, effect
difficult discern.
Table 4(c) shows results proximity-based refinement policy-based refinement combined (no coarsening). Naturally, 3Doors problem either refinement
method alone already obtained optimal policy shows improvement. worldview size |W| differs slightly Table 4(b), policy-based refinement sometimes
directed, |W| tend slightly smaller exploratory proximitybased refinement alone, larger policy-based refinement alone.
506

fiProximity-Based Non-uniform Abstractions Planning

two problems, 1Key 3Keys, show improvement compared either
refinement methods alone. solved 43 runs = 0.999 99
values 20 063.57 30 043.39 represent averages 2 3 unsuccessful runs
8 7 successful ones, compared 4 3 successful runs proximity-based
refinement successful runs policy-based refinement only. = 0.95,
1Key problem solved 8 runs, due discounting length
path, goal near horizon. Again, success meaning reward
19.59 failure receives 20, distinction great. 3Keys problem
= 0.95 find solution parameters, receives uniform
20 runs, suboptimality one unit.
behaviour runs generally quite straightforward. Typically,
initially calculating agent cannot reach goal initial worldview,
worldview size gradually increases, plateaus coarsening here, movement agent, behaviour really possible. successful runs,
planner plans route goal point increase, worldview becomes sufficient, V (s0 ) quickly reaches final value. Rarely, V (s0 ) may oscillate
twice first. omit graphs here, presented Baum (2006).
8.3 Evaluation Simulation Recurrent Deliberation
larger problems, performance evaluated simulation, running agent
simulated world observing reward collects. problems, direct
evaluation possible calculating actual value function using exact
algorithm longer tractable. Simulation recurrent delibertion context
coarsening evaluated. comparison, section presents results
3Keys problem evaluated simulation, without coarsening.
Figure 9 shows representative sample results simulation 3Keys problem
refinement methods coarsening, combination options
shown Table 4(c) previous section, evaluated simulation rather directly.
small graph shows different individual run. seen, agent behaves
reasonably working recurrent planning mode simulation.
left vertical axes graphs represent reward R, plotted thick red lines.
run 1 Figure 9, instance, agent starts receiving reward 1
step, meaning goal, damage domain. 180 onwards, receives
reward 0 per step, meaning goal, damage. right vertical axes
worldview size |W|, thin blue lines. shown details throughout section,
is, scaled actual worldview sizes rather full 1|S| ranges. Taking run 1
Figure 9 again, see |W| grows relatively quickly 80, continues
grow slowly eventually levels little 5 000. full state space,
comparison, 12 800. run 2, agent received reward similarly, state space
grew longer, eventually levelling somewhat 7000. runs 3 4, agent
failed reach goal continued receiving reward 1 throughout. run 3,
worldview size levelled little 3000, run 4 steadily grew 5000.
horizontal axes simulated world time, corresponding discrete time-steps
MDP. two time-scales simulation: wall clock time, indicating
507

fiBaum, Nicholson & Dix

1:

|W|

R

2:

7000
6000

0

7000
6000

0

5000

5000

4000

4000

3000

3000

2000

-1

|W|

R

2000

-1

1000
0

50

100

150

200

1000

0
250

0

|W|

R

50

100

time

3:

150

200

time

R

4:

7000
6000

0

|W|

7000
6000

0

5000

5000

4000

4000

3000

3000

2000

-1

2000

-1

1000
0

50

100

0
250

150

200

1000

0
250

0

time

50

100

150

200

0
250

time

Figure 9: Simulation results, 3Keys problem, policy-based proximity-based refinement,
coarsening (four runs). Reward (left axes, thick red lines) worldview size
|W| (right axes, thin blue lines; detail) world time (horizontal axes).

passage real time, number phases planner performed. simulation configured take 1 time step per 10s wall clock time. number phases
R
controlled simply given speed (1.5GHz Intel
) CPU implementation coded flexibility rather efficiency. Ideally, agent gradually
move general direction goal planning, simplifies problem,
fast agent runs far ahead abstraction planner.
planner algorithm terminate, since planner assumed keep planning
(and agent keep acting) indefinitely. goal-oriented domains,
examples paper, one might consider achieving goal termination
condition, (a) example domains assume agent continue
goal goal maintenance, albeit trivial, (b) apply non-goal-oriented
domains (c) even goal-oriented domains clear apply condition
case agent fails reach goal. simulation, therefore, runs either
terminated manually, succeeded appeared progress
508

fiProximity-Based Non-uniform Abstractions Planning

1:

|W|

R

0

-1

0

2:

9000
8000
7000
6000
5000
4000
3000
2000
1000
0
50 100 150 200 250 300 350 400

0

-1

0

time

|W|

R

9000
8000
7000
6000
5000
4000
3000
2000
1000
0
50 100 150 200 250 300 350 400
time

Figure 10: Simulation results illustrating effect coarsening worldview size, 3Keys
problem, policy-based refinement, proximity-based refinement coarsening
(two runs).

likely made, run fixed number world time steps, selected based
manually-terminated runs allowance variation.
coarsening Figure 9, worldview sizes monotonic increasing.
Different runs refined differently domain algorithm stochastic.
beginning planning, agent receiving reward 1 per step,
yet goal. worldview size increases, planner eventually finds policy
leads goal runs 1 2, seen better reward 0 obtained
runs. simple relationship worldview size performance: runs
worked worldview 5 000 larger generally succeeded, smaller
worldviews generally not. vast majority runs (> 90%), agent reached
goal.
coarsening (Algorithm 7) activated, compared situation turned
off, reward gathered agent declines slightly, still reaches goal
vast majority runs (> 90%). Figure 10 shows two runs, one successful one
unsuccessful, 3Keys problem proximity-based coarsening well two
refinement methods, contrast Figure 9 two refinement methods
used. Note effect interleaving refinement coarsening: worldview
size |W| (thin blue line) longer monotonic, instead alternately increased
decreased, shows jagged line graph. Slightly fewer runs reach
goal. decline solution quality expected, however, since goal coarsening
reduce size worldview.
completeness, tested agent proximity-based methods
(Algorithms 6 7) active policy-based refinement (Algorithm 4) deactivated.
configuration, agent collects reward generally takes steps toward
goal, without directed policy-based refinement, largely exploratory
proximity-based methods discover keys consequently cannot reach goal.
509

fiBaum, Nicholson & Dix

8.4 Effect Discounting Factor
shuttlebot problem similar 3Doors problem requires agent move
back forth two locations repeatedly. interesting preliminary
runs pre-cursor mode solved = 0.999 99 solved optimally
= 0.95. considered whether = 0.999 99 case might behave better
simulation, agent took advantage possibility planning nearest
reward replanning reward obtained. all, agent could function
well even none policies good solution itself. However, illustrated
Figure 11, agents behaviour similar pre-cursor case: = 0.999 99,
(a) refinement only, run 1, (b) coarsening, run 2, would pick reward
immediately adjacent s0 , that. Again, setting planners discounting
factor 0.95, (c) coarsening, runs 3 4, provided much better performance.13
Note effect balance refinement coarsening (b) (c):
worldview size |W| nice steady throughout runs (though admittedly fair
fraction |S| = 4 800).
8.5 Initial Worldview
problems, standard initial worldviews large planner. Even
modified, smaller initial worldviews obtained enabling nexus step Algorithm 3
disabling reward step large. Disabling reward nexus steps
results singleton initial worldview, W = {S}, treats entire state space
single (very) abstract worldview state. Unfortunately, means planner
starts little way hints direction refine and, least
initially, information base crucial decision. upshot
collects reward. cases remains initial state s0 , others moves around
state space sometimes distance, times small loop
reach goal subgoals.14
situation factory domain problems Kim (2001), fact
agent collected reward simulated runs, even though quite runs
substantial actions taken. similar result occurs 10x10 problem (in grid
navigation domain). reward obtained agent problem,
standard initial worldview somewhat large planner and, again, singleton
initial worldview badly. best, runs, agent took limited steps
general direction goal.
interesting case tireworld domain. Again, tire-large large
standard initial worldview fails obtain solution reward step initial
worldview only. However, manually-chosen initial worldview refines locations
along path start state goal planning begins, planner solves
tire-large, tire-large-n0 40% runs (in one less
one minute, although atypical).
13. rewards appear two horizontal lines runs 3 4, one solid one broken, task
cyclic, agent collects reward 1 twice cycle reward 0 steps.
14. details unsuccessful runs, including |W| behaviour, given Baum (2006).

510

fiProximity-Based Non-uniform Abstractions Planning

(a) = 0.999 99, coarsening (one run)
R
|W|
1:
4500

(b) = 0.999 99, coarsening (one run)
R
|W|
2:
4500

4000

4000

3500

1

3500

1

3000

3000

2500

2500

2000

2000

1500
0

1500
0

1000

1000

500
0

1000

2000

3000

500

0
4000

0

200

time

400

600

800

(c) = 0.95, coarsening (two runs)
R
|W|
3:
4500

4:

|W|

R

4000

3500

1

3000

3000

2500

2500

2000

2000

1500
0

1500
0

1000

1000

500
1000

2000

3000

4500
4000

3500

1

0

0
1000

time

500

0
4000

0

time

1000

2000

3000

4000

0
5000

time

Figure 11: Simulation results illustrating effect discounting factor, shuttlebot
problem, policy-based proximity-based refinement.

8.6 Worldview Size Quality
Finally, consider effect worldview size quality robot4 domain,
agent moves series rooms lights. domain excellent example
simulated agent works well. runs robot4 -10 robot4 -15 problems
agent thought small amount time, quickly moved goal stayed
there, small worldviews, seen Figure 12 robot4 -15
problem. Four representative runs shown, two two refinement methods
(runs 1 2) two three methods (runs 3 4). four runs, worldview
sizes |W| reasonable consider full state space contains almost half million
states, 1 000-state worldview represents fifth percent. Despite small
worldview size, however, planner effective. dozen phases, agent
reached goal. planner works well robot4 -10 robot4 -15.
comparison, Kims (2001) largest ROBOT-k problem ROBOT-16, though since
ROBOT-16 216+1 = 131 072 actions robot4 -k domain problems 4,
511

fiBaum, Nicholson & Dix

(a) coarsening (two runs)
R
1:

|W|

2:

1200

|W|

R

1000

1200
1000

1

1
800

800

600

600

400

400

0

0
200

200

0
0 10 20 30 40 50 60 70 80 90 100

0
0 10 20 30 40 50 60 70 80 90 100

time

time

(b) proximity-based coarsening (two runs)
R
|W|
3:
4:
1200

|W|

R

1000

1200
1000

1

1
800

800

600

600

400

400

0

0
200

200

0
0 10 20 30 40 50 60 70 80 90 100

0
0 10 20 30 40 50 60 70 80 90 100

time

time

Figure 12: Simulation results, robot4 -15 problem, = 0.999 99, policy-based proximity-based refinement, without proximity-based coarsening.

direct comparison would valid. hand, values k (10, 15
on) necessarily powers 2, since, unlike Kim, domain specification
always considers room numbers atomic rather binary numbers, particular
advantage powers 2.
results robot4 -20 problem beginning interesting
robot4 -10 robot4 -15. Figure 13(a), showing two runs coarsening
(runs 1 2), agent succeeds reasonably promptly reasonable worldview
sizes. However, illustrated Figure 13(b), coarsening active planner fails
reach goal runs (about 40%, example, run 3) succeeds others
(about 60%, example, run 4). state space contains almost 21 million states,
successful worldviews Figure 13 order 0.01% full state space size.
Figure 14 shows four representative runs robot4 -25 problem, (a) two without coarsening (runs 1 2) (b) two three methods (runs 3 4). problem state space 25 225 839 million states, effect noted robot4 -20
512

fiProximity-Based Non-uniform Abstractions Planning

(a) coarsening (two runs)
R
1:

|W|

2:

8000

|W|

R

7000
1

7000

6000

0

1

6000

5000

5000

4000

4000

3000

3000

2000

0

2000

1000
0

50

100

150

200

1000

0
250

0

50

100

time

150

200

|W|

R

7000
1

0

1

6000

5000

5000

4000

4000

3000

3000

2000

0

2000

1000
100

150

200

8000
7000

6000

50

0
250

time

(b) proximity-based coarsening (two runs)
R
|W|
4:
3:
8000

0

8000

1000

0
250

0

time

50

100

150

200

0
250

time

Figure 13: Simulation results, robot4 -20 problem, = 0.999 99, policy-based proximity-based refinement, without proximity-based coarsening.

much pronounced here: without coarsening, planner tends much larger worldviews15 large worldviews cause planner run slowly. noted Section 8.3
above, horizontal axes world time, planning time. relation two
varies quite significantly runs two policies per time step
smaller worldviews less one ten time steps worldviews grew
large.
far reaching goal concerned, two cases similar. Again, successful
runs maintain reasonably-sized worldview, runs 2 4. Runs
worldview size grows big invariably fail (runs 1 3). difference
time, smallest successful worldview, run 4 Figure 14, used around 2 000 well-chosen
worldview states, 0.000 24% full state space. worldview grows
beyond miniscule fraction state space even 19 243-state worldview
15. Note run 1 plotted different scale worldview size |W| axis compared runs 2, 3
4. makes details behaviour easier see, makes size run 1 less obvious.

513

fiBaum, Nicholson & Dix

(a) coarsening (two runs)
R
1:

0

0

|W|

2:

20000
18000
16000
14000
12000
10000
8000
6000
4000
2000
0
50 100 150 200 250 300 350 400

|W|

R

8000
7000

1

6000
5000
4000
3000

0

2000
1000
0

time

0
50 100 150 200 250 300 350 400
time

(b) proximity-based coarsening (two runs)
R
|W|
4:
3:

|W|

R

8000

8000

7000
1

7000
1

6000

0

6000

5000

5000

4000

4000

3000

3000
0

2000

2000

1000
0

250

500

750

1000

0
1000

0

time

0
20 40 60 80 100 120 140 160
time

Figure 14: Simulation results, robot4 -25 problem, = 0.999 99, policy-based proximity-based refinement, without proximity-based coarsening. Note
different scales worldview size |W| axis run (a)1.

run 1 Figure 14 0.002 3% planner stall progress
possible. Even challenging environment, agent reaches goal almost half
runs.

9. Discussion
Section 8 presented results across range experimental domains showing method
successfully finds solutions planning problem using much less full state space,
well limitations. section discuss results analyse
features domains method exploit give difficulty.
smaller problems, could directly evaluate policies produced method
pre-cursor mode, allowing us better isolate behaviour planner. Without
proximity-based methods, worldviews quite small planner could solve
514

fiProximity-Based Non-uniform Abstractions Planning

3Doors problem. However, even better uniform abstraction, could
little here. Even oracle could best remove one door key 3Keys
two doors 3Doors, giving 25% relative worldview size however, planner
used uniform distribution removed dimension, agent would fail anyway, since
opening doors left worldview would harder hoping best
assumed-50%-open door actually closed. succeed, would
deduce abstracted doors considered closed, considerable feat.
function sample domains. circumstances, uniform abstraction could
effective, either pre-processing step approach integrated W selection
methods.
expected turning proximity-based refinement (Algorithms 5 6) would
lead larger worldviews. general, one would expect larger worldviews yield better
solutions smaller worldviews yield worse solutions, lower computational cost.
means proximity-based refinement should, general, improve solution quality.
results corresponded expectation. worldviews indeed larger
solution quality higher.
larger problems, performance could evaluated simulation, running
agent simulated world observing reward collects. problems,
direct evaluation possible calculating actual value function using
exact algorithm longer tractable. Section 8.3 therefore presented results
3Keys problem comparison obtained direct evaluation policies
pre-cursor deliberation discussed above. seen, results correspond, crossconfirming evaluation methods.
addition, simulation recurrent delibertion context coarsening
could evaluated. proximity-based coarsening activated, compared
situation turned off, reward gathered agent declines slightly.
impression worse performance somewhat misleading. due fact first
comparison takes place one small problems, planner able solve
without coarsening, fact without abstraction all. Thus, disadvantages
(lower reward collected) much apparent advantages (lower computational
cost).
larger problems, working without abstractions option, balance
reversed. fact, somewhat counterintuitively, larger problems
coarsening active, successful runs smaller worldviews unsuccessful
runs. Clearly, size worldview determines success, quality.
good worldview enabled efficient calculation policies progress toward goal
remaining small. poor worldview simply grew larger. smaller problems, growing
worldview may eventually covered state space detail, thus masking
effect. planner would find good policy effectively without real approximation.
larger problems, finding good policy without approximation feasible,
similarly growing worldview simply slowed planner progress
made. situation, worldview-reducing action proximity-based coarsening
became crucial, ensuring least worldview remained tractably small
thereby enabled planner deal problem.
515

fiBaum, Nicholson & Dix

seen results, coarsening successful task
time. agent paused replan part-way goal, reduces size
worldview keep relevant changing circumstances. runs, however,
worldview size grew beyond capabilities planner. cases,
10x10 problem, settled higher balance. others, appears simply
continued growing. latter case, would appear simple question
tuning parameters: find balance, appropriate worldview size.
appears factor, quality worldview determining
success failure runs whether find balance reach goal grow
big fail.
number problems solved poorly due initial abstraction
selection algorithm (Algorithm 3). problems, algorithm produced either large
worldview exceeded available memory (either immediately shortly afterwards),
planning possible, small worldview planning ineffective.
could set produce medium-sized worldview, none four combinations
options produced one. problems, singleton worldview possible
is, steps initial abstraction selection disabled, resulting worldview aggregating
states single, maximally abstract worldview state leading typically reward
collected agent. best, would take actions general direction
goal(s). considerably worse previous work. instance, Kim (2001)
obtains approximate solutions problems larger variants, others
use domain variant, including Hoey et al. (1999) originators, Dearden
Boutilier (1997).
necessity using singleton initial worldview understandably greatly
hurt performance. infelicity worldview initialisation stage could impair
entire planning process, since worldview never completely discarded planner.
worldview-improvement algorithms could made amount weakness
initial worldview, singleton worldview poor starting point indeed.
similar observation made Dean et al. (1995) work using reduced envelope
states (that is, subset state space). high level algorithms, here,
work regardless initial envelope (worldview). practice, however, better
initial envelope chosen intelligence, instance contain least
possible path goal (for goal-oriented domains). find path using simple
depth first search, directly applicable worldviews
gradations abstraction, overall concept remains: reasonable initial worldview
crucial.
tireworld results confirm this. initial worldview reward step
enabled small, since domain rewards single dimension, planning
ineffective. planner given better initial worldview one could
plausibly calculated became quite effective, even modified tire-large-n0
initial state deliberately moved goal. seems, then,
basic approach general, worldview selection modification methods
less so. good worldview selection less fundamental apect approach
easily supplemented additional methods even tuning. domains
516

fiProximity-Based Non-uniform Abstractions Planning

tireworld, seems modified predicate solver generate plausible trajectories
current state goal would well part worldview selection.
interesting compare results Sanner Boutilier (2009)
tireworld, describe passing extremely poorly approximated
going manually tweak domain planner, adding information
locations mutually exclusive. makes planning much easier largely invalidates
comparison approaches.16 fair question, however, extent
weakness planner extent artefact domain. combination
representation narrative seems rather unfortunate, narrative obvious
1-of-n intuition tend obscure real features (and real applicability)
propositional representation hinder rather help intuition. would occur
tighter fit representation narrative.
Others, course, solve tireworld domain well. Some, Barry, Kaelbling,
Lozano-Prez (2010), generate full policy, others take advantage initial state,
planner would do. difficult know extent planners adapted
domain extent flexible. seems recent years
become common planners tested domains researchers
access development, ICAPS IPC domains, rather
greater lesser degree hand-tuned, usually unconsciously, particulars one
another domain, undoubtedly unconsciously tuned grid navigation
domain.
interesting side point provided shuttlebot problem, solved
= 0.999 99 (other collecting trivial reward immediately adjacent
s0 ) solved optimally = 0.95. Since simulator
intrinsic discounting factor reports reward collected one see
even though planner working discounting factor = 0.95, provided
better solution = 0.999 99 case worked = 0.999 99 first
place.
ways better behaviour smaller planner discounting factor reasonable, agents horizon represented world discounting factor,
horizon particular policy therefore planner effectively much shorter,
policy supplanted new one relatively soon. Thus, may useful
occasion set planners discounting factor lower true world discounting factor order facilitate planning. However, may lead suboptimal, short-sighted
policies.

10. Conclusions
theory Markov decision processes provides algorithms optimal planning. However,
larger domains algorithms intractable approximate solutions necessary.
state space expressed terms dimensions, size resulting computational cost exponential number dimensions. Fortunately, results
structured state-space effective approximations possible.
16. Similarly, Kolobov, Mausam, Weld (2008) report results variant tireworld rather
tireworld itself, without providing explanation.

517

fiBaum, Nicholson & Dix

approach based selectively ignoring dimensions parts
state space order obtain approximate solutions lower computational
cost. non-uniform abstraction dynamically adjusted planning (in on-line
situations) execution progress, different dimensions may ignored different parts
state space. strong implications, since resulting approximation longer
Markovian. However, approach intuitive practical. synthesis
two existing approaches: structure-based approximation uniform abstraction
dynamic locality-based approximation envelope methods. envelope methods,
limited reliance initial worldview (or envelope): poor,
tend perform poorly overall. approach subsumes uniform abstraction completely
treated special case general method.
paper extends preliminary work Baum Nicholson (1998) modifying
worldview based proximity measure, enlarging reducing size,
evaluating behaviour simulation. allows us test approach larger
problems, importantly demonstrates full strength approach
limits terms domain features exploit exploit
adjustment all. abstraction becomes truly dynamic, reacting changes
agents current state enabling planning tailored agents situation
changes. shown qualitative quantitative results presented
Baum (2006), approach effective efficient calculating approximate
policies guide agent simulated worlds.
10.1 Future Work
One possible direction future research would find worldview initialisation
modification methods result smaller yet still useful worldviews, probably domainspecific, extend method domains, either larger different features.
example, factory tireworld domains goal-oriented based predicates,
worldview selection modification method based predicate-oriented solver could
find possible paths goal ensure relevant preconditions concrete along
path.
Interestingly, 10x10 problem, proximity-based methods keep
worldview size small, seem find balance larger stil moderate
size. Thus another possibility might tune proximity-based methods develop
self-tuning variants.
number points, instance phase selection, algorithm uses stochastic choice
default. could replaced heuristics, learning, directed methods.
One could adapt method work types MDPs, undiscounted
finite-horizon ones, combine approaches approximate different aspects domains planning problem, described Section 2.5. example,
mentioned section, Gardiol Kaelbling (2004, 2008) combine hierarchical state
space abstraction somewhat similar envelope work Dean et al. (1995).
Many combinations would likely fruitful planning domains features relevant multiple methods. Similarly, additional refinement coarsening methods
518

fiProximity-Based Non-uniform Abstractions Planning

could added, instance one based after-the-fact refinement criterion
roll-back Reyes et al. (2009).
theoretical side, one could look situations optimality
guaranteed, Hansen Zilberstein (2001) LAO* algorithm work
Dean et al. (1995), observing admissible heuristic used evaluate fringe states,
rather pragmatically chosen V (out), algorithm related heuristic search
acquires stopping criterion guaranteed optimality (or -optimality). Perhaps
similar condition could developed approach, rather different heuristic.
two basic directions work extended
fundamental way, relaxing one MDP assumptions, perfect observability knowledge
transition probabilities. Partially Observable Markov Decision Process (POMDP)
gives agent observation instead current state, observation partly
random partly determined preceding action current state.
optimal solution known principle, quite computationally expensive, since transforms POMDP larger, continuous, many-dimensional MDP agents beliefs.
such, non-uniform abstraction approach could applied two different ways: either original POMDP, fairly direct translation, transformed MDP.
extension would apply technique agent learn transition
probabilities. particular, application technique exploration17 would
interesting agent would somehow learn distinctions within single abstract states, distinguish refined remain
abstract.

References
de Alfaro, L., & Roy, P. (2007). Magnifying-lens abstraction Markov decision processes.
Proceedings 19th International Conference Computer Aided Verification,
CAV07, pp. 325338.
Barry, J., Kaelbling, L. P., & Lozano-Prez, T. (2010). Hierarchical solution large Markov
decision processes. Proceedings ICAPS Workshop Planning Scheduling
Uncertain Domains.
Barry, J. L. (2009). Fast approximate hierarchical solution MDPs. Masters thesis,
Massachusetts Institute Technology.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, Special Volume: Computational Research
Interaction Agency, 72 (12), 81138.
Baum, J. (2006). Dynamic Non-uniform Abstractions Approximate Planning Large
Structured Stochastic Domains. Ph.D. thesis, Clayton School Information Technology, Monash University. Available www.baum.com.au/jiri/baum-phd.ps.gz
17. learning problem could transformed MDP agents beliefs experiences,
would computationally prohibitive. standard approaches instead explicitly distinguish exploration, agent learns domain (but ignores goals) exploitation, achieves
goals (but ignores opportunities learn).

519

fiBaum, Nicholson & Dix

Baum, J., & Nicholson, A. E. (1998). Dynamic non-uniform abstractions approximate
planning large structured stochastic domains. Lee, H.-Y., & Motoda, H. (Eds.),
Topics Artificial Intelligence, Proceedings 5th Pacific Rim International Conference Artificial Intelligence (PRICAI-98), pp. 587598.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI
planning automatically learned macro-operators. Journal Articial Intelligence
Research, 24, 581621.
Boutilier, C. (1997). Correlated action effects decision theoretic regression. Geiger, D.,
& Shenoy, P. (Eds.), Proceedings 13th Conference Uncertainty Artificial
Intelligence (UAI-97), pp. 3037.
Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamic programming. Proceedings 13th International Conference Machine Learning,
pp. 5462.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Mellish, C. S. (Ed.), Proceedings 14th International Joint Conference
Artificial Intelligence (IJCAI-95), Vol. 2, pp. 11041111.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121 (1-2), 49107.
Boutilier, C., Goldszmidt, M., & Sabata, B. (1999). Continuous value function approximation sequential bidding policies. Laskey, K., & Prade, H. (Eds.), Proceedings
15th Conference Uncertainty Artificial Intelligence (UAI-99), pp. 8190.
Cassandra, A. R., Kaelbling, L. P., & Kurien, J. A. (1996). Acting uncertainty: Discrete Bayesian models mobile-robot navigation. Tech. rep. TR CS-96-17, Computer
Science, Brown University.
Daoui, C., Abbad, M., & Tkiouat, M. (2010). Exact decomposition approaches Markov
decision processes: survey. Advances Operations Research, 2010, 120.
Dean, T., Kaelbling, L. P., Kirman, J., & Nicholson, A. E. (1995). Planning time
constraints stochastic domains. Artificial Intelligence, 76 (1-2), 3574.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89 (1), 219283.
Dietterich, T. G. (2000). Hierarchical reinforcement learning MAXQ value function
decomposition. Journal Artificial Intelligence Research, 13, 227303.
Drummond, M., & Bresina, J. (1990). Anytime synthetic projection: Maximizing probability goal satisfaction. Dietterich, T., & Swartout, W. (Eds.), Proceedings
8th National Conference Artificial Intelligence (AAAI-90), pp. 138144.
Gardiol, N. H., & Kaelbling, L. P. (2004). Envelope-based planning relational MDPs.
Advances Neural Information Processing Systems 16 NIPS-03.
520

fiProximity-Based Non-uniform Abstractions Planning

Gardiol, N. H., & Kaelbling, L. P. (2008). Adaptive envelope MDPs relational equivalence-based planning. Tech. rep. MIT-CSAIL-TR-2008-050, Computer Science
Artificial Intelligence Laboratory, Massachusetts Institute Technology.
Goldman, R. P., Musliner, D. J., Boddy, M. S., Durfee, E. H., & Wu, J. (2007). Unrolling
complex task models MDPs. Proceedings 2007 AAAI Spring Symposium
Game Theoretic Decision Theoretic Agents.
Goldman, R. P., Musliner, D. J., Krebsbach, K. D., & Boddy, M. S. (1997). Dynamic
abstraction planning. Kuipers, B., & Webber, B. (Eds.), Proceedings 14th
National Conference Artificial Intelligence 9th Innovative Applications Artificial Intelligence Conference (AAAI/IAAI-97), pp. 680686.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
factored MDPs. Journal Artificial Intelligence Research, 19, 399468.
Hansen, E. A., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds
solutions loops. Artificial Intelligence, 129 (12), 3562.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution Markov decision processes using macro-actions. Cooper, G., & Moral,
S. (Eds.), Proceedings 14th Annual Conference Uncertainty Artificial
Intelligence (UAI-98), pp. 220229.
Hoey, J., St.-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using
decision diagrams. Proceedings 15th Annual Conference Uncertainty
Artificial Intelligence (UAI-99), pp. 279288.
Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press.
Kim, K.-E. (2001). Representations Algorithms Large Stochastic Planning Problems.
Ph.D. thesis, Deptartment Computer Science, Brown University.
Kirman, J. (1994). Predicting Real-time Planner Performance Domain Characterization.
Ph.D. thesis, Department Computer Science, Brown University.
Kolobov, A., Mausam, & Weld, D. S. (2008). Regressing deterministic plans MDP
function approximation. Workshop Reality Check Planning Scheduling
Uncertainty ICAPS.
Korf, R. (1985). Macro-operators: weak method learning. Artificial Intelligence, 26 (1),
3577.
Littman, M., Weissman, D., & Bonet, B. (2006). Tireworld domain. Fifth International
Planning Competition (IPC-5) hosted International Conference Automated
Planning Scheduling (ICAPS 2006).
Munos, R., & Moore, A. (1999). Variable resolution discretization high-accuracy solutions optimal control problems. Dean, T. (Ed.), Proceedings 16th International Joint Conference Artificial Intelligence (IJCAI-99), pp. 13481355.
Musliner, D. J., Durfee, E. H., & Shin, K. G. (1995). World modeling dynamic
construction real-time plans. Artificial Intelligence, 74, 83127.
521

fiBaum, Nicholson & Dix

Nicholson, A. E., & Kaelbling, L. P. (1994). Toward approximate planning large
stochastic domains. Proceedings AAAI Spring Symposium Decision Theoretic Planning, pp. 190196.
Parr, R. (1998). unifying framework temporal abstraction stochastic processes.
Proceedings Symposium Abstraction Reformulation Approximation
(SARA-98), pp. 95102.
Puterman, M. L., & Shin, M. C. (1978). Modified policy iteration algorithms discounted
Markov decision processes. Management Science, 24, 11271137.
Reyes, A., Sucar, L. E., & Morales, E. F. (2009). AsistO: qualitative MDP-based recommender system power plant operation. Computacion Sistemas, 13 (1), 520.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques first-order MDPs.
Artificial Intelligence, 173 (56), 748788. Advances Automated Plan Generation.
Srivastava, S., Immerman, N., & Zilberstein, S. (2009). Abstract planning unknown
object quantities properties. Proceedings Eighth Symposium Abstraction, Reformulation Approximation (SARA-09), pp. 143150.
St-Aubin, R., Hoey, J., & Boutilier, C. (2000). APRICODD: Approximate policy construction using decision diagrams. Proceedings Conference Neural Information
Processing Systems, pp. 10891095.
Steinkraus, K. A. (2005). Solving Large Stochastic Planning Problems using Multiple Dynamic Abstractions. Ph.D. thesis, Department Electrical Engineering Computer
Science, Massachusetts Institute Technology.

522


