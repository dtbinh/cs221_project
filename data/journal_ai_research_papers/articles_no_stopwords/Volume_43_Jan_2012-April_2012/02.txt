Journal Artificial Intelligence Research 43 (2012) 87-133

Submitted 07/11; published 01/12

Location-Based Reasoning Complex Multi-Agent Behavior
Adam Sadilek
Henry Kautz

SADILEK @ CS . ROCHESTER . EDU
KAUTZ @ CS . ROCHESTER . EDU

Department Computer Science, University Rochester
Rochester, NY 14627, USA

Abstract
Recent research shown surprisingly rich models human activity learned
GPS (positional) data. However, effort date concentrated modeling single individuals statistical properties groups people. Moreover, prior work focused solely modeling
actual successful executions (and failed attempted executions) activities interest.
We, contrast, take task understanding human interactions, attempted interactions,
intentions noisy sensor data fully relational multi-agent setting. use real-world
game capture flag illustrate approach well-defined domain involves many
distinct cooperative competitive joint activities. model domain using Markov logic,
statistical-relational language, learn theory jointly denoises data infers occurrences high-level activities, player capturing enemy. unified model combines
constraints imposed geometry game area, motion model players,
rules dynamics game probabilistically logically sound fashion. show
may impossible directly detect multi-agent activity due sensor noise malfunction, occurrence activity still inferred considering impact
future behaviors people involved well events could preceded it. Further,
show given model successfully performed multi-agent activities, along set
examples failed attempts activities, system automatically learns augmented
model capable recognizing success failure, well goals peoples actions
high accuracy. compare approach alternatives show unified model,
takes account relationships among individual players, relationships
among activities entire length game, although computationally costly, significantly accurate. Finally, demonstrate explicitly modeling unsuccessful attempts
boosts performance important recognition tasks.

1. Introduction
society founded interplay human relationships interactions. Since every person tightly embedded social structure, vast majority human behavior fully
understood context actions others. Thus, surprisingly, evidence shows want model behavior person, single best predictor often
behavior people social network. instance, behavioral patterns people taking taxis,
rating movies, choosing cell phone provider, sharing music best explained predicted
habits related people, rather single person attributes age, race,
education (Bell, Koren, & Volinsky, 2007; Pentland, 2008).
contrast observations, research effort activity recognition date concentrated modeling single individuals (Bui, 2003; Liao, Fox, & Kautz, 2004, 2005), statistical
properties aggregate groups individuals (Abowd, Atkeson, Hong, Long, Kooper, & Pinkerton,
1997; Horvitz, Apacible, Sarin, & Liao, 2005), combinations (Eagle & Pentland, 2006).
c
2012
AI Access Foundation. rights reserved.

fiS ADILEK & K AUTZ

Notable exceptions isolated individuals approach includes work Kamar Horvitz
(2009) Gupta, Srinivasan, Shi, Davis (2009), simple relationships among people
starting explicitly considered leveraged. instance, Eagle Pentland (2006)
elegantly model location individuals multi-modal sensory data, approach
oblivious explicit effects ones friends, relatives, etc. ones behavior. isolated individuals approximations often made sake tractability representational convenience.
considering individuals independently sufficient constrained tasks,
many interesting domains discards wealth important information results inefficient unnatural data representation. hand, decomposing domain set
entities (representing instance people, objects environment, activities) linked
various relationships (e.g., is-a, has-a, is-involved-in) natural clear way representing
data.
address shortcomings nonrelational behavior modeling, introduce capture
flag domain (described below), argue statistical-relational approach learning models
multi-agent behavior raw GPS data. CTF dataset one hand quite complex
recorded real-world sensors, time well-defined (as per rules game),
thereby allowing unambiguous evaluation results.
able recognize peoples activities reason behavior necessary precondition intelligent helpful machines aware going
human-machine well human-human relationships. many exciting practical applications activity recognition potential fundamentally change peoples lives.
example, cognitive assistants help people teams productive, provide support
(groups of) disabled individuals, efficiently summarize long complex event busy person
without leaving essential information. important applications include intelligent navigation, security (physical well digital), human-computer interaction, crowdsourcing.
applications myriad others build top multi-agent activity recognition therefore require necessary stepping stone. Furthermore, consequence anthropocentrism
technology, modeling human behavior playsperhaps surprisinglya significant role even
applications directly involve people (e.g., unmanned space probes).
Furthermore, reasoning human intentions essential element activity recognition,
since recognize person (or group people) wants do, proactively
try help (orin adversarial situationshinder them). Intent notoriously problematic
quantify (e.g., Baldwin & Baird, 2001), show capture flag domain, notion
naturally captured process learning structure failed activities. know perhaps
well successful action often precededand unfortunately sometimes followedby
multiple failed attempts. Therefore, reasoning attempts typically entails high practical utility,
relatively high frequency. Consider, example, task real-time analysis
security video system. There, detecting person group people (again, relations)
intend steal something much important useful recognizing theft taken
(or even taking) place, certainly late entirely prevent incident,
may late harder merely stop it. believe recognition attempts peoples
activities severely underrepresented topic artificial intelligence needs explored
since opens new realm interesting possibilities.
delve details approach Sections 5 6, briefly introduce
CTF dataset (Section 2), highlight main contributions work (Section 3), review
88

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

background material (Section 4). discuss related work, conclude, outline future work
Sections 7, 8 9 respectively.
paper incorporates extends previous work (Sadilek & Kautz, 2010a, 2010b).

2. Capture Flag Domain
Imagine two teamsseven players eachplaying capture flag (CTF) university campus,
player carries consumer-grade global positioning system (GPS) logs location
(plus noise) every second (see Figure 1). primary goal enter opponents flag area.
Players captured enemy territory tagged enemy. Upon
captured, must remain place freed (tagged teammate) game ends.
games involve many competitive cooperative activities, focus (both successful
attempted) capturing freeing. Visualization games available first authors
website.
collected four games CTF portion University Rochester campus (about
23 acres) Columbus V-900 GPS loggers (one per player) 1 GB memory card
set sampling rate 1 Hz. durations games ranged approximately 4 15
minutes.
work primarily motivated problem annotating strategy games, although
obvious applications results sports combat situations. are, generally, exploring relational learning inference methods recognizing multi-agent activities
location data. accept fact GPS data disposal inherently unreliable
ambiguous one individual. therefore focus methods jointly simultaneously
localize recognize high-level activities groups individuals.
Although CTF domain doesnt capture intricacies life, contains many complex, interesting, yet well-defined (multi-agent) activities. Moreover, based extensive
real-world GPS data (total 40,000+ data points). Thus problems addressing clearly direct analogs everyday-life situations ubiquitous computing needs
addressimagine people going daily lives city instead CTF players,
smart phones instead GPS loggers.
One main challenges overcome successfully model CTF
severe noise present data. Accuracy GPS data varies 1 10 meters.
open areas, readings typically 3 meters, discrepancy much higher locations
tall buildings (which present within game area) obstructions. Compare
scale error granularity activities concern with: capturing
freeing involves players within reaching distance (less 1 meter) apart. Therefore,
signal noise ratio domain daunting.
error systematic component well significant stochastic component. Errors
devices poorly correlated, subtle differences players, angle
device sits players pocket, dramatically affect accuracy. Moreover, since
consider multi-agent scenarios, errors individual players readings add up, thereby
creating large discrepancy reality recorded dataset. players
move freely open areas, cannot reduce data error assuming players move
along road walkways, done much work GPS-based activity recognition (e.g., Liao
et al., 2004). Finally, traditional techniques denoising GPS data, Kalman filtering,

89

fiS ADILEK & K AUTZ

Figure 1: snapshot game capture flag shows game area. Players
represented pins letters. version CTF, two flags stationary
shown white circles near top bottom figure. horizontal road middle image territory boundary. data shown prior
denoising corrections map errors. Videos games available
http://www.cs.rochester.edu/u/sadilek/
90

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

little help, due low data rate (1 sample per second) relative small amount time
required player completely change speed direction.
reliably recognize events happen games presence severe
noise, need consider player, relationships among
actions extended periods time (possibly whole length game). Consider concrete
task inferring individual joint activities intentions CTF players GPS
traces. example, suppose GPS data shows player running toward stationary teammate
B, moving away. occurred? Possibly player freed player B, GPS error
hidden fact player actually reached B. Another possibility player
intention freeing player B, scared opponent last second. Yet another possibility freeing occurred even intended, player B previously
captured.
Understanding game thus consists inferring complex set interactions among various
players well players intentions. conclusions drawn occurs one point
time affect affected inferences past future events. example given,
recognizing player B moving future reinforces conclusion player freeing
player B, failing recognize past event player B captured decreases confidence
conclusion. game CTF illustrates understanding situation much
recognizing attempts intentions recognizing successfully executed actions.
example, course 15 minute game, handful capture freeing events occur. However,
dozens cases one player unsuccessfully tries capture opponent free
teammate. description game restricted actually occurred would
pale reflection original.

Figure 2: Three snapshots game situation successful failed capturing occur.
example illustrates need approach exploits relational
far reaching temporal structure domain. (See text explanation.)

concrete example, consider real game situation illustrated Figure 2. see three
snapshots game projected map campus modification GPS data.
game time shown snapshot. Players D, F, G allies currently
home territory near flag, whereas players L enemies. first snapshot,
players L head opponents flag thenin second framethey intercepted
G. point unclear happening substantial error GPS data

91

fiS ADILEK & K AUTZ

three players appear close other, actuality could 20
meters apart. However, see third snapshot (note tens seconds passed)
realize player G actually captured player didnt capture L since G evidently
still chasing L. fact player remains stationary coupled fact neither F
attempt capture suggests indeed captured. show possible infer
occurrences capturing events even complex situations whereas limited approaches
largely fail. However, need able recognize individual events, need
discover new activities, identify respective goals, distinguish events based
whether outcomes favorable negative. instance, second frame, player G tries
capture L M. Although succeeded former case, failed latter.
Many different kinds cooperative competitive multi-agent activities occur games.
lowest-level joint activities based location movement, include approaching
location. Note, noise GPS data often makes difficult impossible
directly detect simple activities. next level come competitive multi-agent activities
including capturing attacking; cooperative activities include freeing; activities,
chasing guarding, may belong either category categories.
abstract tactical activities, making sacrifice, overall strategies,
playing defensively. paper, concentrate activities first two levels.

3. Contributions
main contributions paper follows. first present novel method simultaneously denoises positional data learns model multi-agent activities occur there.
subsequently evaluate model CTF dataset show achieves high accuracy
recognizing complex game events.
However, creating model manually writing new rules editing existing axioms
laborious prone introduction errors unnecessarily complex theories. Thus, would
automate process learning (or inducing) new axioms training data. people,
much easier provide validate concrete examples directly modify model.
leads us second contribution: show automatically augment preexisting model
(joint) activities capable recognizing successful actions, identifies
failed attempts types activities. line work demonstrates explicitly
modeling attempted interactions unified way improves overall model performance.
third contribution, demonstrate difference (discussed below)
newly learned definitions failed activity original definition corresponding successful activity directly corresponds goal given activity. instance, per rules
capture flag game, captured player cannot move freed. system induces
definition failed capture, new theory contain constraint movement
almost-captured player, thereby allowing move freely.

4. Background
cores models described implemented Markov logic (ML), statisticalrelational language. section, provide brief overview ML, extends finite firstorder logic (FOL) probabilistic setting. detailed (and excellent) treatment FOL,

92

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

ML, inductive logic programming see work Shoenfield (1967), Domingos, Kok, Lowd,
Poon, Richardson, Singla (2008), De Raedt Kersting (2008), respectively.
order compare Markov logic based models alternative approaches, consider
dynamic Bayesian network (DBN) model experiments one baselines.
therefore review relevant aspects DBNs section well.
4.1 Markov Logic
Given inherent uncertainty involved reasoning real-world activities observed
noisy sensor readings, looked methodology would provide elegant combination
probabilistic reasoning expressive, relatively natural, compact unfortunately strictly
true false formulas first-order logic. exactly Markov logic provides thus
allows us elegantly model complex finite relational non-i.i.d. domains. Markov logic network
(MLN) consists set constants C set pairs hFi , wi FOL formula
weight wi R associated it. Optionally, weight scaled
real-valued function subset variables appear corresponding formula. Markov
logic networks contain functions called hybrid MLNs (Wang & Domingos, 2008).
MLN viewed template Markov network (MN) follows: MN contains
one node possible ground atom MLN. value node 0 corresponding
atom false 1 otherwise. Two nodes connected edge corresponding atoms
appear formula. Thus, MN distinct clique corresponding grounding
g
formula. j denote j-th grounding formula . MN feature value fi,j
gj

(
g
1 j true
fi,j =
0 otherwise
weight wi intuitively represents relative importance satisfying (or violating,
weight negative) corresponding formula . formally, weight scales difference
log-probability world satisfies n groundings corresponding formula one
results true groundings formula, else equal (cf. Equation 1). Thus
problem satisfiability relaxed MLNs. longer search satisfying truth assignment
traditional FOL. Instead, looking truth assignment maximizes sum
weights satisfied formulas.
weights either specified knowledge base engineer or, approach,
learned training data. is, provide learning algorithm labeled capture instances pairs raw corresponding denoised trajectories along labeled instances
game events finds optimal set weights maximize likelihood training
data. Weight learning done either generative discriminative fashion. Generative training maximizes joint probability observed (evidence) well hidden (query) predicates,
whereas discriminative learning directly maximizes conditional likelihood hidden predicates given observed predicates. Since prior work demonstrated Markov network models
learned discriminatively consistently outperform generatively trained counterparts (Singla &
Domingos, 2005), focus discriminative learning activity recognition domain.
knowledge base weights specified, ask questions state
hidden atoms given state observed atoms. Let X vector random variables
(one random variable possible ground atom MN) let set possible
93

fiS ADILEK & K AUTZ

instantiations X. Then, x represents possible world. (x )[Pr(X = x) > 0]
holds, probability distribution worlds defined
!
X

1
Pr(X = x) = exp
wi ni x{i}
(1)
Z


ni (x{i} ) number true groundings i-th formula wi weight world x

!
X
X

Z=
exp
wi ni x{i}
(2)
x



Equation 1 viewed assigning score possible world dividing score
sum scores possible worlds (the constant Z) order normalize.
Maximum posteriori (MAP) inference Markov logic given state observed atoms
reduces finding truth assignment hidden atoms weighed sum satisfied
clauses maximal. Even though problem general #P-complete, achieve reasonable
run times applying Cutting Plane MAP Inference (CPI) (Riedel, 2008). CPI thought
meta solver incrementally grounds Markov logic network, step creating Markov
network subsequently solved applicable methodsuch MaxWalkSAT via
reduction integer linear program. CPI refines current solution searching additional
groundings could contribute objective function.
point, focused first-order Markov logic. first-order ML, variable
ranges objects present domain (e.g., apples, players, cars). hand, finite
second-order Markov logic, variabilize objects predicates (relations) (Kok & Domingos, 2007). CTF model contains predicate variable type activity. example, one variable captureType whose domain {capturing, failedCapturing}
analogously freeing events. grounding second-order ML, ground predicate
variables well object variables. preliminary work generalizing ML
well-defined infinite domains, would indeed give full power FOL (Singla &
Domingos, 2007).
Implementations Markov logic include Alchemy1 theBeast2 . experiments used
modified version theBeast.
4.2 Dynamic Bayesian Networks
Bayesian network (BN) directed probabilistic graphical model (Jordan, 1998). Nodes
graph represent random variables edges represent conditional dependencies (cf. Figure 4).
BN n nodes, joint probability distribution given
Pr(X1 , . . . , Xn ) =

n

i=1

1. http://alchemy.cs.washington.edu/
2. http://code.google.com/p/theBeast/

94


Pr Xi |Pa(Xi ) ,

(3)

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Pa(Xi ) denotes parents node Xi . typical setting, subset random variables
observed (we know actual values), others hidden values need
inferred.
dynamic Bayesian network (DBN) BN models sequential data. DBN composed
slicesin case slice represents one second time interval. order specify DBN,
either write learn intra- inter-slice conditional probability distributions (CPDs).
intra-slice CPDs typically constitute observation model inter-slice CPDs model
transitions hidden states. extensive treatment DBNs, see work Murphy
(2002).
number parameter learning inference techniques DBNs. match
Markov logic-based framework, experiments DBN model presented below, focus
supervised learning scenario, hidden labels known training time therefore
maximum likelihood estimate calculated directly.
find set parameters (discrete probability distributions) maximize log-likelihood
training data. achieved optimizing following objective function.

? = argmax log Pr x1:t , y1:t |) ,

(4)



x1:t y1:t represent sequence observed hidden values, respectively,
times 1 t, ? set optimal model parameters. implementation, represent
probabilities likelihoods log-counterparts avoid arithmetic underflow.
testing time, interested likely explanation observed data. is,
want calculate likely assignment states hidden nodes (i.e., Viterbi decoding
DBN) given

?
(5)
y1:t
= argmax log Pr(y1:t |x1:t ) ,
y1:t

Pr(y1:t |x1:t ) conditional probability sequence hidden states y1:t given concrete
sequence observations x1:t times 1 t. calculate Viterbi decoding efficiently
using dynamic programming (Jordan, 1998).

5. Methodology
section, describe three major components approach. short, first manually
construct model captures freeings CTF optimize parameters supervised
learning framework (Section 5.1). constitutes seed theory used denoising raw
location data recognition successful multi-agent activities. show, Section 5.2,
automatically extend seed theory inducing structure learning importance
failed captures freeings well relationships successful counterparts. Finally,
Section 5.3, use augmented theory recognize richer set multi-agent activitiesboth
successful failed attemptsand extract goals activities.
Specifically, investigate following four research questions:
Q1. reliably recognize complex multi-agent activities CTF dataset even presence severe noise?
Q2. models attempted activities automatically learned leveraging existing models
successfully performed actions?
95

fiS ADILEK & K AUTZ

Q3. modeling success failure allow us infer respective goals activities?
Q4. modeling failed attempts activities improve performance recognizing activities themselves?
elaborate three components system turn, subsequently
discuss, light experimental results lessons learned, answers research
questions.
5.1 Recognition Successful Activities
section, present unified framework intelligent relational denoising raw GPS
data simultaneously labeling instances player captured enemy freed
ally. denoising labeling cast learning inference problem Markov
logic. denoising, mean modifying raw GPS trajectories players final
trajectories satisfy constraints imposed geometry game area, motion model
players, well rules dynamics game. paper, refer trajectory
modification snapping since tile game area 3 3 meter cells snap raw
GPS reading appropriate cell. creating cells unobstructed space, ensure final
trajectory consistent map area.
begin modeling domain via Markov logic theory, write logical formulas express structure model hand, learn optimal set weights
formulas training data supervised discriminative fashion (details experimental setup Section 6). following two subsections, show augment seed
Markov logic theory recognize richer set events extract goals players multi-agent
activities.
order perform data denoising recognition successful capturing freeing,
model game weighted formulas Markov logic. formulas hard,
sense interested solutions satisfy them. Hard formulas capture basic
physical constraints (e.g., player one location time) inviolable rules game
(e.g., captured player must stand still freed game ends).3 rest formulas
soft, meaning finite weight associated one. soft constraints
correspond traditional low-level data filter, expressing preferences smooth trajectories
close raw GPS readings. soft constraints capture high-level constraints concerning
individual multi-agent activities likely occur. example, soft constraint states
player encounters enemy enemys territory, player likely captured.
exact weights soft constraints learned labeled data, described below.
distinguish two types atoms models: observed (e.g., GPS(P1 , 4, 43.13 , 77.71 )
hidden (e.g., freeing(P1 , P8 , 6)). observed predicates CTF domain are: GPS, enemies, adjacent, onHomeTer, onEnemyTer;4 whereas capturing, freeing, isCaptured, isFree,
samePlace, snap hidden. Additionally, set hidden predicates expanded structure learning algorithm described (see Table 1 predicate semantics). training phase,
3. Cheating occur CTF games, principle could accommodated making rules highlyweighted soft constraints rather hard constraints.
4. noise GPS data introduces ambiguity last two observed predicates, still reliably
generate since road marks boundary territories constitutes neutral zone.

96

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Hard Rules:
H1. raw GPS reading snapped exactly one cell.
H2.

(a) player frees player b, involved players must snapped common cell
time.
(b) player freed free ally.
(c) player freed currently captured.
(d) Immediately freeing event, freed player transitions free state.
(e) player freed enemy territory.

H3.

(a) player captures player b, involved players must snapped common cell
time.
(b) player captured free enemy.
(c) player captured currently free.
(d) Immediately capture event, captured player transitions captured state.
(e) player captured standing enemy territory.

H4. players free beginning game.
H5. given time, player either captured free both.
H6. player transitions captured state free state via freeing event.
H7. player transitions free state captured state via capture event.
H8. player captured must remain location.

Soft Rules:
S1. Minimize distance raw GPS reading snapped-to cell.
S2. Minimize projection variance, i.e., two consecutive snappings generally correlated.
S3. Maximize smoothness (both terms space time) final player trajectories.
S4. players b enemies, enemy territory b not, b captured already,
close other, probably captures b.
S5. players b allies, enemy territory, b currently captured not,
close other, probably frees b.
S6. Capture events generally rare, i.e., typically captures within game.
S7. Freeing events generally rare.

Figure 3: Descriptions hard soft rules capture flag.
learning algorithm access known truth assignment atoms. testing phase,
still access state observed atoms, infer assignment hidden
atoms.
Figure 3 gives English description hard soft rules low-level movement
player interactions within capture flag. Corresponding formulas language ML
shown Figures 5 6.
97

fiS ADILEK & K AUTZ

Predicate
capturing(a, b, t)
enemies(a, b)
adjacent(c1 , c2 )
failedCapturing(a, b, t)
failedFreeing(a, b, t)
freeing(a, b, t)
isCaptured(a, t)
isFailedCaptured(a, t)

Type
hidden
observed
observed
hidden
hidden
hidden
hidden
hidden

isFailedFree(a, t)

hidden

isFree(a, t)

hidden

onEnemyTer(a, t)
onHomeTer(a, t)
samePlace(a, b, t)

observed
observed
hidden

snap(a, c, t)

hidden

Meaning
Player capturing b time t.
Players b enemies.
Cells c1 c2 mutually adjacent, c1 = c2 .
Player unsuccessfully capturing b time t.
Player unsuccessfully freeing b time t.
Player freeing b time t.
Player captured state time t.
time t, player state follows
unsuccessful attempt capturing a.
state capabilities free.
time t, player state follows
unsuccessful attempt freeing a.
state capabilities captured.
Player free state time
(isFree(a, t) isCaptured(a, t)).
Player enemy territory time t.
Player home territory time t.
Players b either snapped common cell
two adjacent cells time t.
Player snapped cell c time t.

Table 1: Summary logical predicates models use. Predicate names containing word
failed introduced Markov logic theory augmentation method described
Section 5.2.1.

compare unified approach four alternative models. first two models (baseline
baseline states) purely deterministic separate denoising GPS data
labeling game events. implemented Perl. involve
training phase. third alternative model dynamic Bayesian network shown Figure 4.
Finally, two models cast Markov logic: two-step ML model unified ML
model itself. unified model handles denoising labeling joint fashion, whereas
two-step approach first performs snapping given geometric constraints subsequently labels
instances capturing freeing. latter three models evaluated using four-fold crossvalidation order test given game, first train model three games.
models access following observed data: raw GPS position player
time indication whether enemy home territory, location 3 3 meter
cell, cell adjacency, list pairs players enemies. tested five models
observed data. following describes model detail.
Baseline Model (B)
model two separate stages. First snap reading nearest cell afterward label instances player capturing player b. labeling rule simple:
98

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

loop whole discretized (via snapping) data set output capturing(a, b, t) every
time encounter pair players b snapped (in first step)
either cell two mutually adjacent cells time t, enemies,
home territory b not. Freeing recognition considered simple model
since need notion persisting player states (captured free) order model
freeing meaningful way.
Baseline Model States (B+S)
second model builds top previous one introducing notion players states. player captures player b time t, b enters captured state (in logic,
isCaptured(b, + 1)). b remains captured state moves (is snapped different cell later time) game ends. per rules CTF, player captured
state cannot captured again.
Thus, model works previous one except whenever label
capturing event, checks states involved players outputs capturing(a, b, t)
b captured state.
Freeing recognition implemented analogous way capturing recognition. Namely,
every time captured player b transition free state, check b
free teammate nearby (again, within adjacent cells). case, output
freeing(a, b, t).
Dynamic Bayesian Network Model (DBN)
dynamic Bayesian network model viewed probabilistic generalization
baseline model states. structure DBN model one player shown
Figure 4. time slice, one hidden node four observed nodes,
represent binary random variables. want infer likely state
player given time course game. state either free captured
hidden testing time. four observed random variables per time step model
players motion (M ), presence absence least one enemy (EN ) ally (AN ) player
nearby, finally players location either home enemy territory (ET ). player
modeled separate DBN. Therefore, fourteen instantiated DBNs game,
within one game, DBNs share set parameters.
Note DBN model perform GPS trajectory denoising itself. make fair
comparison Markov logic models, use denoising component Markov
logic theory using constraints H1 S1S3 (in Figure 3). produces denoised
discretization data subsequently fed DBN model. random variables
within DBN capture notion player movement players nearby one
another defined occupancy grid game area, two deterministic
baseline models. Namely, player said moving time + 1
snapped two different nonadjacent cells times. Similarly, two players
nearby snapped either cell two adjacent cells.
Two-Step ML Model (2SML)
two-step approach, two separate theories Markov logic. first theory
used perform preliminary snapping player trajectories individually us99

fiS ADILEK & K AUTZ

ETt

...

ENt

ANt

ETt+1

ENt+1

St

St+1

Mt

Mt+1

ANt+1

...

Figure 4: Two consecutive time slices dynamic Bayesian network modeling state
individual player P observations. Shaded nodes represent observed random
variables, unfilled denote hidden variables. random variables binary. (ETt = 1
P enemy territory time t, ENt = 1 enemy nearby time
t, ANt = 1 ally nearby time t, finally Mt = 1 P moved
time 1 t. value hidden state St 1 P captured time
0 P free.)

ing constraints H1 S1S3 (in Figure 3). theory identical one used
discretization step DBN model above.
second theory takes preliminary denoising list observed atoms
form preliminarySnap(a, c, t) (meaning player snapped cell c time t) uses
remaining constraints label instances capturing freeing, considering cell adjacency manner previous three models. two-step model constitutes
decomposition unified model (see below) overall contains virtually formulas, except 2SML operates observed preliminarySnap predicate, whereas unified
model contains hidden snap predicate instead. Thus omit elaborating here.
Unified ML Model (UML)
unified approach, express hard constraints H1H8 soft constraints S1
S7 (Figure 3) Markov logic single theory jointly denoises data labels game
events. Selected interesting formulas shown Figure 6their labels correspond
listing Figure 3. Note formulas S1S3 contain real-valued functions d1 , d2 , d3
respectively. d1 returns distance agent cell c time t. Similarly, d2 returns
dissimilarity two consecutive snapping vectors5 given agent position time
+ 1 location centers two cells c1 c2 . Finally, since people prefer
move straight lines, function d3 quantifies lack smoothness three consecutive
segments trajectory. Since wp , ws , wt assigned negative values
training, formulas S1S3 effectively softly enforce corresponding geometric constraints.
5. initial point snapping (projection) vector raw GPS reading terminal point center
cell snap reading to.

100

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

presence functions d1 d3 renders formulas S1S3 hybrid formulas. means
inference time, instantiated logical part formula evaluates either 1 (true)
0 (false), turn multiplied product corresponding function value
formula weight.
see train, test, evaluate four models, perform
multi-agent activity recognition task Section 6. Next, turn supervised learning method
augmenting unified ML model order recognize successful failed attempts
multi-agent activities.
Hard formulas:
a, c : snap(a, c, t)
0

(H1)
0

0

a, c, c , : (snap(a, c, t) c 6= c ) snap(a, c , t)
a1 , a2 , : freeing(a1 , a2 , t) samePlace(a1 , a2 , t) isFree(a1 , t)

(H2)

enemies(a1 , a2 ) isCaptured(a2 , t) isFree(a2 , + 1)

onEnemyTer(a1 , t) onEnemyTer(a2 , t)

a1 , a2 , : capturing(a1 , a2 , t) samePlace(a1 , a2 , t) isFree(a1 , t)

(H3)

enemies(a1 , a2 ) isFree(a2 , t) isCaptured(a2 , + 1)

onHomeTer(a1 , t) onEnemyTer(a2 , t)

a1 , a2 , : samePlace(a1 , a2 , t) c1 , c2 : snap(a1 , c1 , t) snap(a2 , c2 , t) adjacent(c1 , c2 )



a, : (t = 0) isFree(a, t)

(H4)

a, : isCaptured(a, t) isFree(a, t)

(H5)

a, : (isFree(a, t) isCaptured(a, + 1)) (=1 a1 : capturing(a1 , a, t))

(H6)

a, : (isCaptured(a, t) isFree(a, + 1)) (=1 a1 : freeing(a1 , a, t))

(H7)

a, t, c : (isCaptured(a, t) isCaptured(a, + 1) snap(a, c, t)) snap(a, c, + 1)

(H8)

Figure 5: hard formulas Markov logic. See corresponding rules Figure 3 English
description Table 1 explanation predicates. implementation,
actual rules written syntax used theBeast, Markov logic toolkit. (=1
denotes unique existential quantification, designates exclusive or.)

5.2 Learning Models Failed Attempts
work described above, manually designed structure Markov logic network
models capture flag domain allows us jointly denoise raw GPS data recognize
101

fiS ADILEK & K AUTZ

Soft formulas:


a, c, : snap(a, c, t) d1 (a, c, t) wp

(S1)



a,c1 , c2 , : snap(a, c1 , t) snap(a, c2 , + 1) d2 (a, c1 , c2 , t) ws

(S2)



a,c1 , c2 , c3 , : snap(a, c1 , t) snap(a, c2 , + 1) snap(a, c3 , + 2) d3 (a, c1 , c2 , c3 , t) wt
a1 , a2 , : [(enemies(a1 , a2 ) onHomeTer(a1 , t)

(S3)
(S4)

onEnemyTer(a2 , t) isFree(a2 , t)
samePlace(a1 , a2 , t)) capturing(a1 , a2 , t)] wc
a1 , a2 , : [(enemies(a1 , a2 ) onEnemyTer(a1 , t)

(S5)

onEnemyTer(a2 , t) samePlace(a1 , a2 , t) isFree(a1 , t)
isCaptured(a2 , t)) freeing(a1 , a2 , t)] wf


a, c, : capturing(a, c, t) wcb

(S6)

a, c, : [freeing(a, c, t)] wf b

(S7)

Figure 6: Soft formulas Markov logic. See corresponding rules Figure 3 English description. soft formula
written

traditional quantified finite first-order logic
formula (e.g., a, c, : snap(a, c, t) ), followed optional function (e.g., d1 (a, c, t)),
followed weight formula (e.g., wp ). syntax denotes inference
time, instantiated logical part formula evaluates either 1 (true) 0 (false),
effectively multiplied product corresponding function value
formula weight.

instances actual capturing freeing. show automaticallyin supervised
learning settingextend theory encompass correctly label successful actions,
failed attempts interactions. is, given raw GPS data represent
CTF games, want new model label instances player captures (or frees) player
b successful captures (successful frees) instances player almost captures (or frees)
player b failed captures (failed frees). example, failed capturing mean instance
players interactions whereup pointit appeared capturing b, carefully
consider events (potentially) preceded well impacts supposed capture
future unfolding game, conclude false alarm capture actually
occurred. words, conditions capture right, later on, pivotal
moment foiled capturing agents attempt.
activities (capturing freeing), model jointly finds optimal separation success failure. Note since cast model second-order Markov logic,
learn, e.g., isolated rule separates successful freeing failed attempt freeing.
Rathersince capturing freeing events (both actual failed) related thus labeling
activity as, say, successful capturing far-reaching impact past, present, future
102

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

labelingwe learn separations joint unified way. Namely, structure (logical
form) importance (weight) formula theory considered consequences
influence axioms theory. system thus finds optimal balance success failure capturing freeing activities respect training data.
5.2.1 HEORY AUGMENTATION LGORITHM
follows, describe Markov logic theory augmentation algorithm (Algorithm 1).
clarity, explain works concrete context ML models capture flag
discussed previous sections. However, underlying assumption successful actions
many ways similar failed counterparts, minorbut crucialdeviations cause
failure occur, often hold beyond capture flag. Therefore, algorithm applicable
domains different activities, long modeled Markov logic.
Algorithm 1 : Extend ML theory model successful well failed activities.
Input: A: set activities
MS : ML theory models successful instances activities
S: set examples successful activities
F : set examples failed activities
Output: MS+F : augmented ML model learned weights models successful
attempted activities
I: intended goals activities
1:
2:
3:
4:
5:
6:
7:

M2S liftToSecondOrderML(MS , A)
M0S instantiate(M2S , A)
findIncompatibleFormulas(F , M0S )
MS+F M0S \I
MS+F learnWeights(S, F , MS+F )
MS+F removeZeroWeightedFormulas(MS+F )
return MS+F ,

high-level, augmentation algorithm belongs family structure learning methods. Starting seed model successful actions, searches new formulas
added seed theory order jointly model successfully unsuccessfully carried
actions. declarative language biasessentially rules exploring hypothesis space candidate structuresis defined implicitly notion given activity, structure
unsuccessful attempts similar successful attempts. Therefore, augmentation algoritm
goes inflation stage, formulas seed theory generalized, followed
refinement stage, superfluous incompatible formulas inflated model pruned
away. refinement step optimizes weights within newly induced theory.
discuss process detail.
input theory augmentation algorithm consists initial first-order ML theory MS
models successful capturing freeing (such unified ML model defined Section 5.1
contains formulas shown Figures 5 6), set activities interest A, set
examples successful (S) well failed (F ) captures frees. MS need
weights soft formulas specified. case missing, learn scratch

103

fiS ADILEK & K AUTZ

final steps augmentation algorithm. weights specified, final weight learning
step MS+F leverage estimate initial weight values. specified
set predicate names, e.g., {capturing, freeing}. example sets F describes game
segment constitutes truth assignment appropriate literals instantiated MS . Table 2
shows two toy examples sets F three time steps. Since goal learn model
failed (and successful) attempts supervised way, example game segment F contain
activities labeled predicates failedCapturing() failedFreeing().
MS contains hybrid formulas (such formulas S1S3 Figure 6), appropriate function
definitions provided part F well. definition consists implicit mapping
input arguments function values. instance, function d1 formula S1 quantifies L2
distance
agent cell c time projected Mercator space: d1 (a, c, t) =
p
(a.gpsXt c.gpsX)2 + (a.gpsYt c.gpsY )2 .
system goes following process order induce new theory MS+F
augments MS definition failed attempts activity already defined MS .
First lift MS second-order Markov logic variabilizing predicates correspond
activities interest (step 1 Algorithm 1). yields lifted theory M2S . concretely, order apply technique domain, introduce new predicate variables captureType (whose domain {capturing, failedCapturing}), freeType (over {freeing, failedFreeing}),
stateType (over {isCaptured, isFailedCaptured, isFree, isFailedFree}). instance, variabilizing first-order ML formula freeing(a, b, t) enemies(a, b) yields second-order ML formula
freeType(a, b, t) enemies(a, b) (note freeType variable). Instantiating back
first-order yields two formulas: freeing(a, b, t) enemies(a, b) failedFreeing(a, b, t)
enemies(a, b).
far agents behavior concerned, CTF domain, isCaptured equivalent isFailedFree, isFree equivalent isFailedCaptured. soon see, theory augmentation
process learns equivalence classes relationships states training examples expanding subsequently refining formula H5 Figure 5. could work
isCaptured predicate negation represent agents states, feel explicit failure states makes discussion clearer. Furthermore, future work need address
hierarchies activities, including failures. context, representation explicit failure
states may convenient, may necessary.
Next, instantiate predicate variables M2S produce new first-order ML theory M0S
contains original theory MS entirety plus new formulas correspond failed captures frees (step 2). Since events are, e.g., near-captures appear similar actual successful
captures, hypothesis need drastically modify original successful formulas order model failed activities well. practice, process lifting
instantiating indeed results good seed theory. could emulate lifting grounding
steps scheme copying formulas renaming predicates duplicates appropriately,
cast approach principled second-order Markov logic, ties work closely
previous research results extensible framework. Specifically, second-order Markov
logic successfully used deep transfer learning (Davis & Domingos, 2009) predicate invention (Kok & Domingos, 2007). Therefore, interesting direction future work
combine theory augmentation refinement transfer inductive learningoperating
second-order MLto jointly induce models failed attempts different activities different
domains, starting single model successful activities source domain.
104

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Set S: Successful Capture
enemies(P1 , P2 )
enemies(P2 , P1 )
onEnemyTer(P2 , 2)
onEnemyTer(P2 , 3)
capturing(P1 , P2 , 2)
isFree(P1 , 1)
isFree(P1 , 2)
isFree(P1 , 3)
isFree(P2 , 1)
isFree(P2 , 2)
isCaptured(P2 , 3)
snap(P1 , C5, 1)
snap(P1 , C10, 2)
snap(P1 , C10, 3)
snap(P2 , C9, 1)
snap(P2 , C10, 2)
snap(P2 , C10, 3)
samePlace(P1 , P2 , 2)
samePlace(P2 , P1 , 2)
samePlace(P1 , P2 , 3)
samePlace(P2 , P1 , 3)

Set F: Failed Capture
enemies(P4 , P5 )
enemies(P5 , P4 )
onEnemyTer(P5 , 1)
onEnemyTer(P5 , 2)
onEnemyTer(P5 , 3)
failedCapturing(P4 , P5 , 2)
isFree(P4 , 1)
isFailedCaptured(P4 , 1)
isFree(P4 , 2)
isFailedCaptured(P4 , 2)
isFree(P4 , 3)
isFailedCaptured(P4 , 3)
isFree(P5 , 1)
isFailedCaptured(P5 , 1)
isFree(P5 , 2)
isFailedCaptured(P5 , 2)
isFree(P5 , 3)
isFailedCaptured(P5 , 3)
snap(P4 , C17, 1)
snap(P4 , C34, 2)
snap(P4 , C0, 3)
snap(P5 , C6, 1)
snap(P5 , C34, 2)
snap(P5 , C7, 3)
samePlace(P4 , P5 , 2)
samePlace(P5 , P4 , 2)

Table 2: Two examples logical representation successful (S) well failed (F ) capture
events input Algorithm 1. closed-world assumption applied, therefore
atoms listed assumed false. clarity, omit listing adjacent()
predicate.

Typical structure learning inductive logic programming techniques start initial (perhaps empty) theory iteratively grow refine order find form fits training data
well. order avoid searching generally huge space hypotheses, declarative bias either
specified hand mined data. declarative bias restricts set possible refinements formulas search algorithm apply. Common restrictions include limiting
formula length, adding new predicate formula shares least one variable
predicate already present formula. hand, approach, first
generate seed theory instantiating activity-related predicate variables. put
105

fiS ADILEK & K AUTZ

context structure learning, expand input model order generate large seed theory,
apply bottom-up (data-driven) learning prune seed theory, whereby training data
guides search formulas remove well optimal set weights remaining
formulas. conjecture failed attempt activity always violates least one constraint
holds successful executions activity. experiments support conjecture.
pruning done steps 3 4 Algorithm 1. function findIncompatibleFormulas(F ,
M0S ) returns set hard formulas M0S incompatible set examples failed
interactions F . say formula c compatible respect set examples F F
logically entails c (F |= c). Conversely, F entail c, say c incompatible w.r.t.
F . explain find incompatible formulas next section.
step 4 Algorithm 1, simply remove incompatible formulas (I) theory.
point, MS+F model, hard formulas guaranteed logically consistent
examples failed activities (because removed incompatible hard formulas), well
successful activities (because logically consistent start with). However,
soft formulas MS+F missing properly updated weights (in Markov logic, weight
hard formula simply set +). Therefore, run Markov logic weight learning using theBeast
package (step 5).
Recall theBeast implements cutting plane meta solving scheme inference Markov
logic, ground ML network reduced integer linear program subsequently
solved LpSolve ILP solver. chose approach opposed to, e.g., MaxWalkSAT
may find solution merely locally optimal, since resulting run times still relatively
short (under hour even training testing even complex model). Weights
learned discriminatively, directly model posterior conditional probability hidden predicates given observed predicates. set theBeast optimize weights soft
formulas via supervised on-line learning using margin infused relaxed algorithm (MIRA) weight
updates loss function computed number false positives false negatives
hidden atoms. Note soft formulas truly irrelevant respect
training examples, picked findIncompatibleFormulas() function,
weights set zero (or close zero) weight learning step (line 5 Algorithm 1).
zero-weighted formulas subsequently removed following step. Note weight
learning process need experience cold start, initial setting weights
inherited input theory MS .
Finally, return learned theory MS+F , whose formulas optimally weighted respect training examples. Experiments Results section below, use MS+F
recognize successful failed activities. Algorithm 1 returns incompatible hard formulas I. see used extract intended goal activities Section 5.3,
first, let us discuss step 3 Algorithm 1 detail.
5.2.2 C ONSISTENCY C HECK : F INDING NCOMPATIBLE F ORMULAS
turn method finding incompatible formulas (summarized Algorithm 2). Since
method leverages satisfiability testing determine consistency candidate theories
possible worlds (examples),6 Algorithm 2 viewed instance learning
interpretationsa learning setting inductive logic programming literature (De Raedt, 2008).
6. often referred covers relation inductive logic programming.

106

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Algorithm 2 (findIncompatibleFormulas). Find formulas ML theory logically inconsistent examples execution failed activities.
Input: F : set examples failed activities
: unrefined ML theory successful failed activities
Output: smallest set formulas appear unsatisfiable worlds F
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

extractObjects(F )
Thard \ Tsoft
integer n 0
boolean result false
result == false
c Thard
remove new n-tuple formulas c
current n, n-tuples tested
nn+1
end
result testSAT(F , c , O)
end
return Thard \ c

input, take set examples failed activities F seed theory (e.g., produced
step 2 Algorithm 1). output smallest set hard formulas appear
logically inconsistent F . algorithm first extracts set objects appear
F (step 1 Algorithm 2), keeping track type object. example, suppose two example worlds F shown Table 3. extractObjects(F ) returns
{P1 , P2 , P7 , P8 , C3 , C5 , 1, 2}.
Example 1
snap(P1 , C5 , 1)
snap(P2 , C5 , 1)
failedCapturing(P1 , P2 , 1)

Example 2
snap(P7 , C3 , 2)
snap(P8 , C3 , 2)
failedFreeing(P2 , P5 , 2)

Table 3: Two simple examples logical representation failed capture event.

step 2, limit hard formulas testing compatibility. since
prove incompatibility hard formulas. Soft constraints violated many times
data yet may want eliminate them. Instead, want merely adjust
weights, exactly approach. Therefore, Thard contains hard formulas
appear . Next, lines 5 12, check entire unmodified Thard compatible
(since n = 0, remove formulas). compatible, return empty set
indicating hard formulas original seed theory compatible examples.
detect incompatibility, need remove some, perhaps even all, hard formulas
order arrive logically consistent theory. Therefore, incrementally start removing n-tuples
formulas. is, subsequent |Thard | iterations loop, determine
107

fiS ADILEK & K AUTZ

restore consistency removing one hard formulas Thard . can, return
set Thard \ , identified removed incompatible formula. consistency cannot
restored removing single formula, turn begin considering pairs formulas (n = 2),
triples (n = 3), etc. find pruned theory c consistent examples.
general, need consider n-tuples formulas, rather testing formula
isolation. due disjunctive formulas conjunction possibly incomplete truth
assignment training data. Consider following theory propositional logic:
f1 = b
f2 = b c
Data: c
(Following closed-world assumption, negated atom c would actually appear training data, explicitly include example clarity.) f1 f2 individually consistent data, f1 f2 inconsistent data. complicated examples
constructed, every group k formulas inconsistent data, even though
individual formulas are. special case truth values atoms training examples known, formulas tested consistency individually, reduces original
exponential number iterations Algorithm 2 executes, worst case, linear complexity.
interesting direction future work explore applications logical methods lower
computational cost general case partially observed data.
note hard formulas model physical constraints inviolable rules capture
flag, therefore hold universally. Appropriately, formulas eliminated Algorithm 2. example, consider formula H1 Figure 5, asserts player occupies
exactly one cell given time. formula satisfied games include successful failed activities. hand, consider formula H8 figure. contains
captured player cell captured (following captured players cannot move rule
CTF). holds successful capturing events, necessarily hold failed
attempts capturing. Therefore, rule H8 expanded via second-order ML,
derived formulas going consistent observations.
Specifically, candidate formula Equation 6 pruned away, inconsistent
training examples, i.e., players nearly captured continue free move about.
However, remaining three variants formula H8 pruned away. Equation 7
always evaluate true, since someone attempts re-capture already captured player a,
indeed remain stationary. Similarly, Equation 8 consistent example CTF games
failed attempt capture immediately followed successful capture,
captured player remain place time onward. Finally, Equation 9 compatible well,
since original formula H8 consistent observations.

a, t, c : isFailedCaptured(a, t) isFailedCaptured(a, + 1) snap(a, c, t) snap(a, c, + 1)
(6)

a, t, c : isCaptured(a, t) isFailedCaptured(a, + 1) snap(a, c, t) snap(a, c, + 1) (7)

108

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR


a, t, c : isFailedCaptured(a, t) isCaptured(a, + 1) snap(a, c, t) snap(a, c, + 1) (8)


a, t, c : isCaptured(a, t) isCaptured(a, + 1) snap(a, c, t) snap(a, c, + 1)

(9)

function testSAT() (line 11 Algorithm 2) checks whether given candidate theory c
compatible examples F following process. First, ground c using objects
O, thereby creating ground theory G. example, c = {p(x) q(x)} = {B, W },
grounding would G = {p(B) q(B), p(W ) q(W )}. check G Fhidden
satisfiable using miniSAT solver, Fhidden simply set hidden atoms appear
F . Intuitively, corresponds testing whether plug worlds F c
satisfying hard constraints. Though satisfiability NP-complete problem, practice
testSAT() completes within tenths second even largest problems CTF domain.
instance, suppose Fhidden = {p(B), q(B)}. test satisfiability formula



p(B) q(B) p(W ) q(W ) p(B) q(B).
case cannot satisfy since forced set p(B) true q(B) false,
renders first clauseand therefore whole formulafalse.
alternative approach pruning formulas via satisfiability testing, described,
would treat types formulas (hard soft) inflated theory M0S strictly soft
formulas learning weight formula examples successful failed game
events. However, introduces several complications negatively impact systems performance well model clarity. First, number formulas inflated theory
exponentially larger seed theory. instantiation second-order ML representation quantified limit expansion, still worst-case exponential blow-up.
treating formulas soft ones, need potentially learn many weights.
especially problematic activities occur rarely, may enough training data
properly learn weights. Eliminating hard candidate formulas proving inconsistent
dramatically reduces number parameters model. satisfiability testing
NP-complete, weight learning Markov logic entails running inference multiple times,
#P-complete problem.
second reason distinguishing soft hard formulas resulting clarity
elegance final learned model MS+F . Even situations enough training data
properly learn large number weights, run overfitting problems, neither
structure parameters model represent domain natural way. experiments
shown skip pruning stage (steps 3 4 Algorithm 1), models recognition
performance differ pruned model significant way (p-value 0.45).
However, end large number soft formulas mixture positive negative
weights learning algorithm carefully tuned balanced fit training data.
however bear little relationship concepts underlying domain. make
hard human expert analyze model, makes even harder modify
model.
109

fiS ADILEK & K AUTZ

reasons, softening hard formulas is, general, infeasible. interesting direction
future work identify small amount key inconsistent hard formulas soften,
eliminating rest inconsistent hard formulas. however entails searching large
space candidate subsets softened formulas, iteration requires expensive re-learning
weights.
Note Algorithm 2 terminates soon finds compatible theory requires smallest
number formula-removals. experimented active learning component
system, modify Algorithms 1 2 present several possible refinements
theory user selects one looks best. proposed modifications
shown ML theory level modified sections (formulas) highlighted well
data level program shows inferred consequences modifications.
candidate modification, corresponding consequences displayed collection animations
animation shows results activity recognition would committed
particular candidate theory. Note even people background ML
interact system since visualization easy understand. Interestingly, case
captures frees, least modified theory off-line version algorithm finds
best one therefore need query user. One view differential
variant Occams razor. However, different activities domains, active learning
approach may worth revisiting leave exploration future work.
Finally, general structure learning techniques statistical-relational AI inductive
logic programming applicable substitute theory augmentation algorithm
several reasons. main reason that, efficiency reasons, existing techniques literature
typically operate restricted set formula templates. is, consider Horn
clauses, formulas without existential quantifier, formulas k literals
l variables, on. set restrictions part language bias given
approach. principle, structure learning possible without language bias, one often
carefully define one sake tractability (see Section 7 details). approach,
language bias defined implicitly discussed Section 5.2.1.
5.3 Extracting Goal Success Failure
Recall applying theory augmentation process (Algorithm 1) CTF seed theory
successful interactions (shown Figures 5 6) induces new set formulas capture
structure failed activities ties together existing formulas seed theory.
logically inconsistent formulas Algorithm 2 returns ones satisfiable
worlds failed activities. time, variants formulas consistent
examples successful actions occurring games. Therefore, represents difference
theory models successful activities augmented theory successful
failed actions, derived it. Intuitively, difference success
failure viewed intended purpose given activity rational agent executes,
consequently goal agent mind engages particular activity.
next section, explore goals extracted CTF domain fashion.
concludes discussion models methodology, turn experimental
evaluation framework presented above.

110

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

6. Experiments Results
evaluate approach along three major directions outlined Section 5 (Methodology),
focusing answering four research questions formulated ibidem. structure
section closely follows Methodology section.
nutshell, first interested Markov logic models perform standard
multi-agent activity recognition tasklabeling successful activitiesand performance
compares alternative models. Second, examine augmented model captures
successful failed attempts activities. model MS+F induced Algorithm 1,
lets us extract intended goal activities question. Third, compare performance
MS+F task jointly recognizing four activities alternative model.
Finally, investigate extent reasoning failed attempts help recognition
successfully executed activities.
experiments performed capture flag dataset consisting four separate games.
dataset summarized Table 4, game list number raw GPS readings
number instances activity interest. evaluate models via four-fold crossvalidation, always training three games (if training required model) testing
fourth. experimental condition below, report precision, recall, F1 scores attained
respective model four cross-validation runs. purposefully chosen
split data cross-validation fold directly corresponds separate game CTF
conceptual convenience clarity. discussed above, events occurring games often
far-reaching consequences. example, captured players never freed allies.
Therefore, capture beginning game typically profoundly influences entire rest
game. reason, splitting games randomly even manually would introduce unnecessary
complications, segments would dependencies segments. enforcing
fold exactly corresponds different game, make fold self-contained.
quantify statistical significance pair-wise differences models, use
generalized probabilistic interpretation F1 score (Goutte & Gaussier, 2005). Namely, express
F1 scores terms gamma variates derived models true positives, false positives, false
negatives ( = 0.5, h = 1.0, cf., Goutte & Gaussier, 2005). approach makes possible
compare results future work may apply alternative models similar, identical,
datasets. future comparison may, instance, include additional games introduce random
splits data. note standard statistical significance tests cannot applied situations. p-values reported one sided, interested models performance significantly
improves level sophistication increases.
6.1 Recognition Successful Activities
Recall two-step (2SML) unified (UML) Markov logic models, specify
Markov logic formulas hand optimize weights soft formulas via supervised online learning. run modified version theBeast software package perform weight learning
MAP inference. theBeast implements cutting plane meta solving scheme inference
Markov logic, ground ML network reduced integer linear program subsequently solved LpSolve ILP solver. chose approach opposed to, e.g., MaxWalkSAT
get stuck local optimum, since resulting run times still relatively short (under
hour even training testing even complex model).
111

fiS ADILEK & K AUTZ

Game 1
Game 2
Game 3
Game 4
Total

#GPS
13,412
14,420
3,472
10,850
42,154

#AC
2
2
6
3
13

#FC
15
34
12
4
65

#AF
2
2
0
1
5

#FF
1
1
2
0
4

Table 4: CTF dataset overview: #GPS total number raw GPS readings, #AC #FC
number actual (successful) failed captures respectively, analogously freeings
(#AF #FF).

weight learning time, use margin infused relaxed algorithm (MIRA) weight updates loss function computed number false positives false negatives
hidden atoms, described Methodology section. discretization step
dynamic Bayesian network model (DBN) implemented Markov logic executed
fashion. DBN model trained via maximum likelihood described Section 4.2.
two deterministic baselines (B B+S) require training phase.
inference time, interested likely explanation data. Markov logic,
maximum posteriori inference reduces finding complete truth assignment satisfies
hard constraints maximizing sum weights satisfied soft formulas. testing
time, theBeast Markov logic solver finds likely truth assignment hidden atoms
described above, section specifically interested values capturing
freeing atoms.
DBNs, likely explanation observations equivalent Viterbi decoding.
DBN model assigns either free captured state player every time step. label
transitions free captured state capturing transitions captured free
freeing. Note DBN model capable determining player freed captured,
model player freeing capturing. evaluation, give
benefit doubt assume always outputs correct actor.
models, inference done simultaneously entire game (on average, 10
minutes worth data). Note restrict inference (small) sliding time window.
experiments described show, many events domain definitely recognized
long occur. example, GPS noise may make impossible determine whether player
captured moment encounter enemy, player thereafter remains
place long time, possibility capture becomes certain.
Figures 7 8 summarize performance models successful capturing freeing
terms precision, recall, F1 score calculated four cross-validation runs. clarity,
present results two separate plots, model jointly labeling capturing
freeing activities. consider baseline model freeing recognition activity
makes little sense without notion player state (captured free).
see unified approach yields best results activities. Let us focus
capturing first (Figure 7). Overall, unified model labels 11 13 captures correctlythere

112

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Capturing Recogni/on
1.00

1.00
0.80

0.69

0.69

0.77

0.87
0.77

1.00

0.92
0.85

0.60

Precision

0.40
0.20
0.00

Recall

0.26
0.16
0.01 0.02

0.03 0.06

B

B+S

F1

DBN

2SML

UML

Figure 7: Comparison performance five models capturing recognition joint
inference capturing freeing events. See Table 5 statistical significance
analysis pairwise differences models. (B = baseline model, B+S = baseline
model states, 2SML = two-step Markov logic model, UML = unified Markov logic
model)

two false negatives. fact, two capture events missed models
involve two enemies appear unusually far apart (about 12 meters) raw data. Even
unified approach fails instance since cost adjusting players trajectoriesthereby
losing score due violation geometry-based constraintsis compensated
potential gain labeling additional capture.
Note even two-step approach recognizes 10 13 captures. compared
unified model, misses one additional instance involved players, moderately
far apart, snapped mutually nonadjacent cells. hand, unified model
fail situation limited prior nonrelational snapping nearby cells.
However, difference performance dataset statistically significant even
0.05 level (p-value 0.32).
deterministic baseline models (B B+S) perform poorly. Although yield
respectable recall, produce overwhelming amount false positives. shows even
relatively comprehensive pattern matching work domain. Interestingly,
performance DBN model leaves much desired well, especially terms precision.
DBN model significantly better baselines (p-value less 5.9 105 ),
achieves significantly worse performance Markov logic models (p-value less
0.0002; see Table 5).
Table 5 summarizes p-values pairwise differences models actual (i.e., successful)
capturing. difference Markov logic-based models (2SML UML)

113

fiS ADILEK & K AUTZ

Freeing Recogni+on
1.00

1.00

1.00

0.80

0.75
0.57

0.60
0.40

0.40
0.20

0.20
0.15
0.13

0.22

0.60

Precision

0.40

Recall

0.29

F1

0.00
B+S

DBN

2-SML

UML

Figure 8: Comparison performance three models freeing recognition joint
inference capturing freeing events. See Table 6 statistical significance
analysis pairwise differences models. (B+S = baseline model states,
2SML = two-step Markov logic model, UML = unified Markov logic model)

B
B+S
DBN
2SML

B+S
0.0192
-

DBN
3.6 106
5.9 105
-

2SML
5.1 107
9.4 106
0.0002
-

UML
2.9 107
1.4 106
8.0 105
0.3230

Table 5: Summary statistical significance (one sided p-values) pairwise differences F1 scores models actual capturing. (B = baseline model, B+S = baseline
model states, DBN = dynamic Bayesian network model, 2SML = two-step Markov
logic model, UML = unified Markov logic model)

statistically significant (p-value 0.32), pairwise differences F1 scores models
significant 0.02 level, often even much lower p-values.
Though unified model still outperforms alternatives case freeing recognition
well, performance ideal compared capture recognition case (Figure 8).
correctly identifies 3 5 freeing events games, produce false
positives. partly due dependency freeing capturing. failure model
recognize capture precludes recognition future freeing. Another reason extreme
sparseness freeing events (there five 40,000+ datapoints). Finally,

114

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

B+S
DBN
2SML

DBN
0.2739
-

2SML
0.0733
0.1672
-

UML
0.0162
0.0497
0.2743

Table 6: Summary statistical significance (one sided p-values) pairwise differences F1 scores models actual freeing. (B+S = baseline model states, DBN =
dynamic Bayesian network model, 2SML = two-step Markov logic model, UML = unified
Markov logic model)

instances players barely move freed. may occur number reasons
ranging already occupying strategic spot simply tired. freeing instances
challenging automated system, even people familiar game recognize
(several situations would extremely hard disambiguate didnt access
notes data collection).
two-step ML model slightly worse job unified model freeing recognition.
correctly identifies 2 5 freeings reasons capturing recognition
case. Similarly models actual captures, difference unified two-step freeing
models statistically significant (p-value 0.27).
Table 6 summarizes p-values pairwise differences models actual (i.e., successful) freeing. see difference B+S UML models statistically
significant (p-value 0.01), whereas differences rest model pairs
statistically significant. Since five instances successful freeing, 2SML model
perform significantly better B+S model 0.05 significance level (p-value
0.07). However, UML model achieves better recognition results even DBN model
high confidence (p-value less 0.05). Therefore, see although 2SML model strictly
dominates non-Markov logic models evaluated capturing recognition, need full
power unified ML model strictly outperform nonrelational alternatives freeing.
suggests move complex interdependent activities, relational unified
modeling approaches winning larger larger margins.
Even though statistical significance tests suggest 2SML likely give similar results
UML, important note 2SML, design, precludes recognition activities question
certain situations. Namely, experiments demonstrate, players snapped cells
far apart, two-step model even consider instances candidates
labeling, inevitably fails recognizing them. Therefore, one needs look beyond p-values
obtained comparing fully unified models various alternatives.
expected experiments capturing recognition, deterministic baseline models perform poorly freeing recognition well. produce overwhelming
amount false positives, fail recognize freeing events.
Thus, see models cast Markov logic perform significantly better
deterministic baseline models, better probabilistic, nonrelational, DBN model.
note DBN model potential quite powerful similar DBNs
applied great success previous work activity recognition location data (Eagle &
115

fiS ADILEK & K AUTZ

Pentland, 2006; Liao, Patterson, Fox, & Kautz, 2007). many similarities twostep ML model. share denoising discretization step, operate
observed data. key difference DBN model considers players individually,
whereas two-step ML model performs joint reasoning.
Looking actual CTF game data, see several concrete examples hurts DBNs
labeling accuracy. instance, consider situation two allies captured near
other. Performing inference individual players isolation allows DBN model infer
two players effectively free other, even though reality captured cannot
so. occurs DBN model oblivious explicit states ones teammates
well opponents. Since capturing freeing interdependent, obliviousness DBN
model state actors negatively impacts recognition performance activities.
example gave illustrates one type freeing false positives. hallucinated freeings
create opportunities often lead false positives captures, creating vicious cycle. False
negatives freeing (capturing) events often occur players model incorrectly believes
already freed (captured) prior time.
Since Markov logic based models significantly betterwith high level confidence
alternatives fully relational, experiments validate hypothesis
need exploit rich relational temporal structure domain probabilistic way
time affirmatively answer research question Q1 (Can reliably recognize complex
multi-agent activities CTF dataset even presence severe noise?). Namely, show
although relatively powerful probabilistic models sufficient achieve high labeling
accuracy, gain significant improvements formulating recognition problem learning
inference Markov logic networks.
turn evaluation method learning models success failure
peoples activities.
6.2 Learned Formulas Intentions
Applying theory augmentation process (Algorithm 1) CTF seed theory (shown Figures 5 6) induces new set formulas capture structure failed activities ties
together existing formulas theory. call model MS+F . Figure 9 shows
examples new weighted formulas modeling failed freeing capturing attempts appear
MS+F .
First, note system correctly carries basic preconditions activity (contrast
formulas S4 S40 S5 S50 Figures 6 9 respectively). allows reliably
recognize successful failed actions instead of, e.g., merely labeling events
point time appear resemble capture near-capture. re-use preconditions directly
follows language bias theory augmentation algorithm.
Turning attention learned hard formulas, observe system correctly induced
equivalence classes states, derived mutual exclusion relationships (H50 ).
furthermore tied new failure states corresponding instantaneous interactions (H60
H70 ).
Finally, algorithm correctly discovers rule player captured
must remain location (H8, Figure 5) key distinction successful
failed capture (since players actually captured still move). Therefore, introduces

116

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

appropriate rule failed captures (H80 , Figure 9) explicitly stating failed capturing
confine near-captured player remain stationary. analogous process yields fitting
separation failed successful freeings. Namely, model learns unsuccessfully
freed player remains stationary. learned difference success failure players
actions directly corresponds goal activity consequently intent rational actors. difference system outputs intended goal capturing activity (and
analogously freeing).
experimental results provide evidence resounding yes Q2 (Can models
attempted activities automatically learned leveraging existing models successfully performed actions?) Q3 (Does modeling success failure allow us infer respective
goals activities?) within CTF domain.
note instead applying automated theory augmentation method, person could,
principle, manually formulate Markov logic theory successful well failed activities
observing games. all, designed initial seed model successful
events. However, process extremely time consuming, one tends omit encoding facts
us, humans, seem self-evident need explicitly articulated machine (e.g.,
single person cannot ten different places once, player either free captured
both). surprisingly easy introduce errors theory, difficult debug,
mostly complex weight learning techniques involved. Therefore, believe
theory augmentation method significant step forward enhancing models capabilities
requiring small amounts human effort. complexity domains models increases,
advantage gain larger larger importance.
6.3 Recognition Successful Failed Activities
compare performance model MS+F alternative (baseline) method
labels four activities following way. Similarly baseline states model successful interactions defined Section 5.1, two separate stages. First snap GPS
reading nearest cell applying geometric constraints (H1 S1S3) theory, afterward label instances activities. following labeling rule applied.
loop whole discretized (via snapping) data set look instances pair
players b snapped (in first step) either cell two adjacent cells
time t, enemies, b captured already, home territory b not.
b moves (is snapped different cell later time) without ally nearby, output
failedCapturing(a,b,t), otherwise output capturing(a,b,t). labeling rule freeing defined analogously four events tied together. tested variant DBN model
introduced Section 5.1 two additional hidden state values node St : isFailedFree
isFailedCaptured. However, difference results obtained model statistically significant (p-value 0.38), therefore focus conceptually straightforward
baseline model described above.
Model MS+F evaluated using four-fold cross-validation (always training three games
testing fourth). Figure 10 compares models terms precision, recall, F1
score. Note four activities modeled jointly models. F1 score augmented
model significantly better baseline four target activities (p-value less
1.3 104 ).

117

fiS ADILEK & K AUTZ

a1 , a2 , : [(enemies(a1 , a2 ) onHomeTer(a1 , t)

(S40 )

onEnemyTer(a2 , t) samePlace(a1 , a2 , t) isFree(a1 , t)
isFree(a2 , t)) failedCapturing(a1 , a2 , t)] 11.206
a1 , a2 , : [(enemies(a1 , a2 ) onEnemyTer(a1 , t)

(S50 )

onEnemyTer(a2 , t) samePlace(a1 , a2 , t) isFree(a1 , t)
isCaptured(a2 , t)) failedFreeing(a1 , a2 , t)] 1.483
a1 , a2 , : [failedCapturing(a1 , a2 , t)] (0.0001)

(S60 )

a1 , a2 , : [failedFreeing(a1 , a2 , t)] (0.002)

(S70 )

a, : isFailedCaptured(a, t) isFree(a, t)

(H50 )

a, : isCaptured(a, t) isFailedFree(a, t)
a, : isFailedCaptured(a, t) isFree(a, t)
a, : isCaptured(a, t) isFailedFree(a, t)
a, : (isFree(a, t) isFailedCaptured(a, + 1)) (=1 a1 : failedCapturing(a1 , a, t))
a, : (isCaptured(a, t) isFailedFree(a, + 1)) (=1 a1 : failedFreeing(a1 , a, t))

(H60 )
(H70 )

a, t, c : (isFailedCaptured(a, t) isFailedCaptured(a, + 1) snap(a, c, t)) snap(a, c, + 1)
(H80 )

Figure 9: Example formulas, learned Algorithm 1, model unsuccessful capturing freeing events. crucial intent recognition formula (H80 ) highlighted bold. Formulas
eliminated Algorithm 2 preceded symbol, included
induced model MS+F . identity isCaptured(a, t) = isFree(a, t) applied throughout refining show formulas intuitive fashion. concreteness sake,
values learned weights come one cross-validation run (and similar
runs).

see baseline model has, general, respectable recall produces large
number false positives activities. false positives stem fact algorithm
greedy typically labels situation several players appear close
certain period time sequence many captures subsequent frees even though none
actually occurred. Model MS+F gives significantly better results takes full
advantage structure game probabilistic fashion. similar labeling
tendency case failed captures, single capture attempt often labeled
several consecutive attempts. hurts precision score, significant deficiency,

118

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

0.15

Baseline

AC (13)

0.23

FC (65)

0.97

0.13
0.04

AF (5)

0.40

0.02
0.06

(4)
Augmented ML Model

0.46

0.09

F1

0.75

0.03

Recall
0.96
0.92
1.00

AC (13)
0.86

FC (65)

0.78

AF (5)

0.80

(4)

0.75

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

Precision

0.97

0.89
1.00
0.86
1.00

0.9

1

Figure 10: Performance baseline augmented (MS+F ) models joint recognition
successful failed capturing freeing. F1 score augmented model
significantly better baseline four target activities (p-value less
1.3 104 ). AC = actual (successful) capturing, FC = failed capturing, AF =
actual freeing, = failed freeing.

practice, small number short game segments labeled possible near-captures
useful well.
note even though original model (UML) contain information
failed capturing failed freeing, performance MS+F respectable even two
newly introduced activities. provided examples game situations attempts
occur system augmented subsequently labeled four activities. Thus, see
indeed extend preexisting models automated fashion unified model
capable recognizing individual activities, success failure peoples
behavior.
6.4 Effect Modeling Failed Attempts Recognition Successful Activities
address research question Q4 (Does modeling failed attempts activities improve performance recognizing activities themselves?), want see much recognition
attempted activities help modeling successful actions (the latter standard activity

119

fiS ADILEK & K AUTZ

Capturing

F1

0.92

Recall

0.85

Precision

+0.08

1.00

F1
Freeing

+0.04

0.75

Recall

+0.14

0.60

+0.20

Precision

1.00

0

0.1

0.2

0.3

0.4

Without Modeling Failure

0.5

0.6

0.7

0.8

0.9

1

Modeling Failure

Figure 11: Considering unsuccessfully attempted activities strictly improves performance standard activity recognition. Blue bars show scores obtained unified Markov logic
model considers successful activities (MS ). red bars indicate additive improvement provided augmented model considers successful
failed activities (MS+F , output Algorithm 1). model labels target activities jointly, separate capturing freeing plot clarity. Precision value
1 models. F1 scores obtained explicitly modeling failed attempts
statistically different F1 scores obtained without modeling attempts high
confidence level (p-value 0.20). However, results still show importance
reasoning peoples attempts recognizing activities; see text details.

recognition problem). Toward end, compare Markov logic model MS jointly labels
successful capturing freeing model MS+F jointly labels successful
failed attempts capturing freeing (see Section 5.2.1 detailed description two
models). However, evaluate terms precision, recall, F1 score successful
interactions, four types activities.
Figure 11 summarizes results. see evaluated actual capturing, MS+F
performs better MS , similarly freeing. However, difference F1 scores
model captures attempted successful activities (MS+F ) model successful activities (MS ) statistically significant (p-value 0.20). partly MS
already produces solid results, leaving little room improvement. Additionally, CTF
dataset contains relatively events interest. terms labeling performance testing time,
difference two models 11% (MS MS+F recognize, respectively,
120

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

14 16 18 successful activities correctly). Thus, believe trends shown Figure 11
promising modeling attempted actions improve recognition performance capturing freeing, evaluation dataset larger number events needed show
difference statistically significant higher confidence level. However, mean
recognizing attempts unimportant. show above, induced augmented model
recognize failed (as well successful) activities complex CTF domain high accuracy,
argue significant contribution.
Finally, comparison MS MS+F shows applying learning algorithm augments model recognition capabilities hurt model labeling performance.
fact binary classification problems typically easier solve multi-class counterparts well reported machine learning literature (Allwein, Schapire, & Singer, 2001).
Therefore, introducing new activities model, especially automated way, likely degrade performance. Contrary intuition, experiments show MS+F worse
MS successful activity recognition (i.e., intersection) high confidence, even though
MS+F clearly richer useful.

7. Related Work
world single-agent location-based reasoning, work Bui (2003) presents evaluates system probabilistic plan recognition cast abstract hidden Markov memory model.
Subsequently, work Liao et al. (2004) implements system denoising raw GPS traces
simultaneously inferring individuals mode transportation (car, bus, etc.) goal destination. cast problem learning inference dynamic Bayesian network achieve
encouraging results. follow-up work, Liao et al. (2005) introduce framework locationbased activity recognition, implemented efficient learning inference relational
Markov network.
work Ashbrook Starner (2003) focuses inferring significant locations raw
GPS logs via clustering. transition probabilities important places subsequently
used number user modeling tasks, including location prediction. work Eagle
Pentland (2006) explores harnessing data collected regular smart phones modeling human
behavior. Specifically, infer individuals general location nearby cell towers Bluetooth devices various times day. Applying hidden Markov model (HMM), show
predicting person home, work, someplace else achieved 90% accuracy. Similarly, work Eagle Pentland (2009) extracts significant patterns signatures
peoples movement applying eigenanalysis smart phone logs.
work Hu, Pan, Zheng, Liu, Yang (2008) concentrates recognition interleaving
overlapping activities. show publicly available academic datasets contain significant
number instances activities, formulate conditional random field (CRF) model
capable detecting high (more 80%) accuracy. However, focus solely
single-agent household activities.
Peoples conversation primary focus multi-agent modeling effort (Barbuceanu
& Fox, 1995). fields multi-agent activity recognition studies human behavior, researchers either modeled conversation explicitly (e.g., Busetta, Serafini, Singh, & Zini, 2001),
leveraged peoples communication implicitly via call location logs mobile phones.
data successfully used infer social networks, user mobility patterns, model socially

121

fiS ADILEK & K AUTZ

significant locations dynamics, others (Eagle & Pentland, 2006; Eagle, Pentland,
& Lazer, 2009). arguably excellent stepping stone full-fledged multi-agent activity
recognition since location is, times, practically synonymous ones activity (e.g.,
store often implies shopping) (Tang, Lin, Hong, Siewiorek, & Sadeh, 2010), social networks
tremendous influence behavior (Pentland, 2008).
Additionally, number researchers machine vision worked problem recognizing events videos sporting events, impressive recent work learning models
baseball plays (Gupta et al., 2009). work area focused recognizing individual
actions (e.g., catching throwing), state art beginning consider relational
actions (e.g., ball thrown player player B). computational challenges dealing
video data make necessary limit time windows seconds. contrast,
demonstrate work many events capture flag data disambiguated
considering arbitrarily long temporal sequences. general, however, work
machine vision rely upon similar probabilistic models, already evidence
statistical-relational techniques similar Markov logic used activity recognition
video (Biswas, Thrun, & Fujimura, 2007; Tran & Davis, 2008).
Looking beyond activity recognition, recent work relational spacial reasoning includes
attempt locateusing spacial abductioncaches weapons Iraq based information
attacks area (Shakarian, Subrahmanian, & Spaino, 2009). Additionally, work Abowd
et al. (1997) presents location- context-aware system, Cyberguide, helps people explore
fully experience foreign locations. researchers explore intelligent nonintrusive
navigation system takes advantage predictions traffic conditions along model
users knowledge competence (Horvitz et al., 2005). Finally, work Kamar Horvitz
(2009) explore automatic generation synergistic plans regarding sharing vehicles across multiple
commuters.
interesting line work cognitive science focuses intent goal recognition probabilistic framework (Baker, Tenenbaum, & Saxe, 2006, 2007). Specifically, cast goal inference
inverse planning problem Markov decision processes, Bayesian inversion used estimate posterior distribution possible goals. Recent extensions work begin consider
simulated multi-agent domains (Baker, Goodman, & Tenenbaum, 2008; Ullman, Baker, Macindoe,
Evans, Goodman, & Tenenbaum, 2010; Baker, Saxe, & Tenenbaum, 2011). Comparison
computational models human judgement synthetic domains shows strong correlation
peoples predicted actual behavior. However, computational challenges involved
dealing underlying partially observable Markov decision processes prohibitive
complex domains large state spaces, ours.
focus work different aspect reasoning peoples goals. Rather
inferring distribution possible, priori known goals, automatically induce goals
complex multi-agent activities themselves.
researchers concentrated modeling behavior people general agents reinforcement learning problems single-agent multi-agent settings. work (2008)
proposes system household activity recognition cast single-agent Markov decision process
problem subsequently solved using probabilistic model checker. Wilson colleagues address problem learning agents roles multi-agent domain derived real-time strategy
computer game (Wilson, Fern, Ray, & Tadepalli, 2008; Wilson, Fern, & Tadepalli, 2010). Experiments synthetic domain show strongly encouraging results. perform role
122

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

learning ourselves, anticipate work Wilson et al. going play important role
learning hierarchies peoples activities. capture flag domain, one imagine automatically identifying particular player as, example, defender subsequently leveraging
information model behavior personalized way.
work Hong (2001) concentrates recognizing goal agent course
activities deterministic, relational setting. Interesting work goal recognition
applied computer-aided monitoring complex multi-agent systems, relationships
agents leveraged compensate noise sparse data (Kaminka, Tambe, Pynadath,
& Tambe, 2002). contrast, work focus learning respective goals given set
multi-agent activities probabilistic setting. knowledge turn leveraged achieve
stronger robustness recognition tasks. Similarly approach Hong, system
need supplied plan library either.
work touches anomaly detection since system reasons failed attempts
players. Anomaly detection concerns revealing segments data
way violate expectations. excellent survey subject, refer reader
results Chandola, Banerjee, Kumar (2009). realm anomaly detection within peoples
activities, work Moore Essa (2001) addresses problem error detection recovery
card games involve two players recorded video. system models domain
stochastic context-free grammar achieves excellent results.
note recognizing failed attempt activity fine-grained problem
anomaly detection. failed event anomalous general.7 Rather, specific
distinction success failure human activities interested in. distinction lies fact unsuccessful attempt yield certain desired state whereas
successful action does. desired state exactly approach extracts activity
question. knowledge, exists prior work explicit modeling recognition
attempted activities learning intended purpose activity multi-agent setting.
One components contribution focuses joint learning inference across multiple tasks (capturing, freeing, respective attempted counterparts). contrast
traditional pipeline learning architecture, system decomposed series modules module performs partial computation passes result next stage.
main benefits set-up reduced computational complexity often higher modularity.
However, since stage myopic, may take full advantage dependencies broader
patterns within data. Additionally, even though errors introduced module may small,
accumulate beyond tolerable levels data passes pipeline.
extensive body work shown joint reasoning improves model performance
number natural language processing data mining tasks including information extraction (i.e.,
text segmentation coupled entity resolution) (Poon & Domingos, 2007), co-reference resolution (Poon & Domingos, 2008), information extraction coupled co-reference resolution (Wellner, McCallum, Peng, & Hay, 2004), temporal relation identification (Yoshikawa, Riedel, Asahara,
& Matsumoto, 2009; Ling & Weld, 2010), record de-duplication (Domingos, 2004; Culotta
& McCallum, 2005). Similarly work, models cast Markov logic.
However, prior work uses sampling techniques perform learning inference, whereas apply
7. situation player CTF moves campus speed 100 mph way passes enemy
player certainly anomalous (and probably caused GPS sensor noise), want say failed
attempt capturing.

123

fiS ADILEK & K AUTZ

reduction integer linear programming. Interestingly, work Denis Baldridge (2007)
jointly addresses problems anaphoricity co-reference via manual formulation
integer linear program.
Joint activity modeling shown yield better recognition accuracy, compared
pipeline baselines well baselines make strong inter-activity independence assumptions.
work Wu, Lian, Hsu (2007) performs joint learning inference concurrent singleagent activities using factorial conditional random field model. Similarly, work Helaoui,
Niepert, Stuckenschmidt (2010) models interleaved activities Markov logic. distinguish
foreground background activities infer time window activity takes
place RFID sensory data. contrast, focus joint reasoning multi-agent activities
attempts fully relationaland arguably significantly noisysetting.
work Manfredotti, Hamilton, Zilles (2010) propose hierarchical activity recognition
system formulated learning inference relational dynamic Bayesian networks. model
jointly leverages observed interactions individual objects domain relationships
objects. Since method outperforms hidden Markov model significant margin,
contributes additional experimental evidence relational decomposition domain improves
model quality.
work Landwehr, Gutmann, Thon, Philipose, De Raedt (2007) casts single-agent
activity recognition relational transformation learning problem, building transformationbased tagging natural language processing. system induces set transformation rules
used infer activities sensory data. Since transformation rules applied
adaptively, step, system leverages observed data, currently assigned
labels (inferred activities). However, transformation rules learned greedy fashion
experiments show model perform significantly better simple HMM.
hand, representation quite general, intuitive, extensible. see,
Markov logic model similar level representational convenience performing global
instead greedyoptimization significantly complex domain.
denoising component model formulated tracking problem. Prior work
proposed relational dynamic Bayesian network model multi-agent tracking (Manfredotti &
Messina, 2009). evaluation shows considering relationships tracked entities
significantly improves model performance, compared nonrelational particle filter baseline.
contrast, work explores joint tracking activity recognition. However, GPS reading
annotated identity corresponding agent. work Manfredotti Messina
suggests model generalized, associations GPS agent
identities inferred need observed.
Markov logic theory viewed template conditional random field (Lafferty,
2001), undirected graphical model captures conditional probability hidden labels
given observations, rather joint probability labels observations, one would
typically directed graphical model. relational world, directed formalisms include
relational Bayesian networks (Jaeger, 1997) dynamic counterparts (Manfredotti, 2009),
probabilistic relational models (Koller, 1999; Friedman, Getoor, Koller, & Pfeffer, 1999), Bayesian
logic programs (Kersting & De Raedt, 2000), first-order conditional influence language (Natarajan, Tadepalli, Altendorf, Dietterich, Fern, & Restificar, 2005). Conditional random fields
extensively applied activity recognition, superior labeling performance generative
models demonstrated number single-agent multi-agent domains (Liao
124

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

et al., 2005; Limketkai, Fox, & Liao, 2007; Vail, 2008; Vail & Veloso, 2008; Hu et al., 2008).
Since MLNs often solved propositionalized CRFs, directed alternatives compiled Bayesian network, expected discriminative relational models generally
outperform generative counterparts labeling tasks. However, work needs done
answer question entirety.
Since Markov logic based on, fact subsumes, finite first-order logic, immediately
gain access number techniques developed rich field traditional logic. Current Markov
logic solvers take advantage underlying logical structure perform powerful optimizations, Alchemys lifted inference belief propagation MC-SAT (Poon & Domingos,
2006). Additionally, domain pruning, one uses hard constraints infer reduced domains
predicates, shown lead significant speed-ups (Papai, Singla, & Kautz, 2011).
leverage relationship Markov first-order logic inducing augmented model. Furthermore, presence dependency cycles introduces additional problems
directed graphical (relational) models. Thus, fact that, Markov logic, knowledge
expressed weighted first-order formulas combined factors make powerful
framework best suited multi-agent reasoning tasks considered work.
Traditional hidden Markov models operate alphabet unstructured (i.e., flat) symbols. makes relational reasoning difficult, one either propositionalize domain,
thereby incurring combinatorial increase number symbols model parameters, ignore
relational structure sacrifice information. Logical hidden Markov models (LHMMs)
proposed address problem (Kersting, De Raedt, & Raiko, 2006). LHMMs generalization standard HMMs compactly represents probability distributions sequences
logical atoms rather flat symbols. LHMMs proven strictly powerful
propositional counterparts (HMMs). applying techniques logic-based reasoning,
unification, leveraging logical structure component model, Kersting et al. show
LHMMs often require fewer parameters achieve higher accuracy HMMs.
LHMMs recently applied activity recognition. context intelligent user interfaces, work Shen (2009) designs evaluates LHMM model recognition peoples
activities workflows carried desktop computer. researchers proposed hierarchical extension LHMMs along efficient particle filter-based inference technique,
apply activity recognition problems synthetic domains (Natarajan, Bui, Tadepalli, Kersting,
& Wong, 2008). lines work show LHMMs learned applied efficiently,
perform better plain HMMs.
However, LHMMs generative model therefore ideal pure labeling
recognition tasks, typically want make strong independence assumptions
observations, want explicitly model dependencies input space. TildeCRFa
relational extension traditional conditional random fieldshas introduced address
issue (Gutmann & Kersting, 2006). TildeCRF allows discriminative learning inference CRFs
encode sequences logical atoms, opposed sequences unstructured symbols. TildeCRF
specifically focuses efficient learning models sequential data via boosting, subsumed
Markov logic, produce discriminative generative models. cast model
latter framework make general, extensible, interpretable.
PRISM, probabilistic extension Prolog, shown subsume wide variety generative models, including Bayesian networks, probabilistic context-free grammars, HMMs (along
logical extension) (Sato & Kameya, 2001, 2008). However, since focus PRISM
125

fiS ADILEK & K AUTZ

representational elegance generality, rather scalability, sheer size state space
complexity CTF domain precludes application here.
Finally, Markov logic theory augmentation process related structure learning, transfer learning, inductive logic programming. fact, Algorithm 1 implements special case
structure learning, search target theory explains training data well,
declarative bias forces target theory differ source theory much necessary.
Again, intuition failed attempts similar failed counterparts. number
researchers focused structure learning specifically Markov logic networks. includes
early work top-down structure learning, clauses knowledge base greedily modified adding, flipping, deleting logical literals (Kok & Domingos, 2005). search guided
likelihood training data current model. work Mihalkova Mooney
(2007) exploit patterns ground Markov logic networks introduce bottom-up declarative
bias makes algorithm less susceptible finding local optima, compared alternative greedy methods. Similarly, work Kok Domingos (2009) introduce bottom-up
declarative bias based lifted hypergraph representation relational database. bias
guides search clauses fit data. Since hypergraph lifted, relational path finding
tractable. Interesting work predicate invention applies relational clustering technique formulated
second-order Markov logic discover new predicates relational databases (Kok & Domingos, 2007). systems capable modeling relatively rich family logical formulas.
approaches perform discriminative structure learning achieve excellent results, focus
restricted set types formulas (e.g., Horn clauses) (Huynh & Mooney, 2008; Biba, Ferilli, &
Esposito, 2008). work Davis Domingos (2009) successfully uses second-order Markov
logic deep transfer learning. lift model source domain second-order ML
identify high-level structural patterns. subsequently serve declarative bias structure
learning target domain.
nature, inductive logic programming discipline extensively studied structure
learning deterministic, well probabilistic settings (e.g., Muggleton, 2002; De Raedt, 2008;
De Raedt, Frasconi, Kersting, & Muggleton, 2008). fact, theory augmentation algorithm
viewed efficient Markov logic based version theory refinement, well-established ILP
technique aims improve quality theory terms simplicity, fit newly acquired
data, efficiency factors (Wrobel, 1996).
approach differs work three main points. First, declarative bias defined
implicitly seed theory successful activities. Therefore, theory augmentation algorithm
limited hard-wired set formula types consider. Rather, search space
defined run time extracting motifs seed theory. second distinction lies computational tractability exactness results. distinguishing soft hard formulas,
able search candidate formulas systematic, rather greedy manner. Consequently, final learned model requires fewer parameters, especially important
amount training data relatively small. Additionally, weight learning experience cold starts, leverage seed theory. final difference that, knowledge,
first explore structure learning context interplay success failure,
relationship intended goals peoples actions.

126

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

8. Conclusions
paper took task understanding game capture flag GPS data
exemplar general problem inferring human interactions intentions sensor data.
presented novel methodologycast Markov logicfor effectively combining data
denoising higher-level relational reasoning complex multi-agent domain. Specifically,
demonstrated given raw noisy data, automatically reliably detect
recognize successful failed interactions adversarial well cooperative settings.
Additionally, shown success, failure, goal activity intimately tied
together model successful events allows us naturally learn models
two important aspects life. Specifically, demonstrated intentions rational
agents automatically discovered process resolving inconsistencies theory
models successful instances set activities examples failed attempts activities.
formulated four research questions designed experiments within CTF domain
empirically answer them. Compared alternative approaches solving multi-agent activity recognition problem, augmented Markov logic model, takes account
relationships among individual players, relationships among activities entire length
game, although computationally costly, significantly accurate real-world data.
Furthermore, illustrated explicitly modeling unsuccessful attempts boosts performance
important recognition tasks.

9. Future Work
Multi-agent activity recognition especially interesting context current unprecedented
growth on-line social networksin terms size, popularity, impact offline lives. paper, show location information alone allows rich models peoples
interactions, case on-line social networks, additionally access content
users posts explicit implicit network interactions. instance, recent
study shows that, interestingly, 30% Twitter status updates reveal authors location
(Sadilek, Kautz, & Bigham, 2012). data sources available machines massive
volumes ever-increasing real-time streaming rate. note substantial fraction posts
services Facebook Twitter talk everyday activities users (Naaman, Boase,
& Lai, 2010), information channel become available research community
recently. Thus, able reason human behavior interactions automated
way, tap colossal amounts knowledge isat presentdistributed across whole
population.
currently extending model handle explicit GPS traces, able
infer location people broadcast GPS coordinates. basic idea is, again,
leverage structure relationships among people. vast majority us participate on-line
social networks typically friends publish location. thus view
GPS-enabled people noisy location sensors use network interactions dynamics
estimate location rest users. present, testing approach public
tweets.

127

fiS ADILEK & K AUTZ

Acknowledgments
thank anonymous reviewers constructive feedback. thank Sebastian Riedel
help theBeast, Radka Sadlkova Wendy Beatty helpful comments.
work supported ARO grant #W911NF-08-1-0242, DARPA SBIR Contract #W31P4Q08-C-0170, gift Kodak.

References
Abowd, G. D., Atkeson, C. G., Hong, J., Long, S., Kooper, R., & Pinkerton, M. (1997). Cyberguide:
mobile context-aware tour guide. Wirel. Netw., 3(5), 421433.
Allwein, E., Schapire, R., & Singer, Y. (2001). Reducing multiclass binary: unifying approach
margin classifiers. Journal Machine Learning Research, 1, 113141.
Ashbrook, D., & Starner, T. (2003). Using GPS learn significant locations predict movement
across multiple users. Personal Ubiquitous Comput., 7, 275286.
Baker, C., Tenenbaum, J., & Saxe, R. (2006). Bayesian models human action understanding.
Advances Neural Information Processing Systems, 18, 99.
Baker, C., Goodman, N., & Tenenbaum, J. (2008). Theory-based social goal inference. Proceedings thirtieth annual conference cognitive science society, pp. 14471452.
Baker, C., Saxe, R., & Tenenbaum, J. (2011). Bayesian theory mind: Modeling joint belief-desire
attribution. Proceedings Thirty-Second Annual Conference Cognitive Science
Society.
Baker, C., Tenenbaum, J., & Saxe, R. (2007). Goal inference inverse planning. Proceedings
29th annual meeting cognitive science society.
Baldwin, D. A., & Baird, J. A. (2001). Discerning intentions dynamic human action. Trends
Cognitive Sciences, 5(4), 171 178.
Barbuceanu, M., & Fox, M. (1995). COOL: language describing coordination multi
agent systems. Proceedings First International Conference Multi-Agent Systems
(ICMAS-95), pp. 1724.
Bell, R., Koren, Y., & Volinsky, C. (2007). Modeling relationships multiple scales improve
accuracy large recommender systems. KDD, pp. 95104, New York, NY, USA. ACM.
Biba, M., Ferilli, S., & Esposito, F. (2008). Discriminative structure learning Markov logic
networks.. pp. 5976. Springer.
Biswas, R., Thrun, S., & Fujimura, K. (2007). Recognizing activities multiple cues. Workshop Human Motion, pp. 255270.
Bui, H. H. (2003). general model online probabilistic plan recognition. Eighteenth International Joint Conference Artificial Intelligence (IJCAI-2003).
Busetta, P., Serafini, L., Singh, D., & Zini, F. (2001). Extending multi-agent cooperation overhearing. Cooperative Information Systems, pp. 4052. Springer.
Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: survey. ACM Comput.
Surv., 41, 15:115:58.
128

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Culotta, A., & McCallum, A. (2005). Joint deduplication multiple record types relational data.
Proceedings 14th ACM international conference Information knowledge
management, pp. 257258. ACM.
Davis, J., & Domingos, P. (2009). Deep transfer via second-order Markov logic. Proceedings
26th Annual International Conference Machine Learning, pp. 217224. ACM.
De Raedt, L. (2008). Logical relational learning. Springer-Verlag New York Inc.
De Raedt, L., Frasconi, P., Kersting, K., & Muggleton, S. (Eds.). (2008). Probabilistic Inductive
Logic Programming - Theory Applications, Vol. 4911 Lecture Notes Computer
Science. Springer.
De Raedt, L., & Kersting, K. (2008). Probabilistic inductive logic programming. (De Raedt et al.,
2008), pp. 127.
Denis, P., & Baldridge, J. (2007). Joint determination anaphoricity coreference resolution
using integer programming. Proceedings NAACL HLT, pp. 236243.
Domingos, P. (2004). Multi-relational record linkage. Proceedings KDD-2004 Workshop
Multi-Relational Data Mining.
Domingos, P., Kok, S., Lowd, D., Poon, H., Richardson, M., & Singla, P. (2008). Markov logic.
(De Raedt et al., 2008), pp. 92117.
Eagle, N., & Pentland, A. (2006). Reality mining: sensing complex social systems. Personal
Ubiquitous Computing, 10(4), 255268.
Eagle, N., & Pentland, A. (2009). Eigenbehaviors: Identifying structure routine. Behavioral
Ecology Sociobiology, 63(7), 10571066.
Eagle, N., Pentland, A., & Lazer, D. (2009). Inferring social network structure using mobile phone
data. Proceedings National Academy Sciences.
Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational models.
International Joint Conference Artificial Intelligence, Vol. 16, pp. 13001309.
Goutte, C., & Gaussier, E. (2005). probabilistic interpretation precision, recall f-score,
implication evaluation.. pp. 345359. Springer.
Gupta, A., Srinivasan, P., Shi, J., & Davis, L. S. (2009). Understanding videos, constructing plots:
Learning visually grounded storyline model annotated videos. CVPR.
Gutmann, B., & Kersting, K. (2006). TildeCRF: conditional random fields logical sequences.
Machine Learning: ECML 2006, pp. 174185. Springer.
Helaoui, R., Niepert, M., & Stuckenschmidt, H. (2010). statistical-relational activity recognition
framework ambient assisted living systems. Ambient Intelligence Future TrendsInternational Symposium Ambient Intelligence (ISAmI 2010), pp. 247254. Springer.
Hong, J. (2001). Goal recognition goal graph analysis. Journal Artificial Intelligence
Research, 15, 130.
Horvitz, E., Apacible, J., Sarin, R., & Liao, L. (2005). Prediction, expectation, surprise: Methods, designs, study deployed traffic forecasting service. Twenty-First Conference
Uncertainty Artificial Intelligence.

129

fiS ADILEK & K AUTZ

Hu, D., Pan, S., Zheng, V., Liu, N., & Yang, Q. (2008). Real world activity recognition multiple
goals. UbiComp, Vol. 8, pp. 3039.
Huynh, T., & Mooney, R. (2008). Discriminative structure parameter learning Markov
logic networks. Proceedings 25th international conference Machine learning,
pp. 416423. ACM.
Jaeger, M. (1997). Relational Bayesian networks. Proceedings 13th Conference Uncertainty Artificial Intelligence, pp. 266273.
Jordan, M. (1998). Learning graphical models. Kluwer Academic Publishers.
Kamar, E., & Horvitz, E. (2009). Collaboration shared plans open world: Studies
ridesharing. IJCAI.
Kaminka, G. A., Tambe, D. V. P. M., Pynadath, D. V., & Tambe, M. (2002). Monitoring teams
overhearing: multi-agent plan-recognition approach. Journal Artificial Intelligence
Research, 17, 2002.
Kersting, K., & De Raedt, L. (2000). Bayesian logic programs. Proceedings Work-inProgress Track 10th International Conference Inductive Logic Programming.
Kersting, K., De Raedt, L., & Raiko, T. (2006). Logical hidden Markov models. Journal Artificial
Intelligence Research, 25(1), 425456.
Kok, S., & Domingos, P. (2005). Learning structure Markov logic networks. Proceedings
22nd international conference Machine learning, pp. 441448. ACM.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. Proceedings 24th international conference Machine learning, pp. 433440. ACM.
Kok, S., & Domingos, P. (2009). Learning Markov logic network structure via hypergraph lifting.
Proceedings 26th Annual International Conference Machine Learning, pp. 505512.
ACM.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. ICML 07: Proceedings
24th international conference Machine learning, pp. 433440, New York, NY, USA.
ACM.
Koller, D. (1999). Probabilistic relational models. Inductive Logic Programming, pp. 313.
Springer.
Lafferty, J. (2001). Conditional random fields: Probabilistic models segmenting labeling
sequence data. International Conference Machine Learning (ICML), pp. 282289.
Morgan Kaufmann.
Landwehr, N., Gutmann, B., Thon, I., Philipose, M., & De Raedt, L. (2007). Relational
transformation-based tagging human activity recognition. Proceedings 6th International Workshop Multi-relational Data Mining (MRDM07), pp. 8192.
Liao, L., Patterson, D., Fox, D., & Kautz, H. (2007). Learning inferring transportation routines.
Artificial Intelligence, 171(5-6), 311331.
Liao, L., Fox, D., & Kautz, H. (2004). Learning inferring transportation routines. Proceedings Nineteenth National Conference Artificial Intelligence.

130

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Liao, L., Fox, D., & Kautz, H. (2005). Location-based activity recognition using relational Markov
networks. IJCAI.
Limketkai, B., Fox, D., & Liao, L. (2007). CRF-filters: Discriminative particle filters sequential
state estimation. Robotics Automation, 2007 IEEE International Conference on, pp.
31423147.
Ling, X., & Weld, D. (2010). Temporal information extraction. Proceedings Twenty Fifth
National Conference Artificial Intelligence.
Ma, Z. (2008). Modelling PRISM intelligent system. MSc. Thesis, Linacre College, University Oxford.
Manfredotti, C. (2009). Modeling inference relational dynamic Bayesian networks.
Advances Artificial Intelligence, pp. 287290. Springer.
Manfredotti, C., & Messina, E. (2009). Relational dynamic Bayesian networks improve multitarget tracking. Advanced Concepts Intelligent Vision Systems, pp. 528539. Springer.
Manfredotti, C., Hamilton, H., & Zilles, S. (2010). Learning RDBNs activity recognition.
Neural Information Processing Systems.
Mihalkova, L., & Mooney, R. (2007). Bottom-up learning Markov logic network structure.
Proceedings 24th international conference Machine learning, pp. 625632. ACM.
Moore, D., & Essa, I. (2001). Recognizing multitasked activities using stochastic context-free grammar. Proceedings AAAI Conference.
Muggleton, S. (2002). Learning structure parameters stochastic logic programs. Proceedings 12th international conference Inductive logic programming, pp. 198206.
Springer-Verlag.
Murphy, K. P. (2002). Dynamic bayesian networks: representation, inference learning. Ph.D.
thesis, University California, Berkeley.
Naaman, M., Boase, J., & Lai, C.-H. (2010). really me?: message content social
awareness streams. CSCW 10: Proceedings 2010 ACM conference Computer
supported cooperative work, pp. 189192, New York, NY, USA. ACM.
Natarajan, S., Tadepalli, P., Altendorf, E., Dietterich, T., Fern, A., & Restificar, A. (2005). Learning
first-order probabilistic models combining rules. Proceedings 22nd international conference Machine learning, pp. 609616. ACM.
Natarajan, S., Bui, H. H., Tadepalli, P., Kersting, K., & Wong, W. (2008). Logical hierarchical
hidden Markov models modeling user activities. Proc. ILP-08.
Papai, T., Singla, P., & Kautz, H. (2011). Constraint propagation efficient inference Markov
logic. Seventeenth International Conference Principles Practice Constraint
Programming.
Pentland, A. S. (2008). Honest Signals: Shape World. MIT Press.
Poon, H., & Domingos, P. (2006). Sound efficient inference probabilistic deterministic
dependencies. Proceedings National Conference Artificial Intelligence, Vol. 21,
p. 458. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.

131

fiS ADILEK & K AUTZ

Poon, H., & Domingos, P. (2007). Joint inference information extraction. Proceedings
22nd national conference Artificial intelligence-Volume 1, pp. 913918. AAAI Press.
Poon, H., & Domingos, P. (2008). Joint unsupervised coreference resolution Markov logic.
Proceedings Conference Empirical Methods Natural Language Processing, pp.
650659. Association Computational Linguistics.
Riedel, S. (2008). Improving accuracy efficiency map inference Markov logic.
Proceedings Proceedings Twenty-Fourth Conference Annual Conference Uncertainty Artificial Intelligence (UAI-08), pp. 468475, Corvallis, Oregon. AUAI Press.
Sadilek, A., & Kautz, H. (2010a). Modeling reasoning success, failure, intent
multi-agent activities. Mobile Context-Awareness Workshop, Twelfth ACM International
Conference Ubiquitous Computing.
Sadilek, A., & Kautz, H. (2010b). Recognizing multi-agent activities GPS data. TwentyFourth AAAI Conference Artificial Intelligence.
Sadilek, A., Kautz, H., & Bigham, J. P. (2012). Finding friends following
are. Fifth ACM International Conference Web Search Data Mining (WSDM).
Sato, T., & Kameya, Y. (2001). Parameter learning logic programs symbolic-statistical modeling. Journal Artificial Intelligence Research.
Sato, T., & Kameya, Y. (2008). New advances logic-based probabilistic modeling PRISM.
Probabilistic inductive logic programming, pp. 118155. Springer.
Shakarian, P., Subrahmanian, V., & Spaino, M. L. (2009). SCARE: Case Study Baghdad.
Proceedings Third International Conference Computational Cultural Dynamics.
AAAI.
Shen, J. (2009). Activity recognition desktop environments. Ph.D. Thesis, Oregon State University.
Shoenfield, J. R. (1967). Mathematical Logic. Addison-Wesley.
Singla, P., & Domingos, P. (2005). Discriminative training Markov logic networks. Proceedings National Conference Artificial Intelligence, Vol. 20, p. 868. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press; 1999.
Singla, P., & Domingos, P. (2007). Markov logic infinite domains. UAI-07.
Tang, K., Lin, J., Hong, J., Siewiorek, D., & Sadeh, N. (2010). Rethinking location sharing: exploring implications social-driven vs. purpose-driven location sharing. Proceedings
12th ACM international conference Ubiquitous computing, pp. 8594. ACM.
Tran, S., & Davis, L. (2008). Visual event modeling recognition using Markov logic networks.
Proceedings 10th European Conference Computer Vision.
Ullman, T., Baker, C., Macindoe, O., Evans, O., Goodman, N., & Tenenbaum, J. (2010). Help
hinder: Bayesian models social goal inference. Advances Neural Information
Processing Systems (NIPS), Vol. 22.
Vail, D. (2008). Conditional random fields activity recognition. Ph.D. Thesis, Carnegie Mellon
University.

132

fiL OCATION -BASED R EASONING C OMPLEX ULTI -AGENT B EHAVIOR

Vail, D., & Veloso, M. (2008). Feature selection activity recognition multi-robot domains.
Proceedings AAAI, Vol. 2008.
Wang, J., & Domingos, P. (2008). Hybrid Markov logic networks. Proceedings 23rd
national conference Artificial intelligence - Volume 2, pp. 11061111. AAAI Press.
Wellner, B., McCallum, A., Peng, F., & Hay, M. (2004). integrated, conditional model information extraction coreference application citation matching. Proceedings
20th conference Uncertainty artificial intelligence, pp. 593601. AUAI Press.
Wilson, A., Fern, A., Ray, S., & Tadepalli, P. (2008). Learning transferring roles multi-agent
mdps. Proceedings AAAI.
Wilson, A., Fern, A., & Tadepalli, P. (2010). Bayesian role discovery multi-agent reinforcement learning. Proceedings 9th International Conference Autonomous Agents
Multiagent Systems: volume 1-Volume 1, pp. 15871588. International Foundation
Autonomous Agents Multiagent Systems.
Wrobel, S. (1996). First order theory refinement. Advances inductive logic programming, pp.
1433. IOS Press, Amsterdam.
Wu, T., Lian, C., & Hsu, J. (2007). Joint recognition multiple concurrent activities using factorial
conditional random fields. Proc. 22nd Conf. Artificial Intelligence (AAAI-2007).
Yoshikawa, K., Riedel, S., Asahara, M., & Matsumoto, Y. (2009). Jointly identifying temporal relations Markov logic. Proceedings Joint Conference 47th Annual Meeting
ACL 4th International Joint Conference Natural Language Processing
AFNLP: Volume 1-Volume 1, pp. 405413. Association Computational Linguistics.

133


