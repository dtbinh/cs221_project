Journal Artificial Intelligence Research 43 (2012) 1-42

Submitted 08/11; published 01/12

Learning Reasoning Action-Related Places
Robust Mobile Manipulation
Freek Stulp

stulp@clmc.usc.edu

Computational Learning Motor Control Lab
University Southern California
3710 S. McClintock Avenue, Los Angeles, CA 90089, USA

Andreas Fedrizzi
Lorenz Mosenlechner
Michael Beetz

fedrizza@cs.tum.edu
moesenle@cs.tum.edu
beetz@cs.tum.edu

Intelligent Autonomous Systems Group
Technische Universitat Munchen
Boltzmannstrae 3, D-85747 Garching bei Munchen, Germany

Abstract
propose concept Action-Related Place (ARPlace) powerful flexible representation task-related place context mobile manipulation. ARPlace
represents robot base locations single position, rather collection positions, associated probability manipulation action succeed
located there. ARPlaces generated using predictive model acquired
experience-based learning, take account uncertainty robot
location location object manipulated.
executing task, rather choosing one specific goal position based
initial knowledge task context, robot instantiates ARPlace,
bases decisions ARPlace, updated new information
task becomes available. show advantages least-commitment approach,
present transformational planner reasons ARPlaces order optimize
symbolic plans. empirical evaluation demonstrates using ARPlaces leads
robust efficient mobile manipulation face state estimation uncertainty
simulated robot.

1. Introduction
Recent advances design robot hardware software enabling robots solve
increasingly complex everyday tasks. performing tasks, robot must continually decide course action, decision commitment plan
action parameterization based evidence expected costs benefits associated
outcome. (Resulaj, Kiani, Wolpert, & Shadlen, 2009). definition highlights
complexity decision making. involves choosing appropriate action action
parameterization, costs minimized benefits maximized. robot
must therefore able predict costs benefits arise executing
action. Furthermore, due stochasticity hidden state, exact outcome action
known advance. robot must therefore reason expected outcomes,
able predict probability different outcomes given action action paramec
2012
AI Access Foundation. rights reserved.

fiStulp, Fedrizzi, Mosenlechner, & Beetz

terization. Finally, robot commits decisions based current observable evidence,
represented belief state. evidence changes, rationale committing
decision may longer valid. robot therefore needs methods efficiently reconsider decisions belief state changes action execution, possibly commit
another plan necessary.
Mobile manipulation good case point. Even basic mobile manipulation
tasks, picking object table, require complex decision making. pick
object robot must decide stand order pick object,
hand(s) use, reach it, grasp type apply, grasp, much
grasp force apply, lift object, much force apply lift it,
hold object, hold it. decision problems complex depend
specific task context, consists many task-relevant parameters. Furthermore,
decisions must continually updated verified, task context, robots
knowledge context, often changes task execution.
Consequently, tasks complexity require robust hardware low-level
controllers, least-commitment approach making decisions, abstract planning
capabilities, probabilistic representations, principled ways updating beliefs
task execution. article, demonstrate implementing core AI topics
contributes robustness flexibility mobile manipulation platform.
task-relevant decision consider article base position robot
navigate order perform manipulation action. decision alone presents
several challenges, 1) successfully executing reaching manipulation action
critically depends position base; 2) due imperfect state-estimation,
uncertainty position robot target object. positions
known exactly, fundamental successfully grasping object, possible
determine single-best base position manipulation; 3) complete knowledge required
determine appropriate base position often available initially, rather acquired
on-line task execution.
solution idea address challenges concept Action-Related Places
(ARPlace), powerful flexible representation task-related place context
mobile manipulation. ARPlace represented probability mapping, specifies
expected probability target object successfully grasped, given positions
target object robot

ARPlace : { P (Success|fkrob , hf obj , obj i) }K
k=1

(1)

Here, estimated position target object represented multi-variate Gaussian
distribution mean f obj covariance matrix obj 1 . discrete set robot positions
{fkrob }K
k=1 thought possible base positions robot considers grasping,
i.e. potential positions navigate to. Typically, set positions arranged grid,
exemplary ARPlace depicted Figure 1.
1. feature vectors f rob f obj contain poses robot object relative tables
edge. Details given Section 3.1. Positions without uncertainty denoted f , estimated
positions uncertainty hf , i.

2

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Figure 1: ARPlace: probability successful manipulation, given current estiobj
rob K
, obj
mated object position hfcur
cur i. set potential robot positions {fk }k=1
arranged grid along x y-axis, whereby position leads
obj
, obj
different probability successful grasping P (Success|fkrob , hfcur
cur i).
black isolines represent grasp success probability levels 0.2 0.8.

ARPlace three important properties: 1) models base places single
position f rob , rather set positions {fkrob }K
k=1 , different expectation
success manipulation action; 2) depends upon estimated target object
position, updating f obj obj task execution thus leads different probabilities
ARPlace; 3) using probabilistic representation takes account uncertainty
target object position leads robust grasping.
1.1 Example Scenario
Figure 2, present example scenario demonstrates properties
ARPlace address challenges stated above, supports decision-making
mobile manipulation. images top row show current situation robot
outside view, images lower visualize robots internal ARPlace
representation. ARPlace visualized colors red, white green,
represent low, medium high grasp success probabilities respectively. Grasp success
probability levels 0.2 0.8 depicted isolines, Figure 1.
scenario robots task clean table. Scene 1 robot enters
kitchen vision system detects cup. robot far away cup,
uncertainty arising vision-based pose estimation cup high, indicated
large circle around cup lower left image. exact position cup
known, possible determine single-best base position grasping
cup. ARPlace representation takes uncertainty account modeling
base position probability mapping.
3

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Scene 1

Scene 2

Scene 3

Scene 4

Figure 2: Example scenario.
Scene 1 ARPlace distribution low probabilities overall, maximum
probability grasp success 0.52. Note although initial uncertainty
cups position precludes robot determining specific base position
reliably grasp cup, robot know general area navigate.
navigation, robot able determine position cup accurately,
depicted Scene 2. new sensor data comes in, robot refines ARPlace
therefore ARPlace Scene 2 much higher probabilities overall, maximum
0.96.
Scene 3 robot detected second cup. grasping cups
single position much efficient approaching two locations, robot
merges two ARPlaces cup one ARPlace representing probability
successfully grasping cups single position2 . Scene 4 measurements
helped reduce pose estimation uncertainties cups. maximum grasp success
probability ARPlace reaches 0.97; sufficient robot commit
goal position attempt grasp cups once.
scenario illustrates real-world tasks often planned start
finish, initial knowledge often complete accurate enough determine
optimal goal position. rather committing particular base position early
based robots initial knowledge task context, robot instantiates
ARPlace particular task context, bases decisions ARPlace.
place concept instantiation represented explicitly course action
enables robot reconsider reevaluate decisions on-line whenever new
information task context comes in. instance, decision grasp
cups one position Scene 3 would possible robot would
committed plan given initial knowledge one cup detected. Even
environment completely observable, dynamic properties make pre-planned
optimal position suboptimal unaccessible. least-commitment implementation,
2. Section 4.4.1 explains ARPlaces merged compute ARPlace joint tasks.

4

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

decisions delayed must taken flexible, leads robust
efficient mobile manipulation. demonstrated empirical evaluation.
1.2 Contributions, System Overview Outline
system overview learning, computing, reasoning ARPlaces depicted
Figure 3. serves outline rest article.

Figure 3: System Overview. Numbers refer sections article. Green ovals represent
algorithms procedures, blue rectangles models result them.
Procedures models briefly described contributions section;
detail given throughout article. Yellow rectangles cluster conceptually
related procedures, delineate different sections article.

main contributions article are:
Representing ARPlace Section 1. propose ARPlace flexible representation
place least-commitment decision making mobile manipulation.
Model Learning Section 3. generate ARPlace, robot must able
predict outcome action given action parameterization. propose
generic, off-line learning approach acquiring compact prediction model two
steps: 1) learn predict whether action succeed given task parameterization. supervised classification problem implement Support
Vector Machines (Sonnenburg, Raetsch, Schaefer, & Schoelkopf, 2006); 2) generalize
several task parameterizations generalizing learned SVM classifiers,
5

fiStulp, Fedrizzi, Mosenlechner, & Beetz

implement Point Distribution Models (Cootes, Taylor, Cooper, & Graham, 1995). resulting success prediction model enables robot predict
whether manipulation action given object position succeed given
base position 3 .
Generating ARPlace Section 4. demonstrate ARPlaces generated online, take object position uncertainty account Monte-Carlo simulation. Furthermore, ARPlace conditioned robot position uncertainty,
thus taken account.
Reasoning ARPlace Section 5. show ARPlace integrated symbolic transformational planner, automate decision-making ARPlaces.
particular, consider scenario shows ARPlaces merged joint
manipulation tasks.
Empirical Evaluation Section 6. demonstrate reasoning ARPlaces
leads robust efficient behavior simulated mobile manipulation platform.
turning contributions, first compare approach related work
Section 2.

2. Related Work
state-of-the-art mobile manipulation platforms use sampling-based motion planners
solve manipulation problems (LaValle, 2006). advantages using symbolic
planning general, ARPlaces particular, are: 1. Abstraction. Representing planning abstract symbolic actions reduces complexity planning
problem. Although computational power ever increasing, still intractable solve
extended tasks, preparing meal (Beetz et al., 2008), state-space search alone.
2. Least-commitment. Friedman Weld (1996) show setting open conditions
abstract actions later refining choice particular concrete action lead exponential savings. Note principle used reduce number collision
checks building Probabilistic Roadmaps (Bohlin & Kavraki, 2000). 3. Modular replanning. symbolic planning, causal links actions explicitly represented.
robot navigates table order perform grasping motion represented
plan generated sampling-based motion planner. Therefore, execution, motion
planners cannot reconsider appropriate base position decision right,
must rather inefficiently replan entire trajectory belief state changes. 4. Reflection. explicit symbolic representation causality allows robot reason
reflect plans monitor execution, instance report reasons
3. using experience-based learning, approach applied variety robots environments.
model learned however, obviously specific environment experience
generated. instance, one table height considered data collection, learned
prediction model specific table height. different table heights used experience
collection, table height included task-relevant parameter, model able
generalize table heights well. refer Section 3.5 full discussion.

6

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

plan failure: could find cup. could determine position cup
sufficient accuracy robustly perform grasp. obstacle blocking
path.. obvious achieve introspection motion planning methods.
Also, contrast sampling based motion-planning, ARPlace generate trajectories motion itself, rather representation supports decisions,
decision robot move order manipulate. goal position
given motion planner order find trajectory gets robot
goal position. system instance, navigation trajectory determined
Wavefront planner. Also, ARPlace Reinforcement Learning policy (Sutton
& Barto, 1998). policy maps states actions, whereas ARPlace maps (uncertain)
states expected probabilities successfully executing certain action. ARPlaces
thus models actions, executable actions right. distinction
become apparent Section 5, ARPlaces used transformational
planner detect repair performance flaws symbolic plans.
perspective, similar work aSyMov (Cambon, Gravot, &
Alami, 2004) RL-TOPs (Ryan, 2002), use symbolic planners generate
sequences motion plans/reinforcement learning policies respectively. specific contributions article enable robot learn grounded, probabilistic models
actions support symbolic decision making, well using flexible transformational
planners reason models. focus thus grounding improving
representations enable symbolic planning, rather underlying actions
generate trajectories and/or actual motion.
Okada, Kojima, Sagawa, Ichino, Sato, Inaba (2006) develop representations
place enable symbolic planning, denote good base placement grasping
spot. Different spots hand-coded different tasks, manipulating faucet,
cupboard, trashcan. symbolic representations place used
LISP-based motion planner perform tool manipulation behavior. ARPlace extends
concept spot learning autonomously, grounding observed behavior,
providing probabilistic representation place. Berenson, Choset, Kuffner (2008)
address issue finding optimal start goal configurations manipulating objects
pick-and-place operations. explicitly take placement mobile base
account. interested optimal start goal configurations, instead
probabilistic representation, approach enable least-commitment planning.
Diankov, Ratliff, Ferguson, Srinivasa, Kuffner (2008) use model reachable
workspace robot arm decide robot may stand grasp object
focus search. However, uncertainties robots base position objects position
considered, thus cannot compensated for. recent work Berenson,
Srinivasa, Kuffner (2009) addresses issues, still relies accurate model
environment, high computational cost. hand, ARPlace
compact representation computed negligible computational load, allowing
continuous updating.
Recently, similar methods ones presented article used determine successful grasps, rather base positions grasping. instance, Detry et al.
(2009) determine probability density function represents graspability specific
objects. function learned samples successful robot grasps, biased
7

fiStulp, Fedrizzi, Mosenlechner, & Beetz

observed human grasps. However, approach take examples failed grasps
account. shall see Section 4, distance failed successful
grasp quite small, determined taking failed grasps account.
classification boundaries Section 3.2 similar Workspace Goal Regions, except
boundaries refer base positions, whereas Workspace Goal Regions refer grasp
positions (Berenson, Srinivasa, Ferguson, Romea, & Kuffner, 2009). Also, generalize
boundaries Point Distribution Model, use generate probabilistic
concept successful grasp positions.
Kuipers, Beeson, Modayil, Provost (2006) present bootstrapping approach
enables robots develop high-level ontologies low-level sensor data including distinctive states, places, objects, actions. high level states used choose
trajectory-following control laws move one distinctive state another. approach exactly way around: given manipulation navigation skills
robot (which far high-dimensional learn trajectory-following control laws),
learn places skills (e.g. grasping) executed successfully. focus
action affordance, recognition localization. us, place means cluster
locations execute (grasping) skill successfully, whereas Kuipers
et al. refers location perceptually distinct others, therefore
well-recognized. Furthermore, work yet considered physical manipulation
objects, relates place.
Learning success models considered probabilistic pre-condition learning.
research field far focussed learning symbolic predicates symbolic
examples (Clement, Durfee, & Barrett, 2007; Chang & Amir, 2006; Amir & Chang, 2008).
approaches applied robots, representations
learned able encapsulate complex conditions arise robot dynamics
action parameterization. robotics, focus pre-condition learning therefore
rather grounding pre-conditions robot experience. realistic domain considered Zettlemoyer, Pasula, Kaelbling (2005), simulated gripper stacks objects
blocks world. Here, focus predicting possible outcomes actions completely observable, unambiguous description current state; emphasis rather
taking state estimation uncertainty account. Dexter learns sequences manipulation
skills searching grasping object (Hart, Ou, Sweeney, & Grupen, 2006).
Declarative knowledge length arm learned experience. Learning
success models done context robotic soccer, instance learning
success rate passing (Buck & Riedmiller, 2000), approaching ball (Stulp & Beetz,
2008). system extends approaches explicitly representing regions
successful instances observed, computing Generalized Success Model
regions.
interesting line research shares paradigms ARPlaces,
learning relation objects actions building prediction models, ObjectAction Complexes (OACs). Geib, Mourao, Petrick, Pugeault, Steedman, Kruger,
Worgotter (2006) Pastor, Hoffmann, Asfour, Schaal (2009) present OACs
used integrate high-level artificial intelligence planning technology continuous low-level robot control. work stresses that, cognitive agent, objects
actions inseparably intertwined therefore paired single interface.
8

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

physically interacting world applying machine learning techniques, OACs allow
acquisition high-level action representations low-level control representations.
OACs meant generalize principle affordances (Gibson, 1977).

3. Learning Generalized Success Model ARPlace
section, describe implementation off-line phase depicted Figure 3,
Generalized Success Model (GSM) learned. goal acquire function g
P (Success|f rob , f obj )

=

g(f rob , f obj ) 7 {0, 1}

(2)

predicts chance successful manipulation action, given relative positions
robot object, stored feature vectors f rob f obj respectively.
Note off-line learning phase, known positions. Uncertainty
positions taken account on-line phase, described Section 4.
Performing mobile manipulation complex task involves many hardware
software modules. overview modules platform described Appendix A.
overview demonstrates large number modules required implement mobile
manipulation platform. Many modules results years
decades research development within companies, research groups, open-source
projects. global behavior robot, e.g. whether grasp cup certain
base position, depends modules, interactions them.
cases, analytic models certain modules available (such Capability Map
arms workspace, Section 3.1). However, general way composing models
acquire global model systems behavior task execution. Therefore,
rather learn model observed experience.
However, component computes ARPlaces requires exactly global model
predict circumstances manipulation action fail succeed. attempting theoretical analysis model foreseeable events uncertainties world
best tedious error-prone, worst infeasible, therefore use experience-based
learning acquire global models behavior. so, model grounded
observation actual robot behavior.
off-line learning phase consists three steps: 1) repeatedly execute action
sequence observe result N different target object positions; 2) learn N Support
Vector Machines classifiers N specific cup positions 3) generalize N classifiers
Point Distribution Model.
3.1 Data Acquisition
robot acquires experience executing following action sequence: 1) navigate
specified base position table; 2) reach cup; 3) close gripper; 4) lift
cup (Stulp, Fedrizzi, & Beetz, 2009a). action sequence, task context
determined following parameters 1) pose robot navigates to4 ; 2)
4. Note navigation planner parameterized robot always directly facing table.
limitation planner, rather constraint added make behavior

9

fiStulp, Fedrizzi, Mosenlechner, & Beetz

pose target object table. execution, robot logs whether object
successfully grasped not. efficiently acquire sufficient data, perform training
experiments Gazebo simulator (Gerkey, Vaughan, & Howard, 2003). robot
modeled accurately, thus simulator provides training data valid
real robot. Examples failed successful grasp depicted Figure 4.

Figure 4: Two experiment runs different samples robot position. navigatereach-grasp sequence upper row succeeds, fails lower sequence.

vector field controller use perform reaching movement proven
robust wide range robots workspace (Beetz et al., 2010).
low computational load, easy debug, quickly adapted novel objects.
disadvantage occasionally gets stuck local minima, motion
must restarted. probabilistic motion planner arm suffer local
minima, plan generation fails border workspace; even though vector
field controller able grasp there. Every planner controller advantages
disadvantages, always sources failure real world, especially
complex embodied agents. article aims modelling failures experiencebased learning, basing decisions models.
feature space data collected depicted Figure 5. coordinate
system relative tables edge, position cup table.
enable us apply model learned data different tables
different locations kitchen, contrast previous work (Stulp, Fedrizzi, &
Beetz, 2009b). on, refer f obj = [xobj obj ] observable taskrelevant parameters, robot observes cannot influence directly. xobj
distance object table edge, obj angle object
orientation normal goes table edge object, depicted
physical robot predictable; makes robot safe, required operate
robot human environments (cf. Figure 20). principle, methods paper could take
orientation account.

10

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Figure 5. f rob = [xrob rob ] controllable action parameters, robot
use navigation system change them.

Figure 5: Relative feature space used rest paper.
robot gathers data 16 target object poses, depicted Figure 6. target
object poses listed matrix Fobj . given target object position, determine
rectangular area generous estimation upper bound
robot grasp object. rectangle 16 object poses. Within
rectangle uniform grid almost 200 positions, stored matrix Frob ,
defined. Figure 6 depicts results data gathering positions. Here,
markers represent position robot base table. three types
markers, represent following classes:
3.1.1 Theoretically Unreachable (Light Round Markers)
cup cannot grasped many positions bounding rectangle simply
arm long enough. formally, base positions, kinematics
arm inverse kinematic solution exists end-effector
position required grasp target object. exploit analytic models arm
kinematics filter base positions bounding rectangle cup theoretically unreachable. analytic model use capability map, compiled
representation robots kinematic workspace (Zacharias, Borst, & Hirzinger, 2007).
Capability maps usually used answer question: given position base,
positions reach end-effector? article, use capability map
answer inverse question: given position target object (and therefore
desired position end-effector), base positions reach end-effector
position? Figure 7, answer question visualized specific target object
position. depicted area theoretical kinematic upper bound base positions
robot reach target.
example base position bounding box, use capability map
determine target object theoretically reachable. not, corresponding base
position labeled failure without executing navigate-reach-grasp action sequence.
saves time gathering data. Another obvious theoretical bound implemented
robots distance table least big robots radius.
Otherwise robot would bump table. Again, labeled base positions
failures without executing order save time.
11

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 6: Results data acquisition 16 target object poses, listed matrix Fobj .
Markers correspond center robot base. Green squares red
circles represent successful failed grasps respectively. Bright circles
executed successful grasp deemed theoretically impossible capability
maps. dark green hulls classification boundaries (Section 3.2).

3.1.2 Practically Unreachable (Red Filled Round Markers)
capability map considers theoretical reachability position, given kinematics robots arm. take self-collisions account, constraints
imposed vector-field controller reaching, specific hardware gripper,
way gripper interacts target object. Red markers Figure 6 represent
base positions capability map deems possible, lead failure performing reaching motion. causes failure are: 1) bumping table due
imprecision navigation routine 2) bumping cup grasping it; 3) closing
gripper without cup handle it; 4) cup slipping gripper 5)
vector field controller getting caught local minimum.
One aim article demonstrate practical problems, arise
interaction many hard- software modules, properly addressed experiencebased learning. approach use analytic models available, use
experience-based learning necessary. interacting world, robot observes
12

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Figure 7: Inverse capability map right arm specific object position.
global behavior, learns difference possible theory
works practice.
3.1.3 Reachable (Green Square Markers)
base positions robot able successfully grasp cup.
task execution deemed successful cup 10cm table
action sequence completed, case robot holding
it. prefer empirical measure instance force-closure measure,
latter requires accurate models object, always have. Furthermore,
argued theoretical grounds (Zheng & Qian, 2005), well demonstrated
empirically (Morales, Chinellato, Fagg, & del Pobil, 2004), force-closure grasps may
always lead successful grasps practice. course, force-closure may well
used measure successful grasping; methods described article
depend upon design choice.
data acquisition yields set discrete robot object positions, associated
resulting outcome manipulation action, success failure:
obj N
P (Success|{f rob }M
}j=1 ) = bi,j , bi,j {0, 1}
i=1 , {f

(3)

article, number sampled object positions N = 4 4 = 16 (i.e. number
graphs Figure 6), number sampled robot positions = 11 17 = 187 (i.e.
number data points per graph Figure 6).
remainder section, first generalize discrete robot positions
training Support Vector Machines (Section 3.2), generalize N cup
positions Point Distribution Models (Section 3.3)
3.2 Generalization Robot Positions
step, generalize discrete robot positions, acquire compact boolean
classifier efficiently predicts whether manipulation succeed:
13

fiStulp, Fedrizzi, Mosenlechner, & Beetz

P (Success|f rob , {f obj }N
j=1 )

=

gj=1...N (f rob ) 7 {0, 1}

(4)

generalization implemented follows. separate classifier gi=1...N learned
N = 16 object poses, i.e. one classifier 16 data sets depicted
Figure 4. acquire prediction models, compute classification boundary around
successful samples Support Vector Machines (SVM), using implementation
Sonnenburg et al. (2006), Gaussian kernel =0.1 cost parameter C=40.0.
successful grasps rarer, weight twice much failed grasp attempts.
Figure 6 depicts resulting classification boundaries different configurations taskrelevant parameters dark-green boundaries. Manipulation predicted succeed
robots base position lies within boundary given target object pose Fobj .
accuracy learned classifiers listed Section 6.1.
3.3 Generalization Object Positions
next step, generalize discrete object positions:
P (Success|f rob , f obj )

=

g(f rob , f obj ) 7 {0, 1}

(5)

determining low-dimensional set parameters allows us interpolate
individual classification boundaries Support Vector Machines generate. done Point Distribution Model (PDM), established method
modelling variations medical images faces (Cootes et al., 1995; Wimmer, Stulp,
Pietzsch, & Radig, 2008). result one compact model incorporates individual boundaries, able interpolate make predictions target object poses
observed training.
input PDM requires n points distributed contour.
landmarks distributed described Appendix B. Given landmarks classification boundaries, compute PDM. Although PDMs well-known
use computer vision (Cootes et al., 1995; Wimmer et al., 2008), use notation
Roduit, Martinoli, Jacot (2007), focus robotic applications. First, 16
boundaries 20 2D points merged one 40x16 matrix H, columns
concatenation xrob rob coordinates 20 landmarks along classification boundary. column thus represents one boundary. next step compute
P, matrix eigenvectors covariance matrix H. P represents
principal modes variation. Given H P, decompose boundary h1..16
set mean boundary linear combination columns P follows
hk = H + P bk . Here, bk so-called deformation mode k th boundary.
Point Distribution Model. get intuition PDM represents,
first three deformation modes depicted Figure 8, values first, second
third deformation modes (columns 1, 2, 3 B) varied maximal
minimal value, whilst deformation modes set 0.
eigenvalues covariance matrix H indicate first 2 components already contain 96% deformation energy. reasons compactness achieve
14

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Figure 8: first 3 deformation modes Point Distribution Model (in B).

better generalization, use first 2 deformation modes, without losing much accuracy.

{fjobj , gj (f rob ) = inboundary(f rob , hj )}N
j=1
{fjobj , gj (f rob ) = inboundary(f rob , H + P bj )}N
j=1
g(f

rob

,f

obj

) = inboundary(f

rob

, H + P b(f

obj

))

N Support Vector Machines

(6)

Point Distribution Model

(7)

Regression

fjobj

bj

(8)

PDM several advantages: 1) instead store N = 16 classification
boundaries hj 20 2D points capture variation classification hulls due
different target object positions, store N = 16 deformation modes 2 degrees
freedom each. greatly reduces dimensionality; 2) 2 degrees freedom b
used interpolate principled way computed classification boundaries
hj , generate boundaries object positions observed learning; 3)
simple regression two degrees freedom PDM b position f obj
feasible, object position related directly shape classification
boundary. regression explained next section.
3.4 Relation Task-Relevant Parameters
step, acquire function b, computes appropriate deformation modes b
given object position f obj . so, compute regression matrix
deformation modes specific object positions B, 16 object positions
Fobj , depicted Figure 6. found simple second order polynomial regression
model suffices compute regression, yields high coefficients determination
R2 = 0.99 R2 = 0.96 first second deformation modes respectively.
coefficients polynomial model stored two 3x3 upper triangular matrices W1
W2 , B [ diag([T 1] W1 [Fobj 1]T ) diag([T 1] W2 [T 1]T ]
Generalized Success Model consists 1) H, mean classification
boundaries computed SVM; 2) P, principal modes variation clas15

fiStulp, Fedrizzi, Mosenlechner, & Beetz

sification boundaries; 3) W1,2 , mapping task-relevant parameters deformation
modes.
Let us summarize Generalized Success Model used predict successful
manipulation behavior:
1. Generalized Success Model takes (observed) relative position object
obj
obj
table fcur
= [xobj
cur cur ] input (Figure 5).
2. appropriate deformation values given object position computed
obj
obj
obj
bcur = b(fcur
) = [ q W1 qT q W2 qT ], q = [fcur
1] = [xobj
cur cur 1]
(Section 3.4).
3. boundary computed hcur = H + P bcur (Section 3.3).
4. relative robot base center f rob = [xrob rob ] within boundary hcur ,
model predicts robot able successfully grasp object position
obj
obj
= [xobj
fcur
cur cur ].
Note steps involve simple multiplications additions small matrices,
thus performed efficiently5 . reason efficiency lies fact
directly relate task-relevant parameters, position cup table,
predictions global behavior robot, whether manipulation action
succeed not. on-line efficiency made possible experience-based learning,
wealth information observation global behavior compiled
compact model off-line. approach adheres proposed strategy learning taskrelevant features map actions, instead attempting reconstruct detailed model
world plan actions (Kemp, Edsinger, & Torres-Jara, 2007).
summary, observed behavior outcomes, learned mapping Equation 2 (which repeat Equation 9), maps continuous robot target object
positions boolean prediction success action:
P (Success|f rob , f obj ) = g(f rob , f obj ) 7 {0, 1}

(9)

mapping used predict current base position robot lead
successful manipulation, determine appropriate base positions navigate to.
Equation 9 assumes true values robot target object positions
known robot. Section 4, discuss uncertainties estimates
positions taken account task execution.
3.5 Generality Generalized Success Model
explaining Generalized Success Model used generate ARPlaces online task-execution Section 4, discuss generalization properties
limitations Generalized Success Model. so, must distinguish
5. indication, model described paper four steps take 0.2ms 2.2GHz
machine Matlab implementation.

16

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

general applicability approach different robots, objects domains,
specificity model factors learned. essentially holds
data-driven approach: model principle learned data, independent
robot system generates data, learned, specific
data generated robot system, thus specific robot system itself.
practical purposes, assume domain robot hardware remain fixed, learning
domain- robot-specific model grave limitation.
3.5.1 Generalization Object Poses
learned model generalizes different object poses, relative object pose
table f obj = [xobj obj ] part feature space Generalized Success
Model parameterized (see step 1. calling GSM Section 3.4). Generalized
actually refers capability generalizing Success Models specific object poses.
3.5.2 Grasp-Specific ARPlaces
specific object, lot data would required learn ARPlace
object robot manipulate. practice however, found
grasps suffice grasp everyday objects kitchen environments real robot
platform (Maldonado, Klank, & Beetz, 2010). particular, approach required
2 grasps (one top one side) achieve 47 successful grasps
51 attempts 14 everyday kitchen objects. Therefore, propose use grasp-specific
ARPlaces, rather object-specific ARPlaces. learned Generalized Success
Models grasps, depicted Figure 9.

Figure 9: two Point Distribution Models side top grasp. Examples
objects manipulated grasps depicted.

two deformation modes Point Distribution Model depicted Figure 9
already contain 99% deformation energy, even side grasps.
success side grasp relatively independent orientation
object, robot need reach around object. leads
symmetric classification boundaries top grasp, seen Figure 9.
summary, two Generalized Success Models must learned two different
grasps, two grasps suffice grasp 14 everyday kitchen objects tested
17

fiStulp, Fedrizzi, Mosenlechner, & Beetz

real robot Maldonado et al. (2010). rest article, focus
side grasp; ARPlaces top grasps presented Fedrizzi (2010).

4. Computing Action-Related Places
previous section, demonstrated Generalized Success Model learned
observed experience variety task parameterizations. resulting function
maps known robot object positions prediction whether action execution
succeed feel.
section, describe ARPlaces manipulation computed on-line
specific task contexts. depicted Figure 3, module takes Generalized
Success Model estimated robot pose target object pose input, returns
ARPlace depicted Figure 1.
4.1 Taking Object Position Uncertainty Account
Equation 9, prediction whether manipulation action succeeds fails based
known robot target object positions. However, task execution, robot
estimates positions, varying levels uncertainty. uncertainties
must taken account predicting outcome, manipulation action
predicted succeed might well fail target object position
robot expects be. Given Generalized Success Model Equation 9, goal
section therefore compute mapping

Generalized Success Model

P (Succ|f

rob

,f

obj

ARPlace (with object uncertainty)

) 7 {0, 1} Monte Carlo { P (Succ|fkrob , hf obj , obj i) }K
k=1 7 [0, 1]
(10)

takes estimates target object position, returns continuous probability
value, rather discrete {0, 1} probability value Equation 9. belief state,
uncertainties object positions modelled Gaussian distribution mean f obj
covariance matrix obj .
robot platform described Appendix A, f obj obj obtained
vision-based object localization module (Klank, Zia, & Beetz, 2009). Typical values along
2
2
2
diagonal 6x6 covariance matrix are: x,x
= 0.05, y,y
= 0.03, z,z
= 0.07,
2
2
2
yaw,yaw = 0.8, pitch,pitch = 0.06, roll,roll = 0.06. uncertainties position specified
meters angular uncertainties specified radians. estimation object
position quite accurate, vision system problems detect handle,
important estimating orientation (yaw) cup. Due constraints enforced
assumption cup standing upright table, uncertainty z, pitch
roll set 0. remaining 3x3 covariance matrix mapped relative feature
space, yields obj .
end Section 3.4, demonstrated classification boundary hnew reobj
obj
constructed, given known task relevant parameters fnew
= [ xobj
new new ].
obj
uncertainty fnew , suffice compute one classification boundary given
18

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

probable position cup ARPlace grasp. might
lead failure cup position expected. Therefore, use
Monte-Carlo simulation generate whole set classification boundaries. done
taking 100 samples Gaussian distribution object position, given mean
position associated covariance matrix. yields matrix task-relevant parameters
obj obj ]. corresponding classification boundaries computed
Fobj


s=1...100 = [x
samples hs = H + P b(fsobj )) Equation 8. Figure 10(a), 20
100 boundaries depicted.
task-relevant
parameters
h
2 generated

obj obj
2 obj obj
2
x

0.03
0
=
.
f obj = [ xobj obj ] = [ 0.2 1.5 ] obj = 2x x
2
2
0 0.30

obj xobj

obj obj

(a) Sampled classification boundaries (b) Discretized relative sum (c) Final distribution, condi(hs=1..20 ).
boundaries.
tioning robot pose uncertainty.

Figure 10: Monte-Carlo simulation classification boundaries compute ARPlace.
described Appendix C, 0 definition, FGSM defined relative
cups position along tables edge. uncertainty y, described
2 , leads uncertainty origin F
y,y
GSM . Therefore, sampling
task-relevant parameters, sample values y, translate FGSM accordingly. Uncertainty influence shape classification boundary PDM,
simply translates classification boundary along tables edge. sampling
actually already done Figure 10(a), yobj yobj = 0.03. fact,
2

obj obj
2 obj obj
2 obj obj


x
x
x

x

0.032 0
0
2
2
2
obj





2
=
=
0 0.03
0
obj xobj
obj obj
obj obj
2

obj xobj

2

obj obj

0

2

obj obj

0

0.302

computed sampled classification boundaries, generate discrete
grid 2.52.5cm cells, represent discrete robot positions {fkrob }K
k=1 Equation 1.
cell, number classification boundaries classify cell success
counted. thus computing histogram predicted successful grasps. Dividing
result overall number boundaries yields probability grasping cup
succeed position. corresponding distribution, takes uncertainty
cup position account, depicted Figure 10(b).
interesting note steep decline right side distribution near
table, probability successful grasp drops 0.8 0.2 5cm.
intuitive, table located right side, robot bumps table
moving sampled initial position, leading unsuccessful navigate-reach19

fiStulp, Fedrizzi, Mosenlechner, & Beetz

grasp sequence. Therefore, none 16 boundaries contain area close
table, variation P right side PDM low. Variations B
large effect boundary, seen Figure 10(b). summing
sampled boundaries, leads steep decline success probability.
Note ARPlace normalized probability distribution (which sums 1),
rather probability mapping, element (discrete grid cell) probability
distribution itself. Thus sum probabilities grid cell 1, i.e. P (Succ) +
P (Succ) = 1.
4.2 Taking Robot Position Uncertainty Account
robot uncertainty position target object,
position. uncertainty must taken account ARPlace. instance,
although position near left steep incline Figure 10(b) predicted
successful, might still fail robot actually right expected.
Therefore, condition probabilities Figure 10(b) robot actually
rob ,
rob )6 , acquire
certain grid cell (xrob , rob ) given position estimate (x
final ARPlace mapping as:
ARPlace prob. mapping, Figure 10(c).
P (Success|hf rob , rob i, hf obj , obj i) =
P (Success|f rob , hf obj , obj i)

P (f rob |hf rob , rob i)
(11)

Prob. mapping Equation 10, Figure 10(b).

Prob. distribution robot uncertainty (Gaussian).

equation, hf rob , rob interpreted two ways. First all, represent
actual estimate robots position current time. case, P (Success| . . . )
predicts probability success manipulation current position. However,
interpreted possible goal positions robot could navigate order
rob , rob i, throughout paper. so,
perform navigation, i.e. hfgoal
goal
rob
make assumption future position uncertainty rob
goal goal position fgoal
rob . believe fair assumption because;
currently, i.e. rob
goal =
1) realistic assuming rob
goal = 0; 2) robot approaches navigation
rob
goal, continually updating , thus P (Success| . . . ). reached
rob .
goal, rob
goal equivalent
4.3 Refining ARPlace On-line
summary, ARPlaces computed on-line learned Generalized Success Model,
given task-relevant parameters current task context, includes uncertainties
6. Since navigation planner parameterized robot always faces table (cf. Section 3.1),
ignored orientation robot computing GSM. Note therefore ignore
uncertainty parameter here, ARPlaces take account. expect
improved robustness (evaluated Section 6.2) could improved taking (the uncertainty)
parameter account.

20

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

poses robot target object. yields probability mapping maps
robot base positions probability grasping target object succeed.
Learning Generalized Success Model costly step involves extensive data
collection, thus performed off-line. learned, model compact,
used efficiently compute ARPlaces on-line7 Therefore, ARPlaces updated
execution task progresses, incorporate new knowledge taskrelevant parameters changes environment. Figure 11 depicts ARPlace
probability mapping affected new knowledge task-relevant parameters comes
in. first row demonstrates accurate knowledge target objects
position (lower uncertainty, e.g. lower xobj xobj ) leads focussed ARPlace
higher overall probabilities, higher mode. second third row depict
similar effects estimates target objects position orientation change.
figure serves two purposes: gives reader visual intuition effects several taskrelevant parameters shape ARPlace, demonstrates robots
internal ARPlace representation might change new (more accurate) information
target object pose comes in.
Decreasing uncertainty cup position perpendicular table edge
xobj =

0.28

xobj =

0.38

obj =

0.60

0.19

0.10

0.01

Decreasing distance cup table edge
0.33

0.23

0.13

Changing orientation cup table
1.30

1.95

2.65

Figure 11: images demonstrate varying certain task-relevant parameters affects
shape ARPlace distribution.
decision whether certain probability success suffices execute manipulation
action critically depends domain task. Failing grasp full glass wine
7. indication, takes average 110ms 2.2GHz machine Matlab implementation
perform steps Section 4.1 4.2.

21

fiStulp, Fedrizzi, Mosenlechner, & Beetz

grave consequences failing grasp tennis ball. general, ARPlace provides
representation enables high-level planners make rational decisions
scenarios, specify decisions made, minimal
success probability order perform task. Section 5 present use
ARPlace concrete scenario.
4.4 Generality ARPlaces
Section 3.5, discussed generality learning Generalized Success Model,
specificity model respect robot skills, model
learned off-line. section, demonstrate generality flexibility
ARPlace representation, generated on-line using Generalized Success Model.
present various ways ARPlaces extended, lay groundwork Section 5, explains ARPlaces used context high-level
transformational planner.
4.4.1 Merging ARPlaces Multiple Actions
ARPlaces multiple actions composed intersecting them. Assume
computed ARPlaces two different actions (a1 a2 ). success probabilities
ARPlaces independent, compute ARPlace executing actions
parallel multiplying probabilities ARPlaces action a1 a2 .
first two graphs Figure 12 instance, ARPlaces grasping cup
left right gripper depicted. piecewise multiplication probabilities,
acquire merged ARPlace, depicted right graph. robot use
merged ARPlace determine probability use left right gripper
grasp cups one base position (Fedrizzi, Moesenlechner, Stulp, & Beetz, 2009).
Another similar application merging ARPlaces two cup positions, grasped
gripper. ARPlace represents probability able grasp
cup one position, placing position, without moving base.
compositions would impossible robot commits specific positions advance.

Figure 12: Left distribution: grasp cup left gripper. Center distribution: grasp cup
right gripper. Right distribution (element-wise product two
distributions): Grasp cups left/right gripper one base position.

22

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

navigating one position grasp two cups much efficient navigating two positions, implemented decision transformation rule
Reactive Planning Language (McDermott, 1991), described detail Section 5.
4.4.2 Different Supporting Planes
Defining feature space Generalized Success Model relative tables edge
allows robot compute ARPlaces general table shapes one presented far. done determining ARPlace straight edges
table, computing union individual ARPlaces. example depicted
Figure 13.

Figure 13: ARPlace complex table shape.

4.4.3 Different Uncertainty Distributions
article, uncertainty position robot target objects modelled
multi-variate Gaussian distribution. approach expects
distribution, state estimation systems represent uncertainty.
Section 4.1, described specific target object positions sampled
distribution Monte Carlo simulation. general, method applies distribution
sampling done. distributions need Gaussian,
might well multi-modal even non-parametric. particle filter instance,
particle could directly used sample compute classification boundaries
Figure 10(a).
4.4.4 Applicability Domains
demonstrate generality ARPlaces briefly showing ARPlace able
represent task-relevant place different task domain: approaching ball
robotic soccer. task frequently fails robot bumps ball
achieving desired position ball. Figure 14(a), examples successful (S)
failed (F) attempt depicted. Here, robot approach ball
top. goal acquire ARPlace maps robots position field
predicted probability successfully approach ball.
23

fiStulp, Fedrizzi, Mosenlechner, & Beetz

procedure learning ARPlace equivalent mobile manipulation
domain: 1) gather data log successful failed episodes (Figure 14(a)); 2) learn classification boundaries generalized success model data; 3) generate ARPlaces
specific task contexts (Figure 14(b)). example demonstrates ARPlace
approach limited mobile manipulation, generalizes actions domains.

(a) Successful failed
attempts approaching
ball.
Data taken
(Stulp & Beetz,
2008).

(b) robots ARPlace approaching ball. Green
plateau: high probability robot succeed approaching ball orientation indicated thick
black arrow.

Figure 14: ARPlace robot soccer domain.
Note two bumps left right ball. intuitively clear
robot succeed approaching ball locations, surrounding
ones. assume depends particular morphology robot,
controller used approach ball; described Stulp Beetz (2008). One
main advantages using approach based learning assumptions
intuitions play role acquiring model. Whatever reason may be,
successful approaches obvious observed data (Figure 14(a)), hence
ARPlace represents them.
4.4.5 Using General Cost Functions
article, probability success considered utility relevant determining
appropriate base position. principle, ARPlace able represent kind
utility cost, example given Figure 15. Here, task robot
collect one two cups table. probabilistic ARPlaces two cups
depicted left graph. Given parameters, chance success 0.99
cups, reason prefer fetching one other. However, cup B much
closer robot, therefore would efficient collect cup B. preference
expressed ARPlace. First, compute distance robot
24

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

grid cells probabilistic ARPlace, depicted center graph. Finally,
merge probability P distance one cost u, u = (1 P )5 + d/0.3.
expresses takes average 5 seconds reposition robot another grasp
attempt case failure, average navigation speed 0.3m/s. cost
thus expresses expected time overall task take8
depicted Figure 15, mode ARPlace cup closer
robot higher, reflecting fact prefer robot fetch cups closer.
in-depth discussion utility-based ARPlaces, affect behavior
robot, refer Fedrizzi (2010).
Probability: P

Distance:

Cost: u = (1 P )5 + d/0.3

Figure 15: Example general cost-based ARPlace (right), including
probability success (left) distance robot (center). including
distance part cost, mode cost-based ARPlace closer
cup higher distant cup.

5. Transformational Planning ARPlace
far, described ARPlaces generated on-line using learned Generalized Success Model. ability predict (probability an) outcome action
makes ARPlaces powerful tool combined high level planning system.
section, demonstrate ARPlace used context symbolic transformational planner. Reasoning ARPlace enables planner generate robust
efficient plans, demonstrates flexibility least-commitment ARPlace
representation.
particular, consider task retrieving two cups table. One action
sequence solves task is: Plan A: navigate location near cup1, pick cup1
left gripper, navigate location near cup2, pick cup2 right gripper,
depicted Figure 16. However, cups sufficiently close (as
Figure 12, right), much efficient replace plan Plan B: navigate
location near cup1 cup2, pick cup1 left gripper, pick cup2
right gripper, saves entire navigation action.
8. cost chosen simplicity, illustrate generality ARPlace representation.
realistic, complex cost functions used.

25

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 16: Improving performance transformational planning (merged)
ARPlace. Plan A: navigate two separate poses grasping object,
using ARPlaces objects. Plan B: navigate one pose grasping
objects, using merged ARPlace.

Deciding whether use two base locations (Plan A) one (Plan B) difficult solve
control program without sacrificing generality. keep solution general,
want write two separate control programs options, choose
if-then-else statement. would mean provide control programs
choice points every option robot has. space choices prohibitively large
everyday tasks allow approach. Instead, use transformational planner
takes general program (Plan A) and, appropriate, applies generic transformation rules
change program locally (to yield Plan B). transformational planner consists
following components:
Plan projection. projection mechanism predicting outcome plan. ARPlace
compact representation projection mechanism, able predict
probability success action, given parameters.
Flaw detection. mechanism detecting behavior flaws within predicted plan outcome. Flaws errors hinder robot completing task,
may performance flaws, suboptimal efficiency. Using two navigation actions approach cups close (Plan A) flawed,
much efficient navigate one position close cups.
Plan transformation. mechanism fix detected flaws applying transformation
rules plan code. problem consider, local transformation rule
applied Plan yield efficient Plan B.
next sections, describe mechanisms detail,
explain implemented exploit ARPlace representation. Note
article, use transformational planner exemplify ARPlace used
context larger planning system. information transformational
planning framework, examples behavior flaws transformation rules,
refer work Mosenlechner Beetz (2009).
26

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

5.1 Plan Design
detect flaws apply transformation rules repair, transformational planner
must able reason intention code parts, infer goal achieved
not, deduce reason possible failure was. so, control programs
written rpl (McDermott, 1991), provides functionality annotating code
parts indicate purpose make transparent transformational planner.
purpose article, important rpl instructions semantic annotation
context pick-and-place tasks achieve, perceive at-location. formal
definition semantics instructions given Mosenlechner Beetz (2009);
describe informally.
(achieve ?expression) achieve statement executes successfully, logical expression passed argument asserted true. instance, successful
execution (achieve (entity-picked-up ?cup)), object referenced variable ?cup
must robots gripper9 .
(perceive ?object) manipulating objects, robot must find objects
instantiate belief state. successful execution, statement (perceive ?cup)
asserts object referenced ?cup found, reference internal
representation returned.
(at-location ?location ?expression) Manipulation implies execution actions
specific locations. Therefore, must assured pick-up actions executed
robot specific location. (at-location ?location ...) asserts code within
context either executed specified location fails. Please note transformations affect location actions performed directly modify ?location
parameter at-location expressions. Therefore, at-location important
declarative plan expression optimizing ARPlaces. specify locations at-location,
use so-called designators, symbolic descriptions entities locations, objects
actions. instance, designator location stand picking cup
specified follows: (a location (to pick-up) (the object (type cup))). symbolic
description resolved reasoning mechanisms ARPlaces Prolog
actual pose generated needed. general, infinite number poses provide
valid solution pose. ARPlace gives us way evaluate utility select
best pose.
declarative expressions explained combined form tree. Every
achieve statement contain several achieve, perceive at-location statements
sub-plans. example plan tree sketched Figure 17. tree, goal (achieve
(entity-at-location ?object ?location)) first perceives object, picks achieving
entity-picked-up, executes pick-up action within at-location block, puts
object achieving entity-put-down, contains at-location block.
shall see Section 5.4, behavior flaws repaired applying transformation rules
replace sub-trees within plan tree new code.

9. Please note lisp syntax, variables prefixed ?, example ?cup, predicates
functions pure symbols.

27

fiStulp, Fedrizzi, Mosenlechner, & Beetz

(entity-at-location ?o ?l)

(perceive ?o)

(achieve (entity-put-down ?o ?l))

(achieve (entity-picked-up ?o)
..
.
..
.

.. (at-location ?obj-l)
.

(at-location ?obj-l)
..
.
..
.

..
.

..
.

Figure 17: example plan tree created executing pick-up plan
5.2 Plan Projection
central component transformational planner plan projection, simulates
behavior robot arises executing plan. approach, plan projection generates temporally ordered set events based plan code presented
previous section. use Gazebo based mechanism projection
used generating training data learning ARPlaces. particular, use information collisions, perception events locations objects robot.
executing plan simulation, generate extensive execution trace
used reasoning engine infers behavior flaws fixed transformation rules (Mosenlechner & Beetz, 2009). execution trace contains low-level data
representing position objects robot, well collisions objects,
visibility objects robot, information reconstruct state program
throughout execution.
ARPlaces efficient way performing plan projection, predict
probability successful outcome without requiring on-line generation execution traces.
reason execution trace sampling required on-line, task
already executed frequently off-line data acquisition (cf. Section 3.1).
results task executions compiled ARPlaces learning
GSM, yields compact representation experience acquired. Therefore,
experience must generated anew plan generation.
5.3 Behavior Flaws Reasoning Plan Execution
Plan projection simulates robot behavior executing plan. second component transformational planner reasoning engine finds pre-defined flaws
projected robot behavior. Examples flaws collisions, e.g. caused underparameterized goal locations, blocked goals, e.g. chair standing location
robot wants navigate to. examples behavior flaws lead critical
errors plan execution (i.e. plan fails), consider behavior inefficient
flawed (i.e. plan succeeds, unnecessarily inefficient). task consider
paper example performance flaw, performing two navigation actions one required highly inefficient. Behavior flaws specified using
28

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Prolog-like reasoning engine implemented Common Lisp (Mosenlechner & Beetz,
2009).
execution trace generated plan projection transparently integrated
reasoning engine, i.e. execution trace queried using Prolog predicates. information recorded execution trace valuable information order find behavior flaws.
Additional information used find behavior flaws set facts model semantics declarative expressions achieve at-location concepts world,
instance objects placed supporting planes (table, cup-board, ...). find
behavior flaws, Prolog specifications matched logical representation
execution trace solutions found, corresponding flaw present plan
fixed.
instance, code match two locations perform actions merged
one ARPlace looks follows:
Listing 1: Flaw definition match two different pick-up tasks.
1
2
3
4
5

(
( k g l ? k 1 ( c h e v e ( e n p c k e ? b j e c 1)))
( k g l ? k 2 ( c h e v e ( e n p c k e ? b j e c 2))) ( h n
(== ? k 1 ? k 2)) ( p z e c n l c n ? b j e c 1
? b j e c 2 ? p z e l c n ) )

code first matches two different pick-up tasks. predicate optimizedaction-location holds ?optimized-location ARPlace two objects
picked up. bind variable, predicate implemented calculate
ARPlace.
Another example flaw definition failed navigation, i.e. robot
standing location supposed drive to:

Listing 2: Flaw definition find locations reached robot although
told reach them.
1
2
3
4
5

(
( k g l ? k ( c h e v e ( l c Robot ? g l l c ) ) )
( k u ? k Done ? )
( h l ( l c Robot ? r b l c ) ( ? ) )
( (== ? g l l c ? r b l c ) ) )

code first matches code navigating robot location ?goalloc. infers actual location robot navigation task terminated
binds variable ?robot-loc finally asserts two locations
equal. Prolog expression proven execution trace, found
flaw indicating unachieved goal location.
5.4 Plan Transformations Transformation Rules
behavior flaw detected, last step planner iteration application
transformation rule fix behavior flaw. Transformation rules applied parts
plan tree cause substantial changes structure corresponding robot
behavior.
29

fiStulp, Fedrizzi, Mosenlechner, & Beetz

transformation rule consists three parts. input schema matched
plan part transformed binds required code parts variables order
reassemble output part. transformation part performs transformations
matched parts, output plan describes new code respective
plan part reassembled.
input schema
transformation
output plan
Besides integration ARPlace robot control program at-location
statements, ARPlace integrated reasoning engine transformational
planner. Using two locations grasping considered performance flaw one location
would suffice. Informally, investigate execution trace occurrence two different pick-up actions, one executed location L1 , one executed
location L2 . request location L3 perform actions corresponding success probability. L3 computed merging ARPlace Figure 12.
probability success merged ARPlace sufficiently high, apply plan
transformation, replace locations L1 L2 location L3 .
transformation rule optimizing ARPlaces shown Listing 3. Please note
variables bound matching flaw definition still bound
used transformation rule.
Listing 3: Transformation rule fixing flaw.
1
2
3
4
5
6
7
8
9
10
11

( e f r r u l e f x unoptimized l c n
: n p u schema
( ( ( k g l ? l c n k 1
( atl c n ( ? l c n 1) . ? code 1))
( subt k ? l c n k 1 ? k 1))
( ( k g l ? l c n k 2
( atl c n ( ? l c n 2) . ? code ) )
( subt k ? l c n k 2 ? k 2)))
: outputp l n
( ( atl c n ( ? p z e l c n ) . ? code 1)
( atl c n ( ? p z e l c n ) . ? code 2)))

input schema code consists two similar patterns, matching
at-location sub-plan pick-up goals matched flaw. planner replaces
matching code parts corresponding entries output plan. transformation
rule, location passed at-location replaced optimized location
calculated flaw definition.
behavior flaw defined match two different pick-up executions.
ARPlace query performed find probability successfully grasping
objects one location. probability sufficiently high (> 0.85) Prolog query
succeeds, i.e. flaw detected sufficiently good location grasping
objects found. Note sufficiently high depends much scenario
context. robotic soccer beneficial choose fast risky moves, whereas
safe human-robot interaction, certainty successful execution important
mere speed. article focusses principled ways integrating thresholds
transformational planner, relating grounded models robots behavior.
30

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

thresholds be, determined, depends application
domain users.

6. Empirical Evaluation
section 1) determine many samples needed learn accurate SVM
classifier; 2) compare robustness default strategy determining base positions
strategy uses ARPlaces; 3) compare efficiency plans without
fixing performance flaws transformational planner; 4) present preliminary results
physical robot platform.
6.1 Classification Accuracy Training Set Size
Figure 18 depicts accuracy SVM classifier predicting base positions
lead successful grasps one particular cup position, evaluated separate test set
150 samples. Without using capability map filter kinematically impossible
base positions, graph levels 300 examples10 . filtering theoretically
impossible base positions capability map, classifier achieves accuracy
within 173 examples (Stulp et al., 2009).

Figure 18: Accuracy dependent training set size one cup position.
effect dramatic entire dataset containing data 16 different cup
positions. applying capability map, number trials need executed
reduces 2992 (all markers Figure 6) 666 (only red/green filled markers Figure 6).
capability map reduces unsuccessful attempts, influence final
classification accuracy, 94%.
10. graph applies another dataset described Stulp, Fedrizzi, Zacharias, Tenorth, Bandouch,
Beetz (2009), similar one used rest article.

31

fiStulp, Fedrizzi, Mosenlechner, & Beetz

6.2 Results Simulated Robot
compare robustness navigation based probabilistic ARPlaces
strategy based deterministic navigation goals. evaluation, position
robot navigates position ARPlace returns highest probability
grasping target object succeed. compare strategy previous
hand-coded implementation Fixed, always navigates location
relative offset target object, whilst time taking care bump
table.
experiments, vary position cup (xobj , obj ), well
uncertainties robot position position cup, varying
diagonal elements covariance matrices associated position robot
(xrob xrob ,yrob yrob ) cup (xobj xobj ,obj obj ). combination
variables, robot performs navigate-reach-grasp-lift sequence. result
recorded, data acquisition learning Generalized Success Model.
simulate uncertainty, sample specific perceived robot cup position
distribution defined means covariance matrices. result action
determined true simulated state world, robot bases decisions
perceived samples.
results evaluation summarized three bar plots Figure 19,
depict success ratios ARPlace-based Fixed strategies. ratio
number successful executions, divided number examples, 100. pvalue pair bars computed 2 test them, tests whether
number successful failed attempts sampled distribution
ARPlace Fixed.
first graph depicts success ratios increasing uncertainty object
position (i.e. xobj xobj = [ 0.00 0.05 0.10 0.15 0.20 ]), fixed robot position uncertainty xrob = 0.05. cases, ARPlace strategy significantly outperforms
Fixed strategy. Furthermore, performance ARPlace much robust towards
increasing object position uncertainty, ARPlace takes explicitly account.
trend seen increasing uncertainty robot position (i.e.
xrob xrob = yrob yrob = [ 0.00 0.05 0.10 0.15 0.20 ]), fixed object position uncertainty
xobj = 0.05. However, xrob xrob > 0.1 difference ARPlace
Fixed longer significant.
Finally, last graph depicts success ratios increasing robot object
uncertainty. Again, ARPlace significantly outperforms Fixed ( < 0.15).
robot quite uncertain objects position ( > 0.15), grasp success
probabilities drop 50% strategies.
Summarizing, ARPlace robust towards state-estimation uncertainties
previous default strategy. effect pronounced object positions robot
positions.
6.3 Transformational Planning Merged ARPlaces
evaluated merging ARPlaces joint grasping, application transformation rules rpl planner, discussed Section 4.4.1. Two cups placed
32

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Figure 19: Success ratios ARPlace Fixed approaches changing object
and/or robot pose uncertainties.

table, distance varied 20 60cm, increments
5cm. evaluation shows grasping two cups separate base positions requires
average 48 seconds, independent relative distance cups other.
applying transformation rules, default plan optimized 32 seconds, significant (t-test: p < 0.001) substantial performance gain 50% (Fedrizzi et al., 2009).
45cm, two cups cannot grasped one position, plan transformation
applied.
6.4 Integration ARPlace Physical Robot System
day open house, B21 mobile manipulation platform continually performed
application scenario, locates, grasps, lifts cup table moves
kitchen oven. Figure 20 shows two images taken demonstration. robot
performed scenario 50 times approximately 6 hours, convinced us
robot hardware software robust enough deployed amongst general public.
open day, ran experiment, time determined goal
location navigating table mode ARPlace computed
executing navigation action. Since main focus experiment
error-recovery system described Beetz et al. (2010), improved robot performance
observed cannot quantitatively attributed use ARPlace error-recovery
system. However, major qualitative improvement certainly attribute using
ARPlace cup grasped much larger area table.
Without ARPlaces, cup always placed position table
enable successful grasping.

7. Conclusion
article, present system enables robots learn action-related places
observed experience, reason places generate robust, flexible, least33

fiStulp, Fedrizzi, Mosenlechner, & Beetz

1. robot navigates table

2. robot reaches cup

Figure 20: reach grasp trajectory performed public demonstration. (Note
operator holding camera, remote control!)

commitment plans mobile manipulation. ARPlace modeled probability distribution maps locations predicted outcome action.
believe system several advantages. First all, learned model
compact, 2 (deformation) parameters, directly related task-relevant
parameters. Querying model on-line therefore efficient. advantage
compiling experience compact models, rather running novel search
situation.
hand, model acquired experience-based learning,
model grounded observed experience, takes account robot hardware,
control programs, interactions environment. applied mobile
manipulation platform, independent manipulators, navigation base, algorithms
run them.
output model set positions associated success probabilities,
instead one specific position. Rather constraining specific position prematurely, robot efficiently update ARPlace new sensor data comes in. enables
least-commitment planning. ARPlace representation enables optimization
secondary criteria, execution duration, determining best position grasping two objects simultaneously. previous work, proposed subgoal refinement (Stulp
& Beetz, 2008) optimizing secondary criteria respect subgoals.
Finally, using ARPlaces determine appropriate base positions, difficult positions
grasping avoided, leads robust behavior face state estimation
uncertainty, demonstrated empirical evaluation.
currently extending approach several directions. process
including ARPlace general utility-based framework, probability
success one aspects task needs optimized. New utilities,
34

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

execution duration power consumption, easily included framework,
enables robot trade efficiency robustness on-line task execution.
applying approach complex scenarios different domains.
instance, learning higher-dimensional ARPlace concepts, take aspects
scenario account, i.e. different object sizes objects require different
types grasps. Instead mapping specific objects places, map object grasp
properties deformation modes. investigating extensions machine
learning algorithms enable methods generalize larger space. Objects
require different grasps, using two hands manipulate them, require
sophisticated methods acquiring reasoning place. Generalization
place concept respect situations task contexts research challenge
mid-term research agenda.

Acknowledgments
grateful Pierre Roduit providing us Matlab code described Roduit
et al. (2007). thank Ingo Kresse, Alexis Maldonado, Federico Ruiz assistance
robotic platform, robot system overview. grateful Franziska
Zacharias providing capability map (Zacharias et al., 2007) robot. thank
Dominik Jain Franziska Meier fruitful discussions Section 4.2.
work partly funded DFG project ActAR (Action Awareness Autonomous Robots) CoTeSys cluster excellence (Cognition Technical Systems, http://www.cotesys.org), part Excellence Initiative German Research
Foundation (DFG). Freek Stulp supported post-doctoral Research Fellowship
(STU-514/1-1) DFG, well Japanese Society Promotion
Science (PE08571). Freek Stulps contributions work made Intelligent
Autonomous Systems Group (Technische Universitat Munchen, Munich, Germany),
Computational Neuroscience Laboratories (Advanced Telecommunications Research Institute International, Kyoto, Japan), Computational Learning Motor Control Lab
(University Southern California, Los Angeles, USA),

Appendix A. Robot Platform
action sequence consider article is: 1) navigate specified base position
near table; 2) reach object; 3) close gripper; 4) lift object.
sequentially describe various hard- software components involved executing
actions. overview components data communicated
depicted Figure 21.
main hardware component B21r mobile robot Real World Interfaces
(RWI), frontal 180 degrees Sick LMS 200 laser range scanner. task execution,
robot acquires map (kitchen) environment using pmap map building.
navigate specified base position, robot uses Adaptive Monte Carlo Localization
algorithm localization, AMCL Wavefront Planner global path planning.
three software modules (map building, localization planning), use
implementations Player project (Gerkey et al., 2003).
35

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 21: Overview mobile manipulation hardware software modules.

robot close table, detects tracks target object using
approach proposed Klank et al. (2009). stereo-vision hardware consists two
high dynamic range cameras mounted PTU-46 pan-tilt unit Directed
Perception resolution 1390x1038 pixels.
manipulation, robot equipped two 6-DOF Powercube lightweight arms
Amtec Robotics. control arms reach target cup, use Kinematics Dynamics Library (Orocos-KDL) (Smits, ) Vector Field approach. Within
vector field, handle cup attractor, cup itself, table
obstacles repellors. Details position shape attractors
repellors given Beetz et al. (2010). On-line every control cycle, task space velocity end-effector computed given attractors repellors, velocity
mapped joint space velocities using damped least squares inverse kinematics algorithm.
reaching desired end-effector pose, 1-DOF slide gripper closes.
High-level decision making, monitoring error-recovery done planning module written Reactive Planning Language (McDermott, 1991). requests ARPlaces
module described article, reasons them, performs navigation
manipulation requests based them.
Communication modules described done middleware layer
consisting Player (Gerkey et al., 2003) YARP (Metta, Fitzpatrick, & Natale, 2006).
overview simplification actual system. instance, role RFID tags
Belief State omitted. complete detailed description
mobile manipulation platform, refer work Beetz et al. (2010, Section 1.2).
36

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Appendix B. Landmark Distribution Point Distribution Model
Point Distribution Model (PDM) takes set landmarks n contours input,
represented n matrix H, returns matrices H (mean contours), P
(deformation modes), B (deformation mode weighting per contour), original
contours reconstructed.
application PDMs, free choose locations landmarks. Therefore, goal procedure described determine landmark locations leads
compact PDM accurately reconstructs original contours, i.e. classification
boundaries. explicitly optimizing two measures: 1) model compactness:
amount energy e stored first degrees freedom PDM, 0 e 1;
2) reconstruction accuracy: mean distance l landmarks original
contours reconstructed contours. measures combined cost function
(2 e)l2 , expressing want low error high energy given number degrees
freedom d.
Given number landmarks number degrees freedom d, explicitly
optimize cost function search. varying position
landmark, one landmark time, greedily selecting position leads
lowest cost. optimization first done = 1, number degrees freedom
incremented optimization leads energy lies 95%. ensures
number degrees freedom distance l landmarks remains
low, whilst energy e high. Therefore, resulting PDM model compact yet
accurate.
optimization step far computationally intensive step off-line
learning phase. currently investigating use alignment methods computer
vision (Huang, Paragios, & Metaxas, 2006), replace iterative optimization approach.

Appendix C. Robot Coordinate Systems Relative Feature
Space
robot uses variety coordinate systems. goal compute matrix GSM ,
describes objects position relative feature space Generalized Success
Model. GSM used reconstruct classification boundaries successfully
grasping object, described Section 3.4. present required coordinate
systems, transformed yield Generalized Success Model required
feature space depicted Figure 5.
coordinate frames involved transformation depicted Figure 22:
world frame FW , table frame FT centered middle top table,
robot frame FR centered robots base center floor, camera frame
FC centered cameras sensor chip, frame pan-tilt unit
camera mounted FP , relative feature space FGSM .
acquire position target object relative FGSM , compute GSM
follows:
GSM

= (W TGSM )1
37

W



(12)

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Figure 22: Relevant coordinate frames

global position object
W

=

W

WT


TR

computed follows:
R

TP

PT

TC

C



(13)

Here, W TR location robots base frame relative world frame.
robot uses AMCL particle filter estimating position. R TP pose pan
tilt unit relative robots base frame. transformation matrix R TP constant
specified manually measuring distances angular offsets B21
robot base pan tilt unit. careful measurement, assume maximum
errors 1mm distance measurements along x-, y-, z-axis, 2
yaw angle measurement. P TC pose cameras sensor relative pan tilt
unit. P TC changes according current pan tilt angles, read
pan tilt units driver high accuracy. C position target object relative
camera frame. estimated vision-based object localization module
described Klank et al. (2009).
order compute W TGSM need know global position object W ,
already computed above, global position table W TT . Currently,
get world coordinates tables position map, possible
estimate position vision-based object localization module. compute
normal object table edge closest robot, seen
Figure 5. origin FGSM therefore W TGSM table edge object
normal intersect.
critical parts computations angular estimations
robots localization vision system. First, estimation uncertainty rather big.
Second, error localization angle significant impact estimated object
pose, follows Equation 13.
pose cup frame FGSM 6D vector [x, y, z, yaw, pitch, roll]. However,
since assume cup standing upright table, set z tables height,
roll pitch 0 . Since origin FGSM perpendicular tables edge
passes y, 0 definition. remaining parameters x yaw
correspond features xobj obj respectively.
38

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

References
Amir, E., & Chang, A. (2008). Learning partially observable deterministic action models.
Journal Artificial Intelligence Research (JAIR), 33, 349402.
Beetz, M., Stulp, F., Esden-Tempski, P., Fedrizzi, A., Klank, U., Kresse, I., Maldonado,
A., & Ruiz, F. (2010). Generality legibility mobile manipulation. Autonomous
Robots Journal (Special Issue Mobile Manipulation), 28 (1), 2144.
Beetz, M., Stulp, F., Radig, B., Bandouch, J., Blodow, N., Dolha, M., Fedrizzi, A., Jain,
D., Klank, U., Kresse, I., Maldonado, A., Marton, Z., Mosenlechner, L., Ruiz, F.,
Rusu, R. B., & Tenorth, M. (2008). assistive kitchen demonstration scenario
cognitive technical systems. IEEE 17th International Symposium Robot
Human Interactive Communication (RO-MAN), Muenchen, Germany. Invited paper.
Berenson, D., Choset, H., & Kuffner, J. (2008). optimization approach planning
mobile manipulation. Proceedings IEEE International Conference
Robotics Automation (ICRA) 2008, pp. 11871192.
Berenson, D., Srinivasa, S., Ferguson, D., Romea, A. C., & Kuffner, J. (2009a). Manipulation planning workspace goal regions. Proceedings IEEE International
Conference Robotics Automation (ICRA), pp. 13971403.
Berenson, D., Srinivasa, S. S., & Kuffner, J. J. (2009b). Addressing pose uncertainty
manipulation planning using task space regions. Proceedings IEEE/RSJ
International Conference Intelligent Robots Systems (IROS, pp. 14191425.
Bohlin, R., & Kavraki, L. E. (2000). Path planning using lazy prm. IEEE International
Conference Robototics Automation, pp. 521528.
Buck, S., & Riedmiller, M. (2000). Learning situation dependent success rates actions
RoboCup scenario. Pacific Rim International Conference Artificial Intelligence,
p. 809.
Cambon, S., Gravot, F., & Alami, R. (2004). robot task planner merges symbolic
geometric reasoning.. Proceedings 16th European Conference Artificial
Intelligence (ECAI), pp. 895899.
Chang, A., & Amir, E. (2006). Goal achievement partially known, partially observable domains. International Conference Automated Planning Scheduling
(ICAPS), pp. 203211.
Clement, B. J., Durfee, E. H., & Barrett, A. C. (2007). Abstract reasoning planning
coordination. Journal Artificial Intelligence Research, 28, 453515.
Cootes, T. F., Taylor, C. J., Cooper, D., & Graham, J. (1995). Active shape models -
training application. Computer Vision Image Understanding, 61 (1), 3859.
Detry, R., Baseski, E., Popovic, M., Touati, Y., Krueger, N., Kroemer, O., Peters, J., &
Piater, J. (2009). Learning object-specific grasp affordance densities. Proceedings
International Conference Development Learning (ICDL), pp. 17.
Diankov, R., Ratliff, N., Ferguson, D., Srinivasa, S., & Kuffner, J. (2008). Bispace planning:
Concurrent multi-space exploration. Proc. Int. Conf. Robotics: Science
Systems.
39

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Fedrizzi, A. (2010). Action-Related Places Mobile Manipulation. Ph.D. thesis, Technische
Universiat Munchen.
Fedrizzi, A., Moesenlechner, L., Stulp, F., & Beetz, M. (2009). Transformational planning mobile manipulation based action-related places. Proceedings
International Conference Advanced Robotics (ICAR)., pp. 18.
Friedman, M., & Weld, D. S. (1996). Least-commitment action selection. Proceedings
3rd International Conference A.I. Planning Systems, pp. 8693. AAAI Press.
Geib, C., Mourao, K., Petrick, R., Pugeault, M., Steedman, M., Kruger, N., & Worgotter,
F. (2006). Object action complexes interface planning robot control.
Proceedings 2006 IEEE RAS International Conference Humanoid Robots,
Genova.
Gerkey, B., Vaughan, R. T., & Howard, A. (2003). Player/Stage Project: Tools
multi-robot distributed sensor systems. Proceedings 11th International
Conference Advanced Robotics (ICAR), pp. 317323.
Gibson, J. J. (1977). Theory Affordances. John Wiley & Sons.
Hart, S., Ou, S., Sweeney, J., & Grupen, R. (2006). framework learning declarative
structure. RSS-06 Workshop: Manipulation Human Environments.
Huang, X., Paragios, N., & Metaxas, D. N. (2006). Shape registration implicit spaces
using information theory free form deformations. IEEE Trans. Pattern Analysis
Machine Intelligence (TPAMI), 28, 13031318.
Kemp, C., Edsinger, A., & Torres-Jara, E. (2007). Challenges robot manipulation
human environments. IEEE Robotics Automation Magazine, 14 (1), 2029.
Klank, U., Zia, M. Z., & Beetz, M. (2009). 3D Model Selection Internet Database
Robotic Vision. International Conference Robotics Automation (ICRA),
pp. 24062411.
Kuipers, B., Beeson, P., Modayil, J., & Provost, J. (2006). Bootstrap learning foundational representations. Connection Science, 18, 145158.
LaValle, S. M. (2006). Planning Algorithms, chap. Chapter 5: Sampling-Based Motion
Planning. Cambridge University Press.
Maldonado, A., Klank, U., & Beetz, M. (2010). Robotic grasping unmodeled objects
using time-of-flight range data finger torque information. 2010 IEEE/RSJ
International Conference Intelligent Robots Systems (IROS), pp. 25862591,
Taipei, Taiwan.
McDermott, D. (1991). Reactive Plan Language. Research Report YALEU/DCS/RR864, Yale University.
Metta, G., Fitzpatrick, P., & Natale, L. (2006). YARP: Yet Another Robot Platform. International Journal Advanced Robotics Systems, special issue Software Development
Integration Robotics, 3 (1), 4348.
Morales, A., Chinellato, E., Fagg, A. H., & del Pobil, A. P. (2004). Using experience
assessing grasp reliability. International Journal Humanoid Robotics, 1 (4), 671691.
40

fiLearning Reasoning Action-Related Places Robust Mobile Manipulation

Mosenlechner, L., & Beetz, M. (2009). Using physics- sensor-based simulation
high-fidelity temporal projection realistic robot behavior. 19th International
Conference Automated Planning Scheduling (ICAPS09).
Okada, K., Kojima, M., Sagawa, Y., Ichino, T., Sato, K., & Inaba, M. (2006). Vision
based behavior verification system humanoid robot daily environment tasks.
Proceedings 6th IEEE-RAS International Conference Humanoid Robots
(Humanoids), pp. 712.
Pastor, P., Hoffmann, H., Asfour, T., & Schaal, S. (2009). Learning generalization
motor skills learning demonstration. Proceedings International
Conference Robotics Automation (ICRA), pp. 12931298.
Resulaj, A., Kiani, R., Wolpert, D. M., & Shadlen, M. N. (2009). Changes mind
decision-making. Nature, 461 (7261), 263266.
Roduit, P., Martinoli, A., & Jacot, J. (2007). quantitative method comparing trajectories mobile robots using point distribution models. Proceedings IEEE/RSJ
International Conference Intelligent Robots Systems (IROS), pp. 24412448.
Ryan, M. R. K. (2002). Using abstract models behaviours automatically generate reinforcement learning hierarchies. Proceedings 19th International Conference
Machine Learning, Sydney, Australia, pp. 522529.
Smits, R. KDL: Kinematics Dynamics Library. http://www.orocos.org/kdl.
Sonnenburg, S., Raetsch, G., Schaefer, C., & Schoelkopf, B. (2006). Large scale multiple
kernel learning. Journal Machine Learning Research, 7, 15311565.
Stulp, F., & Beetz, M. (2008). Refining execution abstract actions learned action
models. Journal Artificial Intelligence Research (JAIR), 32.
Stulp, F., Fedrizzi, A., & Beetz, M. (2009a). Action-related place-based mobile manipulation. Proceedings International Conference Intelligent Robots Systems
(IROS), pp. 31153120.
Stulp, F., Fedrizzi, A., & Beetz, M. (2009b). Learning performing place-based mobile
manipulation. Proceedings 8th International Conference Development
Learning (ICDL)., pp. 17.
Stulp, F., Fedrizzi, A., Zacharias, F., Tenorth, M., Bandouch, J., & Beetz, M. (2009c).
Combining analysis, imitation, experience-based learning acquire concept
reachability. 9th IEEE-RAS International Conference Humanoid Robots, pp.
161167.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: Introduction. MIT Press.
Wimmer, M., Stulp, F., Pietzsch, S., & Radig, B. (2008). Learning local objective functions
robust face model fitting. IEEE Transactions Pattern Analysis Machine
Intelligence (PAMI), 30 (8), 13571370.
Zacharias, F., Borst, C., & Hirzinger, G. (2007). Capturing robot workspace structure: representing robot capabilities. Proceedings IEEE/RSJ International Conference
Intelligent Robots Systems (IROS), pp. 32293236.
41

fiStulp, Fedrizzi, Mosenlechner, & Beetz

Zettlemoyer, L. S., Pasula, H. M., & Kaelbling, L. P. (2005). Learning planning rules noisy
stochastic worlds. Proceedings Twentieth National Conference Artificial
Intelligence (AAAI), pp. 911918.
Zheng, Y., & Qian, W.-H. (2005). Coping grasping uncertainties force-closure
analysis. International Journal Robotics Research, 24 (4), 311327.

42


