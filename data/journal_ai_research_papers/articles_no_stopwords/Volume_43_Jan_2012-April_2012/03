journal artificial intelligence

submitted published

proximity non uniform abstractions
approximate
jir baum
ann e nicholson
trevor dix

jiri baum com au
ann nicholson monash edu
trevor dix monash edu

faculty information technology
monash university clayton victoria australia

abstract
deterministic world agent certain consequences
planned sequence actions however dynamic stochastic domains
markov decision processes commonly used unfortunately suffer curse
dimensionality state space cartesian product many small sets dimensions
exponential number dimensions
technique exploits intuitive strategy selectively ignoring dimensions different parts state space resulting non uniformity strong
implications since approximation longer markovian requiring use modified planner use spatial temporal proximity measure responds
continued well movement agent state space dynamically adapt abstraction progresses
present qualitative quantitative across range experimental domains
showing agent exploiting novel approximation method successfully finds solutions much less full state space assess
analyse features domains method exploit

introduction
deterministic world agent certain consequences
actions plan sequence actions knowing execution necessarily
achieve goals assumption appropriate flexible multi purpose robots
intelligent software agents need able plan dynamic stochastic
domains operate outcome taking action uncertain
small medium sized stochastic domains theory markov decision processes
provides generating optimal plan bellman howard puterman shin plan takes account uncertainty outcome taking
action specified distribution possible outcomes flexibility
reward function rather simple goal relative desirability
otherwise situation specified
however domain becomes larger become intractable approximate solutions become necessary instance drummond bresina dean
kaelbling kirman nicholson kim steinkraus particular
state space expressed terms dimensions cartesian product sets
size resulting computational cost exponential number dimensions
c

ai access foundation rights reserved

fibaum nicholson dix

hand fortunately fairly structured state space effective
approximations often possible
solution selectively ignoring dimensions parts
state space time words obtain approximate solutions
dynamically varying level abstraction different parts state space
two aspects firstly varying level abstraction introduces
artefacts must somewhat modified eliminate
secondly interestingly appropriate abstraction must selected later modified
action progress
work extension synthesis two existing approaches approximate locality approximation envelope methods dean et al
structure approximation uniform abstraction nicholson kaelbling dearden boutilier work extends exploiting structure
locality broadening scope contemplated baum nicholson
introduced main concepts full details experimental
presented baums thesis studies arbitrary
abstraction instance bertsekas tsitsiklis however generally
theoretical case tended treat approximation markovian
would resulted unacceptable performance practice improve extending deal non markovian aspects approximation
finally use measure locality introduced baum nicholson
similar flexible influence measure munos moore
assume agent continues improve plan acting
failures generally fatal deal control error exclusively sensor
error considered assumed agent accurately discern current
world state fully observable accurately knows state space goal
reward function distribution effect actions learning
remainder organised follows section reviews background
introduces abstraction provides framework section discusses
static non uniform abstraction section presents method initially selecting
non uniform abstraction description section presents method
changing abstraction policy planned sections introduce
proximity measure method varying abstraction measure
respectively section presents direct evaluation calculated
policy simulation finally section discusses section gives
conclusions outlines possible directions future work

non uniform abstractions
non deterministic world agent cannot certain consequences
actions except probabilities cannot plan simple sequence actions achieve
goals able plan dynamic stochastic domains must use
sophisticated markov decision processes appropriate commonly used
representation sort


fiproximity non uniform abstractions

illustrative
aid exposition present two example full set experimental
domains presented section
two illustrative grid navigation domain shown figure
integer x coordinates three doors
open closed damage indication yes agent
move one four cardinal directions open door next nothing
doors fairly difficult open probability success per time step
moving chance success effect case failure running
wall closed door causes damage cannot repaired transitions
shown table agent starts location marked figure doors
closed damage goal reach location marked damage
x
























k



x




















k











k










































k








k







k


b

figure layout grid navigation domain blue arrows optimal
path suboptimal path b keys doors
grid layout walls doors keys

keys contains keys required open doors agent
may one time additional action allows agent
pick key location shown figure open action requires
corresponding key effective separate unlock action doors
contains keys doors unlocked closed therefore corresponding
keys pickup action
optimal policy obtained exact doors simply takes
shortest path door keys optimal plan collect
keys pass south door east door shown figure
suboptimal plan shown figure b


fibaum nicholson dix

x
stay




pre state




post state



dmg



x







































south












open






open

























north












open






open

























east






x




























open

























west






x




























open




















open












































































dmg



























yes
yes




























yes
yes








x





































yes
yes








x





































yes
yes


















open
open








open
open








open
open








yes

table transitions doors showing important changed dimensions
first matching transition used percentage shown given
post state occur probability otherwise state unchanged transitions without percentages deterministic



fiproximity non uniform abstractions

exact
one exact stochastic
domains
involves markov decision




processes mdps mdp tuple r state space
set available actions transition function r reward function
initial state agent begins state time step agent selects
action together current state applies obtain distribution
current state next time step random according distribution
write prt probability action taken state state
next time step agent given reward time step calculated
r current state possibly action selected aim agent
maximise cumulative function rewards typically expected discounted
sum discounting factor fully observable mdp agent full knowledge
particular agent aware r current state selecting action
well known fully observable mdp optimal solution expressed
policy mapping current state optimum action
calculation side effect calculation standard
calculate value function v r expected discounted sum
rewards starting state table summarises notation used
well known iterative bellmans value iteration howards
policy iteration modified policy iteration puterman shin
computing optimal policy however becomes larger calculation
becomes computationally expensive particularly state space structured
cartesian product dimensions sd exponential
since explicitly store v usually functions
space complexity therefore exponential since iterate arrays
time complexity least exponential even consideration
fast iterative converge typically grows
quickly becomes intractable since practice amount computation allowed
agent limited necessitates approximations process
doors six dimensions two x coordinates three
doors one damage open closed
open closed open closed damage damage action space
set five actions north south east west open transition function
specifies outcomes taking action state reward function r
agent h location marked diagram damage
location damage damage finally state
agent h location doors closed damage
exact listed v comparison
approximation planner must consider whole state space therefore
measure cost directly terms space indirectly terms
time hand since exact optimal value function v
illustrative simple goals achievement use time discounting order
remain general mathematical convenience



fibaum nicholson dix

symbol
original

sd

meaning

abstract
w p


state space specific state space worldview resp
dimension state space
n
number dimensions
ss
ww
state



initial state
scur

current state line
sd sd
wd sd
dimension state w resp

set actions action space

default action


transition function formal
prt sas prt waw transition function use
r r
r wr
reward function one step reward
v sr
v w r
value function expected discounted sum rewards

discount factor reward
sa
wa
policy


optimal policy
v r

optimal value function exact value function

w
approximate policy ith approximate policy

exact value function note may abstract
v r
v r
v w r
approximate value function approx v
table summary notation first column notation original mdp
second notation non uniform abstraction applied

obtained along optimal policy ensuring agent expect obtain
value figures approximations must measure
uniform abstraction
one method approximation take advantage dimensions ignoring
irrelevant marginally relevant order obtain
approximate solution uniform sense dimensions ignored
throughout state space since attacks curse dimensionality
originates dimensions effective counteracting
dearden boutilier use obtain exact solution boutilier approximate one boutilier dearden dearden boutilier however abstractions fixed throughout execution dimensions deleted
pre determined sequence makes somewhat inflexible similarly
nicholson kaelbling propose technique approximate
delete dimensions sensitivity analysis refine abstraction execution time permits still uniform dietterich uses kind
abstraction combination hierarchical good effect subtask
navigate location ignore irrelevant dimensions location items


fiproximity non uniform abstractions

picked even ultimate destination agent generally time
description derived general source rather specified particular
uniform abstraction help gardiol kaelbling use dimensions relevant marginally ignoring approximate
solution improved progresses
unfortunately however least human specified one would generally
expect mentioned dimensions way relevant irrelevant dimensions
eliminated human designer natural course specifying
depending domain situation marginally relevant dimensions might
included often nearly enough effective approximation
list comparisons uniform abstraction reason
sample domains makes little sense almost dimensions
important solving case methods exist
effective uniform abstraction integrated easily
non uniform abstraction
approximation non uniform abstraction replaces state space w particular type partition originally introduced baum nicholson call
w worldview members w worldview states members specific
states non uniform abstraction intuitive idea ignoring dimensions
parts state space example door interest agent
walk ignored parts state space
distant door particular member worldview wi w dimension
taken account
concrete refined ignored altogether abstract
q completely
w singleton subset corresponding
w
coarsened wi



concrete dimensions equal sd abstract dimensions worldview
selection modification methods ensure w remains partition times
give example doors one possible worldview location
damage dimensions concrete every state door dimensions concrete
states within two steps respective door
note domain still fully observable question lack knowledge
dimensions question wilful conditional ignorance
matter computational expediency approximation subsumes exact
uniform abstraction exact dimensions set uniformly
concrete w worldview state corresponds one specific state
uniform abstraction combination abstract concrete dimensions fixed
entire worldview treated special cases general
previously used word envelope concept baum nicholson however
worldview better describes approximation used envelope
allow dimension partially considered abstract level dimensions
within dimension x coordinate particular value fully
abstract never instance
modified calculation reduces standard uniform fully concrete worldviews
planner obtains standard cases



fibaum nicholson dix

hand approximation longer markovian dimension
abstracted away indeterminate notation markov decision processes
represented distribution concrete states dimension
stochastic specific ignored value distinction important
truly stochastic outcome quite valid plan retry action
succeeds instance opening door doors dimension
merely ignored agent obtain outcome door closed time moves
region dimension ignored within worldview previous
states appear matter discuss section
comparison approaches
non uniform abstractions began appear literature first usually side effect
structured method state space represented decision tree
individual dimensions boutilier dearden goldszmidt note
however decision tree structure imposes restriction kinds non uniform
abstraction represented dimension root tree considered
throughout state space significant restriction
representation much limited representation similar restriction affects
de alfaro roys magnifying lens abstraction refinement multivalued dimensions taken bit bit bits interleaved level
decision tree halves space along different dimension pre determined order
note would work well dimensions correspond less connected
space gridworld would less well features doors
grid navigation domain magnifying lens abstraction calculates upper lower bounds
value function rather single approximation advantage guiding
abstraction selection allows definite termination condition lack
hand considers fully concrete states part limiting
space savings square root state space whereas work
mixture variously abstract states necessarily including fully concrete
ones another related variable grids used discretisation
indirectly used discrete domains boutilier goldszmidt sabata
dimensions reasonably approximated continuous instance money unlike
variable grids completely inapplicable predicates binary
enumerated dimensions reyes sucar morales use techniques
ways quite similar continuous mdps though quite different
ways consider refinement coarsening use sampling rather
directly dealing domain model use different refinement method
refinement evaluated fact committed rolled back
perhaps similar one modules steinkraus
ignore state variables module however module appears completely manual
requiring input variables dimensions ignored parts state
space uses values dimensions current state scur rather
distribution obviously restricts situations may used instance
doors doors could ignored starting state finally since


fiproximity non uniform abstractions

steinkraus analyse report relative contributions modules
solution meta selecting arranging modules
difficult know extent particular module useful
approaches take advantage different features different domains instance
factored mdp used instance boutilier et al guestrin
koller parr venkataraman suitable domains parts state
action spaces grouped together within group actions action
dimensions affect corresponding states state dimensions interaction
groups weak st aubin hoey boutilier iterate symbolic representation
form algebraic decision diagrams produce approximate solutions sanner
boutilier iterate symbolic representation whole class
domain symbolic dynamic programming first order algebraic decision diagrams
linear value approximation pre compute generic solution used
quickly solve specific class focus state space others approximate action space typically grouping actions possibly hierarchically
macro actions korf instance hauskrecht meuleau kaelbling dean
boutilier botea enzenberger muller schaeffer take
parr uses finite state automata macro actions srivastava immerman zilberstein take plans branches
loops goldman musliner boddy durfee wu reduce state space generating limited horizon undiscounted mdp different non mdp representation
including reachable states pruning detected clearly
immediately poor inferior equivalent already generated states naturally many
approaches combined instance gardiol kaelbling combine state space abstraction envelope work dean et al steinkraus
uses modular planner view combining many approaches may
appropriate given details approaches variants
refer reader recent survey field daoui abbad tkiouat
dynamic approximate
top level shown initialisation consisting
selecting initial abstraction setting policy value proximity
proportionally size worldview state respectively planner enters
infinite loop stochastically alternates among five possible calculations
described following sections elsewhere use
stochastic choice default absence directed method
agent assumed processing power available acting
continually improve policy modify approximation updates focus
current state means agent need plan
well unlikely possibilities therefore expend effort
likely paths closer future expecting reaches parts
state space improve approximation appropriate
initialising approximate policy action constitutes domain specific heuristic namely
known default action reasonably safe states nothing action



fibaum nicholson dix

high level approximate dynamic non uniform
abstractions
select initial abstraction
worldview states w
w v w p w w

policy value calculation
loop
choose stochastically
policy value calculation

policy refinement

proximity calculation

proximity refinement

proximity coarsening
input latest current state output policy

actual execution policy assumed separate thread executive
planner concern timeliness requirements
domain whenever action needs taken executive simply uses policy
recently received planner
dean et al call recurrent deliberation use locality
approximation similar architecture used circa system musliner durfee
shin goldman musliner krebsbach boddy guarantee hard deadlines
circa terminology planner ais ai subsystem executive
rts real time subsystem
alternative recurrent deliberation pre cursor deliberation agent first
plans finished begin act making
adjustments plan policy effectively planner current state constant
equal initial state throughout work pre cursor mode used
measurements involves fewer potentially confounding variables
conceptually divided two broad parts open ended selecting good abstraction relatively closed within
abstraction since latter part closed deal first next
section covering explore open ended part sections
covering

solving non uniformly abstracted mdps
given non uniform abstraction simplest way use take one
standard mdp modified policy iteration puterman
shin adapt non uniform abstraction minimally formulae translate


fiproximity non uniform abstractions

directly obvious fashion becomes function worldview states instead concrete
states shown simple variant update policy
w procedure probabilities transition one worldview state another
approximated uniform distribution concrete states possibly
distribution information available
policy value calculation
repeat n times
worldview states w
update value w
worldview states w
update policy w
update value w
procedure update value w
prt w w w
optimisation v w calculated directly case
v w r w

else
p
v w r w w prt w w w v w

procedure update policy
p w variant simple
w min arg maxa w prt w w v w

procedure update policy w variant locally uniform abstraction
see section discussion locally uniform abstraction
absdims
w prt w w w abstract
w abstract
absdims
lua w w
dimension w dimension w
absdims
p
w w


v w w w w v w
p
w min arg maxa w prt w w v lua w

note considered ordered set smallest element
minimum used arg max gives one possibility
two aspects domain specific heuristic instance breaking ties favour
default action possible b avoid policy basedp
refinement see section
actions equal value secondly efficiency w calculated
states w prt w w since states make contribution
sum finally number n tuning parameter particularly critical use
n
course replacing state space worldview w way general
preserve markov property since actual dynamics may depend aspects state
space abstracted worldview simple variant ignore assume
markov property anyway grounds approximation
unfortunately resulting performance unacceptably large error including
outright non attainment goals


fibaum nicholson dix

instance doors situation occur three
doors whenever abstract concrete near door question
doors relatively difficult open probability success per try
hand moving area abstract area
concrete assumed probability door already open
calculations performed turns preferable plan loop repeatedly trying
illusory chance success rather attempting open door
chance success agent never reach goal worse still ways
estimate quality solution quite good v
fact better even optimal solutions v true quality
solution poor v corresponding never reaching goal
incurring damage figures discounting factor
regions take account particularly bad piece information may seem unattractive described vice versa call ostrich effect
agent refusing accept unpleasant fact mythical ostrich buries
head sand solution locally uniform abstraction described next section
abstracted approximation simply treated mdp agent
know state reach near closed door near open door correspond
underlying process might reach particular state deterministically
especially obvious example planner plans loop
reminiscent noted cassandra kaelbling kurien
plan derived pomdp failed actual robot got loop particular
situation sensor completely reliable contrary model
locally uniform abstraction
ostrich effect occurs states different abstraction considered instance
one door abstract one door concrete closed
solution make abstraction locally uniform therefore locally markovian
duration policy generation iterative step making abstraction locally
temporarily uniform iterative step policy generation never work
across edge abstract region since information available
states considered point impetus favoured
avoided basis instance avoiding state door concrete closed
favour one door abstract action chosen chosen
information presence absence
modification update policy w procedure
states considered one one region around state accessed
function returns locally uniform version states concrete state
considered averaged ignore distinctions different states
considered sometimes states taken sometimes estimated
values v averaged adjacent states means dimensions
partially considered states cases mean
concrete region must extend one step beyond region dimension


fiproximity non uniform abstractions

immediately relevant dimension fully considered state possible
outcomes actions state must concrete dimension
modified procedure proceeds follows first dimensions abstract
possible outcome state updated w collected variable absdims
function lua constructed takes worldview states w returns potential
worldview states w w abstract dimensions absdims
core modification named lua locally uniform abstraction since
potential states returned lua general members w therefore
necessarily value stored v function v constructed calculates
weighted
averages value function v potential states sum
p

calculated states w w w efficiency finally

w
update step carried two functions lua v
unfortunately modification applied may may converge depending worldview failure converge occurs concrete region
small cases cycle two policies conceivably
instead converging one must careful therefore worldview avoid
situations else detect modify worldview accordingly policybased worldview refinement described section ensures convergence
practice

initial abstraction
beginning planner must select initial abstraction since
worldview never completely discarded planner infelicity stage may
impair entire process worldview improvement
make amount weakness
different ways select initial abstraction propose one heuristic
method selecting initial worldview description
variants consider example door doors associated
two locations immediately side makes sense consider
status door two locations association read
specification intuitively structure solution likely resemble structure
incorporates structure transition function initial
worldview reward function incorporated reflecting assumption
dimensions reward important
use two step method derive initial worldview shown
firstly reward function specified particular dimensions make
dimensions concrete throughout worldview leave dimensions abstract
doors x dmg dimensions step
states worldview
secondly transition function specified decision trees one per action use
nexuses dimensions linking points points
dimensions interact nexus corresponds one path root
tree leaf example doors decision tree open action
contains leaf whose ancestors x stochastic node choices leading


fibaum nicholson dix

select initial abstraction
set worldview completely abstract
w
reward step
reward step enabled
dimensions mentioned reward tree
refine whole worldview dimension
nexus step
nexus step enabled
leaf nodes action trees
worldview states w matching pre state
refine w dimensions mentioned pre state

leaf labelled respectively closed corresponds nexus
sx sy sd closed stochastic node ignored determining nexus
total four nexuses side door two locations immediately
adjacent shown figure connecting relevant door dimension x
coordinates initial worldview shown figure b x dmg concrete
everywhere doors abstract except concrete one location directly
side door corresponding location nexuses figure
steps w compared specific states

x



















x
















































































b

figure nexus step initial abstraction showing location nexuses
doors four nexuses b locations
door dimensions concrete initial worldview



fiproximity non uniform abstractions

keys location nexuses figure except
nexuses location involve corresponding
key dimensions thus initial worldview locations shown figure b
concrete corresponding door dimension closed
corresponding key dimension states doors open key dimension
remains abstract initial worldview size keys w
due locally uniform abstraction concrete door dimensions taken
account minimal degree worldview used without
refinement expected resulting policies would poor
bear expectation worldview initialization methods therefore
intended used rather basis refinement thus
real test methods well work coupled worldview
modification methods described

policy refinement
section presents first worldview modification methods policy refinement method modifies worldview directly current approximate
policy particular refines states differences actions planned
adjacent differently abstract states differences indicate dimension may
important adjacent states abstract dimension refined e
dimension made concrete states
method previously introduced baum nicholson showed
small navigation domain example doors
refinement method resulted good policy though optimal present quantitative consider complex domains
motivation
motivation method twofold firstly already indicated method detects
areas particular dimension important affects action planned
ensures concrete adjacent states thus regions dimension taken
account expand long dimension matters stop secondly
method fulfils requirements choosing worldview avoid non convergence
policy calculation mentioned section
dimensions important affect policy since policy planners
output less important parts state space affect
policy thus dimensions need concrete remain abstract
gleaned part state space comparing optimal actions
states optimal actions equal states abstract
differ states concrete however optimal policy
approximate policy worldview difficult however planner compare
policies areas dimension concrete found important
expand area concrete policy refinement policy calculation
omitted uninteresting presented baum



fibaum nicholson dix

alternate refinement continue area dimension concrete covers
whole region important
section noted requires worldview chosen
care described section detects situations potentially problematic locally uniform abstraction modifies worldview preclude
intuitively incorrect behaviour occurs edge concrete region intersects
place two fairly similarly valued courses action corresponding
two different paths goal
method
method uses transition function definition adjacent states worldview states w w considered adjacent prt w w definition
symmetrical general since transition function
method seen shown
policy refinement
candidates
worldview states w
actions
w p r w w

dimensions
w abstract w concrete
w

abstract


construct w
dimension w dimension w

b

w w w wb wa w wb w
policy throughout w
candidates candidates w
w candidates
w w
replace w
group states concrete
anew
w
concrete

wnew
dimension wnew dimension w
w w wnew

wnew w v wnew v w p wnew w w p w
w w w discarding stored w v w p w

example doors instance applying method
increases number worldview states initial depending
stochastic choices recall comparison produces concrete regions
nice tight around doors shown figure allowing
converge reasonable solution solution fact optimal given initial
state though simply coincidence since taken account
states somewhat suboptimal actions agent would reach
goal states shortest route


fiproximity non uniform abstractions

x



















x
































k

















k





























k






k









k







k k k





k k k







b

figure example non uniform abstraction doors b keys
policy refinement x dmg dimensions concrete everyd
indicate corresponding door concrete k


k k indicate corresponding door concrete corresponding
key concrete door closed

worldview obtained method often quite compact instance rather
refining simple rectangular region side door doors human
might makes locations concrete side door
enough obtain good solution seen north sides doors
well west side door concrete locations due edge
departure side doors even better makes refinement
south door east door action move toward goal regardless
status door actions equal refinement takes place
south side door seems rather less compact concrete area fact
big locations doors seems excessive compared compact
concrete areas elsewhere occur nexus close region
best action take genuinely depends status dimension nexus
difference small somehow agent found h policy
refinement independent scur optimal path genuinely would depend whether
door open path slightly suboptimal case theory
region could arbitrarily large extent seems relatively minor effect
practice instance adds couple states w
found real domains domains used baum



fibaum nicholson dix

limitations
policy refinement deal cases single dimension makes difference two dimensions needed combination often miss
instance keys key quite distant corresponding door
policy refinement therefore never relationship two
key appears reason pick door appears
means unlocking
obviously fixed ad hoc rewarding picking keys sake
indeed domain formulations literature exactly rewarding agent
partial achievement goal however clean solution effect
domain specifications cheat providing hints
another policy refinement provide coarsening
worldview modifying ways instance execution progresses
planner needs update plan indeed policy refinement ignores initial state
altogether current state scur recurrent thus produces
solution regardless part agent actually asked solve
waste computation solving parts agent unlikely actually
visit perhaps importantly carries penalty corresponding loss
quality relevant parts
following sections describe proximity worldview modification needed
solve domains combinations dimensions important makes
use scur appropriate

proximity measure
general worldview mostly concrete near agent planned path
goal allow detailed mostly abstract elsewhere conserve computational resources section describe measure originally baum nicholson
realises concept proximity p decreases state
future less probable section extends brief description baum
nicholson following section present worldview modification
methods directly measure
motivation
proximity p realisation intuitive concept states near agent
likely visited opposed distant agent unlikely naturally
takes account current state scur recurrent initial state
pre cursor unlike policy refinement ignores altogether thus
planner selecting worldviews proximity measure produce solutions tailored
particular scur ignore parts mdp irrelevant nearirrelevant performance state thus saves computation would otherwise
baum nicholson used word likelihood measure prefer proximity
avoid confusion meanings word likelihood munos moore use word
influence somewhat similar measure continuous domains



fiproximity non uniform abstractions

wasted solving parts agent unlikely actually visit perhaps
importantly carries advantage corresponding gain quality
relevant parts allows agent deal keys beyond
reach policy refinement
implicitly agent plans reaches mostly abstract parts
state space improve approximation appropriate planner thus continually
improves policy modifies approximation updates focus
current state scur means refining regions agent finds
likely visit coarsening away details regions longer likely
visit already traversed
three aspects proximity temporal spatial probabilistic firstly
temporal aspect indicates states may encountered near future exponentially decaying scale second aspect spatial nearness states terms
state space agent planned path spatial aspect somewhat indirect
spatial structure domain represented implicitly transition
matrix proximity measure reflect two aspects combined
proximity give single real number state denoted p p
proximity spatial aspect temporal aspect number
interpreted probability namely probability encountering state p
interpreted probability distribution states giving final probabilistic
aspect proximity
calculation
formula proximity p similar formula value function
three differences firstly instead beginning reward function
current state function cur secondly transition probabilities time reversed
matrix transposed value calculation
reward function occurs future taking actions current state
function present taking actions since order taking actions
function upon formula reversed time similar reversal must
b used
applied transition probabilities thirdly estimated future policy
b
b
instead estimate stochastic policy defined making distribution
actions assigns constant probability current distributes
remaining probability mass among actions equally distributed probability
mass corresponds probability policy change sometime future
alternately probability currently selected action yet correct
formula therefore
x

b p
pr

p cur p







p proximity discounting factor p

p scur
cur

otherwise




fibaum nicholson dix

p
constant p chosen current state function p converges
words p probability distribution checked near future
agent probability p state assuming follow policy
near future defined probability checking time proportional
pt p interpreted stopping probability value calculation
one instead solve set linear equations
x

b p
pr

p cur p




matrix notation
p tbt p cur



b identity
tb transition matrix induced stochastic policy
matrix implementation uses matrix form shown proximity
measure needs little adjustment work non uniformly abstract worldview
simply replaced w scur becoming scur w
proximity calculation
solve matrix equation p linear system
p tbt p cur
measure two tuning parameters replanning probability discounting
factor p replanning probability controls spatial aspect trades focus
likely path less likely eventualities nearby similarly p controls
temporal aspect smaller p short sighted greedy
conversely p close planner spend time future
might better spent set depending
reward discounting factor mode planner use p
replanning probability
example proximities doors shown figure initial situation agent h doors closed possible situation later execution
agent h doors closed larger symbols correspond higher proximity one
immediately see agents planned path goal large symbols correspond
states agent expects visit conversely small proximities locations
agents planned path goal example agent expect visit
states south western room especially already passed door
similarly proximities around initial state much lower agent
h expect need return
discussion
one interesting feature resulting numbers emphasise absorbing nearabsorbing states somewhat might intuitively expected however considering


fiproximity non uniform abstractions

x



















x









































hx



















hx

figure proximities doors possible later scur symbol size
logarithmic proximities range p replanning probability

absorbing states general important good feature especially since
normally planner try minimise probability entering absorbing state
unless goal feature help ensure absorbing states kept
mind long chance falling dean et al instance
note undesirable absorbing states along path goal
tend come candidates removal consideration due low probability
reaching current policy make special accommodation
removed consideration proximity measure emphasising
states special handling necessary
contrast kirman uses probabilities es steps
es estimate number steps agent take switching
previous policy policy currently calculated assumes es
estimated well current policy policy executive oneplanning cycle probability appropriate measure fact one would prefer least
two cycle look ahead agent begins within area focus
policy remains throughout validity policy probably
longer since planners foresight extend beyond next thinking cycle
philosophically reliance cycle length desirable
artefact planner rather intrinsic domain
somewhat related prioritised sweeping see instance barto bradtke
singh present defines measure states
way interesting unlike applies measure determine


fibaum nicholson dix

order formulae v calculation applied
applied preferentially interesting states less frequently uninteresting
unimportant states well known order calculation mdp
varied greatly without forfeiting convergence optimal policy
prioritised sweeping takes advantage often done measure change
v previous calculations approaches use look ahead current state
ways simple version proximity fact corresponds
threshold p replanning probability set proximity measure p might well
good candidate apply v calculation states chosen directly
according p distribution
munos moore use influence measure deterministic continuous
domains similar p fact main difference measure
two parameters uses replanning probability
effectively zero means cannot take account replanning neither
difference horizon entails possibility policy may change
acted upon absorbing states instance would emphasised
proximities

proximity dynamic abstraction
proximity measure described previous section used focus planners attention areas likely useful near future firstly means worldview
made match proximities refining coarsening appropriate secondly since proximity measure takes account current state method
automatically update worldview agents circumstances change recurrent
mode execution concurrent
refinement
high proximity indicates states agent likely visit near future
planner therefore plan states carefully abstract reason
refine allow detailed states high proximity
therefore considered candidates refinement
high proximity defined simple threshold shown
refinement occurs anomaly sometimes appears anomaly led
policy refinement method arises different levels abstraction
adjacent abstract state causes rather recentlyrefined one state refined values v states initially estimated
states previous value v however typically means
overestimated others underestimated policy calculated
state overestimated value attractive
since directly follows moment refinement self correcting
iterations planner converges correct policy values however
retaining theoretical guarantee convergence desired care would taken since

p zero states reachable current state practice course optimality
otherwise unreachable states immaterial



fiproximity non uniform abstractions

proximity refinement
stochastically choose dimension
worldview states w
p w threshold w abstract
replace w
group states concrete
anew
w
concrete

w


dimension wnew dimension w
w w wnew

wnew w v wnew v w p wnew w w p w
w w w discarding stored w v w p w

transient anomalies appear policy worst case
planner may replan path refine states trigger
anomaly rather large parts state space spuriously refined way
occurs combined v calculation phase may update
v chance converge solution create variant phase v
calculation replaces v calculation phase values stabilise
two iterations appears sufficient alternative solution would
copy difference values adjacent concrete states
possible thus obtaining better estimated values newly refined states however since
simpler solution v calculation works satisfactorily complex possibility
explored
coarsening
low proximity indicates states agent unlikely visit near future
planner therefore need plan states carefully usually already
abstract never refined first place however concrete
previously refined reason coarsen free memory
cpu time detailed elsewhere states low proximity
therefore considered candidates coarsening
proximity coarsening useful primarily line scenario recurrent agent moves state space current state
scur changes states likely visited near future especially useful agent finds unexpected part state space instance
due low probability outcomes agent planned path leading part way
goal perhaps partial reward case however parts state
space already traversed coarsened favour refinement front agent
one might imagine progresses planner may wish concentrate
different parts state space coarsening might useful cull abandoned
explorations switch focus however observed domains
states already traversed cannot discarded even agent never visit since
worldview partition since agent necessarily know whether need revisit
end revisiting states



fibaum nicholson dix

found pre cursor mode coarsening generally worsens quality policies
positive contribution
coarsening proceeds three steps shown first step
similar proximity refinement time proximity coarsening phase
invoked worldview scanned states low proximity threshold
put list candidates second step tricky coarsening needs join
several states one however representation allow arbitrary partitions
worldviews therefore allow coarsening together arbitrary set
worldview states planner must therefore group states among lowproximity candidates coarsened valid worldview state groups
detected fact differ one dimension size
dimension therefore covering completely finally groups found
replaced single abstract state
proximity coarsening
collect candidates coarsening
candidates w p threshold
groups candidates coarsened together
partition candidates according pattern abstract concrete dimensions
patterns candidates wa wb wa concrete wb concrete
groups
p patterns
dimensions
states p concrete
partition p dimensions except giving potential groups
potgroups p wa wb dimension wa dimension wb
add potential groups size dimension groups
groups groups g potgroups g sd
replace group states single abstract state
g groups
g w
stochastically choose
wa g

w
abstract
construct wnew
dimension wnew dimension wa
w w wnew
p
p



wnew wa v wnew g
wg v w p w
wg p w

w w g discarding stored w v w p w w g

cases may impossible coarsen section worldview despite low
proximity due situation somewhat akin grid lock probably simplest example
one figure shows worldview five states three dimensional binary
specific state space three states ignoring different dimension
remaining two take account three situation group states


fiproximity non uniform abstractions























































w

w

w

w

w

figure non uniform worldview cannot immediately coarsened state space
three binary dimensions eight states worldview two concrete states
w w three abstract states w w w abstract different
dimension

coarsened single dimension coarsening possible one states must
first refined low p candidates proximity
refinement integration uniform abstraction method coarsening
would straightforward selecting initial worldview refinement
unless worldview kept uniform however even non uniform worldview would
difficult instance dimension could simply removed possible
rather everywhere


run range different domains demonstrate
domains divide two broad groups first group consists grid navigation
domain domain intuition gathered preliminary runs
done however domain well performs
cannot generality second group consists domains literature
demonstrating well generalises
experimental domains
introduce domains section first five grid navigation
domain two already described section shown figure three additional
remaining domains domains literature particular
used kim barry
described section grid navigation domain shown figure
x dimensions size three door dimensions binary open closed
damage dimension binary far doors keys
keys agent must pick keys open corresponding doors
three remaining key shuttlebot key
similar keys except agent capable holding one key time
instead three binary dimensions keys one four valued dimension
indicating key agent holds none shuttlebot introduces
cyclic goal extra loaded dimension damage dimension tri valued


fibaum nicholson dix

grid navigation domain
keys world

doors

key


keys

shuttlebot



keys held time






note

cyclic
tiled

dimensions













b robot k domain

robot
robot
robot
robot

dimensions











c factory domain

factory
factory
factory
tireworld domain
locations

tire small


tire medium
tire large


tire large n

initial
n
n
n
n

dimensions




goal
n
n
n
n






route length





dimensions











table experimental domains dimensionality state
space size

variant increases size tiling grid
direction two extra dimensions xx yy size table summarises
domain
next two domains kim firstly robot k domain
kims robot k domain reducing number actions
four robot k domain consist cycle k rooms shown figure
room light analogous doors doors
enable agent move four actions variant go forward turn light
current room nothing original formulation allowed agent
combination toggling lights going forward total k actions
reduced intended approximate action space
goal move first room last k dimensions state space
k k states listed table b


fiproximity non uniform abstractions


k








figure robot k domain
drill b
part b

shape b

drilled

polish b

polish b

shaped

dip b
polished

spray b
handpaint b

painted
glue

connected

bolt
drill
part

shape

shaped

drilled
polish

polish

dip
polished

spray

painted

handpaint

figure factory domain
kims factory domain series variants simple manufacturing represented purely predicates dimensions size agent make
product two parts must drilled painted finally joined together
figure shows simplified diagram omitting interactions options
instance achieve painted predicate agent may spray dip handpaint
object connect two objects may use glue bolt latter requires
drilled unlike domains partial rewards available
agent achieving certain subgoals used listed table c
final domain tireworld domain icaps ipc competition
littman weissman bonet used barry domain robotic
car trying drive point point b car room carry one spare tire
locations additional spare tires locations car
carrying spare pick one n locations car
n binary dimensions follows n dimensions used represent
location car valid states states one location dimension
true explicitly stated anywhere domain another n dimensions
used represent locations spare tire final two
dimensions represent whether car carrying spare whether flat tire
domain previously used hoey st aubin hu boutilier
builder domain dearden boutilier adapted standard job shop scheduling
used test partial order planners
touch aspect discussion section case include domain without
change order facilitate comparison literature



fibaum nicholson dix

n

n
n
n

n

n
n

n

n

n

n

n
n
n

n

n

n

n

n
n

n

n
n

n

n

n
n
n

n

tire small

n

n

tire medium

n

tire large

goal
locations
figure tireworld domain indicating initial

barry uses two tireworld labelled small large
small tireworld locations variables actions large
one locations variables actions curiously large
direct road initial goal locations takes single action
solve makes difficult assess whether barrys method fact
scaled addition two created medium sized tireworld locations
variables removing locations large tireworld moving initial
location n goal variants listed table shown
figure final variant tire large n shown identical large tireworld
except initial location moved n
direct evaluation policies pre cursor deliberation
smaller doors key keys directly evaluate
approximate policies produced planner running pre cursor deliberation
small enough use exact calculate actual value
function corresponding approximate policies noted section
useful involves fewer potentially confounding variables exploit full
potential
table shows policy refinement
proximity methods disabled table
lists size value optimal solution initial state
proximity coarsening primarily aimed regions state space agent
already traversed pre cursor deliberation traversal coarsening would therefore
expected bring limited benefit pre cursor deliberation direct evaluation would
meaningful evaluate performance therefore evaluated recurrent deliberation



fiproximity non uniform abstractions

olu
tio
nv
alu
wo
e
rld
vie
w
siz
e
rel
ati

wo
rld
vie
w
siz
pla
e
nn
er

esti
sol
uti te

val
ue
act
ua
ls
olu
tio
nv
alu
e

ize

op
tim
al


sta
te
sp
ce


dis

cou
nti
ng

fac
tor

v representing costs exact followed
size worldview w absolute number percentage planners
estimate value solution initial state v actual value
solution initial state v first half part table discounting
factor second half averages runs
shown planner run chosen approximation
assumed phases sufficient planner converge practice
convergence generally took place much earlier detected however
overall assumption planner continues plan forever responding changing
inputs makes convergence somewhat irrelevant




v
w
policy refinement

doors


key

keys




doors

key



keys

b proximity refinement
doors

key

keys


doors

key

keys

c policy proximity refinement
doors

key

keys


doors

key

keys


w


v

v
































































table direct evaluation policies pre cursor deliberation three
different refinement methods evaluated phases



fibaum nicholson dix

part table divide neatly two types without keys doors
planner succeeds ten runs getting perfect policies given starting
state two key keys invariably fails two
agent must pick key far door opens version
planner simply cannot think ahead extent three planner
somewhat optimistic estimating better value obtains cases even
better optimum instance doors planners
estimate value v better true value
optimum v v fractional w table due
averaged ten runs final size worldview sometimes depends extent
order dimensions states refined order randomised
runs instance doors w
sizes ranging states end ten runs average

part similar two values main difference
smaller leads smaller numbers instance value indicating failure


values tend multiples

smaller value rather cases
smaller range make differences less obvious instance estimated value

column v clear whether numbers approximations

failure reach goal
minus small number representing success

represent rewards costs
units represent rewards costs multiples
obtained perpetuity however expected desired behaviour smaller
represents disinterest distant future reward cost perpetuity
much important reward cost
table b shows ten runs pre cursor mode proximitybased refinement policy refinement coarsening
seen doors solved optimally cases
surprising complex
key keys interesting figures table b arise
average successful runs values close equal optimal values
v unsuccessful runs values
planner found successful policy runs key times
keys similarly case times times

respectively since optimal path quite long compared
success
means reward failure punished effect
difficult discern
table c shows proximity refinement policy refinement combined coarsening naturally doors refinement
method alone already obtained optimal policy shows improvement worldview size w differs slightly table b policy refinement sometimes
directed w tend slightly smaller exploratory proximitybased refinement alone larger policy refinement alone


fiproximity non uniform abstractions

two key keys improvement compared
refinement methods alone solved runs
values represent averages unsuccessful runs
successful ones compared successful runs proximity
refinement successful runs policy refinement
key solved runs due discounting length
path goal near horizon success meaning reward
failure receives distinction great keys
solution parameters receives uniform
runs suboptimality one unit
behaviour runs generally quite straightforward typically
initially calculating agent cannot reach goal initial worldview
worldview size gradually increases plateaus coarsening movement agent behaviour really possible successful runs
planner plans route goal point increase worldview becomes sufficient v quickly reaches final value rarely v may oscillate
twice first omit graphs presented baum
evaluation simulation recurrent deliberation
larger performance evaluated simulation running agent
simulated world observing reward collects direct
evaluation possible calculating actual value function exact
longer tractable simulation recurrent delibertion context
coarsening evaluated comparison section presents
keys evaluated simulation without coarsening
figure shows representative sample simulation keys
refinement methods coarsening combination options
shown table c previous section evaluated simulation rather directly
small graph shows different individual run seen agent behaves
reasonably working recurrent mode simulation
left vertical axes graphs represent reward r plotted thick red lines
run figure instance agent starts receiving reward
step meaning goal damage domain onwards receives
reward per step meaning goal damage right vertical axes
worldview size w thin blue lines shown details throughout section
scaled actual worldview sizes rather full ranges taking run
figure see w grows relatively quickly continues
grow slowly eventually levels little full state space
comparison run agent received reward similarly state space
grew longer eventually levelling somewhat runs agent
failed reach goal continued receiving reward throughout run
worldview size levelled little run steadily grew
horizontal axes simulated world time corresponding discrete time steps
mdp two time scales simulation wall clock time indicating


fibaum nicholson dix



w

r





























w

r























w

r





time







time

r








w















































time












time

figure simulation keys policy proximity refinement
coarsening four runs reward left axes thick red lines worldview size
w right axes thin blue lines detail world time horizontal axes

passage real time number phases planner performed simulation configured take time step per wall clock time number phases
r
controlled simply given speed ghz intel
cpu implementation coded flexibility rather efficiency ideally agent gradually
move general direction goal simplifies
fast agent runs far ahead abstraction planner
planner terminate since planner assumed keep
agent keep acting indefinitely goal oriented domains
examples one might consider achieving goal termination
condition example domains assume agent continue
goal goal maintenance albeit trivial b apply non goal oriented
domains c even goal oriented domains clear apply condition
case agent fails reach goal simulation therefore runs
terminated manually succeeded appeared progress


fiproximity non uniform abstractions



w

r



























time

w

r












time

figure simulation illustrating effect coarsening worldview size keys
policy refinement proximity refinement coarsening
two runs

likely made run fixed number world time steps selected
manually terminated runs allowance variation
coarsening figure worldview sizes monotonic increasing
different runs refined differently domain stochastic
beginning agent receiving reward per step
yet goal worldview size increases planner eventually finds policy
leads goal runs seen better reward obtained
runs simple relationship worldview size performance runs
worked worldview larger generally succeeded smaller
worldviews generally vast majority runs agent reached
goal
coarsening activated compared situation turned
reward gathered agent declines slightly still reaches goal
vast majority runs figure shows two runs one successful one
unsuccessful keys proximity coarsening well two
refinement methods contrast figure two refinement methods
used note effect interleaving refinement coarsening worldview
size w thin blue line longer monotonic instead alternately increased
decreased shows jagged line graph slightly fewer runs reach
goal decline solution quality expected however since goal coarsening
reduce size worldview
completeness tested agent proximity methods
active policy refinement deactivated
configuration agent collects reward generally takes steps toward
goal without directed policy refinement largely exploratory
proximity methods discover keys consequently cannot reach goal


fibaum nicholson dix

effect discounting factor
shuttlebot similar doors requires agent move
back forth two locations repeatedly interesting preliminary
runs pre cursor mode solved solved optimally
considered whether case might behave better
simulation agent took advantage possibility nearest
reward replanning reward obtained agent could function
well even none policies good solution however illustrated
figure agents behaviour similar pre cursor case
refinement run b coarsening run would pick reward
immediately adjacent setting planners discounting
factor c coarsening runs provided much better performance
note effect balance refinement coarsening b c
worldview size w nice steady throughout runs though admittedly fair
fraction
initial worldview
standard initial worldviews large planner even
modified smaller initial worldviews obtained enabling nexus step
disabling reward step large disabling reward nexus steps
singleton initial worldview w treats entire state space
single abstract worldview state unfortunately means planner
starts little way hints direction refine least
initially information base crucial decision upshot
collects reward cases remains initial state others moves around
state space sometimes distance times small loop
reach goal subgoals
situation factory domain kim fact
agent collected reward simulated runs even though quite runs
substantial actions taken similar occurs x grid
navigation domain reward obtained agent
standard initial worldview somewhat large planner singleton
initial worldview badly best runs agent took limited steps
general direction goal
interesting case tireworld domain tire large large
standard initial worldview fails obtain solution reward step initial
worldview however manually chosen initial worldview refines locations
along path start state goal begins planner solves
tire large tire large n runs one less
one minute although atypical
rewards appear two horizontal lines runs one solid one broken task
cyclic agent collects reward twice cycle reward steps
details unsuccessful runs including w behaviour given baum



fiproximity non uniform abstractions

coarsening one run
r
w



b coarsening one run
r
w























































time







c coarsening two runs
r
w





w

r
















































time








time












time

figure simulation illustrating effect discounting factor shuttlebot
policy proximity refinement

worldview size quality
finally consider effect worldview size quality robot domain
agent moves series rooms lights domain excellent example
simulated agent works well runs robot robot
agent thought small amount time quickly moved goal stayed
small worldviews seen figure robot
four representative runs shown two two refinement methods
runs two three methods runs four runs worldview
sizes w reasonable consider full state space contains almost half million
states state worldview represents fifth percent despite small
worldview size however planner effective dozen phases agent
reached goal planner works well robot robot
comparison kims largest robot k robot though since
robot actions robot k domain


fibaum nicholson dix

coarsening two runs
r


w





w

r


































time

time

b proximity coarsening two runs
r
w




w

r


































time

time

figure simulation robot policy proximity refinement without proximity coarsening

direct comparison would valid hand values k
necessarily powers since unlike kim domain specification
considers room numbers atomic rather binary numbers particular
advantage powers
robot beginning interesting
robot robot figure showing two runs coarsening
runs agent succeeds reasonably promptly reasonable worldview
sizes however illustrated figure b coarsening active planner fails
reach goal runs example run succeeds others
example run state space contains almost million states
successful worldviews figure order full state space size
figure shows four representative runs robot two without coarsening runs b two three methods runs state space million states effect noted robot


fiproximity non uniform abstractions

coarsening two runs
r


w





w

r






















































time





w

r













































time

b proximity coarsening two runs
r
w















time












time

figure simulation robot policy proximity refinement without proximity coarsening

much pronounced without coarsening planner tends much larger worldviews large worldviews cause planner run slowly noted section
horizontal axes world time time relation two
varies quite significantly runs two policies per time step
smaller worldviews less one ten time steps worldviews grew
large
far reaching goal concerned two cases similar successful
runs maintain reasonably sized worldview runs runs
worldview size grows big invariably fail runs difference
time smallest successful worldview run figure used around well chosen
worldview states full state space worldview grows
beyond miniscule fraction state space even state worldview
note run plotted different scale worldview size w axis compared runs
makes details behaviour easier see makes size run less obvious



fibaum nicholson dix

coarsening two runs
r






w
















w

r

















time



time

b proximity coarsening two runs
r
w



w

r


















































time



time

figure simulation robot policy proximity refinement without proximity coarsening note
different scales worldview size w axis run

run figure planner stall progress
possible even challenging environment agent reaches goal almost half
runs

discussion
section presented across range experimental domains showing method
successfully finds solutions much less full state space
well limitations section discuss analyse
features domains method exploit give difficulty
smaller could directly evaluate policies produced method
pre cursor mode allowing us better isolate behaviour planner without
proximity methods worldviews quite small planner could solve


fiproximity non uniform abstractions

doors however even better uniform abstraction could
little even oracle could best remove one door key keys
two doors doors giving relative worldview size however planner
used uniform distribution removed dimension agent would fail anyway since
opening doors left worldview would harder hoping best
assumed open door actually closed succeed would
deduce abstracted doors considered closed considerable feat
function sample domains circumstances uniform abstraction could
effective pre processing step integrated w selection
methods
expected turning proximity refinement would
lead larger worldviews general one would expect larger worldviews yield better
solutions smaller worldviews yield worse solutions lower computational cost
means proximity refinement general improve solution quality
corresponded expectation worldviews indeed larger
solution quality higher
larger performance could evaluated simulation running
agent simulated world observing reward collects
direct evaluation possible calculating actual value function
exact longer tractable section therefore presented
keys comparison obtained direct evaluation policies
pre cursor deliberation discussed seen correspond crossconfirming evaluation methods
addition simulation recurrent delibertion context coarsening
could evaluated proximity coarsening activated compared
situation turned reward gathered agent declines slightly
impression worse performance somewhat misleading due fact first
comparison takes place one small planner able solve
without coarsening fact without abstraction thus disadvantages
lower reward collected much apparent advantages lower computational
cost
larger working without abstractions option balance
reversed fact somewhat counterintuitively larger
coarsening active successful runs smaller worldviews unsuccessful
runs clearly size worldview determines success quality
good worldview enabled efficient calculation policies progress toward goal
remaining small poor worldview simply grew larger smaller growing
worldview may eventually covered state space detail thus masking
effect planner would good policy effectively without real approximation
larger finding good policy without approximation feasible
similarly growing worldview simply slowed planner progress
made situation worldview reducing action proximity coarsening
became crucial ensuring least worldview remained tractably small
thereby enabled planner deal


fibaum nicholson dix

seen coarsening successful task
time agent paused replan part way goal reduces size
worldview keep relevant changing circumstances runs however
worldview size grew beyond capabilities planner cases
x settled higher balance others appears simply
continued growing latter case would appear simple question
tuning parameters balance appropriate worldview size
appears factor quality worldview determining
success failure runs whether balance reach goal grow
big fail
number solved poorly due initial abstraction
selection produced large
worldview exceeded available memory immediately shortly afterwards
possible small worldview ineffective
could set produce medium sized worldview none four combinations
options produced one singleton worldview possible
steps initial abstraction selection disabled resulting worldview aggregating
states single maximally abstract worldview state leading typically reward
collected agent best would take actions general direction
goal considerably worse previous work instance kim
obtains approximate solutions larger variants others
use domain variant including hoey et al originators dearden
boutilier
necessity singleton initial worldview understandably greatly
hurt performance infelicity worldview initialisation stage could impair
entire process since worldview never completely discarded planner
worldview improvement could made amount weakness
initial worldview singleton worldview poor starting point indeed
similar observation made dean et al work reduced envelope
states subset state space high level
work regardless initial envelope worldview practice however better
initial envelope chosen intelligence instance contain least
possible path goal goal oriented domains path simple
depth first search directly applicable worldviews
gradations abstraction overall concept remains reasonable initial worldview
crucial
tireworld confirm initial worldview reward step
enabled small since domain rewards single dimension
ineffective planner given better initial worldview one could
plausibly calculated became quite effective even modified tire large n
initial state deliberately moved goal seems
basic general worldview selection modification methods
less good worldview selection less fundamental apect
easily supplemented additional methods even tuning domains


fiproximity non uniform abstractions

tireworld seems modified predicate solver generate plausible trajectories
current state goal would well part worldview selection
interesting compare sanner boutilier
tireworld describe passing extremely poorly approximated
going manually tweak domain planner adding information
locations mutually exclusive makes much easier largely invalidates
comparison approaches fair question however extent
weakness planner extent artefact domain combination
representation narrative seems rather unfortunate narrative obvious
n intuition tend obscure real features real applicability
propositional representation hinder rather help intuition would occur
tighter fit representation narrative
others course solve tireworld domain well barry kaelbling
lozano prez generate full policy others take advantage initial state
planner would difficult know extent planners adapted
domain extent flexible seems recent years
become common planners tested domains researchers
access development icaps ipc domains rather
greater lesser degree hand tuned usually unconsciously particulars one
another domain undoubtedly unconsciously tuned grid navigation
domain
interesting side point provided shuttlebot solved
collecting trivial reward immediately adjacent
solved optimally since simulator
intrinsic discounting factor reports reward collected one see
even though planner working discounting factor provided
better solution case worked first
place
ways better behaviour smaller planner discounting factor reasonable agents horizon represented world discounting factor
horizon particular policy therefore planner effectively much shorter
policy supplanted one relatively soon thus may useful
occasion set planners discounting factor lower true world discounting factor order facilitate however may lead suboptimal short sighted
policies

conclusions
theory markov decision processes provides optimal however
larger domains intractable approximate solutions necessary
state space expressed terms dimensions size resulting computational cost exponential number dimensions fortunately
structured state space effective approximations possible
similarly kolobov mausam weld report variant tireworld rather
tireworld without providing explanation



fibaum nicholson dix

selectively ignoring dimensions parts
state space order obtain approximate solutions lower computational
cost non uniform abstraction dynamically adjusted line
situations execution progress different dimensions may ignored different parts
state space strong implications since resulting approximation longer
markovian however intuitive practical synthesis
two existing approaches structure approximation uniform abstraction
dynamic locality approximation envelope methods envelope methods
limited reliance initial worldview envelope poor
tend perform poorly overall subsumes uniform abstraction completely
treated special case general method
extends preliminary work baum nicholson modifying
worldview proximity measure enlarging reducing size
evaluating behaviour simulation allows us test larger
importantly demonstrates full strength
limits terms domain features exploit exploit
adjustment abstraction becomes truly dynamic reacting changes
agents current state enabling tailored agents situation
changes shown qualitative quantitative presented
baum effective efficient calculating approximate
policies guide agent simulated worlds
future work
one possible direction future would worldview initialisation
modification methods smaller yet still useful worldviews probably domainspecific extend method domains larger different features
example factory tireworld domains goal oriented predicates
worldview selection modification method predicate oriented solver could
possible paths goal ensure relevant preconditions concrete along
path
interestingly x proximity methods keep
worldview size small seem balance larger stil moderate
size thus another possibility might tune proximity methods develop
self tuning variants
number points instance phase selection uses stochastic choice
default could replaced heuristics learning directed methods
one could adapt method work types mdps undiscounted
finite horizon ones combine approaches approximate different aspects domains described section example
mentioned section gardiol kaelbling combine hierarchical state
space abstraction somewhat similar envelope work dean et al
many combinations would likely fruitful domains features relevant multiple methods similarly additional refinement coarsening methods


fiproximity non uniform abstractions

could added instance one fact refinement criterion
roll back reyes et al
theoretical side one could look situations optimality
guaranteed hansen zilberstein lao work
dean et al observing admissible heuristic used evaluate fringe states
rather pragmatically chosen v related heuristic search
acquires stopping criterion guaranteed optimality optimality perhaps
similar condition could developed rather different heuristic
two basic directions work extended
fundamental way relaxing one mdp assumptions perfect observability knowledge
transition probabilities partially observable markov decision process pomdp
gives agent observation instead current state observation partly
random partly determined preceding action current state
optimal solution known principle quite computationally expensive since transforms pomdp larger continuous many dimensional mdp agents beliefs
non uniform abstraction could applied two different ways original pomdp fairly direct translation transformed mdp
extension would apply technique agent learn transition
probabilities particular application technique exploration would
interesting agent would somehow learn distinctions within single abstract states distinguish refined remain
abstract

references
de alfaro l roy p magnifying lens abstraction markov decision processes
proceedings th international conference computer aided verification
cav pp
barry j kaelbling l p lozano prez hierarchical solution large markov
decision processes proceedings icaps workshop scheduling
uncertain domains
barry j l fast approximate hierarchical solution mdps masters thesis
massachusetts institute technology
barto g bradtke j singh p learning act real time dynamic programming artificial intelligence special computational
interaction agency
baum j dynamic non uniform abstractions approximate large
structured stochastic domains ph thesis clayton school information technology monash university available www baum com au jiri baum phd ps gz
learning could transformed mdp agents beliefs experiences
would computationally prohibitive standard approaches instead explicitly distinguish exploration agent learns domain ignores goals exploitation achieves
goals ignores opportunities learn



fibaum nicholson dix

baum j nicholson e dynamic non uniform abstractions approximate
large structured stochastic domains lee h motoda h eds
topics artificial intelligence proceedings th pacific rim international conference artificial intelligence pricai pp
bellman r e dynamic programming princeton university press
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific
botea enzenberger muller schaeffer j macro improving ai
automatically learned macro operators journal articial intelligence

boutilier c correlated action effects decision theoretic regression geiger
shenoy p eds proceedings th conference uncertainty artificial
intelligence uai pp
boutilier c dearden r approximating value trees structured dynamic programming proceedings th international conference machine learning
pp
boutilier c dearden r goldszmidt exploiting structure policy construction mellish c ed proceedings th international joint conference
artificial intelligence ijcai vol pp
boutilier c dearden r goldszmidt stochastic dynamic programming
factored representations artificial intelligence
boutilier c goldszmidt sabata b continuous value function approximation sequential bidding policies laskey k prade h eds proceedings
th conference uncertainty artificial intelligence uai pp
cassandra r kaelbling l p kurien j acting uncertainty discrete bayesian mobile robot navigation tech rep tr cs computer
science brown university
daoui c abbad tkiouat exact decomposition approaches markov
decision processes survey advances operations
dean kaelbling l p kirman j nicholson e time
constraints stochastic domains artificial intelligence
dearden r boutilier c abstraction approximate decision theoretic artificial intelligence
dietterich g hierarchical reinforcement learning maxq value function
decomposition journal artificial intelligence
drummond bresina j anytime synthetic projection maximizing probability goal satisfaction dietterich swartout w eds proceedings
th national conference artificial intelligence aaai pp
gardiol n h kaelbling l p envelope relational mdps
advances neural information processing systems nips


fiproximity non uniform abstractions

gardiol n h kaelbling l p adaptive envelope mdps relational equivalence tech rep mit csail tr computer science
artificial intelligence laboratory massachusetts institute technology
goldman r p musliner j boddy durfee e h wu j unrolling
complex task mdps proceedings aaai spring symposium
game theoretic decision theoretic agents
goldman r p musliner j krebsbach k boddy dynamic
abstraction kuipers b webber b eds proceedings th
national conference artificial intelligence th innovative applications artificial intelligence conference aaai iaai pp
guestrin c koller parr r venkataraman efficient solution
factored mdps journal artificial intelligence
hansen e zilberstein lao heuristic search finds
solutions loops artificial intelligence
hauskrecht meuleau n kaelbling l p dean boutilier c hierarchical
solution markov decision processes macro actions cooper g moral
eds proceedings th annual conference uncertainty artificial
intelligence uai pp
hoey j st aubin r hu boutilier c spudd stochastic
decision diagrams proceedings th annual conference uncertainty
artificial intelligence uai pp
howard r dynamic programming markov processes mit press
kim k e representations large stochastic
ph thesis deptartment computer science brown university
kirman j predicting real time planner performance domain characterization
ph thesis department computer science brown university
kolobov mausam weld regressing deterministic plans mdp
function approximation workshop reality check scheduling
uncertainty icaps
korf r macro operators weak method learning artificial intelligence

littman weissman bonet b tireworld domain fifth international
competition ipc hosted international conference automated
scheduling icaps
munos r moore variable resolution discretization high accuracy solutions optimal control dean ed proceedings th international joint conference artificial intelligence ijcai pp
musliner j durfee e h shin k g world modeling dynamic
construction real time plans artificial intelligence


fibaum nicholson dix

nicholson e kaelbling l p toward approximate large
stochastic domains proceedings aaai spring symposium decision theoretic pp
parr r unifying framework temporal abstraction stochastic processes
proceedings symposium abstraction reformulation approximation
sara pp
puterman l shin c modified policy iteration discounted
markov decision processes management science
reyes sucar l e morales e f asisto qualitative mdp recommender system power plant operation computacion sistemas
sanner boutilier c practical solution techniques first order mdps
artificial intelligence advances automated plan generation
srivastava immerman n zilberstein abstract unknown
object quantities properties proceedings eighth symposium abstraction reformulation approximation sara pp
st aubin r hoey j boutilier c apricodd approximate policy construction decision diagrams proceedings conference neural information
processing systems pp
steinkraus k solving large stochastic multiple dynamic abstractions ph thesis department electrical engineering computer
science massachusetts institute technology




