Journal Artificial Intelligence Research 43 (2012) 661-704

Submitted 09/11; published 04/12

Learning Win Reading Manuals
Monte-Carlo Framework
S.R.K. Branavan

branavan@csail.mit.edu

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology

David Silver

d.silver@cs.ucl.ac.uk

Department Computer Science
University College London

Regina Barzilay

regina@csail.mit.edu

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology

Abstract
Domain knowledge crucial effective performance autonomous control systems.
Typically, human effort required encode knowledge control algorithm.
paper, present approach language grounding automatically interprets
text context complex control application, game, uses domain
knowledge extracted text improve control performance. text analysis
control strategies learned jointly using feedback signal inherent application.
effectively leverage textual information, method automatically extracts text
segment relevant current game state, labels task-centric predicate
structure. labeled text used bias action selection policy game,
guiding towards promising regions action space. encode model text
analysis game playing multi-layer neural network, representing linguistic decisions
via latent variables hidden layers, game action quality via output layer.
Operating within Monte-Carlo Search framework, estimate model parameters using
feedback simulated games. apply approach complex strategy game
Civilization II using official game manual text guide. results show
linguistically-informed game-playing agent significantly outperforms language-unaware
counterpart, yielding 34% absolute improvement winning 65% games
playing built-in AI Civilization.

1. Introduction
paper, study task grounding document content control applications
computer games. applications, agent attempts optimize utility
function (e.g., game score) learning select situation-appropriate actions. complex
domains, finding winning strategy challenging even humans. Therefore, human
players typically rely manuals guides describe promising tactics provide
general advice underlying task. Surprisingly, textual information never
utilized control algorithms despite potential greatly improve performance.
goal, therefore, develop methods achieve automatic fashion.
c
2012
AI Access Foundation. rights reserved.

fiBranavan, Silver, & Barzilay

natural resources available population settles aects ability produce food
goods. Cities built near water sources irrigate increase crop yields,
cities near mineral resources mine raw materials. Build city plains grassland
square river running possible.

Figure 1: excerpt user manual game Civilization II.
explore question context strategy games, challenging class large scale
adversarial planning problems.
Consider instance text shown Figure 1. excerpt user
manual game Civilization II.1 text describes game locations action
build-city effectively applied. stochastic player access
text would gain knowledge hard way: would repeatedly attempt
action myriad states, thereby learning characterization promising state-action
pairs based observed game outcomes. games large state spaces, long planning
horizons, high-branching factors, approach prohibitively slow ineffective.
algorithm access text, however, could learn correlations words
text game attributes e.g., word river places rivers game
thus leveraging strategies described text select better actions.
improve performance control applications using domain knowledge automatically extracted text, need address following challenges:
Grounding Text State-Action Space Control Application Text
guides provide wealth information effective control strategies, including
situation-specific advice well general background knowledge. benefit
information, algorithm learn mapping text
guide, states actions control application. mapping allows
algorithm find state-specific advice matching state attributes verbal
descriptions. Furthermore, relevant sentence found, mapping biases
algorithm select action proposed guide document. mapping
modeled word-level, ideally would use information encoded
structure sentence predicate argument structure. instance,
algorithm explicitly identify predicates state attribute descriptions,
map directly structures inherent control application.
Annotation-free Parameter Estimation text analysis tasks relate
well-known methods information extraction, prior work primarily focused
supervised methods. setup, text analysis state dependent, therefore annotations need representative entire state space. Given enormous state
space continually changes game progresses, collecting annotations
impractical. Instead, propose learn text analysis based feedback signal
inherent control application, e.g., game score. feedback computed
automatically step game, thereby allowing algorithm continuously
adapt local, observed game context.
1. http://en.wikipedia.org/wiki/Civilization II

662

fiLearning Win Reading Manuals Monte-Carlo Framework

Effective Integration Extracted Text Information Control Application text guides provide complete, step-by-step advice situations player may encounter. Even advice available, learned
mapping may noisy, resulting suboptimal choices. Therefore, need design method achieve effective control absence textual advice,
robustly integrating automatically extracted information available.
address challenge incorporating language analysis Monte-Carlo Search,
state-of-the-art framework playing complex games. Traditionally framework
operates state action features. extending Monte-Carlo search
include textual features, integrate two sources information principled
fashion.
1.1 Summary Approach
address challenges unified framework based Markov Decision Processes
(MDP), formulation commonly used game playing algorithms. setup consists
game stochastic environment, goal player maximize given
utility function R(s) state s. players behavior determined action-value
function Q(s, a) assesses goodness action state based attributes
a.
incorporate linguistic information MDP formulation, expand action
value function include linguistic features. state action features known
point computation, relevant words semantic roles observed.
Therefore, model text relevance hidden variable. Similarly, use hidden variables
discriminate words describe actions describe state attributes
rest sentence. incorporate hidden variables action-value function,
model Q(s, a) non-linear function approximation using multi-layer neural network.
Despite added complexity, parameters non-linear model effectively learned Monte-Carlo Search framework. Monte-Carlo Search, actionvalue function estimated playing multiple simulated games starting current
game state. use observed reward simulations update parameters
neural network via backpropagation. focuses learning current game state,
allowing method learn language analysis game-play appropriate observed
game context.
1.2 Evaluation
test method strategy game Civilization II, notoriously challenging game
immense action space.2 source knowledge guiding model, use
official game manual. baseline, employ similar Monte-Carlo search based player
access textual information. demonstrate linguisticallyinformed player significantly outperforms baseline terms number games
won. Moreover, show modeling deeper linguistic structure sentences improves performance. full-length games, algorithm yields 34% improve2. Civilization II #3 IGNs 2007 list top video games time.
(http://top100.ign.com/2007/ign top game 3.html)

663

fiBranavan, Silver, & Barzilay

ment language unaware baseline wins 65% games built-in,
hand-crafted AI Civilization II. video method playing game available
http://groups.csail.mit.edu/rbg/code/civ/video. code data work, along
complete experimental setup preconfigured environment virtual machine
available http://groups.csail.mit.edu/rbg/code/civ.
1.3 Roadmap
Section 2, provide intuition benefits integrating textual information
learning algorithms control. Section 3 describes prior work language grounding, emphasizing unique challenges opportunities setup. section positions
work large body research Monte-Carlo based players. Section 4 presents
background Monte-Carlo Search applied game playing. Section 5 present
multi-layer neural network formulation action-value function combines information text control application. Next, present Monte-Carlo method
estimating parameters non-linear function. Sections 6 7 focus application algorithm game Civilization II. Section 8 compare method
range competitive game-playing baselines, empirically analyze properties algorithm. Finally, Section 9 discuss implications research,
conclude.

2. Learning Game Play Text
section, provide intuitive explanation textual information help improve action selection complex game. clarity, first discuss benefits textual
information supervised scenario, thereby decoupling questions concerning modeling
representation related parameter estimation. Assume every state
represented set n features [s1 , s2 , . . . , sn ]. Given state s, goal select
best possible action aj fixed set A. model task multiclass classification, choice aj represented feature vector [(s1 , aj ), (s2 , aj ), . . . , (sn , aj )].
Here, (si , aj ), [1, n] represents feature created taking Cartesian product [s1 , s2 , . . . , sn ] aj . learn classifier effectively, need training set
sufficiently covers possible combinations state features actions. However, domains complex state spaces large number possible actions, many instances
state-action feature values unobserved training.
show generalization power classifier improved using textual information. Assume training example, addition state-action pair,
contains sentence may describe action taken given state attributes. Intuitively, want enrich basic classifier features capture correspondence
states actions, words describe them. Given sentence w composed
word types w1 , w2 , . . . , wm , features form (si , wk ) (aj , wk )
every [1, n], k [1, m] aj A. Assuming action described using similar words throughout guide, expect text-enriched classifier would able
learn correspondence via features (aj , wk ). similar intuition holds learning
correspondence state-attributes descriptions represented features
(si , wk ). features, classifier connect state action aj based
664

fiLearning Win Reading Manuals Monte-Carlo Framework

evidence provided guiding sentence occurrences contexts
throughout training data. text-free classifier may support association
action appear similar state context training set.
benefits textual information extend models trained using control
feedback rather supervised data. training scenario, algorithm assesses
goodness given state-action combination simulating limited number game turns
action taken observing control feedback provided underlying
application. algorithm built-in mechanism (see Section 4) employs
observed feedback learn feature weights, intelligently samples space search
promising state-action pairs. algorithm access collection sentences,
similar feedback-based mechanism used find sentences match given stateaction pair (Section 5.1). state- action-description features (si , wk )
(aj , wk ), algorithm jointly learns identify relevant sentences map actions
states descriptions. Note used classification basis
discussion section, reality methods learn regression function.

3. Related Work
section, first discuss prior work field grounded language acquisition.
Subsequently look two areas specific application domain i.e., natural language
analysis context games, Monte-Carlo Search applied game playing.
3.1 Grounded Language Acquisition
work fits broad area research grounded language acquisition
goal learn linguistic analysis non-linguistic situated context (Oates, 2001;
Barnard & Forsyth, 2001; Siskind, 2001; Roy & Pentland, 2002; Yu & Ballard, 2004; Chen
& Mooney, 2008; Zettlemoyer & Collins, 2009; Liang, Jordan, & Klein, 2009; Branavan,
Chen, Zettlemoyer, & Barzilay, 2009; Branavan, Zettlemoyer, & Barzilay, 2010; Vogel & Jurafsky, 2010; Clarke, Goldwasser, Chang, & Roth, 2010; Tellex, Kollar, Dickerson, Walter,
Banerjee, Teller, & Roy, 2011; Chen & Mooney, 2011; Liang, Jordan, & Klein, 2011; Goldwasser, Reichart, Clarke, & Roth, 2011). appeal formulation lies reducing
need manual annotations, non-linguistic signals provide powerful, albeit
noisy, source supervision learning. traditional grounding setup assumed
non-linguistic signals parallel content input text, motivating machine
translation view grounding task. alternative approach models grounding
control framework learner actively acquires feedback non-linguistic environment uses drive language interpretation. summarize approaches,
emphasizing similarity differences work.
3.1.1 Learning Grounding Parallel Data
many applications, linguistic content tightly linked perceptual observations, providing rich source information learning language grounding. Examples parallel
data include images captions (Barnard & Forsyth, 2001), Robocup game events paired
text commentary (Chen & Mooney, 2008), sequences robot motor actions de665

fiBranavan, Silver, & Barzilay

scribed natural language (Tellex et al., 2011). large diversity properties
parallel data resulted development algorithms tailored specific grounding
contexts, instead application-independent grounding approach. Nevertheless, existing
grounding approaches characterized along several dimensions illuminate
connection algorithms:
Representation Non-Linguistic Input first step grounding words
perceptual data discretize non-linguistic signal (e.g., image) representation facilitates alignment. instance, Barnard Forsyth (2001) segment images regions subsequently mapped words. approaches
intertwine alignment segmentation single step (Roy & Pentland, 2002),
two tasks clearly interrelated. application, segmentation required
state-action representation nature discrete.
Many approaches move beyond discretization, aiming induce rich hierarchical structures non-linguistic input (Fleischman & Roy, 2005; Chen & Mooney, 2008,
2011). instance, Fleischman Roy (2005) parse action sequences using
context-free grammar subsequently mapped semantic frames. Chen
Mooney (2008) represent action sequences using first order logic. contrast,
algorithm capitalizes structure readily available data state-action
transitions. inducing richer structure state-action space may benefit mapping, difficult problem right field hierarchical
planning (Barto & Mahadevan, 2003).
Representation Linguistic Input Early grounding approaches used bagof-words approach represent input documents (Yu & Ballard, 2004; Barnard &
Forsyth, 2001; Fleischman & Roy, 2005). recent methods relied richer
representation linguistic data, syntactic trees (Chen & Mooney, 2008)
semantic templates (Tellex et al., 2011). method incorporates linguistic information multiple levels, using feature-based representation encodes words
well syntactic information extracted dependency trees. shown
results, richer linguistic representations significantly improve model performance.
Alignment Another common feature existing grounding models training
procedure crucially depends well words aligned non-linguistic structures.
reason, models assume alignment provided part training
data (Fleischman & Roy, 2005; Tellex et al., 2011). grounding algorithms,
alignment induced part training procedure. Examples approaches
methods Barnard Forsyth (2001), Liang et al. (2009).
models jointly generate text attributes grounding context, treating
alignment unobserved variable.
contrast, explicitly model alignment model due lack
parallel data. Instead, aim extract relevant information text infuse
control application.
666

fiLearning Win Reading Manuals Monte-Carlo Framework

3.1.2 Learning Grounding Control Feedback
recent work moved away reliance parallel corpora, using control feedback primary source supervision. assumption behind setup
textual information used drive control application, applications performance
correlate quality language analysis. assumed performance measurement obtained automatically. setup conducive reinforcement learning
approaches estimate model parameters feedback signal, even noisy
delayed.
One line prior work focused task mapping textual instructions
policy control application, assuming text fully specifies actions executed environment. example, previous work (Branavan et al., 2009, 2010),
approach applied task translating instructions computer manual
executable GUI actions. Vogel Jurafsky (2010) demonstrate grounding
framework effectively map navigational directions corresponding path map.
second line prior work focused full semantic parsing converting given text
formal meaning representation first order logic (Clarke et al., 2010).
methods applied domains correctness output accurately evaluated based control feedback example, output database
query executed provides clean, oracle feedback signal learning. line
work assumes text fully specifies required output.
method driven control feedback, language interpretation task
fundamentally different. assume given text document provides highlevel advice without directly describing correct actions every potential game state.
Furthermore, textual advice necessarily translate single strategy fact,
text may describe several strategies, contingent specific game states.
reason, strategy text cannot simply interpreted directly policy. Therefore,
goal bias learned policy using information extracted text. end,
aim achieve complete semantic interpretation, rather use partial text analysis
compute features relevant control application.
3.2 Language Analysis Games
Even though games provide rich domain situated text analysis,
prior attempts leveraging opportunity (Gorniak & Roy, 2005; Eisenstein,
Clarke, Goldwasser, & Roth, 2009).
Eisenstein et al. (2009) aim automatically extract information collection
documents help identify rules game. information, represented predicate logic formulae, estimated unsupervised fashion via generative model.
extracted formulae, along observed traces game play subsequently fed Inductive Logic Program, attempts reconstruct rules game.
high-level, goal similar, i.e., extract information text useful external
task, several key differences. Firstly, Eisenstein et al. (2009) analyze
text game two disjoint steps, model tasks integrated fashion.
allows model learn text analysis pertinent game play, time
using text guide game play. Secondly, method learns text analysis game
667

fiBranavan, Silver, & Barzilay

play feedback signal inherent game, avoiding need pre-compiled game
traces. enables method operate effectively complex games collecting
sufficiently representative set game traces impractical.
Gorniak Roy (2005) develop machine controlled game character responds
spoken natural language commands. Given traces game actions manually annotated
transcribed speech, method learns structured representation text
aligned action sequences. learned model used interpret spoken instructions
grounding actions human player current game state.
method learn play game, enables human control additional game
character via speech. contrast Gorniak Roy (2005), aim develop algorithms
fully autonomously control actions one player game. Furthermore,
method operates games user manual rather human provided, contextually
relevant instructions. requires model identify text contains information
useful current game state, addition mapping text productive actions.
Finally, method learns game feedback collected via active interaction without
relying manual annotations. allows us effectively operate complex games
collecting traditional labeled traces would prohibitively expensive.
3.3 Monte-Carlo Search Game AI
Monte-Carlo Search (MCS) state-of-the-art framework successfully
applied, prior work, playing complex games Go, Poker, Scrabble, real-time
strategy games (Gelly, Wang, Munos, & Teytaud, 2006; Tesauro & Galperin, 1996; Billings,
Castillo, Schaeffer, & Szafron, 1999; Sheppard, 2002; Schafer, 2008; Sturtevant, 2008; Balla
& Fern, 2009). framework operates playing simulated games estimate goodness value different candidate actions. games state action spaces
complex, number simulations needed effective play become prohibitively large.
Previous application MCS addressed issue using two orthogonal techniques: (1)
leverage domain knowledge either guide prune action selection, (2) estimate
value untried actions based observed outcomes simulated games. estimate used bias action selection. MCS based algorithm games relies
techniques. describe differences application
techniques prior work.
3.3.1 Leveraging Domain Knowledge
Domain knowledge shown critically important achieving good performance MCS complex games. prior work achieved manually
encoding relevant domain knowledge game playing algorithm example, via
manually specified heuristics action selection (Billings et al., 1999; Gelly et al., 2006),
hand crafted features (Tesauro & Galperin, 1996), value functions encoding expert
knowledge (Sturtevant, 2008). contrast approaches, goal automatically extract use domain knowledge relevant natural language documents, thus
bypassing need manual specification. method learns text interpretation
game action selection based outcomes simulated games MCS. allows
identify leverage textual domain knowledge relevant observed game context.
668

fiLearning Win Reading Manuals Monte-Carlo Framework

Action selection
according
policy function

Stochastic state
transition according
distribution

Figure 2: Markov Decision Process. Actions selected according policy function (s, a)
given current state s. execution selected action ai (e.g., a1 ), causes
MDP transition new state s0 according stochastic state transition
distribution (s0 | s, a).

3.3.2 Estimating Value Untried Actions
Previous approaches estimating value untried actions relied two techniques.
first, Upper Confidence bounds Tree (UCT) heuristic used concert
Monte-Carlo Tree Search variant MCS. augments actions value exploration
bonus rarely visited state-action pairs, resulting better action selection better
overall game performance (Gelly et al., 2006; Sturtevant, 2008; Balla & Fern, 2009).
second technique learn linear function approximation action values current
state s, based game feedback (Tesauro & Galperin, 1996; Silver, Sutton, & Muller, 2008).
Even though method follows latter approach, model action-value Q(s, a) via
non-linear function approximation. Given complexity application domain,
non-linear approximation generalizes better linear one, shown results
significantly improves performance. importantly, non-linear model enables
method represent text analysis latent variables, allowing use textual information
estimate value untried actions.

4. Monte-Carlo Search
task leverage textual information help us win turn-based strategy game
given opponent. section, first describe Monte-Carlo Search framework
within method operates. details linguistically informed Monte-Carlo
Search algorithm given Section 5.
4.1 Game Representation
Formally, represent given turn-based stochastic game Markov Decision Process
(MDP). MDP defined 4-tuple hS, A, T, Ri,
State space, S, set possible states. state represents complete
configuration game in-between player turns.
Action space, A, set possible actions. turn-based strategy game,
player controls multiple game units turn. Thus, action represents
joint assignment unit actions executed current player turn.
669

fiBranavan, Silver, & Barzilay

Transition distribution, (s0 | s, a), probability executing action state
result state s0 next game turn. distribution encodes way
game state changes due game rules, opposing players actions.
reason, (s0 | s, a) stochastic shown Figure 2, executing
action given state result different outcomes s0 .
Reward function, R(s) R, immediate reward received transitioning
state s. value reward correlates goodness actions executed
now, higher reward indicating better actions.
aspects MDP representation game i.e., S, A, () R()
defined implicitly game rules. step game, game-playing
agent observe current game state s, select best possible action a.
agent executes action a, game state changes according state transition
distribution. (s0 | s, a) known priori, state transitions sampled
distribution invoking game code black-box simulator i.e., playing
game. action, agent receives reward according reward function R(s).
game playing setup, value reward indication chances winning
game state s. Crucially, reward signal may delayed i.e., R(s) may
non-zero value game ending states win, loss, tie.
game playing agent selects actions according stochastic policy (s, a),
specifies probability selecting action state s. expected total reward
executing action state s, following policy termed action-value function
Q (s, a). goal find optimal policy (s, a) maximizes expected
total reward i.e., maximizes chances winning game. optimal action-value

function Q (s, a) known, optimal game-playing behavior would select action

highest Q (s, a). may computationally hard find optimal policy

(s, a) Q (s, a), many well studied algorithms available estimating effective
approximation (Sutton & Barto, 1998).
4.2 Monte-Carlo Framework Computer Games
Monte-Carlo Search algorithm, shown Figure 3, simulation-based search paradigm
dynamically estimating action-values Q (s, a) given state st (see Algorithm 1
pseudo code). estimate based rewards observed multiple roll-outs,
simulated game starting state st .3 Specifically, roll-out,
algorithm starts state st , repeatedly selects executes actions according
simulation policy (s, a), sampling state transitions (s0 | s, a). game completion
time , final reward R(s ) measured, action-value function updated
accordingly.4 Monte-Carlo control (Sutton & Barto, 1998), updated action-value
3. Monte-Carlo Search assumes possible play simulated games. simulations may
played heuristic AI player. experiments, built-in AI game used
opponent.
4. general, roll-outs run game completion. simulations expensive, case
domain, roll-outs truncated fixed number steps. however depends availability
approximate reward signal truncation point. experiments, use built-in score
game reward. reward noisy, available every stage game.

670

fiLearning Win Reading Manuals Monte-Carlo Framework

Game

Copy game
state
simulator

Apply action
best simulation
outcome game
Single
simulation
rollout

Update rollout
policy
game feedback
rollout

Simulation

Simulation

Figure 3: Overview Monte-Carlo Search algorithm. game state st , independent set simulated games roll-outs done find best possible game
action . roll-out starts state st , actions selected according
simulation policy (s, a). policy learned roll-outs
roll-outs improving policy, turn improves roll-out action selection. process repeated every actual game state, simulation
policy relearned scratch time.

function Q (s, a) used define improved simulation policy, thereby directing subsequent roll-outs towards higher scoring regions game state space. fixed number
roll-outs performed, action highest average final reward
simulations selected played actual game state st . process repeated
state encountered actual game, action-value function
relearned scratch new game state.5 simulation policy usually selects
actions maximize action-value function. However, sometimes valid actions
randomly explored case valuable predicted current es5. conceivable sharing action-value function across roll-outs different game states
would beneficial, empirically case experiments. One possible reason
domain, game dynamics change radically many points game e.g.,
new technology becomes available. change occurs, may actually detrimental play
according action-value function previous game step. Note however, action-value
function indeed shared across roll-outs single game state st , parameters updated
successive roll-outs. learned model helps improve roll-out action selection, thereby
improves game play. setup relearning scratch game state shown
beneficial even stationary environments (Sutton, Koop, & Silver, 2007).

671

fiBranavan, Silver, & Barzilay

timate Q (s, a). accuracy Q (s, a) improves, quality action selection
improves vice versa, cycle continual improvement (Sutton & Barto, 1998).
success Monte-Carlo Search depends ability make fast, local estimate
action-value function roll-outs collected via simulated play. However games
large branching factors, may feasible collect sufficient roll-outs, especially
game simulation computationally expensive. Thus crucial learned
action-value function generalizes well small number roll-outs i.e., observed
states, actions rewards. One way achieve model action-value function
linear combination state action attributes:
Q (s, a) = w
~ f~(s, a).
f~(s, a) Rn real-valued feature function, w
~ weight vector. Prior work
shown linear value function approximations effective Monte-Carlo Search
framework (Silver et al., 2008).
Note learning action-value function Q(s, a) Monte-Carlo Search related
Reinforcement Learning (RL) (Sutton & Barto, 1998). fact, approach, use
standard gradient descent updates RL estimate parameters Q(s, a). is,
however, one crucial difference two techniques: general, goal RL
find Q(s, a) applicable state agent may observe existence.
Monte-Carlo Search framework, aim learn Q(s, a) specialized current state
s. essence, Q(s, a) relearned every observed state actual game, using
states, actions feedback simulations. relearning may seem suboptimal,
two distinct advantages: first, since Q(s, a) needs model current state,
representationally much simpler global action-value function. Second, due
simpler representation, learned fewer observations global actionvalue function (Sutton et al., 2007). properties important state
space extremely large, case domain.

5. Adding Linguistic Knowledge Monte-Carlo Framework
goal work improve performance Monte-Carlo Search framework
described above, using information automatically extracted text. section,
describe achieve terms model structure parameter estimation.
5.1 Model Structure
achieve aim leveraging textual information improve game-play, method
needs perform three tasks: (1) identify sentences relevant current game state, (2)
label sentences predicate structure, (3) predict good game actions combining
game features text features extracted via language analysis steps. first describe
tasks modeled separately showing integrate
single coherent model.
672

fiLearning Win Reading Manuals Monte-Carlo Framework

procedure PlayGame ()
Initialize game state fixed starting state
s1 s0
= 1 . . .
Run N simulated games
= 1 . . . N
(ai , ri ) SimulateGame (st )
end
Compute average observed utility action
1 X
arg max
ri
Na

i:ai =a

Execute selected action game
st+1 (s0 | st , )
end

procedure SimulateGame (st )
u = . . .
Compute Q function approximation
Q (su , a) = w
~ f~(su , a)
Sample action action-value function -greedy fashion:

uniform (a A)
probability
au (su , a) =

arg max Q (su , a) otherwise


Execute selected action game:
su+1 (s0 | su , au )
game lost
break
end
Update parameters w
~ Q (st , a)
Return action observed utility:
return , R(s )
Algorithm 1: general Monte-Carlo algorithm.
673

fiBranavan, Silver, & Barzilay

5.1.1 Modeling Sentence Relevance
discussed Section 1, small fraction strategy document likely provide
guidance relevant current game context. Therefore, effectively use information
given document d, first need identify sentence yi relevant
current game state action a.6 model decision log-linear distribution,
defining probability yi relevant sentence as:
~

p(y = yi |s, a, d) e~u(yi ,s,a,d) .

(1)

~ , s, a, d) Rn feature function, ~u parameters need estimate.
(y
~
function ()
encodes features combine attributes sentence yi
attributes game state action. features allow model learn correlations
game attributes attributes relevant sentences.
5.1.2 Modeling Predicate Structure
using text guide action selection, addition using word-level correspondences,
would leverage information encoded structure sentence.
example, verbs sentence might likely describe suggested game actions.
aim access information inducing task-centric predicate structure
sentences. is, label words sentence either action-description, statedescription background. Given sentence precomputed dependency parse q,
model word-by-word labeling decision log-linear fashion i.e., distribution
predicate labeling z sentence given by:
p(z |y, q) = p(~e |y, q)

=
p(ej |j, y, q),

(2)

j
~

p(ej |j, y, q) e~v(ej ,j,y,q) ,
~ j , j, y, q) Rn ,
ej predicate label j th word. feature function (e
addition encoding word type part-of-speech tag, includes dependency parse
information word. features allow predicate labeling decision condition
syntactic structure sentence.
5.1.3 Modeling Action-Value Function
relevant sentence identified labeled predicate structure,
algorithm needs use information along attributes current game state
select best possible game action a. end, redefine action-value function
Q(s, a) weighted linear combination features game text information:
Q(s0 , a0 ) = w
~ f~(s, a, yi , zi ).

(3)

6. use approximation selecting single relevant sentence alternative combining
features sentences text, weighted relevance probability p(y = yi |s, a, d).
setup computationally expensive one used here.

674

fiLearning Win Reading Manuals Monte-Carlo Framework

Input layer:

Deterministic feature
layer:

Output layer

Hidden layer encoding
sentence relevance
Hidden layer encoding
predicate labeling

Figure 4: structure neural network model. rectangle represents collection
units layer, shaded trapezoids show connections layers.
fixed, real-valued feature function ~x(s, a, d) transforms game state s, action
a, strategy document input vector ~x. second layer contains
two disjoint sets hidden units ~y ~z, ~y encodes sentence relevance
decisions, ~z predicate labeling. softmax layers, one
unit active time. units third layer f~(s, a, yi , zi ) set
fixed real valued feature functions s, a, active units yi zi ~y
~z respectively.

s0 = hs, di, a0 = ha, yi , zi i, w
~ weight vector, f~(s, a, yi , zi ) Rn feature
function state s, action a, relevant sentence yi , predicate labeling zi .
structure action-value function allows explicitly learn correlations
textual information, game states actions. action maximizes Q(s, a)
selected best action state s: 7
= arg max Q(s, a).


5.1.4 Complete Joint Model
two text analysis models, action-value function described form three
primary components text-aware game playing algorithm. construct single
principled model components representing via different layers
multi-layer neural network shown Figure 4. Essentially, text analysis decisions
modeled latent variables second, hidden layer network, final
output layer models action-value function.
7. Note select action based Q(s, a), depends relevant sentence yi . sentence
selected conditioned action a. may look cyclic dependency actions
sentence relevance. However, case since Q(s, a), therefore sentence relevance
p(y|s, a, d), computed every candidate action A. actual game action selected
estimate Q(s, a).

675

fiBranavan, Silver, & Barzilay

input layer ~x neural network encodes inputs model i.e.,
current state s, candidate action a, document d. second layer consists two
disjoint sets hidden units ~y ~z, set operates stochastic 1-of-n softmax
selection layer (Bridle, 1990). activation function units layer standard
softmax function:
.X
p(yi = 1|~x) = e~ui ~x
e~uk ~x ,
k

ith

yi
hidden unit ~y , ~ui weight vector corresponding yi , k
number units layer. Given activation function mathematically
equivalent log-linear distribution, layers ~y ~z operate log-linear models.
Node activation softmax layer simulates sampling log-linear distribution.
use layer ~y replicate log-linear model sentence relevance Equation (1),
node yi representing single sentence. Similarly, unit zi layer ~z represents
complete predicate labeling sentence, Equation (2).8
third feature layer f~ neural network deterministically computed given
active units yi zi softmax layers, values input layer. unit
layer corresponds fixed feature function fk (s, a, yi , zi ) R. Finally output layer
encodes action-value function Q(s, a) weighted linear combination units
feature layer, thereby replicating Equation (3) completing joint model.
example kind correlations learned model, consider Figure 5.
Here, relevant sentence already selected given game state. predicate
labeling sentence identified words irrigate settler describing
action take. game roll-outs return higher rewards irrigate action
settler unit, model learn association action words
describe it. Similarly, learn association state description words
feature values current game state e.g., word city binary feature nearcity. allows method leverage automatically extracted textual information
improve game play.
5.2 Parameter Estimation
Learning method performed online fashion: game state st ,
algorithm performs simulated game roll-out, observes outcome simulation,
updates parameters ~u, ~v w
~ action-value function Q(st , ). shown
Figure 3, three steps repeated fixed number times actual game state.
information roll-outs used select actual game action.
algorithm relearns parameters action-value function every new game state
st . specializes action-value function subgame starting st . Learning
specialized Q(st , ) game state common useful games complex
state spaces dynamics, learning single global function approximation
particularly difficult (Sutton et al., 2007). consequence function specialization
need online learning since cannot predict games states seen
8. intention incorporate, action-value function, information relevant
sentence. Therefore, practice, perform predicate labeling sentence selected
relevance component model.

676

fiLearning Win Reading Manuals Monte-Carlo Framework

Settlers unit, candidate action 1:
plains

Features:
action = irrigate action-word = "irrigate"
action = irrigate state-word = "land"
action = irrigate terrain = plains
action = irrigate unit-type = settler
state-word = "city" near-city = true

city

Settlers unit, candidate action 2:

settler unit

Relevant text: "Use settlers irrigate land near city"
Predicted action words:

"irrigate", "settler"

Predicted state words:

"land", "near", "city"

irrigate

Features:
action = build-city
action = build-city
action = build-city
action = build-city
state-word = "city"







build-city

action-word = "irrigate"
state-word = "land"
terrain = plains
unit-type = settler
near-city = true

Figure 5: example text game attributes, resulting candidate action features.
left portion game state arrows indicating game attributes.
left sentence relevant game state along action
state words identified predicate labeling. right two candidate
actions settler unit along corresponding features. mentioned
relevant sentence, irrigate better two actions executing
lead future higher game scores. feedback features shown
allow model learn effective mappings actionword irrigate action irrigate, state-word city game
attribute near-city.

testing, function specialization states cannot done priori, ruling
traditional training/test separation.
Since model non-linear approximation underlying action-value function
game, learn model parameters applying non-linear regression observed final
utilities simulated roll-outs. Specifically, adjust parameters stochastic
gradient descent, minimize mean-squared error action-value Q(s, a)
final utility R(s ) observed game state action a. resulting update
model parameters form:

= [R(s ) Q(s, a)]2
2
= [R(s ) Q(s, a)] Q(s, a; ),
learning rate parameter. minimization performed via standard error
backpropagation (Bryson & Ho, 1969; Rumelhart, Hinton, & Williams, 1986), resulting
following online parameter updates:
w
~ w
~ + w [Q R(s )] f~(s, a, yi , zj ),
~ui ~ui + u [Q R(s )] Q ~x [1 p(yi |)],
~vi ~vi + v [Q R(s )] Q ~x [1 p(zi |)].
677

fiBranavan, Silver, & Barzilay

w learning rate, Q = Q(s, a), w,
~ ~ui ~vi parameters
final layer, sentence relevance layer predicate labeling layer respectively.
derivations update equations given Appendix

6. Applying Model
game test model on, Civilization II, multi-player strategy game set either
Earth randomly generated world. player acts ruler one civilization,
starts game units i.e., two Settlers, two Workers one Explorer.
goal expand civilization developing new technologies, building cities new
units, win game either controlling entire world, successfully sending
spaceship another world. map game world divided grid typically
4000 squares, grid location represents tile either land sea. Figure 6 shows
portion world map particular instance game, along game
units one player. experiments, consider two-player game Civilization II
map 1000 squares smallest map allowed Freeciv. map size used
novice human players looking easier game, well advanced players wanting
game shorter duration. test algorithms built-in AI player
game, difficulty level default Normal setting.9
6.1 Game States Actions
define game state Monte-Carlo search, map game world, along
attributes map tile, location attributes players cities
units. examples attributes shown Figure 7. space possible
actions city unit defined game rules given current game state.
example, cities construct buildings harbors banks, create new units
various types; individual units move around grid, perform unit
specific actions irrigation Settlers, military defense Archers. Since
player controls multiple cities units, players action space turn defined
combination possible actions cities units. experiments,
average, player controls approximately 18 units unit 15 possible actions.
resulting action space player large i.e., 1021 . effectively deal
large action space, assume given state, actions individual city
unit independent actions cities units player.10
time, maximize parameter sharing using single action-value function
cities units player.

9. Freeciv five difficulty settings: Novice, Easy, Normal, Hard Cheating. evidenced discussions games online forum (http://freeciv.wikia.com/index.php?title=Forum:Playing Freeciv),
human players new game find even Novice setting hard.
10. Since player executes game actions turn, i.e. opposing units fixed individual players
turn, opponents moves enlarge players action space.

678

fiLearning Win Reading Manuals Monte-Carlo Framework

Figure 6: portion game map one instance Civilization II game. Three
cities, several units single player visible map. visible
different terrain attributes map tiles, grassland, hills, mountains
deserts.

Nation attributes:
-

City attributes:
-

Amount gold treasury
% world controlled
Number cities
Population
Known technologies

Map tile attributes:
-

City population
Surrounding terrain resources
Amount food & resources produced
Number units supported city
Number & type units present

Unit attributes:

Terrain type (e.g. grassland, mountain, etc)
Tile resources (e.g. wheat, coal, wildlife, etc)
Tile river
Construction tile (city, road, rail, etc)
Types units (own enemy) present

-

Unit type (e.g., worker, explorer, archer, etc)
Unit health & hit points
Unit experience
unit city?
unit fortied?

Figure 7: Example attributes game state.

679

fiBranavan, Silver, & Barzilay

6.2 Utility Function
Critically important Monte-Carlo search algorithm, availability utility
function evaluate outcomes simulated game roll-outs. typical application algorithm, final game outcome terms victory loss used
utility function (Tesauro & Galperin, 1996). Unfortunately, complexity Civilization
II, length typical game, precludes possibility running simulation roll-outs
game completion. game, however, provides player real valued game
score, noisy indicator strength civilization. Since playing
two-player game, players score relative opponents used utility
function. Specifically, use ratio game score two players.11
6.3 Features
components method operate features computed basic set text
game attributes. text attributes include words sentence along
parts-of-speech dependency parse information dependency types parent
words. basic game attributes encode game information available human players
via games graphical user interface. examples attributes shown
Figure 7.
identify sentence relevant current game state candidate action,
sentence relevance component computes features combined basic attributes
~ two types first
game sentence text. features ,
computes Cartesian product attributes game attributes
candidate sentence. second type consists binary features test overlap
words candidate sentence, text labels current game state
candidate action. Given 3.2% word tokens manual overlap
labels game, similarity features highly sparse. However, serve
signposts guide learner shown results, method able operate
effectively even absence features, performs better present.
Predicate labeling, unlike sentence relevance, purely language task
~ compute
operates basic text attributes. features component, ,
Cartesian product candidate predicate label words type, part-of-speech
tag, dependency parse information. final component model, action-value
approximation, operates attributes game state, candidate action,
sentence selected relevant, predicate labeling sentence. features
layer, f~, compute three way Cartesian product attributes candidate
action, attributes game state, predicate labeled words relevant
~
~ f~ compute approximately 158,500, 7,900, 306,800 features
sentence. Overall, ,
respectively resulting total 473,200 features full model. Figure 8 shows
examples features.

11. difference players scores used utility function. However, practice
score ratio produced better empirical performance across algorithms baselines.

680

fiLearning Win Reading Manuals Monte-Carlo Framework

Sentence relevance features:
1 action = build-city
& tile-has-river = true
& word = "build"

1 action = irrigate
& tile-is-next-to-city = true
& word = "irrigate"

0 otherwise

0 otherwise

Predicate labeling features:
1 label = action
& word = "city"
& parent-word = "build"

1 label = state
& word = "city"
& parent-label = "near"

0 otherwise

0 otherwise

Action-value features:
1 action = build-city
& tile-has-river = true
& action-word = "build"
& state-word = "river"

1 action = irrigate
& tile-terrain = plains
& action-word = "irrigate"
& state-word = "city"

0 otherwise

0 otherwise

Figure 8: examples features used model. feature, conditions
test game attributes highlighted blue, test words
game manual highlighted red.

7. Experimental Setup
section, describe datasets, evaluation metrics, experimental framework
used test performance method various baselines.
7.1 Datasets
use official game manual Civilization II strategy guide document.12
text manual uses vocabulary 3638 word types, composed 2083 sentences,
average 16.9 words long. manual contains information rules
game, game user interface, basic strategy advice different aspects
game. use Stanford parser (de Marneffe, MacCartney, & Manning, 2006),
default settings, generate dependency parse information sentences game
manual.
7.2 Experimental Framework
apply method Civilization II game, use games open source reimplementation Freeciv.13 instrumented FreeCiv allow method programmatically
12. www.civfanatics.com/content/civ2/reference/Civ2manual.zip
13. http://freeciv.wikia.com. Game version 2.2

681

fiBranavan, Silver, & Barzilay

Primary Game
Monte-Carlo
Player

Game
Server

Modied Game
GUI Client

In-memory
File System

Game Simulation 1

Game
Strategy Guide

Game
Server

Modied Game
GUI Client

Game State

Game Simulation 2
Game
Server

Modied Game
GUI Client

Game Simulation 8
Game
Server

Modied Game
GUI Client

Figure 9: diagram experimental framework, showing Monte-Carlo player,
server primary game playing aims win, multiple game
servers simulated play. Communications multiple processes comprising framework via UNIX sockets in-memory file system.

control game i.e., measure current game state, execute game actions,
save/load current game state, start end games.14
Across experiments, start game initial state run 100
steps. step, perform 500 Monte-Carlo roll-outs. roll-out run 20
simulated game steps halting simulation evaluating outcome. Note
simulated game step, algorithm needs select action game unit.
Given average number units per player 18, results 180,000 decisions
500 roll-outs. pairing decisions corresponding roll-out
outcome used datapoint update model parameters. use fixed learning rate
0.0001 experiments. method, baselines, run 200
independent games manner, evaluations averaged across 200 runs.
use experimental settings across methods, model parameters
initialized zero.
experimental setup consists Monte-Carlo player, primary game
aim play win, set simulation games. primary game simula14. addition instrumentation, code FreeCiv (both server client) changed increase
simulation speed several orders magnitude, remove bugs caused game crash.
best knowledge, game rules functionality identical unmodified Freeciv
version 2.2

682

fiLearning Win Reading Manuals Monte-Carlo Framework

tions simply separate instances Freeciv game. instance Freeciv game
made one server process, runs actual game, one client process,
controlled Monte-Carlo player. start roll-out, simulations
initialized current state primary game via game save/reload functionality
Freeciv. Figure 9 shows diagram experimental framework.
experiments run typical desktop PCs single Intel Core i7 CPUs (4
hyper-threaded cores per CPU). algorithms implemented execute 8 simulation
roll-outs parallel connecting 8 independent simulation games. computational
setup, approximately 5 simulation roll-outs executed per second full model,
single game 100 steps runs 3 hours. Since treat Freeciv game code
black box, special care taken ensure consistency across experiments: code
compiled one specific machine, single fixed build environment (gcc 4.3.2);
experiments run identical settings fixed set machines running fixed
OS configuration (Linux kernel 2.6.35-25, libc 2.12.1).

7.3 Evaluation Metrics

wish evaluate two aspects method: well improves game play leveraging textual information, accurately analyzes text learning game feedback.
evaluate first aspect comparing method various baselines terms
percentage games built-in AI Freeciv. AI fixed heuristic
algorithm designed using extensive knowledge game, intention challenging human players.15 such, provides good open-reference baseline. evaluate
method measuring percentage games won, averaged 100 independent runs.
However, full games sometimes last multiple days, making difficult extensive analysis model performance contributing factors. reason, primary
evaluation measures percentage games within first 100 game steps, averaged
200 independent runs. evaluation underestimate model performance
game player gaining control entire game map within
100 steps considered loss. Since games remain tied 100 steps, two equally
matched average players, playing other, likely win rate close
zero evaluation.

8. Results

adequately characterize performance method, evaluate respect
several different aspects. section, first describe game playing performance
analyze impact textual information. Then, investigate quality text
analysis produced model terms sentence relevance predicate labeling.
683

fiBranavan, Silver, & Barzilay

Method
Random
Built-in AI
Game
Latent variable
Full model
Randomized text

% Win
0
0
17.3
26.1
53.7
40.3

% Loss
100
0
5.3
3.7
5.9
4.3

Std. Err.


2.7
3.1
3.5
3.4

Table 1: Win rate method several baselines within first 100 game steps,
playing built-in game AI. Games neither lost still
ongoing. models win rate statistically significant baselines.
results averaged across 200 independent game runs. standard errors shown
percentage wins.

Method
Game
Latent variable
Full model

% Wins
24.8
31.5
65.4

Standard Error
4.3
4.6
4.8

Table 2: Win rate method two text-unaware baselines built-in AI.
results averaged across 100 independent game runs.

8.1 Game Performance
Table 1 shows performance method several baselines primary 100-step
evaluation. scenario, language-aware Monte-Carlo algorithm wins average
53.7% games, substantially outperforming baselines, best non-languageaware method win rate 26.1%. dismal performance Random baseline
games Built-in AI, playing itself, indications difficulty
winning games within first 100 steps. shown Table 2, evaluated full length
games, method win rate 65.4% compared 31.5% best text-unaware
baseline.16

15. AI constrained follow rules game, access information typically
available human players, information technology, cities units opponents.
methods hand restricted actions information available human players.
16. Note performance methods full games different listed previous
publications (Branavan, Silver, & Barzilay, 2011a, 2011b). previous numbers biased
code flaw FreeCiv caused game sporadically crash middle game play.
originally believed crash random, subsequently discovered happen often losing
games, thereby biasing win rates methods upwards. numbers presented
game bug fixed, crashes observed experiments.

684

fiObserved game score

Learning Win Reading Manuals Monte-Carlo Framework

Monte-Carlo rollouts

Figure 10: Observed game score function Monte-Carlo roll-outs text-aware
full model, text-unaware latent-variable model. Model parameters
updated roll-out, thus performance improves roll-outs.
seen, full models performance improves dramatically small number
roll-outs, demonstrating benefit derives textual information.

8.1.1 Textual Advice Game Performance
verify characterize impact textual advice models performance,
compare several baselines access textual information.
simplest methods, Game only, models action-value function Q(s, a) linear
approximation games state action attributes. non-text-aware method wins
17.3% games (see Table 1). confirm methods improved performance
simply due inherently richer non-linear approximation, evaluate two
ablative non-linear baselines. first these, Latent variable extends linear actionvalue function Game set latent variables. essence four layer
neural network, similar full model, second layers units activated
based game information. baseline wins 26.1% games (Table 1), significantly
improving linear Game baseline, still trailing text-aware method
27%. second ablative baseline, Randomized text, identical model,
except given randomly generated document input. generate document
randomly permuting locations words game manual, thereby maintaining
documents statistical properties terms type frequencies. ensures
number latent variables baseline equal full model. Thus,
baseline model capacity equal text-aware method access
textual information. performance baseline, wins 40.3% games,
confirms information extracted text indeed instrumental performance
method.
685

fiBranavan, Silver, & Barzilay

Figure 10 provides insight textual information helps improve game performance
shows observed game score Monte-Carlo roll-outs full model
latent-variable baseline. seen figure, textual information guides
model high-score region search space far quicker non-text aware
method, thus resulting better overall performance. evaluate performance
method varies amount available textual-information, conduct
experiment random portions text given algorithm. shown
Figure 11, methods performance varies linearly function amount text,
Randomized text experiment corresponding point information
available text.
8.1.2 Impact Seed Vocabulary Performance
sentence relevance component model uses features compute similarity
words sentence, text labels game state action. assumes
availability seed vocabulary names game attributes. domain, 256
unique text labels present game, 135 occur vocabulary game manual.
results sparse seed vocabulary 135 words, covering 3.7% word types
3.2% word tokens manual. Despite sparsity, seed vocabulary
potentially large impact model performance since provides initial set word
groundings. evaluate importance initial grounding, test method
empty seed vocabulary. setup, full model wins 49.0% games, showing
seed words important, method operate effectively
absence.
8.1.3 Linguistic Representation Game Performance
characterize contribution language game performance, conduct series
evaluations vary type complexity linguistic analysis performed
method. results evaluation shown Table 3. first these, Sentence
relevance, highlights contributions two language components model.
algorithm, identical full model lacks predicate labeling component,
wins 46.7% games, showing essential identify textual advice relevant
current game state, deeper syntactic analysis extracted text substantially
improves performance.
evaluate importance dependency parse information language analysis,
vary type features available predicate labeling component model.
first ablative experiments, dependency information, removes dependency
features leaving predicate labeling operate word type features. performance
baseline, win rate 39.6%, clearly shows dependency features crucial
model performance. remaining three methods dependency label, dependency
parent POS tag dependency parent word drop dependency feature
named after. contribution features model performance seen
Table 3.
686

fiWin rate

Learning Win Reading Manuals Monte-Carlo Framework

Random
text

Percentage document text given model

Figure 11: performance text-aware model function amount text
available it. construct partial documents randomly sub-sampling sentences full game manual. x-axis shows amount sentences
given method ratio full text. leftmost extreme
performance Randomized Text baseline, showing fits
performance trend point useful textual information.
Method
Full model
Sentence relevance
dependency information
dependency label
depend. parent POS tag
depend. parent word

% Win
53.7
46.7
39.6
50.1
42.6
33.0

% Loss
5.9
2.8
3.0
3.0
4.0
4.0

Std. Err.
3.5
3.5
3.4
3.5
3.5
3.3

Table 3: Win rates several ablated versions model, showing contribution
different aspects textual information game performance. Sentence relevance
identical Full model, except lacks predicate labeling component.
four methods bottom table ablate specific dependency features
(as indicated methods name) predicate labeling component
full model.

8.1.4 Model Complexity vs Computation Time Trade-off
One inherent disadvantage non-linear models, compared simpler linear models,
increase computation time required parameter estimation. Monte-Carlo
Search setup, model parameters re-estimated simulated roll-out. Therefore,
given fixed amount time, roll-outs done simpler faster model.
nature, performance Monte-Carlo Search improves number rollouts. trade-off model complexity roll-outs important since simpler
687

fiBranavan, Silver, & Barzilay

60%
Full model
Latent variable
Game
ro
llo

ut



50%

0



ts

20

0

50

ut

0r
oll
-ou

30%

10

Win rate

40%


llro

20%

10%

0%

0

20

40

60

80

100

120

140

Computation time per game step (seconds)

Figure 12: Win rate function computation time per game step. MonteCarlo search method, win rate computation time measured 100,
200 500 roll-outs per game step, respectively.

model could compensate using roll-outs, thereby outperform complex
ones. scenario particularly relevant games players limited amount
time turn.
explore trade-off, vary number simulation roll-outs allowed
method game step, recording win-rate average computation time per
game. Figure 12 shows results evaluation 100, 200 500 roll-outs.
complex methods higher computational demands, results clearly show
even given fixed amount computation time per game step, text-aware
model still produces best performance wide margin.
8.1.5 Learned Game Strategy
Qualitatively, methods described learn basic rush strategy. Essentially,
attempt develop basic technologies, build army, take opposing cities
quickly possible. performance difference different models essentially
due well learn strategy.
two basic reasons algorithms learn rush strategy. First, since
attempting maximize game score, methods implicitly biased towards finding
fastest way win happens rush strategy playing
built-in AI Civilization 2. Second, complex strategies typically require
coordination multiple game units. Since models assume game units independent,
688

fiLearning Win Reading Manuals Monte-Carlo Framework

Phalanxes twice eective defending cities warriors.
Build city plains grassland river running it.




rename city like, we'll refer washington.
many dierent strategies dictating order
advances researched

road built, use settlers start improving terrain.
















settlers becomes active, chose build road.












Use settlers engineers improve terrain square within city radius
























Figure 13: Examples methods sentence relevance predicate labeling decisions.
box shows two sentences (identified green check marks)
predicted relevant, two not. box shows
predicted predicate structure three sentences, indicating state
description,A action description background words unmarked. Mistakes
identified crosses.

cannot explicitly learn coordination putting many complex strategies beyond
capabilities algorithms.
8.2 Accuracy Linguistic Analysis
described Section 5, text analysis method tightly coupled game playing
terms modeling, terms learning game feedback. seen
results thus far, text analysis indeed help game play. section
focus game-driven text analysis itself, investigate well conforms
common notions linguistic correctness. comparing model predictions
sentence relevance predicate labeling manual annotations.
8.2.1 Sentence Relevance
Figure 13 shows examples sentence relevance decisions produced method.
evaluate accuracy decisions, would ideally use ground-truth
relevance annotation games user manual. however, impractical since
relevance decision dependent game context, hence specific time step
game instance. Therefore, evaluate sentence relevance accuracy using synthetic
document. create document combining original game manual equal
689

fiSentence relevance accuracy

Branavan, Silver, & Barzilay

1.0
0.8
0.6
0.4
Sentence relevance
Moving average

0.2
0

20

40

60

80

100

Game step

Figure 14: Accuracy methods sentence relevance predictions, averaged 100 independent runs.

number sentences known irrelevant game. sentences
collected randomly sampling Wall Street Journal corpus (Marcus, Santorini,
& Marcinkiewicz, 1993).17 evaluate sentence relevance synthetic document
measuring accuracy game manual sentences picked relevant.
evaluation, method achieves average accuracy 71.8%. Given
model differentiate game manual text Wall Street Journal,
number may seem disappointing. Furthermore, seen Figure 14,
sentence relevance accuracy varies widely game progresses, high average
94.2% initial 25 game steps. reality, pattern high initial accuracy followed lower average entirely surprising: official game manual Civilization
II written first time players. such, focuses initial portion game,
providing little strategy advice relevant subsequent game play.18 reason
observed sentence relevance trend, would expect final layer neural
network emphasize game features text features first 25 steps game.
indeed case, seen Figure 15.
test hypothesis, perform experiment first n steps
game played using full model, subsequent 100 n steps played without
using textual information. results evaluation several values n
given Figure 16, showing initial phase game indeed information
game manual useful. fact, hybrid method performs well
full model n = 50, achieving 53.3% win rate. shows method
17. Note sentences WSJ corpus contain words city potentially confuse
algorithm, causing select sentences relevant game play.
18. reminiscent opening books games Chess Go, aim guide player
playable middle game, without providing much information subsequent game play.

690

fiLearning Win Reading Manuals Monte-Carlo Framework

0.5

Game features dominate

1.0

Text features dominate

Text feature importance

1.5

0
20

40

60

80

100

Game step

Figure 15: Difference norms text features game features
output layer neural network. Beyond initial 25 steps game,
method relies increasingly game features.

Win rate

60%

40%

20%

0%
20

40

60

80

100

# initial game steps text information used

Figure 16: Graph showing availability textual information initial steps
game affects performance full model. Textual information
given model first n steps (the x axis), beyond point
algorithm access text, becomes equivalent Latent Variable
model i.e., best non-text model.

able accurately identify relevant sentences information contain
pertinent game play, likely produce better game performance.
691

fiBranavan, Silver, & Barzilay

Method
Random labeling
Model, first 100 steps
Model, first 25 steps

S/A/B
33.3%
45.1%
48.0%

S/A
50.0%
78.9%
92.7%

Table 4: Predicate labeling accuracy method random baseline. Column
S/A/B shows performance three-way labeling words state, action
background, column S/A shows accuracy task differentiating
state action words.
game attribute

word

state: grassland

"city"

state: grassland

"build"

state: hills

"build"

action: settlers_build_city

"city"

action: set_research

"discovery"

action: settlers_build_city

"settler"

action: settlers_goto_location

"build"

action: city_build_barracks

"construct"

action: research_alphabet

"develop"

action: set_research

"discovery"

Figure 17: Examples word game attribute associations learned via feature
weights model.
8.2.2 Predicate Labeling
Figure 13 shows examples predicate structure output model. evaluate
accuracy labeling comparing gold-standard annotation game
manual.19 Table 4 shows performance method terms accurately labels
words state, action background, accurately differentiates state
action words. addition showing performance improvement random
baseline, results display clear trend: evaluations, labeling accuracy
higher initial stages game. expected since model relies
heavily textual features beginning game (see Figure 15).
verify usefulness methods predicate labeling, perform final set
experiments predicate labels selected uniformly random within full model.
random labeling results win rate 44% performance similar sentence
relevance model uses predicate information. confirms method
able identify predicate structure which, noisy, provides information relevant
game play. Figure 17 shows examples textual information grounded
game, way associations learned words game attributes final
layer full model. example, model learns strong association
19. Note ground truth labeling words either action-description, state-description, background
based purely semantics sentence, independent game state. reason,
manual annotation feasible, unlike case sentence relevance.

692

fiLearning Win Reading Manuals Monte-Carlo Framework

game-state attribute grassland words city build, indicating textual
information building cities maybe useful players unit near grassland.

9. Conclusions
paper presented novel approach improving performance control
applications leveraging information automatically extracted text documents,
time learning language analysis based control feedback. model biases
learned strategy enriching policy function text features, thereby modeling
mapping words manual state-specific action selection. effectively learn
grounding, model identifies text relevant current game state, induces
predicate structure text. linguistic decisions modeled jointly using
non-linear policy function trained Monte-Carlo Search framework.
Empirical results show model able significantly improve game win rate
leveraging textual information compared strong language-agnostic baselines.
demonstrate despite increased complexity model, knowledge
acquires enables sustain good performance even number simulations
reduced. Moreover, deeper linguistic analysis, form predicate labeling text,
improves game play. show information syntactic structure
text crucial analysis, ignoring information large impact
model performance. Finally, experiments demonstrate tightly coupling control
linguistic features, model able deliver robust performance presence
noise inherent automatic language analysis.

Bibliographical Note
Portions work previously presented two conference publications (Branavan
et al., 2011a, 2011b). article significantly extends previous work, notably
providing analysis model properties impact linguistic representation
model performance, dependence model bootstrapping conditions, tradeoff models representational power empirical complexity (Section 8).
paper significantly increases volume experiments base
conclusions. addition, provide comprehensive description model, providing
full mathematical derivations supporting algorithm (Section 5.1 Appendix A).

Acknowledgments
authors acknowledge support NSF (CAREER grant IIS-0448168, grant IIS0835652), DARPA BOLT Program (HR0011-11-2-0008), DARPA Machine Reading
Program (FA8750-09-C-0172, PO#4910018860), Batelle (PO#300662) Microsoft
Research New Faculty Fellowship. Thanks anonymous reviewers, Michael Collins,
Tommi Jaakkola, Leslie Kaelbling, Nate Kushman, Sasha Rush, Luke Zettlemoyer,
MIT NLP group suggestions comments. opinions, findings, conclusions,
recommendations expressed paper authors, necessarily
reflect views funding organizations.
693

fiBranavan, Silver, & Barzilay

Appendix A. Parameter Estimation
parameter model estimated via standard error backpropagation (Bryson &
Ho, 1969; Rumelhart et al., 1986). derive parameter updates, consider slightly
simplified neural network shown below. network identical model,
sake clarity, single second layer ~y instead two parallel second layers
~y ~z. parameter updates parallel layers ~y ~z similar, therefore
show derivation ~y addition updates final layer.

model, nodes yi network activated via softmax function;
third layer, f~, computed deterministically active nodes second layer
via function ~g (yi , ~x); output Q linear combination f~ weighted w:
~
p(yi = 1 | ~x; ~ui ) =

e~ui ~x
X
,
e~uk ~x
k

f~ =

X

~g (~x, yi ) p(yi | ~x; ~ui ),



Q = w
~ f~.
goal minimize mean-squared error e gradient descent. achieve
updating model parameters along gradient e respect parameter. Using
general term indicate models parameters, update takes form:
1
(Q R)2 ,
2
e
=

Q
= (Q R)
.


e =


Equation (4), updates final layer parameters given by:
Q
wi

= (Q R)
w
~ f~
wi
= (Q R) .

wi = (Q R)

694

(4)

fiLearning Win Reading Manuals Monte-Carlo Framework

Since model samples one relevant sentence yi , best predicate labeling
zi , resulting online updates output layer parameters w
~ are:
w
~ w
~ + w [Q R(s )] f~(s, a, yi , zj ),
w learning rate, Q = Q(s, a). updates second layers parameters similar, somewhat involved. Again, Equation (4),
ui,j

Q
ui,j

= (Q R)
w
~ f~
ui,j
X

w
~
~g (~x, yk ) p(yi | ~x; ~uk )
= (Q R)
ui,j
= (Q R)

k

= (Q R) w
~ ~g (~x, yi )


p(yi | ~x; ~ui ).
ui,j

(5)

Considering final term equation separately,

p(yi | ~x; ~ui ) =
ui,j

e~ui ~x
,
ui,j Z


=

Z =

X

e~uk ~x

k

eu~ ~x
e~ui ~x ui,j Z
u~ ~x
e
Z
Z

=
=
=
=
=
=

e~ui ~x

~ui ~x

e
log
Z
ui,j
Z
~ui ~x

e

xj
log Z
Z
ui,j
~ui ~x

e
1 Z
xj
Z
Z ui,j
"
#
~ui ~x
e
1 X ~uk ~x
xj
e
Z
Z ui,j
k
~ui ~x

e
1
~
uk ~
x
xj xj e
Z
Z
~ui ~x

e
e~ui ~x
xj 1
.
Z
Z




695

fiBranavan, Silver, & Barzilay

Therefore, Equation (5),
ui,j


p(yi | ~x; ~ui )
ui,j
~ui ~x


e
e~ui ~x
= (Q R) w
~ ~g (~x, yi )
xj 1
Z
Z
= (Q R) xj w
~ ~g (~x, yi ) p(yi | ~x; ~ui ) [1 p(yi | ~x; ~ui )]
= (Q R) w
~ ~g (~x, yi )

= (Q R) xj Q [1 p(yi | ~x; ~ui )] ,
Q = w
~ ~g (~x, yi ) p(yi | ~x; ~ui ).

resulting online updates sentence relevance predicate labeling parameters
~u ~v are:
~ui ~ui + u [Q R(s )] Q ~x [1 p(yi |)],
~vi ~vi + v [Q R(s )] Q ~x [1 p(zi |)].

696

fiLearning Win Reading Manuals Monte-Carlo Framework

Appendix B. Example Sentence Relevance Predictions
Shown portion strategy guide Civilization II. Sentences
identified relevant text-aware model highlighted green.

Choosing location.
building new city, carefully plan place it. Citizens
work terrain surrounding city square x-shaped pattern (see
city radius diagram showing exact dimensions). area called
city radius (the terrain square settlers standing
becomes city square). natural resources available
population settles affect ability produce food goods. Cities built
near water sources irrigate increase crop yields, cities
near mineral outcroppings mine raw materials. hand,
cities surrounded desert always handicapped aridness
terrain, cities encircled mountains find arable cropland
premium. addition economic potential within city's radius,
need consider proximity cities strategic value
location. Ideally, want locate cities areas offer combination
benefits : food population growth, raw materials production,
river coastal areas trade. possible, take advantage
presence special resources terrain squares (see terrain & movement
details benefits).
Strategic value.
strategic value city site final consideration. city square's
underlying terrain increase defender's strength city
comes attack. circumstances, defensive value
particular city's terrain might important economic value;
consider case continent narrows bottleneck rival
holds side. Good defensive terrain (hills, mountains, jungle)
generally poor food production inhibits early growth city.
need compromise growth defense, build city
plains grassland square river running possible.
yields decent trade production gains 50 percent defense bonus.
Regardless city built, city square easier defend
unimproved terrain. city build city walls
improvement, triples defense factors military units stationed
there. Also, units defending city square destroyed one time
lose. Outside cities, units stacked together destroyed
military unit stack defeated (units fortresses
exception; see fortresses). Placing cities seacoast gives
access ocean. launch ship units explore world
transport units overseas. coastal cities, sea power
inhibited.

697

fiBranavan, Silver, & Barzilay

Appendix C. Examples Predicate Labeling Predictions
Listed predicate labellings computed text-aware method example
sentences game manual. predicted labels indicated words
letters A, S, B action-description, state-description background respectively.
Incorrect labels indicated red check mark, along correct label brackets.
road built, use settlers start improving terrain.
















settlers becomes active, chose build road.












Use settlers engineers improve terrain square within city radius

(A)







(S)











Bronze working allows build phalanx units


B (S)









order expand civilization , need build cities

(S)





B

B (A)



order protect city , phalanx must remain inside

B(S)

B(S)



S(A)





B(A)

soon you've found decent site , want settlers build

B(S)

B(S)



B (A)

(B)





permanent settlement - city

(A)



city build city walls improvement

(S)

B (A)







city undefended , move friendly army city capture


B (S)











B

build city terrain square except ocean.


(A)

B (S)



(S)



launch ship units explore world transport units overseas


(A)





B (S)

B (S)



B

city disorder, disband distant military units, return home cities,

(S)







(A)



(A)



change home cities






build wonder discovered advance makes possible


(A)



698







fiLearning Win Reading Manuals Monte-Carlo Framework

Appendix D. Examples Learned Text Game Attribute Mappings
Shown examples word game-attribute associations learned
model. top ten game attributes strongest association feature weight
listed three example words attack, build grassland. fourth
word, settler, seven attributes non-zero weights experiments used collect
statistics.

attack

build

phalanx (unit)

worker_goto (action)

warriors (unit)

settler_autosettle (action)

colossus (wonder)

worker_autosettle (action)

city walls (city improvement)

pheasant (terrain attribute)

archers (unit)

settler_irrigate (action)

catapult (unit)

worker_mine (action)

palace (city improvement)

build_city_walls (action)

coinage (city production)

build_catapult (action)

city_build_warriors (action)

swamp (terrain attribute)

city_build_phalanx (action)

grassland (terrain attribute)

grassland

settler

settler_build_city (action)

settlers (state attribute)

worker_continue_action (action)

settler_build_city (action)

pheasant (terrain attribute)

city (state_attribute)

city_build_improvement (action)

grassland (terrain_attribute)

city_max_production (action)

plains (terrain_attribute)

settlers (state attribute)

road (terrain_attribute)

city_max_food (action)

workers (state attribute)

settler_goto (action)
worker_build_road (action)
pyramids (city attribute)

699

fiBranavan, Silver, & Barzilay

Appendix E. Features Used Model
Features used predict sentence relevance
following templates used compute features sentence relevance:
Word W present sentence.
Number words match text label current unit, attribute
immediate neighborhood unit, action consideration.
units type U, (e.g., worker) word W present sentence.
action type A, (e.g., irrigate) word W present sentence.
Features used predict predicate structure
following templates used compute features predicate labeling words.
label considered word (i.e., action, state background) denoted
L.
Label L word type W.
Label L part-of-speech tag word T.
Label L parent word dependency tree W.
Label L dependency type dependency parent word D.
Label L part-of-speech dependency parent word T.
Label L word leaf node dependency tree.
Label L word leaf node dependency tree.
Label L word matches state attribute name.
Label L word matches unit type name.
Label L word matches action name.
Features used model action-value function
following templates used compute features action-value approximation.
Unless otherwise mentioned, features look attributes player controlled
model.
Percentage world controlled.
Percentage world explored.
Players game score.
Opponents game score.
Number cities.
Average size cities.
Total size cities.
700

fiLearning Win Reading Manuals Monte-Carlo Framework

Number units.
Number veteran units.
Wealth gold.
Excess food produced.
Excess shield produced.
Excess trade produced.
Excess science produced.
Excess gold produced.
Excess luxury produced.
Name technology currently researched.
Percentage completion current research.
Percentage remaining current research.
Number game turns current research completed.
following feature templates applied city controlled player:
Current size city.
Number turns city grows size.
Amount food stored city.
Amount shield stored city (shields used construct new buildings
units city).
Turns remaining current construction completed.
Surplus food production city.
Surplus shield production city.
Surplus trade production city.
Surplus science production city.
Surplus gold production city.
Surplus luxury production city.
Distance closest friendly city.
Average distance friendly cities.
City governance type.
Type building unit currently construction.
Types buildings already constructed city.
Type terrain surrounding city.
Type resources available citys neighborhood.
701

fiBranavan, Silver, & Barzilay

another city neighborhood.
enemy unit neighborhood.
enemy city neighborhood.
following feature templates applied unit controlled player:
Type unit.
Moves left unit current game turn.
Current health unit.
Hit-points unit.
unit veteran.
Distance closest friendly city.
Average distance friendly cities.
Type terrain surrounding unit.
Type resources available units neighborhood.
enemy unit neighborhood.
enemy city neighborhood.
following feature templates applied predicate-labeled word sentence
selected relevant, combined current state action attributes:
Word W present sentence, action considered A.
Word W predicate label P present sentence, action considered
A.
Word W present sentence, current units type U, action
considered A.
Word W predicate label P present sentence, current units type U,
action considered A.
Word W present sentence, current units type U.
Word W predicate label P present sentence, current units type
U.
Word W present sentence, attribute text label present
current units neighborhood.
Word W predicate label P present sentence, attribute text label
present current units neighborhood.

702

fiLearning Win Reading Manuals Monte-Carlo Framework

References
Balla, R., & Fern, A. (2009). UCT tactical assault planning real-time strategy games.
Proceedings IJCAI, pp. 4045.
Barnard, K., & Forsyth, D. A. (2001). Learning semantics words pictures.
Proceedings ICCV, pp. 408415.
Barto, A. G., & Mahadevan, S. (2003). Recent advances hierarchical reinforcement
learning. Discrete Event Dynamic Systems, 13, 341379.
Billings, D., Castillo, L. P., Schaeffer, J., & Szafron, D. (1999). Using probabilistic knowledge
simulation play poker. Proceedings AAAI/IAAI, pp. 697703.
Branavan, S., Chen, H., Zettlemoyer, L., & Barzilay, R. (2009). Reinforcement learning
mapping instructions actions. Proceedings ACL, pp. 8290.
Branavan, S., Silver, D., & Barzilay, R. (2011a). Learning win reading manuals
monte-carlo framework. Proceedings ACL, pp. 268277.
Branavan, S., Silver, D., & Barzilay, R. (2011b). Non-linear monte-carlo search civilization
II. Proceedings IJCAI, pp. 24042410.
Branavan, S., Zettlemoyer, L., & Barzilay, R. (2010). Reading lines: Learning
map high-level instructions commands. Proceedings ACL, pp. 12681277.
Bridle, J. S. (1990). Training stochastic model recognition algorithms networks lead
maximum mutual information estimation parameters. Advances NIPS, pp.
211217.
Bryson, A. E., & Ho, Y.-C. (1969). Applied optimal control: optimization, estimation,
control. Blaisdell Publishing Company.
Chen, D. L., & Mooney, R. J. (2008). Learning sportscast: test grounded language
acquisition. Proceedings ICML, pp. 128135.
Chen, D. L., & Mooney, R. J. (2011). Learning interpret natural language navigation
instructions observations. Proceedings AAAI, pp. 859865.
Clarke, J., Goldwasser, D., Chang, M.-W., & Roth, D. (2010). Driving semantic parsing
worlds response. Proceedings CoNNL, pp. 1827.
de Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses phrase structure parses. Proceedings LREC, pp. 449454.
Eisenstein, J., Clarke, J., Goldwasser, D., & Roth, D. (2009). Reading learn: Constructing
features semantic abstracts. Proceedings EMNLP, pp. 958967.
Fleischman, M., & Roy, D. (2005). Intentional context situated natural language learning.
Proceedings CoNLL, pp. 104111.
Gelly, S., Wang, Y., Munos, R., & Teytaud, O. (2006). Modification UCT patterns
Monte-Carlo Go. Tech. rep. 6062, INRIA.
Goldwasser, D., Reichart, R., Clarke, J., & Roth, D. (2011). Confidence driven unsupervised
semantic parsing. Proceedings ACL, pp. 14861495.

703

fiBranavan, Silver, & Barzilay

Gorniak, P., & Roy, D. (2005). Speaking sidekick: Understanding situated speech
computer role playing games. Proceedings AIIDE, pp. 5762.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning semantic correspondences less
supervision. Proceedings ACL, pp. 9199.
Liang, P., Jordan, M. I., & Klein, D. (2011). Learning dependency-based compositional
semantics. Proceedings ACL, pp. 590599.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated
corpus english: penn treebank. Computational Linguistics, 19 (2), 313330.
Oates, J. T. (2001). Grounding knowledge sensors: Unsupervised learning language
planning. Ph.D. thesis, University Massachusetts Amherst.
Roy, D. K., & Pentland, A. P. (2002). Learning words sights sounds: computational model. Cognitive Science 26, 113146.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations
back-propagating errors. Nature, 323, 533536.
Schafer, J. (2008). UCT algorithm applied games imperfect information.
Diploma Thesis. Otto-von-Guericke-Universitat Magdeburg.
Sheppard, B. (2002). World-championship-caliber Scrabble. Artificial Intelligence, 134 (1-2),
241275.
Silver, D., Sutton, R., & Muller, M. (2008). Sample-based learning search permanent transient memories. Proceedings ICML, pp. 968975.
Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception using
force dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.
Sturtevant, N. (2008). analysis UCT multi-player games. Proceedings ICCG,
pp. 3749.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT
Press.
Sutton, R. S., Koop, A., & Silver, D. (2007). role tracking stationary environments. Proceedings ICML, pp. 871878.
Tellex, S., Kollar, T., Dickerson, S., Walter, M. R., Banerjee, A. G., Teller, S., & Roy, N.
(2011). Understanding natural language commands robotic navigation mobile
manipulation. Proceedings AAAI, pp. 15071514.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using Monte-Carlo search.
Advances NIPS, pp. 10681074.
Vogel, A., & Jurafsky, D. (2010). Learning follow navigational directions. Proceedings
ACL, pp. 806814.
Yu, C., & Ballard, D. H. (2004). integration grounding language learning
objects. Proceedings AAAI, pp. 488493.
Zettlemoyer, L., & Collins, M. (2009). Learning context-dependent mappings sentences
logical form. Proceedings ACL, pp. 976984.

704


