Journal Artificial Intelligence Research 32 (2008) 95-122

Submitted 10/07; published 05/08

Graphical Model Inference Optimal Control Stochastic
Multi-Agent Systems
Bart van den Broek
Wim Wiegerinck
Bert Kappen

B.vandenBroek@science.ru.nl
W.Wiegerinck@science.ru.nl
B.Kappen@science.ru.nl

SNN, Radboud University Nijmegen, Geert Grooteplein 21,
Nijmegen, Netherlands

Abstract
article consider issue optimal control collaborative multi-agent
systems stochastic dynamics. agents joint task
reach number target states. dynamics agents contains additive control
additive noise, autonomous part factorizes agents. Full observation
global state assumed. goal minimize accumulated joint cost, consists
integrated instantaneous costs joint end cost. joint end cost expresses joint
task agents. instantaneous costs quadratic control factorize
agents. optimal control given weighted linear combination single-agent
single-target controls. single-agent single-target controls expressed terms
diffusion processes. controls, closed form expressions, formulated
terms path integrals, calculated approximately Metropolis-Hastings
sampling. weights control interpreted marginals joint distribution
agent target assignments. structure latter represented graphical
model, marginals obtained graphical model inference. Exact inference
graphical model break large systems, approximate inference methods
needed. use naive mean field approximation belief propagation approximate
optimal control systems linear dynamics. compare approximate inference
methods exact solution, show accurately compute optimal
control. Finally, demonstrate control method multi-agent systems nonlinear
dynamics consisting 80 agents reach equal number target states.

1. Introduction
topic control multi-agent systems characterized many issues, originating
various sources, including wide variety possible execution plans, uncertainties
interaction environment, limited operation time supporting resources,
demand robustness joint performance agents. issues encountered
in, example, air traffic management (Tomlin, Pappas, & Sastry, 1998; van Leeuwen,
Hesseling, & Rohling, 2002), formation flight (Ribichini & Frazzoli, 2003; Hu, Prandini,
& Tomlin, 2007), radar avoidance unmanned air vehicles fighter aircraft (Pachter &
Pachter, 2001; Kamal, Gu, & Postlethwaite, 2005; Larson, Pachter, & Mears, 2005; Shi,
Wang, Liu, Wang, & Zu, 2007), persistent area denial (Subramanian & Cruz, 2003;
Liu, Cruz, & Schumacher, 2007; Castanon, Pachter, & Chandler, 2004).
many control approaches multi-agent systems, stochastic influences dynamics
agents taken account assumed negligible, dynamics
c
2008
AI Access Foundation. rights reserved.

fivan den Broek, Wiegerinck & Kappen

modeled deterministically. system truly deterministic, agents
optimally controlled open loop controls. However, stochastic influences
dynamics large ignored, open loop controls become far optimal,
multi-agent system longer modeled deterministically.
usual
approach control multi-agent systems stochastic dynamics model system
Markov Decision Processes (MDP) (Boutilier, 1996; Sadati & Elhamifar, 2006).
principle, solved discrete space time backward dynamic programming.
However, discretization make joint state space multi-agent system increase
exponentially number agents, basic dynamic programming approach
generally infeasible (Boutilier, 1996). attempt overcome exploit structures
problem describe system factored MDP. general structures
conserved value functions, exact computations remain exponential
system size. Guestrin, Koller, Parr (2002a) Guestrin, Venkataraman, Koller
(2002b) assumed predefined approximate structure value functions, thereby
provided efficient approximate MDP model multi-agent systems. similar approach
taken Becker, Zilberstein, Lesser, Goldman (2003, 2004), assuming independent
collaboration agents global reward function, resulting transition-independent
decentralized MDPs.
paper concentrate multi-agent systems agents joint task
reach number target states. model multi-agent system
continuous space time, following approach Wiegerinck, van den Broek,
Kappen (2006). make following assumptions. agents assumed
complete accurate knowledge global state system (assumption 1).
dynamics agent additive control disturbed additive Wiener noise
(assumption 2). performance agents valued global cost function,
integral instantaneous costs plus end cost. joint task agents modeled
end cost. instantaneous costs assumed quadratic control
(assumption 3). noise level dynamics agents inversely proportional
control cost (assumption 4). Finally, assume autonomous dynamics
instantaneous costs factorize agents (assumption 5).
assumptions 1 2, optimal control problem partially solved finding
optimal expected cost-to-go, satisfies so-called stochastic Hamilton-JacobiBellman (SHJB) equation. optimal expected cost-to-go given, optimal
control provided gradient optimal expected cost-to-go adopting assumption 3. SHJB equation nonlinear partial differential equation (PDE),
nonlinearity makes difficult solve. common approach solving SHJB equation
assume, addition assumption 3, instantaneous costs end cost
cost function quadratic state, dynamics linear
state wellthis known linear-quadratic control. optimal expected cost-to-go
quadratic state time-varying coefficients, problem reduces
solving Riccati equations coefficients satisfy (Stengel, 1993; ksendal, 1998).
Otherwise, approximation methods needed. approximate approach given
iterative linear-quadratic Gaussian method (Todorov & Li, 2005); yields locally optimal feedback control, valid case little noise. instead follow approach
Fleming (1978) adopt assumption 4. assumption SHJB equation
96

fiGraphical Model Inference MAS Optimal Control

transformed linear PDE performing logarithmic transformation. solution
equals expectation value stochastic integral diffusion process. general,
closed form expression. paper estimate expression formulating
path integral (Kappen, 2005a, 2005b), estimate latter using MetropolisHastings sampling. several ways estimate path integral,
Hamilton Monte Carlo sampling Laplace approximation, covered
paper.
structure optimal expected cost-to-go generally complex due
dynamic couplings agents. adopting assumption 5, agents
coupled joint end cost, solely determines structure
optimal expected cost-to-go. result state transition probabilities factorize
agents. follows optimal control becomes weighted combination
single-agent single-target controls. weights given joint distribution
agent target assignments. joint distribution structure joint end
cost. structure joint distribution representable factor graph,
optimal control problem becomes graphical model inference problem (Wiegerinck et al.,
2006). complexity graphical model inference exponential tree width
factor graph. Exact inference possible using junction tree algorithm,
given graph sufficiently sparse number agents large.
complex situations approximate inference methods necessary, show
optimal control accurately approximated polynomial time, using naive mean
field (MF) approximation belief propagation (BP). makes distributed coordination
possible multi-agent systems much larger could treated
exact inference.
paper organized follows. Sections 2 3, provide review
single multi-agent stochastic optimal control framework, developed Kappen (2005a, 2005b) Wiegerinck et al. (2006). example, rederive linear
quadratic control. general solution given terms path integral, explain
approximated Metropolis-Hastings sampling.
Section 4, give factor graph representation end cost function. discuss two graphical model approximate inference methods: naive mean field approximation
belief propagation. show approximation optimal control
methods obtained replacing exact weights controls respective
approximations.
Section 5, present numerical results. make comparison approximate
optimal controls, infered naive mean field approximation, belief propagation
greedy method, exact optimal control; multi-agent system
18 agents linear dynamics two-dimensional state space, two target
states. Furthermore, present results control multi-agent systems nonlinear
dynamics four-dimensional state space, agents control forward velocity
driving direction. controls approximated combination MetropolisHastings sampling, infer path integrals, naive mean field approximation, infer
agent target assignments. allowed us control systems 80 agents
80 target states. results regarding nonlinear dynamics illustrative
purpose.
97

fivan den Broek, Wiegerinck & Kappen

2. Stochastic Optimal Control Single Agent
consider agent k-dimensional continuous state space Rk , state x(t) evolving
time according controlled stochastic differential equation
dx(t) = b(x(t), t)dt + u(x(t), t)dt + dw(t),

(1)

accordance assumptions 1 2 introduction. control agent
Rk -valued function u x(t) t. noise dynamics modeled Wiener
process w(t), i.e., normally distributed k-dimensional stochastic process continuous
time mean 0 variance t, k k matrix represents variance
noise. autonomous dynamics modeled b, Rk -valued function
x(t) t. state change dx(t) sum noisy control autonomous
dynamics.
behavior agent valued cost function. Given agents state x(t) = x
present time t, control u, expected future cost agent:



Z
1

C u (x, t) = Eux,t (x(T )) +
kRu(x(), )k2 + V (x(), ) .
(2)
2

expectation Eux,t taken respect probability measure x(t)
solution (1) given control law u condition x(t) = x. cost
combination end cost (x(T )), function end state x(T ),
integral instantaneous costs. instantaneous cost sum state control
dependent term. state dependent term V (x(), ) cost state x()
time . function V arbitrary, represents environment agent.
control dependent term 12 kRu(x(), )k2 cost control state x() time ,
kzk2 = z z Euclidean norm, R full rank k k matrix. quadratic
control, accordance assumption 3 introduction, assumption 4,
R related variance noise control via relation
= (R R)1 ,

(3)

scalar.
expected cost-to-go time minimized controls u defines optimal
expected cost-to-go
J(x, t) = min C u (x, t).
(4)
u

Appendix A, explained due linear-quadratic form optimization
problemthe dynamics (1) linear action u, cost function (2) quadratic
actionthe minimization performed explicitly, yielding nonlinear partial differential equation J, so-called stochastic Hamilton-Jacobi-Bellman (SHJB) equation.
minimum attained
u(x, t) = (R R)1 x J(x, t).

(5)

optimal control. Note explicitely depends state x agent
time t, making feedback control.
98

fiGraphical Model Inference MAS Optimal Control

optimal expected cost-to-go re-expressed terms diffusion process (for
derivation, refer Appendix A):
J(x, t) = log Z(x, t)

(6)

Z(x, t) expectation value



Z
1
1
Z(x, t) = Ex,t exp (y(T ))
V (y(), )



(7)

y() diffusion process y(t) = x satisfying uncontrolled dynamics:
dy() = b(y(), )d + dw().

(8)

Substituting relations (3) (6) (5), find optimal control terms Z(x, t):
u(x, t) = x log Z(x, t).

(9)

Example 1. Consider agent one dimension state x(t) described dynamical equation (1) without autonomous dynamics (b = 0). instantaneous cost V zero,
end cost quadratic function around target state :
(y) =


|y |2 .
2

diffusion process y() satisfies uncontrolled dynamics (8) normally distributed
around agents state x = y(t) time variance 2 ( t), hence state
transition probability agent go (x, t) (y, ) space-time given
Gaussian density


|y x|2
(y, |x, t) = p
.
exp 2
2 (T t)
2 2 (T t)
1

expectation value (7) given integral

Z(x, t) =

Z

1

(y)

dy(y, |x, t)e

=





R2 /
|x |2
exp 2
,
+ R2 /
2 (T + R2 /)

relation (3) used. optimal control follows (6) (9) reads
u(x, t) =

x
.
+ R2 /

result well known (Stengel, 1993).
99

(10)

fivan den Broek, Wiegerinck & Kappen

2.1 Path Integral Formulation
Example 1 shows simple system autonomous dynamics (b = 0) costs
due environment (V = 0), write control explicitly.
uncontrolled dynamics normally distributed, consequently expectation value
(7) quadratic end cost closed form expression. general situation b
V arbitrary, longer exists explicit expression expectation value,
optimal control obtained approximation. discuss
done taking path integral approach (Kleinert, 2006). detailed derivation
expressions presented given Appendix B.
path integral approach, write expectation value (7) path integral:
Z(x, t) = lim Z (x(t0 ), t0 )

(11)

0

x(t0 ) = x, t0 =
1

Z (x(t0 ), t0 ) = p
det(2 2 )N

Z

dx(t1 ) . . .

Z

1

dx(tN ) e (x(t0 ),...,x(tN ),t0 ) .

integral paths (x(t0 ), . . . , x(tN )) discrete time, start x(t0 ) kept fixed
N = t, taken continuous time limit sending length time steps
= ti+1 ti zero. Note limit N goes infinity paths become infinite
dimensional objects. function exponent cost path:
(x(t0 ), . . . , x(tN ), t0 ) =
(x(T )) +

N
1
X

V (x(ti ), ti ) +

N
1
X
i=0

i=0


2

1
x(ti+1 ) x(ti )

R
b(x(ti ), ti )
,
2


optimal control becomes weighted average controls derived single
path:
u(x(t0 ), t0 ) = lim
0

Z

dx(t1 ) . . .

Z

dx(tN ) p(x(t0 ), . . . , x(tN ), t0 ) u(x(t0 ), . . . , x(tN ), t0 ). (12)

weights given
1

p(x(t0 ), . . . , x(tN ), t0 ) = p

e (x(t0 ),...,x(tN ),t0 )
det(2 2 )N Z (x(t0 ), t0 )

.

control derived path (x(t0 ), . . . , x(tN )) reads
u(x(t0 ), . . . , x(tN ), t0 ) =

x(t1 ) x(t0 )
b(x(t0 ), t0 ).


Note depends first two entries x(t0 ) x(t1 ) path.
100

(13)

fiGraphical Model Inference MAS Optimal Control

2.2 Path Integration Metropolis-Hastings Sampling
path integral formulation (12) optimal control generally computed,
integral uncountably many paths, exist several ways
approximate it. natural approach goes stochastic sampling paths. Several methods
stochastic sampling exist, one use known Metropolis-Hastings
sampling (Hastings, 1970). implementation time discretized: take
limit (12) decreasing zero, instead keep fixed value. sample path
sequence (xs (t0 ), . . . , xs (tN )) vectors state space Rk , x(t0 ) = x
current state agent current time t0 = t. According equation (13),
need xs (t0 ) xs (t1 ) derive control sample path (x(t0 ), . . . , x(tN )).
Metropolis-Hastings sampling ensures different paths properly weighted, hence
optimal control approximated follows:
u(x(t0 ), t0 )

hx(t1 )i x(t0 )
b(x(t0 ), t0 ),
t1 t0

(14)

hx(t1 )i mean value xs (t1 ) taken sample paths. Pseudo-code
algorithm given Algorithm 1.
Algorithm 1: Metropolis-Hastings sampling
Input: initial path (x(t0 ), . . . , x(tN ))
1: = 1
2: repeat times:
3: define Gaussian proposal distribution centered around (x(t1 ), . . . , x(tN ))
variance equal noise
4: draw sample path (x (t1 ), . . . , x (tN )) proposal distribution

5: = exp 1 (x(t0 ), x(t1 ), . . . , x(tN ), t0 ) 1 (x(t0 ), x (t1 ), . . . , x (tN ), t0 )
6: 1
7:
set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN ))
8: else
9:
set (x(t1 ), . . . , x(tN )) = (x (t1 ), . . . , x (tN )) probability
10: end
11: (xs (t0 ), . . . , xs (tN )) = (x(t0 ), . . . , x(tN ))
12: = + 1
13: end repeat
14: compute approximate control equation (14)

3. Stochastic Optimal Control Multi-Agent System
turn issue optimally controlling multi-agent system n agents.
principle, theory developed single agent straightforwardly generalizes multiagent situation. agent k-dimensional state xa satisfies dynamics similar
(1):
dxa (t) = ba (xa (t), t)dt + ua (x(t), t)dt + dwa (t),
(15)
accordance assumptions 1, 2 5 introduction. Note control
agent depends state xa , joint state x = (x1 , . . . , xn )
101

fivan den Broek, Wiegerinck & Kappen

system. system joint cost function similar (2), depending joint
state x joint control u = (u1 , . . . , un ) system:



n Z
X
1
u
u
2

C (x, t) = Ex,t (x(T )) +
kRa ua (x(), )k + V (xa (), ) .
2

a=1

expectation Eux,t taken respect probability measure x(t)
solution (15) given control law u condition x(t) = x. cost
combination joint end cost (x(T )), function joint end state x(T ),
integral instantaneous costs. instantaneous cost factorizes agents,
accordance assumption 5 introduction. agent, sum state
dependent term V (xa (), ) control dependent term 12 kRa ua (xa (), )k2 , similar
single agent case. accordance assumption 4 introduction, control cost
agent related noise agents dynamics via relation
= (Ra Ra )1 ,
agent. joint cost function minimized joint
control, yielding optimal expected cost-to-go J. optimal expected cost-to-go
expressed terms diffusion process via relation
J(x, t) = log Z(x, t),
Z(x, t) joint expectation value
!#
"
n Z
1X
1
V (ya (), )
Z(x, t) = Ex,t exp (y(T ))




(16)

a=1

y1 (t), . . . , yn (t) diffusion processes, = (y1 , . . . , yn ) y(t) = x, satisfying
uncontrolled dynamics
dya () = ba (ya (), )d + dwa (),

= 1, . . . , n.

(17)

multi-agent equivalent optimal control (9) reads
ua (x, t) = xa log Z(x, t).

(18)

show optimal control agent understood expected
control, is, integral target states ya transition probability target
times optimal control target. end, write expectation (16)
integral end state:
Z
n

1
Z(x, t) = dye (y)
Za (ya , ; xa , t),
(19)
a=1

Za (ya , ; xa , t) implicitly defined



Z
Z
1
V (ya (), )
dya Za (ya , ; xa , t)f (ya ) = Exa ,t f (ya (T )) exp

102

fiGraphical Model Inference MAS Optimal Control

arbitrary functions f . Substituting (19) (18) yields
Z
ua (x, t) = dya pa (ya |x, t) ua (ya ; xa , t)

(20)


ua (ya ; xa , t) = xa log Za (ya , ; xa , t)

(21)

optimal control agent go state xa current time state ya
end time , pa (ya |x, t) marginal
n


1
1
p(y|x, t) =
Za (ya , ; xa , t).
e (y)
Z(x, t)
a=1

3.1 Discrete End States
agents fulfill task arriving number target states end time
according initially specified way: example, arrive
target, arrive different targets. targets considered regions
G1 , . . . , Gm state space, end cost modeled follows:
1

e (y) =

X


w(s)

n


wa (ya ; sa ),

1

wa (ya ; sa ) = e (ya ;sa ) ,

(22)

a=1

sum runs assignments = (s1 , . . . , sn ) agents regions Gsa . (ya ; sa )
cost function associated region Gsa , returning low cost end state ya agent
lies region Gsa high cost otherwise. w(s) weight, grading assignments
thereby specifying joint task agents. Assignments result better
fulfillment task higher weight. situation agents go
target, example, vector assigns agent different target
low weight w(s).
choice end cost, equation (19) factorizes
Z(x, t) =

X



Za (sa ; xa , t) =

Z

w(s)

n


Za (sa ; xa , t)

a=1

dya Za (ya , ; xa , t)wa (ya ; sa ).

(23)

interpretation Za (sa ; xa , t) log Za (sa ; xa , t) expected cost agent
move xa target sa . optimal control (20) single agent becomes
ua (x, t) =


X

p(sa |x, t)ua (sa ; xa , t),

(24)

sa =1


ua (sa ; xa , t) = xa log Za (sa ; xa , t)
103

(25)

fivan den Broek, Wiegerinck & Kappen

control agent go target sa , weights p(sa |x, t) single-agent
marginals
X
p(s|x, t)
(26)
p(sa |x, t) =
s\sa

joint distribution
n


1
p(s|x, t) =
Za (sa ; xa , t).
(27)
w(s)
Z(x, t)
a=1


1
1
weight p(s|x,
Pnt) equals ratio exp J(s; x, t) / exp J(x, t) , J(s; x, t) =
log w(s) a=1 log Za (sa ; x, t) optimal expected cost-to-go case agents
predetermined targets specified assignment s; assignment agents
targets low expected cost J(s; x, t) yield high weight p(s|x, t),
associated single-agent single-target controls ua (sa ; xa , t) predominant
optimal controls ua (x, t).
3.2 Metropolis-Hastings Sampling Multi-Agent Systems
general, controls ua (sa ; xa , t) marginals p(sa |x, t) optimal control (24) closed form solution, inferred approximately.
controls ua (sa ; xa , t) approximated Metropolis-Hastings sampling discussed
Section 2.2. Inference marginals involves inference path integral formulations Za (sa ; xa , t):
Z
Z
1
1
Za (sa ; xa , t) = lim p
dxa (t1 ) . . . dxa (tN )e (xa (t0 ),...,xa (tN ),t0 ;sa )
2
N
0
det(2 )
xa (t0 ) = xa , t0 =

S(xa (t0 ), . . . , xa (tN ), t0 ; sa ) = (xa (T ); sa )
+

N
1
X

V (xa (ti ), ti ) +

i=0

N
1
X
i=0


2

1
xa (ti+1 ) xa (ti )

Ra
ba (xa (ti ), ti )
.
2


value Za (sa ; xa , t) generally hard determine (MacKay, 2003). Possible approximations include maximum posteriori (MAP) estimate inclusion variance
sample paths. third approximation take average path costs
estimate log Za (sa ; xa , t); means entropy distribution path
integral neglected.

4. Graphical Model Inference
additional computational effort multi-agent control compared single-agent control
lies computation marginals p(sa |x, t) joint distribution p(s|x, t),
involves sum mn assignments s. small systems feasible, large
systems summation performed efficiently. efficient approach
provided graphical model inference, relies factor graph representation
joint distribution.
104

fiGraphical Model Inference MAS Optimal Control

1,4

1,2

1

2,4

4

3,4

2

2,3

3

Figure 1: Example factor graph multi-agent system four agents. couplings
represented factors A, = {1, 4}, {1, 2}, {2, 4}, {3, 4}, {2, 3}.

4.1 Factor Graph Representation Joint Distribution
complexity joint distribution part determined weights w(s) end
cost function (22). weights determine agents consider states
agents. complex case, way one agent takes state another agent
account depend states agents. situation less complicated
agent considers states agents independently states others.
means joint end cost factorized form:

w(s) =
wA (sA ),
(28)


subsets agents. structure represented graphically so-called factor
graph (Kschischang, Frey, & Loeliger, 2001). See Figure 1 example. agents
factors nodes factor graph, represented circles squares respectively,
edge agent factor member subset A,
is, wA factorization w depends sa . (27) immediate
joint distribution p(s|x, t) factorizes according factor graph.
4.2 Junction Tree Algorithm
Efficient inference distribution p(s|x, t) means factor graph representation
accomplished using junction tree algorithm (Lauritzen & Spiegelhalter, 1988).
complexity algorithm exponential induced tree width graph. small
tree width expected systems factor graph sparse, case
agents take states account limited number agents.
implies multi-agent systems sparse graphs limited number targets
tractable (Wiegerinck et al., 2006). factor graph Figure 1 example sparse
graph. hand, agent take state agent account,
junction tree algorithm really help: underlying factor graph fully
connected tree width graph equals number agents system.
Exact computation optimal control intractable large complex multiagent systems, since junction tree algorithm requires memory exponential tree
width factor graph. Instead use graphical model approximate inference
methods approximately infer marginals (26). proceed discussion
two methods: naive mean field (MF) approximation (Jordan, Ghahramani, Jaakkola,
& Saul, 1999) belief propagation (BP) (Kschischang et al., 2001; Yedidia, Freeman, &
Weiss, 2001).
105

fivan den Broek, Wiegerinck & Kappen

4.3 Naive Mean Field Approximation
starting point note optimal expected cost-to-go log partition sum,
known free energy. Consider variational free energy
X
F (q) = h log wiq
hlog Za iqa H(q),


h iq h iqa denote expectation values respect distribution q marginals
qa respectively, H(q) entropy q:
X
H(q) =
q(s) log q(s).


optimal expected cost-to-go equals variational free energy minimized distributions q. naive mean field approximation
one considers variational free energy
Q
restricted factorized distributions q(s) = qa (sa ). minimum
F (q)
JMF = min
Q
q=

qa

upper bound optimal expected cost-to-go J, equals J case agents
uncoupled. F zero gradient local minima, is,
0=

F (q1 (s1 ) qn (sn ))
qa (sa )

= 1, . . . , n,

(29)

additional constraints normalization distributions qa . Solutions set
equations implicitly given mean field equations
Za (sa )hw|sa iq
qa (sa ) = Pn


sa =1 Za (sa )hw|sa iq

(30)

hw|sa iq conditional expectation w q given sa :

X
qa (sa ) w(s1 , . . . , sn ).
hw|sa iq =
s1 ,...,sn \sa

6=a

mean field equations solved means iteration; procedure results
convergence local minimum free energy.
mean field approximation optimal control found taking gradient
respect x minimum JMF free energy. similar exact case
optimal control gradient optimal expected cost-to-go, equation (18).
Using (29), find
X
1
ua (x, t) = xa JMF (x, t) =
qa (sa )ua (xa , t; sa ).




Similar exact case, average single-agent single-target optimal controls
ua (xa , t; sa ) given equation (25), average taken respect mean
field approximate marginal qa (sa ) agent a.
106

fiGraphical Model Inference MAS Optimal Control

4.4 Belief Propagation
belief propagation, approximate free energy Bethe free energy,
minimize latter. Bethe free energy defined
FBethe ({qa , qA }) =

X

h log wA iqA



X

h log Za iqa



X

H(qA ) +



X

(na 1)H(qa ).



(31)
function beliefs qa (sa ) qA (sA ), non-negative normalized functions
satisfy consistency relations:
:

X

qA (sA ) = qa (sa ).

sA\a

H(qa ) H(qA ) entropies beliefs qa qA , na denotes number
neighbors node factor graph.
Belief propagation algorithm computes beliefs (Kschischang et al., 2001).
case joint distribution p factor graph representation tree, belief propagation converge beliefs exact marginals p, Bethe free energy
beliefs equals optimal expected cost-to-go J. factor graph representation
p contains cycles, may still apply belief propagation. Yedidia et al. (2001) showed
fixed points algorithm correspond local extrema Bethe free energy.
particular, advanced variations algorithm (Heskes, Albers, & Kappen, 2003;
Teh & Welling, 2001; Yuille, 2002) guaranteed converge local minima Bethe
free energy (Heskes, 2003).
find BP approximation optimal control taking gradient
minimum JBethe Bethe free energy:
X
1
ua (x, t) = xa JBethe (x, t) =
qa (sa )ua (xa , t; sa ),




ua (xa , t; sa ) given equation (25). Similar exact case mean field
approximation, BP approximation optimal control average single-agent
single-target optimal controls, average taken respect belief qa (sa ).

5. Numerical Results
section, present numerical results simulations optimal control multiagent systems. problem computing optimal controls (24) consists two parts:
inference single-agent single-target controls (25), inference
marginals (26) global distribution agent target assignments. dynamics linear, instantaneous costs V zero, single-agent single-target
controls given closed form. multi-agent systems therefore know issue
infering marginal distributions. Section 5.1 consider multi-agent systems
kind. Section 5.2 deals general problem infering optimal controls
dynamics nonlinear instantaneous costs V nonzero. sections
107

fiExpected Target

van den Broek, Wiegerinck & Kappen

Position

1
0
1
0

0.5

1
Time

1.5

2

1
0
1
0

(a) Positions

0.5

1
Time

1.5

2

(b) Expected Targets

Figure 2: Two agents, noise control positions, need reach target locations -1 1 end time = 2, agent different target location.
positions (a) expected targets (b) time.

joint end cost given equation (22),
w(s) =

n

a,b

wa,b (sa , sb ),

c

wa,b (sa , sb ) = exp sa ,sb ,
n

(32)




1
(33)
(ya ; sa ) = |ya sa |2 ,
wa (ya ; sa ) = exp (ya ; sa ) ,

2
c determines coupling strength agents, sa target
states.
5.1 Linear Dynamics
begin illustration optimal control showing simulation exactly
solvable stochastic multi-agent system. system two agents one dimension,
agents satisfy dynamics (15) ba equal zero. two target states, x = 1 = 1
x = 2 = 1. task agents one go different target.
instantaneous costs V cost function zero, end cost function given
equations (22), (32) (33) = 20 c = 4. negative sign coupling
strength c implies repulsion agents. control cost parameter R equals 1,
noise level 2 lies 0.5. agents start x = 0 time = 0, end time lies
= 2. prevent overshooting targets, udt small compared distance
target states. done choosing dt = 0.05(T + 0.05).
P
Figure 2 shows agents positions expected targets
sa =1,2 p(sa |x, t)sa
time. see time = 1, agents decided target
go, remain two targets. Then, = 1, final decision
seems made. delayed choice due symmetry breaking cost-togo time increases. symmetry breaking, better keep options open,
see effect noise is. symmetry breaking, time short wait
longer choice made. phenomenon typical multi-modal problems.
proceed quantitative comparison different control methods arise
exact approximate inferences marginals joint distribution (27).
108

fiGraphical Model Inference MAS Optimal Control

3

10

4

2

CPU Time

Cost Difference

5
3
2
1
0
1
0

10

1

10

0

10

1

0.2

0.4 0.6
Noise

0.8

10

1

(a) Costs

0

0.2

0.4 0.6
Noise

0.8

1

(b) CPU Time

Figure 3: deviation optimal cost (a) required CPU Time seconds
(b) functions noise. lines represent exact ( ), Greedy ( ), MF
() BP () control.

example consider multi-agent system n = 18 agents two-dimensional
state space zero instantaneous costs (V = 0) autonomous dynamics (ba = 0).
end cost function given equations (22), (32) (33). two targets located
1 = (1, 0) 2 = (1, 0). = 20 c = 0.5. control cost matrix R equals
identity matrix. agents start (0, 0) time = 0, end time lies = 2,
time steps size dt = 0.05(T + 0.05).
approximations naive mean field approximation belief propagation, described Section 4, greedy control. greedy control mean time step
agent chooses go nearest target. include approximation
simple requires little computation time, reasons obvious choice
naive approximation. greedy control policy neglects choices
agents, expect give inferior performance.
approximation, Figure 3(a) shows cost approximate (optimal)
control minus cost exact (optimal) control, averaged 100 simulations,
different noise levels. noise samples used approximate exact
control. see naive mean field approximation belief propagation yield
costs average coincide cost exact control: average cost difference
methods significantly differ zero. Greedy control,
hand, yields costs significantly higher costs exact control;
deterministic limit converge cost exact control, controls
coincide. Figure 3(b) shows CPU time required calculation controls
different control methods. average CPU time entire simulation.
simulation consists 73 time steps, time step control calculated
agent. observe greedy control least 10 times faster methods,
exact control nearly 100 times time consuming methods. Belief
propagation gives performance considered noise levels bit quicker
naive mean field approximation, may result implementation details.
done simulations attractive coupling c = 0.5; returned results similar
ones repulsive coupling c = 0.5 presented here.
109

fiCumulative Control Cost

van den Broek, Wiegerinck & Kappen

20
15
10
5
0
0

0.5

1
Time

1.5

2

Figure 4: cumulative control cost time, case strong repulsive coupling
c = 2 low noise level 2 = 0.1. curves represent exact ( ), MF
(), BP control ().

Although Figure 3 suggests belief propagation naive mean field approximation
perform equally well, always case, since certain combinations noise
level coupling strength BP control costly MF control exact
control. origin difference lies symmetry breaking, tends occur
later BP earlier MF compared exact control. observe
Figure 4, shows cumulative cost time control methods
multi-agent system, coupling strength c = 2 fixed noise level 2 = 0.1.
cumulative costs averages 100 simulations. cost MF control lies
bit higher cost exact control, whereas cost BP control initially
lower cost control methods, = 1.7 starts increase
much faster eventually ends higher. Including end costs, found total costs
26.13 0.12 exact control, 26.19 0.12 MF control, 35.5 0.4 BP
control. suggests better early symmetry breaking late
symmetry breaking.
time required computing control various methods depends
number agents multi-agent system. Figure 5 shows required CPU time
function number agents n two-dimensional multi-agent system considered
above. see exact method requires CPU time increases exponentially
number agents. may expected theory,
exact method uses junction tree algorithm complexity exponential
tree width underlying graph, i.e., exponential n. greedy method,
CPU time increases linearly number agents, agreement
fact greedy control coupling agents. required CPU
time increases polynomially mean field approximation belief propagation.
5.2 Nonlinear Dynamics
turn multi-agent systems nonlinear dynamics. control systems,
must approximate graphical model inference well single-agent singletarget control problem (12). consider multi-agent system agents move
110

fiGraphical Model Inference MAS Optimal Control

3

10
CPU Time

2

10

1

10

0

10

1

10

10

15

20
Agents

25

30

Figure 5: required CPU time seconds calculation controls different
number agents. Exact ( ), greedy ( ), MF (), BP control ().

two dimensions four-dimensional state specified agents location
(xa , ya ), forward velocity va , driving direction . dynamics agent
given equations
dxa = va cos dt
dya = va sin dt
dva = ua dt + dwa
da = dt + da .
first two equations model kinematics agents position given forward
velocity driving direction. last two equations describe control speed
driving direction application forward acceleration ua angular velocity
. noise control modeled standard normal Wiener processes wa
noise level parameters . Note noise act dimensions
control. Although control space counts less dimensions
state space, example fit general framework: refer Appendix C
details.
look two different tasks. first task obstacle avoidance multiagent system three agents. agents reach one three target locations
avoid obstacles environment. target location reached precisely
one agent; model end cost function, given equations (22), (32) (33),
= c = 0.5. targets located (10, 15), (45, 12) (26, 45),
agents arrive zero velocity. control cost matrix R identity matrix.
= 0.1. instantaneous cost V equaled 1000 locations obstacles, zero
otherwise. agents start time = 0, end time lies = 20, time steps dt
size 0.2. starting locations agents (18, 31), (25, 12) (39, 33),
agents start zero velocity. sample paths discrete time paths twodimensional space forward velocity v driving direction . specified
values times ti = + , = 0, . . . , N 1, = NT
1 N = 7, value
time t0 equals current state one agents, value time tN equals
one target end states. control agent one targets computed
111

fivan den Broek, Wiegerinck & Kappen

50

50

40

40

30

30

20

20

10

10

0

0

10

20

30

40

50

0

(a) Trajectories

0

10

20

30

40

50

(b) Sample paths

Figure 6: Three agents, noise control forward velocities driving directions, reach three targets (marked X) environment containing
number walls. agent starts different location (marked O)
zero forward velocity, agent arrive different target
zero velocity without hitting walls. (a) trajectories agents
followed reach targets. (b) Sample paths.

Metropolis-Hastings sampling paths, according Subsection 3.2. proposal
distribution 2N -dimensional Gaussian, centered around agents current planned
path, variance equal noise level agents dynamics. expectation
values Za (sa ; xa , t) estimated average costs sample paths.
tried MAP estimation Za (sa ; xa , t) inclusion variance sample paths,
former show significant difference, latter returned estimates
fluctuated heavily. Figure 6(a) shows environment trajectories agents
starting locations targets. agent manages avoid obstacles
arrive one targets zero velocity, target reached different
agent.
second task coordination multi-agent system shown Figure 7(a). system instantaneous costs (V = 0). agents move
initial positions number target locations. arrive
locations zero velocity horizontal driving direction. equal number
agents target locations, agent reach different target. initial
locations aligned vertically, target locations, vertical displacement two. Thus agents coordinate movements order
reach targets satisfactory way.
agents start time 0, end time lies 100, make time steps size

dt = 2(N
1) , N = 7, dt < 0.01. time step controls computed
Metropolis-Hastings sampling paths naive mean field approximation infer
marginals pa (sa |x, t) weigh single-agent single-target controls, equations (24)
(26). sample paths discretized seven equidistant time points
present time end time. proposal distribution taken Gaussian,
112

fiGraphical Model Inference MAS Optimal Control

centered around agents current planned path variance equal noise
level agents dynamics. Figure 7(a) shows example trajectories system
10 agents. obtained 10 sample paths per agent-target combination.
observe agents reach targets, target reached precisely one
agent, required. Due noise second order dynamics agents, takes
agents less effort approach target remain there, since former allows
exploitation noise latter requires constant correction state changes
caused noise. result trajectories agents curved
elongated would expected situation without noise. simulation
carried well larger number agents. Figure 7(b) shows required CPU time
function number agents, exact MF inference marginals
agents. Note complexity graphical model inference problem scales
nn , n number agents. Exact inference using junction tree algorithm
feasible n < 10.

6. Discussion
studied use graphical model inference methods optimal control stochastic
multi-agent systems continuous space time agents joint task
reach number target states. Rather discretizing, commonly done typically
makes large systems intractable due curse dimensionality, followed approach
developed Wiegerinck et al. (2006), modeling system continuous space time.
certain assumptions dynamics cost function, solution given
terms path integral.
path integral computed closed form special cases,
linear-quadratic case, general approximated. done
variety methods. method considered paper MCMC sampling.
dimension sample paths kept low (N = 7) limit curvature sample
paths. gain limiting curvature variance samples reduced
less samples needed. limiting curvature, however, introduce bias.
addition, presence obstacles insufficient curvature would make sampler return
sample paths run obstacles. believe advanced MCMC
methods Hybrid MC sampling (Duane, Kennedy, Pendleton, & Roweth, 1987)
overrelaxation (Neal, 1998) improve inference path integrals.
Apart MCMC sampling, approximation methods one could
consider, Laplace approximation variational approximation. Laplace
approximation becomes exact noiseless limit could useful low noise regimes
well. variational approximation approximates path integral (11) Gaussian
process (Archambeau, Opper, Shen, Cornford, & Shawe-Taylor, 2007), could particularly useful high noise regime. drawback variational approach, however,
cannot straightforwardly applied situations infinite instantaneous costs,
hard obstacles environment considered here.
Wiegerinck et al. (2006) showed systems sufficiently sparse
single-agent single-target controls determined closed form, e.g. linearquadratic control time-independent coefficients, exact inference achieved using
113

fivan den Broek, Wiegerinck & Kappen

50

40

30

20

10

0

10

20

30

40

50
20

15

10

5

0

5

10

15

(a) Trajectories
5

10

4

CPU time

10

3

10

2

10

1

10

0

20

40
60
Number Agents

80

(b) CPU time

Figure 7: (a) trajectories 10 agents starting locations 10 targets X. (b)
required CPU time seconds function number agents,
number targets equal number agents. lines represent exact
( ) MF () inference marginals.

114

fiGraphical Model Inference MAS Optimal Control

junction tree algorithm. Van den Broek, Wiegerinck, Kappen (2007) considered
multi-agent system second-order dynamics, linear autonomous dynamics zero
instantaneous costs, showed graphical model inference naive mean field approximation significantly outperformed greedy inference. showed close
optimal result achieved well dense systems, using graphical model approximate
inference methods. approximation methods considered naive mean field
approximation belief propagation. demonstrated performances example
system exact inference significantly time consuming. Mean field approximation showed work well, returning costs control equal optimal ones, belief
propagation performed similarly. certain value ratio coupling strength
noise level, symmetry breaking control process takes place earlier
mean field approximation compared exact inference, later belief propagation. early symmetry breaking increase costs coordination much,
however, late symmetry breaking does, making performance belief propagation
suboptimal.
variations considered case possible within general framework.
Wiegerinck, van den Broek, Kappen (2007) discuss situations agents sequentially
visit number targets, end time fixed. focusses prefered
trajectories state space time, instead prefered states end time;
achieved modeling path cost way similar modeled end cost.
problem agents intercept moving target noisy dynamics
covered there.
control formalism developed Kappen (2005a, 2005b) applied multi-agent
coordination Wiegerinck et al. (2006) article, demands noise
control act dimensions. One way satisfy constraint assume
agents identical. addition, single agent dynamics
noise control act dimensions. saw two-dimensional
second order system Section 5.2 condition satisfied natural way. However,
general one think examples control problems equation (3) violated.
interesting future direction research investigate extend path integral
approach used approximation cases.
paper assumes joint state space agents observable agents.
large multi-agent systems, however, realistic agent observes
state states agents physically nearby. approach
directly apply situations. Depending joint task agents, may
valid approximation optimal control sub-system consisting agents
one agent observe. task agents avoid collisions,
sufficient consider states agents nearby, task go
target crucial information states
agents. natural alternative deal partial observability describe multi-agent
system decentralized POMDP (Seuken & Zilberstein, 2008). clear however,
approach would combine path integral formalism.
topic learning addressed paper, clearly great
interest. However, one could argue sampling procedure compute path integral
115

fivan den Broek, Wiegerinck & Kappen

corresponds learning environment. discussion line thought
found (Kappen, 2007).
many possible model extensions worthwhile exploring future research.
Obvious examples bounded controls, limited observation global state
system; issues already interest study single agent situation. Others
apply typically multi-agent situation. context physical agents, introducing penalties collisions agents would become relevant. Typically, types
model extensions solution closed form, require additional
approximate numerical methods. suggestions given Kappen (2005a, 2005b).
Acknowledgments
thank reviewers useful comments. thank Joris Mooij making
available useful software (www.mbfys.ru.nl/~jorism/libDAI/). research part
Interactive Collaborative Information Systems (ICIS) project, supported Dutch
Ministry Economic Affairs, grant BSIK03024.

Appendix A. Stochastic Optimal Control
appendix give derivation (5), (6) (7), starting (1), (2), (3)
(4). Detailed discussions found many works stochastic optimal control,
example Kushner (1967), Fleming Rishel (1975), Fleming (1978), ksendal
(1998), Stengel (1993), Kappen (2005a, 2005b).
optimal expected cost-to-go J state x time defined
J(x, t) = min C u (x, t),
u


u

C (x, t) =

Eux,t


Z
(x(T )) +










1
2
kRu(x(), )k + V (x(), )
2

(34)

(35)

expected cost given control law u. equations (4) (2)
main text. first show J satisfies stochastic Hamilton-Jacobi-Bellman (SHJB)
equation


1
1 2
2

J = min
kRuk + (b + u) x J + Tr x J + V ,
(36)
u
2
2

boundary condition J(x, ) = (x). equation derived following way.
moment time holds


Z
1
2
u
ds
kRu(x(s), s)k + V (x(s), s)
J(x, t) =
C (x(), ) +
2



Z
1
2
u
kRu(x(s), s)k + V (x(s), s) .
ds
= min Ex,t J(x(), ) +
u
2

min Eux,t
u

first line follows dividing integral two integrals, one
one , using definition cost function C, second line
116

fiGraphical Model Inference MAS Optimal Control

follows definition J. rewriting yields


Z
J(x(), ) J(x, t)
1
1
u
2
0 = min Ex,t
ds
+
kRu(x(s), s)k + V (x(s), s) .
u


2
Taking limit obtain


dJ(x(t), t) 1
2
u
+ kRu(x(t), t)k + V (x(t), t) .
0 = min Ex,t
u
dt
2

(37)

Subsequently, apply dJ(x(t), t) well known chain rule diffusion processes:
dJ(x(t), t) =

X J(x(t), t)


xi

dxi (t) +

J(x(t), t)
1 X 2 J(x(t), t)
dt +
dxi (t)dxj (t). (38)

2
xi xj
i,j

differs chain rule deterministic processes contains term
quadratic dx. extra term vanish, Wiener process appearing
dynamics (1) quadratic variation increases linear time:
Eux,t [dwi (t)dwj (t)] = ij dt.

(39)

follows expectation dxi (t)dxj (t) equal ( )ij dt. substituting dynamics (1) (38), taking expectation values, using (39), obtain


2
J(x, t)
J(x, t)
J(x, t)
u
dt + (b(x, t) + u(x, t))
dt + Tr
dt.
Ex,t [dJ(x(t), t)] =

x
xx
Substitution equation (37) yields equation (36).
minimum right-hand side equation (36) given
u = (R R)1 x J.
optimal control.
minimization (36) removed inserting optimal control. yields
nonlinear equation J. remove nonlinearity using logarithmic transformation: introduce constant , define Z(x, t) J(x, t) = log Z(x, t),

1
1
u R Ru + u x J = 2 Z 2 (x Z) (R R)1 x Z,
2
2


1 2
1
1 2
Tr x J
=
Z (x Z) x Z Z 1 Tr x2 Z .
2
2
2

terms quadratic x Z vanish R related via equation (3),
= (R R)1 .
relation satisfied, SHJB equation becomes



V
1

2
Z =
b x Tr x Z

2
= HZ,
117

(40)

fivan den Broek, Wiegerinck & Kappen

H linear operator acting function Z.
Equation (40) must solved backwards time boundary condition Z(x, ) =
1

e (x) . present solution terms forward diffusion process. common approach theory stochastic processes give solutions partial differential equations
terms diffusion processes. solution equation (40) expectation value



Z
1
1
Z(x, t) = Ex,t exp (y(T ))
V (y(), ) ,



(41)

y() process satisfies uncontrolled dynamics
dy() = b(y(), )d + dw(),
y(t) = x. expectation Ex,t taken respect probability measure
y() satisfies uncontrolled dynamics condition y(t) = x. clear (41)
matches boundary condition. verify satisfies equation (40), let


Z
1
V (y(), ) .
I(t) = exp

see

1
V (y(t), t)I(t)dt.


Let f function f (y) = exp 1 (y) . use chain rule stochastic
processes apply f (y(T )) find
dI(t) =

k
k
X
f (y(T ))
1 X 2 f (y(T ))
dyi (T ) +
dyi (T )dyj (T )
df (y(T )) =
yi
2
yi yj
i=1
i,j=1


f (y(T ))
(b(y(T ), )d + dw(T ))
=



2
1
f (y(T ))
Tr
d.
2
yy

choose = 0 = dt combine identity previous one
obtain
df (y(T ))I(t) = f (y(T ))dI(t) + I(t)df (y(T ))
= Hf (y(T ))I(t)dt + f (y(T ))I(t)dw(T ).
Taking expectation value sides makes term f (y(T ))I(t)dw(T ) disappear,
remaining part,
dE [f (y(T ))I(t)] = [f (y(T ))I(t)] dt,
equation (40).
118

fiGraphical Model Inference MAS Optimal Control

Appendix B. Path Integral Formulation
going write expectation value (7) path integral. Partitioning time
interval N intervals equal length , = t0 < t1 < . . . < tN = ,
expectation value written follows:
Z(x, t) =

Z

dx1 . . .

Z

1

dxN e (xN )

N
1


Z(xi+1 , ti+1 ; xi , ti )

(42)

i=0

x0 = x Z(xi+1 , ti+1 ; xi , ti ) implicitly defined




Z
Z

1 ti+1

dxi+1 Z(xi+1 , ti+1 ; xi , ti )f (xi+1 ) = E f (xi+1 ) exp
V (y(), ) fiy(ti ) = xi
ti

arbitrary functions f . limit infinitesimal , Z(xi+1 , ti+1 ; xi , ti ) satisfy


1
Z(xi+1 , ti+1 ; xi , ti ) = (xi+1 , ti+1 |xi , ti ) exp V (xi , ti ) ,
(43)


(xi+1 , ti+1 |xi , ti ) transition probability uncontrolled dynamics (8) go
(xi , ti ) (xi+1 , ti+1 ) space-time. transition probability given


1
k 1 (xi+1 xi b(xi , ti ))k2
.
(xi+1 , ti+1 |xi , ti ) = p
exp
2
det(2 2 )
follows dynamics

xi+1 xi = b(xi , ti ) + w
infinitesimal time interval observation Wiener process w normally distributed around zero variance . Using equation (3), may rewrite
transition probability

2 !

1
1
x

x
i+1

exp
(xi+1 , ti+1 |xi , ti ) = p
R
b(xi , ti )
(44)

.
2
2

det(2 )

obtain path integral representation Z(x, t) combining equations (42), (43)
(44) limit going zero:
Z(x, t) = lim Z (x0 , t0 )

(45)

0

x0 = x, t0 = t,
1



Z (x0 , t0 ) = p
det(2 2 )N

(x0 , . . . , xN , t0 ) = (xN ) +

N
1
X

Z

dx1 . . .

V (xi , ti ) +

Z

N
1
X
i=0

i=0

119

1

dxN e (x0 ,...,xN ,t0 )


2

xi+1 xi
1

b(xi , ti )
R
.
2


fivan den Broek, Wiegerinck & Kappen

optimal control given equation (9) proportional gradient
log Z(x, t). Substituting path integral representation (45) Z(x, t), find
u(x0 , t0 ) = lim
0

= lim
0

Z

Z

dx1 . . .
dx1 . . .

Z

Z

1

e (x0 ,...,xN ,t0 )

dxN p
x0
det(2 2 )N Z (x, t0 )




1
(x0 , . . . , xN , t0 )


dxN p(x0 , . . . , xN , t0 )u(x0 , . . . , xN , t0 )


u(x0 , . . . , xN , t0 ) =




x1 x0
b(x0 , t0 )

1

e (x0 ,...,xN ,t0 )
p(x0 , . . . , xN , t0 ) = p
.
det(2 2 )N Z (x0 , t0 )

Note control u(x0 , . . . , xN , t0 ) results path (x0 , . . . , xN ) depends
first two entries x0 x1 path.

Appendix C. Dimension Reduction
derivation path integral Appendix B given case
state control k-dimensional. particular case dimensions
state controlled deduced taking limit infinite control cost along
dimensions without control. control along latter dimensions becomes zero,
seen equation (5). noise dimensions equal zero accordance
relation (3). path integral formalism transition probabilities (44)
reduce delta functions along dimensions without control. implications
MCMC sampling dimension space sample reduced,
since sampling performed dimensions noise.

References
Archambeau, C., Opper, M., Shen, Y., Cornford, D., & Shawe-Taylor, J. (2007). Variational inference
diffusion processes. Advances Neural Information Processing Systems.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent decentralized Markov decision processes. Proceedings Second International Joint Conference
Autonomous Agents Multiagent Systems, pp. 4148.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent
decentralized Markov decision processes. Journal Artificial Intelligence Research, 22, 423
455.
Boutilier, C. (1996). Planning, learning coordination multiagent decision processes.
Proceedings Sixth Conference Theoretical Aspects Rationality Knowledge, pp.
195210.
Castanon, D. A., Pachter, M., & Chandler, P. R. (2004). game deception. Proceedings
43rd IEEE Conference Decision Control, pp. 33643369.
Duane, S., Kennedy, A., Pendleton, B., & Roweth, D. (1987). Hybrid Monte Carlo. Physics Letters
B, 195 (2), 216222.
120

fiGraphical Model Inference MAS Optimal Control

Fleming, W. H. (1978). Exit probabilities optimal stochastic control. Applied Mathematics
Optimization, 4, 329346.
Fleming, W. H., & Rishel, R. W. (1975). Deterministic Stochastic Optimal Control. SpringerVerlag, New York.
Guestrin, C., Koller, D., & Parr, R. (2002a). Multiagent planning factored MDPs. Advances
Neural Information Processing Systems, Vol. 14, pp. 15231530.
Guestrin, C., Venkataraman, S., & Koller, D. (2002b). Context-specific multiagent coordination
planning factored MDPs. Eighteenth National Conference Artificial Intelligence,
pp. 253259.
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains applications.
Biometrika, 57 (1), 97109.
Heskes, T. (2003). Stable fixed points loopy belief propagation minima Bethe free
energy. Advances Neural Information Processing Systems, Vol. 15, pp. 343350.
Heskes, T., Albers, K., & Kappen, B. (2003). Approximate inference constrained optimization.
Proceedings 19th Conference Uncertainty Artificial Intelligence, pp. 313320.
Hu, J., Prandini, M., & Tomlin, C. (2007). Conjugate points formation constrained optimal
multi-agent coordination: case study. SIAM Journal Control Optimization, 45 (6),
21192137.
Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1999). introduction variational methods
graphical models. Learning Graphical Models. MIT Press, Cambridge.
Kamal, W. A., Gu, D.-W., & Postlethwaite, I. (2005). Real time trajectory planning UAVs using
MILP. Proceedings 4th IEEE Conference Decision Control, European
Control Conference 2005, pp. 33813386.
Kappen, H. J. (2005a). Path integrals symmetry breaking optimal control theory. Journal
statistical mechanics: theory experiment, P11011.
Kappen, H. J. (2005b). Linear theory control nonlinear stochastic systems. Physical Review
Letters, 95 (20), 200201.
Kappen, H. J. (2007). introduction stochastic control theory, path integrals reinforcement
learning. AIP conference proceedings, Vol. 887, pp. 149181.
Kleinert, H. (2006). Path Integrals Quantum Mechanics, Statistics, Polymer Physics, Financial Markets. World Scientific, Singapore.
Kschischang, F. R., Frey, B. J., & Loeliger, H.-A. (2001). Factor graphs sum-product
algorithm. IEEE Transactions Information Theory, 47 (2), 498519.
Kushner, H. J. (1967). Stochastic Stability Control. Academic Press Inc., New York.
Larson, R. A., Pachter, M., & Mears, M. (2005). Path planning unmanned air vehicles
engaging integrated radar network. Proceedings AIAA Guidance, Navigation,
Control Conference Exhibit.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical structures application expert systems (with discussion). J. Royal Statistical Society
Series B, 50, 157224.
Liu, Y., Cruz, J. B., & Schumacher, C. J. (2007). Pop-up threat models persistent area denial.
IEEE Transactions Aerospace Electronic Systems, 43 (2), 509521.
MacKay, D. J. (2003). Information Theory, Inference, Learning Algorithms. Cambridge University Press.
Neal, R. M. (1998). Learning Graphical Models, pp. 205225. Kluwer Academic Publishers.
121

fivan den Broek, Wiegerinck & Kappen

ksendal, B. (1998). Stochastic Differential Equations: Introduction Applications. SpringerVerlag.
Pachter, L., & Pachter, M. (2001). Optimal paths avoiding radiating source. Proceedings
40th IEEE Conference Decision Control, pp. 35813586.
Ribichini, G., & Frazzoli, E. (2003). Efficient coordination multiple-aircraft systems. Proceedings
42nd IEEE Conference Decision Control, Vol. 1, pp. 10351040.
Sadati, N., & Elhamifar, E. (2006). Semi-decentralized control multi-agent systems based
redundant manipulator optimization methods. Proceedings 9th IEEE International
Workshop Advanced Motion Control, pp. 278283.
Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision making
uncertainty. Journal Autonomous Agents Multi-Agent Systems.
Shi, X., Wang, X., Liu, Y., Wang, C., & Zu, C. (2007). Optimization fighter aircraft evasive trajectories radar threats avoidance. Proceedings 2007 IEEE International Conference
Control Automation, pp. 303307.
Stengel, R. (1993). Optimal Control Estimation. Dover Publications, New York.
Subramanian, S. K., & Cruz, J. B. (2003). Adaptive models pop-up threats multi-agent
persistent area denial. Proceedings 42nd IEEE Conference Decision Control,
pp. 510515.
Teh, Y., & Welling, M. (2001). unified propagation scaling algorithm. Advances
Neural Information Processing Systems, Vol. 14, pp. 953960.
Todorov, E., & Li, W. (2005). generalized iterative LQG method locally-optimal feedback
control constrained nonlinear stochastic systems. Proceedings American Control
Conference, pp. 300306.
Tomlin, C., Pappas, G. J., & Sastry, S. (1998). Conflict resolution air traffic management: study
multiagent hybrid systems. IEEE Transactions Automatic Control, 43 (4), 509521.
van den Broek, B., Wiegerinck, W., & Kappen, B. (2007). Optimal control large stochastic multiagent systems. Proceedings Seventh Symposium Adaptive Learning Agents
Multi-Agent Systems, pp. 920.
van Leeuwen, P., Hesseling, H., & Rohling, J. (2002). Scheduling aircraft using constraint satisfaction.
Electronic Notes Theoretical Computer Science, 76, 252268.
Wiegerinck, W., van den Broek, B., & Kappen, B. (2006). Stochastic optimal control continuous
space-time multi-agent systems. Proceedings 22nd Conference Uncertainty
Artificial Intelligence, pp. 528535.
Wiegerinck, W., van den Broek, B., & Kappen, B. (2007). Optimal on-line scheduling stochastic
multi-agent systems continuous space-time. Proceedings Sixth International Joint
Conference Autonomous Agents Multiagent Systems, pp. 744751.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. Advances Neural
Information Processing Systems, Vol. 13, pp. 689695.
Yuille, A. (2002). CCCP algorithms minimize Bethe Kikuchi free energies: Convergent
alternatives belief propagation. Neural Computation, 14 (7), 16911722.

122


