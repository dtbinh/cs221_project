Journal Artificial Intelligence Research 32 (2008) 663-704

Submitted 03/08; published 07/08

Online Planning Algorithms POMDPs
Stephane Ross
Joelle Pineau

stephane.ross@mail.mcgill.ca
jpineau@cs.mcgill.ca

School Computer Science
McGill University, Montreal, Canada, H3A 2A7

Sebastien Paquet
Brahim Chaib-draa

spaquet@damas.ift.ulaval.ca
chaib@damas.ift.ulaval.ca

Department Computer Science Software Engineering
Laval University, Quebec, Canada, G1K 7P4

Abstract
Partially Observable Markov Decision Processes (POMDPs) provide rich framework
sequential decision-making uncertainty stochastic domains. However, solving
POMDP often intractable except small problems due complexity. Here,
focus online approaches alleviate computational complexity computing
good local policies decision step execution. Online algorithms generally consist lookahead search find best action execute time step
environment. objectives survey various existing online POMDP
methods, analyze properties discuss advantages disadvantages;
thoroughly evaluate online approaches different environments various metrics (return, error bound reduction, lower bound improvement). experimental results
indicate state-of-the-art online heuristic search methods handle large POMDP
domains efficiently.

1. Introduction
Partially Observable Markov Decision Process (POMDP) general model sequential decision problems partially observable environments. Many planning control problems modeled POMDPs, solved exactly
computational complexity: finite-horizon POMDPs PSPACE-complete (Papadimitriou & Tsitsiklis, 1987) infinite-horizon POMDPs undecidable (Madani, Hanks,
& Condon, 1999).
last years, POMDPs generated significant interest AI community many approximation algorithms developed (Hauskrecht, 2000; Pineau,
Gordon, & Thrun, 2003; Braziunas & Boutilier, 2004; Poupart, 2005; Smith & Simmons,
2005; Spaan & Vlassis, 2005). methods offline algorithms, meaning
specify, prior execution, best action execute possible situations.
approximate algorithms achieve good performance, often take significant time (e.g. hour) solve large problems, many
possible situations enumerate (let alone plan for). Furthermore, small changes
environments dynamics require recomputing full policy, may take hours days.
c
2008
AI Access Foundation. rights reserved.

fiRoss, Pineau, Paquet, & Chaib-draa

hand, online approaches (Satia & Lave, 1973; Washington, 1997; Barto,
Bradtke, & Singhe, 1995; Paquet, Tobin, & Chaib-draa, 2005; McAllester & Singh, 1999;
Bertsekas & Castanon, 1999; Shani, Brafman, & Shimony, 2005) try circumvent complexity computing policy planning online current information state. Online algorithms sometimes called agent-centered search algorithms (Koenig, 2001).
Whereas offline search would compute exponentially large contingency plan considering possible happenings, online search considers current situation small
horizon contingency plans. Moreover, approaches handle environment
changes without requiring computation, allows online approaches applicable many contexts offline approaches applicable, instance,
task accomplish, defined reward function, changes regularly environment.
One drawback online planning generally needs meet real-time constraints,
thus greatly reducing available planning time, compared offline approaches.
Recent developments online POMDP search algorithms (Paquet, Chaib-draa, & Ross,
2006; Ross & Chaib-draa, 2007; Ross, Pineau, & Chaib-draa, 2008) suggest combining
approximate offline online solving approaches may efficient way tackle
large POMDPs. fact, generally compute rough policy offline using existing
offline value iteration algorithms, use approximation heuristic function
guide online search algorithm. combination enables online search algorithms
plan shorter horizons, thereby respecting online real-time constraints retaining
good precision. exact online search fixed horizon, guarantee
reduction error approximate offline value function. overall time (offline
online) required obtain good policy dramatically reduced combining
approaches.
main purpose paper draw attention AI community online
methods viable alternative solving large POMDP problems. support this,
first survey various existing online approaches applied POMDPs,
discuss strengths drawbacks. present various combinations online algorithms
various existing offline algorithms, QMDP (Littman, Cassandra, & Kaelbling,
1995), FIB (Hauskrecht, 2000), Blind (Hauskrecht, 2000; Smith & Simmons, 2005)
PBVI (Pineau et al., 2003). compare empirically different online approaches
two large POMDP domains according different metrics (average discounted return, error
bound reduction, lower bound improvement). evaluate available online
planning time offline planning time affect performance different algorithms.
results experiments show many state-of-the-art online heuristic search methods
tractable large state observation spaces, achieve solution quality stateof-the-art offline approaches fraction computational cost. best methods
achieve focusing search relevant future outcomes current
decision, e.g. likely high uncertainty (error) longterm values, minimize quickly possible error bound performance
best action found. tradeoff solution quality computing time offered
combinations online offline approaches attractive tackling increasingly
large domains.
664

fiOnline Planning Algorithms POMDPs

2. POMDP Model
Partially observable Markov decision processes (POMDPs) provide general framework
acting partially observable environments (Astrom, 1965; Smallwood & Sondik, 1973;
Monahan, 1982; Kaelbling, Littman, & Cassandra, 1998). POMDP generalization
MDP model planning uncertainty, gives agent ability
effectively estimate outcome actions even cannot exactly observe state
environment.
Formally, POMDP represented tuple (S, A, T, R, Z, O) where:
set environment states. state description environment
specific moment capture information relevant agents
decision-making process.
set possible actions.
: [0, 1] transition function, (s, a, s0 ) = Pr(s0 |s, a)
represents probability ending state s0 agent performs action state
s.
R : R reward function, R(s, a) reward obtained
executing action state s.
Z set possible observations.
: Z [0, 1] observation function, O(s0 , a, z) = Pr(z|a, s0 ) gives
probability observing z action performed resulting state s0 .
assume paper S, Z finite R bounded.
key aspect POMDP model assumption states directly
observable. Instead, given time, agent access observation
z Z gives incomplete information current state. Since states
observable, agent cannot choose actions based states. consider
complete history past actions observations choose current action.
history time defined as:
ht = {a0 , z1 , . . . , zt1 , at1 , zt }.

(1)

explicit representation past typically memory expensive. Instead,
possible summarize relevant information previous actions observations
probability distribution state space S, called belief state (Astrom, 1965).
belief state time defined posterior probability distribution
state, given complete history:
bt (s) = Pr(st = s|ht , b0 ).

(2)

belief state bt sufficient statistic history ht (Smallwood & Sondik, 1973),
therefore agent choose actions based current belief state bt instead
past actions observations. Initially, agent starts initial belief state b0 ,
665

fiRoss, Pineau, Paquet, & Chaib-draa

representing knowledge starting state environment. Then, time
t, belief state bt computed previous belief state bt1 , using previous
action at1 current observation zt . done belief state update function
(b, a, z), bt = (bt1 , at1 , zt ) defined following equation:
bt (s0 ) = (bt1 , at1 , zt )(s0 ) =

1
Pr(zt |bt1 , at1 )

O(s0 , at1 , zt )

X

(s, at1 , s0 )bt1 (s), (3)

sS

Pr(z|b, a), probability observing z action belief b, acts
normalizing constant bt remains probability distribution:
Pr(z|b, a) =

X

O(s0 , a, z)

s0

X

(s, a, s0 )b(s).

(4)

sS

agent way computing belief, next interesting question
choose action based belief state.
action determined agents policy , specifying probability
agent execute action given belief state, i.e. defines agents strategy
possible situations could encounter. strategy maximize amount
reward earned finite infinite time horizon. article, restrict attention
infinite-horizon POMDPs optimality criterion maximize expected
sum discounted rewards (also called return discounted return). formally,
optimal policy defined following equation:
#
"
X
X X
(5)
bt (s)
R(s, a)(bt , a) |b0 ,
= argmax E



t=0

sS

aA

[0, 1) discount factor (bt , a) probability action
performed belief bt , prescribed policy .
return obtained following specific policy , certain belief state b,
defined value function equation V :
"
#
X
X


V (b) =
(b, a) RB (b, a) +
Pr(z|b, a)V ( (b, a, z)) .
(6)
aA

zZ

function RB (b, a) specifies immediate expected reward executing action
belief b according reward function R:
RB (b, a) =

X

b(s)R(s, a).

(7)

sS

sum Z Equation 6 interpreted expected future return infinite
horizon executing action a, assuming policy followed afterwards.
Note definitions RB (b, a), Pr(z|b, a) (b, a, z), one view
POMDP MDP belief states (called belief MDP), Pr(z|b, a) specifies
probability moving b (b, a, z) action a, RB (b, a) immediate
reward obtained action b.
666

fiOnline Planning Algorithms POMDPs

optimal policy defined Equation 5 represents action-selection strategy
maximize equation V (b0 ). Since always exists deterministic policy
maximizes V belief states (Sondik, 1978), generally consider deterministic policies (i.e. assign probability 1 specific action every belief
state).
value function V optimal policy fixed point Bellmans equation
(Bellman, 1957):
"
#
X
V (b) = max RB (b, a) +
Pr(z|b, a)V ( (b, a, z)) .
(8)
aA

zZ

Another useful quantity value executing given action belief state b,
denoted Q-value:
Q (b, a) = RB (b, a) +

X

Pr(z|b, a)V ( (b, a, z)).

(9)

zZ

difference definition V max operator omitted. Notice
Q (b, a) determines value assuming optimal policy followed
every step action a.
review different offline methods solving POMDPs. used guide
online heuristic search methods discussed later, cases form
basis online solutions.
2.1 Optimal Value Function Algorithm
One solve optimally POMDP specified finite horizon H using value
iteration algorithm (Sondik, 1971). algorithm uses dynamic programming compute
increasingly accurate values belief state b. value iteration algorithm
begins evaluating value belief state immediate horizon = 1. Formally,
let V value function takes belief state parameter returns numerical
value R belief state. initial value function is:
V1 (b) = max RB (b, a).
aA

(10)

value function horizon constructed value function horizon 1
using following recursive equation:
"
#
X
Vt (b) = max RB (b, a) +
Pr(z|b, a)Vt1 ( (b, a, z)) .
(11)
aA

zZ

value function Equation 11 defines discounted sum expected rewards
agent receive next time steps, belief state b. Therefore, optimal
policy finite horizon simply choose action maximizing Vt (b):
"
#
X
Pr(z|b, a)Vt1 ( (b, a, z)) .
(12)
(b) = argmax RB (b, a) +
aA

zZ

667

fiRoss, Pineau, Paquet, & Chaib-draa

last equation associates action specific belief state, therefore must
computed possible belief states order define full policy.
key result Smallwood Sondik (1973) shows optimal value function
finite-horizon POMDP represented hyperplanes, therefore convex
piecewise linear. means value function Vt horizon represented
set |S|-dimensional hyperplanes: = {0 , 1 , . . . , }. hyperplanes often
called -vectors. defines linear value function belief state space associated
action A. value belief state maximum value returned one
-vectors belief state. best action one associated -vector
returned best value:
X
(s)b(s).
(13)
Vt (b) = max


sS

number exact value function algorithms leveraging piecewise-linear convex
aspects value function proposed POMDP literature (Sondik, 1971;
Monahan, 1982; Littman, 1996; Cassandra, Littman, & Zhang, 1997; Zhang & Zhang,
2001). problem exact approaches number -vectors
needed represent value function grows exponentially number observations
iteration, i.e. size set O(|A||t1 ||Z| ). Since new -vector
requires computation time O(|Z||S|2 ), resulting complexity iteration exact
approaches O(|A||Z||S|2 |t1 ||Z| ). work exact approaches focused
finding efficient ways prune set , effectively reduce computation.
2.2 Offline Approximate Algorithms
Due high complexity exact solving approaches, many researchers worked
improving applicability POMDP approaches developing approximate offline
approaches applied larger problems.
online methods review below, approximate offline algorithms often used
compute lower upper bounds optimal value function. bounds
leveraged orient search promising directions, apply branch-and-bound pruning
techniques, estimate long term reward belief states, show Section
3. However, generally want use approximate methods require low
computational cost. particularly interested approximations use
underlying MDP1 compute lower bounds (Blind policy) upper bounds (MDP, QMDP,
FIB) exact value function. investigate usefulness using precise
lower bounds provided point-based methods. briefly review offline methods
featured empirical investigation. recent publications provide
comprehensive overview offline approximate algorithms (Hauskrecht, 2000; Pineau,
Gordon, & Thrun, 2006).
2.2.1 Blind policy
Blind policy (Hauskrecht, 2000; Smith & Simmons, 2005) policy
action always executed, regardless belief state. value function Blind
1. MDP defined (S, A, T, R) components POMDP model.

668

fiOnline Planning Algorithms POMDPs

policy obviously lower bound V since corresponds value one specific
policy agent could execute environment. resulting value function
specified set |A| -vectors, -vector specifies long term expected
reward following corresponding blind policy. -vectors computed using
simple update rule:
at+1 (s) = R(s, a) +

X

(s, a, s0 )at (s),

(14)

s0

a0 = minsS R(s, a)/(1). -vectors computed, use Equation 13
obtain lower bound value belief state. complexity iteration
O(|A||S|2 ), far less exact methods. lower bound
computed quickly, usually tight thus informative.
2.2.2 Point-Based Algorithms
obtain tighter lower bounds, one use point-based methods (Lovejoy, 1991; Hauskrecht,
2000; Pineau et al., 2003). popular approach approximates value function updating selected belief states. point-based methods sample belief
states simulating random interactions agent POMDP environment,
update value function gradient sampled beliefs. approaches circumvent complexity exact approaches sampling small set beliefs
maintaining one -vector per sampled belief state. Let B represent set
sampled beliefs, set -vectors time obtained follows:
(s)
a,z

bt


=
=
=
=

R(s, a),
P
a,z
0 , a, z)0 (s0 ), 0
{a,z
s0 (s, a, s0 )O(s
t1 },


|i (s) = P
P



a,z
{b |b = + zZ argmaxt
sS (s)b(s), A},
P
{b |b = argmaxb sS b(s)(s), b B}.

(15)



ensure gives lower bound, 0 initialized single -vector 0 (s) =

mins0 S,aA R(s0 ,a)
.
1

Since |t1 | |B|, iteration complexity O(|A||Z||S||B|(|S|+
|B|)), polynomial time, compared exponential time exact approaches.
Different algorithms developed using point-based approach: PBVI (Pineau
et al., 2003), Perseus (Spaan & Vlassis, 2005), HSVI (Smith & Simmons, 2004, 2005)
recent methods. methods differ slightly choose
belief states update value function chosen belief states.
nice property approaches one tradeoff complexity
algorithm precision lower bound increasing (or decreasing) number
sampled belief points.
2.2.3 MDP
MDP approximation consists approximating value function V POMDP
value function underlying MDP (Littman et al., 1995). value function
upper bound value function POMDP computed using Bellmans
equation:
669

fiRoss, Pineau, Paquet, & Chaib-draa

"

DP
Vt+1
(s) = max R(s, a) +
aA

X

#

(s, a, s0 )VtM DP (s0 ) .

s0

(16)

P
value V (b) belief state b computed V (b) = sS V DP (s)b(s).
computed quickly, iteration Equation 16 done O(|A||S|2 ).
2.2.4 QMDP
QMDP approximation slight variation MDP approximation (Littman et al.,
1995). main idea behind QMDP consider partial observability disappear
single step. assumes MDP solution computed generate VtM DP (Equation
16). Given this, define:
DP
QM
t+1 (s, a) = R(s, a) +

X

(s, a, s0 )VtM DP (s0 ).

(17)

s0

approximation defines -vector action, gives upper bound V
tighter V DP ( i.e. VtQM DP (b) VtM DP (b) belief b). Again,
obtain value belief state, use Equation 13, contain one -vector
DP (s, a) A.
(s) = QM

2.2.5 FIB
two upper bounds presented far, QMDP MDP, take account
partial observability environment. particular, information-gathering actions
may help identify current state always suboptimal according bounds.
address problem, Hauskrecht (2000) proposed new method compute upper bounds,
called Fast Informed Bound (FIB), able take account (to degree)
partial observability environment. -vector update process described
follows:
at+1 (s) = R(s, a) +

X

zZ

a0

max



X

O(s0 , a, z)T (s, a, s0 )t (s0 ).

(18)

s0

initialized -vectors found QMDP convergence, i.e.
-vectors
a0 (s) = QM DP (s, a). FIB defines single -vector action value belief
state computed according Equation 13. FIB provides tighter upper bound
QMDP ( i.e. VtF IB (b) VtQM DP (b) b ). complexity algorithm remains
acceptable, iteration requires O(|A|2 |S|2 |Z|) operations.

3. Online Algorithms POMDPs
offline approaches, algorithm returns policy defining action execute
every possible belief state. approaches tend applicable dealing
small mid-size domains, since policy construction step takes significant time. large
POMDPs, using rough value function approximation (such ones presented
Section 2.2) tends substantially hinder performance resulting approximate
670

fiOnline Planning Algorithms POMDPs

Offline Approaches
Policy Construction

Policy Execution

Online Approaches

Small policy construction step policy execution steps

Figure 1: Comparison offline online approaches.
policy. Even recent point-based methods produce solutions limited quality
large domains (Paquet et al., 2006).
Hence large POMDPs, potentially better alternative use online approach,
tries find good local policy current belief state agent.
advantage approach needs consider belief states reachable current belief state. focuses computation small set beliefs.
addition, since online planning done every step (and thus generalization beliefs
required), sufficient calculate maximal value current belief
state, full optimal -vector. setting, policy construction steps
execution steps interleaved one another shown Figure 1. cases, online
approaches may require extra execution steps (and online planning), since policy
locally constructed therefore always optimal. However policy construction time
often substantially shorter. Consequently, overall time policy construction
execution normally less online approaches (Koenig, 2001). practice, potential
limitation online planning need meet short real-time constraints.
case, time available construct plan small compared offline algorithms.
3.1 General Framework Online Planning
subsection presents general framework online planning algorithms POMDPs.
Subsequently, discuss specific approaches literature describe vary
tackling various aspects general framework.
online algorithm divided planning phase, execution phase,
applied alternately time step.
planning phase, algorithm given current belief state agent
computes best action execute belief. usually achieved two steps.
First tree reachable belief states current belief state built looking
several possible sequences actions observations taken current
belief. tree, current belief root node subsequent reachable beliefs (as
calculated (b, a, z) function Equation 3) added tree child nodes
immediate previous belief. Belief nodes represented using OR-nodes (at
must choose action) actions included layer belief nodes using
AND-nodes (at must consider possible observations lead subsequent
671

fiRoss, Pineau, Paquet, & Chaib-draa

b0

[14.4, 18.7]

1

3

[14.4, 17.9]

[12, 18.7]

a1

a2

0.7

z1
[13.7, 16.9]

b1

[15, 20]

b2

z1
[6, 14] b
5

b3

0.5

z2
[9, 15]

b4

[10, 18]

4

a1
0.6

z1

z2

-1

[5.8, 11.5]

0.5

0.3

a2
0.2

0.4

z2
b6

[9, 12]

z1

[13.7, 16.9]
0.8

[11, 20]

z2
b8

b7

[10, 12]

Figure 2: AND-OR tree constructed search process POMDP 2 actions 2 observations. belief states OR-nodes represented triangular nodes action AND-nodes
circular nodes. rewards RB (b, a) represented values outgoing arcs
OR-nodes probabilities Pr(z|b, a) shown outgoing arcs AND-nodes.
values inside brackets represent lower upper bounds computed according
Equations 19 - 22, assuming discount factor = 0.95. notice example
action a1 belief state b1 could pruned since upper bound (= 11.5) lower
lower bound (= 13.7) action a2 b1 .

beliefs). value current belief estimated propagating value estimates
fringe nodes, ancestors, way root, according Bellmans
equation (Equation 8). long-term value belief nodes fringe usually estimated
using approximate value function computed offline. methods maintain
lower bound upper bound value node. example
tree contructed evaluated presented Figure 2.
planning phase terminates, execution phase proceeds executing best
action found current belief environment, updating current belief
tree according observation obtained.
Notice general, belief MDP could graph structure cycles.
online algorithms handle structure unrolling graph tree. Hence,
reach belief already elsewhere tree, duplicated. algorithms could
always modified handle generic graph structures using technique proposed
LAO* algorithm (Hansen & Zilberstein, 2001) handle cycles. However
advantages disadvantages this. in-depth discussion issue
presented Section 5.4.
generic online algorithm implementing planning phase (lines 5-9) execution
phase (lines 10-13) presented Algorithm 3.1. algorithm first initializes tree
contain initial belief state (line 2). given current tree, planning phase
algorithm proceeds first selecting next fringe node (line 6)
pursue search (construction tree). Expand function (line 7) constructs
672

fiOnline Planning Algorithms POMDPs

1: Function OnlinePOMDPSolver()
Static: bc : current belief state agent.
: AND-OR tree representing current search tree.
D: Expansion depth.
L: lower bound V .
U : upper bound V .

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

bc b0
Initialize contain bc root
ExecutionTerminated()
PlanningTerminated()
b ChooseNextNodeToExpand()
Expand(b , D)
UpdateAncestors(b )
end
Execute best action bc
Perceive new observation z
bc (bc , a, z)
Update tree bc new root
end

Algorithm 3.1: Generic Online Algorithm.
next reachable beliefs (using Equation 3) selected leaf pre-determined
expansion depth evaluates approximate value function newly created
nodes. new approximate value expanded node propagated ancestors
via UpdateAncestors function (line 8). planning phase conducted
terminating condition met (e.g. planning time available -optimal action
found).
execution phase algorithm executes best action found planning
(line 10) gets new observation environment (line 11). Next, algorithm
updates current belief state search tree according recent action
observation z (lines 12-13). online approaches reuse previous computations
keeping subtree new belief resuming search subtree
next time step. cases, algorithm keeps nodes tree
new belief bc deletes nodes tree. algorithm loops back
planning phase next time step, task terminated.
side note, online planning algorithm useful improve precision
approximate value function computed offline. captured Theorem 3.1.
Theorem 3.1. (Puterman, 1994; Hauskrecht, 2000) Let V approximate value function = supb |V (b) V (b)|. approximate value V (b) returned Dstep lookahead belief b, using V estimate fringe node values, error bounded
|V (b) V (b)| .
notice [0, 1), error converges 0 depth search tends
. indicates online algorithm effectively improve performance obtained
approximate value function computed offline, find action arbitrarily close
optimal current belief. However, evaluating tree reachable beliefs
within depth complexity O((|A||Z|)D |S|2 ), exponential D.
becomes quickly intractable large D. Furthermore, planning time available
execution may short exploring beliefs depth may infeasible.
673

fiRoss, Pineau, Paquet, & Chaib-draa

Hence motivates need efficient online algorithms guarantee similar
better error bounds.
efficient, online algorithms focus limiting number reachable beliefs explored tree (or choose relevant ones). approaches
generally differ subroutines ChooseNextNodeToExpand Expand
implemented. classify approaches three categories : Branch-and-Bound
Pruning, Monte Carlo Sampling Heuristic Search. present survey
approaches discuss strengths drawbacks. online algorithms
proceed via tree search; approaches discussed Section 3.5.
3.2 Branch-and-Bound Pruning
Branch-and-Bound pruning general search technique used prune nodes
known suboptimal search tree, thus preventing expansion unnecessary
lower nodes. achieve AND-OR tree, lower bound upper bound
maintained value Q (b, a) action a, every belief b tree.
bounds computed first evaluating lower upper bound fringe nodes
tree. bounds propagated parent nodes according following
equations:

L(b),
b F(T )
LT (b) =
(19)
maxaA LT (b, a), otherwise
X
LT (b, a) = RB (b, a) +
Pr(z|b, a)LT ( (b, a, z)),
(20)
zZ



U (b),
b F(T )
maxaA UT (b, a), otherwise
X
UT (b, a) = RB (b, a) +
Pr(z|b, a)UT ( (b, a, z)),
UT (b) =

(21)
(22)

zZ

F(T ) denotes set fringe nodes tree , UT (b) LT (b) represent upper
lower bounds V (b) associated belief state b tree , UT (b, a) LT (b, a)
represent corresponding bounds Q (b, a), L(b) U (b) bounds used fringe
nodes, typically computed offline. equations equivalent Bellmans equation
(Equation 8), however use lower upper bounds children, instead V .
Several techniques presented Section 2.2 used quickly compute lower bounds
(Blind policy) upper bounds (MDP, QMDP, FIB) offline.
Given bounds, idea behind Branch-and-Bound pruning relatively simple:
given action belief b upper bound UT (b, a) lower another action
lower bound LT (b, a), know guaranteed value Q (b, a) Q (b, a).
Thus suboptimal belief b. Hence branch pruned belief reached
taking action b considered.
3.2.1 RTBSS
Real-Time Belief Space Search (RTBSS) algorithm uses Branch-and-Bound approach
compute best action take current belief (Paquet et al., 2005, 2006). Starting
674

fiOnline Planning Algorithms POMDPs

1: Function Expand(b, d)
Inputs: b:
d:
Static: :
L:
U:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

belief node want expand.
depth expansion b.
AND-OR tree representing current search tree.
lower bound V .
upper bound V .

= 0
LT (b) L(b)
else
Sort actions {a1 , a2 , . . . , a|A| } U (b, ai ) U (b, aj ) j
i1
LT (b)
|A| U (b, ai ) >
PLT (b)
LT (b, ai ) RB (b, ai ) + zZ Pr(z|b, ai )Expand( (b, ai , z), 1)
LT (b) max{LT (b), LT (b, ai )}
ii+1
end
end
return LT (b)

Algorithm 3.2: Expand subroutine RTBSS.
current belief, expands AND-OR tree depth-first search fashion,
pre-determined search depth D. leaves tree evaluated using lower
bound computed offline, propagated upwards lower bound maintained
node tree.
limit number nodes explored, Branch-and-Bound pruning used along way
prune actions known suboptimal, thus excluding unnecessary nodes
actions. maximize pruning, RTBSS expands actions descending order
upper bound (first action expanded one highest upper bound). expanding
actions order, one never expands action could pruned actions
expanded different order. Intuitively, action higher upper bound
actions, cannot pruned actions since lower
bound never exceed upper bound. Another advantage expanding actions
descending order upper bound soon find action
pruned, know remaining actions pruned, since upper
bounds necessarily lower. fact RTBSS proceeds via depth-first search
increases number actions pruned since bounds expanded actions
become precise due search depth.
terms framework Algorithm 3.1, RTBSS requires ChooseNextNodeToExpand subroutine simply return current belief bc . UpdateAncestors function need perform operation since bc ancestor (root tree
). Expand subroutine proceeds via depth-first search fixed depth D, using
Branch-and-Bound pruning, mentioned above. subroutine detailed Algorithm
3.2. expansion performed, PlanningTerminated evaluates true
best action found executed. end time step, tree simply reinitialized
contain new current belief root node.
efficiency RTBSS depends largely precision lower upper bounds
computed offline. bounds tight, pruning possible, search
efficient. algorithm unable prune many actions, searching
675

fiRoss, Pineau, Paquet, & Chaib-draa

limited short horizons order meet real-time constraints. Another drawback
RTBSS explores observations equally. inefficient since algorithm
could explore parts tree small probability occurring thus
small effect value function. result, number observations large,
algorithm limited exploring short horizon.
final note, since RTBSS explores reacheable beliefs within depth (except
reached suboptimal actions), guarantee error bound D-step
lookahead (see Theorem 3.1). Therefore, online search directly improves precision
original (offline) value bounds factor . aspect confirmed empirically
different domains RTBSS authors combined online search bounds given
various offline algorithms. cases, results showed tremendous improvement
policy given offline algorithm (Paquet et al., 2006).
3.3 Monte Carlo Sampling
mentioned above, expanding search tree fully large set observations
infeasible except shallow depths. cases, better alternative may sample
subset observations expansion consider beliefs reached sampled
observations. reduces branching factor search allows deeper search
within set planning time. strategy employed Monte Carlo algorithms.
3.3.1 McAllester Singh
approach presented McAllester Singh (1999) adaptation online MDP
algorithm presented Kearns, Mansour, Ng (1999). consists depth-limited
search AND-OR tree certain fixed horizon instead exploring
observations action choice, C observations sampled generative model.
probabilities Pr(z|b, a) approximated using observed frequencies sample.
advantage approach sampling observation distribution
Pr(z|b, a) achieved efficiently O(log |S| + log |Z|), computing exact
probabilities Pr(z|b, a) O(|S|2 ) observation z. Thus sampling useful
alleviate complexity computing Pr(z|b, a), expense less precise estimate.
Nevertheless, samples often sufficient obtain good estimate observations
effect Q (b, a) (i.e. occur high probability)
likely sampled. authors apply belief state factorization Boyen
Koller (1998) simplify belief state calculations.
implementation algorithm, Expand subroutine expands tree
fixed depth D, using Monte Carlo sampling observations, mentioned (see
Algorithm 3.3). end time step, tree reinitialized contain
new current belief root.
Kearns et al. (1999) derive bounds depth number samples C needed
obtain -optimal policy high probability show number samples
required grows exponentially desired accuracy. practice, number samples
required infeasible given realistic online time constraints. However, performance terms
returns usually good even many fewer samples.
676

fiOnline Planning Algorithms POMDPs

1: Function Expand(b, d)
Inputs: b:
d:
Static: :
C:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

belief node want expand.
depth expansion b.
AND-OR tree representing current search tree.
number observations sample.

= 0
LT (b) maxaA RB (b, a)
else
LT (b)

Sample Z = {z1 , z2 , . . . zC } distribution Pr(z|b, a)
P
N (Z)
LT (b, a) RB (b, a) + zZ|Nz (Z)>0 zC Expand( (b, a, z), 1)
LT (b) max{LT (b), LT (b, a)}
end
end
return LT (b)

Algorithm 3.3: Expand subroutine McAllester Singhs Algorithm.

One inconvenience method action pruning done since Monte
Carlo estimation guaranteed correctly propagate lower (and upper) bound
property tree. article, authors simply approximate value
fringe belief states immediate reward RB (b, a); could improved using
good estimate V computed offline. Note approach may difficult
apply domains number actions |A| large. course may
impact performance.
3.3.2 Rollout
Another similar online Monte Carlo approach Rollout algorithm (Bertsekas & Castanon, 1999). algorithm requires initial policy (possibly computed offline).
time step, estimates future expected value action, assuming initial policy followed future time steps, executes action highest estimated value.
estimates obtained computing average discounted return obtained
set sampled trajectories depth D. trajectories generated first taking
action evaluated, following initial policy subsequent belief states,
assuming observations sampled generative model. Since approach
needs consider different actions root belief node, number actions |A|
influences branching factor first level tree. Consequently, generally
scalable McAllester Singhs approach. Bertsekas Castanon (1999)
show enough sampling, resulting policy guaranteed perform least
well initial policy high probability. However, generally requires many sampled
trajectories provide substantial improvement initial policy. Furthermore,
initial policy significant impact performance approach. particular,
cases might impossible improve return initial policy changing
immediate action (e.g. several steps need changed reach specific subgoal
higher rewards associated). cases, Rollout policy never improve
initial policy.
677

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function Expand(b, d)
Inputs: b: belief node want expand.
d: depth expansion b.
Static: : AND-OR tree representing current search tree.
: set initial policies.
: number trajectories depth sample.

2: LT (b)
3:
4:
5:
Q (b, a) 0
6:
= 1
7:
b b
8:

9:
j = 0
1 j
10:
Q (b, a) Q (b, a) +
RB (b, a)
11:
z SampleObservation(b, a)
12:
b (b, a, z)
13:
(b)
14:
end
15:
end
16: end
17: LT (b, a) = max Q (b, a)
18: end

Algorithm 3.4: Expand subroutine Parallel Rollout Algorithm.
address issue relative initial policy, Chang, Givan, Chong (2004)
introduced modified version algorithm, called Parallel Rollout. case,
algorithm starts set initial policies. algorithm proceeds Rollout
initial policies set. value considered immediate action
maximum set initial policies, action highest value executed.
algorithm, policy obtained guaranteed perform least well best
initial policy high probability, given enough samples. Parallel Rollout handle
domains large number actions observations, perform well
set initial policies contain policies good different regions belief space.
Expand subroutine Parallel Rollout algorithm presented Algorithm 3.4.
original Rollout algorithm Bertsekas Castanon (1999) algorithm
special case set initial policies contains one policy.
subroutines proceed McAllester Singhs algorithm.
3.4 Heuristic Search
Instead using Branch-and-Bound pruning Monte Carlo sampling reduce branching factor search, heuristic search algorithms try focus search relevant reachable beliefs using heuristics select best fringe beliefs node expand.
relevant reachable beliefs ones would allow search algorithm
make good decisions quickly possible, i.e. expanding nodes possible.
three different online heuristic search algorithms POMDPs
proposed past: Satia Lave (1973), BI-POMDP (Washington, 1997) AEMS
(Ross & Chaib-draa, 2007). algorithms maintain lower upper bounds
value node tree (using Equations 19 - 22) differ
specific heuristic used choose next fringe node expand AND/OR tree.
678

fiOnline Planning Algorithms POMDPs

first present common subroutines algorithms, discuss different
heuristics.
Recalling general framework Algorithm 3.1, three steps interleaved several
times heuristic search algorithms. First, best fringe node expand (according
heuristic) current search tree found. tree expanded
node (usually one level). Finally, ancestor nodes values updated;
values must updated choose next node expand, since heuristic
value usually depends them. general, heuristic search algorithms slightly
computationally expensive standard depth- breadth-first search algorithms, due
extra computations needed select best fringe node expand, need
update ancestors iteration. required previous methods using
Branch-and-Bound pruning and/or Monte Carlo sampling. complexity extra
steps high, benefit expanding relevant nodes might
outweighed lower number nodes expanded (assuming fixed planning time).
heuristic search algorithms, particular heuristic value associated every fringe
node tree. value indicate important expand node
order improve current solution. iteration algorithm, goal find
fringe node maximizes heuristic value among fringe nodes.
achieved efficiently storing node tree reference best fringe node
expand within subtree, well associated heuristic value. particular,
root node always contains reference best fringe node whole tree.
node expanded, ancestors nodes tree best fringe node
reference, corresponding heuristic value, need updated. updated
efficiently using references heuristic values stored lower nodes via
dynamic programming algorithm, described formally Equations 23 24. HT (b)
denotes highest heuristic value among fringe nodes subtree b, bT (b)
reference fringe node, HT (b) basic heuristic value associated fringe node b,
HT (b, a) HT (b, a, z) factors weigh basic heuristic value level
tree . example, HT (b, a, z) could Pr(z|b, a) order give higher weight
(and hence favor) fringe nodes reached likely observations.

HT (b)
b F(T )

HT (b) =

maxaA HT (b, a)HT (b, a) otherwise
(23)
HT (b, a) = maxzZ HT (b, a, z)HT ( (b, a, z))

b
b F(T )

bT (b) =
bT (b, aTb ) otherwise
))
bT (b, a) = bT ( (b, a, zb,a
(24)

ab = argmaxaA HT (b, a)HT (b, a)

zb,a
= argmaxzZ HT (b, a, z)HT ( (b, a, z))
procedure finds fringe node b F(T ) maximizes overall heuristic value
QdT (b)
HT (bi , ai )HT (bi , ai , zi ), bi , ai zi represent ith belief,
HT (bc , b) = HT (b) i=1
action observation path bc b , dT (b) depth fringe
node b. Note HT bT updated ancestor nodes last expanded
node. reusing previously computed values nodes, procedure
679

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function Expand(b)
Inputs: b:
Static: bc :
T:
L:
U:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

OR-Node want expand.
current belief state agent.
AND-OR tree representing current search tree.
lower bound V .
upper bound V .


z Z
b0 (b, a, z)
UT (b0 ) U (b0 )
LT (b0 ) L(b0 )
HT (b0 ) HT (b0 )
bT (b0 ) b0
end
P
LT (b, a) RB (b, a) + P zZ Pr(z|b, a)LT ( (b, a, z))
UT (b, a) RB (b, a) + zZ Pr(z|b, a)UT ( (b, a, z))

argmax
zb,a
zZ HT (b, a, z)HT ( (b, a, z))

))


HT (b, a) = HT (b, a, zb,a )HT ( (b, a, zb,a



bT (b, a) bT ( (b, a, zb,a ))
end
LT (b) max{maxaA LT (b, a), LT (b)}
UT (b) min{maxaA UT (b, a), UT (b)}


b argmaxaA HT (b, a)HT (b, a)
(b, )
)H
HT (b) HT (b,
b

b
bT (b) bT (b,
b )

Algorithm 3.5: Expand : Expand subroutine heuristic search algorithms.
find best fringe node expand tree time linear depth
tree (versus exponential depth tree exhaustive search fringe
nodes). updates performed Expand UpdateAncestors
subroutines, described detail below. iteration,
ChooseNextNodeToExpand subroutine simply returns reference best fringe
node stored root tree, i.e. bT (bc ).
Expand subroutine used heuristic search methods presented Algorithm 3.5.
performs one-step lookahead fringe node b. main difference respect
previous methods Sections 3.2 3.3 heuristic value best fringe node
expand new nodes computed lines 7-8 12-14. best leaf node
bs subtree heuristic value computed according Equations 23 24
(lines 18-20).
UpdateAncestors function presented Algorithm 3.6. goal function update bounds ancestor nodes, find best fringe node expand
next. Starting given OR-Node b0 , function simply updates recursively ancestor nodes b0 bottom-up fashion, using Equations 19-22 update bounds
Equations 23-24 update reference best fringe expand heuristic value.
Notice UpdateAncestors function reuse information already stored
node objects, need recompute (b, a, z), Pr(z|b, a) RB (b, a).
However may need recompute HT (b, a, z) HT (b, a) according new bounds,
depending heuristic defined.
Due anytime nature heuristic search algorithms, search usually keeps
going -optimal action found current belief bc , available planning
680

fiOnline Planning Algorithms POMDPs

1: Function UpdateAncestors(b0 )

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

Inputs: b0 : OR-Node want update ancestors.
Static: bc : current belief state agent.
: AND-OR tree representing current search tree.
L: lower bound V .
U : upper bound V .
b0 6= bc
Set (b, a) action aPin belief b parent node belief node b0
LT (b, a) RB (b, a) + P zZ Pr(z|b, a)LT ( (b, a, z))
UT (b, a) RB (b, a) + zZ Pr(z|b, a)UT ( (b, a, z))

argmax
zb,a
zZ HT (b, a, z)HT ( (b, a, z))
)H ( (b, a, z ))
HT (b, a) HT (b, a, zb,a

b,a
))
bT (b, a) bT ( (b, a, zb,a
LT (b) maxa0 LT (b, a0 )
UT (b) maxa0 UT (b, a0 )
0
0


b argmaxa0 HT (b, )HT (b, )
(b, )
)H
HT (b) HT (b,

b
b
bT (b) bT (b,
b )
b0 b
end

Algorithm 3.6: UpdateAncestors : Updates bounds ancestors ancestors
OR-Node
time elapsed. -optimal action found whenever UT (bc ) LT (bc ) LT (bc )
UT (bc , a0 ), a0 6= argmaxaA LT (bc , a) (i.e. actions pruned, case
optimal action found).
covered basic subroutines, present different heuristics
proposed Satia Lave (1973), Washington (1997) Ross Chaib-draa (2007).
begin introducing useful notation.
Given graph structure G, let us denote F(G) set fringe nodes G
HG (b, b0 ) set sequences actions observations lead belief node b0
belief node b search graph G. tree , HT (b, b0 ) contain
0
0
single sequence denote hb,b
. given sequence h HG (b, b ), define
Pr(hz |b, ha ) probability observe whole sequence observations hz h, given
start belief node b perform whole sequence actions ha h. Finally,
define Pr(h|b, ) probability follow entire action/observation sequence
h start belief b behave according policy . Formally, probabilities
computed follows:
d(h)

Pr(hz |b, ha ) =



Pr(hiz |bhi1 , hia ),

(25)

i=1

d(h)

Pr(h|b, ) =



Pr(hiz |bhi1 , hia )(bhi1 , hia ),

(26)

i=1

d(h) represents depth h (number actions sequence h), hia denotes
ith action sequence h, hiz ith observation sequence h, bhi belief state
obtained taking first actions observations sequence h b. Note
bh0 = b.
681

fiRoss, Pineau, Paquet, & Chaib-draa

3.4.1 Satia Lave
approach Satia Lave (1973) follows heuristic search framework presented
above. main feature approach explore, iteration, fringe node b
current search tree maximizes following term:
bc ,b

HT (bc , b) = d(hT

)

c ,b
c ,b
Pr(hbT,z
|bc , hbT,a
)(UT (b) LT (b)),

(27)

b F(T ) bc root node . intuition behind heuristic simple:
recalling definition V , note weight value V (b) fringe node b
bc ,b
c ,b
c ,b
c ,b
sequence optimal
), provided hbT,a
|bc , hbT,a
V (bc ) would exactly d(hT ) Pr(hbT,z
actions. fringe nodes weight high effect estimate
V (bc ). Hence one try minimize error nodes first. term
UT (b) LT (b) included since upper bound (unknown) error V (b) LT (b).
Thus heuristic focuses search areas tree affect value V (bc )
error possibly large. approach uses Branch-and-Bound pruning,
fringe node reached action dominated parent belief b
never going expanded. Using notation Algorithms 3.5 3.6,
heuristic implemented defining HT (b), HT (b, a) HT (b, a, z), follows:
HT (b) =
UT (b) LT (b),
1 UT (b, a) > LT (b),
HT (b, a) =
0 otherwise,
HT (b, a, z) = Pr(z|b, a),

(28)

condition UT (b, a) > LT (b) ensures global heuristic value HT (bc , b0 ) = 0
bc ,b0
dominated (pruned). guarantees fringe
action sequence hT,a
nodes never expanded.
Satia Laves heuristic focuses search towards beliefs likely
reached future, error large. heuristic likely efficient
domains large number observations, probability distribution
observations concentrated observations. term UT (b) LT (b)
heuristic prevents search unnecessary computations areas tree
already good estimate value function. term efficient
bounds computed offline, U L, sufficiently informative. Similarly, node pruning
going efficient U L sufficiently tight, otherwise actions
pruned.
3.4.2 BI-POMDP
Washington (1997) proposed slightly different approach inspired AO algorithm
(Nilsson, 1980), search conducted best solution graph. case
online POMDPs, corresponds subtree belief nodes reached
sequences actions maximizing upper bound parent beliefs.
b
set fringe nodes best solution graph G, denote F(G),

b
defined formally F(G) = {b F(G)|h HG (bc , b), Pr(h|b, G ) > 0}, G (b, a) = 1
= argmaxa0 UG (b, a0 ) G (b, a) = 0 otherwise. AO algorithm simply specifies
682

fiOnline Planning Algorithms POMDPs

expanding fringe nodes. Washington (1997) recommends exploring fringe
node Fb(G) (where G current acyclic search graph) maximizes UG (b) LG (b).
Washingtons heuristic implemented defining HT (b), HT (b, a) HT (b, a, z),
follows:
HT (b) =
UT (b) LT (b),
1 = argmaxa0 UT (b, a0 ),
HT (b, a) =
0 otherwise,
HT (b, a, z) = 1.

(29)

heuristic tries guide search towards nodes reachable promising
actions, especially loose bounds values (possibly large error). One
nice property approach expanding fringe nodes best solution graph
way reduce upper bound root node bc . case
Satia Laves heuristic. However, Washingtons heuristic take account
probability Pr(hz |b, ha ), discount factor d(h) , may end exploring
nodes small probability reached future, thus
little effect value V (bc ). Hence, may explore relevant nodes
optimizing decision bc . heuristic appropriate upper bound U
computed offline sufficiently informative, actions highest upper bound
would usually tend highest Q-value. cases, algorithm focus
search actions thus find optimal action quickly
explored actions equally. hand, consider observation
probabilities, approach may scale well large observation sets,
able focus search towards relevant observations.
3.4.3 AEMS
Ross Chaib-draa (2007) introduced heuristic combines advantages BIPOMDP, Satia Laves heuristic. based theoretical error analysis tree
search POMDPs, presented Ross et al. (2008).
core idea expand tree reduce error V (bc ) quickly
possible. achieved expanding fringe node b contributes
error V (bc ). exact error contribution eT (bc , b) fringe node b bc tree
defined following equation:
bc ,b

eT (bc , b) = d(hT

)

Pr(hbTc ,b |bc , )(V (b) LT (b)).

(30)

expression requires V computed exactly. practice, Ross Chaibdraa (2007) suggest approximating exact error (V (b) LT (b)) (UT (b) LT (b)),
done Satia Lave, Washington. suggest approximating
policy , (b, a) represents probability action optimal
parent belief b, given lower upper bounds tree . particular, Ross et al. (2008)
considered two possible approximations . first one based uniformity
assumption distribution Q-values lower upper bounds,
yields:
683

fiRoss, Pineau, Paquet, & Chaib-draa

(b, a) =

(

2

(b,a)LT (b))
(U
UT (b,a)LT (b,a)
0

UT (b, a) > LT (b),
otherwise,

(31)

normalization constant sum probabilities (b, a)
actions equals 1.
second inspired AO BI-POMDP, assumes action maximizing
upper bound fact optimal action:

1 = argmaxa0 UT (b, a0 ),
(32)
(b, a) =
0 otherwise.
Given approximation , AEMS heuristic explore fringe node b
maximizes:
bc ,b
)

HT (bc , b) = d(hT

Pr(hbTc ,b |bc , )(UT (b) LT (b)).

(33)

implemented defining HT (b), HT (b, a) HT (b, a, z) follows:
HT (b) = UT (b) LT (b),
HT (b, a) = (b, a),
HT (b, a, z) = Pr(z|b, a).

(34)

refer heuristic AEMS1 defined Equation 31, AEMS2
defined Equation 32.2
Let us examine AEMS combines advantages Satia Lave,
BI-POMDP heuristics. First, AEMS encourages exploration nodes loose bounds
possibly large error considering term UT (b) LT (b) previous heuristics.
Moreover, Satia Lave, focuses exploration towards belief states likely
encountered future. good two reasons. mentioned before, belief
state low probability occurrence future, limited effect value
V (bc ) thus necessary know value precisely. Second, exploring highly
probable belief states increases chance able reuse computations
future. Hence, AEMS able deal efficiently large observation sets,
assuming distribution observations concentrated observations. Finally,
BI-POMDP, AEMS favors exploration fringe nodes reachable actions
seem likely optimal (according ). useful handle large action
sets, focuses search actions look promising. promising actions
optimal, quickly become apparent. work well best
actions highest probabilities . Furthermore, possible define
automatically prunes dominated actions ensuring (b, a) = 0 whenever
UT (b, a) < LT (b). cases, heuristic never choose expand fringe node
reached dominated action.
final note, Ross et al. (2008) determined sufficient conditions
search algorithm using heuristic guaranteed find -optimal action within finite
time. stated Theorem 3.2.
2. AEMS2 heuristic used policy search algorithm Hansen (1998).

684

fiOnline Planning Algorithms POMDPs

Theorem 3.2. (Ross et al., 2008) Let > 0 bc current belief. tree
parent belief b UT (b) LT (b) > , (b, a) > 0 = argmaxa0 UT (b, a0 ),
AEMS algorithm guaranteed find -optimal action bc within finite time.
observe theorem possible define many different policies
AEMS heuristic guaranteed converge. AEMS1 AEMS2
satisfy condition.
3.4.4 HSVI
heuristic similar AEMS2 used Smith Simmons (2004) offline
value iteration algorithm HSVI way pick next belief point perform
-vector backups. main difference HSVI proceeds via greedy search
descends tree root node b0 , going towards action maximizes
upper bound observation maximizes Pr(z|b, a)(U ( (b, a, z))L( (b, a, z)))
level, reaches belief b depth (U (b) L(b)) < .
heuristic could used online heuristic search algorithm instead stopping
greedy search process reaches fringe node tree selecting node
one expanded next. setting, HSVIs heuristic would return greedy
approximation AEMS2 heuristic, may find fringe node actually
bc ,b
maximizes d(hT ) Pr(hbTc ,b |bc , )(UT (b) LT (b)). consider online version
HSVI heuristic empirical study (Section 4). refer extension HSVI-BFS.
Note complexity greedy search finding best fringe node
via dynamic programming process updates HT bT UpdateAncestors
subroutine.
3.5 Alternatives Tree Search
present two alternative online approaches proceed via lookahead
search belief MDP. online approaches presented far, one problem
learning achieved time, i.e. everytime agent encounters belief,
recompute policy starting initial upper lower bounds computed offline.
two online approaches presented next address problem presenting alternative
ways updating initial value functions computed offline performance
agent improves time stores updated values computed time step.
However, argued discussion (Section 5.2), techniques lead
disadvantages terms memory consumption and/or time complexity.
3.5.1 RTDP-BEL
alternative approach searching AND-OR graphs RTDP algorithm (Barto
et al., 1995) adapted solve POMDPs Geffner Bonet (1998).
algorithm, called RTDP-BEL, learns approximate values belief states visited
successive trials environment. belief state visited, agent evaluates
possible actions estimating expected reward taking action current belief
685

fiRoss, Pineau, Paquet, & Chaib-draa

1: Function OnlinePOMDPSolver()
Static: bc : current belief state agent.
V0 : Initial approximate value function (computed offline).
V : hashtable beliefs approximate value.
k: Discretization resolution.

2: Initialize bc initial belief state V empty hashtable.
3: ExecutionTerminated()
P
4: A: Evaluate Q(bc , a) = RB (b, a) + zZ Pr(z|b, a)V (Discretize( (b, a, z), k))
5: argmaxaA Q(bc , a)
6: Execute best action bc
7: V (Discretize(bc , k)) Q(bc , a)
8: Perceive new observation z
9: bc (bc , a, z)
10: end

Algorithm 3.7: RTDP-Bel Algorithm.
state b approximate Q-value equation:
X
Q(b, a) = RB (b, a) +
Pr(z|b, a)V ( (b, a, z)),

(35)

zZ

V (b) value learned belief b.
belief state b value table, initialized heuristic value.
authors suggest using MDP approximation initial value belief state.
agent executes action returned greatest Q(b, a) value. Afterwards,
value V (b) table updated Q(b, a) value best action. Finally,
agent executes chosen action makes new observation, ending new
belief state. process repeated new belief.
RTDP-BEL algorithm learns heuristic value belief state visited.
maintain estimated value belief state memory, needs discretize
belief state space finite number belief states. allows generalization
value function unseen belief states. However, might difficult find best
discretization given problem. practice, algorithm needs substantial amounts
memory (greater 1GB cases) store learned belief state values,
especially POMDPs large state spaces. implementation RTDP-Bel
algorithm presented Algorithm 3.7.
function Discretize(b, k) returns discretized belief b0 b0 (s) = round(kb(s))/k
states S, V (b) looks value belief b hashtable. b present
hashtable, value V0 (b) returned V . Supported experimental data, Geffner
Bonet (1998) suggest choosing k [10, 100], usually produces best results.
Notice discretization resolution k O((k + 1)|S| ) possible discretized
beliefs. implies memory storage required maintain V exponential |S|,
becomes quickly intractable, even mid-size problems. Furthermore, learning good
estimates exponentially large number beliefs usually requires large number
trials, might infeasible practice. technique sometimes applied
large domains factorized representation available. cases, belief
maintained set distributions (one subset conditionaly independent state
variables) discretization applied seperately distribution. greatly
reduce possible number discretized beliefs.
686

fiOnline Planning Algorithms POMDPs

Algorithm
RTBSS
McAllester
Rollout
Satia Lave
Washington
AEMS
HSVI-BFS
RTDP-Bel
SOVI

-optimal
yes
high probability

yes
acyclic graph
yes
yes

yes

Anytime



yes
yes
yes
yes

yes

Branch &
Bound
yes


yes
implicit
implicit
implicit



Monte
Carlo

yes
yes







Heuristic



yes
yes
yes
yes



Learning







yes
yes

Table 1: Properties various online methods.

3.5.2 SOVI
recent online approach, called SOVI (Shani et al., 2005), extends HSVI (Smith &
Simmons, 2004, 2005) online value iteration algorithm. approach maintains
priority queue belief states encountered execution proceeds
-vector updates current belief state k belief states highest priority
time step. priority belief state computed according much value
function changed successor belief states, since last time updated. authors
propose improvements HSVI algorithm improve scalability,
efficient -vector pruning technique, avoiding use linear programs update
evaluate upper bound. main drawback approach hardly applicable
large environments short real-time constraints, since needs perform value
iteration update -vectors online, high complexity number
-vectors representing value function increases (i.e. O(k|S||A||Z|(|S| + |t1 |))
compute ).
3.6 Summary Online POMDP Algorithms
summary, see online POMDP approaches based lookahead search.
improve scalability, different techniques used: branch-and-bound pruning, search
heuristics, Monte Carlo sampling. techniques reduce complexity different angles. Branch-and-bound pruning lowers complexity related action space
size. Monte Carlo sampling used lower complexity related observation space size, could potentially used reduce complexity related
action space size (by sampling subset actions). Search heuristics lower complexity
related actions observations orienting search towards relevant actions observations. appropriate, factored POMDP representations used
reduce complexity related state. summary different properties
online algorithm presented Table 1.
687

fiRoss, Pineau, Paquet, & Chaib-draa

4. Empirical Study
section, compare several online approaches two domains found POMDP
literature: Tag (Pineau et al., 2003) RockSample (Smith & Simmons, 2004). consider modified version RockSample, called FieldVisionRockSample (Ross & Chaib-draa,
2007), higher observation space original RockSample. environment
introduced means test compare different algorithms environments
large observation spaces.
4.1 Methodology
environment, first compare real-time performance different heuristics
presented Section 3.4 limiting planning time 1 second per action. heuristics
given lower upper bounds results would comparable.
objective evaluate search heuristic efficient different types
environments. end, implemented different search heuristics (Satia
Lave, BI-POMDP, HSVI-BFS AEMS) best-first search algorithm,
directly measure efficiency heuristic itself. Results obtained
different lower bounds (Blind PBVI) verify choice affects heuristics
efficiency. Finally, compare online offline times affect performance
approach. Except stated otherwise, experiments run Intel Xeon 2.4
Ghz 4GB RAM; processes limited 1GB RAM.
4.1.1 Metrics compare online approaches
compare performance first foremost terms average discounted return execution time. However, really seek online approaches guarantee better
solution quality provided original bounds. words, seek
reduce error original bounds much possible. suggests good
metric efficiency online algorithms compare improvement terms
error bounds current belief online search. Hence, define
error bound reduction percentage be:
UT (b) LT (b)
,
(36)
U (b) L(b)
UT (b), LT (b), U (b) L(b) defined Section 3.2. best online algorithm
provide highest error bound reduction percentage, given initial bounds
real-time constraint.
EBR metric necessarily reflect true error reduction, compare return guarantees provided algorithm, i.e. lower bounds expected
return provided computed policies current belief. improvement
lower bound compared initial lower bound computed offline direct indicator
true error reduction, best online algorithm provide greatest lower bound
improvement current belief, given initial bounds real-time constraint.
Formally, define lower bound improvement be:
EBR(b) = 1

LBI(b) = LT (b) L(b).
688

(37)

fiOnline Planning Algorithms POMDPs

experiments, EBR LBI metrics evaluated time step
current belief. interested seeing approach provides highest EBR
LBI average.
consider metrics pertaining complexity efficiency. particular,
report average number belief nodes maintained search tree. Methods
lower complexity generally able maintain bigger trees, results
show always relate higher error bound reduction returns.
measure efficiency reusing part search tree recording percentage
belief nodes reused one time step next.
4.2 Tag
Tag initially introduced Pineau et al. (2003). environment
used recently work several authors (Poupart & Boutilier, 2003; Vlassis &
Spaan, 2004; Pineau, 2004; Spaan & Vlassis, 2004; Smith & Simmons, 2004; Braziunas &
Boutilier, 2004; Spaan & Vlassis, 2005; Smith & Simmons, 2005). environment,
approximate POMDP algorithm necessary large size (870 states, 5 actions
30 observations). Tag environment consists agent catch (Tag)
another agent moving 29-cell grid domain. reader referred work
Pineau et al. (2003) full description domain. Note results presented
below, belief state represented factored form. domain exact
factorization possible.
obtain results Tag, run algorithm starting configuration 5 times,
( i.e. 5 runs 841 different starting joint positions, excluding 29 terminal
states ). initial belief state runs consists uniform distribution
possible joint agent positions.
Table 2 compares different heuristics presenting 95% confidence intervals
average discounted return per run (Return), average error bound reduction percentage per
time step (EBR), average lower bound improvement per time step (LBI), average belief
nodes search tree per time step (Belief Nodes), average percentage belief nodes
reused per time step (Nodes Reused), average online planning time used per time step
(Online Time). cases, use FIB upper bound Blind lower bound. Note
average online time slightly lower 1 second per step algorithms
sometimes find -optimal solutions less second.
observe efficiency HSVI-BFS, BI-POMDP AEMS2 differs slightly
environment outperform three heuristics: RTBSS, Satia
Lave, AEMS1. difference explained fact latter three
methods restrict search best solution graph. consequence,
explore many irrelevant nodes, shown lower error bound reduction percentage,
lower bound improvement, nodes reused. poor reuse percentage explains
Satia Lave, AEMS1 limited lower number belief nodes search
tree, compared methods reached averages around 70K. results
three heuristics differ much three heuristics differ
way choose observations explore search. Since two observations
possible first action observation, one observations leads directly
689

fiRoss, Pineau, Paquet, & Chaib-draa

Heuristic
RTBSS(5)
Satia Lave
AEMS1
HSVI-BFS
BI-POMDP
AEMS2

Return
-10.31 0.22
-8.35 0.18
-6.73 0.15
-6.22 0.19
-6.22 0.15
-6.19 0.15

EBR (%)
22.3 0.4
22.9 0.2
49.0 0.3
75.7 0.4
76.2 0.5
76.3 0.5

LBI
3.03 0.07
2.47 0.04
3.92 0.03
7.69 0.06
7.81 0.06
7.81 0.06

Belief
Nodes
45066 701
36908 209
43693 314
64870 947
79508 1000
80250 1018

Nodes
Reused (%)
0
10.0 0.2
25.1 0.3
54.1 0.7
54.6 0.6
54.8 0.6

Online
Time (ms)
580 9
856 4
814 4
673 5
622 4
623 4

Table 2: Comparison different search heuristics Tag environment using Blind
policy lower bound.

EXIT



Figure 3: RockSample[7,8].
terminal belief state, possibility heuristics differed significantly
limited. Due limitation Tag domain, compare online algorithms
larger complex domain: RockSample.
4.3 RockSample
RockSample problem originally presented Smith Simmons (2004).
domain, agent explore environment sample rocks (see Figure 3),
similarly real robot would planet Mars. agent receives rewards
sampling rocks leaving environment (at extreme right environment).
rock scientific value not, agent sample good rocks.
define RockSample[n, k] instance RockSample problem n n
grid k rocks. state characterized k + 1 variables: XP , defines position
robot take values {(1, 1), (1, 2), . . . , (n, n)} k variables, X1R XkR ,
representing rock, take values {Good, Bad}.
agent perform k + 5 actions: {N orth, South, East, W est, Sample, Check1 , . . . ,
Checkk }. four motion actions deterministic. Sample action samples
rock agents current location. Checki action returns noisy observation
{Good, Bad} rock i.
belief state represented factored form known position set k
probabilities, namely probability rock good. Since observation rock
690

fiOnline Planning Algorithms POMDPs

Heuristic
Satia Lave
AEMS1
RTBSS(2)
BI-POMDP
HSVI-BFS
AEMS2
AEMS1
Satia Lave
RTBSS(2)
BI-POMDP
AEMS2
HSVI-BFS

Belief
Nodes
EBR (%)
LBI
Nodes
Reused (%)
Blind: Return:7.35, || = 1, Time:4s
7.35 0
3.64 0
00
509 0
8.92 0
10.30 0.08
9.50 0.11
0.90 0.03
579 2
5.31 0.03
10.30 0.15
9.65 0.02
1.00 0.04
439 0
00
18.43 0.14
33.3 0.5
4.33 0.06
2152 71
29.9 0.6
20.53 0.31
51.7 0.7
5.25 0.07
2582 72
36.5 0.5
20.75 0.15
52.4 0.6
5.30 0.06
3145 101
36.4 0.5
PBVI: Return:5.93, |B| = 64, || = 54, Time:2418s
17.10 0.28
26.1 0.4
1.39 0.03
1461 28
12.2 0.1
19.09 0.21
16.9 0.1
1.17 0.01
2311 25
13.5 0.1
19.45 0.30
22.4 0.3
1.37 0.04
426 1
00
21.36 0.22
49.5 0.2
2.73 0.02
2781 38
32.2 0.2
21.37 0.22
57.7 0.2
3.08 0.02
2910 46
38.2 0.2
21.46 0.22
56.3 0.2
3.03 0.02
2184 33
37.3 0.2
Return

Online
Time (ms)
900
916
886
953
885
859








0
1
2
2
5
6

954
965
540
892
826
826








2
1
7
2
3
2

Table 3: Comparison different search heuristics RockSample[7,8] environment, using
Blind policy PBVI lower bound.

state independent rock states (it depends known robot position),
complexity computing Pr(z|b, a) (b, a, z) greatly reduced. Effectively,
computation Pr(z|b, Checki ) reduces to: Pr(z|b, Checki ) = Pr(Accurate|XP , Checki )
Pr(XiR = z) + (1 Pr(Accurate|XP , Checki )) (1 Pr(XiR = z)). probability
1+(Xp ,i)
, (Xp , i) =
sensor accurate rock i, Pr(Accurate|XP , Checki ) =
2
d(X
p ,i)/d0
2
, d(Xp , i) euclidean distance position Xp position rock i,
d0 constant specifying half efficiency distance. Pr(XiR = z) obtained directly
probability (stored b) rock good. Similarly, (b, a, z) computed
quite easily move actions deterministically affect variable XP , Checki action
changes probability associated XiR according sensors accuracy.
obtain results RockSample, run algorithm starting rock configuration 20 times (i.e. 20 runs 2k different joint rock states). initial
belief state runs consists 0.5 rock good, plus
known initial robot position.
4.3.1 Real-Time Performance Online Search
Table 3, present 95% confidence intervals mean metrics interest,
RockSample[7,8] (12545 states, 13 actions, 2 observations), real-time contraints 1
second per action. compare performance using two different lower bounds, Blind
policy PBVI, use QMDP upper bound cases. performance
policy defined lower bound shown comparison header. RTBSS,
notation RTBSS(k) indicates k-step lookahead; use depth k yields average
online time closest 1 second per action.
Return terms return, first observe AEMS2 HSVI-BFS heuristics
obtain similar results. obtains highest return slight margin
one lower bounds. BI-POMDP obtains similar return combined
691

fiRoss, Pineau, Paquet, & Chaib-draa

PBVI lower bound, performs much worse Blind lower bound. two
heuristics, Satia Lave, AEMS1, perform considerably worse terms return
either lower bound.
EBR LBI terms error bound reduction lower bound improvement, AEMS2
obtains best results lower bounds. HSVI-BFS close second. indicates AEMS2 effectively reduce true error heuristics,
therefore, guarantees better performance. BI-POMDP tends less efficient
AEMS2 HSVI-BFS, significantly better RTBSS, Satia Lave,
AEMS1, slightly improve bounds case. Satia Lave unable
increase Blind lower bound, explains obtains return
Blind policy. observe higher error bound reduction lower bound
improvement, higher average discounted return usually is. confirms intuition guiding search minimize error current belief bc good
strategy obtain better return.
Nodes Reused terms percentage nodes reused, AEMS2 HVSI-BFS
generally obtain best scores. allows algorithms maintain higher number
nodes trees, could partly explain outperform
heuristics terms return, error bound reduction lower bound improvement. Note
RTBSS reuse node tree algorithm store
tree memory. consequence, reuse percentage always 0.
Online Time Finally, observe AEMS2 requires less average online time per
action algorithms attain performance. general, lower average
online time means heuristic efficient finding -optimal actions small amount
time. running time RTBSS determined chosen depth, cannot stop
completing full lookahead search.
Summary Overall, see AEMS2 HSVI-BFS obtain similar results. However
AEMS2 seems slightly better HSVI-BFS, provides better performance guarantees
(lower error) within shorter period time. difference significant.
may due small number observations environment, case two
heuristics expand tree similar ways. next section, explore domain
many observations evaluate impact factor.
lower performances three heuristics explained various reasons.
case BI-POMDP, due fact take account
observation probabilities Pr(z|b, a) discount factor heuristic value. Hence
tend expand fringe nodes affect significantly value current
belief. Satia Lave, poor performance case Blind policy
explained fact fringe nodes maximize heuristic always leaves
reached sequence Move actions. Due deterministic nature Move actions
(Pr(z|b, a) = 1 actions, whereas Check actions Pr(z|b, a) = 0.5 initially),
heuristic value fringe nodes reached Move actions much higher error
reduced significantly. result, algorithm never explores nodes Check
actions, robot always follows Blind policy (moving east, never checking
sampling rocks). demonstrates importance restricting choice
692

fiOnline Planning Algorithms POMDPs

30

25

V(b0)

20

15

AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

10

5 2
10

1

10

0

1

10

10

2

10

3

10

Time (s)

Figure 4: Evolution upper lower bounds RockSample[7,8].

leaves explore reached sequence actions maximizing upper bound,
done AEMS2, HSVI-BFS BI-POMDP. case AEMS1, probably behaves
less efficiently term uses estimate probability certain action
optimal good approximation environment. Moreover, AEMS1
restrict exploration best solution graph, probably suffers, part,
problems Satia Lave heuristic. RTBSS perform
well Blind lower bound. due short depth allowed search
tree, required running time 1 second/action. confirms
significantly better exhaustive search good heuristics guide search.
4.3.2 Long-Term Error Reduction Online Heuristic Search
compare long term performance different heuristics, let algorithms run
offline mode initial belief state environment, log changes lower
upper bound values initial belief state 1000 seconds. Here, initial lower
upper bounds provided Blind policy QMDP respectively. see
Figure 4 Satia Lave, AEMS1 BI-POMDP efficient HSVI-BFS
AEMS2 reducing error bounds. One interesting thing note
upper bound tends decrease slowly continuously, whereas lower bound often
increases stepwise manner. believe due fact upper bound
much tighter lower bound. observe error bound reduction
happens first seconds search. confirms nodes expanded earlier
tree much impact error bc expanded far
tree (e.g. hundreds seconds). important result support using online
(as opposed offline) methods.
693

fi22

22

20

20

Average Discounted Return

Average Discounted Return

Ross, Pineau, Paquet, & Chaib-draa

18
16
AEMS2
HSVIBFS
BIPOMDP

14
12
10
8
6 1
10

0

10

18
16

12
10
8
6 1
10

1

10

AEMS2 & Blind
AEMS2 & PBVI(8)
AEMS2 & PBVI(16)

14

Online Time (s)

0

10

1

10

Online Time (s)

Figure 5: Comparison return Figure 6: Comparison return
function online time
function online time
RockSample(10,10) different
RockSample(10,10) different
online methods.
offline lower bounds.

4.3.3 Influence Offline Online Time
compare performance online approaches influenced available
online offline times. allows us verify particular method better
available online time shorter (or longer), whether increasing offline time could
beneficial.
consider three approaches shown best overall performance far (BIPOMDP, HSVI-BFS AEMS2) compare average discounted return function online time constraint per action. Experiments run RockSample[10,10]
(102,401 states, 15 actions, 2 observations) following online time constraints:
0.1s, 0.2s, 0.5s, 1s, 2s, 5s 10s. vary offline time, used 3 different lower
bounds: Blind policy, PBVI 8 belief points, PBVI 16 belief points, taking
respectively 15s, 82s, 193s. upper bound used QMDP cases. results
obtained Intel Xeon 3.0 Ghz processor.
Figure 5, observe AEMS2 fares significantly better HSVI-BFS
BI-POMDP short time constraints. time constraint increases, AEMS2
HSVI-BFS performs similarly (no significant statistical difference). notice
performance BI-POMDP stops improving 1 second planning time.
explained fact take account observation probabilities
Pr(z|b, a), discount factor. search tree grows bigger, fringe
nodes small probability reached future, becomes
important take probabilities account order improve performance.
Otherwise, observe case BI-POMDP, expanded nodes affect
quality solution found.
Figure 6, observe increasing offline time beneficial effect mostly
short real-time constraints. online planning time available,
694

fiOnline Planning Algorithms POMDPs

difference performances AEMS2 Blind lower bound, AEMS2
PBVI becomes insignificant. However, online time constraints smaller one
second, difference performance large. Intuitively, short real-time
constraints algorithm enough time expand lot nodes,
policy found relies much bounds computed offline. hand,
longer time constraints, algorithm enough time significantly improve bounds
computed offline, thus policy found rely much offline bounds.
4.4 FieldVisionRockSample
seems results presented thus far HSVI-BFS AEMS2 comparable
performance standard domains. note however environments
small observation sets (assuming observations zero probability removed).
believe AEMS2 especially well suited domains large observation spaces. However,
standard problems literature. therefore consider modified
version RockSample environment, called FieldVisionRockSample (Ross & Chaib-draa,
2007), observation space size exponential number rocks.
FieldVisionRockSample (FVRS) problem differs RockSample problem
way robot able perceive rocks environment. Recall
RockSample, agent Check action specific rock observe state
noisy sensor. FVRS, robot observes state rocks,
noisy sensor, action conducted environment. Consequently,
eliminates use Check actions, remaining actions robot include
four move actions {North, East, South, West} Sample action. robot
perceive rock either Good Bad, thus observation space size 2k
instance problem k rocks. RockSample, efficiency sensor
defined parameter = 2d/d0 , distance rock d0
half efficiency distance. assume sensors observations independent rock.
FVRS, partial observability environment directly proportional
parameter d0 : d0 increases, sensor becomes accurate uncertainty
state environment decreases. value d0 defined different instances
RockSample work Smith Simmons (2004) high FVRS problem
(especially bigger instances RockSample), making almost completely observable.
Consequently, re-define value d0 different instances FieldVisionRockSample according size grid (n). considering fact
p n n grid,
largest possible distance rock robot (n 1) (2), seems reasonable distance, probability observing real state rock
close 50%
p problem remain partially observable. Consequently, define
d0 = (n 1) (2)/4.
obtain results FVRS domain, run algorithm starting rock
configurations 20 times (i.e. 20 runs 2k different joint rock states).
initial belief state runs corresponds probability 0.5
rock good, well known initial position robot.
695

fiRoss, Pineau, Paquet, & Chaib-draa

Heuristic
RTBSS(2)
AEMS1
Satia Lave
HSVI-BFS
AEMS2
BI-POMDP
RTBSS(1)
BI-POMDP
Satia Lave
AEMS1
AEMS2
HSVI-BFS

Belief
Nodes
Return
EBR (%)
LBI
Nodes
Reused (%)
FVRS[5,5] [Blind: Return:8.15, || = 1, Time=170ms]
16.54 0.37
18.4 1.1
2.80 0.19
18499 102
00
16.88 0.36
17.1 1.1
2.35 0.16
8053 123
1.19 0.07
18.68 0.39
15.9 1.2
2.17 0.16
7965 118
0.88 0.06
20.27 0.44
23.8 1.4
2.64 0.14
4494 105
4.50 0.80
21.18 0.45
31.5 1.5
3.11 0.15
12301 440
3.93 0.22
22.75 0.47
31.1 1.2
3.30 0.17
12199 427
2.26 0.44
FVRS[5,7] [Blind: Return:8.15, || = 1, Time=761ms]
20.57 0.23
7.72 0.13
2.07 0.11
516 1
00
22.75 0.25
11.1 0.4
2.08 0.07
4457 61
0.37 0.11
22.79 0.25
11.1 0.4
2.05 0.08
3683 52
0.36 0.07
23.31 0.25
12.4 0.4
2.24 0.08
3856 55
1.36 0.13
23.39 0.25
13.3 0.4
2.35 0.08
4070 58
1.64 0.14
23.40 0.25
13.0 0.4
2.30 0.08
3573 52
1.69 0.27

Online
Time (ms)
3135 27
876 5
878 4
857 12
854 13
782 12
254
923
947
942
944
946








1
2
3
3
2
3

Table 4: Comparison different search heuristics different instances FieldVisionRockSample environment.

4.4.1 Real-Time Performance Online Search
Table 4, present 95% confidence intervals mean metrics interest.
consider two instances environment, FVRS[5,5] (801 states, 5 actions, 32 observations) FVRS[5,7] (3201 states, 5 actions, 128 observations). cases, use
QMDP upper bound Blind lower bound, real-time constraints 1 second per
action.
Return terms return, observe clear winner. BI-POMDP performs surpringly well FVRS[5,5] significantly worse AEMS2 HSVI-BFS FVRS[5,7].
hand, AEMS2 significantly better HSVI-BFS FVRS[5,5]
get similar performances FVRS[5,7]. Satia Lave performs better environment RockSample. likely due fact transitions belief
space longer deterministic (as case Move actions RockSample).
FVRS[5,5], observe even RTBSS given 3 seconds per action
perform two-step lookahead, performance worse heuristic search
methods. clearly shows expanding observations equally search
good strategy, many observations negligible impact current
decision.
EBR LBI terms error bound reduction lower bound improvement, observe AEMS2 performs much better HSVI-BFS FVRS[5,5], significantly
better FVRS[5,7]. hand, BI-POMDP obtains similar results AEMS2
FVRS[5,5] significantly worse terms EBR LBI FVRS[5,7].
suggests AEMS2 consistently effective reducing error, even environments
large branching factors.
Nodes Reused percentage belief nodes reused much lower FVRS due
much higher branching factor. observe HSVI-BFS best reuse percentage
696

fiOnline Planning Algorithms POMDPs

26

35

24
30

22
20

16
14

25
AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

V(b0)

V(b0)

18

20

AEMS2
AEMS1
BIPOMDP
HSVIBFS
Satia

15

12
10

10

8
6 2
10

1

10

0

1

10

10

2

10

5 1
10

3

10

Time (s)

0

10

1

10
Time (s)

2

10

3

10

Figure 7: Evolution upper lower Figure 8: Evolution upper lower
bounds FieldVisionRockSambounds FieldVisionRockSample[5,5].
ple[5,7].

environments, however significantly higher AEMS2. methods
reuse significantly larger portion tree methods. confirms
two methods able guide search towards likely beliefs.
4.4.2 Long-Term Error Reduction Online Heuristic Search
Overall, Table 4 confirms consistent performance HSVI-BFS AEMS2,
difference heuristics modest. Considering complexity environment,
may due fact algorithms enough time expand
significant number nodes within 1 second. long-term analysis bounds evolution
Figures 7 8 confirms this. observe figures lower bound converges
slightly rapidly AEMS2 heuristics. AEMS1 heuristic
performs well long run problem, seems second best heuristic,
Satia Lave far behind. hand, HSVI-BFS heuristic far
worse problem RockSample. seems part due fact
heuristic takes time find next node expand others, thus
explores fewer belief states.

5. Discussion
previous sections presented evaluated several online POMDP algorithms.
discuss important issues arise applying online methods practice, summarize
advantages disadvantages. help researchers decide whether
online algorithms good approach solving given problem.
697

fiRoss, Pineau, Paquet, & Chaib-draa

5.1 Lower Upper Bound Selection
Online algorithms combined many valid lower upper bounds. However,
properties bounds satisfy online search perform efficiently practice. One desired properties lower upper
bound functions

property states b : L(b)


P monotone. monotone
maxaA RB (b, a) + PzZ Pr(z|b, a)L( (b, a, z)) lower bound b : U (b)
maxaA RB (b, a) + zZ Pr(z|b, a)U ( (b, a, z)) upper bound. property
guarantees certain fringe node expanded, lower bound non-decreasing
upper bound non-increasing. sufficient guarantee error bound
UT (b) LT (b) b non-increasing expansion b, error bound
given algorithm value root belief state bc , cannot worse
error bound defined initial bounds given. Note however monotonicity
necessary AEMS converge -optimal solution, shown previous work (Ross
et al., 2008); boundedness sufficient.
5.2 Improving Bounds Time
mentioned survey online algorithms, one drawback many online approaches store improvements made offline bounds
online search, that, belief state encountered again, computations need performed again, restarting offline bounds. trivial way
improve maintain large hashtable (or database) belief states
improved lower upper bounds previous search, associated new
bounds. however many drawbacks this. First every time want
evaluate lower upper bound fringe belief, search hashtable needs
performed check better bounds available. may require significant
time hashtable large (e.g. millions beliefs). Furthermore, experiments conducted
RTDP-Bel large domains, RockSample[7,8], shown process
usually runs memory (i.e. requires 1 GB) good performance
achieved requires several thousands episodes performing well (Paquet, 2006).
authors RTBSS tried combining search algorithm RTDPBel preserve improvements made search (Paquet, 2006).
combination usually performed better learned faster RTDP-Bel alone, found
domains, thousand episodes still required improvement
seen (in terms return). Hence, point updates offline bounds tend
useful large domains task accomplish repeated large number
times.
better strategy improve lower bound might save time perform
-vector updates beliefs expanded search, offline
lower bound improves time. updates advantage improving lower
bound whole belief space, instead single belief state. However
time consuming, especially large domains. Hence, need act within short
time constraints, approach infeasible. However several seconds planning time
available per action, might advantageous use time perform
-vector updates, rather use available time search tree. good
698

fiOnline Planning Algorithms POMDPs

idea would perform -vector updates subset beliefs search tree,
lower bound improves.
5.3 Factored POMDP Representations
efficiency online algorithms relies heavily ability quickly compute (b, a, z)
Pr(z|b, a), must computed evey belief state search tree. Using
factored POMDP representations effective way reduce time complexity computing quantities. Since environments large state spaces structured
described sets features, obtaining factored representation complex systems
issue cases. However, domains significant dependencies
state features, may useful use algorithms proposed Boyen Koller
(1998) Poupart (2005) find approximate factored representations features
independent, minimal degradation solution quality. upper
lower bounds might hold anymore computed approximate factored
representation, usually may still yield good results practice.
5.4 Handling Graph Structure
mentioned before, general tree search algorithm used online algorithms
duplicate belief states whenever multiple paths leading posterior
belief current belief bc . greatly simplifies complexity related updating
values ancestor nodes, reduces complexity related finding
best fringe node expand (using technique Section 3.4 valid
trees). disadvantage using tree structure inevitably, computations
redundant, algorithm potentially expand subtree every
duplicate belief. avoid this, could use LAO algorithm proposed Hansen
Zilberstein (2001) extension AO handle generic graph structure, including
cyclic graphs. expansion, runs value (or policy) iteration algorithm
convergence among ancestor nodes order update values.
heuristics surveyed Section 3.4 generalized guide best-first search
algorithms handle graph structure, LAO . first thing notice that,
graph, fringe node reached multiple paths, error contributes multiple
times error value bc . error contribution perspective, heuristic
value fringe node sum heuristic values paths reaching
it. instance, case AEMS heuristic, using notation defined
Section 3.4, global heuristic value given fringe node b, current belief state
bc graph G, computed follows:
HG (bc , b) = (U (b) L(b))

X

d(h) Pr(h|bc , G ).

(38)

hHG (bc ,b)

Notice cyclic graphs, infinitely many paths HG (bc , b).
case, could use dynamic programming estimate heuristic value.
solving HG (bc , b) fringe nodes b graph G require lot time
practice, especially many fringe nodes, experimented
method Section 4. However, would practical use heuristic could find
699

fiRoss, Pineau, Paquet, & Chaib-draa

alternative way determine best fringe node without computing HG (bc , b) separately
fringe node b performing exhaustive search fringe nodes.
5.5 Online vs. Offline Time
One important aspect determining efficiency applicability online algorithms
amount time available execution planning. course often taskdependent. real-time problems robot navigation, amount time may
short, e.g. 0.1 1 second per action. hand tasks portfolio
management, acting every second necessary, several minutes could easily
taken plan stock buying/selling action. seen experiments,
shorter available online planning time, greater importance good
offline value function start with. case, often necessary reserve sufficient
time compute good offline policy. planning time available online,
influence offline value function becomes negligible, rough offline
value function sufficient obtain good performance. best trade-off online
offline time often depends large problem is. branching factor
(|A||Z|) large and/or computing successor belief states takes long time, online
time required achieve significant improvement offline value function.
However, small problems, online time 0.1 second per action may sufficient
perform near-optimally even rough offline value function.
5.6 Advantages Disadvantages Online Algorithms
discuss advantages disadvantages online planning algorithms general.
5.6.1 Advantages
online algorithms combined offline solving algorithm, assuming
provides lower bound upper bound V , improve quality
policy found offline.
Online algorithms require little offline computation executable
environment, perform well even using loose bounds, quick
compute.
Online methods exploit knowledge current belief focus computation
relevant future beliefs current decision, scale well
large action observation spaces.
Anytime online methods applicable real-time environments,
stopped whenever planning time runs out, still provide best solution found
far.
5.6.2 Disadvantages
branching factor depends number actions observations. Thus
many observations and/or actions, might impossible search deep
700

fiOnline Planning Algorithms POMDPs

enough, provide significant improvement offline policy. cases, sampling methods designed reduce branching factor could useful.
cannot guarantee lower upper bounds still valid sampling
used, guarantee valid high probability, given enough
samples drawn.
online algorithms store improvements made offline policy
online search, algorithm plan bounds time
environment restarted. time available, could advantageous add
-vector updates belief states explored tree, offline bounds
improve time.

6. Conclusion
POMDPs provide rich elegant framework planning stochastic partially observable domains, however time complexity major issue preventing
application complex real-world systems. paper thoroughly surveys various existing online algorithms key techniques approximations used solve POMDPs
efficiently. empirically compare online approaches several POMDP domains different metrics: average discounted return, average error bound reduction
average lower bound improvement, using different lower upper bounds: PBVI,
Blind, FIB QMDP.
empirical results, observe heuristic search methods, namely
AEMS2 HSVI-BFS, obtain good performances, even domains large branching factors large state spaces. two methods similar perform well
orient search towards nodes improve current approximate
value function quickly possible; i.e. belief nodes largest error
likely reached future promising actions. However, environments
large branching factors, may time expand nodes turn.
Hence, would interesting develop approximations reduce branching
factor cases.
conclusion, believe online approaches important role play
improving scalability POMDP solution methods. good example succesful
applications RTBSS algorithm RobocupRescue simulation Paquet et al.
(2005). environment challenging state space orders magnitude
beyond scope current algorithms. Offline algorithms remain important obtain
tight lower upper bounds value function. interesting question whether
online offline approaches better, improve kinds approaches,
synergy exploited solve complex real-world problems.

Acknowledgments
research supported Natural Sciences Engineering Council Canada
Fonds Quebecois de la Recherche sur la Nature et les Technologies. would
thank anonymous reviewers helpful comments suggestions.
701

fiRoss, Pineau, Paquet, & Chaib-draa

References
Astrom, K. J. (1965). Optimal control Markov decision processes incomplete state
estimation. Journal Mathematical Analysis Applications, 10, 174205.
Barto, A. G., Bradtke, S. J., & Singhe, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72 (1), 81138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ,
USA.
Bertsekas, D. P., & Castanon, D. A. (1999). Rollout algorithms stochastic scheduling
problems. Journal Heuristics, 5 (1), 89108.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings Fourteenth Conference Uncertainty Artificial Intelligence
(UAI-98), pp. 3342.
Braziunas, D., & Boutilier, C. (2004). Stochastic local search POMDP controllers.
Nineteenth National Conference Artificial Intelligence (AAAI-04), pp. 690696.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,
exact method partially observable Markov decision processes. Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence (UAI-97), pp. 5461.
Chang, H. S., Givan, R., & Chong, E. K. P. (2004). Parallel rollout online solution
partially observable Markov decision processes. Discrete Event Dynamic Systems,
14 (3), 309341.
Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming. Proceedings Fall AAAI symposium POMDPs, pp. 6168.
Hansen, E. A. (1998). Solving POMDPs searching policy space. Fourteenth Conference Uncertainty Artificial Intelligence (UAI-98), pp. 211219.
Hansen, E. A., & Zilberstein, S. (2001). LAO * : heuristic search algorithm finds
solutions loops. Artificial Intelligence, 129 (1-2), 3562.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes. Journal Artificial Intelligence Research, 13, 3394.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101, 99134.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm nearoptimal planning large markov decision processes. Proceedings Sixteenth
International Joint Conference Artificial Intelligence (IJCAI-99), pp. 13241331.
Koenig, S. (2001). Agent-centered search. AI Magazine, 22 (4), 109131.
Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, Brown
University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies partially observable environments: scaling up. Proceedings 12th International
Conference Machine Learning (ICML-95), pp. 362370.
702

fiOnline Planning Algorithms POMDPs

Lovejoy, W. S. (1991). Computationally feasible bounds POMDPs. Operations Research,
39 (1), 162175.
Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision problems. Proceedings
Sixteenth National Conference Artificial Intelligence. (AAAI-99), pp. 541548.
McAllester, D., & Singh, S. (1999). Approximate Planning Factored POMDPs using Belief State Simplification. Proceedings 15th Annual Conference Uncertainty
Artificial Intelligence (UAI-99), pp. 409416.
Monahan, G. E. (1982). survey partially observable Markov decision processes: theory,
models algorithms. Management Science, 28 (1), 116.
Nilsson, N. (1980). Principles Artificial Intelligence. Tioga Publishing.
Papadimitriou, C., & Tsitsiklis, J. N. (1987). complexity Markov decision processes.
Mathematics Operations Research, 12 (3), 441450.
Paquet, S. (2006). Distributed Decision-Making Task Coordination Dynamic, Uncertain Real-Time Multiagent Environments. Ph.D. thesis, Laval University.
Paquet, S., Chaib-draa, B., & Ross, S. (2006). Hybrid POMDP algorithms. Proceedings
Workshop Multi-Agent Sequential Decision Making Uncertain Domains
(MSDM-06), pp. 133147.
Paquet, S., Tobin, L., & Chaib-draa, B. (2005). online POMDP algorithm complex
multiagent environments. Proceedings fourth International Joint Conference
Autonomous Agents Multi Agent Systems (AAMAS-05), pp. 970977.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm POMDPs. Proceedings International Joint Conference Artificial
Intelligence (IJCAI-03), pp. 10251032.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations large
POMDPs. Journal Artificial Intelligence Research, 27, 335380.
Pineau, J. (2004). Tractable planning uncertainty: exploiting structure. Ph.D. thesis,
Carnegie Mellon University.
Poupart, P. (2005). Exploiting structure efficiently solve large scale partially observable
Markov decision processes. Ph.D. thesis, University Toronto.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Advances Neural
Information Processing Systems 16 (NIPS).
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Ross, S., & Chaib-draa, B. (2007). Aems: anytime online search algorithm approximate policy refinement large POMDPs. Proceedings 20th International
Joint Conference Artificial Intelligence (IJCAI-07), pp. 25922598.
Ross, S., Pineau, J., & Chaib-draa, B. (2008). Theoretical analysis heuristic search
methods online POMDPs. Advances Neural Information Processing Systems
20 (NIPS).
703

fiRoss, Pineau, Paquet, & Chaib-draa

Satia, J. K., & Lave, R. E. (1973). Markovian decision processes probabilistic observation states. Management Science, 20 (1), 113.
Shani, G., Brafman, R., & Shimony, S. (2005). Adaptation changing stochastic environments online POMDP policy learning. Proceedings Workshop
Reinforcement Learning Non-Stationary Environments, ECML 2005, pp. 6170.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
Markov processes finite horizon. Operations Research, 21 (5), 10711088.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. Proceedings 20th Conference Uncertainty Artificial Intelligence (UAI-04), pp.
520527.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysis
implementation. Proceedings 21th Conference Uncertainty Artificial
Intelligence (UAI-05), pp. 542547.
Sondik, E. J. (1971). optimal control partially observable Markov processes. Ph.D.
thesis, Stanford University.
Sondik, E. J. (1978). optimal control partially observable Markov processes
infinite horizon: Discounted costs. Operations Research, 26 (2), 282304.
Spaan, M. T. J., & Vlassis, N. (2004). point-based POMDP algorithm robot planning.
Proceedings IEEE International Conference Robotics Automation
(ICRA-04), pp. 23992404.
Spaan, M. T. J., & Vlassis, N. (2005). Perseus: randomized point-based value iteration
POMDPs. Journal Artificial Intelligence Research, 24, 195220.
Vlassis, N., & Spaan, M. T. J. (2004). fast point-based algorithm POMDPs.
Benelearn 2004: Proceedings Annual Machine Learning Conference Belgium
Netherlands, pp. 170176.
Washington, R. (1997). BI-POMDP: bounded, incremental partially observable Markov
model planning. Proceedings 4th European Conference Planning, pp.
440451.
Zhang, N. L., & Zhang, W. (2001). Speeding convergence value iteration partially observable Markov decision processes. Journal Artificial Intelligence Research,
14, 2951.

704


