journal artificial intelligence

submitted published

online pomdps
stephane ross
joelle pineau

stephane ross mail mcgill ca
jpineau cs mcgill ca

school computer science
mcgill university montreal canada h

sebastien paquet
brahim chaib draa

spaquet damas ift ulaval ca
chaib damas ift ulaval ca

department computer science software engineering
laval university quebec canada g k p

abstract
partially observable markov decision processes pomdps provide rich framework
sequential decision making uncertainty stochastic domains however solving
pomdp often intractable except small due complexity
focus online approaches alleviate computational complexity computing
good local policies decision step execution online generally consist lookahead search best action execute time step
environment objectives survey existing online pomdp
methods analyze properties discuss advantages disadvantages
thoroughly evaluate online approaches different environments metrics return error bound reduction lower bound improvement experimental
indicate state art online heuristic search methods handle large pomdp
domains efficiently

introduction
partially observable markov decision process pomdp general model sequential decision partially observable environments many control modeled pomdps solved exactly
computational complexity finite horizon pomdps pspace complete papadimitriou tsitsiklis infinite horizon pomdps undecidable madani hanks
condon
last years pomdps generated significant interest ai community many approximation developed hauskrecht pineau
gordon thrun braziunas boutilier poupart smith simmons
spaan vlassis methods offline meaning
specify prior execution best action execute possible situations
approximate achieve good performance often take significant time e g hour solve large many
possible situations enumerate let alone plan furthermore small changes
environments dynamics require recomputing full policy may take hours days
c

ai access foundation rights reserved

fiross pineau paquet chaib draa

hand online approaches satia lave washington barto
bradtke singhe paquet tobin chaib draa mcallester singh
bertsekas castanon shani brafman shimony try circumvent complexity computing policy online current information state online sometimes called agent centered search koenig
whereas offline search would compute exponentially large contingency plan considering possible happenings online search considers current situation small
horizon contingency plans moreover approaches handle environment
changes without requiring computation allows online approaches applicable many contexts offline approaches applicable instance
task accomplish defined reward function changes regularly environment
one drawback online generally needs meet real time constraints
thus greatly reducing available time compared offline approaches
recent developments online pomdp search paquet chaib draa ross
ross chaib draa ross pineau chaib draa suggest combining
approximate offline online solving approaches may efficient way tackle
large pomdps fact generally compute rough policy offline existing
offline value iteration use approximation heuristic function
guide online search combination enables online search
plan shorter horizons thereby respecting online real time constraints retaining
good precision exact online search fixed horizon guarantee
reduction error approximate offline value function overall time offline
online required obtain good policy dramatically reduced combining
approaches
main purpose draw attention ai community online
methods viable alternative solving large pomdp support
first survey existing online approaches applied pomdps
discuss strengths drawbacks present combinations online
existing offline qmdp littman cassandra kaelbling
fib hauskrecht blind hauskrecht smith simmons
pbvi pineau et al compare empirically different online approaches
two large pomdp domains according different metrics average discounted return error
bound reduction lower bound improvement evaluate available online
time offline time affect performance different
experiments many state art online heuristic search methods
tractable large state observation spaces achieve solution quality stateof art offline approaches fraction computational cost best methods
achieve focusing search relevant future outcomes current
decision e g likely high uncertainty error longterm values minimize quickly possible error bound performance
best action found tradeoff solution quality computing time offered
combinations online offline approaches attractive tackling increasingly
large domains


fionline pomdps

pomdp model
partially observable markov decision processes pomdps provide general framework
acting partially observable environments astrom smallwood sondik
monahan kaelbling littman cassandra pomdp generalization
mdp model uncertainty gives agent ability
effectively estimate outcome actions even cannot exactly observe state
environment
formally pomdp represented tuple r z
set environment states state description environment
specific moment capture information relevant agents
decision making process
set possible actions
transition function pr
represents probability ending state agent performs action state

r r reward function r reward obtained
executing action state
z set possible observations
z observation function z pr z gives
probability observing z action performed resulting state
assume z finite r bounded
key aspect pomdp model assumption states directly
observable instead given time agent access observation
z z gives incomplete information current state since states
observable agent cannot choose actions states consider
complete history past actions observations choose current action
history time defined
ht z zt zt



explicit representation past typically memory expensive instead
possible summarize relevant information previous actions observations
probability distribution state space called belief state astrom
belief state time defined posterior probability distribution
state given complete history
bt pr st ht b



belief state bt sufficient statistic history ht smallwood sondik
therefore agent choose actions current belief state bt instead
past actions observations initially agent starts initial belief state b


fiross pineau paquet chaib draa

representing knowledge starting state environment time
belief state bt computed previous belief state bt previous
action current observation zt done belief state update function
b z bt bt zt defined following equation
bt bt zt


pr zt bt

zt

x

bt

ss

pr z b probability observing z action belief b acts
normalizing constant bt remains probability distribution
pr z b

x

z



x

b



ss

agent way computing belief next interesting question
choose action belief state
action determined agents policy specifying probability
agent execute action given belief state e defines agents strategy
possible situations could encounter strategy maximize amount
reward earned finite infinite time horizon article restrict attention
infinite horizon pomdps optimality criterion maximize expected
sum discounted rewards called return discounted return formally
optimal policy defined following equation


x
x x

bt
r bt b
argmax e





ss

aa

discount factor bt probability action
performed belief bt prescribed policy
return obtained following specific policy certain belief state b
defined value function equation v


x
x


v b
b rb b
pr z b v b z

aa

zz

function rb b specifies immediate expected reward executing action
belief b according reward function r
rb b

x

b r



ss

sum z equation interpreted expected future return infinite
horizon executing action assuming policy followed afterwards
note definitions rb b pr z b b z one view
pomdp mdp belief states called belief mdp pr z b specifies
probability moving b b z action rb b immediate
reward obtained action b


fionline pomdps

optimal policy defined equation represents action selection strategy
maximize equation v b since exists deterministic policy
maximizes v belief states sondik generally consider deterministic policies e assign probability specific action every belief
state
value function v optimal policy fixed point bellmans equation
bellman


x
v b max rb b
pr z b v b z

aa

zz

another useful quantity value executing given action belief state b
denoted q value
q b rb b

x

pr z b v b z



zz

difference definition v max operator omitted notice
q b determines value assuming optimal policy followed
every step action
review different offline methods solving pomdps used guide
online heuristic search methods discussed later cases form
basis online solutions
optimal value function
one solve optimally pomdp specified finite horizon h value
iteration sondik uses dynamic programming compute
increasingly accurate values belief state b value iteration
begins evaluating value belief state immediate horizon formally
let v value function takes belief state parameter returns numerical
value r belief state initial value function
v b max rb b
aa



value function horizon constructed value function horizon
following recursive equation


x
vt b max rb b
pr z b vt b z

aa

zz

value function equation defines discounted sum expected rewards
agent receive next time steps belief state b therefore optimal
policy finite horizon simply choose action maximizing vt b


x
pr z b vt b z

b argmax rb b
aa

zz



fiross pineau paquet chaib draa

last equation associates action specific belief state therefore must
computed possible belief states order define full policy
key smallwood sondik shows optimal value function
finite horizon pomdp represented hyperplanes therefore convex
piecewise linear means value function vt horizon represented
set dimensional hyperplanes hyperplanes often
called vectors defines linear value function belief state space associated
action value belief state maximum value returned one
vectors belief state best action one associated vector
returned best value
x
b

vt b max


ss

number exact value function leveraging piecewise linear convex
aspects value function proposed pomdp literature sondik
monahan littman cassandra littman zhang zhang zhang
exact approaches number vectors
needed represent value function grows exponentially number observations
iteration e size set z since vector
requires computation time z resulting complexity iteration exact
approaches z z work exact approaches focused
finding efficient ways prune set effectively reduce computation
offline approximate
due high complexity exact solving approaches many researchers worked
improving applicability pomdp approaches developing approximate offline
approaches applied larger
online methods review approximate offline often used
compute lower upper bounds optimal value function bounds
leveraged orient search promising directions apply branch bound pruning
techniques estimate long term reward belief states section
however generally want use approximate methods require low
computational cost particularly interested approximations use
underlying mdp compute lower bounds blind policy upper bounds mdp qmdp
fib exact value function investigate usefulness precise
lower bounds provided point methods briefly review offline methods
featured empirical investigation recent publications provide
comprehensive overview offline approximate hauskrecht pineau
gordon thrun
blind policy
blind policy hauskrecht smith simmons policy
action executed regardless belief state value function blind
mdp defined r components pomdp model



fionline pomdps

policy obviously lower bound v since corresponds value one specific
policy agent could execute environment resulting value function
specified set vectors vector specifies long term expected
reward following corresponding blind policy vectors computed
simple update rule
r

x







minss r vectors computed use equation
obtain lower bound value belief state complexity iteration
far less exact methods lower bound
computed quickly usually tight thus informative
point
obtain tighter lower bounds one use point methods lovejoy hauskrecht
pineau et al popular approximates value function updating selected belief states point methods sample belief
states simulating random interactions agent pomdp environment
update value function gradient sampled beliefs approaches circumvent complexity exact approaches sampling small set beliefs
maintaining one vector per sampled belief state let b represent set
sampled beliefs set vectors time obtained follows

z

bt







r
p
z
z
z




p
p



z
b b zz argmaxt
ss b
p
b b argmaxb ss b b b





ensure gives lower bound initialized single vector

mins aa r



since b iteration complexity z b
b polynomial time compared exponential time exact approaches
different developed point pbvi pineau
et al perseus spaan vlassis hsvi smith simmons
recent methods methods differ slightly choose
belief states update value function chosen belief states
nice property approaches one tradeoff complexity
precision lower bound increasing decreasing number
sampled belief points
mdp
mdp approximation consists approximating value function v pomdp
value function underlying mdp littman et al value function
upper bound value function pomdp computed bellmans
equation


fiross pineau paquet chaib draa



dp
vt
max r
aa

x



vtm dp





p
value v b belief state b computed v b ss v dp b
computed quickly iteration equation done
qmdp
qmdp approximation slight variation mdp approximation littman et al
main idea behind qmdp consider partial observability disappear
single step assumes mdp solution computed generate vtm dp equation
given define
dp
qm
r

x

vtm dp





approximation defines vector action gives upper bound v
tighter v dp e vtqm dp b vtm dp b belief b
obtain value belief state use equation contain one vector
dp
qm

fib
two upper bounds presented far qmdp mdp take account
partial observability environment particular information gathering actions
may help identify current state suboptimal according bounds
address hauskrecht proposed method compute upper bounds
called fast informed bound fib able take account degree
partial observability environment vector update process described
follows
r

x

zz



max



x

z





initialized vectors found qmdp convergence e
vectors
qm dp fib defines single vector action value belief
state computed according equation fib provides tighter upper bound
qmdp e vtf ib b vtqm dp b b complexity remains
acceptable iteration requires z operations

online pomdps
offline approaches returns policy defining action execute
every possible belief state approaches tend applicable dealing
small mid size domains since policy construction step takes significant time large
pomdps rough value function approximation ones presented
section tends substantially hinder performance resulting approximate


fionline pomdps

offline approaches
policy construction

policy execution

online approaches

small policy construction step policy execution steps

figure comparison offline online approaches
policy even recent point methods produce solutions limited quality
large domains paquet et al
hence large pomdps potentially better alternative use online
tries good local policy current belief state agent
advantage needs consider belief states reachable current belief state focuses computation small set beliefs
addition since online done every step thus generalization beliefs
required sufficient calculate maximal value current belief
state full optimal vector setting policy construction steps
execution steps interleaved one another shown figure cases online
approaches may require extra execution steps online since policy
locally constructed therefore optimal however policy construction time
often substantially shorter consequently overall time policy construction
execution normally less online approaches koenig practice potential
limitation online need meet short real time constraints
case time available construct plan small compared offline
general framework online
subsection presents general framework online pomdps
subsequently discuss specific approaches literature describe vary
tackling aspects general framework
online divided phase execution phase
applied alternately time step
phase given current belief state agent
computes best action execute belief usually achieved two steps
first tree reachable belief states current belief state built looking
several possible sequences actions observations taken current
belief tree current belief root node subsequent reachable beliefs
calculated b z function equation added tree child nodes
immediate previous belief belief nodes represented nodes
must choose action actions included layer belief nodes
nodes must consider possible observations lead subsequent


fiross pineau paquet chaib draa

b

















z


b



b

z
b


b



z


b








z

z














z
b



z






z
b

b



figure tree constructed search process pomdp actions observations belief states nodes represented triangular nodes action nodes
circular nodes rewards rb b represented values outgoing arcs
nodes probabilities pr z b shown outgoing arcs nodes
values inside brackets represent lower upper bounds computed according
equations assuming discount factor notice example
action belief state b could pruned since upper bound lower
lower bound action b

beliefs value current belief estimated propagating value estimates
fringe nodes ancestors way root according bellmans
equation equation long term value belief nodes fringe usually estimated
approximate value function computed offline methods maintain
lower bound upper bound value node example
tree contructed evaluated presented figure
phase terminates execution phase proceeds executing best
action found current belief environment updating current belief
tree according observation obtained
notice general belief mdp could graph structure cycles
online handle structure unrolling graph tree hence
reach belief already elsewhere tree duplicated could
modified handle generic graph structures technique proposed
lao hansen zilberstein handle cycles however
advantages disadvantages depth discussion issue
presented section
generic online implementing phase lines execution
phase lines presented first initializes tree
contain initial belief state line given current tree phase
proceeds first selecting next fringe node line
pursue search construction tree expand function line constructs


fionline pomdps

function onlinepomdpsolver
static bc current belief state agent
tree representing current search tree
expansion depth
l lower bound v
u upper bound v















bc b
initialize contain bc root
executionterminated
planningterminated
b choosenextnodetoexpand
expand b
updateancestors b
end
execute best action bc
perceive observation z
bc bc z
update tree bc root
end

generic online
next reachable beliefs equation selected leaf pre determined
expansion depth evaluates approximate value function newly created
nodes approximate value expanded node propagated ancestors
via updateancestors function line phase conducted
terminating condition met e g time available optimal action
found
execution phase executes best action found
line gets observation environment line next
updates current belief state search tree according recent action
observation z lines online approaches reuse previous computations
keeping subtree belief resuming search subtree
next time step cases keeps nodes tree
belief bc deletes nodes tree loops back
phase next time step task terminated
side note online useful improve precision
approximate value function computed offline captured theorem
theorem puterman hauskrecht let v approximate value function supb v b v b approximate value v b returned dstep lookahead belief b v estimate fringe node values error bounded
v b v b
notice error converges depth search tends
indicates online effectively improve performance obtained
approximate value function computed offline action arbitrarily close
optimal current belief however evaluating tree reachable beliefs
within depth complexity z exponential
becomes quickly intractable large furthermore time available
execution may short exploring beliefs depth may infeasible


fiross pineau paquet chaib draa

hence motivates need efficient online guarantee similar
better error bounds
efficient online focus limiting number reachable beliefs explored tree choose relevant ones approaches
generally differ subroutines choosenextnodetoexpand expand
implemented classify approaches three categories branch bound
pruning monte carlo sampling heuristic search present survey
approaches discuss strengths drawbacks online
proceed via tree search approaches discussed section
branch bound pruning
branch bound pruning general search technique used prune nodes
known suboptimal search tree thus preventing expansion unnecessary
lower nodes achieve tree lower bound upper bound
maintained value q b action every belief b tree
bounds computed first evaluating lower upper bound fringe nodes
tree bounds propagated parent nodes according following
equations

l b
b f
lt b

maxaa lt b otherwise
x
lt b rb b
pr z b lt b z

zz



u b
b f
maxaa ut b otherwise
x
ut b rb b
pr z b ut b z
ut b




zz

f denotes set fringe nodes tree ut b lt b represent upper
lower bounds v b associated belief state b tree ut b lt b
represent corresponding bounds q b l b u b bounds used fringe
nodes typically computed offline equations equivalent bellmans equation
equation however use lower upper bounds children instead v
several techniques presented section used quickly compute lower bounds
blind policy upper bounds mdp qmdp fib offline
given bounds idea behind branch bound pruning relatively simple
given action belief b upper bound ut b lower another action
lower bound lt b know guaranteed value q b q b
thus suboptimal belief b hence branch pruned belief reached
taking action b considered
rtbss
real time belief space search rtbss uses branch bound
compute best action take current belief paquet et al starting


fionline pomdps

function expand b
inputs b

static
l
u















belief node want expand
depth expansion b
tree representing current search tree
lower bound v
upper bound v


lt b l b
else
sort actions u b ai u b aj j

lt b
u b ai
plt b
lt b ai rb b ai zz pr z b ai expand b ai z
lt b max lt b lt b ai
ii
end
end
return lt b

expand subroutine rtbss
current belief expands tree depth first search fashion
pre determined search depth leaves tree evaluated lower
bound computed offline propagated upwards lower bound maintained
node tree
limit number nodes explored branch bound pruning used along way
prune actions known suboptimal thus excluding unnecessary nodes
actions maximize pruning rtbss expands actions descending order
upper bound first action expanded one highest upper bound expanding
actions order one never expands action could pruned actions
expanded different order intuitively action higher upper bound
actions cannot pruned actions since lower
bound never exceed upper bound another advantage expanding actions
descending order upper bound soon action
pruned know remaining actions pruned since upper
bounds necessarily lower fact rtbss proceeds via depth first search
increases number actions pruned since bounds expanded actions
become precise due search depth
terms framework rtbss requires choosenextnodetoexpand subroutine simply return current belief bc updateancestors function need perform operation since bc ancestor root tree
expand subroutine proceeds via depth first search fixed depth
branch bound pruning mentioned subroutine detailed
expansion performed planningterminated evaluates true
best action found executed end time step tree simply reinitialized
contain current belief root node
efficiency rtbss depends largely precision lower upper bounds
computed offline bounds tight pruning possible search
efficient unable prune many actions searching


fiross pineau paquet chaib draa

limited short horizons order meet real time constraints another drawback
rtbss explores observations equally inefficient since
could explore parts tree small probability occurring thus
small effect value function number observations large
limited exploring short horizon
final note since rtbss explores reacheable beliefs within depth except
reached suboptimal actions guarantee error bound step
lookahead see theorem therefore online search directly improves precision
original offline value bounds factor aspect confirmed empirically
different domains rtbss authors combined online search bounds given
offline cases showed tremendous improvement
policy given offline paquet et al
monte carlo sampling
mentioned expanding search tree fully large set observations
infeasible except shallow depths cases better alternative may sample
subset observations expansion consider beliefs reached sampled
observations reduces branching factor search allows deeper search
within set time strategy employed monte carlo
mcallester singh
presented mcallester singh adaptation online mdp
presented kearns mansour ng consists depth limited
search tree certain fixed horizon instead exploring
observations action choice c observations sampled generative model
probabilities pr z b approximated observed frequencies sample
advantage sampling observation distribution
pr z b achieved efficiently log log z computing exact
probabilities pr z b observation z thus sampling useful
alleviate complexity computing pr z b expense less precise estimate
nevertheless samples often sufficient obtain good estimate observations
effect q b e occur high probability
likely sampled authors apply belief state factorization boyen
koller simplify belief state calculations
implementation expand subroutine expands tree
fixed depth monte carlo sampling observations mentioned see
end time step tree reinitialized contain
current belief root
kearns et al derive bounds depth number samples c needed
obtain optimal policy high probability number samples
required grows exponentially desired accuracy practice number samples
required infeasible given realistic online time constraints however performance terms
returns usually good even many fewer samples


fionline pomdps

function expand b
inputs b

static
c













belief node want expand
depth expansion b
tree representing current search tree
number observations sample


lt b maxaa rb b
else
lt b

sample z z z zc distribution pr z b
p
n z
lt b rb b zz nz z zc expand b z
lt b max lt b lt b
end
end
return lt b

expand subroutine mcallester singhs

one inconvenience method action pruning done since monte
carlo estimation guaranteed correctly propagate lower upper bound
property tree article authors simply approximate value
fringe belief states immediate reward rb b could improved
good estimate v computed offline note may difficult
apply domains number actions large course may
impact performance
rollout
another similar online monte carlo rollout bertsekas castanon requires initial policy possibly computed offline
time step estimates future expected value action assuming initial policy followed future time steps executes action highest estimated value
estimates obtained computing average discounted return obtained
set sampled trajectories depth trajectories generated first taking
action evaluated following initial policy subsequent belief states
assuming observations sampled generative model since
needs consider different actions root belief node number actions
influences branching factor first level tree consequently generally
scalable mcallester singhs bertsekas castanon
enough sampling resulting policy guaranteed perform least
well initial policy high probability however generally requires many sampled
trajectories provide substantial improvement initial policy furthermore
initial policy significant impact performance particular
cases might impossible improve return initial policy changing
immediate action e g several steps need changed reach specific subgoal
higher rewards associated cases rollout policy never improve
initial policy


fiross pineau paquet chaib draa

function expand b
inputs b belief node want expand
depth expansion b
static tree representing current search tree
set initial policies
number trajectories depth sample

lt b



q b



b b



j
j

q b q b
rb b

z sampleobservation b

b b z

b

end

end
end
lt b max q b
end

expand subroutine parallel rollout
address issue relative initial policy chang givan chong
introduced modified version called parallel rollout case
starts set initial policies proceeds rollout
initial policies set value considered immediate action
maximum set initial policies action highest value executed
policy obtained guaranteed perform least well best
initial policy high probability given enough samples parallel rollout handle
domains large number actions observations perform well
set initial policies contain policies good different regions belief space
expand subroutine parallel rollout presented
original rollout bertsekas castanon
special case set initial policies contains one policy
subroutines proceed mcallester singhs
heuristic search
instead branch bound pruning monte carlo sampling reduce branching factor search heuristic search try focus search relevant reachable beliefs heuristics select best fringe beliefs node expand
relevant reachable beliefs ones would allow search
make good decisions quickly possible e expanding nodes possible
three different online heuristic search pomdps
proposed past satia lave bi pomdp washington aems
ross chaib draa maintain lower upper bounds
value node tree equations differ
specific heuristic used choose next fringe node expand tree


fionline pomdps

first present common subroutines discuss different
heuristics
recalling general framework three steps interleaved several
times heuristic search first best fringe node expand according
heuristic current search tree found tree expanded
node usually one level finally ancestor nodes values updated
values must updated choose next node expand since heuristic
value usually depends general heuristic search slightly
computationally expensive standard depth breadth first search due
extra computations needed select best fringe node expand need
update ancestors iteration required previous methods
branch bound pruning monte carlo sampling complexity extra
steps high benefit expanding relevant nodes might
outweighed lower number nodes expanded assuming fixed time
heuristic search particular heuristic value associated every fringe
node tree value indicate important expand node
order improve current solution iteration goal
fringe node maximizes heuristic value among fringe nodes
achieved efficiently storing node tree reference best fringe node
expand within subtree well associated heuristic value particular
root node contains reference best fringe node whole tree
node expanded ancestors nodes tree best fringe node
reference corresponding heuristic value need updated updated
efficiently references heuristic values stored lower nodes via
dynamic programming described formally equations ht b
denotes highest heuristic value among fringe nodes subtree b bt b
reference fringe node ht b basic heuristic value associated fringe node b
ht b ht b z factors weigh basic heuristic value level
tree example ht b z could pr z b order give higher weight
hence favor fringe nodes reached likely observations

ht b
b f

ht b

maxaa ht b ht b otherwise

ht b maxzz ht b z ht b z

b
b f

bt b
bt b atb otherwise

bt b bt b zb


ab argmaxaa ht b ht b

zb
argmaxzz ht b z ht b z
procedure finds fringe node b f maximizes overall heuristic value
qdt b
ht bi ai ht bi ai zi bi ai zi represent ith belief
ht bc b ht b
action observation path bc b dt b depth fringe
node b note ht bt updated ancestor nodes last expanded
node reusing previously computed values nodes procedure


fiross pineau paquet chaib draa

function expand b
inputs b
static bc

l
u





















node want expand
current belief state agent
tree representing current search tree
lower bound v
upper bound v


z z
b b z
ut b u b
lt b l b
ht b ht b
bt b b
end
p
lt b rb b p zz pr z b lt b z
ut b rb b zz pr z b ut b z

argmax
zb
zz ht b z ht b z




ht b ht b zb ht b zb



bt b bt b zb
end
lt b max maxaa lt b lt b
ut b min maxaa ut b ut b


b argmaxaa ht b ht b
b
h
ht b ht b
b

b
bt b bt b
b

expand expand subroutine heuristic search
best fringe node expand tree time linear depth
tree versus exponential depth tree exhaustive search fringe
nodes updates performed expand updateancestors
subroutines described detail iteration
choosenextnodetoexpand subroutine simply returns reference best fringe
node stored root tree e bt bc
expand subroutine used heuristic search methods presented
performs one step lookahead fringe node b main difference respect
previous methods sections heuristic value best fringe node
expand nodes computed lines best leaf node
bs subtree heuristic value computed according equations
lines
updateancestors function presented goal function update bounds ancestor nodes best fringe node expand
next starting given node b function simply updates recursively ancestor nodes b bottom fashion equations update bounds
equations update reference best fringe expand heuristic value
notice updateancestors function reuse information already stored
node objects need recompute b z pr z b rb b
however may need recompute ht b z ht b according bounds
depending heuristic defined
due anytime nature heuristic search search usually keeps
going optimal action found current belief bc available


fionline pomdps

function updateancestors b
















inputs b node want update ancestors
static bc current belief state agent
tree representing current search tree
l lower bound v
u upper bound v
b bc
set b action apin belief b parent node belief node b
lt b rb b p zz pr z b lt b z
ut b rb b zz pr z b ut b z

argmax
zb
zz ht b z ht b z
h b z
ht b ht b zb

b

bt b bt b zb
lt b maxa lt b
ut b maxa ut b




b argmaxa ht b ht b
b
h
ht b ht b

b
b
bt b bt b
b
b b
end

updateancestors updates bounds ancestors ancestors
node
time elapsed optimal action found whenever ut bc lt bc lt bc
ut bc argmaxaa lt bc e actions pruned case
optimal action found
covered basic subroutines present different heuristics
proposed satia lave washington ross chaib draa
begin introducing useful notation
given graph structure g let us denote f g set fringe nodes g
hg b b set sequences actions observations lead belief node b
belief node b search graph g tree ht b b contain


single sequence denote hb b
given sequence h hg b b define
pr hz b ha probability observe whole sequence observations hz h given
start belief node b perform whole sequence actions ha h finally
define pr h b probability follow entire action observation sequence
h start belief b behave according policy formally probabilities
computed follows
h

pr hz b ha



pr hiz bhi hia





h

pr h b



pr hiz bhi hia bhi hia





h represents depth h number actions sequence h hia denotes
ith action sequence h hiz ith observation sequence h bhi belief state
obtained taking first actions observations sequence h b note
bh b


fiross pineau paquet chaib draa

satia lave
satia lave follows heuristic search framework presented
main feature explore iteration fringe node b
current search tree maximizes following term
bc b

ht bc b ht



c b
c b
pr hbt z
bc hbt
ut b lt b



b f bc root node intuition behind heuristic simple
recalling definition v note weight value v b fringe node b
bc b
c b
c b
c b
sequence optimal
provided hbt
bc hbt
v bc would exactly ht pr hbt z
actions fringe nodes weight high effect estimate
v bc hence one try minimize error nodes first term
ut b lt b included since upper bound unknown error v b lt b
thus heuristic focuses search areas tree affect value v bc
error possibly large uses branch bound pruning
fringe node reached action dominated parent belief b
never going expanded notation
heuristic implemented defining ht b ht b ht b z follows
ht b
ut b lt b
ut b lt b
ht b
otherwise
ht b z pr z b



condition ut b lt b ensures global heuristic value ht bc b
bc b
dominated pruned guarantees fringe
action sequence ht
nodes never expanded
satia laves heuristic focuses search towards beliefs likely
reached future error large heuristic likely efficient
domains large number observations probability distribution
observations concentrated observations term ut b lt b
heuristic prevents search unnecessary computations areas tree
already good estimate value function term efficient
bounds computed offline u l sufficiently informative similarly node pruning
going efficient u l sufficiently tight otherwise actions
pruned
bi pomdp
washington proposed slightly different inspired ao
nilsson search conducted best solution graph case
online pomdps corresponds subtree belief nodes reached
sequences actions maximizing upper bound parent beliefs
b
set fringe nodes best solution graph g denote f g

b
defined formally f g b f g h hg bc b pr h b g g b
argmaxa ug b g b otherwise ao simply specifies


fionline pomdps

expanding fringe nodes washington recommends exploring fringe
node fb g g current acyclic search graph maximizes ug b lg b
washingtons heuristic implemented defining ht b ht b ht b z
follows
ht b
ut b lt b
argmaxa ut b
ht b
otherwise
ht b z



heuristic tries guide search towards nodes reachable promising
actions especially loose bounds values possibly large error one
nice property expanding fringe nodes best solution graph
way reduce upper bound root node bc case
satia laves heuristic however washingtons heuristic take account
probability pr hz b ha discount factor h may end exploring
nodes small probability reached future thus
little effect value v bc hence may explore relevant nodes
optimizing decision bc heuristic appropriate upper bound u
computed offline sufficiently informative actions highest upper bound
would usually tend highest q value cases focus
search actions thus optimal action quickly
explored actions equally hand consider observation
probabilities may scale well large observation sets
able focus search towards relevant observations
aems
ross chaib draa introduced heuristic combines advantages bipomdp satia laves heuristic theoretical error analysis tree
search pomdps presented ross et al
core idea expand tree reduce error v bc quickly
possible achieved expanding fringe node b contributes
error v bc exact error contribution et bc b fringe node b bc tree
defined following equation
bc b

et bc b ht



pr hbtc b bc v b lt b



expression requires v computed exactly practice ross chaibdraa suggest approximating exact error v b lt b ut b lt b
done satia lave washington suggest approximating
policy b represents probability action optimal
parent belief b given lower upper bounds tree particular ross et al
considered two possible approximations first one uniformity
assumption distribution q values lower upper bounds
yields


fiross pineau paquet chaib draa

b





b lt b
u
ut b lt b


ut b lt b
otherwise



normalization constant sum probabilities b
actions equals
second inspired ao bi pomdp assumes action maximizing
upper bound fact optimal action

argmaxa ut b

b
otherwise
given approximation aems heuristic explore fringe node b
maximizes
bc b


ht bc b ht

pr hbtc b bc ut b lt b



implemented defining ht b ht b ht b z follows
ht b ut b lt b
ht b b
ht b z pr z b



refer heuristic aems defined equation aems
defined equation
let us examine aems combines advantages satia lave
bi pomdp heuristics first aems encourages exploration nodes loose bounds
possibly large error considering term ut b lt b previous heuristics
moreover satia lave focuses exploration towards belief states likely
encountered future good two reasons mentioned belief
state low probability occurrence future limited effect value
v bc thus necessary know value precisely second exploring highly
probable belief states increases chance able reuse computations
future hence aems able deal efficiently large observation sets
assuming distribution observations concentrated observations finally
bi pomdp aems favors exploration fringe nodes reachable actions
seem likely optimal according useful handle large action
sets focuses search actions look promising promising actions
optimal quickly become apparent work well best
actions highest probabilities furthermore possible define
automatically prunes dominated actions ensuring b whenever
ut b lt b cases heuristic never choose expand fringe node
reached dominated action
final note ross et al determined sufficient conditions
search heuristic guaranteed optimal action within finite
time stated theorem
aems heuristic used policy search hansen



fionline pomdps

theorem ross et al let bc current belief tree
parent belief b ut b lt b b argmaxa ut b
aems guaranteed optimal action bc within finite time
observe theorem possible define many different policies
aems heuristic guaranteed converge aems aems
satisfy condition
hsvi
heuristic similar aems used smith simmons offline
value iteration hsvi way pick next belief point perform
vector backups main difference hsvi proceeds via greedy search
descends tree root node b going towards action maximizes
upper bound observation maximizes pr z b u b z l b z
level reaches belief b depth u b l b
heuristic could used online heuristic search instead stopping
greedy search process reaches fringe node tree selecting node
one expanded next setting hsvis heuristic would return greedy
approximation aems heuristic may fringe node actually
bc b
maximizes ht pr hbtc b bc ut b lt b consider online version
hsvi heuristic empirical study section refer extension hsvi bfs
note complexity greedy search finding best fringe node
via dynamic programming process updates ht bt updateancestors
subroutine
alternatives tree search
present two alternative online approaches proceed via lookahead
search belief mdp online approaches presented far one
learning achieved time e everytime agent encounters belief
recompute policy starting initial upper lower bounds computed offline
two online approaches presented next address presenting alternative
ways updating initial value functions computed offline performance
agent improves time stores updated values computed time step
however argued discussion section techniques lead
disadvantages terms memory consumption time complexity
rtdp bel
alternative searching graphs rtdp barto
et al adapted solve pomdps geffner bonet
called rtdp bel learns approximate values belief states visited
successive trials environment belief state visited agent evaluates
possible actions estimating expected reward taking action current belief


fiross pineau paquet chaib draa

function onlinepomdpsolver
static bc current belief state agent
v initial approximate value function computed offline
v hashtable beliefs approximate value
k discretization resolution

initialize bc initial belief state v empty hashtable
executionterminated
p
evaluate q bc rb b zz pr z b v discretize b z k
argmaxaa q bc
execute best action bc
v discretize bc k q bc
perceive observation z
bc bc z
end

rtdp bel
state b approximate q value equation
x
q b rb b
pr z b v b z



zz

v b value learned belief b
belief state b value table initialized heuristic value
authors suggest mdp approximation initial value belief state
agent executes action returned greatest q b value afterwards
value v b table updated q b value best action finally
agent executes chosen action makes observation ending
belief state process repeated belief
rtdp bel learns heuristic value belief state visited
maintain estimated value belief state memory needs discretize
belief state space finite number belief states allows generalization
value function unseen belief states however might difficult best
discretization given practice needs substantial amounts
memory greater gb cases store learned belief state values
especially pomdps large state spaces implementation rtdp bel
presented
function discretize b k returns discretized belief b b round kb k
states v b looks value belief b hashtable b present
hashtable value v b returned v supported experimental data geffner
bonet suggest choosing k usually produces best
notice discretization resolution k k possible discretized
beliefs implies memory storage required maintain v exponential
becomes quickly intractable even mid size furthermore learning good
estimates exponentially large number beliefs usually requires large number
trials might infeasible practice technique sometimes applied
large domains factorized representation available cases belief
maintained set distributions one subset conditionaly independent state
variables discretization applied seperately distribution greatly
reduce possible number discretized beliefs


fionline pomdps


rtbss
mcallester
rollout
satia lave
washington
aems
hsvi bfs
rtdp bel
sovi

optimal
yes
high probability

yes
acyclic graph
yes
yes

yes

anytime



yes
yes
yes
yes

yes

branch
bound
yes


yes
implicit
implicit
implicit



monte
carlo

yes
yes







heuristic



yes
yes
yes
yes



learning







yes
yes

table properties online methods

sovi
recent online called sovi shani et al extends hsvi smith
simmons online value iteration maintains
priority queue belief states encountered execution proceeds
vector updates current belief state k belief states highest priority
time step priority belief state computed according much value
function changed successor belief states since last time updated authors
propose improvements hsvi improve scalability
efficient vector pruning technique avoiding use linear programs update
evaluate upper bound main drawback hardly applicable
large environments short real time constraints since needs perform value
iteration update vectors online high complexity number
vectors representing value function increases e k z
compute
summary online pomdp
summary see online pomdp approaches lookahead search
improve scalability different techniques used branch bound pruning search
heuristics monte carlo sampling techniques reduce complexity different angles branch bound pruning lowers complexity related action space
size monte carlo sampling used lower complexity related observation space size could potentially used reduce complexity related
action space size sampling subset actions search heuristics lower complexity
related actions observations orienting search towards relevant actions observations appropriate factored pomdp representations used
reduce complexity related state summary different properties
online presented table


fiross pineau paquet chaib draa

empirical study
section compare several online approaches two domains found pomdp
literature tag pineau et al rocksample smith simmons consider modified version rocksample called fieldvisionrocksample ross chaib draa
higher observation space original rocksample environment
introduced means test compare different environments
large observation spaces
methodology
environment first compare real time performance different heuristics
presented section limiting time second per action heuristics
given lower upper bounds would comparable
objective evaluate search heuristic efficient different types
environments end implemented different search heuristics satia
lave bi pomdp hsvi bfs aems best first search
directly measure efficiency heuristic obtained
different lower bounds blind pbvi verify choice affects heuristics
efficiency finally compare online offline times affect performance
except stated otherwise experiments run intel xeon
ghz gb ram processes limited gb ram
metrics compare online approaches
compare performance first foremost terms average discounted return execution time however really seek online approaches guarantee better
solution quality provided original bounds words seek
reduce error original bounds much possible suggests good
metric efficiency online compare improvement terms
error bounds current belief online search hence define
error bound reduction percentage
ut b lt b


u b l b
ut b lt b u b l b defined section best online
provide highest error bound reduction percentage given initial bounds
real time constraint
ebr metric necessarily reflect true error reduction compare return guarantees provided e lower bounds expected
return provided computed policies current belief improvement
lower bound compared initial lower bound computed offline direct indicator
true error reduction best online provide greatest lower bound
improvement current belief given initial bounds real time constraint
formally define lower bound improvement
ebr b

lbi b lt b l b




fionline pomdps

experiments ebr lbi metrics evaluated time step
current belief interested seeing provides highest ebr
lbi average
consider metrics pertaining complexity efficiency particular
report average number belief nodes maintained search tree methods
lower complexity generally able maintain bigger trees
relate higher error bound reduction returns
measure efficiency reusing part search tree recording percentage
belief nodes reused one time step next
tag
tag initially introduced pineau et al environment
used recently work several authors poupart boutilier vlassis
spaan pineau spaan vlassis smith simmons braziunas
boutilier spaan vlassis smith simmons environment
approximate pomdp necessary large size states actions
observations tag environment consists agent catch tag
another agent moving cell grid domain reader referred work
pineau et al full description domain note presented
belief state represented factored form domain exact
factorization possible
obtain tag run starting configuration times
e runs different starting joint positions excluding terminal
states initial belief state runs consists uniform distribution
possible joint agent positions
table compares different heuristics presenting confidence intervals
average discounted return per run return average error bound reduction percentage per
time step ebr average lower bound improvement per time step lbi average belief
nodes search tree per time step belief nodes average percentage belief nodes
reused per time step nodes reused average online time used per time step
online time cases use fib upper bound blind lower bound note
average online time slightly lower second per step
sometimes optimal solutions less second
observe efficiency hsvi bfs bi pomdp aems differs slightly
environment outperform three heuristics rtbss satia
lave aems difference explained fact latter three
methods restrict search best solution graph consequence
explore many irrelevant nodes shown lower error bound reduction percentage
lower bound improvement nodes reused poor reuse percentage explains
satia lave aems limited lower number belief nodes search
tree compared methods reached averages around k
three heuristics differ much three heuristics differ
way choose observations explore search since two observations
possible first action observation one observations leads directly


fiross pineau paquet chaib draa

heuristic
rtbss
satia lave
aems
hsvi bfs
bi pomdp
aems

return







ebr







lbi







belief
nodes







nodes
reused







online
time ms







table comparison different search heuristics tag environment blind
policy lower bound

exit



figure rocksample
terminal belief state possibility heuristics differed significantly
limited due limitation tag domain compare online
larger complex domain rocksample
rocksample
rocksample originally presented smith simmons
domain agent explore environment sample rocks see figure
similarly real robot would planet mars agent receives rewards
sampling rocks leaving environment extreme right environment
rock scientific value agent sample good rocks
define rocksample n k instance rocksample n n
grid k rocks state characterized k variables xp defines position
robot take values n n k variables x r xkr
representing rock take values good bad
agent perform k actions n orth south east w est sample check
checkk four motion actions deterministic sample action samples
rock agents current location checki action returns noisy observation
good bad rock
belief state represented factored form known position set k
probabilities namely probability rock good since observation rock


fionline pomdps

heuristic
satia lave
aems
rtbss
bi pomdp
hsvi bfs
aems
aems
satia lave
rtbss
bi pomdp
aems
hsvi bfs

belief
nodes
ebr
lbi
nodes
reused
blind return time






























pbvi return b time






























return

online
time ms










































table comparison different search heuristics rocksample environment
blind policy pbvi lower bound

state independent rock states depends known robot position
complexity computing pr z b b z greatly reduced effectively
computation pr z b checki reduces pr z b checki pr accurate xp checki
pr xir z pr accurate xp checki pr xir z probability
xp
xp
sensor accurate rock pr accurate xp checki

x
p

xp euclidean distance position xp position rock
constant specifying half efficiency distance pr xir z obtained directly
probability stored b rock good similarly b z computed
quite easily move actions deterministically affect variable xp checki action
changes probability associated xir according sensors accuracy
obtain rocksample run starting rock configuration times e runs k different joint rock states initial
belief state runs consists rock good plus
known initial robot position
real time performance online search
table present confidence intervals mean metrics interest
rocksample states actions observations real time contraints
second per action compare performance two different lower bounds blind
policy pbvi use qmdp upper bound cases performance
policy defined lower bound shown comparison header rtbss
notation rtbss k indicates k step lookahead use depth k yields average
online time closest second per action
return terms return first observe aems hsvi bfs heuristics
obtain similar obtains highest return slight margin
one lower bounds bi pomdp obtains similar return combined


fiross pineau paquet chaib draa

pbvi lower bound performs much worse blind lower bound two
heuristics satia lave aems perform considerably worse terms return
lower bound
ebr lbi terms error bound reduction lower bound improvement aems
obtains best lower bounds hsvi bfs close second indicates aems effectively reduce true error heuristics
therefore guarantees better performance bi pomdp tends less efficient
aems hsvi bfs significantly better rtbss satia lave
aems slightly improve bounds case satia lave unable
increase blind lower bound explains obtains return
blind policy observe higher error bound reduction lower bound
improvement higher average discounted return usually confirms intuition guiding search minimize error current belief bc good
strategy obtain better return
nodes reused terms percentage nodes reused aems hvsi bfs
generally obtain best scores allows maintain higher number
nodes trees could partly explain outperform
heuristics terms return error bound reduction lower bound improvement note
rtbss reuse node tree store
tree memory consequence reuse percentage
online time finally observe aems requires less average online time per
action attain performance general lower average
online time means heuristic efficient finding optimal actions small amount
time running time rtbss determined chosen depth cannot stop
completing full lookahead search
summary overall see aems hsvi bfs obtain similar however
aems seems slightly better hsvi bfs provides better performance guarantees
lower error within shorter period time difference significant
may due small number observations environment case two
heuristics expand tree similar ways next section explore domain
many observations evaluate impact factor
lower performances three heuristics explained reasons
case bi pomdp due fact take account
observation probabilities pr z b discount factor heuristic value hence
tend expand fringe nodes affect significantly value current
belief satia lave poor performance case blind policy
explained fact fringe nodes maximize heuristic leaves
reached sequence move actions due deterministic nature move actions
pr z b actions whereas check actions pr z b initially
heuristic value fringe nodes reached move actions much higher error
reduced significantly never explores nodes check
actions robot follows blind policy moving east never checking
sampling rocks demonstrates importance restricting choice


fionline pomdps





v b





aems
aems
bipomdp
hsvibfs
satia


























time

figure evolution upper lower bounds rocksample

leaves explore reached sequence actions maximizing upper bound
done aems hsvi bfs bi pomdp case aems probably behaves
less efficiently term uses estimate probability certain action
optimal good approximation environment moreover aems
restrict exploration best solution graph probably suffers part
satia lave heuristic rtbss perform
well blind lower bound due short depth allowed search
tree required running time second action confirms
significantly better exhaustive search good heuristics guide search
long term error reduction online heuristic search
compare long term performance different heuristics let run
offline mode initial belief state environment log changes lower
upper bound values initial belief state seconds initial lower
upper bounds provided blind policy qmdp respectively see
figure satia lave aems bi pomdp efficient hsvi bfs
aems reducing error bounds one interesting thing note
upper bound tends decrease slowly continuously whereas lower bound often
increases stepwise manner believe due fact upper bound
much tighter lower bound observe error bound reduction
happens first seconds search confirms nodes expanded earlier
tree much impact error bc expanded far
tree e g hundreds seconds important support online
opposed offline methods










average discounted return

average discounted return

ross pineau paquet chaib draa



aems
hsvibfs
bipomdp

























aems blind
aems pbvi
aems pbvi



online time









online time

figure comparison return figure comparison return
function online time
function online time
rocksample different
rocksample different
online methods
offline lower bounds

influence offline online time
compare performance online approaches influenced available
online offline times allows us verify particular method better
available online time shorter longer whether increasing offline time could
beneficial
consider three approaches shown best overall performance far bipomdp hsvi bfs aems compare average discounted return function online time constraint per action experiments run rocksample
states actions observations following online time constraints
vary offline time used different lower
bounds blind policy pbvi belief points pbvi belief points taking
respectively upper bound used qmdp cases
obtained intel xeon ghz processor
figure observe aems fares significantly better hsvi bfs
bi pomdp short time constraints time constraint increases aems
hsvi bfs performs similarly significant statistical difference notice
performance bi pomdp stops improving second time
explained fact take account observation probabilities
pr z b discount factor search tree grows bigger fringe
nodes small probability reached future becomes
important take probabilities account order improve performance
otherwise observe case bi pomdp expanded nodes affect
quality solution found
figure observe increasing offline time beneficial effect mostly
short real time constraints online time available


fionline pomdps

difference performances aems blind lower bound aems
pbvi becomes insignificant however online time constraints smaller one
second difference performance large intuitively short real time
constraints enough time expand lot nodes
policy found relies much bounds computed offline hand
longer time constraints enough time significantly improve bounds
computed offline thus policy found rely much offline bounds
fieldvisionrocksample
seems presented thus far hsvi bfs aems comparable
performance standard domains note however environments
small observation sets assuming observations zero probability removed
believe aems especially well suited domains large observation spaces however
standard literature therefore consider modified
version rocksample environment called fieldvisionrocksample ross chaib draa
observation space size exponential number rocks
fieldvisionrocksample fvrs differs rocksample
way robot able perceive rocks environment recall
rocksample agent check action specific rock observe state
noisy sensor fvrs robot observes state rocks
noisy sensor action conducted environment consequently
eliminates use check actions remaining actions robot include
four move actions north east south west sample action robot
perceive rock good bad thus observation space size k
instance k rocks rocksample efficiency sensor
defined parameter distance rock
half efficiency distance assume sensors observations independent rock
fvrs partial observability environment directly proportional
parameter increases sensor becomes accurate uncertainty
state environment decreases value defined different instances
rocksample work smith simmons high fvrs
especially bigger instances rocksample making almost completely observable
consequently define value different instances fieldvisionrocksample according size grid n considering fact
p n n grid
largest possible distance rock robot n seems reasonable distance probability observing real state rock
close
p remain partially observable consequently define
n
obtain fvrs domain run starting rock
configurations times e runs k different joint rock states
initial belief state runs corresponds probability
rock good well known initial position robot


fiross pineau paquet chaib draa

heuristic
rtbss
aems
satia lave
hsvi bfs
aems
bi pomdp
rtbss
bi pomdp
satia lave
aems
aems
hsvi bfs

belief
nodes
return
ebr
lbi
nodes
reused
fvrs blind return time ms






























fvrs blind return time ms































online
time ms



























table comparison different search heuristics different instances fieldvisionrocksample environment

real time performance online search
table present confidence intervals mean metrics interest
consider two instances environment fvrs states actions observations fvrs states actions observations cases use
qmdp upper bound blind lower bound real time constraints second per
action
return terms return observe clear winner bi pomdp performs surpringly well fvrs significantly worse aems hsvi bfs fvrs
hand aems significantly better hsvi bfs fvrs
get similar performances fvrs satia lave performs better environment rocksample likely due fact transitions belief
space longer deterministic case move actions rocksample
fvrs observe even rtbss given seconds per action
perform two step lookahead performance worse heuristic search
methods clearly shows expanding observations equally search
good strategy many observations negligible impact current
decision
ebr lbi terms error bound reduction lower bound improvement observe aems performs much better hsvi bfs fvrs significantly
better fvrs hand bi pomdp obtains similar aems
fvrs significantly worse terms ebr lbi fvrs
suggests aems consistently effective reducing error even environments
large branching factors
nodes reused percentage belief nodes reused much lower fvrs due
much higher branching factor observe hsvi bfs best reuse percentage


fionline pomdps















aems
aems
bipomdp
hsvibfs
satia

v b

v b





aems
aems
bipomdp
hsvibfs
satia



































time








time









figure evolution upper lower figure evolution upper lower
bounds fieldvisionrocksambounds fieldvisionrocksample
ple

environments however significantly higher aems methods
reuse significantly larger portion tree methods confirms
two methods able guide search towards likely beliefs
long term error reduction online heuristic search
overall table confirms consistent performance hsvi bfs aems
difference heuristics modest considering complexity environment
may due fact enough time expand
significant number nodes within second long term analysis bounds evolution
figures confirms observe figures lower bound converges
slightly rapidly aems heuristics aems heuristic
performs well long run seems second best heuristic
satia lave far behind hand hsvi bfs heuristic far
worse rocksample seems part due fact
heuristic takes time next node expand others thus
explores fewer belief states

discussion
previous sections presented evaluated several online pomdp
discuss important issues arise applying online methods practice summarize
advantages disadvantages help researchers decide whether
online good solving given


fiross pineau paquet chaib draa

lower upper bound selection
online combined many valid lower upper bounds however
properties bounds satisfy online search perform efficiently practice one desired properties lower upper
bound functions

property states b l b


p monotone monotone
maxaa rb b pzz pr z b l b z lower bound b u b
maxaa rb b zz pr z b u b z upper bound property
guarantees certain fringe node expanded lower bound non decreasing
upper bound non increasing sufficient guarantee error bound
ut b lt b b non increasing expansion b error bound
given value root belief state bc cannot worse
error bound defined initial bounds given note however monotonicity
necessary aems converge optimal solution shown previous work ross
et al boundedness sufficient
improving bounds time
mentioned survey online one drawback many online approaches store improvements made offline bounds
online search belief state encountered computations need performed restarting offline bounds trivial way
improve maintain large hashtable database belief states
improved lower upper bounds previous search associated
bounds however many drawbacks first every time want
evaluate lower upper bound fringe belief search hashtable needs
performed check better bounds available may require significant
time hashtable large e g millions beliefs furthermore experiments conducted
rtdp bel large domains rocksample shown process
usually runs memory e requires gb good performance
achieved requires several thousands episodes performing well paquet
authors rtbss tried combining search rtdpbel preserve improvements made search paquet
combination usually performed better learned faster rtdp bel alone found
domains thousand episodes still required improvement
seen terms return hence point updates offline bounds tend
useful large domains task accomplish repeated large number
times
better strategy improve lower bound might save time perform
vector updates beliefs expanded search offline
lower bound improves time updates advantage improving lower
bound whole belief space instead single belief state however
time consuming especially large domains hence need act within short
time constraints infeasible however several seconds time
available per action might advantageous use time perform
vector updates rather use available time search tree good


fionline pomdps

idea would perform vector updates subset beliefs search tree
lower bound improves
factored pomdp representations
efficiency online relies heavily ability quickly compute b z
pr z b must computed evey belief state search tree
factored pomdp representations effective way reduce time complexity computing quantities since environments large state spaces structured
described sets features obtaining factored representation complex systems
issue cases however domains significant dependencies
state features may useful use proposed boyen koller
poupart approximate factored representations features
independent minimal degradation solution quality upper
lower bounds might hold anymore computed approximate factored
representation usually may still yield good practice
handling graph structure
mentioned general tree search used online
duplicate belief states whenever multiple paths leading posterior
belief current belief bc greatly simplifies complexity related updating
values ancestor nodes reduces complexity related finding
best fringe node expand technique section valid
trees disadvantage tree structure inevitably computations
redundant potentially expand subtree every
duplicate belief avoid could use lao proposed hansen
zilberstein extension ao handle generic graph structure including
cyclic graphs expansion runs value policy iteration
convergence among ancestor nodes order update values
heuristics surveyed section generalized guide best first search
handle graph structure lao first thing notice
graph fringe node reached multiple paths error contributes multiple
times error value bc error contribution perspective heuristic
value fringe node sum heuristic values paths reaching
instance case aems heuristic notation defined
section global heuristic value given fringe node b current belief state
bc graph g computed follows
hg bc b u b l b

x

h pr h bc g



hhg bc b

notice cyclic graphs infinitely many paths hg bc b
case could use dynamic programming estimate heuristic value
solving hg bc b fringe nodes b graph g require lot time
practice especially many fringe nodes experimented
method section however would practical use heuristic could


fiross pineau paquet chaib draa

alternative way determine best fringe node without computing hg bc b separately
fringe node b performing exhaustive search fringe nodes
online vs offline time
one important aspect determining efficiency applicability online
amount time available execution course often taskdependent real time robot navigation amount time may
short e g second per action hand tasks portfolio
management acting every second necessary several minutes could easily
taken plan stock buying selling action seen experiments
shorter available online time greater importance good
offline value function start case often necessary reserve sufficient
time compute good offline policy time available online
influence offline value function becomes negligible rough offline
value function sufficient obtain good performance best trade online
offline time often depends large branching factor
z large computing successor belief states takes long time online
time required achieve significant improvement offline value function
however small online time second per action may sufficient
perform near optimally even rough offline value function
advantages disadvantages online
discuss advantages disadvantages online general
advantages
online combined offline solving assuming
provides lower bound upper bound v improve quality
policy found offline
online require little offline computation executable
environment perform well even loose bounds quick
compute
online methods exploit knowledge current belief focus computation
relevant future beliefs current decision scale well
large action observation spaces
anytime online methods applicable real time environments
stopped whenever time runs still provide best solution found
far
disadvantages
branching factor depends number actions observations thus
many observations actions might impossible search deep


fionline pomdps

enough provide significant improvement offline policy cases sampling methods designed reduce branching factor could useful
cannot guarantee lower upper bounds still valid sampling
used guarantee valid high probability given enough
samples drawn
online store improvements made offline policy
online search plan bounds time
environment restarted time available could advantageous add
vector updates belief states explored tree offline bounds
improve time

conclusion
pomdps provide rich elegant framework stochastic partially observable domains however time complexity major issue preventing
application complex real world systems thoroughly surveys existing online key techniques approximations used solve pomdps
efficiently empirically compare online approaches several pomdp domains different metrics average discounted return average error bound reduction
average lower bound improvement different lower upper bounds pbvi
blind fib qmdp
empirical observe heuristic search methods namely
aems hsvi bfs obtain good performances even domains large branching factors large state spaces two methods similar perform well
orient search towards nodes improve current approximate
value function quickly possible e belief nodes largest error
likely reached future promising actions however environments
large branching factors may time expand nodes turn
hence would interesting develop approximations reduce branching
factor cases
conclusion believe online approaches important role play
improving scalability pomdp solution methods good example succesful
applications rtbss robocuprescue simulation paquet et al
environment challenging state space orders magnitude
beyond scope current offline remain important obtain
tight lower upper bounds value function interesting question whether
online offline approaches better improve kinds approaches
synergy exploited solve complex real world

acknowledgments
supported natural sciences engineering council canada
fonds quebecois de la recherche sur la nature et les technologies would
thank anonymous reviewers helpful comments suggestions


fiross pineau paquet chaib draa

references
astrom k j optimal control markov decision processes incomplete state
estimation journal mathematical analysis applications
barto g bradtke j singhe p learning act real time dynamic
programming artificial intelligence
bellman r dynamic programming princeton university press princeton nj
usa
bertsekas p castanon rollout stochastic scheduling
journal heuristics
boyen x koller tractable inference complex stochastic processes
proceedings fourteenth conference uncertainty artificial intelligence
uai pp
braziunas boutilier c stochastic local search pomdp controllers
nineteenth national conference artificial intelligence aaai pp
cassandra littman l zhang n l incremental pruning simple fast
exact method partially observable markov decision processes proceedings
thirteenth conference uncertainty artificial intelligence uai pp
chang h givan r chong e k p parallel rollout online solution
partially observable markov decision processes discrete event dynamic systems

geffner h bonet b solving large pomdps real time dynamic programming proceedings fall aaai symposium pomdps pp
hansen e solving pomdps searching policy space fourteenth conference uncertainty artificial intelligence uai pp
hansen e zilberstein lao heuristic search finds
solutions loops artificial intelligence
hauskrecht value function approximations partially observable markov
decision processes journal artificial intelligence
kaelbling l p littman l cassandra r acting
partially observable stochastic domains artificial intelligence
kearns j mansour ng sparse sampling nearoptimal large markov decision processes proceedings sixteenth
international joint conference artificial intelligence ijcai pp
koenig agent centered search ai magazine
littman l sequential decision making ph thesis brown
university
littman l cassandra r kaelbling l p learning policies partially observable environments scaling proceedings th international
conference machine learning icml pp


fionline pomdps

lovejoy w computationally feasible bounds pomdps operations

madani hanks condon undecidability probabilistic
infinite horizon partially observable markov decision proceedings
sixteenth national conference artificial intelligence aaai pp
mcallester singh approximate factored pomdps belief state simplification proceedings th annual conference uncertainty
artificial intelligence uai pp
monahan g e survey partially observable markov decision processes theory
management science
nilsson n principles artificial intelligence tioga publishing
papadimitriou c tsitsiklis j n complexity markov decision processes
mathematics operations
paquet distributed decision making task coordination dynamic uncertain real time multiagent environments ph thesis laval university
paquet chaib draa b ross hybrid pomdp proceedings
workshop multi agent sequential decision making uncertain domains
msdm pp
paquet tobin l chaib draa b online pomdp complex
multiagent environments proceedings fourth international joint conference
autonomous agents multi agent systems aamas pp
pineau j gordon g thrun point value iteration anytime pomdps proceedings international joint conference artificial
intelligence ijcai pp
pineau j gordon g thrun anytime point approximations large
pomdps journal artificial intelligence
pineau j tractable uncertainty exploiting structure ph thesis
carnegie mellon university
poupart p exploiting structure efficiently solve large scale partially observable
markov decision processes ph thesis university toronto
poupart p boutilier c bounded finite state controllers advances neural
information processing systems nips
puterman l markov decision processes discrete stochastic dynamic programming john wiley sons inc
ross chaib draa b aems anytime online search approximate policy refinement large pomdps proceedings th international
joint conference artificial intelligence ijcai pp
ross pineau j chaib draa b theoretical analysis heuristic search
methods online pomdps advances neural information processing systems
nips


fiross pineau paquet chaib draa

satia j k lave r e markovian decision processes probabilistic observation states management science
shani g brafman r shimony adaptation changing stochastic environments online pomdp policy learning proceedings workshop
reinforcement learning non stationary environments ecml pp
smallwood r sondik e j optimal control partially observable
markov processes finite horizon operations
smith simmons r heuristic search value iteration pomdps proceedings th conference uncertainty artificial intelligence uai pp

smith simmons r point pomdp improved analysis
implementation proceedings th conference uncertainty artificial
intelligence uai pp
sondik e j optimal control partially observable markov processes ph
thesis stanford university
sondik e j optimal control partially observable markov processes
infinite horizon discounted costs operations
spaan j vlassis n point pomdp robot
proceedings ieee international conference robotics automation
icra pp
spaan j vlassis n perseus randomized point value iteration
pomdps journal artificial intelligence
vlassis n spaan j fast point pomdps
benelearn proceedings annual machine learning conference belgium
netherlands pp
washington r bi pomdp bounded incremental partially observable markov
model proceedings th european conference pp

zhang n l zhang w speeding convergence value iteration partially observable markov decision processes journal artificial intelligence





