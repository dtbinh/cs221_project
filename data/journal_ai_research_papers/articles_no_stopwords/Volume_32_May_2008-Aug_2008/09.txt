Journal Artificial Intelligence Research 32 (2008) 419 - 452

Submitted 11/07; published 06/08

Dynamic Control Real-Time Heuristic Search
Vadim Bulitko

BULITKO @ UALBERTA . CA

Department Computing Science, University Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Mitja Lustrek

MITJA . LUSTREK @ IJS . SI

Department Intelligent Systems, Jozef Stefan Institute
Jamova 39, 1000 Ljubljana, SLOVENIA

Jonathan Schaeffer

JONATHAN @ CS . UALBERTA . CA

Department Computing Science, University Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Yngvi Bjornsson

YNGVI @ RU .

School Computer Science, Reykjavik University
Kringlan 1, IS-103 Reykjavik, ICELAND

Sverrir Sigmundarson

SVERRIR . SIGMUNDARSON @ LANDSBANKI .

Landsbanki London Branch, Beaufort House,
15 St Botolph Street, London EC3A 7QR, GREAT BRITAIN

Abstract
Real-time heuristic search challenging type agent-centered search agents
planning time per action bounded constant independent problem size. common problem
imposes restrictions pathfinding modern computer games large number
units must plan paths simultaneously large maps. Common search algorithms (e.g., A*,
IDA*, D*, ARA*, AD*) inherently real-time may lose completeness constant
bound imposed per-action planning time. Real-time search algorithms retain completeness
frequently produce unacceptably suboptimal solutions. paper, extend classic
modern real-time search algorithms automated mechanism dynamic depth subgoal
selection. new algorithms remain real-time complete. large computer game maps,
find paths within 7% optimal average expanding roughly single state per action.
nearly three-fold improvement suboptimality existing state-of-the-art algorithms
and, time, 15-fold improvement amount planning per action.

1. Introduction
paper study problem agent-centered real-time heuristic search (Koenig, 2001).
distinctive property search agent must repeatedly plan execute actions
within constant time interval independent size problem solved.
restriction severely limits range applicable heuristic search algorithms. instance, static
search algorithms A* (Hart, Nilsson, & Raphael, 1968) IDA* (Korf, 1985), re-planning
algorithms D* (Stenz, 1995), anytime algorithms ARA* (Likhachev, Gordon, &
Thrun, 2004) anytime re-planning algorithms AD* (Likhachev, Ferguson, Gordon,
Stentz, & Thrun, 2005) cannot guarantee constant bound planning time per action. LRTA*
c
2008
AI Access Foundation. rights reserved.

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

can, potentially low solution quality due need fill heuristic depressions (Korf,
1990; Ishida, 1992).
motivating example, consider autonomous surveillance aircraft context disaster response (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada, 1999).
surveying disaster site, locating victims, assessing damage, aircraft ordered fly
particular location. Radio interference may make remote control unreliable thereby requiring
certain degree autonomy aircraft using AI. task presents two challenges. First,
due flight dynamics, AI must control aircraft real time, producing minimum number
actions per second. Second, aircraft needs reach target location quickly due limited
fuel supply need find rescue potential victims promptly.
study simplified version problem captures two AI challenges abstracting away robot-specific details. Specifically, line work real-time heuristic
search (e.g., Furcy & Koenig, 2000; Shimbo & Ishida, 2003; Koenig, 2004; Botea, Muller, & Schaeffer, 2004; Hernandez & Meseguer, 2005a, 2005b; Likhachev & Koenig, 2005; Sigmundarson &
Bjornsson, 2006; Koenig & Likhachev, 2006) consider agent finite search graph
task traveling path current state given goal state. Within context measure
amount planning agent conducts per action length path traveled
start goal locations. two measures antagonistic reducing amount planning per action leads suboptimal actions results longer paths. Conversely, shorter paths
require better actions obtained larger planning effort per action.
use navigation grid world maps derived computer games testbed. games,
agent tasked go location map current location. Examples include
real-time strategy games (e.g., Blizzard, 2002), first-person shooters (e.g., id Software, 1993),
role-playing games (e.g., BioWare Corp., 1998). Size complexity game maps well
number simultaneously moving units maps continues increase every new generation games. Nevertheless, game unit agent must react quickly users command
regardless maps size complexity. Consequently, game companies impose time-peraction limit pathfinding algorithms. instance, Bioware Corp., major game company
collaborate with, sets limit 1-3 ms units computing paths time.
Search algorithms produce entire solution agent takes first action (e.g., A*
Hart et al., 1968) lead increasing action delays map size increases. Numerous optimizations
suggested remedy problems decrease delays (for recent example deployed forthcoming computer game refer Sturtevant, 2007). Real-time search addresses
problem fundamentally different way. Instead computing complete, possibly abstract, solution first action taken, real-time search algorithms compute (or plan)
first actions agent take. usually done conducting lookahead search fixed
depth (also known search horizon, search depth lookahead depth) around agents
current state using heuristic (i.e., estimate remaining travel cost) select next
actions. actions taken planning-execution cycle repeats (e.g., Korf, 1990).
Since goal state reached local searches, agent runs risks heading
dead end or, generally, selecting suboptimal actions. address problem, real-time
heuristic search algorithms update (or learn) heuristic function experience. existing
algorithms constant amount planning (i.e., lookahead search) per action. result,
tend waste CPU cycles heuristic function fairly accurate and, conversely, plan
enough heuristic function particularly inaccurate. Additionally, compute heuris420

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

tic respect distant global goal state put unrealistic requirements heuristic
accuracy demonstrate paper.
paper address problems making following three contributions. First,
propose two ways selecting lookahead search depth dynamically, per action basis. Second,
propose way selecting intermediate subgoals per action basis. Third, apply
extensions classic LRTA* (Korf, 1990) state-of-the-art real-time PR LRTS (Bulitko,
Sturtevant, Lu, & Yau, 2007) demonstrate improvements performance. resulting
algorithms new state art real-time search. illustrate, large computer game
maps new algorithms find paths within 7% optimal expanding single state
action. comparison, previous state-of-the-art, PR LRTS, 15 times slower per
action finding paths two three times suboptimal. Furthermore,
dynamically controlled LRTA* PR LRTS one two orders magnitude faster per action
A*, weighted A* state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro,
2005). Finally, unlike A* modern extensions used games, new algorithms provably
real-time slow maps become larger.
rest paper organized follows. Section 2 formulate problem real-time
heuristic search show core LRTA* algorithm extended dynamic lookahead
subgoal selection. Section 3 analyzes related research. Section 4 provides intuition dynamic
control search. Section 5 describe two approaches dynamic lookahead selection: one
based induction decision-tree classifiers (Section 5.1) one based precomputing depth
table using state abstraction (Section 5.2). Section 6 present approach selecting subgoals
dynamically. Section 7 evaluates efficiency extensions domain pathfinding.
conclude discussion applicability new approach general planning.
paper extends conference publication (Bulitko, Bjornsson, Lustrek, Schaeffer, & Sigmundarson, 2007) new set features decision tree approach, new way selecting
subgoals, additional real-time heuristic search algorithm (PR LRTA*) extended dynamic
control, numerous additional experiments detailed presentation.

2. Problem Formulation
define heuristic search problem directed graph containing finite set states weighted
edges, single state designated goal state. every time step, search agent single
current state, vertex search graph, takes action traversing out-edge current
state. edge positive cost associated it. total cost edges traversed agent
start state arrives goal state called solution cost. require algorithms
complete produce path start goal finite amount time path exists.
order guarantee completeness real-time heuristic search make assumption safe
explorability search problems. Namely, costs finite goal state reachable
state agent possibly reach start state.
Formally, algorithms discussed paper applicable heuristic search problem. keep presentation focused intuitive well afford large-scale empirical
evaluation, use particular type heuristic search problems, pathfinding grid worlds,
rest paper. However, discuss applicability new methods suggest
heuristic search problems Section 5.3 general planning problems Section 9.
421

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

computer-game map settings, states vacant square grid cells. cell connected
four cardinally (i.e., west, north, east, south) four diagonally neighboring cells. Outbound
edges vertex moves available corresponding cell rest paper

use terms action move interchangeably. edge costs 1 cardinal moves
2 diagonal moves. agent plans next action considering states local search space
surrounding current position. heuristic function (or simply heuristic) estimates (remaining)
travel cost state goal. used agent rank available actions select
promising one. paper consider admissible heuristic functions
overestimate actual remaining cost goal. agent modify heuristic function
state avoid getting stuck local minima heuristic function, well improve action
selection experience.
defining property real-time heuristic search amount planning agent
per action upper bound depend problem size. enforce property
setting real-time cut-off amount planning action. algorithm exceeds
cut-off discarded. Fast planning preferred guarantees agents quick reaction
new goal specification changes environment. measure mean planning time per action
terms CPU time well machine-independent measure number states expanded
planning. state called expanded successor states considered/generated
search. second performance measure study sub-optimality defined ratio
solution cost found agent minimum solution cost. Ratios close one indicate
near-optimal solutions.
core real-time heuristic search algorithms algorithm called Learning RealTime A* (LRTA*) (Korf, 1990). shown Figure 1 operates follows. long goal
state sglobal goal reached, algorithm interleaves planning execution lines 4 7.
generalized version added new step line 3 selecting search depth goal sgoal
individually execution step (the original algorithm uses fixed sglobal goal planning
searches). line 4, d-ply breadth-first search duplicate detection used find frontier states
precisely actions away current state s. frontier state s, value sum
cost shortest path s, denoted g(s, s), estimated cost shortest path
sgoal (i.e., heuristic value h(s, sgoal )). use standard path-max technique (Mero,
1984) deal possible inconsistencies heuristic function computing g + h values.
result, g + h values never decrease along branch lookahead tree. state
minimizes sum identified sfrontier line 5. heuristic value current state
updated line 6 (we keep separate heuristic tables different goals). Finally, take one step
towards promising frontier state sfrontier line 7.

3. Related Research
algorithms single-agent real-time heuristic search use fixed search depth, notable
exceptions. Russell Wefald (1991) proposed estimate utility expanding state use
control lookahead search on-line. one needs estimate likely additional search
change actions estimated value. Inaccuracies estimates overhead metalevel control led reasonable unexciting benefits combinatorial puzzle pathfinding.
additional problem relatively low branching factor combinatorial puzzles makes
difficult eliminate parts search space early on. problem likely occur grid422

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

LRTA*(sstart , sglobal goal )
1 sstart
2 6= sglobal goal
3
select search depth goal sgoal
4
expand successor states actions away, generating frontier
5
find frontier state sfrontier lowest g(s, sfrontier ) + h(sfrontier , sgoal )
6
update h(s, sgoal ) g(s, sfrontier ) + h(sfrontier , sgoal )
7
change one step towards sfrontier
8 end
Figure 1: LRTA* algorithm dynamic control.
based pathfinding. Finally, method adds substantial implementation complexity requires
non-trivial changes underlying search algorithm. contrast, approach search depth
selection easily interfaced real-time search algorithm search depth parameter
without modifying existing code.
Ishida (1992) observed LRTA*-style algorithms tend get trapped local minima
heuristic function, termed heuristic depressions. proposed remedy switch limited
A* search heuristic depression detected use results A* search
correct depression once. different approach two ways: first,
need mechanism decide switch real-time A* search thus avoid
need hand-tune control parameters Ishidas control module. Instead, employ automated
approach decide search horizon depth every action. Additionally, spend extra
time filling heuristic values within heuristic depression A* estimates.
Bulitko (2003a) showed optimal search depth selection highly beneficial realtime heuristic search. linked benefits avoiding so-called lookahead pathologies
deeper lookahead leads worse moves suggest practical way selecting lookahead depth dynamically. way proposed 2004 via use generalized definition
heuristic depressions (Bulitko, 2004). proposed algorithm extends search horizon incrementally search finds way depression. actions leading found
frontier state executed. cap search horizon depth set user. idea precomputing depth table heuristic values real-time pathfinding first suggested Lustrek
Bulitko (2006). paper extends work follows: (i) introduce intermediate goals,
(ii) propose alternative approach require map-specific pre-computation (iii)
extend evaluate state-of-the-art algorithm addition classic LRTA*.
long tradition search control two-player search. High-performance game-playing
programs games chess checkers rely extensively search decide actions
take. search performed strict real-time constraints programs typically
minutes seconds deliberating next action. Instead using fixed-depth lookahead strategy programs employ sophisticated search control mechanisms maximizing
quality action decisions within given time constraints. search control techniques
coarsely divided three main categories: move ordering, search extensions/reductions,
time allotment. One earlier works dynamic move ordering history heuristic technique (Schaeffer, 1989), recent attempts include work training neural networks (Kocsis, 2003). exist large variety techniques adjusting search horizon
423

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

different branches within game tree; interesting continuations explored deeply
less promising ones terminated prematurely. Whereas early techniques
static, research focus shifted towards dynamic control well using machine-learning
approaches automatic parameterization (Buro, 2000; Bjornsson & Marsland, 2003). best
knowledge, none techniques applied single-agent real-time search.

4. Intuition Dynamic Search Control
observed literature common heuristic functions uniformly inaccurate (Pearl, 1984). Namely, tend accurate closer goal state less accurate
farther away. intuition fact follows: heuristic functions usually ignore certain constraints search space. instance, Manhattan distance heuristic sliding tile puzzle
would perfectly accurate tiles could pass other. Likewise, Euclidian distance
map ignores obstacles. closer state goal fewer constraints heuristic function
likely ignore and, result, accurate (i.e., closer optimal solution cost)
heuristic likely be.
intuition motivates adaptive search control real-time heuristic search. First, heuristic values inaccurate, agent conduct deeper lookahead search compensate
inaccuracies maintain quality actions. Deeper lookaheads generally found
beneficial real-time heuristic search (Korf, 1990), though lookahead pathologies (i.e., detrimental
effects deeper lookaheads action quality) observed well (Bulitko, Li, Greiner, &
Levner, 2003; Bulitko, 2003b; Lustrek, 2005; Lustrek & Bulitko, 2006). illustration, consider
Figure 2. Every state map shaded according minimum lookahead depth
LRTA* agent use select optimal action. Darker shades correspond deeper lookahead
depths. Notice many areas bright white, indicating shallowest lookahead depth
one sufficient. use intuition first control mechanism: dynamic selection
lookahead depth Section 5.

Figure 2: partial grid world map computer game Baldurs Gate (BioWare Corp., 1998).
Shades grey indicate optimal search depth values white representing one ply.
Completely black cells impassable obstacles (e.g., walls).
424

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Dynamic search depth selection helps eliminate wasted computation switching shallower
lookahead heuristic function fairly accurate. Unfortunately, help
heuristic function grossly inaccurate. Instead, calls deep lookahead order select
optimal action. deep search tremendously increases planning time and, sometimes, leads
violating real-time cut-off planning time per move. address issue, Section 6
propose second control mechanism: dynamic selection subgoals. idea straightforward:
far goal leads grossly inaccurate heuristic values, let us move goal closer
agent, thereby improving heuristic accuracy. computing heuristic function
respect intermediate, thus nearby, goal opposed distant global goal
final destination agent. Since intermediate goal closer global goal, heuristic
values states around agent likely accurate thus search depth picked
first control mechanism likely shallower. agent gets intermediate goal,
next intermediate goal selected agent makes progress towards actual global goal.

5. Dynamic Search Depth Selection
First, define optimal search depth follows. (s, sglobal goal ) state pair, true optimal action (s, sglobal goal ) take edge lies optimal path sglobal goal (there
one optimal action). (s, sglobal goal ) known, run series progressively
deeper LRTA* searches state s. shallowest search depth yields (s, sglobal goal )
optimal search depth (s, sglobal goal ). may search depth forfeit LRTA*s real-time
property impractical compute. Thus, following subsections present two
different practical approaches approximating optimal search depth. equips LRTA*
dynamic search depth selection (i.e., realizing first part line 3 Figure 1). first
approach uses decision-tree classifier select search depth based features agents
current state recent history. second approach uses pre-computed depth database based
automatically built state abstraction.
5.1 Decision-Tree Classifier Approach
effective classifier needs input features useful predicting optimal search
depth, efficiently computable agent real time. features use
classifier selected compromise two considerations, well domain independent. features calculated based properties states agent recently
visited, well features gathered shallow pre-search agents current state. Example
features are: distance state agent n steps ago, estimate distance
agents goal, number states visited pre-search phase updated heuristics.
Appendix features listed rationale behind explained.
classifier predicts optimal search depth current state. optimal depth
shallowest search depth returns optimal action. training classifier must thus label
training states optimal search depths. However, avoid pre-computing optimal actions,
make simplifying assumption deeper search always yields better action. Consequently,
training phase agent first conducts lookahead search pre-defined maximum depth, dmax ,
derive optimal action (under assumption). choice maximum depth domain
dependent would typically set largest depth still guarantees search return
within acceptable real-time requirement task hand. series progressively
425

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

shallower searches performed determine shallowest search depth, dDT , still returns
optimal action. process, given depth action returned differs
optimal action, progression stopped. enforces depths dDT dmax
agree best action. important improving overall robustness classification,
classifier must generalize large set states. depth dDT set class label
vector features describing current state.
classifier choosing lookahead depth, LRTA* augmented
(line 3 Figure 1). overhead using classifier consists time required collecting
features running classifier. overhead negligible classifier
implemented handful nested conditional statements. Collecting features takes
somewhat time but, careful implementation, overhead made negligible
well. Indeed, four history-based features efficiently computed small constant time,
keeping lookahead depth pre-search small (e.g., one two) overhead collecting
pre-search features usually dwarfed time planning phase (i.e., lookahead search)
takes. process gathering training data building classifier carried off-line
time overhead thus lesser concern.
5.2 Pattern Database Approach
nave approach would precompute optimal depth (s, sgoal ) state pair.
two problems approach. First, (s, sgoal ) priori upper-bounded independently
map size, thereby forfeiting LRTA*s real-time property. Second, pre-computing (s, sgoal )
(s, sgoal ) pairs (s, sgoal ) states on, instance, 512 512 cell computer game map
prohibitive time space complexity. solve first problem capping (s, sgoal )
fixed constant c 1 (henceforth called cap). solve second problem using automatically built abstraction original search space. entire map partitioned regions (or
abstract states) single search depth value pre-computed pair abstract states. run-time single search depth value shared children abstract state pair (Figure 3).
search depth values stored table refer pattern database PDB
short. past, pattern databases used store approximate heuristic values (Culberson
& Schaeffer, 1998) important board features (Schaeffer, 2000). work appears first
use pattern databases store search depth values.
Computing search depths abstract states speeds pre-computation reduces memory
overhead (both important considerations commercial computer games). paper use
previously published clique abstraction (Sturtevant & Buro, 2005). preserves overall topology
map requires storing abstraction links explicitly.1 clique abstraction works
finding fully connected subgraphs (i.e., cliques) original graph abstracting states
within clique single abstract state. Two abstract states connected abstract
action single original action leads state first clique
state single clique (Figure 4). costs abstract actions computed Euclidean
distances average coordinates states cliques.
typical grid world computer-game maps, single application clique abstraction reduces
number states factor two four. average, abstraction level five (i.e.,
five applications abstraction procedure), region contains one hundred original
1. alternative use regular rectangular tiles (e.g., Botea et al., 2004).

426

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Figure 3: single optimal lookahead depth value shared among children abstract state.
memory-efficient approximation true per-ground-state values Figure 2.

Level 0 (original graph)

Level 1

Level 2

Figure 4: Two iterations clique abstraction procedure produce two abstract levels
ground-level search graph.
(or ground-level) states. Thus, single search depth value shared among ten thousand
state pairs. result, five-level clique abstraction yields four orders magnitude reduction
memory two orders magnitude reduction pre-computation time (as analyzed later).
downside, higher levels abstraction effectively make search depth selection less
less dynamic depth value shared among progressively states. abstraction
level pattern database control parameter trades pre-computation time pattern
database size on-line performance algorithm uses database.
Two alternatives storing optimal search depth store optimal action optimal
heuristic value. combination abstraction real-time search precludes them. Indeed,
sharing optimal action computed single ground-level representative abstract region
among states region may cause agent run wall (Figure 5, left). Likewise,
sharing single heuristic value among states region leaves agent without sense
427

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON


G

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84

5.84



5.84

5.84

5.84

G

Figure 5: Goal shown G, agent A. Abstract states four tiles separated dashed lines.
Diamonds indicate representative states tile. Left: Optimal actions shown
representative abstract tile; applying optimal action agents tile
agents current location leads wall. Right: Optimal heuristic value (h ) lower
left tiles representative state (5.84) shared among states tile. result,
agent preference among three legal actions shown.
direction states vicinity would look equally close goal (Figure 5, right).
contrast sharing heuristic value among states within abstract state (known pattern)
using optimal non-real-time search algorithms A* IDA* (Culberson & Schaeffer,
1996). case real-time search, agents using either alternative guaranteed reach
goal, let alone minimize travel. contrary, sharing search depth among number
ground-level states safe LRTA* complete search depth.
compute single depth table per map off-line (Figure 6). line 1 state space abstracted ` times. Lines 2 7 iterate pairs abstract states. pair (s0 , s0goal ),
representative ground-level states sgoal (i.e., ground-level states closest centroids regions) picked optimal search depth value calculated them. this, Dijkstras
algorithm (Dijkstra, 1959) run ground-level search space (V, E) compute true
minimal distances state sgoal . distances known successors s,
optimal action (s, sgoal ) computed greedily. optimal search depth (s, sgoal )
computed previously described capped c (line 5). resulting value stored pair
abstract states (s0 , s0goal ) line 6. Figures 2 3 show optimal search depth values single
goal state grid world game map without abstraction respectively.
run-time, LRTA* agent going state state sgoal takes search depth
depth table value pair (s0 , s0goal ), s0 s0goal images sgoal `-level
abstraction. additional run-time complexity minimal s0 , s0goal , d(s0 , s0goal ) computed
small constant-time overhead action.
building pattern database Dijkstras algorithm run V` times2 graph (V, E)
time complexity O(V` (V log V + E)) sparse graphs (i.e., E = O(V )). optimal
search depth computed V`2 times. time, c LRTA* invocations total
2. brevity, use V E mean sets vertices/edges sizes (i.e., |V | |E|).

428

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

BuildPatternDatabase(V, E, c, `)
1 apply abstraction procedure ` times (V, E) compute abstract space S` = (V` , E` )
2 pair states (s0 , s0goal ) V` V`
3
select V representative s0 V`
4
select sgoal V representative s0goal V`
5
compute c-capped optimal search depth value state respect goal sgoal
6
store capped pair (s0 , s0goal )
7 end
Figure 6: Pattern database construction.
complexity O(bc ) b maximum degree V . Thus, overall time complexity
O(V` (V log V + E + V` bc )). space complexity lower store optimal search depth
values pairs abstract states: O(V`2 ). Table 1 lists bounds sparse graphs.
Table 1: Reduction complexity due state abstraction.

time
space

abstraction
O(V 2 log V )
O(V 2 )

`-level abstraction
O(V` V log V )
O(V`2 )

reduction
V /V`
(V /V` )2

5.3 Discussion Two Approaches
Selecting search depth pattern database two advantages. First, search depth values
stored pair abstract states optimal non-abstract representatives, unless either
value capped states local search space visited heuristic values modified. (conditional) optimality contrast classifier approach
optimal actions ever computed deeper searches merely assumed lead
better action. assumption always hold phenomenon known lookahead pathology, found abstract graphs (Bulitko et al., 2003) well grid-based pathfinding (Lustrek &
Bulitko, 2006). second advantage need features current state, recent
history pre-search. search depth retrieved depth table simply basis
current states identifier, coordinates.
decision-tree classifier approach two advantages depth table approach. First,
classifier training need happen search space agent operates in.
long training maps used collect features build decision tree representative
run-time maps, approach run never-before-seen maps (e.g., user-created maps
computer game). Second, much smaller memory overhead method
classifier specified procedurally pattern database needs loaded memory.
Note approaches assume structure heuristic search problem
hand. Namely, pattern database approach shares single search depth value across region
states. works effectively states region indeed lookahead
depth best them. abstraction mechanism forms regions basis search
graph structure, regard search depth. empirical study show, clique abstraction
429

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

seems right choice pathfinding. However, choice best abstraction technique
general heuristic search problem open question.
Similarly, decision-tree approach assumes states share similar feature values
share best search depth value. appears hold large extent pathfinding domain
feature selection arbitrary heuristic search problems open question well.

6. Dynamic Goal Selection
two methods described allow agent select individual search depth state.
However, original LRTA*, heuristic still computed respect global goal
sgoal . illustrate: Figure 7, map partitioned eight abstract states (in case, 4 4
square tiles) whose representative states shown diamonds (18). optimal path
agent (A) goal (G) shown well. straight-line distance heuristic ignore
wall agent goal lead agent south-western direction. LRTA*
search depth 11 higher needed produce optimal action (such ). Thus,
cap value 11, agent left suboptimal action spend long time
horizontal wall raising heuristic values. Spending large amounts time corners
heuristic depressions primary weakness real-time heuristic search agents and,
example, remedied dynamic search depth selection due cap.

1

2

3

4



5

G

7

6

8

Figure 7: Goal shown G, agent A. Abstract states eight tiles separated dashed
lines. Diamonds indicate ground-level representative tile. optimal path
shown. Entry points path abstract states marked circles.

5a compute sintermediate goal goal (s, sgoal )
5b compute capped optimal search depth value respect sintermediate goal
6 store (d , sintermediate goal ) pair (s0 , s0goal )
Figure 8: Switching sgoal sintermediate goal ; replaces lines 56 Figure 6.
430

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Figure 9: three maps used experiments.
address issue, switch intermediate goals pattern-database construction well
on-line LRTA* operation. example Figure 7 compute heuristic around
respect intermediate goal marked double-border circle map. Consequently,
eleven times shallower search depth needed optimal action towards next abstract state
(right-most upper tile). approach replaces lines 5 - 6 Figure 6 Figure 8. line
5a, compute intermediate goal sintermediate goal ground-level state optimal path
sgoal enters next abstract state. entry points marked circles Figure 7.
compared entry states centroids abstract states intermediate goals (Bulitko et al., 2007)
found former superior terms algorithms performance. Note optimal path
easily available off-line run Dijkstras algorithm (Section 5.2).
intermediate goal computed, line 5b computes capped optimal search depth
respect intermediate goal sintermediate goal . depth computation done described
Section 5.2. search depth intermediate goal added pattern database
line 6. run-time, agent executes LRTA* stored search depth computes
heuristic h respect stored goal (i.e., sgoal set sintermediate goal line 3 Figure 1).
words, search depth agents goal selected dynamically, per action.
approach works heuristic functions used practice tend become accurate states closer goal state. Therefore, switching distant global goal nearby
intermediate goal makes heuristics around current state accurate leads shallower search depth necessary achieve optimal action. result, algorithm
run quickly shallower search per move search depth cap reached less
frequently therefore search depth values actually result optimal moves.

7. Empirical Evaluation
section presents results empirical evaluation algorithms dynamic control search
depth goals classic state-of-the-art published algorithms. algorithms avoid reexpanding states planning move via transposition table. report sub-optimality
solution found average amount computation per action, expressed number
states expanded. believe algorithms implemented way single
expanded state takes amount time. case testbed code
optimized other. reason avoid clutter, report CPU times
Section 7.7. used fixed tie-breaking scheme real-time algorithms.
431

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

use grid world maps computer game testbed. Game maps provide realistic
challenging environment real-time search seen number recent publications (e.g., Nash, Daniel, & Felner, 2007; Hernandez & Meseguer, 2007). original maps
sized 161161 193193 cells (Figure 9). line Sturtevant Buro (2005) Sturtevant
Jansen (2007), experimented maps upscaled 512 512 closer size
maps used modern computer games. Note three maps depicted figure
outdoor-type maps, ran preliminary experiments indoor-type game maps (e.g., one
shown Figure 2). trends similar decided focus larger outdoor maps.
100 search problems defined three original size maps. start
goal locations chosen randomly, although constrained optimal solution paths cost
90 100 order generate difficult instances. upscaled maps 300
problems upscaled well. data point plots average 300 problems (3
maps 100 runs each). different legend entry used algorithm, multiple points
legend entry represent alternative parameter instantiation algorithm.
heuristic function used octile distance natural extension Manhattan distance maps
diagonal actions. enforce real-time constraint disqualified parameter settings
caused algorithm expand 1000 states move problem. points
excluded empirical evaluation. Maps known priori off-line order build
decision-tree classifiers pattern databases.
use following notation identify algorithms variants: AlgorithmName
(X, Y) X defined follows. X denotes search depth control: F fixed search
depth, DT search depth selected dynamically decision tree, ORACLE search depth
selected decision-tree oracle (see next section details) PDB search depth
selected dynamically pattern databases. denotes goal state selection: G heuristic
computed respect single global goal, PDB heuristic computed respect
intermediate goal pattern databases. instance, classic LRTA* LRTA* (F, G).
empirical evaluation organized eight parts follows. Section 7.1 describes six
algorithms compute heuristic respect global goal discusses performance.
Section 7.2 describes five algorithms use intermediate goals. Section 7.3 compares global
intermediate goals. Section 7.4 studies effects path-refinement without dynamic
control. Secton 7.5 pits new algorithms state-of-the-art real-time non-real-time
algorithms. provide algorithm selection guide different time limits planning per
move Section 7.6. Finally, Section 7.7 considers issue amortizing off-line pattern-database
build time on-line pathfinding.
7.1 Algorithms Global Goals
subsection describe following algorithms compute heuristic respect
single global goal (i.e., use intermediate goals):
1. LRTA* (F, G) Learning Real-Time A* (Korf, 1990). action conducts breadthfirst search fixed depth around agents current state. first move towards
best depth state taken heuristic agents previous state updated using
Korfs mini-min rule.3 used {4, 5, . . . , 20}.
3. Instead using LRTA* could used RTA*. experiments showed grid pathfinding
significant performance difference two search depth beyond one. Indeed deeper searches

432

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

2. LRTA* (DT, G) LRTA* search depth dynamically controlled decision
tree described Section 5.1. used following parameters: dmax {5, 10, 15, 20}
history trace length n = 60. building decision-tree classifier WEKA (Witten & Frank, 2005) pruning factor set 0.05 minimum number data items
per leaf 100 original size maps 25 upscaled ones. opposed learning
tailor-made classifier game map, single common decision-tree classifier built
based data collected maps (using 10-fold cross-validation). done
demonstrate ability classifier generalize across maps.
3. LRTA* (ORACLE, G) LRTA* search depth dynamically controlled
oracle. oracle always selects best search depth produce move given
LRTA* (F, G) fixed lookahead depth dmax (Bulitko et al., 2007). words,
oracle acts perfect decision-tree thus sets upper bound LRTA* (DT, G)
performance. oracle run dmax {5, 10, 15, 20}, original size
maps proved prohibitively expensive compute upscaled maps. Note
practical real-time algorithm used reference point experiments.
4. LRTA* (PDB, G) LRTA* search depth dynamically controlled
pattern database described Section 5.2. original size maps, used abstraction
level ` {0, 1, . . . , 5} depth cap c {10, 20, 30, 40, 50, 1000}. upscaled maps,
used abstraction level ` {3, 4, . . . , 7} depth cap c {20, 30, 40, 50, 80, 3000}.
Considering size maps, cap value 1000 3000 means virtually capless search.
5. K LRTA* (F, G) variant LRTA* proposed Koenig (2004). Unlike original
LRTA*, uses A*-shaped lookahead search space updates heuristic values states
within using Dijkstras algorithm.4 number states K LRTA* expands per move
took values: {10, 20, 30, 40, 100, 250, 500, 1000}.
6. P LRTA* (F, G) Prioritized LRTA* variant LRTA* proposed Rayner, Davison,
Bulitko, Anderson, Lu (2007). uses lookahead depth 1 moves. However,
every state whose heuristic value updated, neighbors put onto update queue,
sorted magnitude update. Thus, algorithm propagates heuristic function
updates space fashion Prioritized Sweeping (Moore & Atkeson, 1993).
control parameter (queue size) set {10, 20, 30, 40, 100, 250, 500, 1000} original
size maps {10, 20, 30, 40, 100, 250} upscaled maps.
Figure 10 evaluate performance new dynamic depth selection algorithms
original size maps. see decision-tree pattern-database approach improve
significantly upon LRTA* algorithm, expanding two three times fewer states generating
solutions comparable quality. Furthermore, perform par current state-of-the-art realtime search algorithms without abstraction, seen compared K LRTA* (F, G).
solutions generated acceptable quality domain (e.g., 50% suboptimal), even
expanding 100 states per action. interest decision-tree approach performs
likelihood multiple actions equally low g + h cost high, reducing distinction RTA*
LRTA*. using LRTA* agents learn repeated trials.
4. experimented A*-shaped lookahead new algorithms found inferior breadth-first lookahead deeper searches.

433

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Original size maps

Realtime cutoff: 1000

4
LRTA* (F, G)
LRTA* (ORACLE, G)
LRTA* (DT, G)
LRTA* (PDB, G)
P LRTA* (F, G)
K LRTA* (F, G)

Suboptimality (times)

3.5
3
2.5
2
1.5

1

0

100

200
300
400
Mean number states expanded per move

500

600

Figure 10: Global-goal algorithms original size maps.
quite close theoretical best case, seen compared LRTA* (ORACLE, G).
shows features use, although seemingly simplistic, good job predicting
appropriate search depth.
ran similar sets experiments upscaled maps. However, none global goal
algorithms generated solutions acceptable quality given real-time cut-off (the solutions
300 1700% suboptimal). experimental results upscaled maps provided
Appendix B. shows inherent limitations global goal approaches; large search
spaces cannot compete equal footing abstraction-based methods. brings us
intermediate goal selection methods.
7.2 Algorithms Intermediate Goals
section describe algorithms use intermediate goals search. best
knowledge, one previously published real-time heuristic search algorithm
so. Thus, compare new algorithms proposed paper. Given intermediate
goals increase performance algorithms significantly, present results
challenging upscaled maps. full roster algorithms used section follows:
1. PR LRTA* (F, G) Path Refinement Learning Real-Time Search (Bulitko et al., 2007).
algorithm two components: runs LRTA* fixed search depth global
goal abstract space (abstraction level ` clique abstraction hierarchy) refines
first move using corridor-constrained A* running original ground-level map.5
Constraining A* small set states, collectively called corridor Sturtevant Buro
5. algorithm actually called PR LRTS (Bulitko et al., 2007). Based findings Lustrek Bulitko (2006),
modified refine single abstract action order reduce susceptibility lookahead pathologies.
modification equivalent substituting LRTS component LRTA*. Hence, rest paper,
call PR LRTA*.

434

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

(2005) tunnel Furcy (2006), speeds makes real-time corridor size
independent map size (Bulitko, Sturtevant, & Kazakevich, 2005). heuristic
computed abstract space respect fixed global goal, A* component
computes path current state intermediate goal. qualifies PR LRTA*
enter section empirical evaluation. control parameters follows: abstraction
level ` {3, 4, . . . , 7}, LRTA* lookahead depth {1, 3, 5, 10, 15} LRTA* heuristic
weight {0.2, 0.4, 0.6, 1.0} ( imposed g line 5 Figure 1).
2. LRTA* (F, PDB) LRTA* fixed search depth uses pattern database select
intermediate goals. control parameters follows: abstraction level ` {3, 4, . . . , 7}
search depth {1, 2, . . . , 9, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30}.
3. LRTA* (PDB, PDB) LRTA* generalized dynamic search depth intermediate goal selection pattern databases presented Sections 5.2 6. control parameters follows: abstraction level ` {3, 4, . . . , 7} lookahead cap
c {20, 30, 40, 50, 80, 3000}.
4. PR LRTA* (PDB, G) PR LRTA* whose LRTA* component equipped dynamic search depth uses global (abstract) goal respect computes abstract heuristic. pattern database search depth constructed
abstraction level ` LRTA* component runs on, making component optimal lookahead cap allows. used abstraction level `
{3, 4, . . . , 7} lookahead cap c {5, 10, 15, 20, 1000}.
ran version PR LRTA* (PDB, G) pattern database constructed abstraction level `2 level ` LRTA* operates (Table 2). used (`, `2 )
{(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)}.
5. PR LRTA* (PDB, PDB) two-database version PR LRTA* (PDB, G)
except uses second database goal selection well depth selection. used
(`, `2 ) {(1, 3), (2, 4), (3, 5), (4, 6), (5, 7), (1, 4), (2, 6), (3, 7), (4, 8), (5, 9)} (Table 2).
Table 2: PR LRTA* (PDB, G PDB) uses LRTA* abstraction level ` define corridor within
refines path using A*. Dynamic depth (and goal) selection performed either
abstraction level ` `2 > `.
Abstraction level
`2
`
0

Single abstraction PR LRTA*(PDB,G)
abstract-level LRTA*
dynamic depth selection
corridor-constrained ground-level A*

Dual abstraction PR LRTA*(PDB,{G,PDB})
dynamic depth (and goal) selection
abstract-level LRTA*
corridor-constrained ground-level A*

pattern database algorithms presented stores depth value intermediate
ground-level goal pair abstract states. present performance results algorithms
intermediate goals Sections 7.37.6 analyze complexity pattern database
computation effects performance Section 7.7.
435

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Upscaled maps

Realtime cutoff: 10000

20
LRTA* (F, G)
LRTA* (F, PDB)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

200

400

600
800
1000
1200
Mean number states expanded per move

1400

1600

Figure 11: Effects intermediate goals: LRTA* (F, G) versus LRTA* (F, PDB).

7.3 Global versus Intermediate Goals
Sections 7.1 7.2 presented algorithms global intermediate goals respectively.
section compare algorithms across two groups. include LRTA* (PDB, G), increased
real-time cut-off 1000 10000 graphs section. start baseline LRTA* fixed lookahead. effects adding intermediate goal selection dramatic:
LRTA* intermediate goals (F, PDB) finds five times better solutions three orders
magnitude faster LRTA* global goals (F, G) (see Figure 11). believe
result octile distance heuristic substantially accurate around goal. Consequently,
LRTA* (F, PDB) benefiting much better heuristic function.
second experiment, equip versions dynamic search depth control compare LRTA* (PDB, G) LRTA* (PDB, PDB) Figure 12. performance gap less
dramatic: planning speed-up still around three orders magnitude, suboptimality
advantage went five two times. Again, note increase real-time
cut-off order magnitude get points plot.
Finally, evaluate beneficial: dynamic depth control dynamic goal control
comparing baseline LRTA* (F, G) LRTA* (PDB, G) LRTA* (F, PDB) Figure 13.
clear dynamic goal selection much stronger addition baseline LRTA* dynamic
search depth selection. Dynamic depth selection sometimes actually performs worse fixed
depth, evidenced data points LRTA* (F, G) line. happens primarily
high abstraction levels small caps. optimal lookahead depth computed high
abstraction level, depth value shared among many ground-level states. selected
depth value beneficial near entry point abstract state, abstract state
large, depth likely become inappropriate ground-level states away.
example, optimal depth entry point 1, worse moderate fixed depth
436

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Upscaled maps

Realtime cutoff: 10000
LRTA* (PDB, G)
LRTA* (PDB, PDB)

14

Suboptimality (times)

12
10
8
6
4
2
0

100

200

300
400
500
600
Mean number states expanded per move

700

800

Figure 12: Effects intermediate goals: LRTA* (PDB, G) versus LRTA* (PDB, PDB).
Upscaled maps

Realtime cutoff: 10000

20
LRTA* (F, G)
LRTA* (F, PDB)
LRTA* (PDB, G)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

200

400

600
800
1000
1200
Mean number states expanded per move

1400

1600

Figure 13: Dynamic search depth control versus dynamic goal control.

ground-level states far entry point. Small caps compound problem sometimes
preventing selection optimal depth even entry point.
shown plot, running (i.e., LRTA* (PDB, PDB)) leads marginal improvements. best parameterizations LRTA* (F, PDB) already expands
single state per move virtually times. Consequently, benefit adding dynamic
depth control slight improvement suboptimality next section.
437

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Upscaled maps

Realtime cutoff: 1000

1.5
LRTA* (F, PDB)
LRTA* (PDB, PDB)
PR LRTA* (F, G)
PR LRTA* (F, PDB)
PR LRTA* (PDB, PDB)
PR LRTA* (PDB, G)

Suboptimality (times)

1.4

1.3

1.2

1.1

1

0

5

10
15
Mean number states expanded per move

20

25

Figure 14: Effects path refinement: LRTA* versus PR LRTA*.
7.4 Effects Path Refinement
Path-refinement algorithms (denoted PR prefix) run learning real-time search (LRTA*)
abstract space refine path running A* ground level. Non-PR algorithm run
A* real-time search happens ground-level space. examine effects pathrefinement comparing LRTA* PR LRTA*. Note even statically controlled baseline
PR LRTA* (F, G) uses intermediate goals refining abstract actions. match using
dynamic intermediate goal selection LRTA*. Thus, compare four versions PR LRTA*: (F,
G), (PDB, G), (F, PDB) (PDB, PDB) two versions LRTA*: (F, PDB) (PDB, PDB).
results found Figure 14. sake clarity, show high performance area
capping number states expanded per move 25 suboptimality 1.5.
best parameterizations LRTA* find near-optimal solutions expanding one state
per move virtually times. astonishing performance one state expansion per
move corresponds search depth one fastest possible operation algorithm
framework. Thus, LRTA* (F, PDB) LRTA* (PDB, PDB) virtually unbeatable terms
planning time. hand, PR LRTA* incurs planning overhead due path-refinement
component (i.e., running corridor-constrained A*). result, PR LRTA* finds nearlyoptimal solutions incurs least five times higher planning cost per move. Dynamic control
PR LRTA* results moderate performance gains.
7.5 Comparison Existing State Art
Traditionally, computer games used A* pathfinding needs (Stout, 2000). map size
number simultaneously planning agents increase, game developers find even highly optimized
implementations A* insufficient. result, variants A* use state abstraction
used (Sturtevant, 2007). Another way speeding A* introduce weight computing travel
cost state. done f = g + h, 0 values 1 make agent
438

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

greedy (more weight put h) usually leads fewer states expanded price
suboptimal solutions. section, compare new algorithms weighted A* (Korf, 1993)
state-of-the-art Partial Refinement A* (PRA*) (Sturtevant & Buro, 2005). Note neither
algorithm real-time and, thus, planning times per move map-size specific. is,
larger maps, A*s PRA*s planning times per move increase algorithms compute
complete (abstract) path start goal states take first move. instance,
maps used PRA* expands 3454 states expensive move. Weighted A*
= 15 expands 40734 states classic A* expands 88138 states worst moves. Thus,
include two algorithms comparison effectively remove real-time cut-off.
results found Table 3. Dynamically controlled LRTA* one two orders magnitude faster average planning time per move. produces shorter paths existing stateof-the-art real-time algorithm (PR LRTA*) fastest weighted A* tried. original A*
provably optimal solution quality PRA* nearly optimal. argue hundreds
units simultaneously planning paths computer game, LRTA* (PDB, PDB)s low planning time per move real-time guarantees worth 6.1% path-length suboptimality (e.g., 106
screen pixels versus optimal 100 screen pixels).

Table 3: Comparison high-performance algorithms, best values bold. Standard errors
reported .
Algorithm, parameters
PR LRTA* (F, G), ` = 4, = 5, = 1.0
LRTA* (PDB, PDB), ` = 3, c = 3000
A*
weighted A*, f = 15 g + h
PRA*

Planning per move
15.06 0.0722
1.032 0.0054
119.8 3.5203
24.86 1.4404
10.83 0.0829

Suboptimality (times)
1.161 0.0177
1.061 0.0027
1 0.00
1.146 0.0072
1.001 0.0003

7.6 Best Solution Quality Time Limit
section identify algorithms deliver best solution quality time limit.
Specifically, impose hard limit planning time per move, expressed number states
expanded. algorithm exceeds limit even single move made 300
problems upscaled maps excluded consideration. Among remaining algorithms,
select one highest solution quality (i.e., lowest suboptimality). results found
Table 4. algorithms expand least one state per move move, leaving first row
empty. LRTA* (F, PDB) = 1, ` = 3 best choice time limit one
eight states expanded per move. limit rises, expensive optimal algorithms
become affordable. Note best choices dynamically controlled algorithms
time limit rises 3454 states. point, non-real-time PRA* takes ending domain
real-time algorithms. cross-over point specific problem map sizes. larger
problems/maps, PRA*s maximum planning time per move necessarily increase, making
best choice progressively higher planning-time-per-move limits.
439

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 4: Best solution quality strict limit planning time per move. Planning time
states expanded per move. sake readability, suboptimality shown
percentage (e.g., 1.102267 = 10.2267%).
Planning time limit
0
[1, 8]
[9, 24]
[25, 48]
[49, 120]
[121, 728]
[729, 753]
[754, 1223]
[1224, 1934]
[1935, 3453]
[3454, 88137]
[88138, )

Algorithm, parameters
LRTA* (F, PDB) = 1, ` = 3
LRTA* (F, PDB) = 2, ` = 3
LRTA* (F, PDB) = 3, ` = 3
LRTA* (F, PDB) = 4, ` = 3
LRTA* (F, PDB) = 6, ` = 3
LRTA* (F, PDB) = 14, ` = 4
PR LRTA* (PDB, G) c = 15, = 1.0, ` = 3
PR LRTA* (PDB, G) c = 20, = 1.0, ` = 3
PR LRTA* (PDB, G) c = 1000, = 1.0, ` = 3
PRA*
A*

Suboptimality (%)
10.2267%
8.6692%
5.6793%
5.6765%
5.6688%
5.6258%
4.2074%
3.6907%
3.5358%
0.1302%
0%

7.7 Amortization Pattern-database Build Time
pattern-database approach invests time computing PDB map. section
study amortization off-line investment multiple problem instances. PDB build
times 3 GHz Pentium CPU listed Table 5 single map. Consider algorithm LRTA*
(PDB, PDB) cap c = 20 pattern databases built level ` = 3. average,
solution suboptimality 1.058 expanding 1.536 states per move 31.065 microseconds.
closest statically controlled competitor PR LRTA* (F, G) ` = 4, = 15, = 0.6
suboptimality 1.059 expanding average 28.63 states per move 131.128 microseconds. Thus, LRTA* (PDB, PDB) 100 microseconds faster move. Consequently,
4.7 108 moves necessary recoup off-line PDB build time 13 hours. move
taking 31 microseconds, LRTA* lower total run-time first four hours
pathfinding. computed recoup times parameterizations LRTA* (PDB, PDB)
whose closest statically controlled competitor slower per move. results found Table 6
demonstrate LRTA* (PDB, PDB) recoups PDB build time first 1.4 27 hours
pathfinding time. Note numbers highly implementation domain-specific. particular, code building PDBs leaves substantial room optimization. completeness
sake, report detailed times Appendix C.

8. Discussion Empirical Results
section recap trends observed previous sections. Dynamic selection
lookahead either decision-tree PDB approach helps reduce planning time per move
well solution suboptimality (Section 7.1). result, LRTA* becomes competitive
modern algorithms Koenigs LRTA*. However, real-time search algorithms global goals
scale well large maps.
440

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Table 5: Pattern database average 512512 map, computed intermediate goals. Database
size listed number abstract state pairs. Suboptimality planning per move
listed representative algorithm: LRTA* (PDB, PDB) cap c = 20.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13 hours
3 hours
1 hour
24 minutes
10 minutes

Planning per move
1.5
3.2
41.3
104.4
169.3

Suboptimality (times)
1.058
1.059
1.535
2.315
2.284

Table 6: Amortization PDB build times. dynamically controlled LRTA*, list
statically controlled PR LRTA* closest terms solution suboptimality.
LRTA* (PDB, PDB)
c = 20, ` = 3
c = 20, ` = 4
c = 30, ` = 3
c = 40, ` = 3
c = 40, ` = 4
c = 50, ` = 3
c = 50, ` = 4
c = 80, ` = 3

PR LRTA* (F, G)
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.4
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6
` = 4, = 15, = 0.6

Amortization moves
4.7 108
1.2 108
5.1 108
5.3 108
3.4 108
6.2 108
6.7 108
1.1 109

Amortization run-time
4 hours
1.4 hours
5.1 hours
6 hours
9.3 hours
9 hours
21.1 hours
27 hours

Adding intermediate goals brings even classic LRTA* par previous state-of-theart real-time search algorithm PR LRTA* much stronger addition dynamic lookahead
depth selection (Section 7.3). Using dynamic lookahead depth subgoals brings
improvements. Section 7.5 details, LRTA* equipped dynamic lookahead depth
subgoal selection expands barely state per move less 7% solution suboptimality.
better previous state-of-the-art algorithms PR LRTA*, PRA* A*
solution quality planning time per move, believe trade-offs makes appealing
practice. aid practitioners further, provide algorithm selection guide Section 7.6
makes clear LRTA* dynamic subgoal selection best algorithms time
per move severely limited. speed advantage deliver state-of-the-art PR LRTA*
algorithm allows recoup PDB build time several hours pathfinding.

9. Current Limitations Future Work
project opens several interesting avenues future research. particular, would worthwhile investigate performance algorithms paper dynamic environments (e.g.,
bridge gets destroyed real-time strategy game goal moves away agent).
441

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Another area future research application proposed algorithms general planning.
Heuristic search successful approach planning planners ASP (Bonet,
Loerincs, & Geffner, 1997), HSP-family (Bonet & Geffner, 2001), (Hoffmann, 2000),
SHERPA (Koenig, Furcy, & Bauer, 2002) LDFS (Bonet & Geffner, 2006). line recent
planning work (Likhachev & Koenig, 2005) Bonet Geffner (2006), evaluate
proposed algorithms general STRIPS-style planning problem. Nevertheless, believe
new real-time heuristic search algorithms may offer benefits wider range planning
problems. Indeed, core heuristic search algorithm extended paper (LRTA*) previously applied general planning (Bonet et al., 1997). extensions introduced may
beneficial effect similar way B-LRTA* improved performance ASP planner.
Subgoal selection long studied planning central part intermediate-goal
depth-table approach. Decision trees search depth selection induced sample trajectories space appear scalable general planning problems. part
approach requires solving numerous ground-level problems optimally pre-computation
optimal search depth PDB approach. conjecture approach still effective if,
instead computing optimal search depth based optimal action , one solve
relaxed planning problem use resulting action place . idea deriving heuristic
guidance solving relaxed problems quite common planning heuristic search
community.

10. Conclusions
Real-time pathfinding non-trivial problem algorithms must trade solution quality
amount planning per move. two measures antagonistic thus interested
Pareto optimal algorithms outperformed measures algorithms.
classic LRTA* provides smooth trade-off curve, parameterized lookahead depth. Since
introduction 1990, variety extensions proposed. recent extension,
PR LRTS (Bulitko et al., 2005) first application automatic state abstraction real-time
search. large-scale empirical study pathfinding game maps, PR LRTS outperformed
many algorithms respect several antagonistic measures (Bulitko et al., 2007).
paper employ automatic state abstraction instead using pathrefinement, pre-compute pattern databases use select amount planning
intermediate goals dynamically, per move. Several mechanisms dynamic control proposed used virtually existing real-time search algorithm. demonstration,
equip classic LRTA* state-of-the-art PR LRTS dynamic control.
resulting improvements substantial. instance, LRTA* equipped PDB-based control
lookahead intermediate goal selection significantly outperforms existing state art (PR
LRTS) simultaneously planning per move solution quality. Furthermore, average expands little one state per move minimum amount planning
LRTA*-based algorithm.
new algorithms compare favorably A* state-of-the-art extension, PRA*,
presently popular industrial choices pathfinding computer games (Stout, 2000; Sturtevant,
2007). First, per-move planning time algorithms provably unaffected increase
map size. Second, two orders magnitude faster A* one order magnitude
faster PRA* planning time per move. improvements come price 7%
442

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

suboptimality, likely unnoticed computer game player scenarios. Thus appears
new algorithms redefine state art real-time search arena
well-suited industrial applications.

Acknowledgments
Sverrir Sigmundarson School Computer Science, Reykjavik University
project. appreciate consultation Robert C. Holte detailed feedback anonymous
reviewers. research supported grants National Science Engineering Research Council Canada (NSERC); Albertas Informatics Circle Research Excellence (iCORE);
Slovenian Ministry Higher Education, Science Technology; Icelandic Centre Research
(RANNIS); Marie Curie Fellowship European Community programme Structuring
ERA contract number MIRG-CT-2005-017284. Special thanks Nathan Sturtevant
development support HOG.

Appendix A. Decision-Tree Features
devised two different categories classifier features: first consists features based
agents recent history, whereas second contains features sampled shallow pre-search
agents current state. Thus, collectively, features two categories make predictions
based agents recent history well current situation.
first category four features listed Table 7. features computed
execution step. aggregated recent states agent in,
done incremental fashion improved performance. parameter n set user
controls long history aggregate over. use notation s1 refer state agent
one step ago, s2 state two steps ago, etc.; agent thus aggregates states s1 ,
..., sn . Feature f1 provides rough estimate location agent relative goal.
distance goal state affect required lookahead depth, example heuristics
closer goal usually accurate. feature makes possible classifier make
decisions based deemed necessary. Features f2 (known mobility) f3 provide
measure much progress agent made towards reaching goal past steps.
Frequent state revisits may indicate heuristic depression deeper search usually beneficial
situations (Ishida, 1992). Feature f4 measure inaccuracies inconsistencies
heuristic around agent; again, many heuristic updates may warrant deeper search.
features second category listed Table 8. computed execution step. planning phase starts, shallow lookahead pre-search performed gather
information nearby part search space. types features category
coarsely divided features (i) compute fraction states pre-search lookahead
frontier satisfy property, (ii) compare action chosen pre-search previous
actions (either previous state taken last time current state visited), (iii)
check heuristic estimates immediate successors current state. Feature f5 rough
measure density obstacles agents vicinity: obstacles are,
beneficial deeper search might be. Feature f6 indicator difficulty traversing
local area. proportion high, many states updated, possibly suggesting heuristic
depression. feature f7 , pre-search selects action might indicate
443

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 7: History based classifier features.
Feature
f1
f2
f3
f4

Description
initial heuristic estimate distance current state goal:
hoctile (s, sglobal goal ).
heuristic estimate distance current state state
agent n steps ago: h(s, sn ).
number distinct states agent visited last n steps:
|{s1 , s2 , ..., sn }|.
P
total volume heuristic updates last n steps: ni=1 hafter update (si )
hbefore update (si ) (line 6 Figure 1).
Table 8: Pre-search based classifier features.

Feature
f5
f6
f7

f8
f9
f10
f11

Description
ratio actual number states pre-search frontier expected
number states obstacles map.
fraction frontier states updated heuristic value.
boolean feature telling whether action chosen pre-search
action chosen planning phase last time state visited.
first time state visited feature false.
boolean feature telling whether direction suggested pre-search
direction agent took previous step.
ratio current states heuristic best successor state suggested
pre-search: h(s, sgoal )/h(s, sgoal ).
boolean feature telling whether best action proposed pre-search phase
would lead successor state updated heuristic value.
boolean feature telling whether heuristic value current state larger
heuristic value best immediate successor found pre-search.

heuristic values part search space already mutually consistent thus
shallow lookahead needed; applies feature f8 . Features f9 f11 compare current
state successor state suggested pre-search.

Appendix B. Experiments Upscaled Maps Using Global Goals
Empirical results running global-goal algorithms upscaled maps shown Figure 15.
LRTA* (DT, G) shows significant improvement LRTA* (F, G), making comparable
quality existing state-of-the-art algorithms: par P LRTA* (F, G) slightly better
K LRTA* (F, G) allowed expand 200 states per move. worth noting
LRTA* (PDB, G) longer competitive algorithms and, fact, make
real-time cut-off 1000 states parameters combinations (and thus shown
plot). reason lies fact problems simply difficult LRTA* find
optimal move small lookahead depth. instance, abstraction level ` = 3 cap
444

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

c = 80, LRTA* (PDB, G) suboptimality 1.36. Unfortunately, lookahead depth hits cap
11% visited states. result, algorithm expands average 1214 states per move
disqualifies cut-off 1000.
Upscaled maps

Realtime cutoff: 1000

20
LRTA* (F, G)
LRTA* (DT, G)
P LRTA* (F, G)
K LRTA* (F, G)

18

Suboptimality (times)

16
14
12
10
8
6
4
2
0

100

200

300
400
500
600
Mean number states expanded per move

700

800

Figure 15: Performance global-goal algorithms upscaled maps.
Looking collectively small upscaled map results, LRTA* (DT, G) demonstrates
excellent performance among global goal algorithms robust respect map
upscaling one efficient ones (the comparable algorithm K LRTA* (F, G)).
However, within provided 1000 states cut-off limit, none real-time global-goal algorithms
returned solutions would considered acceptable quality pathfinding. Indeed, even
best solutions found approximately four times worse optimal.

Appendix C. Pattern Database Build Times
order operate LRTA* PR LRTA* use lookahead depth intermediate goals controlled dynamically, build pattern databases. pattern database built off-line contains
single entry pair abstract states. three types entries: (i) intermediate goal
ground-level entry state next abstract state; (ii) capped optimal lookahead depth
respect intermediate goal (iii) optimal lookahead depth respect global
goal. running algorithms capped lookaheads (i.e., c < 1000) need two databases
per map: one containing intermediate goals one containing capped optimal lookahead depths.
running effectively uncapped algorithms (i.e., c = 1000 c = 3000) need third
database lookahead depths global goals (see Appendix discussion). Tables 5
912 report build times LRTA* (PDB, PDB) performance capped (i.e.,
build two pattern databases). Tables 13 14 report build times performance
effectively cap (i.e., built three pattern databases).
Finally, interest speeding experiments fact compute pattern databases
pairs abstract states. Instead, took advantage prior benchmark problem availability
445

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 9: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 30.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.4 hours
3.1 hours
1.1 hours
24 minutes
11 minutes

Planning per move
2.1
12.3
60.7
166.6
258.8

Suboptimality (times)
1.058
1.083
1.260
1.843
1.729

Table 10: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 40.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.1 hours
3.1 hours
1.0 hours
24 minutes
10 minutes

Planning per move
2.7
10.2
53.4
217.3
355.4

Suboptimality (times)
1.058
1.060
1.102
1.474
1.490

computed PDBs abstract goal states come play problems agents
solve. Thus, times tables estimates possible pairs.

Appendix D. Intermediate Goals Loops
shown Korf original paper, LRTA* complete lookahead depth
heuristic taken respect single global goal. completeness guarantee lost one
uses intermediate goals (i.e., LRTA* (F, PDB), LRTA* (PDB, PDB) well PR LRTA*
counter-parts). Indeed, abstract tile A, dynamic goal control module guide
agent towards entry state tile B. However, way, agent may stumble different
abstract tile C. soon happens, dynamic control module may select entry state tile
new intermediate goal. unsuspecting agent heads back everything repeats.
combat loops equipped algorithms use intermediate goals state reentrance detector. Namely, soon agent re-visits ground-level state, dynamic control
switches intermediate goal global goal. Additionally, new lookahead depth selected. Ideally, lookahead depth optimal depth respect global goal,
446

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Table 11: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 50.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.6 hours
3.1 hours
1.0 hours
24 minutes
11 minutes

Planning per move
3.5
11.1
68.5
279.4
452.3

Suboptimality (times)
1.058
1.059
1.098
1.432
1.386

Table 12: Pattern databases average 512 512 map, computed intermediate goals.
Database size listed number abstract state pairs. Suboptimality planning
per move listed LRTA* (PDB, PDB) cap c = 80.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 2 years
est. 1.5 months
est. 4 days
13.5 hours
3.2 hours
1.0 hours
25 minutes
10 minutes

Planning per move
6.6
22.9
109.7
523.3
811.5

Suboptimality (times)
1.058
1.059
1.087
1.411
1.301

capped c. Unfortunately, computing optimal lookahead depths global goals quite expensive
off-line (Tables 13 14). Given loops occur fairly infrequently, normally compute
optimal lookahead depths global goals. Instead, state re-visit detected, switch
global goals simply set lookahead cap c. saves off-line PDB computation time
sometimes causes agent conduct deeper search (c plies) really necessary.6
alternative solution investigated future research progressively increase lookahead on-line re-visits detected (i.e., every time re-visit occurs, lookahead depth
state increased certain number plies).

6. exception practice cases c = 1000 c = 3000 setting lookahead depth c
would immediately disqualified algorithm, provided reasonable real-time cut-off. Consequently,
two cap values, invest large amount time computed effectively uncapped optimal lookahead depth
respect global goals.

447

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Table 13: Pattern databases average 512 512 map, computed global goals. Database
size listed number abstract state pairs. Suboptimality planning per move
listed LRTA* (PDB, PDB) cap c = 3000.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 350 years
est. 25 years
est. 2 years
73 days
10.3 days
1.7 days
9.8 hours
2.5 hours

Planning per move
1.0
4.8
27.9
86.7
174.1

Suboptimality (times)
1.061
1.062
1.133
3.626
3.474

Table 14: Pattern databases average 512 512 map, computed global goals. Database
size listed number abstract state pairs. Suboptimality planning per move
listed LRTA* (PDB, G) cap c = 20.
Abstraction level
0
1
2
3
4
5
6
7

Size
1.1 1010
7.4 108
5.9 107
6.1 106
8.6 105
1.5 105
3.1 104
6.4 103

Time
est. 12 years
est. 6 months
est. 13 days
38.0 hours
7.5 hours
2.3 hours
52 minutes
21 minutes

448

Planning per move
349.9
331.6
281.0
298.1
216.1

Suboptimality (times)
6.468
8.766
10.425
8.155
14.989

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

References
BioWare Corp. (1998). Baldurs Gate., Published Interplay, http://www.bioware.com/bgate/,
November 30, 1998.
Bjornsson, Y., & Marsland, T. A. (2003). Learning extension parameters game-tree search. Inf.
Sci, 154(34), 95118.
Blizzard (2002). Warcraft 3: Reign chaos. http://www.blizzard.com/war3.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),
533.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: unified approach heuristic search
deterministic non-deterministic settings, application MDPs. Proceedings
International Conference Automated Planning Scheduling (ICAPS), pp. 142
151, Cumbria, UK.
Bonet, B., Loerincs, G., & Geffner, H. (1997). fast robust action selection mechanism
planning.. Proceedings National Conference Artificial Intelligence (AAAI), pp.
714719, Providence, Rhode Island. AAAI Press / MIT Press.
Botea, A., Muller, M., & Schaeffer, J. (2004). Near optimal hierarchical path-finding. Journal
Game Development, 1(1), 728.
Bulitko, V. (2003a). Lookahead pathologies meta-level control real-time heuristic search.
Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316, Porto,
Portugal.
Bulitko, V. (2003b). Lookahead pathologies meta-level control real-time heuristic search.
Proceedings 15th Euromicro Conference Real-Time Systems, pp. 1316.
Bulitko, V. (2004). Learning adaptive real-time search. Tech. rep. http://arxiv.org/abs/cs.AI/
0407016, Computer Science Research Repository (CoRR).
Bulitko, V., Bjornsson, Y., Lustrek, M., Schaeffer, J., & Sigmundarson, S. (2007). Dynamic Control Path-Planning Real-Time Heuristic Search. Proceedings International
Conference Automated Planning Scheduling (ICAPS), pp. 4956, Providence, RI.
Bulitko, V., Li, L., Greiner, R., & Levner, I. (2003). Lookahead pathologies single agent search.
Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp.
15311533, Acapulco, Mexico.
Bulitko, V., Sturtevant, N., & Kazakevich, M. (2005). Speeding learning real-time search via
automatic state abstraction. Proceedings National Conference Artificial Intelligence (AAAI), pp. 13491354, Pittsburgh, Pennsylvania.
Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2007). Graph Abstraction Real-time Heuristic
Search. Journal Artificial Intelligence Research (JAIR), 30, 51100.
Buro, M. (2000). Experiments Multi-ProbCut new high-quality evaluation function
Othello. van den Herik, H. J., & Iida, H. (Eds.), Games AI Research, pp. 7796. U.
Maastricht.
Culberson, J., & Schaeffer, J. (1996). Searching pattern databases. CSCI (Canadian AI
Conference), Advances Artificial Intelligence, pp. 402416. Springer-Verlag.
449

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Culberson, J., & Schaeffer, J. (1998). Pattern Databases. Computational Intelligence, 14(3), 318
334.
Dijkstra, E. W. (1959). note two problems connexion graphs.. Numerische Mathematik,
1, 269271.
Furcy, D. (2006). ITSA*: Iterative tunneling search A*. Proceedings National
Conference Artificial Intelligence (AAAI), Workshop Heuristic Search, Memory-Based
Heuristics Applications, Boston, Massachusetts.
Furcy, D., & Koenig, S. (2000). Speeding convergence real-time search. Proceedings
National Conference Artificial Intelligence (AAAI), pp. 891897.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions Systems Science Cybernetics, 4(2), 100107.
Hernandez, C., & Meseguer, P. (2005a). Improving convergence LRTA*(k). Proceedings
International Joint Conference Artificial Intelligence (IJCAI), Workshop Planning
Learning Priori Unknown Dynamic Domains, Edinburgh, UK.
Hernandez, C., & Meseguer, P. (2005b). LRTA*(k). Proceedings 19th International Joint
Conference Artificial Intelligence (IJCAI), Edinburgh, UK.
Hernandez, C., & Meseguer, P. (2007). Improving real-time heuristic search initially unknown
maps. Proceedings International Conference Automated Planning Scheduling
(ICAPS), Workshop Planning Games, p. 8, Providence, Rhode Island.
Hoffmann, J. (2000). heuristic domain independent planning use enforced hillclimbing algorithm. Proceedings 12th International Symposium Methodologies
Intelligent Systems (ISMIS), pp. 216227.
id Software (1993). Doom., Published id Software, http://en.wikipedia.org/ wiki/Doom, December 10, 1993.
Ishida, T. (1992). Moving target search intelligence. Proceedings National Conference
Artificial Intelligence (AAAI), pp. 525532.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).
Robocup rescue: Search rescue large-scale disasters domain autonomous agents
research. Man, Systems, Cybernetics, pp. 739743.
Kocsis, L. (2003). Learning Search Decisions. Ph.D. thesis, University Maastricht.
Koenig, S. (2004). comparison fast search methods real-time situated agents. Proceedings International Joint Conference Autonomous Agents Multiagent Systems
(AAMAS), pp. 864871.
Koenig, S. (2001). Agent-centered search. AI Magazine, 22(4), 109132.
Koenig, S., Furcy, D., & Bauer, C. (2002). Heuristic search-based replanning. Proceedings
Int. Conference Artificial Intelligence Planning Scheduling, pp. 294301.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. Proceedings International
Joint Conference Autonomous Agents Multiagent Systems, pp. 281288.
Korf, R. (1985). Depth-first iterative deepening : optimal admissible tree search. Artificial
Intelligence, 27(3), 97109.
450

fiDYNAMIC C ONTROL R EAL -T IME H EURISTIC EARCH

Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(23), 189211.
Korf, R. (1993). Linear-space best-first search. Artificial Intelligence, 62, 4178.
Likhachev, M., Ferguson, D., Gordon, G., Stentz, A., & Thrun, S. (2005). Anytime dynamic A*:
anytime, replanning algorithm. Proceedings International Conference Automated
Planning Scheduling (ICAPS).
Likhachev, M., Gordon, G. J., & Thrun, S. (2004). ARA*: Anytime A* provable bounds
sub-optimality. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances Neural Information Processing Systems 16. MIT Press, Cambridge, MA.
Likhachev, M., & Koenig, S. (2005). generalized framework lifelong planning A*. Proceedings International Conference Automated Planning Scheduling (ICAPS),
pp. 99108.
Lustrek, M. (2005). Pathology single-agent search. Proceedings Information Society Conference, pp. 345348, Ljubljana, Slovenia.
Lustrek, M., & Bulitko, V. (2006). Lookahead pathology real-time path-finding. Proceedings
National Conference Artificial Intelligence (AAAI), Workshop Learning Search,
pp. 108114, Boston, Massachusetts.
Mero, L. (1984). heuristic search algorithm modifiable estimate. Artificial Intelligence, 23,
1327.
Moore, A., & Atkeson, C. (1993). Prioritized sweeping: Reinforcement learning less data
less time. Machine Learning, 13, 103130.
Nash, A., Daniel, K., & Felner, S. K. A. (2007). Theta*: Any-angle path planning grids.
Proceedings National Conference Artificial Intelligence, pp. 11771183.
Pearl, J. (1984). Heuristics. Addison-Wesley.
Rayner, D. C., Davison, K., Bulitko, V., Anderson, K., & Lu, J. (2007). Real-time heuristic search
priority queue. Proceedings International Joint Conference Artificial
Intelligence (IJCAI), pp. 23722377, Hyderabad, India.
Russell, S., & Wefald, E. (1991). Right Thing: Studies Limited Rationality. MIT Press.
Schaeffer, J. (1989). history heuristic alpha-beta search enhancements practice. IEEE
Transactions Pattern Analysis Machine Intelligence, PAMI-11(1), 12031212.
Schaeffer, J. (2000). Search ideas Chinook. van den Herik, H. J., & Iida, H. (Eds.), Games
AI Research, pp. 1930. U. Maastricht.
Shimbo, M., & Ishida, T. (2003). Controlling learning process real-time heuristic search.
Artificial Intelligence, 146(1), 141.
Sigmundarson, S., & Bjornsson, Y. (2006). Value Back-Propagation vs. Backtracking RealTime Search. Proceedings National Conference Artificial Intelligence (AAAI),
Workshop Learning Search, pp. 136141, Boston, Massachusetts, USA.
Stenz, A. (1995). focussed D* algorithm real-time replanning. Proceedings
International Joint Conference Artificial Intelligence (IJCAI), pp. 16521659.
Stout, B. (2000). basics A* path planning. Game Programming Gems. Charles River
Media.
451

fiB ULITKO , L U STREK , CHAEFFER , B J ORNSSON , IGMUNDARSON

Sturtevant, N. (2007). Memory-efficient abstractions pathfinding. Proceedings third
conference Artificial Intelligence Interactive Digital Entertainment, pp. 3136, Stanford, California.
Sturtevant, N., & Buro, M. (2005). Partial pathfinding using map abstraction refinement.
Proceedings National Conference Artificial Intelligence, pp. 13921397.
Sturtevant, N., & Jansen, R. (2007). analysis map-based abstraction refinement.
Proceedings 7th International Symposium Abstraction, Reformulation Approximation, Whistler, British Columbia.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools techniques
(2nd edition). Morgan Kaufmann, San Fransisco.

452


