journal artificial intelligence

submitted published

learning geometrically constrained hidden markov
robot navigation bridging topological geometrical gap
hagit shatkay

hagit shatkay celera com

informatics group
celera genomics rockville md

leslie pack kaelbling

artificial intelligence laboratory
massachusetts institute technology cambridge

lpk ai mit edu

come place streets marked
windows lighted mostly darked
place could sprain elbow chin
dare stay dare go
go turn left right
right three quarters maybe quite
simple afraid
mind maker upper make mind

oh places go dr seuss

abstract
hidden markov hmms partially observable markov decision processes
pomdps provide useful tools modeling dynamical systems particularly
useful representing topology environments road networks oce
buildings typical robot navigation work presented
describes formal framework incorporating readily available odometric information geometrical constraints learns
taking advantage information learning hmms pomdps made
generate better solutions require fewer iterations robust face
data reduction experimental obtained simulated real robot
data demonstrate effectiveness

introduction

work concerned robots need perform tasks structured environments
robot moving environment suffers two main limitations noisy sensors prevent
confidently knowing noisy effectors prevent knowing
certainty actions take concentrate structured environments
turn characterized two main properties environments consist vast uneventful uninteresting areas interspersed relatively interesting positions
situations consider instance robot delivering bagel oce building interesting
situations doors intersections building hallways well
c ai access foundation morgan kaufmann publishers rights reserved

fishatkay kaelbling

positions bagel might respect robot arm e g robot holding
bagel puts etc aspects environment desk positions
oces inconsequential bagel delivery task
natural way represent combination environment robot interactions
probabilistic automaton states represent interesting situations
edges states represent actions leading one situation another probability
distributions transitions possible observations robot may perceive
situation model robot noisy effectors sensors respectively
formally known pomdp partially observable markov decision process proven useful robot acting inherent world uncertainty simmons koenig nourbakhsh powers birchfield cassandra kaelbling kurien
despite much work task learning directly automatically
data widely addressed concerning immediate topic date
consists mostly work done simmons koenig b assumption underlying
work human provides rather accurate topological model states
connections exact probability distributions learned top model
version baum welch rabiner another interesting
acquisition topological thrun bucken b thrun
focused extracting deterministic topological maps previously acquired geometricalgrid maps latter learned directly data discussion
related geometrical topological approaches probabilistic
deterministic versions given next section
work reported first successful attempt aware learn purely probabilistictopological directly completely recorded data without previous humanprovided grid weak geometric information recorded
robot help learn topology environment represent probabilistic
model therefore directly bridges historically perceived gap topological
geometrical information addresses claim presented thrun work
main shortcoming topological failure utilize inherent geometry
learnt environment
robots equipped wheel encoders enable odometer record change
robot position moves environment data typically noisy
inaccurate oors environment rarely smooth wheels robot
aligned neither motors mechanics imperfect resulting slippage
drift effects accumulate mark initial position robot
try estimate current position summing long sequence odometric recordings
resulting estimate incorrect raw recorded odometric information
effective tool determining absolute location robot
environment
aimed determining absolute locations idea underlying
weak odometric information despite noise inaccuracy still provides geometrical cues
help distinguish different states well identify revisitation
state hence information enhances ability learn topological however


filearning geometrically constrained hmms

use geometrical information requires careful treatment geometrical constraints
directional data demonstrate existing extended
take advantage noisy odometric data geometrical constraints geometrical
information directly incorporated probabilistic topological framework producing
significant improvement standard baum welch without need humanprovided model
rest organized follows section provides survey previous work
area learning maps robot navigation brie refers earlier work learning
automata section presents formal framework work section presents main
aspects iterative learning section describes strategies selecting
initial point iterative process begins section presents experimental
obtained simulated real robot data traditionally hard learn environments
experiments demonstrate indeed converges better fewer
iterations standard baum welch method robust face data reduction

approaches learning maps

work presented lies intersection theoretical area learning computational particular learning automata data sequences applied area
map acquisition robot navigation concentrate surveying work latter
area pointing distinction predecessors brie review
automata computational learning theory comprehensive review
theoretical given shatkay

modeling environments robot navigation

context maps robot navigation distinction usually made two
principal kinds maps geometric topological geometric maps describe environment
collection objects occupied positions space geometric relationships among
topological framework less concerned geometrical positions
world collection states connectivity states reachable
states actions lead one state next
draw additional distinction world centric maps provide objective
description environment independent agent map robot centric
capture interaction particular subjective agent environment
learning map agent needs take account noisy sensors actuators try
obtain objectively correct map agents could use well similarly agents
map need compensate limitations order assess position
according map learning model captures interaction agent acquiring
model one hence noisy sensors actuators specific agent
ected model different model likely needed different agents
related work described especially within geometrical framework centered
around learning objective maps world rather agent specific shall point
survey work concerned latter kind
work focuses acquiring purely topological less concerned learning
geometrical relationships locations objects objective maps although geometrical
thank sebastian thrun terminology



fishatkay kaelbling

relationships serve aid acquisition process concept state used
topological framework general concept geometrical location since state
include information battery level arm position etc information
great importance non geometrical nature therefore cannot readily
captured purely geometrical framework following sections provide survey work
done within geometrical framework within topological framework well
combinations two approaches

geometric maps

geometric maps provide description environment terms objects placed
positions example grid maps instance geometric
grid map environment modeled grid array position
grid vacant occupied object binary values placed array
refined ect uncertainty world grid cells
contain occupancy probabilities rather binary values lot work done
learning grid maps robot navigation use sonar readings
interpretation moravec elfes others moravec elfes moravec elfes
asada
underlying assumption learning maps robot tell
grid obtains sonar reading indicating object therefore
place object correctly grid similar localization assumption requiring robot
identify geometrical location underlies geometric mapping techniques leonard
et al smith et al thrun et al b dissanayake et al even
explicit grid part model explicit localization hard satisfy
leonard et al smith et al address issue use geometrical
beacons estimate location robot known kalman filter method
gaussian probability distribution used model robot possible current location
observations collected current point without allowing refinement previous
position estimates later observations area recently extended
two directions leonard feder partition task learning one large map
learning multiple smaller map sections thus addressing issue computational eciency
dissanayake et al conduct theoretical study convergence
properties latter may lead computational eciency identifying cases
steady state solution readily obtained accordingly bounding number steps required
reach useful solution cases
work thrun et al uses similar probabilistic obtaining grid maps
work refined thrun et al b first learn location significant landmarks
environment fill details complete geometrical grid laser range
scans latter work extends smith et al observations obtained
location visited order derive probability distribution
possible locations achieve authors use forward backward procedure similar
one used baum welch rabiner order determine possible
locations observed data resembles use forwardbackward estimation procedure probabilistic basis aiming obtaining maximum
likelihood map environment still significantly differs initial
assumptions final data assumed provided learner includes


filearning geometrically constrained hmms

motion model perceptual model robot consist transition
observation probabilities within grid components learnt
although grid context coarser grained topological framework end
probabilistic grid map probabilistic topological model
explained next section
addition concerned locations rather richer notion state
fundamental drawback geometrical maps fine granularity high accuracy geometrical maps particularly grid ones tend give accurate detailed picture
environment cases necessary robot know exact location terms
metric coordinates metric maps indeed best choice however many tasks
require fine granularity accurate measurements better facilitated
abstract representation world example robot needs deliver bagel
oce oce b needs map depicting relative location respect
b passageways two oces perhaps landmarks help orient
gets lost reasonably well operating low level obstacle avoidance mechanism
help bypass ower pots chairs might encounter way objects
need part environment map driver traveling cities needs
know neither longitude latitude coordinates globe location specific
houses along way robot need know exact location within building
exact location items environment order get one point
another hence effort obtaining detailed maps usually justified addition
maps large makes even though polynomial
size map inecient

topological maps

alternative detailed geometric maps abstract topological maps
maps specify topology important landmarks situations states routes transitions arcs concerned less physical location landmarks
topological relationships situations typically less complex
support much ecient metric maps topological maps built lowerlevel abstractions allow robot move along arcs perhaps wall road following
recognize properties locations distinguish significant locations states
exible allowing general notion state possibly including information
non geometrical aspects robot situation
two typical strategies deriving topological maps one learn topological
map directly first learn geometric map derive topological model
process analysis
nice example second provided thrun bucken b thrun
use occupancy grid techniques build initial map strategy appropriate
primary cues decomposition abstraction map geometric however
many cases nodes topological map defined terms sensory data e g
labels door whether robot holding bagel learning geometric map first
relies odometric abilities robot weak space large
dicult derive consistent map



fishatkay kaelbling

contrast work concentrates learning topological model directly assuming abstraction robot perception action abilities already done abstractions
manually encoded lower level robot navigational software described
section work pierce kuipers discusses automatic method extracting
abstract states features raw perceptual information
kuipers byun provide strategy learning deterministic topological maps works
well domains noise robot perception action abstracted
away learning single visits nodes traversals arcs strong underlying assumption
strategies building map current state reliably identified
local information distance traversed previous well identified
state methods unable handle situations long sequences actions
observations necessary disambiguate robot state
mataric provides alternative learning deterministic topological maps
represented distributed graphs learning process relies assumption
current state distinguished states local information includes
compass sonar readings uncertainty modeled probability distributions
instead matching current readings already existing states required exact
thresholds tolerated error set empirically another difference work presented
learn complete probabilistic topology environment mataric
work overall topology graph assumed advance linear list additional
edges added learning process probability distribution associated
edges mechanism choosing edge take determined part goal seeking
process part model
engelson mcdermott learn diktiometric maps topological maps metric relations nodes experience uncertainty model use interval rather
probabilistic learned representation deterministic ad hoc routines handle resulting failures uncertainty representation
prefer learn combined model world robot interaction world
allows robust takes account likelihood error sensing action
work closely related koenig simmons b learn pomdp
stochastic topological robot hallway environment recognize
diculty learning good model without initial information solve
human provided topological map together constraints structure
model modified version baum welch learns parameters
model developed incremental version baum welch used line
contain weak metric information representing hallways chains one meter
segments allowing learning select probable chain length
method effective large size proportional hallways length
strongly depends quality human provided initial model

learning automata data

informally speaking automaton consists set states set transitions lead
one state another context work automaton states correspond
states modeled environments transitions state changes due actions
performed environment transition automaton tagged symbol


filearning geometrically constrained hmms

input alphabet corresponding action input system caused state
transition classical automata theory e g hopcroft ullman distinguishes
deterministic non deterministic automata alphabet symbol single
edge tagged going state automaton deterministic otherwise
transition states uniquely determined input symbol automaton
non deterministic augment transition edge non deterministic automaton
probability taking given certain input resulting automaton called probabilistic
basic learning finite deterministic automata given data roughly
described follows given set positive set negative example strings
respectively alphabet fixed number states k construct minimal deterministic
finite automaton k states accepts accept
shown np complete gold despite hardness positive
shown possible special settings angluin showed oracle
answer membership queries provide counterexamples conjectures automaton
polynomial time learning positive negative examples rivest
schapire provide several effective methods settings learn
deterministic automata correct high probability work deals
learning noise free data basye dean kaelbling presented several
high probability learn input output deterministic automata data observed
learner corrupted forms noise
cases learned automaton deterministic rather probabilistic basic
learning probabilistic context automaton assigns
distribution true one data sequences training data generated
true automaton another form learning finding probabilistic
automaton assigns maximum likelihood training data automaton
maximizes pr j
abe warmuth finding probabilistic automaton states even
small error respect true model allowed probability probably
approximately correct pac learning model cannot done polynomial time polynomial number examples unless np rp work arises broadly accepted
conjecture yet proven learning hidden markov hard even
pac sense two ways address hardness one restrict class
probabilistic learned learn unrestricted hidden markov
good practical pac guarantees quality
work ron et al pursues first learning restricted classes
automata namely acyclic probabilistic finite automata probabilistic finite sux automata
classes useful applications related natural language processing
learned polynomial time within pac framework
second one predominantly taken work learn model
member complete unrestricted class hidden markov weak guarantees
exist goodness model learning procedure may directed obtain
practically good guessing automaton model
iterative procedure make automaton fit better training data one
commonly used purpose baum welch baum petrie soules weiss
presented detail rabiner iterative updates model


fishatkay kaelbling

gathering sucient statistics data given current automaton
update procedure guaranteed converge model locally maximizes likelihood
function pr datajmodel since maximum local model might close enough
true automaton data generated challenging
ways force converging higher likelihood maxima least make
converge faster facilitating multiple guesses initial thus raising probability
converging higher likelihood maxima one taken work
presented
assume throughout number states model learning
known strong assumption since methods learning number
states regularization methods deciding number states model parameters
discussed instance vapnik book address issue
rest work describes learning topological use noisy
odometric information readily available robots geometrical information
typically used topological mapping methods demonstrate topological model
used learn extended directly incorporate weak odometric
information avoid use human provided priori
still learn stochastic environment eciently effectively

assumptions
section describes formal framework work starts introducing classic
hidden markov model model extended accommodate noisy odometric information
nave form ignoring information robot heading orientation later
adapted accommodate heading information
concentrate describing learning hmms rather
pomdps means robot decisions make regarding next action
every state one action executed state experiments human operator gave action command associated state robot gathering data
note action necessarily one every state e g robot told
turn right state move forward state however state one action taken extension complete pomdps implemented
learning hmm possible actions straightforward although notationally
cumbersome thus limit discussion hmms

hmms basics
hidden markov model consists states transitions observations probabilistic behavior
formally defined tuple hs b satisfying following conditions

fs sn g finite set n states
fo om g finite set possible observation values


filearning geometrically constrained hmms

stochastic transition matrix ai j pr qt sj jqt si j n
nx

qt state time every state si

j

ai j

ai j holds transition probability state si state sj
b stochastic observation matrix bj k pr vt ok jqt sj j n
mx

k vt observation recorded time every state sj
bj k
bj k holds probability observing ok state sj

k

stochastic initial distribution vector pr q si n

nx





holds probability state si time starting record observations
model corresponds world whose actual state given time qt hidden
directly observable observable aspects state vt detected
recorded state visited time agent moves one hidden state
next according probability distribution encoded matrix observed information
state governed probability matrix b although work concerned
discrete observations extension continuous observations straightforward
well addressed work hidden markov liporace juang
simply stated learning hmm reverse engineering hidden markov
model stochastic system sampled data generated system formalize
learning task section next section extends hmms account geometric
information

adding odometry hidden markov

world composed finite set states fundamental distinction
framework term state term location state robot
directly correspond location state may include information robot
battery level orientation location robot standing entrance oce
facing right different state robot standing place facing left similarly
robot standing bagel arm different state robot
position without bagel
dynamics world described state transition distributions specify probability making transitions one state next certain action
finite set observations perceived state relative frequency
observation described probability distribution depends current state
model observations multi dimensional observation vector values
chosen finite domain factorize observation associated state
several components instance demonstrated section view observation
recorded robot standing oce environment consisting three components
corresponding three cardinal directions front left right example observation vector thus dimensional assumed vector components conditionally
independent given state


fishatkay kaelbling

addition components state assumed associated position
metric space whenever state transition made robot records odometry vector
estimates position current state relative previous one time assume odometry vector consists readings along x coordinates global coordinate system readings corrupted independent normal noise latter
independence assumption strict one relaxed introducing complete covariance matrix although done work section extend odometry vector include information heading robot drop global coordinate
framework
note odometric relationship characterizes transition rather state
described receives different treatment observations associated
states
two important assumptions underlying treatment odometric relations
states first inherent true odometric relation position every
two states world second robot moves one state next
normal mean noise around correct expected odometric reading along odometric
dimension noise ects two kinds odometric error sources

lack precision discretization real world states e g rather

large area robot stand regarded doorway ai
lab
lack precision odometric measures recorded robot due slippage
friction disalignment wheels imprecision measuring instruments etc

formally introduce odometric information hidden markov model framework
define augmented hidden markov model tuple hs b r

fs sn g finite set n states
qli oi finite set observation vectors length l ith element

observation vector chosen finite set oi
stochastic transition matrix ai j pr qt sj jqt si j n
nx

qt state time every state si ai j
j

ai j holds transition probability state si state sj
b array l stochastic observation matrices bi j k pr vt ok jqt sj
l j n ok oi vt observation vector time vt ith

component
bi j k holds probability observing ok along ith component observation
vector state sj
r relation matrix specifying pair states si sj mean variance
dimensional odometric relation ri j mean mth

time consider corresponding x readings



filearning geometrically constrained hmms

component relation si sj ri j variance furthermore
r geometrically consistent component relation b ra b
must directed metric satisfying following properties states b c
def


b b anti symmetry
c b b c additivity
representation odometric relations ects two assumptions previously stated
regarding nature odometric information true odometric relation
position every two states represented mean noise around correct
expected odometric relation accounting lack precision real world
discretization inaccuracy measurement represented variance

stochastic initial probability vector describing distribution initial state
simplicity assumed form h implying
one designated initial state si robot started

model extends standard hidden markov model described section two ways
facilitates observations factored components represented vectors
components assumed conditionally independent given
state factorization together conditional independence assumption allows
simple calculation probability complete observation vector
probabilities components therefore fewer probabilistic parameters
learnt model view observation vector consisting possible
combination component values single atomic observation

introduces odometric relation matrix r constraints components
r constraints explained section proven useful learning
model parameters demonstrated section

handling directional data

extend model accommodate directional changes addition positional
changes two issues stemming directional changes moving environment need non traditional distributions model directional changes need
correct cumulative rotational error severely interferes location estimation
within global coordinate framework detailed discussion two
solution given earlier authors shatkay kaelbling sake
completeness brie review two issues
circular distributions

robot change direction moves environment expressed terms
angular change respect original heading since angular measures inherently circular treating normally distributed standard procedures obtaining
sucient statistics data adequate trivial example average


fishatkay kaelbling




x
x
x
















x



figure simple average two angles depicted

vectors unit circle average angle
formed dashed vector

figure directional data represented angles
vectors unit circle

two angular readings simple average obtain angle
far intuitive illustrated figure
address circularity issue use von mises distribution circular version
normal distribution model change heading two states explained
collection changes heading within two dimensional space represented terms
cartesian polar coordinates cartesian system n changes headings
recorded sequence dimensional vectors hx hxn yn unit circle
shown figure changes represented corresponding angles
radii center unit circle x axis n respectively
relationship two representations
xi cos yi sin n
vector mean n points hx yi calculated
pn cos
pn sin





x



n
n



polar coordinates express mean vector terms angle length
except case x

arctan xy

x



angle mean angle length measure
concentrated sample angles around closer concentrated
sample around mean corresponds smaller sample variance
intuitively satisfactory circular version normal distribution would mean
maximum likelihood estimate average angle calculated way
analogous gauss derivation normal distribution von mises developed circular
version gumbel greenwood durand mardia defined follows
definition circular random variable said von mises
distribution parameters probability density


filearning geometrically constrained hmms

function

f e cos


modified bessel function first kind order




x
r

r

r



parameters correspond distribution mean concentration respectively
circular normal distributions exist von mises desirable estimation
procedure alluded earlier given set heading samples angles n von mises
distribution maximum likelihood estimate

arctan xy

x defined equation
maximum likelihood estimate concentration parameter satisfies
n
max x

n cos

modified bessel function first kind order




x

r
r r r



information estimation procedure beyond scope
found elsewhere gumbel et al mardia
conclude assume change heading von mises distributed around mean
concentration parameter assumption ected model learning procedures
explained later section change heading h b b pair
states b completes set parameters included relation matrix r
introduced earlier section
cumulative rotational error

tend think environment consisting landmarks fixed global coordinate
system corridors transitions connecting landmarks idea underlies typical
maps constructed used everyday life however view environment may
problematic robots involved
conceptually robot two levels operates abstract level centers
corridors follows walls avoids obstacles physical level motors
turn wheels robot moves physical level many inaccuracies manifest
wheels unaligned resulting drift right
left one motor slightly faster another resulting similar drifts obstacle
one wheels cause robot rotate around slightly uneven oors may cause


fishatkay kaelbling



actual position
recorded position

figure robot moving along solid arrow correcting drift direction dashed
arrow dotted arrow marks recorded change position

robot slip certain direction addition measuring instrumentation odometric
information may accurate abstract level corrective actions
constantly executed overcome physical drift drag example left wheel
misaligned drags robot leftwards corrective action moving right constantly
taken higher level keep robot centered corridor
phenomena described significant effect odometry recorded robot
data interpreted respect one global framework example consider robot
depicted figure drifts left moving one state next
corrects moving right order maintain centered corridor
let us assume states meters apart along center corridor center
corridor aligned axis global coordinate system robot steps back
forth corridor one state next whenever robot reaches state
odometry reading changes hx along hx headingi dimensions respectively
robot proceeds deviation respect x axis becomes severe thus
going several transitions odometric changes recorded every pair
states taken respect global coordinate system become larger larger similar
inconsistent odometric changes recorded pairs states arise along
odometric dimensions especially severe inconsistencies arise respect
heading since lead mistakenly switching movement along x
axes well confusion forwards backwards movement deviation
heading around respectively
early work shatkay kaelbling assumed perpendicularity corridors
taken advantage robot collected data odometric readings recorded
respect global coordinate system robot could align origin
turn trajectory odometry recorded perpendicularity assumption
robot ramona along x axes given figure sequence shown recorded
robot drove repeatedly around loop corridors details data
gathering process provided section contrast figure shows trajectory another
sequence odometric readings recorded ramona driving corridors without
perpendicularity assumption data collected latter setting subjected
cumulative rotational error


filearning geometrically constrained hmms
































figure sequence gathered ramona perpendicularity assumed





figure sequence gathered ramona per

pendicularity assumed

data handled state relative coordinate systems shatkay kaelbling
latter implies state si coordinate system shown figure
origin anchored si axis aligned robot heading state denoted
bold arrows figure x axis perpendicular contrast global
coordinate system anchored initial starting state within global coordinate
system relations recorded may vary greatly among multiple instances transition
pair states state relative system recorded learned
relationship pair states hsi sj reliable despite fact
multiple transitions recorded si sj
state relative coordinate systems geometric relation stored rij introduced section expressed pair states si sj respect
coordinate system associated state si accordingly constraints imposed x
components relation matrix must specified respect explicit coordinate
system used explained
given pair states b denote hx yi b vector h ra b x ra b let
us define tab transformation maps hxa ya point represented respect
coordinate system state point represented respect coordinate
system state b hxb yb
explicitly let ab mean change heading state state b applying tab
vector h xyaa vector h xybb follows





xb
x
x cos ab ya sin ab
tab
yb
ya
xa sin ab ya cos ab





consistency constraints within framework must restated

hx yi h
hx yi b tba hx yi b anti symmetry
hx yi c hx yi b tba hx yi b c additivity


fishatkay kaelbling


x
sj
si





x

figure robot state si faces axis direction relation si sj wrt si coordinate
system

consistency constraints ones need enforced learning
constructs hmm important note transformation
constitute set additional parameters need learnt rather calculated terms
heading change parameter already integral part relation matrix
defined sections
introduced basic formal model use representing environments
robot interaction following section state learning
describe basic learning model data

learning hmms odometric information

section formalizes learning hmms discusses odometric information
incorporated learning overview complete provided
appendix

learning

learning hidden markov generally stated follows given
experience sequence e hidden markov model could generated sequence
useful close original according criterion explicit common statistical
look model maximizes likelihood data sequence e given
model formally stated maximizes pr ej however given complicated landscape
typical likelihood functions multi parameter domain obtaining maximum likelihood
model feasible studied practical methods particular well known baumwelch rabiner references therein guarantee local maximum
likelihood model
another way evaluating quality learned model comparing true model
note stochastic hmms induce probability distribution observation sequences given length kullback leibler kullback leibler divergence
learned distribution true one commonly used measure estimating good


filearning geometrically constrained hmms

learned model obtaining model minimizes measure possible learning goal
culprit practice learn model data ground
truth model compare learned model still evaluate learning
measuring well perform data obtained known reasonable expect learns well data generated model
perform well data generated unknown model assuming indeed form
suitable representation true generating process discuss kullback leibler kl
divergence detail section context evaluating experimental
summarize learning address work obtaining model
attempting locally maximize likelihood evaluating
kl divergence respect true underlying distribution distribution
available

learning

learning starts initial model given experience sequence e
returns revised model locally maximizes likelihood p ej experience
sequence e length element et pair hrt vt rt
observed relation vector along x dimensions states qt qt vt
observation vector time
extends standard baum welch deal relational information factored observation sets baum welch expectationmaximization em dempster laird rubin alternates
e step computing state occupation state transition probabilities
time sequence given e current model
step finding model maximizes p ej
providing monotone convergence likelihood function p ej local maximum
however extension introduces additional component namely relation matrix r
viewed two kinds observations state observations ordinary hmm
distinction observe integer vectors rather integers transition observations odometry relations states latter must satisfy geometrical constraints
hence extension standard update formulae described required
state occupation probabilities

following rabiner first compute forward backward matrices fft
denotes probability density value observing e et qt si given fit
probability density observing et et given qt si formally
fft pr e et qt sij
fit pr et et jqt si
measurements continuous case r matrices contain
probability density values rather probabilities
forward procedure calculating matrix initialized

b
otherwise



fishatkay kaelbling

continued
fft j

nx



fft ai j f rt jri j bjt



expression f rt jri j denotes density point rt according distribution represented
means variances entry j q
relation matrix r bjt probability
j
observing vector vt state sj bt li bi j vt
backward procedure calculating matrix initialized fit j continued

nx

fit fit j ai j f rt jri j bjt

j

given compute given time point state occupation statetransition probabilities state occupation probabilities representing
probability state si time given experience sequence current model
computed follows


pr qt si je pnff fit
j fft j fit j
similarly j state transition probabilities state state j time given
experience sequence current model computed
j pr qt si qt sj je
fft ai j bjt f rt jri j fit j



nx
nx

j

fft ai j bjt f rt jri j fit j

essentially formulae appearing rabiner tutorial rabiner
take account density odometric relations
next phase goal model maximizes likelihood conditioned current transition observation probabilities pr ej usually
simply done maximum likelihood estimation probability distributions
b computing expected transition observation frequencies model must
compute relation matrix r constraint remain geometrically consistent
rest section use notation v denote reestimated value v
denotes current value
updating transition observation parameters

b matrices straightforwardly reestimated ai j expected number
transitions si sj divided expected number transitions si b j k
expected number times ok observed along ith dimension state sj divided
expected number times sj
pt
pt j



b j k pt v ok j

ai j pt


expression c denotes indicator function value condition c true otherwise


filearning geometrically constrained hmms

p

q



p



































q

figure examples two sets normally distributed points constrained means
dimensions

updating relation parameters

reestimating relation matrix r geometrical constraints induce interdependencies
among optimal mean estimates well optimal variance estimates mean
estimates parameter estimation form constraints almost untreated mainstream statistics bartels found previous existing solutions estimation
addressed illustration issues involved estimation constraints
consider following estimation normal means
example data consists two sample sets points p fp p pn g q
fq q qk g independently drawn two distinct normal distributions means p q
variances p q respectively asked maximum likelihood estimates
two distribution parameters moreover told means two distributions
related q p illustrated figure latter constraint task
simple degroot
pn p
pn
pi p
p
p
n
n

similarly q q however constraint p q requires finding single mean
setting one negated value intuitively choosing maximum
likelihood single mean concentrated sample effect
varied sample submissive thus overall sample deviation means
would minimized likelihood data maximized therefore mutual
dependence estimation mean estimation variance
since samples independently drawn joint likelihood function
pi p

n
p

f p qjp q p q e p
p




yk e
j

qj q
q

p



q



taking derivatives joint log likelihood function respect p p q
equating constraint q p obtain following set mutual
equations maximum likelihood estimators
p
p
q ni pi p kj qj
p
q p
nq kp
pk q
pn p

p




p
q j j p

n

k



fishatkay kaelbling

substituting expressions p q expression p obtain cubic equation cumbersome still solvable simple case solution provides maximum likelihood estimate mean variance constraint q p

proceed actual update relation matrix constraints clarity
initially discuss first two geometrical constraints discuss additivity constraint
section recall concentrate enforcement global constraints appropriate
perpendicularity assumption although idea applied case staterelative constraints
zero distances states trivially enforced setting diagonal
entries r matrix small variance
anti symmetry within global coordinate system enforced data recorded along
transition state sj si well state si sj reestimating ri j
demonstrated example variance taken account leading following
set mutual equations



mi j



mi j

pt

rt j rt j


j
j

pt mi j mj
j j
pt j r

pt j j








x dimensions x amounts complicated still solvable cubic
equation however general case accounting orientation robot
complete additivity enforced obtain closed form reestimation
formulae
avoid hardships use lag behind update rule yet unupdated estimate
variance used calculating estimate mean mean estimate
used update variance equation thus mean updated variance
parameter lags behind update process reestimation equation needs
use rather follows pt h rt j rt j
j




mi j pt hi jt j j



j

j



shown shatkay lag behind policy instance generalized em mclachlan krishnan latter guarantees monotone convergence local maximum
likelihood function even maximization step increases rather strictly maximizes expected likelihood data given current model
similarly reestimation formula von mises mean concentration parameters
heading change states si sj solution equations

tx


bb sin rt j j j j cc

cc
arctan b
b tx


cos rt j j j j


j







similar termed one step late update taken others applying em highly non linear optimization mclachlan krishnan



filearning geometrically constrained hmms

j
max
j

pt



j cos rt j
tp

j






modified bessel functions defined equations section
avoid need solve mutual equations take advantage lag behind strategy updating mean current estimates concentration parameters j j
follows
pt sin r j j


j
j
j arctan ptt

cos r j j


j
j

calculating concentration parameters newly updated mean
solution equation use lookup tables
possible alternative lag behind update mean though assumption j j holds assumption variance terms equation cancel
mean update independent variance variances updated
stated equation without assuming constraints taken
earlier stages work shatkay kaelbling lag behind strategy
superior according experiments due instance generalized em

enforcing additivity

note additivity constraint directly implies two geometrical constraints thus
enforcing complete geometrical consistency present method directly
enforcing additivity reestimation procedure along x dimensions
heading dimension describe complete geometrical consistency achieved
projection anti symmetric estimates onto geometrically consistent space
simplify presentation focus case global coordinate systems basic
idea applies state relative coordinate systems relationship used recover mean
ij individual state coordinates complex
additivity x dimensions

main observation underlying additivity constraint
fact states embedded geometrical space assuming n states
sn points x axes x xn yn n
respectively state si associated coordinates hxi yi assuming
one global coordinate system mean odometric relation state si state sj
expressed hxj xi yj yi j
maximization phase em iteration rather try maximize respect
n odometric relation vectors hxij yij ij reparameterize specifically
express odometric relation function two n state positions maximize
respect unconstrained n state positions instance x dimension rather
search n maximum likelihood estimates xij use maximization step
n dimensional points x xn calculate xij xj xi moreover since
interested finding best relationships xi xj fix one
f g f b b g b b



fishatkay kaelbling

xi e g x optimal estimates remaining n state positions
variance reestimation remains lag behind policy used eliminate
interdependency update mean variance parameters
additive heading estimation

unfortunately reparameterization described feasible estimation changes
heading due von mises distribution assumption heading measures reparameterizing ij j trying maximize likelihood function respect
parameters obtain set n trigonometric equations terms form cos j sin
enable simple solution
alternative possible use anti symmetric reestimation procedure described
earlier followed perpendicular projection operator mapping resulting headings vector
h ij n n j n satisfy additivity onto vector
headings within additive linear vector space simple orthogonal projection satisfactory
within setting since simply looks additive vector closest non additive one
procedure ignores fact entries non additive vector
lot observations therefore reliable less reliable ones
hardly data intuitively would keep estimates well accounted
intact adapt less reliable estimates meet additivity constraint precisely
heading change estimates states better accounted others
sense transitions
states higher expected counts transition
p
states higher j would project non additive heading
estimates vector onto subspace additive vector space vectors
values non additive
p vector entries well accounted
highest values j diculty latter subspace linear vector
space instance satisfy closure scalar multiplication projection
operator linear spaces cannot applied directly still set vectors form
ane vector space project onto algebraic technique explained
definition
rn n dimensional ane space vectors va set vectors
def
va fua va jua ag linear space
hence pick vector ane space va define translation ta v
v linear space v va translation trivially extended vector
v rn defining ta v v va order project vector v rn onto apply
translation ta v project ta v onto v vector p ta v v
applying inverse transform ta obtain projection v demonstrated
figure linear space figure two dimensional vector space fhx yij xg
ane space fhx yij x g transform ta consists subtracting vector
h solid arrow corresponds direct projection vector v onto point p v
ane space dotted arrows represent projection via translation v ta v
projection latter onto linear vector space inverse translation
p ta v onto ane space






many thanks john hughes introducing us technique



filearning geometrically constrained hmms


x x

p v

v










ta v
p ta v

x x



figure projecting v onto ane vector space fhx yij x g
although procedure preserving additivity headings formally proven preserve monotone convergence likelihood function towards local maximum extensive
experiments consisting hundreds runs shown monotone convergence preserved

choosing initial model

typically instances baum welch initial model picked uniformly
random space possible perhaps trying multiple initial different local likelihood maxima alternative reported shatkay kaelbling
clustering accumulated odometric information simple k means
duda hart taking clusters states observations
recorded obtain state observation counts estimate model parameters
perpendicularity assumed collecting data shown figure k means
assigns cluster state odometric readings recorded close locations
leading reasonable initial however assumption dropped illustrated
figure cumulative rotational error distorts odometric location recorded within
global coordinate system location assigned state multiple visits
varies greatly would recognized simple location clustering
overcome developed alternative initialization heuristics call
tag initialization directly recorded relations states rather
states absolute location clarity description consists mostly illustrative
example concentrates case global consistency constraints enforced
given sequence observations odometric readings e begin clustering odometric
readings buckets number buckets number distinct state transitions
recorded sequence goal stage bucket contain odometric
readings close along three dimensions
achieve start fixing predetermined small standard deviation value along x
dimensions denote standard deviation values x respectively typically
x first odometric reading assigned bucket mean bucket
set value reading rest process subsequent odometric
readings examined next reading within standard deviations along
three dimensions mean existing non empty bucket add bucket


fishatkay kaelbling





































figure bucket assignment example sequence
update bucket mean accordingly assign empty bucket set mean
bucket reading
intuitively heuristic resulting buckets tightly concentrated
mean note clustering duda hart could used
bucketing stage
example would learn state model sequence odometric readings
hx follows
h h h h
h h h h
first stage place readings buckets suppose standard deviation constant
placement shown figure mean value associated bucket shown
well

next stage state tagging phase odometric reading
rt assigned pair states si sj denoting origin state transition took
place destination state transition led respectively conjunction
mean entries ij relation matrix r populated

example cont returning sequence process demonstrated figure assume data recording starts state odometric change
self transitions small standard deviation use well
shown part figure
since first element sequence h two standard deviations away
mean entry relation row state populated pick
next state populate mean mean bucket
h belongs maintain geometrical consistency mean set
shown part b figure populated diagonal entries state
sequence h entry matrix becomes associated bucket
information recorded helping tagging future odometric readings belonging
bucket
next odometric reading h standard deviations populated mean
row current believed state hence pick state set mean
mean bucket reading belongs figure c entry
recorded associated bucket preserve anti symmetry additivity
set set sum set


filearning geometrically constrained hmms






b
















































bucket r

c















































































bucket r

bucket r


bucket r


figure populating odometric relation matrix creating state tagging sequence
similarly updated mean bucket causing setting
bucket associated
stage odometric table fully populated shown part figure state
sequence point h next reading h within one standard
deviation therefore next state entry associated bucket
bucket reading assigned state sequence becomes h
next reading bucket associated relation state tagged
bucket namely state repeating last two readings final state transition
sequence becomes h

note process described illustration simplified general case
need take account rotational error data use state relative coordinate
systems therefore populate entries transformed anti symmetry additivity
constraints
hx yi b tba hx yi b
hx yi c hx yi b tba hx yi b c
defined section


fishatkay kaelbling

possible end tagging rows columns relation
matrix still unpopulated happens little data learn
number states provided large respect actual model
cases trim model number populated rows number
states pick random odometric readings populate rest table improving
estimates later note first suggests method learning number states
model given starting gross estimate number truncating number populated rows odometric table initialization performed
state transition sequence obtained rest initialization
k means initialization deriving state transition counts state transition
sequence assigning observations states assumption state sequence
correct obtaining state transition observation probabilities initialization phase
incur much computational overhead equivalent time wise performing one
additional iteration em procedure

experiments

goal work described far use odometry improve learning topological
fewer iterations less data tested simple robotnavigation world experiments consist running data obtained
simulated model data gathered mobile robot ramona amount
data gathered ramona used proof concept sucient statistical
analysis latter use data obtained simulated model gathered data
used without perpendicularity assumption see section
provided settings

robot domain

robot used experiments ramona modified rwi b robot cylindrical
synchro drive base ultrasonic sensors infrared sensors situated evenly around
circumference infrared sensors used mostly short range obstacle avoidance
ultrasonic sensors longer ranged used obtaining noisy observations
environment experiments described robot follows prescribed path
corridors oce environment department thus decision making
involved hmm sucient model rather complete pomdp
low level software provides level abstraction allows robot move hallways
intersection intersection turn ninety degrees left right software
uses sonar data distinguish doors openings intersections along path stop
robot current action whenever landmark detected stop due
natural termination action due landmark detection considered robot
state
stop ultrasonic data interpretation allows robot perceive three
cardinal directions front left right whether open space door wall
something unknown
encoders robot wheels allow estimate pose position orientation respect pose previous intersection recording sonar observations
low level software written maintained james kurien



filearning geometrically constrained hmms


























































































figure true model corridors ramona traversed arrows represent prescribed path direction

figure true model prescribed path
simulated hallway environment

odometric information robot goes execute next prescribed action
action command issued manually human operator course action performance perception routines subject error path ramona followed consists
connected corridors building include states shown figure
simulation manually generated hmm representing prescribed path robot
complete oce environment department consisting states
associated transition observation odometric distributions transition probabilities
ect action failure rate probability moving
current state correct next state environment predetermined action
probability self transition typically
small probability typically smaller sometimes assigned transitions
experience real robot proves reasonable transition model since
typically robot moves next state correctly error occurs
significant frequency move due sonar interpretation indicating
barrier actually none action command repeated robot usually
performs action correctly moving expected next state observation distribution
typically assigns probabilities true observation perceived
robot state probabilities observations might
perceived example door actually perceived door typically assigned
probability wall assigned probability open space assigned
probability perceived standard deviation around odometric readings
mean
figure shows hmm corresponding simulated hallway environment observations
orientation omitted figure clarity nodes correspond states
environment directed edges correspond corridors arrows point direction
corridors traversed interpretation figures provided
following section


fishatkay kaelbling

evaluation method

number different ways evaluating model learning
none completely satisfactory give insight utility
domain transitions observations usually take place therefore
likely others furthermore relational information gives us rough estimate
metric locations states get qualitative sense plausibility learnt
model extract essential map learnt model consisting states
likely transitions metric measures associated ask whether map
corresponds essential map underlying true world
figures essential versions true figures shown
later essential versions representative learnt ones obtained sequences gathered
perpendicularity assumption black dots represent physical locations states
state assigned unique number multiple state numbers associated single
location typically correspond different orientations robot location larger
black circle represents initial state solid arrows represent likely non self transitions
states dashed arrows represent transitions probability
higher typically due predetermined path taken connectivity
modeled environment low therefore transitions represented dashed arrows
almost likely likely ones note length arrows within plot
significant represents length corridors drawn scale
important note figures provide complete representation
first lack observation orientation information stress fact figures
serve visual aid plot true model looking good topological
model rather geometrical model figures provide geometrical embedding
topological model however even geometry described relation matrix
different topology described transition observation matrices still valid
traditionally simulation experiments learnt model quantitatively compared
actual model generated data induces probability distribution
strings observations asymmetric kullback leibler divergence kullback leibler
two distributions measure good learnt model respect
true model given true probability distribution p fp pn g learnt one
q fq qn g kl divergence q respect p

p jjq

def

n
x


pi log pqi


report terms sampled version kl divergence described juang
rabiner generating sequences sucient length sequences
observations case according distribution induced true model comparing
log likelihood according learnt model true model log likelihood total
difference log likelihood divided total number observations accumulated
sequences giving number roughly measures difference log likelihood
per observation formally stated let true model learnt one generating
k sequences sk length true model sampled kl divergence
ds
k
x
log pr si jm log pr si jm


ds jjm

kt


filearning geometrically constrained hmms


































figure sequence gathered ramona
perpendicularity assumed

figure sequence generated simulator perpendicularity assumed

ignore odometric information applying kl measure thus allowing comparison
purely topological learnt without odometry

within global framework

let ramona go around path depicted figure collect sequence
observations assuming perpendicularity environment every turning
point angle turn thus turn ramona realigns odometric readings
initial x axes figure plots sequence metric coordinates gathered
way accumulating consecutive odometric readings projected hx yi applied
learning data times runs started k means
initial model started tag initial model started random initial
model addition ran standard baum welch ignoring odometric
information times note non determinism even biased initial
since k means clustering starts random seeds low random noise added
data avoid numerical instabilities thus multiple runs give multiple
report obtained tag method
appropriate initialization method general case contrasted
obtained odometric information used comparison four settings
reader referred complete report work shatkay
figure shows essential representations typical learnt starting tag
initial model geometry learnt model strongly corresponds true environment states positions learnt correctly although figure
learnt observation distributions state usually match well true
observations
demonstrate effect odometry quality learnt topological model contrast
plotted learnt odometry representative topological model learnt without
random number cm cm added recorded distances typically several meters
long



fishatkay kaelbling











































































mona traversed







figure learnt model corridors ra



figure topology model learnt
without use odometry

use odometric information figure shows topology typical model learnt without
use odometric information case arcs represent topological relationships
length meaningful initial state shown bold circle clear
topology learnt match characteristic loop topology true environment

obtaining statistically sucient information generated data sequences length
monte carlo sampling hidden markov model whose projection shown
figure one sequences depicted figure figure demonstrates
noise model used simulation indeed compatible noise pattern associated
real robot data used four different settings learning

starting biased tag initial model odometric information
starting biased k means initial model odometric information
starting initial model picked uniformly random odometric information
starting random initial model without odometric information standard baumwelch

sequence four algorithmic settings ran times
keep discussion focused concentrate first last settings
reader referred extensive report shatkay complete discussion
experiments n set correct number states generalization necessary use cross validation regularization methods select model
complexity section suggests one possible heuristic obtaining estimate number
states
figure shows essential version one learnt model obtained sequence shown
figure tag initialization note learnt model completely


filearning geometrically constrained hmms























































figure learnt model simulated hallway environment
accurate respect true model however obvious correspondence
groups states learnt true transitions well
observations shown learnt correctly quality geometry
learnt model simulated large environment varies geometrical
uniformly good case learning smaller environment real robot data
environment gets large global relations remote states ected
geometrical consistency constraints become harder learn still topology
learnt model demonstrated statistical experiments good
table lists kl divergence true learnt model well number
runs convergence reached sequences setting
uses odometric information tag initialization learning
use odometric information averaged runs per sequence stress kl
divergence measure calculated data sequences generated true
model described section sequences learnt
participate testing process
kl divergence respect true model learnt odometry
times smaller learnt without odometric data standard deviation around
means kl distances learnt odometry noodometry setting check significance used simple two sample test
learnt odometric information statistically significantly p lower
average kl divergence others

seq
kl
odo iter

kl
odo iter































table average two learning settings five training sequences


fishatkay kaelbling

addition number iterations required convergence learning odometric
information roughly times smaller required ignoring information
test verifies significance
three initialization settings learnt topologically somewhat inferior
high statistical significance terms kl divergence learnt without
enforcing additivity reported earlier papers shatkay kaelbling likely
strong constraints enforced learning process prevent
searching better areas learning space restrict reach poor local
maxima geometry looks superior cases significantly better however
seems less variability quality geometrical across multiple runs
additivity enforced
details extensive comparison different initialization methods
beyond scope point studies small large
large long data sequences involved random initialization often
lower kl divergence tag initialization
strong bias tag initialization lead peaked compared
less peaked distributions associated true model random initialization leads atter
kl divergence strongly penalizes much peaked
true ones randomly initialized often closer terms measure true
peaked ones learnt initial learning small
sucient training data available tag initialization
clearly superior random ones reader referred complete report
work shatkay comparative study initialization methods
settings

within relative framework

applied described section extended accommodate state relative
constraints listed section data used gathered robot
environment generated simulated model figures
however data generated without assuming perpendicularity means x
coordinates realigned turn global x axes rather
recorded evaluation methods stay described
figure shows projection odometric readings ramona recorded along
x dimensions traversing environment obtaining statistically sucient
information generated data sequences length monte carlo sampling
hidden markov model whose projection shown figure one sequences
depicted figure
figure shows typical model obtained applying enforcing complete
geometrical consistency robot data shown figure tag initialization
note rectangular geometry environment preserved although state
participate loop explained observing corresponding area true
environment depicted figure consisting states clustered bottom left
corner due relatively large number states close together
area true environment recognized ever returned particularly
state loop therefore one transition recorded state state


filearning geometrically constrained hmms






































figure sequence gathered ramona

figure sequence generated simula

perpendicularity assumed

tor perpendicularity assumed





























figure learnt model corridors ramona traversed initialization tag
according expected transition counts calculated projecting
angles maintain additivity described section angle state
therefore compromised allowing geometrical consistency maintain rectangular geometry
among regularly visited states
purpose quantitatively evaluating learning list table kl
divergence true learnt model well number iterations convergence reached simulation sequences without odometric information
averaged runs per sequence table demonstrates kl divergence respect true model learnt odometric data times smaller
learnt without check significance use simple
two sample test learnt odometric information highly statistically significantly p lower average kl divergence others addition number


fishatkay kaelbling

seq
kl
odo iter

kl
odo iter











table average learning settings training sequences
iterations required convergence learning odometric information smaller
required ignoring information test verifies significance p

important point number iterations although much lower automatically imply runs less time non odometric baum welch
major bottleneck caused need compute within forward backward calculations
described section values normal von mises densities require calculation exponent terms rather simple multiplications slowing
iteration current nave implementation however solve augmenting
program look tables obtaining relevant values rather calculating
addition take advantage symmetry relations table cut
amount calculation required possible use fact many odometric relations remain unchanged particularly later iterations one iteration
next therefore values cached shared iterations rather
recalculated iteration

reducing amount data
learning hmms obviously requires visiting states transitioning multiple times
gather sucient data robust statistical estimation intuitively exploiting odometric data
help reduce number visits needed obtaining reliable model
examine uence reduction length data sequences quality learnt
took one sequences used prefixes length complete
sequence increments training sequences ran two algorithmic settings
prefix sequences times repeatedly used kl divergence described
evaluate resulting respect true model prefix
length averaged kl divergence runs
plot figure depicts average kl divergence function sequence length
two settings demonstrates terms kl divergence
uses odometric information robust face data reduction data
points contrast learning without use odometry quickly deteriorates amount
data reduced
note data sequence twice wide odometry used
information element sequence odometry data
recorded however effort recording additional odometric information negligible
well rewarded fact fewer observations less exploration required
obtaining data sequence sucient adequate learning


filearning geometrically constrained hmms






odometry

kl



odometry used





seq length





figure average kl divergence function sequence length

conclusions
odometric information often readily available robotics domain makes possible
learn hidden markov eciently effectively shorter training sequences
importantly contrast traditional perception viewing topological
geometric two distinct types entities shown odometric information
directly incorporated traditional topological hmm model maintaining
convergence reestimation local maximum likelihood function
method uses odometric information two ways first choose initial model
odometric information iterative procedure extends baum welch
used learn topological model environment learning
additional set constrained geometric parameters additional set constrained parameters constitutes extension basic hmm pomdp model transitions observations
even though primarily interested underlying topological model transition
observation probabilities experiments demonstrate use odometric relations
reduce number iterations amount data required improve
resulting model
initialization procedure enforcement additivity constraint relatively
small prove helpful topologically geometrically extensive study shatkay
shows long data sequences generated large enforcing antisymmetry rather additivity leads better topological
cases initialization good additivity may constrain learning
unfavorable area learning large may benefit enforcing anti symmetry
first iterations complete additivity later iterations alternatively may use
enforcing additivity learn separate small portions environment
combining later one complete model similar idea combining small modelfragments complete map environments applied context geometrical
maps recent work leonard feder


fishatkay kaelbling

work presented demonstrates domain specific information constraints
enforced part statistical estimation process resulting better requiring
shorter data sequences strongly believe idea applied domains
robotics particular acquisition hmms use molecular biology may greatly benefit
exploiting geometrical constraints molecular structures similarly temporal
constraints may exploited domains pomdps appropriate decision support
air trac control medicine

acknowledgments
thank sebastian thrun insightful comments throughout work john hughes luis ortiz
helpful advice anthony cassandra code generating random distributions bill smart
sustaining ramona jim kurien providing low level code driving presentation
benefited comments made anonymous referees grateful
work done authors computer science department brown university
supported darpa rome labs initiative grant f nsf grants
iri iri brown university graduate fellowship



filearning geometrically constrained hmms

appendix overview odometric learning
takes input experience sequence e hr v consisting odometric
sequence r observation sequence v defined beginning section
number states assumed given
learn odometric hmm e
initialize matrices b r
see section
max change
max change
calculate forward probabilities
equation

calculate backward probabilities
equation

calculate state occupation probabilities equation

calculate state transition probabilities equation

old old b b

reestimate
equation left

b reestimate b
equation right

r reestimate r
equations
x

x


hr r reestimate r r equations

max change max get max change old
get max change b old b
equations referenced step correspond updates perpendicularity assumption global framework used see shatkay update formulae within
state relative framework
additivity enforced step followed projection reestimated r onto additive
ane space described section addition step substituted procedure
described section reader referred shatkay detail
get max change function takes two matrices returns maximal element wise
absolute difference constant set denote margin error changes
parameters change parameters small enough model regarded
unchanged



fishatkay kaelbling

references
abe n warmuth k computational complexity approximating distributions probabilistic automata machine learning
angluin learning regular sets queries counterexamples information
computation
asada map building mobile robot sensory data iyengar
elfes eds autonomous mobile robots pp ieee computer society press
bartels r estimation bidirectional mixture von mises distributions biometrics

basye k dean kaelbling l p learning dynamics system identification
perceptually challenged agents artificial intelligence
baum l e petrie soules g weiss n maximization technique occurring
statistical analysis probabilistic functions markov chains annals
mathematical statistics
cassandra r kaelbling l p kurien j acting uncertainty discrete
bayesian mobile robot navigation proceedings ieee rsj international
conference intelligent robots systems
degroot h probability statistics nd edition addison wesley
dempster p laird n rubin b maximum likelihood incomplete
data via em journal royal statistical society
dissanayake g newman p clark durrant whyte h f csorba solution
simultaneous localization map building slam ieee transactions
robotics automation
duda r hart p e unsupervised learning clustering chap john wiley
sons
elfes occupancy grids mobile robot perception navigation computer
special issue autonomous intelligent machines
engelson p mcdermott v error correction mobile robot map learning
proceedings ieee international conference robotics automation pp
nice france
gold e complexity automaton identification given data information
control
gumbel e g greenwood j durand circular normal distribution
theory tables american statistical society journal
hopcroft j e ullman j introduction automata theory languages
computation addison wesley


filearning geometrically constrained hmms

juang b h maximum likelihood estimation mixture multivariate stochastic observations markov chains technical journal
juang b h rabiner l r probabilistic distance measure hidden markov
technical journal
koenig simmons r g passive distance learning robot navigation
proceedings thirteenth international conference machine learning pp

koenig simmons r g b unsupervised learning probabilistic robot
navigation proceedings ieee international conference robotics automation
kuipers b byun robot exploration mapping strategy semantic hierarchy spatial representations journal robotics autonomous systems

kullback leibler r information suciency annals mathematical
statistics
leonard j durrant whyte h f cox j dynamic map building autonomous mobile robot iyengar elfes eds autonomous mobile robots
pp ieee computer society press
leonard j j feder h j computationally ecient method large scale concurrent mapping localization hollerbach j kodischek eds proceedings
ninth international symposium robotics
liporace l maximum likelihood estimation multivariate observations markov
sources ieee transactions information theory
mardia k v statistics directional data academic press
mataric j distributed model mobile robot environment learning navigation master thesis mit artificial intelligence laboratory
mclachlan g j krishnan em extensions john wiley
sons
moravec h p sensor fusion certainty grids mobile robots ai magazine

moravec h p elfes high resolution maps wide angle sonar proceedings
international conference robotics automation pp
nourbakhsh powers r birchfield dervish oce navigating robot ai
magazine
pierce kuipers b map learning uninterpreted sensors effectors artificial intelligence


fishatkay kaelbling

rabiner l r tutorial hidden markov selected applications speech
recognition proceedings ieee
rivest r l schapire r e diversity inference finite automata
proceedings ieee twenty eighth annual symposium foundations computer
science pp los angeles california
rivest r l schapire r e inference finite automata homing sequences
proceedings twenty first annual symposium theory computing pp
seattle washington
ron singer tishbi n learning probabilistic automata variable memory length proceedings seventh annual workshop computational learning
theory pp
ron singer tishbi n learnability usage acyclic probabilistic
finite automata proceedings eighth annual workshop computational learning
theory pp
ron singer tishby n learnability usage acyclic probabilistic
finite automata journal computer systems science
shatkay h learning robot navigation ph thesis department computer science brown university providence ri
shatkay h kaelbling l p learning topological maps weak local odometric
information proceedings fifteenth international joint conference artificial
intelligence nagoya japan
shatkay h kaelbling l p heading right direction proceedings
fifteenth international conference machine learning madison wisconsin
simmons r g koenig probabilistic navigation partially observable environments proceedings international joint conference artificial intelligence
smith r self cheeseman p stochastic map uncertain spatial relationships iyengar elfes eds autonomous mobile robots pp ieee
computer society press
thrun learning metric topological maps indoor mobile robot navigation ai
journal
thrun bucken integrating grid topological maps mobile robot
navigation proceedings thirteenth national conference artificial intelligence
pp
thrun bucken b learning maps indoor mobile robot navigation tech rep
cmu cs school computer science carnegie mellon university pittsburgh
pa
thrun burgard w fox probabilistic concurrent map acquisition localization mobile robots machine learning


filearning geometrically constrained hmms

thrun gutmann j fox burgard w kuipers b j b integrating topological metric maps mobile robot navigation statistical proceedings
fifteenth national conference artificial intelligence pp
vapnik v n nature statistical learning theory springer




