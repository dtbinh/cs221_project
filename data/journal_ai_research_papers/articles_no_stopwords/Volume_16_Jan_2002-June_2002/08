journal artificial intelligence

submitted published

smote synthetic minority sampling technique
nitesh v chawla

chawla csee usf edu

department computer science engineering enb
university south florida
e fowler ave
tampa fl usa

kevin w bowyer

kwb cse nd edu

department computer science engineering
fitzpatrick hall
university notre dame
notre dame usa

lawrence hall

hall csee usf edu

department computer science engineering enb
university south florida
e fowler ave
tampa fl usa

w philip kegelmeyer

wpk california sandia gov

sandia national laboratories
biosystems department p box ms
livermore ca usa

abstract
construction classiers imbalanced datasets described
dataset imbalanced classication categories approximately equally represented often real world data sets predominately composed normal examples
small percentage abnormal interesting examples case
cost misclassifying abnormal interesting example normal example
often much higher cost reverse error sampling majority normal class proposed good means increasing sensitivity classier
minority class shows combination method sampling
minority abnormal class sampling majority normal class achieve
better classier performance roc space sampling majority class
shows combination method sampling minority class
sampling majority class achieve better classier performance roc
space varying loss ratios ripper class priors naive bayes method
sampling minority class involves creating synthetic minority class examples
experiments performed c ripper naive bayes classier method
evaluated area receiver operating characteristic curve auc
roc convex hull strategy

introduction
dataset imbalanced classes approximately equally represented imbalance
order prevalent fraud detection imbalance
c

ai access foundation morgan kaufmann publishers rights reserved

fichawla bowyer hall kegelmeyer

reported applications provost fawcett
attempts deal imbalanced datasets domains fraudulent telephone calls
fawcett provost telecommunications management ezawa singh norton
text classication lewis catlett dumais platt heckerman sahami
mladenic grobelnik lewis ringuette cohen detection
oil spills satellite images kubat holte matwin
performance machine learning typically evaluated predictive
accuracy however appropriate data imbalanced costs
dierent errors vary markedly example consider classication pixels mammogram images possibly cancerous woods doss bowyer solka priebe kegelmeyer
typical mammography dataset might contain normal pixels abnormal
pixels simple default strategy guessing majority class would give predictive accuracy however nature application requires fairly high rate correct
detection minority class allows small error rate majority class
order achieve simple predictive accuracy clearly appropriate situations receiver operating characteristic roc curve standard technique
summarizing classier performance range tradeos true positive false
positive error rates swets area curve auc accepted traditional performance metric roc curve duda hart stork bradley lee
roc convex hull used robust method identifying potentially
optimal classiers provost fawcett line passes point convex
hull line slope passing another point
larger true positive tp intercept thus classier point optimal
distribution assumptions tandem slope
machine learning community addressed issue class imbalance two ways
one assign distinct costs training examples pazzani merz murphy ali hume
brunk domingos sample original dataset oversampling minority class sampling majority class kubat matwin
japkowicz lewis catlett ling li chawla
bowyer hall kegelmeyer blends sampling majority class
special form sampling minority class experiments datasets
c decision tree classier quinlan ripper cohen b naive bayes
classier improves previous sampling modifying loss
ratio class priors approaches auc roc convex hull
section gives overview performance measures section reviews
closely related work dealing imbalanced datasets section presents details
section presents experimental comparing
sampling approaches section discusses suggests directions future
work

performance measures
performance machine learning typically evaluated confusion matrix
illustrated figure class columns predicted class
rows actual class confusion matrix n number negative examples


fismote

predicted
negative

predicted
positive

actual
negative

tn

fp

actual
positive

fn

tp

figure confusion matrix
correctly classied true negatives f p number negative examples incorrectly
classied positive false positives f n number positive examples incorrectly
classied negative false negatives p number positive examples correctly
classied true positives
predictive accuracy performance measure generally associated machine learning dened accuracy p n p f p n f n
context balanced datasets equal error costs reasonable use error rate
performance metric error rate accuracy presence imbalanced datasets
unequal error costs appropriate use roc curve similar
techniques ling li drummond holte provost fawcett bradley
turney
roc curves thought representing family best decision boundaries
relative costs tp fp roc curve x axis represents f p f p n f p
axis represents p p p f n ideal point roc curve would
positive examples classied correctly negative examples
misclassied positive one way roc curve swept manipulating
balance training samples class training set figure shows illustration
line x represents scenario randomly guessing class area roc
curve auc useful metric classier performance independent decision
criterion selected prior probabilities auc comparison establish dominance
relationship classiers roc curves intersecting total auc
average comparison lee however specic cost class
distributions classier maximum auc may fact suboptimal hence
compute roc convex hulls since points lying roc convex hull
potentially optimal provost fawcett kohavi provost fawcett

previous work imbalanced datasets
kubat matwin selectively sampled majority class keeping
original population minority class used geometric mean performance measure classier related single point roc curve
minority examples divided four categories noise overlapping positive class decision region borderline samples redundant samples safe samples
borderline examples detected tomek links concept tomek another


fichawla bowyer hall kegelmeyer

roc




ideal point

percent
true

x

positive
increased undersampling
majority class moves
operating point
upper right
original data set



percent false positive



figure illustration sweeping roc curve sampling increased
sampling majority negative class move performance
lower left point upper right

related work proposed shrink system classies overlapping region minority positive majority negative classes positive searches best positive
region kubat et al
japkowicz discussed eect imbalance dataset evaluated three
strategies sampling resampling recognition induction scheme focus
sampling approaches experimented articial data order easily
measure construct concept complexity two resampling methods considered
random resampling consisted resampling smaller class random consisted
many samples majority class focused resampling consisted resampling
minority examples occurred boundary minority
majority classes random sampling considered involved sampling
majority class samples random numbers matched number minority
class samples focused sampling involved sampling majority class samples
lying away noted sampling approaches eective
observed sophisticated sampling techniques give clear advantage
domain considered japkowicz
one particularly relevant work ling li
combined sampling minority class sampling majority
class used lift analysis instead accuracy measure classiers performance
proposed test examples ranked condence measure lift used
evaluation criteria lift curve similar roc curve tailored


fismote

marketing analysis ling li one experiment sampled
majority class noted best lift index obtained classes equally
represented ling li another experiment sampled positive
minority examples replacement match number negative majority examples
number positive examples sampling sampling combination
provide signicant improvement lift index however oversampling diers
solberg solberg considered imbalanced data sets oil slick
classication sar imagery used sampling sampling techniques
improve classication oil slicks training data distribution oil
slicks look alikes giving prior probability look alikes imbalance
would lead learner without appropriate loss functions methodology modify
priors classify almost look alikes correctly expense misclassifying many
oil slick samples solberg solberg overcome imbalance
sampled replacement samples oil slick randomly sampled
samples non oil slick class create dataset equal probabilities
learned classier tree balanced data set achieved error rate
oil slicks leave one method error estimation look alikes achieved
error rate solberg solberg
another similar work domingos compares
metacost majority sampling minority sampling
nds metacost improves sampling preferable minority sampling error classiers made cost sensitive probability
class example estimated examples relabeled optimally
respect misclassication costs relabeling examples expands decision
space creates samples classier may learn domingos
feed forward neural network trained imbalanced dataset may learn discriminate enough classes derouin brown fausett schneider
authors proposed learning rate neural network adapted statistics
class representation data calculated attention factor proportion
samples presented neural network training learning rate network
elements adjusted attention factor experimented articially
generated training set real world training set multiple two
classes compared replicating minority class samples
balance data set used training classication accuracy minority class
improved
lewis catlett examined heterogeneous uncertainty sampling supervised
learning method useful training samples uncertain classes training
samples labeled incrementally two phases uncertain instances passed
next phase modied c include loss ratio determining class
values leaves class values determined comparison probability
threshold lr lr lr loss ratio lewis catlett
information retrieval ir domain dumais et al mladenic grobelnik
lewis ringuette cohen faces class imbalance
dataset document web page converted bag words representation


fichawla bowyer hall kegelmeyer

feature vector reecting occurrences words page constructed usually
instances interesting category text categorization overrepresentation negative class information retrieval cause
evaluating classiers performances since error rate good metric skewed
datasets classication performance information retrieval usually
measured precision recall
recall

tp
tp fn

precision

tp
tp fp

mladenic grobelnik proposed feature subset selection deal
imbalanced class distribution ir domain experimented
feature selection methods found odds ratio van rijsbergen harper porter
combined naive bayes classier performs best domain odds
ratio probabilistic measure used rank documents according relevance
positive class minority class information gain word hand
pay attention particular target class computed per word class
imbalanced text dataset assuming negative class features
associated negative class odds ratio incorporates target class information
metric giving better compared information gain text categorization
provost fawcett introduced roc convex hull method estimate
classier performance imbalanced datasets note unequal
class distribution unequal error costs related little work done
address provost fawcett roc convex hull method
roc space used separate classication performance class cost distribution
information
summarize literature sampling majority class enables better classiers
built sampling minority class combination two done
previous work lead classiers outperform built utilizing undersampling however sampling minority class done sampling
replacement original data uses dierent method sampling

smote synthetic minority sampling technique
minority sampling replacement
previous ling li japkowicz discussed sampling
replacement noted doesnt signicantly improve minority class recognition
interpret underlying eect terms decision regions feature space essentially
minority class sampled increasing amounts eect identify similar
specic regions feature space decision region minority class
eect decision trees understood plots figure


fismote

attributes data original mammography dataset

attributes data original mammography dataset










attribute

attribute















































attribute









attribute



b

attributes data original mammography dataset


attribute



















attribute







c
figure decision region three minority class samples shown reside
building decision tree decision region indicated solid line
rectangle b zoomed view chosen minority class samples
dataset small solid line rectangles decision regions oversampling minority class replication c zoomed view chosen
minority class samples dataset dashed lines decision region
sampling minority class synthetic generation



fichawla bowyer hall kegelmeyer

data plot figure extracted mammography dataset woods
et al minority class samples shown majority class samples
shown plot figure region indicated solid line rectangle
majority class decision region nevertheless contains three minority class samples
shown false negatives replicate minority class decision region
minority class becomes specic cause splits decision tree
lead terminal nodes leaves learning tries learn
specic regions minority class essence overtting replication minority
class cause decision boundary spread majority class region thus
figure b three samples previously majority class decision region
specic decision regions
smote
propose sampling minority class sampled creating synthetic examples rather sampling replacement
inspired technique proved successful handwritten character recognition ha
bunke created extra training data performing certain operations
real data case operations rotation skew natural ways perturb
training data generate synthetic examples less application specic manner
operating feature space rather data space minority class sampled
taking minority class sample introducing synthetic examples along line
segments joining k minority class nearest neighbors depending upon
amount sampling required neighbors k nearest neighbors randomly
chosen implementation currently uses nearest neighbors instance
amount sampling needed two neighbors nearest neighbors chosen one sample generated direction synthetic samples
generated following way take dierence feature vector sample
consideration nearest neighbor multiply dierence random number
add feature vector consideration causes
selection random point along line segment two specic features
eectively forces decision region minority class become general
smote next page pseudo code smote table shows
example calculation random synthetic samples amount sampling
parameter system series roc curves generated dierent
populations roc analysis performed
synthetic examples cause classier create larger less specic decision
regions shown dashed lines figure c rather smaller specic
regions general regions learned minority class samples rather
subsumed majority class samples around eect decision trees generalize better figures compare minority sampling
replacement smote experiments conducted mammography dataset
examples majority class examples minority class
originally approximately examples majority class examples
data available usf intelligent systems lab http morden csee usf edu chawla



fismote

minority class training set used fold cross validation minority class
sampled original size graphs
tree sizes minority sampling replacement higher degrees
replication much greater smote minority class recognition
minority sampling replacement technique higher degrees replication isnt
good smote
smote n k
input number minority class samples amount smote n number nearest
neighbors k
output n synthetic minority class samples
n less randomize minority class samples random
percent smoted
n

randomize minority class samples

n

n
endif
n int n amount smote assumed integral multiples

k number nearest neighbors
numattrs number attributes
sample array original minority class samples
newindex keeps count number synthetic samples generated initialized
synthetic array synthetic samples
compute k nearest neighbors minority class sample


compute k nearest neighbors save indices nnarray

populate n nnarray
endfor
populate n nnarray function generate synthetic samples
n

choose random number k call nn step chooses one
k nearest neighbors

attr numattrs

compute dif sample nnarray nn attr sample attr

compute gap random number

synthetic newindex attr sample attr gap dif

endfor

newindex

n n
endwhile
return end populate
end pseudo code



fichawla bowyer hall kegelmeyer

consider sample let nearest neighbor
sample k nearest neighbors identied
one k nearest neighbors
let
f f f f
f f f f
samples generated
f f rand
rand generates random number
table example generation synthetic examples smote

pruned decision tree size vs degree minority oversampling




decisiion tree size number nodes










synthetic data
replicated data




















degree minority oversampling







figure comparison decision tree sizes replicated sampling smote
mammography dataset



fismote

minority correct vs degree minority oversampling


minority correct







synthetic data
replicated data


























degree minority oversampling

figure comparison minority correct replicated sampling smote
mammography dataset

sampling smote combination
majority class sampled randomly removing samples majority class
population minority class becomes specied percentage majority class
forces learner experience varying degrees sampling higher degrees
sampling minority class larger presence training set describing
experiments terminology sample majority class
would mean modied dataset contain twice many elements
minority class majority class minority class samples
majority class samples sample majority majority
class would end samples applying combination sampling
sampling initial bias learner towards negative majority class reversed
favor positive minority class classiers learned dataset perturbed
smoting minority class sampling majority class

experiments
used three dierent machine learning experiments figure provides
overview experiments
c compared combinations smote sampling plain
sampling c release quinlan base classier


fichawla bowyer hall kegelmeyer

smote
undersampling

c

loss ratio
modify costs majority minority
varied
classes changing priors

ripper

naive bayes

rocs generated smote undersampling
loss ratio comparisons performance
evaluated auc roc convex hull

rocs generated comparison
smote sampling c
smote c naive bayes
performance evaluated auc roc convex hull

figure experiments overview

ripper compared combinations smote sampling
plain sampling ripper cohen b base classier
varied rippers loss ratio cohen singer lewis catlett
means varying misclassication cost compared eect
variation combination smote sampling reducing loss
ratio able build set rules minority class
naive bayes classifier naive bayes classier made cost sensitive
varying priors minority class varied priors minority
class times majority class compared c smote
sampling combination

dierent learning allowed smote compared methods
handle misclassication costs directly fp tp averaged fold
cross validation runs data combinations minority class examples
sampled calculating nearest neighbors generating synthetic examples
auc calculated trapezoidal rule extrapolated extra point tp
fp roc curve computed roc convex hull
identify optimal classiers points lying hull potentially optimal
classiers provost fawcett
source code downloaded http fuzzy cs uni magdeburg de borgelt software html



fismote

datasets
experimented nine dierent datasets datasets summarized table
datasets vary extensively size class proportions thus oering dierent
domains smote order increasing imbalance
pima indian diabetes blake merz classes samples
data used identify positive diabetes cases population near phoenix
arizona number positive class samples good sensitivity
detection diabetes cases desirable attribute classier
phoneme dataset elena project aim dataset
distinguish nasal class oral sounds class features
class distribution samples class samples class
adult dataset blake merz samples samples
belonging minority class dataset continuous features nominal
features smote smote nc see section evaluated
dataset smote extracted continuous features generated
dataset continuous features
e state data hall mohney kier consists electrotopological state
descriptors series compounds national cancer institutes yeast anticancer drug screen e state descriptors nci yeast anticancer drug screen
generated tripos inc briey series compounds
tested series yeast strains given concentration test
high throughput screen one concentration subject contamination etc growth inhibition yeast strain exposed given
compound respect growth yeast neutral solvent measured
activity classes active least one single yeast strain inhibited
inactive yeast strain inhibited
dataset samples samples active compounds
satimage dataset blake merz classes originally chose
smallest class minority class collapsed rest classes one
done provost et al gave us skewed class dataset
majority class samples minority class samples
forest cover dataset uci repository blake merz
dataset classes samples dataset prediction forest
cover type cartographic variables since system currently works binary classes extracted data two classes dataset ignored rest
approaches work two classes ling li japkowicz
kubat matwin provost fawcett two classes considered ponderosa pine samples cottonwood willow
ftp dice ucl ac directory pub neural nets elena databases
would thank steven eschrich providing dataset description us



fichawla bowyer hall kegelmeyer

dataset
pima
phoneme
adult
e state
satimage
forest cover
oil
mammography


majority class










minority class










table dataset distribution
samples nevertheless smote technique applied multiple class well specifying class smote however
focused classes explicitly represent positive negative classes
oil dataset provided robert holte used kubat et al
dataset oil slick samples non oil slick samples
mammography dataset woods et al samples calcications look predictive accuracy measure goodness classier
case default accuracy would every sample labeled noncalcication desirable classier predict calcications
correctly
dataset generated exodusii data avatar
chawla hall version mustafa visualization tool portion
crushed marked interesting rest
marked unknown dataset size samples samples marked
interesting generated
roc creation
roc curve smote produced c ripper create classier
one series modied training datasets given roc curve produced rst
sampling minority class specied degree sampling majority
class increasing degrees generate successive points curve amount
sampling identical plain sampling corresponding point
roc curve dataset represents number majority class samples dierent
roc curves produced starting dierent levels minority sampling roc
curves generated varying loss ratio ripper
varying priors minority class original distribution times
majority class naive bayes classier
mustafa visualization tool developed mike glass sandia national labs



fismote

phoneme roc








tp

underc
smotec
naive bayes
hull




























fp

figure phoneme comparison smote c c naive bayes smotec dominates naive bayes c roc space smotec classiers potentially optimal classiers

figures experimental roc curves obtained nine datasets
three classiers roc curve plain sampling majority class
ling li japkowicz kubat matwin provost fawcett
compared combining synthetic minority class sampling smote
majority class sampling plain sampling curve labeled
smote sampling combination roc curve labeled smote depending size relative imbalance dataset one smote undersampling curves created best smote combined
sampling plain sampling curve graphs smote roc curve
c compared roc curve obtained varying priors minority
class naive bayes classier labeled naive bayes smote
loss ratio roc curves generated ripper compared given family
roc curves roc convex hull provost fawcett generated roc
convex hull generated grahams orourke reference
roc curve would obtained minority sampling replication
figure
point roc curve classier c ripper learned
particular combination sampling smote classier c ripper
learned plain sampling classier ripper learned loss ratio
classier naive bayes learned dierent prior minority class point
represents average tp fp fold cross validation lower leftmost
point given roc curve raw dataset without majority class

fichawla bowyer hall kegelmeyer

phoneme roc ripper




tp





underripper
smoteripper
loss ratio
hull


















fp











figure phoneme comparison smote ripper ripper modifying loss
ratio ripper smote ripper dominates ripper loss ratio
roc space smote ripper classiers lie roc convex hull

pima roc








tp





underc
smotec
naive bayes
hull






























fp

figure pima indians diabetes comparison smote c c naive
bayes naive bayes dominates smote c roc space



fismote

pima roc ripper








tp


underripper
smoteripper
loss ratio
hull






























fp

figure pima indians diabetes comparison smote ripper ripper
modifying loss ratio ripper smote ripper dominates ripper
loss ratio roc space

sampling minority class sampling minority class sampled
majority class sampled

amount majority class sampling minority class oversampling depended dataset size class proportions instance consider
roc curves figure mammography dataset three curves one
plain majority class sampling range sampling varied
dierent intervals one combination smote majority class
sampling one naive bayes one roc convex hull curve roc
curve shown figure minority class sampled point
smote roc curves represents combination synthetic sampling undersampling amount sampling follows range plain sampling
better understanding roc graphs shown dierent sets roc curves
one datasets appendix
dataset smote lesser degree datasets
due structural nature dataset dataset structural
neighborhood already established mesh geometry smote lead creating
neighbors surface hence interesting since looking
feature space physics variables structural information
roc curves trend increase amount sampling coupled
sampling minority classication accuracy increases course expense
majority class errors almost roc curves smote dom

fichawla bowyer hall kegelmeyer

satimage roc







underc
smotec
naive bayes
hull

tp


























fp











figure satimage comparison smote c c naive bayes
roc curves naive bayes smote c overlap however
higher tps points smote c lie roc convex hull

satimage roc ripper








tp


underripper
smoteripper
loss ratio
hull
























fp











figure satimage comparison smote ripper ripper modifying loss
ratio ripper smote ripper dominates roc space roc convex
hull mostly constructed points smote ripper



fismote

covtype roc








tp


underc
smotec
naive bayes
hull






















fp











figure forest cover comparison smote c c naive bayes
smote c c roc curves close however points smote c roc curve lie roc convex
hull thus establishing dominance

inates adhering denition roc convex hull potentially optimal
classiers ones generated smote
auc calculation
area roc curve auc calculated form trapezoid rule
lower leftmost point given roc curve classiers performance raw data
upper rightmost point curve naturally end
point point added necessary order aucs compared
range fp
aucs listed table datasets combined synthetic minority sampling majority sampling able improve plain majority
sampling c base classier thus smote provides
improvement correct classication data underrepresented class
conclusion holds examination roc convex hulls entries
missing table smote applied amounts datasets
amount smote less less skewed datasets included aucs
ripper naive bayes roc convex hull identies smote classiers potentially optimal compared plain sampling treatments misclassication
costs generally exceptions follows pima dataset naive bayes dominates
smote c oil dataset ripper dominates smote ripper
dataset smote classifier classifier c ripper classifier roc


fichawla bowyer hall kegelmeyer

covtype roc ripper






tp


underripper
smoteripper
loss ratio
hull




















fp











figure forest cover comparison smote ripper ripper modifying
loss ratio ripper smote ripper shows domination roc space
points smote ripper curve lie roc convex hull

oil roc








tp






underc
smotec
naive bayes
hull




















fp











figure oil comparison smote c c naive bayes although
smote c c roc curves intersect points points
smote c curve lie roc convex hull



fismote

oil roc ripper






tp


underripper
smoteripper
loss ratio
hull




















fp











figure oil comparison smote ripper ripper modifying loss ratio
ripper ripper smote ripper curves intersect points
ripper curve lie roc convex hull

mammography roc







underc
smotec
naive bayes
hull

tp


























fp











figure mammography comparison smote c c naive bayes
smote c c curves intersect roc space however
virtue number points roc convex hull smote c
potentially optimal classiers



fichawla bowyer hall kegelmeyer

mammography roc ripper








tp


underripper
smoteripper
loss ratio
hull






















fp











figure mammography comparison smote ripper ripper modifying
loss ratio ripper smote ripper dominates roc space tp
mammography roc c








tp






smote
replicate
hull




















fp











figure comparison sampling minority class examples smote oversampling minority class examples replication mammography
dataset



fismote

estate roc








tp






underc
smotec
naive bayes
hull




















fp











figure e state comparison smote c c naive bayes
smote c c curves intersect roc space however
smote c potentially optimal classiers number
points roc convex hull

estate roc ripper








tp






underripper
smoteripper
loss ratio
hull




















fp











figure e state comparison smote ripper ripper modifying loss
ratio ripper smote ripper potentially optimal classiers
number points roc convex hull



fichawla bowyer hall kegelmeyer

roc








tp







underc
smotec
naive bayes
hull




















fp











figure comparison smote c c naive bayes smotec c roc curves overlap roc space

roc ripper








tp







underripper
smoteripper
loss ratio
hull




















fp











figure comparison smote ripper ripper modifying loss ratio
ripper smote ripper ripper roc curves overlap
roc space



fismote

dataset



pima
phoneme
satimage
forest cover
oil
mammography
e state












smote




smote










smote


smote


smote


smote






























table aucs c base classier best highlighted bold

curves overlap roc space datasets smote classifier
potentially optimal classiers
additional comparison changing decision thresholds
provost suggested simply changing decision threshold
considered alternative sophisticated approaches case c
would mean changing decision threshold leaves decision trees example
leaf could classify examples minority class even training
examples leaf represent majority class experimented setting decision
thresholds leaves c decision tree learner
experimented phoneme
dataset figure shows comparison smote sampling combination
c learning tuning bias towards minority class graph shows
smote sampling combination roc curve dominating entire
range values
additional comparison one sided selection shrink
oil dataset followed slightly dierent line experiments obtain
comparable kubat et al alleviate imbalanced datasets
authors proposed one sided selection sampling majority class kubat
matwin b shrink system kubat et al table contains
kubat et al acc accuracy positive minority examples
acc accuracy negative majority examples figure shows trend
acc acc one combination smote strategy varying degrees undersampling majority class axis represents accuracy x axis represents
percentage majority class sampled graphs indicate band
sampling comparable achieved
shrink better shrink cases table summarizes
smote sampling combination tried combinations smote
varying degrees sampling achieved comparable


fichawla bowyer hall kegelmeyer

phoneme roc comparison smote c variation decision thresholds




tp


smote
varying c decision thresholds
hull

























fp

figure smote sampling combination c learning tuning
bias towards minority class

smote undersampling




accuracy



accuracy majority negative class
accuracy minority positive class





















percentage undersampling majority class





figure smote ou sampling combination performance

shrink smote directly comparable though
see dierent data points smote oers clear improvement one sided selection


fismote

method
shrink
one sided selection

acc



acc



table cross validation kubat et al

sampling

















acc

















acc

















table cross validation smote smote oil data set



fichawla bowyer hall kegelmeyer

future work
several topics considered line automated adaptive
selection number nearest neighbors would valuable dierent strategies
creating synthetic neighbors may able improve performance selecting
nearest neighbors focus examples incorrectly classied may improve
performance minority class sample could possibly majority class sample
nearest neighbor rather minority class sample crowding likely contribute
redrawing decision surfaces favor minority class addition
topics following subsections discuss two possible extensions smote
application smote information retrieval
smote nc
smote currently handle data sets nominal features
generalized handle mixed datasets continuous nominal features call
synthetic minority sampling technique nominal continuous smote nc
tested adult dataset uci repository smote nc
described
median computation compute median standard deviations continuous
features minority class nominal features dier sample
potential nearest neighbors median included euclidean distance
computation use median penalize dierence nominal features
amount related typical dierence continuous feature values
nearest neighbor computation compute euclidean distance feature
vector k nearest neighbors identied minority class sample
feature vectors minority class samples continuous feature space
every diering nominal feature considered feature vector
potential nearest neighbor include median standard deviations previously
computed euclidean distance computation table demonstrates example
f b c let sample computing nearest
neighbors
f e
f b k
euclidean distance f f would
eucl sqrt med med
med median standard deviations continuous features minority class
median term included twice feature numbers bd ce
dier two feature vectors f f

table example nearest neighbor computation smote nc



fismote

populate synthetic sample continuous features synthetic minority
class sample created smote described earlier
nominal feature given value occuring majority k nearest neighbors
smote nc experiments reported set smote
except fact examine one dataset smote nc adult dataset
diers typical performs worse plain sampling auc
shown figures extracted continuous features separate eect
smote smote nc dataset determine whether oddity
due handling nominal features shown figure even smote
continuous features applied adult dataset achieve better performance
plain sampling minority class continuous features high
variance synthetic generation minority class samples could overlapping
majority class space thus leading false positives plain sampling
hypothesis supported decreased auc measure smote degrees
greater higher degrees smote lead minority class samples
dataset thus greater overlap majority class decision space
adult smotenc








tp


underc
smotencc
naive bayes
hull
























fp











figure adult comparison smote c c naive bayes smotec c roc curves overlap roc space

smote n
potentially smote extended nominal features smote n
nearest neighbors computed modied version value dierence metric stanll
waltz proposed cost salzberg value dierence metric vdm
looks overlap feature values feature vectors matrix dening distance


fichawla bowyer hall kegelmeyer

adult roc ripper






tp





underripper
smoteripper
loss ratio
hull




















fp











figure adult comparison smote ripper ripper modifying loss ratio ripper smote ripper ripper roc curves overlap
roc space
adult continuous c




tp






smote


















fp











figure adult continuous features overlap smote c underc observed scenario well



fismote

corresponding feature values feature vectors created distance
two corresponding feature values dened follows
v v

n

c





c



c k

c



equation v v two corresponding feature values c total
number occurrences feature value v c number occurrences feature
value v class similar convention applied c c k constant
usually set equation used compute matrix value dierences
nominal feature given set feature vectors equation gives geometric distance
xed nite set values cost salzberg cost salzbergs modied vdm
omits weight term wfa included computation stanll waltz
eect making symmetric distance two feature vectors given

x wx wy

n


xi yi r





r yields manhattan distance r yields euclidean distance cost
salzberg wx wy exemplar weights modied vdm wy
example feature vector wx bias towards reliable examples feature
vectors computed ratio number uses feature vector number
correct uses feature vector thus accurate feature vectors wx
smote n ignore weights equation smote n used
classication purposes directly however redene weights give weight
minority class feature vectors falling closer majority class feature vectors thus
making minority class features appear away feature vector
consideration since interested forming broader accurate regions
minority class weights might used avoid populating along neighbors fall
closer majority class generate minority class feature vectors create
set feature values taking majority vote feature vector consideration
k nearest neighbors table shows example creating synthetic feature vector
let f b c e feature vector consideration
let nearest neighbors
f f c g n
f h b c n
application smote n would create following feature vector
fs b c n
table example smote n



fichawla bowyer hall kegelmeyer

application smote information retrieval
investigating application smote information retrieval ir ir come plethora features potentially many categories smote would
applied conjunction feature selection transforming given
document web page bag words format
interesting comparison smote would combination naive bayes
odds ratio odds ratio focuses target class ranks documents according
relevance target positive class smote focuses target class creating
examples class

summary
smote improve accuracy classiers
minority class smote provides sampling combination
smote sampling performs better plain sampling smote
tested variety datasets varying degrees imbalance varying amounts
data training set thus providing diverse testbed combination smote
sampling performs better domination roc space varying
loss ratios ripper varying class priors naive bayes classier methods
could directly handle skewed class distribution smote forces focused learning
introduces bias towards minority class pima least skewed dataset
naive bayes classier perform better smote c oil
dataset ripper perform better smote ripper dataset
smote classifier classifier roc curves overlap roc space
rest datasets smote classifier performs better classifier loss ratio
naive bayes total experiments performed smote classifier
perform best experiments
interpretation synthetic minority sampling improves performance
minority sampling replacement fairly straightforward consider
eect decision regions feature space minority sampling done
replication sampling replacement versus introduction synthetic examples
replication decision region classication decision minority
class actually become smaller specic minority samples region
replicated opposite desired eect method synthetic sampling
works cause classier build larger decision regions contain nearby minority
class points reasons may applicable smote performs better
rippers loss ratio naive bayes methods nonetheless still learning
information provided dataset albeit dierent cost information smote
provides related minority class samples learn thus allowing learner carve
broader decision regions leading coverage minority class

acknowledgments
partially supported united states department energy
sandia national laboratories asci views data discovery program contract number


fismote

de ac thank robert holte providing oil spill dataset used
thank foster provost clarifying method satimage
dataset would thank anonymous reviewers insightful
comments suggestions



fichawla bowyer hall kegelmeyer

appendix roc graphs oil dataset
following gures dierent sets roc curves oil dataset figure
shows roc curves oil dataset included main text figure b shows
roc curves without roc convex hull figure c shows two convex hulls
obtained without smote roc convex hull shown dashed lines stars
figure c computed including c naive bayes family
roc curves roc convex hull shown solid line small circles figure c
computed including smote c c naive bayes family
roc curves roc convex hull smote dominates roc convex hull without
smote hence smote c contributes optimal classiers
oil

















tp











underc
smotec
naive bayes












underc
smotec
naive bayes
hull














fp


























fp









b
oil roc convex hulls










tp

tp

oil roc



convex hull smote
convex hull without smote






















fp











c
figure roc curves oil dataset roc curves smote c underc naive bayes roc convex hull b roc curves smotec c naive bayes c roc convex hulls without
smote





fismote

references
blake c merz c
uci repository machine learning databases
http www ics uci edu mlearn mlrepository html department information
computer sciences university california irvine
bradley p use area roc curve evaluation
machine learning pattern recognition
chawla n bowyer k hall l kegelmeyer p smote synthetic minority
sampling technique international conference knowledge computer systems pp national center software technology mumbai india
allied press
chawla n hall l modifying mustafa capture salient data tech rep
isl university south florida computer science eng dept
cohen w learning classify english text ilp methods proceedings th international workshop inductive logic programming pp
department computer science katholieke universiteit leuven
cohen w w b fast eective rule induction proc th international conference machine learning pp lake tahoe ca morgan kaufmann
cohen w w singer context sensitive learning methods text categorization frei h p harman schauble p wilkinson r eds proceedings
sigir th acm international conference development
information retrieval pp zurich ch acm press york us
cost salzberg weighted nearest neighbor learning
symbolic features machine learning
derouin e brown j fausett l schneider neural network training
unequally represented classes intellligent engineering systems artificial
neural networks pp york asme press
domingos p metacost general method making classiers cost sensitive
proceedings fifth acm sigkdd international conference knowledge
discovery data mining pp san diego ca acm press
drummond c holte r explicitly representing expected cost alternative
roc representation proceedings sixth acm sigkdd international
conference knowledge discovery data mining pp boston acm
duda r hart p stork pattern classification wiley interscience
dumais platt j heckerman sahami inductive learning representations text categorization proceedings seventh
international conference information knowledge management pp


fichawla bowyer hall kegelmeyer

ezawa k j singh norton w learning goal oriented bayesian
networks telecommunications risk management proceedings international conference machine learning icml pp bari italy morgan
kauman
fawcett provost f combining data mining machine learning effective user prole proceedings nd international conference knowledge
discovery data mining pp portland aaai
ha bunke h line handwritten numeral recognition perturbation
method pattern analysis machine intelligence
hall l mohney b kier l electrotopological state structure information
atomic level molecular graphs journal chemical information
computer science
japkowicz n class imbalance signicance strategies proceedings international conference artificial intelligence ic ai
special track inductive learning las vegas nevada
kubat holte r matwin machine learning detection oil
spills satellite radar images machine learning
kubat matwin addressing curse imbalanced training sets one
sided selection proceedings fourteenth international conference machine
learning pp nashville tennesse morgan kaufmann
lee noisy replication skewed binary classication computational statistics
data analysis
lewis catlett j heterogeneous uncertainity sampling supervised learning proceedings eleventh international conference machine learning pp
san francisco ca morgan kaufmann
lewis ringuette comparison two learning text
categorization proceedings sdair rd annual symposium document
analysis information retrieval pp
ling c li c data mining direct marketing solutions
proceedings fourth international conference knowledge discovery data
mining kdd york ny aaai press
mladenic grobelnik feature selection unbalanced class distribution
naive bayes proceedings th international conference machine
learning pp morgan kaufmann
orourke j computational geometry c cambridge university press uk
pazzani merz c murphy p ali k hume brunk c reducing
misclassication costs proceedings eleventh international conference
machine learning san francisco ca morgan kaumann


fismote

provost f fawcett robust classication imprecise environments machine learning
provost f fawcett kohavi r case accuracy estimation
comparing induction proceedings fifteenth international
conference machine learning pp madison wi morgan kaumann
quinlan j c programs machine learning morgan kaufmann san mateo
ca
solberg solberg r large scale evaluation features automatic
detection oil spills ers sar images international geoscience remote
sensing symposium pp lincoln ne
stanll c waltz toward memory reasoning communications
acm
swets j measuring accuracy diagnostic systems science
tomek two modications cnn ieee transactions systems man
cybernetics
turney p cost sensitive bibliography http ai iit nrc ca bibiliographies costsensitive html
van rijsbergen c harper porter selection good search terms
information processing management
woods k doss c bowyer k solka j priebe c kegelmeyer p comparative evaluation pattern recognition techniques detection microcalcications
mammography international journal pattern recognition artificial intelligence




