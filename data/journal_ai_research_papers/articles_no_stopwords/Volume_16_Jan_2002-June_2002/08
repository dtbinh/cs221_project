Journal Artificial Intelligence Research 16 (2002) 321357

Submitted 09/01; published 06/02

SMOTE: Synthetic Minority Over-sampling Technique
Nitesh V. Chawla

chawla@csee.usf.edu

Department Computer Science Engineering, ENB 118
University South Florida
4202 E. Fowler Ave.
Tampa, FL 33620-5399, USA

Kevin W. Bowyer

kwb@cse.nd.edu

Department Computer Science Engineering
384 Fitzpatrick Hall
University Notre Dame
Notre Dame, 46556, USA

Lawrence O. Hall

hall@csee.usf.edu

Department Computer Science Engineering, ENB 118
University South Florida
4202 E. Fowler Ave.
Tampa, FL 33620-5399, USA

W. Philip Kegelmeyer

wpk@california.sandia.gov

Sandia National Laboratories
Biosystems Research Department, P.O. Box 969, MS 9951
Livermore, CA, 94551-0969, USA

Abstract
approach construction classiers imbalanced datasets described.
dataset imbalanced classication categories approximately equally represented. Often real-world data sets predominately composed normal examples
small percentage abnormal interesting examples. case
cost misclassifying abnormal (interesting) example normal example
often much higher cost reverse error. Under-sampling majority (normal) class proposed good means increasing sensitivity classier
minority class. paper shows combination method over-sampling
minority (abnormal) class under-sampling majority (normal) class achieve
better classier performance (in ROC space) under-sampling majority class.
paper shows combination method over-sampling minority class
under-sampling majority class achieve better classier performance (in ROC
space) varying loss ratios Ripper class priors Naive Bayes. method
over-sampling minority class involves creating synthetic minority class examples.
Experiments performed using C4.5, Ripper Naive Bayes classier. method
evaluated using area Receiver Operating Characteristic curve (AUC)
ROC convex hull strategy.

1. Introduction
dataset imbalanced classes approximately equally represented. Imbalance
order 100 1 prevalent fraud detection imbalance 100,000
c
2002
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiChawla, Bowyer, Hall & Kegelmeyer

1 reported applications (Provost & Fawcett, 2001).
attempts deal imbalanced datasets domains fraudulent telephone calls
(Fawcett & Provost, 1996), telecommunications management (Ezawa, Singh, & Norton,
1996), text classication (Lewis & Catlett, 1994; Dumais, Platt, Heckerman, & Sahami,
1998; Mladenic & Grobelnik, 1999; Lewis & Ringuette, 1994; Cohen, 1995a) detection
oil spills satellite images (Kubat, Holte, & Matwin, 1998).
performance machine learning algorithms typically evaluated using predictive
accuracy. However, appropriate data imbalanced and/or costs
dierent errors vary markedly. example, consider classication pixels mammogram images possibly cancerous (Woods, Doss, Bowyer, Solka, Priebe, & Kegelmeyer,
1993). typical mammography dataset might contain 98% normal pixels 2% abnormal
pixels. simple default strategy guessing majority class would give predictive accuracy 98%. However, nature application requires fairly high rate correct
detection minority class allows small error rate majority class
order achieve this. Simple predictive accuracy clearly appropriate situations. Receiver Operating Characteristic (ROC) curve standard technique
summarizing classier performance range tradeos true positive false
positive error rates (Swets, 1988). Area Curve (AUC) accepted traditional performance metric ROC curve (Duda, Hart, & Stork, 2001; Bradley, 1997; Lee,
2000). ROC convex hull used robust method identifying potentially
optimal classiers (Provost & Fawcett, 2001). line passes point convex
hull, line slope passing another point
larger true positive (TP) intercept. Thus, classier point optimal
distribution assumptions tandem slope.
machine learning community addressed issue class imbalance two ways.
One assign distinct costs training examples (Pazzani, Merz, Murphy, Ali, Hume, &
Brunk, 1994; Domingos, 1999). re-sample original dataset, either oversampling minority class and/or under-sampling majority class (Kubat & Matwin,
1997; Japkowicz, 2000; Lewis & Catlett, 1994; Ling & Li, 1998). approach (Chawla,
Bowyer, Hall, & Kegelmeyer, 2000) blends under-sampling majority class
special form over-sampling minority class. Experiments various datasets
C4.5 decision tree classier (Quinlan, 1992), Ripper (Cohen, 1995b), Naive Bayes
Classier show approach improves previous re-sampling, modifying loss
ratio, class priors approaches, using either AUC ROC convex hull.
Section 2 gives overview performance measures. Section 3 reviews
closely related work dealing imbalanced datasets. Section 4 presents details
approach. Section 5 presents experimental results comparing approach
re-sampling approaches. Section 6 discusses results suggests directions future
work.

2. Performance Measures
performance machine learning algorithms typically evaluated confusion matrix
illustrated Figure 1 (for 2 class problem). columns Predicted class
rows Actual class. confusion matrix, N number negative examples
322

fiSMOTE

Predicted
Negative

Predicted
Positive

Actual
Negative

TN

FP

Actual
Positive

FN

TP

Figure 1: Confusion Matrix
correctly classied (True Negatives), F P number negative examples incorrectly
classied positive (False Positives), F N number positive examples incorrectly
classied negative (False Negatives) P number positive examples correctly
classied (True Positives).
Predictive accuracy performance measure generally associated machine learning algorithms dened Accuracy = (T P + N )/(T P + F P + N + F N ).
context balanced datasets equal error costs, reasonable use error rate
performance metric. Error rate 1 Accuracy. presence imbalanced datasets
unequal error costs, appropriate use ROC curve similar
techniques (Ling & Li, 1998; Drummond & Holte, 2000; Provost & Fawcett, 2001; Bradley,
1997; Turney, 1996).
ROC curves thought representing family best decision boundaries
relative costs TP FP. ROC curve X-axis represents %F P = F P/(T N +F P )
Y-axis represents %T P = P/(T P +F N ). ideal point ROC curve would
(0,100), positive examples classied correctly negative examples
misclassied positive. One way ROC curve swept manipulating
balance training samples class training set. Figure 2 shows illustration.
line = x represents scenario randomly guessing class. Area ROC
Curve (AUC) useful metric classier performance independent decision
criterion selected prior probabilities. AUC comparison establish dominance
relationship classiers. ROC curves intersecting, total AUC
average comparison models (Lee, 2000). However, specic cost class
distributions, classier maximum AUC may fact suboptimal. Hence,
compute ROC convex hulls, since points lying ROC convex hull
potentially optimal (Provost, Fawcett, & Kohavi, 1998; Provost & Fawcett, 2001).

3. Previous Work: Imbalanced datasets
Kubat Matwin (1997) selectively under-sampled majority class keeping
original population minority class. used geometric mean performance measure classier, related single point ROC curve.
minority examples divided four categories: noise overlapping positive class decision region, borderline samples, redundant samples safe samples.
borderline examples detected using Tomek links concept (Tomek, 1976). Another
323

fiChawla, Bowyer, Hall & Kegelmeyer

ROC

(100, 100)

100
Ideal point

Percent
True

y=x

Positive
increased undersampling
majority class moves
operating point
upper right
original data set

0

Percent False Positive

100

Figure 2: Illustration sweeping ROC curve under-sampling. Increased
under-sampling majority (negative) class move performance
lower left point upper right.

related work proposed SHRINK system classies overlapping region minority (positive) majority (negative) classes positive; searches best positive
region (Kubat et al., 1998).
Japkowicz (2000) discussed eect imbalance dataset. evaluated three
strategies: under-sampling, resampling recognition-based induction scheme. focus
sampling approaches. experimented articial 1D data order easily
measure construct concept complexity. Two resampling methods considered.
Random resampling consisted resampling smaller class random consisted
many samples majority class focused resampling consisted resampling
minority examples occurred boundary minority
majority classes. Random under-sampling considered, involved under-sampling
majority class samples random numbers matched number minority
class samples; focused under-sampling involved under-sampling majority class samples
lying away. noted sampling approaches eective,
observed using sophisticated sampling techniques give clear advantage
domain considered (Japkowicz, 2000).
One approach particularly relevant work Ling Li (1998).
combined over-sampling minority class under-sampling majority
class. used lift analysis instead accuracy measure classiers performance.
proposed test examples ranked condence measure lift used
evaluation criteria. lift curve similar ROC curve, tailored
324

fiSMOTE

marketing analysis problem (Ling & Li, 1998). one experiment, under-sampled
majority class noted best lift index obtained classes equally
represented (Ling & Li, 1998). another experiment, over-sampled positive
(minority) examples replacement match number negative (majority) examples
number positive examples. over-sampling under-sampling combination
provide signicant improvement lift index. However, approach oversampling diers theirs.
Solberg Solberg (1996) considered problem imbalanced data sets oil slick
classication SAR imagery. used over-sampling under-sampling techniques
improve classication oil slicks. training data distribution 42 oil
slicks 2,471 look-alikes, giving prior probability 0.98 look-alikes. imbalance
would lead learner (without appropriate loss functions methodology modify
priors) classify almost look-alikes correctly expense misclassifying many
oil slick samples (Solberg & Solberg, 1996). overcome imbalance problem,
over-sampled (with replacement) 100 samples oil slick, randomly sampled
100 samples non oil slick class create new dataset equal probabilities.
learned classier tree balanced data set achieved 14% error rate
oil slicks leave-one-out method error estimation; look alikes achieved
error rate 4% (Solberg & Solberg, 1996).
Another approach similar work Domingos (1999). compares
metacost approach majority under-sampling minority over-sampling.
nds metacost improves either, under-sampling preferable minority over-sampling. Error-based classiers made cost-sensitive. probability
class example estimated, examples relabeled optimally
respect misclassication costs. relabeling examples expands decision
space creates new samples classier may learn (Domingos, 1999).
feed-forward neural network trained imbalanced dataset may learn discriminate enough classes (DeRouin, Brown, Fausett, & Schneider, 1991).
authors proposed learning rate neural network adapted statistics
class representation data. calculated attention factor proportion
samples presented neural network training. learning rate network
elements adjusted based attention factor. experimented articially
generated training set real-world training set, multiple (more two)
classes. compared approach replicating minority class samples
balance data set used training. classication accuracy minority class
improved.
Lewis Catlett (1994) examined heterogeneous uncertainty sampling supervised
learning. method useful training samples uncertain classes. training
samples labeled incrementally two phases uncertain instances passed
next phase. modied C4.5 include loss ratio determining class
values leaves. class values determined comparison probability
threshold LR/(LR + 1), LR loss ratio (Lewis & Catlett, 1994).
information retrieval (IR) domain (Dumais et al., 1998; Mladenic & Grobelnik,
1999; Lewis & Ringuette, 1994; Cohen, 1995a) faces problem class imbalance
dataset. document web page converted bag-of-words representation;
325

fiChawla, Bowyer, Hall & Kegelmeyer

is, feature vector reecting occurrences words page constructed. Usually,
instances interesting category text categorization. overrepresentation negative class information retrieval problems cause problems
evaluating classiers performances. Since error rate good metric skewed
datasets, classication performance algorithms information retrieval usually
measured precision recall:
recall =

TP
TP + FN

precision =

TP
TP + FP

Mladenic Grobelnik (1999) proposed feature subset selection approach deal
imbalanced class distribution IR domain. experimented various
feature selection methods, found odds ratio (van Rijsbergen, Harper, & Porter,
1981) combined Naive Bayes classier performs best domain. Odds
ratio probabilistic measure used rank documents according relevance
positive class (minority class). Information gain word, hand,
pay attention particular target class; computed per word class.
imbalanced text dataset (assuming 98 99% negative class), features
associated negative class. Odds ratio incorporates target class information
metric giving better results compared information gain text categorization.
Provost Fawcett (1997) introduced ROC convex hull method estimate
classier performance imbalanced datasets. note problems unequal
class distribution unequal error costs related little work done
address either problem (Provost & Fawcett, 2001). ROC convex hull method,
ROC space used separate classication performance class cost distribution
information.
summarize literature, under-sampling majority class enables better classiers
built over-sampling minority class. combination two done
previous work lead classiers outperform built utilizing undersampling. However, over-sampling minority class done sampling
replacement original data. approach uses dierent method over-sampling.

4. SMOTE: Synthetic Minority Over-sampling TEchnique
4.1 Minority over-sampling replacement
Previous research (Ling & Li, 1998; Japkowicz, 2000) discussed over-sampling
replacement noted doesnt signicantly improve minority class recognition.
interpret underlying eect terms decision regions feature space. Essentially,
minority class over-sampled increasing amounts, eect identify similar
specic regions feature space decision region minority class.
eect decision trees understood plots Figure 3.
326

fiSMOTE

2attributes, 10% data original Mammography dataset

2attributes, 10% data original Mammography dataset
450

200

400

350

150

Attribute 2

Attribute 2

300

250

200

100

150

100

50

50

0

0

2

4

6

8

10

12

14

0

16

1

2

3

4

Attribute 1

5

6

7

8

Attribute 1

(a)

(b)

2attributes, 10% data original Mammography dataset
200

Attribute 2

150

100

50

0

1

2

3

4

5

Attribute 1

6

7

8

(c)
Figure 3: a) Decision region three minority class samples (shown +) reside
building decision tree. decision region indicated solid-line
rectangle. b) zoomed-in view chosen minority class samples
dataset. Small solid-line rectangles show decision regions result oversampling minority class replication. c) zoomed-in view chosen
minority class samples dataset. Dashed lines show decision region
over-sampling minority class synthetic generation.

327

fiChawla, Bowyer, Hall & Kegelmeyer

data plot Figure 3 extracted Mammography dataset1 (Woods
et al., 1993). minority class samples shown + majority class samples
shown plot. Figure 3(a), region indicated solid-line rectangle
majority class decision region. Nevertheless, contains three minority class samples
shown + false negatives. replicate minority class, decision region
minority class becomes specic cause new splits decision tree.
lead terminal nodes (leaves) learning algorithm tries learn
specic regions minority class; essence, overtting. Replication minority
class cause decision boundary spread majority class region. Thus,
Figure 3(b), three samples previously majority class decision region
specic decision regions.
4.2 SMOTE
propose over-sampling approach minority class over-sampled creating synthetic examples rather over-sampling replacement. approach
inspired technique proved successful handwritten character recognition (Ha
& Bunke, 1997). created extra training data performing certain operations
real data. case, operations rotation skew natural ways perturb
training data. generate synthetic examples less application-specic manner,
operating feature space rather data space. minority class over-sampled
taking minority class sample introducing synthetic examples along line
segments joining any/all k minority class nearest neighbors. Depending upon
amount over-sampling required, neighbors k nearest neighbors randomly
chosen. implementation currently uses nearest neighbors. instance,
amount over-sampling needed 200%, two neighbors nearest neighbors chosen one sample generated direction each. Synthetic samples
generated following way: Take dierence feature vector (sample)
consideration nearest neighbor. Multiply dierence random number
0 1, add feature vector consideration. causes
selection random point along line segment two specic features.
approach eectively forces decision region minority class become general.
Algorithm SMOTE , next page, pseudo-code SMOTE. Table 4.2 shows
example calculation random synthetic samples. amount over-sampling
parameter system, series ROC curves generated dierent
populations ROC analysis performed.
synthetic examples cause classier create larger less specic decision
regions shown dashed lines Figure 3(c), rather smaller specic
regions. general regions learned minority class samples rather
subsumed majority class samples around them. eect decision trees generalize better. Figures 4 5 compare minority over-sampling
replacement SMOTE. experiments conducted mammography dataset.
10923 examples majority class 260 examples minority class
originally. approximately 9831 examples majority class 233 examples
1. data available USF Intelligent Systems Lab, http://morden.csee.usf.edu/chawla.

328

fiSMOTE

minority class training set used 10-fold cross-validation. minority class
over-sampled 100%, 200%, 300%, 400% 500% original size. graphs
show tree sizes minority over-sampling replacement higher degrees
replication much greater SMOTE, minority class recognition
minority over-sampling replacement technique higher degrees replication isnt
good SMOTE.
Algorithm SMOTE (T, N, k)
Input: Number minority class samples ; Amount SMOTE N %; Number nearest
neighbors k
Output: (N/100) * synthetic minority class samples
1. ( N less 100%, randomize minority class samples random
percent SMOTEd. )
2. N < 100
3.
Randomize minority class samples
4.
= (N/100)
5.
N = 100
6. endif
7. N = (int)(N/100) ( amount SMOTE assumed integral multiples
100. )
8. k = Number nearest neighbors
9. numattrs = Number attributes
10. Sample[ ][ ]: array original minority class samples
11. newindex: keeps count number synthetic samples generated, initialized 0
12. Synthetic[ ][ ]: array synthetic samples
( Compute k nearest neighbors minority class sample only. )
13. 1
14.
Compute k nearest neighbors i, save indices nnarray
15.
Populate(N , i, nnarray)
16. endfor
Populate(N, i, nnarray) ( Function generate synthetic samples. )
17. N = 0
18.
Choose random number 1 k, call nn. step chooses one
k nearest neighbors i.
19.
attr 1 numattrs
20.
Compute: dif = Sample[nnarray[nn]][attr] Sample[i][attr]
21.
Compute: gap = random number 0 1
22.
Synthetic[newindex][attr] = Sample[i][attr] + gap dif
23.
endfor
24.
newindex++
25.
N = N 1
26. endwhile
27. return ( End Populate. )
End Pseudo-Code.

329

fiChawla, Bowyer, Hall & Kegelmeyer

Consider sample (6,4) let (4,3) nearest neighbor.
(6,4) sample k-nearest neighbors identied.
(4,3) one k-nearest neighbors.
Let:
f1 1 = 6 f2 1 = 4 f2 1 - f1 1 = -2
f1 2 = 4 f2 2 = 3 f2 2 - f1 2 = -1
new samples generated
(f1,f2) = (6,4) + rand(0-1) * (-2,-1)
rand(0-1) generates random number 0 1.
Table 1: Example generation synthetic examples (SMOTE).

Pruned decision tree size vs degree minority oversampling
260

240

Decisiion tree size (Number nodes)

220

200

180

160

140
Synthetic data
Replicated data
120

100

80

60

0

50

100

150

200
250
300
350
Degree minority oversampling

400

450

500

Figure 4: Comparison decision tree sizes replicated over-sampling SMOTE
Mammography dataset

330

fiSMOTE

% Minority Correct vs Degree Minority Oversampling
75

%Minority Correct

70

65

60

Synthetic data
Replicated data

55

50
0

50

100

150

200

250

300

350

400

450

500

Degree Minority Oversampling

Figure 5: Comparison % Minority correct replicated over-sampling SMOTE
Mammography dataset

4.3 Under-sampling SMOTE Combination
majority class under-sampled randomly removing samples majority class
population minority class becomes specied percentage majority class.
forces learner experience varying degrees under-sampling higher degrees
under-sampling minority class larger presence training set. describing
experiments, terminology under-sample majority class
200%, would mean modied dataset contain twice many elements
minority class majority class; is, minority class 50 samples
majority class 200 samples under-sample majority 200%, majority
class would end 25 samples. applying combination under-sampling
over-sampling, initial bias learner towards negative (majority) class reversed
favor positive (minority) class. Classiers learned dataset perturbed
SMOTING minority class under-sampling majority class.

5. Experiments
used three dierent machine learning algorithms experiments. Figure 6 provides
overview experiments.
1. C4.5: compared various combinations SMOTE under-sampling plain
under-sampling using C4.5 release 8 (Quinlan, 1992) base classier.
331

fiChawla, Bowyer, Hall & Kegelmeyer

SMOTE
Undersampling.

C4.5

Loss-Ratio
Modify costs majority minority
varied 0.9 0.001.
classes changing priors.

Ripper

Naive Bayes

ROCs generated SMOTE, Undersampling
Loss Ratio comparisons. Performance
evaluated AUC ROC convex hull.

ROCs generated comparison
SMOTE Under-sampling using C4.5,
SMOTE using C4.5 Naive bayes.
Performance evaluated AUC ROC convex hull.

Figure 6: Experiments Overview

2. Ripper: compared various combinations SMOTE under-sampling
plain under-sampling using Ripper (Cohen, 1995b) base classier.
varied Rippers loss ratio (Cohen & Singer, 1996; Lewis & Catlett, 1994) 0.9
0.001 (as means varying misclassication cost) compared eect
variation combination SMOTE under-sampling. reducing loss
ratio 0.9 0.001 able build set rules minority class.
3. Naive Bayes Classifier: Naive Bayes Classier2 made cost-sensitive
varying priors minority class. varied priors minority
class 1 50 times majority class compared C4.5s SMOTE
under-sampling combination.

dierent learning algorithms allowed SMOTE compared methods
handle misclassication costs directly. %FP %TP averaged 10-fold
cross-validation runs data combinations. minority class examples
over-sampled calculating nearest neighbors generating synthetic examples.
AUC calculated using trapezoidal rule. extrapolated extra point TP
= 100% FP = 100% ROC curve. computed ROC convex hull
identify optimal classiers, points lying hull potentially optimal
classiers (Provost & Fawcett, 2001).
2. source code downloaded http://fuzzy.cs.uni-magdeburg.de/borgelt/software.html.

332

fiSMOTE

5.1 Datasets
experimented nine dierent datasets. datasets summarized Table 5.2.
datasets vary extensively size class proportions, thus oering dierent
domains SMOTE. order increasing imbalance are:
1. Pima Indian Diabetes (Blake & Merz, 1998) 2 classes 768 samples.
data used identify positive diabetes cases population near Phoenix,
Arizona. number positive class samples 268. Good sensitivity
detection diabetes cases desirable attribute classier.
2. Phoneme dataset ELENA project3 . aim dataset
distinguish nasal (class 0) oral sounds (class 1). 5 features.
class distribution 3,818 samples class 0 1,586 samples class 1.
3. Adult dataset (Blake & Merz, 1998) 48,842 samples 11,687 samples
belonging minority class. dataset 6 continuous features 8 nominal
features. SMOTE SMOTE-NC (see Section 6.1) algorithms evaluated
dataset. SMOTE, extracted continuous features generated new
dataset continuous features.
4. E-state data4 (Hall, Mohney, & Kier, 1991) consists electrotopological state
descriptors series compounds National Cancer Institutes Yeast AntiCancer drug screen. E-state descriptors NCI Yeast AntiCancer Drug Screen
generated Tripos, Inc. Briey, series 60,000 compounds
tested series 6 yeast strains given concentration. test
high-throughput screen one concentration results subject contamination, etc. growth inhibition yeast strain exposed given
compound (with respect growth yeast neutral solvent) measured.
activity classes either active least one single yeast strain inhibited
70%, inactive yeast strain inhibited 70%.
dataset 53,220 samples 6,351 samples active compounds.
5. Satimage dataset (Blake & Merz, 1998) 6 classes originally. chose
smallest class minority class collapsed rest classes one
done (Provost et al., 1998). gave us skewed 2-class dataset, 5809
majority class samples 626 minority class samples.
6. Forest Cover dataset UCI repository (Blake & Merz, 1998).
dataset 7 classes 581,012 samples. dataset prediction forest
cover type based cartographic variables. Since system currently works binary classes extracted data two classes dataset ignored rest.
approaches work two classes (Ling & Li, 1998; Japkowicz,
2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001). two classes considered Ponderosa Pine 35,754 samples Cottonwood/Willow 2,747
3. ftp.dice.ucl.ac.be directory pub/neural-nets/ELENA/databases.
4. would thank Steven Eschrich providing dataset description us.

333

fiChawla, Bowyer, Hall & Kegelmeyer

Dataset
Pima
Phoneme
Adult
E-state
Satimage
Forest Cover
Oil
Mammography


Majority Class
500
3818
37155
46869
5809
35754
896
10923
435512

Minority Class
268
1586
11687
6351
626
2747
41
260
8360

Table 2: Dataset distribution
samples. Nevertheless, SMOTE technique applied multiple class problem well specifying class SMOTE for. However, paper,
focused 2-classes problems, explicitly represent positive negative classes.
7. Oil dataset provided Robert Holte used paper (Kubat et al.,
1998). dataset 41 oil slick samples 896 non-oil slick samples.
8. Mammography dataset (Woods et al., 1993) 11,183 samples 260 calcications. look predictive accuracy measure goodness classier
case, default accuracy would 97.68% every sample labeled noncalcication. But, desirable classier predict calcications
correctly.
9. dataset generated ExodusII data using AVATAR
(Chawla & Hall, 1999) version Mustafa Visualization tool5 . portion
crushed marked interesting rest
marked unknown. dataset size 443,872 samples 8,360 samples marked
interesting generated.
5.2 ROC Creation
ROC curve SMOTE produced using C4.5 Ripper create classier
one series modied training datasets. given ROC curve produced rst
over-sampling minority class specied degree under-sampling majority
class increasing degrees generate successive points curve. amount
under-sampling identical plain under-sampling. So, corresponding point
ROC curve dataset represents number majority class samples. Dierent
ROC curves produced starting dierent levels minority over-sampling. ROC
curves generated varying loss ratio Ripper 0.9 0.001
varying priors minority class original distribution 50 times
majority class Naive Bayes Classier.
5. Mustafa visualization tool developed Mike Glass Sandia National Labs.

334

fiSMOTE

Phoneme ROC
100

95

90

85

%TP

UnderC4.5
200 SMOTEC4.5
Naive Bayes
Hull

80

75

70

65
10

20

30

40

50

60

70

80

90

100

%FP

Figure 7: Phoneme. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. SMOTEC4.5 dominates Naive Bayes Under-C4.5 ROC space. SMOTEC4.5 classiers potentially optimal classiers.

Figures 9 23 show experimental ROC curves obtained nine datasets
three classiers. ROC curve plain under-sampling majority class
(Ling & Li, 1998; Japkowicz, 2000; Kubat & Matwin, 1997; Provost & Fawcett, 2001)
compared approach combining synthetic minority class over-sampling (SMOTE)
majority class under-sampling. plain under-sampling curve labeled Under,
SMOTE under-sampling combination ROC curve labeled SMOTE. Depending size relative imbalance dataset, one SMOTE undersampling curves created. show best results SMOTE combined
under-sampling plain under-sampling curve graphs. SMOTE ROC curve
C4.5 compared ROC curve obtained varying priors minority
class using Naive Bayes classier labeled Naive Bayes. SMOTE, Under,
Loss Ratio ROC curves, generated using Ripper compared. given family
ROC curves, ROC convex hull (Provost & Fawcett, 2001) generated. ROC
convex hull generated using Grahams algorithm (ORourke, 1998). reference,
show ROC curve would obtained using minority over-sampling replication
Figure 19.
point ROC curve result either classier (C4.5 Ripper) learned
particular combination under-sampling SMOTE, classier (C4.5 Ripper)
learned plain under-sampling, classier (Ripper) learned using loss ratio
classier (Naive Bayes) learned dierent prior minority class. point
represents average (%TP %FP) 10-fold cross-validation result. lower leftmost
point given ROC curve raw dataset, without majority class under335

fiChawla, Bowyer, Hall & Kegelmeyer

Phoneme ROC Ripper
100

95

%TP

90

85

UnderRipper
200 SMOTERipper
Loss Ratio
Hull

80

75

70

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 8: Phoneme. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss
Ratio Ripper. SMOTE-Ripper dominates Under-Ripper Loss Ratio
ROC space. SMOTE-Ripper classiers lie ROC convex hull.

Pima ROC
100

95

90

85

%TP

80

75

UnderC4.5
100 SMOTEC4.5
Naive Bayes
Hull

70

65

60

55

50
10

20

30

40

50

60

70

80

90

100

%FP

Figure 9: Pima Indians Diabetes. Comparison SMOTE-C4.5, Under-C4.5, Naive
Bayes. Naive Bayes dominates SMOTE-C4.5 ROC space.

336

fiSMOTE

Pima ROC Ripper
100

95

90

85

%TP

80
UnderRipper
100 SMOTERipper
Loss Ratio
Hull

75

70

65

60

55
10

20

30

40

50

60

70

80

90

100

%FP

Figure 10: Pima Indians Diabetes. Comparison SMOTE-Ripper, Under-Ripper,
modifying Loss Ratio Ripper. SMOTE-Ripper dominates Under-Ripper
Loss Ratio ROC space.

sampling minority class over-sampling. minority class over-sampled 50%,
100%, 200%, 300%, 400%, 500%. majority class under-sampled 10%, 15%,
25%, 50%, 75%, 100%, 125%, 150%, 175%, 200%, 300%, 400%, 500%, 600%, 700%, 800%,
1000%, 2000%. amount majority class under-sampling minority class oversampling depended dataset size class proportions. instance, consider
ROC curves Figure 17 mammography dataset. three curves one
plain majority class under-sampling range under-sampling varied
5% 2000% dierent intervals, one combination SMOTE majority class
under-sampling, one Naive Bayes one ROC convex hull curve. ROC
curve shown Figure 17 minority class over-sampled 400%. point
SMOTE ROC curves represents combination (synthetic) over-sampling undersampling, amount under-sampling follows range plain under-sampling.
better understanding ROC graphs, shown dierent sets ROC curves
one datasets Appendix A.
dataset, SMOTE lesser degree datasets
due structural nature dataset. dataset structural
neighborhood already established mesh geometry, SMOTE lead creating
neighbors surface (and hence interesting), since looking
feature space physics variables structural information.
ROC curves show trend increase amount under-sampling coupled
over-sampling, minority classication accuracy increases, course expense
majority class errors. almost ROC curves, SMOTE approach dom337

fiChawla, Bowyer, Hall & Kegelmeyer

Satimage ROC
100

95

90

85
UnderC4.5
200 SMOTEC4.5
Naive Bayes
Hull

%TP

80

75

70

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 11: Satimage. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
ROC curves Naive Bayes SMOTE-C4.5 show overlap; however,
higher TPs points SMOTE-C4.5 lie ROC convex hull.

Satimage ROC Ripper
100

95

90

85

%TP

80
UnderRipper
300 SMOTERipper
Loss Ratio
Hull

75

70

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 12: Satimage. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss
Ratio Ripper. SMOTE-Ripper dominates ROC space. ROC convex
hull mostly constructed points SMOTE-Ripper.

338

fiSMOTE

Covtype ROC
100

90

80

70

%TP

60
UnderC4.5
300 SMOTEC4.5
Naive Bayes
Hull

50

40

30

20

10

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 13: Forest Cover. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
SMOTE-C4.5 Under-C4.5 ROC curves close other. However, points SMOTE-C4.5 ROC curve lie ROC convex
hull, thus establishing dominance.

inates. Adhering denition ROC convex hull, potentially optimal
classiers ones generated SMOTE.
5.3 AUC Calculation
Area ROC curve (AUC) calculated using form trapezoid rule.
lower leftmost point given ROC curve classiers performance raw data.
upper rightmost point always (100%, 100%). curve naturally end
point, point added. necessary order AUCs compared
range %FP.
AUCs listed Table 5.3 show datasets combined synthetic minority over-sampling majority over-sampling able improve plain majority
under-sampling C4.5 base classier. Thus, SMOTE approach provides
improvement correct classication data underrepresented class.
conclusion holds examination ROC convex hulls. entries
missing table, SMOTE applied amounts datasets.
amount SMOTE less less skewed datasets. Also, included AUCs
Ripper/Naive Bayes. ROC convex hull identies SMOTE classiers potentially optimal compared plain under-sampling treatments misclassication
costs, generally. Exceptions follows: Pima dataset, Naive Bayes dominates
SMOTE-C4.5; Oil dataset, Under-Ripper dominates SMOTE-Ripper.
dataset, SMOTE-classifier (classifier = C4.5 Ripper) Under-classifier ROC
339

fiChawla, Bowyer, Hall & Kegelmeyer

Covtype ROC RIPPER
100

98

96

%TP

94
UnderRipper
100 SMOTERipper
Loss Ratio
Hull

92

90

88

86

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 14: Forest Cover. Comparison SMOTE-Ripper, Under-Ripper, modifying
Loss Ratio Ripper. SMOTE-Ripper shows domination ROC space.
points SMOTE-Ripper curve lie ROC convex hull.

Oil ROC
100

90

80

70

%TP

60

50

40
UnderC4.5
500 SMOTEC4.5
Naive Bayes
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 15: Oil. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. Although,
SMOTE-C4.5 Under-C4.5 ROC curves intersect points, points
SMOTE-C4.5 curve lie ROC convex hull.

340

fiSMOTE

Oil ROC Ripper
100

90

80

%TP

70
UnderRipper
300 SMOTERipper
Loss Ratio
Hull

60

50

40

30

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 16: Oil. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss Ratio
Ripper. Under-Ripper SMOTE-Ripper curves intersect, points
Under-Ripper curve lie ROC convex hull.

Mammography ROC
100

90

80

70
UnderC4.5
400 SMOTEC4.5
Naive Bayes
Hull

%TP

60

50

40

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 17: Mammography. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
SMOTE-C4.5 Under-C4.5 curves intersect ROC space; however,
virtue number points ROC convex hull, SMOTE-C4.5
potentially optimal classiers.

341

fiChawla, Bowyer, Hall & Kegelmeyer

Mammography ROC RIPPER
100

95

90

85

%TP

80
UnderRipper
400 SMOTERipper
Loss Ratio
Hull

75

70

65

60

55

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 18: Mammography. Comparison SMOTE-Ripper, Under-Ripper, modifying
Loss Ratio Ripper. SMOTE-Ripper dominates ROC space TP > 75%.
Mammography ROC C4.5
100

95

90

85

%TP

80

75

70
400 SMOTE
400 Replicate
Hull

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 19: comparison over-sampling minority class examples SMOTE oversampling minority class examples replication Mammography
dataset.

342

fiSMOTE

Estate ROC
100

90

80

70

%TP

60

50

40
UnderC4.5
500 SMOTEC4.5
Naive Bayes
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 20: E-state. (a) Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes.
SMOTE-C4.5 Under-C4.5 curves intersect ROC space; however,
SMOTE-C4.5 potentially optimal classiers, based number
points ROC convex hull.

Estate ROC Ripper
100

90

80

70

%TP

60

50

40
UnderRipper
100 SMOTERipper
Loss Ratio
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 21: E-state. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss
Ratio Ripper. SMOTE-Ripper potentially optimal classiers, based
number points ROC convex hull.

343

fiChawla, Bowyer, Hall & Kegelmeyer

ROC
100

90

80

70

%TP

60

50

40

UnderC4.5
100 SMOTEC4.5
Naive Bayes
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 22: Can. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. SMOTEC4.5 Under-C4.5 ROC curves overlap ROC space.

ROC Ripper
100

90

80

70

%TP

60

50

40

UnderRipper
50 SMOTERipper
Loss Ratio
Hull

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 23: Can. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss Ratio
Ripper. SMOTE-Ripper Under-Ripper ROC curves overlap
ROC space.

344

fiSMOTE

Dataset



Pima
Phoneme
Satimage
Forest Cover
Oil
Mammography
E-state


7242
8622
8900
9807
8524
9260
6811
9535

50
SMOTE

9560

100
SMOTE
7307
8644
8957
9832
8523
9250
6792
9505

200
SMOTE

300
SMOTE

400
SMOTE

500
SMOTE

8661
8979
9834
8368
9265
6828
9505

8963
9849
8161
9311
6784
9494

8975
9841
8339
9330
6788
9472

8960
9842
8537
9304
6779
9470

Table 3: AUCs [C4.5 base classier] best highlighted bold.

curves overlap ROC space. datasets, SMOTE-classifier
potentially optimal classiers approach.
5.4 Additional comparison changing decision thresholds
Provost (2000) suggested simply changing decision threshold always
considered alternative sophisticated approaches. case C4.5,
would mean changing decision threshold leaves decision trees. example,
leaf could classify examples minority class even 50% training
examples leaf represent majority class. experimented setting decision
thresholds leaves C4.5 decision tree learner 0.5, 0.45, 0.42, 0.4, 0.35, 0.32,
0.3, 0.27, 0.25, 0.22, 0.2, 0.17, 0.15, 0.12, 0.1, 0.05, 0.0. experimented Phoneme
dataset. Figure 24 shows comparison SMOTE under-sampling combination
C4.5 learning tuning bias towards minority class. graph shows
SMOTE under-sampling combination ROC curve dominating entire
range values.
5.5 Additional comparison one-sided selection SHRINK
oil dataset, followed slightly dierent line experiments obtain results
comparable (Kubat et al., 1998). alleviate problem imbalanced datasets
authors proposed (a) one-sided selection under-sampling majority class (Kubat
& Matwin, 1997) (b) SHRINK system (Kubat et al., 1998). Table 5.5 contains
results (Kubat et al., 1998). Acc+ accuracy positive (minority) examples
Acc accuracy negative (majority) examples. Figure 25 shows trend
Acc+ Acc one combination SMOTE strategy varying degrees undersampling majority class. Y-axis represents accuracy X-axis represents
percentage majority class under-sampled. graphs indicate band
under-sampling 50% 125% results comparable achieved
SHRINK better SHRINK cases. Table 5.5 summarizes results
SMOTE 500% under-sampling combination. tried combinations SMOTE
100-400% varying degrees under-sampling achieved comparable results.
345

fiChawla, Bowyer, Hall & Kegelmeyer

Phoneme: ROC comparison SMOTE C4.5 variation decision thresholds
100

95

%TP

90
SMOTE
Varying C4.5 decision thresholds
Hull
85

80

75
10

20

30

40

50

60

70

80

90

100

%FP

Figure 24: SMOTE Under-sampling combination C4.5 learning tuning
bias towards minority class

SMOTE Undersampling
100

90

Accuracy

80

Accuracy majority (negative class)
Accuracy minority (positive class)

70

60

50

40

30

0

100

200

300
400
500
600
Percentage undersampling majority class

700

800

Figure 25: SMOTE (500 OU) Under-sampling combination performance

SHRINK approach SMOTE approach directly comparable, though,
see dierent data points. SMOTE oers clear improvement one-sided selection.
346

fiSMOTE

Method
SHRINK
One-sided selection

Acc+
82.5%
76.0%

Acc
60.9%
86.6%

Table 4: Cross-validation results (Kubat et al., 1998)

Under-sampling %
10%
15%
25%
50%
75%
100%
125%
150%
175%
200%
300%
400%
500%
600%
700%
800%

Acc+
64.7%
62.8%
64.0%
89.5%
83.7%
78.3%
84.2%
83.3%
85.0%
81.7%
89.0%
95.5%
98.0%
98.0%
96.0%
90.7%

Acc
94.2%
91.3%
89.1%
78.9%
73.0%
68.7%
68.1%
57.8%
57.8%
56.7%
55.0%
44.2%
35.5%
40.0%
32.8%
33.3%

Table 5: Cross-validation results SMOTE 500% SMOTE Oil data set.

347

fiChawla, Bowyer, Hall & Kegelmeyer

6. Future Work
several topics considered line research. Automated adaptive
selection number nearest neighbors would valuable. Dierent strategies
creating synthetic neighbors may able improve performance. Also, selecting
nearest neighbors focus examples incorrectly classied may improve
performance. minority class sample could possibly majority class sample
nearest neighbor rather minority class sample. crowding likely contribute
redrawing decision surfaces favor minority class. addition
topics, following subsections discuss two possible extensions SMOTE,
application SMOTE information retrieval.
6.1 SMOTE-NC
SMOTE approach currently handle data sets nominal features,
generalized handle mixed datasets continuous nominal features. call
approach Synthetic Minority Over-sampling TEchnique-Nominal Continuous [SMOTE-NC].
tested approach Adult dataset UCI repository. SMOTE-NC
algorithm described below.
1. Median computation: Compute median standard deviations continuous
features minority class. nominal features dier sample
potential nearest neighbors, median included Euclidean distance
computation. use median penalize dierence nominal features
amount related typical dierence continuous feature values.
2. Nearest neighbor computation: Compute Euclidean distance feature
vector k-nearest neighbors identied (minority class sample)
feature vectors (minority class samples) using continuous feature space.
every diering nominal feature considered feature vector
potential nearest-neighbor, include median standard deviations previously
computed, Euclidean distance computation. Table 2 demonstrates example.
F1 = 1 2 3 B C [Let sample computing nearest
neighbors]
F2 = 4 6 5 E
F3 = 3 5 6 B K
So, Euclidean Distance F2 F1 would be:
Eucl = sqrt[(4-1)2 + (6-2)2 + (5-3)2 + Med2 + Med2 ]
Med median standard deviations continuous features minority class.
median term included twice feature numbers 5: BD 6: CE,
dier two feature vectors: F1 F2.

Table 6: Example nearest neighbor computation SMOTE-NC.

348

fiSMOTE

3. Populate synthetic sample: continuous features new synthetic minority
class sample created using approach SMOTE described earlier.
nominal feature given value occuring majority k-nearest neighbors.
SMOTE-NC experiments reported set SMOTE,
except fact examine one dataset only. SMOTE-NC Adult dataset
diers typical result: performs worse plain under-sampling based AUC,
shown Figures 26 27. extracted continuous features separate eect
SMOTE SMOTE-NC dataset, determine whether oddity
due handling nominal features. shown Figure 28, even SMOTE
continuous features applied Adult dataset, achieve better performance
plain under-sampling. minority class continuous features high
variance, so, synthetic generation minority class samples could overlapping
majority class space, thus leading false positives plain under-sampling.
hypothesis supported decreased AUC measure SMOTE degrees
greater 50%. higher degrees SMOTE lead minority class samples
dataset, thus greater overlap majority class decision space.
Adult SMOTENC
100

95

90

85

%TP

80
UnderC4.5
50 SMOTENCC4.5
Naive Bayes
Hull

75

70

65

60

55

50

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 26: Adult. Comparison SMOTE-C4.5, Under-C4.5, Naive Bayes. SMOTEC4.5 Under-C4.5 ROC curves overlap ROC space.

6.2 SMOTE-N
Potentially, SMOTE extended nominal features SMOTE-N
nearest neighbors computed using modied version Value Dierence Metric (Stanll
& Waltz, 1986) proposed Cost Salzberg (1993). Value Dierence Metric (VDM)
looks overlap feature values feature vectors. matrix dening distance
349

fiChawla, Bowyer, Hall & Kegelmeyer

Adult ROC Ripper
100

95

90

%TP

85

80

UnderRipper
50 SMOTERipper
Loss Ratio
Hull

75

70

65

60

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 27: Adult. Comparison SMOTE-Ripper, Under-Ripper, modifying Loss Ratio Ripper. SMOTE-Ripper Under-Ripper ROC curves overlap
ROC space.
Adult continuous [C4.5]
100

90

%TP

80

70


50 SMOTE

60

50

40

0

10

20

30

40

50
%FP

60

70

80

90

100

Figure 28: Adult continuous features. overlap SMOTE-C4.5 UnderC4.5 observed scenario well.

350

fiSMOTE

corresponding feature values feature vectors created. distance
two corresponding feature values dened follows.
(V1 , V2 ) =

n

C1i

|

i=1

C1



C2i k
|
C2

(1)

equation, V1 V2 two corresponding feature values. C1 total
number occurrences feature value V1 , C1i number occurrences feature
value V1 class i. similar convention applied C2i C2 . k constant,
usually set 1. equation used compute matrix value dierences
nominal feature given set feature vectors. Equation 1 gives geometric distance
xed, nite set values (Cost & Salzberg, 1993). Cost Salzbergs modied VDM
omits weight term wfa included computation Stanll Waltz,
eect making symmetric. distance two feature vectors given by:

(X, ) = wx wy

N


(xi , yi )r

(2)

i=1

r = 1 yields Manhattan distance, r = 2 yields Euclidean distance (Cost &
Salzberg, 1993). wx wy exemplar weights modied VDM. wy = 1
new example (feature vector), wx bias towards reliable examples (feature
vectors) computed ratio number uses feature vector number
correct uses feature vector; thus, accurate feature vectors wx
1. SMOTE-N ignore weights equation 2, SMOTE-N used
classication purposes directly. However, redene weights give weight
minority class feature vectors falling closer majority class feature vectors; thus,
making minority class features appear away feature vector
consideration. Since, interested forming broader accurate regions
minority class, weights might used avoid populating along neighbors fall
closer majority class. generate new minority class feature vectors, create
new set feature values taking majority vote feature vector consideration
k nearest neighbors. Table 6.2 shows example creating synthetic feature vector.
Let F1 = B C E feature vector consideration
let 2 nearest neighbors
F2 = F C G N
F3 = H B C N
application SMOTE-N would create following feature vector:
FS = B C N
Table 7: Example SMOTE-N

351

fiChawla, Bowyer, Hall & Kegelmeyer

6.3 Application SMOTE Information Retrieval
investigating application SMOTE information retrieval (IR). IR problems come plethora features potentially many categories. SMOTE would
applied conjunction feature selection algorithm, transforming given
document web page bag-of-words format.
interesting comparison SMOTE would combination Naive Bayes
Odds ratio. Odds ratio focuses target class, ranks documents according
relevance target positive class. SMOTE focuses target class creating
examples class.

7. Summary
results show SMOTE approach improve accuracy classiers
minority class. SMOTE provides new approach over-sampling. combination
SMOTE under-sampling performs better plain under-sampling. SMOTE
tested variety datasets, varying degrees imbalance varying amounts
data training set, thus providing diverse testbed. combination SMOTE
under-sampling performs better, based domination ROC space, varying
loss ratios Ripper varying class priors Naive Bayes Classier: methods
could directly handle skewed class distribution. SMOTE forces focused learning
introduces bias towards minority class. Pima least skewed dataset
Naive Bayes Classier perform better SMOTE-C4.5. Also, Oil
dataset Under-Ripper perform better SMOTE-Ripper. dataset,
SMOTE-classifier Under-classifier ROC curves overlap ROC space.
rest datasets SMOTE-classifier performs better Under-classifier, Loss Ratio,
Naive Bayes. total 48 experiments performed, SMOTE-classifier
perform best 4 experiments.
interpretation synthetic minority over-sampling improves performance
minority over-sampling replacement fairly straightforward. Consider
eect decision regions feature space minority over-sampling done
replication (sampling replacement) versus introduction synthetic examples.
replication, decision region results classication decision minority
class actually become smaller specic minority samples region
replicated. opposite desired eect. method synthetic over-sampling
works cause classier build larger decision regions contain nearby minority
class points. reasons may applicable SMOTE performs better
Rippers loss ratio Naive Bayes; methods, nonetheless, still learning
information provided dataset, albeit dierent cost information. SMOTE
provides related minority class samples learn from, thus allowing learner carve
broader decision regions, leading coverage minority class.

Acknowledgments
research partially supported United States Department Energy
Sandia National Laboratories ASCI VIEWS Data Discovery Program, contract number
352

fiSMOTE

DE-AC04-76DO00789. thank Robert Holte providing oil spill dataset used
paper. thank Foster Provost clarifying method using Satimage
dataset. would thank anonymous reviewers various insightful
comments suggestions.

353

fiChawla, Bowyer, Hall & Kegelmeyer

Appendix A. ROC graphs Oil Dataset
following gures show dierent sets ROC curves oil dataset. Figure 29 (a)
shows ROC curves Oil dataset, included main text; Figure 29(b) shows
ROC curves without ROC convex hull; Figure 29(c) shows two convex hulls,
obtained without SMOTE. ROC convex hull shown dashed lines stars
Figure 29(c), computed including Under-C4.5 Naive Bayes family
ROC curves. ROC convex hull shown solid line small circles Figure 29(c)
computed including 500 SMOTE-C4.5, Under-C4.5, Naive Bayes family
ROC curves. ROC convex hull SMOTE dominates ROC convex hull without
SMOTE, hence SMOTE-C4.5 contributes optimal classiers.
Oil

90

90

80

80

70

70

60

60

%TP

100

50

40

30

20

UnderC4.5
500 SMOTEC4.5
Naive Bayes

30

20

10

0

50

40
UnderC4.5
500 SMOTEC4.5
Naive Bayes
Hull

10

0

10

20

30

40

50
%FP

60

70

80

90

0

100

0

10

20

30

40

(a)

50
%FP

60

70

80

90

(b)
Oil ROC Convex Hulls
100

90

80

70

60

%TP

%TP

Oil ROC
100

50
Convex Hull SMOTE
Convex Hull without SMOTE

40

30

20

10

0

0

10

20

30

40

50
%FP

60

70

80

90

100

(c)
Figure 29: ROC curves Oil Dataset. (a) ROC curves SMOTE-C4.5, UnderC4.5, Naive Bayes, ROC convex hull. (b) ROC curves SMOTEC4.5, Under-C4.5, Naive Bayes. (c) ROC convex hulls without
SMOTE.

354

100

fiSMOTE

References
Blake, C., & Merz, C. (1998).
UCI Repository Machine Learning Databases
http://www.ics.uci.edu/mlearn/MLRepository.html. Department Information
Computer Sciences, University California, Irvine.
Bradley, A. P. (1997). Use Area ROC Curve Evaluation
Machine Learning Algorithms. Pattern Recognition, 30(6), 11451159.
Chawla, N., Bowyer, K., Hall, L., & Kegelmeyer, P. (2000). SMOTE: Synthetic Minority
Over-sampling TEchnique. International Conference Knowledge Based Computer Systems, pp. 4657. National Center Software Technology, Mumbai, India,
Allied Press.
Chawla, N., & Hall, L. (1999). Modifying MUSTAFA capture salient data. Tech. rep.
ISL-99-01, University South Florida, Computer Science Eng. Dept.
Cohen, W. (1995a). Learning Classify English Text ILP Methods. Proceedings 5th International Workshop Inductive Logic Programming, pp. 324.
Department Computer Science, Katholieke Universiteit Leuven.
Cohen, W. W. (1995b). Fast Eective Rule Induction. Proc. 12th International Conference Machine Learning, pp. 115123 Lake Tahoe, CA. Morgan Kaufmann.
Cohen, W. W., & Singer, Y. (1996). Context-sensitive Learning Methods Text Categorization. Frei, H.-P., Harman, D., Schauble, P., & Wilkinson, R. (Eds.), Proceedings
SIGIR-96, 19th ACM International Conference Research Development
Information Retrieval, pp. 307315 Zurich, CH. ACM Press, New York, US.
Cost, S., & Salzberg, S. (1993). Weighted Nearest Neighbor Algorithm Learning
Symbolic Features. Machine Learning, 10 (1), 5778.
DeRouin, E., Brown, J., Fausett, L., & Schneider, M. (1991). Neural Network Training
Unequally Represented Classes. Intellligent Engineering Systems Artificial
Neural Networks, pp. 135141 New York. ASME Press.
Domingos, P. (1999). Metacost: General Method Making Classiers Cost-sensitive.
Proceedings Fifth ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 155164 San Diego, CA. ACM Press.
Drummond, C., & Holte, R. (2000). Explicitly Representing Expected Cost: Alternative
ROC Representation. Proceedings Sixth ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 198207 Boston. ACM.
Duda, R., Hart, P., & Stork, D. (2001). Pattern Classification. Wiley-Interscience.
Dumais, S., Platt, J., Heckerman, D., & Sahami, M. (1998). Inductive Learning Algorithms Representations Text Categorization. Proceedings Seventh
International Conference Information Knowledge Management., pp. 148155.
355

fiChawla, Bowyer, Hall & Kegelmeyer

Ezawa, K., J., Singh, M., & Norton, S., W. (1996). Learning Goal Oriented Bayesian
Networks Telecommunications Risk Management. Proceedings International Conference Machine Learning, ICML-96, pp. 139147 Bari, Italy. Morgan
Kauman.
Fawcett, T., & Provost, F. (1996). Combining Data Mining Machine Learning Effective User Prole. Proceedings 2nd International Conference Knowledge
Discovery Data Mining, pp. 813 Portland, OR. AAAI.
Ha, T. M., & Bunke, H. (1997). O-line, Handwritten Numeral Recognition Perturbation
Method. Pattern Analysis Machine Intelligence, 19/5, 535539.
Hall, L., Mohney, B., & Kier, L. (1991). Electrotopological State: Structure Information
Atomic Level Molecular Graphs. Journal Chemical Information
Computer Science, 31 (76).
Japkowicz, N. (2000). Class Imbalance Problem: Signicance Strategies. Proceedings 2000 International Conference Artificial Intelligence (IC-AI2000):
Special Track Inductive Learning Las Vegas, Nevada.
Kubat, M., Holte, R., & Matwin, S. (1998). Machine Learning Detection Oil
Spills Satellite Radar Images. Machine Learning, 30, 195215.
Kubat, M., & Matwin, S. (1997). Addressing Curse Imbalanced Training Sets: One
Sided Selection. Proceedings Fourteenth International Conference Machine
Learning, pp. 179186 Nashville, Tennesse. Morgan Kaufmann.
Lee, S. (2000). Noisy Replication Skewed Binary Classication. Computational Statistics
Data Analysis, 34.
Lewis, D., & Catlett, J. (1994). Heterogeneous Uncertainity Sampling Supervised Learning. Proceedings Eleventh International Conference Machine Learning, pp.
148156 San Francisco, CA. Morgan Kaufmann.
Lewis, D., & Ringuette, M. (1994). Comparison Two Learning Algorithms Text
Categorization. Proceedings SDAIR-94, 3rd Annual Symposium Document
Analysis Information Retrieval, pp. 8193.
Ling, C., & Li, C. (1998). Data Mining Direct Marketing Problems Solutions.
Proceedings Fourth International Conference Knowledge Discovery Data
Mining (KDD-98) New York, NY. AAAI Press.
Mladenic, D., & Grobelnik, M. (1999). Feature Selection Unbalanced Class Distribution
Naive Bayes. Proceedings 16th International Conference Machine
Learning., pp. 258267. Morgan Kaufmann.
ORourke, J. (1998). Computational Geometry C. Cambridge University Press, UK.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing
Misclassication Costs. Proceedings Eleventh International Conference
Machine Learning San Francisco, CA. Morgan Kaumann.
356

fiSMOTE

Provost, F., & Fawcett, T. (2001). Robust Classication Imprecise Environments. Machine Learning, 42/3, 203231.
Provost, F., Fawcett, T., & Kohavi, R. (1998). Case Accuracy Estimation
Comparing Induction Algorithms. Proceedings Fifteenth International
Conference Machine Learning, pp. 445453 Madison, WI. Morgan Kaumann.
Quinlan, J. (1992). C4.5: Programs Machine Learning. Morgan Kaufmann, San Mateo,
CA.
Solberg, A., & Solberg, R. (1996). Large-Scale Evaluation Features Automatic
Detection Oil Spills ERS SAR Images. International Geoscience Remote
Sensing Symposium, pp. 14841486 Lincoln, NE.
Stanll, C., & Waltz, D. (1986). Toward Memory-based Reasoning. Communications
ACM, 29 (12), 12131228.
Swets, J. (1988). Measuring Accuracy Diagnostic Systems. Science, 240, 12851293.
Tomek, I. (1976). Two Modications CNN. IEEE Transactions Systems, Man
Cybernetics, 6, 769772.
Turney, P. (1996). Cost Sensitive Bibliography. http://ai.iit.nrc.ca/bibiliographies/costsensitive.html.
van Rijsbergen, C., Harper, D., & Porter, M. (1981). Selection Good Search Terms.
Information Processing Management, 17, 7791.
Woods, K., Doss, C., Bowyer, K., Solka, J., Priebe, C., & Kegelmeyer, P. (1993). Comparative Evaluation Pattern Recognition Techniques Detection Microcalcications
Mammography. International Journal Pattern Recognition Artificial Intelligence, 7(6), 14171436.

357


