journal artificial intelligence

submitted published

accelerating reinforcement learning composing
solutions automatically identified subtasks
chris drummond

school information technology engineering
university ottawa ontario canada k n n

cdrummon site uottawa ca

abstract

discusses system accelerates reinforcement learning transfer
related tasks without transfer even two tasks similar
abstract level extensive learning effort required system achieves much
power transferring parts previously learned solutions rather single complete
solution system exploits strong features multi dimensional function produced
reinforcement learning solving particular task features stable easy
recognize early learning process generate partitioning state space
thus function partition represented graph used index
compose functions stored case base form close approximation solution
task experiments demonstrate function composition often produces
order magnitude increase learning rate compared basic reinforcement
learning

introduction
standard reinforcement learning applied series related tasks could learn
task independently requires knowledge present state infrequent
numerical rewards learn actions necessary bring system desired goal
state paucity knowledge slow learning rate shows
exploit prior learning speed process maintaining
robustness general learning method
system proposed achieves much power transferring parts previously
learned solutions rather single complete solution solution pieces represent
knowledge solve certain subtasks might call macro actions precup
sutton singh obvious allusion macro operators commonly found
artificial intelligence systems main contribution work providing way
automatically identifying macro actions mapping tasks
work uses syntactic methods composition much symbolic
novelty arises parts composed multi dimensional real valued functions
functions learned reinforcement learning part complex functions
associated compound tasks ecacy due composition
occurring suciently abstract level much uncertainty removed
function acts much funnel operator christiansen although individual
actions may highly uncertain overall largely predictable
c ai access foundation morgan kaufmann publishers rights reserved

fidrummond

subtasks identified basis strong features multi dimensional
function arise reinforcement learning features world
system interaction world strong means features
stable e relatively insensitive variations low level learning process easy
recognize locate accurately early learning process one important aspect
features largely dictate shape function features differ
small amount one would expect function differ small amount
features generate partitioning function popular technique object
recognition snake kass witkin terzopoulus suetens fua hanson
used produce partition object recognition snake produces closed curve
lies along boundary object defined edges image application snake groups together sets features define region function
boundary region low order polygon demarcating individual subtask
repeated whole function covered polygons converted discrete
graphs vertex polygon becoming node graph merging graphs
produces composite graph representing whole task
composite graph used control transfer accessing case base previously
learned functions case base indexed graphs relevant function determined
matching subgraph composite graph one acting index case
associated functions transformed composed form solution task
used reinitialize lower level learning process necessary transfer
produce exact solution task sucient solution close enough
final solution often enough produce average speed reinforcement learning
refine function quickly remove error
demonstrates applicability transfer two different situations
first system learns task particular goal position goal moved
although function change significantly partition generated initial
task used compose function task second situation considered system placed different environment within domain
partition extracted control composition process
unifies significantly extends previous work author drummond
additional work largely focussed removing limitations
inherent partitioning introduced drummond one limitation
original snake could extract polygons rectangles
relaxes restriction allowing applied different environment
within domain different task domain although lifting restriction
removes desirable bias experiments demonstrate none ecacy
original system lost broadly obtained larger set
related tasks different domain overall function composition often
produces order magnitude increase learning rate compared
basic reinforcement learning
rest begins section giving high level discussion
taken section gives depth discussion techniques used sections present analyze experimental subsequent sections deal
limitations related


fiaccelerating reinforcement learning

overview
intent section appeal intuitions reader leaving much
detail later sections subsections follow demonstrate turn
features function produced reinforcement learning graphs
features used control composition function pieces
features easy detect early learning process features exist
multiple domains

features reinforcement learning function
overview begins high level introduction reinforcement learning
function produces features function
extracted converted graphical representation
one experimental test beds used simulated robot environment
different configurations interconnected rooms robot must learn navigate eciently
rooms reach specified goal start location figure shows one
example rooms goal top right corner robot actions small
steps eight directions indicated arrows location state
simply robot x coordinates thin lines figure walls
rooms thick lines boundary state space

goal

robot

















x




figure robot navigating series rooms


fidrummond

action independent preceding actions task becomes one learning
best action state best overall action would one takes robot
immediately goal possible states close goal suppose
robot particular state number steps goal
neighboring states known indicated numbered squares surrounding robot
figure one step look ahead procedure would consider step select
one reaches neighboring state shortest distance goal figure
robot would move state steps goal process repeated robot
take shortest path goal practice must course learn values
done type reinforcement learning watkins dayan sutton
progressively improves estimates distance goal state
converge correct values












figure value function obtained reinforcement learning
function shown figure called value function subsequently term
function mean value function unless otherwise indicated function
reinforcement learning figure instead representing
actual distance goal represents essentially exponential decay distance goal
reasons made clear section shaded areas represent large
gradients learned function comparing environment shown figure
apparent correspond walls rooms strong
features discussed exist extra distance robot
travel around wall reach inside next room path goal
features visually readily apparent human seems natural use vision
processing techniques locate
edge detection technique called snake used locate features snake
produces polygon instance rectangle locating boundary room
doorways room occur differential function along body
snake local minimum direction differential respect edges


fiaccelerating reinforcement learning

polygon associated walls room determines entrance
exit positive gradient room indicates entrance positive gradient
room indicates exit information plane graph labeled x
coordinate node constructed figure shows one example room
top left corner state space subsequent graphs coordinates
nodes corresponding doorways labeled respectively
positions function indicated dashed arrows

composing function pieces

overview continues showing graphs extracted features
function learned reinforcement learning used produce good approximation
solution goal position left hand side figure shows plane graphs
rooms ignore dashed lines circles node representing
goal labeled g directed edge added g appropriate
associated edge number representing distance nodes
determined value function points doorways individual
graph merged neighbor produce graph whole right
hand side figure doorway nodes relabeled composite
graph represents whole function individual subgraph represents particular part
function information stored case base subgraph index
corresponding part function case



g



extract
graphs

g



merge
graphs

g





g












figure graphical representation
suppose goal moved top right corner top left corner
state space reinforcement learning basic form would required learn
function scratch work goal moved goal position


fidrummond

known node representing goal relocated goal position shown
dashed circle figure edges connecting doorways goal
changed account goal position dashed lines representing edges
replace arrows subgraph produce function idea regress
backwards goal along edges edge small subgraph containing
edge extracted extracted subgraph used index case base functions
retrieved function transformed added appropriate region state space
form function
rotate
stretch

rotate


g

stretch



figure function composition
example existing subgraphs match configuration two
subgraph originally containing goal subgraph containing
goal certainly possible exchange two appropriate transform
graphs case base may better match task best match
subgraph containing goal fact subgraph goal original
fit task plane graph rotated stretched slightly
x direction changing coordinates nodes see figure
transformation applied function room containing original goal
case obtained solving another task better match three rooms use
functions original since changing goal position little effect
actions taken fact height functions must changed simply
multiplication value representing distance goal doorway
discussed detail end section matching subgraphs
allows error asymmetric scaling may used resulting function may
exact experiments demonstrate function often close
reinforcement learning quickly correct error


fiaccelerating reinforcement learning

position goal must established graph modified
function composition occur system told goal moved rather
discovers determining longer maximum existing function
uncertainty exact boundary original goal robot may reach
state believes part original goal region fail detect even
goal moved reasonably certain goal fact moved
required occur ten times intervening occurrence goal detected
maximum
system composes search function assuming particular room contains
goal search functions produced composing previously learned functions
however room assumed contain goal function constant
bias search particular part room allows limited learning
encourage exploration room search function drives robot room
anywhere else state space fails goal fixed number steps
search function composed another room assumed contain goal
process repeated goal located ten times ensures good estimate
center mass goal center mass used position
goal node composite graph requiring old goal goal positions
sampled fixed number times proven effective domains discussed
nevertheless somewhat ad hoc procedure addressed future
work discussed section

detecting features early
previous section existing task task strongly related walls
doorways fixed goal position different section
relationship assumed robot faced brand task must determine
relationship exists task previous tasks
experimental testbed simulated robot environment time
simplified inner rectangular room outer l shaped room figures
two possible room configurations thin lines walls
room thick lines boundary state space suppose robot already
learned function old task figure would hope could adapt
old solution fit closely related task figure
steps example essentially previous one
learning process started afresh features system must wait
emerge normal reinforcement learning process proceed much
first graph inner room extracted best matching graph
case base old task rotated stretched fit task next matching
graph outer l shaped room rotated stretched around larger inner room
transforms applied associated functions height adjustments
carried functions composed form approximate solution task
example first step process locate goal
partition aid search initial value function set mid range constant value
see figure allows limited learning encourages system move


fidrummond

goal

robot

outer
room

inner
room

robot

inner
room

outer
room

goal

figure old task

figure task

away regions explored previously prevent completely random walk
state space goal located learning reinitialized function
goal position walls see figure function exist
case base rough approximation could used instead walls function
used exactly stored case base difference goal rest
state space reduced scaling function adding constant reduces
bias function allowing learning alter relatively easily
information becomes available

figure start function

figure intermediate function

figure shows resultant function exploratory steps beginning
learning process large gradients associated walls readily


fiaccelerating reinforcement learning

apparent figure shows function task allowed converge
good solution functions roughly form large gradients
position although learning latter took steps
walls function introduced features take time clearly emerge snake
typically filter features small well formed additional filtering
graphical level constrains acceptable features total set features must
produce consistent composite graph doorways different subgraphs must align
graph must overlay complete state space must matching case
case base every subtask many checks balances removed
iterative updating technique section incorporated

figure early function

figure task function

different task domain

previous sections dealt simple robot navigation section demonstrates features exist quite different domain two degrees
freedom robot arm shown figure shoulder joint achieve angle radians elbow joint angle radians zero indicated
arrows arm straight shoulder joint rotated elbow joint describe
inner dotted circle hand outer dotted circle eight actions small
rotations clockwise anti clockwise joint separately together aim
learn move arm eciently initial position hand reaches
goal perimeter arm work space
state space purposes reinforcement learning configuration space
arm sometimes called joint space see figure x axis angle
shoulder joint axis elbow joint eight actions mapped actions
configuration space become much actions robot navigation shown
shaded diamond labeled arm figure map obstacle work space
configuration space one must pairs shoulder elbow angles blocked
obstacle obstacles space become elongated form barriers much


fidrummond



shoulder

hand



elbow angle

obstacle












g


l


























































































arm



obstacle

elbow





obstacle











figure work space




























































































obstacle

g


l


shoulder angle



figure configuration space

walls experiments previous sections clear imagine straightening
arm work space rotating intersects one obstacles
middle dotted line figure arm rotated shoulder joint
roughly linearly proportional rotation elbow joint opposite direction
keep intersecting obstacle produces wall configuration space
linearity holds small objects far perimeter work space
complex larger objects would complex shapes configuration
space moment feature extraction method limited simpler shapes
discussed section
reinforcement learning function produced shown figure
features shaded clarity large gradient associated obstacle
left hand side configuration space clearly seen similar large
gradient associated obstacle right hand side configuration space
features used control composition functions goal
moved different task domain

details techniques used
section discuss detail techniques used include reinforcement
learning produce initial function snakes extract features producing graph
transformation composition subgraphs corresponding functions fit task

reinforcement learning

reinforcement learning typically works refining estimate expected future reward
goal directed tasks ones investigated equivalent progressively


fiaccelerating reinforcement learning

figure robot arm function
improving estimate distance goal state estimate updated
best local action e one moving robot arm state
smallest estimated distance early learning process states close goal
likely accurate estimates true distance time action taken estimate
distance state used update estimate old state eventually
process propagate back accurate estimates goal states
rather directly estimating pthe distance goal system uses expected

discounted reward state e
rt uence rewards rt reduced
progressively farther future occur less one work
reward reaching goal farther state goal smaller
value use expectation allows actions stochastic robot
arm takes particular action particular state next state
carry reinforcement learning uses q learning watkins
dayan assumes world discrete markov process thus
states actions discrete action state q learning maintains
rolling average immediate reward r plus maximum value action
next state see equation action selected state usually one
highest score encourage exploration state space uses greedy
policy sutton chooses random action fraction time
effect function composition q learning initial value
state action pair set value zero

qts qts r maxa qts
q function state action usually referred action value function
action value function transformed composed form
solution task value function discussed previous sections shown








fidrummond

figures maximum value q function used generate partition
associated graphs needed control process
watkins dayan proved q learning converge optimal value
certain constraints reward learning rate optimal solution produced taking action greatest value state goal directed tasks
greedy take shortest path goal learning complete
extension continuous spaces may done function approximation simplest
method one used divide state dimensions intervals resulting action value function cells representing average q value taking action
somewhere within region state space line learning action
state executed representation proven converge gordon
line learning current state determined environment
generally successful exists proof convergence

feature extraction
feature extraction uses vision processing technique fits deformable model called
snake kass et al edges image initializing snake process
iterates external forces due edges balance internal forces snake
promote smooth shape external forces due steep gradients value
function piecewise constant function approximator used smoothed cubic b spline
fitted value function used generate necessary derivatives left hand
side figure gradient value function shown figure extracting
features early learning process system added gradient around border
represent state space boundary
locate features curve found lies along ridge hills local
maximum differential right hand side figure dashed lines
contour lines small inner room indicated bold lines right hand side
figure snake different stages process snake first positioned
approximately center room innermost circle expanded
abuts base hills simplify exposition imagine
snake consists number individual hill climbers spread along line representing
snake indicated small white circles instead allowed climb
independently movement relative constrained maintain smooth
shape snake reaches top ridge constrained polygon
instance quadrilateral outside dark line figure point
tend oscillate around equilibrium position limiting step size process
brought stationary state detailed mathematical treatment
given appendix
polygon forms skeleton graph shown top left figure
nodes graph correspond vertices polygon doorways goal
looking gradient plot doorways regions small differential
ridges locations determined magnitude gradient along
boundary polygon example node added goal labeled g
connected doorway labeled polygon delimits region


fiaccelerating reinforcement learning

graph

g

polygon



doorway

figure gradient resultant polygon left extracted snake right
state space therefore region action value function becomes case
case base corresponding graph index constraining snake
polygon done two reasons firstly vertices needed produce nodes
plane graphs important part matching process secondly additional
constraint accurate fit boundaries subtask turn
accurate solution function composition

three extensions snake
section introduces three extensions basic snake facilitate extraction features
first extension affects direction snake moves hill climbing gradient
normal hill climbing step taken direction steepest ascent step size
determined size differential roughly translates forces points
along body snake force points direction steepest ascent locally
interacts forces shape constraints looking gradient
function contour lines figure steep slope leading top
ridge significant slope along ridge away doorway towards
boundary state space thus force single point body snake


fidrummond

directly towards top ridge turned towards apex indicated
bold black arrow left hand side figure
snake

steepest
ascent
tangent
normal

figure controlling forces snake
force broken two components respect snake normal
tangential force latter force acts along body snake shape
constrained quadrilateral cause relevant side shrink effect
partially counteracted force towards top ridge adjacent side
quadrilateral net shrinking two sides associated
ridges inwards forces balanced push corner quadrilateral
near doorway inwards indicated thin black arrow figure extreme
case might cause snake collapse something close triangle
likely outcome degradation accuracy registration ridges
drummond prevented degradation accuracy restricting snakes
rectangular shapes weakening constraint general polygons
effect becomes addressed removing component
force tangential snake hill climbing direction
normal significantly restrict motion snake removed
component along body snake thus mainly prevents stretching
shrinking snake due gradient
second extension controls way snake expanded reach base
hills drummond used ballooning force introduced cohen cohen
arose extending system deal general shapes
rectangles outer l shaped room figure ballooning force expands
snake directions normal body one deleterious effect snake contacts
sharp external corner inner room force tends push snake
corner seen figure bold continuous lines snake
bold dashed lines ridges imagine starting circular snake


fiaccelerating reinforcement learning

middle l shaped outer room time reaches walls inner room
sides snake roughly perpendicular ridges thus little restrain
expansion snake passes completely walls inner room
ridge

ballooning
force

ridge

figure ballooning force
adopted analogous ow mercury imagine starting
somewhere middle l shaped room progressively adding mercury would
tend fill lower regions valley first reach bases hills roughly
time analogy mercury used high surface tension preventing
owing small gaps edges associated doorways increase
effectiveness idea absolute value differential gradient thresholded
values threshold set one zero smoothed
truncated gaussian shown figure smoothing thresholding commonly
used techniques machine vision tanimoto typically used remove
noise aim strongly blur thresholded image produces bowls
associated room example smoothing almost completely obscured
presence doorway although generally case
snake initialized small circle minimum one bowls
shown circle middle figure dashed lines contour
lines function ows outwards follow contour lines bowl
largest component ow direction arrows figure
achieved varying force normal body snake according height
difference average height snake thus points along snake
higher average tend get pushed inwards lower pushed outwards surface
tension mercury produced smoothing constraints first second
differentials snake see appendix
third extension limits changes shape snake expands initial
position reach base hills smoothness constraints snake give
mercury properties prevent snake owing gaps associated


fidrummond

figure smoothed function

figure mercury flow

doorways even proved insucient width rooms width
doorways similar sizes figure looking room left hand side
configuration space robot arm doorway room top
similar width increasing surface tension mercury suciently prevent ow
doorways prevents ow top room
solution limit amount snake change shape grows
achieved constraining much second differential snake change
step step figure apparent snake takes good approximation
shape room time reaches ridges shape
locked reaching ridges described avoided
snake initialized constraint smoothness snake expanded
smoothness constraint progressively weakened curvature constraint progressively
strengthened progressively locks shape still allowing snake make
small local adjustments better fit features
extensions discussed section modify traditional forces act
snake add ones forces associated knot spacing drag
snake moves iteration depends vector addition forces
sum acts accelerate body snake mass velocity
therefore momentum schematic representation forces shown figure
detailed mathematical description given appendix dashed line represents
body snake arrows forces applied one point body snake
parameterized function given f x x individual
cubic b splines giving x coordinates associated variable along body
snake circles represent points equi distant necessarily x
points kept roughly euclidean distance apart x due knot
spacing force momentum although strictly force encourages point move


fiaccelerating reinforcement learning

constant direction drag opposes motion stiffness encourages snake
maintain smooth shape overall stiffness reduced snake grows keep
exibility per unit length roughly constant controlled locally maintain
shape
steepest ascent

mercuryflow
momentum

knot spacing

drag
stiffness

figure forces snake
following algorithmic summary processing snake

initialize coecients produce circular snake middle room
iterate forces roughly equilibrium snake oscillates around
stationary value

modify stiffness enforce polygonal constraints
iterate steps increasing momentum drag step reduce
oscillation small value

use final position snake form polygon delimits boundary
room

transformation

section discusses matching process subgraph used locate transform
function case base matching process first finds subgraphs case base
isomorphic extracted subgraph possible isomorphic mappings
nodes labeling macdonald number isomorphic mappings


fidrummond

potentially exponential number nodes graphs typically
nodes symmetries isomorphic mappings associated
node subgraph x coordinate ane transform equation found
minimizes distances coordinates mapped nodes
isomorphic subgraphs advantage transform relative exibility
simple form

x c x c c c x c c

ideally transformed nodes would positioned exactly mapped nodes
usually possible even simple rectangular shapes case base may
contain graph exactly doorway positions graph
exact match introduce error composed function task
weighting nodes others error occurs controlled one aim
minimize introduction errors affect overall path length however
equal importance errors introduced easily correctable normal reinforcement
learning












figure weighting graph nodes
left hand side figure shows composite graph task right
hand side shows overlaying graph case base fit
doorway outer l shaped room error robot tend miss doorway
collide wall one side farther doorway position longer
normal reinforcement learning take correct error encourage good fit
doorway weight used nodes adjacent doorway given weight
nodes weight one intuition trajectories
different parts state space pass region close doorway
error likely broader effect take longer normal reinforcement


fiaccelerating reinforcement learning

learning correct regions far doorway fit around inner room
improved sacrificing fit far doorway
exact position doorway inner room critical weight
set whatever position doorway shape function correct
inside room goal room however doorway
correct position greater error edge length produce error
composed function expectation error small
reinforcement learning quickly correct
fit good would prefer amount transformation small transforms produce error particularly true
asymmetric scaling discussed later section generally transform produces
translation ection rotation shearing independent scaling dimension
robot navigation domain distance points state space
normal euclidean distance reinforcement learning function exponential decay
distance goal transformation change euclidean distance
transformed function directly applicable

affine similar symmetric

ane transformation one family hierarchy transformations
bottom hierarchy shown equation symmetric transformations
solid body transformations change euclidean distance next step
hierarchy introduces scaling equal dimension affect euclidean
distance multiplicative factor thus change needed transformed
function scale height ane transformations allow addition asymmetric
scaling shear distort euclidean distance determine amount
distortion transformation applied unit circle symmetric rigid body
transformations alter circle transformations symmetric
scaling transform changes diameter circle asymmetric scaling shear
transformations change circle ellipse amount distortion euclidean
distance introduced transform determined ratio lengths major
minor axes ellipse
error sqrt
p
wi
x yi node misalignment




euclidean distortion
log fifi rrmaj

minfi


j
r

r
j
maj
min
log
scaling factor

error fit transformed subgraph combined transformation
error lengths major minor axes rmaj rmin respectively
ellipse penalty euclidean distortion asymmetric scaling shear
log factor added directly error fit shown equation log factors
used penalty functions symmetric small penalty symmetric
scaling best matching subgraph found transformation
applied associated function isomorphic graph found total error less
constant function used default graph overlays
old graph values assigned bilinear interpolation discrete values


fidrummond

function bilinear extrapolation used closest values
cases four values selected value point calculated
shown equation action value function indexed action well state
process carried action turn rotation ection transform
applied predefined matrix actions produces necessary mapping
actions original action value function

v c x c c xy c



finally height action value function must adjusted account
change overall distance goal height value function doorway
dg dg distance goal discount factor value random
point within room dg dd dd distance doorway action value
function first normalized dividing dg height function doorway
original multiplied dng dng distance
goal value point becomes dng dd scaling affect height
function assuming scaling symmetric value function anywhere
room cdd c scale factor thus raising function power c
e dd c account scaling scaling symmetric exact assuming
distance linear combination two dimensions asymmetric scaling
exact difference two scale factors relatively small
useful approximation use maximum
following algorithmic summary whole matching process

sg subgraph extracted task
subgraph g acting index case base

isomorphic mapping g sg
minimum weighted least squares fit g sg mapping
ane transform coecients least squares fit
penalized fit least squares error transform penalty
keep graph transform lowest penalized fit
retrieve function associated best graph case base none use default
apply ane transform function
apply bilinear interpolation extrapolation
adjust function height
add function existing function


fiaccelerating reinforcement learning

composition

section describes function composition transformation applied successively
series subgraphs extracted composite graph function composition uses
slightly modified form dijkstra dijkstra traverse edges
doorway nodes left hand side figure shows composite graph moving
goal robot navigation example section right hand side shows
graph traversed dijkstra

g



g







gr

gr









gr




gr



gr

figure dijkstra
begin process subgraph contains goal extracted best
matching isomorphic subgraph found edge lengths composite graph
updated scaled length corresponding edge matching isomorphic
subgraph figure less next subgraph extracted
gr one sharing doorway node edge length best matching
isomorphic subgraph found edge length updated shortest path
determined less subgraph gr extracted process
repeated subgraphs updated stage subgraph matched
corresponding transformed function retrieved added function
appropriate region
example single path goal room often
multiple paths suppose room additional doorway lower left corner
room labeled b left hand side figure addition original doorway
labeled graph shown right hand side figure would
two possible paths goal lengths length across room
greater absolute difference choice path room
determined decision boundary inside room produced taking


fidrummond


room







room

room



room

room



n





n

gr
b

b

n

figure multiple paths goal
maximum two functions shown figure one entering doorway
leaving doorway b one entering doorway b leaving doorway
principle repeated two paths goal given
room
cross room distance smaller difference jd j decision
boundary would another room general want room
cross room distance larger difference incident paths
repeated every cycle path graph cycle detected node visited twice
indicating reachable two separate paths let us suppose node n
graph figure dijkstra used know previous nodes
path n n already closed must true paths
reached n rooms paths nodes cannot contain decision
boundary must room decide remaining room
compare two path lengths longer decision boundary
room otherwise room
whichever room selected decision boundary produced maximum two
functions heights two functions adjusted path lengths determine
decision boundary occurs within room paths equal length taking
maximum correctly put decision boundary doorway
functions case base functions already include decision boundaries may used
technique produces correct decision boundary difference path lengths
entering room less difference heights function
doorways left hand side figure room two doorways path
significantly longer path decision boundary far left shortest
path goal room via right hand doorway function
combined mirror image produce decision boundary middle


fiaccelerating reinforcement learning

maximum
decision
boundary

figure combining two functions

decision
boundary

path
path

room

figure decision functions



fidrummond

room shown right hand side figure could used
shown left hand side figure two paths length
heights two functions changed move decision boundary
cannot moved anywhere room decision boundary moved
closer particular doorway original function shown figure
decision
boundary
path

path

room

figure combining decision functions

experiments

section compares learning curves function composition simple baseline four sets presented one two types related task
two domains learning curves represent average distance goal
function number actions taken learning distance averaged
different start positions distributed uniformly throughout state space
different experimental runs determine distance normal learning stopped
fixed number actions copy function learned far stored one
start positions selected learning restarted number actions needed reach
goal recorded trial takes actions yet reached goal
stopped distance goal recorded function reinitialized
stored version another start state selected repeated times
function reinitialized normal learning resumed
baseline underlying learning function composition system basic q learning discrete function approximator
discussed section learning rate set greedy policy uses
best action selected time future discount reward
received reaching goal although state spaces different domains represent two quite different things robot hx yi location angle arm two
joints actual representation state space ranges
dimension step dimension separately together giving eight
possible actions actions stochastic uniformly distributed random value
added dimension action robot navigation examples


fiaccelerating reinforcement learning

robot hits wall positioned small distance wall along direction
last action implemented robot arm somewhat
complex calculation instead collision obstacle occurs arm restored
position taking action
learning begins randomly selected start state continues goal reached
start state selected randomly process repeated continues
requisite total number actions achieved speed calculated dividing
number learning steps one specific point baseline learning curve number
learning steps equivalent point function composition system learning curve
knee function composition system curve used occurs low
level learning initialized composed function compared
approximate position knee baseline curve

robot navigation goal relocation
first experiment investigates time taken correct learned function goal
relocated robot navigation domain nine different room configurations
shown figure number rooms varying three five four
different goal positions room one two doorways one two paths
goal initialize case base function learned configurations
goal position shown black square rooms generated randomly
constraints configuration rooms doorways room
small narrow doorway large case base includes
functions generated experiments discussed section necessary
give sucient variety cases cover tasks even addition
subgraphs matched constant valued default functions used
match reduces speed significantly eliminate altogether



















figure different suites rooms


fidrummond

case base loaded basic q learning rerun room
configuration goal position shown steps goal moved
denoted time x axis figure goal moved one
three remaining corners state space task included case base learning
continues steps fixed intervals learning stopped average
number steps reach goal recorded curves figure average
experimental runs three goal positions nine room configurations
function composition

average steps goal

q learning
q learning reinit






























learning steps x

figure learning curves robot navigation goal relocation
basic q learning top curve figure performs poorly
goal moved existing function pushes robot towards old goal position
variant basic reinitializes function zero everywhere detecting
goal moved reinitialized q learning middle curve performed much
better still learn task scratch
function composition system lowest curve performed far best
precise position knee curve dicult determine due effect
default functions examples case base functions considered knee
point sharp steps average number steps goal steps
examples non reinitialized q learning fails reach value within
steps giving speed reinitialized q learning reaches value
steps giving speed function composition generally
produces accurate solutions even error introduced q learning quickly
refines function towards asymptotic value steps


fiaccelerating reinforcement learning

normal q learning reaches average value steps slowly refines solution
reach average value steps

robot arm goal relocation
second experiment essentially repeat first experiment robot arm
domain initial number steps goal moved reduced
speed experiments arm two degrees freedom
restrictions discussed section number variations small three obstacle
configurations used constructed hand two obstacles increase
number experiments allow greater statistical variation configuration
repeated goal three possible positions shown figure
black diamonds represent obstacles black rectangles goal solutions
tasks loaded case base composing function however system
prevented selecting case comes goal obstacle configuration



















figure robot arm obstacle goal positions
curves figure average experimental runs two goal positions
three original goal positions three obstacle configurations shown
figure two learning curves non reinitialized q learning dropped
first experiment function composition system lower curve performed
much better q learning knee function composition system occurs
steps knee q learning steps giving speed experiment
case base contained subgraphs matched tasks default functions
needed composed functions tend accurate little refinement
necessary


fidrummond

function composition

average steps goal

q learning






























learning steps x

figure learning curves robot arm goal relocation

robot navigation environment
third experiment investigates time taken learn related environment
robot navigation domain nine different inner rooms generated randomly
constraints single doorway size position room
location doorway varied shown figure initialize case base
function learned configurations goal inside small room
indicate dark square learning repeated room configurations
turn however composing function system prevented selecting
case learned goal room configuration experimental runs qlearning function composition system initialized function
zero everywhere respectively denoted zero x axis learning continues
steps improve statistical variation experiments configuration
repeated three times time random seed curves figure
therefore average across experimental runs
top curve q learning bottom curve function composition
system experiments locating goal took typically steps
although took steps function composition system introduces
walls function typically steps taken usable features
generated certain experimental runs took longer discussed section
due runs knee function composition system curve occurs
steps knee basic q learning curve occurs approximately steps giving






































q learning





function composition

accelerating reinforcement learning







figure single rooms





learning steps x





figure learning curves robot navigation environment

average steps goal

fidrummond

speed previous experiments initialized function accurate
little refinement necessary basic q learning reaching knee takes
long time remove residual error

robot arm environment

































q learning





function composition

figure different obstacle positions



learning steps x

figure learning curves robot arm environment




fourth experiment essentially third experiment except robot
arm domain three hand crafted configurations single obstacle goal
fixed position used shown figure increase statistical variation
configuration run five times different random seed curves figure
therefore average across experimental runs

average steps goal

fiaccelerating reinforcement learning

top curve figure q learning bottom curve function
composition system knee function composition system curve occurs
steps knee basic q learning steps giving speed


analysis

experiments previous section shown function composition produces
significant speed across two different types related task across two domains
addition composed solutions tend accurate little refinement
required section begins looking possible concerns experimental
methodology might affect measurement speed discusses
properties task solved affect speed achieved function
composition

possible concerns experimental methodology

speed obtained function composition suciently large small variations
experimental set unlikely affect overall nevertheless
number concerns might raised experimental methodology
least partially addressed section others subject future work
first concern might estimated value speed measured
value represents speed average set learning tasks rather
average speed tasks one diculties estimation
curves single tasks average distance goal may oscillate
learning progresses even though general trend downwards makes judging
position knee curves dicult estimate speed questionable even
experimental runs configuration different random seeds exhibit
considerable variation instances speed measured individual curves may
benefit function composition system others baseline nevertheless
probably overall effects cancel
second concern might effect speed limit steps
measuring distance goal comparing two averages values limited way
sometimes misleading gordon segre limit primarily affects
baseline significant goal moved function
reinitialized estimation speed principally concerned comparing position
knees different curves average distance goal relatively small
limiting value likely little effect
third concern might value speed dependent configuration
baseline certainly experience author way
function initialized actions selected impact speed
learning previous work drummond function initialized constant
value technique termed optimistic initial values sutton barto
tie breaking actions value achieved adding small amount
noise circa expected would increase exploration early
learning process speed learning overall however initial value zero












drummond











strict tie breaker randomly selecting amongst actions value turned
produce significant speed baseline learning configuration
used preceding experiments one experimental run caused serious
baseline







learning steps x

figure learning curves partially observable domain



upper learning curve figure baseline one run
goal moved robot arm domain large impact average
learning curve replaced lower curve produced repeating experiment
different random seed slow learning rate arises interaction
partial observability robot arm domain use initial value
zero individual cells function approximator straddle obstacles allowing leakthrough value one side obstacle starting zero value
action receives value remain best action time continual
update action decrease value asymptotically zero
actions state updated selected greedy
action occur higher initial values may domains
degree partial observability small initial values better zero means
improving exploration small values might necessary
variations parameters baseline explored
instance constant learning rate used alternatives
starting higher rate reducing learning progresses might improve
overall speed baseline preliminary experiments however

average steps goal

fiaccelerating reinforcement learning

carried undiscounted reinforcement learning discounting strictly unnecessary goal directed tasks room configuration figure goal
lower right hand corner used experimental task discounting discussed
section turned setting addition value reaching goal state
set zero cost associated every action form learning simplifies
function composition normalization procedures needed compensate value function exponential form longer required normalization disabled snake
successfully partitioned function critical part process however
baseline learner took considerably longer learn function discounted case
discounting learner reached average distance goal steps
learning steps without discounting learner reached average steps
point time average steps learning steps
action value function initialized zero appears standard practice
literature however experience initialization discounted case suggests
might part investigated future work
baseline q learning used basic sophisticated one
would unquestionably reduce speed experimentally obtained instance
form reinforcement learning eligibility traces singh sutton might
used experiments goal moved baseline dyna q sutton
specifically designed deal changing worlds would probably
better reference point
speed obtained transferring pieces action value function
compared alternatives transferring pieces policy transferring pieces
model transferring pieces policy would reduce memory requirements
require rescaling applied pieces action value function however
two disadvantages firstly solution directly composed position
decision boundaries determined learning would necessary decide
appropriate policy room secondly policy indicates best action
action value function orders actions indicating potentially useful small changes
policy might improve accuracy task transferring pieces
model would require first learning model consisting probability distribution function
action state memory requirement considerably larger unless
states reachable action limited beforehand nevertheless model would need
less modification changing world goal moved carries
information might speed learning action value function seems good
compromise terms complexity versus information content would need
empirically validated subject future work

performance variation task configuration
generally function composition outperforms baseline learning amount
dependent complexity learning robot navigation domain
goal moved amount speed increased rooms fewer
paths goal speed average speed obtained
configurations five rooms single path goal configurations three


fidrummond

rooms least speed due relative simplicity






















q learning

function composition

learning steps x





figure failure robot navigation moving goal





top figure shows average four learning curves three room
configurations bottom figure shows one configurations produced
curves one easiest tasks experimental set
baseline solutions case base lowest room
isomorphic subgraphs form rather composing solution
system introduces constant value function room room represents almost
half state space much additional learning required top figure shows
initially significant speed refinement reduces advantage
short baseline better later function composition gains
upper hand converges quickly baseline towards asymptotic
value

average steps goal

fiaccelerating reinforcement learning

robot navigation domain learning task amount speed varied
size inner room primarily due number actions needed
features emerged sucient clarity snake locate function
composition successful inner room small wall long feature
takes time develop refinement q learning needed make apparent
short walls hard identify likelihood robot colliding
small takes many exploratory actions features emerge clearly
features may suciently clear snake form partition yet well
enough defined precisely locate doorways doorway may appear bit wider
actually importantly may appear displaced true position
typically error composed function small normal reinforcement learning
quickly eliminates one experimental runs configuration figure
speed reduced factor due doorway incorrectly positioned
feature representing lower wall completely emerged partition
generated made doorway appear almost exactly corner
fact positioned doorway wrong side corner resulted
significantly reduced speed unclear reinforcement learning took
long correct seems surface least local error
investigated future work

limitations

limitations come roughly two kinds arising overall
arising way implemented former case ways address limitations may highly speculative impossible without abandoning fundamental
ideas behind latter case reasonable expectation future
work address limitations following sections deal cases
turn

limitations

explore possible limitations section reviews fundamental
assumptions
fundamental assumption features arise reinforcement learning function
qualitatively define shape features used violation
smoothness assumption neighboring states similar utility values wall
preventing transitions neighboring states typically causes violation
things actions significant cost would similar effect smaller
much varied costs generate features required offers
little way speed cases mixture large small costs
expected system capture features generated former initialize
function normal reinforcement learning address latter
smoothness assumption less clear dimensions numeric neighborhood relation used predefined distance metric continuous space
nominal binary mixed domains obvious metric would defined
although work metrics applications osborne bridge


fidrummond

dimensions mixed feature location might limited continuous
ones dimensions purely nominal binary generalization snake may
appropriate snake abstract level constrained hill climber whether
idea would usefully generalize way present somewhat speculative
fundamental assumption features clearly delimit subtasks domains discussed obstacles walls subdivide state space regions
connected small doorways subtask reaching one doorway greatly affected
subsequent subtask domains may case doorways
become larger context sensitivity increases long composed solution reasonably accurate reinforcement learning easily correct error although speed
reduced point however due large amount context sensitivity
advantage dividing task subtasks become questionable would possible
account context dependency graph matching stage looking
larger units subgraphs two adjacent subgraphs match might
used pair thereby including contextual relationship even
single subgraphs used context appear e shape neighboring
subgraphs could taken account limit graph matching whole task might
used argued introduction would considerably limit transfer
applicable thus overall effectiveness
fundamental assumption absolute position features unimportant shape delimited region matters increase likelihood
transfer solutions subtasks subjected variety transformations
domains many transformations invalid actions cannot
rotated ected many small costs affect different regions state space
effectiveness transfer reduced would extent addressed
additional penalties different transformations would limit opportunities transfer transformations appropriate whether
determined automatically domain subject future
fundamental assumption vision processing technique locate
features timely fashion even high dimensional domains learning high
dimensional domains likely slow whatever technique used normal reinforcement
learning take time navigate much larger space slowing emergence
features although time taken partition function increase frequency
partitioning applicable decrease thus amortized cost rise
slowly high dimensional spaces generally problematical methods
principal components analysis projection pursuit nason used reduce
dimensionality may prove practice dimensionality important
focus feature extraction much smaller actual dimensionality space

limitations implementation
assumptions previous section met expected remaining limitations due present implementation limitations likely become
apparent system applied domains certainly domains may differ
presented number ways


fiaccelerating reinforcement learning

domain may differ dimensionality space higher two
dimensions tasks investigated implementation snake
updated work higher dimensions bold lines top figure
one simpler tasks robot navigation domain task extended
z dimension snake starts sphere expands outwards fills
room example polygonal constraint used everything else
remains figure shows complete partition task

figure adding z dimension

figure complete partition

mathematics behind snake limited three dimensions seems
nothing principle would prevent processes graph matching
transformation working higher dimensions speed main
unique large body addressing
issue instance although graph matching general np complete
much active speeding matching average special cases gold
rangarajan galil present snake represents principal restriction
speed issue great importance vision processing community current
investigating least two three dimensions one example
hierarchical methods schnabel leroy herlin cohen solutions
snake progressively finer finer resolution scales
undoubtedly importance
domain may differ value function learned might produce features locatable snake present parameter settings values parameters
empirically determined hand crafted examples robot navigation
robot arm domains obvious danger parameters might tuned
examples demonstrate case configurations experiments
robot navigation domain generated randomly configurations robot arm
domain tightly constrained hand crafted examples used experiments nevertheless experiments shown parameters worked successfully
random examples robot navigation domain parameters work successfully second domain robot arm following discussion demonstrates


fidrummond

reasonably effective quite different domain car hill
anticipated current snakes automate selection
many parameters
car hill domain moore task simply stated get car
steep hill figure car stationary part way hill fact anywhere
within dotted line insucient acceleration make top
car must reverse hill achieve sucient forward velocity accelerating
side accelerating hill state space purposes
reinforcement learning defined two dimensions position velocity
car shown figure goal reach top hill small
positive negative velocity domain two possible actions accelerate
forward accelerate backwards unlike previous domains clear mapping
actions onto state space state achieved applying action determined
newton laws motion car insucient acceleration make hill
everywhere state space wall effectively introduced bold line figure
reach top hill car must follow trajectory around wall dashed
line figure
goal

velocity



goal





cit

lo






position

figure car hill

position

figure car state space

figure shows reinforcement learning function exhibits steep gradient
domains important point note unlike domains
physical object causes gradient implicit yet features
still exist figure shows partition produced applying snake car
hill domain main difference previous examples polygonal
constraint used snake initially comes rest mercury force
turned snake allowed minimum energy state
necessary reduce scaling edges factor three quarters achieve
accuracy fit fit around top left corner second snake dashed line


fiaccelerating reinforcement learning

snake growing slowly downwards present
stopped reached maximum number iterations allowed one diculty
example clear delimitation upper lower regions
end feature future work investigate altering stopping condition
eliminate

figure steep gradient

figure regions extracted

domain may differ shape regions partition complex dealt present snake fitting snake task discussed
previous paragraphs goes way towards mitigating concern nevertheless
randomly generated examples section subject certain constraints configurations narrower rooms tried informally snake reliably locate
features configurations section represent limit complexity partition
snake produce present expected ideas large body
already published snakes go long way towards addressing limitation
complex regions locating subtleties underlying shape may unnecessary
even undesirable aim speed low level learning long solution
reasonably accurate speed obtained sensitive minor variations
shape may severely limit opportunities transfer thus reduce speed overall
domain may differ changes environment complex
investigated present system detects goal moved
counting often reward received old goal position rather
ad hoc account possible changes paths
becoming blocked short cuts becoming available present learning task
system restarted required determine present solution longer
applicable future work system decide model world longer
correct decide relationship existing task
might best exploited allow complex interaction function
composition system reinforcement learning instance learning task


fidrummond

robot navigation domain used relatively simple situation two rooms function
composition system initialized low level detecting suitable features
future address complex tasks many rooms incremental
used task learned system progressively build
solution function composition different features become apparent
handle errors system might make feature extraction experiments simple room configurations filtering discussed
section proved sucient prevent complex tasks likely
false doorways detected simply system explored
region state space composed function including extra doorway drive
system region become quickly apparent doorway
exist function composed

related work
strongly related work investigating macro actions reinforcement learning precup sutton singh propose possible semantics macro actions
within framework normal reinforcement learning singh uses policies learned
solve low level primitives reinforcement learning higher level mahadevan connell use reinforcement learning behavior robot control
learn solution task systems require definition subtask
interrelationships solving compound task work presented gives
one way macro actions extracted directly system interaction
environment without hand crafted definitions shows determine
interrelationships macro actions needed solve task thrun identify macro actions finding commonalities multiple tasks
unlike presented mapping actions tasks proposed
hauskrecht et al discuss methods generating macro actions parr
develops control caching policies used multiple tasks
cases need given partitioning state space
automatic generation partition focus much work
presented may well generating partitions
determining interrelationships partitions related tasks prove useful
work
another group closely connected work forms instance case
learning used conjunction reinforcement learning
used address number issues economical representation state
space prioritizing states updating dealing hidden state first issue
addressed peng tadepalli ok use learned instances
combined linear regression set neighboring points sheppard salzberg
use learned instances carefully selected genetic
second issue addressed moore atkeson keep queue interesting
instances predecessors states learning produces large change values
updated frequently improve learning rate third issue addressed
mccallum b uses trees expand state representation include prior


fiaccelerating reinforcement learning

states removing ambiguity due hidden states work mccallum uses
single representation address hidden state general
representing large state space case base state sequences associated
trajectories unlike work presented case
example value function learning instead complete
learning episode method complementary approaches
work related case hammond veloso carbonell
firstly general connection reinforcement learning
analogous ways small change world
goal moved composite plan modified sub plans extracted
composite plans
last least connection object recognition vision suetens
et al chin dyer work presented many methods
final application come field features reinforcement learning
function akin edges image located finding zero crossing point
laplacian introduced marr work presented proposed
features largely dictate form function mallat zhong
shown function accurately reconstructed record steep slopes

conclusions

described system transfers prior learning significantly
speed reinforcement learning related tasks vision processing techniques utilized
extract features learned function features used index case
base control function composition produce close approximation solution
task experiments demonstrated function composition often produces
order magnitude increase learning rate compared basic reinforcement
learning

acknowledgements
author would thank rob holte many useful discussions help preparing
work part supported scholarships natural sciences
engineering council canada ontario government

appendix spline representations

appendix presents underlying mathematics associated spline representations snake meant introduction subject rather
added completeness discuss certain important aspects system addressed
elsewhere knowledge aspects necessary understand basic
principles discussed would necessary one wanted
duplicate system detailed explanation given drummond
specific papers address ideas much greater detail splines terzopoulos
snakes cohen cohen leymarie levine


fidrummond

splines piecewise polynomials degree polynomial determines
continuity smoothness function approximation additional smoothing constraints
introduced penalty terms reduce size differentials one way
view spline fitting form energy functional equation

espline f

z

r





efit f esmooth f ds



energy associated goodness fit measure close
approximating function input function typically least squares
distance functions energy associated smoothness
function two commonly used smoothness controls produce membrane thin
plate splines restricting first second differentials function respectively
fit spline function total energy must minimized necessary condition
euler lagrange differential equation equation controls
tension spline resistance stretching stiffness resistance
bending often error function individual data points left hand
side equation would include delta functions




f f f f








work splines used number purposes fitting
snake measures first second differential needed two dimensional quadratic
spline fitted discrete representation maximum q values used
zero limit overshoot drummond prevent false edges values
identical spline except squared divided differential
values normalizes differentials size edges dependent
occur function type spline used produce bowls associated
rooms discussed section giving roughly
gaussian smoothing values used produce function weighted values close
one given weights lower values weight prevents sides
bowls collapsing smoothing
one dimensional cubic spline used locating doorways found
steepest descent value differential along body snake differential
contains many local minima associated doorways arise
inherent noise process errors fit snake aim remove
ones associated doorways smoothing thresholding achieved
first sampling gradient points along snake values normalized lie
zero one spline weighted least mean
squares fit used weighting function inverse square values preventing
spline overwhelmed large values starting points steepest descent
changes sign coecients gradient spline initial step size
set slightly larger knot spacing decreased time local
minimum found value exceeds threshold rejected
represent snake model spline must changed somewhat snake
one dimensional cubic spline energy minimum sought


fiaccelerating reinforcement learning

differential qmax function subject constraints dynamics
snake defined euler langrange equation shown equation
f
f
f


f




c tp f f



c minimizes changes snake shape grows penalizing
difference second differential previous time step scaled ratio
lengths initial stiffness snake reduced proportionately
snake length give spline degrees freedom control
momentum drag snake respectively cohen cohen
factor added energy associated differential direction normal
body snake shown equation instead constant
variable used produce mercury model discussed section

f f f
n r fifirqmax f fifi
n






energy minimization process carried iteratively interleaving steps x
directions differential r jqmax j x direction given equation
similar equation used direction

qmax
qmax qmax



jrq xmax j q xmax x


x






snake grows forces mercury model reaches approximately
stable position subject small oscillations converted polygon
n
finding corners normal passes n

coecient set zero everywhere coecient set zero corners
produces polygon exible vertices
detect features early possible learning process discussed section
height gradient scaled according signal noise ratio noise
arises variations low level learning process stochastic nature task
size features noise grow time somewhat normalized
scaling process idea collect uniformly sampled values function shown
equation x directions median absolute values
median strongly affected extreme values thus largely ignores size
features measuring noise regions

references
chin c h dyer c r model recognition robot vision computing
surveys
christiansen learning predict uncertain continuous tasks proceedings
ninth international workshop machine learning pp


fidrummond

cohen l cohen finite element methods active contour
balloons images ieee transactions pattern analysis machine
intelligence
dijkstra e w note two connexion graphs numerische
mathematik
drummond c preventing overshoot splines application reinforcement
learning computer science technical report tr school information technology engineering university ottawa ottawa ontario canada
drummond c case base surfaces speed reinforcement learning
proceedings second international conference case reasoning vol
lnai pp
drummond c composing functions speed reinforcement learning changing world proceedings tenth european conference machine learning
vol lnai pp
drummond c symbol role learning low level control functions ph
thesis school information technology engineering university ottawa ottawa ontario canada
galil z ecient finding maximum matching graphs acm
computing surveys
gold rangarajan graduated assignment graph matching
ieee transactions pattern analysis machine intelligence
gordon g j stable function approximation dynamic programming proceedings twelfth international conference machine learning pp
gordon g j segre nonparametric statistical methods experimental evaluations speedup learning proceedings thirteenth international
conference machine learning pp
hammond k j case framework experience
journal cognitive science
hauskrecht meuleau n boutilier c kaelbling l p dean hierarchical
solution markov decision processes macro actions proceedings
fourteenth conference uncertainty artificial intelligence pp
kass witkin terzopoulus snakes active contour international journal computer vision
leroy b herlin l cohen l multi resolution active
contour proceedings twelfth international conference analysis
optimization systems pp


fiaccelerating reinforcement learning

leymarie f levine tracking deformable objects plane
active contour model ieee transactions pattern analysis machine
intelligence
macdonald graphs notes symetries imbeddings decompositions tech
rep electrical engineering department tr ajm brunel university uxbridge
middlesex united kingdom
mahadevan connell j automatic programming behavior robots
reinforcement learning artificial intelligence
mallat zhong characterization signals multiscale edges ieee
transactions pattern analysis machine intelligence
marr vision computational investigation human representation
processing visual information w h freeman
mccallum r instance state identification reinforcement learning
advances neural information processing systems pp
mccallum r b instance utile distinctions reinforcement learning
hidden state proceedings twelfth international conference machine
learning pp
moore w atkeson c g prioritized sweeping reinforcement learning
less data less real time machine learning
moore w variable resolution dynamic programming eciently learning action
maps multivariate real valued state spaces proceedings ninth international
workshop machine learning
nason g three dimensional projection pursuit tech rep department mathematics university bristol bristol united kingdom
osborne h bridge similarity metrics formal unification cardinal
non cardinal similarity measures proceedings second international
conference case reasoning vol lnai pp
parr r flexible decomposition weakly coupled markov decision
proceedings fourteenth conference uncertainty artificial
intelligence pp
peng j ecient memory dynamic programming proceedings
twelfth international conference machine learning pp
precup sutton r singh p closed loop macro actions
working notes aaai fall symposium model directed autonomous
systems pp


fidrummond

precup sutton r singh p theoretical reinforcement
learning temporally abstract options proceedings tenth european
conference machine learning vol lnai pp
schnabel j multi scale active shape description medical imaging ph
thesis university london london united kingdom
sheppard j w salzberg l teaching strategy memory control
artificial intelligence review special issue lazy learning
singh p sutton r reinforcement learning replacing eligibility traces
machine learning
singh p reinforcement learning hierarchy abstract proceedings tenth national conference artificial intelligence pp
suetens p fua p hanson computational strategies object recognition
computing surveys
sutton r integrated architectures learning reacting
approximating dynamic programming proceedings seventh international
conference machine learning pp
sutton r generalization reinforcement learning successful examples
sparse coarse coding advances neural information processing systems pp

sutton r barto g reinforcement learning introduction mit press
tadepalli p ok scaling average reward reinforcement learning approximating domain value function proceedings thirteenth
international conference machine learning pp
tanimoto l elements artficial intelligence w h freeman
terzopoulos regularization inverse visual involving discontinuities
ieee transactions pattern analysis machine intelligence
thrun schwartz finding structure reinforcement learning advances
neural information processing systems pp
veloso carbonell j g derivational analogy prodigy automating
case acquisition storage utilization machine learning
watkins c j dayan p technical note q learning machine learning





