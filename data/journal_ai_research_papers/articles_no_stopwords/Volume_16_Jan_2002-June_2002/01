Journal Artificial Intelligence Research 16 (2002) 59-104

Submitted 5/01; published 2/02

Accelerating Reinforcement Learning Composing
Solutions Automatically Identified Subtasks
Chris Drummond

School Information Technology Engineering
University Ottawa, Ontario, Canada, K1N 6N5

cdrummon@site.uottawa.ca

Abstract

paper discusses system accelerates reinforcement learning using transfer
related tasks. Without transfer, even two tasks similar
abstract level, extensive re-learning effort required. system achieves much
power transferring parts previously learned solutions rather single complete
solution. system exploits strong features multi-dimensional function produced
reinforcement learning solving particular task. features stable easy
recognize early learning process. generate partitioning state space
thus function. partition represented graph. used index
compose functions stored case base form close approximation solution
new task. Experiments demonstrate function composition often produces
order magnitude increase learning rate compared basic reinforcement
learning algorithm.

1. Introduction
standard reinforcement learning algorithm, applied series related tasks, could learn
new task independently. requires knowledge present state infrequent
numerical rewards learn actions necessary bring system desired goal
state. paucity knowledge results slow learning rate. paper shows
exploit results prior learning speed process maintaining
robustness general learning method.
system proposed achieves much power transferring parts previously
learned solutions, rather single complete solution. solution pieces represent
knowledge solve certain subtasks. might call macro-actions (Precup,
Sutton, & Singh, 1997), obvious allusion macro-operators commonly found
Artificial Intelligence systems. main contribution work providing way
automatically identifying macro-actions mapping new tasks.
work uses syntactic methods composition much symbolic planning,
novelty arises parts composed multi-dimensional real-valued functions.
functions learned using reinforcement learning part complex functions
associated compound tasks. ecacy approach due composition
occurring suciently abstract level, much uncertainty removed.
function acts much funnel operator (Christiansen, 1992), although individual
actions may highly uncertain, overall result largely predictable.
c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDrummond

subtasks identified basis strong features multi-dimensional
function arise reinforcement learning. features \in world",
system's interaction world. Here, \strong" means features
stable (i.e. relatively insensitive variations low level learning process) easy
recognize locate accurately early learning process. One important aspect
features largely dictate shape function. features differ
small amount, one would expect function differ small amount.
features generate partitioning function. popular technique object
recognition, snake (Kass, Witkin, & Terzopoulus, 1987; Suetens, Fua, & Hanson, 1992),
used produce partition. object recognition, snake produces closed curve
lies along boundary object, defined edges image. application, snake groups together sets features define region function.
boundary region low order polygon, demarcating individual subtask.
repeated whole function covered. polygons converted discrete
graphs, vertex polygon becoming node graph. Merging graphs
produces composite graph representing whole task.
composite graph used control transfer accessing case base previously
learned functions. case base indexed graphs. relevant function determined
matching subgraph composite graph one acting index case.
associated functions transformed composed form solution new task.
used reinitialize lower level learning process. necessary transfer
produce exact solution new task. sucient solution close enough
final solution often enough produce average speed up. Reinforcement learning
refine function quickly remove error.
paper demonstrates applicability transfer two different situations.
first, system learns task particular goal position goal moved.
Although function change significantly, partition generated initial
task used compose function new task. second situation considered, system placed different environment within domain. Here, new
partition extracted control composition process.
paper unifies significantly extends previous work author (Drummond,
1997, 1998). Additional work largely focussed removing limitations
inherent partitioning approach introduced Drummond (1998). One limitation
original approach snake could extract polygons rectangles.
paper relaxes restriction, allowing applied different environment
within domain different task domain. Although lifting restriction
removes desirable bias, experiments demonstrate none ecacy
original system lost. Further, results broadly obtained larger set
related tasks different domain. Overall, function composition approach often
produces order magnitude increase learning rate compared
basic reinforcement learning algorithm.
rest paper begins Section 2 giving high level discussion
approach taken. Section 3 gives depth discussion techniques used. Sections 4 5 present analyze experimental results. Subsequent sections deal
limitations related research.
60

fiAccelerating Reinforcement Learning

2. Overview
intent section appeal intuitions reader, leaving much
detail later sections paper. subsections follow demonstrate turn:
features function produced reinforcement learning; graphs
based features used control composition function pieces;
features easy detect early learning process; features exist
multiple domains.

2.1 Features Reinforcement Learning Function
overview begins high level introduction reinforcement learning
function produces. show features function
extracted converted graphical representation.
One experimental test beds used paper simulated robot environment
different configurations interconnected rooms. robot must learn navigate eciently
rooms reach specified goal start location. Figure 1 shows one
example 5 rooms goal top right corner. robot's actions small
steps eight directions, indicated arrows. Here, location, state,
simply robot's x coordinates. thin lines Figure 1 walls
rooms, thick lines boundary state space.
+1
Goal

Robot
10



11

11
12

12
13

13

14

-1
X
-1

+1

Figure 1: Robot Navigating Series Rooms
61

fiDrummond

action independent preceding actions, task becomes one learning
best action state. best overall action would one takes robot
immediately goal. possible states close goal. Suppose
robot particular state number steps goal
neighboring states known, indicated numbered squares surrounding robot
Figure 1. one step look ahead procedure would consider step select
one reaches neighboring state shortest distance goal. Figure 1
robot would move state 10 steps goal. process repeated, robot
take shortest path goal. practice must, course, learn values.
done using type reinforcement learning (Watkins & Dayan, 1992; Sutton, 1990)
progressively improves estimates distance goal state
converge correct values.
(-1.0,1.0)

(0.25,1.0)

(0.25,0.9)


(-1.0,0.25)

(-0.9,0.25)
(0.25,0.25)

Figure 2: Value Function Obtained Using Reinforcement Learning
function shown Figure 2 called value function. Subsequently, term
function mean value function unless otherwise indicated. function result
reinforcement learning problem Figure 1, instead representing
actual distance goal, represents essentially exponential decay distance goal.
reasons made clear Section 3.1. shaded areas represent large
gradients learned function. Comparing environment shown Figure 1,
apparent correspond walls various rooms. strong
features discussed paper. exist extra distance robot
travel around wall reach inside next room path goal.
features visually readily apparent human, seems natural use vision
processing techniques locate them.
edge detection technique called snake used locate features. snake
produces polygon, instance rectangle, locating boundary room.
doorways room occur differential function, along body
snake, local minimum. direction differential respect edges
62

fiAccelerating Reinforcement Learning

polygon, associated walls room, determines entrance
exit. positive gradient room indicates entrance; positive gradient
room indicates exit. information, plane graph, labeled (x; y)
coordinate node, constructed. Figure 2 shows one example, room
top left corner state space, subsequent graphs show coordinates.
Nodes corresponding doorways labeled \I" \O" respectively;
positions function indicated dashed arrows.

2.2 Composing Function Pieces

overview continues showing graphs, extracted features
function learned reinforcement learning, used produce good approximation
solution new goal position. left hand side Figure 3 shows plane graphs
rooms (ignore dashed lines circles now). node representing
goal labeled \G". directed edge added \I" \O" \I" \G", appropriate.
Associated edge number representing distance nodes.
determined value function points doorways. individual
graph merged neighbor produce graph whole problem, right
hand side Figure 3. doorway nodes relabeled \D". composite
graph represents whole function. individual subgraph represents particular part
function. information stored case base. subgraph index
corresponding part function case.



G



Extract
Graphs

G



Merge
Graphs

G





G












Figure 3: Graphical Representation
suppose goal moved top right corner top left corner
state space. Reinforcement learning basic form would required learn
new function scratch. work goal moved, new goal position
63

fiDrummond

known, node representing goal relocated. new goal position shown
dashed circle Figure 3. edges connecting doorways goal
changed account new goal position. dashed lines representing new edges
replace arrows subgraph. produce new function, idea regress
backwards goal along edges. edge, small subgraph containing
edge extracted. extracted subgraph used index case base functions.
retrieved function transformed added appropriate region state space
form new function.
Rotate
Stretch

Rotate


G

Stretch



Figure 4: Function Composition
example, existing subgraphs match new configuration. two
subgraph originally containing goal subgraph containing
goal. certainly possible exchange two, using appropriate transform.
graphs case base may better match new task. best match
subgraph containing new goal is, fact, subgraph goal original
problem. fit new task, plane graph rotated stretched slightly
new x direction changing coordinates nodes, see Figure 4.
transformation applied function. room containing original goal,
case obtained solving another task better match. three rooms use
functions original problem, since changing goal position little effect
actions taken. fact, height functions must changed. simply
multiplication value representing distance goal \O" doorway (this
discussed detail end Section 3.3). matching subgraphs
allows error asymmetric scaling may used, resulting function may
exact. experiments demonstrate, function often close
reinforcement learning quickly correct error.
64

fiAccelerating Reinforcement Learning

new position goal must established graph modified
function composition occur. system told goal moved, rather
discovers determining longer maximum existing function.
uncertainty exact boundary original goal. robot may reach
state believes part original goal region, fail detect even
goal moved. reasonably certain goal fact moved,
required occur ten times intervening occurrence goal detected
maximum.
system composes search function, assuming particular room contains
goal. Search functions produced composing previously learned functions.
However, room assumed contain goal function constant.
bias search particular part room allows limited learning
encourage exploration room. search function drives robot room
anywhere else state space. fails find goal fixed number steps,
new search function composed another room assumed contain goal.
process repeated goal located ten times, ensures good estimate
\center mass" goal. \center mass" used new position
goal node composite graph. Requiring old goal new goal positions
sampled fixed number times proven effective domains discussed
paper. Nevertheless, somewhat ad hoc procedure addressed future
work, discussed Section 6.2.

2.3 Detecting Features Early
previous section, existing task new task strongly related, walls
doorways fixed goal position different. section,
relationship assumed. robot faced brand new task must determine
what, any, relationship exists new task previous tasks.
experimental testbed simulated robot environment, time
problem simplified inner rectangular room outer L-shaped room. Figures
5 6 show two possible room configurations. Again, thin lines walls
room, thick lines boundary state space. Suppose robot already
learned function \Old Task" Figure 5. would hope could adapt
old solution fit closely related \New task" Figure 6.
steps, example, essentially previous one.
learning process started afresh, features system must wait
emerge normal reinforcement learning process. proceed much
before. First graph inner room extracted. best matching graph
case base old task rotated stretched fit new task. Next matching
graph outer L-shaped room rotated stretched around larger inner room.
transforms applied associated functions, height adjustments
carried functions composed form approximate solution new task.
example, first step process locate goal.
partition aid search, initial value function set mid-range constant value
(see Figure 7). allows limited learning encourages system move
65

fiDrummond

Goal

Robot

Outer
Room

Inner
Room

Robot

Inner
Room

Outer
Room

Goal

Figure 5: Old Task

Figure 6: New Task

away regions explored previously, prevent completely random walk
state space. goal located, learning algorithm reinitialized function
goal position walls (see Figure 8). function exist
case base, rough approximation could used instead. \no walls" function
used exactly stored case base. difference goal rest
state space reduced scaling function adding constant. reduces
\bias" function, allowing learning algorithm alter relatively easily new
information becomes available.

Figure 7: Start Function

Figure 8: Intermediate Function

Figure 9 shows resultant function 3000 exploratory steps beginning
learning process. Again, large gradients associated walls readily
66

fiAccelerating Reinforcement Learning

apparent. Figure 10 shows function new task allowed converge
good solution. functions roughly form, large gradients
position, although learning latter took 200,000 steps. \no
walls" function introduced features take time clearly emerge. snake
typically filter features small well formed. Additional filtering
graphical level constrains acceptable features. total set features must
produce consistent composite graph, doorways different subgraphs must align
graph must overlay complete state space. must matching case
case base every subtask. Many checks balances removed
iterative updating technique Section 6.2 incorporated.

Figure 9: Early Function

Figure 10: New Task Function

2.4 Different Task Domain

previous sections dealt simple robot navigation problem. section demonstrates features exist quite different domain, two degrees
freedom robot arm, shown Figure 11. shoulder joint achieve angle radians, elbow joint angle =2 radians, zero indicated
arrows. arm straight shoulder joint rotated, elbow joint describe
inner dotted circle, hand outer dotted circle. eight actions, small
rotations either clockwise anti-clockwise joint separately together. aim
learn move arm eciently initial position hand reaches
goal perimeter arm's work space.
state space, purposes reinforcement learning, configuration space
arm, sometimes called joint space (see Figure 12). x-axis angle
shoulder joint, y-axis elbow joint. eight actions mapped actions
configuration space become much actions robot navigation problem, shown
shaded diamond (labeled Arm) Figure 12. map obstacle work space
configuration space, one must find pairs shoulder elbow angles blocked
obstacle. obstacles space become elongated form barriers much
67

fiDrummond

+/2

Shoulder

Hand

0

Elbow Angle

Obstacle

00
11
11
00
00
11
00
11
11
00

G


L

1111111
0000000
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111

Arm

0

Obstacle

Elbow
0

/2


Obstacle
00
11
11
00
00
11
00
11
11
00

Figure 11: Work Space

0000000
1111111
1111111
0000000
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111
0000000
1111111

Obstacle

G


L

0
Shoulder Angle

+

Figure 12: Configuration Space

walls experiments previous sections. clear, imagine straightening
arm work space rotating intersects one obstacles,
middle dotted line Figure 11. arm rotated shoulder joint
roughly linearly proportional rotation elbow joint, opposite direction,
keep intersecting obstacle. produces \wall" configuration space.
linearity holds small objects far perimeter work space.
complex, larger objects, would result complex shapes configuration
space. moment feature extraction method limited simpler shapes,
discussed Section 6.
reinforcement learning function produced problem shown Figure 13.
features shaded clarity. large gradient associated obstacle
left hand side configuration space clearly seen. similar large
gradient associated obstacle right hand side configuration space.
Again, features used control composition functions goal
moved different task domain.

3. Details Techniques Used
section discuss detail techniques used. include: reinforcement
learning produce initial function, snakes extract features producing graph,
transformation composition subgraphs, corresponding functions, fit new task.

3.1 Reinforcement Learning

Reinforcement learning typically works refining estimate expected future reward.
goal-directed tasks, ones investigated here, equivalent progressively
68

fiAccelerating Reinforcement Learning

Figure 13: Robot Arm Function
improving estimate distance goal state. estimate updated
best local action, i.e. one moving robot, arm, new state
smallest estimated distance. Early learning process, states close goal
likely accurate estimates true distance. time action taken, estimate
distance new state used update estimate old state. Eventually
process propagate back accurate estimates goal states.
Rather directly estimating Pthe distance goal, system uses expected

discounted reward state E [ 1
t=1 rt ]. uence rewards, rt , reduced
progressively farther future occur using less one. work,
reward reaching goal. farther state goal smaller
value. use expectation allows actions stochastic, robot,
arm, takes particular action particular state, next state always same.
carry reinforcement learning, research uses Q-learning algorithm (Watkins
& Dayan, 1992). algorithm assumes world discrete Markov process, thus
states actions discrete. action state s, Q-learning maintains
rolling average immediate reward r plus maximum value action a0
next state s0 (see Equation 1). action selected state usually one
highest score. encourage exploration state space, paper uses -greedy
policy (Sutton, 1996) chooses random action fraction time.
effect function composition Q-learning algorithm initial value
state-action pair set value zero.
(1)
Qts;a+1 = (1 , ff)Qts;a + ff(r + maxa Qts ;a )
Q-function state action usually referred action-value function.
paper, action-value function transformed composed form
solution new task. value function, discussed previous sections shown
0

69

0

0

fiDrummond

figures, maximum value Q-function. used generate partition
associated graphs needed control process.
Watkins Dayan (1992) proved Q-learning converge optimal value
certain constraints reward learning rate ff. optimal solution produced taking action greatest value state. So, goal-directed tasks,
greedy algorithm take shortest path goal, learning complete.
extension continuous spaces may done using function approximation. simplest
method, one used here, divide state dimensions intervals. resulting action-value function cells representing average Q-value taking action
somewhere within region state space. off-line learning, action
state executed, representation proven converge (Gordon,
1995). on-line learning, current state determined environment,
approach generally successful, exists proof convergence.

3.2 Feature Extraction
Feature extraction uses vision processing technique fits deformable model called
snake (Kass et al., 1987) edges image. initializing snake, process
iterates external forces, due edges, balance internal forces snake
promote smooth shape. Here, external forces due steep gradients value
function. piecewise constant function approximator used, smoothed cubic b-spline
fitted value-function used generate necessary derivatives. left hand
side Figure 14 gradient value function shown Figure 9 extracting
features early learning process. system added gradient around border
represent state space boundary.
locate features, curve found lies along ridge hills, local
maximum differential. right hand side Figure 14, dashed lines
contour lines small inner room indicated. bold lines, right hand side
Figure 14, snake different stages process. snake first positioned
approximately center room, innermost circle. expanded
abuts base hills. simplify exposition, imagine
snake consists number individual hill climbers spread along line representing
snake, indicated small white circles. instead allowed climb
independently, movement relative constrained maintain smooth
shape. snake reaches top ridge, constrained polygon
{ instance quadrilateral { outside dark line Figure 14. point,
tend oscillate around equilibrium position. limiting step size process
brought stationary state. detailed mathematical treatment
approach given Appendix A.
polygon forms \skeleton" graph, shown top left Figure 14.
Nodes graph correspond vertices polygon doorways goal.
Looking gradient plot, doorways regions small differential
ridges. locations determined magnitude gradient along
boundary polygon. example, node added goal (labeled G)
connected \in" doorway (labeled I). polygon delimits region
70

fiAccelerating Reinforcement Learning

Graph

G

Polygon



Doorway

Figure 14: Gradient Resultant Polygon (Left) Extracted Snake (Right)
state space, therefore region action-value function. becomes case
case base, corresponding graph index. Constraining snake
polygon done two reasons. Firstly, vertices needed produce nodes
plane graphs, important part matching process. Secondly, additional
constraint results accurate fit boundaries subtask. This, turn,
results accurate solution function composition.

3.2.1 Three Extensions Snake Approach
section introduces three extensions basic snake approach facilitate extraction features.
first extension affects direction snake moves hill climbing gradient.
normal hill climbing, step taken direction steepest ascent, step size
determined size differential. Roughly, translates forces points
along body snake. force points direction steepest ascent locally,
interacts forces various shape constraints. Looking gradient
function contour lines Figure 14, steep slope leading top
ridge. significant slope along ridge away doorway towards
boundary state space. Thus force single point body snake
71

fiDrummond

directly towards top ridge turned towards apex, indicated
bold black arrow left hand side Figure 15.
Snake

Steepest
Ascent
Tangent
Normal

Figure 15: Controlling Forces Snake
force broken two components respect snake, normal
tangential force. latter force acts along body snake. shape
constrained quadrilateral, cause relevant side shrink. effect
partially counteracted force towards top ridge adjacent side
quadrilateral. net result shrinking two sides associated
ridges inwards forces balanced. push corner quadrilateral
near doorway inwards, indicated thin black arrow Figure 15. extreme
case, might cause snake collapse something close triangle.
likely outcome degradation accuracy registration ridges.
Drummond (1998) prevented degradation accuracy restricting snakes
rectangular shapes. weakening constraint general polygons,
effect becomes problem. problem addressed removing component
force tangential snake. hill climbing always direction
normal. significantly restrict motion snake: removed
component along body snake. Thus mainly prevents stretching
shrinking snake due gradient.
second extension controls way snake expanded reach base
hills. Drummond (1998) used ballooning force, introduced Cohen Cohen (1993).
problems arose extending system deal general shapes
rectangles, outer L-shaped room Figure 6. ballooning force expands
snake directions normal body. One deleterious effect snake contacts
sharp external corner, inner room, force tends push snake
corner. seen Figure 16; bold continuous lines snake;
bold dashed lines ridges. imagine starting circular snake
72

fiAccelerating Reinforcement Learning

middle L-shaped outer room, time reaches walls inner room
sides snake roughly perpendicular ridges. Thus little restrain
expansion snake passes completely walls inner room.
Ridge

Ballooning
Force

Ridge

Figure 16: Using Ballooning Force
approach adopted analogous ow mercury. imagine starting
somewhere middle L-shaped room progressively adding mercury, would
tend fill lower regions valley first reach bases hills roughly
time. analogy mercury used high surface tension preventing
owing small gaps edges associated doorways. increase
effectiveness idea, absolute value differential gradient thresholded,
values threshold set one zero. smoothed
truncated Gaussian, shown Figure 17. Smoothing thresholding commonly
used techniques machine vision (Tanimoto, 1990). typically used remove
noise, aim strongly blur thresholded image. produces bowls
associated room. example, smoothing almost completely obscured
presence doorway, although generally case.
snake initialized small circle minimum one bowls.
shown circle middle Figure 18, dashed lines contour
lines function. ows outwards, follow contour lines bowl;
largest component ow direction arrows Figure 18.
achieved varying force normal body snake according height
difference average height snake. Thus points along snake
higher average tend get pushed inwards, lower pushed outwards. surface
tension mercury produced various smoothing constraints first second
differentials snake (see Appendix A).
third extension limits changes shape snake expands initial
position reach base hills. smoothness constraints snake, give
mercury-like properties, prevent snake owing gaps associated
73

fiDrummond

Figure 17: Smoothed Function

Figure 18: Mercury Flow

doorways. even proved insucient width rooms width
doorways similar sizes. Figure 12, looking \room" left hand side
configuration space robot arm, \doorway" \room" top
similar width. Increasing surface tension mercury suciently prevent ow
doorways prevents ow top room.
solution limit amount snake change shape grows.
achieved constraining much second differential snake change
step step. Figure 18, apparent snake takes good approximation
shape room time reaches ridges. shape
locked-in reaching ridges, problem described avoided.
snake initialized, constraint smoothness. snake expanded,
smoothness constraint progressively weakened curvature constraint progressively
strengthened. progressively locks shape still allowing snake make
small local adjustments better fit features.
extensions, discussed section, either modify traditional forces act
snake add new ones. forces associated knot spacing drag.
snake moves, iteration, depends vector addition forces.
sum acts accelerate body snake mass velocity,
therefore momentum. schematic representation forces shown Figure 19;
detailed mathematical description given Appendix A. dashed line represents
body snake; arrows forces applied one point body. snake
parameterized function, given f^(s) = (x(s); y(s)) x(s) y(s) individual
cubic b-splines giving x coordinates associated variable along body
snake. circles represent points equi-distant necessarily x y.
points kept roughly Euclidean distance apart x due knot
spacing force. momentum, although strictly force, encourages point move
74

fiAccelerating Reinforcement Learning

constant direction; drag opposes motion. stiffness encourages snake
maintain smooth shape. overall stiffness reduced snake grows, keep
exibility per unit length roughly constant, controlled locally maintain
shape.
Steepest Ascent

MercuryFlow
Momentum

Knot Spacing

Drag
Stiffness

Figure 19: Forces Snake
following algorithmic summary processing snake:

Initialize coecients produce circular snake middle room.
Iterate forces roughly equilibrium snake oscillates around
stationary value.

Modify stiffness enforce polygonal constraints
Iterate 25 steps increasing momentum drag step reduce
oscillation small value.

Use final position snake form polygon delimits boundary
room.

3.3 Transformation

section discusses matching process { subgraph used locate transform
function case base. matching process first finds subgraphs case base
isomorphic extracted subgraph possible isomorphic mappings
nodes, using labeling algorithm (MacDonald, 1992). number isomorphic mappings
75

fiDrummond

potentially exponential number nodes. Here, graphs typically
nodes symmetries, isomorphic mappings. Associated
node subgraph (x; y) coordinate. ane transform, Equation 2, found
minimizes distances coordinates mapped nodes
isomorphic subgraphs. advantage transform relative exibility
simple form.

x0 = C0 x + C1 + C2 y0 = C3 x + C4 + C5
(2)
Ideally transformed nodes would positioned exactly mapped nodes,
usually possible. Even simple rectangular shapes, case base may
contain graph exactly doorway positions. Using graph
exact match introduce error composed function new task.
weighting nodes others error occurs controlled. One aim
minimize introduction errors affect overall path length. However,
equal importance errors introduced easily correctable normal reinforcement
learning.
1

1

2 4

1

2

1

Figure 20: Weighting Graph Nodes
left hand side Figure 20 shows composite graph new task. right
hand side shows result overlaying graph case base. fit
doorway outer L-shaped room error, robot tend miss doorway
collide wall one side. farther doorway position, longer
normal reinforcement learning take correct error. encourage good fit
doorway, weight 4 used. Nodes adjacent doorway given weight 2,
nodes weight one. based intuition trajectories,
different parts state space, pass region close doorway.
error likely broader effect, take longer normal reinforcement
76

fiAccelerating Reinforcement Learning

learning correct, regions far doorway. fit around inner room
improved sacrificing fit far doorway.
exact position doorway inner room critical weight
set 0.5. Whatever position doorway, shape function correct
inside room goal room. However, doorway
correct position, greater error edge length. produce error
composed function, expectation error small
reinforcement learning quickly correct it.
fit good, would prefer amount transformation small. transforms produce error particularly true
asymmetric scaling, discussed later section. Generally transform produces
translation, ection, rotation, shearing independent scaling dimension.
robot navigation domain, distance points state space
normal Euclidean distance. reinforcement learning function exponential decay
distance goal. transformation change Euclidean distance,
transformed function directly applicable.

Affine Similar Symmetric
(3)
ane transformation one family hierarchy transformations.
bottom hierarchy, shown Equation 3, symmetric transformations.
solid body transformations change Euclidean distance. next step
hierarchy introduces scaling, equal dimension. affect Euclidean
distance multiplicative factor. Thus change needed transformed
function scale height. ane transformations allow addition asymmetric
scaling shear, distort Euclidean distance. determine amount
distortion, transformation applied unit circle. symmetric, rigid body,
transformations alter circle, transformations will. symmetric
scaling transform changes diameter circle. asymmetric scaling shear
transformations change circle ellipse. amount distortion Euclidean
distance introduced transform determined ratio lengths major
minor axes ellipse.
error = sqrt
(P
wi(
x2 + yi2 )) (node misalignment)
fi2



(Euclidean Distortion)
+ log2 fifi rrmaj
(4)
minfi
2

j
r
+
r
j
maj
min
+ 0:05 log2
(scaling factor)
2
error fit transformed subgraph combined transformation
error using lengths major minor axes, rmaj rmin respectively,
ellipse. penalty Euclidean Distortion asymmetric scaling shear.
log factor added directly error fit shown Equation 4. Log factors
used, penalty functions symmetric. small penalty symmetric
scaling. best matching subgraph found, transformation
applied associated function. isomorphic graph found total error less
1.5, constant function used default. new graph overlays
old graph, values assigned using bilinear interpolation discrete values
77

fiDrummond

function. not, bilinear extrapolation used, based closest values.
cases four values selected, value new point calculated
shown Equation 5. action-value function indexed action well state,
process carried action turn. rotation ection transform
applied predefined matrix actions. produces necessary mapping
actions original new action-value function.

v = c1 x + c2 + c2 xy + c3

(5)

Finally, height new action-value function must adjusted account
change overall distance goal. height value function \out" doorway
dg dg distance goal discount factor. value random
point within room dg+dd dd distance doorway. action-value
function first normalized dividing dg , height function doorway
original problem. multiplied dng , dng distance
new goal; value point becomes dng+dd . Scaling affect height
function. Assuming scaling symmetric new value function anywhere
room cdd c scale factor. Thus raising function power c
i.e. ( dd )c account scaling. scaling symmetric result exact, assuming
distance based linear combination two dimensions. asymmetric scaling,
result exact. difference two scale factors relatively small,
useful approximation use maximum.
following algorithmic summary whole matching process:

SG = subgraph extracted new task.
subgraph G acting index case base

{ isomorphic mapping G SG
Find minimum weighted least squares fit G SG using mapping
Ane transform = coecients least squares fit
Penalized fit = least squares error + transform penalty
Keep graph transform lowest penalized fit
Retrieve function associated best graph case base (if none use default)
Apply ane transform function
Apply bilinear interpolation/extrapolation
Adjust function height
Add new function existing function
78

fiAccelerating Reinforcement Learning

3.4 Composition

section describes function composition, transformation applied successively
series subgraphs extracted composite graph. Function composition uses
slightly modified form Dijkstra's algorithm (Dijkstra, 1959) traverse edges
doorway nodes. left hand side Figure 21 shows composite graph moving
goal robot navigation example Section 2.2. right hand side shows
graph traversed Dijkstra's algorithm.

G

d2

G



d1



Gr3

Gr1





d3



Gr2




Gr5



Gr4

Figure 21: Using Dijkstra's Algorithm
begin process, subgraph contains goal extracted best
matching isomorphic subgraph found. edge lengths composite graph
updated using scaled length corresponding edge matching isomorphic
subgraph, d1 d2 Figure 21. d2 less d1, next subgraph extracted,
Gr2, one sharing doorway node edge length d2. best matching
isomorphic subgraph found edge length d3 updated. shortest path
determined. d1 less d2 + d3 subgraph, Gr3 extracted. process
repeated subgraphs updated. stage subgraph matched,
corresponding transformed function retrieved added new function
appropriate region.
example, single path goal room. Often
multiple paths. Suppose room 5 additional doorway lower left corner
room, labeled \B" left hand side Figure 22, addition original doorway
labeled \A". graph, shown right hand side Figure 22, would result.
two possible paths goal lengths d4 d5. length across room 5, d6,
greater absolute difference d4 d5, choice path room
determined decision boundary inside room. produced taking
79

fiDrummond

0110
11111110 Room 3
000000
10

10
1111111
0000000
0
Room 2 1
1010
000 10 Room 5
111
000 10
111
Room 4 1
0
Room 1

d5

n2

d4

d6

n1

Gr5
B

B

n3

Figure 22: Multiple Paths Goal
maximum two functions shown Figure 23: one entering doorway \A"
leaving doorway \B"; one entering doorway \B" leaving doorway \A".
principle repeated two paths goal given
room.
cross-room distance, d6, smaller difference (jd4-d5j) decision
boundary would another room. general, want find room
cross-room distance larger difference incident paths.
repeated every cycle path graph. cycle detected node visited twice,
indicating reachable two separate paths. Let us suppose node n3
graph Figure 22. Dijkstra's algorithm used, know previous nodes,
either path, n1 n2 already closed. must true paths
reached n3. rooms paths nodes cannot contain decision
boundary, must either room 4 5. decide remaining room in,
compare two path lengths. d4 longer d5 + d6 decision boundary
room 4; otherwise room 5.
Whichever room selected, decision boundary produced maximum two
functions. heights two functions, adjusted path lengths, determine
decision boundary occurs within room. paths equal length, taking
maximum correctly put decision boundary doorway.
functions case base, functions already include decision boundaries may used.
technique produces correct decision boundary difference path lengths
entering room less difference heights function \out"
doorways. left hand side Figure 24 room two doorways. path
1 significantly longer path 2, decision boundary far left. shortest
path goal room via right hand doorway. function
combined mirror image itself, produce decision boundary middle
80

fiAccelerating Reinforcement Learning

Maximum
Decision
Boundary

Figure 23: Combining Two Functions

Decision
Boundary

Path 1
Path2

Room

Figure 24: Decision Functions

81

fiDrummond

room, shown right hand side Figure 25. could used new
problem shown left hand side Figure 25 two paths length.
heights two functions changed move decision boundary.
cannot moved anywhere room. decision boundary moved
closer particular doorway original function shown Figure 24
Decision
Boundary
Path 1

Path2

Room

Figure 25: Combining Decision Functions

4. Experiments

section compares learning curves function composition simple baseline algorithm. Four sets results presented; one two types related task
two domains. learning curves represent average distance goal
function number actions taken learning. distance averaged
64 different start positions, distributed uniformly throughout state space,
different experimental runs. determine distance, normal learning stopped
fixed number actions copy function learned far stored. One 64
start positions selected, learning restarted number actions needed reach
goal recorded. trial takes 2000 actions yet reached goal,
stopped distance goal recorded 2000. function reinitialized
stored version another start state selected. repeated 64 times.
function reinitialized normal learning resumed.
baseline algorithm underlying learning algorithm function composition system basic Q-learning algorithm, using discrete function approximator
discussed Section 3.1. learning rate set 0.1, greedy policy uses 0.1
(the best action selected 90% time), future discount 0.8 reward 1.0
received reaching goal. Although state spaces different domains represent two quite different things { robot's hx; yi location angle arm's two
joints { actual representation same. state space ranges 1
dimension. step 0:25 dimension either separately together, giving eight
possible actions. actions stochastic, uniformly distributed random value
0:125 added dimension action. robot navigation examples
82

fiAccelerating Reinforcement Learning

robot hits wall, positioned small distance wall along direction
last action. implemented robot arm somewhat
complex calculation. Instead, collision obstacle occurs arm restored
position taking action.
Learning begins randomly selected start state continues goal reached.
new start state selected randomly process repeated. continues
requisite total number actions achieved. Speed calculated dividing
number learning steps one specific point baseline learning curve number
learning steps equivalent point function composition system's learning curve.
knee function composition system's curve used. occurs low
level learning algorithm initialized composed function. compared
approximate position knee baseline curve.

4.1 Robot Navigation, Goal Relocation
first experiment investigates time taken correct learned function goal
relocated robot navigation domain. nine different room configurations,
shown Figure 26, number rooms varying three five four
different goal positions. room one two doorways one two paths
goal. initialize case base, function learned configurations
goal position shown black square. rooms generated randomly,
constraints configuration rooms doorways: room
small narrow, doorway large. case base includes
functions generated experiments discussed Section 4.3. necessary
give sucient variety cases cover new tasks. Even addition,
subgraphs matched. Constant valued default functions used
match. reduces speed significantly, eliminate altogether.

1

2

3

4

5

6

7

8

9

Figure 26: Different Suites Rooms
83

fiDrummond

case base loaded, basic Q-learning algorithm rerun room
configuration goal position shown. 400,000 steps goal moved,
denoted time x-axis Figure 27. goal moved one
three remaining corners state space, task included case base. Learning
continues 300,000 steps. fixed intervals, learning stopped average
number steps reach goal recorded. curves Figure 27 average
27 experimental runs, three new goal positions nine room configurations.
Function Composition

Average No. Steps Goal

Q-Learning
Q-Learning (No Reinit)

10

10

3

2

1

10
t-400....t-100

t-50



t+50

t+100

t+150

t+200

t+250

t+300

+ No. Learning Steps X 1000

Figure 27: Learning Curves: Robot Navigation, Goal Relocation
basic Q-learning algorithm, top curve Figure 27, performs poorly because,
goal moved, existing function pushes robot towards old goal position.
variant basic algorithm reinitializes function zero everywhere detecting
goal moved. reinitialized Q-learning, middle curve, performed much
better, still learn new task scratch.
function composition system, lowest curve, performed far best.
precise position knee curve dicult determine due effect using
default functions. examples using case base functions considered, knee
point sharp 3000 steps. average number steps goal 3000 steps,
examples, 40. non-reinitialized Q-learning fails reach value within
300,000 steps giving speed 100. reinitialized Q-learning reaches value
120,000 steps, giving speed 40. Function composition generally
produces accurate solutions. Even error introduced, Q-learning quickly
refines function towards asymptotic value 17. 150,000 steps,
84

fiAccelerating Reinforcement Learning

normal Q-learning reaches average value 24 steps slowly refines solution
reach average value 21 300,000 steps.

4.2 Robot Arm, Goal Relocation
second experiment essentially repeat first experiment robot arm
domain. initial number steps, goal moved, reduced 300,000
speed experiments. arm two degrees freedom,
restrictions discussed Section 2.4, number variations small. three obstacle
configurations used, constructed hand, two obstacles each. increase
number experiments, allow greater statistical variation, configuration
repeated goal three possible positions, shown Figure 28.
black diamonds represent obstacles, black rectangles goal. Solutions
tasks loaded case base. composing function, however, system
prevented selecting case comes goal obstacle configuration.

1

2

3

4

5

6

7

8

9

Figure 28: Robot Arm Obstacle Goal Positions
curves Figure 29 average 18 experimental runs, two new goal positions
three original goal positions three obstacle configurations shown
Figure 28. two learning curves, non-reinitialized Q-Learning dropped.
first experiment, function composition system, lower curve, performed
much better Q-learning. knee function composition system occurs 2000
steps, knee Q-learning 50,000 steps, giving speed 25. experiment,
case base contained subgraphs matched new tasks, default functions
needed. composed functions tend accurate little refinement
necessary.
85

fiDrummond

Function Composition

Average No. Steps Goal

Q-Learning

10

10

3

2

1

10
t-300....t-100

t-50



t+50

t+100

t+150

t+200

t+250

t+300

+ No. Learning Steps X 1000

Figure 29: Learning Curves: Robot Arm, Goal Relocation

4.3 Robot Navigation, New Environment
third experiment investigates time taken learn new, related, environment
robot navigation domain. Nine different inner rooms generated randomly,
constraints. single doorway, size position room
location doorway varied shown Figure 30. initialize case base,
function learned configurations goal inside small room
indicate dark square. Learning repeated room configurations
turn. However, composing new function system prevented selecting
case learned goal room configuration. Experimental runs Qlearning algorithm function composition system initialized function
zero 0.75 everywhere respectively, denoted zero x-axis. Learning continues
100,000 steps. improve statistical variation, experiments configuration
repeated three times, time new random seed. curves Figure 31 are,
therefore, average across 27 experimental runs.
top curve Q-learning algorithm, bottom curve function composition
system. experiments, locating goal took typically 400 1200 steps,
although took 2000 steps. function composition system introduces \no
walls" function typically 800 4000 steps taken usable features
generated. Again, certain experimental runs took longer, discussed Section
5.2. Due runs, knee function composition system's curve occurs 12,000
steps. knee basic Q-learning curve occurs approximately 54,000 steps giving
86

fi10

10

10

3

2

0

1

1

5

2

9

6

3

30

40

50

60

70

Q-Learning

80

90

Function Composition

Accelerating Reinforcement Learning

4

8

20

Figure 30: Single Rooms

7

10

No. Learning Steps X 1000

87

100

Figure 31: Learning Curves: Robot Navigation, New Environment

Average No. Steps Goal

fiDrummond

speed 4.5. previous experiments initialized function accurate
little refinement necessary. Basic Q-learning, reaching knee, takes
long time remove residual error.

4.4 Robot Arm, New Environment

10

10

10

3

2

0

1

1

20

30

2

40

50

3

60

70

Q-Learning

80

90

Function Composition

Figure 32: Different Obstacle Positions

10

No. Learning Steps X 1000

Figure 33: Learning Curves: Robot Arm, New Environment
88

100

fourth experiment essentially third experiment except robot
arm domain. Here, three, hand crafted, configurations single obstacle goal
fixed position used, shown Figure 32. increase statistical variation
configuration run five times different random seed. curves Figure 33
therefore average across 15 experimental runs.

Average No. Steps Goal

fiAccelerating Reinforcement Learning

top curve Figure 31 Q-learning algorithm, bottom curve function
composition system. knee function composition system's curve occurs
4400 steps. knee basic Q-learning algorithm 68,000 steps giving speed
15.

5. Analysis Results

experiments previous section shown function composition produces
significant speed across two different types related task across two domains.
addition, composed solutions tend accurate little refinement
required. section begins looking possible concerns experimental
methodology might affect measurement speed up. discusses various
properties task solved affect speed achieved using function
composition.

5.1 Possible Concerns Experimental Methodology

speed obtained using function composition suciently large small variations
experimental set unlikely affect overall result. Nevertheless,
number concerns might raised experimental methodology.
be, least partially, addressed section; others subject future work.
first concern might estimated value speed measured.
value represents speed average set learning tasks, rather
average speed tasks. One diculties estimation,
curves single tasks, average distance goal may oscillate
learning progresses, even though general trend downwards. makes judging
position knee curves dicult, estimate speed questionable. Even
experimental runs using configuration, different random seeds, exhibit
considerable variation. instances, speed measured individual curves may
benefit function composition system, others, baseline algorithm. Nevertheless,
probably overall effects cancel out.
second concern might effect speed limit 2000 steps
measuring distance goal. Comparing two averages values limited way
sometimes misleading (Gordon & Segre, 1996). limit primarily affects
baseline algorithm, significant goal moved function
reinitialized. Estimation speed principally concerned comparing position
knees different curves. Here, average distance goal relatively small,
limiting value likely little effect.
third concern might value speed dependent configuration
baseline algorithm. Certainly, experience author way
function initialized, actions selected, impact speed
learning. previous work (Drummond, 1998), function initialized constant
value 0.75, technique termed \optimistic initial values" Sutton Barto (1998).
Tie breaking actions value achieved adding small amount
noise (circa 5 10,5). expected would increase exploration early
learning process speed learning overall. However, using initial value zero
89

fi3

2

0

1

50

Drummond

100

150

200

250

300

strict tie-breaker, randomly selecting amongst actions value, turned
produce significant speed baseline learning algorithm. configuration
used preceding experiments, one experimental run caused serious
problems baseline algorithm.

10

10

10

No. Learning Steps X 1000

Figure 34: Learning Curves Partially Observable Domain

90

upper learning curve Figure 34 baseline algorithm, one run
goal moved robot arm domain. large impact average
learning curve, replaced lower curve, produced repeating experiment
different random seed. slow learning rate arises interaction
partial observability robot arm domain use initial value
zero. Individual cells function approximator straddle obstacles allowing \leakthrough" value one side obstacle other. Starting zero value,
action receives value remain best action time. Continual
update action decrease value, asymptotically approach zero.
actions state updated, always selected greedy
action. occur higher initial values. may domains
degree partial observability, small initial values better zero means
improving exploration small values might necessary.
variations parameters baseline algorithm explored
paper. instance, constant learning rate 0.1 used. Alternatives,
starting higher rate reducing learning progresses might improve
overall speed baseline algorithm. preliminary experiments were, however,

Average No. Steps Goal

fiAccelerating Reinforcement Learning

carried using undiscounted reinforcement learning, discounting strictly unnecessary goal-directed tasks. Room configuration 1 Figure 26, goal
lower right hand corner, used experimental task. discounting, discussed
Section 3.1, turned setting 1. addition, value reaching goal state
set zero cost associated every action. form learning simplifies
function composition, normalization procedures needed compensate value function's exponential form longer required. normalization disabled, snake
successfully partitioned function, critical part process. However,
baseline learner took considerably longer learn function discounted case.
discounting, learner reached average distance goal 72 steps
80,000 learning steps. Without discounting, learner reached average 400 steps
point time average 80 steps 300,000 learning steps.
action-value function initialized zero, appears standard practice
literature. However, experience initialization discounted case suggests
might part problem investigated future work.
baseline Q-learning algorithm used basic sophisticated one
would unquestionably reduce speed experimentally obtained. instance,
form reinforcement learning using eligibility traces (Singh & Sutton, 1996) might
used. experiments goal moved, baseline Dyna-Q+ (Sutton,
1990) specifically designed deal changing worlds would probably
better reference point.
speed obtained, transferring pieces action-value function,
compared alternatives, transferring pieces policy transferring pieces
model. Transferring pieces policy would reduce memory requirements
require rescaling applied pieces action-value function. does, however,
two disadvantages. Firstly, solution directly composed, position
decision boundaries determined. learning would necessary decide
appropriate policy room. Secondly, policy indicates best action.
action-value function orders actions, indicating potentially useful small changes
policy might improve accuracy new task. Transferring pieces
model, would require first learning model consisting probability distribution function
action state. memory requirement considerably larger, unless
states reachable action limited beforehand. Nevertheless, model would need
less modification changing world, goal moved. carries
information might speed learning. action-value function seems good
compromise terms complexity versus information content, would need
empirically validated subject future work.

5.2 Performance Variation Task Configuration
Generally, function composition outperforms baseline learning algorithm amount
dependent complexity learning problem. robot navigation domain
goal moved, amount speed increased rooms fewer
paths goal. speed 60, average speed 40, obtained
configurations five rooms single path goal. Configurations three
91

fiDrummond

rooms least speed up, due relative simplicity
problem.

10

10

10

3

2

0

1

50

6

100

Q-Learning

Function Composition

No. Learning Steps X 1000

6

92

Figure 35: Failure Robot Navigation Moving Goal

6

150

top Figure 35 shows average four learning curves three room
configurations. bottom Figure 35 shows one configurations produced
curves. one easiest tasks (from experimental set)
baseline algorithm, solutions case base lowest room.
isomorphic subgraphs form. Rather composing solution,
system introduces constant value function room. room represents almost
half state space, much additional learning required. top Figure 35 shows,
initially significant speed up. refinement reduces advantage
short baseline algorithm better. later, function composition gains
upper hand converges quickly baseline algorithm towards asymptotic
value.

Average No. Steps Goal

fiAccelerating Reinforcement Learning

robot navigation domain learning new task, amount speed varied
size inner room. primarily due number actions needed
features emerged sucient clarity snake locate them. Function
composition successful inner room small. wall long, feature
takes time develop, refinement Q-learning needed make apparent.
short walls hard identify. likelihood robot colliding
small takes many exploratory actions features emerge clearly.
features may suciently clear snake form partition, yet well
enough defined precisely locate doorways. doorway may appear bit wider
actually is. importantly, may appear displaced true position.
Typically, error composed function small normal reinforcement learning
quickly eliminates it. one experimental runs, configuration 2 Figure 30,
speed reduced factor 2 due doorway incorrectly positioned.
feature representing lower wall completely emerged partition
generated. made doorway appear almost exactly corner.
algorithm, fact, positioned doorway wrong side corner. resulted
significantly reduced speed up. unclear reinforcement learning took
long correct seems, surface least, local error.
investigated future work.

6. Limitations

Limitations come , roughly, two kinds: arising overall approach
arising way implemented. former case, ways address limitations may highly speculative, impossible without abandoning fundamental
ideas behind approach. latter case, reasonable expectation future
work address limitations. following sections deal cases
turn.

6.1 Limitations Approach

explore possible limitations approach, section reviews fundamental
assumptions based.
fundamental assumption features arise reinforcement learning function
qualitatively define shape. features used paper violation
smoothness assumption, neighboring states similar utility values. wall,
preventing transitions neighboring states, typically causes violation.
things, actions significant cost, would similar effect. Smaller,
much varied costs, generate features required approach, offers
little way speed cases. mixture large small costs,
expected system capture features generated former, initialize
function normal reinforcement learning address latter.
smoothness assumption less clear dimensions numeric. neighborhood relation, used here, predefined distance metric continuous space.
nominal, binary mixed domains obvious metric would defined,
although work metrics applications (Osborne & Bridge,
93

fiDrummond

1997). dimensions mixed, feature location might limited continuous
ones. dimensions purely nominal binary, generalization snake may
appropriate. snake is, abstract level, constrained hill climber. whether
idea would usefully generalize way present somewhat speculative.
fundamental assumption features clearly delimit subtasks. domains discussed paper, obstacles walls subdivide state space regions
connected small \doorways". subtask reaching one doorway greatly affected
subsequent subtask. domains may case. doorways
become larger, context sensitivity increases. long composed solution reasonably accurate, reinforcement learning easily correct error although speed
reduced. point however, due large amount context sensitivity,
advantage dividing task subtasks become questionable. would possible
account context dependency graph matching stage, looking
larger units subgraphs. two adjacent subgraphs match new problem, might
used pair, thereby including contextual relationship them. Even
single subgraphs used, context appear, i.e. shape neighboring
subgraphs, could taken account. limit, graph matching whole task might
used. But, argued introduction, would considerably limit transfer
applicable, thus overall effectiveness.
fundamental assumption absolute position features unimportant, shape delimited region matters. increase likelihood
transfer, solutions subtasks subjected variety transformations.
domains, many, all, transformations invalid. actions cannot
rotated ected, many small costs affect different regions state space,
effectiveness transfer reduced. would be, extent, addressed
additional penalties different transformations, would limit opportunities transfer. transformations appropriate, whether
determined automatically domain, subject future research.
fundamental assumption vision processing technique locate
features timely fashion, even high dimensional domains. Learning high
dimensional domains likely slow whatever technique used. Normal reinforcement
learning take time navigate much larger space, slowing emergence
features. Although time taken partition function increase, frequency
partitioning applicable decrease. Thus amortized cost rise
slowly. Further, high dimensional spaces generally problematical, methods
principal components analysis projection pursuit (Nason, 1995) used reduce
dimensionality. may prove practice dimensionality important,
focus feature extraction, much smaller actual dimensionality space.

6.2 Limitations Implementation
assumptions previous section met, expected remaining limitations due present implementation. limitations likely become
apparent system applied domains. Certainly domains may differ
presented paper number ways.
94

fiAccelerating Reinforcement Learning

domain may differ dimensionality space higher two
dimensions tasks investigated paper. implementation snake
updated work higher dimensions. bold lines top Figure 36
one simpler tasks robot navigation domain. task extended
Z-dimension. snake starts sphere expands outwards fills
room. example, polygonal constraint used, everything else
remains same. Figure 37 shows complete partition task.

Figure 36: Adding Z-Dimension

Figure 37: Complete 3D Partition

mathematics behind snake limited three dimensions. seems
nothing principle would prevent processes graph matching,
planning transformation working higher dimensions. Speed main problem.
problem unique approach large body research addressing
issue. instance, although graph matching general NP-complete,
much active research speeding matching average special cases (Gold &
Rangarajan, 1996; Galil, 1986). present, snake represents principal restriction
speed. issue great importance vision processing community. Current
research investigating problem, least two three dimensions. One example
hierarchical methods (Schnabel, 1997; Leroy, Herlin, & Cohen, 1996) find solutions
snake progressively finer finer resolution scales. results research
undoubtedly importance here.
domain may differ value function learned might produce features locatable snake present parameter settings. values parameters
empirically determined, using hand crafted examples robot navigation
robot arm domains. obvious danger parameters might tuned
examples. demonstrate case, configurations experiments
robot navigation domain generated randomly. configurations robot arm
domain tightly constrained, hand crafted examples used experiments. Nevertheless, experiments shown parameters worked successfully
random examples robot navigation domain. parameters work successfully second domain, robot arm. following discussion demonstrates
95

fiDrummond

reasonably effective quite different domain, \car hill".
anticipated using results current research snakes automate selection
many parameters.
\car hill"domain (Moore, 1992), task, simply stated, get car
steep hill, Figure 38. car stationary part way hill, fact anywhere
within dotted line, insucient acceleration make top.
car must reverse hill achieve sucient forward velocity, accelerating
side, accelerating hill. state space, purposes
reinforcement learning, defined two dimensions. position velocity
car, shown Figure 39. goal reach top hill small
positive negative velocity. domain two possible actions: accelerate
forward, accelerate backwards. Unlike previous domains, clear mapping
actions onto state space. state achieved applying action determined
Newton's laws motion. car insucient acceleration make hill
everywhere state space, \wall" effectively introduced, bold line Figure 39.
reach top hill, car must follow trajectory around \wall", dashed
line Figure 39.
Goal

Velocity

+ve

Goal

0



cit

lo


-ve

0

Position

Figure 38: Car Hill

Position

Figure 39: Car State Space

Figure 40 shows reinforcement learning function. exhibits steep gradient
domains. important point note that, unlike domains,
physical object causes gradient. implicit problem itself, yet features
still exist. Figure 41 shows partition produced applying snake \car
hill" domain. main difference previous examples polygonal
constraint used. snake initially comes rest, mercury force
turned snake allowed find minimum energy state.
necessary reduce scaling edges, factor three quarters, achieve
accuracy fit. fit around top left corner second snake, dashed line,
96

fiAccelerating Reinforcement Learning

problems: snake growing slowly downwards is, present,
stopped reached maximum number iterations allowed. One diculty
example clear delimitation upper lower regions
end feature. Future work investigate altering stopping condition
eliminate problem.

Figure 40: Steep Gradient

Figure 41: Regions Extracted

domain may differ shape various regions partition complex dealt present snake. Fitting snake task discussed
previous paragraphs goes way towards mitigating concern. Nevertheless,
randomly generated examples Section 4.1 subject certain constraints. Configurations narrower rooms tried informally, snake reliably locate
features. configurations Section 4 represent limit complexity partition
snake produce present. expected using ideas large body
already published research snakes go long way towards addressing limitation.
complex regions, locating subtleties underlying shape may unnecessary,
even undesirable. aim speed low level learning. long solution
reasonably accurate, speed obtained. sensitive minor variations
shape may severely limit opportunities transfer thus reduce speed overall.
domain may differ changes environment complex
investigated paper. present, system detects goal moved
counting often reward received old goal position. rather
ad hoc approach, account possible changes, paths
becoming blocked short-cuts becoming available. present, learning new task
system restarted required determine present solution longer
applicable. future work, system decide model world longer
correct. decide what, any, relationship existing task
might best exploited. allow complex interaction function
composition system reinforcement learning. instance, learning new task
97

fiDrummond

robot navigation domain used relatively simple situation two rooms. function
composition system initialized low level algorithm detecting suitable features.
future, address complex tasks, many rooms, incremental approach
used. new task learned, system progressively build
solution function composition different features become apparent.
approach handle errors system might make feature extraction. experiments simple room configurations, filtering discussed
Section 2.3 proved sucient prevent problems. complex tasks, likely
false \doorways" detected, simply system explored
region state space. composed function including extra doorway drive
system region. become quickly apparent doorway
exist new function composed.

7. Related Work
strongly related work investigating macro actions reinforcement learning. Precup, Sutton Singh (1997, 1998) propose possible semantics macro actions
within framework normal reinforcement learning. Singh (1992) uses policies, learned
solve low level problems, primitives reinforcement learning higher level. Mahadevan Connell (1992) use reinforcement learning behavior based robot control.
learn solution new task, systems require definition subtask
interrelationships solving compound task. work presented gives
one way macro actions extracted directly system's interaction
environment, without hand-crafted definitions. shows determine
interrelationships macro actions needed solve new task. Thrun's research (1994) identify macro actions, finding commonalities multiple tasks.
unlike research presented here, mapping actions new tasks proposed.
Hauskrecht et al. (1998) discuss various methods generating macro actions. Parr (1998)
develops algorithms control caching policies used multiple tasks.
cases, need given partitioning state space.
automatic generation partition focus much work
presented paper. may well approach generating partitions
determining interrelationships partitions related tasks prove useful
work.
Another group closely connected work various forms instance based case
based learning used conjunction reinforcement learning.
used address number issues: (1) economical representation state
space, (2) prioritizing states updating (3) dealing hidden state. first issue
addressed Peng (1995) Tadepalli Ok (1996) use learned instances
combined linear regression set neighboring points. Sheppard Salzberg
(1997) use learned instances, carefully selected genetic algorithm.
second issue addressed Moore Atkeson (1993) keep queue \interesting"
instances, predecessors states learning produces large change values.
updated frequently improve learning rate. third issue addressed
McCallum (1995b) uses trees expand state representation include prior
98

fiAccelerating Reinforcement Learning

states, removing ambiguity due hidden states. work, McCallum (1995a) uses
single representation address hidden state problem general problem
representing large state space using case base state sequences associated
various trajectories. Unlike research, work presented case
example value function learning. Instead, result complete
learning episode, method complementary approaches.
work related case based planning (Hammond, 1990; Veloso & Carbonell,
1993), firstly general connection reinforcement learning planning.
analogous ways. small change world,
goal moved, composite plan modified using sub-plans extracted
composite plans.
Last, least, connection object recognition vision research (Suetens
et al., 1992; Chin & Dyer, 1986). work presented here, many methods {
final application { come field. features reinforcement learning
function akin edges image. located finding zero crossing point
Laplacian introduced Marr (1982). work presented here, proposed
features largely dictate form function. Mallat Zhong (1992)
shown function accurately reconstructed record steep slopes.

8. Conclusions

paper described system transfers results prior learning significantly
speed reinforcement learning related tasks. Vision processing techniques utilized
extract features learned function. features used index case
base control function composition produce close approximation solution
new task. experiments demonstrated function composition often produces
order magnitude increase learning rate compared basic reinforcement
learning algorithm.

Acknowledgements
author would thank Rob Holte many useful discussions help preparing
paper. work part supported scholarships Natural Sciences
Engineering Research Council Canada Ontario Government.

Appendix A. Spline Representations

appendix presents underlying mathematics associated spline representations snake. meant introduction subject. Rather
added completeness discuss certain important aspects system addressed
elsewhere paper. Knowledge aspects necessary understand basic
principles approach discussed paper, would necessary one wanted
duplicate system. detailed explanation given Drummond (1999).
specific papers address ideas much greater detail are: splines (Terzopoulos,
1986) snakes (Cohen & Cohen, 1993; Leymarie & Levine, 1993).
99

fiDrummond

Splines piecewise polynomials degree polynomial determines
continuity smoothness function approximation. Additional smoothing constraints
introduced penalty terms reduce size various differentials. One way
view spline fitting form energy functional Equation 6.

Espline(f^) =

Z

R





Efit (f^) + Esmooth (f^) ds

(6)

Here, energy associated goodness fit, measure close
approximating function input function. typically least squares
distance functions. energy associated smoothness
function. Two commonly used smoothness controls produce membrane thin
plate splines restricting first second differentials function respectively.
fit spline function, total energy must minimized. necessary condition
Euler-Lagrange differential equation Equation 7. !t controls
tension spline (the resistance stretching) !s stiffness (the resistance
bending). Often error function based individual data points left hand
side Equation 7 would include delta functions.
^

2^

@ (! (s) @ f (s) ) + @ (! (s) @ f (s) ) = f (s) , f^(s)
, @s


@s
@s2 @s2

(7)

work, splines used number purposes. fitting
snake, measures first second differential needed. two dimensional quadratic
spline fitted discrete representation maximum Q-values. !t 0.2 used
(!s zero) limit overshoot (Drummond, 1996) prevent false edges. Values
identical spline except using !t 2.0 squared divided differential
values. normalizes differentials, size edges dependent
occur function. type spline used produce bowls associated
rooms discussed Section 3.2.1. !t 1.0 !s 0.5 giving roughly
Gaussian smoothing. values used produce function weighted. Values close
one given weights 200, lower values weight 1. prevents sides
bowls collapsing smoothing.
one dimensional cubic spline used locating doorways. found
steepest descent value differential along body snake. differential
contains many local minima associated doorways. arise either
inherent noise process errors fit snake. aim remove
ones associated doorways smoothing thresholding. achieved
first sampling gradient points along snake. values normalized lie
zero one. spline !t 0.15 (!s 0.0). weighted least mean
squares fit used. weighting function inverse square values, preventing
spline overwhelmed large values. Starting points steepest descent
changes sign coecients gradient spline. initial step size
set slightly larger knot spacing decreased time. local
minimum found value exceeds threshold (of 0.5), rejected.
represent snake, model spline must changed somewhat. snake
one dimensional cubic spline. energy minimum sought
100

fiAccelerating Reinforcement Learning

differential Qmax function, subject constraints. dynamics
snake defined Euler-Langrange equation shown Equation 8.
2 f^!!
2 f^!
2 f^
^ @ @
@
f
@
@
@
@
@t2 + @t + @t @s2 !c(s) @s2 + @s2 !tp (s) @s2 = F (f^)

(8)

!c 512 minimizes changes snake's shape grows, penalizing
difference second differential previous time step scaled ratio
lengths. !s 8.0 initial stiffness snake. reduced proportionately
snake's length give spline degrees freedom. 96 96 control
momentum drag snake respectively. Cohen Cohen (1993),
factor added energy associated differential direction normal
body snake, shown Equation 9. instead constant,
variable used produce mercury model discussed Section 3.2.1.
2
F (f^) = (f^),!
n (s) + r(, fifirQmax(f^)fifi ),!
n (s)




(9)

energy minimization process carried iteratively interleaving steps x
directions. differential r jQmax j2 x direction given Equation 10,
similar equation used direction.
2
2 Qmax
@Qmax )( @ 2 Qmax )
)
+
(
, @ jrQ@xmax j = ,2 ( @Q@xmax )( @ @x
2
@y
@x@y
"

#

(10)

snake grows forces mercury model reaches approximately
stable position, subject small oscillations. converted polygon
n = 0 : : : 3).
finding corners (where normal passes (2n+1)
4
coecient !1 set zero everywhere. coecient !2 set zero corners
15 them. produces polygon exible vertices.
detect features early possible learning process, discussed Section
2.4, height gradient scaled according signal noise ratio. noise
arises variations low level learning process stochastic nature task.
size features noise grow time somewhat normalized
scaling process. idea collect uniformly sampled values function shown
Equation 10 x directions find median absolute values.
median strongly affected extreme values thus largely ignores size
features, measuring noise regions between.

References
Chin, C. H., & Dyer, C. R. (1986). Model-based recognition robot vision. Computing
Surveys, 18 (1), 67{108.
Christiansen, A. D. (1992). Learning predict uncertain continuous tasks. Proceedings
Ninth International Workshop Machine Learning, pp. 72{81.
101

fiDrummond

Cohen, L. D., & Cohen, I. (1993). Finite element methods active contour models
balloons 2-d 3-d images. IEEE Transactions Pattern Analysis Machine
Intelligence, 15 (11), 1131{1147.
Dijkstra, E. W. (1959). note two problems connexion graphs. Numerische
Mathematik, 1, 269{271.
Drummond, C. (1996). Preventing overshoot splines application reinforcement
learning. Computer science technical report TR-96-05, School Information Technology Engineering, University Ottawa, Ottawa, Ontario, Canada.
Drummond, C. (1997). Using case-base surfaces speed-up reinforcement learning.
Proceedings Second International Conference Case-Based Reasoning, Vol.
1266 LNAI, pp. 435{444.
Drummond, C. (1998). Composing functions speed reinforcement learning changing world. Proceedings Tenth European Conference Machine Learning,
Vol. 1398 LNAI, pp. 370{381.
Drummond, C. (1999). Symbol's Role Learning Low Level Control Functions. Ph.D.
thesis, School Information Technology Engineering, University Ottawa, Ottawa, Ontario, Canada.
Galil, Z. (1986). Ecient algorithms finding maximum matching graphs. ACM
Computing Surveys, 18 (1), 23{38.
Gold, S., & Rangarajan, A. (1996). graduated assignment algorithm graph matching.
IEEE Transactions Pattern Analysis Machine Intelligence, 18 (4), 377{388.
Gordon, G. J. (1995). Stable function approximation dynamic programming. Proceedings Twelfth International Conference Machine Learning, pp. 261{268.
Gordon, G. J., & Segre, A. M. (1996). Nonparametric statistical methods experimental evaluations speedup learning. Proceedings Thirteenth International
Conference Machine Learning, pp. 200{206.
Hammond, K. J. (1990). Case-based planning: framework planning experience.
Journal Cognitive Science, 14 (3), 385{443.
Hauskrecht, M., Meuleau, N., Boutilier, C., Kaelbling, L. P., & Dean, T. (1998). Hierarchical
solution Markov decision processes using macro-actions. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, pp. 220{229.
Kass, M., Witkin, A., & Terzopoulus, D. (1987). Snakes: Active contour models. International Journal Computer Vision, 1, 321{331.
Leroy, B., Herlin, I. L., & Cohen, L. D. (1996). Multi-resolution algorithms active
contour models. Proceedings Twelfth International Conference Analysis
Optimization Systems, pp. 58{65.
102

fiAccelerating Reinforcement Learning

Leymarie, F., & Levine, M. D. (1993). Tracking deformable objects plane using
active contour model. IEEE Transactions Pattern Analysis Machine
Intelligence, 15 (6), 617{634.
MacDonald, A. (1992). Graphs: Notes symetries, imbeddings, decompositions. Tech.
rep. Electrical Engineering Department TR-92-10-AJM, Brunel University, Uxbridge,
Middlesex, United Kingdom.
Mahadevan, S., & Connell, J. (1992). Automatic programming behavior-based robots
using reinforcement learning. Artificial Intelligence, 55, 311{365.
Mallat, S., & Zhong, S. (1992). Characterization signals multiscale edges. IEEE
Transactions Pattern Analysis Machine Intelligence, 14 (7), 710{732.
Marr, D. (1982). Vision: Computational Investigation Human Representation
Processing Visual Information. W.H. Freeman.
McCallum, R. A. (1995a). Instance-based state identification reinforcement learning.
Advances Neural Information Processing Systems 7, pp. 377{384.
McCallum, R. A. (1995b). Instance-based utile distinctions reinforcement learning
hidden state. Proceedings Twelfth International Conference Machine
Learning, pp. 387{395.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning
less data less real time. Machine Learning, 13, 103{130.
Moore, A. W. (1992). Variable resolution dynamic programming: Eciently learning action
maps multivariate real-valued state spaces. Proceedings Ninth International
Workshop Machine Learning.
Nason, G. (1995). Three-dimensional projection pursuit. Tech. rep., Department Mathematics, University Bristol, Bristol, United Kingdom.
Osborne, H., & Bridge, D. (1997). Similarity metrics: formal unification cardinal
non-cardinal similarity measures. Proceedings Second International
Conference Case-Based Reasoning, Vol. 1266 LNAI, pp. 235{244.
Parr, R. (1998). Flexible decomposition algorithms weakly coupled Markov decision
problems. Proceedings Fourteenth Conference Uncertainty Artificial
Intelligence, pp. 422{430.
Peng, J. (1995). Ecient memory-based dynamic programming. Proceedings
Twelfth International Conference Machine Learning, pp. 438{439.
Precup, D., Sutton, R. S., & Singh, S. P. (1997). Planning closed-loop macro actions.
Working notes 1997 AAAI Fall Symposium Model-directed Autonomous
Systems, pp. 70{76.
103

fiDrummond

Precup, D., Sutton, R. S., & Singh, S. P. (1998). Theoretical results reinforcement
learning temporally abstract options. Proceedings Tenth European
Conference Machine Learning, Vol. 1398 LNAI, pp. 382{393.
Schnabel, J. A. (1997). Multi-Scale Active Shape Description Medical Imaging. Ph.D.
thesis, University London, London, United Kingdom.
Sheppard, J. W., & Salzberg, S. L. (1997). teaching strategy memory-based control.
Artificial Intelligence Review: Special Issue Lazy Learning, 11, 343{370.
Singh, S. P., & Sutton, R. S. (1996). Reinforcement learning replacing eligibility traces.
Machine Learning, 22, 123{158.
Singh, S. P. (1992). Reinforcement learning hierarchy abstract models. Proceedings Tenth National Conference Artificial Intelligence, pp. 202{207.
Suetens, P., Fua, P., & Hanson, A. (1992). Computational strategies object recognition.
Computing Surveys, 24 (1), 5{61.
Sutton, R. S. (1990). Integrated architectures learning, planning, reacting based
approximating dynamic programming. Proceedings Seventh International
Conference Machine Learning, pp. 216{224.
Sutton, R. S. (1996). Generalization reinforcement learning: Successful examples using
sparse coarse coding. Advances Neural Information Processing Systems 8, pp.
1038{1044.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press.
Tadepalli, P., & Ok, D. (1996). Scaling average reward reinforcement learning approximating domain models value function. Proceedings Thirteenth
International Conference Machine Learning, pp. 471{479.
Tanimoto, S. L. (1990). Elements Artficial Intelligence. W.H. Freeman.
Terzopoulos, D. (1986). Regularization inverse visual problems involving discontinuities.
IEEE Transactions Pattern Analysis Machine Intelligence, 8 (4), 413{423.
Thrun, S., & Schwartz, A. (1994). Finding structure reinforcement learning. Advances
Neural Information Processing Systems 7, pp. 385{392.
Veloso, M. M., & Carbonell, J. G. (1993). Derivational analogy prodigy: Automating
case acquisition, storage utilization. Machine Learning, 10 (3), 249{278.
Watkins, C. J., & Dayan, P. (1992). Technical note: Q-learning. Machine Learning, 8 (3-4),
279{292.

104


