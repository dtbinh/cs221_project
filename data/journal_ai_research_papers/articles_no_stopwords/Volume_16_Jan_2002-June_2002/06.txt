Journal Artificial Intelligence Research 16 (2002) 259-292

Submitted 9/01; published 4/02

Efficient Reinforcement Learning Using
Recursive Least-Squares Methods
Xin Xu
Han-gen
Dewen Hu

XUXIN_MAIL@263.NET
HEHANGEN@CS.HN.CN
DWHU@NUDT.EDU.CN

Department Automatic Control
National University Defense Technology
ChangSha, Hunan, 410073, P.R.China

Abstract
recursive least-squares (RLS) algorithm one well-known algorithms used
adaptive filtering, system identification adaptive control. popularity mainly due
fast convergence speed, considered optimal practice. paper, RLS methods
used solve reinforcement learning problems, two new reinforcement learning
algorithms using linear value function approximators proposed analyzed. two
algorithms called RLS-TD( ) Fast-AHC (Fast Adaptive Heuristic Critic), respectively.
RLS-TD( ) viewed extension RLS-TD(0) =0 general 0 1,
multi-step temporal-difference (TD) learning algorithm using RLS methods. convergence
probability one limit convergence RLS-TD( ) proved ergodic Markov
chains. Compared existing LS-TD( ) algorithm, RLS-TD( ) advantages
computation suitable online learning. effectiveness RLS-TD( )
analyzed verified learning prediction experiments Markov chains wide range
parameter settings.
Fast-AHC algorithm derived applying proposed RLS-TD( ) algorithm
critic network adaptive heuristic critic method. Unlike conventional AHC algorithm,
Fast-AHC makes use RLS methods improve learning-prediction efficiency critic.
Learning control experiments cart-pole balancing acrobot swing-up problems
conducted compare data efficiency Fast-AHC conventional AHC.
experimental results, shown data efficiency learning control improved
using RLS methods learning-prediction process critic. performance
Fast-AHC compared AHC method using LS-TD( ). Furthermore,
demonstrated experiments different initial values variance matrix RLS-TD( )
required get better performance learning prediction learning control.
experimental results analyzed based existing theoretical work transient
phase forgetting factor RLS methods.

1. Introduction
recent years, reinforcement learning (RL) active research area machine
learning control engineering, operations research robotics (Kaelbling et al.,1996;
Bertsekas, et al.,1996; Sutton Barto,1998; Lin,1992). computational approach
2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiXU, HE, & HU

understand automate goal-directed learning decision-making, without relying
exemplary supervision complete models environment. RL, agent placed
initial unknown environment receives evaluative feedback environment.
feedback called reward reinforcement signal. ultimate goal RL learn strategy
selecting actions expected sum discounted rewards maximized.
Since lots problems real world sequential decision processes delayed
evaluative feedback, research RL focused theory algorithms learning
solve optimal control problem Markov decision processes (MDPs) provide
elegant mathematical model sequential decision-making. operations research, many results
presented solve optimal control problem MDPs model information.
However, reinforcement learning, model information assumed unknown,
different methods studied operations research dynamic programming.
dynamic programming, two elemental processes, policy evaluation process policy improvement process, respectively. RL, two similar processes.
One called learning prediction called learning control. goal learning
control estimate optimal policy optimal value function MDP without knowing
model. Learning prediction aims solve policy evaluation problem stationary-policy
MDP without prior model regarded sub-problem learning control.
Furthermore, RL, learning prediction different supervised learning. pointed
Sutton (1988), prediction problems supervised learning single-step prediction
problems reinforcement learning multi-step prediction problems. solve
multi-step prediction problems, learning system must predict outcomes depend future
sequence decisions. Therefore, theory algorithms multi-step learning prediction
become important topic RL much research work done literature (Sutton,
1988; Tsitsiklis Roy, 1997).
Among proposed multi-step learning prediction methods, temporal-difference (TD)
learning (Sutton, 1988) one popular methods. studied applied early
research machine learning, including celebrated checkers-playing program (Minsky, 1954;
Samuel, 1959). 1988, Sutton presented first formal description temporal- difference
methods TD( ) algorithm (Sutton,1988). Convergence results established tabular
temporal-difference learning algorithms cardinality tunable parameters
state space (Sutton, 1988; Watkins,et al.,1992; Dayan,et al., 1994; Jaakkola, et
al.,1994). Since many real-world applications large infinite state space, value function
approximation (VFA) methods need used cases. combined nonlinear
value function approximators, TD( ) guarantee convergence several results
regarding divergence reported literature (Tsitsiklis Roy,1997). TD( )
linear function approximators, called linear TD( ) algorithms, several convergence
proofs presented. Dayan (1992) showed convergence mean linear TD( )
algorithms arbitrary 0 1 . Tsitsiklis Roy (1994) proved convergence
special class TD learning algorithms, known TD(0), Tsitsiklis Roy (1997),
extended early results general linear TD( ) case proved convergence
probability one.
linear TD( ) algorithms rules updating parameters similar
gradient-descent methods. However, gradient-learning methods, step-size schedule must
carefully designed guarantee convergence obtain good performance.
260

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

addition, inefficient use data slows convergence algorithms. Based
theory linear least-squares estimation, Brartke Barto (1996) proposed two
temporal-difference algorithms called Least-Squares TD(0) algorithm (LS-TD(0))
Recursive Least- Squares TD(0) algorithm (RLS-TD(0)), respectively. LS-TD(0) RLS-TD(0)
efficient statistical sense conventional linear TD( ) algorithms
eliminate design step-size schedules. Furthermore, convergence LS-TD(0)
RLS-TD(0) provided theory. two algorithms viewed
least-squares versions conventional linear TD(0) methods. However, shown
literature, TD learning algorithms TD( ) 0< <1 update predictions based
estimates multiple steps efficient Monte-Carlo methods well TD(0).
employing mechanism eligibility traces, determined , TD( ) algorithms
0< <1 extract information historical data. Recently, class linear
temporal-difference learning algorithms called LS-TD( ) proposed Boyan
(1999,2002), least-squares methods employed compute value-function estimation
TD( ) 0 1. Although LS-TD( ) efficient TD( ), requires much
computation per time-step online updates needed number state features
becomes large.
system identification, adaptive filtering adaptive control, recursive least-squares
(RLS) (Young,1984; Ljung, 1983; Ljung,1977) method, commonly used reduce
computational burden least-squares methods, suitable online estimation control.
Although RLS-TD(0) makes use RLS methods, employ mechanism
eligibility traces. Based work Tsitsiklis Roy (1994, 1997), Boyan (1999,2002)
motivated ideas, new class temporal-difference learning methods, called
RLS-TD( ) algorithm, proposed analyzed formally paper. RLS-TD( ) superior
conventional linear TD( ) algorithms makes use RLS methods improve
learning efficiency statistical point view eliminates step-size schedules.
RLS-TD( ) mechanism eligibility traces viewed extension
RLS-TD(0) =0 general 0 1. convergence probability 1 RLS-TD( )
proved ergodic Markov chains limit convergence analyzed. learning
prediction experiments Markov chains, performance RLS-TD( ) TD( ) well
LS-TD( ) compared, wide range parameter settings tested. addition, influence initialization parameters RLS-TD( ) discussed. observed
rate convergence influenced initialization variance matrix,
phenomenon investigated theoretically adaptive filtering (Moustakides, 1997; Haykin, 1996).
analyzed following sections, two benefits extension
RLS-TD(0) RLS-TD( ). One value (0 1) still affect performance
RLS-based temporal-difference algorithms. Although RLS-TD( ), rate
convergence mainly influenced initialization variance matrix, bound
approximation error dominantly determined parameter . smallest error bound
obtained =1 worst bound obtained =0. bounds suggest
value selected appropriately obtain best approximation error. second
benefit RLS-TD( ) suitable online learning LS-TD( ) since
computation per time-step reduced O(K3) O(K2), K number state
features.
Adaptive-Heuristic-Critic (AHC) learning algorithm class reinforcement learning
261

fiXU, HE, & HU

methods actor-critic architecture used solve full reinforcement learning
learning control problems. applying RLS-TD( ) algorithm critic, Fast-AHC
algorithm proposed paper. Using RLS methods critic, performance learning
prediction critic improved learning control problems solved
efficiently. Simulation experiments learning control cart-pole balancing problem
swing-up acrobot conducted verify effectiveness Fast-AHC method.
comparing conventional AHC methods use TD( ) critic, demonstrated
Fast-AHC obtain higher data efficiency conventional AHC methods. Experiments
performance comparisons AHC methods using LS-TD( ) Fast-AHC
conducted. learning control experiments, illustrated initializing constant
variance matrix RLS-TD( ) influences performance Fast-AHC different values
constant selected get better performance different problems.
results analyzed based theoretical work transient phase RLS methods.
paper organized follows. Section 2, introduction previous linear
temporal-difference algorithms presented. Section 3, RLS-TD( ) algorithm proposed
convergence (with probability one) proved. Section 4, simulation example
value-function prediction absorbing Markov chains presented illustrate effectiveness
RLS-TD( ) algorithm, different parameter settings different algorithms
including LS-TD( ) studied. Section 5, Fast-AHC method proposed
simulation experiments learning control cart-pole balancing acrobot
conducted compare Fast-AHC conventional AHC method well
LS-TD( )-based AHC method. simulation results presented analyzed detail.
last section contains concluding remarks directions future work.

2. Previous Work Linear Temporal-Difference Algorithms
section, brief discussion conventional linear TD( ) algorithm RLS-TD(0)
well LS-TD( ) algorithm given. First all, mathematical notations
presented follows.
Consider Markov chain whose states lie finite countable infinite space S. states
Markov chain indexed {1,2,,n}, n possibly infinite. Although
algorithms results paper applicable Markov chains general state space,
discussion paper restricted within cases countable state space
simplify notation. extension Markov chains general state space requires
translation matrix notation operator notation.
Let trajectory generated Markov chain denoted {xt |t=0,1,2,; xt S}.The
dynamics Markov chain described transition probability matrix P whose (i,j)-th
entry, denoted pij, transition probability xt+1=j given xt=i. state transition
xt xt+1, scalar reward rt defined. value function state defined follows:


V (i ) = E{ rt x 0 = i}

(1)

=0

0< 1 discount factor.
TD( ) algorithm, two basic mechanisms temporal difference
262

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

eligibility trace, respectively. Temporal differences defined differences
two successive estimations following form.
~
~
= rt + Vt ( xt +1 ) Vt ( xt )
(2)
~
xt+1 successive state xt, V ( x) denotes estimate value function V(x) rt
reward received state transition xt xt+1.
Eligibility trace viewed algebraic trick improve learning efficiency
without recording data multi-step prediction process. trick based idea
using truncated return Markov chain. temporal-difference learning eligibility
traces, n-step truncated return defined
~
Rtn = rt + rt +1 + ... + n 1 rt + n 1 + nVt ( + n )
(3)
absorbing Markov chain whose length T, weighted average truncated returns

1

Rt = (1 )

n1 Rtn + 1 RT

(4)

n =1

0 1 decaying factor RT= rt + rt +1 + ... + rT Monte-Carlo return
terminal state. step TD( ) algorithm, update rule value function
estimation determined weighted average truncated returns defined above.
corresponding update equation
~
~
Vt ( ) = ( Rt Vt ( ))
(5)
learning factor.
update equation (5) used whole trajectory Markov chain
observed. realize incremental online learning, eligibility traces defined state
follows:

z ( ) + 1,
z +1 ( ) =
z ( ),

=


online TD( ) update rule eligibility traces
~
~
Vt +1 ( si ) = Vt ( si ) + z +1 ( si )

(6)

(7)

temporal difference time step t, defined (2) z0(s)=0 s.
Since state space Markov chain usually large infinite practice, function
approximators neural networks commonly used approximate value function.
TD( ) algorithms linear function approximators popular well-studied ones.
Consider general linear function approximator fixed basis function vector

( x ) = (1 ( x ), 2 ( x ),..., n ( x ))T
estimated value function denoted

~
Vt ( x) = ( x)Wt
263

(8)

fiXU, HE, & HU

Wt =(w1, w2,,wn)T weight vector.
corresponding incremental weight update rule

r
Wt +1 = Wt + (rt + ( xt +1 )Wt ( xt )Wt ) z +1
r
eligibility trace vector z ( ) = ( z1t ( ), z 2t ( ),..., z nt ( )) defined
r
r
z +1 = z + ( xt )

(9)

(10)

Tsitsiklis Roy (1997), linear TD( ) algorithm proved converge
probability 1 certain assumptions limit convergence W* derived,
satisfies following equation.
E 0 [ A( X )]W * E 0 [b( X )] = 0

(11)

Xt =(xt,xt+1,zt+1) (t=1,2,) form Markov process, E0[] stands expectation
respect unique invariant distribution {Xt}, A(Xt) b(Xt) defined
r
A( X ) = z ( ( xt ) ( xt +1 ))
(12)

r
b( X ) = z rt

(13)

improve efficiency linear TD() algorithms, least-squares methods used
linear TD(0) algorithm, LS-TD(0) RLS-TD(0) algorithms suggested (Brartke
Barto, 1996). LS-TD(0) RLS-TD(0), following quadratic objective function defined.
1

J = [rt ( tT tT+1 )W ] 2

(14)

=1

Thus, aim LS-TD(0) RLS-TD(0) obtain least-squares estimation real
value function satisfies following Bellman equation.
V ( xt ) = E[rt ( xt , xt +1 ) + V ( xt +1 )]

(15)

employing instrumental variables approach (Soderstrom Stoica, 1983),
least-squares solution (14) given




=1

=1

W LS TD ( 0) = ( ( ( +1 ) )) 1 ( rt )

(16)

instrumental variable chosen uncorrelated input output noises.
RLS-TD(0), recursive least-squares methods used decrease computational burden LS-TD(0). update rules RLS-TD(0) follows:
Wt +1 = Wt + Pt (rt ( +1 ) Wt ) /(1 + ( +1 ) Pt )

(17)

Pt +1 = Pt Pt ( +1 ) Pt /(1 + ( +1 ) Pt )

(18)

convergence (with probability one) LS-TD(0) RLS-TD(0) proved periodic
absorbing Markov chains certain assumptions (Brartke Barto,1996).
264

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Boyan (1999,2002), LS-TD( ) proposed solving (11) directly model-based
property LS-TD( ) analyzed. However, LS-TD( ), computation per time-step
O(K3), i.e., cubic order state feature number. Therefore computation required
LS-TD() increases fast K increases, undesirable online learning.
next section, propose RLS-TD( ) algorithm making use recursive
least-squares methods computational burden LS-TD( ) reduced O(K3)
O(K2). give rigorous mathematical analysis algorithm, convergence
(with probability 1) RLS-TD( ) proved.

3. RLS-TD( ) Algorithm
Markov chain discussed above, linear function approximators used,
least-squares estimation problem (11) following objective function.
J=





=1

=1

A( X )W b( X )

2

(19)

A( X ) R nn , b( X ) R n defined (12) (13), respectively, Euclid norm
n number basis functions.
LS-TD( ), least-squares estimate weight vector W computed according
following equation.




=1

=1

W LS TD ( ) = AT1bT = ( A( X )) 1 ( b( X ))

(20)



r
= ( A( X )) = z ( ( xt ) ( xt +1 ))

(21)



r
bT = b( X ) = z rt

(22)



=0

=0

=0

=0

well known system identification, adaptive filtering control, RLS methods
commonly used solve computational memory problems least-squares algorithms.
sequel, present RLS-TD( ) algorithm based idea. First, matrix inverse lemma given follows:
Lemma 1(Ljung, et al.,1983). R nn , B R n1 , C R 1n invertible,

( + BC ) 1 = 1 1 B ( + CA 1 B ) 1 CA 1

(23)

Pt = At1

(24)

Let

265

fiXU, HE, & HU

P0 =

(25)

r
K +1 = Pt +1 z

(26)

positive number identity matrix.
weight update rules RLS-TD( ) given
r
r
K +1 = Pt z /( + ( ( xt ) ( xt +1 )) Pt z )
Wt +1 = Wt + K +1 (rt ( ( xt ) ( xt +1 ))Wt )

Pt +1 =

1



r
r
[ Pt Pt z [ + ( ( xt ) ( xt +1 )) Pt z )] 1 ( ( xt ) ( xt +1 )) Pt ]

(27)
(28)

(29)

standard RLS-TD() algorithm, =1; general forgetting factor RLS-TD()
case, 0<1.
forgetting factor (0<1) usually used adaptive filtering improve
performance RLS methods non-stationary environments. forgetting factor RLS-TD( )
algorithm 0<1 derived using similar techniques Haykin (1996). detailed
derivation RLS-TD() referred Appendix A.
follows, descriptions RLS-TD( ) two different kinds Markov chains
given. First, complete description RLS-TD( ) ergodic Markov chains presented below.

Algorithm 1 RLS-TD( ) ergodic Markov chains

1: Given:
termination criterion algorithm.
set basis functions { j (i ) } (j=1,2,,n) state i, n
number basis functions.
2: Initialize:
(2.1) Let t=0.
(2.2) Initialize weight vector Wt, variance matrix Pt , initial state x0.
r
(2.3) Set eligibility traces vector z 0 =0.
3: Loop:
(3.1) current state xt, observe state transition xt xt+1
reward r(xt ,xt+1).
(3.2) Apply equations (27)-(29) update weight vector.
(3.3) t=t+1.
termination criterion satisfied.

RLS-TD( ) algorithm absorbing Markov chains little different
algorithm coping state features absorbing states. Following description
266

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

RLS-TD( ) absorbing Markov chains.

Algorithm 2 RLS-TD( ) absorbing Markov chains

1: Given:
termination criterion algorithm.
set basis functions { j (i ) } (j=1,2,,n) state i, n
number basis functions.
2: Initialize:
(2.1) Let t=0.
(2.2) Initialize weight vector Wt, variance matrix Pt , initial state x0.
r
(2.3) Set eligibility traces vector z 0 =0.
3: Loop:
(3.1) current state xt,
xt absorbing state, set (xt+1)=0, r(xt)=rT, rT terminal
reward.
Otherwise, observe state transition xt xt+1 reward
r(xt ,xt+1).
(3.2) Apply equations (27)-(29) update weight vector.
(3.3) xt absorbing state, re-initialize process setting xt+1 initial
r
state set eligibility traces z zero vector.
(3.4) t=t+1.
termination criterion satisfied.

RLS-TD( ) algorithm absorbing Markov chains, weight updates
absorbing states treated differently process re-initialized absorbing states
transform absorbing Markov chain equivalent ergodic Markov chain.
following convergence analysis, focus ergodic Markov chains.
similar assumptions Tsitsiklis Roy (1997), prove proposed
RLS-TD( ) algorithm converges probability one.
Assumption 1. Markov chain {xt}, whose transition probability matrix P, ergodic,
unique distribution satisfies



P =
(30)
(i)>0 finite infinite vector, depending cardinality S.


Assumption 2. Transition rewards r(xt,xt+1) satisfy

E 0 [r 2 ( xt , xt +1 )] <

(31)

E0[ ] expectation respect distribution .
Assumption 3. matrix = [1 , 2 ,..., n ] R N n full column rank, is, basis
267

fiXU, HE, & HU

functions (i=1,2,,n) linearly independent.
Assumption 4. every (i=1,2,,n), basis function satisfies
2

E 0 [ ( xt )] <

(32)

1
A( X )] non-singular T>0.
=1
Assumptions 14 almost linear TD() algorithms discussed
Tsitsiklis Roy (1997) except Assumption 1, ergodic Markov chains considered.
Assumption 5 specially needed convergence RLS-TD() algorithm.
Based assumptions, convergence theorem RLS-TD() given
follows:
Assumption 5. matrix [ P01 +

Theorem 1. Markov chain satisfies Assumptions 15, asymptotic estimate found
RLS-TD( ) converges, probability 1, W* determined (11).

proof Theorem 1, please refer Appendix B. condition specified
Assumption 5 satisfied setting P0= appropriately.
According Theorem 1, RLS-TD( ) converges solution conventional linear
TD( ) algorithms do, satisfies (11). limit convergence characterized
following theorem.
Theorem 2 (Tsitsiklis Roy ,1997) Let W* weight vector determined (11) V*
true value function Markov chain, Assumption 14, following relation
holds.
1
W * V *
(33)
V * V *


1



X



=

X DX , = ( ) 1 .

explanations notations Theorem 2, please refer Appendix B.
discussed Tsitsiklis Roy (1997), theorem shows distance
limiting function W* true value function V* bounded smallest bound
approximation error obtained =1. every <1, bound actually deteriorates
decreases. worst bound obtained =0. Although bound, strongly
suggests higher values likely produce accurate approximations V*.
Compared LS-TD(), additional parameter RLS-TD(), value
initial variance matrix P0. pointed Haykin (1996,pp.570), exact value
initializing constant insignificant effect data length large enough.
means limit, final solutions obtained LS RLS almost same.
influence transient phase, positive constant becomes large enough goes
infinity, transient behavior RLS almost LS methods (Ljung,
1983). initialized relatively small value, transient phases RLS LS
different. practice, observed variable performance RLS
function initialization (Moustakides, 1997). cases, RLS exhibit
significantly faster convergence initialized relatively small positive definite matrix
initialized large one (Haykin,1996; Moustakides, 1997; Hubing Alexander,
268

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

1989). first effort toward direction statistical analysis RLS soft exact
initialization limits case number iterations less size
estimation vector (Hubing Alexander, 1989). Moustakides (1997) provided theoretical
analysis relation algorithmic performance RLS initialization .
using settling time performance measure, Moustakides proved well-known
rule initialization relatively small matrix preferable cases high medium
signal-to-noise ratio (SNR), whereas low SNR, relatively large matrix must selected
achieving best results. following learning prediction experiments RLS-TD(), well
learning control simulation Fast-AHC, observed value initializing
constant plays important role convergence performance, theoretical
analyses provide clue explain experimental results.

4. Learning Prediction Experiments Markov Chains
section, illustrative example given show effectiveness proposed
RLS-TD() algorithm. Furthermore, algorithmic performance influence
initializing constant studied.
example finite-state absorbing Markov chain called Hop-World problem (Boyan,
1999). shown Figure 1, Hop-World problem 13-state Markov chain
absorbing state.

Figure 1: Hop-World Problem
Figure 1, state 12 initial state trajectory state 0 absorbing state.
non-absorbing state two possible state transitions transition probability 0.5.
state transition reward 3 except transition state 1 state 0 reward 2.
Thus, true value function state (0i12) 2i.
apply linear temporal-difference algorithms value function prediction problem, set
four-element state features basis functions chosen, shown Figure 1. state
features states 12,8,4 0 are, respectively, [1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]
state features states obtained linearly interpolating these.
simulation, RLS-TD( ) algorithm well LS-TD() conventional linear
TD( ) algorithms used solve value function prediction problem without
knowing model Markov chain. experiments, trial defined period
initial state 12 terminal state 0. performance algorithms evaluated
averaged root mean squared (RMS) error value-function predictions 13 states.
parameter setting, performance averaged 20 independent Monte-Carlo runs.
Figure 2 shows learning curves RLS-TD() conventional linear TD() algorithms
three different parameter settings. parameter set 0.3 algorithms
269

fiXU, HE, & HU

step-size parameter TD() following form.

n = 0

N0 +1
N0 + n

(34)

step-size schedule studied Boyan (1999). experiments, three
different settings used,
(s1) 0 = 0.01 , N 0 = 10 6
(s2) 0 = 0.01 , N 0 = 1000
(s3) 0 = 0.1 , N 0 = 1000 .

(35)

Different Boyan (1999), linear TD() algorithms applied
online forms, update weights every state transitions. parameter n (34)
number state transitions. run, weights initialized zeroes. Figure 2,
learning curves conventional linear TD() algorithms step-size schedules (s1), (s2)
(s3) shown curves 1,2 3, respectively. curve, averaged RMS errors
value function predictions states 20 independent runs plotted trial.
Curve 4 shows learning performance RLS-TD(). One additional parameter RLS-TD()
initial value variance matrix P0. experiment, set 500,
relatively large value. Figure 2, concluded making use RLS methods,
RLS-TD() obtain much better performance conventional linear TD() algorithms
eliminates design problem step-size schedules. experiments linear TD()
RLS-TD() different parameters conducted similar results obtained
initial values RLS-TD() large conclusion confirmed.

Figure 2: Performance comparison RLS-TD() TD()
1,2,3 ---TD(0.3) step-size parameters specified (s1),(s2) (s3)
4RLS-TD(0.3) initial variance matrix P0=500I
done demonstrative experiments investigate influence performance
RLS-TD() algorithm. Figure 3 shows performance comparison RLS-TD()
270

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

algorithms using two different initial parameters variance matrix P0, P0=0.1I
P0=1000I, respectively. forgetting factor =0.995. performance suggested
algorithm measured averaged RMS errors value function prediction first
200 trials 20 independent runs 13 states. experiments, 11 settings
parameter tested, 0.1n (n=0,1,,10).
Figure 3, clearly shown performance RLS-TD() large initial value
much better RLS-TD() small initial value . experiments
different parameter settings , similar results obtained. may refer
phenomenon low SNR case forgetting factor RLS studied Moustakides (1997).
Hop-World problem, stochastic state transitions could introduce high equation
residuals A( X )W b( X ) (19), corresponds additive noise large variance,
i.e., low SNR case. discussed Section 2, forgetting factor RLS low
SNR cases, relatively large initializing constant must selected better results. full
understanding phenomenon yet found.

Figure 3: Performance comparison RLS-TD() different initial value (=0.995)
performance RLS-TD() unit forgetting factor =1 tested
experiments. Although initial value effect RLS =1 discussed intensively
(Moustakides,1997), effects observed empirically case =1
<1, shown Figure 4.
experiments, found initialized small value,
performance sensitive values parameter . case, convergence
speed RLS-TD() increases increases 0 1, shown Figure 3.
Furthermore, fixed, performance RLS-TD() deteriorates becomes smaller,
shown Figure 5 .

271

fiXU, HE, & HU

Figure 4: Performance comparison RLS-TD() different initial value (=1)

Figure 5: Learning curves LS-TD() RLS-TD() different (=1)

Figure 5, learning curves RLS-TD() different initializing constants
shown compared LS-TD(). experiment, set 0.5. Figure 5,
shown performance RLS-TD() approaches LS-TD() becomes large.
well known, becomes large enough, performance RLS LS methods
almost same. Figure 6 shows performance comparison LS-TD()
RLS-TD() large value . initial variance matrix RLS-TD() set 500I
every runs, identity matrix.

272

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Figure 6: Performance comparison LS-TD() RLS-TD() =1 large initial
value
Based experimental results, concluded convergence speed
RLS-TD( ) mainly influenced initial value variance matrix parameter
. Detailed discussions properties RLS-TD( ) given follows:
(1) relatively large, effect becomes small. large enough goes
infinity, performance RLS-TD( ) LS-TD( ) almost same,
discussed above. cases, effect speed convergence insignificant,
coincides discussion Boyan (1999). However, described Theorem 2,
value still affects ultimate error bound value function approximation.
(2) relatively small, observed convergence performance
RLS-TD() different LS-TD() influenced values .
experiments Hop-World problem, results show smaller values lead
slower convergence. results may explained theoretical analysis transient
phase forgetting factor RLS (Moustakides,1997). According theory Moustakides
(1997), larger values needed better performance cases low SNR
smaller values preferable fast convergence cases high medium SNR.
different values must selected faster convergence RLS-TD( ) different cases.
Especially, cases, high SNR case discussed Moustakides (1997), RLS
methods small values obtain fast speed convergence.
(3) Compared conventional linear TD( ) algorithms, RLS-TD( ) algorithm
obtain much better performance making use RLS methods value function prediction
problems. Furthermore, TD( ), step-size schedule needs carefully designed achieve
good performance, RLS-TD( ), initial value variance matrix selected
according criterion large small value.
(4) comparison LS-TD( ) RLS-TD( ), one preferable depends
objective. online applications, RLS-TD( ) advantages computational efficiency
computation per step RLS-TD( ) O(K2) LS-TD( ), O(K3),
273

fiXU, HE, & HU

K number state features. Moreover, seen later, RLS-TD( ) obtain better
transient convergence performance LS-TD( ) cases. hand, LS-TD( )
may preferable RLS-TD( ) long-term convergence performance, seen
Figure 5. system identification point view, LS-TD( ) obtain unbiased
parameter estimates face white additive noises RLS-TD( ) finite would
possess large parameter discrepancies.

5. Fast-AHC Algorithm Two Learning Control Experiments
section, Fast-AHC algorithm proposed based results learning
prediction solve learning control problems. Two learning control experiments conducted
illustrate efficiency Fast-AHC.
5.1 Fast-AHC Algorithm

ultimate goal reinforcement learning learning control, i.e., estimate optimal
policies optimal value functions Markov decision processes (MDPs). now, several
reinforcement learning control algorithms including Q-learning (Watkins Dayan,1992),
Sarsa-learning (Singh, et al.,2000) Adaptive Heuristic Critic (AHC) algorithm (Barto,
Sutton Anderson,1983) proposed. Among methods, AHC method
different Q-learning Sarsa-learning value-function-based methods.
AHC method, value functions policies separately represented value-functionbased methods policies determined value functions directly. two
components AHC method, called critic actor, respectively. actor
used generate control actions according policies. critic used evaluate
policies represented actor provide actor internal rewards without waiting
delayed external rewards. Since objective critic policy evaluation learning
prediction, temporal-difference learning methods chosen critics learning algorithms.
learning algorithm actor determined estimation gradient policies.
following discussion, detailed introduction AHC method given.
Figure 7 shows architecture learning system based AHC method. learning
system consists critic network actor network. inputs critic network include
external rewards state feedback environment. internal rewards provided
critic network called temporal-difference (TD) signals.
reinforcement learning methods, whole system modeled MDP denoted
tuple {S,A,P,R},where state set, action set, P state transition probability
R reward function. policy MDP defined function :SPr(A),
Pr(A) probability distribution action space. objective AHC method
estimate optimal policy * satisfying following equation.


J = max J = max E [ rt ]
*





(36)

=0

discount factor rt reward time-step tE[ ] stands expectation
respect policy state transition probabilities J expected total
reward.

274

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Figure 7: AHC learning system
value function stationary policy optimal value function optimal
policy defined follows:


V ( ) = E [ rt 0 = ]

(37)

V * ( ) = E *[ rt s0 = ]

(38)

=0


=0

According theory dynamic programming, optimal value function satisfies
following Bellman equation.
(39)
V * ( ) = max[ R ( s, ) + EV * ( ' )]


R(s,a) expected reward received taking action state s.
AHC, critic uses temporal-difference learning approximate value function
current policy. linear function approximators used critic, weight update
equation
Wt +1 = Wt + [rt + V ( +1 ) V ( )]z

(40)

zt eligibility trace defined (10).
action selection policy actor determined current state value
function estimation critic. Suppose neural network weight vector u=[u1, u2,, um]
used actor, output actor network

= f (u , st )

(41)

action outputs actor determined following Gaussian probabilistic distribution.
( )2
p r ( ) = exp( 2 )
(42)



mean value given (41) variance given

= k1 /(1 + exp(k 2V ( ))

(43)

equation, k1 k2 positive constants V(st) value function es275

fiXU, HE, & HU

timation critic network.
obtain learning rule actor, estimation policy gradient given
follows:
J
J

=
rt
u
u
u

(44)

rt internal reward TD signal provided critic:

rt = rt + V ( st +1 ) V ( st )

(45)

Since AHC method, critic used estimate value function actors policy
provide internal reinforcement using temporal-difference learning algorithms,
efficiency temporal-different learning learning prediction greatly influence whole
learning systems performance. Although policy actor changing, may change
relatively slowly especially fast convergence learning prediction critic
realized. previous sections, RLS-TD( ) shown better data efficiency
conventional linear TD( ) algorithms fast convergence speed obtained
initializing constant chosen appropriately. Thus, applying RLS-TD( ) policy
evaluation critic network improve learning prediction performance critic
promising enhance whole systems learning control performance. Based
idea, new AHC method called Fast-AHC algorithm proposed paper. efficiency
Fast-AHC algorithm verified empirically detailed analysis results given.
Following complete description Fast-AHC algorithm.

Algorithm 3: Fast-AHC algorithm
1: Given: critic neural network actor neural network, linear
parameters, stop criterion algorithm.
2: Initialize state MDP learning parameters, set t=0.
3: stop criterion satisfied,
(3.1) According current state , compute output actor network ,

(3.2)

determine actual action actor probability distribution given
(42).
Take action MDP, observe state transition

+1 , set reward rt = r ( st , st +1 ) .
(3.3)
(3.4)

(3.5)

Apply RLS-TD( ) algorithm described (27)-(29) update weights
critic network.
Apply following equation update weights actor network,
J
+1 = +
(46)

learning factor actor.
Let t=t+1, return 3.

276

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

5.2 Learning Control Experiments Cart-Pole Balancing Problem
balancing control inverted pendulums typical nonlinear control problem
widely studied control theory artificial intelligence. research
artificial intelligence, learning control inverted pendulums considered standard test
problem machine learning methods, especially RL algorithms. studied
early work Michies BOXES system (Michie,et al.,1968) later Barto Sutton (1983),
learning controllers two output values: +10(N) 10(N). Berenji, et
al.(1992) Lin, et al.(1994), AHC methods continuous outputs applied cart-pole
balancing problem. paper, cart-pole balancing problem continuous control values
used illustrate effectiveness Fast-AHC method.
Figure 8 shows typical cart-pole balancing control system, consists cart moving
horizontally pole one end fixed cart. Let x denote horizontal distance
center cart center track, x negative cart
left part track. Variable denotes angle pole upright position (in
degrees) F amount force (N) applied cart move towards left right.
control system four state variables x, x& , ,& , x& ,& derivatives x ,
respectively.
Figure 8, mass cart M=1.0kg, mass pole m=0.1kg, half-pole
length l=0.5m, coefficient friction cart track c=0.0005 coefficient
friction pole cart p=0.000002. boundary constraints state variables
given follows.
12 12
(47)
2.4m x 2.4m
(48)
dynamics control system described following equations.

p (m + )&
(m + ) g sin cos [ F + ml& 2 sin c sgn( x& )]

ml
&& =

4
2
(49)

( + m)l ml cos
3


F + ml (& 2 sin && cos ) c sgn( x& )
&x& =
+m

g acceleration due gravity, 9.8m/s2. parameters
dynamics equations studied Barto et al. (1983).

Figure 8: cart-pole balancing control system
277

fiXU, HE, & HU

learning control experiments pole-balancing problem, dynamics (49)
assumed unknown learning controller. addition four state variables,
available feedback failure signal notifies controller failure occurs,
means values state variables exceed boundary constraints prescribed inequalities
(47) (48). typical reinforcement learning problem, failure signal serves
reward. Since external reward may available long sequence actions, critic
AHC learning controller used provide internal reinforcement signal accomplish
learning task. Learning control experiments pole-balancing problem conducted
using conventional AHC method uses linear TD() algorithms critic
Fast-AHC method proposed paper.
solve continuous state space problem reinforcement learning, class linear
function approximators, called Cerebellar Model Articulation Controller (CMAC)
used. neural network model based neuro-physiological theory human
cerebellarCMAC first proposed Albus (1975) widely used automatic
control function approximation. CMAC neural networks, dependence adjustable
parameters weights respect outputs linear. detailed discussion structure
CMAC neural networks, one may refer Albus (1975) Sutton & Barto (1998).
AHC Fast-AHC learning controllers, two CMAC neural networks four inputs
one output used function approximators critic actor,
respectively. CMAC C tilings partitions every input. total physical
memory CMAC network M4C. reduce computation memory requirements,
hashing technique described following equations employed experiments. (For
detailed discussion parameters CMAC networks, please refer Appendix C).
A( ) =

4

[a(i) + 1 ]

(50)

=1

F(s)=A(s) mod K
(51)
(50) (51), represents input state vector, a(i) (0 a(i) M) activated tile
i-th element s, K total number physical memory F(s) physical
memory address corresponding state s, remainder A(s) divided K.
order compare performance different learning algorithms, initial parameters
learning controller selected follows: weights critic initialized 0
weights actor initialized random numbers interval [0,0.1].
parameters AHC Fast-AHC algorithms = 0.95 , k1 = 0.4 k 2 = 0.5 .
experiments, trial defined period initial state failure state
initial state trial set randomly generated state near unstable equilibrium
(0,0,0,0) maximum distance 0.05. Equation (49) employed simulate dynamics
system using Euler method, time step 0.02s. trial lasts
120,000 time steps, said successful learning controller assumed
able balance pole. reinforcement signal problem defined

1, failure occurs
rt =
0, otherwise

(52)

performance Fast-AHC method tested extensively, different parameter
settings including initial variance matrix P0 chosen. experiments,
278

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

forgetting factor RLS-TD() critic set value equal 1 close 1.
learning control experiments using conventional AHC methods conducted
comparison. performance comparisons two algorithms shown Figure 9, 10
11.
experiments, initial variance matrixes Fast-AHC algorithm set
P0=0.1I. performance Fast-AHC compared AHC different . numbers
physical memories critic network actor network chosen 30 80,
respectively. parameter setting two algorithms, 5 independent runs tested.
performance evaluated according trial number needed successfully balance pole.
learning factors actor networks set 0.5, manually optimized value
algorithms. experiments, 11 settings tested.

Figure 9: Performance comparison Fast-AHC AHC =0.01

Figure 10: Performance comparison Fast-AHC AHC =0.03
279

fiXU, HE, & HU

Figure 11: Performance comparison Fast-AHC AHC =0.05

Figure 9, 10 11, learning factors critic networks AHC chosen
=0.01, 0.03 0.05, respectively. found <0.01, performance AHC
becomes worse. learning factors greater 0.05, AHC algorithm may
become unstable, even =0.03 =0.05, AHC algorithm becomes unstable
=1. time-varying learning factors specified (s1)-(s3), performance worse
constant learning factors. three settings learning factor typical
near optimal AHC algorithm.
experimental results, concluded using RLS-TD()
critic network, Fast-AHC algorithm obtain better performance conventional AHC
algorithms. Although Fast-AHC requires computation per step AHC,
efficient AHC less trials data needed successfully balance pole.
discussed previous sections, convergence performance RLS-TD()
influenced initial value variance matrix. case Fast-AHC.
learning control experiments, small value =0.1 selected. experiments,
set small values, performance Fast-AHC satisfactory better AHC.
However, equal relatively large value, example =100 500, performance
Fast-AHC deteriorates significantly. Since RLS-TD() large initializing constant
similar performance LS-TD(), deduced AHC method using LS-TD()
critic bad performance cart-pole balancing problem. verify this,
experiments conducted using Fast-AHC large initializing constant AHC using
LS-TD(). parameter setting, 5 independent runs tested. experiments,
maximum trials algorithm one run 200 algorithm fails balance
pole within 200 trials, performance set 200.When using LS-TD() AHC method,
may computational problems matrix inversion first steps learning
two methods tried avoid problem. One usage TD() first 60 steps
updates. actor updated early stage learning LS-TD()
280

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

stable. However, similar results found two methods. Figure 12 shows experimental
results clearly verify performance Fast-AHC large initializing constant
similar AHC using LS-TD() much worse Fast-AHC small . detailed
discussion phenomenon provided subsection 5.4.

Figure 12: Performance comparison Fast-AHC different initial variance
following Figure 13 Figure 14, variations pole angle control
force F plotted, successfully trained Fast-AHC learning controller used control
cart-pole system.

Figure 13: Variation pole angle

Figure 14: Variation control force

5.3 Learning Control Experiments Acrobot

subsection, another learning control example, swing-up control acrobot
minimum time, presented. learning control acrobot class adaptive optimal
control problem difficult pole-balancing problem. investigated
Sutton (1996), CMAC-based Sarsa-learning algorithms employed solve
case discrete control actions studied. experiments, case continuous actions
281

fiXU, HE, & HU

considered.
acrobot moving vertical plane shown Figure 15, OA AB first
link second link, respectively. control torque applied point A. goal
swing-up control swing tip B acrobot line CD higher
joint amount length one link.

Figure 15: acrobot
dynamics acrobot system described following equations.

&&1 = (d 2&&2 + 1 ) / d1

(53)

&&2 = ( + 21 / d1 2 )

(54)

d1 = m1l c21 + m2 (l12 + l c22 + 2l1l c 2 cos 2 ) + 1 + 2

(55)

2 = m2 (l c22 + l1l c 2 cos 2 ) + 2

(56)

1 = m2 l1l c 2&22 sin 2 2m2 l1l c 2&1&2 sin 2 + (m1l c1 + m2 l1 ) g cos( 1 / 2) + 2

(57)

2 = m2 l c 2 g cos( 1 + 2 / 2)

(58)



equations, parameters , &i , mi , li , , l ci angle, angle velocity,
mass, length, moment inertia length center mass link (i=1,2),
respectively.
Let sT denote goal state swing-up control. Since control aim swing
acrobot minimum time, reward function rt defined
1, = sT
rt =
0, else

(59)

simulation experiments, control torque continuous bounded [-3N, 3N].
Similar cart-pole balancing problem, CMAC neural networks applied solve
282

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

learning control problem continuous states actions. CMAC-based actor-critic
controller, actor network critic network C=4 tilings M=7 partitions
input. actor network, uniform coding employed non-uniform coding used
critic network. details coding parameters, please refer Appendix C. sizes
physical memories actor network critic network 100 80, respectively.
CMAC networks, following hashing techniques used. (For definition A(s),a(i)
F(s), please refer Subsection 5.2.)
4

A( ) = [ a(i ) 1 ]

(60)

=1

F(s)=A(s) mod K
(61)
simulation, parameters acrobot chosen m1=m2=1kg, I1=I2=1kgm2,
lc1=lc2=0.5m, l1=l2=1m g=9.8m/s2. time step simulation 0.05s time interval
learning control 0.2s. learning parameters =0.6, =0.90, =0.2, k1=0.4, k2=0.5.
trial defined period starts stable equilibrium ends goal state
reached. trial, state acrobot re-initialized stable equilibrium.
parameter setting, 5 independent runs tested. run consists 50 trials 50-th trial,
actor network tested controlling acrobot alone, i.e., setting action variance
defined (43) zero. performance algorithms evaluated according steps
used actor networks swing acrobot.
performance comparisons Fast-AHC AHC shown Figure 16,17
18. experiments, algorithms tested different AHC tested
different learning factors critic networks.
results, shown Fast-AHC achieve higher data efficiency AHC.
However, example, relatively large used, different previous
cart-pole balancing example. experiments, good performance obtained large
initializing constant small, performance deteriorates significantly. Thus
problem may referred low SNR case Moustakides (1997), large values
preferable best convergence rate RLS methods.

Figure 16: Performance comparison Fast-AHC AHC =0.02
283

fiXU, HE, & HU

Figure 17: Performance comparison Fast-AHC AHC =0.05

Figure 18: Performance comparison Fast-AHC AHC =0.1

following Figure 19 shows performance comparison Fast-AHC large
(300) small (0.01) value , 6 settings parameter tested
algorithm. performance AHC using LS-TD() shown. Figure 20, typical curve
angle first link plotted, acrobot controlled actor network
Fast-AHC method (=0.6) 50 trials.

284

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Figure 19: Performance comparison Fast-AHC AHC using LS-TD()

Figure 20: Variation angle link 1(Controlled Fast-AHC 50 trials)

5.4

Analysis Experimental Results

Based experimental results, concluded using RLS-TD()
algorithm critic network, Fast-AHC algorithm obtain better performance
conventional AHC algorithms less trials data needed converge near optimal
policy. well known, one difficulty applications RL methods slow
convergence, especially cases learning data hard generated.
Fast-AHC algorithm, although computation per step required conventional AHC
methods, serious problem number linear state features small.
learning control experiments, hashing techniques used reduce state features
CMAC networks computation Fast-AHC reduced economical amount.
Nevertheless, state feature number large, conventional AHC methods may
preferable.
experiments, observed performance Fast-AHC affected
initializing constant . results consistent property RLS-TD() RLS
285

fiXU, HE, & HU

method adaptive filtering, discussed Section 4. learning control
experiments cart-pole balancing problem, better performance Fast-AHC obtained
using small values . learning control acrobot, higher data efficiency
achieved using Fast-AHC relatively large . two different properties Fast-AHC
may referred different SNR cases RLS methods (Moustakides,1997). thorough
theoretical analysis problem interesting topic future research.
experiments, performance AHC method using LS-TD() tested.
studied Section 4, initializing constant large, performance
RLS-TD() LS-TD() differ much. performance AHC using LS-TD()
similar Fast-AHC large values .
studied Moustakides (1997), RLS method converge much faster
adaptive filtering methods environment stationary initializing constant selected
appropriately. cases, RLS may converge almost instantly. verified
learning prediction experiments RLS-TD() algorithm. applying RLS-TD()
actor-critic learning controller, although policy actor change time, still
assumed changing speed policy slow compared fast
convergence speed RLS-TD(). Thus good performance learning prediction obtained
critic. Moreover, since learning prediction performance critic important
policy learning actor, improvement learning prediction efficiency contribute
whole performance improvement controller.
6. Conclusions Future Work

Two new reinforcement learning algorithms using RLS methods, called RLS-TD( )
Fast-AHC, respectively, proposed paper. RLS-TD( ) used solve learning
prediction problems efficiently conventional linear TD( ) algorithms.
convergence probability 1 proved RLS-TD( ) limit convergence
analyzed. Experimental results learning prediction problems show RLS-TD( )
algorithm superior conventional TD( ) algorithms data efficiency eliminates
design problem step sizes linear TD( ) algorithms. RLS-TD( ) viewed
extension RLS-TD(0) =0 general 0< 1. Although effect
convergence speed RLS-TD( ) may significant cases, usage >0
still affect approximation error bound. Thus, needs value function
estimation high precision, large values preferable =0. Furthermore, RLSTD( ) superior LS-TD( ) computation weight vector must updated
every observations.
Since learning prediction viewed sub-problem learning control, extend
results learning prediction learning control method called AHC algorithm. Using
RLS-TD( ) critic network, Fast-AHC achieve better performance conventional
AHC method data efficiency. Simulation results learning control pole-balancing
problem acrobot system confirm analyses.
experiments, found performance RLS-TD( ) well Fast-AHC
influenced initializing constant RLS methods. Different values needed best
performance different cases. well-known phenomenon RLS-based adaptive
286

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

filtering theoretical results Moustakides (1997) provide basis explanations
results. complete investigation problem ongoing work.
idea using RLS-TD( ) critic network may applied reinforcement
learning methods actor-critic architectures. Konda Tsitsiklis (1998), new actor-critic
algorithm using linear function approximators proposed convergence certain
conditions proved. One condition convergence algorithm convergence
rate critic much faster actor. Thus application RLS-TD( )
critic may preferable order ensure convergence algorithm. theoretical
empirical work problem deserves studied future.

Acknowledgements
work supported National Natural Science Foundation China Grants
60075020, 60171003 China University Key Teachers Fellowship. would much
thank anonymous reviewers Associate Editor Michael L. Littman insights
constructive criticisms, helped improve paper significantly.

287

fiXU, HE, & HU

Appendix A. Derivation RLS-TD() Algorithm

derivation RLS-TD(), two different cases, determined value
forgetting factor.
(1) RLS-TD() unit forgetting factor.
Since

Pt = At1

(62)

P0 =

(63)

r
K +1 = Pt +1 z

(64)

According Lemma 1,

Pt +1 = At+11

r
r
= Pt Pt z [1 + ( ( xt ) ( x +1 )) Pt z )] 1 ( ( x ) ( xt +1 )) Pt

r
K +1 = Pt +1 z
r
r
= Pt z /(1 + ( ( xt ) ( xt +1 )) Pt z )

(65)

(66)

Wt +1 = At+11bt +1

r
= Pt +1 ( z ri )

(67)

=0

r
= Pt +1 ( Pt 1Wt + z rt )
Thus

r
r
Wt +1 = Pt +1 [( Pt +11 z ( ( xt ) ( x +1 )))Wt + z rt ]
r
r
= Wt + Pt +1 ( z rt z ( ( xt ) ( x +1 ))Wt )

(68)

= Wt + K +1 [rt ( ( xt ) ( xt +1 ))Wt ]
(2) RLS-TD() forgetting factor <1
derivation RLS-TD() forgetting factor <1 similar exponentially weighted
RLS algorithm Haykins (1996, pp.566-569). present results:

Pt +1 =

1



r
r
K +1 = Pt z /( + ( ( x ) ( x +1 )) Pt z )

(69)

Wt +1 = Wt + K +1 (rt ( ( x ) ( xt +1 ))Wt )

(70)

r
r
[ Pt Pt z [ + ( ( xt ) ( xt +1 )) Pt z )]1 ( ( xt ) ( xt +1 )) Pt ]

288

(71)

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

Appendix B. Proof Theorem 1
study steady property Markov chain defined Section 3, construct stationary
process follows. Let {xt} Markov chain evolves according transition matrix P
already steady state, means Pr{xt=i}= (i) t. Given sample path
Markov chain, define

r
zt =



( ) ( x )

(72)

=

r

X = {xt , xt +1 , z } stationary process, discussed (Tsitsiklis
Roy, 1997).
Let denote NN diagonal matrix diagonal entries (1), (2),, (N), N
cardinality state space X. Lemma 2 derived follows.
Lemma 2. (Tsitsiklis Roy, 1997) Assumption 1-4, following equations hold.
1) E 0 [ ( xt ) ( xt + )] = DP , m>0

r
2) E 0 [ z ( xt )] =

(73)



( ) DP ,

(74)

=0

r
3) E 0 [ z rt ( xt , xt +1 )] =



( ) DP r

(75)

=0

r R N , whose Nth component equal E[r ( xt , xt +1 ) xt = ] .
According Lemma 2, E0[A(Xt)] E0[b(Xt)] well defined finite. Furthermore, E0[A(Xt)]
negative definite, invertible.
equation (67),




WRLS TD ( ) = [ P01 + A( X )] 1 [ P01W0 + b( X )]
=1

=1

1
1
1
1
= [ P01 + A( X )] 1 [ P01W0 + b( X )]

=1

=1


(76)

Since

1
A( X )

=1

(77)

1
b( X )

=1

(78)

E 0 [ A( X )] = lim

E 0 [b( X )] = lim
E0[A(Xt)] invertible,
1

lim W RLS TD ( ) = E 0 [ A( X )]E 0 [b( X )] = W *



289

(79)

fiXU, HE, & HU

Thus W RLS TD ( ) converges W* probability 1.

Appendix C. details coding structures CMAC networks
following discussion, coding structures CMAC networks cart-pole balancing
problem acrobot control problem presented.
(1) CMAC coding structures cart-pole balancing problem
CMAC networks, state variables following boundaries.

[12 ,12 ] ,

& [50 deg/ s, 50 deg/ s]

x [2.4, 2.4] ,
x& [1,1]
critic network, C=4 M=7. hashing technique specified equations (50) (51)
employed total memory size 30.
actor network, C=4 M=7. hashing technique specified equations (60) (61)
employed total memory size 100.
(2) CMAC coding structures acrobot swing-up problem
simulation, angles bounded [ , ] angular velocities bounded

&1 [4 ,4 ] , &2 [9 ,9 ] . tiling numbers actor critic equal 4
(C=4). total memory sizes critic actor 80 100, respectively. actor
network, tiling partitions range input 7 equal intervals (M=7). critic
network, partitions input non-uniform, given

1 : { -, -1, -0.5, 0, 0.5, 1, },

&1 : {-4, -1.5, -0.5, 0, 0.5, 1.5, 4}

2 : {-, -1, -0.5, 0, 0.5, 1, },

&2 : {-9, -2, -0.5,0, 0.5,2, 9}

290

fiEFFICIENT REINFORCEMENT LEARNING USING RLS METHODS

References
Albus,J.S.(1975). new approach manipulator control: cerebellar model articulation
controller (CMAC). Journal Dynamic Systems, Measurement, Control, 97(3), 220-227.
Barto,A.G., Sutton R.S., & Anderson C.W. (1983). Neuronlike adaptive elements solve
difficult learning control problems. IEEE Transactions System, Man, Cybernetics,13,
834-846.
Bertsekas D.P. & Tsitsiklis J.N. (1996). Neurodynamic Programming. Belmont, Mass.: Athena
Scientific.
Berenji H.R. & Khedkar P. (1992). Learning tuning fuzzy logic controllers reinforcements, IEEE Trans.On Neural Networks, 3(5), 724-740.
Boyan. J.(1999). Least-squares temporal difference learning. Bratko, I., Dzeroski, S., eds.,
Machine Learning: Proceedings Sixteenth International Conference (ICML).
Boyan, J.(2002). Technical update: least-squares temporal difference learning. Machine Learning,
Special Issue Reinforcement Learning, appear.
Brartke. S.J. & Barto A. (1996). Linear least-squares algorithms temporal difference learning.
Machine Learning, 22, 33-57.
Dayan P.(1992). convergence TD() general . Machine Learning, 8, 341-362.
Dayan P.. & Sejnowski T.J. (1994). TD() converges probability 1. Machine Learning, 14,
295-301.
Eleftheriou E. & Falconer,D.D. (1986). Tracking properties steady state performance RLS
adaptive filter algorithms. IEEE Transactions Acoustics, Speech, Signal Processing, 34,
1097-1110.
Eweda E. & Macchi, O. (1987). Convergence RLS LMS adaptive filters. IEEE Trans.
Circuits Systems, 34, 799-803.
Haykin S. (1996), Adaptive Filter Theory, 3rd edition, Englewood Cliffs, NJ: Prentice-Hall.
Hubing N.E. & Alexander S.T. (1989). Statistical analysis soft constrained initialization
RLS algorithms. Proc. IEEE International Conference Acoustics, Speech
Signal Processing.
Jaakkola T., Jordan M.I., & Singh S.P. (1994). convergence stochastic iterative dynamic
programming algorithms. Neural Computation. 6(6), 1185-1201.
Kaelbling L.P., Littman M.L., & Moore A.W. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research, 4, 237-285.
Konda V.R, & Tsitsiklis J.N. (2000). Actor-critic algorithms. Neural Information Processing
Systems, 2000, MIT Press.
291

fiXU, HE, & HU

Lin L.J. (1992). Self-improving reactive agents based reinforcement learning, planning
teaching. Machine Learning, 8(3/4), 293-321.
Lin C.T. & Lee C.S.G. (1994). Reinforcement structure/parameter learning neural-networkbased fuzzy Logic control system. IEEE Transactions Fuzzy System, 2(1), 46-63.
Ljung L. & Soderstron T. (1983). Theory Practice Recursive Identification. MIT Press.
Ljung L. (1977). Analysis recursive stochastic algorithm. IEEE. Transactions Automatic
Control, 22, 551.
Michie D. & Chambers R.A. (1968). BOXES: experiment adaptive control. Machine
Intelligence 2, Dale E. Michie D., eds., Edinburgh: Oliver Boyd, 137-152.
Minsky M.L. (1954). Theory neural-analog reinforcement systems application
brain-model problem. Ph.D. Thesis, Princeton University.
Moustakides G.V. (1997). Study transient phase forgetting factor RLS. IEEE Trans.
Signal Processing, 45(10), 2468-2476.
Samuel A.L. (1959). studies machine learning using game checkers. IBM Journal
Research Development, 3, 211-229.
Singh, S.P., Jaakkola T., Littman M.L., & Szepesvari C. (2000). Convergence results singlestep on-policy reinforcement-learning algorithms. Machine Learning, 38, 287-308.
Sutton R. & Barto A. (1998). Reinforcement Learning, Introduction. Cambridge MA, MIT
Press.
Sutton R. (1988). Learning predict method temporal differences. Machine Learning,
3(1), 9-44.
Tsitsiklis J.N. (1994). Asynchronous stochastic approximation Q-learning. Machine Learning,
16, 185-202.
Tsitsiklis J.N. & Roy B.V. (1994). Feature-based methods large scale dynamic programming.
Neural Computation. 6(6), 1185-1201.
Tsitsiklis J.N. & Roy B.V. (1997). analysis temporal difference learning function
approximation. IEEE Transactions Automatic Control. 42(5), 674-690.
Watkins C.J.C.H. & Dayan P. (1992). Q-Learning. Machine Learning. 8, 279-292.
Young P. (1984). Recursive Estimation Time-Series Analysis. Springer-Verlag.

292


