Journal Artificial Intelligence Research 45 (2012) 515-564

Submitted 8/12; published 12/12

Safe Exploration State Action Spaces
Reinforcement Learning
Javier Garca
Fernando Fernandez

fjgpolo@inf.uc3m.es
ffernand@inf.uc3m.es

Universidad Carlos III de Madrid,
Avenida de la Universidad 30,
28911 Leganes, Madrid, Spain

Abstract
paper, consider important problem safe exploration reinforcement
learning. reinforcement learning well-suited domains complex transition
dynamics high-dimensional state-action spaces, additional challenge posed
need safe efficient exploration. Traditional exploration techniques
particularly useful solving dangerous tasks, trial error process may lead
selection actions whose execution states may result damage
learning system (or system). Consequently, agent begins interaction
dangerous high-dimensional state-action space, important question arises;
namely, avoid (or least minimize) damage caused exploration
state-action space. introduce PI-SRL algorithm safely improves suboptimal
albeit robust behaviors continuous state action control tasks efficiently
learns experience gained environment. evaluate proposed method
four complex tasks: automatic car parking, pole-balancing, helicopter hovering,
business management.

1. Introduction
Reinforcement learning (RL) (Sutton & Barto, 1998) type machine learning whose
main goal finding policy moves agent optimally environment, generally formulated Markov Decision Process (MDP). Many RL methods used
important complex tasks (e.g., robot control see Smart & Kaelbling, 2002; Hester,
Quinlan, & Stone, 2011, stochastic games see Mannor, 2004; Konen & Bartz-Beielstein,
2009 control optimization complex dynamical systems see Salkham, Cunningham,
Garg, & Cahill, 2008). RL tasks focused maximizing long-term cumulative reward, RL researchers paying increasing attention long-term
reward maximization, safety approaches Sequential Decision Problems
(SDPs) (Mihatsch & Neuneier, 2002; Hans, Schneegass, Schafer, & Udluft, 2008; Martn H.
& Lope, 2009; Koppejan & Whiteson, 2011). Well-written reviews matters
found (Geibel & Wysotzki, 2005; Defourny, Ernst, & Wehenkel, 2008). Nevertheless,
important ensure reasonable system performance consider safety
agent (e.g., avoiding collisions, crashes, etc.) application RL dangerous
tasks, exploration techniques RL offer guarantees issues. Thus,
using RL techniques dangerous control tasks, important question arises; namely,
ensure exploration state-action space cause damage injury
c
2012
AI Access Foundation. rights reserved.

fiGarca & Fernandez

while, time, learning (near-)optimal policies? matter, words,
one ensuring agent able explore dangerous environment safely
efficiently. many domains exploration/exploitation process may lead
catastrophic states actions learning agent (Geibel & Wysotzki, 2005).
helicopter hovering control task one case involving high risk, since policies
crash helicopter, incurring catastrophic negative reward. Exploration/exploitation
strategies greedy may even result constant helicopter crashes (especially
high probability random action selection). Another example found
portfolio theory analysts expected find portfolio maximizes profit
avoiding risks considerable losses (Luenberger, 1998). Since maximization expected
returns necessarily prevent rare occurrences large negative outcomes, different
criteria safe exploration needed. exploration process new policies
evaluated must conducted extreme care. Indeed, environments, method
required explores state-action space, safe manner.
paper, propose Policy Improvement Safe Reinforcement Learning
(PI-SRL) algorithm safe exploration dangerous continuous control tasks.
method requires predefined (and safe) baseline policy assumed suboptimal
(otherwise, learning would pointless). Predefined baseline policies used
different ways approaches. work Koppejan Whiteson (2011), singlelayers perceptrons evolved, albeit starting prototype network whose weights correspond baseline policy provided helicopter control task competition software (Abbeel,
Coates, Hunter, & Ng, 2008). approach viewed simple form population seeding proven advantageous numerous evolutionary methods
(e.g. see Hernandez-Daz, Coello, Perez, Caballero, Luque, & Santana-Quintero, 2008; Poli
& Cagnoni, 1997). work Martn de Lope (2009), weights neural networks evolved inserting several baseline policies (including provided
helicopter control task competition software) initial population. minimize
possibility evaluating unsafe policies, approach prevents crossover mutation
operators permitting anything tiny changes initial baseline policies.
paper, present PI-SRL algorithm, novel approach improving baseline
policies dangerous domains using RL. PI-SRL algorithm composed two different steps. first, baseline behavior (robust albeit suboptimal) approximated
using behavioral cloning techniques (Anderson, Draper, & Peterson, 2000; Abbott, 2008).
order achieve goal, case-based reasoning (CBR) techniques (Aamodt & Plaza,
1994; Bartsch-Sprl, Lenz, & Hbner, 1999) used successfully applied
imitation tasks past (Floyd & Esfandiari, 2010; Floyd, Esfandiari, & Lam, 2008).
second step, PI-SRL algorithm attempts safely explore state-action space
order build accurate policy previously-learned behavior. Thus, set
cases (i.e., state-action pairs) obtained previous phase improved
safe exploration state-action space. perform exploration, small amounts
Gaussian noise randomly added greedy actions baseline policy approach.
exploration strategy used successfully previous works (Argall, Chernova,
Veloso, & Browning, 2009; Van Hasselt & Wiering, 2007).
novelty present study use two new, main components: (i) risk
function determine degree risk particular state (ii) baseline behavior
516

fiSafe Exploration State Action Spaces Reinforcement Learning

capable producing safe actions supposedly risky states (i.e., states lead
damage injury). addition, present new definition risk based
agent unknown known space. described Section 5 greater detail,
new definition completely different traditional definitions risk found literature (Geibel, 2001; Mihatsch & Neuneier, 2002; Geibel & Wysotzki, 2005). paper
reports experimental results obtained application new approach four
different domains: (i) automatic car parking (Lee & Lee, 2008), (ii) pole-balancing (Sutton
& Barto, 1998), (iii) 2009 RL Competition helicopter hovering (Ng, Kim, Jordan, & Sastry,
2003) (iv) business management (Borrajo, Bueno, de Pablo, Santos, Fernandez, Garca,
& Sagredo, 2010). domain, propose learning near-optimal policy which,
learning phase, minimize car crashes, pole disequilibrium, helicopter crashes
company bankruptcies, respectively. important note comparison
approach agent optimal exploration policy possible since,
proposed domains (each high-dimensional continuous state action space,
well complex stochastic dynamics), know optimal exploration policy
is.
Regarding organization remainder paper, Section 2 introduces key
definitions, Section 3 describes detail learning approach proposed. Section 4,
evaluation performed four mentioned domains presented. Section 5
discusses related work Section 6 summarizes main conclusions study.
sections,
term return used refer expected cumulative future discounted
P
reward R = t=0 rt , term reward used refer single real value used
evaluate selection action particular state denoted r.

2. Definitions
illustrate concept safety used approach, navigation problem presented
Figure 1. navigation problem presented Figure 1, control policy must
learned get particular start state goal state, given set demonstration
trajectories. environment, assume task difficult due stochastic
complex dynamic environment (e.g., extremely irregular surface case
robot navigation domain wind effects case helicopter hover task).
stochasticity makes impossible complete task using exactly trajectory
every time. Additionally, problem supposes set demonstrations baseline
controller performing task (the continuous black lines) given. set
demonstrations composed different trajectories covering well-defined region
state space (the region within rectangle).
approach based addition small amounts Gaussian noise perturbations baseline trajectories order find new better ways completing
task. noise affect baseline trajectories different ways, depending
amount noise added which, turn, depends amount risk taken. risk
desired, noise added baseline trajectories 0 and, consequently, new
improved behavior discovered (nevertheless, robot never fall cliff
helicopter never crash). If, however, intermediate level risk desired,
small amounts noise added baseline trajectories new trajectories (the
517

fiGarca & Fernandez

Figure 1: Exploration strategy based addition small amounts noise baseline
policy behavior. Continuous lines represent baseline behavior, newly
explored behaviors indicated dotted dashed lines.
dotted blue lines) complete task discovered. cases, exploration new
trajectories leads robot unknown regions state space (the dashed red lines).
robot assumed able detect situations risk function use
baseline behavior return safe, known states. If, instead, high risk desired,
large amounts noise added baseline trajectories, leading discovery
new trajectories (but higher probability robot gets damaged).
iteration process leads robot progressively safely explore state
action spaces order find new improved ways complete task. degree
safety exploration, however, depend risk taken.
2.1 Error Non-Error States
paper, follow far notation presented Geibel et al. (2005)
definition concept risk. study, Geibel et al. associate risk error
states non-error states, former understood state considered
undesirable dangerous enter.
Definition 1 Error non-error states. Let set states set
error states. state undesirable terminal state control
agent ends reached damage injury agent, learning system
external entities. set considered set non-error terminal states
= control agent ends normally without damage injury.
terms RL, agent enters error state, current episode ends damage
learning system (or systems); whereas enters non-error state, episode
ends normally without damage. Thus, Geibel et al. define risk respect
policy , (s), probability state sequence (si )i0 s0 = s, generated
execution policy , terminates error state s0 . definition, (s) = 1
. , (s) = 0 = . states
/ , risk
taken depends actions selected policy . definitions,
518

fiSafe Exploration State Action Spaces Reinforcement Learning

theoretical framework introduce definition risk associated
known unknown states.
2.2 Known Unknown States Continuous Action State Spaces
assume continuous, n-dimensional state space <n state = (s1 , s2 , . . . ,
sn ) vector real numbers dimension individual domain Dis <.
Similarly, assume continuous m-dimensional action space <m
action = (a1 , a2 , . . . , ) vector real numbers dimension
individual domain Dia <. Additionally, agent considered endowed
memory, case-base B, size . memory element represents state-action pair,
case, agent experienced before.
Definition 2 (Case-base). case-base set cases B = {c1 . . . , c }. Every case
ci consists state-action pair (si , ai ) agent experienced past
associated value V (si ). Thus, ci =< si , ai , V (si ) >, first element represents
cases problem part corresponds state si , following element ai depicts case
solution (i.e., action expected agent state si ) final element
V (si ) value function associated state si . state si composed n
continuous state variables action ai composed continuous action variables.
agent receives new state sq , first retrieves nearest neighbor sq
B according given similarity metric performs associated action.
paper, use Euclidean distance similarity metric (Equation 1).
v
uX
u n
d(sq , si ) = (sq,j si,j )2

(1)

j=0

Euclidean distance metric useful value function expected continuous smooth throughout state space (Santamara, Sutton, & Ram, 1998). However,
since value function unknown priori Euclidean distance metric particularly suitable many problems, many researchers begun ask distance
metric learn adapt order achieve better results (Taylor, Kulis, & Sha,
2011). use distance metric learning techniques would certainly desirable
order induce powerful distance metric specific domain, consideration
lies outside scope present study. paper, therefore, focused
domains Euclidean distance proven successful (i.e., successfully applied car parking (Cichosz, 1995), pole-balancing (Martin H & de Lope, 2009),
helicopter hovering control (Martin H & de Lope, 2009) SIMBA (Borrajo et al., 2010).
Traditionally, case-based approaches use density threshold order determine
new case added memory. distance nearest neighbor
sq greater , new case added. sense, parameter defines size
classification region case B (Figure 2). new case sq within
classification region case ci , considered known state. Hence, cases

associated value function V B
B describe case-based policy agent B
.
519

fiGarca & Fernandez

Figure 2: Known Unknown states.
Definition 3 (Known/Unknown states). Given case-base B = {c1 . . . , c } composed
cases ci = (si , ai , V (si )) density threshold , state sq considered known
min1i d(sq , si ) unknown cases. Formally, set known
states, set unknown states = = S.
Definition 3, states identified known unknown. agent
receives new state , performs action ai case ci d(s, si ) =
min1j d(s, sj ). However, agent receives state where, definition,
distance state B larger , case retrieved. Consequently, action
performed state unknown agent.
Definition 4 (Case-Based risk function). Given case base B = {c1 . . . , c } composed
cases ci = (si , ai , V (si )), risk state defined Equation 2.

B

%


(s) =

0
1

min1j d(s, sj ) <
otherwise

(2)



Thus, %B (s) = 1 holds (i.e., unknown), state
associated case and, hence, action performed given situation

unknown. , %B (s) = 0.
derived caseDefinition 5 (Safe case-based policy). case-based policy B
base B = {c1 . . . . , c } safe when, initial known state s0 respect B,
always produces known non-error states respect B.
execution B






B
s0 | %B (s0 ) = 0, (si )i>0
%B (si ) = 0

(3)

Additionally, assumed probability state sequence (si )i0
, terminates error state
known state s0 , generated executing policy B

B (s0 ) = 0 (i.e., = ).
520

fiSafe Exploration State Action Spaces Reinforcement Learning

Definition 6 (Safe case-based coverage). coverage single state respect
safe case-base B = {c1 . . . . , c } defined state si min1i d(s, si ) .
Therefore, assume safe case-based provide actions entire state
space, rather known states .
Figure 3 graphically represents relationship known/unknown error/non learnt,
error states. green area image denotes safe case-based policy B
area state space corresponding initial known space. agent following
always green area resulting episodes end without
policy B
damages. Consequently, subset non-error states form part known space.
Formally, let subsets non-error states belonging known unknown
spaces, respectively, = . . yellow area Figure,
contrast, represents unknown space . space found error states,
well subset remaining non-error states. Formally, .
Understood way, PI-SRL algorithm summed follows:
.
first step, learn known space (green area) safe case-based policy B

second step, adjust known space (green area) unknown space (yellow
area) order explore new improved behaviors avoiding error states (red
area). process adjusting known space space used safe
better policies, algorithm forget ineffectual known states, shown
Section 4.

Figure 3: Known/unknown error/non-error states given Case Base B.

2.3 Advantages Using Prior Knowledge Predetermined
Exploration Policies
present subsection, advantages using teacher knowledge RL, namely (i)
provide initial knowledge task learned (ii) support exploration
process, highlighted. Furthermore, explain believe knowledge
521

fiGarca & Fernandez

indispensable RL tackling highly complex realistic problems large, continuous
state action spaces particular action may result undesirable
consequence.
2.3.1 Providing Initial Knowledge Task
RL algorithms begin learning without previous knowledge task
learnt. cases, exploration strategies greedy used. application
strategy results random exploration state action spaces gather
knowledge task. enough information discovered environment algorithms behavior improve. random exploration policies, however,
waste significant amount time exploring irrelevant regions state action
spaces optimal policy never encountered. problem compounded
domains extremely large continuous state action spaces random
exploration never likely visit regions spaces necessary learn (near-)optimal
policies. Additionally, many real RL tasks real robots, random exploration
gather information environment cannot even applied. real robots,
considered sufficient information much information real robot
gather environment. Finally, impossible avoid undesirable situations
high-risk environments without certain amount prior knowledge task,
use random exploration would require undesirable state visited
labeled undesirable. However, visits undesirable states may result damage
injury agent, learning system external entities. Consequently, visits
states avoided earliest steps learning process.
Mitigating difficulties described above, finite sets teacher-provided examples
demonstrations used incorporate prior knowledge learning algorithm.
teacher knowledge used two general ways, either (i) bootstrap learning algorithm (i.e., sort initialization procedure) (ii) derive policy
examples. first case, learning algorithm provided examples demonstrations bootstrap value function approximation lead agent
relevant regions space. second way teacher knowledge
used refers Learning Demonstration (LfD) approaches policy derived finite set demonstrations provided teacher. principal drawback
approach, however, performance derived policy heavily limited
teacher ability. one way circumvent difficulty improve performance
exploring beyond provided teacher demonstrations, raises
question agent act encounters state demonstration
exists (an unknown state).
2.3.2 Supporting Exploration Process
furnishing agent initial knowledge helps mitigate problems associated
random exploration, alone sufficient prevent undesirable situations
arise subsequent explorations undertaken improve learner ability. additional mechanism necessary guide subsequent exploration process way
agent may kept far away catastrophic states. paper, teacher,
522

fiSafe Exploration State Action Spaces Reinforcement Learning

rather policy derived current value function approximation used
selection actions unknown states. One way prevent agent encountering
unknown states exploration process would requesting beginning
teacher demonstration every state state space. However, strategy
possible due (i) computational infeasibility given extremely large number states
state space (ii) fact teacher forced give action
every state, given many states ineffectual learning optimal policy.
Consequently, PI-SRL requests teacher action action actually required
(i.e., agent unknown state).
paper supposes teacher available task learned,
teacher taken baseline behavior. Although studies examined use
robotic teachers, hand-written control policies simulated planners, great majority
date made use human teachers. paper uses suboptimal automatic controllers
teachers, taken teachers policy.
Definition 7 (Baseline behavior). Policy considered baseline behavior
three assumptions made: (i) able provide safe demonstrations
task learnt prior knowledge extracted; (ii) able support
subsequent exploration process, advising suboptimal actions unknown states reduce
probability entering error states return system known situation;
(iii) performance far optimal.
optimal baseline behaviors certainly ideal behave safely, non-optimal behaviors often easy (or easier) implement generate optimal ones. PI-SRL
algorithm uses baseline behavior two different ways. First, uses safe demonstrations provide prior knowledge task. step, algorithm builds

initial known space agent derived safe case-based policy B

purpose mimicking B . second step, PI-SRL uses support
subsequent exploration process conducted improve abilities previously-learnt
. exploration process continues, action requested required,
B

is, agent unknown state (Figure 4). step, acts backup
policy case unknown state intention guiding learning away
catastrophic errors or, least, reducing frequency. important note
baseline behavior cannot demonstrate correct action every possible state. However,
baseline behavior might able indicate best action cases,
action supplies should, least, safer obtained random
exploration.
2.4 Risk Parameter
order maximize exploration safety, seems advisable movement
state space arbitrary, rather known space expanded gradually
starting known state. exploration carried perturbation
. Perturbation trajectories
state-action trajectories generated policy B
accomplished addition Gaussian random noise actions B order
obtain new ways completing task. Thus, Gaussian exploration takes place
523

fiGarca & Fernandez

Figure 4: exploration process PI-SRL requests actions baseline behavior, ,
really required.
around current approximation action ai current known state sc ,
ci = (si , ai , V (si )) d(sc , si ) = min1j d(s, sj ). action performed sampled
Gaussian distribution mean action output given instance selected
B. ai denotes algorithm action output, probability selecting action a0i ,
(s, a0i ) computed using Equation 4.
(s, a0i ) =

0
2
2
1 e(ai ai ) /2
2 2

2 > 0.

(4)

shape Gaussian distribution depends parameter (standard deviation).
study, used width parameter. large values imply wide bellshaped distribution, increasing probability selecting actions a0i different
current action ai , small value implies narrow bell-shaped distribution, increasing
probability selecting actions a0i similar current action ai . 2 = 0,
assume (s, ai ) = 1. Hence, value directly related amount perturbation
. Higher values imply
added state-action trajectories generated policy B
greater perturbations (more Gaussian noise) greater probability visiting unknown
states.
Definition 8 (Risk Parameter). parameter considered risk parameter. Large
values increase probability visiting distant unknown states and, hence, increase
probability reaching error states.
exploratory actions drive agent edge known space force
go slightly beyond, unknown space, search better, safer behaviors.
period time, execution exploratory actions increases known space
. risk
improves abilities previously-learned safe case-based policy B
parameter , well , design parameters must selected user.
Section 3.3, guidelines selection offered.
important note approach proposed study based two logical
assumptions RL derived following generalization principles (Kaelbling, Littman,
& Moore, 1996; Sutton & Barto, 1998):
524

fiSafe Exploration State Action Spaces Reinforcement Learning

(i) Nearby states similar optimal actions. continuous state spaces,
impossible agent visit every state store value (or optimal action)
table. generalization techniques needed. large, smooth state spaces,
similar states expected similar values similar optimal actions. Therefore,
possible use experience gathered environment limited subset
state space produce reliable approximation much larger subset (Boyan, Moore,
& Sutton, 1995; Hu, Kostiadis, Hunter, & Kalyviotis, 2001; Fernandez & Borrajo, 2008).
One must note that, proposed domains, optimal action considered
safe action sense never produces error states (i.e., action considered
optimal leads agent catastrophic situation).
(ii) Similar actions similar states tend produce similar effects. Considering deterministic domain, action performed state st always produces
state st+1 . stochastic domain, understood intuitively execution
action state st produce similar effects (i.e., produces states {s1t+1 , s2t+1 , s3t+1 , . . .}
i, j 6= j dist(sit+1 , sjt+1 ) 0). Additionally, execution action a0t
state s0t st produces states {s0 1t+1 , s0 2t+1 , s0 3t+1 , . . .} i, j dist(s0 it+1 , sjt+1 ) 0.
explained earlier, present study uses Euclidean distance similarity metric,
proven successful proposed domains. result assumption,
approximation techniques used, actions generate similar effects
grouped together one action (Jiang, 2004). continuous action spaces, need
generalization techniques even greater (Kaelbling et al., 1996). paper,
assumption allows us assume low values increase probability visiting
known states and, hence, exploring less taking less risks, greater values
increase probability reaching error states.

3. PI-SRL Algorithm
PI-SRL algorithm composed two main steps described detail below.
3.1 First Step: Modeling Baseline Behaviors CBR
first step PI-SRL approach behavioral cloning, using CBR allow software
agent behave similar manner teacher policy (baseline behavior) (Floyd et al.,
2008). Whereas LfD approaches named differently according learned (Argall et al., 2009), prevent terminological inconsistencies here, consider behavioral
cloning (also known imitation learning) area LfD whose goal reproduction/mimicking underlying teacher policy (Peters, Tedrake, Roy, & Morimoto,
2010; Abbott, 2008).
using CBR behavioral cloning, case built using agents state
received environment, well corresponding action command performed
teacher. PI-SRL, objective first step properly imitate behavior
using cases stored case-base. point, important question arises; namely,
case-base B learnt using sample trajectories provided that,
end learning process, resulting policy derived B mimics behavior
? Baseline behavior function maps states actions : or,
525

fiGarca & Fernandez

words, function that, given state si S, provides corresponding action ai A.
paper, want build policy B derived case-base composed cases (sj , aj )
that, new state sq , case minimum Euclidean distance dist(sq , sj )
retrieved corresponding action aj returned. Intuitively, assumed
B built simply storing cases (si , ai ) gathered one interaction
environment limited number episodes K. end K episodes,
one expects resulting B able properly mimic behavior . However,
informal experimentation helicopter hovering domain shows case
(Section 4.3). helicopter hovering, K = 100 episodes prohibitive number
600,000 cases stored, policy derived case-base B unable correctly
imitate baseline behavior and, instead, continuously crashes helicopter. Indeed,
order B mimic large continuous stochastic domains, approach
requires larger number episodes and, consequently, prohibitive number cases.
fact, perfectly mimic domains, infinite number cases would required.
Figure 5 attempts explain believe learning process work.
it, region space represented simply storing cases derived form
c = (s, a) shown. stored case (red circles) covers area space represents
centroid Voronoi region.

Figure 5: Effects storing training cases.
previously-learned policy B used new state sq presented, action aj performed, corresponding case cj = (sj , aj ) Euclidean distance
dist(sq , sj ) less stored cases. However, use policy
provide action situation sq , action ai provided different aj .
point, policy B said classify state sq obtained class aj ,
policy said classify state sq desired class ai (insofar
policy mimicked), |ai aj | > 0. Furthermore, |ai aj | understood
classification error. case-base stored possible pairs (si , ai )
able generate domain, actions aj ai would always identical,
dist(sq , sj ) = 0 |ai aj | = 0. However, stochastic large, continuous domain,
impossible store cases. sum classification errors episode
526

fiSafe Exploration State Action Spaces Reinforcement Learning

leads visiting unexplored regions case space (i.e., regions new
state sq received environment Euclidean distance dist(sq , sj ) >>
respect closest case cj = (sj , aj ) B). unexplored regions visited,
difference obtained class derived B desired class derived
large (i.e., |ai aj | >> 0) probability error states might visited
greatly increases.
may concluded, therefore, simply storing pairs c = (s, a) generated
sufficient properly mimic behavior. reason, algorithm Figure 6
proposed.
CBR Approach Behavioral Cloning
00
01
02
03

Given
Given
Given
1. Set

04
05
06
07

2. Repeat
Set k = 0
k < maxEpisodeLength
Compute case < sc , ac , 0 > closest current state sk

08
09
10
11
12
13
14
15
16
17
18
19
20






baseline behavior
density threshold
maximum number cases
case-base B =



%B (sk ) = 0 // equation 2
Set ak = ac
else
Set ak using baseline behavior
Create new case cnew = (sk , ak , 0)
B := B cnew
Execute ak , receive sk+1
Set k = k + 1
end
kBk >
Remove kBk least-frequently-used cases B
stop criterion becomes true

3. Return B performing safe case-based policy B

Figure 6: CBR algorithm behavioral cloning.


first step algorithm, state-value function V B (si ) initialized 0 (see

line 07). value V B (si ) case computed second step algorithm
Section 3.2. Additionally, step uses case-based risk function (Equation 2)
determine whether new state sk considered risky (line 08). new state
risky (i.e., known state sk ), 1-nearest neighbor strategy followed (line
09). Otherwise, algorithm performs action ak using baseline behavior
new case cnew = (sk , ak , 0) built added case-base B (line 13). Starting
empty case-base, learning algorithm continuously increases competence storing
new experiences. However, number reasons inflow new cases
limited. Large case-bases increase time required find closest cases new
example. may partially solved using techniques reduce retrieval time
(e.g., k-d trees used work), nevertheless reduce storage
527

fiGarca & Fernandez

requirements. Several approaches removal ineffectual cases training exist,
including Ahas IBx algorithms (Aha, 1992) nearest prototype approach (Fernandez
& Isasi, 2008). number cases stored B exceeds critical value kBk >
realization retrieval within certain amount time cannot guaranteed,
removal cases inevitable. efficient approach problem
removal least-frequently-used elements B (line 18).
result step constrained case-base B describing safe case-based policy

B mimics baseline behavior , though perhaps deviation (line 20).
Formally, let U (T ) estimate utility baseline behavior computed
) U ( ).
averaging sum rewards accumulated NT trials. Then, U (B

3.2 Second Step: Improving Learned Baseline Behavior
learned previous
step PI-SRL algorithm, safe case-based policy B
step improved safe exploration state-action space. First, case ci

B, state-value function V B (si ) computed following Monte Carlo (MC) approach
(Figure 7).
MC Algorithm Adapted CBR
00
01
02
03

Given case-base B
1. Initialize, ci B
V (s) arbitrary
Returns(s) empty list

04
05
06
07
08
09

2. k < maxN umberEpisodes

Generate episode using B
appearing episode < s, a, V (s) > B
R return following first occurrence
Append R Returns(s)
V(s) average(Returns(s))

10
11

Set k = k + 1
3. Return B

Figure 7: Monte Carlo algorithm computation state-value function case.
algorithm similar spirit first-visit MC method V (Sutton & Barto,
1998), adapted paper work policy given case-base. algorithm
shown Figure 7, returns state si B accumulated averaged, following
derived case base B (see line 09). important note
policy B
algorithm term return following first occurrence refers expected return
(i.e., expected cumulative future discounted reward starting state), whereas
Returns refers list composed return different episodes. One
principal reasons using MC method allows us quickly easily estimate

state values V B (si ) case ci B. addition, MC methods shown
successful wide variety domains (Sutton & Barto, 1998). state-value

function V B (si ) computed case ci B, small amounts Gaussian noise
order obtain new improved ways
randomly added actions policy B
528

fiSafe Exploration State Action Spaces Reinforcement Learning

complete task. algorithm used improve baseline behavior learned
previous step depicted Figure 8. algorithm composed four steps performed
episode.
- (a) Initialization step. algorithm initializes list used store cases occurring
episode sets cumulative reward counter episode 0.
- (b) Case Generation. algorithm builds case step episode.
new state sk , closest case < s, a, V (s) > B computed using Euclidean
distance metric Equation 1 (see line 09 algorithm Figure 8). order determine
perceived degree risk new state sk , case-based risk function used (line

10). %B (sk ) = 0, sk (known state). case, action ak performed
computed using Equation 4 new case cnew =< s, ak , V (s) > built added
list cases occurred episode (line 13). important note
new case < s, ak , V (s) > built replacing action corresponding closest case
< s, a, V (s) > B, new action ak resulting application random
Gaussian noise Equation 4. Thus, algorithm produces smooth changes

cases B ak a. If, however, %B (sk ) = 1, state sk (i.e., unknown
state [line 14]). unknown states, action ak performed suggested baseline
behavior defines safe behavior (line 15). new case < sk , ak , 0 > built
added list cases episode actions performed using
agent known state. Finally, reward obtained episode accumulated,
r(sk , ak ) immediate reward obtained action ak performed state sk
(line 18).
- (c) Computing state-value function unknown states. step,
state-value function states considered unknown previous step
computed. previous step (line 17), state-value function states set
0. algorithm proceeds manner similar first-visit MC algorithm Figure 7.
case, return unknown state si computed, averaged since
one episode considered (line 24 25). return si computed, taking
account first visit state si episode (each occurrence state episode
called visit si ), although state si could appear multiple times rest
episode.
- (d) Updating cases B using experience gathered. Updates B
made cases gathered episodes cumulative reward similar
best episode found point using threshold (line 27). way, good sequences
provided updates since shown sequences experiences
cause adaptive agent converge stable useful policy, whereas bad sequences may
cause agent converge unstable bad policy (Wyatt, 1997). prevents
degradation initial performance B computed first step algorithm
use bad episodes, episodes errors, updates. step, two types
updates appear, namely, replacements additions new cases. Again, algorithm
iterates case ci = (si , ai , V (si )) listCasesEpisode (line 29). si known state
(line 30), compute case < si , a, V (si ) > B corresponding state si (line 31).
One note case ci = (si , ai , V (si )) listCasesEpisode built line 13
algorithm, replacing action corresponding case < si , a, V (si ) > B
new action ai resulting application random Gaussian noise action
529

fiGarca & Fernandez

Policy Improvement Algorithm
00
01
02
03
04
05
06
07
08
09

Given case-base B, maximum number cases
Given baseline behavior
Given update threshold
1. Set maxT otalRwEpisode = 0, maximum cumulative reward reached episode
2. Repeat
(a) Initialization step:
set k = 0, listCasesEpisode , totalRwEpisode = 0
(b) Case generation:
k < maxEpisodeLength
Compute case < s, a, V (s) > B closest current state sk


10
11
12
13

%B (sk ) = 0 // known state
Chose action ak using equation 4
Perform action ak
Create new instance cnew := (s, ak , V (s))

14
15
16
17
18
19

else // unknown state
Chose action ak using
Perform action ak
Create new instance cnew := (sk , ak , 0)
totalRwEpisode := totalRwEpisode + r(sk , ak )
listCasesEpisode := listCasesEpisode cnew

20
21
22
23
24
25
26
27
28
29

Set k = k + 1
(c) Computing state-value function unknown states:
instance ci listCasesEpisode


%B (si ) = 1 // unknown state
P
return(si ) := kj=n jn r(sj , aj ) // n first ocurrence si episode
V (si ) := return(si )
(d) Updating cases B using experience gathered :
totalRwEpisode > (maxT otalRwEpisode )
maxT otalRwEpisode := max (maxT otalRwEpisode, totalRwEpisode)
case ci =< si , ai , V (si ) > listCasesEpisode


30
31
32

%B (si ) = 0 // known state
Compute case < si , a, V (si ) > B corresponding state si
Compute = r(si , ai ) + V (si+1 ) V (si )

33
34
35
36
37

> 0
Replace case < si , a, V (si ) > B case < si , ai , V (si ) > listCasesEpisode
V (si ) = V (si ) +
else // unknown state
B := B ci

38
39
40
41

kBk >
Remove kBk least-frequently-used cases B
stop criterion becomes true
3. Return B

Figure 8: Description step two PI-SRL algorithm.

Equation 4. Then, temporal distance (TD) error computed (line 32). > 0,
performing action ai results positive change value state. action,
530

fiSafe Exploration State Action Spaces Reinforcement Learning

turn, could potentially lead higher return and, thus, better policy. Van Hasselt
Wiering (2007) update value function using actions potentially lead
higher return. TD error positive, ai considered good selection
reinforced. algorithm, reinforcement carried updating output
case < si , a, V (si ) > B ai (line 34). Therefore, update case-base occurs
TD error positive. similar linear reward-inaction update learning
automata (Narendra & Thathachar, 1974, 1989) sign TD error used
measure success. PI-SRL updates case-base actual improvements
observed, thus avoiding slow learning plateaus value space
TD errors small. shown empirically procedure result
better policies step size depends size TD error (Van Hasselt &
Wiering, 2007). important note replacements produce smooth changes
case-base B since action replaced ai results higher V (si ) ai a.
form updating understood risk-seeking approach, overweighting
transitions successor states promise above-average return (Mihatsch & Neuneier,
2002). Additionally, prevents degradation B, ensuring replacements made
action potentially lead higher V (si ).
If, instead, si known state, case ci added B (line 37). Finally,
algorithm removes cases B necessary (line 39). Complex scoring metrics calculate
cases removed given moment proposed several authors.
Forbes Andres (2002) suggest removal cases contribute least overall
approximation, Driessens Ramon (2003) pursue error-oriented view
propose deletion cases contribute prediction error examples.
principal drawback sophisticated measures complexity.
determination case(s) removed involves computation score value
ci B, turn requires least one retrieval regression, respectively,
cj B (j 6= i). entire repeated sweeps case-base entail enormous
computational load. Gabel Riedmiller (2005) compute different score metric
ci B, requiring computation set k-nearest neighbors around ci .
approaches well-suited systems learning adjusted time requirements
high-dimensional state space, requiring use larger case-bases
proposed here. Rather, paper, propose removal least-frequently-used
cases. idea seems intuitive insofar least-frequently-used cases usually contain
worse estimates corresponding states value; although strategy might lead
function approximator forgets valuable experience made past
(e.g., corner cases). Despite this, PI-SRL performs successfully domains proposed
using strategy, demonstrated Section 4. Thus, ability forget ineffectual
known states described Section 2 result algorithm removing kBk cases
least-frequently-used cases B.
3.3 Parameter Setting Design
One main difficulties applying PI-SRL algorithm given problem
decide appropriate set parameter values threshold , risk parameter ,
update threshold maximum number cases . incorrect value
531

fiGarca & Fernandez

parameter lead mislabeling state known really unknown, potentially
leading damage injury agent. case risk parameter , high values
continuously result damage injury; low values safe, allow
exploration state-action space sufficient reaching near-optimal policy. Unlike
, parameter related risk, instead directly related
performance algorithm. Parameter used determine good episode
must respect best episode obtained, since best episodes used
update case-base B. value large, bad episodes may used update B
(influencing convergence performance algorithm). If, instead, low,
number updates B may insufficient improving baseline behavior. Finally,
high value allows large case-bases, increasing computational effort
retrieval degrading efficiency system. contrast, low value might
excessively restrict size case-base thus negatively affect final performance
algorithm. subsection, solid perspective given automatic definition
parameters. parameter setting proposed taken suitable set
heuristics tested successfully wide variety domains (Section 4).
Parameter : parameter domain-dependent related average size
actions. paper, value parameter established
computing mean distance states execution baseline
behavior . Expressed another way, execution policy provides
state-action sequence form s1 a1 s2 a2 . . . sn . Thus, value
computed using Equation 5.

=

dist(s1 , s2 ) + . . . + dist(sn1 , sn )
n1

(5)

Parameter : Several authors agree impossible completely avoid
accidents (Moldovan & Abbeel, 2012; Geibel & Wysotzki, 2005). important
note PI-SRL completely safe first step algorithm executed.
However, proceeding way, performance algorithm heavily
limited abilities baseline behavior. running subsequent
exploratory process inevitable learner performance improved beyond
baseline behavior. Since agent operates state incomplete knowledge
domain dynamic, inevitable exploratory process
unknown regions state space visited agent may reach error
state. However, possible adjust risk parameter determine level
risk assumed exploratory process. paper, start low
values (low risk) gradually increase. Specifically, propose beginning
= 9 107 increasing value iteratively either accurate policy
obtained amount damage injury high.
Parameter : value parameter set relative best episode obtained.
paper, value set 5% cumulative reward best episode
obtained.
532

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 9: Trajectories generated baseline policy deterministic, slightly
stochastic highly stochastic domain.
Parameter : Previously, estimated maximum number cases stored
case-base estimated maximum number cases required properly mimic baseline behavior . follows description value
computed. Figure 9 presents trajectories (sequences states) followed baseline policy three different domains: deterministic, slightly stochastic highly
stochastic. domain, different sequences states produced
represented {s00 , s01 , s02 , . . . , s0n }, {s00 , s11 , s12 , . . . , s1n },. . ., {s00 , sm1 , sm2 , . . . , smn },
sji i-th state, s00 initial state sjn final state resulting
trajectory episode j. deterministic domain, different executions
always result trajectory. case, set maximum number
cases = n cases computed episode stored.
slightly stochastic domain, trajectories produced different episodes
different, slightly so. Here, suppose case-base beginning
empty. Additionally, assume states {s00 , s01 , s02 , . . . , s0n } corresponding first trajectory produced domain stored case-base.
Furthermore, domain execute different episodes, obtaining different
trajectories. Following execution episodes, compute maximum distance i-th state first trajectory (previously added case-base)
i-th state produced trajectory j max1jm d(s0i , sji ).
slightly stochastic domain, maximum distance exceed threshold
case max1jm d(s0i , sji ) < . point, assume i-th state
trajectory j least one neighbor distance less (corresponding
state s0i ). Thus, i-th state j added case-base.
contrast, highly stochastic domain, maximum distance greatly exceeds
threshold cases max1jm d(s0i , sji ) >> . domain,
estimate total number cases added case-base following
533

fiGarca & Fernandez

way. i-th state sequence
j first trajectory,k estimate number
max1jm d(s0i ,sji )
cases added case-base
or, words,

compute number intervals range [0, max1jm d(s0i , sji )] width
(the threshold used decide whether new case added casebase). Consequently, estimated number cases addedjto case-base, taking

k
Pn
max1jm d(s0i ,sji )
account states sequence, computed i=0
. Finally,

estimated maximum number cases computed shown Equation 6.

=n+


n
X
max1jm d(s0i , sji )


i=0

!
(6)

important remember deterministic domain, summation equation 6 equal 0 that, therefore, = n. increase value element
related increase stochasticity environment, insofar greater
stochasticity environment increases number cases required. Finally,
number cases large nearly infinite, threshold increased
make restrictive addition new cases case-base. However,
increase may adversely affect final performance algorithm.

4. Experimental Results
section presents experimental results collected use PI-SRL policy
learning four different domains presented order increasing complexity (i.e., increasing number variables describing states actions): car parking problem (Lee &
Lee, 2008), pole-balancing (Sutton & Barto, 1998), helicopter hovering (Ng et al., 2003)
business simulator SIMBA (Borrajo et al., 2010). domains,
proposed learning near-optimal policy minimizes car accidents,
pole disequilibrium, helicopter crashes company bankruptcies, respectively,
learning phase. four domains stochastic experimentation.
helicopter hovering business simulator SIMBA are, themselves, stochastic and,
additionally, generalized domains, made car parking pole-balancing domains stochastic intentional addition random Gaussian noise actions
reward function. results PI-SRL four domains compared yielded
two additional techniques, namely, evolutionary RL approach selected winner
helicopter domain 2009 RL Competition (Martn H. & Lope, 2009) Geibel
Wysotzkis risk-sensitive RL approach (Geibel & Wysotzki, 2005). evolutionary approach, several neural networks cloning error-free teacher policies added initial
population (guaranteeing rapid convergence algorithm near-optimal policy and,
indirectly, minimizing agent damage injury). Indeed, winner helicopter
domain agent highest cumulative reward, winner must indirectly
minimize helicopter crashes insofar incur large catastrophic negative rewards.
hand, risk-sensitive approach defines risk probability (s) reaching
terminal error state (e.g., helicopter crash ending agent control), starting initial
534

fiSafe Exploration State Action Spaces Reinforcement Learning

state s. case, new value function weighted sum risk probability,
, value function, V , used (Equation 7).
V (s) = V (s) (s)

(7)

parameter 0 determines influence V (s)-values compared (s)values. = 0, V corresponds computation minimum risk policies. large
values, original value function multiplied dominates weighted criterion.
Geibel Wysotzki (2005) consider finite (discretized) action sets study,
algorithm adapted continuous action sets. use CBR value
risk function approximation Gaussian exploration around current action.
experiments, domain, three different values used, modifying influence
V -values compared -values. cases, goal improve control
policy while, time, minimizing number episodes agent damage
injury. domain, establish different risk levels modifying risk parameter
values according procedure described subsection 3.3. important note
one baseline behavior used initialize evolutionary RL approach exactly
used subsequently first second step PI-SRL. Furthermore, case-base
risk-sensitive approach begin scratch since initialized safe
. makes comparison performances fair possible,
case-based policy B
taking account different techniques make use baseline behaviors.
4.1 Car Parking Problem
car parking problem represented Figure 10 originates RL literature (Cichosz, 1996). car, represented rectangle Figure 10, initially located
inside bounded area, represented dark solid lines, referred driving area.
goal learning agent navigate car initial position garage,
car entirely inside, minimum number steps. car cannot move
outside driving area. Figure 10 (b) shows two possible paths car take
starting point garage obstacle order correctly perform
task. consider optimal policy domain reaches goal
state shortest time which, time, free failures.
state space domain described three continuous variables, namely,
coordinates center car xt yt angle cars axis
X coordinate system. car modeled essentially two control
inputs, speed v steering angle , let us suppose car controlled
steering angle (i.e., moves constant speed). Thus, action space described one
continuous variable [1, 1] corresponding turn radius, used equations
below. agent receives positive reward value r = (1 (dist(Pt , Pg ))) 10,
Pt = (xt , yt ) center car, Pg = (xg , yg ) center garage (i.e., goal
position) normalizing function scaling Euclidean distance dist(Pt , Pg )
Pt Pg range [0, 1] car inside garage (i.e., reward value greater
car parked correctly center garage). agent receives reward
-1 whenever hits wall obstacle. steps receive reward -0.1. Thus,
difficulty problem lies reinforcement delay, fact
535

fiGarca & Fernandez

Figure 10: Car Parking Problem: (a) Model car parking problem. (b) Examples
trajectories generated agent park car garage.
punishments much frequent positive rewards (i.e., much easier hit
wall park car correctly). motion car described following
equations (Lee & Lee, 2008)
t+1 = + v /(l/2) tan( ),

(8)

xt+1 = xt + v cos(t+1 ),

(9)

yt+1 = yt + v sin(t+1 ),

(10)

v linear velocity car (assumed constant value),
maximum steering angle (i.e., car change position maximum angle
directions) simulation time step. Gaussian noise added
actions rewards standard deviation 0.1, since noisy interactions inevitable
real-world applications. Adding noise actuators environment,
transform deterministic domain stochastic domain. important note
noise added transform domain stochastic domain independent
Gaussian noise standard deviation (risk parameter) used explore state
action space second step PI-SRL algorithm. case, Gaussian noise
standard deviation used exploration added noise previously added
actuators. paper, l = 4 (m), v = 1.0 (m/s), = 0.78 (rad) = 0.5 (s)
(the driving area obstacle dimensions detailed Figure 10 [a]). initial position
car fixed xs = 4.0, ys = 4.0 = 0.26 (rad), goal position
xg = 22.5 yg = 13.5. domain, designed baseline behavior
average cumulative reward per trial 4.75.
order perform PI-SRL algorithm, modeling baseline behavior step exe learned demonstrations
cuted. result step safe case-based policy B
provided baseline behavior (see subsection 3.1). computed following
procedure described subsection 3.3 resulting values 0.01 207, respectively.
536

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 11: Car Parking Task Modeling Baseline Behavior Step: (a) Number steps per
trial executed Case Base B baseline behavior . (b) Cumulative
reward per trial baseline behavior , learned Safe Case Based Policy
IBL approach.
B
Figure 11 (a) graphically represents execution modeling baseline behavior step.
it, two different learning processes presented and, one, number steps per
trial executed baseline behavior (continuous red lines) cases B (dashed
green lines) shown. beginning learning process empty case-base B,
steps performed using baseline behavior . learning process continues,
learned. around trials
new cases added B safe case-based policy B
40-50, practically steps performed using cases B rarely used,
means safe case-based policy learned. two learning processes shown
Figure 11 (a), modeling baseline behavior step performed without collisions
wall obstacle. words, baseline behavior cloned safely without
errors. Figure 11 (b) shows cumulative reward three different execution processes:
first (continuous red lines) corresponding performance baseline behavior
, second (dashed green lines) corresponding previously-learned safe case-based
(derived B ) third (dashed blue lines) corresponding instancepolicy B
based learning (IBL) approach consisting storing cases memory. IBL approach,
new items classified examining cases stored memory determining
similar case(s) given particular similarity metric (Euclidean distance used paper). classification nearest neighbor (or nearest neighbors) taken
classification new item using 1-nearest neighbor strategy (Aha & Kibler, 1991).
approach, two different executions carried out. IBL approach, training
process performed saving training cases produced baseline behavior
50 trials (so consider approach IB1 algorithm sense saves every
case training phase, see Aha & Kibler, 1991). Figure 11 (b) shows safe
case-based policy B almost perfectly mimics behavior baseline behavior .
domain, performance IB1 approach similar.
Figure 12 (a) shows results different risk configurations obtained improving
learned baseline behavior step. risk configuration, two different learning pro537

fiGarca & Fernandez

Figure 12: Improving learned baseline behavior step car parking problem: (a) Cumulative reward per episode different risk configurations () obtained
PI-SRL. (b) Cumulative reward per episode evolutionary RL risksensitive RL approaches. cases, episode ending failure marked.
cesses performed. trials ending failure (car hits wall obstacle) marked
(blue triangles). learning processes Figure 12 (a) demonstrate number
failures increases increase parameter . low level risk ( = 9 104 ),
although failures produced, performance nevertheless weak (around baseline behavior ) constant throughout whole learning process. Additional
experiments demonstrated increasing value = 9102 increases
number failures without improving performance. Figure 12 (b) shows results
evolutionary risk-sensitive RL approaches different values. Regarding former,
number failures higher obtained PI-SRL approach, final
performance similar. case latter, performance higher = 1.0 (value
maximization), yet agent consistently crashes car wall.
Figure 13 shows mean number failures (i.e., car collisions) cumulative reward
approach 500 trials red circles corresponding PI-SRL algorithm,
black triangles risk-sensitive approach blue square evolutionary
RL approach. Additionally, Figure 13 shows two asymptotes. horizontal asymptote
established according cumulative reward obtained highest value.
horizontal asymptote indicates higher values increase number failures without
improving cumulative reward (which may, fact, get worse). vertical asymptote
F ailures = 0 indicates reducing risk parameter reduce number
failures. Figure 13 shows performance two additional risk levels,
high level risk ( = 9 101 ) low level risk ( = 0), respect
Figure 12. using low level risk = 0, additional random Gaussian
noise added actions algorithm free failures, although performance
learned first step
improve respect safe case-based policy B
algorithm. PI-SRL medium level risk ( = 9 104 ) free
failures, yet performance slightly improved. PI-SRL algorithm high level
risk ( = 9 102 ) obtains highest cumulative reward, 3053.37, mean
538

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 13: Mean number failures (car collisions) cumulative reward 500 trials
approach car parking task. means computed 10
different executions.

78.8 failures. However, using high level risk ( = 9 101 ), number
failures greatly increases and, consequently, cumulative reward decreases. shown
Figure 12, PI-SRL high risk ( = 9 102 ) evolutionary RL approach obtain
similar performance, PI-SRL demonstrates faster convergence (thus, Figure 13,
cumulative reward obtained PI-SRL higher). Pareto comparison criterion
used compare solutions Figure 13. Using principle, one solution strictly
dominates (or preferred to) solution parameter strictly worse
corresponding parameter least one parameter strictly better.
written y, indicating strictly dominates y. accordance Pareto
principle, assume points Figure 13 corresponding PI-SRL solutions,
save PI-SRL high level risk, Pareto frontier, since points
strictly dominated solution (i.e., solution has, time,
higher cumulative reward lower number failures PI-SRL). domain,
solution PI-SRL medium level risk strictly dominates (or preferred to)
risk-sensitive solutions (PI-SRL = 9 103 risk-sensitive) solution PI-SRL
high level risk strictly dominates solution evolutionary RL solution
(PI-SRL = 9 102 evolutionary RL).
Nevertheless, important note ultimate decision approach
Figure 13 best depends criteria researcher. If, instance, minimization number failures deemed important optimization criterion
(independently improvement obtained respect baseline behavior ),
best approach PI-SRL low level risk ( = 9 104 ). Similarly,
maximization cumulative reward instead judged important optimization criterion (independently number failures generated), best approach
PI-SRL high level risk ( = 9 102 ).
Figure 14 shows evolution cases case-base B (known space) different
trials high-risk learning process. graph presents set known states (green
539

fiGarca & Fernandez

Figure 14: Car parking problem: Evolution known space different trials = 0
(a), = 50 (b), = 100 (c) = 200 (d) high-risk learning process
( = 9 102 ). graph corresponds situation state space
accordance case-base B trial .
area), error states (red area), unknown states (yellow area) non-error states
(orange circles). PI-SRL adapts known space order find safer better policies
complete task. Figure 14 (a) shows initial situation B (corresponding
). robust sense never results
previously-learned safe case-based policy B
collisions, suboptimal (it selects longest parking path driving around upper
side obstacle). learning process progresses (Figure 14 (b)), PI-SRL finds
shorter path park car garage along upper side obstacle (increasing
performance), comes closer obstacle (increasing probability
collisions). Figure 14 (c), PI-SRL finds new even shorter path, time along
lower side obstacle. However, still cases case-base B corresponding
older path along upper side obstacle (so Figure 14 (c) indicates two paths
park car). Finally, Figure 14 (d), cases corresponding suboptimal path
along upper side obstacle removed B replaced new cases
corresponding safe improved path along lower side obstacle.
words, PI-SRL adapts known space exploration unknown space
order find new improved behaviors. process adjusting known space
540

fiSafe Exploration State Action Spaces Reinforcement Learning

safe better policies, algorithm forgets previously-learned, yet ineffective
known states.
following experiment, becomes apparent domain noisy enough, even
taking risk (i.e., noise added actuator exploration),
agent could nevertheless perform poorly constantly produce collisions. experiment
serves explain domain noise never sufficient efficient exploration
space without action selection noise. experiment, intentionally added
noise actuators performed second step PI-SRL again, however
time taking risk (i.e., = 0). test, added random Gaussian noise
standard deviation 0.3, rather standard deviation 0.1 used previously,
actuators. Figure 15 shows two executions second step (improving learned
baseline policy) PI-SRL algorithm x-axis indicating number trials,
y-axis cumulative reward per episode failures (i.e., collisions) marked blue
triangles. experiments Figure 12 (b), case-based policy B low level
risk ( = 9 104 ) never produces failures. contrast, experiments shown
Figure 15, case-based policy B continually collides wall although
risk parameter set 0 ( = 0). Furthermore, increase performance
detected.

Figure 15: Improving learned baseline behavior step car parking task: Two learning processes risk configuration = 0 increase noise
actuators.

increase noise actuators second step algorithm respect
first step (the case-based policy B learned first step using Gaussian random
noise actuator standard deviation 0.1, second step performed
using Gaussian random noise actuator standard deviation 0.3) takes
agent beyond known space case-base B learnt first step PI-SRL
allows find new trajectories parking car garage. new situation,
exploration process guided follows. known state reached, agent performs
action retrieved B without addition Gaussian noise, since risk parameter
= 0 (see line 11 Figure 8 algorithm). unknown state reached, agent performs
541

fiGarca & Fernandez

action advised baseline behavior (see line 15). Using exploration
process, new better trajectory found parking car garage,
resulting cases episode corresponding unknown states added case-base
(see line 37), slightly improving performance Figure 15. important note
replacements cases (see line 34) change actions B, since
replaced action previously retrieved B plus certain amount Gaussian
noise standard deviation (see line 11). Nevertheless, given risk parameter
set 0, actions retrieved case-base replaced. exploration
process, however, = 0 (i.e., taking risk) lead optimal behavior since:
actions performed unknown situations added case-base B performed using baseline behavior supposed perform suboptimal actions
(see definition baseline behavior).
actions cases B replaced improved actions. Gaussian
noise standard deviation used explore different better actions
provided B ; however, case, risk parameter set = 0
new better actions discovered.
Additional experiments demonstrate PI-SRL behaves much worse higher
value noise used actuators (with collisions episodes). assume taking
risk (i.e., = 0) implies always performing actions discovering
newer better actions provided learned case-base B baseline
behavior . PI-SRL, replacements case-base executed towards
promising action which, case, guarantees higher return.
exploration necessary order obtain (near-)optimal behavior, since without
exploration, new better actions discovered PI-SRL performance limited
case-based policy learned first step B baseline behavior
which, one must remember, intended perform suboptimal policies.
4.2 Pole-Balancing
name suggests, objective pole-balancing problem balance pole
vertically top moving cart (Sutton & Barto, 1998). state description consists
four-dimensional vector containing angle , radial speed 0 , cart position
x speed x0 . action consists real-valued force used push cart.
study, reward computed encourage actions keep pole upright
possible cart cart centered possible track. Thus, reward
step computed rt = 1 ((t ) + (xt ))/2, normalizing functions
scaling angle position xt range [0, 1]. episode composed 10,000
steps, although may nevertheless end prematurely pole becomes unbalanced (i.e.,
inclination twelve degrees either direction) cart falls
track (i.e., 2.4m center track),
considered failures. car parking problem, Gaussian noise added actions
rewards, time standard deviation 104 . pole-balancing domain
becomes stochastic addition noise actuators reward function.
542

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 16: Modeling baseline behavior step pole-balancing task: (a) Number steps per
trial executed case-base B baseline behavior . (b) Cumulative reward
IBL approach.
per trial , learned safe case-based policy B
hand-made baseline behavior demonstrates execution safe, yet suboptimal
policy, average cumulative reward per episode/trial 9292.
learnt
modeling baseline behavior step PI-SRL, safe case-based policy B
demonstrations provided baseline behavior . computed following
procedure described subsection 3.3, values 0.02 12572, respectively.
Figure 16 (a) shows two different learning processes modeling baseline behavior step.
learning process, Figure 16 (a) shows number steps per trial executed
baseline behavior (continuous red lines) case-base B (dashed green lines).
beginning learning process, case-base B empty steps performed
using baseline behavior . learning process progresses, however, B filled
learnt. end learning process (after around
safe case-based policy B
45-50 trials), almost steps performed using cases B rarely used.
important note modeling baseline behavior step performed without
failures (i.e., pole disequilibrium cart track) case. previous
task, Figure 16 (b) represents three independent execution processes using previously (derived B indicated dashed green lines),
learned safe case-based policy B
baseline behavior (indicated continuous red lines) approach based
IBL (indicated dashed blue lines) (Aha & Kibler, 1991). average cumulative
9230 (Figure 16 [b]). almost perfectly clones ,
reward per episode B

B
IB1 approach which, cases, results pole disequilibrium cart falling
track averages cumulative reward per episode 8055.
Figure 17 (a) shows results PI-SRL different risk configurations.
configuration, learning curves shown two different learning processes performed.
Additionally, episode ending failure marked (blue triangles). increase
risk increases probability failure, policy obtained nevertheless better terms
cumulative reward. Nevertheless, much greater risk values ( = 9 105 ) produce
failures without accompanying increase cumulative reward. Figure 17 (b) shows
results evolutionary risk-sensitive RL approaches, former
543

fiGarca & Fernandez

Figure 17: Improving learned baseline behavior step pole-balancing task: (a) Cumulative reward per episode different risk configurations () obtained PISRL. (b) Cumulative reward per episode obtained evolutionary risksensitive RL approaches. cases, episode ending failure marked.

clearly algorithm greatest number failures. risk-sensitive approach,
= 2.0 (value maximization), agent selects actions result higher value,
higher risk. contrary, = 0 (risk minimization), agent
learns risk function (at around episode 6000), selects actions lower risk (and
lower number failures), considerably weak performance. value = 0.1
produces intermediate policy. Consequently, concluded PI-SRL high
level risk obtains better policies less failures evolutionary risk-sensitive
RL approaches. Figure 18 reinforces previous conclusions.

Figure 18: Mean number failures (pole disequilibrium cart track) cumulative reward 500 trials approach pole-balancing task.
means computed 10 different executions.
544

fiSafe Exploration State Action Spaces Reinforcement Learning

it, mean number failures cumulative reward 12,000 trials shown,
red circles corresponding PI-SRL, black triangles corresponding risksensitive approach blue square corresponding evolutionary RL approach.
figure shows performance two additional risk levels, high level risk
( = 9 104 ) low level risk ( = 0), respect Figure 17. cumulative
reward number failures increase high level risk ( = 9 105 ).
risk level represents inflection point higher levels risk produce failures
without accompanying improvement cumulative reward. fact, high level
risk ( = 9 104 ) results reduction cumulative reward compared
high level risk ( = 9 105 ). Again, Pareto comparison criterion may used
compare solutions Figure 18. domain, solution PI-SRL
low level risk strictly dominates risk-sensitive solutions = 0.0 = 0.1,
PI-SRL = 9 107 risk-sensitive = 0.0 = 0.1. Additionally,
solution PI-SRL high level risk strictly dominates evolutionary RL solution,
PI-SRL = 9 105 evolutionary RL.
Lastly, Figure 19 shows evolution known space derived case-base
B different trials high-risk learning process. graph, error states (red
area), set unknown states (yellow area), set known states (green area)
set non-error states (orange circles) represented. known space
graph computed taking cases B trials = 0, 3000, 6000 8000.
graph, non-error states computed 10 different executions B
trial (the orange circles representing terminal states executions).
first graph (Figure 19 [a]) presents initial known space resulting modeling
baseline behavior step. evolution Figure 19 demonstrates two different points. First,
PI-SRL progressively adapts known space order encounter better behavior
known space tends compressed toward center coordinates.
due fact reward greater angle pole cart
position x 0 (i.e., pole upright possible cart cart centered
track). Second, risk failure pole-balancing domain greater
early trials learning process. beginning learning process (Figure 19 [a]),
= 0), regions known space close error space. situation,
slight modifications actions consistently produce visits states (i.e., pole
disequilibrium cart falling track). learning process advances (Figure 19
[b], [c] [d]), known space compressed toward origin coordinates away
error space. Consequently, probability visiting error states decreases.
example, returning Figure 17 (a), high-risk learning processes, 52% failures
(126) occur first 4000 trials, remaining 48% (117) occur last 8000
trials.
4.3 Helicopter Hovering
suggested name, objective domain make helicopter hover close
possible defined position duration established episode. task challenging two main reasons. Firstly, state action spaces high-dimensional
continuous (more specifically, state space 12-dimensional action space
545

fiGarca & Fernandez

Figure 19: Pole-balancing task: Evolution known space different trials = 0 (a),
= 3000 (b), = 6000 (c) = 8000 (d) high-risk learning process
( = 9 105 ). graph corresponds situation state space
according case-base B trial .
4-dimensional). Secondly, generalized domain whose behavior modified wind
factor. helicopter episode composed 6000 steps, although may end prematurely
helicopter crashes. first step PI-SRL performed order imitate baseline
behavior . computed following procedure described subection 3.3
values 0.3 49735, respectively. step performed, resulting
able properly imitate baseline behavior .
safe case-based policy B

Figure 20 (a) shows two learning processes modeling baseline behavior step.
Similar previous tasks, learning processes progress, number steps executed
baseline behavior reduced number steps using case-base B
increases. end learning process, case-base B stores safe case-based
. Figure 20 (b) compares performance (in terms cumulative reward per
policy B
IB1 approach. Regarding
episode) , learned case-based policy B
first two, average cumulative reward per episode -78035.93, obtained
-85130.11. Although perfectly mimic baseline behavior ,
B

B
546

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 20: Modeling baseline behavior step helicopter hovering task: (a) Number
steps per trial executed case-base B baseline behavior . (b) Cumula IBL
tive reward per trial , learned safe case-based policy B
approach.
nevertheless performs safe policy without crashing helicopter. regard
training process IB1 approach, every case produced 15 episodes baseline
behavior stored. Figure 20 (b) demonstrates IB1 approach consistently results
helicopter crashes, performance extremely far learned safe case . Improvement policy begins state-action space safely
based policy B
B
explored execution step two PI-SRL.
Figure 21 (a) shows results different risk levels. PI-SRL low medium
levels risk levels produce helicopter crashes PI-SRL, performance nevertheless
quite weak.

Figure 21: Improving learned baseline behavior step helicopter hovering task: (a)
Cumulative reward per episode different risk configurations obtained PISRL. (b) Cumulative reward per episode obtained evolutionary risksensitive RL approaches. cases, episode ending failure marked.
547

fiGarca & Fernandez

Conversely, high level risk established produces near-optimal policy
low number collisions. Extensive experimentation demonstrates increasing risk
parameter = 9 103 increases number crashes without accompanying
improvement cumulative reward. Figure 21 (b) shows results evolutionary
RL approach which, remembered, selected winner RL Competition
2009 domain (Martn H. & Lope, 2009), well risk-sensitive RL algorithm
different values. comparison results evolutionary RL approach
PI-SRL shows similar cumulative reward, significantly higher number
crashes former latter. evolutionary approach, crashes
occur early steps learning process; PI-SRL, accidents occur
advanced steps learning process. case risk-sensitive RL algorithm,
= 0 = 0.01 risk function learned around episode 3000. point,
agent selects lower-risk actions number crashes considerably reduced.
= 0.4 agent selects actions resulting higher values without taking risk
account, performance improves, expense increased number accidents.
Nevertheless whatever value, number crashes higher performance
worse PI-SRL.

Figure 22: Mean number failures (helicopter crashes) cumulative reward 5000
episodes approach helicopter hovering task. means
computed 10 different executions.
information Figure 22, indicating mean number failures cumulative
reward 5000 episodes approach, complements conclusions made above.
data computed 10 independent executions approach.
previous domains, PI-SRL indicated red circles, risk-sensitive approach
black triangles evolutionary RL approach blue square. Figure 22 shows
performance two additional risk levels, high level risk ( = 9 102 )
low level risk ( = 0), respect Figure 21. Figure 22 demonstrates
evolutionary RL approach obtains highest cumulative reward (7.13 107 ),
followed closely PI-SRL (7.57 107 ). approaches far results.
Regarding number failures (i.e., helicopter crashes), PI-SRL low level
risk ( = 0), low level risk ( = 9 105 ) medium level risk ( = 9 104 )
548

fiSafe Exploration State Action Spaces Reinforcement Learning

produces collisions, PI-SRL algorithm medium risk preferable inasmuch
cumulative reward higher (18.01 107 ). Using Pareto comparison criterion,
PI-SRL solution high level risk strictly dominates solutions risksensitive approach (PI-SRL = 9 103 risk-sensitive). Moreover, PI-SRL strictly
dominated solution.

Figure 23: Evolution known space different episodes helicopter hovering
task. (a) Example representation single known state radar chart. (b),
(c), (d) Known states episodes = 0, = 500 = 4000, respectively,
high-risk learning process ( = 9 103 ). graph corresponds
situation known space according case-base B episode .
pole-balancing domain, Figure 23 shows evolution known space
according case-base B different episodes high-risk learning process.
case, radar charts used due high number features describing states.
radar chart graphical method displaying multivariate data two-dimensionally.
Figure, axis represents one features state and, preserve simplicity
representation, charts generated normalizing absolute values features
0 1. Figure 23 (a) example representation single known state.
549

fiGarca & Fernandez

value axis corresponds value individual feature state
line drawn connecting feature values axis. line Figure 23 (a)
represents single state, Figures 23 (b), (c) (d) show known space according
case-base B episodes 0, 500 4000, respectively. three charts represent
single state, rather states B corresponding episode. Thus,
graph, set known states marked (green area). state considered error state
single feature value state greater 1. limits (marked red line
graphs) computed taking account helicopter crashes (i)
velocity along main axes exceeds 5 m/s, (ii) position helicopter
20 m, (iii) angular rate around main axes exceeds 2 2 rad/s
(iv) orientation 30 degrees target orientation. previous
tasks, Figure 23 indicates two different matters. First, learning proceeds, known
space derived B adjusted space used better safer policies.
helicopter domain, agent tries hover helicopter close possible target
position (i.e., origin coordinates), since immediate rewards greater closer
helicopter hovers origin. Thus, known space starts expand (Figure 23
[b]) and, progressively, concentrated origin coordinates (Figure 23 [c] [d]).
regard second matter, probability crashing low since,
beginning, known space already appears concentrated origin far
error space (Figure 23 [b]). words, beginning, features
known space (i.e., forward, sideways downward velocities; x, y, z coordinates; x,
z angular-rates; x, z quaternation) far error space limits,
decreasing probability visiting error state.
previous experiments, second step PI-SRL performed using
initial case-base B free failures built first step algorithm.
following experiments show performance second step PI-SRL different
initial policies used. Figure 24 (a) shows performance policies used initial
policies. continuous black line indicates performance initial safe case-based
policy B , average cumulative reward per episode -85,130.11, used previous
experiments prior execution step two algorithm. remaining lines
Figure correspond performance three different initializations case-base B
used new experiments, prior execution step two algorithm. Using
poor initial policy (dashed green lines) helicopter crashed nearly
episodes, average cumulative reward per episode calculated -108,548.03.
Using different poor (albeit less poor) initial policy (continuous red lines)
helicopter crashed occasionally, average cumulative reward per episode -91,723.89.
Finally, near-optimal policy (dashed blue lines) whereby helicopter hovering free
failures yields average cumulative reward per episode -13,940.1.
Figure 24 (b) shows performance second step (improving baseline behavior
step) PI-SRL, starting case-base B corresponding poor, poor
near-optimal policies presented Figure 24 (a). Figure 24 (b), dashed blue lines
correspond use case-base B containing near-optimal policy, continuous
red lines correspond use case-base B containing poor policy dashed
green lines correspond use case-base B containing poor policy.
experiments Figure conducted using high level risk domain
550

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 24: (a) performance different initial policies helicopter hovering task.
(b) performance different executions second step PI-SRL,
starting case-base B containing policy three different types:
poor, poor near-optimal.

( = 9 103 ). graph indicates use near-optimal policy initial
policy high level risk level, case-base worsen performance which,
fact, appears improve slightly. second step PI-SRL prevents degradation
initial performance B, since updates cases case-base made using bad
episodes. words, updates B made cases gathered episodes
cumulative reward similar best episode found particular point
using threshold (whose value set 5% cumulative reward best episode).
example, cumulative reward best episode -13,940.1, episodes
cumulative reward higher -14,637 used update case-base (discarding
bad episodes episodes failures). way, good sequences experiences
provided updates, since proven good sequences experiences
cause adaptive agent converge stable useful policy, bad sequences
may cause agent converge unstable poor policy (Wyatt, 1997). solid red
lines Figure 24 (b) show using poor policy failures initial policy produces
higher number failures using initial policy free failures. However
despite poor initialization, PI-SRL nevertheless able learn near-optimal policy
well policy free failures used initialize B (see lines corresponding high
level risk, = 9 103 , Figure 21 (a)). Finally, dashed green lines Figure 24
(b) show use poor initial policy many failures results decreased
performance higher number failures produced, even though nevertheless able
learn better behavior. case, algorithm falls local minimum, probably
biased poor initialization. cases poor policies, number
failures higher beginning learning process decreases learning
process proceeds. poor poor initial policies close
error space, stark contrast initial policy shown Figure 23 which,
beginning, already appears concentrated origin, far error space.
551

fiGarca & Fernandez

learning process proceeds, different policies compressed away error
space number failures decreases.
4.4 SIMBA
Business simulators powerful tools improving management decision-making processes. example tool SIMulator Business Administration (SIMBA)
(Borrajo et al., 2010). SIMBA competitive simulator, since agents compete
agents management different virtual companies. simulator,
result twenty years experience university students business
executives, emulates business realities using variables, relationships events
present business world. objective provide users integrated vision
company, using basic techniques business management, simplifying complexity
emphasizing content principles greatest educational value (Borrajo et al.,
2010). experiments performed here, learning agent competes five handcoded agents (Borrajo et al., 2010). Decision-making SIMBA episodic task
decisions made sequentially. make business decision, state must studied
10 continuous decision variables (e.g., selling price, advertising expenses, etc.) must
set, followed study state composed 12 continuous variables (e.g., material
costs, financial expenses, economic productivity, etc.) (Borrajo et al., 2010). episode
composed 52 steps, although may prematurely company goes bankrupt (i.e.,
losses higher 10% net assets).

Figure 25: Modeling baseline behavior step SIMBA Task: (a) Number steps per trial
executed case-base B baseline behavior . (b) Cumulative reward per
IBL approach.
trial , learned safe case-based policy B
Figure 25 (a) shows evolution number steps executed baseline behavior case-base B two learning processes performing modeling baseline
behavior step. computed following procedure described subsection 3.3
values 1 102 513, respectively. episodes (approximately 25),
learned. Figure 25 (b) shows performance previouslysafe case-based policy B

learned B , IB1 approach. study, mean profits per episode
552

fiSafe Exploration State Action Spaces Reinforcement Learning

Figure 26: Improving learned baseline behavior step SIMBA task: (a) mean
profits per episode different risk configurations obtained PI-SRL agent
five hand-coded agents. (b) mean profits per episode obtained
evolutionary risk-sensitive RL agent five hand-coded agents.
cases, episode ending failure (bankruptcy) noted.
4.02 million Euros. IB1
5.24 million Euros, obtained B
approach, cases generated using baseline behavior 25 episodes stored.
experiments demonstrate SIMBA, contrast previous domains, storing cases sufficient obtaining safe policy performance similar using
modeling baseline behavior step (with mean profits per episode 3.98 million Euros).
learned, execute improving learned baseline
safe case-based policy B
behavior step.
Similar findings earlier tasks, Figure 26 (a) indicates low medium
levels risk produce bankruptcies, performance nevertheless weak. highest
level risk produces near-optimal policy low number number failures.
contrast, Figure 26 (b) presents results evolutionary risk-sensitive RL approaches, former clearly yields highest number failures.
risk-sensitive case, number bankruptcies cases insufficient learning risk function . comparative results Figure 26 show PI-SRL
= 9 101 obtains better policies less failures evolutionary risk-sensitive
RL approaches.
Figure 27 shows graphical representation different solutions domain.
shows mean number failures cumulative reward different approaches
100 episodes, data computed 10 independent executions approach.
Figure, red circles correspond PI-SRL algorithm, black triangles correspond
risk-sensitive approach blue square corresponds evolutionary RL approach.
Figure 27 shows performance two additional risk levels, high ( = 9 102 )
low ( = 0), respect Figure 26. experiments Figure 27 demonstrate
PI-SRL high level risk ( = 9 101 ) obtains highest cumulative reward,
6693.58. Additionally, PI-SRL low level risk ( = 0), low level risk
( = 9 101 ) medium level risk ( = 9 100 ) approaches lowest

553

fiGarca & Fernandez

Figure 27: Mean number failures (company bankruptcies) cumulative reward
100 episodes approach SIMBA task. means
computed 10 different executions.
mean number failures, 0.0. However, PI-SRL medium level risk preferred
inasmuch performance superior terms cumulative reward. PI-SRL
high level risk ( = 9 102 ) increases number failures obtains lower cumulative
reward compared PI-SRL high level risk. Using Pareto comparison
criterion, PI-SRL high level risk strictly dominates solutions (PI-SRL
= 9101 risk-sensitive PI-SRL = 9101 evolutionary RL), approach
strictly dominated solution.
Due difficulty representing high-dimensional state action space
SIMBA domain, graphs provided evolution known space.

5. Related Work
Reinforcement learning (RL) case-based reasoning (CBR) techniques combined literature different ways. work Bianchi et al. (2009), new approach
presented permitting use cases heuristics speed RL algorithms. Additionally, Sharma et al. (2007) use combination CBR RL (called CARL) achieve
transfer playing Game AI across variety scenarios MadRTS TM,
commercial Real Time Strategy game. CBR used state value function
approximation RL (Gabel & Riedmiller, 2005). However, present study is,
knowledge, first time CBR RL used conjunction safe exploration dangerous domains. field safe reinforcement learning, three principal
trends observed: (i) approaches based return variance, (ii) risk-sensitive
approaches based definition error states (iii) approaches using teachers.
5.1 Approaches Based Return Variance
literature, long known optimal policy optimal expected
return MDP quite sensitive parameter variations (even optimal policy may
554

fiSafe Exploration State Action Spaces Reinforcement Learning

perform badly cases due stochastic nature problem). mitigate
problem, agent try maximize return associated worst-case scenario,
even though case may highly
unlikely. Thus, trend, risk refers worst
P
r variance. example approach
outcomes return R =


t=0
worst-case control worst possible outcome R optimized (Coraluppi
& Marcus, 1999; Heger, 1994). worst case control strategies, optimality criterion
exclusively focused risk-avoiding policies. policy considered optimal
worst-case return superior. approach, however, restrictive inasmuch takes
rare scenarios fully account.
value return introduced Heger (1994) seen extension
worst case control MDPs. concept establishes returns R <
policy occur probability lower neglected. algorithm less
pessimistic pure worst case control, given extremely rare scenarios effect
policy. work Heger et al., idea weighting return risk, namely
expected value-variance criterion, introduced.
risk-sensitive control based use exponential utility functions, return R
transformed reflect subjective measure utility. Instead maximizing expected
value R, objective maximize U = 1 logE(eR ), parameter
R usual return. shown that, depending parameter , policies
high variance V (R) penalized ( < 0) enforced ( > 0). Instead, Neuneier
Mihatsch (2002) consider worst-case-outcomes policy, (i.e., risk related
variability return). study, authors demonstrate learning algorithm
interpolates risk-neutral worst-case criterion limiting
behavior exponential utility functions. noted approaches based
variability return worst possible outcomes suited problems
policy small variance produce large risk (Geibel & Wysotzki, 2005).
view risk present study, however, concerned variance
return worst possible outcome, instead fact processes generally
possess unsafe states avoided. Consequently, address different class
problems dealt approaches focusing variability return.
5.2 Risk-sensitive Approaches based Error States.
second trend approaches, concept risk based definition error
states fatal transitions. Thus, Geibel et al. (2005) , instance, establish risk
function probability entering error state. Instead, Hans et al (2008) consider
transition fatal corresponding reward less given threshold .
first case demonstrated Section 4, learned TD methods require
error states (i.e., car collisions, pole-balancing disequilibrium, helicopter crashes
company bankruptcies) visited repeatedly order approximate risk function
and, subsequently, avoid dangerous situations. second case, concept risk
joined reward. Moreover, mentioned studies either (i) assume
system dynamics known, (ii) tolerate undesirable states exploration
or, contrast paper, (iii) deal problems high-dimensional
continuous state-action spaces. Regarding latter, Geibel et al. write
555

fiGarca & Fernandez

approach extended continuous action sets (e.g., using actor-critic
method), give details may done entirely continuous
problems. Section 4, present approach solves problem.
5.3 Approaches Using Teachers
last trend approaches based use teachers three different ways:
(i) bootstrap learning algorithm (i.e., initialization procedure), (ii) derive
policy finite demonstration set (iii) guide exploration process.
5.3.1 Bootstrapping Learning Algorithm
work Driessens Szeroski (2004), bootstraping procedure used relational RL finite set demonstrations recorded human expert
later presented regression algorithm. allows regression algorithm build
partial Q-function later used guide exploration state space
using Boltzmann exploration strategy. Smart Kaelbling (2000) use examples,
training runs bootstrap Q-learning approach HEDGER algorithm.
initial knowledge bootstrapped Q-learning approach allows agent learn
effectively helps reduce time spent random actions. Teacher behaviors
used form population seeding neuroevolution approaches (Yao, 1999; Siebel &
Sommer, 2007). Evolutionary methods used optimize weights neural networks,
starting prototype network whose weights correspond teacher (or baseline
policy). Using technique, RL Competition helicopter hovering task winners Martin et
al. (2009) developed evolutionary RL algorithm several teachers provided
initial population. algorithm restricts crossover mutation operators, allowing
slight changes policies given teachers. Consequently, rapid convergence algorithm near-optimal policy ensured, indirect minimization
damage agent. However, teachers included initial population resulting
ad-hoc training regimen conducted competition. Consequently, proposed
approach seems somewhat ad-hoc easily generalizable arbitrary RL problems.
work Koppejan et al. (2009, 2011), neural networks evolved, beginning
one whose weights corresponds teacher behavior. approach
proven advantageous numerous applications evolutionary methods (Hernandez-Daz
et al., 2008; Koppejan & Whiteson, 2009), Koppejans algorithm nevertheless seems
somewhat ad-hoc designed specialized set environments.
5.3.2 Deriving Policy Finite Set Demonstrations
approaches falling category framed according field Learning
Demonstration (LfD) (Argall et al., 2009). Highlighting study Abbeel et al. (2010)
based apprenticeship learning, approach composed three distinct steps.
first, teacher demonstrates task learned state-action trajectories
teachers demonstration recorded. second step, state-action trajectories seen
point used learn dynamics model system. model, (near)optimal policy found using reinforcement learning (RL) algorithm. Finally,
policy obtained tested running real system. work Tang et
556

fiSafe Exploration State Action Spaces Reinforcement Learning

al. (2010), algorithm based apprenticeship learning presented automaticallygenerating trajectories difficult control tasks. proposal based learning
parameterized versions desired maneuvers multiple expert demonstrations. Despite
approachs potential strengths general interest, inherently linked
information provided demonstration dataset. result, learner performance
heavily limited quality teachers demonstrations.
5.3.3 Guiding Exploration Process
Driessens Szeroski (2004), context relational RL, use given teachers
policy, rather policy derived current Q-function hypothesis (which
informative early learning stages), selection actions. approach,
episodes performed teacher interleaved normal exploration episodes.
mixture teacher normal exploration make easier regression algorithm
distinguish beneficial poor actions. context LfD,
approaches include teacher advice (Argall et al., 2009). advice used improve
learner performance, offering information beyond provided demonstration
dataset. approach, following initial task demonstration teacher, agent
directly requests additional demonstration teacher different states
previously demonstrated states single action cannot selected
certainty (Chernova & Veloso, 2007, 2008).
works mentioned trend, explicit definition risk ever given.

6. Conclusions
work, PI-SRL, algorithm policy improvement safe reinforcement
learning high-risk tasks, described. main contributions algorithm
definitions novel case-based risk function baseline behavior safe exploration
state-action space. use case-based risk function presented possible
inasmuch policy stored case-base. represents clear advantage
approaches, e.g., evolutionary RL (Martn H. & Lope, 2009; Koppejan & Whiteson,
2011) extraction knowledge known space agent impossible
using weights neural-networks. Additionally, completely different notion
risk others found literature presented. According notion, risk
independent variance return reward function, require
identification error states learning risk functions. Rather, concept
risk described paper based distance known unknown
space and, therefore, domain-independent parameter (in sense, proposal allows
application parameter-setting method described subsection 3.3).
Koppejan et al. (2011) use function identify dangerous states, contrast
approach, definition function requires strong previous knowledge domain.
Furthermore, approaches risk found literature tackle problems
entirely continuous (Geibel & Wysotzki, 2005) report results
one continuous domain (Koppejan & Whiteson, 2011). Consequently, difficult know
certain approaches literature generalize easily arbitrary domains.
557

fiGarca & Fernandez

paper presents PI-SRL algorithm great detail demonstrates effectiveness four entirely different continuous domains: car parking problem, pole-balancing,
helicopter hovering business management (SIMBA). experiments presented
paper demonstrate different characteristics learning capabilities PI-SRL
algorithm.
(i) PI-SRL obtains higher quality solutions. experiments Section 4 demonstrate
that, save helicopter hovering task, PI-SRL obtains cases best cumulative
reward per episode least number failures. Additionally, using Pareto comparison criterion said that, save high risk configuration car parking
problem, approach strictly dominated approach.
(ii) PI-SRL adjusts initial known space safe better policies. initial known
space resulting first step PI-SRL, modeling baseline behavior, adjusted
improved second step algorithm, improving learned baseline behavior.
Additionally, experiments demonstrate adjustment process compress
known space away error space (e.g., pole-balancing domain, subsection 4.2,
helicopter hovering domain, subsection 4.3) or, occasions, require known
space move closer error space (e.g., car parking problem, subsection 4.1)
event better policies found there.
(iii) PI-SRL works well domains differently structured state-action spaces
value function vary sharply. Although car parking problem, polebalancing domain, helicopter hovering task business simulator represent
differently structured problems, experiments study nevertheless demonstrate
PI-SRL performs well each. Furthermore, even domains car parking
problem value function varies sharply due presence obstacle,
experimental results demonstrate PI-SRL nevertheless successfully handle
difficulty. However, impossible avoid failures known space edge
edge error states algorithm would often explore error states.
(iv) number failures depends distance known space
error space. experiments pole-balancing helicopter hovering domains demonstrate number failures depends close known space error
space. Due structure domains, improving learned baseline behavior
step algorithm tends concentrate known space origin coordinates
away error space. greater distance known space error
space, lower number failures. Additionally, helicopter hovering, known
space is, beginning, far error space (consequently, number failures low beginning). Therefore, initial distribution known space
learned baseline policy later influences number failures obtained
second step PI-SRL.
(v) PI-SRL completely safe first step algorithm executed. However,
proceeding way, algorithm performance would heavily limited
capabilities baseline behavior. learner performance improved beyond
performance baseline behavior, subsequent exploratory process
second step PI-SRL must carried out. Since complete knowledge domain
dynamic possessed, however, inevitable that, exploratory
558

fiSafe Exploration State Action Spaces Reinforcement Learning

process, unknown regions state space visited agent may reach error
states.
(vi) risk parameter allows user configure level risk assumed.
algorithm, user gradually increase value risk parameter order
obtain better policies, assuming greater likelihood damage learning
system.
(vii) PI-SRL performs successfully even poor initial policy failures used.
experiments Figure 24 helicopter hovering domain demonstrate PI-SRL
able learn near-optimal policy despite poor initialization, policy
free failures used initialize case-base B. However, Figure shows
poor initial policy many failures used, PI-SRL decreases performance
produces higher number failures, although better behavior still learnt.
case, algorithm falls local minimum, likely biased poor initialization.
follows, applicability method discussed, allowing reader
clearly understand scenarios proposed PI-SRL approach may applicable.
applicability restricted domains following characteristics.
(i) mandatory scenario satisfy two assumptions described Section 2.
According first assumption, nearby states domain must necessarily similar actions. According other, similar actions similar states produce similar
effects. fact similar actions lead similar states assumes degree smoothness dynamic behavior system which, certain environments, may hold.
However, clearly explain Section 2, consider assumptions logical
assumptions derived generalization principles RL literature (Kaelbling et al.,
1996; Jiang, 2004).
(ii) applicability method limited size case-base B required
mimic baseline behavior. possible apply proposed approach tasks when,
first step PI-SRL algorithm, modeling baseline behavior, prohibitively large
number cases required properly mimic complex baseline behaviors. case,
threshold increased restrict addition new cases casebase. However, increase may adversely affect final performance algorithm.
Nevertheless, experiments performed Section 4 demonstrate relatively simple
baseline behaviors mimicked almost perfectly using manageable number cases.
(iii) PI-SRL algorithm requires presence baseline behavior. proposed
method requires presence baseline behavior safely demonstrates task
learned. baseline behavior conducted human teacher hand-coded
agent. important note, nevertheless, presence baseline behavior
guaranteed domains.
Finally, logical continuation present study would take account automatic
graduation risk parameter along learning process. example, would
particularly interesting exploit fact known space far away error
space order increase risk parameter or, contrary, reduce
close. future work aims deploy algorithm real environments, inasmuch
uncertainty real environments presents biggest challenge autonomous
robots. Autonomous robotic controllers must deal large number factors
robotic mechanical system electrical characteristics, well environmental
559

fiGarca & Fernandez

complexity. However, use PI-SRL algorithm (or risk-sensitive approaches)
learning processes real environments could reduce amount damage incurred
and, consequently, allow lifespan robots extended. might worthwhile
add mechanism algorithm detect known state lead directly
error state. problems currently investigated.

Acknowledgments
study partially supported Spanish MICIIN projects TIN2008-06701-C0303, TRA2009-0080 CCG10-UC3M/TIC-5597. offer gratitude special thanks
Raquel Fuentetaja Pizan, Assistant Professor Universidad Carlos III de Madrid
Planning & Learning Group (PLG), generous invaluable comments
revision paper. would thank Jose Antonio Martn, Assistant
Professor Universidad Complutense de Madrid, invaluable comments regarding
evolutionary RL algorithm.

References
Aamodt, A., & Plaza, E. (1994). Case-Based Reasoning; Foundational Issues, Methodological Variations, System Approaches. AI Communications, 7 (1), 3959.
Abbeel, P., Coates, A., Hunter, T., & Ng, A. Y. (2008). Autonomous Autorotation
RC Helicopter. ISER, pp. 385394.
Abbeel, P., Coates, A., & Ng, A. Y. (2010). Autonomous helicopter aerobatics
apprenticeship learning. I. J. Robotic Res., 29 (13), 16081639.
Abbott, R. G. (2008). Robocup 2007: Robot soccer world cup xi.. chap. Behavioral Cloning
Simulator Validation, pp. 329336. Springer-Verlag, Berlin, Heidelberg.
Aha, D. W. (1992). Tolerating Noisy, Irrelevant Novel Attributes Instance-Based
Learning Algorithms. International Journal Man-Machine Studies, 36 (2), 267287.
Aha, D. W., & Kibler, D. (1991). Instance-based learning algorithms. Machine Learning,
pp. 3766.
Anderson, C. W., Draper, B. A., & Peterson, D. A. (2000). Behavioral cloning student
pilots modular neural networks. Proceedings Seventeenth International
Conference Machine Learning, pp. 2532. Morgan Kaufmann.
Argall, B., Chernova, S., Veloso, M., & Browning, B. (2009). Survey Robot Learning
Demonstration. Robotics Autonomous Systems, 57 (5), 469483.
Bartsch-Sprl, B., Lenz, M., & Hbner, A. (1999). Case-based reasoning: Survey future
directions.. Puppe, F. (Ed.), XPS, Vol. 1570 Lecture Notes Computer Science,
pp. 6789. Springer.
Bianchi, R., Ros, R., & de Mantaras, R. L. (2009). Improving reinforcement learning
using case-based heuristics.. Vol. 5650, pp. 7589. Lecture Notes Artificial Intelligence, Springer, Lecture Notes Artificial Intelligence, Springer.
560

fiSafe Exploration State Action Spaces Reinforcement Learning

Borrajo, F., Bueno, Y., de Pablo, I., Santos, B. n., Fernandez, F., Garca, J., & Sagredo, I.
(2010). SIMBA: Simulator Business Education Research. Decission Support
Systems, 48 (3), 498506.
Boyan, J., Moore, A., & Sutton, R. (1995). Proceedings workshop value function
approximation, machine learning conference 1995... Technical Report CMU-CS-95206.
Chernova, S., & Veloso, M. (2007). Confidence-based policy learning demonstration
using gaussian mixture models. Joint Conference Autonomous Agents
Multi-Agent Systems.
Chernova, S., & Veloso, M. (2008). Multi-thresholded approach demonstration selection
interactive robot learning. Proceedings 3rd ACM/IEEE international
conference Human robot interaction, HRI 08, pp. 225232, New York, NY, USA.
ACM.
Cichosz, P. (1995). Truncating temporal differences: efficient implementation
td(lambda) reinforcement learning. Journal Artificial Intelligence Research
(JAIR), 2, 287318.
Cichosz, P. (1996). Truncated temporal differences function approximation: Successful examples using cmac. Proceedings Thirteenth European Symposium
Cybernetics Systems Research (EMCSR-96).
Coraluppi, S. P., & Marcus, S. I. (1999). Risk-Sensitive Minimax Control DiscreteTime, Finite-State Markov Decision Processes. AUTOMATICA, 35, 301309.
Defourny, B., Ernst, D., & Wehenkel, L. (2008). Risk-aware decision making dynamic
programming. NIPS 2008 Workshop Model Uncertainty Risk RL.
Driessens, K., & Ramon, J. (2003). Relational instance based regression relational rl.
International Conference Machine Learning (ICML), pp. 123130.
Driessens, K., & Dzeroski, S. (2004). Integrating guidance relational reinforcement
learning. Machine Learning, 57 (3), 271304.
Fernandez, F., & Isasi, P. (2008). Local feature weighting nearest prototype classification.
Neural Networks, IEEE Transactions on, 19 (1), 4053.
Fernandez, F., & Borrajo, D. (2008). Two steps reinforcement learning. International
Journal Intelligent Systems, 23 (2), 213245.
Floyd, M. W., & Esfandiari, B. (2010). Toward domain-independent case-based reasoning
approach imitation: Three case studies gaming. Workshop Case-Based
Reasoning Computer Games 18th International Conference Case-Based
Reasoning (ICCBR), pp. 5564.
Floyd, M. W., Esfandiari, B., & Lam, K. (2008). Case-Based Reasoning Approach
Imitating Robocup Players. Proceedings 21st International Florida Artificial
Intelligence Research Society Conference, pp. 251256.
Forbes, J., & Andre, D. (2002). Representations learning control policies.
University New South, pp. 714.
561

fiGarca & Fernandez

Gabel, T., & Riedmiller, M. (2005). Cbr state value function approximation reinforcement learning. Proceedings 6th International Conference Case-Based
Reasoning (ICCBR 2005, pp. 206221. Springer.
Geibel, P. (2001). Reinforcement Learning Bounded Risk. Proceedings 18th
International Conference Machine Learning, pp. 162169. Morgan Kaufmann.
Geibel, P., & Wysotzki, F. (2005). Risk-sensitive Reinforcement Learning Applied Control
Constraints. Journal Artificial Intelligence Research (JAIR), 24, 81108.
Hans, A., Schneegass, D., Schafer, A. M., & Udluft, S. (2008). Safe Exploration Reinforcement Learning. European Symposium Artificial Neural Network, pp.
143148.
Heger, M. (1994). Consideration Risk Reinforcement Learning. 11th International
Conference Machine Learning, pp. 105111.
Hernandez-Daz, A. G., Coello, C. A. C., Perez, F., Caballero, R., Luque, J. M., & SantanaQuintero, L. V. (2008). Seeding initial population multi-objective evolutionary algorithm using gradient-based information. IEEE Congress Evolutionary
Computation, pp. 16171624. IEEE.
Hester, T., Quinlan, M., & Stone, P. (2011). real-time model-based reinforcement learning
architecture robot control. Tech. rep. arXiv e-Prints 1105.1749, arXiv.
Hu, H., Kostiadis, K., Hunter, M., & Kalyviotis, N. (2001). Essex wizards 2001 team
description. Birk, A., Coradeschi, S., & Tadokoro, S. (Eds.), RoboCup, Vol. 2377
Lecture Notes Computer Science, pp. 511514. Springer.
Jiang, A. X. (2004). Multiagent reinforcement learning stochastic games continuous
action spaces..
Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: survey. Journal
Artificial Intelligence Research (JAIR), 4, 237285.
Konen, W., & Bartz-Beielstein, T. (2009). Reinforcement learning games: failures
successes. Proceedings 11th Annual Conference Companion Genetic
Evolutionary Computation Conference: Late Breaking Papers, GECCO 09, pp. 2641
2648, New York, NY, USA. ACM.
Koppejan, R., & Whiteson, S. (2009). Neuroevolutionary reinforcement learning generalized helicopter control. GECCO 2009: Proceedings Genetic Evolutionary
Computation Conference, pp. 145152.
Koppejan, R., & Whiteson, S. (2011). Neuroevolutionary reinforcement learning generalized control simulated helicopters. Evolutionary Intelligence, 4, 219241.
Lee, J.-Y., & Lee, J.-J. (2008). Multiple Designs Fuzzy Controllers Car Parking Using
Evolutionary Algorithm, pp. 16. No. May.
Luenberger, D. G. (1998). Investment science. Oxford University Press.
Mannor, S. (2004). Reinforcement learning average reward zero-sum games. ShaweTaylor, J., & Singer, Y. (Eds.), COLT, Vol. 3120 Lecture Notes Computer Science,
pp. 4963. Springer.
562

fiSafe Exploration State Action Spaces Reinforcement Learning

Martin H, J., & de Lope, J. (2009). Exa: effective algorithm continuous actions
reinforcement learning problems. Industrial Electronics, 2009. IECON 09. 35th
Annual Conference IEEE, pp. 2063 2068.
Martn H., J. A., & Lope, J. (2009). Learning Autonomous Helicopter Flight Evolutionary Reinforcement Learning. 12th International Conference Computer
Aided Systems Theory (EUROCAST), pp. 7582.
Mihatsch, O., & Neuneier, R. (2002). Risk-Sensitive reinforcement learning. Machine Learning, 49 (2-3), 267290.
Moldovan, T. M., & Abbeel, P. (2012). Safe exploration markov decision processes.
CoRR, abs/1205.4810.
Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata - survey. Ieee
Transactions Systems Man Cybernetics, SMC-4 (4), 323334.
Narendra, K. S., & Thathachar, M. A. L. (1989). Learning automata: introduction.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Ng, A. Y., Kim, H. J., Jordan, M. I., & Sastry, S. (2003). Autonomous Helicopter Flight
via Reinforcement Learning. Thrun, S., Saul, L. K., & Scholkopf, B. (Eds.), NIPS.
MIT Press.
Peters, J., Tedrake, R., Roy, N., & Morimoto, J. (2010). Robot learning. Sammut, C.,
& Webb, G. I. (Eds.), Encyclopedia Machine Learning, pp. 865869. Springer.
Poli, R., & Cagnoni, S. (1997). Genetic programming user-driven selection: Experiments evolution algorithms image enhancement. Genetic Programming
1997: Proceedings Second Annual Conference, pp. 269277. Morgan Kaufmann.
Salkham, A., Cunningham, R., Garg, A., & Cahill, V. (2008). collaborative reinforcement learning approach urban traffic control optimization. Web Intelligence
Intelligent Agent Technology, 2008. WI-IAT 08. IEEE/WIC/ACM International
Conference on, Vol. 2, pp. 560566.
Santamara, J. C., Sutton, R. S., & Ram, A. (1998). Experiments reinforcement
learning problems continuous state action spaces. Adaptive Behavior, 6,
163218.
Sharma, M., Holmes, M., Santamaria, J., Irani, A., Isbell, C., & Ram, A. (2007). Transfer
learning real-time strategy games using hybrid cbr/rl. Proceedings
Twentieth International Joint Conference Artificial Intelligence.
Siebel, N. T., & Sommer, G. (2007). Evolutionary reinforcement learning artificial neural
networks. International Journal Hybrid Intelligent Systems, 4, 171183.
Smart, W. D., & Kaelbling, L. P. (2000). Practical reinforcement learning continuous
spaces. Artificial Intelligence, pp. 903910. Morgan Kaufmann.
Smart, W. D., & Kaelbling, L. P. (2002). Effective reinforcement learning mobile robots.
ICRA, pp. 34043410. IEEE.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT
Press.
563

fiGarca & Fernandez

Tang, J., Singh, A., Goehausen, N., & Abbeel, P. (2010). Parameterized maneuver learning autonomous helicopter flight. International Conference Robotics
Automation (ICRA).
Taylor, M. E., Kulis, B., & Sha, F. (2011). Metric learning reinforcement learning agents.
Proceedings International Conference Autonomous Agents Multiagent
Systems (AAMAS).
Van Hasselt, H., & Wiering, M. A. (2007). Reinforcement Learning Continuous Action
Spaces. Approximate Dynamic Programming Reinforcement Learning, 2007.
ADPRL 2007. IEEE International Symposium on, pp. 272279.
Wyatt, J. (1997). Exploration Inference Learning Reinforcement. University
Edinburgh.
Yao, X. (1999). Evolving artificial neural networks. PIEEE: Proceedings IEEE, 87,
14231447.

564


