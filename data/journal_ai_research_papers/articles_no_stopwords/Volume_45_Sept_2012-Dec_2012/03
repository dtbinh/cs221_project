journal artificial intelligence

submitted published

safe exploration state action spaces
reinforcement learning
javier garca
fernando fernandez

fjgpolo inf uc es
ffernand inf uc es

universidad carlos iii de madrid
avenida de la universidad
leganes madrid spain

abstract
consider important safe exploration reinforcement
learning reinforcement learning well suited domains complex transition
dynamics high dimensional state action spaces additional challenge posed
need safe efficient exploration traditional exploration techniques
particularly useful solving dangerous tasks trial error process may lead
selection actions whose execution states may damage
learning system system consequently agent begins interaction
dangerous high dimensional state action space important question arises
namely avoid least minimize damage caused exploration
state action space introduce pi srl safely improves suboptimal
albeit robust behaviors continuous state action control tasks efficiently
learns experience gained environment evaluate proposed method
four complex tasks automatic car parking pole balancing helicopter hovering
business management

introduction
reinforcement learning rl sutton barto type machine learning whose
main goal finding policy moves agent optimally environment generally formulated markov decision process mdp many rl methods used
important complex tasks e g robot control see smart kaelbling hester
quinlan stone stochastic games see mannor konen bartz beielstein
control optimization complex dynamical systems see salkham cunningham
garg cahill rl tasks focused maximizing long term cumulative reward rl researchers paying increasing attention long term
reward maximization safety approaches sequential decision
sdps mihatsch neuneier hans schneegass schafer udluft martn h
lope koppejan whiteson well written reviews matters
found geibel wysotzki defourny ernst wehenkel nevertheless
important ensure reasonable system performance consider safety
agent e g avoiding collisions crashes etc application rl dangerous
tasks exploration techniques rl offer guarantees issues thus
rl techniques dangerous control tasks important question arises namely
ensure exploration state action space cause damage injury
c

ai access foundation rights reserved

figarca fernandez

time learning near optimal policies matter words
one ensuring agent able explore dangerous environment safely
efficiently many domains exploration exploitation process may lead
catastrophic states actions learning agent geibel wysotzki
helicopter hovering control task one case involving high risk since policies
crash helicopter incurring catastrophic negative reward exploration exploitation
strategies greedy may even constant helicopter crashes especially
high probability random action selection another example found
portfolio theory analysts expected portfolio maximizes profit
avoiding risks considerable losses luenberger since maximization expected
returns necessarily prevent rare occurrences large negative outcomes different
criteria safe exploration needed exploration process policies
evaluated must conducted extreme care indeed environments method
required explores state action space safe manner
propose policy improvement safe reinforcement learning
pi srl safe exploration dangerous continuous control tasks
method requires predefined safe baseline policy assumed suboptimal
otherwise learning would pointless predefined baseline policies used
different ways approaches work koppejan whiteson singlelayers perceptrons evolved albeit starting prototype network whose weights correspond baseline policy provided helicopter control task competition software abbeel
coates hunter ng viewed simple form population seeding proven advantageous numerous evolutionary methods
e g see hernandez daz coello perez caballero luque santana quintero poli
cagnoni work martn de lope weights neural networks evolved inserting several baseline policies including provided
helicopter control task competition software initial population minimize
possibility evaluating unsafe policies prevents crossover mutation
operators permitting anything tiny changes initial baseline policies
present pi srl novel improving baseline
policies dangerous domains rl pi srl composed two different steps first baseline behavior robust albeit suboptimal approximated
behavioral cloning techniques anderson draper peterson abbott
order achieve goal case reasoning cbr techniques aamodt plaza
bartsch sprl lenz hbner used successfully applied
imitation tasks past floyd esfandiari floyd esfandiari lam
second step pi srl attempts safely explore state action space
order build accurate policy previously learned behavior thus set
cases e state action pairs obtained previous phase improved
safe exploration state action space perform exploration small amounts
gaussian noise randomly added greedy actions baseline policy
exploration strategy used successfully previous works argall chernova
veloso browning van hasselt wiering
novelty present study use two main components risk
function determine degree risk particular state ii baseline behavior


fisafe exploration state action spaces reinforcement learning

capable producing safe actions supposedly risky states e states lead
damage injury addition present definition risk
agent unknown known space described section greater detail
definition completely different traditional definitions risk found literature geibel mihatsch neuneier geibel wysotzki
reports experimental obtained application four
different domains automatic car parking lee lee ii pole balancing sutton
barto iii rl competition helicopter hovering ng kim jordan sastry
iv business management borrajo bueno de pablo santos fernandez garca
sagredo domain propose learning near optimal policy
learning phase minimize car crashes pole disequilibrium helicopter crashes
company bankruptcies respectively important note comparison
agent optimal exploration policy possible since
proposed domains high dimensional continuous state action space
well complex stochastic dynamics know optimal exploration policy

regarding organization remainder section introduces key
definitions section describes detail learning proposed section
evaluation performed four mentioned domains presented section
discusses related work section summarizes main conclusions study
sections
term return used refer expected cumulative future discounted
p
reward r rt term reward used refer single real value used
evaluate selection action particular state denoted r

definitions
illustrate concept safety used navigation presented
figure navigation presented figure control policy must
learned get particular start state goal state given set demonstration
trajectories environment assume task difficult due stochastic
complex dynamic environment e g extremely irregular surface case
robot navigation domain wind effects case helicopter hover task
stochasticity makes impossible complete task exactly trajectory
every time additionally supposes set demonstrations baseline
controller performing task continuous black lines given set
demonstrations composed different trajectories covering well defined region
state space region within rectangle
addition small amounts gaussian noise perturbations baseline trajectories order better ways completing
task noise affect baseline trajectories different ways depending
amount noise added turn depends amount risk taken risk
desired noise added baseline trajectories consequently
improved behavior discovered nevertheless robot never fall cliff
helicopter never crash however intermediate level risk desired
small amounts noise added baseline trajectories trajectories


figarca fernandez

figure exploration strategy addition small amounts noise baseline
policy behavior continuous lines represent baseline behavior newly
explored behaviors indicated dotted dashed lines
dotted blue lines complete task discovered cases exploration
trajectories leads robot unknown regions state space dashed red lines
robot assumed able detect situations risk function use
baseline behavior return safe known states instead high risk desired
large amounts noise added baseline trajectories leading discovery
trajectories higher probability robot gets damaged
iteration process leads robot progressively safely explore state
action spaces order improved ways complete task degree
safety exploration however depend risk taken
error non error states
follow far notation presented geibel et al
definition concept risk study geibel et al associate risk error
states non error states former understood state considered
undesirable dangerous enter
definition error non error states let set states set
error states state undesirable terminal state control
agent ends reached damage injury agent learning system
external entities set considered set non error terminal states
control agent ends normally without damage injury
terms rl agent enters error state current episode ends damage
learning system systems whereas enters non error state episode
ends normally without damage thus geibel et al define risk respect
policy probability state sequence si generated
execution policy terminates error state definition
states
risk
taken depends actions selected policy definitions


fisafe exploration state action spaces reinforcement learning

theoretical framework introduce definition risk associated
known unknown states
known unknown states continuous action state spaces
assume continuous n dimensional state space n state
sn vector real numbers dimension individual domain dis
similarly assume continuous dimensional action space
action vector real numbers dimension
individual domain dia additionally agent considered endowed
memory case base b size memory element represents state action pair
case agent experienced
definition case base case base set cases b c c every case
ci consists state action pair si ai agent experienced past
associated value v si thus ci si ai v si first element represents
cases part corresponds state si following element ai depicts case
solution e action expected agent state si final element
v si value function associated state si state si composed n
continuous state variables action ai composed continuous action variables
agent receives state sq first retrieves nearest neighbor sq
b according given similarity metric performs associated action
use euclidean distance similarity metric equation
v
ux
u n
sq si sq j si j



j

euclidean distance metric useful value function expected continuous smooth throughout state space santamara sutton ram however
since value function unknown priori euclidean distance metric particularly suitable many many researchers begun ask distance
metric learn adapt order achieve better taylor kulis sha
use distance metric learning techniques would certainly desirable
order induce powerful distance metric specific domain consideration
lies outside scope present study therefore focused
domains euclidean distance proven successful e successfully applied car parking cichosz pole balancing martin h de lope
helicopter hovering control martin h de lope simba borrajo et al
traditionally case approaches use density threshold order determine
case added memory distance nearest neighbor
sq greater case added sense parameter defines size
classification region case b figure case sq within
classification region case ci considered known state hence cases

associated value function v b
b describe case policy agent b



figarca fernandez

figure known unknown states
definition known unknown states given case base b c c composed
cases ci si ai v si density threshold state sq considered known
min sq si unknown cases formally set known
states set unknown states
definition states identified known unknown agent
receives state performs action ai case ci si
min j sj however agent receives state definition
distance state b larger case retrieved consequently action
performed state unknown agent
definition case risk function given case base b c c composed
cases ci si ai v si risk state defined equation

b









min j sj
otherwise





thus b holds e unknown state
associated case hence action performed given situation

unknown b
derived casedefinition safe case policy case policy b
base b c c safe initial known state respect b
produces known non error states respect b
execution b






b
b si
b si



additionally assumed probability state sequence si
terminates error state
known state generated executing policy b

b e


fisafe exploration state action spaces reinforcement learning

definition safe case coverage coverage single state respect
safe case base b c c defined state si min si
therefore assume safe case provide actions entire state
space rather known states
figure graphically represents relationship known unknown error non learnt
error states green area image denotes safe case policy b
area state space corresponding initial known space agent following
green area resulting episodes end without
policy b
damages consequently subset non error states form part known space
formally let subsets non error states belonging known unknown
spaces respectively yellow area figure
contrast represents unknown space space found error states
well subset remaining non error states formally
understood way pi srl summed follows

first step learn known space green area safe case policy b

second step adjust known space green area unknown space yellow
area order explore improved behaviors avoiding error states red
area process adjusting known space space used safe
better policies forget ineffectual known states shown
section

figure known unknown error non error states given case base b

advantages prior knowledge predetermined
exploration policies
present subsection advantages teacher knowledge rl namely
provide initial knowledge task learned ii support exploration
process highlighted furthermore explain believe knowledge


figarca fernandez

indispensable rl tackling highly complex realistic large continuous
state action spaces particular action may undesirable
consequence
providing initial knowledge task
rl begin learning without previous knowledge task
learnt cases exploration strategies greedy used application
strategy random exploration state action spaces gather
knowledge task enough information discovered environment behavior improve random exploration policies however
waste significant amount time exploring irrelevant regions state action
spaces optimal policy never encountered compounded
domains extremely large continuous state action spaces random
exploration never likely visit regions spaces necessary learn near optimal
policies additionally many real rl tasks real robots random exploration
gather information environment cannot even applied real robots
considered sufficient information much information real robot
gather environment finally impossible avoid undesirable situations
high risk environments without certain amount prior knowledge task
use random exploration would require undesirable state visited
labeled undesirable however visits undesirable states may damage
injury agent learning system external entities consequently visits
states avoided earliest steps learning process
mitigating difficulties described finite sets teacher provided examples
demonstrations used incorporate prior knowledge learning
teacher knowledge used two general ways bootstrap learning e sort initialization procedure ii derive policy
examples first case learning provided examples demonstrations bootstrap value function approximation lead agent
relevant regions space second way teacher knowledge
used refers learning demonstration lfd approaches policy derived finite set demonstrations provided teacher principal drawback
however performance derived policy heavily limited
teacher ability one way circumvent difficulty improve performance
exploring beyond provided teacher demonstrations raises
question agent act encounters state demonstration
exists unknown state
supporting exploration process
furnishing agent initial knowledge helps mitigate associated
random exploration alone sufficient prevent undesirable situations
arise subsequent explorations undertaken improve learner ability additional mechanism necessary guide subsequent exploration process way
agent may kept far away catastrophic states teacher


fisafe exploration state action spaces reinforcement learning

rather policy derived current value function approximation used
selection actions unknown states one way prevent agent encountering
unknown states exploration process would requesting beginning
teacher demonstration every state state space however strategy
possible due computational infeasibility given extremely large number states
state space ii fact teacher forced give action
every state given many states ineffectual learning optimal policy
consequently pi srl requests teacher action action actually required
e agent unknown state
supposes teacher available task learned
teacher taken baseline behavior although studies examined use
robotic teachers hand written control policies simulated planners great majority
date made use human teachers uses suboptimal automatic controllers
teachers taken teachers policy
definition baseline behavior policy considered baseline behavior
three assumptions made able provide safe demonstrations
task learnt prior knowledge extracted ii able support
subsequent exploration process advising suboptimal actions unknown states reduce
probability entering error states return system known situation
iii performance far optimal
optimal baseline behaviors certainly ideal behave safely non optimal behaviors often easy easier implement generate optimal ones pi srl
uses baseline behavior two different ways first uses safe demonstrations provide prior knowledge task step builds

initial known space agent derived safe case policy b

purpose mimicking b second step pi srl uses support
subsequent exploration process conducted improve abilities previously learnt
exploration process continues action requested required
b

agent unknown state figure step acts backup
policy case unknown state intention guiding learning away
catastrophic errors least reducing frequency important note
baseline behavior cannot demonstrate correct action every possible state however
baseline behavior might able indicate best action cases
action supplies least safer obtained random
exploration
risk parameter
order maximize exploration safety seems advisable movement
state space arbitrary rather known space expanded gradually
starting known state exploration carried perturbation
perturbation trajectories
state action trajectories generated policy b
accomplished addition gaussian random noise actions b order
obtain ways completing task thus gaussian exploration takes place


figarca fernandez

figure exploration process pi srl requests actions baseline behavior
really required
around current approximation action ai current known state sc
ci si ai v si sc si min j sj action performed sampled
gaussian distribution mean action output given instance selected
b ai denotes action output probability selecting action
computed equation





e ai ai






shape gaussian distribution depends parameter standard deviation
study used width parameter large values imply wide bellshaped distribution increasing probability selecting actions different
current action ai small value implies narrow bell shaped distribution increasing
probability selecting actions similar current action ai
assume ai hence value directly related amount perturbation
higher values imply
added state action trajectories generated policy b
greater perturbations gaussian noise greater probability visiting unknown
states
definition risk parameter parameter considered risk parameter large
values increase probability visiting distant unknown states hence increase
probability reaching error states
exploratory actions drive agent edge known space force
go slightly beyond unknown space search better safer behaviors
period time execution exploratory actions increases known space
risk
improves abilities previously learned safe case policy b
parameter well design parameters must selected user
section guidelines selection offered
important note proposed study two logical
assumptions rl derived following generalization principles kaelbling littman
moore sutton barto


fisafe exploration state action spaces reinforcement learning

nearby states similar optimal actions continuous state spaces
impossible agent visit every state store value optimal action
table generalization techniques needed large smooth state spaces
similar states expected similar values similar optimal actions therefore
possible use experience gathered environment limited subset
state space produce reliable approximation much larger subset boyan moore
sutton hu kostiadis hunter kalyviotis fernandez borrajo
one must note proposed domains optimal action considered
safe action sense never produces error states e action considered
optimal leads agent catastrophic situation
ii similar actions similar states tend produce similar effects considering deterministic domain action performed state st produces
state st stochastic domain understood intuitively execution
action state st produce similar effects e produces states
j j dist sit sjt additionally execution action
state st produces states j dist sjt
explained earlier present study uses euclidean distance similarity metric
proven successful proposed domains assumption
approximation techniques used actions generate similar effects
grouped together one action jiang continuous action spaces need
generalization techniques even greater kaelbling et al
assumption allows us assume low values increase probability visiting
known states hence exploring less taking less risks greater values
increase probability reaching error states

pi srl
pi srl composed two main steps described detail
first step modeling baseline behaviors cbr
first step pi srl behavioral cloning cbr allow software
agent behave similar manner teacher policy baseline behavior floyd et al
whereas lfd approaches named differently according learned argall et al prevent terminological inconsistencies consider behavioral
cloning known imitation learning area lfd whose goal reproduction mimicking underlying teacher policy peters tedrake roy morimoto
abbott
cbr behavioral cloning case built agents state
received environment well corresponding action command performed
teacher pi srl objective first step properly imitate behavior
cases stored case base point important question arises namely
case base b learnt sample trajectories provided
end learning process resulting policy derived b mimics behavior
baseline behavior function maps states actions


figarca fernandez

words function given state si provides corresponding action ai
want build policy b derived case base composed cases sj aj
state sq case minimum euclidean distance dist sq sj
retrieved corresponding action aj returned intuitively assumed
b built simply storing cases si ai gathered one interaction
environment limited number episodes k end k episodes
one expects resulting b able properly mimic behavior however
informal experimentation helicopter hovering domain shows case
section helicopter hovering k episodes prohibitive number
cases stored policy derived case base b unable correctly
imitate baseline behavior instead continuously crashes helicopter indeed
order b mimic large continuous stochastic domains
requires larger number episodes consequently prohibitive number cases
fact perfectly mimic domains infinite number cases would required
figure attempts explain believe learning process work
region space represented simply storing cases derived form
c shown stored case red circles covers area space represents
centroid voronoi region

figure effects storing training cases
previously learned policy b used state sq presented action aj performed corresponding case cj sj aj euclidean distance
dist sq sj less stored cases however use policy
provide action situation sq action ai provided different aj
point policy b said classify state sq obtained class aj
policy said classify state sq desired class ai insofar
policy mimicked ai aj furthermore ai aj understood
classification error case base stored possible pairs si ai
able generate domain actions aj ai would identical
dist sq sj ai aj however stochastic large continuous domain
impossible store cases sum classification errors episode


fisafe exploration state action spaces reinforcement learning

leads visiting unexplored regions case space e regions
state sq received environment euclidean distance dist sq sj
respect closest case cj sj aj b unexplored regions visited
difference obtained class derived b desired class derived
large e ai aj probability error states might visited
greatly increases
may concluded therefore simply storing pairs c generated
sufficient properly mimic behavior reason figure
proposed
cbr behavioral cloning





given
given
given
set






repeat
set k
k maxepisodelength
compute case sc ac closest current state sk




















baseline behavior
density threshold
maximum number cases
case base b



b sk equation
set ak ac
else
set ak baseline behavior
create case cnew sk ak
b b cnew
execute ak receive sk
set k k
end
kbk
remove kbk least frequently used cases b
stop criterion becomes true

return b performing safe case policy b

figure cbr behavioral cloning


first step state value function v b si initialized see

line value v b si case computed second step
section additionally step uses case risk function equation
determine whether state sk considered risky line state
risky e known state sk nearest neighbor strategy followed line
otherwise performs action ak baseline behavior
case cnew sk ak built added case base b line starting
empty case base learning continuously increases competence storing
experiences however number reasons inflow cases
limited large case bases increase time required closest cases
example may partially solved techniques reduce retrieval time
e g k trees used work nevertheless reduce storage


figarca fernandez

requirements several approaches removal ineffectual cases training exist
including ahas ibx aha nearest prototype fernandez
isasi number cases stored b exceeds critical value kbk
realization retrieval within certain amount time cannot guaranteed
removal cases inevitable efficient
removal least frequently used elements b line
step constrained case base b describing safe case policy

b mimics baseline behavior though perhaps deviation line
formally let u estimate utility baseline behavior computed
u
averaging sum rewards accumulated nt trials u b

second step improving learned baseline behavior
learned previous
step pi srl safe case policy b
step improved safe exploration state action space first case ci

b state value function v b si computed following monte carlo mc
figure
mc adapted cbr





given case base b
initialize ci b
v arbitrary
returns empty list








k maxn umberepisodes

generate episode b
appearing episode v b
r return following first occurrence
append r returns
v average returns




set k k
return b

figure monte carlo computation state value function case
similar spirit first visit mc method v sutton barto
adapted work policy given case base
shown figure returns state si b accumulated averaged following
derived case base b see line important note
policy b
term return following first occurrence refers expected return
e expected cumulative future discounted reward starting state whereas
returns refers list composed return different episodes one
principal reasons mc method allows us quickly easily estimate

state values v b si case ci b addition mc methods shown
successful wide variety domains sutton barto state value

function v b si computed case ci b small amounts gaussian noise
order obtain improved ways
randomly added actions policy b


fisafe exploration state action spaces reinforcement learning

complete task used improve baseline behavior learned
previous step depicted figure composed four steps performed
episode
initialization step initializes list used store cases occurring
episode sets cumulative reward counter episode
b case generation builds case step episode
state sk closest case v b computed euclidean
distance metric equation see line figure order determine
perceived degree risk state sk case risk function used line

b sk sk known state case action ak performed
computed equation case cnew ak v built added
list cases occurred episode line important note
case ak v built replacing action corresponding closest case
v b action ak resulting application random
gaussian noise equation thus produces smooth changes

cases b ak however b sk state sk e unknown
state line unknown states action ak performed suggested baseline
behavior defines safe behavior line case sk ak built
added list cases episode actions performed
agent known state finally reward obtained episode accumulated
r sk ak immediate reward obtained action ak performed state sk
line
c computing state value function unknown states step
state value function states considered unknown previous step
computed previous step line state value function states set
proceeds manner similar first visit mc figure
case return unknown state si computed averaged since
one episode considered line return si computed taking
account first visit state si episode occurrence state episode
called visit si although state si could appear multiple times rest
episode
updating cases b experience gathered updates b
made cases gathered episodes cumulative reward similar
best episode found point threshold line way good sequences
provided updates since shown sequences experiences
cause adaptive agent converge stable useful policy whereas bad sequences may
cause agent converge unstable bad policy wyatt prevents
degradation initial performance b computed first step
use bad episodes episodes errors updates step two types
updates appear namely replacements additions cases
iterates case ci si ai v si listcasesepisode line si known state
line compute case si v si b corresponding state si line
one note case ci si ai v si listcasesepisode built line
replacing action corresponding case si v si b
action ai resulting application random gaussian noise action


figarca fernandez

policy improvement











given case base b maximum number cases
given baseline behavior
given update threshold
set maxt otalrwepisode maximum cumulative reward reached episode
repeat
initialization step
set k listcasesepisode totalrwepisode
b case generation
k maxepisodelength
compute case v b closest current state sk







b sk known state
chose action ak equation
perform action ak
create instance cnew ak v








else unknown state
chose action ak
perform action ak
create instance cnew sk ak
totalrwepisode totalrwepisode r sk ak
listcasesepisode listcasesepisode cnew












set k k
c computing state value function unknown states
instance ci listcasesepisode


b si unknown state
p
return si kj n jn r sj aj n first ocurrence si episode
v si return si
updating cases b experience gathered
totalrwepisode maxt otalrwepisode
maxt otalrwepisode max maxt otalrwepisode totalrwepisode
case ci si ai v si listcasesepisode






b si known state
compute case si v si b corresponding state si
compute r si ai v si v si








replace case si v si b case si ai v si listcasesepisode
v si v si
else unknown state
b b ci






kbk
remove kbk least frequently used cases b
stop criterion becomes true
return b

figure description step two pi srl

equation temporal distance td error computed line
performing action ai positive change value state action


fisafe exploration state action spaces reinforcement learning

turn could potentially lead higher return thus better policy van hasselt
wiering update value function actions potentially lead
higher return td error positive ai considered good selection
reinforced reinforcement carried updating output
case si v si b ai line therefore update case base occurs
td error positive similar linear reward inaction update learning
automata narendra thathachar sign td error used
measure success pi srl updates case base actual improvements
observed thus avoiding slow learning plateaus value space
td errors small shown empirically procedure
better policies step size depends size td error van hasselt
wiering important note replacements produce smooth changes
case base b since action replaced ai higher v si ai
form updating understood risk seeking overweighting
transitions successor states promise average return mihatsch neuneier
additionally prevents degradation b ensuring replacements made
action potentially lead higher v si
instead si known state case ci added b line finally
removes cases b necessary line complex scoring metrics calculate
cases removed given moment proposed several authors
forbes andres suggest removal cases contribute least overall
approximation driessens ramon pursue error oriented view
propose deletion cases contribute prediction error examples
principal drawback sophisticated measures complexity
determination case removed involves computation score value
ci b turn requires least one retrieval regression respectively
cj b j entire repeated sweeps case base entail enormous
computational load gabel riedmiller compute different score metric
ci b requiring computation set k nearest neighbors around ci
approaches well suited systems learning adjusted time requirements
high dimensional state space requiring use larger case bases
proposed rather propose removal least frequently used
cases idea seems intuitive insofar least frequently used cases usually contain
worse estimates corresponding states value although strategy might lead
function approximator forgets valuable experience made past
e g corner cases despite pi srl performs successfully domains proposed
strategy demonstrated section thus ability forget ineffectual
known states described section removing kbk cases
least frequently used cases b
parameter setting design
one main difficulties applying pi srl given
decide appropriate set parameter values threshold risk parameter
update threshold maximum number cases incorrect value


figarca fernandez

parameter lead mislabeling state known really unknown potentially
leading damage injury agent case risk parameter high values
continuously damage injury low values safe allow
exploration state action space sufficient reaching near optimal policy unlike
parameter related risk instead directly related
performance parameter used determine good episode
must respect best episode obtained since best episodes used
update case base b value large bad episodes may used update b
influencing convergence performance instead low
number updates b may insufficient improving baseline behavior finally
high value allows large case bases increasing computational effort
retrieval degrading efficiency system contrast low value might
excessively restrict size case base thus negatively affect final performance
subsection solid perspective given automatic definition
parameters parameter setting proposed taken suitable set
heuristics tested successfully wide variety domains section
parameter parameter domain dependent related average size
actions value parameter established
computing mean distance states execution baseline
behavior expressed another way execution policy provides
state action sequence form sn thus value
computed equation



dist dist sn sn
n



parameter several authors agree impossible completely avoid
accidents moldovan abbeel geibel wysotzki important
note pi srl completely safe first step executed
however proceeding way performance heavily
limited abilities baseline behavior running subsequent
exploratory process inevitable learner performance improved beyond
baseline behavior since agent operates state incomplete knowledge
domain dynamic inevitable exploratory process
unknown regions state space visited agent may reach error
state however possible adjust risk parameter determine level
risk assumed exploratory process start low
values low risk gradually increase specifically propose beginning
increasing value iteratively accurate policy
obtained amount damage injury high
parameter value parameter set relative best episode obtained
value set cumulative reward best episode
obtained


fisafe exploration state action spaces reinforcement learning

figure trajectories generated baseline policy deterministic slightly
stochastic highly stochastic domain
parameter previously estimated maximum number cases stored
case base estimated maximum number cases required properly mimic baseline behavior follows description value
computed figure presents trajectories sequences states followed baseline policy three different domains deterministic slightly stochastic highly
stochastic domain different sequences states produced
represented n n sm sm smn
sji th state initial state sjn final state resulting
trajectory episode j deterministic domain different executions
trajectory case set maximum number
cases n cases computed episode stored
slightly stochastic domain trajectories produced different episodes
different slightly suppose case base beginning
empty additionally assume states n corresponding first trajectory produced domain stored case base
furthermore domain execute different episodes obtaining different
trajectories following execution episodes compute maximum distance th state first trajectory previously added case base
th state produced trajectory j max jm sji
slightly stochastic domain maximum distance exceed threshold
case max jm sji point assume th state
trajectory j least one neighbor distance less corresponding
state thus th state j added case base
contrast highly stochastic domain maximum distance greatly exceeds
threshold cases max jm sji domain
estimate total number cases added case base following


figarca fernandez

way th state sequence
j first trajectory k estimate number
max jm sji
cases added case base
words

compute number intervals range max jm sji width
threshold used decide whether case added casebase consequently estimated number cases addedjto case base taking

k
pn
max jm sji
account states sequence computed
finally

estimated maximum number cases computed shown equation

n


n
x
max jm sji







important remember deterministic domain summation equation equal therefore n increase value element
related increase stochasticity environment insofar greater
stochasticity environment increases number cases required finally
number cases large nearly infinite threshold increased
make restrictive addition cases case base however
increase may adversely affect final performance

experimental
section presents experimental collected use pi srl policy
learning four different domains presented order increasing complexity e increasing number variables describing states actions car parking lee
lee pole balancing sutton barto helicopter hovering ng et al
business simulator simba borrajo et al domains
proposed learning near optimal policy minimizes car accidents
pole disequilibrium helicopter crashes company bankruptcies respectively
learning phase four domains stochastic experimentation
helicopter hovering business simulator simba stochastic
additionally generalized domains made car parking pole balancing domains stochastic intentional addition random gaussian noise actions
reward function pi srl four domains compared yielded
two additional techniques namely evolutionary rl selected winner
helicopter domain rl competition martn h lope geibel
wysotzkis risk sensitive rl geibel wysotzki evolutionary several neural networks cloning error free teacher policies added initial
population guaranteeing rapid convergence near optimal policy
indirectly minimizing agent damage injury indeed winner helicopter
domain agent highest cumulative reward winner must indirectly
minimize helicopter crashes insofar incur large catastrophic negative rewards
hand risk sensitive defines risk probability reaching
terminal error state e g helicopter crash ending agent control starting initial


fisafe exploration state action spaces reinforcement learning

state case value function weighted sum risk probability
value function v used equation
v v



parameter determines influence v values compared values v corresponds computation minimum risk policies large
values original value function multiplied dominates weighted criterion
geibel wysotzki consider finite discretized action sets study
adapted continuous action sets use cbr value
risk function approximation gaussian exploration around current action
experiments domain three different values used modifying influence
v values compared values cases goal improve control
policy time minimizing number episodes agent damage
injury domain establish different risk levels modifying risk parameter
values according procedure described subsection important note
one baseline behavior used initialize evolutionary rl exactly
used subsequently first second step pi srl furthermore case base
risk sensitive begin scratch since initialized safe
makes comparison performances fair possible
case policy b
taking account different techniques make use baseline behaviors
car parking
car parking represented figure originates rl literature cichosz car represented rectangle figure initially located
inside bounded area represented dark solid lines referred driving area
goal learning agent navigate car initial position garage
car entirely inside minimum number steps car cannot move
outside driving area figure b shows two possible paths car take
starting point garage obstacle order correctly perform
task consider optimal policy domain reaches goal
state shortest time time free failures
state space domain described three continuous variables namely
coordinates center car xt yt angle cars axis
x coordinate system car modeled essentially two control
inputs speed v steering angle let us suppose car controlled
steering angle e moves constant speed thus action space described one
continuous variable corresponding turn radius used equations
agent receives positive reward value r dist pt pg
pt xt yt center car pg xg yg center garage e goal
position normalizing function scaling euclidean distance dist pt pg
pt pg range car inside garage e reward value greater
car parked correctly center garage agent receives reward
whenever hits wall obstacle steps receive reward thus
difficulty lies reinforcement delay fact


figarca fernandez

figure car parking model car parking b examples
trajectories generated agent park car garage
punishments much frequent positive rewards e much easier hit
wall park car correctly motion car described following
equations lee lee
v l tan



xt xt v cos



yt yt v sin



v linear velocity car assumed constant value
maximum steering angle e car change position maximum angle
directions simulation time step gaussian noise added
actions rewards standard deviation since noisy interactions inevitable
real world applications adding noise actuators environment
transform deterministic domain stochastic domain important note
noise added transform domain stochastic domain independent
gaussian noise standard deviation risk parameter used explore state
action space second step pi srl case gaussian noise
standard deviation used exploration added noise previously added
actuators l v rad
driving area obstacle dimensions detailed figure initial position
car fixed xs ys rad goal position
xg yg domain designed baseline behavior
average cumulative reward per trial
order perform pi srl modeling baseline behavior step exe learned demonstrations
cuted step safe case policy b
provided baseline behavior see subsection computed following
procedure described subsection resulting values respectively


fisafe exploration state action spaces reinforcement learning

figure car parking task modeling baseline behavior step number steps per
trial executed case base b baseline behavior b cumulative
reward per trial baseline behavior learned safe case policy
ibl
b
figure graphically represents execution modeling baseline behavior step
two different learning processes presented one number steps per
trial executed baseline behavior continuous red lines cases b dashed
green lines shown beginning learning process empty case base b
steps performed baseline behavior learning process continues
learned around trials
cases added b safe case policy b
practically steps performed cases b rarely used
means safe case policy learned two learning processes shown
figure modeling baseline behavior step performed without collisions
wall obstacle words baseline behavior cloned safely without
errors figure b shows cumulative reward three different execution processes
first continuous red lines corresponding performance baseline behavior
second dashed green lines corresponding previously learned safe case
derived b third dashed blue lines corresponding instancepolicy b
learning ibl consisting storing cases memory ibl
items classified examining cases stored memory determining
similar case given particular similarity metric euclidean distance used classification nearest neighbor nearest neighbors taken
classification item nearest neighbor strategy aha kibler
two different executions carried ibl training
process performed saving training cases produced baseline behavior
trials consider ib sense saves every
case training phase see aha kibler figure b shows safe
case policy b almost perfectly mimics behavior baseline behavior
domain performance ib similar
figure shows different risk configurations obtained improving
learned baseline behavior step risk configuration two different learning pro

figarca fernandez

figure improving learned baseline behavior step car parking cumulative reward per episode different risk configurations obtained
pi srl b cumulative reward per episode evolutionary rl risksensitive rl approaches cases episode ending failure marked
cesses performed trials ending failure car hits wall obstacle marked
blue triangles learning processes figure demonstrate number
failures increases increase parameter low level risk
although failures produced performance nevertheless weak around baseline behavior constant throughout whole learning process additional
experiments demonstrated increasing value increases
number failures without improving performance figure b shows
evolutionary risk sensitive rl approaches different values regarding former
number failures higher obtained pi srl final
performance similar case latter performance higher value
maximization yet agent consistently crashes car wall
figure shows mean number failures e car collisions cumulative reward
trials red circles corresponding pi srl
black triangles risk sensitive blue square evolutionary
rl additionally figure shows two asymptotes horizontal asymptote
established according cumulative reward obtained highest value
horizontal asymptote indicates higher values increase number failures without
improving cumulative reward may fact get worse vertical asymptote
f ailures indicates reducing risk parameter reduce number
failures figure shows performance two additional risk levels
high level risk low level risk respect
figure low level risk additional random gaussian
noise added actions free failures although performance
learned first step
improve respect safe case policy b
pi srl medium level risk free
failures yet performance slightly improved pi srl high level
risk obtains highest cumulative reward mean


fisafe exploration state action spaces reinforcement learning

figure mean number failures car collisions cumulative reward trials
car parking task means computed
different executions

failures however high level risk number
failures greatly increases consequently cumulative reward decreases shown
figure pi srl high risk evolutionary rl obtain
similar performance pi srl demonstrates faster convergence thus figure
cumulative reward obtained pi srl higher pareto comparison criterion
used compare solutions figure principle one solution strictly
dominates preferred solution parameter strictly worse
corresponding parameter least one parameter strictly better
written indicating strictly dominates accordance pareto
principle assume points figure corresponding pi srl solutions
save pi srl high level risk pareto frontier since points
strictly dominated solution e solution time
higher cumulative reward lower number failures pi srl domain
solution pi srl medium level risk strictly dominates preferred
risk sensitive solutions pi srl risk sensitive solution pi srl
high level risk strictly dominates solution evolutionary rl solution
pi srl evolutionary rl
nevertheless important note ultimate decision
figure best depends criteria researcher instance minimization number failures deemed important optimization criterion
independently improvement obtained respect baseline behavior
best pi srl low level risk similarly
maximization cumulative reward instead judged important optimization criterion independently number failures generated best
pi srl high level risk
figure shows evolution cases case base b known space different
trials high risk learning process graph presents set known states green


figarca fernandez

figure car parking evolution known space different trials
b c high risk learning process
graph corresponds situation state space
accordance case base b trial
area error states red area unknown states yellow area non error states
orange circles pi srl adapts known space order safer better policies
complete task figure shows initial situation b corresponding
robust sense never
previously learned safe case policy b
collisions suboptimal selects longest parking path driving around upper
side obstacle learning process progresses figure b pi srl finds
shorter path park car garage along upper side obstacle increasing
performance comes closer obstacle increasing probability
collisions figure c pi srl finds even shorter path time along
lower side obstacle however still cases case base b corresponding
older path along upper side obstacle figure c indicates two paths
park car finally figure cases corresponding suboptimal path
along upper side obstacle removed b replaced cases
corresponding safe improved path along lower side obstacle
words pi srl adapts known space exploration unknown space
order improved behaviors process adjusting known space


fisafe exploration state action spaces reinforcement learning

safe better policies forgets previously learned yet ineffective
known states
following experiment becomes apparent domain noisy enough even
taking risk e noise added actuator exploration
agent could nevertheless perform poorly constantly produce collisions experiment
serves explain domain noise never sufficient efficient exploration
space without action selection noise experiment intentionally added
noise actuators performed second step pi srl however
time taking risk e test added random gaussian noise
standard deviation rather standard deviation used previously
actuators figure shows two executions second step improving learned
baseline policy pi srl x axis indicating number trials
axis cumulative reward per episode failures e collisions marked blue
triangles experiments figure b case policy b low level
risk never produces failures contrast experiments shown
figure case policy b continually collides wall although
risk parameter set furthermore increase performance
detected

figure improving learned baseline behavior step car parking task two learning processes risk configuration increase noise
actuators

increase noise actuators second step respect
first step case policy b learned first step gaussian random
noise actuator standard deviation second step performed
gaussian random noise actuator standard deviation takes
agent beyond known space case base b learnt first step pi srl
allows trajectories parking car garage situation
exploration process guided follows known state reached agent performs
action retrieved b without addition gaussian noise since risk parameter
see line figure unknown state reached agent performs


figarca fernandez

action advised baseline behavior see line exploration
process better trajectory found parking car garage
resulting cases episode corresponding unknown states added case base
see line slightly improving performance figure important note
replacements cases see line change actions b since
replaced action previously retrieved b plus certain amount gaussian
noise standard deviation see line nevertheless given risk parameter
set actions retrieved case base replaced exploration
process however e taking risk lead optimal behavior since
actions performed unknown situations added case base b performed baseline behavior supposed perform suboptimal actions
see definition baseline behavior
actions cases b replaced improved actions gaussian
noise standard deviation used explore different better actions
provided b however case risk parameter set
better actions discovered
additional experiments demonstrate pi srl behaves much worse higher
value noise used actuators collisions episodes assume taking
risk e implies performing actions discovering
newer better actions provided learned case base b baseline
behavior pi srl replacements case base executed towards
promising action case guarantees higher return
exploration necessary order obtain near optimal behavior since without
exploration better actions discovered pi srl performance limited
case policy learned first step b baseline behavior
one must remember intended perform suboptimal policies
pole balancing
name suggests objective pole balancing balance pole
vertically top moving cart sutton barto state description consists
four dimensional vector containing angle radial speed cart position
x speed x action consists real valued force used push cart
study reward computed encourage actions keep pole upright
possible cart cart centered possible track thus reward
step computed rt xt normalizing functions
scaling angle position xt range episode composed
steps although may nevertheless end prematurely pole becomes unbalanced e
inclination twelve degrees direction cart falls
track e center track
considered failures car parking gaussian noise added actions
rewards time standard deviation pole balancing domain
becomes stochastic addition noise actuators reward function


fisafe exploration state action spaces reinforcement learning

figure modeling baseline behavior step pole balancing task number steps per
trial executed case base b baseline behavior b cumulative reward
ibl
per trial learned safe case policy b
hand made baseline behavior demonstrates execution safe yet suboptimal
policy average cumulative reward per episode trial
learnt
modeling baseline behavior step pi srl safe case policy b
demonstrations provided baseline behavior computed following
procedure described subsection values respectively
figure shows two different learning processes modeling baseline behavior step
learning process figure shows number steps per trial executed
baseline behavior continuous red lines case base b dashed green lines
beginning learning process case base b empty steps performed
baseline behavior learning process progresses however b filled
learnt end learning process around
safe case policy b
trials almost steps performed cases b rarely used
important note modeling baseline behavior step performed without
failures e pole disequilibrium cart track case previous
task figure b represents three independent execution processes previously derived b indicated dashed green lines
learned safe case policy b
baseline behavior indicated continuous red lines
ibl indicated dashed blue lines aha kibler average cumulative
figure b almost perfectly clones
reward per episode b

b
ib cases pole disequilibrium cart falling
track averages cumulative reward per episode
figure shows pi srl different risk configurations
configuration learning curves shown two different learning processes performed
additionally episode ending failure marked blue triangles increase
risk increases probability failure policy obtained nevertheless better terms
cumulative reward nevertheless much greater risk values produce
failures without accompanying increase cumulative reward figure b shows
evolutionary risk sensitive rl approaches former


figarca fernandez

figure improving learned baseline behavior step pole balancing task cumulative reward per episode different risk configurations obtained pisrl b cumulative reward per episode obtained evolutionary risksensitive rl approaches cases episode ending failure marked

clearly greatest number failures risk sensitive
value maximization agent selects actions higher value
higher risk contrary risk minimization agent
learns risk function around episode selects actions lower risk
lower number failures considerably weak performance value
produces intermediate policy consequently concluded pi srl high
level risk obtains better policies less failures evolutionary risk sensitive
rl approaches figure reinforces previous conclusions

figure mean number failures pole disequilibrium cart track cumulative reward trials pole balancing task
means computed different executions


fisafe exploration state action spaces reinforcement learning

mean number failures cumulative reward trials shown
red circles corresponding pi srl black triangles corresponding risksensitive blue square corresponding evolutionary rl
figure shows performance two additional risk levels high level risk
low level risk respect figure cumulative
reward number failures increase high level risk
risk level represents inflection point higher levels risk produce failures
without accompanying improvement cumulative reward fact high level
risk reduction cumulative reward compared
high level risk pareto comparison criterion may used
compare solutions figure domain solution pi srl
low level risk strictly dominates risk sensitive solutions
pi srl risk sensitive additionally
solution pi srl high level risk strictly dominates evolutionary rl solution
pi srl evolutionary rl
lastly figure shows evolution known space derived case base
b different trials high risk learning process graph error states red
area set unknown states yellow area set known states green area
set non error states orange circles represented known space
graph computed taking cases b trials
graph non error states computed different executions b
trial orange circles representing terminal states executions
first graph figure presents initial known space resulting modeling
baseline behavior step evolution figure demonstrates two different points first
pi srl progressively adapts known space order encounter better behavior
known space tends compressed toward center coordinates
due fact reward greater angle pole cart
position x e pole upright possible cart cart centered
track second risk failure pole balancing domain greater
early trials learning process beginning learning process figure
regions known space close error space situation
slight modifications actions consistently produce visits states e pole
disequilibrium cart falling track learning process advances figure
b c known space compressed toward origin coordinates away
error space consequently probability visiting error states decreases
example returning figure high risk learning processes failures
occur first trials remaining occur last
trials
helicopter hovering
suggested name objective domain make helicopter hover close
possible defined position duration established episode task challenging two main reasons firstly state action spaces high dimensional
continuous specifically state space dimensional action space


figarca fernandez

figure pole balancing task evolution known space different trials
b c high risk learning process
graph corresponds situation state space
according case base b trial
dimensional secondly generalized domain whose behavior modified wind
factor helicopter episode composed steps although may end prematurely
helicopter crashes first step pi srl performed order imitate baseline
behavior computed following procedure described subection
values respectively step performed resulting
able properly imitate baseline behavior
safe case policy b

figure shows two learning processes modeling baseline behavior step
similar previous tasks learning processes progress number steps executed
baseline behavior reduced number steps case base b
increases end learning process case base b stores safe case
figure b compares performance terms cumulative reward per
policy b
ib regarding
episode learned case policy b
first two average cumulative reward per episode obtained
although perfectly mimic baseline behavior
b

b


fisafe exploration state action spaces reinforcement learning

figure modeling baseline behavior step helicopter hovering task number
steps per trial executed case base b baseline behavior b cumula ibl
tive reward per trial learned safe case policy b

nevertheless performs safe policy without crashing helicopter regard
training process ib every case produced episodes baseline
behavior stored figure b demonstrates ib consistently
helicopter crashes performance extremely far learned safe case improvement policy begins state action space safely
policy b
b
explored execution step two pi srl
figure shows different risk levels pi srl low medium
levels risk levels produce helicopter crashes pi srl performance nevertheless
quite weak

figure improving learned baseline behavior step helicopter hovering task
cumulative reward per episode different risk configurations obtained pisrl b cumulative reward per episode obtained evolutionary risksensitive rl approaches cases episode ending failure marked


figarca fernandez

conversely high level risk established produces near optimal policy
low number collisions extensive experimentation demonstrates increasing risk
parameter increases number crashes without accompanying
improvement cumulative reward figure b shows evolutionary
rl remembered selected winner rl competition
domain martn h lope well risk sensitive rl
different values comparison evolutionary rl
pi srl shows similar cumulative reward significantly higher number
crashes former latter evolutionary crashes
occur early steps learning process pi srl accidents occur
advanced steps learning process case risk sensitive rl
risk function learned around episode point
agent selects lower risk actions number crashes considerably reduced
agent selects actions resulting higher values without taking risk
account performance improves expense increased number accidents
nevertheless whatever value number crashes higher performance
worse pi srl

figure mean number failures helicopter crashes cumulative reward
episodes helicopter hovering task means
computed different executions
information figure indicating mean number failures cumulative
reward episodes complements conclusions made
data computed independent executions
previous domains pi srl indicated red circles risk sensitive
black triangles evolutionary rl blue square figure shows
performance two additional risk levels high level risk
low level risk respect figure figure demonstrates
evolutionary rl obtains highest cumulative reward
followed closely pi srl approaches far
regarding number failures e helicopter crashes pi srl low level
risk low level risk medium level risk


fisafe exploration state action spaces reinforcement learning

produces collisions pi srl medium risk preferable inasmuch
cumulative reward higher pareto comparison criterion
pi srl solution high level risk strictly dominates solutions risksensitive pi srl risk sensitive moreover pi srl strictly
dominated solution

figure evolution known space different episodes helicopter hovering
task example representation single known state radar chart b
c known states episodes respectively
high risk learning process graph corresponds
situation known space according case base b episode
pole balancing domain figure shows evolution known space
according case base b different episodes high risk learning process
case radar charts used due high number features describing states
radar chart graphical method displaying multivariate data two dimensionally
figure axis represents one features state preserve simplicity
representation charts generated normalizing absolute values features
figure example representation single known state


figarca fernandez

value axis corresponds value individual feature state
line drawn connecting feature values axis line figure
represents single state figures b c known space according
case base b episodes respectively three charts represent
single state rather states b corresponding episode thus
graph set known states marked green area state considered error state
single feature value state greater limits marked red line
graphs computed taking account helicopter crashes
velocity along main axes exceeds ii position helicopter
iii angular rate around main axes exceeds rad
iv orientation degrees target orientation previous
tasks figure indicates two different matters first learning proceeds known
space derived b adjusted space used better safer policies
helicopter domain agent tries hover helicopter close possible target
position e origin coordinates since immediate rewards greater closer
helicopter hovers origin thus known space starts expand figure
b progressively concentrated origin coordinates figure c
regard second matter probability crashing low since
beginning known space already appears concentrated origin far
error space figure b words beginning features
known space e forward sideways downward velocities x z coordinates x
z angular rates x z quaternation far error space limits
decreasing probability visiting error state
previous experiments second step pi srl performed
initial case base b free failures built first step
following experiments performance second step pi srl different
initial policies used figure shows performance policies used initial
policies continuous black line indicates performance initial safe case
policy b average cumulative reward per episode used previous
experiments prior execution step two remaining lines
figure correspond performance three different initializations case base b
used experiments prior execution step two
poor initial policy dashed green lines helicopter crashed nearly
episodes average cumulative reward per episode calculated
different poor albeit less poor initial policy continuous red lines
helicopter crashed occasionally average cumulative reward per episode
finally near optimal policy dashed blue lines whereby helicopter hovering free
failures yields average cumulative reward per episode
figure b shows performance second step improving baseline behavior
step pi srl starting case base b corresponding poor poor
near optimal policies presented figure figure b dashed blue lines
correspond use case base b containing near optimal policy continuous
red lines correspond use case base b containing poor policy dashed
green lines correspond use case base b containing poor policy
experiments figure conducted high level risk domain


fisafe exploration state action spaces reinforcement learning

figure performance different initial policies helicopter hovering task
b performance different executions second step pi srl
starting case base b containing policy three different types
poor poor near optimal

graph indicates use near optimal policy initial
policy high level risk level case base worsen performance
fact appears improve slightly second step pi srl prevents degradation
initial performance b since updates cases case base made bad
episodes words updates b made cases gathered episodes
cumulative reward similar best episode found particular point
threshold whose value set cumulative reward best episode
example cumulative reward best episode episodes
cumulative reward higher used update case base discarding
bad episodes episodes failures way good sequences experiences
provided updates since proven good sequences experiences
cause adaptive agent converge stable useful policy bad sequences
may cause agent converge unstable poor policy wyatt solid red
lines figure b poor policy failures initial policy produces
higher number failures initial policy free failures however
despite poor initialization pi srl nevertheless able learn near optimal policy
well policy free failures used initialize b see lines corresponding high
level risk figure finally dashed green lines figure
b use poor initial policy many failures decreased
performance higher number failures produced even though nevertheless able
learn better behavior case falls local minimum probably
biased poor initialization cases poor policies number
failures higher beginning learning process decreases learning
process proceeds poor poor initial policies close
error space stark contrast initial policy shown figure
beginning already appears concentrated origin far error space


figarca fernandez

learning process proceeds different policies compressed away error
space number failures decreases
simba
business simulators powerful tools improving management decision making processes example tool simulator business administration simba
borrajo et al simba competitive simulator since agents compete
agents management different virtual companies simulator
twenty years experience university students business
executives emulates business realities variables relationships events
present business world objective provide users integrated vision
company basic techniques business management simplifying complexity
emphasizing content principles greatest educational value borrajo et al
experiments performed learning agent competes five handcoded agents borrajo et al decision making simba episodic task
decisions made sequentially make business decision state must studied
continuous decision variables e g selling price advertising expenses etc must
set followed study state composed continuous variables e g material
costs financial expenses economic productivity etc borrajo et al episode
composed steps although may prematurely company goes bankrupt e
losses higher net assets

figure modeling baseline behavior step simba task number steps per trial
executed case base b baseline behavior b cumulative reward per
ibl
trial learned safe case policy b
figure shows evolution number steps executed baseline behavior case base b two learning processes performing modeling baseline
behavior step computed following procedure described subsection
values respectively episodes approximately
learned figure b shows performance previouslysafe case policy b

learned b ib study mean profits per episode


fisafe exploration state action spaces reinforcement learning

figure improving learned baseline behavior step simba task mean
profits per episode different risk configurations obtained pi srl agent
five hand coded agents b mean profits per episode obtained
evolutionary risk sensitive rl agent five hand coded agents
cases episode ending failure bankruptcy noted
million euros ib
million euros obtained b
cases generated baseline behavior episodes stored
experiments demonstrate simba contrast previous domains storing cases sufficient obtaining safe policy performance similar
modeling baseline behavior step mean profits per episode million euros
learned execute improving learned baseline
safe case policy b
behavior step
similar findings earlier tasks figure indicates low medium
levels risk produce bankruptcies performance nevertheless weak highest
level risk produces near optimal policy low number number failures
contrast figure b presents evolutionary risk sensitive rl approaches former clearly yields highest number failures
risk sensitive case number bankruptcies cases insufficient learning risk function comparative figure pi srl
obtains better policies less failures evolutionary risk sensitive
rl approaches
figure shows graphical representation different solutions domain
shows mean number failures cumulative reward different approaches
episodes data computed independent executions
figure red circles correspond pi srl black triangles correspond
risk sensitive blue square corresponds evolutionary rl
figure shows performance two additional risk levels high
low respect figure experiments figure demonstrate
pi srl high level risk obtains highest cumulative reward
additionally pi srl low level risk low level risk
medium level risk approaches lowest



figarca fernandez

figure mean number failures company bankruptcies cumulative reward
episodes simba task means
computed different executions
mean number failures however pi srl medium level risk preferred
inasmuch performance superior terms cumulative reward pi srl
high level risk increases number failures obtains lower cumulative
reward compared pi srl high level risk pareto comparison
criterion pi srl high level risk strictly dominates solutions pi srl
risk sensitive pi srl evolutionary rl
strictly dominated solution
due difficulty representing high dimensional state action space
simba domain graphs provided evolution known space

related work
reinforcement learning rl case reasoning cbr techniques combined literature different ways work bianchi et al
presented permitting use cases heuristics speed rl additionally sharma et al use combination cbr rl called carl achieve
transfer playing game ai across variety scenarios madrts tm
commercial real time strategy game cbr used state value function
approximation rl gabel riedmiller however present study
knowledge first time cbr rl used conjunction safe exploration dangerous domains field safe reinforcement learning three principal
trends observed approaches return variance ii risk sensitive
approaches definition error states iii approaches teachers
approaches return variance
literature long known optimal policy optimal expected
return mdp quite sensitive parameter variations even optimal policy may


fisafe exploration state action spaces reinforcement learning

perform badly cases due stochastic nature mitigate
agent try maximize return associated worst case scenario
even though case may highly
unlikely thus trend risk refers worst
p
r variance example
outcomes return r



worst case control worst possible outcome r optimized coraluppi
marcus heger worst case control strategies optimality criterion
exclusively focused risk avoiding policies policy considered optimal
worst case return superior however restrictive inasmuch takes
rare scenarios fully account
value return introduced heger seen extension
worst case control mdps concept establishes returns r
policy occur probability lower neglected less
pessimistic pure worst case control given extremely rare scenarios effect
policy work heger et al idea weighting return risk namely
expected value variance criterion introduced
risk sensitive control use exponential utility functions return r
transformed reflect subjective measure utility instead maximizing expected
value r objective maximize u loge er parameter
r usual return shown depending parameter policies
high variance v r penalized enforced instead neuneier
mihatsch consider worst case outcomes policy e risk related
variability return study authors demonstrate learning
interpolates risk neutral worst case criterion limiting
behavior exponential utility functions noted approaches
variability return worst possible outcomes suited
policy small variance produce large risk geibel wysotzki
view risk present study however concerned variance
return worst possible outcome instead fact processes generally
possess unsafe states avoided consequently address different class
dealt approaches focusing variability return
risk sensitive approaches error states
second trend approaches concept risk definition error
states fatal transitions thus geibel et al instance establish risk
function probability entering error state instead hans et al consider
transition fatal corresponding reward less given threshold
first case demonstrated section learned td methods require
error states e car collisions pole balancing disequilibrium helicopter crashes
company bankruptcies visited repeatedly order approximate risk function
subsequently avoid dangerous situations second case concept risk
joined reward moreover mentioned studies assume
system dynamics known ii tolerate undesirable states exploration
contrast iii deal high dimensional
continuous state action spaces regarding latter geibel et al write


figarca fernandez

extended continuous action sets e g actor critic
method give details may done entirely continuous
section present solves
approaches teachers
last trend approaches use teachers three different ways
bootstrap learning e initialization procedure ii derive
policy finite demonstration set iii guide exploration process
bootstrapping learning
work driessens szeroski bootstraping procedure used relational rl finite set demonstrations recorded human expert
later presented regression allows regression build
partial q function later used guide exploration state space
boltzmann exploration strategy smart kaelbling use examples
training runs bootstrap q learning hedger
initial knowledge bootstrapped q learning allows agent learn
effectively helps reduce time spent random actions teacher behaviors
used form population seeding neuroevolution approaches yao siebel
sommer evolutionary methods used optimize weights neural networks
starting prototype network whose weights correspond teacher baseline
policy technique rl competition helicopter hovering task winners martin et
al developed evolutionary rl several teachers provided
initial population restricts crossover mutation operators allowing
slight changes policies given teachers consequently rapid convergence near optimal policy ensured indirect minimization
damage agent however teachers included initial population resulting
ad hoc training regimen conducted competition consequently proposed
seems somewhat ad hoc easily generalizable arbitrary rl
work koppejan et al neural networks evolved beginning
one whose weights corresponds teacher behavior
proven advantageous numerous applications evolutionary methods hernandez daz
et al koppejan whiteson koppejans nevertheless seems
somewhat ad hoc designed specialized set environments
deriving policy finite set demonstrations
approaches falling category framed according field learning
demonstration lfd argall et al highlighting study abbeel et al
apprenticeship learning composed three distinct steps
first teacher demonstrates task learned state action trajectories
teachers demonstration recorded second step state action trajectories seen
point used learn dynamics model system model near optimal policy found reinforcement learning rl finally
policy obtained tested running real system work tang et


fisafe exploration state action spaces reinforcement learning

al apprenticeship learning presented automaticallygenerating trajectories difficult control tasks proposal learning
parameterized versions desired maneuvers multiple expert demonstrations despite
approachs potential strengths general interest inherently linked
information provided demonstration dataset learner performance
heavily limited quality teachers demonstrations
guiding exploration process
driessens szeroski context relational rl use given teachers
policy rather policy derived current q function hypothesis
informative early learning stages selection actions
episodes performed teacher interleaved normal exploration episodes
mixture teacher normal exploration make easier regression
distinguish beneficial poor actions context lfd
approaches include teacher advice argall et al advice used improve
learner performance offering information beyond provided demonstration
dataset following initial task demonstration teacher agent
directly requests additional demonstration teacher different states
previously demonstrated states single action cannot selected
certainty chernova veloso
works mentioned trend explicit definition risk ever given

conclusions
work pi srl policy improvement safe reinforcement
learning high risk tasks described main contributions
definitions novel case risk function baseline behavior safe exploration
state action space use case risk function presented possible
inasmuch policy stored case base represents clear advantage
approaches e g evolutionary rl martn h lope koppejan whiteson
extraction knowledge known space agent impossible
weights neural networks additionally completely different notion
risk others found literature presented according notion risk
independent variance return reward function require
identification error states learning risk functions rather concept
risk described distance known unknown
space therefore domain independent parameter sense proposal allows
application parameter setting method described subsection
koppejan et al use function identify dangerous states contrast
definition function requires strong previous knowledge domain
furthermore approaches risk found literature tackle
entirely continuous geibel wysotzki report
one continuous domain koppejan whiteson consequently difficult know
certain approaches literature generalize easily arbitrary domains


figarca fernandez

presents pi srl great detail demonstrates effectiveness four entirely different continuous domains car parking pole balancing
helicopter hovering business management simba experiments presented
demonstrate different characteristics learning capabilities pi srl

pi srl obtains higher quality solutions experiments section demonstrate
save helicopter hovering task pi srl obtains cases best cumulative
reward per episode least number failures additionally pareto comparison criterion said save high risk configuration car parking
strictly dominated
ii pi srl adjusts initial known space safe better policies initial known
space resulting first step pi srl modeling baseline behavior adjusted
improved second step improving learned baseline behavior
additionally experiments demonstrate adjustment process compress
known space away error space e g pole balancing domain subsection
helicopter hovering domain subsection occasions require known
space move closer error space e g car parking subsection
event better policies found
iii pi srl works well domains differently structured state action spaces
value function vary sharply although car parking polebalancing domain helicopter hovering task business simulator represent
differently structured experiments study nevertheless demonstrate
pi srl performs well furthermore even domains car parking
value function varies sharply due presence obstacle
experimental demonstrate pi srl nevertheless successfully handle
difficulty however impossible avoid failures known space edge
edge error states would often explore error states
iv number failures depends distance known space
error space experiments pole balancing helicopter hovering domains demonstrate number failures depends close known space error
space due structure domains improving learned baseline behavior
step tends concentrate known space origin coordinates
away error space greater distance known space error
space lower number failures additionally helicopter hovering known
space beginning far error space consequently number failures low beginning therefore initial distribution known space
learned baseline policy later influences number failures obtained
second step pi srl
v pi srl completely safe first step executed however
proceeding way performance would heavily limited
capabilities baseline behavior learner performance improved beyond
performance baseline behavior subsequent exploratory process
second step pi srl must carried since complete knowledge domain
dynamic possessed however inevitable exploratory


fisafe exploration state action spaces reinforcement learning

process unknown regions state space visited agent may reach error
states
vi risk parameter allows user configure level risk assumed
user gradually increase value risk parameter order
obtain better policies assuming greater likelihood damage learning
system
vii pi srl performs successfully even poor initial policy failures used
experiments figure helicopter hovering domain demonstrate pi srl
able learn near optimal policy despite poor initialization policy
free failures used initialize case base b however figure shows
poor initial policy many failures used pi srl decreases performance
produces higher number failures although better behavior still learnt
case falls local minimum likely biased poor initialization
follows applicability method discussed allowing reader
clearly understand scenarios proposed pi srl may applicable
applicability restricted domains following characteristics
mandatory scenario satisfy two assumptions described section
according first assumption nearby states domain must necessarily similar actions according similar actions similar states produce similar
effects fact similar actions lead similar states assumes degree smoothness dynamic behavior system certain environments may hold
however clearly explain section consider assumptions logical
assumptions derived generalization principles rl literature kaelbling et al
jiang
ii applicability method limited size case base b required
mimic baseline behavior possible apply proposed tasks
first step pi srl modeling baseline behavior prohibitively large
number cases required properly mimic complex baseline behaviors case
threshold increased restrict addition cases casebase however increase may adversely affect final performance
nevertheless experiments performed section demonstrate relatively simple
baseline behaviors mimicked almost perfectly manageable number cases
iii pi srl requires presence baseline behavior proposed
method requires presence baseline behavior safely demonstrates task
learned baseline behavior conducted human teacher hand coded
agent important note nevertheless presence baseline behavior
guaranteed domains
finally logical continuation present study would take account automatic
graduation risk parameter along learning process example would
particularly interesting exploit fact known space far away error
space order increase risk parameter contrary reduce
close future work aims deploy real environments inasmuch
uncertainty real environments presents biggest challenge autonomous
robots autonomous robotic controllers must deal large number factors
robotic mechanical system electrical characteristics well environmental


figarca fernandez

complexity however use pi srl risk sensitive approaches
learning processes real environments could reduce amount damage incurred
consequently allow lifespan robots extended might worthwhile
add mechanism detect known state lead directly
error state currently investigated

acknowledgments
study partially supported spanish miciin projects tin c tra ccg uc tic offer gratitude special thanks
raquel fuentetaja pizan assistant professor universidad carlos iii de madrid
learning group plg generous invaluable comments
revision would thank jose antonio martn assistant
professor universidad complutense de madrid invaluable comments regarding
evolutionary rl

references
aamodt plaza e case reasoning foundational issues methodological variations system approaches ai communications
abbeel p coates hunter ng autonomous autorotation
rc helicopter iser pp
abbeel p coates ng autonomous helicopter aerobatics
apprenticeship learning j robotic res
abbott r g robocup robot soccer world cup xi chap behavioral cloning
simulator validation pp springer verlag berlin heidelberg
aha w tolerating noisy irrelevant novel attributes instance
learning international journal man machine studies
aha w kibler instance learning machine learning
pp
anderson c w draper b peterson behavioral cloning student
pilots modular neural networks proceedings seventeenth international
conference machine learning pp morgan kaufmann
argall b chernova veloso browning b survey robot learning
demonstration robotics autonomous systems
bartsch sprl b lenz hbner case reasoning survey future
directions puppe f ed xps vol lecture notes computer science
pp springer
bianchi r ros r de mantaras r l improving reinforcement learning
case heuristics vol pp lecture notes artificial intelligence springer lecture notes artificial intelligence springer


fisafe exploration state action spaces reinforcement learning

borrajo f bueno de pablo santos b n fernandez f garca j sagredo
simba simulator business education decission support
systems
boyan j moore sutton r proceedings workshop value function
approximation machine learning conference technical report cmu cs
chernova veloso confidence policy learning demonstration
gaussian mixture joint conference autonomous agents
multi agent systems
chernova veloso multi thresholded demonstration selection
interactive robot learning proceedings rd acm ieee international
conference human robot interaction hri pp york ny usa
acm
cichosz p truncating temporal differences efficient implementation
td lambda reinforcement learning journal artificial intelligence
jair
cichosz p truncated temporal differences function approximation successful examples cmac proceedings thirteenth european symposium
cybernetics systems emcsr
coraluppi p marcus risk sensitive minimax control discretetime finite state markov decision processes automatica
defourny b ernst wehenkel l risk aware decision making dynamic
programming nips workshop model uncertainty risk rl
driessens k ramon j relational instance regression relational rl
international conference machine learning icml pp
driessens k dzeroski integrating guidance relational reinforcement
learning machine learning
fernandez f isasi p local feature weighting nearest prototype classification
neural networks ieee transactions
fernandez f borrajo two steps reinforcement learning international
journal intelligent systems
floyd w esfandiari b toward domain independent case reasoning
imitation three case studies gaming workshop case
reasoning computer games th international conference case
reasoning iccbr pp
floyd w esfandiari b lam k case reasoning
imitating robocup players proceedings st international florida artificial
intelligence society conference pp
forbes j andre representations learning control policies
university south pp


figarca fernandez

gabel riedmiller cbr state value function approximation reinforcement learning proceedings th international conference case
reasoning iccbr pp springer
geibel p reinforcement learning bounded risk proceedings th
international conference machine learning pp morgan kaufmann
geibel p wysotzki f risk sensitive reinforcement learning applied control
constraints journal artificial intelligence jair
hans schneegass schafer udluft safe exploration reinforcement learning european symposium artificial neural network pp

heger consideration risk reinforcement learning th international
conference machine learning pp
hernandez daz g coello c c perez f caballero r luque j santanaquintero l v seeding initial population multi objective evolutionary gradient information ieee congress evolutionary
computation pp ieee
hester quinlan stone p real time model reinforcement learning
architecture robot control tech rep arxiv e prints arxiv
hu h kostiadis k hunter kalyviotis n essex wizards team
description birk coradeschi tadokoro eds robocup vol
lecture notes computer science pp springer
jiang x multiagent reinforcement learning stochastic games continuous
action spaces
kaelbling l littman moore reinforcement learning survey journal
artificial intelligence jair
konen w bartz beielstein reinforcement learning games failures
successes proceedings th annual conference companion genetic
evolutionary computation conference late breaking papers gecco pp
york ny usa acm
koppejan r whiteson neuroevolutionary reinforcement learning generalized helicopter control gecco proceedings genetic evolutionary
computation conference pp
koppejan r whiteson neuroevolutionary reinforcement learning generalized control simulated helicopters evolutionary intelligence
lee j lee j j multiple designs fuzzy controllers car parking
evolutionary pp may
luenberger g investment science oxford university press
mannor reinforcement learning average reward zero sum games shawetaylor j singer eds colt vol lecture notes computer science
pp springer


fisafe exploration state action spaces reinforcement learning

martin h j de lope j exa effective continuous actions
reinforcement learning industrial electronics iecon th
annual conference ieee pp
martn h j lope j learning autonomous helicopter flight evolutionary reinforcement learning th international conference computer
aided systems theory eurocast pp
mihatsch neuneier r risk sensitive reinforcement learning machine learning
moldovan abbeel p safe exploration markov decision processes
corr abs
narendra k thathachar l learning automata survey ieee
transactions systems man cybernetics smc
narendra k thathachar l learning automata introduction
prentice hall inc upper saddle river nj usa
ng kim h j jordan sastry autonomous helicopter flight
via reinforcement learning thrun saul l k scholkopf b eds nips
mit press
peters j tedrake r roy n morimoto j robot learning sammut c
webb g eds encyclopedia machine learning pp springer
poli r cagnoni genetic programming user driven selection experiments evolution image enhancement genetic programming
proceedings second annual conference pp morgan kaufmann
salkham cunningham r garg cahill v collaborative reinforcement learning urban traffic control optimization web intelligence
intelligent agent technology wi iat ieee wic acm international
conference vol pp
santamara j c sutton r ram experiments reinforcement
learning continuous state action spaces adaptive behavior

sharma holmes santamaria j irani isbell c ram transfer
learning real time strategy games hybrid cbr rl proceedings
twentieth international joint conference artificial intelligence
siebel n sommer g evolutionary reinforcement learning artificial neural
networks international journal hybrid intelligent systems
smart w kaelbling l p practical reinforcement learning continuous
spaces artificial intelligence pp morgan kaufmann
smart w kaelbling l p effective reinforcement learning mobile robots
icra pp ieee
sutton r barto g reinforcement learning introduction mit
press


figarca fernandez

tang j singh goehausen n abbeel p parameterized maneuver learning autonomous helicopter flight international conference robotics
automation icra
taylor e kulis b sha f metric learning reinforcement learning agents
proceedings international conference autonomous agents multiagent
systems aamas
van hasselt h wiering reinforcement learning continuous action
spaces approximate dynamic programming reinforcement learning
adprl ieee international symposium pp
wyatt j exploration inference learning reinforcement university
edinburgh
yao x evolving artificial neural networks pieee proceedings ieee





