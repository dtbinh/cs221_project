Journal Artificial Intelligence Research 45 (2012) 363-441

Submitted 03/12; published 10/12

Transforming Graph Data Statistical Relational Learning
Ryan A. Rossi

rrossi@purdue.edu

Department Computer Science, Purdue University
West Lafayette, 47907 USA

Luke K. McDowell

lmcdowel@usna.edu

Department Computer Science, U.S. Naval Academy
Annapolis, MD 21402, USA

David W. Aha

david.aha@nrl.navy.mil

Navy Center Applied Research Artificial Intelligence
Naval Research Laboratory (Code 5514)
Washington, DC 20375, USA

Jennifer Neville

neville@purdue.edu

Department Computer Science, Purdue University
West Lafayette, 47907 USA

Abstract
Relational data representations become increasingly important topic due
recent proliferation network datasets (e.g., social, biological, information networks)
corresponding increase application Statistical Relational Learning (SRL)
algorithms domains. article, examine categorize techniques
transforming graph-based relational data improve SRL algorithms. particular, appropriate transformations nodes, links, and/or features data dramatically
affect capabilities results SRL algorithms. introduce intuitive taxonomy
data representation transformations relational domains incorporates link transformation node transformation symmetric representation tasks. specifically,
transformation tasks nodes links include (i) predicting existence, (ii)
predicting label type, (iii) estimating weight importance, (iv) systematically constructing relevant features. motivate taxonomy detailed
examples use survey competing approaches tasks. discuss general conditions transforming links, nodes, features. Finally, highlight
challenges remain addressed.

1. Introduction
article, examine categorize techniques transforming relational data improve Statistical Relational Learning (SRL) algorithms. Below, Section 1.1 first introduces
relational data SRL. summarize primary types representations relational
data, explain focus data represented graphs. Section 1.1 describes
transforming content (rather type) representation improve SRL
analysis. instance, predicting new links graph increase accuracy relational
node classification. Section 1.2 identifies scope article. Finally, Section 1.3
summarizes organization approach article, includes description
taxonomy relational representation transformation.
c
2012
AI Access Foundation. rights reserved.

fiRossi, McDowell, Aha, & Neville

1.1 Relational Data, SRL, Representation Choices
majority research machine learning assumes independently identically distributed data. independence assumption often violated relational data, encode dependencies among data instances. instance, people often linked business
associations, information one person highly informative prediction
task involving associate person. generally, relational data described
set nodes, connected one types relations (or links).
Relational information seemingly ubiquitous; present domains Internet
world-wide web (Faloutsos, Faloutsos, & Faloutsos, 1999; Broder et al., 2000; Albert, Jeong, & Barabasi, 1999), scientific citation collaboration (McGovern et al., 2003;
Newman, 2001b), epidemiology (Pastor-Satorras & Vespignani, 2001; Moore & Newman,
2000; May & Lloyd, 2001; Kleczkowski & Grenfell, 1999) communication analysis (Rossi
& Neville, 2010), metabolism (Jeong, Tombor, Albert, Oltvai, & Barabasi, 2000; Wagner
& Fell, 2001), ecosystems (Dunne, Williams, & Martinez, 2002; Camacho, Guimera, &
Nunes Amaral, 2002), bioinformatics (Maslov & Sneppen, 2002; Jeong, Mason, Barabasi,
& Oltvai, 2001), fraud terrorist analysis (Neville et al., 2005; Krebs, 2002), many
others. links data may represent citations, friendships, associations, metabolic
functions, communications, co-locations, shared mechanisms, many explicit implicit relationships.
Statistical relational learning (SRL) methods developed address problems reasoning learning domains complex relations probabilistic structure
(Getoor & Taskar, 2007). particular, SRL algorithms leverage relational information
attempt learn models higher predictive accuracy. key characteristic many
relational datasets correlation statistical dependence values
attribute across linked instances (e.g., two friends likely share political views
two randomly selected people). relational autocorrelation provides unique opportunity increase accuracy statistical inferences (Jensen, Neville, & Gallagher,
2004). Similarly, relational information exploited many reasoning tasks
identifying useful patterns optimizing systems (Easley & Kleinberg, 2010).
Representation issuesincluding knowledge, model, data representationhave
heart artificial intelligence community decades (Amarel, 1968; Minsky, 1974;
Russell & Norvig, 2009). important, focus data representation issues, simple examples include choices whether discretize continuous
features add higher-order polynomial features. decisions significant
effect accuracy efficiency AI algorithms. especially critical
performance SRL algorithms because, relational domains, even larger space
potential data representations consider. complex structure relational data
often represented variety ways choice specific data representation
impact applicability particular models/algorithms performance.
Specifically, two categories decisions need considered context
relational data representation.
First, consider type data representation use (cf., hierarchy
De Raedt, 2008, ch. 4). instance, relational data propositionalized
application standard, non-relational learning algorithms. often, order fully
364

fiTransforming Graph Data Statistical Relational Learning

exploit relational information, SRL researchers chosen represent data either
using attributed graph relational database (see e.g., Friedman, Getoor, Koller, &
Pfeffer, 1999), via logic programs (see e.g., Kersting & De Raedt, 2002).1 choice
different strengths. article, focus graph-based representation,
common choice addressing growing interest network data applications analyzing electronic communication online social networks Facebook,
Twitter, Flickr, LinkedIn (Mislove, Marcon, Gummadi, Druschel, & Bhattacharjee,
2007; Ahmed, Berchmans, Neville, & Kompella, 2010). Specifically, assume graphbased data representation G = hV, E, XV , XE nodes V entities (e.g., people,
places, events) links E represent relationships among entities (e.g., friendships, citations). XV set features entities V . Likewise, set features
XE provides information relation links E.
Next, given type representation, must consider specific content data
representation, large space choices. instance, features nodes
links graph constructed using wide range aggregation functions, based
multiple kinds links paths. SRL researchers already recognized importance
data representation choices (e.g., Getoor & Diehl, 2005), many separate studies
examined techniques feature construction (Neville, Jensen, Friedland, & Hay, 2003),
node weighting (Tang, Musolesi, Mascolo, & Latora, 2009), link prediction (Taskar, Wong,
Abbeel, & Koller, 2003), etc. However, article first comprehensively survey
approaches relational representation transformation graph-based data.
Given set (graph-based) relational data, define relational representation transformation change space links, nodes, and/or features used represent
data. Typically, goal transformation improve performance
subsequent SRL application. instance, Figure 1 original graph representation G
transformed new representation G links, nodes, features (such link
weights) added, links removed. SRL algorithm
analysis applied new representation, instance classify nodes
identify anomalous links. particular transformations used produce G
vary depending upon intended application, sometimes substantially improve
accuracy, speed, complexity final application. instance, Gallagher, Tong,
Eliassi-Rad, Faloutsos (2008) found adding links similar nodes could increase node classification accuracy 15% tasks. Similarly, Neville
Jensen (2005) demonstrated adding nodes represent underlying groups enabled
simpler inference increased accuracy.
1.2 Scope Article
article focuses examining categorizing various techniques changing
representation graph-based relational data. shown Figure 1, typically view
changes pre-processing step enables increased accuracy speed
task, object classification. However, output techniques
valuable. instance, administrators social network may interested
1. latter case, applicable SRL algorithms often referred probabilistic inductive logic
programming (ILP) (De Raedt & Kersting, 2008).

365

fiRossi, McDowell, Aha, & Neville

C

C
SRL Analysis /
Application

Representation
Transformation

L
L

L

G


G


Result

Figure 1: Example Transformation Subsequent Analysis: original relational representation G transformed G dotted lines represent predicted links, squares represent predicted nodes, bold links represent link
weighting. Changes may based link structure, link features, node features (here, similar node shadings indicate similar feature values). SRL
analysis applied new representation. example, SRL
analysis produces label (C L) node, example task discussed Section 2.1. article focuses representation transformation
(left side figure), subsequent analysis.

link prediction predicted links presented users potential new
friendship links. Alternatively, techniques may applied improve
comprehensibility model. example, prediction protein-protein interactions
provides insights protein function (Ben-Hur & Noble, 2005). Thus, techniques
survey may used multiple purposes, relevant publications may used
different contexts. Regardless original context, examine general
applicability benefits technique. techniques applied,
transformed data used (e.g., friendship suggestions), examined greater
understanding, used task (e.g., object classification), used recursively
input another representation change (e.g., object/node prediction followed
link prediction).
attempt survey many methods could used SRL analysis
(e.g., right side Figure 1), although relevant set methods analysis
overlaps set methods facilitate transformations consider. instance, collective classification (Neville & Jensen, 2000; Taskar, Abbeel, & Koller, 2002)
important SRL application define Section 2 use running example
SRL analysis task. output classification could used create
new attributes nodes (a data representation change). discuss possibility
Section 6.2, focus cases node labeling particularly useful
pre-processing step (e.g., applying certain stacked algorithms), rather surveying wide range possible classification algorithms, whether collective not. Likewise,
survey issues model knowledge representation, whether sta366

fiTransforming Graph Data Statistical Relational Learning

tistical dependencies nodes, links, features modeled Structural
Logistic Regression (Popescul, Popescul, & Ungar, 2003b) Markov Logic Network
(Domingos & Richardson, 2004). consider issues briefly, Section 8.4.
Furthermore, focus transformations change content graph data
representation. particular, examine transformations graph data modify
set links nodes, modify features. consider changing graph data
different type representation, e.g., propositionalizing data changing
logic program. However, transformations discuss, node link
feature aggregation, form propositionalization. addition, Section 6.3.3 describes
number techniques structure learning logic programs, techniques
closely related analogous problem feature construction graph-based representations. Finally, many techniques discuss applicable
logical representations. instance, link weighting could applied weight known
relations using logic program detect anomalous objects. focus, however,
methods useful transforming graph-based representations.
1.3 Approach Organization Article
many dimensions relational data transformation, complicate task
understanding selecting appropriate techniques. assist process,
introduce simple intuitive taxonomy representation transformation identifies link transformation node transformation symmetric representation tasks.
specifically, transformation tasks nodes links include (i) predicting
existence, (ii) predicting label type, (iii) estimating weight importance,
(iv) constructing relevant features. addition, propose taxonomy constructing link node features consists non-relational features, topology features,
relational node-value features, relational link-value features. relational transformation task, survey applicable techniques, examine necessary conditions,
provide detailed examples comparisons.
article organized follows. next section presents taxonomy relational
representation transformation discusses motivating example. Section 3, review
algorithms link prediction, Section 4 examines task link interpretation
(i.e., constructing link labels, link weights, link features). Sections 5 6 consider
corresponding prediction interpretation tasks nodes instead links. Section
7, summarize algorithms jointly transform nodes links. Section 8 discusses
methods evaluating representation transformations challenges future work,
Section 9 concludes.

2. Overview Motivating Example
section first introduce running example based classification data
Facebook, describe relational algorithms could used perform task.
Next, introduce taxonomy relational representation transformation explain
type transformation could aid Facebook classification task. Finally,
formally define type relational representation transformation.
367

fiRossi, McDowell, Aha, & Neville

2.1 Motivating SRL Analysis Example: Classification Task
example, consider hypothetical data inspired Facebook (www.facebook.com),
one popular online social networks. assume given graph
G = hV, E, XV , XE nodes V users 2 links E represent friendships
Facebook. XV set features users V gender, relationship
status, school, favorite movies, musical preference (though information may missing
users). Likewise, set features XE provides information friendship
links E time formation possibly contents message
sent link formation requested one users.
example SRL analysis task (see Figure 1) predict political affiliation (liberal,
moderate, conservative) every node (person) G. assume affiliation,
call class label node, known people G.3
Moreover, assume users political affiliation likely correlated
characteristics user (to lesser degree) users friends. next section
summarizes correlations used classification.
example, assume links simple, binary friendship connections. However, link types could used represent kinds relationships. instance,
link might indicate two people communicated via wall-post message,
two people chosen join Facebook group. addition, notion friendship Facebook weak thus significant portion persons friends often
casual acquaintances. Thus, representation changes link deletion weighting
may significant impact classification accuracy. notational purposes, add
tilde top graph components symbol indicate undergone
transformation (e.g., modified link set E denoted E).
2.2 Background: Features Methods Classification
predict political affiliation Facebook users, conventional classification approaches
would ignore links classify user using information known user,
gender location. assume information represented
form non-relational features, features computed directly
XV without considering links E. refer classification based
features non-relational classification. Alternatively, relational classification, links
explicitly used construct additional relational features capture information
users friends. instance, relational feature could compute, user,
proportion friends male live particular region. Using relational
information potentially increase classification accuracy, though may sometimes decrease
accuracy well (Chakrabarti, Dom, & Indyk, 1998). Finally, even greater (and usually
reliable) increases occur class labels (e.g., political affiliations)
linked users used instead derive relevant features (Jensen et al., 2004). instance,
2. general, may one type node. instance, nodes citation network may
represent papers authors.
3. Later, discuss representation change node labeling, constructs estimated label
every node. discussed Section 1.2, representation changes sometimes resemble output
SRL analysis, focus changes particularly useful pre-processing
subsequent SRL analysis.

368

fiTransforming Graph Data Statistical Relational Learning

class-label relational feature could compute, user, proportion friends
liberal views. However, using features challenging since
labels initially unknown, thus typically must estimated iteratively
refined way. process jointly inferring labels interrelated nodes
known collective classification (CC).
CC requires models inference procedures use inferences one user
affect inferences related users. Many algorithms considered CC,
including Gibbs Sampling (Jensen et al., 2004), relaxation labeling (Chakrabarti, Dom, &
Indyk, 1998), belief propagation (Taskar et al., 2002), ICA (Neville & Jensen, 2000; Lu &
Getoor, 2003), weighted neighbor techniques (Macskassy & Provost, 2007). See
work Sen et al. (2008) survey.
concrete example SRL analysis, explain many techniques survey
terms Facebook classification task, special emphasis CC. However,
features transformation techniques apply many SRL tasks data sets
relationship classification, anomalous link detection, entity resolution, group
discovery (Getoor & Diehl, 2005).
2.3 Representation Transformation Tasks Improving SRL
Figure 2 shows proposed taxonomy relational representation transformation.
two main tasks taxonomy link transformation node transformation.
find powerful elegant symmetry two tasks. particular,
link node representation transformation tasks decomposed prediction
interpretation tasks. former task involves predicting existence new nodes
links. latter task interpretation involves three parts: constructing weights,
labels, features nodes links. Together, yields eight distinct transformation tasks
shown leaves taxonomy Figure 2. Underneath eight tasks
figure, list primary graph component modified task (i.e., V , E, XV ,
XE ), followed illustration possible representation change task.
text below, summarize Figure 2, organized around four larger categories link
prediction, link interpretation, node prediction, node interpretation.
First, link prediction adds new links graph. sample graph task
(Figure 2A) shows link predicted similarity two nodes
used predict new link them. Intuitively, Facebook users share values
many non-relational features may share political affiliation. Thus, adding
links people increase autocorrelation improve accuracy collective classification. many simple link prediction algorithms based similarity,
neighbor properties, shortest path distances, infinite sums paths (i.e. random walks),
strategies. Section 3 provides detail techniques.
Second, several types link interpretation, involves constructing
weights, labels, features existing links. instance, many graphs (including
Facebook data), links (or friendships) equal importance. Thus, Figure 2B
shows result performing link weighting. case, weights based similarity feature values pair linked nodes, assumption
high similarity may indicate stronger relationships. (Link prediction techniques may
369

fiRossi, McDowell, Aha, & Neville

Relational
Representation
Transformation

input


E

weighted link

labeled link

predicted node

weighted node

labeled node

Node Transformation

Link Interpretation

Node Prediction

Link Weighting

Link Labeling

Link Feature
Construction

X E

X E

X E

V

Node Interpretation

Node Weighting

Node Labeling

Node Feature
Construction

X V

X V

X V

p
p

B.

C
w

w

A.

p

L

Link Transformation

Link Prediction

predicted link

2

+

3

3

p

C.

-

C



D.

E.

F.

L
G.

.1 B

.2

L

-

.3 B

.5

.3

H.

Figure 2: Relational Representation Transformation Taxonomy: Link node
transformation formulated symmetric tasks leading four main transformation tasks: predicting links, interpreting links, predicting nodes, interpreting nodes. task yields modified graph component: E, XE , V ,
XV , respectively. Interpretation divided weighting, labeling,
constructing features. Examples tasks relational representation
transformation shown leaves taxonomy. example
graphs, nodes similar shadings similar feature values.

370

fiTransforming Graph Data Statistical Relational Learning

use similarity measures, identifying probable new links, rather weighting
existing links.) Alternatively, link labeling may used assign kind discrete label
link. instance, Figure 2C shows links might labeled either personal
(p) work (w) related, e.g., based known feature values analysis communication events linked users. hand, links might instead labeled
positive negative influence (i.e., labeled +/). Finally, Figure 2D shows
link feature construction used add general kinds feature values
link. instance, link feature might count number communication events
occurred two people number friends common. Link weighting
labeling could perhaps viewed special cases link feature construction,
separate later sections show useful techniques task
differ. three link interpretation tasks could help example classification
problem. particular, model learned predict political affiliation might choose place
special emphasis links highly weighted labeled personal.
link features might used represent complex dependencies, instance modeling influence users work friendships, friendship links nodes
large number friends common. details techniques
provided Section 4.
Third, node prediction adds additional nodes (and associated links) graph.
instance, Figure 2E shows result relational clustering applied
discover two latent groups graph, user connected one latent
group node. discovered node Facebook might represent types social processes,
influences, tightly knit group friends. clustering techniques used
identify new nodes could designed identify people particularly similar
respect relevant characteristic, political affiliation. new nodes
associated links could used several ways. instance, though present
small example Figure 2E, nodes far away (in terms shortest
path length) original graph may much closer new graph. Thus, links
latent node may allow influence propagate effectively algorithm
CC applied. Alternatively, identification distinct latent groups may even enable
efficient accurate algorithms applied separately group (Neville & Jensen,
2005). Node prediction discussed Section 5.
Finally, several types node interpretation, involves constructing
weights, labels, feature values existing nodes. instance, links,
nodes may influential others thus weight. Figure 2F
demonstrates node weighting, weights might assigned based numbers
friends via PageRank/eigenvector techniques. See Section 6.1 details.
Alternatively, Figure 2G shows example node labeling. graph represents
training graph, node given estimated label conservative (C),
liberal (L), moderate (M). labels might estimated using non-relational
features via textual analysis. classification algorithms learn model based
true labels training graph, approaches instead first compute estimated
labels, learn model new representation (Kou & Cohen, 2007). Section 6.2
discusses simplify inference. Finally, Figure 2H shows result node feature
construction, arbitrary feature values added node. instance, suppose
371

fiRossi, McDowell, Aha, & Neville

find users relatively Facebook friends often moderate
many friends often liberal. case, feature counting number friends
node would useful. directly exploit autocorrelation, different feature might
count proportion users friends conservative, common political
affiliation users friends. feature correlated political affiliation could
used improve performance classification algorithm example problem.
Identifying and/or computing features essential performance SRL
algorithms challenging; Section 6.3 considers process.
Table 2.3, summarize prominent techniques performing
tasks link prediction, link interpretation, node prediction, node interpretation.
Sections 3-6 provide detail category turn.
2.4 Relational Representation Transformation: Definitions Terminology
assume initial relational data represented graph G = hV, E, XV , XE
vi V corresponds node edge eij E corresponds
(directed) link nodes j. XV set features nodes V ,
XkV XV k th feature. Likewise, XE set features links
E, XkE k th feature. features XE could refer link weights,
distances, types, among possibilities. preceding notation lets us identify,
instance, values particular feature XkV nodes. Alternatively, xvi refers
vector containing feature values particular node vi , xeij contains
feature values particular edge eij . Table 2.3 summarizes notation.
Relational representation transformation process transforming original
graph G new graph G = hV , E, XV , XE arbitrary set transformation techniques. process, nodes, links, weights, labels, general features may
added, nodes links may removed. theory, transformation seeks
optimize objective function (for instance, maximize autocorrelation), although
practice objective function may completely specified guaranteed improved transformation. define specifically four primary parts
relational representation transformation:
Definition 2.1 (Link Prediction) Given nodes V , observed links E and/or feature
set X = (XE , XV ), link prediction task defined creation modified link set
E E 6= E. Usually, involves adding new links present E,
links may deleted.
Definition 2.2 (Link Interpretation) Given nodes V , observed links E and/or
feature set X = (XE , XV ), link interpretation task defined creation new
link feature XkE XkE
/ XE . task may estimate feature value every link.
Alternatively, values XkE may partially estimated, example, original
features missing values additional links introduced link prediction.
Definition 2.3 (Node Prediction) Given nodes V , links E and/or feature set
X = (XE , XV ), node transformation defined creation modified node set V
V V . addition, many node prediction tasks simultaneously create new links,
372

fiTransforming Graph Data Statistical Relational Learning

Relational Representation Transformation
Links
?

Prediction

?
?

?

Weighting

?

?

?
?

Labeling

?
?

Feature

Nodes

Adamic/Adar (Adamic &
Adar, 2001), Katz (Katz, 1953),
others (Liben-Nowell &
Kleinberg, 2007)
Text Feature Similarity
(Macskassy, 2007)
Classification
via
RMN
(Taskar et al., 2003) SVM
(Hasan, Chaoji, Salem, & Zaki,
2006)
Latent Variable Estimation (Xiang, Neville, & Rogati,
2010)
Linear Combination Features (Gilbert & Karahalios,
2009)
Aggregating Intrinsic Information (Onnela, Saramaki,
Hyvonen, Szabo, Lazer, Kaski,
Kertesz, & Barabasi, 2007)
LDA (Blei et al., 2003), PLSA
(Hofmann, 1999),
Link Classification via Logistic Regression (Leskovec, Huttenlocher, & Kleinberg, 2010),
Bagged Decision Trees (Kahanda & Neville, 2009),
Link
Feature
Similarity
(Rossi & Neville, 2010)
Link Aggregations (Kahanda
& Neville, 2009)

?

Graph Features (Lichtenwalter, Lussier, & Chawla, 2010)

?

?
?

?

Betweenness (Freeman, 1977),
Closeness (Sabidussi, 1966)

?

HITs (Kleinberg, 1999), Prob.
HITs (Cohn & Chang, 2000),
SimRank (Jeh & Widom, 2002)
PageRank (Page, Brin, Motwani, & Winograd, 1999), Topical PageRank (Haveliwala, 2003;
Richardson & Domingos, 2002)
LDA (Blei et al., 2003), PLSA
(Hofmann, 1999),
Node
Classification
via
Stacked Model (Kou & Cohen, 2007) RN (Macskassy &
Provost, 2003)

?

?
?

?
?

Construction
?

Spectral Clustering (Neville
& Jensen,
2005),
MixedMembership
Relational
Clustering (Long et al., 2007)
LDA (Blei, Ng, & Jordan, 2003),
PLSA (Hofmann, 1999),
Hierarchical Clustering via
Edge-betweenness (Newman &
Girvan, 2004)

MLN Structure Learning (Kok
& Domingos, 2009, 2010)
Database
Query
Search
(Popescul et al., 2003b), RPT
(Neville, Jensen, Friedland, et al.,
2003)
FOIL, nFOIL (Landwehr, Kersting, & De Raedt, 2005), kFOIL
(Landwehr, Passerini, De Raedt,
& Frasconi, 2010), Aleph (Srinivasan, 1999),

Table 1: Summary Techniques: summary prominent graph transformation techniques tasks predicting existence nodes links interpreting
weighting, labeling, constructing general features.

373

fiRossi, McDowell, Aha, & Neville

Symbol

Description

G

Initial graph

G

Transformed graph

E

Initial link set

V

Initial node set

E

Initial set link features

V

Initial set node features

X
X

XkE
XkV
xeij
xvi
symbols

(vi )


Initial link feature k (XkE XE ) (for one feature, values links)
Initial node feature k (XkV XV ) (for one feature, values nodes)
Initial feature vector eij (for one link, values link features)
Initial feature vector vi (for one node, values node features)
Description
Adjacency matrix graph
Neighbors vi
Cut-off value

Table 2: Summary Notation used Survey: top half table shows
symbols sometimes written tilde top symbol, indicating
result transformation. conciseness, table demonstrates
notation G G.

e.g., initial node vi V predicted node vj V . Thus, task may
produce modified link set E.
Definition 2.4 (Node Interpretation) Given nodes V , observed links E and/or
feature set X = (XE , XV ), node interpretation task defined creation new
/ XV . link interpretation, values XkV may
node feature XkV XkV
estimated nodes. node feature XkV could represent node weights,
labels, general features.
Section 2.2 introduced notion non-relational feature, node feature
XkV constructed without making use links (i.e., without using E XE ).
features sometimes referred articles attributes intrinsic features.
important terms referred multiple different ways. aid reader,
Table 2.4 summarizes key synonyms terms found often
literature.

3. Link Prediction
section focuses predicting existence links Section 4 considers link
interpretation. Given initial graph G = hV, E, XV , XE i, interested creating
modified link set E, usually prediction new links present
374

fiTransforming Graph Data Statistical Relational Learning

Term

Potential synonyms

Nodes

Vertices, points, objects, entities, individuals, users, constants, ...

Links

Edges, relationships, ties, arcs, events, interactions, predicates

Topology

Link/network/graph structure, relational information

Features

Attributes, variables, co-variates, queries, predicates, ...

Graph Measures

Topology-based metrics (such proximity, centrality, betweenness, ...)

Similarity

Distance (the inverse similarity), likeness

Clusters

Classes, communities, groups, roles, topics

Non-relational Features

Intrinsic attributes/features, local attributes/features, ...

Relational Features

Features, link-based features, graph features, aggregates, queries, ...

Structure Learning

Feature generation/construction, hypothesis learning

Parameter Learning

Model selection, function learning

Table 3: Synonyms Literature: summary possible synonyms found
literature important terms related relational data.

E. task motivated several ways. instance, may need
predict missing links present E incomplete data collection
problems. Similarly, may interested predicting hidden links,
assume exists unobservable interactions goal discover
model interactions. example, network representing criminals terrorist
activity, may seek predict link two people (nodes) directly
connected whose actions share common motivation cause. missing
hidden links, predicting links may improve accuracy subsequent learned
model. Alternatively, may seek predict future links evolving network,
new friendships connections formed next year. might interested
predicting links objects spatially related. Finally, may wish predict
beneficial links, instance, predicting pairs individuals likely successful
working together.
Figure 3 summarizes one general approach often used link prediction
tasks. summary, scores weights computed every pair nodes graph,
shown Figure 3(b). Predicted links weight greater threshold , along
original links, used create new link set E + (shown Figure 3(e)). (At
step, original links low weight could deleted appropriate.)
final step, weights predicted links often discarded, yielding new graph
uniform link weights shown Figure 3(f).
key challenge approach compute weight score possible
link. information used computation provides natural way categorize
link prediction techniques. Below, Section 3.1 describes techniques use
non-relational features nodes (ignoring initial links), Section 3.2 describes
topology-based techniques use graph structure (i.e., links relations).
375

fiRossi, McDowell, Aha, & Neville

(a) Initial Graph G = hE, V

(b) Weighted Links wij E

(c) Predicted Links (E E)

(d) Pruning Predicted Links
(E > )

(e) E + := E > + E

(f) E + Uniform Link
Weights

Figure 3: Example Demonstrating General Approach Link Prediction:
initial graph (a) used input link predictor, yielding complete
graph (b) weights wij estimated pairs nodes.
next step shows removal initial (observed) links consideration (c),
followed pruning predicted links weight cut-off value
(d). remaining predicted links combined initial links (e).
Often, estimated weights initial predicted links discarded,
leaving uniform weight graph (f).

Finally, Section 3.3 describes hybrid techniques exploit node features
graph structure.
3.1 Non-relational (Feature-Based) Link Prediction
section, consider link predictors exploit graph structure relational features derived using graph structure. given arbitrary pair nodes
376

fiTransforming Graph Data Statistical Relational Learning

vi vj graph node represented feature vector xvi
xvj , respectively. Feature-based link prediction defined using arbitrary similarity
measure S(xvi , xvj ) means estimate likelihood link exist vi
vj . Typically, link created similarity exceeds fixed cut-off value; another
strategy predict links among n% node pairs highest similarity.
traditional approach simply define measure similarity two objects,
possibly based knowledge application and/or problem-domain. many
similarity metrics proposed mutual information, cosine similarity,
many others (Lin, 1998). instance, Macskassy (2007) represents textual content
node feature vector uses cosine similarity create new links nodes
graph. Macskassy showed combination initial links predicted
text-based links increased classification accuracy compared using initial links
text-based links. addition leveraging textual information predict links,
might use arbitrary set features combined proper measure similarity
link prediction. instance, many recommender systems implicitly predict link
two users based similarity ratings items movies books
(Adomavicius & Tuzhilin, 2005; Resnick & Varian, 1997). case, cosine similarity
correlation commonly used similarity metrics.
Alternatively, similarity measure learned predicting link existence. link
prediction problem transformed standard supervised classification problem
binary classifier trained determine similarity two nodes based
feature vectors. One approach work Hasan et al. (2006), used
Support Vector Machines (SVMs) link prediction found non-relational feature
(keyword match count) useful predicting links bibliographic network.
many link prediction approaches (Taskar et al., 2003; Getoor, Friedman, Koller,
& Taskar, 2003) apply traditional machine learning algorithms. However,
use features based graph structure well non-relational features
focus section. Thus, discuss techniques Section 3.3.
Finally, variants topic models used link prediction. types models
traditionally use text documents (non-relational information) infer mixture latent topics document. Inter-document topic similarity used
similarity metric link prediction (Chang & Blei, 2009). However, many topic
models capable performing joint transformation nodes links, defer full
discussion techniques Section 7.
3.2 Topology-Based Link Prediction
Topology-based link prediction uses local relational neighborhood and/or global
graph structure predict existence unobserved links. Table 3.2 summarizes
common metrics used task. Below, discuss many
approaches, starting simplest local metrics moving complex
techniques based global measures and/or supervised learning. systematic study
many approaches applied social network data, see work Liben-Nowell
Kleinberg (2007).
377

fiRossi, McDowell, Aha, & Neville

Local Node Metrics

Description

Common Neighbors

Number common neighbors x y, w(x, y) = |(x)(y)| (Newman,
2001a)

Jaccards Coefficient

Probability x share common neighbors (normalized),
|(x)(y)|
(Jaccard, 1901; Salton & McGill, 1983)
|(x)(y)|

Adamic/Adar

Similar toPcommon neighbors, assigns weight rare neighbors,
1
w(x, y) = z(x)(y) log |(z)|
(Adamic & Adar, 2001)

RA

Essentially equivalent Adamic/Adar |(z)| small,
P
1
w(x, y) = z(x)(y) |(z)|
(Zhou, Lu, & Zhang, 2009)

Preferential Attachment

Probability link x product degree x y,
w(x, y) = |(x)| |(y)| (Barabasi & Albert, 1999)

Cosine Similarity

|(x)(y)|
w(x, y) =

(Salton & McGill, 1983)

Sorensen Index

w(x, y) =

(Green, 1972; Zhou et al., 2009)

Hub Index

Nodes large degree likely assigned higher score,
w(x, y) =

|(x)||(y)|
2|(x)(y)|
|(x)|+|(y)|

|(x)(y)|
min{|(x)|,|(y)|}

w(x, y) =

(Ravasz, Somera, Mongru, Oltvai, & Barabasi, 2002)
|(x)(y)|
max{|(x)|,|(y)|}

(Ravasz et al., 2002)

Hub Depressed Index

Analogous Hub Index, w(x, y) =

Leicht-Holme-Newman

Assigns large weight pairs many common neighbors, normalized
|(x)(y)|
expected number common neighbors, w(x, y) = |(x)||(y)| (Leicht,
Holme, & Newman, 2006)

Global Graph Metrics

Description

Graph Distance

Length shortest path x

Katz

Number paths x y, exponentially damped length thereby
assigning weight shorter paths, w(x, y) = [(I A)1 ]xy (Katz, 1953)

Hitting time

Number steps required random walk starting x reach (Brightwell
& Winkler, 1990)

Commute Time

Expected number steps reach node starting x returning
+
+
back x, defined w(x, y) = L+
xx + Lyy 2Lxy L Laplacian matrix
(Gobel & Jagers, 1974)

Rooted PageRank

Similar Hitting time, step probability random
walk reset starting node x, w(x, y) = [(IP)1 ]xy P = D1
(Page et al., 1999)

SimRank

x similar extent joined similar neighbors,
P

w(x, y) =

P

v(y) sim(u,v)
|(x)||(y)|

u(x)

(Jeh & Widom, 2002)

K-walks

Number walks length k x y, defined w(x, y) = [Ak ]xy

Meta-Approaches

Description

Low-rank Approximation

Compute rank-k matrix Ak best approximates (hopefully reducing
noise), compute similarity Ak using local global metric
(Eckart & Young, 1936; Golub & Reinsch, 1970)

Unseen Bigrams

Compute initial scores using local global metric, augment scores
w(x, y) using values w(z, y) nodes z similar x (Essen &
Steinbiss, 1992; Lee, 1999)

Clustering

Compute initial scores using local global metric, discard links
lowest scores, re-compute scores modified graph (Johnson,
1967; Hartigan & Wong, 1979)

Table 4: Topology Metrics: Summary common metrics link prediction.
Notation: Let (x) neighbors x adjacency matrix G.
378

fiTransforming Graph Data Statistical Relational Learning

3.2.1 Metrics Based Local Neighborhood Nodes
simplest approaches use local neighborhood nodes graph devise
measure topology similarity, use pairwise similarities nodes predict
likely links. shown Table 3.2, numerous metrics, often based
number neighbors two nodes share common, varying strategies
normalization.
Zhou et al. (2009) compares nine local similarity measures six datasets finds
simplest link predictor, common neighbors, performs best overall.
propose new metric, RA, outperforms initial nine metrics two datasets.
new metric similar Adamic/Adar metric, uses different normalization factor yields better performance networks higher average degree.
propose method uses additional two-hop information avoid degenerate cases
links assigned similarity score. results highlight importance
selecting appropriate metrics specific problems datasets. another related
investigation, Clauset, Moore, Newman (2008) evaluate hierarchical random graph
predictor local topology metrics common neighbors, Jaccards coefficient
degree product three types networks: metabolic, ecology social network.
find baseline measure based shortest paths performs best metabolic
network, relationships homogeneous, hierarchical metric
performs best links create complex relationships, predator-prey
relationships found ecology network.
Liu Lu (2010) proposed local random-walk algorithm efficient alternative
global random-walk predictors large networks. method evaluated alongside
metrics (i.e., common neighbors, local paths, RA, random-walk variants)
shown perform better networks efficiently global
random-walk models.
3.2.2 Metrics Based Global Graph Structure
sophisticated similarity metrics based global graph properties, often involving
weighted computation based number paths pair nodes.
instance, Katz measure (1953) counts number paths pair nodes,
shorter paths count computation. Rattigan Jensen (2005) demonstrated even fairly simple metric could effective task anomalous link
prediction, identification statistically unlikely links among links
initial graph.
related measure hitting time metric, average number steps
required random walk starting node x reach node y. Gallagher et al. (2008)
use random walks restart estimate similarity every pair nodes.
focus sparsely labeled networks unlabeled nodes may labeled
nodes support learning and/or inference relational classification. prediction
new links improves flow information labeled unlabeled nodes, leading
increase classification accuracy 15%. Note adding teleportation probabilities
random walk approach roughly yields PageRank algorithm said
heart Google search engine (Page et al., 1999).
379

fiRossi, McDowell, Aha, & Neville

SimRank metric (Jeh & Widom, 2002) proposes two nodes x similar
linked neighbors similar. Interestingly, show approach
equivalent metric based time required two backwards, random walks
starting x arrive node. approaches based
random walks, metric could computed via repeated simulations,
efficiently computed via recursive set-point approach.
3.2.3 Meta-approaches Supervised Learning Approaches
metrics modified combined multiple ways. Liben-Nowell Kleinberg (2007) consider several meta-approaches use local global similarity
metric subroutine. instance, metrics discussed defined
terms arbitrary adjacency matrix A. Given formulation, imagine first
computing low-rank approximation Ak matrix using technique singular
value decomposition (SVD), computing local global graph metric using
modified Ak . idea Ak retains key structure original matrix, noise
reduced. Liben-Nowell Kleinberg propose two meta-approaches
based removing spurious links suggested first round similarity computation (the
clustering approach) based augmenting similarity scores node x based
scores nodes similar x (the unseen bigrams approach). compare performance three meta-approaches vs. multiple local global metrics
task predicting future links social network. Katz measure metaapproaches based clustering low-rank approximation perform best three
five arXiv datasets, simple local measures common neighbors Adamic/Adar
perform surprisingly well.
Supervised learning methods used combine augment similarity
metrics discussed. instance, Lichtenwalter et al. (2010) investigate several
supervised methods link prediction sparsely labeled networks, using many metrics Table 3.2. metrics used features simple classifiers C4.5,
J48, naive Bayes. find supervised approach leads 30% improvement
AUC simple unsupervised link prediction metrics. Similarly, Kashima Abe
(2006) propose supervised probabilistic model assumes biological network
evolved time, uses topological features estimate model parameters. evaluate proposed method protein-protein metabolic networks
report increased precision compared simpler metrics Adamic/Adar, Preferential
Attachment, Katz.
3.2.4 Discussion
general, local topology metrics sacrifice amount accuracy computational
gains global graph metrics may perform better costly estimate
infeasible huge networks. appropriate, supervised methods combine multiple
local metrics may offer promising alternative. next subsection discusses additional
work link prediction used supervised methods.
Link prediction using metrics especially sensitive characteristics
domain application. instance, many networks biology, identification
380

fiTransforming Graph Data Statistical Relational Learning

links costly, contain missing incomplete links, removal insignificant links
significant issue social networks. reason, researchers analyzed
proposed many different metrics working domains web analysis (Kleinberg,
1999; Broder et al., 2000), social network analysis (Zheleva, Getoor, Golbeck, & Kuter,
2010; Xiang et al., 2010; Koren, North, & Volinsky, 2007), citation analysis (Borgman &
Furner, 2002), ecology communities (Zhou et al., 2009), biological networks (Jeong et al.,
2000), many others (Barabasi & Crandall, 2003; Newman, 2003).
3.3 Hybrid Link Prediction
subsection, examine approaches perform link prediction using
attributes graph topology. approaches, two key questions. First,
kinds features used? Second, information multiple
features combined single measure probability used prediction?
first consider mix non-relational relational features used.
expected, best features vary based domain specific network. instance,
Taskar et al. (2003) studied link prediction network web pages found simple
local topology metrics (which called transitivity similarity) important
non-relational features based words presents pages. Similarly, Hasan
et al. (2006) found another topology metric (shortest distance) useful
predicting co-authorship links bibliographic network based DBLP.
single metric/feature, hitting time, used link prediction,
must ensure metric works well nodes yields consistent ranking.
However, multiple feature values combined way, may
acceptable use wider range features, especially supervised learner later select
weight important features based training data. Thus, hybrid systems
link prediction tend diverse feature set. instance, Zheleva et al.
(2010) propose new features based combining two different kinds networks (social
affiliation networks). Features based groups topology constructed
combined network used along descriptive non-relational features, yielding
improvement 15-30% compared system without combined-network features.
second example complex features provided Ben-Hur Noble (2005),
design new pairwise kernel predicting links proteins (protein-protein
interactions). pairwise kernel tensor-product two linear kernels original
feature space, especially useful domains two nodes might
common features. approach applied user preference prediction
recommender systems (Basilico & Hofmann, 2004). Vert Yamanishi (2005) propose
related approach, supervised learning used create mapping original
nodes new euclidean space simple distance metrics used link
prediction.
Given great diversity possible features link prediction, interesting approach
system automatically searches relevant features use. example, Popescul,
Popescul, Ungar (2003a) propose unique link prediction approach systematically
generates searches space relational features learn potential link predictors. use logistic regression link prediction consider search space covering
381

fiRossi, McDowell, Aha, & Neville

equi-joins, equality selections, aggregation operations. approach, model selection algorithm continues add one feature time model long Bayesian
Information Criterion (BIC) score training set improved. find
search algorithm discovers number useful topology-based features, co-citation
bibliographic coupling, well complex features. However, complexity
searching large feature space avoiding overfitting present challenges.
next consider second key question: information multiple
features combined single measure used link prediction? prior
work taken supervised learning approach, non-relational topologybased metrics used features describe possible link. supervised
techniques discussed Section 3.2, model learned training data
used predict unseen links.
supervised approaches apply classifier separately possible link,
using classifier support vector machine, decision tree, logistic regression
(Popescul et al., 2003a; Ben-Hur & Noble, 2005; Hasan et al., 2006). approaches,
flat feature representation link created, prediction made
possible link independent predictions.
contrast, early work Relational Bayesian Networks (RBNs) (Getoor et al., 2003)
Relational Markov Networks (RMNs) (Taskar et al., 2003) involved joint inference
computation link prediction, prediction could influenced nearby link
predictions (and sometimes newly predicted node labels). Using webpage network
social network, Taskar et al. demonstrated joint inference using belief propagation could improve accuracy compared independent inference approach. However,
approach computationally intensive, noted getting belief propagation
algorithm converge significant problem. possible solution computational
challenge simpler approach presented Bilgic, Namata, Getoor (2007).
method involved repeatedly predicting labels node, predicting links
nodes using available features (including predicted labels), re-predicting labels
new links, forth. link prediction based independent inference
step using logistic regression, simpler approaches discussed above. However,
repeated application step allows possibility link feature values changing
iterations based intermediate predictions, thus allowing link predictions
influence other.
Recently, Backstrom Leskovec (2011) proposed novel approach supervised,
final predictions based random walk rather directly
output learned classifier. Given particular target node v social network,
along nodes known link v, study predict
links v likely arise future (or recommended). define
simple link features based node profile similarity messaging behavior,
use features estimate initial link weights. show learn weights
(or transition probabilities) manner optimizes likelihood subsequent
random walk, starting v, arrive nodes already known link v.
random walk thus guided links already known exist, call
process supervised random walk. argue learning process greatly reduces
need manually specify complex graph-based features, show outperforms
382

fiTransforming Graph Data Statistical Relational Learning

supervised approaches well unsupervised approaches Adamic/Adar
measure.
final approach link prediction use kind unsupervised dimensionality
reduction yields new matrix way reveals possible new links. instance,
Hoff, Raftery, Handcock (2002) propose latent space approach initial link
information projected low-dimensional space. Link existence predicted
based spatial representation nodes new latent space. models
perform kind factorization link adjacency matrix thus often referred
matrix factorization techniques. advantage models spatial representation enables simpler visualization human interpretation. Related approaches
proposed temporal networks (Sarkar & Moore, 2005), mixed-membership
models (Nowicki & Snijders, 2001; Airoldi, Blei, Fienberg, & Xing, 2008), situations
latent vector representing node usefully constrained binary (Miller,
Griffiths, & Jordan, 2009). Typically, models capability including
attributes covariates affect link prediction directly part latent
space representation. However, Zhu, Yu, Chi, Gong (2007) demonstrated attributes represented related distinct latent space. recently, Menon
Elkan (2011) showed matrix factorization technique link prediction scale
much larger graphs training stochastic gradient descent instead MCMC.
3.4 Discussion
Link prediction remains challenge, part large number possible
links (i.e., N 2 possible links given N observed nodes), widely varying data
characteristics. Depending domain, best approach may use single nonrelational metric topology metric, may use richer set features evaluated
learned model. Future work may wish consider using ensemble link
predictors yield even better accuracy.
discussion link prediction focused predicting new links based existing
links properties nodes. context web, however, link prediction
sometimes taken forms. instance, Sarukkai (2000) used web server traces
predict next page user visit, given recent browsing history. particular,
use Markov chains, related random walks discussed Section 3.2,
task call link prediction. recently, DuBois Smyth (2010)
model relational events (i.e., links) using latent classes event/link arises
latent class properties event (i.e. sender, receiver, type) chosen
distributions nodes conditioned assigned class. work, local
community node influences distribution computed node, way related
computations stochastic block modeling (Airoldi et al., 2008). DuBois & Smyths
task form link prediction, goal predict presence
absence static link, frequency occurrence possible event/link.
One might interested deleting pruning away noisy, less informative links.
instance, friendship links Facebook usually extremely noisy since cost
adding friendship links insignificant. techniques used section could
383

fiRossi, McDowell, Aha, & Neville

used remove existing links wherever link prediction algorithm yields
low score (or weight) observed link original graph.
Indeed, since link prediction algorithms effectively assign score every possible
link, could used assign weight set initial links G.
link weighting one three subtasks link interpretation shown taxonomy
Figure 2. However, practice weights needed initial links, different
features algorithms often possible and/or effective. next section
discusses link weighting algorithms, well link interpretation general. Also,
Section 7 discuss additional methods link prediction seek jointly
transform nodes links.

4. Link Interpretation
Link interpretation process constructing weights, labels, general features
links. three tasks link interpretation related somewhat overlapping. First,
link weighting task assigning weight link. weights may represent
relevance importance link, typically expressed continuous values.
Thus weights provide explicit order links. Second, link labeling similar,
except usually assigns discrete values link. could represent positive
negative relationship, could used, instance, assign one five topics email
communication flows. Finally, link feature construction process generating set
discrete continuous features links. instance, features might count
frequency particular words appeared messages two nodes connected
link, simply count number messages.
sense, link feature construction subsumes link weighting labeling, since
weights labels viewed simply possible link features discovered. However, many tasks makes sense compute one particular feature summarizes
relevance link (the weight) and/or one particular feature summarizes type
link (the label). weights labels may especially useful later processing, example collective classification. Moreover, techniques used general
feature construction tend toward simpler approaches aggregation discretization, whereas best techniques computing weights labels may involve much
complexity, including global path computations supervised learning. reason,
treat link weighting (Section 4.1) link labeling (Section 4.2) separately general
link feature construction (Section 4.3).
4.1 Link Weighting
Given initial graph G = hV, E, XV , XE i, task assign continuous value (the
weight) existing link G, representing importance influence link.
previously discussed, link weighting could potentially accomplished applying link
prediction technique simply retaining computed scores link weights. instance,
Lassez, Rossi, Jeev (2008) perform link prediction weighting applying singular
value decomposition adjacency matrix, retaining k significant
singular-vectors (similar low-rank approximation techniques discussed Section 3.2).
384

fiTransforming Graph Data Statistical Relational Learning

show querying (e.g., PageRank) resultant weighted graph yield
relevant results compared unweighted graph.
Unlike link prediction, however, link weighting techniques designed
work links already exist graph. techniques dont work
predicting unseen links weight links based known properties/features
existing links, compute additional link features yield
sensible results links already exist.
simplest case, link weighting aggregating intrinsic property links.
example, Onnela et al. (2007) defines link weights based aggregated duration
phone calls individuals mobile communication network. cases, simply
counting number interactions two nodes may appropriate.
Thus, link features duration, direction, frequency known,
aggregated way generate link weights. actual link weights already known
links, supervised methods used weight prediction, using
known weights training data. instance, Kahanda Neville (2009) predict link
strength within Facebook dataset, stronger relationships identified based
users explicit identification top friends via popular Facebook application.
Gilbert Karahalios (2009) predict link strength Facebook, form training data survey data collected 35 participants (yielding strength ratings
2000 links). algorithms generate large number (50-70) features
link network, learn predictive model via regression technique bagged decision trees, Kahanda Neville finds performs best among
several alternatives. Gilbert Karahalios generate features based profile similarity
(e.g., two users similar education levels?) based user interactions (e.g.,
frequently topics two users communicate?). find interaction features helpful, especially feature based number days since
last communication event. Kahanda Neville use similar kinds features,
term attribute-based transactional features, add topological features (such
Adamic/Adar discussed Section 3.2) network-transactional (NTR) features.
NTR features based communications users (e.g., number
email messages exchanged) moderated way larger network context.
moderation often takes form normalization, instance dampen influence
node sent large number messages many different friends. find
NTR features far helpful prediction, many
features contribute overall predictive accuracy.
training data sample link weights available, approaches based
parameterized probabilistic model still possible. However, since candidate link features
longer evaluated training data, approaches must (manually)
choose features use much carefully. instance, Xiang et al. (2010)
examine link weight prediction two social network datasets (Facebook LinkedIn),
use 5-11 features link. hypothesize relationship strength hidden
cause user interactions, propose link-based latent variable model capture
dependence. inference, use coordinate ascent optimization procedure predict
strength link. Since actual strength link known, prediction
tasks domain cannot directly evaluate accuracy. However, Xiang et al. demonstrate
385

fiRossi, McDowell, Aha, & Neville

using link strengths produced method leads higher autocorrelation
higher collective classification accuracy predicting user attributes gender
relationship status.
number researchers considered importance recency evaluating link
weight, assumption events interactions occurred recently
weight. instance, Roth et al. (2010) propose Interactions Rank metric
weighting link based messages two nodes. formula separately
weights incoming outgoing messages link, imposes exponential decay
importance message based old is. Roth et al. use metric
weight links call implicit social network, node represents
group users. demonstrate metric used accurately predict users
missing email distribution list. However, basic metric simple
compute could applied many tasks.
Interactions Rank metric weights link heavily connects two nodes
frequently and/or recently communicated. Alternatively, Sharan Neville (2008)
considered weight links graph links (such hyperlinks
friendships) may appear disappear time. particular, construct
summarized graph nodes links ever existed past present.
link new graph weighted based kernel function provide
weight links present often recently past. explain
modify standard relational classifiers use weighted links, demonstrate
variety kernels (including exponential linear decay kernels) produce weighted
links yield higher classification accuracy compared non-weighted graph.
recently, Rossi Neville (2012) extended work handle time-varying attribute
values, may serve basis incorporating temporal dynamics additional
tasks.
4.2 Link Labeling
Given initial graph G = hV, E, XV , XE i, task construct discrete label
one links G. labels used describe type relationship
link represents. instance, Facebook example, link labeling algorithm may
create labels representing work personal relationships. labels would enable
subsequent classification models separately account influence different
kinds relationships.
prior work link labeling assumed text (such message) describes link, based unsupervised textual analysis techniques
Latent Dirichlet Allocation (LDA) (Blei et al., 2003), Latent Semantic Analysis (LSA)
(Deerwester, Dumais, Furnas, Landauer, & Harshman, 1990), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999). Traditionally, techniques used
assign one latent topics document collection documents.
topics formed defined implicitly probability distribution likely
word appear, given topic associated document. topics
always semantically meaningful, often manual inspection reveals
prominent topics represent sensible concepts advertising government re386

fiTransforming Graph Data Statistical Relational Learning

lations. However, even semantic associations obvious, inferring
topics set links still aid analysis, since topics identify links
represent similar kinds relationships.
textual analysis techniques developed independent documents mind,
inter-linked nodes, adapted label links several ways. instance,
Rossi Neville (2010) examined messages developers contributing opensource software project. treat message separate document, use LDA
infer single likely latent topic message (i.e., link label). technique
could used graph textual content associated links. Rossi Neville
go further, consider impact time-varying topics time-varying topic/word
associations, running multiple iterations LDA, one per time epoch. Using model,
study problem predicting effectiveness different developers (nodes)
network. demonstrate accuracy predictions significantly improved
modeling temporal evolution communication topics.
McCallum, Wang, Corrada-Emmanuel (2007) describe alternative way extending LDA-like approaches link labeling. LDA essentially Bayesian network
models probabilistic dependencies documents, associated topics, words associated topics. propose extend model Author-RecipientTopic (ART) model, choice topic document (message) depends
author recipient message. parameters learned
model, inference (e.g., Gibbs sampling) used infer likely latent
topics message. make use topics assign roles people email
communication network, demonstrate outperforms simpler models.
Supervised techniques used link labeling. instance, Taskar et al.
(2003) study academic webpage network consider predict node labels (such
Student Professor) simultaneously predicting link labels (such adviserof). Given labeled training graph, learn complex Relational Markov Network
(RMN) predict labels existence new links. make link
prediction tractable, candidate new links considered, links
suggested textual reference, inside page, entity graph.
RMN utilizes text-based features, instance based anchor text known links
heading HTML section possible link reference found.
demonstrate RMNs joint inference nodes links improves performance
compared separate inference. However, learning inference RMNs often
significant challenge, practice limits number types feature
considered.
RMN approach learns training data uses joint inference
entire graph. simpler supervised approach create set features link
use features learning inference arbitrary classifier treats
link separately. Leskovec, Huttenlocher, Kleinberg (2010) study particular form
approach two link labels, representing positive negative relationship (such friendship vs. animosity). create link features based (signed)
degree nodes involved link based transitivity-like properties computed known labels nearby links. demonstrate approach using data
Epinions, Wikipedia, Slashdot, users manually indicated positive
387

fiRossi, McDowell, Aha, & Neville

negative relationships users. Given network almost edges labeled,
label classifier able predict label (positive negative) single unlabeled edge
high accuracy. Interestingly, show classifiers predictive accuracy
particular dataset decreases slightly classifier trained different dataset
vs. trained dataset used predictions. argue theories
balance status social psychology partially explain ability predictive
models generalize across datasets. Unlike techniques discussed
section, work make use text-based features. However, general problem
predicting sign link related sentiment analysis (or opinion mining)
natural language processing (Godbole, Srinivasaiah, & Skiena, 2007; Pang & Lee, 2008).
sentiment analysis algorithms could reformulated predict label (such
positive negative) link given associated text.
link two nodes established based many different kinds
relationships, many types algorithms could potentially used
labeling links, even original algorithm designed purpose. instance,
Markov Logic Networks (MLNs) used extract semantic networks text,
yielding graph nodes represent objects concepts (Kok & Domingos, 2008).
process produces relations teaches written nodes,
could used link labels analysis. Another example Group-Topic
(GT) model proposed McCallum, Wang, Mohanty (2007), which, previously
mentioned ART model, Bayesian network. model intended graphs two
nodes (such people) become connected participate event,
voting yes political bill. Rather directly labeling links (like
ART), GT model clusters nodes (such people) latent groups based
textual descriptions events/votes. However, GT model simultaneously infers
set likely topics event, could used label implicit links
nodes. results model could used add new nodes graph
represent latent groups discovered.
4.3 Link Feature Construction
Link feature construction systematic construction features links, typically
purpose improving accuracy understandability SRL algorithms. Link feature
construction important many prediction tasks, received considerably
less attention node feature construction literature. Fortunately, many
computations developed node feature construction apply link
features. avoid redundancy, defer analysis feature construction
discussion node feature construction Section 6.3. section briefly discusses
techniques node feature construction applied links, summarizes
major types link features computed.
Section 6.3 later describe feature values relational data often based
aggregating values multiple nodes. instance, feature might compute
average common feature value among neighbors particular node.
aggregation-based features help account varying number neighbors
node may have. links, aggregation less essential, since (usually) link precisely
388

fiTransforming Graph Data Statistical Relational Learning

(a) link-aggregation

(b) link-aggregation

Figure 4: Link Feature Aggregation Example: figure demonstrates unknown link feature value computed aggregating link feature values
surrounding links. aggregation operator Mode.

two endpoint nodes. However, aggregation still useful computing features
collect information larger area graph. instance, Figure 4, link feature
value computed link center subgraph (the target link).
computation considers feature values (positive negative signs) links
adjacent target link. case, aggregation operator Mode,
result new link feature value. example used link features input, node
feature values (e.g., lightly-shaded nodes Figure 4) could aggregated
form new link feature. way, aggregation operators discussed nodes
Section 6.3 applied links.
Figure 5 summarizes kinds features constructed link. figure
organized around sources information go computing single link feature
(i.e., inputs), rather details feature computation (such type
aggregation function used). bottom figure shows four types link
features, represented subgraph. case, emphasized link bottom
subgraph target link new feature value computed.
subgraphs shows varying amounts information displays
features, nodes, and/or links used inputs kind link feature.
simplest type non-relational link feature, computed
link solely information already known link. Thus, Figure 5A shows
feature values already known target link, used
construct new feature value. instance, message associated link,
link feature could count number times certain word occurs, number
distinct words. Alternatively, date associated link, feature might
compute number months since link formed. Onnela et al. (2007) computed
kind feature aggregated duration phone calls two people
form new link feature (which used link weight).
remaining feature types relational, meaning depend way
graph (not single link). First, topology features (Figure 5B)
computed using topology graph. feature might, instance,
compute total number links adjacent target link. Likewise, Kahanda
Neville (2009) computed clustering coefficient pair linked nodes,
measures extent two nodes neighbors common (Newman, 2003),
well topological features Adamic/Adar measure discussed Section 4.1.
389

fiRossi, McDowell, Aha, & Neville

input

V,E,XV,XE

target link

Link Feature
Construction

E
X

link-value

p

V,E,XV,XE

node-value

L
Non-relational
Link Features

Relational Features

V,E V,E,XE V,E,XV
Topology
Features

Link-value
Features

Node-value
Features

C
p

w

C
L

w

L
A.

B.

E

C.

L
D.

X

Figure 5: Link Feature Taxonomy: link feature classes non-relational features,
topology features, relational link-value features, relational node-value features.
subgraphs bottom, information potentially used
class link feature (i.e., nodes V , links E, node features X V , and/or link
features X E ) shown. emphasized link represents feature value
computed (i.e., target link).

used link features help predict link strength, could used
tasks.
Next, relational link-value features computed using feature values
nearby links. instance, Figure 5C shows link labels personal (p) work
(w) might identified links adjacent target link. new link feature could
formed representing distribution labels, taking common
label, (when link features numeric) averaging. Leskovec, Huttenlocher,
Kleinberg (2010) used link-value features working graphs link
sign feature positive negative (as Figure 4). computed features
based signed-degree two nodes connected target link well
complex measures based paths two nodes (e.g., measure sign
transitivity).
390

fiTransforming Graph Data Statistical Relational Learning

Finally, relational node-value features computed using feature
values nodes close attached target link. instance,
Figure 5D shows node labels conservative (C) liberal (L) might identified
nodes close target link. link-value features, labels could used
create new feature value summarization aggregation. Often, two nodes
directly attached target link used. instance, work Gilbert
Karahalios (2009) Kahanda Neville (2009) construct link features based
similarity two nodes social network profiles. However, feature values distant
nodes could used, instance compute new link feature based similar
friends two people (nodes) are.

5. Node Prediction
Node transformation includes node prediction (e.g., predicting existence new nodes)
node interpretation (e.g., constructing node weights, labels, features). section
focuses node prediction, Section 6 considers node interpretation.
Given graph existing nodes V , node prediction used two distinct ways.
First, node prediction algorithm could used discover additional nodes
type already present V . instance, given set people
communicate via email, simple algorithm might used create new nodes
represent email recipients implied messages, explicitly represented
original graph. Alternatively, supervised unsupervised machine learning techniques
could used discover, instance, new research papers people information
available web (Craven et al., 2000; Cafarella, Wu, Halevy, Zhang, & Wang, 2008).
techniques valuable, certainly used add new nodes graph.
However, work examined context general knowledge base
construction, rather relational learning.4
focus second type node prediction, involves predicting nodes
different type already present graph. new nodes might
represent locations, communities (Kleinberg, 1999), roles (McCallum, Wang, & CorradaEmmanuel, 2007; Rossi, Gallagher, Neville, & Henderson, 2012), shared characteristics,
social processes (Tang & Liu, 2009; Hoff et al., 2002), functions (Letovsky & Kasif, 2003),
kind relationship. instance, running Facebook example,
newly discovered node may represent common interest hobby multiple people
share. nodes usually referred latent nodes (and nodes connected
node form latent group).5 meaning nodes depend upon
features and/or links included input node prediction algorithm.
instance, including work-based friendships lead different groups
personal friendships considered.
4. recent work Kim Leskovec (2011) exception. technique uses EM infer
existence missing nodes links based known topology graph.
5. Prior work sometimes refers nodes hidden nodes, especially thought
represent concrete characteristics, geographic location, could measured were,
reason, observed data.

391

fiRossi, McDowell, Aha, & Neville

Figure 6: Alternative Representations Newly Predicted Groups: left
figure shows new feature (with value X Y) could added node,
right figure demonstrates creation two new nodes represent
groups.

many advantages type representation change regards accuracy understandability. instance, nodes directly connected
original graph similar way become, links new nodes,
closer graph space. Intuitively, nodes connected high level concept share
latent properties representing latent structure directly impact classification,
network analysis, many tasks. instance, reducing path length
similar nodes enables influence nodes propagate effectively collective
classification (CC) performed nodes. model still learn exploit
new nodes relationships, even semantic meaning new nodes
precisely understood.
popular methods predicting new nodes based clustering,
context means grouping nodes nodes within group similar
nodes groups. Typically, one new node created
group, links added existing node corresponding
group node (see right side Figure 6). techniques may associate node
multiple groups, link weights representing affinity group.
new groups discovered, whether via clustering via technique,
alternative creating new nodes links simply add new feature(s) node
represent group information. left side Figure 6 demonstrates alternative.
instance, new node feature might represent running hobby, may simply
represent belonging discovered group #17, unknown meaning. Popescul
Ungar (2004) use CiteSeer dataset demonstrate technique derive features
improve predictive accuracy. advantage approach, opposed adding
new nodes, potentially enables simpler, non-relational algorithms make use
new information. potential disadvantage, though, allow
algorithms CC propagate influence newly connected nodes, discussed
above. However, methods use general strategy generate much larger
392

fiTransforming Graph Data Statistical Relational Learning

numbers latent features used classification (Tang & Liu, 2009; Menon &
Elkan, 2010). Tang & Liu demonstrate that, cases, resultant large number
link-based features may make collective inference unnecessary obtaining good accuracy.
Naturally, whether information discovered clusterings best represented
via new nodes new features depend upon dataset inference task.
section, simplicity discuss algorithm assuming new nodes
created (even algorithm originally described terms creating new features).
discussion link prediction, organize discussion around kinds
information used prediction. Section 5.1 discusses non-relational (attributebased) node prediction, Section 5.2 discusses topology-based node prediction, Section 5.3 discusses hybrid approaches use node feature values topology
graph.
5.1 Non-relational (Attribute-Based) Node Prediction
many clustering algorithms used cluster existing nodes using
non-relational features (attributes), used add new nodes
graph. two primary types hierarchical clustering algorithms (e.g., agglomerative
divisive clustering) partitioning algorithms k-means, k-medoids (Berkhin, 2006;
Zhu, 2006), EM-based algorithms, self-organizing maps (Kohonen, 1990).
discuss algorithms since well studied non-relational data
easily applied relational data clustering based attribute values
desired.
5.2 Topology-Based Node Prediction
techniques described section link existing nodes one new nodes (i.e.,
latent groups), based original link structure graph. cases, finding
grouping depends upon computing kind similarity metric every pair
nodes. Two key questions thus serve identify techniques. First, kind
similarity metric used? Second, metric used predict
groupings? address question turn.
5.2.1 Types Metrics Group Prediction
type topology-based link weighting metric (see Table 3.2) could conceivably used
latent node prediction. metric suitable long produces high values
pairs nodes belong group lower values pairs.
instance, high value Katz metric (see Section 3.2) indicates two nodes
many short paths them, thus may belong group. Metrics
representing distance rather similarity used negating metric.
instance, Girvan Newman (2002) focus detecting community structure extending
concept node-betweenness links. Intuitively, network contains latent groups
loosely connected intergroup links, shortest paths
different groups must go along links. links connect different groups
assigned high link-betweenness value (which corresponds low similarity value).
393

fiRossi, McDowell, Aha, & Neville

underlying group structure trivially revealed removing links
highest betweenness.
idea using link-betweenness relational clustering extended
number directions. instance, Newman Girvan (2004) introduced random-walk
betweenness, expected number times random walk pair
nodes pass particular link. addition, Radicchi, Castellano, Cecconi, Loreto,
Parisi (2004) proposed using link-based clustering coefficient metric. showed
metric performs comparably original link-betweenness metric Girvan
Newman, much faster local graph measure instead global graph
measure.
Zhou (2003) describes new metric, dissimilarity index, computed
follows. node i, compute vector di value dij represents
distance node node j (Zhou measures distance based average number
steps needed random walk starting node reach node j, distance metric
could used). nodes k similar, similar distance
vectors. Thus, dissimilarity index nodes k defined based Euclidean-like
distance computation vectors di dk . Zhou demonstrates technique
outperforms link-betweenness approach Girvan & Newman random modular
networks.
Relatively simple metrics often lead useful results. instance, Ravasz et al.
(2002) used simple clustering coefficient metric study metabolic networks. study
reveals metabolic networks forty-three organisms organized many small,
highly-connected modules. Furthermore, find E. coli, hidden hierarchical
modularity closely overlaps known metabolic functions.
5.2.2 Using Metrics Group Prediction
simplest techniques identifying new groups perform kind hierarchical
clustering. instance, similarities weights computed every pair
nodes, links removed graph. Next, weighted links placed
nodes one one, ordered weights. intuition varying degrees
clusters formed links added. particular, approach forms hierarchical
tree leaves represent finest granularity clustering every node
separate cluster. move tree larger clusters formed, reach top
nodes joined one large cluster. type hierarchical approach
used work Zhou (2003). Girvan Newman (2002) use similar strategy,
start instead original graph iteratively remove less similar links
graph reveal underlying community structure. challenge approaches,
clustering general, select appropriate number final clusters,
corresponds selecting level clustering tree.
Spectral clustering (Dhillon, 2001; Ng, Jordan, & Weiss, 2001; Kamvar, Klein, & Manning, 2003) used group identification. Spectral clustering relies upon computing similarity matrix describes data points, transforming matrix
way yields new matrix U clustering rows U using simple clustering
algorithm (such k-means) trivially identify interesting groups data.
394

fiTransforming Graph Data Statistical Relational Learning

matrix transformation several variants, involves computing kind Laplacian
S, computing eigenvectors resultant matrix using eigenvectors represent original data. motivation transformation seen
identifying good graph cuts original graph (those yield good separations
highly-connected nodes groups) identifying nodes closely related
terms random walks; see work von Luxburg (2007) overview. Spectral
clustering originally applied non-relational data, but, hierarchical techniques described above, applied relational data using link-based metrics
computing similarity matrix. instance, Neville Jensen (2005) use node
adjacency matrix spectral clustering technique described Shi Malik (2000)
identify latent groups graphs. show technique enables simpler
inference (since group handled separately), ultimately yields accurate
classification compared approaches ignore group structure. Tang Liu (2011)
use spectral clustering link graph, order create much larger
number latent features used learn supervised classifier. Unlike
latent groups work Neville Jensen, technique allows node
associated one cluster output spectral clustering, Tang
& Liu claim leads improved classification accuracy. Spectral clustering used
complex similarity metrics, described next subsection.
Techniques borrowed web search useful node prediction. instance, given adjacency matrix webpage graph, Hits algorithm (Kleinberg,
1999) computes first eigenvectors AAT A, represent
authoritative nodes (the authorities) well prominent nodes point (the
hubs). Normally, algorithm used find single prominent community authorities hubs (to assist web search), secondary communities
discovered considering non-principal eigenvectors AAT (Gibson, Kleinberg, & Raghavan, 1998). node prediction algorithm could treat
community latent group add new node links represent group.
techniques may especially useful detecting patterns influence graph
adding explicit links represent influence.
5.3 Hybrid Node Prediction
techniques previous section added new nodes graph, often based
clustering, using topology graph. principle, technique used
nodes attributes produce meaningful latent groups/nodes. section
considers add attribute information techniques node prediction.
simple approach define kind similarity metric combines nonrelational topology-based similarity single value, provide similarity
metric one previously mentioned clustering algorithms. instance, Neville,
Adler, Jensen (2004) use weighted combination attribute link information
1X
S(i, j) =
sk (i, j) + (1 ) l
k
k

metric, sk (i, j) = 1 iff nodes j value kth attribute,
l = 1 iff link exists j. constant controls relative
395

fiRossi, McDowell, Aha, & Neville

importance attributes vs. links. use metric NCut spectral
clustering technique add new nodes graph, demonstrate additional
nodes increase performance relational classification. similar weighted combination
attribute link-based similarity used Bhattacharya Getoor (2005) entity
resolution.
Attribute-based information incorporated ad-hoc basis. instance,
Adibi, Chalupsky, Melz, Valente, et al. (2004) describe group finding algorithm
initial seed set clusters formed based handcrafted set logical rules,
clusters refined using probabilistic system based mutual information.
system, logic-based component primarily uses attributes node (person),
probabilistic system primarily uses links describe connections
people. However, components make use attributes links.
principled approach define kind generative model represents
dependence observed attributes links latent group nodes, use
model estimate group membership. instance, Kubica, Moore, Schneider,
Yang (2002) define generative model node belongs one groups,
group members tend link other. particular, use group membership
chart track whether node belongs group, local search possible
states chart (using stochastic hill climbing) try identify membership changes
would better explain known data. step, maximum likelihood used
estimate parameters model. demonstrate usefulness technique
news articles, webpages, synthetic data.
Generative models used sophisticated inference. example,
Taskar, Segal, Koller (2001) treat group membership latent variable uses
loopy belief propagation implicitly perform clustering nodes. Likewise, Mixed
Membership Relational Clustering (MMRC) (Long et al., 2007) uses EM variants estimate group memberships. particular, uses first round hard clustering (where
object assigned exactly one cluster), following round soft clustering continuous strength values associated membership assignment. Mixed membership stochastic blockmodels (Airoldi et al., 2008) assign continuous group membership
values node, use topological information (not attributes) group
assignments use variational inference techniques generative model. Finally,
Long, Zhang, Wu, Yu (2006) demonstrate node clustering performed instead using spectral clustering, focuses particularly simultaneously cluster
multiple types nodes (e.g., simultaneously cluster web pages web users two
distinct sets groups).
group prediction algorithms assume links likely connect nodes
belong group. exception work Anthony desJardins (2007),
use generative model links attributes depend latent group
memberships, types links likely occur nodes
belong group. instance, note groups social network
defined gender, link representing dating likely connect two nodes
different groups.
396

fiTransforming Graph Data Statistical Relational Learning

Figure 7: Lifted Graph Representation: initial graph G clustered transformed lifted graph representation G. lifted graph representation
created clustering nodes, links, both.

5.4 Discussion
techniques described produce single clustering nodes, usually
based assigning every node single group. contrast, multi-clustering emerging
research area aims provide multiple orthogonal clusterings complex data (Strehl
& Ghosh, 2003; Topchy, Law, Jain, & Fred, 2004). instance, individuals Facebook
might clustered multiple ways latent node types might represent friend groups,
work relations, socioeconomic status, locations, family circles. type multi-clustering
performed McCallum, Wang, Corrada-Emmanuel (2007) latent nodes
created based roles topics. addition, Kok Domingos (2007) propose Statistical Predicate Invention (SPI), node transformation approach based Markov Logic
Networks (Richardson & Domingos, 2006). SPI clusters nodes, features links forming basis prediction predicates (or potential nodes). SPI considers multiple
relational clusterings based observation multiple distinct clusterings may
necessary to, instance, group individuals based friendships work relationships. demonstrate MLN inference estimate clusters improves
performance compared two simpler baselines. similar node prediction approach applies
MLNs role labeling (Riedel & Meza-Ruiz, 2008).
Node deletion may useful cases. instance, node deletion might
beneficial removing outdated spurious nodes graph. Alternatively,
may multiple nodes represent real-world object concept, case
deletion purposes entity resolution important (Pasula, Marthi, Milch,
Russell, & Shpitser, 2003; Bhattacharya & Getoor, 2007; Singla & Domingos, 2006).
Finally, node representation changes used improve accuracy,
yield graphs processed efficiently desirable
properties. Section 5.2 already discussed Neville Jensen (2005) used addition
latent nodes enable simpler inference. Another possibility creation super-nodes
represent one original nodes. instance, Figure 7 demonstrates
five original nodes can, clustering, collapsed three super-nodes, yielding lifted
graph representation. kind representation change used efficient
397

fiRossi, McDowell, Aha, & Neville

inference Markov Logic Networks (see Section 6.3) network anonymization (see
Section 8.6).

6. Node Interpretation
Node interpretation process constructing weights, labels, general features
nodes. symmetric tasks link interpretation, node weighting seeks
assign continuous value node, representing nodes importance, node
labeling seeks assign discrete value link, representing type, group, class
node. Likewise, node feature construction process systematically generating
general-purpose node features based on, instance, aggregation, dimensionality reduction,
subgraph patterns.
discussed Section 4 links, node feature construction could viewed subsuming node weighting node labeling, since general feature construction could always
used construct feature values treated weights labels nodes.
practice, however, techniques used tend rather different. instance, PageRank
often used node weighting supervised classification often used node labeling,
techniques rarely used general feature construction. Nonetheless, node
interpretation (more link interpretation) substantial overlap techniques actually used weighting labeling vs. used general
feature construction. Below, first discuss node weighting Section 6.1 labeling
Section 6.2. Section 6.3 discusses node feature construction, mentioning briefly
relevant techniques previously discussed weighting labeling.
6.1 Node Weighting
Given initial graph G = hV, E, XV , XE i, task assign continuous value (the
weight) existing node G, representing importance influence node.
Node weighting techniques used information retrieval, search engines, social
network analysis, many domains way discover important nodes
respect defined measure. node prediction classified based
whether use node attributes, graph topology, construct
weighting.
6.1.1 Non-relational (Attribute-Based) Node Weighting
simplest node weighting techniques use node features XV (i.e., attributes).
instance, nodes representing documents might weighted based number
query-relevant words contain, nodes representing companies might ranked
based gross annual sales. Many sophisticated strategies considered. instance, Latent Semantic Indexing (Deerwester et al., 1990) used
identify important semantic concepts corpus text, nodes ranked
based connection concepts. methods extensively applied
quantify rank importance scientific publications (Egghe & Rousseau, 1990).
However, techniques extensively studied elsewhere ignore
graph structure (such citations), discuss here.
398

fiTransforming Graph Data Statistical Relational Learning

6.1.2 Topology-Based Node Weighting
Several node weighting algorithms use topology graph developed
support early search engines. Examples kind algorithm include PageRank
(Page et al., 1999), Hits (Kleinberg, 1999), SALSA (Lempel & Moran, 2000).
algorithms rank relative importance web sites, conceptually based
kind eigenvector analysis (Langville & Meyer, 2005), though practice iterative computation may used. instance, PageRank models web Markov Chain
implemented systematically computing principal eigenvector limk Ak e
adjacency matrix e unit vector. Hits, previously described, instead computes principal eigenvectors AAT A. algorithms continue
important webpage ranking, applied many kinds
graphs (Kosala & Blockeel, 2000).
social network analysis, objective topology-based node weighting typically
identify influential significant individuals social network.
variety centrality measures devised use local global network structure characterize importance individuals (Wasserman & Faust, 1994). Examples
metrics include node degree, clustering coefficient (Watts & Strogatz, 1998), betweenness (Freeman, 1977), closeness (i.e., distance/shortest paths), eigenvector centrality (Bonacich & Lloyd, 2001), many others (Jackson, 2008; Newman, 2010; Sabidussi,
1966). addition, White Smyth (2003) considered compute relative node
rankings, i.e., rankings relative set particularly interesting nodes. show
compute relative rankings metrics based shortest paths well
Markov chain-based techniques (e.g., produce PageRank priors). addition,
similarity metrics described Table 3.2 alternatively formulated
computing weights nodes.
recently, node weighting techniques extended measure relative
importance nodes temporally-varying data. instance, Kossinets, Kleinberg,
Watts (2008) Tang et al. (2009) define notions temporal distance based
analysis frequently information exchanged nodes. information
used define range new graph metrics, global temporal efficiency, local temporal efficiency, temporal clustering coefficient (Tang et al., 2009). recently,
Tang, Musolesi, Mascolo, Latora, Nicosia (2010) define notions temporal betweenness
temporal closeness. argue incorporating temporal information
metrics provides better understanding dynamic processes network
accurately identifies important nodes (people). metrics primarily concern networks time-varying interactions (e.g., communications people),
could applied types data intermittent interactions
nodes nodes/link join leave network time. metrics
apply links, could possibly used improve link prediction algorithms.
6.1.3 Hybrid Node Weighting
hybrid node weighting approaches use attributes graph
topology (Bharat & Henzinger, 1998; Cohn & Hofmann, 2001). instance,
various approaches modify Hits (Chakrabarti, Dom, Raghavan, et al., 1998; Bharat
399

fiRossi, McDowell, Aha, & Neville

& Henzinger, 1998) PageRank (Haveliwala, 2003) construct node weights based
content links. Topic-Sensitive PageRank (Haveliwala, 2003) seeks compute
biased set PageRank vectors using set representative topics. Alternatively, Kolda,
Bader, Kenny (2005) propose TOPHITS, hybrid approach adds anchor text (i.e.,
clickable text hyperlink) adjacency matrix representation used Hits.
use higher-order analogue SVD known Parallel Factors (PARAFAC)
decomposition (Harshman, 1970) identify key topics graph well
important nodes. hybrid approaches proposed SimRank (Jeh
& Widom, 2002), Topical methods (Haveliwala, 2003; Nie, Davison, & Qi, 2006; Kolda
& Bader, 2006), Probabilistic HITs (Cohn & Chang, 2000), many others (Richardson
& Domingos, 2002; Lassez et al., 2008). Section 7 discusses relevant work
context joint node link transformation techniques.
Recently, node weighting approaches applied Adversarial Information Retrieval (AIR) detect moderate influence spam web sites. Typically, techniques produce weights using topology graph information,
necessarily kind attribute information used techniques discussed
above. instance, TrustRank (Gyongyi, Garcia-Molina, & Pedersen, 2004) based
PageRank uses set trusted sites evaluated humans propagate trust
locally reachable sites. hand, SpamRank (Benczur, Csalogany, Sarlos, &
Uher, 2005) measures amount undeserved PageRank analyzing backlinks
site. algorithms try identify link farms link spam alliances (Wu
& Davison, 2005), given seed set known link farm pages. Among AIR methods,
TrustRank widely known suffers biases human-selected set
trustworthy sites may favor certain communities others.
6.2 Node Labeling
Given initial graph G = hV, E, XV , XE i, task assign discrete label
nodes G. first discuss labeling techniques based classification,
consider unsupervised textual analysis techniques.
many cases, node labeling may considered end itself. instance,
running Facebook example, stated goal predict political affiliation
node label already known. cases, however, node labeling
properly understood representation change supports desired task.
instance, definitions anomalous link detection (Rattigan & Jensen, 2005),
estimated node labels would allow us identify links nodes whose labels indicate
rarely, ever, connected. Alternatively, datasets estimating node
labels may enable us subsequently partition data based node type, enabling us
learn accurate models type node.
Even node labeling final goal, Facebook example, intermediate
label estimation may still useful representation change. particular, Kou Cohen
(2007) describe stacked model relational classification relabels training set
estimated node labels using non-relational classifier. use estimated
labels learn new classifier (one uses attributes relational features),
use new classifier perform relational classification test graph. approach
400

fiTransforming Graph Data Statistical Relational Learning

yields high accuracy, comparable much complex algorithms collective
classification (CC). Fast Jensen (2008) analyze result discuss
explained natural bias CC algorithms: training performed given
node labels inference depends part estimated labels (McDowell, Gupta, &
Aha, 2009). Stacked models compensate bias instead training relabeled
(estimated) training set. addition, inference new classifier needs single
pass test graph, yielding much faster inference CC techniques Gibbs
sampling belief propagation. recently, Maes, Peters, Denoyer, Gallinari (2009)
extend ideas node relabeling order generate larger training set via multiple
simulated iterations classification. show cases approach
outperform stacked models CC algorithms Gibbs sampling.
Thus, multiple reasons creating new labels nodes graph.
labeling accomplished relational-aware algorithms described
well earlier algorithms used relational collective classification (Chakrabarti,
Dom, & Indyk, 1998; Neville & Jensen, 2000; Taskar et al., 2001; Lu & Getoor, 2003;
Macskassy & Provost, 2003). Node labeling course done traditional,
non-relational algorithms SVM, decision trees, kNN, logistic regression, Naive
Bayes, among various others (Lim, Loh, & Shih, 2000; Michie, Spiegelhalter, Taylor, &
Campbell, 1994; Burges, 1998; Cristianini & Shawe-Taylor, 2000; Joachims, 1998).
methods simply use features XV exploit topology link-structure.
techniques assign new labels via supervised learning. Labels
assigned via unsupervised techniques textual analysis. many networks
real-world contain textual content social networks, email/communication
networks, citation networks, many others. Traditional textual analysis models
LSA (Deerwester et al., 1990), PLSA (Hofmann, 1999) LDA (Blei et al., 2003)
used assign node topic representing abstraction textual information.
recent techniques Link-LDA (Erosheva, Fienberg, & Lafferty, 2004) LinkPLSA (Cohn & Hofmann, 2001) aim incorporate link structure traditional
techniques order accurately discover nodes type.6 particular, work
Cohn Hofmann demonstrate technique produce accurate node
labels techniques use node attributes link topology.
sophisticated topic models developed specific tasks
social tagging (Lu, Hu, Chen, & ran Park, 2010) temporal data (Huh & Fienberg,
2010; & Parker, 2010).
6.3 Node Feature Construction
Node feature construction systematic construction features nodes, typically
purpose improving accuracy understandability SRL algorithms. Feature
construction common relational representation change, frequently
done performing task classification. instance, performing CC
classify nodes example Facebook political affiliation task, likely compute
6. names Link-LDA Link-PLDA come work Nallapati, Ahmed, Xing, Cohen
(2008), original papers describing techniques.

401

fiRossi, McDowell, Aha, & Neville

new features representing information node (e.g., age bracket?)
known information nodes neighbors (e.g., many liberal?).
Different techniques node feature construction described many previous
investigations, though feature construction necessarily focus many
investigations. section, summarize explain different aspects feature
construction. particular, Section 6.3.1 presents discusses taxonomy features
based kinds inputs, topology information link feature values,
use computing new feature values. Next, Section 6.3.2 describes possible operators, aggregation discretization, applied inputs. Finally,
Section 6.3.3 examines perform automatic feature search selection support
desired computational task.
6.3.1 Relational Feature Inputs
node feature categorized according types information uses
computing feature values. possible information use includes set nodes V
links E, node features XV , link features XE . Figure 8 shows taxonomy
node features based sources information (the inputs) use.
taxonomy consistent distinctions previously made literature (e.g., non-relational relational features), best knowledge
complete taxonomy never previously described. taxonomy consists
four basic types: non-relational features three types relational features (topology features, relational link-value features, relational node-value features).
describe give examples each.
Non-relational Features: node feature considered non-relational feature
value feature particular node computed using non-relational
features (i.e., attributes) node, ignoring link-based information. instance, Figure 8A shows node corresponding nodes feature vector. new
feature value might constructed vector using kind dimensionality reduction, adding together several feature values, thresholding particular
value, etc.
Topology Features: feature considered topology-based feature values
feature computed using nodes V links E, ignoring existing
node link feature values. instance, Figure 8B, new feature value
computed node bottom left figure (the target node), using
topological information shown. particular, new feature value might count
number adjacent nodes, count many shortest paths graph pass
target node.
Relational Link-value Features: feature considered relational link-value
feature feature values links adjacent target node
used computing new feature. Typically, kind aggregation operator
applied values, count, mode, average, proportion, etc. instance,
Figure 8C, values links shown represent communication topics (work
personal), new link-value feature might compute mode values (p).
402

fiTransforming Graph Data Statistical Relational Learning

input

V,E,XV,XE

target node

Node Feature
Construction

XV



link-value

p

V,E,XV,XE

node-value

L

Non-relational
Node Features

Relational Features

V,E V,E,XE V,E,XV
Topology
Features

Link-value
Features

Node-value
Features

C
p

L

p

.5 P

A.

w

B.

V

C.

L
D.

X

Figure 8: Node Features Taxonomy Based Inputs Used: classes node
features non-relational features, topology features, relational link-value features, relational node-value features. classes defined respect
relational information used construction features (i.e., nodes
V , links E, node features XV , link features XE ). double-lined target node
represents new feature value computed. Parts C show
single feature value link node simplicity, general
one feature may exist used.

Usually computation include links directly connected target
node, links hops away could used.
Relational Node-value Features: feature considered relational node-value
feature feature values nodes linked target node used construction. Links used identifying nodes, although nodes
one hop away target node may included. instance, Figure 8D
shows feature values adjacent nodes (C L) could, instance,
used compute new node-value feature based mode (L) values.
Alternatively, one feature might count number adjacent C nodes another
might count number adjacent L nodes.
403

fiRossi, McDowell, Aha, & Neville

Feature computation may applied recursively. instance, ReFeX system (Henderson, Gallagher, Li, Akoglu, Eliassi-Rad, Tong, & Faloutsos, 2011) first computes features every node based degree (a topology-based feature), considers recursive combinations features (such mean out-degree nodes
neighbors). Henderson et al. show recursive features often improve classification accuracy datasets network structure predictive. Alternatively,
topology-based feature betweenness might computed, relational nodevalue feature might compute average betweenness nodes neighbors
target label C. example hybrid feature uses
node-value topology-based information.
Another interesting aspect relational features potential feature value recomputation. particular, many techniques collective classification involve computing
node feature (such number neighbors currently labeled C) feature
depends feature values estimated (e.g., predicted node labels)
thus may change (Jensen et al., 2004; Sen et al., 2008). addition, McDowell, Gupta,
Aha (2010) describe features similar need recomputation,
meta-features use depend upon estimated label probabilities node
neighborhood target node. contrast, kind feature re-computation much
less applicability non-relational data, nodes assumed independent
other. However, occur techniques semi-supervised learning
co-learning.
6.3.2 Relational Feature Operators
previous section described features according different kinds inputs
use feature value computation, whereas section describes different operators
used computation. Table 5 summarizes operators.
cases, operator used many different types relational input. instance,
aggregation operators computed using graph topology, relational node-value
inputs, and/or relational link-value inputs, indicated appropriate checkmarks
Table 5. contrast, path walk-based operators generally use graph topology;
operators, lighter colored checkmarks Table 5 indicate path/walk-based
operators could sensibly used conjunction relational link-value node-values
inputs, rarely ever done. discuss operators
Table 5 detail.
Relational Aggregates: Aggregation refers function returns single value
collection input values set, bag, list. classical statistical
aggregation operators Average, Mode, Exists, Count, Max, Min, Sum (Neville
& Jensen, 2000; Lu & Getoor, 2003). SRL, another frequent operator Proportion,
computes, instance, fraction nodes neighbors meet criteria
label C (McDowell, Gupta, & Aha, 2007). operators may
combined thresholds, e.g., evaluate whether Count nodes neighbors
labeled C least 3. thresholding turns numerical aggregate Boolean
feature, needed tree-based algorithms (Neville, Jensen, Friedland, et al., 2003).
Perlich Provost (2003) describe set complex relational aggregates depend
404

fiTransforming Graph Data Statistical Relational Learning

Relational aggregates

Mode, Average, Count, Proportion, Degree, ...

Temporal aggregates

Exponential/linear decay, union, ...

X

Set operators

Union, intersection, multiset, ...

X

Clique potentials

Direct link cliques, co-citation cliques, triads, ...

Subgraph patterns

Two star, three-star, triangle (i.e., transitivity), ...

Dimensionality reduction

PCA, SVD, Factor Analysis, Principal Factor Analysis, Independent Component Analysis, ...

Path/walk-based measures

Betweenness, common neighbors, Jaccards coefficient, Adamic/Adar, shortest paths, random-walks,
...

Textual analysis

LSA, LDA, PLSA, Link-LDA, Link-PLSA, ...

X

Relational clustering

Spectral partitioning, Hierarchical clustering, Partitioning relocation methods (k-means, k-medoids),
...

X

X

Relational Node-value

Example Techniques

Relational Link-value

Relational Operators

Topology

Non-relational

Inputs

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

Table 5: Relational Feature Operators: Summary popular types relational feature operators. check used indicate classes inputs (see
Section 6.3.1) operator naturally uses constructing feature values, lighter check indicates operator could sensibly used
input combination rarely ever used.

405

fiRossi, McDowell, Aha, & Neville

distribution attribute values associated node (e.g., via links
relational join). instance, aggregates may use function edit distance
compare nodes distribution reference distribution computed training
data. Perlich Provost demonstrate aggregations cases improve
performance compared simpler alternatives. aggregate operators use
topology-based information. instance, operator Degree, simply counts
number adjacent links, predictive feature, applied carefully
relational data avoid bias (Jensen, Neville, & Hay, 2003).
Temporal Aggregates: Relational information might contain temporal information
form timestamps durations links, node, features. general, data
handled defining special temporal-aggregation features computed raw
data (McGovern, Collier, Matthew Gagne, Brown, & Rodger, 2008) defining graph
summarizes temporal information (usually decreasing importance
less recent information) (Sharan & Neville, 2008; Rossi & Neville, 2010). Rossi Neville
discuss example latter approach, explore impact using various
temporal-relational information various kernels summarization. Alternatively, Section 6.1 discusses notions temporal distance used modify path/walk-based
metrics node betweenness closeness.
Set Operators: traditional domain-independent set operators set union,
intersection, difference applied construct features (Kohavi & John, 1997).
instance, two attributes represent presence word
page (node), new feature might represent case page contains
words (i.e., feature intersection). relational data, complex set-based
features possible. instance, feature collective classification might represent
union class labels nodes adjacent target node. Neville, Jensen,
Gallagher (2003) propose complex approach feature value multiset
represents complete distribution adjacent nodes labels (e.g., {3C, 2M, 5L}
indicate labels ten adjacent nodes). Using feature representation, show
independent-value approach assumes labels independently
drawn distribution yields effective relational classification. Recently,
McDowell et al. (2009) showed that, CC, multiset approach usually outperformed
types features proportion count-based aggregates discussed above.
Clique Potentials: probabilistic models Relational Markov Networks (RMNs)
(Taskar et al., 2002) perform inference related nodes without computing aggregates.
Instead, use clique-specific potential functions represent probabilistic dependencies, product term probability computation naturally expands accommodate
varying number neighbors node. one sense, featureless approach,
since need choose relational aggregation function. However, different kinds
dependencies still represented different cliques. instance, Taskar et al.
consider different sets cliques webpage classification: one based hyperlinks,
including information based links appear within page. Likewise, later
work added additional types cliques enable link prediction (Taskar et al., 2003). Thus,
even models remain important feature choices made.
406

fiTransforming Graph Data Statistical Relational Learning

Figure 9: Subgraph Patterns Link Labels. subgraph represents possible
pattern particular feature could look relation target node (the
bottom-left node case).

probabilistic models use link-based information without computing explicit
features, random walk-based classifier Lin Cohen (2010) weightedneighbor approach Macskassy Provost (2007). Even cases, however, choices
remain types links use. instance, webpage graphs, co-citation
links may predictive class labels direct links (Macskassy & Provost, 2007;
McDowell et al., 2009).
Subgraph Patterns: subgraph pattern feature one based existence
particular pattern graph adjacent target node. feature might count
many times particular pattern exists target node, produce value true
least one pattern exists. simplest pattern called reciprocity; true
target node links node j j links back i. cases, however, patterns
complex involve nodes. Robins, Pattison, Kalish, Lusher (2007)
define many patterns including two-star (a node least two links), three-star (a
node least three links), triangle (also known transitivity, j k
k). patterns defined directed undirected links.
Many patterns possible. instance, Robins, Snijders, Wang, Handcock
(2006) use subgraph patterns probabilistically modeling graphs. argue using
complex patterns alternating k-triangle (based finding k triangles
share common side) help avoid degeneracy might otherwise arise
graph generation. Furthermore, subgraph patterns extended exploit labels
links and/or nodes. instance, assume links labeled 1 2 (representing different topics) links labeled plus minus sign (representing
positive negative relationships). Figure 9 demonstrates three possible subgraph patterns,
based different link labelings, relative target node shown bottom left
subgraph. subgraph feature could compute, node, number matches
one patterns, feature could used later analysis.
Dimensionality Reduction goal dimensionality reduction find lower kdimensional representation initial n features (Sarwar, Karypis, Konstan, & Riedl,
2000; Fodor, 2002). formally, given initial n-dimensional feature vector x =
{x1 , x2 , ..., xn }, find lower k-dimensional representation x x = {x1 , x2 , ..., xk }
k n significant information original data captured, according criterion. many dimensionality reduction methods Principal
407

fiRossi, McDowell, Aha, & Neville

Component Analysis (PCA), Principal Factor Analysis (PFA), Independent Component
Analysis (ICA).
Dimensionality reduction techniques applied adjacency matrix
graph G create low-dimensionality graph representation; Section 3.3 explained
used link prediction. techniques useful feature computation.
instance, Bilgic, Mihalkova, Getoor (2010) investigate active learning improve
accuracy collective classification. technique involves non-relational
relational features, demonstrate first applying dimensionality reduction (with
PCA) non-relational features simplifies learning, leading substantial gains accuracy.
Operators: mention briefly operators already discussed extensively elsewhere. Path-based measures (such betweenness distance)
walk-based measures (such PageRank) discussed Sections 6.1.
types measures used features classifier predict links (Lichtenwalter
et al., 2010) well validating relational sampling techniques (Leskovec, Chakrabarti,
Kleinberg, Faloutsos, & Ghahramani, 2010; Moreno & Neville, 2009; Ahmed, Neville, &
Kompella, 2012a, 2012b). measures typically use topology (not features), one could easily imagine computing metrics based, instance, paths
edge particular label type. Textual analysis techniques discussed
Sections 4.2 6.2, relational clustering techniques discussed Section 5.
operators used specifically node/link prediction, weighting, labeling,
used general feature construction.
Finally, operators based similarity measures. Similarity two
nodes often computed, instance link prediction (Section 3) weighting (Section 4.1). computations easily lead feature value link, since link
obviously refers two endpoint nodes compared. However, computing
node feature value, usually obvious node comparison, similarity measures typically used node feature values. measures can, however, used
node prediction, Section 5 discusses cases newly discovered nodes/groups
used create new node features. particular instance relational similarity
functions, graph kernels structured data (Gartner, 2003) used. kernels
used either nodes single graph (Kondor & Lafferty, 2002)
compute similarity two graphs (Vishwanathan, Schraudolph, Kondor, & Borgwardt, 2010). instance, former type kernel another technique could
used link group prediction.
Discussion: Many feature operators discussed naturally used compute feature values links additions nodes. instance, textual analysis applied
links text associated link, node-centered path-based measures
analogous formulations links. One difference nodes naturally may link
many nodes, whereas assume links two endpoints. Thus, relational aggregates Count initially seem useful computing link features. However,
Figure 4 previously demonstrated link-aggregation accomplished broadening computation include multiple links nodes logically connected
endpoint node target link. Naturally, feature inputs operators
408

fiTransforming Graph Data Statistical Relational Learning

better suited computing node features vs. computing link features. next section
examines select appropriate features given task.
6.3.3 Searching, Evaluating, Selecting Relational Features
Given large number possible features could used task (such
example Facebook classification task), features actually used learn
model? cases, selection done manually based prior experience trial
error. many situations, though, automatic feature selection desirable.
non-relational data, widely studied topic machine learning (Guyon &
Elisseeff, 2003; Koller & Sahami, 1996; Yang & Pedersen, 1997; Dash & Liu, 1997; Jain
& Zongker, 1997; Pudil, Novovicova, & Kittler, 1994), selecting relational features
received considerably less attention. Given large number possible features, efficient
strategies searching evaluating possible features needed. section,
first summarize two key problems feature search feature evaluation,
give examples issues resolved actual SRL systems.
Search: first step searching relational features define possible
relational feature space specifying possible raw feature inputs (e.g., node link
feature values) operators consider. possible operators include domainindependent operators (e.g., mode, count) and/or problem-specific operators (e.g., count
number friends divided number groups). Domain-independent operators
obviously general easier apply, problem-specific operators reduce
number possibilities must considered require effort expert
knowledge. However, approaches vulnerable selection biases (Jensen et al., 2003;
Jensen & Neville, 2002). second step pick appropriate search strategy, usually
either exhaustive, random, guided. exhaustive strategy consider features
possible given specified inputs operators, random strategy
consider fraction space. guided strategy use heuristic subsystem identify features considered. three cases, feature
considered subjected evaluation strategy assesses usefulness;
strategies described next.
Evaluation Selection: feature considered must evaluated
way determine retained use final model. instance, candidate
feature may evaluated adding current classification model; improves
accuracy holdout set, immediately (and greedily) added set retained
features (Davis, Burnside, Castro Dutra, Page, & Costa, 2005; Davis, Ong, Struyf, Burnside,
Page, & Costa, 2007). cases, every candidate feature assigned score
best scoring feature retained (Neville, Jensen, Friedland, et al., 2003),
features added model based decreasing score, long new features
continue improve model (Mihalkova & Mooney, 2007). Simpler techniques
require evaluating overall model used. instances, metrics
correlation mutual information used estimate useful feature
desired task. metrics strategies could used include Akaikes information
criterion (AIC) (Akaike, 1974), Mallows Cp (Mallows, 1973), Bayesian information criterion
(BIC) (Hannan & Quinn, 1979; Schwarz, 1978) many others (Shao, 1996; George &
409

fiRossi, McDowell, Aha, & Neville

Proposed System

Search method

Feature Evaluation

Exhaustive

Chi-square statistic/p-value

RDN-Boosting (Natarajan, Khot, Kersting, Gutmann, & Shavlik, 2012; Khot,
Natarajan, Kersting, & Shavlik, 2011)

Exhaustive

Weighted variance

ReFeX (Henderson et al., 2011)

Exhaustive

Log-binning disagreement

Random

Chi-square statistic/p-value

SAYU (Davis et al., 2005)

Aleph

AUC-PR

nFOIL (Landwehr et al., 2005)

FOIL

Conditional Log-Likelihood

SAYU-VISTA (Davis et al., 2007)

Aleph

AUC-PR

ProbFOIL (De Raedt & Thon, 2010)

FOIL

m-estimate

kFOIL (Landwehr et al., 2010)

FOIL

Kernel target alignment

Greedy hill-climbing

Bayesian model selection

Beam search

WPLL

Template-based

WPLL

Level-wise search

Pseudo-likelihood

Aleph++

m-estimate

RPT (Neville, Jensen, Friedland, et al.,
2003)

Spatiotemporal

RPT

(McGovern

et al., 2008)

PRM struct. learning (Getoor, Friedman, Koller, & Taskar, 2001)

TSDL (Kok & Domingos, 2005)
BUSL (Mihalkova & Mooney, 2007)
PBN

Learn-And-Join

(Khosravi,

Tong Man, Xu, & Bina, 2010)

Discriminative MLN structure
learning (Huynh & Mooney, 2008; Biba,
Ferilli, & Esposito, 2008)

Table 6: Systems Searching Selecting Node Features: summary
systems used automatically search select
appropriate features given task. Note that, depending context,
papers may describe function terms learning best rules
system learning structure (e.g., MLN). MLNbased systems described; these, WPLL weighted pseudo
log-likelihood.

McCulloch, 1993). Frequently, possible feature may particular parameter whose
value must set (such threshold); selecting best value given feature
use evaluation metrics may use simpler estimation technique, e.g., based
maximum likelihood.
Examples: Table 6 summarizes strategies used number SRL systems automatically search features. columns table describe system searches
410

fiTransforming Graph Data Statistical Relational Learning

features features evaluated. instance, Relational Probability Trees
(RPTs) (Neville, Jensen, Friedland, et al., 2003) extension probability estimation
trees relational data use exhaustive search strategy feature selection. particular, RPT learning involves automatically searching space possible features
using aggregation functions Mode, Average, Count, Proportion, Min, Max,
Exists, Degree. aggregations involve node link feature values (e.g.,
Average) topology information (e.g., Degree). features used
classification tasks, predicting class label document. feature
evaluated based using chi-square statistic measure correlation
feature class label; yields feature score associated p-value. Features
p-values level statistical significance discarded, remaining
feature highest score chosen inclusion model. selection process
extended use randomization tests adjust biases common
relational data (Jensen et al., 2003; Jensen & Neville, 2002). RPTs extended
temporal domains (Sharan & Neville, 2008; Rossi & Neville, 2012).
RPTs represent conditional probability distributions using single tree. contrast,
Natarajan et al. (2012) propose using gradient boosting (Friedman, 2001)
conditional probability distribution represented weighted sum regression trees
grown stage-wise optimization. features tree selected via depthlimited, exhaustive search, though note domain knowledge could used
guide search. Natarajan et al. argue resultant set multiple, relatively shallow
trees allows efficient learning complex structures, demonstrate technique
outperform alternatives based single trees Markov Logic Networks discussed
below.
Another system uses exhaustive search ReFeX (Henderson et al., 2011),
uses aggregates Sum Mean operators recursively generate features based
degree node local neighborhood. prune resultant large set, ReFeX uses
logarithmic binning feature values, clusters features based similarity
binned space, retains one feature cluster. logarithmic binning
chosen favors features discriminative high-degree nodes.
recursive approach modified constructing features dynamic
networks (Rossi, Gallagher, Neville, & Henderson, 2012).
Alternatively, spatiotemporal RPTs (McGovern et al., 2008) use random search strategy. particular, RPTs add temporal spatial-based features set possible
features. resultant feature space large exhaustive search, instead random
sampling used. pre-defined number features considered, best
scored feature added model.
remaining systems discuss use guided search strategy,
heuristic sub-system provides candidate features considered. instance,
several systems (Davis et al., 2005; Landwehr et al., 2005) use ILP system
generate candidate features, evaluate features select ultimate use.
particular, SAYU (Davis et al., 2005) uses ILP system Aleph (Srinivasan, 1999)
generate candidate feature (which consider new view original data).
Aleph creates candidates features based positive examples, training data,
concept predicted. proposed feature evaluated learning
411

fiRossi, McDowell, Aha, & Neville

new model includes feature computing area precision-recall
curve (AUC-PR). feature improves AUC-PR score, permanently added
model feature search continues. SAYU-VISTA (Davis et al., 2007) retains
general approach extends types features considered, particular
adding ability dynamically link together objects different types recursively
build new features constructed features. Davis et al. demonstrate link
connections especially helpful improving performance compared original SAYU
system. Landwehr et al. (2005) describe nFOIL system similar SAYU
developed independently, De Raedt Thon (2010) describe ProbFOIL
upgrades deterministic rule learner FOIL probabilistic. Landwehr et al. (2010)
describe related kFOIL system integrates FOIL kernel methods.
consider impact several different feature scoring functions.
number systems considered perform structure learning Probabilistic Relational Models (PRMs) (Getoor et al., 2001) Markov Logic Networks
(MLNs) (Domingos & Richardson, 2004), general case feature selection problems described above. instance, MLN weighted set first-order formulas;
structure learning corresponds learning formulas weight learning corresponds
learning associated weights. first MLN structure learning approaches systematically construct candidate clauses starting empty clause, greedily adding literals
it, testing resulting clauses fit training data using statistical measure (Kok
& Domingos, 2005; Biba et al., 2008). However, top-down approaches inefficient
initial proposal clauses ignores training data, resulting large number
possible features considered possible problems local minima. response,
number bottom-up approaches proposed. particular, Mihalkova
Mooney (2007) use propositional Markov network structure learner construct template
networks guide construction features based training data. recent
work examined enable bottom-up approaches learn longer clauses based
constraining search consider features consistent certain patterns motifs (Kok & Domingos, 2010), clustering input nodes create lifted graph
representation, enabling feature search smaller graph (Kok & Domingos, 2009).
Khosravi et al. (2010) perform MLN structure learning first learning structure
simpler Parametrized Bayes Net (PBN) (Poole, 2003), converting result
MLN. data contains significant number descriptive attributes, show
approach dramatically improves runtime structure learning improves
predictive accuracy. Schulte (2011) given theoretical justification approach.
Another alternative, proposed Khot et al. (2011), extend previously mentioned
work Natarajan et al. (2012) gradient boosting MLNs. Essentially, problem
learning MLNs transformed series relational regression problems
functional gradients represented clauses trees. several datasets demonstrate faster MLN structure learning accurate better baselines including
algorithms Mihalkova Mooney (2007) Kok Domingos (2010).
techniques MLNs seek learn network structure best explains
training data whole. contrast, situations prediction specific predicate desired (e.g., predict political affiliation Facebook example),
Huynh Mooney (2008) Biba et al. (2008) propose discriminative approaches
412

fiTransforming Graph Data Statistical Relational Learning

MLN structure learning. instance, Huynh Mooney use modified version
Aleph (Srinivasan, 1999) compute large number candidate clauses, use form
L1 -regularization force weights subsequently learned clauses
zero clause helpful predicting predicate. regularization,
conjunction appropriate optimization function, effectively leads selecting
smaller set features useful desired task.
Discussion: focus article graph-based data representations (see Section 1.2).
However, many examples discussed use logical representation instead.
include section techniques used constructing searching
features rules similar settings. instance, RPTs (a graphbased approach) RDN-Boosting (a logical approach) use exhaustive search
probabilistic decision trees, different feature scoring strategies.
Popescul et al. (2003a) examine automatically learn new relational features
links (to support link prediction), techniques could applied constructing
node features. particular, treat feature relational database query, use
concept refinement graphs (Shapiro, 1982) consider refining initial query
equi-joins, equality selections, statistical aggregates. refinement,
refinements considered; search guided sampling possible refinements proceeding results particular refinement type seems
promising. features chosen combined logistic regression classifier. evaluation specific features, use Bayesian Information Criterion (BIC) (Schwarz,
1978), includes term penalizes feature complexity reduce danger
overfitting.
discussed multiple systems include notions aggregation including RPTs,
SAYU-VISTA, work Popescul et al. (2003a) discussed above.
aggregate-based learning approaches Crossmine (Yin, Han, Yang, & Yu, 2006),
CLAMF (Frank, Moser, & Ester, 2007), Multi-relational Decision Trees (MRDTL) (Leiva,
Gadia, & Dobbs, 2002), Confidence-based Concept Discovery (C2 D) (Kavurucu, Senkul, &
Toroslu, 2008), many others (Perlich & Provost, 2006; Krogel & Wrobel, 2001; Knobbe,
Siebes, & Marseille, 2002). possibilities feature evaluation.
instance, GleanerSRL (Goadrich & Shavlik, 2007) uses Aleph (Srinivasan, 1999) search
clauses uses metric precision recall evaluating clauses.

7. Jointly Transforming Nodes Links
previous sections, primarily discussed relational representation transformation
techniques applied independently one another. instance, one technique
might used predict links, another builds transformed representation
applying node labeling technique. section instead examines joint transformation
tasks combine node link transformation way, instance label nodes
weight links simultaneously. techniques may enable subtask influence
helpful ways, avoids bias might introduced requiring
serialization two tasks (such link weighting node labeling) might usefully
performed jointly.
413

fiRossi, McDowell, Aha, & Neville

One recent approach proposed Namata, Kok, Getoor (2011) collectively performs link prediction, node labeling, entity resolution (which seen form
node deletion/merging). present iterative algorithm solves three tasks
simultaneously propagating information among solutions three tasks.
particular, introduce notion inter-relational features, relational features one task depend upon predicted values another. results show
using features improve accuracy, inferring predicted values
three tasks simultaneously significantly improve accuracy compared performing
three tasks sequence, even possible orderings considered.
Techniques model full distribution across links attributes RMNs
(Taskar et al., 2002), PRMs (Friedman et al., 1999), MLNs (Domingos & Richardson,
2004) used scenario, instance jointly predict node link labels.
section, however, focus particularly recent techniques presume
existence textual content associated nodes links graph
(although basic algorithms would work kinds features). consider
three types techniques, based kind input text use: stand-alone text
documents (e.g., legal memos links), text documents connected links (e.g.,
webpages hyperlinks), entities connected links associated text (e.g.,
people connected email messages). Table 7 lists prominent models,
grouped according three types. columns table indicate kinds
input models use (middle section) types transformation perform
(right-hand section). text documents corresponds node features table,
text associated links yields link features. discuss three types
techniques detail.
7.1 Using Text Documents Links
First, many techniques used assign topics labels nodes nodes
(such documents) associated text. instance, first row Table 7 indicates
LDA PLSA use nodes node features perform node prediction,
weighting, labeling. Section 6 already mentioned techniques used
label node one discovered topics, typical use. However,
techniques perform node weighting (using weights associated
topics) and/or node prediction (by converting discovered topics new latent nodes
discussed introduction Section 5). Table 7, use lighter checkmarks
represent kind situations transformation task could performed
particular model primary use/output.
LDA PLSA treat document bag words seek assign one
topics (labels) document based words. contrast, Nubbi (Chang, BoydGraber, & Blei, 2009) designs approach based LDA graph defined based
objects (nodes) referenced set documents, links predicted based
relationships implied text documents. addition, nodes
links associated likely topic(s) based relationships. Thus,
model simultaneously performs link prediction, link labeling, node labeling. similar
414

fiTransforming Graph Data Statistical Relational Learning

Link Labeling

Node Prediction

Node Weighting

Node Labeling

E

XE

XE

V

XV

XV

V

XV

LDA/PLSA

X

X

Nubbi

X

X

X

Joint Transformation Model

E

XE

Link Weighting

Input

Nodes

Link Prediction

Links

X

X

X

X

X

X

X

Link-LDA, Link-PLSA

X

X

X

X

X

X

X

X

Pairwise-Link-LDA

X

X

X

X

X

X

X

X

Link-PLSA-LDA

X

X

X

X

X

X

X

X

Relational Topic Model (RTM)

X

X

X

X

X

X

X

X

Topic-Link LDA

X

X

X

X

X

X

X

X

Group-Topic (GT)

X

X

X

X

X

X

X

X

Author-Recipient-Topic (ART)

X

X

X

X

X

Block-LDA

X

X

X

X

X

X
X

X

X

Table 7: Summary Joint Transformation Models: middle section
table indicates types graph features used inputs model,
right side table indicates types link node transformation
performed model. Lighter checkmarks indicate output
model transformed perform particular transformation task (e.g.,
use node labels create new latent group nodes), task
primary goal specified model.

415

fiRossi, McDowell, Aha, & Neville

result produced semantic network extraction Kok Domingos (2008)
discussed Section 4.2.
7.2 Using Text Document Links
second type joint transformation uses text documents, adds known links
documents model. instance, Section 6 discussed Link-LDA
Link-PLSA add link modeling LDA PLSA order perform node labeling;
discussed LDA PLSA modified achieve node prediction
weighting. shown Table 7, Link-LDA Link-PLSA used link
prediction weighting learning model training graph using
predict unseen links new test graph (Nallapati et al., 2008).
Link-LDA Link-PLSA model links way similar model
presence words document (node). instance, Link LDAs generative model,
generate one word, document chooses topic, chooses word topicspecific multinomial. identical process (using topic-specific multinomial) used
generate, particular document, one target document link to. Thus, Link-LDA
Link-PLSA directly extend original LDA PLSA models add links.
Nallapati et al. (2008) argue Link-LDAs Link-PLSAs extensions links,
pragmatic, adequately capture topical relationship two documents
linked together. Instead, propose two alternatives. first, Pairwise LinkLDA, replaces link model Link-LDA model based mixed membership
stochastic blockmodels (Airoldi et al., 2008), possible link modeled
Bernoulli variable conditioned topic chosen based topic distributions
two endpoints link. second approach, Link-PLSA-LDA, retains
link generation model Link-LDA, changes word generation model
documents (the ones incoming links) words document depend
topics documents link it. downside latter approach
works nodes divided set outgoing links set
incoming links. However, Nallapati et al. argue limitation largely
overcome duplicating nodes incoming outgoing links. Moreover,
approach much faster scalable Pairwise Link-LDA. Nallapati et al.
demonstrate models outperform Link-LDA likelihood ranking task,
Link-PLSA-LDA outperforms Link-LDA link prediction task. show
Link-PLSA-LDA Link-LDA comparable terms execution time,
Pairwise Link-LDA much slower.
Changes generative model used approaches encode different assumptions data lead significant performance differences. instance,
Chang Blei (2009) introduce Relational Topic Model (RTM) compare
Pairwise Link-LDA model discussed above. models allow similar flexibility terms
links defined, Chang Blei argue model forces topic
assignments used generate words documents generate
links, true Pairwise Link-LDA. demonstrate RTM provides
accurate predictions link suggestions Pairwise Link-LDA several
baselines.
416

fiTransforming Graph Data Statistical Relational Learning

Another possible change model add types objects. instance,
Topic-Link LDA (Liu, Niculescu-Mizil, & Gryc, 2009) models documents, links,
likely topics associated document, explicitly considers
author document clusters authors multiple communities. Creating
new clustering equivalent finding per-document topics author
associated one document. argue approach analogous
unifying separate tasks (1) assigning topics documents (2) analyzing
social network authors. show approach cases outperform
LDA Link-LDA.
7.3 Using Text Associated Links
final type joint transformation techniques form link features based text associated
links, text email messages (McCallum, Wang, & Corrada-Emmanuel,
2007) scientific abstracts relate particular protein-protein interaction (Balasubramanyan & Cohen, 2011). Several techniques discussed previously
context link interpretation. instance, Section 4.2 discussed models
Author-Recipient-Topic (ART) model (McCallum, Wang, & Corrada-Emmanuel, 2007)
Group-Topic (GT) model (McCallum, Wang, & Mohanty, 2007) extend LDA perform
link labeling; strength predicted labels (topics) used weight
links. addition, GT model directly assigns nodes groups (i.e., node labeling),
labels ART associates link could used label associated
nodes. RART model (McCallum, Wang, & Corrada-Emmanuel, 2007) extends ART
allowing node multiple roles. recently, Block-LDA (Balasubramanyan
& Cohen, 2011) merges ideas latent variables models stochastic blockmodels. specifically, Block-LDA shares information three components:
link model shares information block structure shared topic
model. Unlike GT ART, however, Block-LDA focuses labeling nodes rather
links. Balasubramanyan Cohen evaluate Block-LDA protein dataset
Enron email corpus demonstrate outperforms Link-LDA several
baselines task protein functional category prediction.
7.4 Discussion
techniques discussed variants latent group models focus
node and/or link label prediction, used node prediction
new nodes represent newly discovered topics latent groups. models
extended incorporate notions time (Dietz, Bickel, & Scheffer, 2007; Wang, Blei, &
Heckerman, 2008; Wang & McCallum, 2006), topic hierarchies (Li & McCallum, 2006),
correlations topics (Blei & Lafferty, 2007). addition, links usually assumed
generated based overall topic(s) node link. contrast, Latent
Topic Hypertext Model (LTHM) (Gruber, Rosen-Zvi, & Weiss, 2008) models link
originating specific word document. Somewhat surprisingly, show
approach leads model fewer parameters models Link-LDA,
demonstrate approach outperforms Link-LDA Link-PLSA
evaluated link prediction task.
417

fiRossi, McDowell, Aha, & Neville

(a) Initial Graph

(b) Joint Transformation

Figure 10: Example Joint Transformation: example, new latent nodes
added represent discovered topics, weighted links added
original node new latent node. addition, weighted links added
latent nodes, representing connection strength topics.
Finally, new links original nodes may predicted. Note
example adapted results found work Nallapati et al. (2008).

new nodes added graph represent discovered topics, links
invariably added connect existing nodes new nodes. However, models may
learn information discovered topics related other.
instance, Figure 10 shows two new topics discovered graph
connected existing nodes. addition, topics connected
new links weight link represents frequently document
topic cites document representing different topic. Adding additional links
graph lets original nodes connected closely primary topics
related topics.

8. Discussion Challenges
section discuss additional issues related relational representation
transformation highlight important challenges future work.
8.1 Guiding Evaluating Representation Transformation
goal representation transformation often improve data representation
way leads better results subsequent task possibly understandable representation. evaluate whether particular transformation technique
accomplished goal? first address question, consider final
goal used directly guide initial transformation.
tasks, representation evaluation straightforward provided ground truth
values known hold-out data set. instance, test technique link
418

fiTransforming Graph Data Statistical Relational Learning

prediction effective, accuracy measured links predicted hold-out set
(Taskar et al., 2003; Liu et al., 2009). particular evaluation metric modified
appropriate domain. instance, Chang Blei (2009) evaluate precision
twenty highest-ranked links suggested document, Nallapati et al. (2008)
consider custom metric called RKL measures rank last true link suggested
model. Likewise, desired task involves classification, classification
algorithm run hold-out data, without representation change,
see change increases classification accuracy.
cases, may difficult directly measure well representation change
performed, classification used surrogate measure: accuracy increases,
change assumed beneficial. instance, classification used evaluate link prediction (Gallagher et al., 2008), link weighting (Xiang et al., 2010), link labeling (Rossi & Neville, 2010; Macskassy, 2007), node prediction (Neville & Jensen,
2005). addition, node labeling naturally classification problem, node weighting
usually evaluated ways, e.g., based query relevance.
techniques used direct evaluation feasible, exists
metric believed related. instance, higher autocorrelation
graph associated presence sensible links, algorithms
collective classification typically perform better level autocorrelation higher.
Thus, Xiang et al. (2010) demonstrate success technique estimating relationship strengths (link weights) based part showing increase autocorrelation
measured several attributes social network. Likewise, increased information gain
attributes could used demonstrate improved representation (Lippi,
Jaeger, Frasconi, & Passerini, 2009), link perplexity could used assess topic labelings (Balasubramanyan & Cohen, 2011). Naturally, appropriate evaluation
techniques vary based upon task, comparison transformation techniques may
yield different results depending upon metric chosen.
Ideally, representation transformation would guided directly final goal
executed, rather evaluated transformation complete.
often case feature selection structure learning algorithms discussed
Section 6.3: task accuracy (or surrogate measure) evaluated particular feature
added, retained accuracy improved. cases, transformation
even directly specified desired end goal. instance, supervised random
walk approach discussed Section 3.3 uses gradient descent method obtain new link
weights links predicted subsequent random walk (their final goal)
accurate. Likewise, Menon Elkan (2010) show add supervision methods
generating latent features (see introduction Section 5) features learned
would relevant final classification task. show, however, adding
supervision always helpful. final example, Shi, Li, Yu (2011) use
quadratic program optimize linear combination link weights final link
weights lead directly accurate classification via label propagation algorithm.
general, ensuring particular transformation improve performance
final SRL task remains challenging. Many transformations cannot directly guided
final goal, either suitable supervised data available, clear
419

fiRossi, McDowell, Aha, & Neville

modify transformation algorithms use information (e.g., latent
topic models Section 7 group detection algorithms Section 5).
8.2 Causal Discovery
Causal discovery refers identifying cause-and-effect relationships (i.e., smoking causes
cancer) either online experimentation (Aral & Walker, 2010) observational
data. challenge distinguish true causal relationships mere statistical correlations. One approach use quasi-experimental designs (QEDs), take advantage
circumstances non-experimental data identify situations provide equivalent
experimental control randomization. Jensen, Fast, Taylor, Maier (2008) propose
system discover knowledge applying QEDs discovered automatically.
recently, Oktay, Taylor, Jensen (2010) apply three different QEDs demonstrate
one gain causal understanding social media system. another causal
discovery technique linear models proposed Wang Chan (2010). challenge
remains extend techniques apply broader range relational data.
8.3 Subgraph Transformation Graph Generation
majority article focused transformation tasks centered around nodes
links graphs. However, useful tasks subgraph transformation
seek identify frequent/informative substructures set graphs create features
classify subgraphs (Inokuchi, Washio, & Motoda, 2000; Deshpande, Kuramochi,
Wale, & Karypis, 2005). instance, Kong Yu (2010) consider use semisupervised techniques perform feature selection subgraph classification given
labeled subgraphs. nodes links, subgraphs tasks prediction,
labeling, weighting, feature generation described. Many techniques
described node-centered features used context, full
discussion subgraph transformation beyond scope article.
Recently, graph generation algorithms attracted significant interest. algorithms use model represent family graphs, present way generate multiple samples family. Two prominent models Kronecker Product Graph Models
(KPGMs) (Leskovec, Chakrabarti, et al., 2010) based preferential attachment
(Price, 1976; Barabasi & Albert, 1999). graph generation methods take advantage
global (with KPGMs) local (with preferential attachment models) graph properties
generate distribution graphs potentially include attributes. Sampling
models useful creating robust algorithms, instance training
classifier family related graphs instead single graph. Newman (2003) surveys
additional network models properties relevant graph generation.
8.4 Model Representation
SRL notion model representation: kind statistical model
learned represent relationship nodes, links, features?
prominent models SRL Probabilistic Relational Models (PRMs) (Friedman
et al., 1999), Relational Markov Networks (RMNs) (Taskar et al., 2002), Relational Depen420

fiTransforming Graph Data Statistical Relational Learning

dency Networks (RDNs) (Neville & Jensen, 2007), Structural Logistic Regression (Popescul
et al., 2003b), Conditional Random Fields (CRFs) (Lafferty, McCallum, & Pereira, 2001),
Markov Logic Networks (MLNs) (Domingos & Richardson, 2004; Richardson & Domingos, 2006); full discussion models beyond scope article. many cases
techniques relational representation transformation, link prediction, performed regardless kind statistical model subsequently used. However,
choice statistical model strongly interact kinds node link features
useful (or even possible use); Section 6.3 describes connections.
number relevant comparisons already published (Jensen et al., 2004; Neville
& Jensen, 2007; Macskassy & Provost, 2007; Sen et al., 2008; McDowell et al., 2009; Crane
& McDowell, 2011), work needed evaluate interaction choice
statistical model feature selection, evaluate statistical models work best
domains certain characteristics.
8.5 Temporal Spatial Representation Transformation
appropriate, already discussed multiple techniques incorporate
temporal information graph data (see especially Sections 4.2, 6.1, 6.3).
techniques focused solving particular problems node classification, dealing
data invariably requires studying represent time-varying elements.
However, work needed examine general tradeoffs involved different
temporal representations. instance, Hill, Agarwal, Bell, Volinsky (2006) provide
generic framework modeling temporal dynamic network central goal
build approximate representation satisfies pre-specified objectives. focus
summarization (representing historical behavior two nodes concise manner),
simplification (removing noise edges nodes, spurious transactions, stale relationships), efficiency (supporting fast analysis updating), predictive performance
(optimizing representation maximize predictive performance). work provides
number useful building blocks, comparisons needed to, instance, evaluate merits using summarized networks general-purpose algorithms vs. using
specialized algorithms data maintains temporal distinctions.
Temporal data one particular kind data represented relational
sequence. Kersting, De Raedt, Gutmann, Karwath, Landwehr (2008) survey area
relational sequence learning explains multiple tasks related data,
sequence mining alignment. tasks often involve need identify relevant
features structure, identifying frequent patterns useful similarity functions.
Thus, set useful techniques feature construction search domain overlap
discussed Section 6.3.
8.6 Privacy Preserving Representation
sometimes desire make private graph-based data publicly available (e.g.,
support research public policy) way preserves privacy individuals
described data. goal privacy preserving representation transform
data way minimizes information loss maximizing anonymization, e.g.,
prevent individuals anonymized network identified. Naive approaches
421

fiRossi, McDowell, Aha, & Neville

anonymization operate simply replacing individuals name (or attributes)
arbitrary meaningless unique identifiers. However, social networks many
adversarial methods true identity user often discovered
anonymized network. particular, adversarial methods use network
structure and/or remaining attributes discover identities users within network
(Liu & Terzi, 2008; Zhou, Pei, & Luk, 2008; Narayanan & Shmatikov, 2009).
early approach Zheleva Getoor (2007) examines graph may modified
prevent sensitive relationships (a particular kind labeled link) disclosed.
describe approach terms node anonymization edge anonymization.
Node anonymization clusters nodes equivalence classes based node attributes
only, edge anonymization approaches based cleverly removing
sensitive edges. Backstrom, Dwork, Kleinberg (2007) address related family attacks
adversary able learn whether edge exists targeted pairs nodes.
recently, Hay, Miklau, Jensen, Towsley, Weis (2008) study privacy issues
graphs contain attributes. goal prevent structural re-identification
(i.e., identity reconstruction using graph topology information) anonymizing graph via
creating aggregate network model allows samples drawn model.
approach generalizes graph partitioning nodes summarizing graph
partition level. approach differs approaches described
drastically changes representation opposed making incremental
changes. However, method enforces privacy still preserving enough network
properties allow wide variety network analyses performed.
investigations key factors information available graph,
resources attacker, type attacks must defended against.
addition, attacker possibly obtain additional information related graph
sources, challenges even difficult. work needed
provide strong privacy guarantees still enabling partial public release graph-based
information.

9. Conclusion
Given increasing prevalence importance relational data, article surveyed
significant current issues relational representation transformation. presenting new taxonomy important transformation tasks Section 2, next
discussed four primary tasks link prediction, link interpretation, node prediction,
node interpretation. Section 7 considered tasks accomplished
simultaneously via techniques joint transformation. Finally, Section 8 considered
perform representation evaluation key challenges future work.
additional possible representation transformations space
discuss, fit cleanly taxonomy Figure 2. instance, bipartite
graph customers products, may useful eliminate product nodes, replacing
information content new links among customers purchased
product. somewhat related group discovery techniques Section 5.
considered depth potential transforming nodes edges
422

fiTransforming Graph Data Statistical Relational Learning

vice versa (though representation choices Figure 6 relevant here),
technique sometimes useful pre-processing step.
taxonomy presented Section 2 highlighted symmetry possible
transformation tasks links nodes. symmetry helped organize
survey, suggests areas techniques developed one entities
used analogous task other. instance, Liben-Nowell Kleinberg
(2007) reformulated traditional node weighting algorithms weight links. Likewise, topic
discovery techniques based LDA used node labeling link labeling.
Finally, many techniques used create node features used create link
features, vice versa, although node features studied much thoroughly.
discussed Section 8, remains much work do. instance, link prediction
remains difficult problem, especially general case two arbitrary
nodes might connected together. Even significantly, described
wide range techniques address transformation tasks, end
day practitioner left wide range choices without many guarantees
might work best. instance, node weighting may improve classification accuracy
one dataset decrease another. challenge made difficult
techniques described come wide range areas, including
graph theory, social network analysis, numerical linear algebra (e.g., matrix factorization),
metric learning, information theory, information retrieval, inductive logic programming,
statistical relational learning, probabilistic graphical models. breadth
techniques relevant relational transformation wonderful resource, means
evaluating representation change techniques relevant particular task
time-consuming, technically challenging, incomplete process. Therefore, much
work needed establish theoretical understanding different representation
changes affect data, different data characteristics interact process,
combination techniques data characteristics affect final results
analysis relational data.

Acknowledgments
thank reviewers many helpful suggestions feedback. majority
work completed Naval Research Laboratory, Ryan Rossi supported
ASEE/ONR NREIP summer internship 2010 NSF Graduate Research
Fellowship (at Purdue University). Luke McDowell supported part NSF award
number 1116439 grant ONR. research partly supported
NSF contract number IIS-1149789. views conclusions contained
herein authors interpreted necessarily representing
official policies endorsements, either expressed implied, ONR, NSF, U.S.
Government.

References
Adamic, L. A., & Adar, E. (2001). Friends neighbors web. Social Networks,
25 (3), 211230.
423

fiRossi, McDowell, Aha, & Neville

Adibi, J., Chalupsky, H., Melz, E., Valente, A., et al. (2004). KOJAK group finder:
Connecting dots via integrated knowledge-based statistical reasoning.
Proceedings 16th Conference Innovative Applications Artifical Intelligence,
pp. 800807.
Adomavicius, G., & Tuzhilin, A. (2005). Toward next generation recommender
systems: survey state-of-the-art possible extensions. IEEE Transactions
Knowledge Data Engineering, 17 (5), 734749.
Ahmed, N., Neville, J., & Kompella, R. (2012a). Network sampling designs relational
classification. Proceedings 6th International AAAI Conference Weblogs
Social Media.
Ahmed, N., Neville, J., & Kompella, R. (2012b). Space-efficient sampling social activity
streams. BigMine, pp. 18.
Ahmed, N., Berchmans, F., Neville, J., & Kompella, R. (2010). Time-based sampling
social network activity graphs. Proceedings 8th Workshop Mining
Learning Graphs, pp. 19.
Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed membership
stochastic blockmodels. Journal Machine Learning Research, 9, 19812014.
Akaike, H. (1974). new look statistical model identification. IEEE Transactions
Automatic Control, 19 (6), 716723.
Albert, R., Jeong, H., & Barabasi, A. (1999). Internet: Diameter world-wide web.
Nature, 401 (6749), 130131.
Amarel, S. (1968). representations problems reasoning actions. Machine
Intelligence, 3, 131171.
Anthony, A., & desJardins, M. (2007). Data clustering relational push-pull model.
Proceedings Seventh IEEE International Conference Data Mining Workshops, ICDMW 07, pp. 189194.
Aral, S., & Walker, D. (2010). Creating Social Contagion Viral Product Design:
Randomized Trial Peer Influence Networks. Proceedings 31st International Conference Information Systems.
Backstrom, L., & Leskovec, J. (2011). Supervised random walks: predicting recommending links social networks. Proceedings 4th International Conference
Web Search Data Mining, pp. 635644.
Backstrom, L., Dwork, C., & Kleinberg, J. M. (2007). Wherefore art thou r3579x?:
anonymized social networks, hidden patterns, structural steganography. Proceedings 16th International World Wide Web Conference, pp. 181190.
Balasubramanyan, R., & Cohen, W. (2011). Block-LDA: Jointly modeling entity-annotated
text entity-entity links. Proceedings 7th SIAM International Conference
Data Mining.
Barabasi, A., & Albert, R. (1999). Emergence scaling random networks. Science,
286 (5439), 509512.
424

fiTransforming Graph Data Statistical Relational Learning

Barabasi, A., & Crandall, R. (2003). Linked: new science networks. American journal
Physics, 71 (4), 409410.
Basilico, J. B., & Hofmann, T. (2004). Unifying collaborative content-based filtering.
Proceedings 21st International Conference Machine Learning, pp. 6572.
Ben-Hur, A., & Noble, W. (2005). Kernel methods predicting protein-protein interactions. Bioinformatics, 21 (Suppl. 1), 3846.
Benczur, A., Csalogany, K., Sarlos, T., & Uher, M. (2005). Spamrankfully automatic link
spam detection. Adversarial Information Retrieval Web, pp. 2538.
Berkhin, P. (2006). Survey clustering data mining techniques. Grouping Multidimensional
Data: Recent Advances Clustering, 10, 2571.
Bharat, K., & Henzinger, M. (1998). Improved algorithms topic distillation hyperlinked environment. Proceedings 21st International SIGIR Conference
Research Development Information Retrieval, pp. 104111.
Bhattacharya, I., & Getoor, L. (2005). Relational clustering multi-type entity resolution.
Proceedings 4th International workshop Multi-relational Mining, pp. 312.
Bhattacharya, I., & Getoor, L. (2007). Collective entity resolution relational data. Transactions Knowledge Discovery Data, 1 (1), 136.
Biba, M., Ferilli, S., & Esposito, F. (2008). Discriminative structure learning Markov
logic networks. Inductive Logic Programming, 5194, 5976.
Bilgic, M., Mihalkova, L., & Getoor, L. (2010). Active learning networked data.
Proceedings 27th International Conference Machine Learning.
Bilgic, M., Namata, G. M., & Getoor, L. (2007). Combining collective classification link
prediction. Proceedings 7th IEEE International Conference Data Mining
Workshops, pp. 381386.
Blei, D., Ng, A., & Jordan, M. (2003). Latent Dirichlet allocation. Journal Machine
Learning Research, 3, 9931022.
Blei, D., & Lafferty, J. (2007). correlated topic model science. Annals Applied
Statistics, 1 (1), 1735.
Bonacich, P., & Lloyd, P. (2001). Eigenvector-like measures centrality asymmetric
relations. Social Networks, 23 (3), 191201.
Borgman, C., & Furner, J. (2002). Scholarly communication bibliometrics. Annual
Review Information Science Technology, 36, 372.
Brightwell, G., & Winkler, P. (1990). Maximum hitting time random walks graphs.
Random Structures & Algorithms, 1 (3), 263276.
Broder, A., Kumar, R., Maghoul, F., Raghavan, P., Rajagopalan, S., Stata, R., Tomkins,
A., & Wiener, J. (2000). Graph structure web. Computer networks, 33 (1-6),
309320.
Burges, C. (1998). tutorial support vector machines pattern recognition. Data
mining knowledge discovery, 2 (2), 121167.
425

fiRossi, McDowell, Aha, & Neville

Cafarella, M. J., Wu, E., Halevy, A., Zhang, Y., & Wang, D. Z. (2008). Webtables: Exploring
power tables web. Proceedings VLDB, pp. 538549.
Camacho, J., Guimera, R., & Nunes Amaral, L. (2002). Robust patterns food web
structure. Physical Review Letters, 88 (22), 228102: 14.
Chakrabarti, S., Dom, B., & Indyk, P. (1998). Enhanced hypertext categorization using
hyperlinks. Proceedings ACM SIGMOD International Conference Management Data, pp. 307318.
Chakrabarti, S., Dom, B., Raghavan, P., Rajagopalan, S., Gibson, D., & Kleinberg, J.
(1998). Automatic resource compilation analyzing hyperlink structure associated text. Computer Networks ISDN Systems, 30 (1-7), 6574.
Chang, J., & Blei, D. (2009). Relational topic models document networks.
9th International Conference Artificial Intelligence Statistics (AISTATS), pp.
8188.
Chang, J., Boyd-Graber, J., & Blei, D. (2009). Connections lines: augmenting
social networks text. Proceedings 15th ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 169178.
Clauset, A., Moore, C., & Newman, M. (2008). Hierarchical structure prediction
missing links networks. Nature, 453 (7191), 98101.
Cohn, D., & Chang, H. (2000). Learning probabilistically identify authoritative documents. Proceedings 17th International Conference Machine Learning, pp.
167174.
Cohn, D., & Hofmann, T. (2001). missing link-a probabilistic model document content hypertext connectivity. Advances Neural Information Processing Systems,
13, 430436.
Crane, R., & McDowell, L. K. (2011). Evaluating markov logic networks collective
classification. Proceedings 9th MLG Workshop 17th ACM SIGKDD
Conference Knowledge Discovery Data Mining.
Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., & Slattery,
S. (2000). Learning construct knowledge bases World Wide Web. Artificial
Intelligence, 118 (1-2), 69113.
Cristianini, N., & Shawe-Taylor, J. (2000). Introduction Support Vector Machines
kernel-based learning methods. Cambridge University Press.
Dash, M., & Liu, H. (1997). Feature selection classification. Intelligent data analysis,
1 (3), 131156.
Davis, J., Burnside, E., Castro Dutra, I., Page, D., & Costa, V. (2005). integrated
approach learning Bayesian networks rules. Proceedings European
Conference Machine Learning, pp. 8495.
Davis, J., Ong, I., Struyf, J., Burnside, E., Page, D., & Costa, V. S. (2007). Change representation statistical relational learning. Proceedings 20th International
Joint Conference Artificial Intelligence, pp. 27192725.
426

fiTransforming Graph Data Statistical Relational Learning

De Raedt, L. (2008). Logical relational learning. Springer.
De Raedt, L., & Kersting, K. (2008). Probabilistic inductive logic programming. SpringerVerlag.
De Raedt, L., & Thon, I. (2010). Probabilistic rule learning. Inductive Logic Programming,
6489, 4758.
Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. (1990).
Indexing latent semantic analysis. Journal American Society Information
Science, 41, 391407.
Deshpande, M., Kuramochi, M., Wale, N., & Karypis, G. (2005). Frequent substructurebased approaches classifying chemical compounds. IEEE Transactions Knowledge Data Engineering, 13, 10361050.
Dhillon, I. (2001). Co-clustering documents words using bipartite spectral graph partitioning. Proceedings seventh ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 269274.
Dietz, L., Bickel, S., & Scheffer, T. (2007). Unsupervised prediction citation influences.
Proceedings 24th International Conference Machine Learning, pp. 233240.
Domingos, P., & Richardson, M. (2004). Markov logic: unifying framework statistical
relational learning. Proceedings ICML Workshop Statistical Relational
Learning, pp. 4954.
DuBois, C., & Smyth, P. (2010). Modeling Relational Events via Latent Classes. Proceedings 16th ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 803812.
Dunne, J., Williams, R., & Martinez, N. (2002). Food-web structure network theory:
role connectance size. Proceedings National Academy Sciences
United States America, 99 (20), 12917.
Easley, D., & Kleinberg, J. (2010). Networks, Crowds, Markets: Reasoning
Highly Connected World. Cambridge University Press.
Eckart, C., & Young, G. (1936). approximation one matrix another lower rank.
Psychometrika, 1 (3), 211218.
Egghe, L., & Rousseau, R. (1990). Introduction informetrics. Elsevier Science Publishers.
Erosheva, E., Fienberg, S., & Lafferty, J. (2004). Mixed-membership models scientific
publications. Proceedings National Academy Sciences United States
America, 101 (Suppl 1), 5220.
Essen, U., & Steinbiss, V. (1992). Cooccurrence smoothing stochastic language modeling. Proceedings International Conference Acoustics, Speech, Signal
Processing, pp. 161164.
Faloutsos, M., Faloutsos, P., & Faloutsos, C. (1999). power-law relationships
internet topology. Proceedings ACM SIGCOMM International Conference
Applications, Technologies, Architectures, Protocols Computer Communication, pp. 251262.
427

fiRossi, McDowell, Aha, & Neville

Fast, A., & Jensen, D. (2008). stacked models perform effective collective classification.
Proceedings IEEE International Conference Data Mining, pp. 785790.
Fodor, I. (2002). Survey Dimension Reduction Techniques. US DOE Office Scientific
Technical Information, 18.
Frank, R., Moser, F., & Ester, M. (2007). method multi-relational classification
using single multi-feature aggregation functions. Proceedings Principles
Practice Knowledge Discovery Databases, 1, 430437.
Freeman, L. C. (1977). set measures centrality based betweenness. Sociometry,
40, 3541.
Friedman, J. (2001). Greedy function approximation: gradient boosting machine..
Annals Statistics, 29 (5), 11891232.
Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational models. Proceedings 16th International Joint Conference Artificial
Intelligence, pp. 13001309. Springer-Verlag.
Gallagher, B., Tong, H., Eliassi-Rad, T., & Faloutsos, C. (2008). Using ghost edges
classification sparsely labeled networks. Proceedings 14th ACM SIGKDD
International Conference Knowledge Discovery Data Mining, pp. 256264.
Gartner, T. (2003). survey kernels structured data. ACM SIGKDD Explorations
Newsletter, 5 (1), 4958.
George, E., & McCulloch, R. (1993). Variable selection via Gibbs sampling. Journal
American Statistical Association, 88, 881889.
Getoor, L., Friedman, N., Koller, D., & Taskar, B. (2003). Learning probabilistic models
link structure. Journal Machine Learning Research, 3, 679707.
Getoor, L., & Taskar, B. (Eds.). (2007). Introduction Statistical Relational Learning.
MIT Press.
Getoor, L., & Diehl, C. P. (2005). Link mining. SIGKDD Explorations, 7, 312.
Getoor, L., Friedman, N., Koller, D., & Taskar, B. (2001). Learning probabilistic models
relational structure. Proceedings International Conference Machine Learning,
pp. 170177.
Gibson, D., Kleinberg, J., & Raghavan, P. (1998). Inferring web communities link
topology. Proceedings 9th ACM Conference Hypertext Hypermedia,
pp. 225234.
Gilbert, E., & Karahalios, K. (2009). Predicting tie strength social media. Proceedings 27th CHI International Conference Human Factors Computing
Systems, pp. 211220.
Girvan, M., & Newman, E. J. (2002). Community structure social biological networks.
Proceedings National Academy Sciences, 99 (12), 78217826.
Goadrich, M., & Shavlik, J. (2007). Combining clauses various precisions recalls
produce accurate probabilistic estimates. Proceedings 17th International
Conference Inductive Logic Programming, pp. 122131.
428

fiTransforming Graph Data Statistical Relational Learning

Gobel, F., & Jagers, A. (1974). Random walks graphs. Stochastic processes
applications, 2 (4), 311336.
Godbole, N., Srinivasaiah, M., & Skiena, S. (2007). Large-scale sentiment analysis news
blogs. Proceedings International Conference Weblogs Social
Media.
Golub, G., & Reinsch, C. (1970). Singular value decomposition least squares solutions.
Numerische Mathematik, 14 (5), 403420.
Green, J. (1972). Latitudinal variation associations planktonic Rotifera. Journal
zoology, 167 (1), 3139.
Gruber, A., Rosen-Zvi, M., & Weiss, Y. (2008). Latent topic models hypertext.
Proceedings 24th Conference Uncertainty Artificial Intelligence, pp. 230
239.
Guyon, I., & Elisseeff, A. (2003). introduction variable feature selection. Journal
Machine Learning Research, 3, 11571182.
Gyongyi, Z., Garcia-Molina, H., & Pedersen, J. (2004). Combating web spam trustrank.
Proceedings VLDB, pp. 576587.
Hannan, E., & Quinn, B. (1979). determination order autoregression.
Journal Royal Statistical Society. Series B (Methodological), 41, 190195.
Harshman, R. (1970). Foundations PARAFAC procedure: Models conditions
explanatory multi-modal factor analysis. UCLA Working Papers Phonetics,
16 (1), 84.
Hartigan, J., & Wong, M. (1979). k-means clustering algorithm. Journal Royal
Statistical Society. Series C, Applied statistics, 28, 100108.
Hasan, M. A., Chaoji, V., Salem, S., & Zaki, M. (2006). Link prediction using supervised
learning. Proceedings SDM Workshop Link Analysis, Counterterrorism
Security.
Haveliwala, T. (2003). Topic-sensitive pagerank: context-sensitive ranking algorithm
web search. IEEE transactions knowledge data engineering, 15, 784796.
Hay, M., Miklau, G., Jensen, D., Towsley, D., & Weis, P. (2008). Resisting structural reidentification anonymized social networks. Proceedings VLDB, pp. 102114.
He, D., & Parker, D. (2010). Topic Dynamics: alternative model Bursts Streams
Topics. Proceeding 16th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 443452.
Henderson, K., Gallagher, B., Li, L., Akoglu, L., Eliassi-Rad, T., Tong, H., & Faloutsos,
C. (2011). Know: Graph Mining Using Recursive Structural Features.
Proceedings 17th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 110.
Hill, S., Agarwal, D., Bell, R., & Volinsky, C. (2006). Building effective representation
dynamic networks. Journal Computational Graphical Statistics, 15 (3),
584608.
429

fiRossi, McDowell, Aha, & Neville

Hoff, P., Raftery, A., & Handcock, M. (2002). Latent space approaches social network
analysis. Journal American Statistical Association, 97 (460), 10901098.
Hofmann, T. (1999). Probabilistic latent semantic analysis. Proceedings Uncertainty
Artificial Intelligence, pp. 289296.
Huh, S., & Fienberg, S. (2010). Discriminative Topic Modeling based Manifold Learning.
Proceeding 16th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 653661.
Huynh, T., & Mooney, R. (2008). Discriminative structure parameter learning
markov logic networks. Proceedings 25th International Conference Machine Learning.
Inokuchi, A., Washio, T., & Motoda, H. (2000). apriori-based algorithm mining
frequent substructures graph data. Principles Data Mining Knowledge
Discovery, pp. 1323.
Jaccard, P. (1901). Etude comparative de la distribution florale dans une portion des Alpes
et du Jura. Impr. Corbaz.
Jackson, M. (2008). Social economic networks. Princeton Univ Press.
Jain, A., & Zongker, D. (1997). Feature selection: Evaluation, application, small sample performance. IEEE Transactions Pattern Analysis Machine Intelligence,
19 (2), 153158.
Jeh, G., & Widom, J. (2002). SimRank: measure structural-context similarity.
Proceedings eighth ACM SIGKDD International Conference Knowledge Discovery Data Mining, pp. 538543.
Jensen, D., & Neville, J. (2002). Linkage autocorrelation cause feature selection bias
relational learning. Proceedings 19th International Conference Machine
Learning, pp. 259266.
Jensen, D., Neville, J., & Gallagher, B. (2004). collective inference improves relational
classification. Proceedings 10th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 593598.
Jensen, D., Neville, J., & Hay, M. (2003). Avoiding bias aggregating relational data
degree disparity. Proceedings 20th International Conference Machine
Learning, pp. 274281.
Jensen, D., Fast, A., Taylor, B., & Maier, M. (2008). Automatic identification quasiexperimental designs discovering causal knowledge. Proceeding 14th
ACM SIGKDD International Conference Knowledge Discovery Data Mining,
pp. 372380.
Jeong, H., Mason, S., Barabasi, A., & Oltvai, Z. (2001). Lethality centrality protein
networks. Nature, 411 (6833), 4142.
Jeong, H., Tombor, B., Albert, R., Oltvai, Z., & Barabasi, A. (2000). large-scale
organization metabolic networks. Nature, 407 (6804), 651654.
430

fiTransforming Graph Data Statistical Relational Learning

Joachims, T. (1998). Text categorization support vector machines: Learning many
relevant features. Proceedings European Conference Machine Learning,
pp. 137142.
Johnson, S. (1967). Hierarchical clustering schemes. Psychometrika, 32 (3), 241254.
Kahanda, I., & Neville, J. (2009). Using transactional information predict link strength
online social networks. Proceedings 4th International Conference Weblogs
Social Media, pp. 106113.
Kamvar, S., Klein, D., & Manning, C. (2003). Spectral learning. Proceedings 18th
International Joint Conference Artificial Intelligence, pp. 561566.
Kashima, H., & Abe, N. (2006). parameterized probabilistic model network evolution
supervised link prediction. Proceedings IEEE International Conference
Data Mining, pp. 340349.
Katz, L. (1953). new status index derived sociometric analysis. Psychometrika,
18 (1), 3943.
Kavurucu, Y., Senkul, P., & Toroslu, I. (2008). Aggregation confidence-based concept discovery multi-relational data mining. Proceedings IADIS European Conference
Data Mining, pp. 4352.
Kersting, K., & De Raedt, L. (2002). Basic principles learning Bayesian logic programs.
Tech. rep. 174, Institute Computer Science, University Freiburg.
Kersting, K., De Raedt, L., Gutmann, B., Karwath, A., & Landwehr, N. (2008). Relational
sequence learning. Probabilistic inductive logic programming, 4911, 2855.
Khosravi, H., Tong Man, O., Xu, X., & Bina, B. (2010). Structure learning markov logic
networks many descriptive attributes. Proceedings 24th Conference
Artificial Intelligence, pp. 487493.
Khot, T., Natarajan, S., Kersting, K., & Shavlik, J. (2011). Learning markov logic networks via functional gradient boosting. Data Mining (ICDM), 2011 IEEE 11th
International Conference on, pp. 320329. IEEE.
Kim, M., & Leskovec, J. (2011). network completion problem: Inferring missing nodes
edges networks. Proceedings SIAM International Conference Data
Mining.
Kleczkowski, A., & Grenfell, B. (1999). Mean-field-type equations spread epidemics:
small worldmodel. Physica A: Statistical Mechanics Applications, 274 (12), 355360.
Kleinberg, J. (1999). Authoritative sources hyperlinked environment. Journal
ACM, 46 (5), 604632.
Knobbe, A., Siebes, A., & Marseille, B. (2002). Involving aggregate functions multirelational search. Principles Data Mining Knowledge Discovery, pp. 145
168.
Kohavi, R., & John, G. (1997). Wrappers feature subset selection. Artificial intelligence,
97 (1-2), 273324.
431

fiRossi, McDowell, Aha, & Neville

Kohonen, T. (1990). self-organizing map. Proceedings IEEE, 78 (9), 14641480.
Kok, S., & Domingos, P. (2005). Learning structure Markov logic networks.
Proceedings 22nd International Conference Machine Learning, pp. 441448.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. Proceedings 24th
International Conference Machine Learning, pp. 433440.
Kok, S., & Domingos, P. (2008). Extracting semantic networks text via relational clustering. Proceedings European Conference Machine Learning Principles
Practice Knowledge Discovery Databases, pp. 624639.
Kok, S., & Domingos, P. (2009). Learning markov logic network structure via hypergraph
lifting. Proceedings 26th International Conference Machine Learning, pp.
505512.
Kok, S., & Domingos, P. (2010). Learning markov logic networks using structural motifs.
Proceedings 27th International Conference Machine Learning.
Kolda, T. G., Bader, B. W., & Kenny, J. P. (2005). Higher-order web link analysis using
multilinear algebra. Proceedings IEEE International Conference Data
Mining, pp. 242249.
Kolda, T., & Bader, B. (2006). TopHITS model higher-order web link analysis.
Proceedings SIAM Data Mining Conference Workshop Link Analysis,
Counterterrorism Security, pp. 2629.
Koller, D., & Sahami, M. (1996). Toward optimal feature selection. Proceedings
13th International Conference Machine Learning, pp. 284292.
Kondor, R., & Lafferty, J. (2002). Diffusion kernels graphs discrete structures.
Proceedings 19th International Conference Machine Learning, pp. 315
322.
Kong, X., & Yu, P. (2010). Semi-supervised feature selection graph classification. Proceeding 16th ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 793802.
Koren, Y., North, S., & Volinsky, C. (2007). Measuring extracting proximity graphs
networks. Transactions Knowledge Discovery Data (TKDD), 1 (3), 12:1
12:30.
Kosala, R., & Blockeel, H. (2000). Web Mining Research: Survey. ACM SIGKDD
Explorations Newsletter, 2 (1), 115.
Kossinets, G., Kleinberg, J., & Watts, D. (2008). structure information pathways
social communication network. Proceeding 14th ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 435443.
Kou, Z., & Cohen, W. (2007). Stacked graphical models efficient inference markov
random fields. Proceedings 7th SIAM International Conference Data
Mining, pp. 533538.
Krebs, V. (2002). Mapping networks terrorist cells. Connections, 24 (3), 4352.
432

fiTransforming Graph Data Statistical Relational Learning

Krogel, M., & Wrobel, S. (2001). Transformation-based learning using multirelational aggregation. Inductive logic programming, 2157, 142155.
Kubica, J., Moore, A., Schneider, J., & Yang, Y. (2002). Stochastic link group detection.
Proceedings 18th AAAI Conference Artificial Intelligence, pp. 798806.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models segmenting labeling sequence data. Proceedings 18th
International Conference Machine Learning, pp. 282289.
Landwehr, N., Kersting, K., & De Raedt, L. (2005). nFOIL: Integrating nave bayes
FOIL. Proceedings 20th AAAI Conference Artificial Intelligence, pp.
275282.
Landwehr, N., Passerini, A., De Raedt, L., & Frasconi, P. (2010). Fast learning relational
kernels. Machine learning, 78 (3), 305342.
Langville, A., & Meyer, C. (2005). Survey Eigenvector Methods Web Information
Retrieval. SIAM Review, 47 (1), 135161.
Lassez, J.-L., Rossi, R., & Jeev, K. (2008). Ranking links web: Search surf
engines. IEA/AIE, pp. 199208.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th annual meeting Association Computational Linguistics Computational Linguistics,
pp. 2532.
Leicht, E., Holme, P., & Newman, M. (2006). Vertex similarity networks. Physical Review
E, 73 (2), 026120.
Leiva, H. A., Gadia, S., & Dobbs, D. (2002). Mrdtl: multi-relational decision tree learning
algorithm. Proceedings 13th International Conference Inductive Logic
Programming, pp. 3856.
Lempel, R., & Moran, S. (2000). stochastic approach link-structure analysis
(SALSA) TKC effect. Computer Networks, 33 (1-6), 387401.
Leskovec, J., Chakrabarti, D., Kleinberg, J., Faloutsos, C., & Ghahramani, Z. (2010). Kronecker graphs: approach modeling networks. Journal Machine Learning
Research, 11, 9851042.
Leskovec, J., Huttenlocher, D., & Kleinberg, J. (2010). Predicting positive negative
links online social networks. Proceedings 19th International World Wide
Web Conference, pp. 641650.
Letovsky, S., & Kasif, S. (2003). Predicting protein function protein/protein interaction
data: probabilistic approach. Bioinformatics, 19 (Suppl 1), i197.
Li, W., & McCallum, A. (2006). Pachinko allocation: DAG-structured mixture models
topic correlations. Proceedings 23rd International Conference Machine
Learning, pp. 577584.
Liben-Nowell, D., & Kleinberg, J. (2007). link-prediction problem social networks.
Journal American Society Information Science Technology, 58 (7), 1019
1031.
433

fiRossi, McDowell, Aha, & Neville

Lichtenwalter, R., Lussier, J., & Chawla, N. (2010). New Perspectives Methods Link
Prediction. Proceeding 16th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 243252.
Lim, T., Loh, W., & Shih, Y. (2000). comparison prediction accuracy, complexity,
training time thirty-three old new classification algorithms. Machine Learning,
40 (3), 203228.
Lin, D. (1998). information-theoretic definition similarity. Proceedings 15th
International Conference Machine Learning, pp. 296304.
Lin, F., & Cohen, W. W. (2010). Semi-supervised classification network data using
labels. Proceedings International Conference Advances Social
Network Analysis Mining.
Lippi, M., Jaeger, M., Frasconi, P., & Passerini, A. (2009). Relational information gain.
Machine Learning, 83 (2), 121.
Liu, K., & Terzi, E. (2008). Towards identity anonymization graphs. Proceedings
ACM SIGMOD International Conference Management Data, pp. 93106.
Liu, W., & Lu, L. (2010). Link prediction based local random walk. Europhysics Letters,
89, 58007.
Liu, Y., Niculescu-Mizil, A., & Gryc, W. (2009). Topic-link LDA: joint models topic
author community. Proceedings 26th International Conference Machine
Learning, pp. 665672.
Long, B., Zhang, Z., & Yu, P. (2007). probabilistic framework relational clustering.
Proceedings 13th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 470479.
Long, B., Zhang, Z., Wu, X., & Yu, P. S. (2006). Spectral clustering multi-type relational
data. Proceedings 23rd International Conference Machine Learning, pp.
585592.
Lu, C., Hu, X., Chen, X., & ran Park, J. (2010). Topic-Perspective Model Social
Tagging Systems. Proceeding 16th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 683692.
Lu, Q., & Getoor, L. (2003). Link-based classification. Proceedings 20th International Conference Machine Learning, pp. 496503.
Macskassy, S., & Provost, F. (2003). simple relational classifier. Proceedings
SIGKDD 2nd Workshop Multi-Relational Data Mining, pp. 6476.
Macskassy, S., & Provost, F. (2007). Classification networked data: toolkit
univariate case study. Journal Machine Learning Research, 8, 935983.
Macskassy, S. A. (2007). Improving learning networked data combining explicit
mined links. Proceedings 22nd AAAI Conference Artificial Intelligence,
pp. 590595.
Maes, F., Peters, S., Denoyer, L., & Gallinari, P. (2009). Simulated iterative classification new learning procedure graph labeling. Machine Learning Knowledge
Discovery Databases, 5782, 4762.
434

fiTransforming Graph Data Statistical Relational Learning

Mallows, C. (1973). comments Cp . Technometrics, 42 (1), 8794.
Maslov, S., & Sneppen, K. (2002). Specificity stability topology protein networks.
Science, 296 (5569), 910913.
May, R., & Lloyd, A. (2001). Infection dynamics scale-free networks. Physical Review
E, 64 (6), 66112.
McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic role discovery
social networks experiments enron academic email. Journal Artificial
Intelligence Research, pp. 249272.
McCallum, A., Wang, X., & Mohanty, N. (2007). Joint group topic discovery
relations text. Statistical Network Analysis: Models, Issues New Directions,
Lecture Notes Computer Science 4503, pp. 2844.
McDowell, L., Gupta, K., & Aha, D. (2009). Cautious collective classification. Journal
Machine Learning Research, 10, 27772836.
McDowell, L., Gupta, K., & Aha, D. (2007). Cautious inference collective classification.
Proceedings 22nd AAAI Conference Artificial Intelligence.
McDowell, L., Gupta, K., & Aha, D. (2010). Meta-Prediction Collective Classification.
Proceedings 23rd International FLAIRS Conference.
McGovern, A., Friedland, L., Hay, M., Gallagher, B., Fast, A., Neville, J., & Jensen, D.
(2003). Exploiting relational structure understand publication patterns highenergy physics. SIGKDD Explorations, 5 (2), 165172.
McGovern, A., Collier, N., Matthew Gagne, I., Brown, D., & Rodger, A. (2008). Spatiotemporal Relational Probability Trees: Introduction. Eighth IEEE International
Conference Data Mining, ICDM., pp. 935940.
Menon, A., & Elkan, C. (2011). Link prediction via matrix factorization. Proceedings
European Conference Machine Learning Principles Practice Knowledge Discovery Databases, pp. 437452.
Menon, A., & Elkan, C. (2010). Predicting labels dyadic data. Data Mining
Knowledge Discovery, 21 (2), 327343.
Michie, D., Spiegelhalter, D., Taylor, C., & Campbell, J. (1994). Machine Learning, Neural
Statistical Classification. Ellis Horwood Limited.
Mihalkova, L., & Mooney, R. (2007). Bottom-up learning Markov logic network structure.
Proceedings 24th International Conference Machine Learning, pp. 625
632.
Miller, K., Griffiths, T., & Jordan, M. (2009). Nonparametric latent feature models link
prediction. Advances Neural Information Processing Systems (NIPS), 10, 1276
1284.
Minsky, M. (1974). framework representing knowledge. Tech. rep., Massachusetts
Institute Technology, Cambridge, MA, USA.
Mislove, A., Marcon, M., Gummadi, K., Druschel, P., & Bhattacharjee, B. (2007). Measurement analysis online social networks. Proceedings 7th ACM SIGCOMM
Conference Internet measurement, pp. 2942.
435

fiRossi, McDowell, Aha, & Neville

Moore, C., & Newman, M. (2000). Epidemics percolation small-world networks.
Physical Review E, 61 (5), 56785682.
Moreno, S., & Neville, J. (2009). investigation distributional characteristics
generative graph models. Proceedings 1st Workshop Information
Networks.
Nallapati, R., Ahmed, A., Xing, E., & Cohen, W. (2008). Joint latent topic models text
citations. Proceeding 14th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 542550.
Namata, G., Kok, S., & Getoor, L. (2011). Collective graph identification. Proceedings
17th ACM SIGKDD International Conference Knowledge Discovery Data
Mining, pp. 8795. ACM.
Narayanan, A., & Shmatikov, V. (2009). De-anonymizing social networks. Proceedings
30th IEEE Symposium Security Privacy, pp. 173187.
Natarajan, S., Khot, T., Kersting, K., Gutmann, B., & Shavlik, J. (2012). Gradient-based
boosting statistical relational learning: relational dependency network case.
Machine Learning, 86, 2556.
Neville, J., Adler, M., & Jensen, D. (2004). Spectral clustering links attributes.
Tech. rep. 04-42, Dept Computer Science, University Massachusetts Amherst.
Neville, J., & Jensen, D. (2000). Iterative classification relational data. Proceedings
Workshop SRL, 17th AAAI Conference Artificial Intelligence, pp. 4249.
Neville, J., & Jensen, D. (2005). Leveraging relational autocorrelation latent group
models. Proceedings 5th IEEE International Conference Data Mining,
pp. 322329.
Neville, J., & Jensen, D. (2007). Relational dependency networks. Journal Machine
Learning Research, 8, 653692.
Neville, J., Jensen, D., Friedland, L., & Hay, M. (2003). Learning relational probability
trees. Proceedings 9th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 625630.
Neville, J., Jensen, D., & Gallagher, B. (2003). Simple estimators relational Bayesian
classifers. Proceedings 3rd IEEE International Conference Data Mining,
pp. 609612.
Neville, J., Simsek, O., Jensen, D., Komoroske, J., Palmer, K., & Goldberg, H. (2005). Using
relational knowledge discovery prevent securities fraud. Proceedings 11th
ACM SIGKDD International Conference Knowledge Discovery Data Mining,
pp. 449458.
Newman, M. (2010). Networks: Introduction. Oxford Univ Press.
Newman, M. E. J. (2003). structure function complex networks. SIAM Review,
45, 167256.
Newman, M. (2001a). Clustering preferential attachment growing networks. Physical
Review E, 64 (2), 025102.
436

fiTransforming Graph Data Statistical Relational Learning

Newman, M. (2001b). structure scientific collaboration networks. Proceedings
National Academy Sciences, 98 (2), 404409.
Newman, M., & Girvan, M. (2004). Finding evaluating community structure networks. Physical review E, 69 (2), 26113.
Ng, A., Jordan, M., & Weiss, Y. (2001). spectral clustering: Analysis algorithm.
Advances Neural Information Processing Systems, pp. 849856.
Nie, L., Davison, B., & Qi, X. (2006). Topical link analysis web search. Proceedings
29th International ACM SIGIR Conference Research Development
Information Retrieval, p. 98.
Nowicki, K., & Snijders, T. (2001). Estimation prediction stochastic blockstructures.
Journal American Statistical Association, 96, 10771087.
Oktay, H., Taylor, B., & Jensen, D. (2010). Causal Discovery Social Media Using QuasiExperimental Designs. Proceedings ACM SIGKDD 1st Workshop Social
Media Analytics (SOMA-KDD).
Onnela, J.-P., Saramaki, J., Hyvonen, J., Szabo, G., Lazer, D., Kaski, K., Kertesz, J., &
Barabasi, A.-L. (2007). Structure tie strengths mobile communication networks.
Proceedings National Academy Sciences, 104, 73327336.
Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). pagerank citation ranking:
Bringing order web. Tech. rep., Technical Report, Stanford Digital Library
Technologies Project.
Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations Trends
Information Retrieval, 2 (1-2), 1135.
Pastor-Satorras, R., & Vespignani, A. (2001). Epidemic spreading scale-free networks.
Physical Review Letters, 86 (14), 32003203.
Pasula, H., Marthi, B., Milch, B., Russell, S., & Shpitser, I. (2003). Identity uncertainty
citation matching. NIPS. MIT Press.
Perlich, C., & Provost, F. (2003). Aggregation-based feature invention relational concept classes. Proceedings 9th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 167176.
Perlich, C., & Provost, F. (2006). Acora: Distribution-based aggregation relational
learning identifier attributes. Machine Learning, 62, 65105.
Poole, D. (2003). First-order probabilistic inference. International Joint Conference
Artificial Intelligence, pp. 985991.
Popescul, A., Popescul, R., & Ungar, L. H. (2003a). Statistical relational learning
link prediction. Proceedings Workshop Learning Statistical Models
Relational Data IJCAI.
Popescul, A., Popescul, R., & Ungar, L. H. (2003b). Structural logistic regression link
analysis. Proceedings Second International Workshop Multi-Relational
Data Mining, pp. 92106.
437

fiRossi, McDowell, Aha, & Neville

Popescul, A., & Ungar, L. H. (2004). Cluster-based concept invention statistical relational learning. Proceedings 10th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 665670.
Price, D. (1976). general theory bibliometric cumulative advantage processes.
Journal American Society Information Science, 27 (5), 292306.
Pudil, P., Novovicova, J., & Kittler, J. (1994). Floating search methods feature selection.
Pattern recognition letters, 15 (11), 11191125.
Radicchi, F., Castellano, C., Cecconi, F., Loreto, V., & Parisi, D. (2004). Defining
identifying communities networks. Proceedings National Academy Sciences,
101 (9), 26582663.
Rattigan, M. J., & Jensen, D. (2005). case anomalous link discovery. SIGKDD
Explorations Newsletter, 7 (2), 4147.
Ravasz, E., Somera, A., Mongru, D., Oltvai, Z., & Barabasi, A. (2002). Hierarchical organization modularity metabolic networks. Science, 297 (5586), 15511555.
Resnick, P., & Varian, H. (1997). Recommender systems. Communications ACM,
40 (3), 5658.
Richardson, M., & Domingos, P. (2002). intelligent surfer: Probabilistic combination
link content information pagerank. Advances Neural Information
Processing Systems, pp. 14411448.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine Learning, 62 (1),
107136.
Riedel, S., & Meza-Ruiz, I. (2008). Collective semantic role labelling markov logic.
Proceedings Twelfth Conference Computational Natural Language Learning,
pp. 193197.
Robins, G., Pattison, P., Kalish, Y., & Lusher, D. (2007). introduction exponential
random graph (p*) models social networks. Social Networks, 29, 173191.
Robins, G., Snijders, T., Wang, P., & Handcock, M. (2006). Recent developments exponential random graph (p*) models social networks. Social Networks, 29, 192215.
Rossi, R., Gallagher, B., Neville, J., & Henderson, K. (2012). Dynamic behavioral mixedmembership model large evolving networks. Arxiv preprint arXiv:1205.2056,
pp. 117.
Rossi, R., & Neville, J. (2010). Modeling evolution discussion topics communication improve relational classification. Proceedings ACM SIGKDD 1st
Workshop Social Media Analytics (SOMA-KDD), pp. 110.
Rossi, R., Gallagher, B., Neville, J., & Henderson, K. (2012). Role-Dynamics: Fast Mining
Large Dynamic Networks. LSNA-WWW, pp. 19.
Rossi, R., & Neville, J. (2012). Time-evolving relational classification ensemble methods.
Proceedings 16th Pacific-Asia Conference Knowledge Discovery Data
Mining, pp. 112.
438

fiTransforming Graph Data Statistical Relational Learning

Roth, M., Ben-David, A., Deutscher, D., Flysher, G., Horn, I., Leichtberg, A., Leiser, N.,
Merom, R., & Mattias, Y. (2010). Suggesting Friends Using Implicit Social Graph.
Proceeding 16th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 233242.
Russell, S. J., & Norvig, P. (2009). Artificial Intelligence: Modern Approach (3rd International Edition edition). Prentice Hall.
Sabidussi, G. (1966). centrality index graph. Psychometrika, 31 (4), 581603.
Salton, G., & McGill, M. (1983). Introduction modern information retrieval, Vol. 1.
McGraw-Hill New York.
Sarkar, P., & Moore, A. (2005). Dynamic Social Network Analysis using Latent Space
Models. SIGKDD Explorations Newsletter, 7 (2), 3140.
Sarukkai, R. R. (2000). Link prediction path analysis using markov chains. Proceedings 9th International World Wide Web Conference Computer Networks:
International Journal Computer Telecommunications Networking, pp. 377386.
Sarwar, B., Karypis, G., Konstan, J., & Riedl, J. (2000). Application dimensionality
reduction recommender systema case study. ACM WebKDD 2000 Web Mining
E-Commerce Workshop.
Schulte, O. (2011). tractable pseudo-likelihood function bayes nets applied relational
data. SDM, pp. 462473.
Schwarz, G. (1978). Estimating dimension model. annals statistics, 6 (2),
461464.
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., & Eliassi-Rad, T. (2008). Collective classification network data. AI Magazine, 29 (3), 93.
Shao, J. (1996). Bootstrap model selection. Journal American Statistical Association,
91 (434), 655665.
Shapiro, E. (1982). Algorithmic Program Debugging. ACM Distinguished Dissertation..
Sharan, U., & Neville, J. (2008). Temporal-relational classifiers prediction evolving
domains. Proceedings 8th IEEE International Conference Data Mining,
pp. 540549.
Shi, J., & Malik, J. (2000). Normalized cuts image segmentation. IEEE Transactions
Pattern Analysis Machine Intelligence, 22 (8), 888905.
Shi, X., Li, Y., & Yu, P. (2011). Collective prediction latent graphs. Proceedings
20th ACM Conference Information Knowledge Management, pp. 1127
1136.
Singla, P., & Domingos, P. (2006). Entity resolution markov logic. Proceedings
6th IEEE International Conference Data Mining, pp. 572582.
Srinivasan, A. (1999). aleph manual. Computing Laboratory, Oxford University, 1.
Strehl, A., & Ghosh, J. (2003). Cluster ensemblesa knowledge reuse framework combining multiple partitions. Journal Machine Learning Research, 3, 583617.
439

fiRossi, McDowell, Aha, & Neville

Tang, J., Musolesi, M., Mascolo, C., & Latora, V. (2009). Temporal distance metrics
social network analysis. Proceedings 2nd ACM workshop Online social
networks, pp. 3136.
Tang, J., Musolesi, M., Mascolo, C., Latora, V., & Nicosia, V. (2010). Analysing information
flows key mediators temporal centrality metrics. Proceedings
3rd Workshop Social Network Systems, pp. 16.
Tang, L., & Liu, H. (2009). Relational learning via latent social dimensions. Proceedings
15th ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 817826.
Tang, L., & Liu, H. (2011). Leveraging social media networks classification. Journal
Data Mining Knowledge Discovery, 23, 447478.
Taskar, B., Abbeel, P., & Koller, D. (2002). Discriminative probabilistic models relational
data. Eighteenth Conference Uncertainty Artificial Intelligence, pp. 485492.
Taskar, B., Segal, E., & Koller, D. (2001). Probabilistic classification clustering
relational data. Proceedings 17th International Joint Conference Artificial
Intelligence, pp. 870878.
Taskar, B., Wong, M., Abbeel, P., & Koller, D. (2003). Link prediction relational data.
Advances Neural Information Processing Systems.
Topchy, A., Law, M., Jain, A., & Fred, A. (2004). Analysis consensus partition cluster
ensemble. Proceedings 4th IEEE International Conference Data Mining,
pp. 225232.
Vert, J., & Yamanishi, Y. (2005). Supervised graph inference. Advances Neural Information Processing Systems, 17, 14331440.
Vishwanathan, S., Schraudolph, N., Kondor, R., & Borgwardt, K. (2010). Graph kernels.
Journal Machine Learning Research, 11, 12011242.
von Luxburg, U. (2007). tutorial spectral clustering. Statistics Computing, 17 (4),
395416.
Wagner, A., & Fell, D. (2001). small world inside large metabolic networks. Proceedings
Royal Society London. Series B: Biological Sciences, 268 (1478), 18031810.
Wang, C., Blei, D., & Heckerman, D. (2008). Continuous time dynamic topic models.
Proceedings Uncertainty Artificial Intelligence.
Wang, X., & McCallum, A. (2006). Topics time: non-Markov continuous-time model
topical trends. Proceedings 12th ACM SIGKDD International Conference
Knowledge Discovery Data Mining, pp. 424433.
Wang, Z., & Chan, L. (2010). efficient causal discovery algorithm linear models.
Proceeding 16th ACM SIGKDD International Conference Knowledge
Discovery Data Mining, pp. 11091118.
Wasserman, S., & Faust, K. (1994). Social network analysis: Methods applications.
Cambridge University Press.
440

fiTransforming Graph Data Statistical Relational Learning

Watts, D., & Strogatz, S. (1998). Collective dynamics small-worldnetworks. Nature,
393 (6684), 440442.
White, S., & Smyth, P. (2003). Algorithms estimating relative importance networks.
Proceedings ninth ACM SIGKDD International Conference Knowledge
Discovery Data mining, pp. 266275.
Wu, B., & Davison, B. (2005). Identifying link farm spam pages. Special interest tracks
posters 14th International Conference World Wide Web, pp. 820829.
Xiang, R., Neville, J., & Rogati, M. (2010). Modeling relationship strength online social
networks. Proceedings 19th International World Wide Web Conference, pp.
981990.
Yang, Y., & Pedersen, J. (1997). comparative study feature selection text categorization. Proceedings 14th International Conference Machine Learning,
pp. 412420.
Yin, X., Han, J., Yang, J., & Yu, P. (2006). Crossmine: Efficient classification across multiple
database relations. Transactions Knowledge Data Engineering, 18 (6), 770783.
Zheleva, E., Getoor, L., Golbeck, J., & Kuter, U. (2010). Using friendship ties family
circles link prediction. Advances Social Network Mining Analysis, pp.
97113.
Zheleva, E., & Getoor, L. (2007). Preserving privacy sensitive relationships graph
data. PinKDD, pp. 153171.
Zhou, B., Pei, J., & Luk, W. (2008). brief survey anonymization techniques privacy
preserving publishing social network data. SIGKDD Explorations, 10 (2), 1222.
Zhou, H. (2003). Distance, dissimilarity index, network community structure. Physical
review e, 67 (6), 61901.
Zhou, T., Lu, L., & Zhang, Y. (2009). Predicting missing links via local information.
European Physical Journal B-Condensed Matter Complex Systems, 71 (4), 623
630.
Zhu, S., Yu, K., Chi, Y., & Gong, Y. (2007). Combining content link classification
using matrix factorization. Proceedings 30th Annual International ACM
SIGIR Conference Research Development Information Retrieval, pp. 487
494. ACM.
Zhu, X. (2006). Semi-supervised learning literature survey. Computer Science Tech Reports,
1530, 160.

441


