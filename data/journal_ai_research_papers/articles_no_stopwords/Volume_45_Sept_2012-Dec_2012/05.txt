Journal Artificial Intelligence Research 45 (2012) 685-729

Submitted 08/12; published 12/12

Time Complexity Approximate Heuristics
Multiple-Solution Search Spaces
Hang Dinh

htdinh@iusb.edu

Department Computer & Information Sciences
Indiana University South Bend
1700 Mishawaka Ave. P.O. Box 7111
South Bend, 46634 USA

Hieu Dinh

hieu.dinh@mathworks.com

MathWorks
3 Apple Hill Drive
Natick, 01760-2098 USA

Laurent Michel
Alexander Russell

ldm@engr.uconn.edu
acr@cse.uconn.edu

Department Computer Science & Engineering
University Connecticut
371 Fairfield Way, Unit 2155
Storrs, CT 06269-2155 USA

Abstract


study behavior search algorithm coupled heuristic h satisfying
(1 1 )h h (1 + 2 )h , 1 , 2 [0, 1) small constants h denotes optimal
cost solution. prove rigorous, general upper bound time complexity search
trees depends accuracy heuristic distribution solutions.
upper bound essentially tight worst case; fact, show nearly matching lower bounds
attained even non-adversarially chosen solution sets induced simple stochastic
model. consequence rigorous results effective branching factor search
reduced long 1 + 2 < 1 number near-optimal solutions search tree
large. go provide upper bound search graphs context
establish bound running time determined spectrum graph.
experimentally explore extent rigorous upper bounds predict behavior
natural, combinatorially-rich search spaces. begin applying solve
knapsack problem near-accurate admissible heuristics constructed efficient approximation algorithm problem. additionally apply analysis search partial
Latin square problem, provide quite exact analytic bounds number nearoptimal solutions. results demonstrate dramatic reduction effective branching factor
coupled near-accurate heuristics search spaces suitably sparse solution sets.

1. Introduction
classical search procedure (Hart, Nilson, & Raphael, 1968) method bringing heuristic
information bear natural class search problems. One celebrated features
coupled admissible heuristic function, is, one always returns lower bound
distance solution, guaranteed find optimal solution. worst-case
behavior (even admissible heuristic function) better of, say, breadthfirst search, practice intuition suggest availability accurate heuristic
decrease running time. Indeed, methods computing accurate admissible heuristic functions
various search problems presented literature (see, e.g., Felner, Korf, & Hanan,
2004). article, investigate effect accuracy running time search;
2012 AI Access Foundation. rights reserved.

fiDinh, Dinh, Michel, & Russell

specifically, focus rigorous estimates running time coupled accurate
heuristics.
initial notion accuracy adopt motivated standard framework approximation algorithms: f () hard combinatorial optimization problem (e.g., permanent
matrix, value Euclidean traveling salesman problem, etc.), algorithm efficient -approximation f runs polynomial time (1 )f (x) A(x) (1 + )f (x),
inputs x, f (x) optimal solution cost input x A(x) solution cost
returned algorithm input x. approximation algorithms community developed
efficient approximation algorithms wide swath NP-hard combinatorial optimization problems and, cases, provided dramatic lower bounds asserting various problems cannot
approximated beyond certain thresholds (see Vazirani, 2001; Hochbaum, 1996, surveys
literature). Considering great multiplicity problems successfully addressed
way (including problems believed far outside NP, matrix permanent), natural
study behavior coupled heuristic function possessing properties. Indeed,
interesting cases (e.g., Euclidean travelling salesman, matrix permanent, knapsack), hard
combinatorial problems approximated polynomial time within fixed constant > 0;
cases, polynomial depends constant . remark, also, many celebrated
approximation algorithms provable performance guarantees proceed iterative update methods coupled bounds local change objective value (e.g., basis reduction Lenstra,
Lenstra, & Lovasz, 1981, typical primal-dual methods Vazirani, 2002).
Encouraged possibility utilizing heuristics practice natural question
understanding structural properties heuristics (and search spaces) indeed guarantee
palatable performance part , study behavior provided heuristic
function -approximation cost cheapest path solution. certain natural
situations arise approximation quality asymmetric (i.e., case admissible heuristic),
slightly refine notion accuracy distinguishing multiplicative factors two sides
approximation: say heuristic h (1 , 2 )-approximation actual cost
function h , simply (1 , 2 )-approximate, (1 1 )h h (1 + 2 )h . particular, admissible
heuristics -approximation (, 0)-approximate. call heuristic -accurate
(1 , 2 )-approximate = 1 + 2 . detailed description appears Section 2.1.
1.1 Sketch Results
initially model search space infinite b-ary tree distinguished root. problem
instance determined set nodes treethe solutions problem. cost
associated solution simply depth. search procedure equipped (i.)
oracle which, given node n, determines n S, (ii.) heuristic function h, assigns
node n tree estimate actual length h (n) shortest (descending) path
solution. Let solution set first (and hence optimal) solution appears
depth d. establish family upper bounds number nodes expanded : h
(1 , 2 )-approximation h , finds solution cost worse (1 + 2 )d expands
2b(1 +2 )d + dN1 +2 nodes, N denotes number solutions depth less
(1 + )d. See Lemma 3.1 stronger results. emphasize bound applies
solution space generalized search models non-uniform branching factors
non-uniform edge costs (see Section 5).
go show upper bound essentially tight; fact, show bound
nearly achieved even non-adversarially determined solution spaces selected according simple
stochastic rule (see Theorems 3.1 4.1.). remark bounds running time fall
rapidly accuracy heuristics increases, long number near-optimal solutions
large (although may grow exponentially). instance, effective branching factor
guided admissible -accurate heuristic reduced b N = O(bd ). However,
686

fiThe Time Complexity Approximate Heuristics

worst cases, occur search space overwhelming number near-optimal
solutions, still expand almost many nodes brute-force does, regardless heuristic
accuracy. Likewise, strong guarantees < 1 are, general, necessary effect appreciable
changes average branching factor. discussed Theorem 4.2.
establishing bounds tree-based search model, examine time complexity
graph unrolling graph equivalent tree bounding number
near-optimal solutions tree lift solution original graph. appears
Section 6. Using spectral graph theory, show number N lifted solutions
tree corresponding b-regular graph G O((1+)d ), assuming optimal solution depth
O(logb |G|) number solutions G constant, second largest eigenvalue (in
absolute value) adjacency matrix G. particular,for almost b-regular graphs
b grow size graphs, 2 b, yields effective branching
factor search graphs roughly 8b(1+)/2 heuristic -accurate.
experimentally evaluate heuristics.
Experimental Results Relationship Practice. course, upper
bounds interesting reflect behavior search problems practice. bounds
guarantee, general, E, number nodes expanded -accurate heuristic,
satisfies
E 2bd + dN .
plausible condition N bd , simply E cbd node expansions
constant c depend (c may depend k and/or properties search
space). suggests hypothesis hard combinatorial problems suitably sparse
near-optimal solutions,
E cbd

or, equivalently,

log E log c + log b .

(1)

particular, suggests linear dependence log E .
explore hypothesis, conducted battery experiments natural search-tree
presentation well-studied Knapsack Problem. obtain admissible -accurate heuristic applying Fully Polynomial Time Approximation Scheme (FPTAS) problem due
work Ibarra Kim (1975) (see Vazirani, 2001, p. 70), provides us
convenient method varying without changing parameters search. remark
natural search space problem quite irregular edge-weighted directed graph
avoid reopening node. Thus, search space equivalent one spanning
subtrees terms behaviors. order focus computationally nontrivial examples,
generate Knapsack instances distributions empirically hard best known exact
algorithms (Pisinger, 2005). results experiments yield remarkably linear behavior (of
log E function ) quite wide window values: indeed, tests yield R2 correlation
coefficients (of least-square linear regression model) excess 90% range (.5, 1)
Knapsack instances. See Section 7.1 details.
experimental results discussed Knapsack problem support linear
scaling (1), several actual parameters search unknown: example, cannot rule
possibility approximation algorithm, asked produce -approximation,
fact produce significantly better approximation. seems far-fetched,
behavior could provide spurious evidence linear scaling. explore hypothesis
detail, additionally explore artificial search space partial Latin square completion
(PLS) problem provide precise control (and, fact, N ). PLS problem
featured number benchmarks local search complete search methods. Roughly,
problem finding assignment values empty cells partially filled n n table
row column completed table permutation set {1, . . . , n}.
formulation problem, search space 2n-regular graph, thus brute-force branching
687

fiDinh, Dinh, Michel, & Russell

factor 2n. search space, controlling N , prove asymptotic upper bound

(1 + ) (1 + 1/) n effective branching factor coupled -accurate heuristic.
experimentally evaluate effective branching factor admissible -accurate
heuristic (1)h , expands nodes admissible -accurate heuristic
strictly larger (1 )h .
remark PLS problem well-studied natural, invent specific search
space structure problem allows us analytically control number near-optimal
solutions. Unlike Knapsack problem, construct efficient admissible -accurate
heuristic every fixed thanks given FPTAS, known approximation algorithms PLS
problem much weakerthey provide approximations specific constants (1/e). avoid
hurdle, construct instances PLS known solution, extract heuristics
(1 )h . Despite planted solutions contrived heuristics, infrastructure provides
example combinatorially rich search space known solution multiplicity heuristic
known quality, provides means experimentally measuring relationship
heuristic accuracy running time. empirical data results remarkable agreement
theoretical upper bounds. subtly, empirically analyzing linear dependence log E
, see effective branching factor using heuristic (1 )h given PLS
search space roughly (2n)0.8 ; see Section 7.2.
far aware, first experimental results explore relationship
E. Understanding heuristic accuracy solution space structure general (and
ensuing bounds running time) problems heuristics practical interest remains
intriguing open problem. remark problems (n2 1)-puzzle,
extensively used test cases , seems difficult find heuristics accuracy sufficient
significantly reduce average branching factor. best rigorous algorithms give rather
large constant guarantees (Ratner & Warmuth, 1990; Parberry, 1995): particular, Parberry (1995)
shows one quickly compute solutions (and hence approximate heuristics)
factor 19 worse optimal; situation somewhat better random instances,
establishes 7.5-factor. See Demaines (2001) work general discussion.
Observe search algorithm privy heuristic information requires (bd ) running
time, general, find solution. High probability statements kind made
solution space selected sufficiently rich family. pessimistic lower bounds exist even
situations search space highly structured (Aaronson, 2004). results suggest
accurate heuristic information dramatic impact search, even face substantial
solution multiplicity.
article expands conference article (Dinh, Russell, & Su, 2007) complexity
-approximate heuristic function studied trees. article, generalize
asymmetric approximation, develop analogous bounds general search spaces, establishing
connection algebraic graph theory, report battery supporting experimental results.
1.2 Motivation Related Work
algorithm subject enormous body literature, often investigating
behavior relation specific heuristic search problem combination, (e.g., Zahavi, Felner,
Schaeffer, & Sturtevant, 2007; Sen, Bagchi, & Zhang, 2004; Korf & Reid, 1998; Korf, Reid, &
Edelkamp, 2001; Helmert & Roger, 2008). space complexity (Korf, 1985) time complexity
addressed various levels abstraction. Abstract formulations, involving accuracy
guarantees consider, studied, tree models search
space possesses single solution. single solution framework, Gaschnig (1979) given
exponential lower bounds (bd ) time complexity admissible -accurate heuristics,
def
b = b/(2) b (see Pearl, 1984, p. 180), Pohl (1977) studied restrictive
(additive) approximation guarantees h result linear time complexity. Average-case
688

fiThe Time Complexity Approximate Heuristics

analysis based probabilistic accuracy heuristics given single-solution
search spaces (Huyn, Dechter, & Pearl, 1980). previous analysis suggested effect
heuristic functions would reduce effective branching factor search, consistent
results applied single-solution model (the special case N = 1 > 0).
single solution model, however, appears inappropriate abstraction search
problems featuring multiple solutions, recognized . . . presence multiple
solutions may significantly deteriorate ability benefit improved precision. (Pearl, 1984,
p. 192) (emphasis added).
problem understanding time complexity terms structural properties h
multiple-solution spaces studied Korf Reid (1998), Korf et al. (2001), Korf
(2000), using estimate based distribution h() values. particular, studied
abstract search space given b-ary tree concluded effect heuristic function
reduce effective depth search rather effective branching factor (Korf & Reid,
1998; Korf et al., 2001). case accurate heuristics controlled solution multiplicity,
conclusion directly contradicts findings, indicate dramatic reduction effective branching
factor cases. explain discrepancy, observe analysis relies equilibrium assumption fails accurate heuristics (in fact, fails even much weaker heuristic
guarantees, h(v) h (v) small > 0). basic structure argument, however,
naturally adapted case accurate heuristics, case yields reduction
effective branching factor. give detailed discussion Section 8.
follow-up Korf Reid (1998), Korf et al. (2001), Korfs (2000) work, Edelkamp
(2001) examined (indeed, IDA ) undirected graphs, relying equilibrium assumption.
Edelkamps new technique use graph spectrum estimate number n(`) nodes
certain depth ` brute-force search tree (same cover tree). However, unlike spectral
analysis, original search graph G, Edelkamp analyzed spectrum related
equivalence graph, quite different structural properties. Specifically, Edelkamp found
asymptotic branching factor, defined ratio n(`) /n(`1) large `, equals largest
eigenvalue adjacency matrix equivalence graph certain Puzzle problems. compare,
spectral analysis depends second largest eigenvalue adjacency matrix AG
original search graph G, largest eigenvalue AG always equals branching factor,
assuming G regular.
Additionally, analyses Korf Reid (1998), Korf et al. (2001), Korf (2000) (and
therefore, Edelkamp, 2001) focus particular subclass admissible heuristics, called consistent
heuristics. remark heuristics used experiments Knapsack problem
admissible likely inconsistent. Zhang, Sturtevant, Holte, Schaeffer, Felner (2009) Zahavi
et al. (2007) discuss usages inconsistent heuristics practice.
work explores worst-case average-case time complexity search
trees graphs multiple solutions coupled heuristics possessing accuracy
guarantees. make assumptions regarding consistency admissibility heuristics, though
several results naturally specialized case. addition studying effect
heuristic accuracy, results shed light sensitivity distribution solutions
combinatorial structure underlying search spaces (e.g., graph eigenvalues,
measure, among things, extent connectedness graphs). far aware,
first rigorous results combining search space structure heuristic accuracy single
framework predicting behavior .

2. Preliminaries
typical search problem defined search graph starting node set goal nodes
called solutions. instance search graph, however, simulated search
cover tree without reducing running time; discussed Section 6.1. Since number
689

fiDinh, Dinh, Michel, & Russell

expansions cover tree graph larger equal original graph,
sufficient upper bound running time search cover tree. justification,
begin considering algorithm search problems rooted tree.
Problem Definition Notations. Let tree representing infinite search space,
let r denote root . convenience, use symbol denote set vertices
tree . Solutions specified nonempty subset nodes . edge
assigned positive number called edge cost. vertex v , let
SubTree(v) denote subtree rooted v,
Path(v) denote path root r v,
g(v) denote total (edge) cost Path(v),
h (v) denote cost least costly path v solution SubTree(v). (We write
h (v) = solution exists.)
objective value search problem h (r), cost cheapest path root r
solution. cost solution value g(s). solution cost equal h (r)
referred optimal.
algorithm best-first search employing additive evaluation function f (v) = g(v) +
h(v), h function heuristically estimates actual cost h . Given heuristic
function h : [0, ], algorithm using h defined search problem tree
described follows:
Algorithm 1 search tree
1. Initialize Open := {r}.
2. Repeat Open empty:
(a) Remove Open node v function f = g + h minimum.
(b) v solution, exit success return v.
(c) Otherwise, expand node v, adding children Open.
3. Exit failure.
known (e.g., Dechter & Pearl, 1985, Lemma 2) time terminates,
always vertex v present Open v lies solution path f (v) ,
min-max value defined follows:


def
= min
max f (u) .
(2)
sS

uPath(s)

fact leads following node expansion conditions:
vertex v expanded (with heuristic h) must f (v) . (cf., Dechter & Pearl,
1985, Thm. 3). say vertex v satisfying f (v) potentially expanded .
vertex v
max

f (u) <

uPath(v)

must expanded (with heuristic h) (cf., Dechter & Pearl, 1985, Thm. 5). particular,
function f monotonically increases along path root r v, node v
must expanded f (v) < .
690

fiThe Time Complexity Approximate Heuristics

value obtained solution path search terminates (Dechter &
Pearl, 1985, Lemma 3), implies upper bound cost solution found
search.
remark h reasonable approximation h along path optimal solution,
immediately provides control . particular:
Proposition 2.1. (See Davis, Bramanti-Gregor, & Wang, 1988) Suppose 1,
h(v) h (v) vertices v lying optimal solution path; h (r).
Proof. Let optimal solution. v Path(s),
f (v) g(v) + h (v) = g(v) + (g(s) g(v)) g(s) .
Hence

max

f (v) g(s) = h (r).

vPath(s)

particular, = h (r) heuristic function satisfies h(v) h (v) v ,
case heuristic function called admissible. observation recovers fact
always finds optimal solution coupled admissible heuristic function (cf., Pearl,
1984, Thm. 2, 3.1). Admissible heuristics possess natural dominance property (Pearl, 1984,
Thm. 7, p. 81): admissible heuristic functions h1 h2 , h1 informed
h2 , i.e., h1 (v) > h2 (v) v \S, using h1 dominates using h2 , i.e., every node
expanded using h1 expanded using h2 .
2.1 Approximate Heuristics
Recall introduction shall focus heuristics providing (1 , 2 )-approximation
actual optimal cost reach solution:
Definition. Let 1 , 2 [0, 1]. heuristic function h called (1 , 2 )-approximate
(1 1 )h (v) h(v) (1 + 2 )h (v)

v .

(1 , 2 )-approximate heuristic simply called -approximate 1 2 .
heuristic function h (1 , 2 )-approximate, shall say h heuristic error 1 + 2 , h
(1 + 2 )-accurate.
see below, two approximation factors control performance search
rather different ways: 1 effects running time , 2 impact
running time quality solution found . Particularly, special case 2 = 0
corresponds admissible heuristics, always finds optimal solution. general,
Proposition 2.1, have:
Fact 1. h (1 , 2 )-approximate, (1 + 2 )h (r).
Hence, solution found using (1 , 2 )-approximate heuristic must cost
(1 + 2 )h (r) thus exceeds optimal cost multiplicative factor equal
2 .
Definition. Let 0. solution cost less (1 + )h (r) called -optimal solution.
Assumptions. simplify analysis now, assume search tree b-ary
every edge unit cost unless otherwise specified. case, cost g(v) simply
depth node v h (v) shortest distance v solution descendant v.
Throughout, parameters b 2 (the branching factor search space) 1 (0, 1], 2 [0, 1]
(the quality approximation provided heuristic function) fixed. rule case
1 = 0 simplicity.
691

fiDinh, Dinh, Michel, & Russell

3. Upper Bounds Running Time Trees
going establish upper bounds running time search tree model.
first show generic upper bound applies solution space. apply
generic upper bound natural stochastic solution space model.
3.1 Generic Upper Bound
mentioned introduction, begin upper bound time complexity
search depending weight distribution solution set, addition heuristics
approximation factors. shall, fact, upper bound number potentially expanded nodes,
clearly upper bound number nodes actually expanded :
Lemma 3.1. Let solution set whose optimal solutions lie depth d. Then, every 0,
number nodes expanded search tree (1 , 2 )-approximate heuristic

2b(1 +2 +1)d + (1 1 )dN1 +2
nodes, N number -optimal solutions.
presence independent parameter offers flexible way apply upper bound
Lemma 3.1. particular, applying Lemma 3.1 = 1 using fact 1 1 1,
arrive upper bound 2b(1 +2 )d + dN1 +2 mentioned introduction. bound works
best when1 N1 +2 = (b(1 +2 )d ). general, N1 +2 = O(b(1 +2 )d ), choose least
1 N1 +2 = O(b(1 +2 )d ). opposite case, N1 +2 = (b(1 +2 +c)d )
positive constant c 1 1 , obtain better bound choosing = 1 c/(1 1 ) < 1, since
N1 +2 dominates terms (b(1 +2 +1)d ) N1 +2 given choice .
Proof Lemma 3.1. Let = h (r) let = 1 + 2 . Consider node v lie
path root -optimal solution, h (v) (1 + )d g(v).
f (v) g(v) + (1 1 )[(1 + )d g(v)] = (1 1 )(1 + )d + 1 g(v) .
Recall node potentially expanded f -value less equal . Since
(1 + 2 )d, node v potentially expanded
(1 1 )(1 + )d + 1 g(v) > (1 + 2 )d .

(3)

Since 1 > 0, inequality (3) equivalent
g(v) > (2 /1 /1 + 1 + )d = (1 + 2 + 1 )d .
words, node depths range

(1 + 2 + 1 )d, (1 + 2 )d
potentially expanded lies path root -optimal solution.
hand, -optimal
solution path, (1 1 )d nodes depths

(1 + 2 + 1 )d, (1 + 2 )d . Pessimistically assuming nodes depth
(1 + 2 + 1 )d potentially expanded addition paths -optimal solutions
P`
yields statement lemma. (Note b 2, i=0 bi 2b` every potentially
expanded node v must depth g(v) f (v) (1 + 2 )d.)
1. Recall asymptotic notations: f (n) = (g(n)) means exist constants c1 , c2 > 0 c1 g(n)
f (n) c2 g(n) sufficiently large n; f (n) = (g(n)) means exists constant c > 0 cg(n) f (n)
sufficiently large n.

692

fiThe Time Complexity Approximate Heuristics

3.2 Upper Bound Natural Search Space Model
actual time complexity depend, course, precise structure h, show
bound essentially tight rich family solution spaces. consider sequence
search problems increasing difficulty, expressed terms depth optimal solution.
Stochastic Solution Space Model. parameter p [0, 1], consider solution set
obtained independently placing node probability p.
setting, random variable written Sp . solutions distributed according Sp ,
observe expected number solutions depth precisely pbd p = bd
optimal solution lies depth constant probability. reason, focus specific
values pd = bd consider solution set Spd > 0. Recall model,
likely optimal solutions lie depth and, generally, see
high probability optimal solutions particular subtree located near depth (with
respect root subtree). make precise below.
Lemma 3.2. Suppose solutions distributed according Spk . node v
> 0,
td
1 2btd Pr[h (v) > t] eb .
Pt
Proof. tree SubTree(v), n = i=0 bi = (bt+1 1)/(b 1) nodes depths less,
Pr[h (v) > t] = (1 bd )n .

1 nbd (1 bd )n exp nbd .
first inequality obtained applying Bernoullis inequality, last one implied
fact 1 x ex x. Observing
bt

bt+1 1
2bt
b1

b 2 completes proof.
Observe Spd model, conditioned likely event optimal solutions appear
depth d, expected number -optimal solutions (bd ). situation, according
Lemma 3.1, expands O(b(1 +2 +1)d ) + O(db(1 +2 )d ) vertices expectation,
0. leading exponential term bound equal
max {(1 + 2 + 1 )d, (1 + 2 )d} ,
minimal = 1. suggests best upper bound inferred
family bounds Lemma 3.1 poly(d)b(1 +2 )d (for Spd ).
discussing average-case time complexity search, record following wellknown Chernoff bound, used control tail bounds analysis later.
Lemma 3.3 (Chernoff bound, Chernoff, 1952). Let Z sum mutually independent indicator
random variables expected value = E [Z]. > 0,


e
Pr[Z > (1 + )] <
.
(1 + )1+
detailed proof found book Motwani Raghavan (1995). several cases
below, know exactly expected value variable wish apply
tail bound Lemma 3.3, compute sufficiently good upper bounds expected value.
order P
apply Chernoff
case, actually require monotonicity argument:
Pbound
n
n
0
0
Z =
X

Z
=
X
i=1
i=1 sums independent identically distributed (i.i.d.)
indicator random variables E [Xi ] E [Xi0 ], Pr[Z > ] Pr[Z 0 > ] .
argument applying Lemma 3.3 = e 1, obtain:
693

fiDinh, Dinh, Michel, & Russell

Corollary 3.1. Let Z sum n i.i.d. indicator random variables E [Z] n,
Pr[Z > e] < e .
Adopting search space whose solutions distributed according Spd , ready
bound running time average guided (1 , 2 )-approximate heuristic:
3

Theorem 3.1. Let sufficiently large. probability least 1 ed e2d , search
tree using (1 , 2 )-approximate heuristic function expands 12d4 b(1 +2 )d vertices
solutions distributed according random variable Spd .
Proof. Let X random variable equal total number nodes expanded
(1 , 2 )-approximate heuristic. course exact value of, say, E [X] depends h; prove
upper bounds achieved high probability (1 , 2 )-approximate h. Applying Lemma 3.1
= 1, conclude

X 2b(1 +2 )h (r) + (1 1 )h (r)N1 +2 .
Thus suffices control h (r) number N1 +2 (1 + 2 )-optimal solutions.
utilize fact Spd model, optimal solutions unlikely located
far depth d. end, let Efar event h (r) > + < set

later. Lemma 3.2 immediately gives Pr[Efar ] eb .
Observe conditioned Efar , h (r) d+ N1 +2 Z, Z random
variable equal number solutions depth (1 + 1 + 2 )(d + ).

(1+1 +2 )(d+)
= 2b(1 +2 )d+(1+1 +2 ) < 2b(1 +2 )d+3
E[Z] b 2b

and, applying Chernoff bound Corollary 3.1 control Z,
h



3
Pr Z > 2eb(1 +2 )d+3 exp 2b(1 +2 )d+3 e2b .
Letting Ethick event Z 6b(1 +2 )d+3 , observe
h

3
Pr[Ethick ] Pr Z > 2eb(1 +2 )d+3 e2b .
summarize: neither Efar Ethick occurs,
X 2b(1 +2 )(d+) + (1 1 )(d + )6b(1 +2 )d+3
6(d + )b(1 +2 )d+3
12db(1 +2 )d+3 .
Hence,
h


3
Pr X > 12db(1 +2 )d+3 Pr[Efar Ethick ] eb + e2b .
infer bound stated theorem, set b = b(1 +2 )d+3 = d3 b(1 +2 )d , completing
proof.
Remark similar methods, trade-offs error probability resulting bound
number expanded nodes obtained.
694

fiThe Time Complexity Approximate Heuristics

4. Lower Bounds Running Time Trees

establish upper bounds Theorem 3.1 tight within O(1/ d) term
exponent. begin recording following easy fact solution distances discrete
model.
Fact 2. Let h (r) nonnegative integer. every solution s, node v
Path(s) h (v) = .
Proof. Fix distance h (r). prove lemma induction depth solutions.
lemma clearly holds optimal solutions. Consider solution may optimal,
let v Path(s) node level far h (v) . h (v) < ,
must another solution s0 SubTree(v) closer v. induction assumption,
node v 0 Path(s0 ) h (v 0 ) = . node v 0 must ancestor v, since distance
v s0 less distance v 0 s0 least , completing
proof.
proceed lower bound.
Theorem 4.1.Let sufficiently large. solutions distributed according Spd , probability
least 1 b , exists (1 , 2 )-approximate heuristic function
h number
vertices expanded search tree using h least b(1 +2 )d4 /8.
Proof. plan define pathological heuristic function forces expand many
nodes possible. Note heuristic function allowed overestimate h . Intuitively,
wish construct heuristic function overestimates h nodes close solution
underestimates h nodes far solutions, leading astray whenever possible. Recall
every vertex v, likely solution lying depth SubTree(v). Thus use
quantity h (v) formalize intuitive notion node v close solution,
quantity < determined later. heuristic function h formally defined follows:
(
(1 + 2 )h (v) h (v) ,
h(v) =
(1 1 )h (v) otherwise.
Observe chance node overestimated small since, Lemma 3.2,
Pr[v overestimated] = Pr[h (v) ] 2b

(4)

node v. note node v overestimated ancestor, f
values monotonically increase along path root v.
Naturally, wish ensure optimal solution close root. Let Eclose
event h (r) . Lemma 3.2,
Pr[Eclose ] 2b .
see conditioned event Eclose , means h (r) > , every
solution obscured overestimated node close solution. Concretely,
issues integrality, Fact 2 asserts every solution s, must node v
path root h (v) = , long < h (r).
Assume Eclose : whenever h (v) = , g(v) h (r) (d ) > 0 h(v) =
(1 + 2 )(d ), thus f (v) > (1 + 2 )(d ). Since every solution obscured
overestimated node whose f value larger (1 + 2 )(d ), > (1 + 2 )(d ),
min-max value defined (2). follows node v must expanded Path(v)
695

fiDinh, Dinh, Michel, & Russell

contain overestimated node f (v) (1 + 2 )(d ). Path(v) contain
overestimated node, f (v) = g(v) + (1 1 )h (v),
f (v) (1 + 2 )(d ) (1 1 )h (v) (1 + 2 )(d ) g(v) ,
since 1 < 1. Therefore, say node v required overestimated node Path(v)
(1 1 )h (v) (1 + 2 )(d ) g(v). recap, conditioned Eclose , set required nodes
subset set nodes expanded search using defined heuristic function. use
Chernoff bound control size R` denotes set non-required nodes depth
`.
Let v node depth ` < (1 + 2 )d. Equation (4) implies
Pr[ overestimated node Path(v)] 2`b < 1/16 .
last inequality holds sufficiently large d, long = poly(d). hand,
1 < 1,




(1 + 2 )(d ) `
Pr v R` = Pr h (v) >
1 1


(1+2 )(d)`

11
exp b
(by Lemma 3.2)


(1 +2 )d(1+2 )`
11
.
(5)
= exp b
set ` = (1 + 2 )d (1 + 2 ) logb 4. Equation (5) implies
logd 4


Pr v R` exp b 11 e4 1/16 .
case 1 = 1, event (1 1 )h (v) > (1 + 2)(d ) ` never
given value

happens
` set. Hence, case, Pr v R` 1/8 E fiR` b` /8. Applying
Chernoff bound Corollary 3.1 yields


Pr fiR` > eb` /8 exp(b` /8) .

Let Ethin event fiR` b` /2. Since b` /2 > eb` /8,
Pr[Ethin ] exp(b` /8) .
Putting pieces together,


`
Pr expands less b` /2 nodes Pr[Eclose Ethin ] 2b + eb /8 .


Setting = 2 ` = (1 + 2 )d 2(1 + 2 ) logb 4, thus
h



Pr expands less b(1 +2 )d4 /8 nodes b
sufficiently large d.
contrast, explore behavior adversarially selected solution set;
achieves lower bound nearly tight (in comparison general upper bound
worst-case running time obtained setting = 0 bound Lemma 3.1 above).
696

fiThe Time Complexity Approximate Heuristics

Theorem 4.2. > 1, exists solution set whose optimal solutions lie depth
(1 , 2 )-approximate heuristic function h tree using h expands
least b(1+2 )d12 /1 nodes.
Proof. Consider solution set 2 -optimal solutions share ancestor u lying depth
1. Furthermore, contains every node depth (1 + 2 )d descendant u,
= h (r).
define (1 , 2 )-approximate heuristic h follows: h(u) = (1 + 2 )h (u) h(v) =
(1 1 )h (v) v 6= u. heuristic, every 2 -optimal solution hidden search
procedure ancestor u. Precisely, since f (u) = 1 + (1 + 2 )(d 1) = (1 + 2 )d 2 , every
2 -optimal solution (which descendant u)
max
vPath(s)

f (v) f (u) = (1 + 2 )d 2 .

Thus (1 + 2 )d 2 , min-max value defined Equation (2).
Let v node depth ` (1 + 2 )d lie inside SubTree(u). Note
f values monotonically increase along path root r v, implies node v must
expanded f (v) < . hand, since every non-descendant u depth (1 + 2 )d
solution, ` + h (v) (1 + 2 )d, thus
f (v) ` + (1 1 )[(1 + 2 )d `] = (1 1 )(1 + 2 )d + 1 ` .
Hence, node v must expanded (1 1 )(1 + 2 )d + 1 ` < (1 + 2 )d 2 , equivalent
` < (1 + 2 )d 2 /1 . follows number nodes expanded least
(1+2 )d12 /1

X
`=0

(1+2 )d22 /1

b`

X

b` = b(1+2 )d12 /1 .

`=0

According Theorem 4.2, set 2 = 0 let 1 arbitrarily small provided 1 > 0,
obtain near-accurate heuristic forces expand least many bd1 nodes.
lower bound partially explains perform poorly, even almost perfect
heuristic, certain applications (Helmert & Roger, 2008): adversarially-chosen solution set
given proof Theorem 4.2 overwhelming number near-optimal solutions. Indeed,
N+2 b(1+2 )d b(1+2 )d1 b(1+2 )d1
> 0.

5. Generalizations: Non-uniform Edge Costs Branching Factors
section, discuss generic upper bounds Lemma 3.1 generalized apply
natural search models non-uniform branching factors non-uniform
edge costs; Section 6, show extended general graph search models.
consider general search tree without assumptions uniform branching factor
uniform edge costs. argument given proof Lemma 3.1, derive assertion
heuristic (1 , 2 )-approximate, node cost (1 + 2 + 1 )c
potentially expanded lie (1 + 2 )-optimal solution path,
arbitrary nonnegative number c = h (r) optimal solution cost.
Hence, number nodes potentially expanded (1 , 2 )-approximate heuristic
bounded


F (1 + 2 + 1 )c + R (1 + 2 + 1 )c , 1 + 2 .
(6)
697

fiDinh, Dinh, Michel, & Russell

F () number nodes cost , call free nodes; R(, )
number nodes cost range (, (1 + 2 )c ] lie -optimal solution path,
call restricted nodes.
bound number free restricted nodes, respectively, assume branching
factors upper bounded edge costs lower bounded. Let B 2 maximal branching
factor let minimal edge cost. Since node cost must lie
depth larger /m,
F () 2B /m .
-optimal solution path, ((1 + 2 )c )/m nodes cost range
(, (1 + 2 )c ]. Thus,
(1 + 2 )c
N .
R(, )

Letting = (1 + 2 + 1 )c , = 1 + 2 , applying bounds F () R(, )
bound (6), obtain another upper bound number expanded nodes heuristic
(1 , 2 )-approximate:


2B (1 +2 +1)c

/m

+ N1 +2 (1 1 )c /m

(7)

0. equation (7) generalized version bound Lemma 3.1. Substituting
= 1 (7), arrive following simpler upper bound number expanded nodes:
2B (1 +2 )c



/m

+ N1 +2 (1 1 )c /m .

(8)

6. Bounding Running Time Graphs
previous parts, established bounds running time tree model.
apply bounds graph model. order that, first unroll
graph cover tree, bound number solutions lifted cover tree.
6.1 Unrolling Graphs Trees
preceding generic upper bounds developed tree-based models; section discuss
natural extension general graph search models. principal connection obtained unrolling graph tree expands least many nodes original
graph (including repetitions). specifically, given directed graph G starting node x0 G,
define cover tree (G) whose nodes one-to-one correspondence finite-length paths
G x0 . shall write path (x0 , . . . , x` ) G node (G). root (G)
(x0 ). parent node (x0 , x1 , . . . , x` ) (G) node (x0 , x1 , . . . , x`1 ), edge cost
two nodes (x0 , x1 , . . . , x`1 ) (x0 , x1 , . . . , x` ) (G) equals cost edge
(x`1 , x` ) G. Hence, node P (G), cost value g(P ) equal total edge
cost path P G. node (x0 , . . . , x` ) (G) designated solution whenever x`
solution G.
node (G) corresponds path ending node x G called copy x.
Observe solution G may lift multiple times solutions (G), node G may
multiple copies (G). Figure 1 illustrates example unrolling graph cover tree.
example, node solution graph first two copies cover tree correspond
paths (0, 3, s) (0, 5, 3, s), 0 starting node given graph.
search graph G described Algorithm 2 below, h(x) heuristic
node x, g(x) cost current path x0 x, c(x, x0 ) denotes cost
edge (x, x0 ) G. assume value h(x) depends x, i.e., h(x) depend
particular path x0 x. Unlike search tree, node x Open Closed,
698

fiThe Time Complexity Approximate Heuristics

0
1
6

1

3

5

2

2



3

6

1

1

5

2



2
0

5

3


Figure 1: Unrolling graph cover tree.
Algorithm 2 keeps track current path P x0 x pointers,
current f -value x equal g(P ) + h(x). current path cheapest path x0 x
passes nodes expanded.
Algorithm 2 search graph (Pearl, 1984, p. 64)
1. Initialize Open := {x0 } g(x0 ) := 0.
2. Repeat Open empty.
(a) Remove Open place Closed node x function f = g + h
minimum.
(b) x solution, exit success return x.
(c) Otherwise, expand x, generating successors. successor x0 x,
i. x0 Open Closed, estimate h(x0 ) calculate f (x0 ) = g(x0 ) + h(x0 )
g(x0 ) = g(x) + c(x, x0 ), put x0 Open pointer back x.
ii. x0 Open Closed, compare g(x0 ) g(x) + c(x, x0 ). g(x) + c(x, x0 ) <
g(x0 ), direct pointer x0 back x reopen x0 Closed.
3. Exit failure.
consider search cover tree (G) graph G using heuristic function h:
node P (G), set heuristic value h(P ) equal h(x) P copy node
x G, i.e., P path G x0 x. Observe cover tree (G) graph G share
threshold (defined Equation (2)). Hence, whenever node x G expanded
current path P , must g(P ) + h(x) , implies P potentially expanded
search cover tree (G). shows following fact:
Fact 3. number node expansions G number nodes potentially
expanded (G) using heuristic.
Here, node expansion, mean execution expand step , i.e. Step (2c). Note
that, general, node G expanded many times along different paths.
Remark running time cover tree used upper bound running time
iterative-deepening (IDA ) graph. Recall running time IDA dominated
last iteration. hand, last iteration IDA G merely depth-first search
699

fiDinh, Dinh, Michel, & Russell

cover tree (G) cost threshold . Hence, number expansions last
iteration IDA number nodes potentially expanded cover
tree.
So, upper-bound time complexity IDA graph, suffices unroll graph
cover tree apply upper bounds number nodes potentially expanded
cover tree. particular, bound Equation (7) applied directly search
G.
Note bounds applied directly, problem determining exactly
solutions G lift solutions cover tree depends delicate structural properties G
specifically, depends growth number distinct paths x0 solution
function length paths. particular, order obtain general results
complexity model, must invoke measure connectedness graph G.
show bound complexity terms spectral properties G. choose
approach offers single parameter notion connectedness (the second eigenvalue)
analytically tractable actually analyzed bounded many graphs
interest, including various families Cayley graphs combinatorial graphs methods
conductance.
6.2 Upper Bound via Graph Spectra
shall consider undirected2 graph G n vertices search space. Let x0 starting
node let set solutions G. simplicity, assume G b-regular (2 < b n)
edge costs uniformly equal one, cover tree (G) b-ary uniform edge cost.
assume, additionally, |S| treated constant n .
Fact 3 Lemma 3.1, number node expansions G (1 , 2 )approximate heuristic 2b(1 +2 )d + dN1 +2 , optimal solution cost,
equals optimal solution depth (G), N number -optimal solutions (G).
goal upper bound N (of cover tree (G)) terms spectral properties G.
introduce principal definitions spectral graph theory below, primarily set
notation. complete treatment spectral graph theory found work Chung
(1997).
Graph Spectra. graph G, let adjacency matrix G: A(x, y) = 1 x adjacent
y, 0 otherwise. real, symmetric matrix (AT = A) thus real eigenvalues
b = 1/b
b = 1 2 . . . n b, spectral theorem (Horn & Johnson, 1999). Let
b
denote normalized adjacency matrix G; eigenvalues 1 = 1 2 . . . n 1,
referred spectrum G, = /b. eigenvalues, along
associated eigenvectors, determine many combinatorial aspects graph G. applications
def
graph eigenvalues, however, critical value = (G) = max {|2 |, |n |} invoked
and, moreover, real parameter interest gap = /b largest eigenvalue
1 = 1 normalized adjacency matrix. Intuitively, measures connectedness G.
Sparsely connected graphs 1; n-cycle, example, = 1 O(1/n). hypercube
N = 2n vertices = 1 (1/ log N ). Similar bounds , known many
families Cayley graphs. Random graphs, even constant degree b 3, achieve = o(1)
high probability. fact, recent result Friedman (2003) strengthens this:
Theorem 6.1. (Friedman, 2003) Fix real c > 0 integer b 2. probability
1 o(1) (as n ),

(Gn,b ) 2 b 1 + c
2. one produce analogous cover tree directed case, spectral machinery apply next
section somewhat complicated presence directed edges. See work Chung (2006) Horn
Johnson (1999, Perron-Frobenius theorem) details.

700

fiThe Time Complexity Approximate Heuristics

Gn,b random b-regular graph n vertices.
remark non-bipartite connected graph diameter D, always
b 1/(Dn). stronger conditions, graph vertex-transitive (which say
pair v0 , v1 vertices G automorphism G sending v0 v1 ), one
b (1/D2 ) (Babai, 1991). vertex transitivity strong condition, satisfied
many natural algebraic search problems (e.g., 15-puzzle-like search spaces Rubiks cube).
principal spectral tool apply section described Lemma 6.1 below. begin
notation.
Notations. function G viewed column vector indexed vertices G
vice versa. vertex x G, let 1x denote function G value 1 x
0 everyPvertex x. real-valued functions , G, define
pinner product
h, = xG (x)(x). shall use k k denote L2 -norm, i.e., kk = h,
function G.
b symmetric real, spectral theorem (Horn & Johnson, 1999),
Recall since
exist associated eigenfunctions 1 , . . . , n form orthonormal basis space real-valued
b particular,
functions G, eigenfunction associated eigenvalue A.
b

Pn Ai = ki k = 1 i, hi , j = 0 6= j. basis, write
= i=1 h, real-valued function G.
Lemma 6.1. Let G undirected b-regular graph n vertices, = (G)/b.
probability distributions p q vertices G, integers s, 0,

fiD


E


b p,
bt q 1 s+t kpk kqk 1 .


nfi
n
Pn

Pn
ai q = j=1 bj j ai = hp, , bj = hq, j i.
+
* n
n
n
n

E
X
X
X
X




b p,
bq =
s+t
ai bi .
ai bj si tj hi , j =
bj j j =

ai ,


Proof. Write p =

i=1

i=1

i,j=1

j=1

i=1

Cauchy-Schwartz inequality,
n
X

v
!
! n
u n
X
u X
2
2

|ai bi |
bi = kpk kqk .
ai

i=1

i=1

i=1
1/n

Without loss generality, assume 1 (x) =
vertices x G. Since p probability
distribution,
X
1
1 X
p(x) = .
a1 = hp, 1 =
p(x)1 (x) =
n
n
xG

Similarly, b1 =

1 .
n

Thus, a1 b1 =

xG

1
n.


n

fiD

E 1 fifi fiX



s+t
b p,
bt q =



b






nfi
i=2

s+t

n
X

|ai bi |

(as = max |i |)
2in

i=2



s+t kpk kqk
completing proof lemma.
701

1
n


,

fiDinh, Dinh, Michel, & Russell

Lemma 6.1 hand, establish following bound number paths prescribed
length ` connecting pair vertices. apply control number -optimal solutions
cover tree G. Let P` (u, v) denote number paths G length ` u v.
Lemma 6.2. Let G undirected b-regular graph n vertices, = (G). vertices
u, v G ` 0,




`fi

fiP` (u, v) b ` 1 1 < ` .

nfi
n
Proof. Since P` (u, v) number `-length paths u v, P` (u, v) = b` p(`) (v),
p(`) (v) probability natural random walk G length
` starting
u ends

E



P` (u,v)
`
(`)
`
(`)
(`)
b
b
= 1v , 1u . Applying Lemma 6.1
v. Since p = 1u p (v) = 1v , p ,
`
b

yields






E 1 fifi
P` (u, v)
1 fifi fifiD
`
b

` k1v k k1u k 1 = ` 1 1 .
1
,

1


=
v
u
b`
nfi
nfi
n
n
= /b, multiplying sides last inequality b` completes proof lemma.
major consequence Lemma 6.2 application following bound number
-optimal solutions (G).
Theorem 6.2. Let G undirected b-regular graph n vertices, = (G). sufficiently
large n 0, number -optimal solutions (G)
(1+)d

b
N < 2|S|
+ (1+)d ,
n
depth optimal solutions (G), set solution nodes G.
Proof. solution S, number copies level ` (G) equals P` (x0 , s),
less b`/n + ` Lemma 6.2. Hence, number solutions level ` (G)
`

X
b
P` (x0 , s) < |S|
+ ` .
(9)
n
sS

Summing sides (9) ` ranging (1 + )d,


(1+)d
(1+)d
(1+)d
X X
X
X
1
N =
P` (x0 , s) < |S|
b` +
` .
n
`=d sS

`=d

`=d

n sufficiently large, 2. Thus,


1 (1+)d
N < |S|
2b
+ 2(1+)d .
n

Note b(1+)d
/n = O(1) =O(logb n). mentioned earlier (Theorem 6.1), b-regular
graphs 2 b 1 + o(1) 2 b. Assuming G spectral property = O(logb n),
Theorem 6.2 gives


N = O((1+)d ) = 2(1+)d b(1+)d/2 .
cases, number node expansions G using (1 , 1 )-approximate heuristic
O(d2(1+)d b(1+)d/2 ), implies effective branching factor roughly bounded
21+ b(1+)/2 < 8b(1+)/2 .
702

fiThe Time Complexity Approximate Heuristics

7. Experimental Results
discussed introduction, bounds established thus far guarantee E, number
nodes expanded using -accurate heuristic, satisfies
E 2bd + dN cbd
assumption N bd . (Here, before, b branching factor, optimal
solution depth, c constant.) suggests hypothesis hard combinatorial
problems suitably sparse near-optimal solutions,
log E log b + .

(10)

constant determined search space heuristic independent .
particular, suggests linear dependence log E . experimentally investigated
hypothesized relationship family results involving Knapsack problem partial
Latin square problem. far aware, first experimental results specifically
investigating dependence.
remark order experimental framework really cast light bounds
presented , one must able furnish heuristic known approximation guarantees.
7.1 Search Knapsack
begin describing family experimental results search coupled approximate
heuristics solving Knapsack problem. problem extremely well-studied
wide variety fields including finance, operations research, cryptography (Kellerer, Pferschy,
& Pisinger, 2004). Knapsack problem NP-hard (Karp, 1972), efficient algorithm
solve exactly unless NP = P. Despite that, problem admits FPTAS (Vazirani, 2001, p.
70), algorithm return -approximation optimal solution time polynomial
1/ input size. use FPTAS construct approximate admissible heuristics
search, yields exact algorithm Knapsack may expand far fewer nodes
straightforward exhaustive search. (Indeed, resulting algorithm is, general, efficient
exhaustive search.)
7.1.1 Search Model Knapsack
Consider Knapsack instance given n items, let [n] = {1, . . . , n}. item [n]
weight wi > 0 profit pi > 0. knapsack capacity c > 0. task find set items
maximal total profit total weight c. Knapsack instance
denoted tuple h[n], p, w, ci. Knapsack instance restricted subset X [n] denoted
hX, p, w, ci. subset X [n], let w(X) P
p(X) denote P
total weight
total profit, respectively, items X, i.e., w(X) = iX wi p(X) = iX pi .
Search Space. represent Knapsack instance h[n], p, w, ci search space follows.
state (or node) search space nonempty subset X [n]. move (or edge) one state
X another state taken removing item X. cost move profit
removed item. state X [n] designated solution w(X) c. initial state
set [n]. See Figure 2 example search space n = 4.
search space irregular directed graph whose out-degrees span wide range, 2
n 1. Moreover, two states X1 , X2 X2 X1 [n], |X1 \ X2 |! paths
search graph X1 X2 . Moreover, every path X1 X2 cost equal
p(X1 ) p(X2 ). feature search graph makes behave spanning subtree
graph: state search graph reopened. Hence, state X [n], cost
703

fiDinh, Dinh, Michel, & Russell

{1, 2, 3, 4}

{1, 2, 3}

{1, 2}

{1, 2, 4}

{1, 3}

{1, 3, 4}

{1, 4}

{1}

{2, 3}

{2}

{2, 3, 4}

{2, 4}

{3}

{3, 4}

{4}

Figure 2: search space Knapsack instance given set 4 items {1, 2, 3, 4}. Solution
states edge costs indicated figure.
path starting state X
g(X) = p([n] \ X) = p([n]) p(X) ,
cheapest cost reach solution state X [n]
h (X) = p(X) Opt(X) ,
Opt(X) total profit optimal solution Knapsack instance hX, p, w, ci, i.e.,
Opt(X) = max {p(X 0 ) | X 0 X w(X 0 ) c} .
def

Observe solution state X [n] search space h[n], p, w, ci optimal
g(X ) minimal, equivalently, p(X ) maximal, means X optimal solution
Knapsack instance h[n], p, w, ci.
Heuristic Construction. Fix constant (0, 1). order prove linear dependence
log E , wish efficient -accurate heuristic H aforementioned Knapsack
search space h[n], p, w, ci. Moreover, order guarantee solution returned
search optimal, insist H admissible, H must satisfy:
(1 )h (X) H (X) h (X) X [n] .
main ingredient constructing heuristic FPTAS described book Vazirani
(2001, p. 70). FPTAS algorithm, denoted A, returns solution total
profit
least (1 )Opt(X) Knapsack instance hX, p, w, ci runs time |X|3 / ,
(0, 1). nonempty subset X [n], let (X) denote total profit solution
returned algorithm error parameter Knapsack instance hX, p, w, ci.
(0, 1),
(1 )Opt(X) (X) Opt(X) ,
implies
p(X)

(X)
h (X) p(X) (X) .
1

(11)

(X)
Thus may work heuristic H (X) = p(X) A1
, guarantees admissibility.
However, definition H guarantee -approximation H : definition,
condition (1 )h (X) H (X) equivalent

(1 )h (X) p(X)
704

(X)
,
1

(12)

fiThe Time Complexity Approximate Heuristics

always hold. Since h (X) p(X) (X), condition (12) satisfied
(1 )(p(X) (X)) p(X)

(X)
.
1

(13)

(X)
Equation (13) holds. Otherwise, define
Hence, define H (X) = p(X) A1
H (X) differently, still ensuring -approximate admissible. Note X
solution, least one item X must removed order reach solution contained X, thus
h (X) = p(X) Opt(X) m, smallest profit items. gives another option
define H (X) guarantee admissibility. summary, define heuristic function
H follows: non-solution state X,
(
(X)
(13) holds
p(X) A1
def
(14)
H (X) =

otherwise,

determined later H -approximate. X solution, simply set
H (X) = 0, h (X) = 0 case. H admissible, regardless .
make sure H -approximate, remains consider case (13) hold,
(X)
i.e., p(X) A1
< (1 )(p(X) (X)), non-solution state X. case,
p(X) (X)



(X)
(p([n]) m) .
(1 )
(1 )

(15)

last inequality due assumption X solution. want choose



(p([n]) m)
(16)
(1 )
1
which, combining (11) (15), imply (1 )h (X) = H (X). Therefore,
choose

1 = 1 + 1 1 (p([n])/m 1) .

3 1
Since running time
compute

(X)


|X|

, running time compute H (X)


3 1
|X| p([n])/m , polynomial n 1 profits bounded
range [m, poly(n)m]. search using heuristic H given Knapsack space
h[n], p, w, ci described Algorithm 3 below.
7.1.2 Experiments
order avoid easy instances, focus two families Knapsack instances identified studied
Pisinger (2005) difficult existing exact algorithms, including dynamic programming
algorithms branch-and-bound algorithms:
Strongly Correlated: item [n], choose weight wi random integer
range [1, R] set profit pi = wi + R/10. correlation weights profits
reflects real-life situation profit item proportional weight plus
fixed charge.
Subset Sum: item [n], choose weight wi random integer range [1, R]
set profit pi = wi . Knapsack instances type instances subset sum
problem.
tests
P set data range parameter R := 1000 choose knapsack capacity
c = (t/101) i[n] wi , random3 integer range [30, 70].
3. paper Pisinger (2005), fixed integer 1 100, average runtime tests
corresponding values reported.

705

fiDinh, Dinh, Michel, & Russell

Algorithm 3 Search Knapsack
Input: hn, p, w, c, i; n number items, pi wi profit weight item
[n], c capacity knapsack, (0, 1) error parameter heuristic.
Oracle: FPTAS algorithm Knapsack problem
(2001, p. 70).
P described Vazirani
P
Notation: subset X [n] items, let p(X) = iX pi , w(X) = iX wi .
Output: subset X [n] items w(X ) c p(X ) maximal.
1. Put start node [n] Open. Let = min1in pi . Set

1 = 1 + 1 1 (p([n])/m 1) .
2. Repeat Open empty:
(a) Remove Open place Closed node X g(X) + h(X) minimum.
(b) w(X) c, exit success return X, optimal solution.
(c) Otherwise, expand X: item X, let X 0 = X \ {i},
i. X 0 Open Closed, set g(X 0 ) := g(X) + p(i) = p([n]) p(X 0 ),
compute heuristic h(X 0 ) follows:
A. X 0 solution, set h(X 0 ) := 0.
B. Otherwise, run algorithm Knapsack input hX 0 , p, w, ci error parameter , let A(X 0 ) denote total profit solution returned algorithm
A. set
(
0
0
)
)
0
0
p(X 0 ) A(X
p(X 0 ) A(X
0
1
1 (1 )(p(X ) A(X ))
h(X ) :=

otherwise.
put X 0 Open pointer back X.
ii. Otherwise (X 0 Open Closed, g(X 0 ) calculated), g(X)+p(i) <
g(X 0 ), direct pointer X 0 back X reopen X 0 Closed.
[Remark: Since paths starting node X 0 cost,
condition g(X) + p(i) < g(X 0 ) never holds. fact, step discarded.]
3. Exit failure.

706

fiThe Time Complexity Approximate Heuristics

generating Knapsack instance h[n], p, w, ci either type described above, run series
search using given heuristic H , various values , well breath first search
(BFS), solve Knapsack instance. search finishes, values E reported,
E number nodes (states) expanded search, depth optimal
solution found search. Knapsack search space, k equals number items removed
original set [n] obtain optimal solution found search. overall runtime
search, including time computing heuristic, reported. addition, report
optimal value h ([n]) minimal edge cost (i.e., minimal profit) search space
Knapsack instance tested.
specify appropriate size n Knapsack instance type, ran exploratory experiments identified largest possible value n search instances would finish
within hours. chose values n (n = 23 Strongly Correlated type,
n = 20 Subset Sum type) final experiments. Observing optimal solution
depths resulted Knapsack instances sizes fairly small, ranging 5 15,
selected sample points high interval [0.5, 1) distance two consecutive
points large enough sensitiveness E seen. particular, selected eight
sample points 8/16 = 0.5 15/16 = 0.9375 distance 1/16 = 0.0625
two consecutive points. final experiments, generated 20 Knapsack instances type
selected parameters n .
Experimental Results. Results final experiments shown Tables 1, 2, 3, 4, 5, 6,
rows corresponding breath first search indicated BFS column
. data show, expected, search outperforms breath first search terms
number nodes expanded and, naturally, smaller , fewer nodes expands.
result, effective branching factor decrease decreases (as long optimal
solutions given search space located depth). Recall expands E
nodes finds solution depth d, effective branching factor branching factor
uniform tree depth E nodes (Russell & Norvig, 1995, p. 102), i.e., number b satisfying
E = 1 + b + (b )2 + + (b )d . Clearly, (b )d E and, b 2, E 2(b )d . shall
focus solely values b 2, simply use E 1/d proxy effective branching factor, content
differs actually quantity factor 21/d . (Of course, b grows
error decays even further). effective branching factors, calculated E 1/d , search
breath first search Knapsack instances type Strongly Correlated shown Tables 1,
2, 3. Note Knapsack instances Subset Sum type, one cannot directly compare
effective branching factors, optimal solutions found different search instances appear
different depths.
primary goal experiments investigate proposed linear dependence which,
case non-uniform branching factors non-uniform edge costs, may express
log E log bBFS + ,

(17)

average optimal solution depth, bBFS effective branching factor breath first
search, constant depending . examine extend data supports
hypothesis, calculate least-squares linear fit (or linear fit short) log E (for
Knapsack instance, varying ) using least-squares linear regression model, measure
coefficient determination R2 . experiments, 17 20 Knapsack instances type
Strongly Correlated 20 Knapsack instances type Subset Sum R2 value least
0.9. instances, 90% variation log E depends linearly , remarkable fit.
See Figure 5 detailed histograms R2 values Knapsack instances. median R2
0.9534 Knapsack instances type Strongly Correlated, 0.9797 type Subset
Sum. Graphs log E linear fit Knapsack instances median R2 among
type shown Figures 3 4. Note even number instances
707

fiDinh, Dinh, Michel, & Russell

type, single instance median value. instances shown graphs
actually R2 value median.
Knapsack instance type Strongly Correlated median R2
Instance 17
Linear fit
BFS

log10 E

6
5
4
3
2
0.5

0.6

0.7
0.8
0.9
Heuristic error

1

Figure 3: Graph log10 E least-squares linear fit Knapsack instance type Strongly
Correlated median R2 (see data Table 3).

Knapsack instance type Subset Sum median R2
5.3

Instance 14
Linear fit
BFS

log10 E

5.25

5.2

5.15
0.5

0.6

0.7
0.8
0.9
Heuristic error

1

Figure 4: Graph log10 E least-squares linear fit Knapsack instance type Subset
Sum median R2 (see data Table 5).

Remark course, may instances poorly fit prediction linear dependence,
instance #20 Strongly Correlated type whose R2 value 0.486, though instances
708

fiThe Time Complexity Approximate Heuristics

rarely show experiments. instance, search using heuristic function H
may explore even fewer nodes search using H does, small > 0.
phenomenon explained degree control accuracy heuristic
function H . particular, guarantee H admissible -approximate,
reality may provide approximation better nodes opened. Note H
proportional (1 ). Hence, H may occasionally accurate H
small > 0, resulting fewer nodes expanded.

Frequency

Histograms R2 values Knapsack instance families
Strongly correlated
Subset Sum

10

5

[0.97, )

[0.93, )

[0.90, )

[0.87, )

[0.83, )

[0.80, )

[0.77, )

[0.73, )

[0.70, )

[0.67, )

[0.63, )

[0.60, )

[0.57, )

[0.53, )

[0.50, )

[0.47, )

[0.43, )

[0.40, )

0

R2 value bin limits
Figure 5: Histograms R2 values Knapsack instances.
analyze deeply data fit model Equation (17), calculate slope
least-squares linear fit log10 E Knapsack instance type Strongly Correlated. Note
instance, every search optimal solution depth, denoted d, thus,
= d. data, given Figure 6, show one instance worst R2 value,
slope linear fit log10 E fairly close log10 bBFS , slope hypothesized
line given Equation (17). Specifically, Knapsack instance type Strongly Correlated,
except instance #20,
0.73d log10 bBFS 1.63d log10 bBFS .

7.2 Search Partial Latin Square Completion
experimental results discussed Knapsack problem support hypothesis linear
scaling (cf., Equation (1) (10)). However, several structural features search space
heuristic unknown: example, cannot rule possibility approximation
algorithm, asked produce -approximation, fact produce significantly
better approximation; likewise, explicit control number near-optimal solutions.
order explore hypothesis detail, experimentally analytically investigate
search space partial Latin square completion problem provide precise analytic
control heuristic error well number -optimal solutions N .
7.2.1 Partial Latin Square completion (PLS) Problem
Latin square order n n n table row column permutation
set [n] = {1, . . . , n}. cells n n table filled values [n]
709

fiDinh, Dinh, Michel, & Russell

Knapsack instance type: Strongly Correlated
Instance
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Optimal
solution
depth
11
9
6
7
7
6
7
8
6
9
7
5
7
5
6
9
7
8
7
7

Effective branching
factor breath first
search bBFS
4.2092
5.3928
10.8551
8.2380
8.0194
10.6780
8.7068
6.7742
11.4102
5.5412
8.3260
18.0486
8.0308
15.0964
10.0070
5.7863
8.3155
6.9106
8.3602
7.0964

Slope

linear fit

a/(d log10 bBFS )

Coefficient
determination R2

7.6583
4.8966
10.1038
6.4279
5.0882
6.4511
7.9087
6.5616
8.6847
6.3690
9.7685
7.7848
6.0376
7.3004
4.4219
7.1815
9.1738
9.2837
7.1807
1.0055

1.1154
0.7435
1.6260
1.0027
0.8040
1.0454
1.2021
0.9872
1.3690
0.9517
1.5161
1.2392
0.9533
1.2385
0.7368
1.0466
1.4247
1.3823
1.1123
0.1688

0.9395
0.9183
0.9647
0.9710
0.9161
0.9696
0.9436
0.9782
0.9571
0.9461
0.9689
0.9314
0.9646
0.9676
0.8788
0.8698
0.9498
0.9729
0.9770
0.4860

Figure 6: Slopes least-squares linear fits log10 E (varying ) Knapsack instances
type Strongly Correlated. Details least-squares linear fits given Tables 1, 2, 3.
R2 values Knapsack instances included figure.
way value appears twice single row column, table called partial Latin
square. completion partial Latin square L Latin square obtained filling
empty cells L, see Figure 7 example. Note every partial Latin square
completion. Since problem determining whether partial Latin square completion
NP-complete (Colbourn, 1984), search version (denoted PLS), i.e., given partial Latin square
L find completion L one exists, NP-hard.
1

2
5

3
1
4

1
2

1
2
3
4
5

4
3

2
3
5
1
4

3
5
4
2
1

4
1
2
5
3

5
4
1
3
2

Figure 7: 5 5 partial Latin square (right) unique completion (left).
PLS problem (also known partial quasi-group completion) used recent
past source benchmarks evaluation search techniques constraint satisfaction
Boolean satisfiability (Gomes & Shmoys, 2002). Indeed, partially filled Latin squares carry
embedded structures trademark real-life applications scheduling time-tabling.
Furthermore, hard instances partially filled Latin square trigger heavy-tail behaviors
backtrack search algorithms common-place real-life applications require randomization restarting (Gomes, Selman, & Kautz, 1998). Additionally, PLS problem exhibits
strong phase transition phenomena satisfiable/unsatisfiable boundary (when 42%
cells filled) exploited produce hard instances. remark underlying
710

fiThe Time Complexity Approximate Heuristics

structure Latin squares found real-word applications including scheduling, timetabling (Tay, 1996), error-correcting code design, psychological experiments design wavelength
routing fiber optics networks (Laywine & Mullen, 1998; Kumar, Russell, & Sundaram, 1996).
7.2.2 Search Model PLS
Fix partial Latin square L order n c > 0 completions. divide cells n n table
two types: black cells, filled L, white cells,
left blank L. Let k number white cells. white cells indexed 0 k 1
fixed order, e.g., left right top bottom table. task search find
completion L. Hard instances obtained white cells uniformly distributed within
every row every column density black cells (n2 k)/n2 42% tap
phase transition. insure number completions c = O(1) (c exactly 1
experiments).
structure search space problem, place white cells virtual circle
white cells index (i + 1) mod k adjacent. move along circle, step
either forward (from white cell index cell index (i + 1) mod k) backward (from
white cell index cell index (i 1) mod k) may set content current cell.
Formally, define search graph, denoted GL , PLS instance given L follows:
state (or node) GL pair (, p), p {0, . . . k 1} indicates index current
white cell, : {0, . . . , k 1} {0, . . . , n} function representing current assignment
values white cells (we adopt convention (j) = 0 means white cell index j
filled). directed link (or edge) state (, p) state (, q) search
graph GL q = (p 1) mod k, (q)6= 0, (j) = (j) j 6= q. words,
link state (, p) state (, q) represents step consisting moving white
cell index p white cell index q, setting value (q) white cell index q.
Figure 8 illustrates links one state another GL . cost every link GL unit.
Obviously, search graph regular (out-)degree 2n.
starting state (0 , 0) 0 (j) = 0 j. goal state (or solution) form
( , p), assignment corresponding completion L, p {0, . . . , k 1}. So,
solution cover tree GL path search graph GL starting state
goal state, length optimal solution equal k. show number
-optimal solutions cover tree GL large.
Lemma 7.1. Let L n n partial Latin square k white cells. Let assignment
corresponding completion L. 0 < k, number paths
length k + GL

starting state goal state form ( , ) 2 + 2 + k+t
nt .

Proof. represent path GL length k + starting state pair hP, ~v i,
P (k + t)-length path circle white cells starting white cell index 0,
~v = (v1 , . . . , vk+t ) sequence values [n] vi value assigned white cell
visited ith step path P . Consider pair hP, ~v represents path GL ending
goal state ( , ). Since (j) 6= 0 j, every white cell must visited non-zero step
P . Let sj > 0 last step white cell index j visited. must
vsj = (j) j {0, . . . , k 1}. Given path P , nt ways assigning values
white cells order eventually obtain assignment . Thus, number (k + t)-length
paths GL starting state goal state ( , ) equal |Pt |nt , Pt set
(k + t)-length paths circle white cells start white cell index 0 visit every
white cell.
remains upper bound |Pt |. Consider path P Pt ; strategy bound number
backward (or forward) steps P . < k, least k 1 white cells visited exactly
711

fiDinh, Dinh, Michel, & Russell

0

k1

5

k2

2

1

4

2

3

3

1
0

k1

5

k2

2

1

4

3

1, v
h

2

3

4



3

2

3

2

2

3

5

1

5

5

2

1

0

k1
k2

5

5
3

p

4
2

p

4

1

1

p1

v

4

3

h+

1, v


1

4

2

3

4

1

1

3

p

v

2

p+1

2

5
3

5

1

Figure 8: links connecting states PLS search graph. label h+1, vi (resp., h1, vi)
links means moving forward (resp., backward) setting value v [n] next white cell.

P . Let w index white cell visited exactly P let step
white cell w visited.
Assume step forward step, i.e., white cell visited step 1 (w 1) mod k.
Let w0 farthest white cell w backward direction visited step s.
Precisely, w0 = (w `) mod k, ` maximal number {0, . . . , k 1} white
cell (w `) mod k visited step s. Let wj = (w0 + j) mod k, j = 0, . . . , k 1. Note
w` = w. set white cells visited first steps {w0 , w1 , . . . , w` }, deleting
steps among first steps P obtain path (w0 , w1 , . . . , w` ) w0 w`
forward direction. white cells w`+1 , . . . , wk1 must visited step step
forward direction white cell w` visited forward
step. Thus, deleting steps P obtain path visiting white cells w0 , w1 , . . . , wk1
forward direction. Let s0 , . . . , sk1 steps P deleted, wj visited
step sj P , 1 s0 < s1 < . . . < sk1 k + t. steps s1 , . . . , sk1 forward steps
(step s0 forward backward). Moreover, number backward steps number
forward steps sj1 sj must equal j = 1, . . . , k 1. Let number
deleted steps s0 sk1 exactly (t )/2 backward steps
s0 sk . shows
+ 1 + (t )/2 = 1 + (t + )/2 + 1 backward steps

P . Note k+t
paths Pt exactly j backward steps. Path P
j
+ 1 backward steps = (and thus sj = sj1 + 1 j = 1, . . . , k 1) every
step 1 s0 sk1 backward. + 1 paths Pt , corresponding
choice s0 {1, . . . , + 1}.
Similarly, step backward step, + 1 forwardsteps P . Also,
+ 1 paths Pt exactly + 1 forward steps, k+t
paths Pt
j
712

fiThe Time Complexity Approximate Heuristics

exactly j forward steps. Hence,

|Pt | 2 + 1 +



X
k+t
j=0

last inequality holds since coefficient

j






2 t+2+t k+t
.


k+t
j



increases j increases j < (k + t)/2.

upper bound Lemma 7.1 achieved = 0. fact, four ways visit
every white cell k steps starting white cell 0: taking either k forward steps k backward
steps one backward step followed k 1 forward steps one forward steps followed k 1
backward steps. number optimal solutions cover tree GL equal 4c, since
c completions initial partial Latin square.
Theorem 7.1. Let L n n partial Latin square k white cells c completions.
0 < < 1, number nodes expanded search GL -accurate heuristic
B(),
B() =

(
2(2n)k + 4ck
2(2n)

k

k < 1 ,

+ 4ck bkc + 2 + bkc

k+bkc
bkc



n

bkc

k 1 .

Proof. Lemma 3.1, number nodes expanded search GL -accurate heuristic
upper-bounded 2(2n)k + kN , N number -optimal solutions cover tree
GL . So, need bound N .
k < 1, N equals number optimal solutions, implies upper bound
2(2n)k + 4ck number expanded nodes .
general case, let ` = bkc. Since k < k, Lemma 7.1,



`
X
k+t
2 t+2+t
nt

t=0
!

X
`
`
X
k+`


2c
(t + 2)n + 2c
tn
`
t=0
t=0


k+`
`
4c(` + 2)n + 4c
`n` .
`

N c



second inequality holds k+t
k+`
`. last inequality obtained

`
P`
P`

`
applying fact t=0 tn 2`n t=0 nt 2n` integers n 2 ` 0,
proved easily induction `. Hence, number nodes expanded




k+`
2(2n)k + 4ck ` + 2 + `
n` .
`

Corollary 7.1. Suppose 0 < < 1. number nodes expanded search GL
-accurate heuristic


k
k
k 3/2 (1 + ) (1 + 1/) nk .

Proof. Theorem 7.1, need upper bound binomial coefficient k+`
large k,
`
` = bkc. Since k ` large, bound binomial coefficient using Stirlings
713

fiDinh, Dinh, Michel, & Russell



n
formula, asserts n! 2n ne . precisely, write n! = 2n
n .


k+`
(k + `)!
=
`
k!`!
p
k+`
2(k + `) k+`
k+`
e
=


`
k
2k ke k 2` e` `

k + ` (k + `)k+`
k+`

.

=
k `
k k ``
2k`


n n
e

n , n 1

Stirlings formula, term k+` /k ` O(1). Since
k+`
k
k
=1+
1+
1 + 2/
`
bkc
k 1



k > 2/, term k + `/ 2k` O(1/ k). remaining term
(k + `)k+`
=
k k ``


k
`

k
`
k
1
k
1+
1+
(1 + ) 1 +
k
`

x

since ` k function g(x) = (1 + k/x) monotonically increases x > 0. Hence,



k !
k+`
1
1
= (1 + )k 1 +
.
`

k
Theorem 7.1, number nodes expanded


k + ` k
B() = 2(2n)k + k 2
n
`
!

k
1
3/2
k
k
= k (1 + ) 1 +
n
.


follows corollary effective branching factor using -accurate

heuristic GL asymptotically (1 + ) (1 + 1/) n , significantly smaller

brute-force branching factor 2n, since (1 + )n (1 + 1/) converge 1 0.
7.2.3 Experiments
Given search model PLS problem described above, provide experimental results
search PLS instances, determined large partial Latin square
single completion. PLS instance experiments, run search different
heuristics form (1 )h given various values [0, 1). emphasize dominance property admissible heuristics, number nodes expanded using admissible
-accurate heuristic strictly larger (1 )h less equal using
heuristic (1 )h . words, heuristic (1 )h worse admissible -accurate
heuristics.
build oracle heuristic (1 )h search graph GL , use information
completion partial Latin square L compute h . Consider partial Latin square
L k white cells, arbitrary state (, p) GL . show compute optimal
714

fiThe Time Complexity Approximate Heuristics

cost h (, p) reach goal state GL state (, p). Let X() set white cells
disagrees completion L, h (, p) equal length shortest
paths cycle starting p visiting every point X(). case
|X() \ {p} | 1 easy handle, shall assume |X() \ {p} | 2 on. particular,
suppose X() \ {p} = {p1 , . . . , p` } ` > 1, pj j th point X() \ {p} visited
moving forward (clockwise) p; see Figure 9. two types paths cycle
starting p visiting every point X() \ {p}: type includes visit p, type
II includes visiting p. Let `1 `2 length shortest paths type type II,
respectively.
(
min {`1 , `2 }
p 6 X() ,

h (, p) =
min {`1 + 2, `2 } p X() .
need compute `1 `2 . Computing `1 straightforward: realized either
moving forward p p` moving backward p p1 .
`1 = min {p` p, p p1 }
def

z = z mod k integer z. compute `2 , consider two options j: option
(a) moving forward p pj moving backward pj pj+1 , option (b) moving
backward p pj+1 moving forward pj+1 pj . Thus,


`2 = min min {pj p, p pj+1 } + pj pj+1 .
1j<`

moving forward
p1

p

p+1

p1

pm

p2

pm1
pj+1

pj

Figure 9: Layout points X().
describe experiments detail. generate six partial Latin squares orders
10 20 following way. Initially, generate several partial Latin squares obtained
near phase transition white cells uniformly distributed within every row column.
instance generated complete Latin square suitably chosen random subsets
cells cleared. candidate partial Latin square solved exhaustive backtracking
search method find completions. subset candidates exactly one completion
retained experiments. partial Latin square L chosen value , record
total number E nodes expanded search graph GL (1 )h heuristic.
Then, Knapsack experiments, effective branching factor calculated E 1/k ,
since optimal solution depth GL equals number white cells L. first purpose
compare effective branching factors obtained experiments upper bound obtained
715

fiDinh, Dinh, Michel, & Russell

theoretical analysis. Recall Theorem 7.1 E B(), case
B() =

(
2(2n)k + 4k
2(2n)

k

k < 1 ,

+ 4k bkc + 2 + bkc

k+bkc
bkc



n

bkc

k 1 .

1/k

Therefore, calculate theoretical upper bound B()
effective branching factor E 1/k .
1/k
deeper comparison, calculate multiplicative gap B() /E 1/k theoretical
bound actual values. empirical results given Tables 7 8, multiplicative
gaps close 1 small k large. Notice given k, upper bounds
B() almost value bkc. multiplicative
gaps sometimes increase decrease. However, multiplicative gaps decrease
bkc decreases, fixed k. upper bounds cases k < 1 much tighter
others (with k) cases k < 1 compute number
-optimal solutions exactly. observe that, fixed , multiplicative gaps decrease k
increases. Finally, experiments show dramatic gap effective branching factors
corresponding brute-force branching factor, equals 2n. fact, instance,
1/k
effective branching factor E 1/k theoretical upper bound B()
approach 1 approaches
0.
experiments Knapsack problem, data partial Latin square problem
support linear dependance log E . particular, one partial Latin square
instances R2 larger 0.9 (the worst one R2 value equal 0.8698). median R2
value partial Latin square instances 0.9304. graph instance median
R2 shown Figure 10.
Partial Latin Square instance median R2
Instance 4
Linear fit

6

log10 E

5
4
3
2
0

0.5

1
1.5
2
Heuristic error

2.5

3
102

Figure 10: Graph log10 E least-squares least-squares linear fit (or Linear fit)
partial Latin square instance median R2 (see data Table 8).
investigate slope least-squares linear fit log E approximates slope
log b hypothesized linear dependence Equation (10). Recall case,
branching factor b = 2n optimal solution depth = k. Figure 11 shows that, every
PLS instance experiment, slope least-squares linear fit log10 E approximates
716

fiThe Time Complexity Approximate Heuristics

k log10 (2n) factor 0.8, i.e., 0.8k log10 (2n). words, experimental results
PLS indicate following relationship:
log10 E 0.8k log10 (2n) + ,

equivalently, E (2n)0.8k .

Thus, empirically, effective branching factor search using heuristic (1)h given
PLS search space approximates (2n)0.8 . dominance property admissible heuristics,
empirical upper bound effective branching factor using admissible
-accurate search space.
Instance #

n

k

1
2
3
4
5
6

10
12
14
16
18
20

44
63
86
113
143
176

Slope linear
fit line
43.3901
73.7527
98.3613
142.7056
179.1665
225.4152

/(k log10 (2n))
0.7580
0.8482
0.7903
0.8390
0.8050
0.7995

Figure 11: Slopes least-squares linear fits log10 E partial Latin square instances.

8. Reduction Depth vs. Branching Factor; Comparison Previous
Work
section compare results obtained Korf et al. (Korf & Reid, 1998; Korf
et al., 2001). mentioned introduction, concluded effect heuristic function
reduce effective depth search rather effective branching factor. Considering
striking qualitative difference findings ours, seems interesting discuss
conclusions apply accurate heuristics.
study b-ary tree search model, above, permit multiple solutions. However,
analysis depends critically following equilibrium assumption:
Equilibrium Assumption: number nodes depth heuristic value exceeding `
bi P (`), P (`) probability h(v) ` v chosen uniformly random among
nodes given depth, limit large depth.
remark equilibrium assumption strong structural requirement, holds
expectation rich class symmetric search spaces. specific, state-transitive
search space,4 Rubiks cube, quantity bi P (`) precisely expected number vertices
depth h(v) ` goal state (or initial state) chosen uniformly random. Korf
et al. (2001) observe equilibrium assumption, one directly control number
expanded
P nodes total weight `, quantity denote E(`): indeed, case
E(`) = i` bi P (` i). hand, consider ratio
P`
P`

bi1 P (` i)
E(`)
i=0 b P (` i)
= P`1
= b Pi=0
b,
(18)
`

i1 P (` i)
E(` 1)
i=0 b P (` 1 i)
i=1 b
conclude E(d) bd1 E(1); thus effective branching factor
q
p

bd1 E(1) b E(1)
4. say search space state-transitive structure search graph independent starting
node. Note Cayley graph property, natural search spaces formed algebraic problems
Rubiks cube 15-puzzle, right choice generators, property.

717

fiDinh, Dinh, Michel, & Russell

optimal solution lies depth d.
difficulty approach even presence mildly accurate heuristic satisfying, example,
h(v) h (v) small, constant, > 0 ,
actual values quantities satisfy
E(1) = E(2) = = E(t) = 0
d. (Even root tree h(root) d.) Observe,
then,
E(d) = 1
p

argument actually results effective branching factor bdd E(d) = b(1)d = b1 ,
yielding reduction branching factor. Indeed, applying technique infer estimates
complexity , even assuming equilibrium
P assumption, appears require control
threshold quantity `0 quantities
b P (`0 i) become non-negligible. course,
equilibrium assumption may well apply heuristics weaker or, example, nonuniform
accuracy.
One perspective issue obtained considering case search b-regular
(non-bipartite, connected) graph G = (V, E) observing selection node uniformly
random nodes given depth, limit large depth is, case, equivalent
selection random node graph. consider mildly accurate heuristic h
which, say, h(v) h (v) small constant , bi P (`) bi Prv [ dist(v, S) `], v
chosen uniformly random graph, set solution nodes, dist(v, S) denotes
length shortest path v node S.
|S| b`
|{v | dist(v, S) `/}|

Pr[dist(v, S) `/] =
v
|V |
|V |

1

b-regular graph, expect relation equation (18) hold past threshold
value `0 logb (|S|/|V |).

Acknowledgments
wish thank anonymous reviewers constructive comments. Author Hang Dinh
supported IU South Bend Faculty Research Grant. Author Laurent Michel supported
NSF grant #0642906. Author Alexander Russell supported NSF grant
#1117426.

718

fiThe Time Complexity Approximate Heuristics

Appendix A: Tables Experimental Results
#

n

Heuristic
error

1

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
5627
5882
167660
211946
772257
1470135
6118255
7154310
7347748

2

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

3

23
23
23
23
23
23
23
23
23

4

Optimal
solution
depth

Search
time
(seconds)

11
11
11
11
11
11
11
11
11

125
101
858
744
1341
1318
2025
1653
1101

10887/200
10887/200
10887/200
10887/200
10887/200
10887/200
10887/200
10887/200

44481
45537
372163
474221
1358735
2508134
3508255
3569052
3857597

9
9
9
9
9
9
9
9
9

622
507
1497
1293
1751
1734
1469
1047
566

7820/157
7820/157
7820/157
7820/157
7820/157
7820/157
7820/157
7820/157

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

94
125
5528
9002
31800
109080
879884
1477032
1636093

6
6
6
6
6
6
6
6
6

6
7
98
105
206
301
707
560
224

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

3696
21847
44166
53464
253321
760792
1975195
2317663
2574876

7
7
7
7
7
7
7
7
7

5

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

23645
30501
72597
308417
968504
1681026
1833872
1833644
2132977

6

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

7

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

Effective
branching


factor
E
2.192473
2.201325
2.985026
3.049315
3.42966
3.636376
4.139674
4.198968
4.209164

log10 E

Linear
fit
log10 E

3.7503
3.7695
5.2244
5.3262
5.8878
6.1674
6.7866
6.8546
6.8662

3.7956
4.2742
4.7529
5.2315
5.7102
6.1888
6.6674
7.1461

3.284459
3.293033
4.158822
4.272327
4.802412
5.140898
5.336203
5.3464
5.392783

4.6482
4.6584
5.5707
5.6760
6.1331
6.3994
6.5451
6.5526
6.5863

4.7018
5.0078
5.3139
5.6199
5.9259
6.2320
6.5380
6.8441

5991/121
5991/121
5991/121
5991/121
5991/121
5991/121
5991/121
5991/121

2.132331
2.236068
4.204955
4.560962
5.628654
6.912326
9.788983
10.671652
10.855121

1.9731
2.0969
3.7426
3.9543
4.5024
5.0377
5.9444
6.1694
6.2138

1.9674
2.5989
3.2304
3.8619
4.4934
5.1248
5.7563
6.3878

86
256
303
258
553
788
957
694
383

6343/154
6343/154
6343/154
6343/154
6343/154
6343/154
6343/154
6343/154

3.233523
4.16786
4.608759
4.73628
5.914977
6.921191
7.93182
8.115082
8.23801

3.5677
4.3394
4.6451
4.7281
5.4037
5.8813
6.2956
6.3651
6.4108

3.7471
4.1489
4.5506
4.9524
5.3541
5.7558
6.1576
6.5593

7
7
7
7
7
7
7
7
7

305
285
429
754
1074
1047
823
585
306

6785/205
6785/205
6785/205
6785/205
6785/205
6785/205
6785/205
6785/205

4.215217
4.371357
4.947855
6.083628
7.164029
7.751179
7.848145
7.848005
8.019382

4.3737
4.4843
4.8609
5.4891
5.9861
6.2256
6.2634
6.2633
6.3290

4.3803
4.6983
5.0163
5.3343
5.6523
5.9703
6.2883
6.6064

1981
12316
21699
26575
131561
395118
1080314
1282206
1482293

6
6
6
6
6
6
6
6
6

46
139
151
131
290
431
547
409
219

5012/148
5012/148
5012/148
5012/148
5012/148
5012/148
5012/148
5012/148

3.543894
4.80557
5.281289
5.462761
7.131615
8.566192
10.129585
10.423006
10.677978

3.2969
4.0905
4.3364
4.4245
5.1191
5.5967
6.0336
6.1080
6.1709

3.4645
3.8677
4.2709
4.6741
5.0773
5.4805
5.8837
6.2869

1834
1956
2039
23275
30974
173886
675468
3440759
3793204

7
7
7
7
7
7
7
7
7

51
43
36
159
138
332
526
984
568

6187/122
6187/122
6187/122
6187/122
6187/122
6187/122
6187/122
6187/122

2.925499
2.952538
2.970119
4.20573
4.380978
5.605434
6.80457
8.586333
8.706789

3.2634
3.2914
3.3094
4.3669
4.4910
5.2403
5.8296
6.5367
6.5790

2.8110
3.3053
3.7996
4.2939
4.7882
5.2825
5.7768
6.2711

R2

0.9395

0.9183

0.9647

0.9710

0.9161

0.9696

0.9436

Table 1: Results Knapsack instances type Strongly Correlated.

719

fiDinh, Dinh, Michel, & Russell

#

n

Heuristic
error

8

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
8299
8741
58455
93500
216413
536713
2569955
4096150
4434697

9

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

10

23
23
23
23
23
23
23
23
23

11

Optimal
solution
depth

Search
time
(seconds)

8
8
8
8
8
8
8
8
8

129
105
335
332
479
558
1066
1027
655

6400/153
6400/153
6400/153
6400/153
6400/153
6400/153
6400/153
6400/153

430
460
5313
9507
11268
88158
790402
2008558
2206805

6
6
6
6
6
6
6
6
6

19
16
84
91
75
229
646
673
334

5835/121
5835/121
5835/121
5835/121
5835/121
5835/121
5835/121
5835/121

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

14162
15321
178178
214332
872080
2128661
3942938
4543001
4924992

9
9
9
9
9
9
9
9
9

192
162
669
574
1052
1306
1379
1118
721

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

315
330
974
22374
26883
199464
783863
2579423
2773773

7
7
7
7
7
7
7
7
7

12

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

1029
1163
1310
3968
14820
75333
363263
1710935
1915195

13

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

14

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

Effective
branching


factor
E
3.089429
3.109533
3.943235
4.181686
4.644195
5.202568
6.327624
6.707288
6.7742

log10 E

Linear
fit
log10 E

3.9190
3.9416
4.7668
4.9708
5.3353
5.7297
6.4099
6.6124
6.6469

3.7753
4.1854
4.5955
5.0056
5.4157
5.8258
6.2359
6.6460

2.747334
2.778388
4.177245
4.602643
4.734867
6.671297
9.615562
11.232611
11.410219

2.6335
2.6628
3.7253
3.9780
4.0518
4.9453
5.8978
6.3029
6.3438

2.3749
2.9177
3.4605
4.0033
4.5461
5.0889
5.6317
6.1745

6762/171
6762/171
6762/171
6762/171
6762/171
6762/171
6762/171
6762/171

2.892252
2.917641
3.832024
3.911497
4.571533
5.048042
5.405911
5.491674
5.541159

4.1511
4.1853
5.2509
5.3311
5.9406
6.3281
6.5958
6.6573
6.6924

4.1618
4.5599
4.9579
5.3560
5.7541
6.1521
6.5502
6.9482

19
15
29
232
195
514
751
880
406

6465/106
6465/106
6465/106
6465/106
6465/106
6465/106
6465/106
6465/106

2.274582
2.289748
2.672619
4.182076
4.293214
5.716412
6.950792
8.240087
8.326044

2.4983
2.5185
2.9886
4.3497
4.4295
5.2999
5.8942
6.4115
6.4431

2.1619
2.7724
3.3830
3.9935
4.6040
5.2146
5.8251
6.4356

5
5
5
5
5
5
5
5
5

35
29
25
50
92
212
380
589
283

5073/106
5073/106
5073/106
5073/106
5073/106
5073/106
5073/106
5073/106

4.003899
4.103136
4.201983
5.244624
6.826053
9.449244
12.943277
17.646017
18.048562

3.0124
3.0656
3.1173
3.5986
4.1708
4.8770
5.5602
6.2332
6.2822

2.5015
2.9880
3.4746
3.9611
4.4477
4.9342
5.4208
5.9073

6701
7084
43514
71911
85427
376321
1441947
1963475
2154280

7
7
7
7
7
7
7
7
7

154
127
379
383
313
573
862
655
324

6072/122
6072/122
6072/122
6072/122
6072/122
6072/122
6072/122
6072/122

3.520395
3.548459
4.598978
4.941148
5.064232
6.259049
7.583154
7.925079
8.030775

3.8261
3.8503
4.6386
4.8568
4.9316
5.5756
6.1589
6.2930
6.3333

3.6957
4.0730
4.4504
4.8277
5.2050
5.5824
5.9597
6.3371

418
3629
7016
8503
51480
178163
550403
668276
784088

5
5
5
5
5
5
5
5
5

15
63
74
62
162
258
352
246
110

4636/140
4636/140
4636/140
4636/140
4636/140
4636/140
4636/140
4636/140

3.343761
5.151781
5.877842
6.108217
8.756443
11.22441
14.064884
14.621475
15.096385

2.6212
3.5598
3.8461
3.9296
4.7116
5.2508
5.7407
5.8250
5.8944

2.8386
3.2949
3.7512
4.2075
4.6637
5.1200
5.5763
6.0326

R2

0.9782

0.9571

0.9461

0.9689

0.9314

0.9646

0.9676

Table 2: Results Knapsack instances type Strongly Correlated.

720

fiThe Time Complexity Approximate Heuristics

#

n

Heuristic
error

15

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

Total
node
expansions
E
15713
17658
126261
172936
511397
809884
814774
814389
1004228

16

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

17

23
23
23
23
23
23
23
23
23

18

Optimal
solution
depth

Search
time
(seconds)

6
6
6
6
6
6
6
6
6

218
184
536
466
647
600
435
291
140

5825/211
5825/211
5825/211
5825/211
5825/211
5825/211
5825/211
5825/211

1851
1870
2504
2551
22976
43228
267829
2798746
7270715

9
9
9
9
9
9
9
9
9

44
36
35
29
113
122
283
842
1104

7275/117
7275/117
7275/117
7275/117
7275/117
7275/117
7275/117
7275/117

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

656
665
711
17143
28608
190546
844063
2558990
2749381

7
7
7
7
7
7
7
7
7

33
26
21
192
194
514
813
895
405

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

683
772
18190
24869
136138
308550
2311528
4805568
5201719

8
8
8
8
8
8
8
8
8

19

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

2854
3140
11500
38170
51667
270043
1107776
2600747
2854529

20

23
23
23
23
23
23
23
23
23

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

158012
505837
589456
700571
682245
682583
682855
682455
906305

h ([n])/m

Effective
branching


factor
E
5.004682
5.102977
7.082907
7.464159
8.942515
9.654663
9.664355
9.663593
10.007034

log10 E

Linear
fit
log10 E

4.1963
4.2469
5.1013
5.2379
5.7088
5.9084
5.9110
5.9108
6.0018

4.3104
4.5868
4.8631
5.1395
5.4159
5.6922
5.9686
6.2450

2.306987
2.309606
2.385756
2.390691
3.052011
3.274048
4.009547
5.203904
5.786254

3.2674
3.2718
3.3986
3.4067
4.3613
4.6358
5.4279
6.4470
6.8616

2.7061
3.1549
3.6038
4.0526
4.5015
4.9503
5.3992
5.8480

6501/102
6501/102
6501/102
6501/102
6501/102
6501/102
6501/102
6501/102

2.525892
2.530814
2.555112
4.025961
4.331527
5.679181
7.024655
8.23073
8.315545

2.8169
2.8228
2.8519
4.2341
4.4565
5.2800
5.9264
6.4081
6.4392

2.3428
2.9162
3.4895
4.0629
4.6363
5.2096
5.7830
6.3564

14
13
114
107
280
323
889
1083
790

6012/164
6012/164
6012/164
6012/164
6012/164
6012/164
6012/164
6012/164

2.261011
2.295896
3.407839
3.543703
4.382757
4.854737
6.244352
6.842552
6.910641

2.8344
2.8876
4.2598
4.3957
5.1340
5.4893
6.3639
6.6817
6.7161

2.7250
3.3052
3.8855
4.4657
5.0459
5.6262
6.2064
6.7866

7
7
7
7
7
7
7
7
7

65
56
121
210
185
412
682
772
415

5503/119
5503/119
5503/119
5503/119
5503/119
5503/119
5503/119
5503/119

3.116279
3.159085
3.802767
4.51369
4.713203
5.969239
7.302863
8.249784
8.360249

3.4555
3.4969
4.0607
4.5817
4.7132
5.4314
6.0445
6.4151
6.4555

3.2041
3.6529
4.1017
4.5505
4.9993
5.4481
5.8969
6.3457

7
7
7
7
7
7
7
7
7

866
1173
965
797
631
484
357
235
123

6592/295
6592/295
6592/295
6592/295
6592/295
6592/295
6592/295
6592/295

5.529298
6.52918
6.673447
6.840134
6.814281
6.814763
6.815151
6.814581
7.096418

5.1987
5.7040
5.7705
5.8455
5.8339
5.8342
5.8343
5.8341
5.9573

5.5119
5.5748
5.6376
5.7005
5.7633
5.8262
5.8890
5.9518

R2

0.8788

0.8698

0.9498

0.9729

0.9770

0.4860

Table 3: Results Knapsack instances type Strongly Correlated.

721

fiDinh, Dinh, Michel, & Russell

Linear fit
log10 E
5.8687
5.8803
5.8919
5.9036
5.9152
5.9268
5.9384
5.9500

#

n

Total
node
expansions E
731425
761013
782339
805295
828252
845545
865626
885943
900630

Optimal
soln. depth
11
15
12
12
12
10
11
14
13

Search time,
seconds
1090
878
716
579
463
360
267
179
80

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

1

5509/28
5509/28
5509/28
5509/28
5509/28
5509/28
5509/28
5509/28

5.8642
5.8814
5.8934
5.9060
5.9182
5.9271
5.9373
5.9474
5.9545

2

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

67164
71824
76627
80614
84553
90166
96506
99536
104144

6
9
7
8
8
9
7
7
8

259
208
168
136
107
82
58
35
10

2984/28
2984/28
2984/28
2984/28
2984/28
2984/28
2984/28
2984/28

4.8271
4.8563
4.8844
4.9064
4.9271
4.9550
4.9846
4.9980
5.0176

4.8311
4.8558
4.8804
4.9050
4.9297
4.9543
4.9790
5.0036

3

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

222293
232989
244871
256250
266235
274056
279890
283160
291239

11
12
8
9
9
8
11
9
9

533
432
353
285
226
173
126
81
28

3687/26
3687/26
3687/26
3687/26
3687/26
3687/26
3687/26
3687/26

5.3469
5.3673
5.3889
5.4087
5.4253
5.4378
5.4470
5.4520
5.4642

5.3552
5.3706
5.3861
5.4015
5.4170
5.4324
5.4479
5.4633

4

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

290608
304974
313598
323477
331235
336665
340874
342644
360837

10
10
9
9
9
10
9
9
8

329
272
225
185
151
121
92
64
33

3883/56
3883/56
3883/56
3883/56
3883/56
3883/56
3883/56
3883/56

5.4633
5.4843
5.4964
5.5098
5.5201
5.5272
5.5326
5.5348
5.5573

5.4734
5.4834
5.4935
5.5035
5.5136
5.5237
5.5337
5.5438

5

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

851515
873968
893378
912734
927408
940724
950209
958343
967863

11
14
12
14
13
12
12
13
12

740
609
498
410
335
267
206
142
88

7731/77
7731/77
7731/77
7731/77
7731/77
7731/77
7731/77
7731/77

5.9302
5.9415
5.9510
5.9603
5.9673
5.9735
5.9778
5.9815
5.9858

5.9348
5.9421
5.9494
5.9567
5.9641
5.9714
5.9787
5.9860

6

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

75858
81410
88494
94585
100329
106409
110656
114601
117496

10
8
6
9
7
5
9
9
4

488
363
287
225
177
134
94
55
11

2327/11
2327/11
2327/11
2327/11
2327/11
2327/11
2327/11
2327/11

4.8800
4.9107
4.9469
4.9758
5.0014
5.0270
5.0440
5.0592
5.0700

4.8895
4.9155
4.9416
4.9676
4.9936
5.0197
5.0457
5.0717

7

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

712138
748095
778565
799378
823236
844925
870175
897407
909075

11
12
15
11
13
13
13
12
14

1178
947
765
618
490
378
280
185
80

6456/33
6456/33
6456/33
6456/33
6456/33
6456/33
6456/33
6456/33

5.8526
5.8740
5.8913
5.9028
5.9155
5.9268
5.9396
5.9530
5.9586

5.8590
5.8727
5.8864
5.9001
5.9138
5.9275
5.9412
5.9549

R2

0.9918

0.9959

0.9649

0.9369

0.9687

0.9833

0.9895

Table 4: Results Knapsack instances type Subset Sum.

722

fiThe Time Complexity Approximate Heuristics

Linear fit
log10 E
5.4113
5.4416
5.4719
5.5023
5.5326
5.5629
5.5932
5.6236

#

n

Total
node
expansions E
252054
279643
299328
324182
340530
361756
385942
423848
454094

Optimal
soln. depth
10
12
9
11
9
10
10
9
9

Search time,
seconds
2274
1607
1159
878
666
494
344
201
42

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

8

3514/7
3514/7
3514/7
3514/7
3514/7
3514/7
3514/7
3514/7

5.4015
5.4466
5.4761
5.5108
5.5322
5.5584
5.5865
5.6272
5.6571

9

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

284146
301301
318308
330924
338590
345335
351027
356374
369094

9
8
7
9
9
9
10
10
8

628
507
412
334
263
203
146
92
34

4494/34
4494/34
4494/34
4494/34
4494/34
4494/34
4494/34
4494/34

5.4535
5.4790
5.5028
5.5197
5.5297
5.5382
5.5453
5.5519
5.5671

5.4677
5.4812
5.4947
5.5083
5.5218
5.5353
5.5489
5.5624

10

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

812828
852539
881657
903389
923450
941277
954861
970871
985526

11
13
15
12
15
11
14
14
12

1078
874
711
579
466
356
266
180
88

6963/39
6963/39
6963/39
6963/39
6963/39
6963/39
6963/39
6963/39

5.9100
5.9307
5.9453
5.9559
5.9654
5.9737
5.9799
5.9872
5.9937

5.9193
5.9298
5.9403
5.9508
5.9613
5.9717
5.9822
5.9927

11

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

872387
892404
907719
920529
930373
939495
945766
948094
961185

12
13
12
12
12
13
12
11
11

527
441
366
306
260
214
169
125
85

7270/102
7270/102
7270/102
7270/102
7270/102
7270/102
7270/102
7270/102

5.9407
5.9506
5.9580
5.9640
5.9687
5.9729
5.9758
5.9769
5.9828

5.9456
5.9507
5.9558
5.9609
5.9660
5.9711
5.9762
5.9813

12

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

544749
572592
596732
622826
644836
662145
682257
705866
720827

13
8
12
9
11
12
11
11
13

997
804
656
528
420
329
242
158
64

5752/35
5752/35
5752/35
5752/35
5752/35
5752/35
5752/35
5752/35

5.7362
5.7578
5.7758
5.7944
5.8094
5.8210
5.8339
5.8487
5.8578

5.7422
5.7579
5.7736
5.7893
5.8050
5.8207
5.8364
5.8521

13

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

592766
628513
662306
684651
713728
745263
781953
824260
861415

10
10
11
13
12
10
11
11
11

1824
1319
1040
828
645
487
344
216
74

7445/30
7445/30
7445/30
7445/30
7445/30
7445/30
7445/30
7445/30

5.7729
5.7983
5.8211
5.8355
5.8535
5.8723
5.8932
5.9161
5.9352

5.7767
5.7963
5.8159
5.8355
5.8552
5.8748
5.8944
5.9140

14

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

137368
148933
157793
165368
172383
179983
186068
191426
197634

8
7
10
9
9
7
9
8
10

561
450
363
289
226
173
123
73
18

3509/22
3509/22
3509/22
3509/22
3509/22
3509/22
3509/22
3509/22

5.1379
5.1730
5.1981
5.2185
5.2365
5.2552
5.2697
5.2820
5.2959

5.1513
5.1713
5.1913
5.2113
5.2314
5.2514
5.2714
5.2914

R2

0.9925

0.9282

0.9593

0.9409

0.9901

0.9963

0.9761

Table 5: Results Knapsack instances type Subset Sum.

723

fiDinh, Dinh, Michel, & Russell

Linear fit
log10 E
4.5311
4.5760
4.6209
4.6658
4.7107
4.7556
4.8006
4.8455

#

n

Total
node
expansions E
34937
38617
41757
45036
49231
54428
62409
75602
84284

Optimal
soln. depth
9
6
10
9
10
7
9
7
8

Search time,
seconds
1022
772
529
353
272
186
128
72
8

log10 E

20
20
20
20
20
20
20
20
20

Heuristic
error
0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

h ([n])/m

15

3124/9
3124/9
3124/9
3124/9
3124/9
3124/9
3124/9
3124/9

4.5433
4.5868
4.6207
4.6536
4.6922
4.7358
4.7952
4.8785
4.9257

16

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

476547
498939
523867
558927
592373
626403
668497
725325
768536

10
11
10
10
9
10
12
12
13

3224
2097
1536
1181
911
675
468
281
71

5442/11
5442/11
5442/11
5442/11
5442/11
5442/11
5442/11
5442/11

5.6781
5.6980
5.7192
5.7474
5.7726
5.7969
5.8251
5.8605
5.8857

5.6718
5.6976
5.7235
5.7493
5.7751
5.8010
5.8268
5.8527

17

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

641544
666837
702032
737893
772405
810089
852271
902227
964897

15
11
12
14
14
14
14
12
14

3751
2791
1991
1495
1124
827
570
337
86

7157/11
7157/11
7157/11
7157/11
7157/11
7157/11
7157/11
7157/11

5.8072
5.8240
5.8464
5.8680
5.8878
5.9085
5.9306
5.9553
5.9845

5.8045
5.8256
5.8468
5.8679
5.8891
5.9102
5.9313
5.9525

18

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

321490
338267
358571
379827
399061
419052
443204
486366
508524

9
10
9
10
9
10
9
10
10

1215
952
760
600
466
356
252
157
47

4631/20
4631/20
4631/20
4631/20
4631/20
4631/20
4631/20
4631/20

5.5072
5.5293
5.5546
5.5796
5.6010
5.6223
5.6466
5.6870
5.7063

5.5047
5.5293
5.5540
5.5786
5.6033
5.6279
5.6525
5.6772

19

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

104698
110845
116893
122710
128398
131887
133658
134205
142348

7
8
10
8
6
9
10
5
8

251
206
169
137
110
84
60
37
13

3373/44
3373/44
3373/44
3373/44
3373/44
3373/44
3373/44
3373/44

5.0199
5.0447
5.0678
5.0889
5.1086
5.1202
5.1260
5.1278
5.1534

5.0322
5.0482
5.0641
5.0800
5.0959
5.1119
5.1278
5.1437

20

20
20
20
20
20
20
20
20
20

0.5
0.5625
0.625
0.6875
0.75
0.8125
0.875
0.9375
BFS

275501
286961
296924
305914
315286
322234
324077
324471
348398

10
9
9
7
9
8
9
10
9

352
292
240
196
159
126
94
65
32

5262/94
5262/94
5262/94
5262/94
5262/94
5262/94
5262/94
5262/94

5.4401
5.4578
5.4726
5.4856
5.4987
5.5082
5.5106
5.5112
5.5421

5.4489
5.4594
5.4699
5.4804
5.4909
5.5013
5.5118
5.5223

R2

0.9739

0.9947

0.9988

0.9932

0.9349

0.9299

Table 6: Results Knapsack instances type Subset Sum.

724

fiThe Time Complexity Approximate Heuristics

Effective
branching
factor
E 1/k
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.10682761
1.11793532
1.12483883
1.13029527
1.13481129
1.13744262
1.13983570
1.14203051
1.14306342
1.14405770
1.15327789
1.19493326
1.20344942
1.21724042
1.22842928
1.23335496
1.24222170
1.24615895
1.24988516
1.25689894
1.26697571
1.28154406
1.28838000
1.29446689
1.29905304
1.30420852
1.30895150
1.31267920
1.31682768
1.32748452
1.34592652

Upper
bound
B(d)1/k

B(d)1/k

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475
0.05
0.0525
0.055
0.0575
0.06
0.0625
0.065
0.0675
0.07
0.0725
0.075
0.0775
0.08
0.0825
0.085
0.0875
0.09
0.0925
0.095
0.0975

Total
node
expansions
E
87
87
87
87
87
87
87
87
87
87
135
177
219
261
289
317
345
359
373
531
2530
3458
5709
8539
10183
13956
16041
18293
23400
33251
54989
69492
85507
99904
118924
139520
158117
181666
258998
475269

1.12498287
1.12509476
1.12524953
1.12546320
1.12575740
1.12616102
1.12671203
1.12745936
1.12846421
1.12980027
1.29413023
1.29413756
1.29414775
1.29416190
1.29418158
1.29420890
1.29424685
1.29429954
1.29437264
1.48549510
1.48549548
1.48549601
1.48549674
1.48549775
1.48549917
1.48550113
1.48550386
1.48550766
1.68167021
1.68167024
1.68167029
1.68167036
1.68167046
1.68167059
1.68167077
1.68167103
1.68167138
1.88726770
1.88726771
1.88726771

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475
0.05
0.0525
0.055
0.0575
0.06
0.0625
0.065
0.0675
0.07

125
125
125
125
125
125
125
295
599
789
979
1093
1207
1759
8006
18159
31829
39898
53605
63934
151470
240217
418262
569663
823942
1.03E+06
1.39E+06
3.35E+06
6.43E+06

1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.07965322
1.09446915
1.10684330
1.11169422
1.11550813
1.11746021
1.11922136
1.12593198
1.15334431
1.16843520
1.17889026
1.18312592
1.18868491
1.19201428
1.20844644
1.21732463
1.22808758
1.23412462
1.24137536
1.24580697
1.25172483
1.26929396
1.28251719

1.09187259
1.09196102
1.09210593
1.09234240
1.09272569
1.09334029
1.09430956
1.21404534
1.21404945
1.21405622
1.21406738
1.21408579
1.21411611
1.34843434
1.34843446
1.34843466
1.34843500
1.34843555
1.34843647
1.34843797
1.48271141
1.48271142
1.48271144
1.48271147
1.48271152
1.48271161
1.62031036
1.62031036
1.62031037

#

n

k

Heuristic
error

1

10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10
10

44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44
44

2

12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12
12

63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63
63

log10 E

Linear
fit
log10 E

1.01640297
1.01650406
1.01664390
1.01683694
1.01710275
1.01746741
1.01796524
1.01864044
1.01954830
1.02075541
1.15760743
1.15050932
1.14496431
1.14042036
1.13779944
1.13543461
1.13328571
1.13230772
1.13138755
1.28806345
1.24316189
1.23436514
1.22038072
1.20926599
1.20443766
1.19584220
1.19206612
1.18851532
1.33795181
1.32731056
1.31222199
1.30525960
1.29912203
1.29453574
1.28941863
1.28474663
1.28109852
1.43319261
1.42168717
1.40220709

1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
1.9395
2.1303
2.2480
2.3404
2.4166
2.4609
2.5011
2.5378
2.5551
2.5717
2.7251
3.4031
3.5388
3.7566
3.9314
4.0079
4.1448
4.2052
4.2623
4.3692
4.5218
4.7403
4.8419
4.9320
4.9996
5.0753
5.1446
5.1990
5.2593
5.4133
5.6769

1.2674
1.3758
1.4843
1.5928
1.7013
1.8097
1.9182
2.0267
2.1352
2.2436
2.3521
2.4606
2.5691
2.6775
2.7860
2.8945
3.0030
3.1114
3.2199
3.3284
3.4369
3.5454
3.6538
3.7623
3.8708
3.9793
4.0877
4.1962
4.3047
4.4132
4.5216
4.6301
4.7386
4.8471
4.9555
5.0640
5.1725
5.2810
5.3894
5.4979

1.01131786
1.01139977
1.01153399
1.01175301
1.01210802
1.01267728
1.01357504
1.10925497
1.09685756
1.09207747
1.08835368
1.08646892
1.08478640
1.19761616
1.16915170
1.15405173
1.14381723
1.13972277
1.13439352
1.13122636
1.22695666
1.21800824
1.20733363
1.20142768
1.19441030
1.19016159
1.29446211
1.27654461
1.26338296

2.0969
2.0969
2.0969
2.0969
2.0969
2.0969
2.0969
2.4698
2.7774
2.8971
2.9908
3.0386
3.0817
3.2453
3.9034
4.2591
4.5028
4.6010
4.7292
4.8057
5.1803
5.3806
5.6214
5.7556
5.9159
6.0134
6.1431
6.5244
6.8080

1.3953
1.5797
1.7641
1.9485
2.1328
2.3172
2.5016
2.6860
2.8704
3.0547
3.2391
3.4235
3.6079
3.7923
3.9767
4.1610
4.3454
4.5298
4.7142
4.8986
5.0829
5.2673
5.4517
5.6361
5.8205
6.0049
6.1892
6.3736
6.5580

E 1/k

R2

0.9482

0.9693

Table 7: Results partial Latin square instances.

725

fiDinh, Dinh, Michel, & Russell

Effective
branching
factor
E 1/k
1.06161017
1.06161017
1.06161017
1.06161017
1.06161017
1.06615920
1.07302532
1.07624311
1.07854600
1.07979831
1.10835983
1.12621485
1.13582148
1.14198131
1.14598774
1.15596951
1.16381138
1.16872905
1.17320907
1.17776024

Upper
bound
B(d)1/k

B(d)1/k

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03
0.0325
0.035
0.0375
0.04
0.0425
0.045
0.0475

Total
node
expansions
E
171
171
171
171
171
247
429
555
667
737
6959
27506
57104
90923
122879
259053
463344
665871
925306
1.29E+06

1.07034588
1.07042098
1.07057335
1.07087962
1.07148429
1.16291112
1.16291347
1.16291827
1.16292811
1.16294821
1.26274158
1.26274166
1.26274182
1.26274214
1.36083647
1.36083648
1.36083648
1.36083649
1.36083651
1.45985179

113
113
113
113
113
113
113
113
113
113
113
113
113

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02
0.0225
0.025
0.0275
0.03

225
225
225
225
799
1719
2317
2731
50236
144797
258735
516942
1.97E+06

1.04909731
1.04909731
1.04909731
1.04909731
1.06092884
1.06814635
1.07097198
1.07253118
1.10053004
1.11088842
1.11660964
1.12346988
1.13686805

18
18
18
18
18
18
18
18
18

143
143
143
143
143
143
143
143
143

0
0.0025
0.005
0.0075
0.01
0.0125
0.015
0.0175
0.02

285
285
285
743
2579
3659
39137
246338
535932

20
20
20
20
20
20
20

176
176
176
176
176
176
176

0
0.0025
0.005
0.0075
0.01
0.0125
0.015

351
351
351
2425
4125
107153
619190

#

n

k

Heuristic
error

3

14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14
14

86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86
86

4

16
16
16
16
16
16
16
16
16
16
16
16
16

5

6

log10 E

Linear
fit
log10 E

1.00822873
1.00829948
1.00844300
1.00873150
1.00930108
1.09074810
1.08377077
1.08053493
1.07823691
1.07700503
1.13928848
1.12122626
1.11174321
1.10574676
1.18747908
1.17722523
1.16929298
1.16437295
1.15992669
1.23951526

2.2330
2.2330
2.2330
2.2330
2.2330
2.3927
2.6325
2.7443
2.8241
2.8675
3.8425
4.4394
4.7567
4.9587
5.0895
5.4134
5.6659
5.8234
5.9663
6.1109

1.4986
1.7445
1.9904
2.2363
2.4822
2.7281
2.9740
3.2199
3.4658
3.7117
3.9576
4.2035
4.4494
4.6953
4.9412
5.1871
5.4330
5.6789
5.9248
6.1707

1.05563497
1.05570312
1.05588217
1.05634285
1.12838087
1.12838284
1.12838808
1.12840202
1.20572650
1.20572656
1.20572671
1.28088203
1.28088203

1.00623170
1.00629666
1.00646733
1.00690645
1.06357828
1.05639348
1.05361121
1.05209251
1.09558708
1.08537144
1.07981041
1.14011248
1.12667607

2.3522
2.3522
2.3522
2.3522
2.9025
3.2353
3.3649
3.4363
4.7010
5.1608
5.4129
5.7134
6.2952

1.6772
2.0340
2.3907
2.7475
3.1042
3.4610
3.8178
4.1745
4.5313
4.8881
5.2448
5.6016
5.9584

1.04031952
1.04031952
1.04031952
1.04731385
1.05646789
1.05905525
1.07675277
1.09069423
1.09663904

1.04542550
1.04549145
1.04572413
1.10463010
1.10463134
1.10463580
1.16693654
1.16693656
1.16693665

1.00490809
1.00497148
1.00519515
1.05472691
1.04558912
1.04303887
1.08375532
1.06990257
1.06410278

2.4548
2.4548
2.4548
2.8710
3.4115
3.5634
4.5926
5.3915
5.7291

1.8665
2.3144
2.7623
3.2103
3.6582
4.1061
4.5540
5.0019
5.4498

1.03386057
1.03386057
1.03386057
1.04527681
1.04843662
1.06802045
1.07871841

1.03797380
1.03804139
1.03837263
1.08739144
1.08739402
1.13899860
1.13899862

1.00397851
1.00404389
1.00436428
1.04029040
1.03715761
1.06645766
1.05588132

2.5453
2.5453
2.5453
3.3847
3.6154
5.0300
5.7918

1.9462
2.5098
3.0733
3.6368
4.2004
4.7639
5.3275

E 1/k

R2

0.9335

0.9274

0.9120

0.8698

Table 8: Results partial Latin square instances.

726

fiThe Time Complexity Approximate Heuristics

References
Aaronson, S. (2004). Lower bounds local search quantum arguments. Proceedings
36th Annual ACM Symposium Theory Computing (STOC). ACM Press.
Babai, L. (1991). Local expansion vertex-transitive graphs random generation finite groups.
Proceedings 23rd annual ACM symposium Symposium Theory Computing,
pp. 164174.
Chernoff, H. (1952). measure asymptotic efficiency tests hypothesis based sum
observations. Annals Mathematical Statistics, 23, 493507.
Chung, F. (2006). diameter Laplacian eigenvalues directed graphs. Electronic Journal
Combinatorics, 13 (4).
Chung, F. R. K. (1997). Spectral Graph Theory. American Mathematical Society.
Colbourn, C. J. (1984). complexity completing partial Latin squares. Discrete Applied
Mathematics, 8 (1), 2530.
Davis, H., Bramanti-Gregor, A., & Wang, J. (1988). advantages using depth breadth
components heuristic search. Ras, Z., & Saitta, L. (Eds.), Proceedings Third
International Symposium Methodologies Intelligent Systems, pp. 1928, North-Holland,
Amsterdam. Elsevier.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies optimality A*. J.
ACM, 32 (3), 505536.
Demaine, E. D. (2001). Playing games algorithms: Algorithmic combinatorial game theory.
Proc. 26th Symp. Math Found. Comp. Sci., Lect. Notes Comp. Sci., pp. 1832.
Springer-Verlag.
Dinh, H., Russell, A., & Su, Y. (2007). value good advice: complexity A*
accurate heuristics. Proceedings Twenty-Second Conference Artificial Intelligence
(AAAI-07), pp. 11401145.
Edelkamp, S. (2001). Prediction regular search tree growth spectral analysis. Proceedings
Joint German/Austrian Conference AI: Advances Artificial Intelligence, KI 01,
pp. 154168, London, UK, UK. Springer-Verlag.
Felner, A., Korf, R. E., & Hanan, S. (2004). Additive pattern database heuristics. Journal
Artificial Intelligence Research, 22, 279318.
Friedman, J. (2003). proof Alons second eigenvalue conjecture. STOC 03: Proceedings
thirty-fifth annual ACM symposium Theory computing, pp. 720724, New York, NY,
USA. ACM.
Gaschnig, J. (1979). Perfomance measurement analysis certain search algorithms. Ph.D.
thesis, Carnegie-Mellon University, Pittsburgh, PA.
Gomes, C., & Shmoys, D. (2002). Completing quasigroups Latin squares: structured graph
coloring problem. Johnson, D. S., Mehrotra, A., & Trick, M. (Eds.), Proceedings
Computational Symposium Graph Coloring Generalizations, pp. 2239, Ithaca, New
York, USA.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search randomization. AAAI 98/IAAI 98: Proceedings fifteenth national/tenth conference Artificial intelligence/Innovative applications artificial intelligence, pp. 431437, Menlo Park,
CA, USA. American Association Artificial Intelligence.
Hart, P., Nilson, N., & Raphael, B. (1968). formal basis heuristic determination minimum
cost paths. IEEE Transactions Systems Science Cybernetics, SCC-4 (2), 100107.
727

fiDinh, Dinh, Michel, & Russell

Helmert, M., & Roger, G. (2008). good almost perfect?. Proceedings AAAI-08.
Hochbaum, D. (1996). Approximation Algorithms NP-hard Problems. Brooks Cole.
Horn, R., & Johnson, C. (1999). Matrix Analysis. Cambridge University Press, Cambridge, UK.
Huyn, N., Dechter, R., & Pearl, J. (1980). Probabilistic analysis complexity A*. Artificial
Intelligence, 15, 241254.
Ibarra, O. H., & Kim, C. E. (1975). Fast approximation algorithms knapsack sum
subset problems. Journal ACM, 22 (4), 463468.
Karp, R. M. (1972). Reducibility among combinatorial problems. Miller, R. E., & Thatcher,
J. W. (Eds.), Complexity Computer Computations, p. 85103. New York: Plenum.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack problems. Springer.
Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. Artificial
Intelligence, 27, 97109.
Korf, R., & Reid, M. (1998). Complexity analysis admissible heuristic search. Proceedings
National Conference Artificial Intelligence (AAAI-98), pp. 305310.
Korf, R., Reid, M., & Edelkamp, S. (2001). Time complexity iterative-deepening-A*. Artificial
Intelligence, 129 (1-2), 199218.
Korf, R. E. (2000). Recent progress design analysis admissible heuristic functions.
AAAI/IAAI 2000, pp. 11651170. SARA 02: Proceedings 4th International
Symposium Abstraction, Reformulation, Approximation.
Kumar, R., Russell, A., & Sundaram, R. (1996). Approximating Latin square extensions. COCOON 96: Proceedings Second Annual International Conference Computing
Combinatorics, pp. 280289, London, UK. Springer-Verlag.
Laywine, C., & Mullen, G. (1998). Discrete Mathematics using Latin Squares. Interscience Series
Discrete mathematics Optimization. Wiley.
Lenstra, A. K., Lenstra, H. W., & Lovasz, L. (1981). Factoring polynomials rational coefficients.
Tech. rep. 82-05, Universiteit Amsterdam.
Motwani, R., & Raghavan, P. (1995). Randomized Algorithms. Cambridge University Press.
Parberry, I. (1995). real-time algorithm (n2 1)-puzzle. Inf. Process. Lett, 56, 2328.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley, MA.
Pisinger, D. (2005). hard knapsack problems?. Computers Operations Research,
32, 22712284.
Pohl, I. (1977). Practical theoretical considerations heuristic search algorithms. Elcock,
W., & Michie, D. (Eds.), Machine Intelligence, Vol. 8, pp. 5572. Ellis Horwood, Chichester.
Ratner, D., & Warmuth, M. (1990). (n2 1)-puzzle related relocation problems. Journal
Symbolic Computation, 10 (2), 111137.
Russell, S., & Norvig, P. (1995). Artificial Intelligence - Modern Approach. Prentice Hall, New
Jersey.
Sen, A. K., Bagchi, A., & Zhang, W. (2004). Average-case analysis best-first search two
representative directed acyclic graphs. Artif. Intell., 155 (1-2), 183206.
Tay, T.-S. (1996). results generalized Latin squares. Graphs Combinatorics, 12, 199
207.
Vazirani, V. (2001). Approximation Algorithms. Springer-Verlag.
728

fiThe Time Complexity Approximate Heuristics

Vazirani, V. (2002). Primal-dual schema based approximation algorithms. Theoretical Aspects
Computer Science: Advanced Lectures, pp. 198207. Springer-Verlag, New York.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. (2007). Inconsistent heuristics. Proceedings
AAAI-07, pp. 12111216.
Zhang, Z., Sturtevant, N. R., Holte, R., Schaeffer, J., & Felner, A. (2009). A* search inconsistent
heuristics. Proceedings 21st international jont conference Artifical intelligence,
IJCAI09, pp. 634639, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.

729


