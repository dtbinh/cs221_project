Journal Artificial Intelligence Research 2 (1995) 475-500

Submitted 10/94; published 5/95

Adaptive Load Balancing: Study Multi-Agent
Learning
Andrea Schaerf

aschaerf@dis.uniroma1.it

Dipartimento di Informatica e Sistemistica
Universita di Roma \La Sapienza", Via Salaria 113, I-00198 Roma, Italy

Yoav Shoham

Robotics Laboratory, Computer Science Department
Stanford University, Stanford, CA 94305, USA

Moshe Tennenholtz

Faculty Industrial Engineering Management
Technion, Haifa 32000, Israel

shoham@flamingo.stanford.edu
moshet@ie.technion.ac.il

Abstract
study process multi-agent reinforcement learning context load balancing distributed system, without use either central coordination explicit communication. first define precise framework study adaptive load balancing,
important features stochastic nature purely local information
available individual agents. Given framework, show illuminating results
interplay basic adaptive behavior parameters effect system eciency.
investigate properties adaptive load balancing heterogeneous populations,
address issue exploration vs. exploitation context. Finally, show
naive use communication may improve, might even harm system eciency.

1. Introduction
article investigates multi-agent reinforcement learning context concrete
problem undisputed importance { load balancing. Real life provides us many examples emergent, uncoordinated load balancing: trac alternative highways tends
even time; members computer science department tend use powerful networked workstations, eventually find lower load machines
inviting; on. would understand dynamics emergent
load-balancing systems apply lesson design multi-agent systems.
define formal yet concrete framework study issues, called multiagent multi-resource stochastic system, involves set agents, set resources,
probabilistically changing resource capacities, probabilistic assignment new jobs agents,
probabilistic job sizes. agent must select resource new job,
eciency resource handles job depends capacity resource
lifetime job well number jobs handled resource
period time. performance measure system aims globally optimizing
resource usage system ensuring fairness (that is, system shouldn't made
ecient expense particular agent), two common criteria load balancing.
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiSchaerf, Shoham, & Tennenholtz

agent choose appropriate resource order optimize measures?
make important assumption, spirit reinforcement learning (Sutton,
1992): information available agent prior experience. particular,
agent necessarily know past, present, future capacities resources,1
unaware past, current, future jobs submitted various agents, even
relevant probability distributions. goal agent thus adapt resourceselection behavior behavior agents well changing capacities
resources changing load, without explicitly knowing are.
interested several basic questions:

good resource-selection rules?
fact different agents may use different resource-selection rules affect
system behavior?

communication among agents improve system eciency?
following sections show illuminating answers questions. contribution paper therefore twofold. apply multi-agent reinforcement learning
domain adaptive load balancing use basic domain order demonstrate
basic phenomena multi-agent reinforcement learning.
structure paper follows. Section 2 discuss general setting.
objective section motivate study point impact. formal
framework defined discussed Section 3. Section 4 completes discussion
framework introducing resource selection rule parameters, function
\control knobs" adaptive process. Section 5 present experimental results
adaptive behavior within framework show various parameters affect
eciency adaptive behavior. case heterogeneous populations investigated
Section 6, case communicating populations discussed Section 7. Section 8
discuss impact results. Section 9 put work perspective
related work. Finally, Section 10 conclude brief summary.

2. General Setting

paper applies reinforcement learning domain adaptive load balancing. However, presenting model use detailed study, need clarify several
points general setting. particular, need explain interpretation
reinforcement learning interpretation load balancing adopt.
Much work devoted recent years distributed adaptive load balancing. One find related work field distributed computer systems (e.g., Pulidas,
Towsley, & Stankovic, 1988; Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993;
Glockner & Pasquale, 1993; Mirchandaney, Towsley, & Stankovic, 1989; Zhou, 1988; Eager,
Lazowska, & Zahorjan, 1986), organization theory management science (e.g., Malone,
1. many applications capacities resources known, least extent. point
discussed later. Basically, paper wish investigate far one go using purely
local feedback without use global information (Kaelbling, 1993; Sutton, 1992).

476

fiAdaptive Load Balancing: Study Multi-Agent Learning

1987), distributed AI (e.g., Bond & Gasser, 1988). Although motivations
above-mentioned lines research similar, settings discussed essential
differences.
Work distributed computer systems adopts view set computers
controls certain resources, autonomous decision-making capability, jobs
arrive dynamic fashion. decision-making agents different computers
(also called nodes) try share system load coordinate activities means
communication. actual action performed, based information received
computers, may controlled various ways. One ways adopted control
related decisions learning automata (Narendra & Thathachar, 1989).
above-mentioned work agent associated set resources,
agent related resources associated node distributed system.
Much work management science distributed AI adopts somewhat complementary
view. difference classical work distributed operating systems, agent
associated set resources controls. agents autonomous entities
negotiate among (Zlotkin & Rosenschein, 1993; Kraus & Wilkenfeld, 1991)
use shared resources. Alternatively, agents (called managers case) may
negotiate task executed processors may execute (Malone, 1987).
model adopt avor models used distributed AI organization
theory. assume strict separation agents resources. Jobs arrive agents
make decisions execute them. resources passive (i.e.,
make decisions). typical example setting computerized framework set
PCs, controlled different user submits jobs executed
one several workstations. workstations assumed independent
shared among users. example real-life situation motivated
study terminology adopt taken framework. However,
real-life situations related model areas different classical distributed
computer systems.
canonical problem related model following one (Arthur, 1994): agent,
embedded multi-agent system, select among set bars (or set restaurants).
agent makes autonomous decision performance bar (and therefore
agents use it) function capacity number agents use it.
decision going bar stochastic process decision bar use
autonomous decision respective agent. similar situation arises product
manager decides processor use order perform particular task. model
present Section 3 general model situations investigated.
situations job arrives agent (rather node consisting particular resources)
decides upon resource (e.g., restaurant) job executed;
a-priori association agents resources.
discuss way agents behave framework. common theme
among above-mentioned lines research load-balancing achieved means
communication among active agents active resources (through related decisionmaking agents). study adopt complementary view. consider agents
act purely local fashion, based purely local information described recent
reinforcement learning literature. mentioned, learning automata used
477

fiSchaerf, Shoham, & Tennenholtz

field distributed computer systems order perform adaptive load balancing. Nevertheless, related learning procedures rely heavily communication among agents (or
among decision-making agents autonomous computers). work applies recent work
reinforcement learning AI information agent gets purely local. Hence,
agent know ecient service restaurant choosing
place eat. don't assume agents may informed agents load
restaurants restaurants announce current load. makes
work strictly different work applying reinforcement learning adaptive load
balancing.
features make model study basic general. Moreover,
discussion raises question whether reinforcement learning (based purely
local information feedback) guarantee useful load balancing. combination
model use perspective reinforcement learning makes contribution
novel. Nevertheless, mentioned (and discuss Section 9) model
use original us captures many known problems situations distributed
load balancing. apply reinforcement learning, discussed recent AI literature,
model investigate properties related process.

3. Multi-Agent Multi-Resource Stochastic System

section define concrete framework study dynamic load balancing.
model present captures adaptive load balancing general setting mentioned
Section 2. restrict discussion discrete, synchronous systems (and thus
definition refer N , natural numbers); similar definitions possible
continuous case. concentrate case job executed using
resources. Although somewhat restricting, common practice much work
distributed systems (Mirchandaney & Stankovic, 1986).

Definition 3.1 multi-agent multi-resource stochastic system 6-tuple hA; R; P ; D; C;
SRi, = fa1; : : :; g set agents, R = fr1; : : :; rM g set resources,
P : N ! [0; 1] job submission function, : N ! < probabilistic job size
function, C : RN ! < probabilistic capacity function, SR resource-selection
rule.

intuitive interpretation system follows. resources
certain capacity, real number; capacity changes time, determined
function C . time point agent either idle engaged. idle, may
submit new job probability given P . job certain size
real number. size submitted job determined function D. (We
use unit token referring job sizes resource capacities, mean
tokens come integer quantities.) new job agent selects one
resources. choice made according rule SR; since much say
rule, discuss separately next section.
model, job may run resource. Furthermore, limit
number jobs served simultaneously given resource (and thus queuing occurs).
However, quality service provided resource given time deteriorates
478

fiAdaptive Load Balancing: Study Multi-Agent Learning

number agents using time. Specifically, every time point resource
distributes current capacity (i.e., tokens) equally among jobs served it.
size job reduced amount and, drops (or below) zero, job
completed, agent notified this, becomes idle again. Thus, execution time
job j depends size, capacity time resource processing it,
number agents using resource execution j .
measure system's performance twofold: aim minimize timeper-token, averaged jobs, well minimize standard deviation
random variable. Minimizing quantities ensure overall system eciency well
fairness. question selection rules yield ecient behavior; turn next
definition rules.

4. Adaptive Resource-Selection Rules
rule agents select resource new job, selection rule (SR),
heart adaptive scheme topic section. Throughout section
following one make assumption homogeneity. Namely, assume
agents use SR. Notice although system homogeneous, agent
act based local information. Sections 6 7 relax homogeneity
assumption discuss heterogeneous communicating populations.
already emphasized, among possible adaptive SRs interested
purely local SRs, ones access experience particular agent.
setting experience consists results previous job submissions; job submitted
agent already completed, agent knows name r resource used,
point time, tstart , job started, point time, tstop , job finished,
job size . Therefore, input SR is, principle, list elements form
(r; tstart; tstop ; ). Notice type input captures general type systems
interested in. Basically, wish assume little possible information
available agent order capture real loosely-coupled systems global
information unavailable.
Whenever agent selects resource job execution, may get feedback
non-negligible time, feedback may depend decisions made agents
agent i's decision. forces agent rely non-trivial portion
history makes problem much harder.
uncountably many possible adaptive SRs aim gain exhaustive understanding them. Rather, experimented family intuitive
relatively simple SRs compared non-adaptive ones. motivation choosing particular family SRs partially due observations made
cognitive psychologists people tend behave multi-agent stochastic recurrent situations. principle, set SRs captures two robust aspects
observations: \The law effect" (Thronkide, 1898) \Power law practice" (Blackburn, 1936). family rules, called
, partially resembles learning rules
discussed learning automata literature (Narendra & Thathachar, 1989), partially resembles interval estimation algorithm (Kaelbling, 1993), agents maintain
complete history experience. Instead, agent, A, condenses history
479

fiSchaerf, Shoham, & Tennenholtz

vector, called eciency estimator, denoted eeA . length vector
number resources, i'th entry vector represents agent's evaluation
current eciency resource (specifically, eeA (R) positive real number).
vector seen state learning automaton. addition eeA , agent keeps
vector jdA, stores number completed jobs submitted agent
resources, since beginning time. Thus, within
, need specify
two elements:
1. agent updates eeA job completed
2. agent selects resource new job, given eeA jdA
Loosely speaking, eeA maintained weighted sum new feedback
previous value eeA , resource selected probably one highest
eeA entry except low probability resource chosen. two
steps explained precisely following two subsections.

4.1 Updating Eciency Estimator
take function updating eeA

eeA (R) := WT + (1 , W )eeA (R)
represents time-per-token newly completed job computed
feedback (R; tstart; tstop; ) following way:2

= (tstop , tstart)=S
take W real value interval [0; 1], whose actual value depends jdA(R).
means take weighted average new feedback value old
value eciency estimator, W determines weights given pieces
information. value W obtained following function:

W = w + (1 , w)=jdA(R)
formula w real-valued constant. term (1 , w)=jdA(R) correcting
factor, major effect jdA (R) low; jdA (R) increases, reaching
value several hundreds, term becomes negligible respect w.

4.2 Selecting Resource

second ingredient adaptive SRs
function pdA selecting resource
new job based eeA jdA . function probabilistic. first define following
function
(
jdA(R) > 0
(R),n
0
pdA(R) := ee
,
n
E [ee ]
jd (R) = 0




2. Using parallel processing terminology, viewed stretch factor, quantifies stretching
program's processing time due multiprogramming (Ferrari, Serazzi, & Zeigner, 1983).

480

fiAdaptive Load Balancing: Study Multi-Agent Learning

n positive real-valued parameter E [eeA] represents average values
eeA (R) resources satisfying jdA (R) > 0. turn probability function,
define pdA normalized version pd0A :

pdA(R) := pd0A(R)=
= Rpd0A (R) normalization factor.3
function pdA clearly biases selection towards resources performed
well past. strength bias depends n; larger value n,
stronger bias. extreme cases, value n high (e.g., 20), agent
always choose resource best record. strategy \always choosing
best", although perhaps intuitively appealing, general good one;
allow agent exploit improvements capacity load resources.
discuss SR following subsection, expand issue exploration versus
exploitation Sections 6 7.
summarize, defined general setting investigate emergent load
balancing. particular, defined family adaptive resource-selection rules,
parameterized pair (w; n). parameters serve knobs tune
system optimize performance. next section turn experimental
results obtained system.

4.3 Best Choice SR (BCSR)

Best Choice SR (BCSR) learning rule assumes high value n, i.e,
always chooses best resource given point. assume w fixed given
value discussing BCSR. previous work (Shoham & Tennenholtz, 1992, 1994),
showed learning rules strongly resemble BCSR useful several natural
multi-agent learning settings. suggests need carefully study case
adaptive load balancing. demonstrate, BCSR always useful load
balancing setting.
difference BCSR learning rule value n low,
latter case agent gives relatively high probability selection resource
didn't give best results past. case agent might able notice
behavior one resources improved due changes system.
Note exploration \non-best" resources crucial dynamics
system includes changes capacities resources. cases, agent could
take advantage possible increases capacity resources uses BCSR. One
might wonder, however, whether cases main dynamic changes system
stem load changes, relying BCSR sucient. latter true,
able ignore parameter n concentrate BCSR, systems
capacity resources fixed. order clarify point, consider following
example.
3. R jdA (R) = 0, (i.e., agent going submit first job), assume
agent chooses resource randomly (with uniform probability distribution).

481

fiSchaerf, Shoham, & Tennenholtz

Suppose two resources, R1 R2 , whose respective (fixed) capacities,
cR1 cR2 , satisfy equality cR1 = 2cR2 . Assume load system varies

certain low value certain high one.
system's load low agents adopt BCSR, system evolve
way almost agents would preferring R1 R2. due
fact that, case low load, overlaps jobs, hence R1 much
ecient. hand, system's load high, R1 could busy
agents would prefer R2, since performance obtained using
less crowded resource R2 could better one obtained using overly crowded
resource R1. extreme case high load, expect agents use R2 one
third time.
Assume load system starts low level, increases
high value, decreases reach original value. load increases,
agents, mostly using R1, start observing R1's performance becoming
worse and, therefore, following BCSR start using R2 too. Now, load
decreases, agents using R2 observe improvement performance
R2, value stored R1 (i.e., eeA (1)), still ect previous
situation. Hence, agents keep using R2, ignoring possibility obtaining
much better results moved back R1. situation, randomized selection
makes agents able use R1 (with certain probability) therefore
may discover performance R1 better R2 switch back R1.
improve system's eciency significant manner.
example shows BCSR is, general case, good choice.
general true value n high.
discussion assumed changes load unforeseen.
able predict changes load, agents simply use BCSR
load fixed use low value n changes. case, instead,
without even realizing system changed way, agents would need
(and, see, would able to) adapt dynamic changes well other.

5. Experimental Results
section compare SRs
another, well non-adaptive,
benchmark selection rules.
non-adaptive SRs consider paper agents partition
according capacities load system fixed predetermined
manner agent uses always resource. Later paper, SR
kind identified configuration vector, specifies, resource, many
agents use it. test adaptive SRs, compare performance nonadaptive SRs perform best particular problem. creates highly competitive
set benchmarks adaptive SRs.
addition, compare adaptive SRs load-querying SR defined
follows: agent, new job, asks resources busy
always chooses less crowded one.
482

fiAdaptive Load Balancing: Study Multi-Agent Learning

5.1 Experimental Setting
introduce particular experimental setting, many results described
obtained. present order concrete experiments; however,
qualitative results experiments observed variety experimental
settings.
One motivation particular setting stems PCs workstations problem
mentioned Section 2. example, part study related set computers
located single site. computers relatively high load peak hours
day low load night (i.e., chances user PC submits job
higher day time week days night weekend). Another
part study related set computers split around world,
load quite random structure (i.e., due difference time zones, users may use PCs
unpredictable hours).
Another motivation particular setting stems restaurant problem mentioned Section 2 (for discussion related \bar problem" see Arthur, 1994).
example, consider set snack bars located industrial park. snack
bars relatively high loads peak hours day low load night
(i.e., chances employee choose go snack-bar higher day
employees present day). Conversely, assume
set bars near airport load quite random structure (i.e., airport
employees may use snack-bars quite unpredicted hours).
Although particular real-situations, would emphasize general
motivation study fact related phenomena observed
various different settings.
take N , number agents, 100, , number resources,
5. first set experiments take capacities resources fixed.
particular, take c1 = 40; c2 = 20; c3 = 20; c4 = 10; c5 = 10. assume
agents probability submitting new job. assume
agents distribution size jobs submit; specifically, assume
uniform distribution integers range [50,150].
ease exposition, assume point time corresponds second,
consequently count time minutes, hours, days, weeks. hour
main point reference; assume, simplicity, changes system (i.e., load
change capacity change) happen beginning new hour. probability
submitting job second, corresponds load system, vary
time; crucial factor agents must adapt. Note agents
submit jobs second, probability submission may change. particular
concentrate three different values quantity, called Llo ; Lhi Lpeak ,
assume system load switches values. actual values Llo ; Lhi
Lpeak following quantitative results 0:1%, 0:3% 1%, roughly correspond
agent submitting 3.6, 10.8, 36 jobs per hour (per agent) respectively.
483

fiSchaerf, Shoham, & Tennenholtz

load

configuration
time-per-token
Llo
f100; 0; 0; 0; 0g
38.935
Lhi
f66; 16; 16; 1; 1g
60.768
Lpeak f40; 20; 20; 10; 10g
196.908
Figure 1: Best non-adaptive SRs fixed load
following, measuring success, refer average time-pertoken.4 However, adaptive SRs give best average time-per-token
found fair.

5.2 Fixed Load

start case load fixed. case interesting
adaptive behavior; however, satisfactory SR show reasonably ecient behavior
basic case, order useful system stabilizes.
start showing behavior non-adaptive benchmark SRs case fixed
load.5 Figure 1 shows give best results, three loads.
see, big difference three loads mentioned above.
load particularly high, agents scatter around resources rate
proportional capacities; load low use best resource.
Given above, easy see adaptive SR effective enables
moving quickly one configuration other.
static setting this, expect best non-adaptive SRs perform better adaptive ones, since information gained exploration adaptive SRs
built-in non-adaptive ones. experimental results confirm intuition,
shown Figure 2 Lhi . figure shows performance obtained population
value n varies 2 10 three values w: 0.1, 0.3, 0.5.
Note values (n; w) good choices dynamic cases (see later
paper, values intervals [3; 5] [0:1; 0:5], respectively), deterioration
performance adaptive SRs respect non-adaptive ones small.
encouraging result, since adaptive SRs meant particularly suitable dynamic
systems. following subsections see indeed are.

5.3 Changing Load

begin explore dynamic settings. consider case
load system (that is, probability agents submitting job time) changes
time. paper present two dynamic settings: One load changes
according fixed pattern random perturbations another
load varies random fashion. Specifically, first case fix load Lhi
4. data shown later refer, convenience, time 1000 tokens.
5. non-adaptive SRs human-designed SRs used benchmarks; assume knowledge
load capacity, available adaptive SRs design.

484

fiAdaptive Load Balancing: Study Multi-Agent Learning


v
e
r

g
e



e
p
e
r


k
e
n

6
67





Weight: w = 0.5
Weight: w = 0.3
Weight: w = 0.1

66





65
64





63
62
61
2



























3
4 5
6 7
8
9 10
Exponent Randomization Function: n

-

Figure 2: Performance adaptive Selection Rules fixed load
ten consecutive hours, five days week, two randomly chosen hours
Lpeak , Llo rest week. second case, fix number
hours week load first case, distribute completely
randomly week.
results obtained two cases similar. Figure 3 shows results obtained
adaptive SRs case random load. best non-adaptive deterministic
SR gives time-per-token value 69:201 obtained configuration (partition
agents) f52; 22; 22; 2; 2g; adaptive SRs superior. load-querying SR instead gets
time-per-token value 48:116, obviously better, far
performances adaptive SRs.
observe following phenomenon: Given fixed n (resp. fixed w) average
time-per-token non-monotonic w (resp. n). phenomenon strongly related
issue exploration versus exploitation mentioned phenomena observed
study Q-learning (Watkins, 1989).
notice two parameters n w interplay. fact, value
w minimum time per token value obtained different value n.
precisely, higher w lower n must order obtain best results. means
that, order obtain high performance, highly exploratory activity (low n)
matched giving greater weight recent experience (high w). \parameter
485

fiSchaerf, Shoham, & Tennenholtz


v
e
r

g
e



e
p
e
r


k
e
n

6
71
70
69


Weight: w = 0.5




Weight: w = 0.3
Weight: w = 0.1













68












67
66
65
2













3
4 5
6 7
8
9 10
Exponent Randomization Function: n

-

Figure 3: Performance adaptive Selection Rules random load
matching" intuitively explained following qualitative way: exploration
activity pays allows agent detect changes system. However,
effective if, change detected, significantly affect eciency estimator
(i.e., w high). Otherwise, cost exploration activity greater gain.

5.4 Changing Capacities
consider case capacity resources vary time.
particular, demonstrate results case previously mentioned setting.
assume capacities rotate randomly among resources and, five consecutive
days, resource gets capacity 40 one day, 20 2 days, 10
2 days.6 load varies randomly.
results experiment shown Figure 4. best non-adaptive SR
case gives time-per-token value 118:561 obtained configuration
f20; 20; 20; 20; 20g.7 adaptive SRs give much better results, slightly
6. Usually capacities change less dramatic fashion. use above-mentioned setting
order demonstrate applicability approach severe conditions.
7. load-querying SR gives results case fixed capacities, SR
obviously uenced change.

486

fiAdaptive Load Balancing: Study Multi-Agent Learning


v
e
r

g
e



e
p
e
r


k
e
n

6




92.5




90





87.5



82.5






80




85

77.5



2








Weight: w = 0.5

Weight: w = 0.3
Weight: w = 0.1




-

3
4
5
6 7 8
9 10
Exponent Randomization Function: n

Figure 4: Performance adaptive Selection Rules changing capacities
worse case fixed capacities. phenomena mentioned visible
case too. See example weight 0:1 mismatches low values n.

6. Heterogeneous Populations
Throughout previous section assumed agents use SR, i.e.
Homogeneity Assumption. assumption models situation sort
centralized off-line controller which, beginning, tells agents behave
leaves agents make decisions.
situation described different on-line centralized controller makes every decision. However, would move even
investigate situation agent able make decision
strategy use and, maybe, adjust time.
step toward study systems kind, drop Homogeneity Assumption
consider situation part population uses one SR part
uses second one.
first set experiments, consider setting discussed Subsection 5.1
confront one other, two populations (called 1 2) size (50 agents
each). population uses different SR
. SR population (for = 1; 2)
487

fiSchaerf, Shoham, & Tennenholtz




v
e
r

g
e



e
p
e
r


k
e
n

6



67
66



65























63
61










64
62


: T1
: T2



2 3
4 5
6 7
8
9 10
Exponent Randomization Function (n2 )

-

Figure 5: Performance 2 populations 50 agents n1 = 4 w1 = w2 = 0:3
determined pair parameters (wi; ni ). measure success population
defined average time-per-token members, denoted Ti .
Figure 5 shows result obtained w1 = w2 = 0:3, n1 = 4, different
values n2 , case randomly varying load.
results expose following phenomenon: two populations obtain different
outcomes ones obtain homogeneous case. specifically, 4
n2 6 , results obtained agents use n2 generally better results
obtained ones use n1 , despite fact homogeneous population
uses n1 gets better results homogeneous population uses n2 .
phenomenon described following intuitive explanation. n2
above-mentioned range, population uses n2 less \exploring" (i.e.,
\exploiting") one, left might able
adapt changes satisfactory manner. However, joined
population, gets advantages experimental activity agents population,
without paying it. fact, exploring agents, trying unload
crowded resources, make service agents well.
worth observing Figure 5 n2 low (e.g., n2 3) agents use
n2 take role explorers lose lot, agents use n1 gain
situation. Conversely, high values n2 (e.g., n2 7) performances exploiters,
488

fiAdaptive Load Balancing: Study Multi-Agent Learning


v
e
r

g
e



e
p
e
r


k
e
n



6
67
66



65


: T1
: T2










64
63
62










61
2
















3
4 5
6 7
8
9 10
Exponent Randomization Function: n2

-

Figure 6: Performance 2 populations 90/10 agents n1 = 4 w1 = w2 = 0:3
use n2 , deteriorate. means exploiters static, hinder
other, explorers take advantage it.
better understanding phenomena involved, experimented
asymmetric population, composed one large group one small one, instead two
groups similar size. Figure 6 shows results obtained using setting similar
one above, population 1 composed 90 members population 2 consists
10 members. case, every value n2 4, exploiters better
explorers. experiments show case, higher n2 better T2
is, i.e. exploiters exploit, gain.
results suggest single agent gets best results noncooperative always adopting resource best performance (i.e., use BCSR),
given rest agents use adaptive (i.e., cooperative) SR. However,
agents non-cooperative lose.8 conclusion, selfish interest
agent match interest population. contrary results
obtained basic contexts multi-agent learning (Shoham & Tennenholtz, 1992).
shown how, fixed value w, coexisting populations adopting
different values n interact. Similar results obtained fix value n
8. fact illuminating instance well-known prisoners dilemma (Axelrod, 1984).

489

fiSchaerf, Shoham, & Tennenholtz


v
e
r

g
e



e
p
e
r


k
e
n

6
67




: T1
: T2

66
65
64




















63

























62
61

-

0.01 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Weight estimator parameter (w2)

1

Figure 7: Performance 2 populations 50 agents n1 = n2 = 4 w1 = 0:3
use two different values w. cases, agents adopting lower value w
general winners, shown Figure 7 n1 = n2 = 4 w1 = 0:3. w
low corresponding agents get poor results longer winners,
case high n Figure 5.
Another interesting phenomenon obtained confronting adaptive agents
load-querying agents. Load-querying agents agents able consult resources
submit jobs. load-querying agent submit job
unloaded resource given point. confronting load-querying agents
adaptive ones, results obtained adaptive agents obviously worse
results obtained load-querying ones, better results obtained
complete population adaptive agents. means load-querying agents play
role \parasites", above-mentioned \exploiters"; load-querying agents help
maintaining load balancing among resources, therefore help rest
agents. Another result obtain agents adopt deterministic SRs may behave
parasites worsen performance adaptive agents.
assertions supported experiments described Figure 8, population 90 agents, uses adaptive SR parameters (n; w), faced
minority 10 agents use different SRs, stated above. particular, four
cases consider, minority behaves following ways: (i) choose resource
490

fiAdaptive Load Balancing: Study Multi-Agent Learning

90 agents
10 agents
T1
(.3,4)
(.3,20)
65.161
(.3,4)
(.1,4)
64.630
(.3,4) Load-querying 62.320
(.3,4)
Using Res. 0 65.499

T2

59.713
63.818
47.236
55.818

Figure 8: Performance 2 populations 90/10 agents various SRs
gave best results, (ii) conservative updating history, (iii)
load-querying agents, (iiii) use deterministically resource capacity 40
(in basic experimental setting).

7. Communication among Agents
point, assumed direct communication among agents.
motivation considered situations absolutely
transmission channels protocols. assumption agreement idea
multi-agent reinforcement learning. systems massive communication feasible
much concerned multiple agent adaptation, problem reduces
supplying satisfactory communication mechanisms. Multi-agent reinforcement learning
interesting real life forces agents act without a-priori arranged communication channels must rely action-feedback mechanisms. However, interest
understand effects communication system eciency (as Shoham & Tennenholtz, 1992; Tan, 1993), agents augmented sort communication
capabilities. study extension led illuminating results,
present.
assume agent communicate agents,
call neighbors. therefore consider relation neighbor-of assume exive, symmetric transitive. consequence, relation neighbor-of partitions
population equivalence classes, call neighborhoods.
form communication consider based idea eciency estimators agents within neighborhood shared among decision made
(i.e., agent chooses resource). reader notice naive
form communication sophisticated types communication possible.
However, form communication natural concentrate agents
update behavior based past information. particular, type
communication similar ones used above-mentioned work incorporating
communication framework multi-agent reinforcement learning.
suppose different SRs may used different agents population,
impose condition within single neighborhood, SR used
members.
assume agent keeps history updates
usual way. choice, instead, based agent eciency estimator,
491

fiSchaerf, Shoham, & Tennenholtz


v
e
r

g
e



e
p
e
r


k
e
n

6
71
70














68





67



66



65
2














69
















: 5 CNs 20 agents
: 20 CNs 5 agents
: 50 CNs 2 agents

3
4 5
6 7
8
9 10
Exponent Randomization Function: n

-

Figure 9: Performance adaptive Selection Rules random load profile communicating agents
average eciency estimators agents corresponding neighborhood.
average called neighborhood eciency estimator. neighborhood eciency
estimator physical storage: value recalculated time member needs it.
order compare behavior communicating agents non-communicating ones,
assume single population might be, aside neighborhoods defined
above, neighborhoods allow sharing eciency estimators among
members. members neighborhoods behave described previous
sections, i.e., agent relies history. thing common
among members neighborhood members use SR.
call communicating neighborhood (CN), neighborhood eciency estimators shared decision taken non-communicating neighborhood (NCN),
neighborhood done.
first set experiments ran, regards population composed CNs,
size. particular, considered CNs various sizes, starting 50 CNs
size 2, going 5 CNs size 20. load profile exploited random load change
defined Subsection 5.3, value w taken 0:3, n taken various
values. results obtained shown Figure 9.
492

fiAdaptive Load Balancing: Study Multi-Agent Learning

results show communicating populations get good results.
reason members CN tend conservative, sense
mostly use best resource. fact, since rely average several agents,
picture system tends much static. particular, bigger
CN conservative members tend be. example, consider values
(n; w) give best results non-communicating agents, values give quite bad
performance CNs since turn conservative.
Using adaptive values (n; w), behavior communicating population improves reaches performance slightly worse performance
non-communicating population. Tuning parameters using finer grain, possible
obtain performance equal one obtained non-communicating population.
However, seems clear obvious gain achieved form communication
capability. intuitive explanation two opposite effects caused
communication. one hand, agents get fairer picture system prevents using bad resources therefore getting bad performance.
hand, since agents CN \better" picture system, tend
use best resources thus compete them. fact, agents behave
selfishly selfish interest may agree interest population
whole.
interesting message get fact agents may
\distorted" picture system (which typical non-communicating populations),
turns advantage population whole.
Sharing data among agents leads poorer performances case
agents common views loads target jobs toward (lightly loaded)
resources, quickly become overloaded. order profitably use shared data,
allow form reasoning fact data shared.
problem however scope paper (see e.g., Lesser, 1991).
order understand behavior system CNs NCNs face other,
consider NCN 80 agents together set CNs equal size, different values
size. results corresponding experiments shown Figure 10.
members CNs, inclined use best resources, behave parasites
sense explained Section 6. exploit adaptiveness rest population
obtain good performance best resources. reason get better results
rest population, shown experimental results.
interesting observe NCN uses conservative selection rule,
CNs obtain even better results. intuitive explanation behavior
although groups, i.e., communicating ones one high value n,
tend conservative, communicating ones \win" conservative
\clever" way, making use better picture situation.
conclusion draw section proposed form communication
agents may provide useful means improve performance population
setting. However, claim communication agents completely
useless. Nevertheless, observed provide straightforward significant
improvement. results support claim sole past history agent
493

fiSchaerf, Shoham, & Tennenholtz

80 agents
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,4) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN
(.3,10) 1 NCN

20 agents
(.3,4) 1 CN
(.3,4) 2 CNs
(.3,4) 5 CNs
(.3,4) 10 CNs
(.3,4) 1 CN
(.3,4) 2 CNs
(.3,4) 5 CNs
(.3,4) 10 CNs

T1

65.287
65.069
65.091
64.895
68.419
68.319
68.529
68.351

T2

63.054
63.307
62.809
63.840
60.018
59.512
60.674
61.711

Figure 10: Performance CNs NCNs together
reasonable information base decision, assuming consider available
kind real-time information (e.g., current load resources).

8. Discussion
previous sections devoted report experimental study. synthesize observations view motivation, discussed Sections 1 2.
mentioned, model general model active autonomous agents
select among several resources dynamic fashion based local information.
fact agents use local information makes possibility ecient loadbalancing questionable. However, showed adaptive load balancing based purely
local feedback feasible task. Hence, results complementary ones obtained
distributed computer systems literature. Mirchandaney Stankovic (1986) put
it: \: : : significant work illustrated possible design
learning controller able dynamically acquire relevant job scheduling information
process trial error, use information provide good performance."
study presented paper supplies complementary contribution able
show useful adaptive load balancing obtained using purely local information
framework general organizational-theoretic model.
study identified various parameters adaptive process investigated
affect eciency adaptive load balancing. part study supplies
useful guidelines systems designer may force agents work based
common selection rule. observations, although somewhat related previous observations made contexts models (Huberman & Hogg, 1988), enable demonstrate
aspects purely local adaptive behavior non-trivial model.
results disagreement selfish interest agents common
interest population sharp contrast previous work multi-agent learning
(Shoham & Tennenholtz, 1992, 1994) dynamic programming perspective
earlier work distributed systems (Bertsekas & Tsitsiklis, 1989). Moreover, explore
interaction different agent types affects system's eciency well
494

fiAdaptive Load Balancing: Study Multi-Agent Learning

individual agent's eciency. related results interpreted guidelines
designer may partial control system.
synthesis observations teaches us adaptive load balancing
one adopts reinforcement learning perspective agents rely local
information activity. additional step performed attempts bridge
gap local view previous work adaptive load balancing communicating
agents, whose decisions may controlled learning automata means.
therefore rule possibility communication current status resources
joint decision-making, enable limited sharing previous history. show
limited communication may help, even deteriorate system eciency.
leaves us major gap previous work communication among agents
basic tool adaptive load balancing work. Much left done attempting
bridge gap. see major challenge research.

9. Related Work
Section 2 mentioned related work field distributed computer systems
(Mirchandaney & Stankovic, 1986; Billard & Pasquale, 1993; Glockner & Pasquale, 1993;
Mirchandaney et al., 1989; Zhou, 1988; Eager et al., 1986). typical example work
paper Mirchandaney Stankovic (1986). work learning automata
used order decide action taken. However, suggested algorithms heavily
rely communication information sharing among agents. sharp contrast
work. addition, differences type model use
model presented above-mentioned work work distributed computer
systems.
Applications learning algorithms load balancing problems given Mehra
(1992), Mehra Wah (1993). However, work well, agents (sites,
authors' terminology) ability communicate exchange workload values,
even though values subject uncertainty due delays. addition, differently
work, learning activity done off-line. particular, learning phase
whole system dedicated acquisition workload indices. load indices
used running phase threshold values job migration different sites.
spite differences, similarities work abovementioned work. One important similarity use learning procedures.
difference classical work parallel distributed computation (Bertsekas
& Tsitsiklis, 1989) applies numerical iterative methods solution problems
network ow parallel computing. similarities related study
division society groups. somewhat resembles work group formation
(Billard & Pasquale, 1993) distributed computer systems. information sharing
allow Section 7 similar limited communication discussed Tan (1993).
classification load-balancing problems given Ferrari (1985), work falls
category load-independent non-preemptive pure load-balancing. problems
investigate seen sender-initiated problems, although case sender
agent (overloaded) resource.
495

fiSchaerf, Shoham, & Tennenholtz

One may wonder work differs work adaptive load balancing
Operations Research (OR) (e.g., queuing theory Bonomi, Doshi, Kaufmann, Lee, &
Kumar, 1990). Indeed, commonalities. work, individual
decisions made locally, based information obtained dynamically runtime.
cases systems constructed suciently complex interesting
results tend obtained experimentally. However, careful look relevant
literature reveals essential difference perspective topic
reinforcement-learning perspective: permits free communication within system,
thus significant element uncertainty framework. particular, issue
exploration versus exploitation, lies heart approach, completely
absent work OR.
work adaptive load balancing related topics carried
Artificial Intelligence community (see e.g., Kosoresow, 1993; Gmytrasiewicz, Durfee, &
Wehe, 1991; Wellman, 1993). work too, however, tends based form
communication among agents, whereas case load balancing obtained purely
learning activity.
article related previous work co-learning (Shoham & Tennenholtz,
1992, 1994). framework co-learning framework multi-agent learning,
differs frameworks discussed multi-agent reinforcement learning (Narendra &
Thathachar, 1989; Tan, 1993; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994) due
fact considers case stochastic interactions among subsets agents,
purely local feedback revealed agents based interactions.
framework co-learning similar respects number dynamic frameworks
economics (Kandori, Mailath, & Rob, 1991), physics (Kinderman & Snell, 1980), computational ecologies (Huberman & Hogg, 1988), biology (Altenberg & Feldman, 1987).
study adaptive load balancing treated study co-learning.
Relevant work literature field Learning Automata (see Narendra & Thathachar, 1989). fact, agent setting seen learning automaton. Therefore, one may hope theoretical results interconnected automata
N-player games (see e.g., El-Fattah, 1980; Abdel-Fattah, 1983; Narendra & Wheeler Jr.,
1983; Wheeler Jr. & Narendra, 1985) could imported framework. Unfortunately,
due stochastic nature job submissions (i.e., agent interactions) real-valued
(instead binary) feedback, problem fit completely theoretical
framework learning automata. Hence, results concerning optimality, convergence expediency learning rules Linear Reward-Penalty Linear Reward-Inaction,
easily adapted setting. fact use stochastic model
interaction among agents, makes work closely related above-mentioned work
co-learning. Nevertheless, work largely uenced learning automata theory
resource-selection rules closely resemble reinforcement schemes learning automata.
Last least, work related work applying organization theory management techniques field Distributed AI (Fox, 1981; Malone, 1987; Durfee, Lesser,
& Corkill, 1987). model closely related models decision-making management
organization theory (e.g., Malone, 1987) applies reinforcement learning perspective context. makes work related psychological models decision-making
(Arthur, 1994).
496

fiAdaptive Load Balancing: Study Multi-Agent Learning

10. Summary

work applies idea multi-agent reinforcement learning problem load
balancing loosely-coupled multi-agent system, agents need adapt one another well changing environment. demonstrated adaptive behavior
useful ecient load balancing context identified pair parameters
affect eciency non-trivial fashion. parameter, holding parameter
fixed, gives rise certain tradeoff, two parameters interplay non-trivial
illuminating way. exposed illuminating results regarding heterogeneous
populations, group parasitic less adaptive agents gain exibility agents. addition, showed naive use communication may
improve, might even deteriorate, system eciency.

Acknowledgments

thank anonymous reviewers Steve Minton, whose stimulating comments helped
us improving earlier version paper.

References

Abdel-Fattah, Y. M. (1983). Stochastic automata modeling certain problems collective
behavior. IEEE Transactions Systems, Man, Cybernetics, 13 (3), 236{241.
Altenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission,
evolution modifier genes. I. reduction principle. Genetics, 117, 559{572.
Arthur, W. (1994). Inductive reasoning, bounded rationality bar problem. Tech. rep.
94-03-014 (working paper), Santa Fe Institute. Appeared American Economic
Review 84.
Axelrod, R. (1984). Evolution Cooperation. New York: Basic Books.
Bertsekas, D., & Tsitsiklis, J. (1989). Parallel Distributed Computation: Numerical
Methods. Prentice Hall.
Billard, E., & Pasquale, J. (1993). Effects delayed communication dynamic group
formation. IEEE Transactions Systems, Man, Cybernetics, 23 (5), 1265{1275.
Blackburn, J. M. (1936). Acquisition skill: analysis learning curves. IHRB Report
No. 73.
Bond, A. H., & Gasser, L. (1988). Readings Distributed Artificial Intelligence. Ablex
Publishing Corporation.
Bonomi, F., Doshi, B., Kaufmann, J., Lee, T., & Kumar, A. (1990). case study
adaptive load balancing algorithm. Queuing Systems, 7, 23{49.
Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent cooperation among communicating problem solvers. IEEE Transactions Computers, 36, 1275{1291.
497

fiSchaerf, Shoham, & Tennenholtz

Eager, D., Lazowska, E., & Zahorjan, J. (1986). Adaptive load sharing homogeneous
distributed systems. IEEE Transactions Software Engineering, 12 (5), 662{675.
El-Fattah, Y. M. (1980). Stochastic automata modeling certain problems collective
behavior. IEEE Transactions Systems, Man, Cybernetics, 10 (6), 304{314.
Ferrari, D. (1985). study load indices load balancing schemes. Tech. rep. Ucb/CSD
86/262, Computer Science Division (EECS), Univ. California, Berkeley.
Ferrari, D., Serazzi, G., & Zeigner, A. (1983). Measurement Tuning Computer
Systems. Prentice Hall.
Fox, M. S. (1981). organizational view distributed systems. IEEE Transactions
Systems, Man, Cybernetics, 11, 70{80.
Glockner, A., & Pasquale, J. (1993). Coadaptive behavior simple distributed job
scheduling system. IEEE Transactions Systems, Man, Cybernetics, 23 (3),
902{907.
Gmytrasiewicz, P., Durfee, E., & Wehe, D. (1991). utility communication coordinating intelligent agents. Proc. 9th Nat. Conf. Artificial Intelligence
(AAAI-91), pp. 166{172.
Huberman, B. A., & Hogg, T. (1988). behavior computational ecologies. Huberman, B. A. (Ed.), Ecology Computation. Elsevier Science.
Kaelbling, L. (1993). Learning Embedded Systems. MIT Press.
Kandori, M., Mailath, G., & Rob, R. (1991). Learning, mutation long equilibria
games. Mimeo. University Pennsylvania.
Kinderman, R., & Snell, S. L. (1980). Markov Random Fields Applications.
American Mathematical Society.
Kosoresow, A. P. (1993). fast first-cut protocol agent coordination. Proc.
11th Nat. Conf. Artificial Intelligence (AAAI-93), pp. 237{242.
Kraus, S., & Wilkenfeld, J. (1991). function time cooperative negotiations.
Proc. 9th Nat. Conf. Artificial Intelligence (AAAI-91), pp. 179{184.
Lesser, V. R. (1991). retrospective view FA/C distributed problem solving. IEEE
Transactions Systems, Man, Cybernetics, 21 (6), 1347{1362.
Malone, T. W. (1987). Modeling coordination organizations markets. Management
Science, 33 (10), 1317{1332.
Mehra, P. (1992). Automated Learning Load-Balancing Strategies Distributed
Computer System. Ph.D. thesis, Department Electrical Computer Engineering,
University Illinois Urbana-Champaign.
498

fiAdaptive Load Balancing: Study Multi-Agent Learning

Mehra, P., & Wah, B. W. (1993). Population-based learning load balancing policies
distributed computer system. Proceedings Computing Aerospace 9 Conference,
AIAA, pp. 1120{1130.
Mirchandaney, R., & Stankovic, J. (1986). Using stochastic learning automata job
scheduling distributed processing systems. Journal Parallel Distributed
Computing, 3, 527{552.
Mirchandaney, R., Towsley, D., & Stankovic, J. (1989). Analysis effects delays
load sharing. IEEE Transactions Computers, 38 (11), 1513{1525.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.
Prentice Hall.
Narendra, K., & Wheeler Jr., R. M. (1983). N-player sequential stochastic game
identical payoffs. IEEE Transactions Systems, Man, Cybernetics, 13 (6), 1154{
1158.
Pulidas, S., Towsley, D., & Stankovic, J. (1988). Imbedding gradient estimators load balancing algorithms. Proceedings 8th International Conference Distributed
Computer Systems, IEEE, pp. 482{489.
Sen, S., Sekaran, M., & Hale, J. (1994). Learning coordinate without sharing information.
Proc. 12th Nat. Conf. Artificial Intelligence (AAAI-94).
Shoham, Y., & Tennenholtz, M. (1992). Emergent conventions multi-agent systems: initial experimental results observations. Proc. 3rd Int. Conf. Principles
Knowledge Representation Reasoning (KR-92), pp. 225{231.
Shoham, Y., & Tennenholtz, M. (1994). Co-learning evolution social activity.
Tech. rep. STAN-CS-TR-94-1511, Dept. Computer Science, Stanford University.
Sutton, R. (1992). Special issue reinforcement learning. Machine Learning, 8 (3{4).
Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents.
Proceedings 10th International Conference Machine Learning.
Thronkide, E. L. (1898). Animal intelligence: experimental study associative
processes animals. Psychological Monographs, 2.
Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge University.
Wellman, M. P. (1993). market-oriented programming environment application
distributed multicommodity ow problems. Journal Artificial Intelligence Research,
1, 1{23.
Wheeler Jr., R. M., & Narendra, K. (1985). Learning models decentralized decision
making. Automatica, 21 (4), 479{484.
499

fiSchaerf, Shoham, & Tennenholtz

Yanco, H., & Stein, L. (1993). adaptive communication protocol cooperating mobile robots. Animals Animats: Proceedings Second International
Conference Simulation Adaptive Behavior, pp. 478{485.
Zhou, S. (1988). trace-driven simulation study dynamic load balancing. IEEE Transactions Software Engineering, 14 (9), 1327{1341.
Zlotkin, G., & Rosenschein, J. S. (1993). domain theory task oriented negotiation.
Proc. 13th Int. Joint Conf. Artificial Intelligence (IJCAI-93), pp. 416{422.

500


