journal artificial intelligence

submitted published

rerepresenting restructuring domain theories
constructive induction
steven k donoho
larry rendell

department computer science univeristy illinois
n mathews ave urbana il usa

donoho cs uiuc edu
rendell cs uiuc edu

abstract

theory revision integrates inductive learning background knowledge combining
training examples coarse domain theory produce accurate theory
two challenges theory revision theory guided systems face first
representation language appropriate initial theory may inappropriate
improved theory original representation may concisely express initial theory
accurate theory forced use representation may bulky cumbersome
dicult reach second theory structure suitable coarse domain theory may
insucient fine tuned theory systems produce small local changes
theory limited value accomplishing complex structural alterations may
required
consequently advanced theory guided learning systems require exible representation
exible structure analysis theory revision systems theory guided
learning systems reveals specific strengths weaknesses terms two desired
properties designed capture underlying qualities system system uses
theory guided constructive induction experiments three domains improvement
previous theory guided systems leads study behavior limitations
potential theory guided constructive induction

introduction
inductive learners normally use training examples use background knowledge effectively integrating knowledge induction widely studied work date area theory revision
knowledge given coarse perhaps incomplete incorrect theory domain
training examples used shape initial theory refined accurate
theory ourston mooney thompson langley iba cohen pazzani
kibler baffes mooney mooney develop exible
robust learning data theory knowledge
addressing two following desirable qualities

flexible representation theory guided system utilize knowledge contained initial domain theory without adhere closely initial
theory representation language

flexible structure theory guided system unnecessarily restricted
structure initial domain theory

c ai access foundation morgan kaufmann publishers rights reserved

fidonoho rendell

giving precise definitions terms motivate work intuitively

intuitive motivation
first desirable quality exibility representation arises theory representation appropriate describing coarse initial domain theory may inadequate
final revised theory initial domain theory may compact concise
one representation accurate theory may quite bulky cumbersome representation furthermore representation best expressing initial theory may
best carrying refinements helpful refinement step may clumsy
make initial representation yet carried quite simply another representation
simple example coarse domain theory may expressed logical conjunction
n conditions met accurate theory though one
n conditions holds expressing accurate theory dnf representation
used describe initial theory would cumbersome unwieldy murphy pazzani
furthermore arriving final theory refinement operators suitable
dnf drop condition add condition modify condition would cumbersome task
n representation adopted refinement simply involves empirically
finding appropriate final theory expressed concisely baffes mooney

similarly second desirable quality exibility structure arises theory
structure suitable coarse domain theory may insucient fine tuned
theory order achieve desired accuracy restructuring initial theory may
necessary many theory revision systems act making series local changes
lead behavior two extremes first extreme rigidly retain backbone
structure initial domain theory allowing small local changes figure illustrates
situation minor revisions made conditions added dropped
modified refined theory trapped backbone structure initial
theory local changes needed techniques proven useful ourston
mooney often required required systems often
move extreme drop entire rules groups rules build entire
rules groups rules scratch replace thus restructure
forfeit valuable knowledge process ideal theory revision system would glean
knowledge theory substructures cannot fixed small local changes
use restructured theory
intuitive illustration consider piece software almost works sometimes
made useful local operations fixing couple bugs adding
needed subroutine cases though piece software almost
works fact far full working order may need redesigned restructured
mistake one extreme try fix program making series patches
original code mistake extreme discard original program without
learning anything start scratch best would examine
original program see learned design use knowledge
redesign likewise attempting improve coarse domain theory series
local changes may yield little improvement theory trapped initial


firerepresenting restructuring domain theories

initial theory



b

c

j

e

h

refined theory

n

k l





b

c

p q r

j

w e

f g



n

k l

u



p

v

r

f g

figure typical theory revision allows limited structural exibility although conditions added dropped modified revised theory much
constrained structure initial theory
structure render original domain theory useless careful analysis
initial domain theory give valuable guidance design best final theory
illustrated figure many substructures taken initial
theory adapted use refined theory information initial theory
used structure revised theory restricted structure
initial theory
initial theory



b

c

n

j

e

h

refined theory

k l

k

p q r

l

f



g



e

h







p

u

v

f g q r

f g

figure exible structural modification revised theory taken many substructures initial theory adapted recombined use
structure revised theory restricted structure
initial theory



fidonoho rendell

terminology

training data consist examples classified vectors feature value pairs assume initial theory set conditions combined
operators indicating one classes unreasonable
believe theories form covers much existing theory revision

work intended informal exploration exible representation exible
structure flexible representation means allowing theory revised representation language initial theory example exible representation
introduction operator combining features operator used
initial theory section example given introducing n operator
represent theory originally expressed dnf flexible structure means limiting
revision theory series small incremental modifications example
breaking theory components building blocks
construction theory
constructive induction process whereby training examples redescribed
set features features combinations original features bias
knowledge may used construction features subtle point
speak exible representation referring representation
domain theory training data although phrase change representation
often applied constructive induction refers change data
term exible representation reserved change theory representation thus
system performing constructive induction changing feature language
data without exhibiting exible representation changing representation theory

overview

theory revision constructive induction embody complementary aspects machine
learning community ultimate goals theory revision uses data improve
theory constructive induction use theory improve data facilitate learning
present theory guided constructive induction addresses
two desirable qualities discussed section initial theory analyzed
features constructed components theory constructed features
need expressed representational language initial theory
refined better match training examples finally standard inductive learning
c quinlan applied redescribed examples
begin analyzing landmark theory revision learning systems exhibited exibility handling domain theory part played performance analysis extract guidelines system design apply
design limited system effort integrate learning theory data
borrow heavily theory revision multistrategy learning constructive induction communities guidelines system design fall closest classical constructive
induction methods central focus presentation another
system rather study exible representation structure manifestation
previous work guidance future design


firerepresenting restructuring domain theories

section gives context work analyzing previous uence work section explores promoter recognition domain demonstrates
related theory revision systems behave domain section guidelines
theory guided constructive induction presented guidelines synthesis
positive aspects related address two desirable qualities exibility
representation exibility structure section presents specific theory guided
constructive induction instantiation guidelines set forth earlier
section experiments three domains given section followed
discussion strengths theory guided constructive induction section section presents experimental analysis limits applicability simple
followed discussion limitations future directions work section

context related work

although work bears resemblance form objective many papers constructive induction michalski fu buchanan utgoff schlimmer
drastal raatz matheus rendell pagallo haussler ragavan
rendell hirsh noordewier theory revision ourston mooney feldman serge koppel thompson et al cohen pazzani kibler
baffes mooney multistrategy approaches flann dietterich towell
shavlik noordeweir dzerisko lavrac bloedorn michalski wnek
clark matwin towell shavlik focus upon handful systems significant underlying similarities work section
analyze miro focl labyrinthk kbann neither mofn grendel
discuss related underlying contributions relationship perspective



miro

drastal raatz seminal work knowledge guided constructive induction takes knowledge low level features interact uses knowledge
construct high level features training examples standard learning
run examples described features domain theory used
shift bias induction utgoff empirical showed
describing examples high level abstract terms improved learning accuracy
miro provides means utilizing knowledge domain theory without
restricted structure theory substructures domain theory
used construct high level features standard induction arrange
concept constructed features used others ignored
others combined low level features still others may used differently
multiple contexts end knowledge domain theory utilized
structure final theory restricted structure initial theory
miro provides exible structure
another benefit miro techniques applied even partial
domain theory exists e domain theory specifies high level features
link together domain theory specifies high level features
others one miro shortcomings provided means making minor changes
miro



fidonoho rendell

domain theory rather constructed features exactly domain theory
specified representation miro constructed features primitive
example met conditions high level feature example miro
behavior given section





labyrinthk

focl





ourston mooney labyrinthk thompson et al
focl pazzani kibler systems represent broad spectrum theory revision
work make steps toward effective integration background knowledge inductive
learning although systems many superficial differences regard supervised unsupervised learning concept description language etc share underlying
principle incrementally revising initial domain theory series local changes
discuss representative class systems theory
revision operators include removing unwanted conditions rule adding needed conditions rule removing rules adding totally rules first classifies
training examples according current theory misclassified seeks repair
theory applying theory revision operator correct classification
previously misclassified examples without losing correct examples thus
series local changes made allow improvement accuracy training
set without losing examples previously classified correctly
type methods provide simple yet powerful tools repairing many important
common faults domain theories fail meet qualities exible representation exible structure theory revision operators make small local
modifications existing domain theory final theory constrained similar
structure initial theory accurate theory significantly different structure initial theory systems forced one two extremes discussed
section first extreme become trapped local maximum similar
initial theory unable reach global maximum local changes made
extreme drop entire rules groups rules replace
rules built scratch thus forfeiting knowledge contained domain theory
carries theory revision steps representation initial
theory consequently representation final theory initial
theory another representation may appropriate revised theory
one initial theory comes facilities provided accommodate
advanced theory revision system would combine locally acting strengths eithertype systems exibility structure exibility representation example
behavior given section



kbann

neither mofn

kbann system towell et al towell shavlik makes unique contributions theory revision work kbann takes initial domain theory described symbolically
logic creates neural network whose structure initial weights encode theory
backpropagation rumelhart hinton mcclelland applied refinement tool fine tuning network weights kbann empirically shown give


firerepresenting restructuring domain theories

significant improvement many theory revision systems widely used promoter
recognition domain although work different implementation kbann
abstract ideologies similar
one kbann important contributions takes domain theory one representation propositional logic translates less restricting representation neural
network logic appropriate representation initial domain theory
promoter neural network representation convenient refining
theory expressing best revised theory change representation
kbann real source power much attention given fact kbann combines symbolic knowledge subsymbolic learner combination viewed
generally means implementing important change representation may
change representation gives kbann power necessarily specific
symbolic subsymbolic implementation thus kbann system embodies higher level
principle allowing refinement occur appropriate representation
alternative representation kbann source power question must raised
whether actual kbann implementation best means achieving
goal neural network representation may expressive required accordingly backpropagation often refinement power needed thus kbann may
carry excess baggage translating neural net representation performing expensive
backpropagation extracting symbolic rules refined network although full
extent kbann power may needed many important may
solvable applying kbann principles symbolic level less expensive tools
neither mofn baffes mooney descendant second example
system allows theory revised representation
initial theory domain theory input neither mofn expressed propositional
logic tree neither mofn interprets theory less rigidly
rule true time n conditions true initially set equal n
conditions must true rule true one theory refinement operator
lower particular rule end examples close enough
partial match initial theory accepted neither mofn since built upon
framework includes theory revision operators add condition
drop condition etc
thus neither mofn allows revision take place representation appropriate
revision appropriate concisely expressing best refined theory neithermofn achieved comparable kbann promoter recognition domain
suggests change representation two systems share
give power rather particular implementation neither mofn
demonstrates small amount representational exibility sometimes enough
n representation employs big change original representation
neural net representation kbann employs yet achieves similar
arrives much quickly kbann baffes mooney
shortcoming neither mofn since acts making local changes
initial theory still become trapped structure initial theory advanced
theory revision system would incorporate neither mofn kbann exibility


fidonoho rendell

representation allow knowledge guided theory restructuring examples
neither mofn behavior given sections





kbann

grendel

cohen analyzes class theory revision systems draws insightful conclusions one generality theory interpretation comes expense power
draws principle fact system focl treats every
domain theory therefore must treat every domain theory general way argues rather applying general refinement strategy
every small set refinement strategies available narrow
enough gain leverage yet narrow apply single cohen
presents grendel toolbox translators transforms domain theory
explicit bias translator interprets domain theory different way
appropriate interpretation applied given
apply cohen principle representation domain theories domain
theories translated representation general adaptable representation used order accommodate general case comes
expense higher computational costs possibly lower accuracy due overfit
stemming unbridled adaptability neural net representation kbann
translates domain theories allows measure partial match domain theory different parts domain theory weighted differently conditions added
dropped domain theory options adaptability probably
necessary may even detrimental options kbann
require computationally expensive backpropagation method
representation used neither mofn adaptable kbann
allow individual parts domain theory weighted differently neithermofn runs quickly kbann small probably matches even
surpasses kbann accuracy many domains domains fine grained weighting
unfruitful even detrimental toolbox theory rerepresentation translators analogous
grendel would allow domain theory translated representation
appropriate forms adaptability

outlook summary

summary brie reexamine exible representation exible structure two
desirable qualities set forth section consider systems exemplify
subset desirable qualities
kbann neither mofn interpreted theory exibly original
representation allowed revised theory adaptable representation
final refined theory often many exceptions rule may tolerate partial
matches missing pieces evidence may weight evidence heavily
evidence kbann neither mofn representation may
concise appropriate representation initial theory
representation allows concise expression otherwise cumbersome final theory
cases principle exible representation


firerepresenting restructuring domain theories

standard induction programs quite successful building concise theories
high predictive accuracy target concept concisely expressed
original set features constructive induction means creating
features target concept concisely expressed miro uses
constructive induction take advantage strengths domain theory
standard induction knowledge theory guides construction appropriate
features standard induction structures concise description
concept thus miro construction coupled standard induction provides
ready powerful means exibly restructuring knowledge contained
initial domain theory case principle exible structure

following section introduce dna promoter recognition domain order
illustrate tangibly systems discussed integrate knowledge
induction

demonstrations related work
section introduces promoter recognition domain harley reynolds noordewier
brie illustrates miro system kbann neithermofn behave domain implemented miro system promoter domain versions neither mofn available ray mooney group
kbann behavior described analyzing towell shavlik chose promoter domain non trivial real world number theory
revision researchers used test work ourston mooney thompson
et al wogulis cohen pazzani kibler baffes mooney
towell shavlik promoter domain one three domains evaluate
work theory guided constructive induction section

promoter recognition domain

promoter sequence region dna marks beginning gene example promoter recognition domain region dna classified promoter
non promoter illustrated figure examples consist features representing sequence dna nucleotides feature take values g c
representing adenine guanine cytosine thymine corresponding dna position
features labeled according position p p zero
position notation p n denotes nucleotide n positions upstream
beginning gene goal predict whether sequence promoter
nucleotides total examples available promoters non promoters
promoter recognition comes initial domain theory shown figure quoted almost verbatim towell shavlik entry uci machine learning
repository theory states promoter sequences must two regions make
contact protein must acceptable conformation pattern
four possibilities contact region minus nucleotides upstream beginning gene match four possibilities satisfy minus
contact condition thus joined disjunction similarly four possibilities


fidonoho rendell

dna sequence

p

p

c g c
figure instance promoter domain consists sequence nucleotides
labeled p p nucleotide take values g c
representing adenine guanine cytosine thymine
contact region minus four acceptable conformation patterns figure
gives pictorial presentation portions theory examples
dataset none matched domain theory exactly yielding accuracy



miro

promoter domain

miro system promoter domain would use rules figure construct high level features dna segment figure shows example
dna segment shown position p position p minus rules
theory shown four features feat minus feat minus
constructed dna segment one minus rule features feat minus feat minus value dna fragment
matches first fourth minus rules likewise feat minus b feat minus c
value dna fragment match second third
rules furthermore since four minus rules joined disjunction feature
feat minus created group would value least one
minus rules matches
features would similarly created minus rules conformation
rules standard induction could applied implemented mirolike system figure gives example theory created drastal original miro used
candidate elimination mitchell underlying induction
used c quinlan opposed theory revision systems incrementally
modify domain theory miro broken theory components
fashioned components theory standard induction program thus
miro exhibited exible structure principle domain restricted
way structure initial theory rather miro exploited strengths
standard induction concisely characterize training examples features


firerepresenting restructuring domain theories

promoters region protein rna polymerase must make contact
helical dna sequence must valid conformation two pieces
contact region spatially align prolog notation used
promoter contact conformation
two regions upstream beginning gene
rna polymerase makes contact
contact

minus minus

following rules describe compositions possible contact regions
minus
minus
minus
minus

p c p p p g p p c
p p p g
p c p
p p p g p p c p
p p p g p p c

minus
minus
minus
minus

p p p p p p
p p
p
p
p p p p p p
p p
p

following rules describe sequences produce acceptable conformations
conformation p c
p
p c
conformation p
conformation p
conformation p
p

p p p p p p c p g
p c p g p c p g p c p c p c
p p
p p p p p p g p
p p p p p p p
p

figure initial domain theory recognizing promoters towell shavlik
weakness miro displays example allows exibility representation
theory representation features constructed miro basically
none representation initial theory dna segment matched rule






promoter domain

system refines initial promoter theory dropping adding
conditions rules simulated turning n option neither
ran promoter domain figure shows refined theory produced
randomly selected training set size initial promoter domain theory
lend revision small local changes limited success


fidonoho rendell

dna sequence

p

p

contact minus

contact minus



c

g c



















g c











g c







g c




















figure contact portion theory four possibilities
minus minus portions theory matches nucleotide
conformation portion theory spread display pictorially
run program exhibited second behavioral extreme discussed section
entirely removed groups rules tried build rules replace
lost minus conformation rules essentially removed rules
added minus group minus rules contain condition
p previously found minus group condition p previously found
conformation group
behavior example direct lack exibility representation exibility structure dicult transform minus conformation
rules something useful initial representation locally acting operators handles dropping sets rules losing knowledge
attempting rediscover lost knowledge empirically end loss
knowledge lower optimal accuracy shown later section



kbann

promoter domain

figure modeled figure towell shavlik shows setup kbann
network promoter theory slot along bottom represents one nucleotide
dna sequence node first level bottom embodies single
domain rule higher levels encode groups rules final concept top
links shown figure ones initially high weighted net next filled
fully connected low weight links backpropagation applied refine
network weights


firerepresenting restructuring domain theories

dna segment fragment


p g p c p p p g p p c p p



minus group rules corresponding constructed features
minus p c p p p g p p c
minus p p p g
p c p
minus p p p g p p c p
minus p p p g p p c

feat minus
feat minus b
feat minus c
feat minus

feat minus feat minus feat minus b feat minus c feat minus

figure example feature construction miro system constructed
features first fourth rules minus group true value
dna segment matches rules constructed feature
entire group feat minus true four minus rules joined
disjunction
feat minus



promoter

feat conf b




feat minus

non promoter

promoter


promoter

figure example theory created miro system dna segment recognized
promoter matches minus rules second conformation
rule fourth minus rule
neural net representation appropriate domain propositional
logic representation initial theory allows measurement partial match
weighting links way subset rule conditions enough surpass
node threshold allows variable weightings different parts theory therefore predictive nucleotides weighted heavily slightly predictive
nucleotides weighted less heavily kbann limited exibility structure
refined network series incremental modifications
initial network fundamental restructuring theory embodies unlikely kbann


fidonoho rendell

promoter contact conformation
contact

minus minus

minus
minus
minus
minus
minus
minus



p
p
p
p g
p g
p

p g
p p c
p c p c
p
p
p g

minus true
conformation true

figure revised theory produce
promoter

contact

conformation
minus

p

minus

dna sequence

p

figure setup kbann network promoter theory
limited finding best network fundamental structure imposed
initial theory
one kbann advantages uses standard learning foundation backpropagation widely used consequently improved previous
researchers theory refinement tools built ground use standard
tool tangentially suffer invent methods handling standard
overfit noisy data etc wealth neural net experience resources
available kbann user neural net technology advances kbann technology
passively advance


firerepresenting restructuring domain theories



neither mofn

promoter domain

refines initial promoter theory dropping adding conditions rules allowing conjunctive rules true subset
conditions true ran neither mofn randomly selected training set size
figure shows refined promoter theory produced theory expressed
n rules would require rules propositional logic initial theory
representation importantly unclear system initial representation would reach rule theory initial theory thus n representation
adopted allows concise expression final theory facilitates
refinement process
neither mofn

promoter contact conformation
contact

minus minus

minus p p p g
p c p
minus p p p g p p c

minus
minus
minus
minus















p
p p

p

p p
p
p
p p p p
p
p

p
p




p g

conformation true

figure revised theory produced neither mofn
neither mofn displays exibility representation allowing n interpretation original propositional logic allow fine grained refinement
kbann allow measure partial match kbann could weight
predictive features heavily example minus rules perhaps p
predictive dna segment promoter p g therefore
weighted heavily neither mofn simply counts number true conditions
rule therefore every condition weighted equally kbann fine grained weighting may
needed domains others may actually detrimental domains
advanced theory revision system offer range representations
kbann neither mofn limited exibility structure refined
theory reached series small incremental modifications initial theory
precluding fundamental restructuring neither mofn therefore limited finding
best theory fundamental structure initial theory

theory guided constructive induction

first half section present guidelines theory guided constructive induction
summarize work discussed sections remainder section


fidonoho rendell

presents instantiates guidelines evaluate
section

guidelines

following guidelines synthesis strengths previously discussed related
work
miro features constructed components domain
theory features combinations existing features final theory
created applying standard induction training examples described
features allows knowledge gleaned initial theory
without forcing final theory conform initial theory backbone structure
takes full advantage domain theory building high level features
original low level features takes advantage strength standard induction
building concise theories high predictive accuracy target concept
concisely expressed given features
constructed features modifiable operators
act locally adding dropping conjuncts constructed feature
kbann neither mofn representation constructed features
need exact representation initial theory given example
initial theory may given set rules written propositional logic
feature constructed rule need boolean feature
telling whether conditions met example may count
many conditions rule met allows final theory formed
expressed representation suitable representation
initial theory
grendel complete system offer library interpreters allowing
domain theory translated range representations differing adaptability one interpreter might emulate miro strictly translating domain theory
boolean constructed features another interpreter might construct features
count number satisfied conditions corresponding component domain theory thus providing measure partial match still another interpreter
might construct features weighted sums satisfied conditions
weights could refined empirically examining set training examples thus
appropriate amount expressive power applied given
without incurring unnecessary expense

specific interpreter

section describes limited instantiation guidelines
described intended demonstration distillation synthesis
principles embodied previous landmark systems contains main module
tgci described figure specific interpreter tgci described figure
main module tgci redescribes training testing examples calling tgci


firerepresenting restructuring domain theories

applies c redescribed examples miro applied candidate
elimination examples redescribing tgci viewed
single interpreter potential grendel toolbox takes input single example
domain theory expressed tree one shown figure
returns vector features example measure partial match
example theory thus creates features components domain theory
miro measures partial match allows exibility representing
information contained initial theory kbann neither mofn one
aspect guidelines appear locally
acting operators adding dropping conditions portion theory
following two paragraphs explain detail workings tgci tgci
respectively
given example e domain theory root node r domain
theory tree leaves conditions
tested true false
return pair p f f f top feature measuring partial
match e whole domain theory f vector features measuring partial match e parts subparts domain theory

r directly testable condition return p r true e
p r false e
n number children r
child rj r call tgci rj e store respective
pj fj fj
major operator r fnew max fj
return p fnew concatenate fnew f f fn
p
major operator r fnew nj fj n
return p fnew concatenate fnew f f fn
major operator r fnew f
return p fnew f
figure tgci
tgci given figure recursive inputs example e
domain theory root node r ultimately returns redescription e form
vector features f returns value f called top feature used
intermediate calculations described base case occurs domain theory
single leaf node e r simple condition case line tgci returns
top feature condition true condition false features
returned base case would simply duplicate existing features


fidonoho rendell

domain theory single leaf node tgci recursively calls r children
line child r rj processed returns vector features fj
measures partial match example j th child r subparts
returns top feature fj included fj marked special
measures partial match example whole j th child r n
children line n vectors features f fn n top features f
fn operator node r line fnew feature created
node maximum fj thus fnew measures closely best r children come
conditions met example vector features returned
case concatenation fnew features r children operator
node r line fnew average fj thus fnew measures closely
r children group come conditions met example
vector features returned case concatenation fnew
features r children operator node r line r
one child fnew f negated thus fnew measures extent conditions
r child met example
given set training examples etrain set testing examples etest
domain theory root node r
return learned concept accuracy testing examples
example ei etrain call tgci r ei returns pi
etrain ffig
example ei etest call tgci r ei returns pi
etest ffi g
call c training examples etrain testing examples
etest return decision tree accuracy etest
figure tgci
tgci called twice two different examples domain theory
two vectors features size furthermore corresponding features
measure match corresponding parts domain theory tgci main module
figure takes advantage creating redescribed example sets input
example sets line redescribes example training set producing training
set line testing set line runs standard induction program
c redescribed example sets returned decision tree easily interpreted
examining features used part domain theory
correspond



tgci

examples

example tgci interpreter works consider toy theory shown
figure tgci redescribes input example constructing feature node


firerepresenting restructuring domain theories

input theory consider situation input example matches conditions
b c e tgci evaluates children node gets values
f f f f f since operator node fnew
average values received children
likewise condition g matchs f h fnew node value
two three matching conditions node give
value negated node since node disjunction
feature measures best partial match two children value
max











b c e



f g h



j k



l n

figure example theory form tree might used
interpreter generate constructed features
figure shows tgci redescribes particular dna segment minus
rules promoter theory partial dna segment shown along four minus
rules feature constructed rule given features names
simplify illustration first rule four six nucleotides match therefore dna segment feat minus value
second rule four five nucleotides match therefore feat minus b
value two minus rules joined disjunction original domain theory feat minus feature constructed
group takes maximum value four children therefore feat minus
value feat minus b value highest group intuitively feat minus represents best partial match grouping extent
disjunction partially satisfied running tgci dna
sequence set redescribed training examples redescribed example value
feat minus feat minus feat minus nodes promoter domain theory training set essentially redescribed feature vector
derived information contained domain theory form shelf
induction program applied example set
anomalous situations created tgci gives good score seemingly
bad example bad score good example situations created
logically equivalent theories give different scores single example occur


fidonoho rendell

dna segment fragment


p g p c p p p g p c p p p



minus group rules corresponding constructed features
minus p c p p p g p p c
minus p p p g
p c p
minus p p p g p p c p
minus p p p g p p c

feat minus
feat minus b
feat minus c
feat minus

feat minus max feat minus feat minus b feat minus c feat minus

figure example tgci generates constructed features portion
promoter domain theory dna segment four conditions first
minus rule match dna segment therefore constructed feature
rule value feat minus
feature entire minus group takes maximum value
children thus embodying best partial match group
biased favor situations matched conditions desirable
matched conditions necessarily better eliminating anomalies
would remove bias
tgci

experiments analysis
section presents applying theory guided constructive induction three
domains promoter domain harley et al primate splice junction domain noordewier shavlik towell gene identification domain craven shavlik
case tgci interpreter applied domain theory examples
order redescribe examples features c quinlan
applied redescribed examples

promoter domain
figure shows learning curve theory guided constructive induction promoter
domain accompanied curves labyrinthk kbann neither mofn
following methodology described towell shavlik set examples
randomly divided training set size testing set size learning
curve created training subsets training set size
examples testing curves labyrinthk kbann
taken ourston mooney thompson langley iba towell


firerepresenting restructuring domain theories

shavlik respectively obtained similar methodology curve
fortgci average independent random data partitions given along
confidence ranges neither mofn program obtained ray mooney group
used generating neither mofn curve data partitions
used tgci




labyrinth k
neither mofn
kbann
tgci
confidence neither mofn
confidence tgci






error

















size training sample

figure learning curves theory guided constructive induction systems
promoter domain
tgci showed improvement labyrinthk portions curve
performed better kbann neither mofn except smallest training sets confidence intervals available labyrinthk



used testing set size use conformation portion domain theory
testing set labyrinthk consisted promoters non promoters
baffes mooney report slightly better learning curve neither mofn obtained
communication paul baffes think difference caused random selection
data partitions




fidonoho rendell

kbann pairwise comparison neither mofn improvement tgci
significant level confidence training sets size larger

structure initial promoter theory


first
conf
rule


first
minus
rule


second
minus
rule


third
minus
rule


fourth
minus
rule


first
minus
rule


second
minus
rule


third
minus
rule


second
conf
rule


third
conf
rule


fourth
conf
rule


fourth
minus
rule

structure revised promoter theory


second
minus
rule


first
minus
rule


second
minus
rule


third
minus
rule


fourth
minus
rule

figure revised theory produced theory guided constructive induction borrowed substructures initial theory whole restricted structure
figure compares initial promoter theory theory created tgci reasons
tgci improvement inferred figure tgci extracted components original theory helpful restructured
concise theory neither kbann neither mofn facilitates radical extraction
restructuring seen leaf nodes theory measures partial match
example components original theory aspect similar kbann
neither mofn
part tgci improvement kbann neither mofn may due knowledge bias con ict latter two systems situation revision biases con ict
knowledge way undo knowledge benefits occur
whenever detailed knowledge opened revision set examples revision
guided examples rather examples interpreted set


firerepresenting restructuring domain theories

algorithmic biases biases useful absence knowledge may undo good
knowledge improperly applied yet biases developed perfected pure induction often unquestioningly applied revision theories biases governing
dropping conditions neither mofn reweighting conditions kbann may
neutralizing promoter theory potential speculate conducted
experiments allowed bias guided dropping adding conditions within tgci
found techniques actually reduced accuracy domain




c
backpropagation
kbann
tgci
confidence tgci
domain theory



error

































size training sample

figure learning curves tgci systems primate splice junction domain

primate splice junction domain

primate splice junction domain noordewier et al involves analyzing dna
sequence identifying boundaries introns exons exons parts
dna sequence kept splicing introns spliced task involves placing


fidonoho rendell

given boundary one three classes intron exon boundary exon intron boundary neither imperfect domain theory available error rate
entire set available examples
figure shows learning curves c backpropagation kbann tgci
primate splice junction domain kbann backpropagation taken
towell shavlik curves plain c tgci
created training sets size testing set size
curves c tgci average independent data partitions
comparison made neither mofn implementation obtained
could handle two class concepts training sets larger kbann tgci
backpropagation performed similarly
accuracy tgci appears slightly worse kbann perhaps significantly kbann advantage tgci ability assign fine grained weightings
individual parts domain theory tgci advantage kbann ability
easily restructure information contained domain theory speculate
kbann capability assign fine grained weights outweighted somewhat rigid structuring domain theory theory guided constructive induction advantage
speed kbann c underlying learner runs much quickly
backpropagation kbann underlying learning

gene identification domain
gene identification domain craven shavlik involves classifying given dna
segment coding sequence one codes protein non coding sequence
domain theory available gene identification domain therefore created
artificial domain theory information organisms may favor certain nucleotide
triplets others gene coding domain theory embodies knowledge dna
segment likely gene coding segment triplets coding favoring triplets
triplets noncoding favoring triplets decision triplets codingfavoring noncoding favoring favored neither made empirically
analyzing makeup coding noncoding sequences specific artificial domain theory used described online appendix
figure shows learning curves c tgci gene identification domain
original domain theory yields error curves created training
example sets size testing separate example set size
curves average independent data partitions
partial curve given neither mofn became prohibitively slow
larger training sets promoter domain training sets smaller
tgci neither mofn ran comparable speeds approximately seconds sun
workstation domain tgci ran approximately minutes larger training sets
neither mofn took times long tgci training sets size times
long size times long size consequently neither mofn
curve extends represents five randomly selected data partitions
reasons solid comparison neither mofn tgci cannot made
curves appears tgci accuracy slightly better speculate neither

firerepresenting restructuring domain theories





tgci
confidence tgci
c
neither mofn
domain theory



error




















number training examples

figure learning curves tgci systems gene identification domain
slightly lower accuracy partially due fact revises theory
correctly classify training examples theory likely overfits
training examples tgci need explicitly avoid overfit handled
underlying learner
mofn

summary experiments
goal present technique rather understand
behavior landmark systems distill strengths synthesize simple
system tgci evaluation shows accuracy roughly matches
exceeds predecessors promoter domain tgci showed sizable improvement
many published splice junction domain tgci narrowly falls short
kbann accuracy gene identification domain tgci outperforms neither mofn
domains tgci greatly improves original theory alone c alone


fidonoho rendell

faster closest competitors tgci runs much times faster
large datasets strict quantitative comparison speeds tgci
kbann made backpropagation known much slower
decision trees mooney shavlik towell gove kbann uses multiple hidden
layers makes training time even longer towell shavlik towell
shavlik point run kbann must made multiple times
different initial random weights whereas single run tgci sucient
overall experiments support two claims first accuracy tgci
substantiates delineation system strengths terms exible theory representation
exible theory structure since characterization basis
design second tgci combination speed accuracy suggest unnecessary computational complexity avoided synthesizing strengths landmark systems
following section take closer look strengths theory guided constructive
induction
tgci

neither mofn

discussion strengths
number strengths theory guided constructive induction discussed within
context tgci used experiments

flexible representation

discussed section many domains representation appropriate
initial theory may appropriate refined theory theory guided constructive induction allows translation initial theory different representation
accommodate domains experiments representation
needed allowed measurement partial match domain theory tgci
accomplished simply counting matching features propagating information theory appropriately labyrinthk easily afford
measure partial match therefore appropriate best
representation final theory initial theory kbann allows
finer grained measurement partial match neither mofn work
price paid computational complexity theory guided constructive induction framework allows variety potential tools varying degrees granularity
partial match although one tool used experiments

flexible structure

discussed section strength existing induction programs fashioning concise
highly predictive description concept target concept concisely
described given features consequently value domain theory lies
overall structure feature language sucient induction program build
good overall theory structure instead value domain theory lies information
contains redescribe examples high level features high level
features facilitate concise description target concept systems
neither mofn reach final theory series modifications initial


firerepresenting restructuring domain theories

theory hope gain something keeping theory overall structure intact initial
theory suciently close accurate theory method works often clinging
structure hinders full exploitation domain theory theory guided constructive
induction provides means fully exploiting information domain theory
strengths existing induction programs figure section gives comparison
structure initial promoter theory structure revised theory produced
theory guided constructive induction substructures borrowed revised
theory whole restructured

use standard induction underlying learner

theory guided constructive induction uses standard induction program
underlying learner need reinvent solutions overfit avoidance multi class
concepts noisy data etc overfit avoidance widely studied standard induction
many standard techniques exist system modifies theory accommodate
set training examples must address issue overfit training examples
many theory revision systems existing overfit avoidance techniques cannot easily adapted
must addressed scratch theory guided constructive induction
take advantage full range previous work overfit avoidance standard induction
multiple theory parts available multi class concepts interpreter
run multiple theory parts resulting feature sets combined
primate splice junction domain presented section three classes intron exon
boundaries exon intron boundaries neither theories given intron exon
exon intron theories used create features features
concatenated together learning interpreters tgci trivially handle
negation domain theory

use theory fragments

theory guided constructive induction limited full domain theories
part theory available used demonstrate three experiments
run fragments promoter domain theory used first
experiment four minus rules used five features constructed one
feature rule additional feature group similar experiments
run minus group conformation group
figure gives learning curves three experiments along curves entire theory theory c original features although conformation
portion theory gives significant improvement c minus
minus portions theory give significant improvements performance thus even
partial theories theory fragments used theory guided constructive induction
yield sizable performance improvements
use theory fragments explored means evaluating contribution
different parts theory figure conformation portion theory shown
yield improvement could signal knowledge engineer knowledge
conveyed portion theory useful learner
present form


fidonoho rendell


c
conformation portion theory
minus portion theory
minus portion theory
whole theory








error


















size training sample

figure learning curves theory guided constructive induction fragments
promoter domain theory minus portion theory minus
portion theory conformation portion theory used
separately feature construction curves given full theory
c alone comparison

use multiple theories

theory guided constructive induction use multiple competing even incompatible
domain theories multiple theories exist theory guided constructive induction provides
natural means integrating way extract best theories
tgci would called input theory producing features next
features simply pooled together induction program selects among
fashioning final theory seen small scale promoter domain


firerepresenting restructuring domain theories

error

figure minus rules subsume minus rules according entry
uci database biological evidence inconclusive respect
correct specificity handled simply four possibilities selection
useful knowledge left induction program
tgci could used evaluate contributions competing theories
used evaluate theory fragments knowledge engineer could use evaluation
guide revision synthesis competing theories












tgci c
tgci lfc
confidence lfc




size training sample

figure theory guided constructive induction lfc c underlying
learning system theory guided constructive induction use inductive
learner underlying learning component therefore sophisticated
underlying induction programs improve accuracy

easy adoption techniques

since theory guided constructive induction use standard induction method
underlying learner improvements made standard induction theory guided constructive induction passively improves demonstrate tests run lfc
ragavan rendell underlying induction program lfc decision tree
learner performs example constructive induction looking ahead combinations features characteristically lfc improves accuracy moderate number
examples figure shows resulting learning curve along c tgci curve
curves average separate runs data partitions used
program pairwise comparison improvement lfc c significant
level confidence training sets size sophisticated underlying
induction programs improve accuracy


fidonoho rendell

testing limits tgci
purpose section explore performance theory guided constructive
induction theory revision ranging easy dicult easy
underlying concept embodied training testing examples matches domain
theory fairly closely therefore examples match domain theory fairly
closely dicult underlying concept embodied examples
match domain theory well examples although many
factors determine diculty individual aspect important component worth exploring experiment section intended relate ranges
diculty amount improvement produced tgci
since number factors affect diculty chose theory revision
experiment variations single
able hold factors constant vary closeness match domain
theory wanted avoid totally artificial domains chose start
promoter domain create domains perverting example set
domains created perverting examples original promoter
closely match promoter domain theory less closely match
promoter domain theory positive examples altered example one domain
created fewer matches domain theory original promoter
domain follows feature value given example examined see matched
part theory probability randomly reassigned value
set possible values feature end set examples
fewer matches domain theory original example set experiment
domains created fewer matches
features multiple values may match theory different disjuncts
theory specify different values single feature example referring back
figure feature p matches two minus rules value another
two rules value single feature might accidentally match one part
theory fact example whole closely matches another part theory
cases true matches separated accidental matches examining
part theory clearly matched example whole expecting
match part theory
domains closely matched theory created similar manner
example domain created fewer mismatches domain theory
original promoter domain follows feature value given example examined
see matched corresponding part theory probability
reassigned value matched theory end set examples
mismatches domain theory eliminated experiment
domains created fewer mismatches
ten different example sets created level closeness domain theory
fewer matches fewer mismatches total forty
example sets created matched original theory less closely original
precisely would slightly matches fewer matches features
would randomly reassigned back original matching value



firerepresenting restructuring domain theories






error



c
tgci






























closeness theory

figure seven altered promoter domains created three closely matched
theory original domains four less closely matched
x axis indicates domain positive examples match
domain theory negative indicates domain match
positive examples domain theory purely chance accuracy
c tgci plotted different levels proximity domain
theory

example set thirty example sets created matched original theory
closely original example set example sets tested leaveone methodology c tgci summarized
figure x axis measure theory proximity closeness example set
domain theory x axis indicates change original promoter examples
x axis means positive example exactly matches domain theory
x axis means match feature value positive example


fidonoho rendell

domain theory totally chance datapoint figure averaging
accuracies ten example sets level theory proximity except
point zero accuracy exact original promoter examples
one notable portion figure section x axis domains
region greater trivial level mismatch domain theory
moderate mismatch region tgci best performance
domains tgci achieves high accuracy standard learner c original
feature set gives mediocre performance second region examine
x axis level mismatch ranges moderate extreme region
tgci performance falls improvement original feature set remains high
shown figure plots improvement tgci c final two
regions notice greater less x axis level
mismatch theory examples becomes trivially small x axis greater
c able pick theory patterns leading high accuracy approaches
tgci level mismatch becomes extreme x axis less theory gives
little help solving resulting similarly poor accuracy methods
summary shown figure variants promoter wide range
theory proximity centered around real promoter theory guided
constructive induction yields sizable improvement standard learners



error difference

error






























closeness theory

figure difference error c tgci different levels proximity
example set domain theory

scale left half graph may directly comparable scale
right half graph since equal number matches mismatches
original examples



firerepresenting restructuring domain theories

conclusion
goal present another system rather
study two qualities exible representation exible structure capabilities
intended frame reference analyzing theory guided systems two principles
provide guidelines purposeful design distilled essence systems
miro kbann neither mofn theory guided constructive induction
natural synthesis strengths experiments demonstrated even
simple application two principles effectively integrate theory knowledge
training examples yet much room improvement two principles could
quantified made precise implementations proceed
explored refined
quantifying representational exibility one step section gave three degrees
exibility one measured exact match theory one counted number matching
conditions one allowed weighted sum matching conditions amount
exibility quantified finer grained degrees exibility explored
accuracy assorted domains evaluated function representational
exibility
finer grained structural exibility would advantageous presented systems
make small incremental modifications theory lacking structural exibility yet
theory guided constructive induction falls extreme perhaps allowing excessive
structural exibility fortunately existing induction tools capable fashioning simple
yet highly predictive theory structures features suitably high level
nevertheless approaches explored take advantage structure
initial theory without unduly restricted
strength discussed section given attention although
promoter domain gives small example synthesizing competing theories
explored domain entire competing inconsistent theories available
synthesizing knowledge given multiple experts point made section
tgci use theory fragments evaluate contribution different parts
theory explored
exploration bias standard induction utgoff refers biases ranging
weak strong incorrect correct strong bias restricts concepts
represented weak bias thus providing guidance learning
bias becomes stronger may become incorrect ruling useful concept descriptions similar situation arises theory revision theory representation language
inappropriately rigid may impose strong incorrect bias revision language
allows adaptability along many dimensions may provide weak bias grendellike toolbox would allow theory translated range representations
varying dimensions adaptability utgoff advocates starting strong possibly incorrect bias shifting appropriately weak correct bias similarly theory could
translated successively adaptable representations appropriate bias
found implemented single tool many open remain along line



fidonoho rendell

converse relationship theory revision constructive induction warrants
examination theory revision uses data improve theory constructive induction
use theory improve data facilitate learning since long term goal machine
learning use data inference theory improve believe
consideration related methods beneficial particularly
area strengths lacks
analysis landmark theory revision theory guided learning systems led
two principles exible representation exible structure theory guided
constructive induction upon high level principles simple yet achieves
good accuracy principles provide guidelines future work yet discussed
principles imprecise call exploration

acknowledgements
would thank geoff towell kevin thompson ray mooney jeff mahoney
assistance getting datapoints kbann labyrinthk would
thank paul baffes making neither program available advice
setting program parameters thank anonymous reviewers constructive
criticism earlier draft gratefully acknowledge support
work dod graduate fellowship nsf grant iri

references
baffes p mooney r symbolic revision theories n rules
proceedings ijcai
bloedorn e michalski r wnek j multistrategy constructive induction
aq mci proceeding second international workshop multistrategy learning
clark p matwin qualitative guide inductive learning
proceedings international conference machine learning
cohen w compiling prior knowledge explicit bias proceedings
international conference machine learning
craven w shavlik j w investigating value good input representation computational learning theory natural learning systems forthcoming
drastal g raatz empirical learning abstraction space
proceedings ijcai
dzerisko lavrac n learning relations noisy examples empirical
comparison linus foil proceedings international conference
machine learning


firerepresenting restructuring domain theories

feldman r serge koppel incremental refinement approximate
domain theories proceedings international conference machine
learning
flann n dietterich study explanation methods inductive
learning machine learning
fu l buchanan b g learning intermediate concepts constructing
hierarchical knowledge base proceedings ijcai
harley c reynolds r noordewier creators original promoter dataset
hirsh h noordewier background knowledge improve inductive
learning dna sequences tenth ieee conference ai applications san
antonio tx
matheus c j rendell l constructive induction decision trees
proceedings ijcai
michalski r theory methodology inductive learning artificial intelligence
mitchell version spaces candidate elimination rule learning
proceedings ijcai
mooney r j induction unexplained overly general domain theories
aid concept learning machine learning
mooney r j shavlik j w towell g g gove experimental comparison symbolic connectionist learning proceedings
ijcai
murphy p pazzani id constructive induction n concepts
discriminators decision trees proceedings international conference
machine learning
noordewier shavlik j towell g donors original primate splice junction
dataset
ourston mooney r changing rules comprehensive theory
refinement proceedings national conference artificial intelligence
pagallo g haussler boolean feature discovery empirical learning machine
learning
pazzani kibler utility knowledge inductive learning machine
learning
quinlan j r c programs machine learning san mateo ca morgan
kaufmann


fidonoho rendell

ragavan h rendell l lookahead feature construction learning hard concepts proceedings international conference machine learning
rumelhart e hinton g e mcclelland j l general framework
parallel distributed processing rumelhart e mcclelland j l eds
parallel distributed processing explorations microarchitecture cognition
cambridge mit press
schlimmer j c learning representation change kaufmann ed
proceedings national conference artificial intelligence
thompson k langley p iba w background knowledge concept
formation proceedings international conference machine learning
towell g shavlik j knowledge artificial neural networks artificial
intelligence
towell g shavlik j noordeweir refinement approximately correct
domain theories knowledge neural networks proceedings
national conference artificial intelligence
utgoff p e shift bias inductive concept learning michalski carbonell
mitchell eds machine learning vol chap pp san mateo ca
morgan kaufmann
wogulis j revising relational domain theories proceedings international conference machine learning




