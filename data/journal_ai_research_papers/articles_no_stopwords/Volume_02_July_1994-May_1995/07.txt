Journal Artificial Intelligence Research 2 (1995) 369-409

Submitted 10/94; published 3/95

Cost-Sensitive Classification: Empirical Evaluation
Hybrid Genetic Decision Tree Induction Algorithm
Peter D. Turney
Knowledge Systems Laboratory, Institute Information Technology
National Research Council Canada, Ottawa, Ontario, Canada, K1A 0R6.

TURNEY@AI.IIT.NRC.CA

Abstract
paper introduces ICET, new algorithm cost-sensitive classification. ICET
uses genetic algorithm evolve population biases decision tree induction algorithm. fitness function genetic algorithm average cost classification
using decision tree, including costs tests (features, measurements)
costs classification errors. ICET compared three algorithms
cost-sensitive classification EG2, CS-ID3, IDX C4.5, classifies without regard cost. five algorithms evaluated empirically five realworld medical datasets. Three sets experiments performed. first set examines
baseline performance five algorithms five datasets establishes ICET
performs significantly better competitors. second set tests robustness
ICET variety conditions shows ICET maintains advantage. third
set looks ICETs search bias space discovers way improve search.

1. Introduction
prototypical example problem cost-sensitive classification medical diagnosis, doctor would balance costs various possible medical tests
expected benefits tests patient. several aspects problem:
benefit test, terms accurate diagnosis, justify cost test?
time stop testing make commitment particular diagnosis? much
time spent pondering issues? extensive examination various
possible sequences tests yield significant improvement simpler, heuristic choice
tests? questions investigated here.
words cost, expense, benefit used paper broadest sense,
include factors quality life, addition economic monetary cost. Cost
domain-specific quantified arbitrary units. assumed costs tests
measured units benefits correct classification. Benefit treated
negative cost.
paper introduces new algorithm cost-sensitive classification, called ICET
(Inexpensive Classification Expensive Tests pronounced iced tea). ICET uses
genetic algorithm (Grefenstette, 1986) evolve population biases decision tree
induction algorithm (a modified version C4.5, Quinlan, 1992). fitness function
genetic algorithm average cost classification using decision tree, including
costs tests (features, measurements) costs classification errors. ICET
following features: (1) sensitive test costs. (2) sensitive classification
error costs. (3) combines greedy search heuristic genetic search algorithm. (4)
handle conditional costs, cost one test conditional whether second

1995 National Research Council Canada. rights reserved. Published permission.

fiT URNEY

test selected yet. (5) distinguishes tests immediate results tests
delayed results.
problem cost-sensitive classification arises frequently. problem medical
diagnosis (Nez, 1988, 1991), robotics (Tan & Schlimmer, 1989, 1990; Tan, 1993), industrial production processes (Verdenius, 1991), communication network troubleshooting
(Lirov & Yue, 1991), machinery diagnosis (where main cost skilled labor), automated
testing electronic equipment (where main cost time), many areas.
several machine learning algorithms consider costs tests,
EG2 (Nez, 1988, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX
(Norton, 1989). several algorithms consider costs classification
errors (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon &
Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al.,
1994). However, little work considers costs together.
good reasons considering costs tests costs classification errors. agent cannot rationally determine whether test performed without
knowing costs correct incorrect classification. agent must balance cost
test contribution test accurate classification. agent must consider testing economically justified. often happens benefits
testing worth costs tests. means cost must assigned
tests classification errors.
Another limitation many existing cost-sensitive classification algorithms (EG2, CSID3) use greedy heuristics, select step whatever test contributes
accuracy least cost. sophisticated approach would evaluate interactions among tests sequence tests. test appears useful considered isolation,
using greedy heuristic, may appear useful considered combination
tests. Past work demonstrated sophisticated algorithms superior
performance (Tcheng et al., 1989; Ragavan & Rendell, 1993; Norton, 1989; Schaffer, 1993;
Rymon, 1993; Seshu, 1989; Provost, 1994; Provost & Buchanan, press).
Section 2 discusses decision tree natural form knowledge representation
classification expensive tests measure average cost classification
decision tree. Section 3 introduces five algorithms examine here, C4.5
(Quinlan, 1992), EG2 (Nez, 1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993),
IDX (Norton, 1989), ICET. five algorithms evaluated empirically five realworld medical datasets. datasets discussed detail Appendix A. Section 4 presents three sets experiments. first set (Section 4.1) experiments examines baseline performance five algorithms five datasets establishes ICET
performs significantly better competitors given datasets. second set (Section 4.2) tests robustness ICET variety conditions shows ICET
maintains advantage. third set (Section 4.3) looks ICETs search bias space
discovers way improve search. discuss related work future work Section 5. end summary learned research statement
general motivation type research.

2. Cost-Sensitive Classification
section first explains decision tree natural form knowledge representation classification expensive tests. discusses measure average
cost classification decision tree. method measuring average cost handles
370

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

aspects problem typically ignored. method applied standard
classification decision tree, regardless tree generated. end discussion
relation cost accuracy.
2.1

Decision Trees Cost-Sensitive Classification

decision trees used decision theory (Pearl, 1988) somewhat different
classification decision trees typically used machine learning (Quinlan, 1992).
refer decision trees paper, mean standard classification decision
trees machine learning. claims make classification decision trees
apply decision theoretical decision trees, modification. full discussion
decision theoretical decision trees outside scope paper.
decision test must based cost tests cost classification errors. test costs $10 maximum penalty classification error $5,
clearly point test. hand, penalty classification
error $10,000, test may quite worthwhile, even information content relatively low. Past work algorithms sensitive test costs (Nez, 1988, 1991;
Tan, 1993; Norton, 1989) overlooked importance considering cost classification errors.
tests inexpensive, relative cost classification errors, may rational
tests (i.e., measure features; determine values attributes) seem possibly relevant. kind situation, convenient separate selection tests
process making classification. First decide set tests relevant, focus problem learning classify case, using results
tests. common approach classification machine learning literature.
Often paper focuses problem learning classify case, without mention
decisions involved selecting set relevant tests.1
tests expensive, relative cost classification errors, may suboptimal separate selection tests process making classification. may
able achieve much lower costs interleaving two. First choose test,
examine test result. result test gives us information, use influence choice next test. point, decide cost tests
justified, stop testing make classification.
selection tests interleaved classification way, decision tree
natural form representation. root decision tree represents first test
choose. next level decision tree represents next test choose.
decision tree explicitly shows outcome first test determines choice
second test. leaf represents point decide stop testing make classification.
Decision theory used define constitutes optimal decision tree, given (1)
costs tests, (2) costs classification errors, (3) conditional probabilities
test results, given sequences prior test results, (4) conditional probabilities
classes, given sequences test results. However, searching optimal tree infeasible
(Pearl, 1988). ICET designed find good (but necessarily optimal) tree,
good defined better competition (i.e., IDX, CS-ID3, EG2).
1. papers this. Decision tree induction algorithms C4.5 (Quinlan, 1992) automatically
select relevant tests. Aha Bankert (1994), among others, used sequential test selection procedures
conjunction supervised learning algorithm.

371

fiT URNEY

2.2

Calculating Average Cost Classification

section, describe calculate average cost classification decision
tree, given set testing data. method described applied uniformly decision trees generated five algorithms examined (EG2, CS-ID3, IDX, C4.5,
ICET). method assumes standard classification decision tree (such generated
C4.5); makes assumptions tree generated. purpose
method give plausible estimate average cost expected real-world
application decision tree.
assume dataset split training set testing set.
expected cost classification estimated average cost classification testing set. average cost classification calculated dividing total cost
whole testing set number cases testing set. total cost includes
costs tests costs classification errors. simplest case, assume
specify test costs simply listing test, paired corresponding cost.
complex cases considered later section. assume specify
costs classification errors using classification cost matrix.
Suppose c distinct classes. classification cost matrix c c matrix,
element C i, j cost guessing case belongs class i, actually
belongs class j. need assume constraints matrix, except costs
finite, real values. allow negative costs, interpreted benefits. However, experiments reported here, restricted attention classification cost
matrices diagonal elements zero (we assume correct classification
cost) off-diagonal elements positive numbers. 2
calculate cost particular case, follow path decision tree.
add cost test chosen (i.e., test occurs path root
leaf). test appears twice, charge first occurrence test.
example, one node path may say patient age less 10 years another node
may say patient age 5 years, charge cost determining patients age. leaf tree specifies trees guess class case.
Given actual class case, use cost matrix determine cost trees
guess. cost added costs tests, determine total cost classification
case.
core method calculating average cost classification decision tree. two additional elements method, handling conditional test
costs delayed test results.
allow cost test conditional choice prior tests. Specifically,
consider case group tests shares common cost. example, set blood
tests shares common cost collecting blood patient. common cost
charged once, decision made first blood test. charge
collecting blood second blood test, since may use blood collected
first blood test. Thus cost test group conditional whether another
member group already chosen.
Common costs appear frequently testing. example, diagnosis aircraft
engine, group tests may share common cost removing engine plane
2. restriction seems reasonable starting point exploring cost-sensitive classification. future work,
investigate effects weakening restriction.

372

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

installing test cell. semiconductor manufacturing, group tests may share
common cost reserving region silicon wafer special test structure. image
recognition, group image processing algorithms may share common preprocessing
algorithm. examples show realistic assessment cost using decision
tree frequently need make allowances conditional test costs.
often happens result test available immediately. example,
medical doctor typically sends blood test laboratory gets result next day.
allow test labelled either immediate delayed. test delayed, cannot
use outcome influence choice next test. example, blood tests
delayed, cannot allow outcome one blood test play role decision
second blood test. must make commitment (or doing) second
blood test know results first blood test.
Delayed tests relatively common. example, many medical tests must shipped
laboratory analysis. gas turbine engine diagnosis, main fuel control frequently shipped specialized company diagnosis repair. classification problem requires multiple experts, one experts might immediately available.
handle immediate tests decision tree described above. handle delayed tests
follows. follow path case root decision tree appropriate
leaf. encounter node, anywhere along path, delayed test,
committed performing tests subtree rooted node. Since
cannot make decision tests node conditional outcome test
node, must pledge pay tests might possibly need perform,
point onwards decision tree.
method handling delayed tests may seem bit puzzling first. difficulty
decision tree combines method selecting tests method classifying
cases. tests delayed, forced proceed two phases. first phase,
select tests. second phase, collect test results classify case. example,
doctor collects blood patient sends blood laboratory. doctor must tell
laboratory tests done blood. next day, doctor gets results
tests laboratory decides diagnosis patient. decision
tree naturally handle situation this, selection tests isolated
classification cases. method, first phase, doctor uses decision
tree select tests. long tests immediate, problem. soon
first delayed test encountered, doctor must select tests might possibly
needed second phase.3 is, doctor must select tests subtree rooted
first delayed test. second phase, test results arrive next day,
doctor information required go root tree leaf, make
classification. doctor must pay tests subtree, even though
tests along one branch subtree actually used. doctor know
advance branch actually used, time necessary order
blood tests. laboratory blood tests naturally want doctor pay
tests ordered, even used making diagnosis.
general, makes sense desired immediate tests
desired delayed tests, since outcome immediate test used influence
decision delayed test, vice versa. example, medical doctor question
3. simplification situation real world. realistic treatment delayed tests one
areas future work (Section 5.2).

373

fiT URNEY

patient (questions immediate tests) deciding blood tests order (blood
tests delayed tests).4
tests delayed (as BUPA data Appendix A.1),
must decide advance (before see test results) tests performed.
given decision tree, total cost tests cases. situations
type, problem minimizing cost simplifies problem choosing best subset
set available tests (Aha Bankert, 1994). sequential order tests
longer important reducing cost.
Let us consider simple example illustrate method. Table 1 shows test costs
four tests. Two tests immediate two delayed. two delayed tests
share common cost $2.00. two classes, 0 1. Table 2 shows classification cost matrix. Figure 1 shows decision tree. Table 3 traces path tree
particular case shows cost calculated. first step test root
tree (test alpha). second step, encounter delayed test (delta), must
calculate cost entire subtree rooted node. Note epsilon costs $8.00,
since already selected delta, delta epsilon common cost. third
step, test epsilon, need pay, since already paid second step.
fourth step, guess class case. Unfortunately, guess incorrectly,
pay penalty $50.00.
Table 1: Test costs simple example.
Test

Group

Cost

Delayed

1

alpha

$5.00



2

beta

$10.00



3

delta



$7.00 first test group A,
$5.00 otherwise

yes

4

epsilon



$10.00 first test group A,
$8.00 otherwise

yes

Table 2: Classification costs simple example.
Actual Class

Guess Class

Cost

0

0

$0.00

0

1

$50.00

1

0

$50.00

1

1

$0.00

4. real world, many factors influence sequence tests, length delay
probability delayed test needed. ignore many factors pay attention
simplified model presented here, makes sense desired immediate tests
desired delayed tests. know extent actually occurs real world. One
complication medical doctors industrialized countries directly affected cost
tests select. fact, fear law suits gives incentive order unnecessary tests.

374

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

alpha < 3


F

beta > 6


delta = 2
F

0



F

1
beta < 5


epsilon < 4
F

1



0

0

F

1

Figure 1: Decision tree simple example.

Table 3: Calculating cost particular case.
Step

Action

Result

Cost

1

alpha

alpha = 6

$5.00

2

delta

delta = 3

$7.00 + $10.00 + $8.00 = $25.00

3

epsilon

epsilon = 2

already paid, step #2

4

guess class = 0

actual class = 1

$50.00

total cost

$80.00

summary, section presents method estimating average cost using
given decision tree. decision tree standard classification decision tree;
special assumptions made tree; matter tree generated.
method requires (1) decision tree (Figure 1), (2) information calculation test
costs (Table 1), (3) classification cost matrix (Table 2), (4) set testing data (Table
3). method (i) sensitive cost tests, (ii) sensitive cost classification
errors, (iii) capable handling conditional test costs, (iv) capable handling delayed
tests. experiments reported Section 4, method applied uniformly five
algorithms.
2.3

Cost Accuracy

method calculating cost explicitly deal accuracy; however,
handle accuracy special case. test cost set $0.00 tests classification cost matrix set positive constant value k guess class equal
actual class j, set $0.00 equals j, average total cost using
decision tree pk , p [0,1] frequency errors testing dataset
375

fiT URNEY

100 ( 1 p ) percentage accuracy testing dataset. Thus linear relationship average total cost percentage accuracy, situation.
generally, let C classification cost matrix cost x diagonal,
C i, = x , cost diagonal, ( j ) ( C i, j = ) , x less y, x < .
call type classification cost matrix simple classification cost matrix. cost
matrix simple called complex classification cost matrix.5
simple cost matrix test costs zero (equivalently, test costs ignored), minimizing
cost exactly equivalent maximizing accuracy.
follows algorithm sensitive misclassification error costs
ignores test costs (Breiman et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974;
Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press;
Knoll et al., 1994) interesting complex cost matrix.
simple cost matrix, algorithm CART (Breiman et al., 1984) sensitive
misclassification error cost advantage algorithm C4.5 (Quinlan, 1992)
maximizes accuracy (assuming differences two algorithms negligible). experiments paper use simple cost matrix (the exception
Section 4.2.3). Therefore focus comparison ICET algorithms sensitive
test cost (IDX, CS-ID3, EG2). future work, examine complex cost matrices
compare ICET algorithms sensitive misclassification error cost.
difficult find information costs misclassification errors medical practice, seems likely complex cost matrix appropriate simple cost
matrix medical applications. paper focuses simple cost matrices because,
research strategy, seems wise start simple cases attempt complex cases.
Provost (Provost, 1994; Provost & Buchanan, press) combines accuracy classification error cost using following formula:
score = accuracy B cost

(1)

formula, B arbitrary weights user set particular application. accuracy cost, defined Provost (Provost, 1994; Provost & Buchanan, press), represented using classification cost matrices. represent
accuracy using simple cost matrix. interesting applications, cost represented complex cost matrix. Thus score weighted sum two classification cost
matrices, means score classification cost matrix. shows
equation (1) handled special case method presented here. loss
information translation Provosts formula cost matrix. mean
criteria represented costs. example criterion cannot represented cost stability (Turney, press).

3. Algorithms
section discusses algorithms used paper: C4.5 (Quinlan, 1992), EG2 (Nez,
1991), CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993), IDX (Norton, 1989), ICET.

5. occasionally say simple cost matrix complex cost matrix. cause confusion,
since test costs represented matrix.

376

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

3.1

C4.5

C4.5 (Quinlan, 1992) builds decision tree using standard TDIDT (top-down induction
decision trees) approach, recursively partitioning data smaller subsets, based
value attribute. step construction decision tree, C4.5 selects
attribute maximizes information gain ratio. induced decision tree pruned
using pessimistic error estimation (Quinlan, 1992). several parameters
adjusted alter behavior C4.5. experiments C4.5, used default settings parameters. used C4.5 source code distributed (Quinlan,
1992).
3.2

EG2

EG2 (Nez, 1991) TDIDT algorithm uses Information Cost Function (ICF)
(Nez, 1991) selection attributes. ICF selects attributes based information gain cost. implemented EG2 modifying C4.5 source code
ICF used instead information gain ratio.
ICF i-th attribute, ICF , defined follows:6


2 1
ICF = ------------------------
( Ci + 1)

0 1

(2)

equation, information gain associated i-th attribute given stage
construction decision tree C cost measuring i-th attribute. C4.5
selects attribute maximizes information gain ratio, function
information gain . modified C4.5 selects attribute maximizes ICF .
parameter adjusts strength bias towards lower cost attributes.
= 0 , cost ignored selection ICF equivalent selection .
= 1 , ICF strongly biased cost. Ideally, would selected way sensitive classification error cost (this done ICET see Section 3.5). Nez (1991)
suggest principled way setting . experiments EG2, set 1.
words, used following selection measure:


2 1
----------------Ci + 1

(3)

addition sensitivity cost tests, EG2 generalizes attributes using ISA
tree (a generalization hierarchy). implement aspect EG2, since
relevant experiments reported here.
3.3

CS-ID3

CS-ID3 (Tan & Schlimmer, 1989, 1990; Tan, 1993) TDIDT algorithm selects
attribute maximizes following heuristic function:
2

( )
--------------Ci

(4)

6. inverse ICF, defined Nez (1991). Nez minimizes criterion. facilitate comparison
algorithms, use equation (2). criterion intended maximized.

377

fiT URNEY

implemented CS-ID3 modifying C4.5 selects attribute maximizes
(4).
CS-ID3 uses lazy evaluation strategy. constructs part decision tree
classifies current case. implement aspect CS-ID3, since
relevant experiments reported here.
3.4

IDX

IDX (Norton, 1989) TDIDT algorithm selects attribute maximizes following heuristic function:

------Ci

(5)

implemented IDX modifying C4.5 selects attribute maximizes (5).
C4.5 uses greedy search strategy chooses step attribute highest
information gain ratio. IDX uses lookahead strategy looks n tests ahead, n
parameter may set user. implement aspect IDX. lookahead strategy would perhaps make IDX competitive ICET, would complicate comparison heuristic function (5) heuristics (3) (4) used EG2
CS-ID3.
3.5

ICET

ICET hybrid genetic algorithm decision tree induction algorithm. genetic
algorithm evolves population biases decision tree induction algorithm.
genetic algorithm use GENESIS (Grefenstette, 1986).7 decision tree induction
algorithm C4.5 (Quinlan, 1992), modified use ICF. is, decision tree induction
algorithm EG2, implemented described Section 3.2.
ICET uses two-tiered search strategy. bottom tier, EG2 performs greedy
search space decision trees, using standard TDIDT strategy. top
tier, GENESIS performs genetic search space biases. biases used
modify behavior EG2. words, GENESIS controls EG2s preference one
type decision tree another.
ICET use EG2 way designed used. n costs, C , used
EG2s attribute selection function, treated ICET bias parameters, costs.
is, ICET manipulates bias EG2 adjusting parameters, C . ICET, values
bias parameters, C , direct connection actual costs tests.
Genetic algorithms inspired biological evolution. individuals evolved
GENESIS strings bits. GENESIS begins population randomly generated
individuals (bit strings) measures fitness individual. ICET,
individual (a bit string) represents bias EG2. individual evaluated running EG2
data, using bias given individual. fitness individual average cost classification decision tree generated EG2. next generation, population replaced new individuals. new individuals generated
previous generation, using mutation crossover (sex). fittest individuals
first generation offspring second generation. fixed number
7. used GENESIS Version 5.0, available URL ftp://ftp.aic.nrl.navy.mil/pub/galist/src/ga/genesis.tar.Z ftp://alife.santafe.edu/pub/USER-AREA/EC/GA/src/gensis-5.0.tar.gz.

378

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

generations, ICET halts output decision tree determined fittest individual. Figure 2 gives sketch ICET algorithm.

GENESIS
fittest

genetic algorithm

decision tree
population
biases
EG2

data

fitness

decision tree

classifier

function
EG2
decision tree

classifier
EG2

decision tree

classifier

Figure 2: sketch ICET algorithm.
GENESIS several parameters used alter performance. parameters used listed Table 4. essentially default parameter settings
(Grefenstette, 1986). used population size 50 individuals 1,000 trials,
results 20 generations. individual population consists string n + 2 numbers, n number attributes (tests) given dataset. n + 2 numbers
represented binary format, using Gray code.8 binary string used bias
EG2. first n numbers string treated n costs, C , used ICF
(equation (2)). first n numbers range 1 10,000 coded 12 binary digits each. last two numbers string used set CF. parameter
used ICF. parameter CF used C4.5 control level pruning decision
tree. last two numbers coded 8 binary digits each. ranges 0 (cost
ignored) 1 (maximum sensitivity cost) CF ranges 1 (high pruning) 100 (low
pruning). Thus individual string 12n + 16 bits.
trial individual consists running EG2 (implemented modification
C4.5) given training dataset, using numbers specified binary string set C
( = 1, , n ), , CF. training dataset randomly split two equal-sized subsets
( 1 odd-sized training sets), sub-training set sub-testing set. different random
split used trial, outcome trial stochastic. cannot assume
identical individuals yield identical outcomes, every individual must evaluated.
means duplicate individuals population, slightly different fitness
scores. measure fitness individual average cost classification
sub-testing set, using decision tree generated sub-training set. aver8. Gray code binary code designed avoid Hamming cliffs. standard binary code, 7 represented 0111 8 represented 1000. numbers adjacent, yet Hamming distance
0111 1000 large. Gray code, adjacent numbers represented binary codes small
Hamming distances. tends improve performance genetic algorithm (Grefenstette, 1986).

379

fiT URNEY

Table 4: Parameter settings GENESIS.
Parameter

Setting

Experiments

1

Total Trials

1000

Population Size

50

Structure Length

12n + 16

Crossover Rate

0.6

Mutation Rate

0.001

Generation Gap

1.0

Scaling Window

5

Report Interval

100

Structures Saved

1

Max Gens w/o Eval

2

Dump Interval

0

Dumps Saved

0

Options

acefgl

Random Seed

123456789

Rank Min

0.75

age cost measured described Section 2.2. 1,000 trials, fit (lowest cost)
individual used bias EG2 whole training set input. resulting
decision tree output ICET given training dataset.9
n costs (bias parameters), C , used ICF, directly related true costs
attributes. 50 individuals first generation generated randomly, initial
values C relation true costs. 20 generations, values C may
relation true costs, simple relationship. values
C appropriately thought biases costs. Thus GENESIS searching
bias space biases C4.5 result decision trees low average cost.
biases C range 1 10,000. bias C greater 9,000, i-th
attribute ignored. is, i-th attribute available C4.5 include decision tree, even might maximize ICF . threshold 9,000 arbitrarily chosen.
attempt optimize value experimentation.
chose use EG2 ICET, rather IDX CS-ID3, EG2 parameter , gives GENESIS greater control bias EG2. ICF partly based
data (via information gain, ) partly based bias (via pseudo9. 50/50 partition sub-training sub-testing sets could mean ICET may work well small
datasets. smallest dataset five examine Hepatitis dataset, 155 cases.
training sets 103 cases testing sets 52 cases. sub-training sub-testing sets 51 52
cases. see Figure 3 ICET performed slightly better algorithms dataset
(the difference significant).

380

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

cost, C ). exact mix data bias controlled varying . Otherwise,
reason prefer EG2 IDX CS-ID3, could easily used instead EG2.
treatment delayed tests conditional test costs hard-wired EG2.
built fitness function used GENESIS, average cost classification (measured described Section 2). makes relatively simple extend ICET handle
pragmatic constraints decision trees.
effect, GENESIS lies EG2 costs tests. lies improve
performance EG2? EG2 hill-climbing algorithm get trapped local optimum. greedy algorithm looks one test ahead builds decision tree.
looks one step ahead, EG2 suffers horizon effect. term taken
literature chess playing programs. Suppose chess playing program
fixed three-move lookahead depth finds loose queen three moves,
follows certain branch game tree. may alternate branch program first sacrifices pawn loses queen four moves. loss
queen three-move horizon, program may foolishly decide sacrifice pawn.
One move later, faced loss queen. Analogously, EG2 may try
avoid certain expensive test selecting less expensive test. One test later,
faced expensive test. exhausted cheaper tests, may
forced expensive test, spite efforts avoid test. GENESIS prevent
short-sighted behavior telling lies EG2. GENESIS exaggerate cost
cheap tests understate cost expensive test. Based past trials, GENESIS
find lies yield best performance EG2.
ICET, learning (local search EG2) evolution (in GENESIS) interact. common
form hybrid genetic algorithm uses local search improve individuals population
(Schaffer et al., 1992). improvements coded strings represent
individuals. form Lamarckian evolution. ICET, improvements due EG2
coded strings. However, improvements accelerate evolution altering fitness landscape. phenomenon (and phenomena result form
hybrid) known Baldwin effect (Baldwin, 1896; Morgan, 1896; Waddington, 1942;
Maynard Smith, 1987; Hinton & Nowlan, 1987; Ackley & Littman, 1991; Whitley & Gruau,
1993; Whitley et al., 1994; Anderson, press). Baldwin effect may explain much
success ICET.

4. Experiments
section describes experiments performed five datasets, taken Irvine collection (Murphy & Aha, 1994). five datasets described detail
Appendix A. five datasets involve medical problems. test costs based information Ontario Ministry Health (1992). main purpose experiments
gain insight behavior ICET. cost-sensitive algorithms, EG2, CS-ID3,
IDX, included mainly benchmarks evaluating ICET. C4.5 included
benchmark, illustrate behavior algorithm makes use cost information.
main conclusion experiments ICET performs significantly better
competitors, wide range conditions. access Irvine collection
information Appendix A, possible researchers duplicate
results reported here.
Medical datasets frequently missing values.10 conjecture many missing values medical datasets missing doctor involved generating dataset
381

fiT URNEY

decided particular test economically justified particular patient. Thus
may information content fact certain value missing. may
many reasons missing values cost tests. example, perhaps
doctor forgot order test perhaps patient failed show test. However,
seems likely often information content fact value missing.
experiments, information content hidden learning algorithms,
since using (at least testing sets) would form cheating. Two five
datasets selected missing data. avoid accusations cheating, decided
preprocess datasets data presented algorithms missing values.
preprocessing described Appendices A.2 A.3.
Note ICET capable handling missing values without preprocessing inherits ability C4.5 component. preprocessed data avoid accusations
cheating, ICET requires preprocessed data.
experiments, dataset randomly split 10 pairs training testing
sets. training set consisted two thirds dataset testing set consisted
remaining one third. 10 pairs used experiments, order facilitate
comparison results across experiments.
three groups experiments. first group experiments examines baseline performance algorithms. second group considers robust ICET
variety conditions. final group looks ICET searches bias space.
4.1

Baseline Performance

section examines baseline performance algorithms. Section 4.1.1, look
average cost classification five algorithms five datasets. Averaged
across five datasets, ICET lowest average cost. Section 4.1.2, study test
expenditures error rates functions penalty misclassification errors.
five algorithms studied here, ICET adjusts test expenditures error rates functions penalty misclassification errors. four algorithms ignore penalty
misclassification errors. ICET behaves one would expect, increasing test expenditures
decreasing error rates penalty misclassification errors rises. Section 4.1.3,
examine execution time algorithms. ICET requires 23 minutes average
single-processor Sparc 10. Since ICET inherently parallel, significant room
speed increase parallel machine.
4.1.1

AVERAGE COST CLASSIFICATION

experiment presented establishes baseline performance five algorithms.
hypothesis ICET will, average, perform better four algorithms. classification cost matrix set positive constant value k guess
class equal actual class j, set $0.00 equals j. experimented seven settings k, $10, $50, $100, $500, $1000, $5000, $10000.
Initially, used average cost classification performance measure,
found three problems using average cost classification compare
five algorithms. First, differences costs among algorithms become relatively

10. survey 54 datasets Irvine collection (URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/
SUMMARY-TABLE) indicates 85% medical datasets (17 20) missing values,
24% (8 34) non-medical datasets missing values.

382

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

small penalty classification errors increases. makes difficult see
algorithm best. Second, difficult combine results five datasets fair
manner.11 fair average five datasets together, since test costs different scales (see Appendix A). test costs Heart Disease dataset, example,
substantially larger test costs four datasets. Third, difficult combine average costs different values k fair manner, since weight given
situations k large situations small.
address concerns, decided normalize average cost classification.
normalized average cost dividing standard cost. Let f [0,1] frequency class given dataset. is, f fraction cases dataset
belong class i. calculate f using entire dataset, training set. Let C i, j
cost guessing case belongs class i, actually belongs class j. Let
total cost possible tests. standard cost defined follows:
+ min (1 f i) max C i, j

(6)

i, j



decompose formula (6) three components:


(7)

min (1 f i)

(8)

max C i, j

(9)



i, j

may think (7) upper bound test expenditures, (8) upper bound error
rate, (9) upper bound penalty errors. standard cost always less
maximum possible cost, given following formula:
+ max C i, j
i, j

(10)

point (8) really upper bound error rate, since possible
wrong every guess. However, experiments suggest standard cost better
normalization, since realistic (tighter) upper bound average cost.
experiments, average cost never went standard cost, although occasionally came close.
Figure 3 shows result using formula (6) normalize average cost classification. plots, x axis value k axis average cost classification
percentage standard cost classification. see that, average (the sixth plot
Figure 3), ICET lowest classification cost. one dataset ICET
perform particularly well Heart Disease dataset (we discuss later, Sections 4.3.2
4.3.3).
come single number characterizes performance algorithm,
averaged numbers sixth plot Figure 3.12 calculated 95% confidence
regions averages, using standard deviations across 10 random splits
11. want combine results order summarize performance algorithms five datasets.
analogous comparing students calculating GPA (Grade Point Average), students
courses algorithms datasets.
12. GPA, datasets (courses) weight. However, unlike GPA, algorithms (students) applied datasets (have taken courses). Thus approach perhaps fair
algorithms GPA students.

383

fiT URNEY

BUPA Liver Disease

Heart Disease
100
Average % Standard Cost

Average % Standard Cost

100
80
60
40
20
0
10

80
60
40
20
0

100

1000

10000

10

Cost Misclassification Error

Hepatitis Prognosis

Pima Indians Diabetes
Average % Standard Cost

Average % Standard Cost

60
40
20
0

80
60
40
20
0

100

1000

10000

10

Cost Misclassification Error

100

1000

10000

Cost Misclassification Error

Thyroid Disease

Average Five Datasets
100
Average % Standard Cost

100
Average % Standard Cost

10000

100

80

80
60
40
20
0
10

1000

Cost Misclassification Error

100

10

100

80
60
40
20
0

100

1000

10000

10

Cost Misclassification Error

100

1000

10000

Cost Misclassification Error

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 3: Average cost classification percentage standard cost
classification baseline experiment.
datasets. result shown Table 5.
Table 5 shows averages first three misclassification error costs alone ($10,
$50, $100), addition showing averages seven misclassification error costs
($10 $10000). two averages (the two columns Table 5), based two groups
data, address following argument: penalty misclassification errors increases,
cost tests becomes relatively insignificant. high misclassification error
cost, test cost effectively zero, task becomes simply maximize accuracy.
384

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 5: Average percentage standard cost baseline experiment.
Algorithm

Average Classification Cost Percentage Standard
95% Confidence
Misclassification Error Costs
10.00 10,000.00

Misclassification Error Costs
10.00 100.00

ICET

49 7

29 7

EG2

58 5

43 3

CS-ID3

61 6

49 4

IDX

58 5

43 3

C4.5

77 5

82 4

see Figure 3, gap C4.5 (which maximizes accuracy) algorithms becomes smaller cost misclassification error increases. Therefore benefit
sensitivity test cost decreases cost misclassification error increases. could
argued one would bother algorithm sensitive test cost tests
relatively expensive, compared cost misclassification errors. Thus realistic measure performance examine average cost classification cost
tests order magnitude cost misclassification errors ($10 $100).
Table 5 shows averages.
conclusion, based Table 5, ICET performs significantly better
four algorithms cost tests order magnitude cost
misclassification errors ($10, $50, $100). cost misclassification errors dominates test costs, ICET still performs better competition, difference less
significant. three cost-sensitive algorithms (EG2, CS-ID3, IDX) perform significantly better C4.5 (which ignores cost). performance EG2 IDX indistinguishable, CS-ID3 appears consistently costly EG2 IDX.
4.1.2

TEST EXPENDITURES ERROR RATES FUNCTIONS PENALTY ERRORS

argued Section 2 expenditures tests conditional penalty
misclassification errors. Therefore ICET designed sensitive cost tests
cost classification errors. leads us hypothesis ICET tends spend
tests penalty misclassification errors increases. expect
error rate ICET decrease test expenditures increase. two hypotheses
confirmed Figure 4. plots, x axis value k axis (1) average expenditure tests, expressed percentage maximum possible expenditure
tests, , (2) average percent error rate. average (the sixth plot Figure 4), test
expenditures rise error rate falls penalty classification errors increases.
minor deviations trend, since ICET guess value test
(in terms reduced error rate), based sees training dataset. testing
dataset may always support guess. Note plots four algorithms, corresponding plots ICET Figure 4, would straight horizontal lines, since four
algorithms ignore cost misclassification error. generate decision trees
every possible misclassification error cost.

385

fiT URNEY

80

60

60
40
40
20

20
0

0
1000

10000

30
40
20
20

10

Average % Maximum Test Expenditures

30
20
20
10
0

Average % Error Rate

Average % Maximum Test Expenditures

40

0
1000

10000

80

60

40
40
20
20

0
10

0
100

1000

10000

Cost Misclassification Error

Average Five Datasets

80

Average % Maximum Test Expenditures

Thyroid Disease
10
8

60

6
40
4
20

2

0

Average % Error Rate

Average % Maximum Test Expenditures

10000

60

Cost Misclassification Error

0
100

1000

Pima Indians Diabetes
40

10

0
100

Cost Misclassification Error

Hepatitis Prognosis
50

100

10

0

Cost Misclassification Error

10

40

60

Average % Error Rate

100

50

1000

10000

Cost Misclassification Error

80

50
40

60

30
40
20
20

10

0
10

Average % Error Rate

10

80

Average % Error Rate

Average % Maximum Test Expenditures

Heart Disease
80

Average % Error Rate

Average % Maximum Test Expenditures

BUPA Liver Disease
100

0
100

1000

10000

Cost Misclassification Error

% Test Expenditures:
% Error Rate:

Figure 4: Average test expenditures average error rate
function misclassification error cost.
4.1.3

EXECUTION TIME

essence, ICET works invoking C4.5 1000 times (Section 3.5). Fortunately, Quinlans
(1992) implementation C4.5 quite fast. Table 6 shows run-times algorithms,
using single-processor Sun Sparc 10. One full experiment takes one week (roughly
23 minutes average run, multiplied 5 datasets, multiplied 10 random splits, multiplied 7 misclassification error costs equals one week). Since genetic algorithms
easily executed parallel, substantial room speed increase parallel
machine. generation consists 50 individuals, could evaluated parallel,
reducing average run-time half minute.
4.2

Robustness ICET

group experiments considers robust ICET variety conditions.
section considers different variation operating environment ICET. ICET
386

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 6: Elapsed run-time five algorithms.
Algorithm

Average Elapsed Run-Time Dataset Minutes:Seconds
BUPA

Heart

Hepatitis

Pima

Thyroid

Average

ICET

15:43

13:14

10:29

28:19

45:25

22:38

EG2

0:1

0:1

0:1

0:3

0:3

0:2

CS-ID3

0:1

0:1

0:1

0:3

0:3

0:2

IDX

0:1

0:1

0:1

0:3

0:3

0:2

C4.5

0:2

0:1

0:1

0:4

0:3

0:2

algorithm modified. Section 4.2.1, alter environment labelling
tests immediate. Section 4.2.2, recognize shared costs, discount
group tests common cost. Section 4.2.3, experiment complex classification cost matrices, different types errors different costs. Section 4.2.4,
examine happens ICET trained certain penalty misclassification
errors, tested different penalty. four experiments, find ICET continues perform well.
4.2.1

TESTS IMMEDIATE

critic might object previous experiments show ICET superior
algorithms due sensitivity test costs classification error costs. Perhaps
ICET superior simply handle delayed tests, algorithms treat
tests immediate.13 is, method estimating average classification cost
(Section 2.2) biased favor ICET (since ICET uses method fitness function)
algorithms. experiment, labelled tests immediate. Otherwise, nothing changed baseline experiments. Table 7 summarizes results
experiment. ICET still performs well, although advantage algorithms
decreased slightly. Sensitivity delayed tests part explanation ICETs performance, whole story.
4.2.2

GROUP DISCOUNTS

Another hypothesis ICET superior simply handle groups tests
share common cost. experiment, eliminated group discounts tests share
common cost. is, test costs conditional prior tests. Otherwise, nothing
changed baseline experiments. Table 8 summarizes results experiment.
ICET maintains advantage algorithms.
4.2.3

COMPLEX CLASSIFICATION COST MATRICES

far, used simple classification cost matrices, penalty classification error types error. assumption inherent ICET.
13. algorithms cannot currently handle delayed tests, possible alter
way, handle delayed tests. comment extends groups tests share common
cost. ICET might viewed alteration EG2 enables EG2 handle delayed tests common
costs.

387

fiT URNEY

Table 7: Average percentage standard cost no-delay experiment.
Algorithm

Average Classification Cost Percentage Standard
95% Confidence
Misclassification Error Costs
10.00 10,000.00

Misclassification Error Costs
10.00 100.00

ICET

47 6

28 4

EG2

54 4

36 2

CS-ID3

54 5

39 3

IDX

54 4

36 2

C4.5

64 6

59 4

Table 8: Average percentage standard cost no-discount experiment.
Algorithm

Average Classification Cost Percentage Standard
95% Confidence
Misclassification Error Costs
10.00 10,000.00

Misclassification Error Costs
10.00 100.00

ICET

46 6

25 5

EG2

56 5

42 3

CS-ID3

59 5

48 4

IDX

56 5

42 3

C4.5

75 5

80 4

element classification cost matrix different value. experiment,
explore ICETs behavior classification cost matrix complex.
use term positive error refer false positive diagnosis, occurs
patient diagnosed sick, patient actually healthy. Conversely, term
negative error refers false negative diagnosis, occurs patient diagnosed healthy, actually sick. term positive error cost cost
assigned positive errors, negative error cost cost assigned negative
errors. See Appendix examples. interested ICETs behavior ratio
negative positive error cost varied. Table 9 shows ratios examined.
Figure 5 shows performance five algorithms ratio.
hypothesis difference performance ICET algorithms would increase move away middles plots, ratio 1.0,
since algorithms mechanism deal complex classification cost;
designed implicit assumption simple classification cost matrices. fact,
Figure 5 shows difference tends decrease move away middles.
pronounced right-hand sides plots. ratio 8.0 (the
extreme right-hand sides plots), advantage using ICET. ratio
0.125 (the extreme left-hand sides plots), still advantage using ICET.
388

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

BUPA Liver Disease

Heart Disease
100
Average % Standard Cost

Average % Standard Cost

100
80
60
40
20
0
0.1

80
60
40
20
0

1.0

10.0

0.1

Ratio Negative Positive Error Cost

Hepatitis Prognosis
Average % Standard Cost

Average % Standard Cost

100

80
60
40
20
0

80
60
40
20
0

1.0

10.0

0.1

Ratio Negative Positive Error Cost

Thyroid Disease

10.0

Average Five Datasets
100
Average % Standard Cost

Average % Standard Cost

1.0
Ratio Negative Positive Error Cost

100
80
60
40
20
0
0.1

10.0

Pima Indians Diabetes

100

0.1

1.0
Ratio Negative Positive Error Cost

80
60
40
20
0

1.0

10.0

0.1

Ratio Negative Positive Error Cost

1.0

10.0

Ratio Negative Positive Error Cost

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 5: Average cost classification percentage standard cost
classification, complex classification cost matrices.
interpretation plots complicated fact gap algorithms tends decrease penalty classification errors increases (as see
Figure 3 retrospect, held sum negative error cost positive error cost constant value, varied ratio). However, clearly
asymmetry plots, expected symmetrical vertical line centered
1.0 x axis. plots close symmetrical algorithms,
asymmetrical ICET. apparent Table 10, focuses comparison
performance ICET EG2, averaged across five datasets (see sixth plot
Figure 5). suggests difficult reduce negative errors (on right-hand
sides plots, negative errors weight) reduce positive errors (on
389

fiT URNEY

Table 9: Actual error costs ratio negative positive error cost.
Ratio Negative
Positive Error Cost

Negative
Error Cost

Positive
Error Cost

0.125

50

400

0.25

50

200

0.5

50

100

1.0

50

50

2.0

100

50

4.0

200

50

8.0

400

50

Table 10: Comparison ICET EG2
various ratios negative positive error cost.

Algorithm

Average Classification Cost Percentage Standard
95% Confidence, Ratio
Negative Positive Error Cost Varied
0.125

0.25

0.5

1.0

2.0

4.0

8.0

ICET

25 10

25 8

29 6

29 4

34 6

39 6

39 6

EG2

39 5

40 4

41 4

44 3

42 3

41 4

40 5

ICET/EG2 (as %)

64

63

71

66

81

95

98

left-hand sides, positive errors weight). is, easier avoid false positive diagnoses (a patient diagnosed sick, patient actually healthy)
avoid false negative diagnoses (a patient diagnosed healthy, actually
sick). unfortunate, since false negative diagnoses usually carry heavier penalty,
real-life. Preliminary investigation suggests false negative diagnoses harder avoid
sick class usually less frequent healthy class, makes
sick class harder learn.
4.2.4

POORLY ESTIMATED CLASSIFICATION COST

believe advantage ICET sensitive test costs classification error costs. However, might argued difficult calculate cost classification errors many real-world applications. Thus possible algorithm
ignores cost classification errors (e.g., EG2, CS-ID3, IDX) may robust
useful algorithm sensitive classification errors (e.g., ICET). address
possibility, examine happens ICET trained certain penalty classification errors, tested different penalty.
hypothesis ICET would robust reasonable differences
penalty training penalty testing. Table 11 shows happens
ICET trained penalty $100 classification errors, tested penalties
390

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 11: Performance training set classification error cost $100.

Algorithm

Average Classification Cost Percentage
Standard 95% Confidence, Testing Set
Classification Error Cost of:
$50

$100

$500

ICET

33 10

41 10

62 9

EG2

44 3

49 4

63 6

CS-ID3

49 5

54 6

65 7

IDX

43 3

49 4

63 6

C4.5

82 5

82 5

78 7

$50, $100, $500. see ICET best performance five algorithms,
although edge quite slight case penalty $500 testing.
examined happens (1) ICET trained penalty $500
tested penalties $100, $500, $1,000 (2) ICET trained penalty
$1,000 tested penalties $500, $1,000, $5,000. results show essentially
pattern Table 11: ICET relatively robust differences training
testing penalties, least penalties order magnitude. suggests ICET applicable even situations reliability estimate
cost classification errors dubious.
penalty errors testing set $100, ICET works best penalty
errors training set $100. penalty errors testing set
$500, ICET works best penalty errors training set $500.
penalty errors testing set $1,000, ICET works best penalty errors
training set $500. suggests might advantage situations
underestimating penalty errors training. other, words ICET may
tendency overestimate benefits tests (this likely due overfitting training
data).
4.3

Searching Bias Space

final group experiments analyzes ICETs method searching bias space. Section
4.3.1 studies roles mutation crossover operators. appears crossover
mildly beneficial, compared pure mutation. Section 4.3.2 considers happens
ICET constrained search binary bias space, instead real bias space. constraint actually improves performance ICET. hypothesized improvement
due hidden advantage searching binary bias space: searching binary
bias space, ICET direct access true costs tests. However, advantage
available searching real bias space, initial population biases seeded
true costs tests. Section 4.3.3 shows seeding improves performance ICET.
4.3.1

CROSSOVER VERSUS MUTATION

Past work shown genetic algorithm crossover performs better genetic
algorithm mutation alone (Grefenstette et al., 1990; Wilson, 1987). section
391

fiT URNEY

attempts test hypothesis crossover improves performance ICET. test
hypothesis, sufficient merely set crossover rate zero. Since crossover
randomizing effect, similar mutation, must increase mutation rate, compensate loss crossover (Wilson, 1987; Spears, 1992).
difficult analytically calculate increase mutation rate required
compensate loss crossover (Spears, 1992). Therefore experimentally tested
three different mutation settings.14 results summarized Table 12. crossover rate set zero, best mutation rate 0.10. misclassification error costs
$10 $10,000, performance ICET without crossover good performance ICET crossover, difference statistically significant. However,
comparison entirely fair crossover, since made attempt optimize
crossover rate (we simply used default value). results suggest crossover
mildly beneficial, prove pure mutation inferior.
Table 12: Average percentage standard cost mutation experiment.
Average Classification Cost Percentage
Standard 95% Confidence

ICET
Crossover
Rate

Mutation
Rate

Misclassification
Error Costs
10.00 10,000.00

Misclassification
Error Costs
10.00 100.00

0.6

0.001

49 7

29 7

0.0

0.05

51 8

32 9

0.0

0.10

50 8

29 8

0.0

0.15

51 8

30 9

4.3.2

SEARCH BINARY SPACE

ICET searches biases space n + 2 real numbers. Inspired Aha Bankert
(1994), decided see would happen ICET restricted space n
binary numbers 2 real numbers. modified ICET EG2 given true cost
test, instead pseudo-cost bias. conditional test costs, used nodiscount cost (see Section 4.2.2). n binary digits used exclude include test.
EG2 allowed use excluded tests decision trees generated.
precise, let B 1, , B n n binary numbers let C 1, , C n n real numbers. experiment, set C true cost i-th test. experiment, GENESIS change C . is, C constant given test given dataset. Instead,
GENESIS manipulates value B i. binary number B used determine
whether EG2 allowed use test decision tree. B = 0 , EG2 allowed
use i-th test (the i-th attribute). Otherwise, B = 1 , EG2 allowed use i-th
test. EG2 uses ICF equation usual, true costs C . Thus modified version
ICET searching binary bias space instead real bias space.
hypothesis ICET would perform better searching real bias space
14. three experiments took one week Sparc 10, tried three settings
mutation rate.

392

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

searching binary bias space. Table 13 shows hypothesis confirmed. appears better search binary bias space, rather real bias space.
However, differences statistically significant.
Table 13: Average percentage standard cost binary search experiment.
Algorithm

Average Classification Cost Percentage
Standard 95% Confidence
Misclassification
Error Costs
10.00 10,000.00

Misclassification
Error Costs
10.00 100.00

ICET Binary Space

48 6

26 5

ICET Real Space

49 7

29 7

EG2

58 5

43 3

CS-ID3

61 6

49 4

IDX

58 5

43 3

C4.5

77 5

82 4

searched binary space, set C true cost i-th test. GENESIS
manipulated B instead C . searched real space, GENESIS set C whatever
value found useful attempt optimize fitness. hypothesized gives
advantage binary space search real space search. Binary space search direct
access true costs tests, real space search learns true costs
tests indirectly, feedback gets fitness function.
examined experiment detail, found ICET well Heart
Disease dataset searching binary bias space, although poorly
searching real bias space (see Section 4.1.1). hypothesized ICET,
searching real space, suffered lack direct access true costs
applied Heart Disease dataset. hypotheses tested next experiment.
4.3.3

SEEDED POPULATION

experiment, returned searching real bias space, seeded initial population biases true test costs. gave ICET direct access true test costs.
conditional test costs, used no-discount cost (see Section 4.2.2). baseline
experiment (Section 4.1), initial population consists 50 randomly generated strings,
representing n + 2 real numbers. experiment, initial population consists 49 randomly generated strings one manually generated string. manually generated
string, first n numbers true test costs. last two numbers set 1.0 (for
) 25 (for CF). string exactly bias EG2, implemented (Section
3.2).
hypotheses (1) ICET would perform better (on average) initial
population seeded purely random, (2) ICET would perform better (on
average) searching real space seeded population searching binary
space,15 (3) ICET would perform better Heart Disease dataset ini393

fiT URNEY

tial population seeded purely random. Table 14 appears support first
two hypotheses. Figure 6 appears support third hypothesis. However, results
statistically significant.16
Table 14: Average percentage standard cost seeded population
experiment.
Algorithm

Average Classification Cost Percentage
Standard 95% Confidence
Misclassification
Error Costs
10.00 10,000.00

Misclassification
Error Costs
10.00 100.00

ICET Seeded
Search Real Space

46 6

25 5

ICET Unseeded
Search Real Space

49 7

29 7

ICET Unseeded
Search Binary Space

48 6

26 5

EG2

58 5

43 3

CS-ID3

61 6

49 4

IDX

58 5

43 3

C4.5

77 5

82 4

experiment raises interesting questions: seeding population
built ICET algorithm? seed whole population true costs, perturbed random noise? Perhaps right approach, prefer modify
ICF (equation (2)), device GENESIS controls decision tree induction.
could alter equation contains true costs bias parameters.17
seems make sense current approach, deprives EG2 direct
access true costs. discuss ideas modifying equation
Section 5.2.
Incidentally, experiment lets us answer following question: genetic
search bias space anything useful? start true costs tests reasonable values parameters CF, much improvement get
genetic search? experiment, seeded population individual represents exactly bias EG2 (the first n numbers true test costs last two numbers 1.0 25 CF). Therefore determine value genetic search
comparing EG2 ICET. ICET starts bias EG2 (as seed first genera15. Note make sense seed binary space search, since already direct access true
costs.
16. would need go current 10 trials (10 random splits data) 40 trials make
results significant. experiments reported took total 63 days continuous computation Sun
Sparc 10, 40 trials would require six months.
17. idea suggested conversation K. De Jong.

394

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Heart Disease
100

80

80

Average % Standard Cost

Average % Standard Cost

BUPA Liver Disease
100

60

40

20
0
10

60

40

20
0

100

1000

10000

10

Cost Misclassification Error

80

80

60

40

20
0

40

20
0

100

1000

10000

10

100

1000

10000

Cost Misclassification Error

Thyroid Disease

Average Five Datasets

100

100

80

80

Average % Standard Cost

Average % Standard Cost

10000

60

Cost Misclassification Error

60

40

20
0
10

1000

Pima Indians Diabetes
100
Average % Standard Cost

Average % Standard Cost

Hepatitis Prognosis
100

10

100

Cost Misclassification Error

60

40

20
0

100

1000

10000

10

Cost Misclassification Error

100

1000

10000

Cost Misclassification Error

ICET:
EG2:
CS-ID3:
IDX:
C4.5:

Figure 6: Average cost classification percentage standard cost
classification seeded population experiment.
tion) attempts improve bias. score EG2 Table 14 shows value
bias built EG2. score ICET Table 14 shows genetic search bias space
improve built-in bias EG2. cost misclassification errors
order magnitude test costs ($10 $100), EG2 averages 43% standard cost,
ICET averages 25% standard cost. cost misclassification errors
ranges $10 $10,000, EG2 averages 58% standard cost, ICET averages
46% standard cost. differences significant 95% confidence. makes clear genetic search adding value.
395

fiT URNEY

5. Discussion
section compares ICET related work outlines possibilities future work.
5.1

Related Work

several algorithms sensitive test costs (Nez, 1988, 1991; Tan &
Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989). discussed, main limitation algorithms consider cost classification errors. cannot rationally determine whether test performed know cost
test cost classification errors.
several algorithms sensitive classification error costs (Breiman
et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al., 1994). None
algorithms consider cost tests. Therefore focus complex classification
cost matrices, since, tests cost classification error matrix simple,
problem reduces maximizing accuracy.
FIS system (Pipitone et al., 1991) attempts find decision tree minimizes
average total cost tests required achieve certain level accuracy. approach
could implemented ICET altering fitness function. main distinction
FIS (Pipitone et al., 1991) ICET FIS learn data. information
gain test estimated using qualitative causal model, instead training cases. Qualitative causal models elicited domain experts, using special knowledge acquisition
tool. training data available, ICET used avoid need knowledge
acquisition. Otherwise, ICET applicable FIS approach suitable.
Another feature ICET perform purely greedy search. Several
authors proposed non-greedy classification algorithms (Tcheng et al., 1989; Ragavan &
Rendell, 1993; Norton, 1989; Schaffer, 1993; Rymon, 1993; Seshu, 1989). general,
results show advantage sophisticated search procedures. ICET
different algorithms uses genetic algorithm applied minimizing test costs classification error costs.
ICET uses two-tiered search strategy. bottom tier, EG2 performs greedy search
space classifiers. second tier, GENESIS performs non-greedy search
space biases. idea two-tiered search strategy (where first tier
search classifier space second tier search bias space) appears (Provost,
1994; Provost & Buchanan, press; Aha & Bankert, 1994; Schaffer, 1993). work goes
beyond Aha Bankert (1994) considering search real bias space, rather search
binary space. work fits general framework Provost Buchanan (in
press), differs many details. example, method calculating cost special
case (Section 2.3).
researchers applied genetic algorithms classification problems. example, Frey Slate (1991) applied genetic algorithm (in particular, learning classifier system (LCS)) letter recognition. However, Fogarty (1992) obtained higher accuracy using
simple nearest neighbor algorithm. recent applications genetic algorithms classification successful (De Jong et al., 1993). However, work described
first application genetic algorithms problem cost-sensitive classification.
mentioned Section 2.1 decision theory may used define optimal solution problem cost-sensitive classification. However, searching optimal solution computationally infeasible (Pearl, 1988). attempted take decision theoretic
396

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

approach problem implementing AO* algorithm (Pearl, 1984) designing
heuristic evaluation function speed AO* search (Lirov & Yue, 1991).
unable make approach execute fast enough practical.
attempted apply genetic programming (Koza, 1993) problem costsensitive classification. Again, unable make approach execute fast enough
practical, although faster AO* approach.
cost-sensitive classification problem, treated here, essentially
problem reinforcement learning (Sutton, 1992; Karakoulas, preparation). average
cost classification, measured described Section 2.2, reward/punishment signal
could optimized using reinforcement learning techniques. something
might explored alternative approach.
5.2

Future Work

paper discusses two types costs, cost tests cost misclassification
errors. two costs treated together decision theory, ICET first
machine learning system handles costs together. experiments paper
compared ICET machine learning systems handle test costs (Nez, 1988,
1991; Tan & Schlimmer, 1989, 1990; Tan, 1993; Norton, 1989), compared
ICET machine learning systems handle classification error costs (Breiman
et al., 1984; Friedman & Stuetzle, 1981; Hermans et al., 1974; Gordon & Perlis, 1989; Pazzani et al., 1994; Provost, 1994; Provost & Buchanan, press; Knoll et al., 1994). future
work, plan address omission. proper treatment issue would make
paper long.
absence comparison machine learning systems handle classification
error costs impact experiments reported here. experiments
paper focussed simple classification cost matrices (except Section 4.2.3).
classification cost matrix simple cost tests ignored, minimizing cost exactly
equivalent maximizing accuracy (see Section 2.3). Therefore, C4.5 (which designed
maximize accuracy) suitable surrogate systems handle classification error costs.
experiment setting test costs zero. However, behavior
ICET penalty misclassification errors high (the extreme right-hand sides
plots Figure 3) necessarily behavior cost tests
low, since ICET sensitive relative differences test costs error costs,
absolute costs. Therefore (given behavior observe extreme right-hand
sides plots Figure 3) expect performance ICET tend converge performance algorithms cost tests approaches zero.
One natural addition ICET would ability output dont know class.
easily handled GENESIS component, extending classification cost matrix
cost assigned classifying case unknown. need make small
modification EG2 component, generate decision trees leaves
labelled unknown. One way would introduce parameter defines
confidence threshold. Whenever confidence certain leaf drops confidence
threshold, leaf would labelled unknown. confidence parameter would made
accessible GENESIS component, could tuned minimize average classification cost.
mechanism ICET handling conditional test costs limitations.
397

fiT URNEY

currently implemented, handle cost attributes calculated
attributes. example, Thyroid dataset (Appendix A.5), FTI test calculated
based results TT4 T4U tests. FTI test selected, must pay
TT4 T4U tests. TT4 T4U tests already selected, FTI test free
(since calculation trivial). ability deal calculated test results could
added ICET relatively little effort.
ICET, currently implemented, handles two classes test results: tests
immediate results tests delayed results. Clearly continuous range
delays, seconds years. chosen treat delays distinct test costs,
could argued delay simply another type test cost. example, could
say group blood tests shares common cost one-day wait results. cost
one blood tests conditional whether prepared commit
one tests group, see results first test.
One difficultly approach handling delays problem assigning cost
delay. much cost bring patient two blood samples, instead one?
include disruption patients life estimate cost? avoid
questions, treated delays another type test cost, approach
readily handle continuous range delays.
cost test function several things: (1) function prior
tests selected. (2) function actual class case. (3)
function aspects case, information aspects may
available tests. (4) function test result. list seems
comprehensive, may possibilities overlooked. Let us consider
four possibilities.
First, cost test function prior tests selected. ICET
handles special case this, group tests shares common cost. currently
implemented, ICET handle general case. However, could easily add
capability ICET modifying fitness function.
Second, cost test function actual class case. example,
test heart disease might involve heavy exercise (Appendix A.2). patient actually
heart disease, exercise might trigger heart attack. risk included
cost particular test. Thus cost test vary, depending whether
patient actually heart disease. implemented this, although could easily
added ICET modifying fitness function.
Third, cost test function results tests. example, drawing blood newborn costly drawing blood adult. assign cost
blood test, need know age patient. age patient represented result another test patient-age test. slightly complex
preceding cases, must insure blood test always accompanied patient-age test. implemented this, although could added
ICET.
Fourth, cost test function test result. example, injecting
radio-opaque die X-ray might cause allergic reaction patient. risk
added cost test. makes cost test function one possible outcomes test. situation this, may wise precede injection
die screening test allergies. could simple asking question
patient. question may relevance determining correct diagnosis
398

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

patient, may serve reduce average cost classification. case similar
third case, above. Again, implemented this, although could added
ICET.
Attribute selection EG2, CS-ID3, IDX shares common form. may view
n
attribute selection function { 1, , n } , takes input n information
gain values 1, , n (one attribute) generates output index one
attributes. may view C 1, , C n parameters attribute selection function. parameters may used control bias attribute selection procedure.
view, ICET uses GENESIS tune parameters EG2s attribute selection function.
future, would investigate general attribute selection functions.
n
example, might use neural network implement function { 1, , n } .
GENESIS would used tune weights neural network.18 attribute
selection function might benefit addition input specifies depth
decision tree current node, information gain values measured.
would enable bias test vary, depending many tests already
selected.
Another area future work explore parameter settings control GENESIS
(Table 4). many parameters could adjusted GENESIS. think significant ICET works well default parameter settings GENESIS, since
shows ICET robust respect parameters. However, might possible
substantially improve performance ICET tuning parameters. recent
trend genetic algorithm research let genetic algorithm adjust
parameters, mutation rate crossover rate (Whitley et al., 1993). Another possibility stop breeding fitness levels stop improving, instead stopping
fixed number generations. Provost Buchanan (in press) use goodness measure
stopping condition bias space search.

6. Conclusions
central problem investigated problem minimizing cost classification
tests expensive. argued requires assigning cost classification
errors. argued decision tree natural form knowledge representation
type problem. presented general method calculating average cost
classification decision tree, given decision tree, information calculation
test costs, classification cost matrix, set testing data. method applicable
standard classification decision trees, without regard decision tree generated.
method sensitive test costs, sensitive classification error costs, capable handling conditional test costs, capable handling delayed tests.
introduced ICET, hybrid genetic decision tree induction algorithm. ICET uses
genetic algorithm evolve population biases decision tree induction algorithm.
individual population represents one set biases. fitness individual
determined using generate decision tree training dataset, calculating
average cost classification decision tree testing dataset.
analyzed behavior ICET series experiments, using five real-world medical datasets. Three groups experiments performed. first group looked
baseline performance five algorithms five datasets. ICET found sig18. idea suggested conversation M. Brooks.

399

fiT URNEY

nificantly lower costs algorithms. Although executes slowly, average time 23 minutes (for typical dataset) acceptable many applications,
possibility much greater speed parallel machine. second group experiments studied robustness ICET variety modifications input.
results show ICET robust. third group experiments examined ICETs search
bias space. discovered search could improved seeding initial population biases.
general, research concerned pragmatic constraints classification problems (Provost & Buchanan, press). believe many real-world classification problems involve merely maximizing accuracy (Turney, press). results
presented indicate that, certain applications, decision tree merely maximizes
accuracy (e.g., trees generated C4.5) may far performance possible
algorithm considers realistic constraints test costs, classification error
costs, conditional test costs, delayed test results. pragmatic
constraints faced real-world classification problems.

Appendix A. Five Medical Datasets
appendix presents test costs five medical datasets, taken Irvine collection (Murphy & Aha, 1994). costs based information Ontario Ministry
Health (1992). Although none medical data gathered Ontario, reasonable
assume areas similar relative test costs. purposes, relative costs
important, absolute costs.
A.1

BUPA Liver Disorders

BUPA Liver Disorders dataset created BUPA Medical Research Ltd.
donated Irvine collection Richard Forsyth.19 Table 15 shows test costs
BUPA Liver Disorders dataset. tests group blood tests thought
sensitive liver disorders might arise excessive alcohol consumption. tests
share common cost $2.10 collecting blood. target concept defined using
sixth column: Class 0 defined drinks < 3 class 1 defined drinks
3. Table 16 shows general form classification cost matrix used
experiments Section 4. experiments, classification error cost equals
positive error cost equals negative error cost. exception Section 4.2.3,
experiments complex classification cost matrices. terms positive error cost
negative error cost explained Section 4.2.3. 345 cases dataset,
missing values. Column seven originally used split data training testing sets. use column, since required ten different random splits
data. ten random splits, ten training sets 230 cases ten testing sets
115 cases.
A.2

Heart Disease

Heart Disease dataset donated Irvine collection David Aha.20 princi19. BUPA Liver Disorders dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/liverdisorders/bupa.data.
20. Heart Disease dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/heart-disease/
cleve.mod.

400

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 15: Test costs BUPA Liver Disorders dataset.
Test

Description

Group

Cost

Delayed

1

mcv

mean corpuscular volume



$7.27 first test group A,
$5.17 otherwise

yes

2

alkphos

alkaline phosphotase



$7.27 first test group A,
$5.17 otherwise

yes

3

sgpt

alamine aminotransferase



$7.27 first test group A,
$5.17 otherwise

yes

4

sgot

aspartate aminotransferase



$7.27 first test group A,
$5.17 otherwise

yes

5

gammagt

gamma-glutamyl transpeptidase



$9.86 first test group A,
$7.76 otherwise

yes

6

drinks

number half-pint equivalents
alcoholic beverages drunk per day

diagnostic class: drinks < 3
drinks 3

-

7

selector

field used split data two sets

used

-

Table 16: Classification costs BUPA Liver Disorders dataset.
Actual Class

Guess Class

Cost

0 (drinks < 3)

0 (drinks < 3)

$0.00

0 (drinks < 3)

1 (drinks 3)

Positive Error Cost

1 (drinks 3)

0 (drinks < 3)

Negative Error Cost

1 (drinks 3)

1 (drinks 3)

$0.00

pal medical investigator Robert Detrano, Cleveland Clinic Foundation. Table 17
shows test costs Heart Disease dataset. nominal cost $1.00 assigned
first four tests. tests group blood tests thought relevant
heart disease. tests share common cost $2.10 collecting blood. tests
groups B C involve measurements heart exercise. nominal cost $1.00
assigned tests first test groups. class variable
values buff (healthy) sick. fifteenth column, specified class
variable H (healthy), S1, S2, S3, S4 (four different types sick),
deleted column. Table 18 shows classification cost matrix. 303 cases
dataset. deleted cases missing values. reduced
dataset 296 cases. ten random splits, training sets 197 cases testing
sets 99 cases.
A.3

Hepatitis Prognosis

Hepatitis Prognosis dataset donated Gail Gong.21 Table 19 shows test costs
Hepatitis dataset. Unlike four datasets, dataset deals prognosis,
21. Hepatitis Prognosis dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/hepatitis/
hepatitis.data.

401

fiT URNEY

Table 17: Test costs Heart Disease dataset.
Test

Description

1

age

2

Group

Cost

Delayed

age years

$1.00



sex

patients gender

$1.00



3

cp

chest pain type

$1.00



4

trestbps

resting blood pressure

$1.00



5

chol

serum cholesterol



$7.27 first test group A,
$5.17 otherwise

yes

6

fbs

fasting blood sugar



$5.20 first test group A,
$3.10 otherwise

yes

7

restecg

resting electrocardiograph

$15.50

yes

8

thalach

maximum heart rate
achieved

B

$102.90 first test group B,
$1.00 otherwise

yes

9

exang

exercise induced angina

C

$87.30 first test group C,
$1.00 otherwise

yes

10

oldpeak

ST depression induced
exercise relative rest

C

$87.30 first test group C,
$1.00 otherwise

yes

11

slope

slope peak exercise ST
segment

C

$87.30 first test group C,
$1.00 otherwise

yes

12

ca

number major vessels
coloured fluoroscopy

$100.90

yes

13

thal

3 = normal; 6 = fixed defect;
7 = reversible defect

$102.90 first test group B,
$1.00 otherwise

yes

14

num

diagnosis heart disease

diagnostic class

-

B

Table 18: Classification costs Heart Disease dataset.
Actual Class

Guess Class

Cost

buff

buff

$0.00

buff

sick

Positive Error Cost

sick

buff

Negative Error Cost

sick

sick

$0.00

diagnosis. prognosis, diagnosis known, problem determine likely
outcome disease. tests assigned nominal cost $1.00 either involve
asking question patient performing basic physical examination patient.
tests group share cost $2.10 collecting blood. Note that, although performing histological examination liver costs $81.64, asking patient whether
histology performed costs $1.00. Thus prognosis exploit information
conveyed decision (to perform histological examination) made
diagnosis. class variable values 1 (die) 2 (live). Table 20 shows classification costs. dataset contains 155 cases, many missing values. ten random
402

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Table 19: Test costs Hepatitis Prognosis dataset.
Test

Description

1

class

2

Group

Cost

Delayed

prognosis hepatitis

prognostic class: live die

-

age

age years

$1.00



3

sex

gender

$1.00



4

steroid

patient steroids

$1.00



5

antiviral

patient antiviral

$1.00



6

fatigue

patient reports fatigue

$1.00



7

malaise

patient reports malaise

$1.00



8

anorexia

patient anorexic

$1.00



9

liver big

liver big physical exam

$1.00



10

liver firm

liver firm physical exam

$1.00



11

spleen palpable

spleen palpable physical

$1.00



12

spiders

spider veins visible

$1.00



13

ascites

ascites visible

$1.00



14

varices

varices visible

$1.00



15

bilirubin

bilirubin blood test



$7.27 first test group A,
$5.17 otherwise

yes

16

alk phosphate

alkaline phosphotase



$7.27 first test group A,
$5.17 otherwise

yes

17

sgot

aspartate aminotransferase



$7.27 first test group A,
$5.17 otherwise

yes

18

albumin

albumin blood test



$7.27 first test group A,
$5.17 otherwise

yes

19

protime

protime blood test



$8.30 first test group A,
$6.20 otherwise

yes

20

histology

histology performed?

$1.00



Table 20: Classification costs Hepatitis Prognosis dataset.
Actual Class

Guess Class

Cost

1 (die)

1 (die)

$0.00

1 (die)

2 (live)

Negative Error Cost

2 (live)

1 (die)

Positive Error Cost

2 (live)

2 (live)

$0.00

splits, training sets 103 cases testing sets 52 cases. filled missing values, using simple single nearest neighbor algorithm (Aha et al., 1991). missing
values filled using whole dataset, dataset split training
testing sets. nearest neighbor algorithm, data normalized mini403

fiT URNEY

mum value feature 0 maximum value 1. distance measure used
sum absolute values differences. difference two values
defined 1 one two values missing.
A.4

Pima Indians Diabetes

Pima Indians Diabetes dataset donated Vincent Sigillito. 22 data collected National Institute Diabetes Digestive Kidney Diseases. Table 21
shows test costs Pima Indians Diabetes dataset. tests group share
cost $2.10 collecting blood. remaining tests assigned nominal cost
$1.00. patients females least 21 years old Pima Indian heritage.
class variable values 0 (healthy) 1 (diabetes). Table 22 shows classification costs.
dataset includes 768 cases, missing values. ten random splits, training
sets 512 cases testing sets 256 cases.
Table 21: Test costs Pima Indians Diabetes dataset.
Test

Description

Group

Cost

Delayed

1

times pregnant

number times pregnant

$1.00



2

glucose tol

glucose tolerance test

$17.61 first test group A,
$15.51 otherwise

yes

3

diastolic bp

diastolic blood pressure

$1.00



4

triceps

triceps skin fold thickness

$1.00



5

insulin

serum insulin test

$22.78 first test group A,
$20.68 otherwise

yes

6

mass index

body mass index

$1.00



7

pedigree

diabetes pedigree function

$1.00



8

age

age years

$1.00



9

class

diagnostic class

diagnostic class

-





Table 22: Classification costs Pima Indians Diabetes dataset.

A.5

Actual Class

Guess Class

Cost

0 (healthy)

0 (healthy)

$0.00

0 (healthy)

1 (diabetes)

Positive Error Cost

1 (diabetes)

0 (healthy)

Negative Error Cost

1 (diabetes)

1 (diabetes)

$0.00

Thyroid Disease

Thyroid Disease dataset created Garavan Institute, Sydney, Australia.
file donated Randolf Werner, obtained Daimler-Benz. Daimler-Benz obtained
22. Pima Indians Diabetes dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/pimaindians-diabetes/pima-indians-diabetes.data.

404

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

data J.R. Quinlan.23 Table 23 shows test costs Thyroid Disease dataset.
nominal cost $1.00 assigned first 16 tests. tests group share cost
$2.10 collecting blood. FTI test involves calculation based results
TT4 T4U tests. complicates calculation costs three tests,
chose use FTI test experiments. class variable values 1
(hypothyroid), 2 (hyperthyroid), 3 (normal). Table 24 shows classification costs.
3772 cases dataset, missing values. ten random splits,
training sets 2515 cases testing sets 1257 cases.
Table 23: Test costs Thyroid Disease dataset.
Test

Description

1

age

2

Group

Cost

Delayed

age years

$1.00



sex

gender

$1.00



3

thyroxine

patient thyroxine

$1.00



4

query thyroxine

maybe thyroxine

$1.00



5

antithyroid

antithyroid medication

$1.00



6

sick

patient reports malaise

$1.00



7

pregnant

patient pregnant

$1.00



8

thyroid surgery

history thyroid surgery

$1.00



9

I131 treatment

patient I131 treatment

$1.00



10

query hypothyroid

maybe hypothyroid

$1.00



11

query hyperthyroid

maybe hyperthyroid

$1.00



12

lithium

patient lithium

$1.00



13

goitre

patient goitre

$1.00



14

tumour

patient tumour

$1.00



15

hypopituitary

patient hypopituitary

$1.00



16

psych

psychological symptoms

$1.00



17

TSH

TSH value, measured



$22.78 first test group A, yes
$20.68 otherwise

18

T3

T3 value, measured



$11.41 first test group A, yes
$9.31 otherwise

19

TT4

TT4 value, measured



$14.51 first test group A, yes
$12.41 otherwise

20

T4U

T4U value, measured



$11.41 first test group A, yes
$9.31 otherwise

21

FTI

FTI calculated
TT4 T4U

used

-

22

class

diagnostic class

diagnostic class

-

23. Thyroid Disease dataset URL ftp://ftp.ics.uci.edu/pub/machine-learning-databases/thyroid-disease/ann-train.data.

405

fiT URNEY

Table 24: Classification costs Thyroid Disease dataset.
Actual Class

Guess Class

Cost

1 (hypothyroid)

1 (hypothyroid)

$0.00

1 (hypothyroid)

2 (hyperthyroid)

Minimum(Negative Error Cost, Positive Error Cost)

1 (hypothyroid)

3 (normal)

Negative Error Cost

2 (hyperthyroid)

1 (hypothyroid)

Minimum(Negative Error Cost, Positive Error Cost)

2 (hyperthyroid)

2 (hyperthyroid)

$0.00

2 (hyperthyroid)

3 (normal)

Negative Error Cost

3 (normal)

1 (hypothyroid)

Positive Error Cost

3 (normal)

2 (hyperthyroid)

Positive Error Cost

3 (normal)

3 (normal)

$0.00

Acknowledgments
Thanks Dr. Louise Linney help interpretation Ontario Ministry
Healths Schedule Benefits. Thanks Martin Brooks, Grigoris Karakoulas, Cullen Schaffer, Diana Gordon, Tim Niblett, Steven Minton, three anonymous referees JAIR
helpful comments earlier versions paper. work presented
informal talks University Ottawa Naval Research Laboratory. Thanks
audiences feedback.

References
Ackley, D., & Littman, M. (1991). Interactions learning evolution. Proceedings Second Conference Artificial Life, C. Langton, C. Taylor, D. Farmer, S.
Rasmussen, editors. California: Addison-Wesley.
Aha, D.W., Kibler, D., & Albert, M.K. (1991). Instance-based learning algorithms, Machine
Learning, 6, 37-66.
Aha, D.W., & Bankert, R.L. (1994). Feature selection case-based classification cloud
types: empirical comparison. Case-Based Reasoning: Papers 1994 Workshop, edited D.W. Aha, Technical Report WS-94-07, pp. 106-112. Menlo Park, CA:
AAAI Press.
Anderson, R.W. (in press). Learning evolution: quantitative genetics approach. Journal Theoretical Biology.
Baldwin, J.M. (1896). new factor evolution. American Naturalist, 30, 441-451.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification regression
trees. California: Wadsworth.
De Jong, K.A., Spears, W.M., & Gordon, D.F. (1993). Using genetic algorithms concept
learning. Machine Learning, 13, 161-188.

406

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Fogarty, T.C. (1992). Technical note: First nearest neighbor classification Frey Slates
letter recognition problem. Machine Learning, 9, 387-388.
Frey, P.W., & Slate, D.J., (1991). Letter recognition using Holland-style adaptive classifiers.
Machine Learning, 6, 161-182.
Friedman, J.H., & Stuetzle, W. (1981). Projection pursuit regression. Journal American Statistics Association, 76, 817-823.
Gordon, D.F., & Perlis, D. (1989). Explicitly biased generalization. Computational Intelligence, 5, 67-81.
Grefenstette, J.J. (1986). Optimization control parameters genetic algorithms. IEEE
Transactions Systems, Man, Cybernetics, 16, 122-128.
Grefenstette, J.J., Ramsey, C.L., & Schultz, A.C. (1990). Learning sequential decision rules
using simulation models competition. Machine Learning, 5, 355-381.
Hermans, J., Habbema, J.D.F., & Van der Burght, A.T. (1974). Cases doubt allocation
problems, k populations. Bulletin International Statistics Institute, 45, 523-529.
Hinton, G.E., & Nowlan, S.J. (1987). learning guide evolution. Complex Systems,
1, 495-502.
Karakoulas, G. (in preparation). Q-learning approach cost-effective classification. Technical Report, Knowledge Systems Laboratory, National Research Council Canada.
submitted Twelfth International Conference Machine Learning, ML-95.
Knoll, U., Nakhaeizadeh, G., & Tausend, B. (1994). Cost-sensitive pruning decision trees.
Proceedings Eight European Conference Machine Learning, ECML-94, pp.
383-386. Berlin, Germany: Springer-Verlag.
Koza, J.R. (1992). Genetic Programming: programming computers means
natural selection. Cambridge, MA: MIT Press.
Lirov, Y., & Yue, O.-C. (1991). Automated network troubleshooting knowledge acquisition.
Journal Applied Intelligence, 1, 121-132.
Maynard Smith, J. (1987). learning guides evolution. Nature, 329, 761-762.
Morgan, C.L. (1896). modification variation. Science, 4, 733-740.
Murphy, P.M., & Aha, D.W. (1994). UCI Repository Machine Learning Databases. University California Irvine, Department Information Computer Science.
Norton, S.W. (1989). Generating better decision trees. Proceedings Eleventh International Joint Conference Artificial Intelligence, IJCAI-89, pp. 800-805. Detroit, Michigan.
Nez, M. (1988). Economic induction: case study. Proceedings Third European
Working Session Learning, EWSL-88, pp. 139-145. California: Morgan Kaufmann.

407

fiT URNEY

Nez, M. (1991). use background knowledge decision tree induction. Machine
Learning, 6, 231-250.
Ontario Ministry Health (1992). Schedule benefits: Physician services health
insurance act, October 1, 1992. Ontario: Ministry Health.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing misclassification costs: Knowledge-intensive approaches learning noisy data. Proceedings Eleventh International Conference Machine Learning, ML-94, pp. 217225. New Brunswick, New Jersey.
Pearl, J. (1984). Heuristics: Intelligent search strategies computer problem solving. Massachusetts: Addison-Wesley.
Pearl, J. (1988). Probabilistic reasoning intelligent systems: Networks plausible inference. California: Morgan Kaufmann.
Pipitone, F., De Jong, K.A., & Spears, W.M. (1991). artificial intelligence approach
analog systems diagnosis. Testing Diagnosis Analog Circuits Systems,
Ruey-wen Liu, editor. New York: Van Nostrand-Reinhold.
Provost, F.J. (1994). Goal-directed inductive learning: Trading accuracy reduced error
cost. AAAI Spring Symposium Goal-Driven Learning.
Provost, F.J., & Buchanan, B.G. (in press). Inductive policy: pragmatics bias selection. Machine Learning.
Quinlan, J.R. (1992). C4.5: Programs machine learning. California: Morgan Kaufmann.
Ragavan, H., & Rendell, L. (1993). Lookahead feature construction learning hard concepts. Proceedings Tenth International Conference Machine Learning, ML-93,
pp. 252-259. California: Morgan Kaufmann.
Rymon, R. (1993). SE-tree based characterization induction problem. Proceedings
Tenth International Conference Machine Learning, ML-93, pp. 268-275. California: Morgan Kaufmann.
Schaffer, C. (1993). Selecting classification method cross-validation. Machine Learning, 13, 135-143.
Schaffer, J.D., Whitley, D., & Eshelman, L.J. (1992). Combinations genetic algorithms
neural networks: survey state art. Combinations Genetic Algorithms Neural Networks, D. Whitley J.D. Schaffer, editors. California: IEEE
Computer Society Press.
Seshu, R. (1989). Solving parity problem. Proceedings Fourth European Working
Session Learning, EWSL-89, pp. 263-271. California: Morgan Kaufmann.
Spears, W.M. (1992). Crossover mutation? Foundations Genetic Algorithms 2, FOGA92, edited D. Whitley. California: Morgan Kaufmann.

408

fiC OST -S ENSITIVE C LASSIFICATION : E MPIRICAL E VALUATION

Sutton, R.S. (1992). Introduction: challenge reinforcement learning. Machine Learning, 8, 225-227.
Tan, M., & Schlimmer, J. (1989). Cost-sensitive concept learning sensor use approach
recognition. Proceedings Sixth International Workshop Machine Learning,
ML-89, pp. 392-395. Ithaca, New York.
Tan, M., & Schlimmer, J. (1990). CSL: cost-sensitive learning system sensing
grasping objects. IEEE International Conference Robotics Automation. Cincinnati, Ohio.
Tan, M. (1993). Cost-sensitive learning classification knowledge applications
robotics. Machine Learning, 13, 7-33.
Tcheng, D., Lambert, B., Lu, S., Rendell, L. (1989). Building robust learning systems
combining induction optimization. Proceedings Eleventh International Joint
Conference Artificial Intelligence, IJCAI-89, pp. 806-812. Detroit, Michigan.
Turney, P.D. (in press). Technical note: Bias quantification stability. Machine
Learning.
Verdenius, F. (1991). method inductive cost optimization. Proceedings Fifth
European Working Session Learning, EWSL-91, pp. 179-191. New York: SpringerVerlag.
Waddington, C.H. (1942). Canalization development inheritance acquired characters. Nature, 150, 563-565.
Whitley, D., Dominic, S., Das, R., & Anderson, C.W. (1993). Genetic reinforcement learning
neurocontrol problems. Machine Learning, 13, 259-284.
Whitley, D., & Gruau, F. (1993). Adding learning cellular development neural networks: Evolution Baldwin effect. Evolutionary Computation, 1, 213-233.
Whitley, D., Gordon, S., & Mathias, K. (1994). Lamarckian evolution, Baldwin effect
function optimization. Parallel Problem Solving Nature PPSN III. Y. Davidor, H.P. Schwefel, R. Manner, editors, pp. 6-15. Berlin: Springer-Verlag.
Wilson, S.W. (1987). Classifier systems animat problem. Machine Learning, 2, 199228.

409


