journal artificial intelligence

submitted published

system induction oblique decision trees
sreerama k murthy
simon kasif
steven salzberg

department computer science
johns hopkins university baltimore md usa

murthy cs jhu edu
kasif cs jhu edu
salzberg cs jhu edu

abstract

article describes system induction oblique decision trees system
oc combines deterministic hill climbing two forms randomization good
oblique split form hyperplane node decision tree oblique decision
tree methods tuned especially domains attributes numeric although
adapted symbolic mixed symbolic numeric attributes present extensive empirical studies real artificial data analyze oc ability
construct oblique trees smaller accurate axis parallel counterparts examine benefits randomization construction oblique
decision trees

introduction
current data collection technology provides unique challenge opportunity automated machine learning techniques advent major scientific projects
human genome project hubble space telescope human brain mapping initiative generating enormous amounts data daily basis streams data
require automated methods analyze filter classify presenting
digested form domain scientist decision trees particularly useful tool context perform classification sequence simple easy understand tests
whose semantics intuitively clear domain experts decision trees used
classification tasks since moret safavin landgrebe
breiman et al book classification regression trees cart quinlan work id quinlan provided foundations become
large body one central techniques experimental machine learning
many variants decision tree dt introduced last decade
much work concentrated decision trees node checks value
single attribute breiman friedman olshen stone quinlan
quinlan initially proposed decision trees classification domains symbolic valued
attributes later extended numeric domains attributes
numeric tests form xi k xi one attributes example
k constant class decision trees may called axis parallel tests
node equivalent axis parallel hyperplanes attribute space example
decision tree given figure shows tree partitioning
creates attribute space

c ai access foundation morgan kaufmann publishers rights reserved

fifigure left side figure shows simple axis parallel tree uses two attributes
right side shows partitioning tree creates attribute space
researchers studied decision trees test node uses boolean
combinations attributes pagallo pagallo haussler sahami
linear combinations attributes see section different methods measuring
goodness decision tree nodes well techniques pruning tree reduce overfitting
increase accuracy explored discussed later sections
examine decision trees test linear combination attributes
internal node precisely let example take form x x x xd cj
cj class label xi real valued attributes test node
form

x
ai xi ad



ad real valued coecients tests equivalent hyperplanes oblique orientation axes call class decision trees oblique
decision trees trees form called multivariate brodley utgoff
prefer term oblique multivariate includes non linear combinations variables e curved surfaces trees contain linear tests clear
simply general form axis parallel trees since setting ai
coecients one test eq becomes familiar univariate test note
oblique decision trees produce polygonal polyhedral partitionings attribute
space axis parallel trees produce partitionings form hyper rectangles
parallel feature axes
intuitively clear underlying concept defined polygonal space partitioning preferable use oblique decision trees classification
example exist many domains one two oblique hyperplanes
best model use classification domains axis parallel methods ap constraint x xd real valued necessarily restrict oblique decision trees
numeric domains several researchers studied converting symbolic unordered
domains numeric ordered domains vice versa e g breiman et al hampson volper
utgoff brodley van de merckt keep discussion simple however
assume attributes numeric values



fifigure left side shows simple domain two oblique hyperplanes define
classes right side shows approximation sort axis parallel
decision tree would create model domain
proximate correct model staircase structure oblique tree building
method could capture tree smaller accurate figure
gives illustration
breiman et al first suggested method inducing oblique decision trees however little trees relatively recently utgoff
brodley heath kasif salzberg b murthy kasif salzberg beigel
brodley utgoff comparison existing approaches given detail
section purpose study review strengths weaknesses existing
methods design system combines strengths overcomes weaknesses evaluate system empirically analytically main contributions
conclusions study follows

developed randomized inducing oblique decision trees

examples extends original work breiman et al
randomization helps significantly learning many concepts

fully implemented oblique decision tree induction system
available internet code retrieved online appendix
anonymous ftp ftp ftp cs jhu edu pub oc oc tar z

randomized hill climbing used oc ecient

existing randomized oblique decision tree methods described fact
current implementation oc guarantees worst case running time
log n times greater worst case time inducing axis parallel trees e
dn log n vs dn

ability generate oblique trees often produces small trees compared
axis parallel methods underlying requires oblique split oblique

note though given oblique tree may fewer leaf nodes axis parallel tree
mean smaller oblique tree may cases larger terms information content
increased complexity tests node



fimurthy kasif salzberg

trees accurate axis parallel trees allowing tree building system
use oblique axis parallel splits broadens range domains
system useful
remaining sections follow outline remainder section
brie outlines general paradigm decision tree induction discusses complexity issues involved inducing oblique decision trees section brie reviews
existing techniques oblique dt induction outlines limitations
introduces oc system section describes oc system detail section
describes experiments compare performance oc several
axis parallel oblique decision tree induction methods range real world datasets
demonstrate empirically oc significantly benefits randomization
methods section conclude discussion open directions


top induction decision trees

inducing decision trees follow described quinlan top
induction decision trees called greedy divide conquer
method basic outline follows
begin set examples called training set examples belong
one class halt
consider tests divide two subsets score test according
well splits examples
choose greedily test scores highest
divide examples subsets run procedure recursively subset
quinlan original model considered attributes symbolic values model
test node splits attribute values thus test attribute
three values three child nodes one corresponding value
considers possible tests chooses one optimizes pre defined
goodness measure one could split symbolic values two subsets values
gives many choices split examples explain next oblique
decision tree methods cannot consider tests due complexity considerations

complexity induction oblique decision trees

one reason relatively papers inducing oblique decision trees
increased computational complexity compared axis parallel
case two important issues must addressed context top
decision tree must address complexity finding optimal separating
hyperplanes decision surfaces given node decision tree optimal hyperplane
minimize impurity measure used e g impurity might measured total
number examples mis classified second issue lower bound complexity
finding optimal e g smallest size trees


fifigure n points dimensions
n n distinct axis parallel splits

nd distinct dimensional oblique splits shows distinct
oblique axis parallel splits two specific points
let us first consider issue complexity selecting optimal oblique hyperplane single node tree domain
n training instances described

real valued attributes nd distinct dimensional oblique splits e
hyperplanes divide training instances uniquely two nonoverlapping subsets
upper bound derives observation every subset size n points
define dimensional hyperplane hyperplane rotated slightly
directions divide set points possible ways figure illustrates
upper limits two points two dimensions axis parallel splits n
distinct possibilities axis parallel methods c quinlan cart
breiman et al exhaustively search best split node
searching best oblique split therefore much dicult searching
best axis parallel split fact np hard
precisely heath proved following np hard given
set labelled examples hyperplane minimizes number misclassified
examples hyperplane implies method
finding optimal oblique split likely exponential cost
assuming p np

intuitively impractical enumerate nd distinct hyperplanes
choose best done axis parallel decision trees however non exhaustive
deterministic searching hyperplanes prone getting stuck
local minima
throughout use terms split hyperplane interchangeably refer test
node decision tree first usage standard moret refers fact
test splits data two partitions second usage refers geometric form test



fimurthy kasif salzberg

hand possible define impurity measures
finding optimal hyperplanes solved polynomial time example one
minimizes sum distances mis classified examples optimal solution
found linear programming methods distance measured along one dimension
however classifiers usually judged many points classify correctly
regardless close decision boundary point may lie thus standard
measures computing impurity base calculation discrete number examples
category side hyperplane section discusses several commonly
used impurity measures
let us address second issue complexity building small tree
easy inducing smallest axis parallel decision tree
np hard observation follows directly work hyafil rivest note
one generate smallest axis parallel tree consistent training
set polynomial time number attributes constant done
dynamic programming branch bound techniques see moret several
pointers tree uses oblique splits clear even fixed number
attributes generate optimal e g smallest decision tree polynomial time
suggests complexity constructing good oblique trees greater
axis parallel trees
easy see constructing optimal e g smallest oblique
decision tree np hard conclusion follows work blum rivest
implies dimensions e attributes producing
node oblique decision tree consistent training set np complete
specifically following decision np complete given training
set n examples boolean attributes exist node neural network
consistent easy following question np complete
given training set exist leaf node oblique decision tree consistent

complexity considerations took pragmatic trying
generate small trees looking smallest tree greedy used
oc virtually decision tree implicitly tries generate small trees
addition easy construct example optimal split node
lead best tree thus philosophy embodied oc locally
good splits spend excessive computational effort improving quality
splits

previous work oblique decision tree induction
describing oc brie discuss existing oblique dt
induction methods including cart linear combinations linear machine decision
trees simulated annealing decision trees methods induce
tree classifiers linear discriminants node notably methods
linear programming mangasarian setiono wolberg bennett mangasarian
b though methods optimal linear discriminants
specific goodness measures size linear program grows fast number


fiinduction oblique decision trees

induce split node decision tree
normalize values attributes
l
true
l l

let current split sl v c v pdi ai xi


search maximizes goodness split v ai c
let settings highest goodness searches
ai ai c c
perturb c maximize goodness sl keeping ad constant
jgoodness sl goodness sl j exit loop
eliminate irrelevant attributes fa ad g backward elimination
convert sl split un normalized attributes
return better sl best axis parallel split split
figure procedure used cart linear combinations cart lc node
decision tree
instances number attributes less closely related work
train artificial neural networks build decision tree classifiers brent
cios liu herman yeung
first oblique decision tree proposed cart linear combinations breiman et al chapter referred henceforth cart lc
important basis oc figure summarizes breiman et al notation
cart lc node decision tree core idea cartlc finds value maximizes goodness split
idea used oc explained detail section
describing cart lc breiman et al point still much room
development oc represents extension cart lc
includes significant additions addresses following limitations cart lc

cart lc fully deterministic built mechanism escaping local

minima although minima may common domains figure
shows simple example cart lc gets stuck

cart lc produces single tree given data set
cart lc sometimes makes adjustments increase impurity split
feature probably included allow escape local minima

upper bound time spent node decision tree halts
perturbation changes impurity impurity may
increase decrease spend arbitrarily long time node


fimurthy kasif salzberg



oc





initial loc





cart lc







figure deterministic perturbation cart lc fails correct
split data even starts location best axis parallel
split oc finds correct split one random jump
another oblique decision tree one uses different
cart lc linear machine decision trees lmdt system utgoff brodley
brodley utgoff successor perceptron tree method utgoff
utgoff brodley internal node lmdt tree linear machine nilsson
training presents examples repeatedly node linear
machine converges convergence cannot guaranteed lmdt uses heuristics
determine node stabilized make training stable even set
training instances linearly separable thermal training method frean
used similar simulated annealing
third system creates oblique trees simulated annealing decision trees
sadt heath et al b oc uses randomization sadt uses simulated
annealing kirkpatrick gelatt vecci good values coecients
hyperplane node tree sadt first places hyperplane canonical
location iteratively perturbs coecients small random amounts initially temperature parameter high sadt accepts almost perturbation
hyperplane regardless changes goodness score however system
cools changes improve goodness split likely accepted
though sadt use randomization allows effectively avoid local minima
compromises eciency runs much slower cart lc lmdt oc
sometimes considering tens thousands hyperplanes single node finishes
annealing
experiments section include showing methods
perform three artificial domains
next describe way combine strengths methods mentioned
avoiding oc uses deterministic hill climbing
time ensuring computational eciency addition uses two kinds
randomization avoid local minima limiting number random choices
guaranteed spend polynomial time node tree addition
randomization produced several benefits example means


fiinduction oblique decision trees

split set examples
best axis parallel split let impurity split
repeat r times
choose random hyperplane h
first iteration initialize h best axis parallel split
step impurity measure improve
perturb coecients h sequence
step repeat j times
choose random direction attempt perturb h direction
reduces impurity h go step
let impurity h set
output split corresponding
figure overview oc single node decision tree
produce many different trees data set offers possibility
family classifiers k decision tree example classified
majority vote k trees heath et al shown k decision tree methods
call k dt consistently outperform single tree methods classification
accuracy main criterion finally experiments indicate oc eciently finds
small accurate decision trees many different types classification

oblique classifier oc

section discuss details oblique decision tree induction system oc
part description include
method finding coecients hyperplane tree node
methods computing impurity goodness hyperplane
tree pruning strategy
methods coping missing irrelevant attributes
section focuses complicated algorithmic details e question
hyperplane splits given set instances two reasonably pure nonoverlapping subsets randomized perturbation main novel contribution
oc figure summarizes basic oc used node decision
tree figure explained following sections

perturbation

oc imposes restrictions orientation hyperplanes however order
least powerful standard dt methods first finds best axis parallel univariate
split node looking oblique split oc uses oblique split
improves best axis parallel split
pointed breiman et al chapter make sense use oblique split
number examples node n less almost equal number features



fimurthy kasif salzberg

search strategy space possible hyperplanes defined procedure
perturbs current hyperplane h location exponential
number distinct ways partition examples hyperplane procedure
simply enumerates unreasonably costly two main alternatives
considered past simulated annealing used sadt system heath
et al b deterministic heuristic search cart lc breiman et al
oc combines two ideas heuristic search finds local minimum
non deterministic search step get local minimum nondeterministic step oc simulated annealing however
start explaining perturb hyperplane split training set
node decision tree let n number examples number
attributes dimensions example k number categories
write tj xj xj xjd cj j th example training set xji
value attribute cj category label defined p
eq equation
current hyperplane h node decision tree written di
x ad
pd
substitute point example tj equation h get aixji ad vj
sign vj tells us whether point tj hyperplane h
e vj tj h h splits training set perfectly points
belonging category sign vj e sign vi sign vj iff
category ti category tj
oc adjusts coecients h individually finding locally optimal value one
coecient time key idea introduced breiman et al works follows
treat coecient variable treat coecients constants
vj viewed function particular condition tj h
equivalent
vj
uj
xxjm vj def
jm



assuming xjm ensure normalization definition uj
point tj h uj otherwise plugging points
equation obtain n constraints value
value satisfies many constraints
possible constraints satisfied perfect split
easy solve optimally simply sort values uj consider setting
midpoint pair different values illustrated figure figure
categories indicated font size larger ui belong one category
smaller another distinct placement coecient oc computes
impurity resulting split e g location u u illustrated two
examples left one example right would misclassified see section
different ways computing impurity figure illustrates simply
best one dimensional split u requires considering n values
value obtained solving one dimensional considered
data underfits concept default oc uses axis parallel splits tree nodes n
user vary threshold



fifigure finding optimal value single coecient large u correspond
examples one category small u another

perturb h
j n
compute uj eq
sort u un non decreasing order
best univariate split sorted uj
h substituting h
impurity h impurity h
f pmove pstag g
else impurity h impurity h
f probability pmove
pmove pmove pstag g
figure perturbation single coecient

replacement let h hyperplane obtained perturbing
h better lower impurity h h discarded h lower impurity h
becomes location hyperplane h h identical impurities
h replaces h probability pstag figure contains pseudocode perturbation
procedure
method locally improving coecient hyperplane need
decide coecients pick perturbation experimented
three different methods choosing coecient adjust namely sequential best
first random
seq repeat none coecient values modified loop
perturb h
best repeat coecient remains unmodified
coecient perturbed
maximum improvement impurity measure
perturb h
r repeat fixed number times experiments
random integer
perturb h
parameter pstag denoting stagnation probability probability hyperplane perturbed
location change impurity measure prevent impurity remaining
stagnant long time pstag decreases constant amount time oc makes stagnant
perturbation thus constant number perturbations occur node constant
set user pstag reset every time global impurity measure improved



fimurthy kasif salzberg

previous experiments murthy et al indicated order perturbation
coecients affect classification accuracy much parameters
especially randomization parameters see since none orders uniformly better used sequential seq perturbation experiments
reported section

randomization

perturbation halts split reaches local minimum impurity
measure oc search space local minimum occurs perturbation
single coecient current hyperplane decrease impurity measure course
local minimum may global minimum implemented two ways
attempting escape local minima perturbing hyperplane random vector
starting perturbation different random initial hyperplane
technique perturbing hyperplane random vector works follows
system reaches local minimum chooses random vector add
coecients current hyperplane computes optimal amount
hyperplane bep perturbed along random direction precise
hyperplane h di ai xi ad cannot improved deterministic perturbation
oc repeats following loop j times j user specified parameter set
default

choose random vector r r r rd
let amount
want perturb h direction r
pd
words let h ai ffri xi ad ffrd
optimal value
hyperplane h thus obtained decreases overall impurity replace h h
exit loop begin deterministic perturbation individual
coecients

note treat variable equation h therefore
n examples plugged equation h imposes constraint value
oc therefore use coecient perturbation method see section compute
best value j random jumps fail improve impurity oc halts uses
h split current tree node
intuitive way understanding random jump look atpthe dual space
actually searching note equation h di ai xi ad defines
space axes coecients ai rather attributes xi every point
space defines distinct hyperplane original formulation deterministic
used oc picks hyperplane adjusts coecients one time thus
dual space oc chooses point perturbs moving parallel axes
random vector r represents random direction space finding best value
oc finds best distance adjust hyperplane direction r


fiinduction oblique decision trees

note additional perturbation random direction significantly increase time complexity see appendix found experiments
even single random jump used local minimum proves helpful
classification accuracy improved every one data sets perturbations
made see section examples
second technique avoiding local minima variation idea performing
multiple local searches technique multiple local searches natural extension
local search widely mentioned optimization literature see roth
early example steps perturbation
deterministic initial hyperplane largely determines local minimum
encountered first perturbing single initial hyperplane thus unlikely lead best
split given data set cases random perturbation method fails escape
local minima may helpful simply start afresh initial hyperplane
use word restart denote one run perturbation one node
decision tree one random initial hyperplane restart cycles
perturbs coecients one time tries perturb hyperplane
random direction reaches local minimum last perturbation
reduces impurity goes back perturbing coecients one time
restart ends neither deterministic local search random jump
better split one optional parameters oc specifies many restarts use
one restart used best hyperplane found thus far saved
experiments classification accuracies increased one restart
accuracy tended increase point level restarts
depending domain overall use multiple initial hyperplanes substantially
improved quality decision trees found see section examples
carefully combining hill climbing randomization oc ensures worst case time
dn log n inducing decision tree see appendix derivation upper
bound

best axis parallel split clear axis parallel splits suitable

data distributions oblique splits take account distributions oc computes best axis parallel split oblique split node picks better
two calculating best axis parallel split takes additional dn log n time
increase asymptotic time complexity oc simple variant
oc system user opt switch oblique perturbations thus building
axis parallel tree training data section empirically demonstrates
axis parallel variant oc compares favorably existing axis parallel
first run node begins location best axis parallel
hyperplane subsequent restarts begin random locations
sometimes simple axis parallel split preferable oblique split even oblique split slightly
lower impurity user specify bias input parameter oc



fimurthy kasif salzberg

details
impurity measures

oc attempts divide dimensional attribute space homogeneous regions e
regions contain examples one category goal adding nodes
tree split sample space minimize impurity training
set measure goodness instead impurity difference
goodness values maximized impurity minimized many different
measures impurity studied breiman et al quinlan mingers
b buntine niblett fayyad irani heath et al b
oc system designed work large class impurity measures stated
simply impurity measure uses counts examples belonging every category
sides split oc use see murthy salzberg ways
mapping kinds impurity measures class impurity measures user
plug impurity measure fits description oc implementation includes
six impurity measures namely







information gain
gini index
twoing rule
max minority
sum minority
sum variances

though six measures defined elsewhere literature
cases made slight modifications defined precisely appendix b
experiments indicated average information gain gini index twoing rule
perform better three measures axis parallel oblique trees
twoing rule current default impurity measure oc used
experiments reported section however artificial data sets
sum minority max minority perform much better rest measures
instance sum minority easily induces exact tree pol data set described
section methods diculty finding best tree

twoing rule twoing rule first proposed breiman et al value

computed defined

k
x

twoingvalue jtlj n jtrj n



jli jtlj ri jtrjj

jtlj jtrj number examples left right split node n
number examples node li ri number examples category
left right split twoingvalue actually goodness measure rather
impurity measure therefore oc attempts minimize reciprocal value
remaining five impurity measures implemented oc defined appendix b


fiinduction oblique decision trees

pruning

virtually decision tree induction systems prune trees create order avoid
overfitting data many studies found judicious pruning smaller
accurate classifiers decision trees well types machine learning
systems quinlan niblett cestnik kononenko bratko kodratoff
manago cohen hassibi stork wolpert schaffer
oc system implemented existing pruning method note tree
pruning method work fine within oc experimental evaluations
mingers work cited chose breiman et al cost complexity
cc pruning default pruning method oc method
called error complexity weakest link pruning requires separate pruning set
pruning set randomly chosen subset training set approximated
cross validation oc randomly chooses default value training data
use pruning experiments reported used default value
brie idea behind cc pruning create set trees decreasing size
original complete tree trees used classify pruning set accuracy
estimated cc pruning chooses smallest tree whose accuracy within k
standard errors squared best accuracy obtained se rule k used
tree highest accuracy pruning set selected k smaller tree size
preferred higher accuracy details cost complexity pruning see breiman et
al mingers
irrelevant attributes

irrelevant attributes pose significant machine learning methods breiman
et al aha almuallin dietterich kira rendell salzberg
cardie schlimmer langley sage brodley utgoff decision
tree even axis parallel ones confused many irrelevant attributes
oblique decision trees learn coecients attribute dt node one
might hope values chosen coecient would ect relative importance
corresponding attributes clearly though process searching good coecient
values much ecient fewer attributes search space much
smaller reason oblique dt induction methods benefit substantially
feature selection method selects subset original attribute set
conjunction coecient learning breiman et al brodley utgoff

currently oc built mechanism select relevant attributes however easy include several standard methods e g stepwise forward selection
stepwise backward selection even ad hoc method select features running
tree building process example separate experiments data hubble
space telescope salzberg chandar ford murthy white used feature selection methods preprocessing step oc reduced number attributes
resulting decision trees simpler accurate work currently
underway incorporate ecient feature selection technique oc system


fimurthy kasif salzberg

regarding missing values example missing value attribute oc uses
mean value attribute one course use techniques handling
missing values considered study

experiments

section present two sets experiments support following two claims
oc compares favorably variety real world domains several existing
axis parallel oblique decision tree induction methods
randomization form multiple local searches random jumps improves quality decision trees produced oc
experimental method used experiments described section sections describe experiments corresponding two claims experimental section begins description data sets presents experimental
discussion

experimental method

used five fold cross validation cv experiments estimate classification
accuracy k fold cv experiment consists following steps
randomly divide data k equal sized disjoint partitions
partition build decision tree data outside partition test
tree data partition
sum number correct classifications k trees divide total number
instances compute classification accuracy report accuracy
average size k trees
entry tables ten fold cv experiments e tests
used decision trees ten fold cross validations used different random
partitioning data entry tables reports mean standard deviation
classification accuracy followed mean standard deviation decision
tree size measured number leaf nodes good high values
accuracy low values tree size small standard deviations
addition oc included experiments axis parallel version oc
considers axis parallel hyperplanes call version described section
oc ap experiments oc oc ap used twoing rule section
measure impurity parameters oc took default values unless stated
otherwise defaults include following number restarts node number
random jumps attempted local minimum order coecient perturbation
sequential pruning method cost complexity se rule training
set exclusively pruning
comparison used oblique version cart cart lc
implemented version cart lc following description breiman et
al chapter however may differences version


fiinduction oblique decision trees

versions system note cart lc freely available implementation
cart lc measured impurity twoing rule used se cost complexity
pruning separate test set oc include feature selection
methods cart lc oc implement normalization
cart coecient perturbation may alternate indefinitely two locations
hyperplane see section imposed arbitrary limit perturbations
forcing perturbation halt
included axis parallel cart c comparisons used implementations ind package buntine default
cart c styles defined package used without altering parameter
settings cart style uses twoing rule se cost complexity pruning
fold cross validation pruning method impurity measure defaults
c style described quinlan

oc vs decision tree induction methods

table compares performance oc three well known decision tree induction
methods plus oc ap six different real world data sets next section
consider artificial data concept definition precisely characterized
description data sets

star galaxy discrimination two data sets came large set astronom

ical images collected odewahn et al odewahn stockwell pennington humphreys
zumach study used images train artificial neural networks
running perceptron back propagation goal classify example star galaxy image characterized real valued attributes
attributes measurements defined astronomers likely relevant
task objects image divided odewahn et al bright dim
data sets image intensity values dim images inherently
dicult classify note bright objects bright relation others
data set actuality extremely faint visible powerful
telescopes bright set contains objects dim set contains objects
addition reported table following appeared
star galaxy data odewahn et al reported accuracy accuracy
bright objects dim ones although noted study
used single training test set partition heath reported accuracy
bright objects sadt average tree size leaves study used
single training test set salzberg reported accuracies bright
objects dim objects nearest neighbor nn coupled
feature selection method reduces number features
breast cancer diagnosis mangasarian bennett compiled data diagnosing breast cancer test several classification methods mangasarian
et al bennett mangasarian data represents set patients
breast cancer patient characterized nine numeric attributes plus
diagnosis tumor benign malignant data set currently entries


fimurthy kasif salzberg

bright g


cart lc


oc ap


cart ap


c



oc

dim g











cancer











iris











housing











diabetes











table comparison oc decision tree induction methods six different
data sets first line method gives accuracies second line gives
average tree sizes highest accuracy domain appears boldface
available uc irvine machine learning repository murphy aha
heath et al b reported accuracy subset data set
instances average decision tree size nodes sadt salzberg
reported accuracy nn smaller data set herman
yeung reported accuracy piece wise linear classification
somewhat smaller data set

classifying irises fisher famous iris data extensively studied

statistics machine learning literature data consists examples
example described four numeric attributes examples
three different types iris ower weiss kapouleas obtained accuracies
data back propagation nn respectively

housing costs boston data set available part uci ml repos

itory describes housing values suburbs boston function continuous
attributes binary attribute harrison rubinfeld category variable median value owner occupied homes actually continuous discretized
category value otherwise uses data see belsley
quinlan b

diabetes diagnosis data catalogs presence absence diabetes among pima

indian females years older function eight numeric valued attributes
original source data national institute diabetes digestive kidney
diseases available uci repository smith et al reported
accuracy data adap learning different experimental
method used


fiinduction oblique decision trees

discussion

table shows six data sets considered oc consistently finds better
trees original oblique cart method accuracy greater six domains
although difference significant standard deviations dim
star galaxy average tree sizes roughly equal five six domains
dim stars galaxies oc found considerably smaller trees differences
analyzed quantified artificial data following section
five decision tree induction methods oc highest accuracy four
six domains bright stars dim stars cancer diagnosis diabetes diagnosis
remaining two domains oc second highest accuracy case surprisingly
oblique methods oc cart lc generally much smaller trees axisparallel methods difference quite striking domains note example
oc produced tree nodes average dim star galaxy
c produced tree nodes times larger course domains
axis parallel tree appropriate representation axis parallel methods compare
well oblique methods terms tree size fact iris data methods
found similar sized trees

randomization helps oc

second set experiments examine closely effect introducing randomized steps finding oblique splits experiments demonstrate
oc ability produce accurate tree set training data clearly enhanced
two kinds randomization uses precisely use three artificial data sets
underlying concept known experimenters oc performance improves substantially deterministic hill climbing augmented
three ways

multiple restarts random initial locations
perturbations random directions local minima
randomization steps
order clear differences one needs know concept
underlying data indeed dicult learn simple concepts say two linearly
separable classes many different learning produce accurate
classifiers therefore advantages randomization may detectable
known many commonly used data sets uci repository easy
learn simple representations holte therefore data sets may
ideal purposes thus created number artificial data sets present different
learning know correct concept definition allows
us quantify precisely parameters affect performance
second purpose experiment compare oc search strategy
two existing oblique decision tree induction systems lmdt brodley utgoff
sadt heath et al b quality trees induced oc
good better trees induced existing systems three


fimurthy kasif salzberg

artificial domains oc achieves good balance amount
effort expended search quality tree induced
lmdt sadt used information gain experiment however
change oc default measure twoing rule observed experiments
reported oc information gain produce significantly different
maximum number successive unproductive perturbations allowed
node set sadt parameters used default settings provided
systems
description artificial data

ls ls data set instances divided two categories instance

described ten attributes x x whose values uniformly distributed range
data linearly separable hyperplane thus name ls defined
equation x x x x x x x x x x instances
generated randomly labelled according side hyperplane fell
oblique dt induction methods intuitively prefer linear separator one
exists interesting compare search techniques data set
know separator exists task relatively simple lower dimensions chose
dimensional data make dicult

pol data set shown figure instances two dimensions

divided two categories underlying concept set four parallel oblique lines
thus name pol dividing instances five homogeneous regions concept
dicult learn single linear separator minimal size tree still quite
small

rcb rcb stands rotated checker board data set subject

experiments hard classification decision trees murthy salzberg
data set shown figure instances belonging one
eight categories concept dicult learn axis parallel method obvious
reasons quite dicult oblique methods several reasons biggest
correct root node shown figure separate
class impurity measures sum minority fail miserably
although others e g twoing rule work much better another
deterministic coecient perturbation get stuck local minima
many places data set
table summarizes experiment three smaller tables one
data set smaller table compare four variants oc lmdt sadt
different oc obtained varying number restarts
number random jumps random jumps used twenty random jumps
tried local minimum soon one found improved impurity
current hyperplane moved hyperplane started running
deterministic perturbation procedure none random jumps improved
impurity search halted restarts tried training
test partitions used methods cross validation run recall


fiinduction oblique decision trees

lr

ot

rr



l



r

rl


rr

r


oo
r





































































































































































































































































































































































































































ro


rrr






































































































































































































































































































































































































































figure pol rcb data sets
linearly separable ls data
r j accuracy
size
hyperplanes








lmdt

sadt

parallel oblique lines pol data
r j accuracy
size
hyperplanes








lmdt

sadt

rotated checker board rcb data
r j accuracy
size
hyperplanes








lmdt

sadt

table effect randomization oc first column labelled r j shows
number restarts r followed maximum number random jumps j
attempted oc local minimum lmdt sadt
included comparison four variants oc size average tree size
measured number leaf nodes third column shows average
number hyperplanes considered building one tree


fimurthy kasif salzberg

average ten fold cvs trees pruned
data noise free furthermore emphasis search
table includes number hyperplanes considered
building complete tree note oc sadt number hyperplanes considered generally much larger number perturbations actually made
compare newly generated hyperplanes existing hyperplanes
adjusting existing one nevertheless number good estimate much effort
expends every hyperplane must evaluated according
impurity measure lmdt number hyperplanes considered identical
actual number perturbations
discussion

oc quite clear first line table labelled gives
accuracies tree sizes randomization used variant similar
cart lc increase use randomization accuracy increases
tree size decreases exactly hoped decided
introduce randomization method
looking closely tables ask effect random jumps alone
illustrated second line table attempted random
jumps local minimum restarts accuracy increased domain
tree size decreased dramatically roughly factor two pol rcb
domains note noise domains high accuracies
expected thus increases percent accuracy possible
looking third line sub table table see effect multiple restarts
oc restarts random jumps escape local minima improvement
even noticeable ls data random jumps alone used
data set accuracy jumped significantly tree size dropped
nodes pol rcb data improvements comparable
obtained random jumps rcb data tree size dropped factor
leaf nodes leaf nodes accuracy increased
fourth line table shows effect randomized steps among
oc entries line highest accuracies smallest trees three
data sets clear randomization big win kinds
addition note smallest tree rcb data eight leaf nodes
oc average trees without pruning leaf nodes clear data
set thought dicult one oc came close finding optimal
tree nearly every run recall numbers table average fold
cv experiments e average decision trees ls data dicult
simple concept higher dimensions optimal tree
single hyperplane two nodes oc unable current parameter
settings pol data required minimum leaf nodes oc found minimalsize tree time seen table although shown table
separate experiment found oc consistently finds linear separator ls data
restarts random jumps used



fiinduction oblique decision trees

oc sum minority performed better pol data twoing rule
impurity measure e found correct tree less time
lmdt sadt data lead interesting insights
surprisingly lmdt well linearly separable ls data
require inordinate amount search clearly data linearly separable one
use method lmdt linear programming oc sadt diculty finding
linear separator although experiments oc eventually given sucient
time
hand non linearly separable data sets lmdt produces
much larger trees significantly less accurate produced oc
sadt even deterministic variant oc zero restarts zero random jumps
outperforms lmdt much less search
although sadt sometimes produces accurate trees main weakness
enormous amount search time required roughly times greater oc even
setting one explanation oc advantage use directed search
opposed strictly random search used simulated annealing overall table shows
oc use randomization quite effective non linearly separable data
natural ask randomization helps oc task inducing decision trees
researchers combinatorial optimization observed randomized search usually
succeeds search space holds abundance good solutions gupta smolka
bhaskar furthermore randomization improve upon deterministic search
many local maxima search space lead poor solutions oc search
space local maximum hyperplane cannot improved deterministic
search procedure solution complete decision tree significant fraction
local maxima lead bad trees stop first local maximum
encounter perform poorly randomization allows oc consider many
different local maxima modest percentage maxima lead good trees
good chance finding one trees experiments oc thus far indicate
space oblique hyperplanes usually contains numerous local maxima
substantial percentage locally good hyperplanes lead good decision trees

conclusions future work
described oc system constructing oblique decision trees
shown experimentally oc produce good classifiers range real world
artificial domains shown use randomization improves upon
original proposed breiman et al without significantly increasing
computational cost
use randomization might beneficial axis parallel tree methods note
although optimal test respect impurity measure
node tree complete tree may optimal well known
finding smallest tree np complete hyafil rivest thus even axis parallel
decision tree methods produce ideal decision trees quinlan suggested
windowing might used way introducing randomization c even
though designed another purpose quinlan windowing


fimurthy kasif salzberg

selects random subset training data builds tree
believe randomization powerful tool context decision trees
experiments one example might exploited process
conducting experiments quantify accurately effects different forms
randomization
clear ability produce oblique splits node broadens capabilities decision tree especially regards domains numeric attributes
course axis parallel splits simpler sense description split
uses one attribute node oc uses oblique splits impurity less
impurity best axis parallel split however one could easily penalize
additional complexity oblique split remains open area
general point domain best captured tree uses
oblique hyperplanes desirable system generate tree
shown including used experiments oc builds small
decision trees capture domain well

appendix complexity analysis oc

following oc runs eciently even worst case data
set n examples points attributes per example oc uses dn log n
time assume n analysis
analysis assume coecients hyperplane adjusted sequential order seq method described number restarts node
r number random jumps tried j r j constants fixed
advance running
initializing hyperplane random position takes time need
consider first maximum amount work oc finds location
hyperplane need consider many times move hyperplane
attempting perturb first coecient takes dn n log n time computing
ui points equation requires dn time sorting ui takes
n log n gives us dn n log n work
perturbing improve things try perturb computing
ui take n time one term different ui sorting
take n log n step takes n n log n n log n time
likewise ad take n log n additional time assuming still
found better hyperplane checking coecient thus total time cycle
attempt perturb additional coecients n log n
dn log n
summing time cycle coecients dn log n dn n log n
dn log n
none coecients improved split attempt make j random
jumps since j constant consider j analysis step


fiinduction oblique decision trees

involves choosing random vector running perturbation solve
explained section need compute set ui sort
takes dn n log n time amount time dominated
time adjust coecients total time far still dn log n
time oc spend node halting finding improved
hyperplane
assuming oc sum minority max minority error measure
reduce impurity hyperplane n times clear improvement
means one example correctly classified hyperplane thus
total amount work node limited n dn log n dn log n
analysis extends linear cost factors information gain gini index
twoing rule two categories apply measure
example uses distances mis classified objects hyperplane practice
found number improvements per node much smaller n
assuming oc adjusts hyperplane improves impurity measure
dn log n work worst case
however oc allows certain number adjustments hyperplane
improve impurity although never accept change worsens impurity
number allowed determined constant known stagnant perturbations let
value works follows
time oc finds hyperplane improves old one resets counter
zero move hyperplane different location equal impurity
times moves repeats perturbation whenever
impurity reduced starts counter allows moves equally good
locations thus clear feature increases worst case complexity oc
constant factor
finally note overall cost oc dn log n e upper
bound total running time oc independent size tree ends
creating upper bound applies sum minority max minority open question
whether similar upper bound proven information gain gini index
thus worst case asymptotic complexity system comparable systems
construct axis parallel decision trees dn worst case complexity
sketch intuition leads bound let g total impurity summed
leaves partially constructed tree e sum currently misclassified points
tree observe time run perturbation node
tree halt improve g least one unit worst case analysis one node
realized perturbation run every one n examples
happens would longer mis classified examples tree
would complete

appendix b definitions impurity measures available oc

addition twoing rule defined text oc contains built definitions five
additional impurity measures defined follows following definitions


fimurthy kasif salzberg

set examples node split contains n instances belong
one k categories initially set entire training set hyperplane h divides
two non overlapping subsets tl tr e left right lj rj number
instances category j tl tr respectively impurity measures initially
check see tl tr homogeneous e examples belong category
return minimum zero impurity

information gain measure information gained particular split pop

ularized context decision trees quinlan quinlan definition makes
information gain goodness measure e something maximize oc attempts
minimize whatever impurity measure uses use reciprocal standard value
information gain oc implementation

gini index gini criterion index proposed decision trees breiman et
al gini index originally defined measures probability misclassification
set instances rather impurity split implement following
variation
ginil
ginir

k
x

k
x


li jtlj
ri jtrj

impurity jtlj ginil jtrj ginir n
ginil gini index left side hyperplane ginir
right

max minority measures max minority sum minority sum variances

defined context decision trees heath kasif salzberg b max
minority theoretical advantage tree built minimizing measure
depth log n experiments indicated great advantage
practice seldom impurity measures produce trees substantially deeper
produced max minority definition
minorityl
minorityr

k
x
max li
k
x
max ri

li
ri

max minority max minorityl minorityr
sum variances called sum impurities heath et al



fiinduction oblique decision trees

sum minority measure similar max minority minorityl minorityr defined max minority measure sum minority sum
two values measure simplest way quantifying impurity simply
counts number misclassified instances
though sum minority performs well domains obvious aws
one example consider domain n k e examples
numeric attribute classes suppose examples sorted according
single attribute first instances belong category followed instances category followed instances category possible splits distribution
sum minority therefore impossible sum minority distinguish split preferable although splitting alternations categories
clearly better
sum variances definition measure
jx
tl j
jx
tl j
variancel cat tli cat tlj jtlj
variancer



j

jx
trj

jx
trj



cat tri

j

cat trj jtrj

sum variances variancel variancer
cat ti category instance ti measure computed actual
class labels easy see impurity computed varies depending numbers
assigned classes instance consists points category
points category consists points category points category
sum variances values different avoid
oc uniformly reassigns category numbers according frequency occurrence
category node computing sum variances

acknowledgements
authors thank richard beigel yale university suggesting idea jumping
random direction thanks wray buntine nasa ames center providing
ind package carla brodley providing lmdt code david heath
providing sadt code assisting us thanks three anonymous
reviewers many helpful suggestions material upon work supported
national science foundation grant nos iri iri iri

references
aha study instance supervised learning mathematical empirical psychological evaluations ph thesis department information
computer science university california irvine


fimurthy kasif salzberg

almuallin h dietterich learning many irrelevant features proceedings ninth national conference artificial intelligence pp san
jose ca
belsley regression diagnostics identifying uential data sources
collinearity wiley sons york
bennett k mangasarian robust linear programming discrimination two
linearly inseparable sets optimization methods software
bennett k mangasarian multicategory discrimination via linear programming optimization methods software
bennett k mangasarian b serial parallel multicategory discrimination
siam journal optimization
blum rivest r training node neural network np complete proceedings workshop computational learning theory pp boston
morgan kaufmann
breiman l friedman j olshen r stone c classification regression
trees wadsworth international group
brent r p fast training multilayer neural nets ieee transactions
neural networks
brodley c e utgoff p e multivariate versus univariate decision trees tech
rep coins cr dept computer science university massachusetts
amherst
brodley c e utgoff p e multivariate decision trees machine learning
appear
buntine w tree classification software technology third national
technology transfer conference exposition
buntine w niblett comparison splitting rules decision tree
induction machine learning
cardie c decision trees improve case learning proceedings
tenth international conference machine learning pp university
massachusetts amherst
cestnik g kononenko bratko assistant knowledge acquisition
tool sophisticated users bratko lavrac n eds progress machine
learning sigma press
cios k j liu n machine learning method generation neural network
architecture continuous id ieee transactions neural networks



fiinduction oblique decision trees

cohen w ecient pruning methods separate conquer rule learning systems proceedings th international joint conference artificial intelligence pp morgan kaufmann
fayyad u irani k b attribute specification decision tree
generation proceedings tenth national conference artificial intelligence
pp san jose ca aaai press
frean small nets short paths optimising neural computation ph
thesis centre cognitive science university edinburgh
gupta r smolka bhaskar randomization sequential distributed
acm computing surveys
hampson volper linear function neurons structure training biological cybernetics
harrison rubinfeld hedonic prices demand clean air journal
environmental economics management
hassibi b stork second order derivatives network pruning optimal
brain surgeon advances neural information processing systems pp
morgan kaufmann san mateo ca
heath geometric framework machine learning ph thesis johns
hopkins university baltimore maryland
heath kasif salzberg k dt multi tree learning method
proceedings second international workshop multistrategy learning pp
harpers ferry wv george mason university
heath kasif salzberg b learning oblique decision trees proceedings
th international joint conference artificial intelligence pp
chambery france morgan kaufmann
herman g yeung k piecewise linear classification ieee transactions pattern analysis machine intelligence
holte r simple classification rules perform well commonly used
datasets machine learning
hyafil l rivest r l constructing optimal binary decision trees npcomplete information processing letters
kira k rendell l practical feature selection proceedings
ninth international conference machine learning pp aberdeen
scotland morgan kaufmann
kirkpatrick gelatt c vecci optimization simulated annealing
science


fimurthy kasif salzberg

kodratoff manago generalization noise international journal
man machine studies
langley p sage scaling domains many irrelevant features learning
systems department siemens corporate princeton nj
mangasarian setiono r wolberg w pattern recognition via linear programming theory application medical diagnosis siam workshop
optimization
mingers j empirical comparison pruning methods decision tree induction machine learning
mingers j b empirical comparison selection measures decision tree induction machine learning
moret b decision trees diagrams computing surveys
murphy p aha uci repository machine learning databases machinereadable data repository maintained department information computer
science university california irvine anonymous ftp ics uci edu
directory pub machine learning databases
murthy k kasif salzberg beigel r oc randomized induction
oblique decision trees proceedings eleventh national conference artificial
intelligence pp washington c mit press
murthy k salzberg structure improve decision trees tech rep
jhu department computer science johns hopkins university
niblett constructing decision trees noisy domains bratko lavrac
n eds progress machine learning sigma press england
nilsson n learning machines morgan kaufmann san mateo ca
odewahn stockwell e pennington r humphreys r zumach w automated star galaxy descrimination neural networks astronomical journal

pagallo g adaptive decision tree learning examples ph
thesis university california santa cruz
pagallo g haussler boolean feature discovery empirical learning machine
learning
quinlan j r learning ecient classification procedures application
chess end games michalski r carbonell j mitchell eds machine
learning artificial intelligence morgan kaufmann san mateo ca
quinlan j r induction decision trees machine learning


fiinduction oblique decision trees

quinlan j r simplifying decision trees international journal man machine
studies
quinlan j r c programs machine learning morgan kaufmann publishers san mateo ca
quinlan j r b combining instance model learning proceedings
tenth international conference machine learning pp university
massachusetts amherst morgan kaufmann
roth r h solving linear discrete optimization journal
acm
safavin r landgrebe survey decision tree classifier methodology
ieee transactions systems man cybernetics
sahami learning non linearly separable boolean functions linear threshold unit trees madaline style networks proceedings eleventh national
conference artificial intelligence pp aaai press
salzberg nearest hyperrectangle learning method machine learning

salzberg combining learning search create good classifiers tech rep
jhu johns hopkins university baltimore md
salzberg chandar r ford h murthy k white r decision trees
automated identification cosmic rays hubble space telescope images publications astronomical society pacific appear
schaffer c overfitting avoidance bias machine learning
schlimmer j eciently inducing determinations complete systematic
search uses optimal pruning proceedings tenth international
conference machine learning pp morgan kaufmann
smith j everhart j dickson w knowler w johannes r
adap learning forecast onset diabetes mellitus proceedings
symposium computer applications medical care pp ieee
computer society press
utgoff p e perceptron trees case study hybrid concept representations
connection science
utgoff p e brodley c e incremental method finding multivariate
splits decision trees proceedings seventh international conference
machine learning pp los altos ca morgan kaufmann
utgoff p e brodley c e linear machine decision trees tech rep
university massachusetts amherst


fimurthy kasif salzberg

van de merckt nfdt system learns exible concepts decision
trees numerical attributes proceedings ninth international workshop
machine learning pp
van de merckt decision trees numerical attribute spaces proceedings
th international joint conference artificial intelligence pp
weiss kapouleas empirical comparison pattern recognition neural
nets machine learning classification methods proceedings th international joint conference artificial intelligence pp detroit mi morgan
kaufmann
wolpert overfitting avoidance bias tech rep sfi tr
santa fe institute santa fe mexico




