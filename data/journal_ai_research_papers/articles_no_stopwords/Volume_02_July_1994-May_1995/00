Journal Artificial Intelligence Research 2 (1994) 1-32

Submitted 4/94; published 8/94

System Induction Oblique Decision Trees
Sreerama K. Murthy
Simon Kasif
Steven Salzberg

Department Computer Science
Johns Hopkins University, Baltimore, MD 21218 USA

murthy@cs.jhu.edu
kasif@cs.jhu.edu
salzberg@cs.jhu.edu

Abstract

article describes new system induction oblique decision trees. system,
OC1, combines deterministic hill-climbing two forms randomization find good
oblique split (in form hyperplane) node decision tree. Oblique decision
tree methods tuned especially domains attributes numeric, although
adapted symbolic mixed symbolic/numeric attributes. present extensive empirical studies, using real artificial data, analyze OC1's ability
construct oblique trees smaller accurate axis-parallel counterparts. examine benefits randomization construction oblique
decision trees.

1. Introduction
Current data collection technology provides unique challenge opportunity automated machine learning techniques. advent major scientific projects
Human Genome Project, Hubble Space Telescope, human brain mapping initiative generating enormous amounts data daily basis. streams data
require automated methods analyze, filter, classify presenting
digested form domain scientist. Decision trees particularly useful tool context perform classification sequence simple, easy-to-understand tests
whose semantics intuitively clear domain experts. Decision trees used
classification tasks since 1960s (Moret, 1982; Safavin & Landgrebe, 1991).
1980's, Breiman et al.'s book classification regression trees (CART) Quinlan's work ID3 (Quinlan, 1983, 1986) provided foundations become
large body research one central techniques experimental machine learning.
Many variants decision tree (DT) algorithms introduced last decade.
Much work concentrated decision trees node checks value
single attribute (Breiman, Friedman, Olshen, & Stone, 1984; Quinlan, 1986, 1993a).
Quinlan initially proposed decision trees classification domains symbolic-valued
attributes (1986), later extended numeric domains (1987). attributes
numeric, tests form xi > k, xi one attributes example
k constant. class decision trees may called axis-parallel, tests
node equivalent axis-parallel hyperplanes attribute space. example
decision tree given Figure 1, shows tree partitioning
creates 2-D attribute space.

c 1994 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiFigure 1: left side figure shows simple axis-parallel tree uses two attributes.
right side shows partitioning tree creates attribute space.
Researchers studied decision trees test node uses boolean
combinations attributes (Pagallo, 1990; Pagallo & Haussler, 1990; Sahami, 1993)
linear combinations attributes (see Section 2). Different methods measuring
goodness decision tree nodes, well techniques pruning tree reduce overfitting
increase accuracy explored, discussed later sections.
paper, examine decision trees test linear combination attributes
internal node. precisely, let example take form X = x1 ; x2; : : :; xd; Cj
Cj class label xi 's real-valued attributes.1 test node
form:

X
ai xi + ad+1 > 0
(1)
i=1

a1 ; : : :; ad+1 real-valued coecients. tests equivalent hyperplanes oblique orientation axes, call class decision trees oblique
decision trees. (Trees form called \multivariate" (Brodley & Utgoff,
1994). prefer term \oblique" \multivariate" includes non-linear combinations variables, i.e., curved surfaces. trees contain linear tests.) clear
simply general form axis-parallel trees, since setting ai = 0
coecients one, test Eq. 1 becomes familiar univariate test. Note
oblique decision trees produce polygonal (polyhedral) partitionings attribute
space, axis-parallel trees produce partitionings form hyper-rectangles
parallel feature axes.
intuitively clear underlying concept defined polygonal space partitioning, preferable use oblique decision trees classification.
example, exist many domains one two oblique hyperplanes
best model use classification. domains, axis-parallel methods ap1. constraint x1 ; : : : ; xd real-valued necessarily restrict oblique decision trees
numeric domains. Several researchers studied problem converting symbolic (unordered)
domains numeric (ordered) domains vice versa; e.g., (Breiman et al., 1984; Hampson & Volper,
1986; Utgoff & Brodley, 1990; Van de Merckt, 1992, 1993). keep discussion simple, however,
assume attributes numeric values.

2

fiFigure 2: left side shows simple 2-D domain two oblique hyperplanes define
classes. right side shows approximation sort axis-parallel
decision tree would create model domain.
proximate correct model staircase-like structure, oblique tree-building
method could capture tree smaller accurate.2 Figure 2
gives illustration.
Breiman et al. first suggested method inducing oblique decision trees 1984. However, little research trees relatively recently (Utgoff
& Brodley, 1990; Heath, Kasif, & Salzberg, 1993b; Murthy, Kasif, Salzberg, & Beigel, 1993;
Brodley & Utgoff, 1994). comparison existing approaches given detail
Section 2. purpose study review strengths weaknesses existing
methods, design system combines strengths overcomes weaknesses, evaluate system empirically analytically. main contributions
conclusions study follows:

developed new, randomized algorithm inducing oblique decision trees

examples. algorithm extends original 1984 work Breiman et al.
Randomization helps significantly learning many concepts.

algorithm fully implemented oblique decision tree induction system
available Internet. code retrieved Online Appendix 1
paper (or anonymous ftp ftp://ftp.cs.jhu.edu/pub/oc1/oc1.tar.Z).

randomized hill-climbing algorithm used OC1 ecient

existing randomized oblique decision tree methods (described below). fact,
current implementation OC1 guarantees worst-case running time
O(log n) times greater worst-case time inducing axis-parallel trees (i.e.,
O(dn2 log n) vs. O(dn2)).

ability generate oblique trees often produces small trees compared
axis-parallel methods. underlying problem requires oblique split, oblique

2. Note though given oblique tree may fewer leaf nodes axis-parallel tree|which
mean \smaller"|the oblique tree may cases larger terms information content,
increased complexity tests node.

3

fiMurthy, Kasif & Salzberg

trees accurate axis-parallel trees. Allowing tree-building system
use oblique axis-parallel splits broadens range domains
system useful.
remaining sections paper follow outline: remainder section
brie outlines general paradigm decision tree induction, discusses complexity issues involved inducing oblique decision trees. Section 2 brie reviews
existing techniques oblique DT induction, outlines limitations approach,
introduces OC1 system. Section 3 describes OC1 system detail. Section 4
describes experiments (1) compare performance OC1 several
axis-parallel oblique decision tree induction methods range real-world datasets
(2) demonstrate empirically OC1 significantly benefits randomization
methods. Section 5, conclude discussion open problems directions
research.

1.1 Top-Down Induction Decision Trees

Algorithms inducing decision trees follow approach described Quinlan top-down
induction decision trees (1986). called greedy divide-and-conquer
method. basic outline follows:
1. Begin set examples called training set, . examples belong
one class, halt.
2. Consider tests divide two subsets. Score test according
well splits examples.
3. Choose (\greedily") test scores highest.
4. Divide examples subsets run procedure recursively subset.
Quinlan's original model considered attributes symbolic values; model,
test node splits attribute values. Thus test attribute
three values three child nodes, one corresponding value.
algorithm considers possible tests chooses one optimizes pre-defined
goodness measure. (One could split symbolic values two subsets values,
gives many choices split examples.) explain next, oblique
decision tree methods cannot consider tests due complexity considerations.

1.2 Complexity Induction Oblique Decision Trees

One reason relatively papers problem inducing oblique decision trees
increased computational complexity problem compared axis-parallel
case. two important issues must addressed. context top-down
decision tree algorithms, must address complexity finding optimal separating
hyperplanes (decision surfaces) given node decision tree. optimal hyperplane
minimize impurity measure used; e.g., impurity might measured total
number examples mis-classified. second issue lower bound complexity
finding optimal (e.g., smallest size) trees.
4

fiFigure 3: n points dimensions
(n d), n distinct axis-parallel splits,
,
2d nd distinct d-dimensional oblique splits. shows distinct
oblique axis-parallel splits two specific points 2-D.
Let us first consider issue complexity selecting optimal oblique hyperplane single node tree. domain
n training instances, described using
,
real-valued attributes, 2d nd distinct d-dimensional oblique splits; i.e.,
hyperplanes3 divide training instances uniquely two nonoverlapping subsets.
upper bound derives observation every subset size n points
define d-dimensional hyperplane, hyperplane rotated slightly
2d directions divide set points possible ways. Figure 3 illustrates
upper limits two points two dimensions. axis-parallel splits, n
distinct possibilities, axis-parallel methods C4.5 (Quinlan, 1993a) CART
(Breiman et al., 1984) exhaustively search best split node. problem
searching best oblique split therefore much dicult searching
best axis-parallel split. fact, problem NP-hard.
precisely, Heath (1992) proved following problem NP-hard: given
set labelled examples, find hyperplane minimizes number misclassified
examples hyperplane. result implies method
finding optimal oblique split likely exponential cost
(assuming P 6= NP ).
,
Intuitively, problem impractical enumerate 2d nd distinct hyperplanes
choose best, done axis-parallel decision trees. However, non-exhaustive
deterministic algorithm searching hyperplanes prone getting stuck
local minima.
3. Throughout paper, use terms \split" \hyperplane" interchangeably refer test
node decision tree. first usage standard (Moret, 1982), refers fact
test splits data two partitions. second usage refers geometric form test.

5

fiMurthy, Kasif & Salzberg

hand, possible define impurity measures problem
finding optimal hyperplanes solved polynomial time. example, one
minimizes sum distances mis-classified examples, optimal solution
found using linear programming methods (if distance measured along one dimension
only). However, classifiers usually judged many points classify correctly,
regardless close decision boundary point may lie. Thus standard
measures computing impurity base calculation discrete number examples
category either side hyperplane. Section 3.3 discusses several commonly
used impurity measures.
let us address second issue, complexity building small tree.
easy show problem inducing smallest axis-parallel decision tree
NP-hard. observation follows directly work Hyafil Rivest (1976). Note
one generate smallest axis-parallel tree consistent training
set polynomial time number attributes constant. done
using dynamic programming branch bound techniques (see Moret (1982) several
pointers). tree uses oblique splits, clear, even fixed number
attributes, generate optimal (e.g., smallest) decision tree polynomial time.
suggests complexity constructing good oblique trees greater
axis-parallel trees.
easy see problem constructing optimal (e.g., smallest) oblique
decision tree NP-hard. conclusion follows work Blum Rivest (1988).
result implies dimensions (i.e., attributes) problem producing
3-node oblique decision tree consistent training set NP-complete.
specifically, show following decision problem NP-complete: given training
set n examples Boolean attributes, exist 3-node neural network
consistent ? easy show following question NP-complete:
given training set , exist 3-leaf-node oblique decision tree consistent
T?
result complexity considerations, took pragmatic approach trying
generate small trees, looking smallest tree. greedy approach used
OC1 virtually decision tree algorithms implicitly tries generate small trees.
addition, easy construct example problems optimal split node
lead best tree; thus philosophy embodied OC1 find locally
good splits, spend excessive computational effort improving quality
splits.

2. Previous Work Oblique Decision Tree Induction
describing OC1 algorithm, brie discuss existing oblique DT
induction methods, including CART linear combinations, Linear Machine Decision
Trees, Simulated Annealing Decision Trees. methods induce
tree-like classifiers linear discriminants node, notably methods using
linear programming (Mangasarian, Setiono, & Wolberg, 1990; Bennett & Mangasarian,
1992, 1994a, 1994b). Though methods find optimal linear discriminants
specific goodness measures, size linear program grows fast number
6

fiInduction Oblique Decision Trees

induce split node decision tree:
Normalize values attributes.
L=0
(TRUE)
L = L+1

Let current split sL v c, v = Pdi=1 ai xi.
= 1; : : :;
= -0.25,0,0.25
Search maximizes goodness split v , (ai + ) c.
Let , settings result highest goodness 3 searches.
ai = ai , , c = c , .
Perturb c maximize goodness sL , keeping a1 ; : : :; ad constant.
jgoodness(sL) - goodness(sL,1)j exit loop.
Eliminate irrelevant attributes fa1; : : :; ad g using backward elimination.
Convert sL split un-normalized attributes.
Return better sL best axis-parallel split split .
Figure 4: procedure used CART linear combinations (CART-LC) node
decision tree.
instances number attributes. less closely related work
algorithms train artificial neural networks build decision tree-like classifiers (Brent,
1991; Cios & Liu, 1992; Herman & Yeung, 1992).
first oblique decision tree algorithm proposed CART linear combinations (Breiman et al., 1984, chapter 5). algorithm, referred henceforth CART-LC,
important basis OC1. Figure 4 summarizes (using Breiman et al.'s notation)
CART-LC algorithm node decision tree. core idea CARTLC algorithm finds value maximizes goodness split.
idea used OC1, explained detail Section 3.1.
describing CART-LC, Breiman et al. point still much room
development algorithm. OC1 represents extension CART-LC
includes significant additions. addresses following limitations CART-LC:

CART-LC fully deterministic. built-in mechanism escaping local

minima, although minima may common domains. Figure 5
shows simple example CART-LC gets stuck.

CART-LC produces single tree given data set.
CART-LC sometimes makes adjustments increase impurity split.
feature probably included allow escape local minima.

upper bound time spent node decision tree. halts
perturbation changes impurity , impurity may
increase decrease, algorithm spend arbitrarily long time node.
7

fiMurthy, Kasif & Salzberg

1

OC1

2

1

Initial Loc.

1

2

CART-LC

1

2

2

Figure 5: deterministic perturbation algorithm CART-LC fails find correct
split data, even starts location best axis-parallel
split. OC1 finds correct split using one random jump.
Another oblique decision tree algorithm, one uses different approach
CART-LC, Linear Machine Decision Trees (LMDT) system (Utgoff & Brodley, 1991;
Brodley & Utgoff, 1992), successor Perceptron Tree method (Utgoff, 1989;
Utgoff & Brodley, 1990). internal node LMDT tree Linear Machine (Nilsson,
1990). training algorithm presents examples repeatedly node linear
machine converges. convergence cannot guaranteed, LMDT uses heuristics
determine node stabilized. make training stable even set
training instances linearly separable, \thermal training" method (Frean, 1990)
used, similar simulated annealing.
third system creates oblique trees Simulated Annealing Decision Trees
(SADT) (Heath et al., 1993b) which, OC1, uses randomization. SADT uses simulated
annealing (Kirkpatrick, Gelatt, & Vecci, 1983) find good values coecients
hyperplane node tree. SADT first places hyperplane canonical
location, iteratively perturbs coecients small random amounts. Initially, temperature parameter high, SADT accepts almost perturbation
hyperplane, regardless changes goodness score. However, system
\cools down," changes improve goodness split likely accepted.
Though SADT's use randomization allows effectively avoid local minima,
compromises eciency. runs much slower either CART-LC, LMDT OC1,
sometimes considering tens thousands hyperplanes single node finishes
annealing.
experiments Section 4.3 include results showing methods
perform three artificial domains.
next describe way combine strengths methods mentioned,
avoiding problems. algorithm, OC1, uses deterministic hill climbing
time, ensuring computational eciency. addition, uses two kinds
randomization avoid local minima. limiting number random choices,
algorithm guaranteed spend polynomial time node tree. addition,
randomization produced several benefits: example, means algorithm
8

fiInduction Oblique Decision Trees

find split set examples :
Find best axis-parallel split . Let impurity split.
Repeat R times:
Choose random hyperplane H .
(For first iteration, initialize H best axis-parallel split.)
Step 1: impurity measure improve, do:
Perturb coecients H sequence.
Step 2: Repeat J times:
Choose random direction attempt perturb H direction.
reduces impurity H , go Step 1.
Let I1 = impurity H . I1 < , set = I1 .
Output split corresponding .
Figure 6: Overview OC1 algorithm single node decision tree.
produce many different trees data set. offers possibility new
family classifiers: k-decision-tree algorithms, example classified
majority vote k trees. Heath et al. (1993a) shown k-decision tree methods
(which call k-DT) consistently outperform single tree methods classification
accuracy main criterion. Finally, experiments indicate OC1 eciently finds
small, accurate decision trees many different types classification problems.

3. Oblique Classifier 1 (OC1)

section discuss details oblique decision tree induction system OC1.
part description, include:
method finding coecients hyperplane tree node,
methods computing impurity goodness hyperplane,
tree pruning strategy,
methods coping missing irrelevant attributes.
Section 3.1 focuses complicated algorithmic details; i.e. question
find hyperplane splits given set instances two reasonably \pure" nonoverlapping subsets. randomized perturbation algorithm main novel contribution
OC1. Figure 6 summarizes basic OC1 algorithm, used node decision
tree. figure explained following sections.

3.1 Perturbation algorithm

OC1 imposes restrictions orientation hyperplanes. However, order
least powerful standard DT methods, first finds best axis-parallel (univariate)
split node looking oblique split. OC1 uses oblique split
improves best axis-parallel split.4
4. pointed (Breiman et al., 1984, Chapter 5), make sense use oblique split
number examples node n less almost equal number features d,

9

fiMurthy, Kasif & Salzberg

search strategy space possible hyperplanes defined procedure
perturbs current hyperplane H new location. exponential
number distinct ways partition examples hyperplane, procedure
simply enumerates unreasonably costly. two main alternatives
considered past simulated annealing, used SADT system (Heath
et al., 1993b), deterministic heuristic search, CART-LC (Breiman et al., 1984).
OC1 combines two ideas, using heuristic search finds local minimum,
using non-deterministic search step get local minimum. (The nondeterministic step OC1 simulated annealing, however.)
start explaining perturb hyperplane split training set
node decision tree. Let n number examples , number
attributes (or dimensions) example, k number categories.
write Tj = (xj 1 ; xj 2; : : :; xjd; Cj ) j th example training set , xji
value attribute Cj category label. defined P
Eq. 1, equation
current hyperplane H node decision tree written di=1
(a x )+ad+1 = 0.
Pd
substitute point (an example) Tj equation H , get i=1 (aixji )+ ad+1 = Vj ,
sign Vj tells us whether point Tj hyperplane H ;
i.e., Vj > 0, Tj H . H splits training set perfectly, points
belonging category sign Vj . i.e., sign(Vi) = sign(Vj ) iff
category(Ti) = category(Tj ).
OC1 adjusts coecients H individually, finding locally optimal value one
coecient time. key idea introduced Breiman et al. works follows.
Treat coecient variable, treat coecients constants.
Vj viewed function . particular, condition Tj H
equivalent
Vj > 0
= Uj
> xxjm , Vj def
jm

(2)

assuming xjm > 0, ensure normalization. Using definition Uj ,
point Tj H > Uj , otherwise. plugging points
equation, obtain n constraints value .
problem find value satisfies many constraints
possible. (If constraints satisfied, perfect split.) problem
easy solve optimally: simply sort values Uj , consider setting
midpoint pair different values. illustrated Figure 7. figure,
categories indicated font size; larger Ui 's belong one category,
smaller another. distinct placement coecient , OC1 computes
impurity resulting split; e.g., location U6 U7 illustrated here, two
examples left one example right would misclassified (see Section 3.3.1
different ways computing impurity). figure illustrates, problem simply
find best one-dimensional split U s, requires considering n , 1 values
. value a0m obtained solving one-dimensional problem considered
data underfits concept. default, OC1 uses axis-parallel splits tree nodes n < 2d.
user vary threshold.

10

fiFigure 7: Finding optimal value single coecient . Large U's correspond
examples one category small u's another.

Perturb(H,m)
j = 1; : : :; n
Compute Uj (Eq. 2)
Sort U1; : : :; Un non-decreasing order.
a0m = best univariate split sorted Uj s.
H1 = result substituting a0m H .
(impurity(H1 ) < impurity(H))
f = a0m ; Pmove = Pstag g
Else (impurity(H) = impurity(H1 ))
f = a0m probability Pmove
Pmove = Pmove , 0:1 Pstag g
Figure 8: Perturbation algorithm single coecient .

replacement . Let H1 hyperplane obtained \perturbing" a0m .
H better (lower) impurity H1, H1 discarded. H1 lower impurity, H1
becomes new location hyperplane. H H1 identical impurities,
H1 replaces H probability Pstag .5 Figure 8 contains pseudocode perturbation
procedure.
method locally improving coecient hyperplane, need
decide + 1 coecients pick perturbation. experimented
three different methods choosing coecient adjust, namely, sequential, best
first random.
Seq: Repeat none coecient values modified loop:
= 1 d, Perturb(H; i)
Best: Repeat coecient remains unmodified:
= coecient perturbed, results
maximum improvement impurity measure.
Perturb(H; m)
R-50: Repeat fixed number times (50 experiments):
= random integer 1 + 1
Perturb(H; m)
5. parameter Pstag , denoting \stagnation probability", probability hyperplane perturbed
location change impurity measure. prevent impurity remaining
stagnant long time, Pstag decreases constant amount time OC1 makes \stagnant"
perturbation; thus constant number perturbations occur node. constant
set user. Pstag reset 1 every time global impurity measure improved.

11

fiMurthy, Kasif & Salzberg

previous experiments (Murthy et al., 1993) indicated order perturbation
coecients affect classification accuracy much parameters,
especially randomization parameters (see below). Since none orders uniformly better other, used sequential (Seq) perturbation experiments
reported Section 4.

3.2 Randomization

perturbation algorithm halts split reaches local minimum impurity
measure. OC1's search space, local minimum occurs perturbation
single coecient current hyperplane decrease impurity measure. (Of course,
local minimum may global minimum.) implemented two ways
attempting escape local minima: perturbing hyperplane random vector,
re-starting perturbation algorithm different random initial hyperplane.
technique perturbing hyperplane random vector works follows.
system reaches local minimum, chooses random vector add
coecients current hyperplane. computes optimal amount
hyperplane beP perturbed along random direction. precise,
hyperplane H = di=1 ai xi + ad+1 cannot improved deterministic perturbation,
OC1 repeats following loop J times (where J user-specified parameter, set 5
default).

Choose random vector R = (r1; r2; : : :; rd+1).
Let amount
want perturb H direction R.
Pd
words, let H1 = i=1 (ai + ffri )xi + (ad+1 + ffrd+1 ).
Find optimal value ff.
hyperplane H1 thus obtained decreases overall impurity, replace H H1,
exit loop begin deterministic perturbation algorithm individual
coecients.

Note treat variable equation H1 . Therefore
n examples , plugged equation H1, imposes constraint value
ff. OC1 therefore use coecient perturbation method (see Section 3.1) compute
best value ff. J random jumps fail improve impurity, OC1 halts uses
H split current tree node.
intuitive way understanding random jump look atPthe dual space
algorithm actually searching. Note equation H = di=1 ai xi + ad+1 defines
space axes coecients ai rather attributes xi . Every point
space defines distinct hyperplane original formulation. deterministic
algorithm used OC1 picks hyperplane adjusts coecients one time. Thus
dual space, OC1 chooses point perturbs moving parallel axes.
random vector R represents random direction space. finding best value
ff, OC1 finds best distance adjust hyperplane direction R.
12

fiInduction Oblique Decision Trees

Note additional perturbation random direction significantly increase time complexity algorithm (see Appendix A). found experiments
even single random jump, used local minimum, proves helpful.
Classification accuracy improved every one data sets perturbations
made. See Section 4.3 examples.
second technique avoiding local minima variation idea performing
multiple local searches. technique multiple local searches natural extension
local search, widely mentioned optimization literature (see Roth
(1970) early example). steps perturbation algorithm
deterministic, initial hyperplane largely determines local minimum
encountered first. Perturbing single initial hyperplane thus unlikely lead best
split given data set. cases random perturbation method fails escape
local minima, may helpful simply start afresh new initial hyperplane.
use word restart denote one run perturbation algorithms, one node
decision tree, using one random initial hyperplane.6 is, restart cycles
perturbs coecients one time tries perturb hyperplane
random direction algorithm reaches local minimum. last perturbation
reduces impurity, algorithm goes back perturbing coecients one time.
restart ends neither deterministic local search random jump find
better split. One optional parameters OC1 specifies many restarts use.
one restart used, best hyperplane found thus far always saved.
experiments, classification accuracies increased one restart.
Accuracy tended increase point level (after 20{50 restarts,
depending domain). Overall, use multiple initial hyperplanes substantially
improved quality decision trees found (see Section 4.3 examples).
carefully combining hill-climbing randomization, OC1 ensures worst case time
O(dn2 log n) inducing decision tree. See Appendix derivation upper
bound.

Best Axis-Parallel Split. clear axis-parallel splits suitable

data distributions oblique splits. take account distributions, OC1 computes best axis-parallel split oblique split node, picks better
two.7 Calculating best axis-parallel split takes additional O(dn log n) time,
increase asymptotic time complexity OC1. simple variant
OC1 system, user opt \switch off" oblique perturbations, thus building
axis-parallel tree training data. Section 4.2 empirically demonstrates
axis-parallel variant OC1 compares favorably existing axis-parallel algorithms.
6. first run algorithm node always begins location best axis-parallel
hyperplane; subsequent restarts begin random locations.
7. Sometimes simple axis-parallel split preferable oblique split, even oblique split slightly
lower impurity. user specify bias input parameter OC1.

13

fiMurthy, Kasif & Salzberg

3.3 Details
3.3.1 Impurity Measures

OC1 attempts divide d-dimensional attribute space homogeneous regions; i.e.,
regions contain examples one category. goal adding new nodes
tree split sample space minimize \impurity" training
set. algorithms measure \goodness" instead impurity, difference
goodness values maximized impurity minimized. Many different
measures impurity studied (Breiman et al., 1984; Quinlan, 1986; Mingers,
1989b; Buntine & Niblett, 1992; Fayyad & Irani, 1992; Heath et al., 1993b).
OC1 system designed work large class impurity measures. Stated
simply, impurity measure uses counts examples belonging every category
sides split, OC1 use it. (See Murthy Salzberg (1994) ways
mapping kinds impurity measures class impurity measures.) user
plug impurity measure fits description. OC1 implementation includes
six impurity measures, namely:
1.
2.
3.
4.
5.
6.

Information Gain
Gini Index
Twoing Rule
Max Minority
Sum Minority
Sum Variances

Though six measures defined elsewhere literature,
cases made slight modifications defined precisely Appendix B.
experiments indicated that, average, Information Gain, Gini Index Twoing Rule
perform better three measures axis-parallel oblique trees.
Twoing Rule current default impurity measure OC1, used
experiments reported Section 4. are, however, artificial data sets
Sum Minority and/or Max Minority perform much better rest measures.
instance, Sum Minority easily induces exact tree POL data set described
Section 4.3.1, methods diculty finding best tree.

Twoing Rule. Twoing Rule first proposed Breiman et al. (1984). value

computed defined as:

k
X

TwoingValue = (jTLj=n) (jTRj=n) (

i=1

jLi=jTLj , Ri=jTRjj)2

jTLj (jTRj) number examples left (right) split node , n
number examples node , Li (Ri ) number examples category
left (right) split. TwoingValue actually goodness measure rather
impurity measure. Therefore OC1 attempts minimize reciprocal value.
remaining five impurity measures implemented OC1 defined Appendix B.
14

fiInduction Oblique Decision Trees

3.3.2 Pruning

Virtually decision tree induction systems prune trees create order avoid
overfitting data. Many studies found judicious pruning results smaller
accurate classifiers, decision trees well types machine learning
systems (Quinlan, 1987; Niblett, 1986; Cestnik, Kononenko, & Bratko, 1987; Kodratoff
& Manago, 1987; Cohen, 1993; Hassibi & Stork, 1993; Wolpert, 1992; Schaffer, 1993).
OC1 system implemented existing pruning method, note tree
pruning method work fine within OC1. Based experimental evaluations
Mingers (1989a) work cited above, chose Breiman et al.'s Cost Complexity
(CC) pruning (1984) default pruning method OC1. method,
called Error Complexity Weakest Link pruning, requires separate pruning set.
pruning set randomly chosen subset training set, approximated
using cross validation. OC1 randomly chooses 10% (the default value) training data
use pruning. experiments reported below, used default value.
Brie y, idea behind CC pruning create set trees decreasing size
original, complete tree. trees used classify pruning set, accuracy
estimated that. CC pruning chooses smallest tree whose accuracy within k
standard errors squared best accuracy obtained. 0-SE rule (k = 0) used,
tree highest accuracy pruning set selected. k > 0, smaller tree size
preferred higher accuracy. details Cost Complexity pruning, see Breiman et
al. (1984) Mingers (1989a).
3.3.3 Irrelevant attributes

Irrelevant attributes pose significant problem machine learning methods (Breiman
et al., 1984; Aha, 1990; Almuallin & Dietterich, 1991; Kira & Rendell, 1992; Salzberg, 1992;
Cardie, 1993; Schlimmer, 1993; Langley & Sage, 1993; Brodley & Utgoff, 1994). Decision
tree algorithms, even axis-parallel ones, confused many irrelevant attributes.
oblique decision trees learn coecients attribute DT node, one
might hope values chosen coecient would ect relative importance
corresponding attributes. Clearly, though, process searching good coecient
values much ecient fewer attributes; search space much
smaller. reason, oblique DT induction methods benefit substantially using
feature selection method (an algorithm selects subset original attribute set)
conjunction coecient learning algorithm (Breiman et al., 1984; Brodley & Utgoff,
1994).
Currently, OC1 built-in mechanism select relevant attributes. However, easy include several standard methods (e.g., stepwise forward selection
stepwise backward selection) even ad hoc method select features running
tree-building process. example, separate experiments data Hubble
Space Telescope (Salzberg, Chandar, Ford, Murthy, & White, 1994), used feature selection methods preprocessing step OC1, reduced number attributes 20
2. resulting decision trees simpler accurate. Work currently
underway incorporate ecient feature selection technique OC1 system.
15

fiMurthy, Kasif & Salzberg

Regarding missing values, example missing value attribute, OC1 uses
mean value attribute. One course use techniques handling
missing values, considered study.

4. Experiments

section, present two sets experiments support following two claims.
1. OC1 compares favorably variety real-world domains several existing
axis-parallel oblique decision tree induction methods.
2. Randomization, form multiple local searches random jumps, improves quality decision trees produced OC1.
experimental method used experiments described Section 4.1. Sections 4.2 4.3 describe experiments corresponding two claims. experimental section begins description data sets, presents experimental
results discussion.

4.1 Experimental Method

used five-fold cross validation (CV) experiments estimate classification
accuracy. k-fold CV experiment consists following steps.
1. Randomly divide data k equal-sized disjoint partitions.
2. partition, build decision tree using data outside partition, test
tree data partition.
3. Sum number correct classifications k trees divide total number
instances compute classification accuracy. Report accuracy
average size k trees.
entry Tables 1 2 result ten 5-fold CV experiments; i.e., result tests
used 50 decision trees. ten 5-fold cross validations used different random
partitioning data. entry tables reports mean standard deviation
classification accuracy, followed mean standard deviation decision
tree size (measured number leaf nodes). Good results high values
accuracy, low values tree size, small standard deviations.
addition OC1, included experiments axis-parallel version OC1,
considers axis-parallel hyperplanes. call version, described Section 3.2,
OC1-AP. experiments, OC1 OC1-AP used Twoing Rule (Section
3.3.1) measure impurity. parameters OC1 took default values unless stated
otherwise. (Defaults include following: number restarts node: 20. Number
random jumps attempted local minimum: 5. Order coecient perturbation:
Sequential. Pruning method: Cost Complexity 0-SE rule, using 10% training
set exclusively pruning.)
comparison, used oblique version CART algorithm, CART-LC.
implemented version CART-LC, following description Breiman et
al. (1984, Chapter 5); however, may differences version
16

fiInduction Oblique Decision Trees

versions system (note CART-LC freely available). implementation
CART-LC measured impurity Twoing Rule used 0-SE Cost Complexity
pruning separate test set, OC1 does. include feature selection
methods CART-LC OC1, implement normalization.
CART coecient perturbation algorithm may alternate indefinitely two locations
hyperplane (see Section 2), imposed arbitrary limit 100 perturbations
forcing perturbation algorithm halt.
included axis-parallel CART C4.5 comparisons. used implementations algorithms IND 2.1 package (Buntine, 1992). default
cart0 c4.5 \styles" defined package used, without altering parameter
settings. cart0 style uses Twoing Rule 0-SE cost complexity pruning
10-fold cross validation. pruning method, impurity measure defaults
c4.5 style described Quinlan (1993a).

4.2 OC1 vs. Decision Tree Induction Methods

Table 1 compares performance OC1 three well-known decision tree induction
methods plus OC1-AP six different real-world data sets. next section
consider artificial data, concept definition precisely characterized.
4.2.1 Description Data Sets

Star/Galaxy Discrimination. Two data sets came large set astronom-

ical images collected Odewahn et al. (Odewahn, Stockwell, Pennington, Humphreys, &
Zumach, 1992). study, used images train artificial neural networks
running perceptron back propagation algorithms. goal classify example either \star" \galaxy." image characterized 14 real-valued attributes,
attributes measurements defined astronomers likely relevant
task. objects image divided Odewahn et al. \bright" \dim"
data sets based image intensity values, dim images inherently
dicult classify. (Note \bright" objects bright relation others
data set. actuality extremely faint, visible powerful
telescopes.) bright set contains 2462 objects dim set contains 4192 objects.
addition results reported Table 1, following results appeared
Star/Galaxy data. Odewahn et al. (1992) reported accuracy 99.8% accuracy
bright objects, 92.0% dim ones, although noted study
used single training test set partition. Heath (1992) reported 99.0% accuracy
bright objects using SADT, average tree size 7.03 leaves. study used
single training test set. Salzberg (1992) reported accuracies 98.8% bright
objects, 95.1% dim objects, using 1-Nearest Neighbor (1-NN) coupled
feature selection method reduces number features.
Breast Cancer Diagnosis. Mangasarian Bennett compiled data problem diagnosing breast cancer test several new classification methods (Mangasarian
et al., 1990; Bennett & Mangasarian, 1992, 1994a). data represents set patients
breast cancer, patient characterized nine numeric attributes plus
diagnosis tumor benign malignant. data set currently 683 entries
17

fiMurthy, Kasif & Salzberg

Bright S/G
98.90.2
4.31.0
CART-LC
98.80.2
3.91.3
OC1-AP
98.10.2
6.92.4
CART-AP
98.50.5
13.95.7
C4.5
98.50.5
14.32.2
Algorithm
OC1

Dim S/G
95.00.3
13.08.7
92.80.5
24.28.7
94.00.2
29.38.8
94.20.7
30.410
93.30.8
77.97.4

Cancer
96.20.3
2.80.9
95.30.6
3.50.9
94.50.5
6.41.7
95.01.6
11.57.2
95.32.0
9.82.2

Iris
94.73.1
3.10.2
93.52.9
3.20.3
92.72.4
3.20.3
93.83.7
4.31.6
95.13.2
4.60.8

Housing
82.40.8
6.93.2
81.41.2
5.83.2
81.81.0
8.64.5
82.13.5
15.110
83.23.1
28.23.3

Diabetes
74.41.0
5.43.8
73.71.2
8.05.2
73.81.0
11.47.5
73.93.4
11.59.1
71.43.3
56.37.9

Table 1: Comparison OC1 decision tree induction methods six different
data sets. first line method gives accuracies, second line gives
average tree sizes. highest accuracy domain appears boldface.
available UC Irvine machine learning repository (Murphy & Aha, 1994).
Heath et al. (1993b) reported 94.9% accuracy subset data set (it
470 instances), average decision tree size 4.6 nodes, using SADT. Salzberg
(1991) reported 96.0% accuracy using 1-NN (smaller) data set. Herman
Yeung (1992) reported 99.0% accuracy using piece-wise linear classification, using
somewhat smaller data set.

Classifying Irises. Fisher's famous iris data, extensively studied

statistics machine learning literature. data consists 150 examples,
example described four numeric attributes. 50 examples
three different types iris ower. Weiss Kapouleas (1989) obtained accuracies 96.7%
96.0% data back propagation 1-NN, respectively.

Housing Costs Boston. data set, available part UCI ML repos-

itory, describes housing values suburbs Boston function 12 continuous
attributes 1 binary attribute (Harrison & Rubinfeld, 1978). category variable (median value owner-occupied homes) actually continuous, discretized
category = 1 value < $21000, 2 otherwise. uses data, see (Belsley,
1980; Quinlan, 1993b).

Diabetes diagnosis. data catalogs presence absence diabetes among Pima

Indian females, 21 years older, function eight numeric-valued attributes.
original source data National Institute Diabetes Digestive Kidney
Diseases, available UCI repository. Smith et al. (1988) reported 76%
accuracy data using ADAP learning algorithm, using different experimental
method used here.
18

fiInduction Oblique Decision Trees

4.2.2 Discussion

table shows that, six data sets considered here, OC1 consistently finds better
trees original oblique CART method. accuracy greater six domains,
although difference significant (more 2 standard deviations) dim
star/galaxy problem. average tree sizes roughly equal five six domains,
dim stars galaxies, OC1 found considerably smaller trees. differences
analyzed quantified using artificial data, following section.
five decision tree induction methods, OC1 highest accuracy four
six domains: bright stars, dim stars, cancer diagnosis, diabetes diagnosis.
remaining two domains, OC1 second highest accuracy case. surprisingly,
oblique methods (OC1 CART-LC) generally find much smaller trees axisparallel methods. difference quite striking domains|note, example,
OC1 produced tree 13 nodes average dim star/galaxy problem,
C4.5 produced tree 78 nodes, 6 times larger. course, domains
axis-parallel tree appropriate representation, axis-parallel methods compare
well oblique methods terms tree size. fact, Iris data, methods
found similar-sized trees.

4.3 Randomization Helps OC1

second set experiments, examine closely effect introducing randomized steps algorithm finding oblique splits. experiments demonstrate
OC1's ability produce accurate tree set training data clearly enhanced
two kinds randomization uses. precisely, use three artificial data sets
(for underlying concept known experimenters) show OC1's performance improves substantially deterministic hill climbing augmented
three ways:

multiple restarts random initial locations,
perturbations random directions local minima,
randomization steps.
order find clear differences algorithms, one needs know concept
underlying data indeed dicult learn. simple concepts (say, two linearly
separable classes 2-D), many different learning algorithms produce accurate
classifiers, therefore advantages randomization may detectable.
known many commonly-used data sets UCI repository easy
learn simple representations (Holte, 1993); therefore data sets may
ideal purposes. Thus created number artificial data sets present different
problems learning, know \correct" concept definition. allows
us quantify precisely parameters algorithm affect performance.
second purpose experiment compare OC1's search strategy
two existing oblique decision tree induction systems { LMDT (Brodley & Utgoff, 1992)
SADT (Heath et al., 1993b). show quality trees induced OC1
good as, better than, trees induced existing systems three
19

fiMurthy, Kasif & Salzberg

artificial domains. show OC1 achieves good balance amount
effort expended search quality tree induced.
LMDT SADT used information gain experiment. However,
change OC1's default measure (the Twoing Rule) observed, experiments
reported here, OC1 information gain produce significantly different
results. maximum number successive, unproductive perturbations allowed
node set 10000 SADT. parameters, used default settings provided
systems.
4.3.1 Description Artificial Data

LS10 LS10 data set 2000 instances divided two categories. instance

described ten attributes x1 ,: : : ,x10, whose values uniformly distributed range
[0,1]. data linearly separable 10-D hyperplane (thus name LS10) defined
equation x1 + x2 + x3 + x4 + x5 < x6 + x7 + x8 + x9 + x10. instances
generated randomly labelled according side hyperplane fell on.
oblique DT induction methods intuitively prefer linear separator one
exists, interesting compare various search techniques data set
know separator exists. task relatively simple lower dimensions, chose
10-dimensional data make dicult.

POL data set shown Figure 9. 2000 instances two dimensions,

divided two categories. underlying concept set four parallel oblique lines
(thus name POL), dividing instances five homogeneous regions. concept
dicult learn single linear separator, minimal-size tree still quite
small.

RCB RCB stands \rotated checker board"; data set subject

experiments hard classification problems decision trees (Murthy & Salzberg,
1994). data set, shown Figure 9, 2000 instances 2-D, belonging one
eight categories. concept dicult learn axis-parallel method, obvious
reasons. quite dicult oblique methods, several reasons. biggest
problem \correct" root node, shown figure, separate
class itself. impurity measures (such Sum Minority) fail miserably
problem, although others (e.g., Twoing Rule) work much better. Another problem
deterministic coecient perturbation algorithm get stuck local minima
many places data set.
Table 2 summarizes results experiment three smaller tables, one
data set. smaller table, compare four variants OC1 LMDT SADT.
different results OC1 obtained varying number restarts
number random jumps. random jumps used, twenty random jumps
tried local minimum. soon one found improved impurity
current hyperplane, algorithm moved hyperplane started running
deterministic perturbation procedure again. none 20 random jumps improved
impurity, search halted restarts (if any) tried. training
test partitions used methods cross-validation run (recall results
20

fiInduction Oblique Decision Trees

lr1

ot

rr1

-1

l-1

1

r-1

rl-

1
rr-

r-1

t-1
oo
R

3
4
4
4
77
4
33
333
33
4
33
33
7
3 4 4
4 44 44 4
1 33
3
3 3 3
4 4 7
4 44
3
4
4
44
33 3 3
3 33
4
4
7
4
33 3 3
4 44
111 1 3 33 3
7 7 77 7
4
3
3 3 3 33 3 3
4
1
4 4 444
1
44
7
4 4
3
3
4
4 4
3 333 3
4 44444
44 4 4 44
47
1 1
777
3
33 3 4 4
3
4
7
4 4
444 4
7
4444 4
3
7
334 4 4
44
44
33
1
77 77 777
3
4 4
34 44
1 1
7 777 7
44
113
4 7 777 7
4
7
7
4
1
1
4
1
4 4 44 4 4 44 4 4
1
3
7
1
7
1 111
4
4
1
7
7 7 7 777 7
44
4 4 4447
44 4 4
4
77
7
111 1 1 1 4 4 44 4 4
7 7 77 7
44 4
1
4
1 1
4 44 4
7
444
7 7
44 4 4
1 111 1
4
1
4
1
7 77
11
1 1
44 4
4
4
1
7
7
7
7 77
1 1 11 14
1 1 111
4 4 44 477 7
7
4 44
7
1
77
4 4
77 7 7 7 7 7
11
4
8
4
11
7
1 1 1
4
8
2
77 77
7
4 4 7
1
2 4 44 4
7
11 1
7
8
7 77
7
1
1
1
2
1
2
7
7
8
11
11
22 2444 4 4
7 777 77 7 77 7 7 77 7 7 8
4
1 2 22 2 2
7
1
88 8
ll-2
2
7
77 7
4
1 1 22
8
7 7
22 2 22 44 4 4 77
7
7 77888 8
11
1
22 22
8
888
2 22 22
5
7
1
8
22 222
1
2 2
88
7 77 7
2
2
2
11 2 2
8
22
2
5
7777
8
8
222 22 2
8
222
2 2
2
77 8 8 8
2
2
55
77 8
2 2 22 2
22
8 88 8 8 8
5
55
8
2 22 2
2
2 2
8 88
7 8
2 222
8
5 5 77
888 8
2 2
8
2
5
5
5
5
2
2
8
2
55 55
8
88 8
22 222 2
7
2 22
8
2
25
8 88 8
5
2
22
88
55
5
8 8 8 8 8 88
5 5
2 22 2 22 22 2 22 22
8
8 88 8
5
5
2 2
55
22
8 8 88 8
5
2 22 2 5 5 5
8
2 2
5 56
8
8 8
5
5
8
2
2
55
5
5 5 55 5 6 6
8 88 8888
22
5
2
8
5
5
8 88
6 66 8 8
2 2 2 2 2 2 2 2 5 5 55 5 5
6
8
6
8
8
2 2 2 2
8
55
8
5
6
8
8 8
5
6
55
2 2222 2 5
5
5
5
5
5
6
6
6
5 5 5
5 55
6
55 5 5
222 22 2 22
8 8
6
5
55 5 5
6
6
6
66
8
5
55
8
22
6 66 6
8 8
5 5 5
2 22
55
6
6
6 6
2 2
2
5 55 5 5 5
5
6 66 8 8
6
2
5
66 6
5
6 66
55 5
5
5 6
5
55 5
6 6 66
55
5
22 2
55 5
5 5
66 6 6
6
55
6666
8
5 5
6
5
6
6
6
66
55
6 6 66
55
55
8 8
6 66 6
5 5555
6 66 6
5
6
66
2
5 5555
5
66
6
5
5
8
66 6 6
5 5 5 55 555 5
6 66
55 5
6 68
5
6 6
6
6 6

Ro

-1
rrr

2
2
1
2
11
1
22
222
11
2
22
22
1
2 2 2
1 11 11 1
1 11
2
2 2 2
1 1 1
2 22
2
2
1
11
22 2 2
2 22
2
2
1
2
22 2 2
2 22
111 1 1 11 2
1 1 11 1
1
2
1 2 2 22 2 2
1
1
2 2 222
1
22
1
1 1
2
2
2
2 2
2 222 2
2 22222
22 2 2 11
11
1 1
111
2
22 2 2 2
1
2
1
2 1
222 2
1
2222 2
1
1
222 2 2
22
22
22
1
11 11 111
1
2 2
22 22
1 1
1 111 1
22
111
2 2 222 1
2
1
2
2
1
1
2
1
2 2 22 2 2 22 2 2
1
1
1
1
1
1 111
2
2
1
2
2 1 1 111 1
22
2 2 2222
22 2 2
1
22
2
111 1 1 1 1 1 11 2 2
1 1 11 1
22 2
1
2
1 1
2 22 2
2
222
1 1
22 2 2
1 111 1
2
1
2
1
2 22
11
1 1
22 2
2
1
1
2
1
2
2 22
1 1 11 11
1 1 111
2 2 22 222 2
2
2 22
2
1
22
2 2
22 2 2 2 2 2
11
1
2
2
11
2
1 1 1
2
2
1
22 22
2
2 2 2
1
1 1 11 1
2
21 1
2
2
2 22
2
1
1
1
1
2
1
2
2
2
11
111
11 1111 1 1
2 222 22 2 22 2 2 22 2 2 2
1
11 1
2
1
22 2
1 1
1
2
22 2
1
2 1 11
2
2 2
11 1 11 11 1 1 12
2
2 22222 2
22
2
11 11
2
222
1 11 11
1
2
2
2
22 111
2
1 1
22
2 22 2
1
1
1
22 2 2
2
11
1
1
2222
2
2
111 11 1
2
111
2 2
2
22 2 2 2
2
1
11
22 2
2 2 22 2
11
2 22 2 2 2
1
11
2
1 11 1
2
1 1
2 22
2 2
2 222
2
1 1 11
222 2
2 2
2
1
1
1
1
1
1
1
2
1
11 11
2
22 2
22 222 2
1
2 22
2
1
11
2 22 2
1
2
22
22
11
1
1 2 2 2 2 22
1 1
2 22 2 22 22 2 22 21
2
2 22 2
1
1
2 2
11
22
2 2 22 2
1
2 22 2 1 1 1
2
2 2
1 11
2
2 2
1
1
1
2
2
22
1
1 1 11 1 1 1
1 22 2222
22
2
2
2
1
2
2 22
1 11 1 1
1 2 2 2 2 2 2 2 2 2 22 1 1
1
2
1
1
1
1 1 2 2
2
11
2
1
1
1
2 2
1
1
11
2 2222 2 2
1
1
1
1
2
1
1
1
2 2 2
1 11
1
22 2 2
111 11 1 22
2 2
1
1
22 2 2
1
1
1
11
1
1
22
2
11
1 11 1
1 2
2 2 2
1
1 11
11
1
1
1 1
1 1
1
2 22 2 2 2
2
1 11 1 2
1
1
2
11 1
2
1 11
11 1
2
1 1
2
22 2
1 1 11
22
1
11 1
22 2
1 2
11 1 1
1
22
1111
1
2 2
2
2
1
1
2
11
11
1 1 11
11
22
1 1
1 11 1
1 1111
1 11 1
2
1
11
1
2 2222
1
22
1
1
2
1
22 1 1
1 1 1 11 111 2
1 11
11 1
1 11
1
2 2
1
1 1

Figure 9: POL RCB data sets
Linearly Separable 10-D (LS10) data
R:J Accuracy
Size
Hyperplanes
0:0 89.81.2 67.05.8
2756
0:20 91.51.5 55.27.0
3824
20:0 95.00.6 25.62.4
24913
20:20 97.20.7 13.93.2
30366
LMDT 99.70.2 2.20.5
9089
SADT 95.21.8 15.55.7
349067
Parallel Oblique Lines (POL) data
R:J Accuracy
Size
Hyperplanes
0:0 98.30.3 21.61.9
164
0:20 99.30.2 9.01.0
360
20:0 99.10.2 14.21.1
3230
20:20 99.60.1 5.50.3
4852
LMDT 89.610.2 41.919.2
1732
SADT 99.30.4 8.42.1
85594
Rotated Checker Board (RCB) data
R:J Accuracy
Size
Hyperplanes
0:0 98.40.2 35.51.4
573
0:20 99.30.3 19.70.8
1778
20:0 99.60.2 12.01.4
6436
20:20 99.80.1 8.70.4
11634
LMDT 95.72.3 70.19.6
2451
SADT 97.91.1 32.54.9
359112
Table 2: effect randomization OC1. first column, labelled R:J, shows
number restarts (R) followed maximum number random jumps (J)
attempted OC1 local minimum. Results LMDT SADT
included comparison four variants OC1. Size average tree size
measured number leaf nodes. third column shows average
number hyperplanes algorithm considered building one tree.
21

fiMurthy, Kasif & Salzberg

average ten 5-fold CVs). trees pruned algorithms,
data noise-free furthermore emphasis search.
Table 2 includes number hyperplanes considered algorithm
building complete tree. Note OC1 SADT, number hyperplanes considered generally much larger number perturbations actually made,
algorithms compare newly generated hyperplanes existing hyperplanes
adjusting existing one. Nevertheless, number good estimate much effort
algorithm expends, every new hyperplane must evaluated according
impurity measure. LMDT, number hyperplanes considered identical
actual number perturbations.
4.3.2 Discussion

OC1 results quite clear. first line table, labelled 0:0, gives
accuracies tree sizes randomization used | variant similar
CART-LC algorithm. increase use randomization, accuracy increases
tree size decreases, exactly result hoped decided
introduce randomization method.
Looking closely tables, ask effect random jumps alone.
illustrated second line (0:20) table, attempted 20 random
jumps local minimum restarts. Accuracy increased 1-2% domain,
tree size decreased dramatically, roughly factor two, POL RCB
domains. Note noise domains, high accuracies
expected. Thus increases percent accuracy possible.
Looking third line sub-table Table 2, see effect multiple restarts
OC1. 20 restarts random jumps escape local minima, improvement
even noticeable LS10 data random jumps alone used.
data set, accuracy jumped significantly, 89.8 95.0%, tree size dropped
67 26 nodes. POL RCB data, improvements comparable
obtained random jumps. RCB data, tree size dropped factor 3
(from 36 leaf nodes 12 leaf nodes) accuracy increased 98.4 99.6%.
fourth line table shows effect randomized steps. Among
OC1 entries, line highest accuracies smallest trees three
data sets, clear randomization big win kinds problems.
addition, note smallest tree RCB data eight leaf nodes,
OC1's average trees, without pruning, 8.7 leaf nodes. clear data
set, thought dicult one, OC1 came close finding optimal
tree nearly every run. (Recall numbers table average 10 5-fold
CV experiments; i.e., average 50 decision trees.) LS10 data show dicult
find simple concept higher dimensions|the optimal tree
single hyperplane (two nodes), OC1 unable find current parameter
settings.8 POL data required minimum 5 leaf nodes, OC1 found minimalsize tree time, seen table. Although shown Table,
8. separate experiment, found OC1 consistently finds linear separator LS10 data
10 restarts 200 random jumps used.

22

fiInduction Oblique Decision Trees

OC1 using Sum Minority performed better POL data Twoing Rule
impurity measure; i.e., found correct tree using less time.
results LMDT SADT data lead interesting insights.
surprisingly, LMDT well linearly separable (LS10) data,
require inordinate amount search. Clearly, data linearly separable, one
use method LMDT linear programming. OC1 SADT diculty finding
linear separator, although experiments OC1 eventually find it, given sucient
time.
hand, non-linearly separable data sets, LMDT produces
much larger trees significantly less accurate produced OC1
SADT. Even deterministic variant OC1 (using zero restarts zero random jumps)
outperforms LMDT problems, much less search.
Although SADT sometimes produces accurate trees, main weakness
enormous amount search time required, roughly 10-20 times greater OC1 even
using 20:20 setting. One explanation OC1's advantage use directed search,
opposed strictly random search used simulated annealing. Overall, Table 2 shows
OC1's use randomization quite effective non-linearly separable data.
natural ask randomization helps OC1 task inducing decision trees.
Researchers combinatorial optimization observed randomized search usually
succeeds search space holds abundance good solutions (Gupta, Smolka,
& Bhaskar, 1994). Furthermore, randomization improve upon deterministic search
many local maxima search space lead poor solutions. OC1's search
space, local maximum hyperplane cannot improved deterministic
search procedure, \solution" complete decision tree. significant fraction
local maxima lead bad trees, algorithms stop first local maximum
encounter perform poorly. randomization allows OC1 consider many
different local maxima, modest percentage maxima lead good trees,
good chance finding one trees. experiments OC1 thus far indicate
space oblique hyperplanes usually contains numerous local maxima,
substantial percentage locally good hyperplanes lead good decision trees.

5. Conclusions Future Work
paper described OC1, new system constructing oblique decision trees.
shown experimentally OC1 produce good classifiers range real-world
artificial domains. shown use randomization improves upon
original algorithm proposed Breiman et al. (1984), without significantly increasing
computational cost algorithm.
use randomization might beneficial axis-parallel tree methods. Note
although find optimal test (with respect impurity measure)
node tree, complete tree may optimal: well known, problem
finding smallest tree NP-Complete (Hyafil & Rivest, 1976). Thus even axis-parallel
decision tree methods produce \ideal" decision trees. Quinlan suggested
windowing algorithm might used way introducing randomization C4.5, even
though algorithm designed another purpose (Quinlan, 1993a). (The windowing
23

fiMurthy, Kasif & Salzberg

algorithm selects random subset training data builds tree using that.)
believe randomization powerful tool context decision trees,
experiments one example might exploited. process
conducting experiments quantify accurately effects different forms
randomization.
clear ability produce oblique splits node broadens capabilities decision tree algorithms, especially regards domains numeric attributes.
course, axis-parallel splits simpler, sense description split
uses one attribute node. OC1 uses oblique splits impurity less
impurity best axis-parallel split; however, one could easily penalize
additional complexity oblique split further. remains open area
research. general point domain best captured tree uses
oblique hyperplanes, desirable system generate tree.
shown problems, including used experiments, OC1 builds small
decision trees capture domain well.

Appendix A. Complexity Analysis OC1

following, show OC1 runs eciently even worst case. data
set n examples (points) attributes per example, OC1 uses O(dn2 log n)
time. assume n > analysis.
analysis here, assume coecients hyperplane adjusted sequential order (the Seq method described paper). number restarts node
r, number random jumps tried j . r j constants, fixed
advance running algorithm.
Initializing hyperplane random position takes O(d) time. need
consider first maximum amount work OC1 finds new location
hyperplane. need consider many times move hyperplane.
1. Attempting perturb first coecient (a1 ) takes O(dn + n log n) time. Computing
Ui 's points (equation 2) requires O(dn) time, sorting Ui 's takes
O(n log n). gives us O(dn + n log n) work.
2. perturbing a1 improve things, try perturb a2 . Computing new
Ui 's take O(n) time one term different Ui . Re-sorting
take O(n log n), step takes O(n) + O(n log n) = O(n log n) time.
3. Likewise a3; : : :; ad take O(n log n) additional time, assuming still
found better hyperplane checking coecient. Thus total time cycle
attempt perturb additional coecients (d , 1) O(n log n) =
O(dn log n).
4. Summing up, time cycle coecients O(dn log n)+O(dn+n log n) =
O(dn log n).
5. none coecients improved split, attempt make j random
jumps. Since j constant, consider j = 1 analysis. step
24

fiInduction Oblique Decision Trees

involves choosing random vector running perturbation algorithm solve
ff, explained Section 3.2. before, need compute set Ui 's sort
them, takes O(dn + n log n) time. amount time dominated
time adjust coecients, total time far still O(dn log n).
time OC1 spend node either halting finding improved
hyperplane.
6. Assuming OC1 using Sum Minority Max Minority error measure,
reduce impurity hyperplane n times. clear improvement
means one example correctly classified new hyperplane. Thus
total amount work node limited n O(dn log n) = O(dn2 log n). (This
analysis extends, linear cost factors, Information Gain, Gini Index
Twoing Rule two categories. apply measure that,
example, uses distances mis-classified objects hyperplane.) practice,
found number improvements per node much smaller n.
Assuming OC1 adjusts hyperplane improves impurity measure,
O(dn2 log n) work worst case.
However, OC1 allows certain number adjustments hyperplane
improve impurity, although never accept change worsens impurity.
number allowed determined constant known \stagnant-perturbations". Let
value s. works follows.
time OC1 finds new hyperplane improves old one, resets counter
zero. move new hyperplane different location equal impurity
times. moves repeats perturbation algorithm. Whenever
impurity reduced, re-starts counter allows moves equally good
locations. Thus clear feature increases worst-case complexity OC1
constant factor, s.
Finally, note overall cost OC1 O(dn2 log n), i.e., upper
bound total running time OC1 independent size tree ends
creating. (This upper bound applies Sum Minority Max Minority; open question
whether similar upper bound proven Information Gain Gini Index.)
Thus worst-case asymptotic complexity system comparable systems
construct axis-parallel decision trees, O(dn2 ) worst-case complexity.
sketch intuition leads bound, let G total impurity summed
leaves partially constructed tree (i.e., sum currently misclassified points
tree). observe time run perturbation algorithm node
tree, either halt improve G least one unit. worst-case analysis one node
realized perturbation algorithm run every one n examples,
happens, would longer mis-classified examples tree
would complete.

Appendix B. Definitions impurity measures available OC1

addition Twoing Rule defined text, OC1 contains built-in definitions five
additional impurity measures, defined follows. following definitions,
25

fiMurthy, Kasif & Salzberg

set examples node split contains n (> 0) instances belong
one k categories. (Initially set entire training set.) hyperplane H divides
two non-overlapping subsets TL TR (i.e., left right). Lj Rj number
instances category j TL TR respectively. impurity measures initially
check see TL TR homogeneous (i.e., examples belong category),
return minimum (zero) impurity.

Information Gain. measure information gained particular split pop-

ularized context decision trees Quinlan (1986). Quinlan's definition makes
information gain goodness measure; i.e., something maximize. OC1 attempts
minimize whatever impurity measure uses, use reciprocal standard value
information gain OC1 implementation.

Gini Index. Gini Criterion (or Index) proposed decision trees Breiman et
al. (1984). Gini Index originally defined measures probability misclassification
set instances, rather impurity split. implement following
variation:
GiniL = 1:0 ,
GiniR = 1:0 ,

k
X
i=1
k
X
i=1

(Li =jTLj)2
(Ri=jTRj)2

Impurity = (jTLj GiniL + jTRj GiniR)=n
GiniL Gini Index \left" side hyperplane GiniR
right.

Max Minority. measures Max Minority, Sum Minority Sum Variances

defined context decision trees Heath, Kasif, Salzberg (1993b).9 Max
Minority theoretical advantage tree built minimizing measure
depth log n. experiments indicated great advantage
practice: seldom impurity measures produce trees substantially deeper
produced Max Minority. definition is:
MinorityL =
MinorityR =

k
X
i=1;i6=max Li
k
X
i=1;i6=max Ri

Li
Ri

Max Minority = max(MinorityL; MinorityR)
9. Sum Variances called Sum Impurities Heath et al.

26

fiInduction Oblique Decision Trees

Sum Minority. measure similar Max Minority. MinorityL MinorityR defined Max Minority measure, Sum Minority sum
two values. measure simplest way quantifying impurity, simply
counts number misclassified instances.
Though Sum Minority performs well domains, obvious aws.
one example, consider domain n = 100; = 1, k = 2 (i.e., 100 examples, 1
numeric attribute, 2 classes). Suppose examples sorted according
single attribute, first 50 instances belong category 1, followed 24 instances category 2, followed 26 instances category 1. possible splits distribution
sum minority 24. Therefore impossible using Sum Minority distinguish split preferable, although splitting alternations categories
clearly better.
Sum Variances. definition measure is:
jX
TL j
jX
TL j
VarianceL = (Cat(TLi ) , Cat(TLj )=jTLj)2
VarianceR =

i=1

j =1

jX
TRj

jX
TRj

i=1

(Cat(TRi ) ,

j =1

Cat(TRj )=jTRj)2

Sum Variances = VarianceL + VarianceR
Cat(Ti) category instance Ti . measure computed using actual
class labels, easy see impurity computed varies depending numbers
assigned classes. instance, T1 consists 10 points category 1 3
points category 2, T2 consists 10 points category 1 3 points category
5, Sum Variances values different T1 T2. avoid problem,
OC1 uniformly reassigns category numbers according frequency occurrence
category node computing Sum Variances.

Acknowledgements
authors thank Richard Beigel Yale University suggesting idea jumping
random direction. Thanks Wray Buntine Nasa Ames Research Center providing
IND 2.1 package, Carla Brodley providing LMDT code, David Heath
providing SADT code assisting us using it. Thanks three anonymous
reviewers many helpful suggestions. material based upon work supported
National Science foundation Grant Nos. IRI-9116843, IRI-9223591, IRI-9220960.

References
Aha, D. (1990). Study Instance-Based Algorithms Supervised Learning: Mathematical, empirical psychological evaluations. Ph.D. thesis, Department Information
Computer Science, University California, Irvine.
27

fiMurthy, Kasif & Salzberg

Almuallin, H., & Dietterich, T. (1991). Learning many irrelevant features. Proceedings Ninth National Conference Artificial Intelligence, pp. 547{552. San
Jose, CA.
Belsley, D. (1980). Regression Diagnostics: Identifying uential Data Sources
Collinearity. Wiley & Sons, New York.
Bennett, K., & Mangasarian, O. (1992). Robust linear programming discrimination two
linearly inseparable sets. Optimization Methods Software, 1, 23{34.
Bennett, K., & Mangasarian, O. (1994a). Multicategory discrimination via linear programming. Optimization Methods Software, 3, 29{39.
Bennett, K., & Mangasarian, O. (1994b). Serial parallel multicategory discrimination.
SIAM Journal Optimization, 4 (4).
Blum, A., & Rivest, R. (1988). Training 3-node neural network NP-complete. Proceedings 1988 Workshop Computational Learning Theory, pp. 9{18. Boston,
MA. Morgan Kaufmann.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification Regression
Trees. Wadsworth International Group.
Brent, R. P. (1991). Fast training algorithms multilayer neural nets. IEEE Transactions
Neural Networks, 2 (3), 346{354.
Brodley, C. E., & Utgoff, P. E. (1992). Multivariate versus univariate decision trees. Tech.
rep. COINS CR 92-8, Dept. Computer Science, University Massachusetts
Amherst.
Brodley, C. E., & Utgoff, P. E. (1994). Multivariate decision trees. Machine Learning,
appear.
Buntine, W. (1992). Tree classification software. Technology 2002: Third National
Technology Transfer Conference Exposition.
Buntine, W., & Niblett, T. (1992). comparison splitting rules decision-tree
induction. Machine Learning, 8, 75{85.
Cardie, C. (1993). Using decision trees improve case-based learning. Proceedings
Tenth International Conference Machine Learning, pp. 25{32. University
Massachusetts, Amherst.
Cestnik, G., Kononenko, I., & Bratko, I. (1987). Assistant 86: knowledge acquisition
tool sophisticated users. Bratko, I., & Lavrac, N. (Eds.), Progress Machine
Learning. Sigma Press.
Cios, K. J., & Liu, N. (1992). machine learning method generation neural network
architecture: continuous ID3 algorithm. IEEE Transactions Neural Networks,
3 (2), 280{291.
28

fiInduction Oblique Decision Trees

Cohen, W. (1993). Ecient pruning methods separate-and-conquer rule learning systems. Proceedings 13th International Joint Conference Artificial Intelligence, pp. 988{994. Morgan Kaufmann.
Fayyad, U. M., & Irani, K. B. (1992). attribute specification problem decision tree
generation. Proceedings Tenth National Conference Artificial Intelligence,
pp. 104{110. San Jose CA. AAAI Press.
Frean, M. (1990). Small Nets Short Paths: Optimising neural computation. Ph.D.
thesis, Centre Cognitive Science, University Edinburgh.
Gupta, R., Smolka, S., & Bhaskar, S. (1994). randomization sequential distributed
algorithms. ACM Computing Surveys, 26 (1), 7{86.
Hampson, S., & Volper, D. (1986). Linear function neurons: Structure training. Biological Cybernetics, 53, 203{217.
Harrison, D., & Rubinfeld, D. (1978). Hedonic prices demand clean air. Journal
Environmental Economics Management, 5, 81{102.
Hassibi, B., & Stork, D. (1993). Second order derivatives network pruning: optimal
brain surgeon. Advances Neural Information Processing Systems 5, pp. 164{171.
Morgan Kaufmann, San Mateo, CA.
Heath, D. (1992). Geometric Framework Machine Learning. Ph.D. thesis, Johns
Hopkins University, Baltimore, Maryland.
Heath, D., Kasif, S., & Salzberg, S. (1993a). k-DT: multi-tree learning method.
Proceedings Second International Workshop Multistrategy Learning, pp. 138{
149. Harpers Ferry, WV. George Mason University.
Heath, D., Kasif, S., & Salzberg, S. (1993b). Learning oblique decision trees. Proceedings
13th International Joint Conference Artificial Intelligence, pp. 1002{1007.
Chambery, France. Morgan Kaufmann.
Herman, G. T., & Yeung, K. D. (1992). piecewise-linear classification. IEEE Transactions Pattern Analysis Machine Intelligence, 14 (7), 782{786.
Holte, R. (1993). simple classification rules perform well commonly used
datasets. Machine Learning, 11 (1), 63{90.
Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees NPcomplete. Information Processing Letters, 5 (1), 15{17.
Kira, K., & Rendell, L. (1992). practical approach feature selection. Proceedings
Ninth International Conference Machine Learning, pp. 249{256. Aberdeen,
Scotland. Morgan Kaufmann.
Kirkpatrick, S., Gelatt, C., & Vecci, M. (1983). Optimization simulated annealing.
Science, 220 (4598), 671{680.
29

fiMurthy, Kasif & Salzberg

Kodratoff, Y., & Manago, M. (1987). Generalization noise. International Journal
Man-Machine Studies, 27, 181{204.
Langley, P., & Sage, S. (1993). Scaling domains many irrelevant features. Learning
Systems Department, Siemens Corporate Research, Princeton, NJ.
Mangasarian, O., Setiono, R., & Wolberg, W. (1990). Pattern recognition via linear programming: Theory application medical diagnosis. SIAM Workshop
Optimization.
Mingers, J. (1989a). empirical comparison pruning methods decision tree induction. Machine Learning, 4 (2), 227{243.
Mingers, J. (1989b). empirical comparison selection measures decision tree induction. Machine Learning, 3, 319{342.
Moret, B. M. (1982). Decision trees diagrams. Computing Surveys, 14 (4), 593{623.
Murphy, P., & Aha, D. (1994). UCI repository machine learning databases { machinereadable data repository. Maintained Department Information Computer
Science, University California, Irvine. Anonymous FTP ics.uci.edu
directory pub/machine-learning-databases.
Murthy, S. K., Kasif, S., Salzberg, S., & Beigel, R. (1993). OC1: Randomized induction
oblique decision trees. Proceedings Eleventh National Conference Artificial
Intelligence, pp. 322{327. Washington, D.C. MIT Press.
Murthy, S. K., & Salzberg, S. (1994). Using structure improve decision trees. Tech. rep.
JHU-94/12, Department Computer Science, Johns Hopkins University.
Niblett, T. (1986). Constructing decision trees noisy domains. Bratko, I., & Lavrac,
N. (Eds.), Progress Machine Learning. Sigma Press, England.
Nilsson, N. (1990). Learning Machines. Morgan Kaufmann, San Mateo, CA.
Odewahn, S., Stockwell, E., Pennington, R., Humphreys, R., & Zumach, W. (1992). Automated star-galaxy descrimination neural networks. Astronomical Journal,
103 (1), 318{331.
Pagallo, G. (1990). Adaptive Decision Tree Algorithms Learning Examples. Ph.D.
thesis, University California Santa Cruz.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. Machine
Learning, 5 (1), 71{99.
Quinlan, J. R. (1983). Learning ecient classification procedures application
chess end games. Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), Machine
Learning: Artificial Intelligence Approach. Morgan Kaufmann, San Mateo, CA.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.
30

fiInduction Oblique Decision Trees

Quinlan, J. R. (1987). Simplifying decision trees. International Journal Man-Machine
Studies, 27, 221{234.
Quinlan, J. R. (1993a). C4.5: Programs Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.
Quinlan, J. R. (1993b). Combining instance-based model-based learning. Proceedings
Tenth International Conference Machine Learning, pp. 236{243 University
Massachusetts, Amherst. Morgan Kaufmann.
Roth, R. H. (1970). approach solving linear discrete optimization problems. Journal
ACM, 17 (2), 303{313.
Safavin, S. R., & Landgrebe, D. (1991). survey decision tree classifier methodology.
IEEE Transactions Systems, Man Cybernetics, 21 (3), 660{674.
Sahami, M. (1993). Learning non-linearly separable boolean functions linear threshold unit trees madaline-style networks. Proceedings Eleventh National
Conference Artificial Intelligence, pp. 335{341. AAAI Press.
Salzberg, S. (1991). nearest hyperrectangle learning method. Machine Learning, 6,
251{276.
Salzberg, S. (1992). Combining learning search create good classifiers. Tech. rep.
JHU-92/12, Johns Hopkins University, Baltimore MD.
Salzberg, S., Chandar, R., Ford, H., Murthy, S. K., & White, R. (1994). Decision trees
automated identification cosmic rays Hubble Space Telescope images. Publications Astronomical Society Pacific, appear.
Schaffer, C. (1993). Overfitting avoidance bias. Machine Learning, 10, 153{178.
Schlimmer, J. (1993). Eciently inducing determinations: complete systematic
search algorithm uses optimal pruning. Proceedings Tenth International
Conference Machine Learning, pp. 284{290. Morgan Kaufmann.
Smith, J., Everhart, J., Dickson, W., Knowler, W., & Johannes, R. (1988). Using
ADAP learning algorithm forecast onset diabetes mellitus. Proceedings
Symposium Computer Applications Medical Care, pp. 261{265. IEEE
Computer Society Press.
Utgoff, P. E. (1989). Perceptron trees: case study hybrid concept representations.
Connection Science, 1 (4), 377{391.
Utgoff, P. E., & Brodley, C. E. (1990). incremental method finding multivariate
splits decision trees. Proceedings Seventh International Conference
Machine Learning, pp. 58{65. Los Altos, CA. Morgan Kaufmann.
Utgoff, P. E., & Brodley, C. E. (1991). Linear machine decision trees. Tech. rep. 10,
University Massachusetts Amherst.
31

fiMurthy, Kasif & Salzberg

Van de Merckt, T. (1992). NFDT: system learns exible concepts based decision
trees numerical attributes. Proceedings Ninth International Workshop
Machine Learning, pp. 322{331.
Van de Merckt, T. (1993). Decision trees numerical attribute spaces. Proceedings
13th International Joint Conference Artificial Intelligence, pp. 1016{1021.
Weiss, S., & Kapouleas, I. (1989). empirical comparison pattern recognition, neural
nets, machine learning classification methods. Proceedings 11th International Joint Conference Artificial Intelligence, pp. 781{787. Detroit, MI. Morgan
Kaufmann.
Wolpert, D. (1992). overfitting avoidance bias. Tech. rep. SFI TR 92-03-5001,
Santa Fe Institute, Santa Fe, New Mexico.

32


