Journal Artificial Intelligence Research 2 (1995) 501-539

Submitted 9/94; published 5/95

Pac-Learning Recursive Logic Programs:
Ecient Algorithms
William W. Cohen

AT&T Bell Laboratories
600 Mountain Avenue, Murray Hill, NJ 07974 USA

wcohen@research.att.com

Abstract

present algorithms learn certain classes function-free recursive logic programs polynomial time equivalence queries. particular, show single
k-ary recursive constant-depth determinate clause learnable. Two-clause programs consisting one learnable recursive clause one constant-depth determinate non-recursive
clause learnable, additional \basecase" oracle assumed. results immediately imply pac-learnability classes. Although classes learnable
recursive programs constrained, shown companion paper
maximally general, generalizing either class natural way leads computationally dicult learning problem. Thus, taken together companion paper,
paper establishes boundary ecient learnability recursive logic programs.

1. Introduction
One active area research machine learning learning concepts expressed firstorder logic. Since researchers used variant Prolog represent learned
concepts, subarea sometimes called inductive logic programming (ILP) (Muggleton,
1992; Muggleton & De Raedt, 1994).
Within ILP, researchers considered two broad classes learning problems.
first class problems, call logic based relational learning problems,
first-order variants sorts classification problems typically considered within
AI machine learning community: prototypical examples include Muggleton et al.'s (1992)
formulation ff-helix prediction, King et al.'s (1992) formulation predicting drug activity, Zelle Mooney's (1994) use ILP techniques learn control heuristics
deterministic parsers. Logic-based relational learning often involves noisy examples ect relatively complex underlying relationship; natural extension propositional
machine learning, already enjoyed number experimental successes.
second class problems studied ILP researchers, target concept Prolog
program implements common list-processing arithmetic function; prototypical
problems class might learning append two lists, multiply two numbers.
learning problems similar character studied area automatic
programming examples (Summers, 1977; Biermann, 1978), hence might appropriately called automatic logic programming problems. Automatic logic programming
problems characterized noise-free training data recursive target concepts. Thus
problem central enterprise automatic logic programming|but not, perhaps,
logic-based relational learning|is problem learning recursive logic programs.
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCohen

goal paper formally analyze learnability recursive logic programs
Valiant's (1984) model pac-learnability, thus hopefully shedding light
task automatic logic programming. summarize results, show
simple recursive programs pac-learnable examples alone, examples plus
small number additional \hints". largest learnable class identify standard
learning model class one-clause constant-depth determinate programs
constant number \closed" recursive literals. largest learnable class identify
requires extra \hints" class constant-depth determinate programs consisting
single nonrecursive base clause single recursive clause class described
above. results proved model identification equivalence queries
(Angluin, 1988, 1989), somewhat stronger pac-learnability. Identification
equivalence queries requires target concept exactly identified, polynomial
time, using polynomial number equivalence queries . equivalence query
asks hypothesis program H equivalent target program C ; answer
query either \yes" adversarily chosen example H C differ.
model learnability arguably appropriate automatic logic programming tasks
weaker model pac-learnability, unclear often approximately
correct recursive program useful.
Interestingly, learning algorithms analyzed different existing ILP
learning methods; employ unusual method generalizing examples called forced
simulation . Forced simulation simple analytically tractable alternative
methods generalizing recursive programs examples, n-th root finding
(Muggleton, 1994), sub-unification (Aha, Lapointe, Ling, & Matwin, 1994) recursive
anti-unification (Idestam-Almquist, 1993), rarely used experimental
ILP systems (Ling, 1991).
paper organized follows. presenting preliminary definitions,
begin presenting (primarily pedagogical reasons) procedure identifying
equivalence queries single non-recursive constant-depth determinate clause. Then,
Section 4, extend learning algorithm, corresponding proof correctness,
simple class recursive clauses: class \closed" linear recursive constant-depth
determinate clauses. Section 5, relax assumptions made make analysis
easier, present several extensions algorithm: extend algorithm linear
recursion k-ary recursion, show k-ary recursive clause non-recursive
clause learned simultaneously given additional \basecase" oracle. discuss
related work conclude.
Although learnable class programs large enough include well-known
automatic logic programming benchmarks, extremely restricted. companion paper
(Cohen, 1995), provide number negative results, showing relaxing
restrictions leads dicult learning problems: particular, learning problems
either hard learning DNF (an open problem computational learning theory),
hard cracking certain presumably secure cryptographic schemes. Thus, taken together
results companion paper, results delineate boundary learnability
recursive logic programs.
Although two papers independent, suggest readers wishing read
paper companion paper read paper first.
502

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2. Background

section present technical background necessary state results.
assume, however, reader familiar basic elements logic programming; readers without background referred one standard texts, example
(Lloyd, 1987).

2.1 Logic Programs

treatment logic programs standard, except usually consider body
clause ordered set literals.
paper, consider logic programs without function symbols|
i.e., programs written Datalog.1 purpose logic program answer
certain questions relative database , DB , set ground atomic facts. (When
convenient, think DB conjunction ground unit clauses.) simplest
use Datalog program check status simple instance . simple instance
(for program P database DB ) fact f . pair (P; DB ) said cover f iff
DB ^ P ` f . set simple instances covered (P; DB ) precisely minimal model
logic program P ^ DB .
paper, primarily consider extended instances consist two parts:
instance fact f , simply ground fact, description D, finite set
ground unit clauses. extended instance e = (f; D) covered (P; DB ) iff
DB ^ ^ P ` f

extended instances allowed, function-free programs expressive enough
encode surprisingly interesting programs. particular, many programs usually
written function symbols re-written function-free programs, example
illustrates.

Example. Consider usual program appending two lists.
append([],Ys,Ys).
append([XjXs1],Ys,[XjZs1])

append(Xs1,Ys,Zs1).

One could use program classify atomic facts containing function symbols
append([1,2],[3],[1,2,3]). program rewritten Datalog
program classifies extended instances follows:

Program P :

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
1. assumption made primarily convenience. Section 5.2 describe assumption
relaxed.

503

fiCohen

append(Xs1,Ys,Zs1).

Database DB :
null(nil).

predicate components(A,B,C) means list head B tail
C; thus extended instance equivalent append([1,2],[3],[1,2,3]) would

Instance fact f :

append(list12,list3,list123).

Description D:

components(list12,1,list2).
components(list2,2,nil).
components(list123,1,list23).
components(list23,2,list3).
components(list3,3,nil).
note using extended instances examples closely related using ground
clauses entailed target clause examples: specifically, instance e = (f; D)
covered P; DB iff P ^ DB ` (f D). example shows, close
relationship extended instances literals function symbols
removed \ attening" (Rouveirol, 1994; De Raedt & Dzeroski, 1994). elected
use Datalog programs model extended instances paper several
reasons. Datalog relatively easy analyze. close connection Datalog
restrictions imposed certain practical learning systems, FOIL (Quinlan,
1990; Quinlan & Cameron-Jones, 1993), FOCL (Pazzani & Kibler, 1992), GOLEM
(Muggleton & Feng, 1992).
Finally, using extended instances addresses following technical problem. learning problems considered paper involve restricted classes logic programs. Often,
restrictions imply number simple instances polynomial; note
polynomial-size domain, questions pac-learnability usually trivial. Requiring
learning algorithms work domain extended instances precludes trivial learning
techniques, however, number extended instances size n exponential n even
highly restricted programs.

2.2 Restrictions Logic Programs

paper, consider learnability various restricted classes logic programs. define restrictions; however, first introduce
terminology.
B1 ^ : : : ^ Br (ordered) definite clause, input variables literal
Bi variables appearing Bi appear clause B1 ^ : : : ^ Bi,1 ;
variables appearing Bi called output variables . Also, B1 ^ : : : ^ Br
definite clause, Bi said recursive literal predicate symbol
arity A, head clause.
504

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2.2.1 Types Recursion

first set restrictions concern type recursion allowed program.
every clause program one recursive literal, program linear
recursive . every clause program k recursive literals, program
k-ary recursive . Finally, every recursive literal program contains output variables,
say program closed recursive.
2.2.2 Determinacy Depth

second set restrictions variants restrictions originally introduced Muggleton
Feng (1992). B1 ^ : : : ^ Br (ordered) definite clause, literal Bi
determinate iff every possible substitution unifies fact e
DB ` B1 ^ : : : ^ Bi,1

one maximal substitution DB ` Bi . clause determinate
literals determinate. Informally, determinate clauses
evaluated without backtracking Prolog interpreter.
define depth variable appearing clause B1 ^ : : : ^ Br follows.
Variables appearing head clause depth zero. Otherwise, let Bi first
literal containing variable V , let maximal depth input variables
Bi ; depth V +1. depth clause maximal depth variable
clause.
Muggleton Feng define logic program ij -determinate determinate,
constant depth i, contains literals arity j less. paper use phrase
\constant-depth determinate" instead denote class programs.
examples constant-depth determinate programs, taken Dzeroski, Muggleton
Russell (1992).

Example. Assuming successor functional, following program determinate. maximum depth variable one, variable C second
clause, hence program depth one.
less than(A,B)
less than(A,B)

successor(A,B).
successor(A,C) ^ less than(C,B).
!

following program, computes C , determinate depth
two.
choose(A,B,C)
zero(B) ^
one(C).
choose(A,B,C)
decrement(B,D) ^
decrement(A,E) ^
505

fiCohen

multiply(B,C,G) ^
divide(G,A,F) ^
choose(E,D,F).
program GOLEM (Muggleton & Feng, 1992) learns constant-depth determinate
programs, related restrictions adopted several practical learning
systems (Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). learnability
constant-depth determinate clauses received formal study,
review Section 6.
2.2.3 Mode Constraints Declarations

define mode literal L appearing clause C string initial
character predicate symbol L, j > 1 j -th character \+"
(j , 1)-th argument L input variable \," (j , 1)-th argument L
output variable. (This definition coincides usual definition Prolog modes
arguments head clause inputs. simplification justified,
however, considering clauses behave classifying extended instances,
ground.) mode constraint simply set mode strings R = fs1 ; : : :; sk g,
clause C said satisfy mode constraint R p every literal L body
C , mode L R.

Example. following append program, every literal annotated
mode.

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
append(Xs1,Ys,Zs1).

% mode: null+
% mode: components + ,,
% mode: components + +,
% mode: append + ++

clauses program satisfy following mode constraint:
f components + ,,; components + +,; components + ,+;
components , ++; components + ++; null +
append + +,;
append + ,+;
append , ++;
append + ++
g
Mode constraints commonly used analyzing Prolog code; instance,
used many Prolog compilers. sometimes use alternative syntax mode
constraints parallels syntax used Prolog systems: instance, may
write mode constraint \components + ,," \components (+; ,; ,)".
define declaration tuple (p; a0; R) p predicate symbol, a0
integer, R mode constraint. say clause C satisfies declaration
head C arity a0 predicate symbol p, every literal L body
C mode L appears R.
506

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2.3 Model Learnability

section, present model learnability. first review necessary
definitions standard learning model, model learning equivalence queries
(Angluin, 1988, 1989), discuss relationship learning models.
introduce extension model necessary analyzing ILP problems.
2.3.1 Identification Equivalence Queries

Let X set. call X domain , call elements X instances . Define
concept C X representation subset X , define language Lang
set concepts. paper, rather casual distinction
concept set represents; risk confusion refer set
represented concept C extension C . Two concepts C1 C2
extension said (semantically) equivalent .
Associated X Lang two size complexity measures , use
following notation:

size complexity concept C 2 Lang written j C j .
size complexity instance e 2 X written j ej .
set, Sn stands set elements size complexity greater
n. instance, Xn = fe 2 X : j ej ng Langn = fC 2 Lang : j C j ng.
assume size measures polynomially related number bits needed
represent C e.
first learning model consider model identification equivalence
queries . goal learner identify unknown target concept C 2 Lang|
is, construct hypothesis H 2 Lang H C . Information
target concept gathered equivalence queries . input equivalence
query C hypothesis H 2 Lang. H C , response query
\yes". Otherwise, response query arbitrarily chosen counterexample |an
instance e symmetric difference C H .
deterministic algorithm Identify identifies Lang equivalence queries iff
every C 2 Lang, whenever Identify run (with oracle answering equivalence queries
C ) eventually halts outputs H 2 Lang H C . Identify
polynomially identifies Lang equivalence queries iff polynomial poly (nt; ne )
point execution Identify total running time bounded
poly (nt ; ne ), nt = j C j ne size largest counterexample seen far,
0 equivalence queries made.
2.3.2 Relation Pac-Learnability

model identification equivalence queries well-studied (Angluin, 1988,
1989). known language learnable model, learnable
Valiant's (1984) model pac-learnability. (The basic idea behind result
equivalence query hypothesis H emulated drawing set random
507

fiCohen

examples certain size. counterexample H , one returns
found counterexample answer equivalence query. counterexamples
found, one assume high confidence H approximately equivalent
target concept.) Thus identification equivalence queries strictly stronger model
pac-learnability.
existing positive results pac-learnability logic programs rely showing
every concept target language emulated boolean concept
pac-learnable class (Dzeroski et al., 1992; Cohen, 1994). results
illuminating, disappointing, since one motivations considering firstorder representations first place allow one express concepts cannot
easily expressed boolean logic. One advantage studying exact identification
model considering recursive programs essentially precludes use sort
proof technique: many recursive programs approximated boolean functions
fixed set attributes, exactly emulated boolean functions.
2.3.3 Background Knowledge Learning

framework described standard, one possible formalization usual
situation inductive concept learning, user provides set examples (in
case counterexamples queries) learning system attempts find useful
hypothesis. However, typical ILP system, setting slightly different, usually
user provides clues target concept addition examples. ILP
systems user provides database DB \background knowledge" addition set
examples; paper, assume user provides declaration.
account additional inputs necessary extend framework described
setting learner accepts inputs training examples.
formalize this, introduce following notion \language family". Lang
set clauses, DB database Dec declaration, define Lang[DB ; Dec]
set pairs (C; DB ) C 2 Lang C satisfies Dec . Semantically,
pair denote set extended instances (f; D) covered (C; DB ). Next,
DB set databases DEC set declarations, define
Lang[DB ; DEC ] = fLang[DB ; Dec ] : DB

2 DB Dec 2 DECg

set languages called language family .
extend definition identification equivalence queries language families follows. language family Lang[DB; DEC ] identifiable equivalence
queries iff every language set identifiable equivalence queries. language
family Lang[DB; DEC ] uniformly identifiable equivalence queries iff single
algorithm Identify (DB ; Dec) identifies language Lang[DB ; Dec ] family
given DB Dec .
Uniform polynomial identifiability language family defined analogously:
Lang[DB; DEC ] uniformly polynomially identifiable equivalence queries iff
polynomial time algorithm Identify (DB ; Dec ) identifies language Lang[DB ; Dec]
family given DB Dec . Note Identify must run time polynomial
size inputs Dec DB well target concept.
508

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

2.3.4 Restricted Types Background Knowledge

describe number restricted classes databases declarations.
One restriction make throughout paper assume
predicates interest bounded arity. use notation a-DB set
databases contain facts arity less, notation a-DEC set
declarations (p; a0; R) every string 2 R length + 1 less.
technical reasons, often convenient assume database contains
equality predicate |that is, predicate symbol equal equal (ti ; ti) 2 DB every
constant ti appearing DB , equal (ti ; tj ) 62 DB ti 6= tj . Similarly,
often wish assume declaration allows literals form equal(X,Y), X
input variables. DB (respectively DEC ) set databases (declarations)
use DB = (DEC = ) denote corresponding set, additional restriction
database (declaration) must contain equality predicate (respectively mode
equal (+; +)).
sometimes convenient assume declaration (p; a0; R) allows
single valid mode predicate: i.e., predicate q R
single mode constraint form qff. declaration called unique-mode
declaration. DEC set declarations use DEC 1 denote corresponding
set declarations additional restriction declaration unique-mode.
Finally, note typical setting, facts appear database DB
descriptions extended instances arbitrary: instead, representative
\real" predicate (e.g., relationship list components example above).
One way formalizing assume facts drawn restricted set F ;
using assumption one define notion determinate mode . f = p(t1 ; : : :; tk )
fact predicate symbol p pff mode, define inputs (f; pff)
tuple hti1 ; : : :; tik i, i1, : : : , ik indices containing \+". define
outputs (f; pff) tuple htj1 ; : : :; tjl i, j1 , : : : , jl indices containing
\,". mode string pff predicate p determinate F iff relation

fhinputs (f; pff); outputs (f; pff)i : f 2 Fg
function. Informally, mode determinate input positions facts F
functionally determine output positions.
set declarations containing modes determinate F denoted
DetDEC F . However, paper, set F assumed fixed, thus
generally omit subscript.
program consistent determinate declaration Dec 2 DetDEC must determinate, defined above; words, consistency determinate declaration
sucient condition semantic determinacy. condition verified
simple syntactic test.
2.3.5 Size Measures Logic Programs

Assuming predicates arity less constant allows simple
size measures used. paper, measure size database DB
cardinality; size extended instance (f; D) cardinality D; size
509

fiCohen

declaration (p; a0; R) cardinality R; size clause B1 ^ : : : ^ Br
number literals body.

3. Learning Nonrecursive Clause

learning algorithms presented paper use generalization technique
call forced simulation. way introduction technique, consider
learning algorithm non-recursive constant-depth clauses. result presented
primarily pedagogical reasons, may interest own: independent
previous proofs pac-learnability class (Dzeroski et al., 1992),
somewhat rigorous previous proofs.
Although details analysis algorithm non-recursive clauses somewhat involved, basic idea behind algorithm quite simple. First, highlyspecific \bottom clause" constructed, using two operations call DEEPEN
CONSTRAIN . Second, bottom clause generalized deleting literals covers positive examples: algorithm generalizing clause cover example
(roughly) simulate clause example, delete literals would cause
clause fail. remainder section describe analyze learning
algorithm detail.

3.1 Constructing \Bottom Clause"

Let Dec = (p; a0; R) declaration let B1 ^ : : : ^ Br definite clause.
define
^
DEEPEN Dec (A B1 ^ : : : ^ Br ) B1 ^ : : : ^ Br ^ (
Li )
Li 2LD

LD maximal set literals Li satisfy following conditions:
clause B1 ^ : : : ^ Br ^ Li satisfies mode constraints given R;
Li 2 LD mode predicate symbol Lj 2 LD ,
input variables Li different input variables Lj ;
every Li least one output variable, output variables Li
different other, difference output variables
Lj 2 LD .
extension notation, define DEEPEN iDec (C ) result applying
function DEEPEN Dec repeatedly times C , i.e.,
(
= 0

DEEPEN Dec (C ) C
i,
1
DEEPEN Dec (DEEPEN Dec (C )) otherwise
define function CONSTRAIN Dec
^
CONSTRAIN Dec (A B1 ^ : : : ^ Br ) B1 ^ : : : ^ Br ^ (
Li )
Li 2LC

LC set literals Li B1 ^ : : : ^ Br ^ Li satisfies mode
constraints given R, Li contains output variables.
510

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Example. Let D0 declaration (p; 2; R) R contains mode
constraints mother (+; ,), father (+; ,), male (+), female (+), equal (+; +).



DEEPEN D0(p(X,Y) )
p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)
DEEPEN 2D0(p(X,Y) ) DEEPEN D0 (DEEPEN D0 (p(X,Y) ))
p(X,Y)
mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
mother(XM,XMM)^father(XM,XMF)^ mother(XF,XFM)^father(XF,XFF)^
mother(YM,YMM)^father(YM,YMF)^ mother(YF,YFM)^father(YF,YFF)
CONSTRAIN D0(DEEPEN D0(p(X,Y) ))
p(X,Y)
mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
male(X)^female(X)^male(Y)^female(Y)^
male(XM)^female(XM)^male(XF)^female(XF)^
male(YM)^female(YM)^male(YF)^female(YF)^
equal(X,X)^equal(X,XM)^equal(X,XF)^
equal(X,Y)^equal(X,YM)^equal(X,YF)^
equal(XM,X)^equal(XM,XM)^equal(XM,XF)^
equal(XM,Y)^equal(XM,YM)^equal(XM,YF)^
equal(XF,X)^equal(XF,XM)^equal(XF,XF)^
equal(XF,Y)^equal(XF,YM)^equal(XF,YF)^
equal(Y,X)^equal(Y,XM)^equal(Y,XF)^
equal(Y,Y)^equal(Y,YM)^equal(Y,YF)^
equal(YM,X)^equal(YM,XM)^equal(YM,XF)^
equal(YM,Y)^equal(YM,YM)^equal(YM,YF)^
equal(YF,X)^equal(YF,XM)^equal(YF,XF)^
equal(YF,Y)^equal(YF,YM)^equal(YF,YF)

Let us say clause C1 subclause clause C2 heads C1 C2
identical, every literal body C1 appears C2 , literals
body C1 appear order C2. functions DEEPEN
CONSTRAIN allow one easily describe clause interesting property.
Theorem 1 Let Dec = (p; a0; R) declaration a-DetDEC =, let X1; : : :; Xa distinct
variables, define clause BOTTOM follows:
BOTTOM (Dec ) CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))
constants a, following true:
size BOTTOM d(Dec) polynomial j Decj ;
every depth-d clause satisfies Dec (and hence, determinate) (semantically)
equivalent subclause BOTTOM (Dec ).
0

0

511

fiCohen

begin algorithm Force1NR (d ; Dec; DB ):

% BOTTOM specific possible clause
let H BOTTOM d(Dec)

repeat

Ans answer query \Is H correct?"
Ans =\yes" return H
elseif Ans negative example
return \no consistent hypothesis"
elseif Ans positive example e+
% generalize H minimally cover e+
let (f; D) components extended instance e+
H ForceSimNR (H ; f ; Dec; (DB [ ))
H = FAILURE
return \no consistent hypothesis"

end

endif
endif
endrepeat

Figure 1: learning algorithm nonrecursive depth-d determinate clauses

Proof: See Appendix A. related result appears Muggleton Feng (1992).
Example. C1 D1 equivalent, C2 D2. Notice D1
D2 subclauses BOTTOM 1 (D0).

C1 : p(A,B) mother(A,C)^father(A,D)^ mother(B,C)^father(B,D)^male(A)
D1 : p(X,Y) mother(X,XM)^father(X,XF)^ mother(Y,YM)^father(Y,YF)^
male(X)^equal(XM,YM)^equal(XF,YF)
C2 : p(A,B) father(A,B)^female(A)
D2 : p(X,Y) father(X,XF)^female(X)^equal(XF,Y)
C1 D1, p(X,Y) true X 's brother. C2 D2, p(X,Y)
true X 's daughter, X 's father.

3.2 Learning Algorithm

Theorem 1 suggests may possible learn non-recursive constant-depth determinate clauses searching space subclauses BOTTOM ecient
manner. Figures 1 2 present algorithm called Force1 NR Dec
unique-mode declaration.
Figure 1 presents top-level learning algorithm, Force1 NR . Force1 NR takes
input database DB declaration Dec , begins hypothesizing clause
BOTTOM (Dec ). positive counterexample e+ , current hypothesis generalized little possible order cover e+ . strategy means hypothesis
512

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSimNR(H ; f ; Dec; DB ):

% \forcibly simulate" H fact f
f 2 DB return H
elseif head H f cannot unified
return FAILURE

else

let H 0 H
let mgu f head H 0
literal L body H 0
substitution 0 L0 2 DB
0, 0 general substitution
else
delete L body H 0 , together
literals L0 supported (directly indirectly) L

end

endif
endfor
return H 0
endif

Figure 2: Forced simulation nonrecursive depth-d determinate clauses
always least general hypothesis covers positive examples; hence, negative
counterexample e, ever seen, algorithm abort message consistent
hypothesis exists.
minimally generalize hypothesis H , function ForceSimNR used. subroutine shown Figure 2. figure, following terminology used.
output variable L input variable L0 , say L directly supports L0.
say L supports L0 iff L directly supports L0, L directly supports literal
L00 supports L0. (Thus \supports" transitive closure \directly supports".)
ForceSim NR deletes H minimal number literals necessary let H cover e+ .
this, ForceSim NR simulates action Prolog interpreter evaluating H , except
whenever literal L body H would fail, literal deleted, along
literals L0 supported L.
idea learning repeated generalization old one; particular, previous
methods exist learning definite clause generalizing highly-specific one. example, CLINT (De Raedt & Bruynooghe, 1992) generalizes \starting clause" guided
queries made user; PROGOL (Srinivasan, Muggleton, King, & Sternberg, 1994)
guides top-down generalization process known bottom clause; Rouveirol (1994)
describes method generalizing bottom clauses created saturation. Force1 NR algorithm thus interest novelty, provably correct ecient,
noted theorem below.
513

fiCohen

particular, let d-DepthNonRec language nonrecursive clauses depth
less (and hence i-DepthNonRec[DB; j -DetDEC ] language nonrecursive ij determinate clauses). following result:

Theorem 2 constants d, language family
d-DepthNonRec[DB= ; a-DetDEC =1]
uniformly identifiable equivalence queries.

Proof: show Force1 NR uniformly identifies language family polyno-

mial number queries. begin following important lemma, characterizes
behavior ForceSimNR .

Lemma 3 Let Dec declaration DetDEC =1 , let DB database, let f fact, let

H determinate nonrecursive clause satisfies Dec. one following conditions

must hold:
ForceSimNR(H ; f ; Dec; DB ) returns FAILURE, subclause H 0 H satisfies
Dec constraint H 0 ^ DB ` f ; or,
ForceSimNR(H ; f ; Dec; DB ) returns clause H 0, H 0 unique syntactically
largest subclause H satisfies Dec constraint H 0 ^ DB ` f .

Proof lemma: avoid repetition, refer syntactically maximal subclauses
H 0 H satisfy Dec constraint H 0 ^ DB ` f \admissible subclauses"

proof below.
Clearly lemma true H FAILURE returned ForceSim NR . remaining
cases loop algorithm executed, must establish two claims
(under assumptions f unify, f 62 DB ):
Claim 1. L retained, every admissible subclause contains L.
Claim 2. L deleted, admissible subclause contains L.
First, however, observe deleting literal L may cause mode
literals violate mode declarations Dec . easy see L deleted
clause C , mode literals L0 directly supported L change. Thus C
satisfies unique-mode declaration prior deletion L, deletion L
literals L0 directly supported L invalid modes.
Now, see Claim 1 true, suppose instead false. must
maximal subclause C 0 H satisfies Dec , covers fact f ,
contain L. argument above, C 0 contain L satisfied Dec , C 0
contains literals L0 H supported L. Hence output variables L
disjoint variables appearing C 0. means L added
C 0 resulting clause would still satisfy Dec cover f , leads contradiction
since C 0 assumed maximal.
verify Claim 2, let us introduce following terminology. C = (A B1 ^ : : : ^ Br )
clause DB database, say substitution (DB ; f )-witness
514

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

C iff associated proof C ^ DB ` f (or precisely, iff = f
8i : 1 r; Bi 2 DB .) claim following condition invariant
loop ForceSim NR algorithm.
Invariant 1. Let C admissible subclause contains literals H 0 preceding L (i.e., contains literals H retained previous
iterations algorithm). every (DB ; f )-witness C superset .
easily established induction number iterations loop.
condition true loop first entered, since initially general unifier
f . condition remains true iteration L deleted, since
unchanged. Finally, condition remains true iteration L retained:
0 maximally general, may assign values output variables L,
determinacy one assignment output variables L make L true. Hence
every (DB ; f )-witness C must contain bindings .
Next, inductive argument Claim 1 one show every admissible
subclause C must contain literals retained previous iterations
loop, leading following strengthening Invariant 1:
Invariant 10. Let C admissible subclause. every (DB ; f )-witness C
superset .
Now, notice two types literals deleted: (a) literals L superset
make L true, (b) literals L0 supported literal L preceding
type. case (a), clearly L cannot part admissible subclause, since superset
makes L succeed, supersets witnesses admissible clauses.
case (b), L0 cannot part admissible subclause, since declaration invalid
unless L present clause, argument L cannot clause.
concludes proof lemma.
prove theorem, must establish following properties identification
algorithm.
Correctness. Theorem 1, target program d-DepthNonRec[DB ; Dec],
clause CT equivalent target, subclause
BOTTOM (Dec ). H initially BOTTOM hence superclause CT . consider
invoking ForceSim NR positive counterexample e+ . Lemma 3, invocation
successful, H replaced H 0, longest subclause H covers e+ . Since
CT subclause H covers e+ , means H 0 superclause
CT . Inductively, then, hypothesis always superclause target.
Further, since counterexample e+ always instance covered
current hypothesis H , every time hypothesis updated, new hypothesis proper
subclause old. means Force1 NR eventually identify target clause.
Eciency. number queries made polynomial j Decj j DB j , since H
initially size polynomial j Dec j , reduced size time counterexample
provided. see counterexample processed time polynomial nr , ne ,
nt, notice since length H polynomial, number repetitions
loop ForceSim NR polynomial; further, since arity literals L bounded
515

fiCohen

a, anb + ane constants exist DB [ D, hence (anb + ane )a
substitutions 0 check inside loop, polynomial. Thus execution

ForceSim NR requires polynomial time.
concludes proof.

4. Learning Linear Closed Recursive Clause

Recall clause one recursive literal, clause linear recursive ,
recursive literal contains output variables, clause closed linear
recursive. section, describe Force1 algorithm extended
learn single linear closed recursive clause.2 presenting extension, however,
would first discuss reasonable-sounding approach that, closer examination, turns
incorrect.

4.1 Remark Recursive Clauses

One plausible first step toward extending Force1 recursive clauses allow recursive
literals hypotheses, treat way literals|that is, include
recursive literals initial clause BOTTOM , delete literals gradually
positives examples received. problem approach simple
way check recursive literal clause succeeds fails particular example.
makes impossible simply run ForceSimNR clauses containing recursive literals.
straightforward (apparent) solution problem assume oracle exists
queried success failure recursive literal. closed recursive
clauses, sucient assume oracle MEMBERCt (DB ; f ) answers
question
DB ^ P ` f ?
Ct unknown target concept, f ground fact, DB database. Given
oracle, one determine closed recursive literal Lr retained
checking MEMBERCT (DB ; Lr ) true. oracle close notion
membership query used computational learning theory.
natural extension Force1NR learning algorithm recursive clauses|in
fact algorithm based similar ideas previously conjectured pac-learn
closed recursive constant-depth determinate clauses (Dzeroski et al., 1992). Unfortunately,
algorithm fail return clause consistent positive counterexample.
illustrate this, consider following example.

Example. Consider using extension Force1NR described learn
following target program:
append(Xs,Ys,Zs)
2. reader may object useful recursive programs always least two clauses|a recursive
clause nonrecursive base case. posing problem learning single recursive clause,
thus assuming non-recursive \base case" target program provided background knowledge,
either background database DB , description atoms extended instances.

516

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

components(Xs,X,Xs1),
components(Zs,Z,Zs1),
X1=Z1,
append(Xs1,Ys,Zs1).
program determinate, depth 1, satisfies following set
declarations:
components(+,,,,).
null(+).
equal(+,+).
odd(+).
append(+,+,+).
assume database DB defines predicate null true
empty lists, odd true constants 1 3.
see forced simulation fail, consider following positive instance
e = (f; D):

f = append (l12 ; l3 ; l123 )
= f cons(l123,1,l23), cons(l23,2,l3), cons(l3,3,nil),
cons(l12,1,l2), cons(l2,2,nil),
append(nil,l3,l3) g

simply \ attened" form append([1,2],[3],[1,2,3]), together
appropriate base case append([],[3],[3]). consider beginning clause
BOTTOM 1 generalizing using ForceSimNR cover positive instance.
process illustrated Figure 3. clause left figure
BOTTOM (Dec ); clause right output forcibly simulating
clause f ForceSimNR . (For clarity we've assumed
single correct recursive call remains forced simulation.)
resulting clause incorrect, cover given example e.
easily seen stepping actions Prolog interpreter
generalized clause Figure 3. nonrecursive literals succeed, leading subgoal append(l2,l3,l23) (or usual Prolog notation,
append([2],[3],[2,3])). subgoal fail literal odd(X1), X1
bound 2 subgoal, fact odd(2) true DB [ D.
example illustrates pitfall policy treating recursive non-recursive
literals uniform manner (For discussion, see (Bergadano & Gunetti, 1993; De
Raedt, Lavrac, & Dzeroski, 1993).) Unlike nonrecursive literals, truth fact Lr
(corresponding recursive literal Lr ) imply clause containing Lr
succeed; may first subgoal Lr succeeds, deeper subgoals fail.
517

fiCohen

BOTTOM 1 (Dec ):
ForceSimNR (BOTTOM 1(Dec); f; Dec; DB [ D) :
append(Xs,Ys,Zs)
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^
components(Xs,X1,Xs1)^
components(Ys,Y1,Ys1)^
components(Ys,Y1,Ys1)^
components(Zs,Z1,Zs1)^
components(Zs,Z1,Zs1)^
null(Xs)^
null(Ys1)^
null(Ys)^
equal(X1,Z1)^
..
odd(X1)^
.
odd(Y1)^
null(Ys1)^
odd(Z1)^
null(Zs1),
append(Xs1,Ys,Zs1).
equal(Xs,Xs)^
..
.
equal(X1,Z1)^
..
.
equal(Zs1,Zs1)^
odd(Xs)^
..
.
odd(X1)^
odd(Y1)^
odd(Z1)^
..
.
odd(Zs1)^
append(Xs,Xs,Xs)^
..
.
append(Zs1,Zs1,Zs1).

Figure 3: recursive clause generalization ForceSimNR

4.2 Forced Simulation Recursive Clauses
solution problem replace calls membership oracle algorithm
sketched call routine forcibly simulates actions top-down
theorem-prover recursive clause. particular, following algorithm suggested.
First, build nonrecursive \bottom clause", done ForceSimNR . Second, find
recursive literal Lr appending Lr bottom clause yields recursive clause
generalized cover positive examples.
nonrecursive case, clause generalized deleting literals, using straightforward generalization procedure forced simulation nonrecursive clauses.
forced simulation, failing nonrecursive subgoals simply deleted; however,
recursive literal Lr encountered, one forcibly simulates hypothesis clause recursively
518

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSim (H ; f ; Dec; DB ; h ):

% \forcibly simulate" recursive clause H f
% 1. check infinite loops
h < 0 return FAILURE
% 2. check see f already covered
elseif f 2 DB return H
% 3. check see f cannot covered
elseif head H f cannot unified
return FAILURE

else

let Lr recursive literal H
let H 0 H , fLrg

% 4. delete failing non-recursive literals ForceSimNR
let head H 0
let mgu e
literal L body H 0
substitution 0 L0 2 DB
0, 0 general substitution

else

delete L body H 0 , together
literals L0 supported (directly indirectly) L

endif
endfor

% 5. generalize H 0 recursive subgoal Lr
Lr ground return ForceSim(H 0 [ fLr g; Lr; Dec; DB ; h , 1)
else return FAILURE

end

endif
endif

Figure 4: Forced simulation linear closed recursive clauses

519

fiCohen

corresponding recursive subgoal. implementation forced simulation linear
closed recursive clauses shown Figure 4.
extended algorithm similar ForceSimNR , differs recursive
literal Lr reached simulation H , corresponding subgoal Lr created,
hypothesized clause recursively forcibly simulated subgoal. ensures
generalized clause succeed subgoal. reasons become clear
shortly, would algorithm terminate, even original clause H enters
infinite loop used top-down interpreter. order ensure termination, extra
argument h passed ForceSim . argument h represents depth bound forced
simulation.
summarize, basic idea behind algorithm Figure 4 simulate hypothesized clause H f , generalize H deleting literals whenever H would fail
f subgoal f .

Example.

Consider using ForceSim forcibly simulate following recursive clause
BOTTOM 1(Dec ) [ Lr
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Xs)^: : : ^null(Zs1)^
odd(Xs)^: : : ^odd(Zs1)^
equal(Xs,Xs)^: : : ^equal(Zs1,Zs1)^
append(Xs1,Ys,Zs1)
recursive literal Lr append(Xs1,Ys,Zs1). assume f
taken extended query e = (f; D), attened version
instance append([1,2],[3],[1,2,3]) used previous example; Dec
set declarations previous example; database DB
[ null (nul ).
executing steps 1-4 ForceSim, number failing literals deleted,
leading substitution3 fXs = [1; 2], Ys = [3], Zs = [1; 2; 3], X1 = 1,
Xs1 = [2], Y1 = 3, Ys1 = [], Z1 = 1, Zs1 = [2; 3]g following reduced
clause:
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Ys1)^odd(X1)^odd(Y1)^odd(Z1)^equal(X1,Z1)^
append(Xs1,Ys,Zs1)
Hence recursive subgoal

Lr = append (Xs1 ; Ys ; Zs1 ) = append ([2]; [3]; [2; 3])
3. Note readability, using term notation rather attened notation Xs = l12,
Ys = l3, etc.

520

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Recursively applying ForceSim goal produces substitution fXs = [2],
Ys = [3], Zs = [2; 3], X1 = 2, Xs1 = [], Y1 = 3, Ys1 = [], Z1 = 2, Zs1 = [3]g
results deleting additional literals odd(X1) odd(Z1).
next recursive subgoal Lr = append ([]; [3]; [3]); since clause included
database DB , ForceSim terminate. final clause returned
ForceSim case following:
append(Xs,Ys,Zs)
components(Xs,X1,Xs1)^components(Ys,Y1,Ys1)^components(Zs,Z1,Zs1)^
null(Ys1)^odd(Y1)^equal(X1,Z1)^
append(Xs1,Ys,Zs1)
Notice clause cover e.
Section 3 begin analysis showing correctness forced simulation
algorithm|i.e., showing forced simulation indeed produce unique maximally
specific generalization input clause covers example.
proof correctness uses induction depth proof. Let us introduce
additional notation, write P ^ DB `h f Prolog program (P; DB )
used prove fact f proof depth h less. (The notion depth proof
usual one; define looking f database DB proof depth zero.)
following result concerning ForceSim algorithm.

Theorem 4 Let Dec declaration DetDEC =1, let DB database, let f fact,

let H determinate closed linear recursive clause satisfies Dec. one
following conditions must hold:

ForceSim(H; f; Dec; DB ; h) returns FAILURE, recursive subclause H 0 H
satisfies Dec constraint H 0 ^ DB `h f ; or,
ForceSim(H; f; Dec; DB ; h) returns clause H 0, H 0 unique syntactically
largest recursive subclause H satisfies Dec constraint H 0^DB `h f .

Proof: avoid repetition, refer syntactically maximal recursive (nonrecursive) subclauses H 0 H satisfy Dec constraint H 0 ^ DB `h f

\admissible recursive (nonrecursive) subclauses" respectively.
proof largely parallels proof Lemma 3|in particular, similar arguments
show clause returned ForceSim satisfies conditions theorem whenever
FAILURE returned whenever H returned. Note correctness ForceSim
H returned establishes base case theorem h = 0.
case depth h > 0, let us assume theorem holds depth h , 1
proceed using mathematical induction. arguments Lemma 3 show following
condition true loop terminates.

Invariant 10. H 0 unique maximal nonrecursive admissible subclause H , every
(DB ; f )-witness H 0 superset .
521

fiCohen

begin algorithm Force1 (d ; Dec; DB ):

% BOTTOM specific possible clause
let Lr1 ; : : :; Lrp possible closed recursive literals BOTTOM d(Dec)
choose unmarked recursive literal Lri
let H BOTTOM d(Dec) [ fLri g

repeat

answer query \Is H correct?"

Ans

Ans =\yes" return H
elseif Ans negative example e,
H

FAILURE

elseif Ans positive example e+

% generalize H minimally cover e+
let (f; D) components e+
H ForceSim (H ; f ; Dec; (DB [ ); (a j Dj + j DBj )a )
a0 arity clause head given Dec
0

endif
H = FAILURE
recursive literals marked
return \no consistent hypothesis"
else

mark Lri
choose unmarked recursive literal Lrj
let H BOTTOM d(Dec) [ fLrj g

end

endif
endif
endrepeat

Figure 5: learning algorithm nonrecursive depth-d determinate clauses
Now, let us assume admissible recursive subclause H . Clearly H must
contain recursive literal Lr H , since Lr recursive literal H . Further,
nonrecursive clause H^ = H , fLr g must certainly satisfy Dec H^ ^ DB ` f ,
must (by maximality H 0) subclause H 0. Hence H must subclause
H 0 [ fLr g. Finally, Lr ground (i.e., Lr closed clause H 0 [ Lr )
Invariant 10, clause H must satisfy H ^ DB ` Lr proof depth h , 1.
(This simply equivalent saying recursive subgoal Lr generated proof
must succeed.)
inductive hypothesis, then, recursive call must return unique maximal
admissible recursive subclause H 0 [ Lr , argument must
unique maximal admissible recursive subclause H .
Thus induction theorem holds.
522

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

4.3 Learning Algorithm Linear Recursive Clauses

Given method generalizing recursive clauses, one construct learning algorithm recursive clauses follows. First, guess recursive literal Lr , make
H = BOTTOM [ Lr initial hypothesis learner. Then, ask series equivalence
queries. positive counterexample e+ , use forced simulation minimally generalize
H cover e+ . negative example, choose another recursive literal L0r , reset
hypothesis H = BOTTOM [ L0r .
Figure 5 presents algorithm operates along lines. Let d-DepthLinRec
denote language linear closed recursive clauses depth less.
following result:
Theorem 5 constants d, language family
d-DepthLinRec[DB=; a-DetDEC =1]
uniformly identifiable equivalence queries.
Proof: show Force1 uniformly identifies language family polynomial number queries.
Correctness query eciency. aj Dj + aj DBj constants
set DB [ D, (aj Dj + aj DB j )a a0 -tuples constants, hence
(aj Dj + aj DB j )a distinct recursive subgoals Lr might produced proving
linear recursive clause C covers extended instance (f; D). Thus every terminating proof
fact f using linear recursive clause C must depth (aj Dj + aj DB j )a less; i.e.,
h = (aj Dj + aj DB j )a ,
C ^ DB ^ `h f iff C ^ DB ^ ` f
Thus Theorem 4 strengthened: value h used Force1, subroutine
ForceSim returns syntactically largest subclause H covers example (f; D)
whenever subclause exists, returns FAILURE otherwise.
argue correctness algorithm follows. Assume hypothesized recursive literal \correct"|i.e., target clause CT subclause
BOTTOM [ Lr . case easy see Force1 identify CT , using argument parallels one made Force1 NR . analogy Force1 NR , easy
see polynomial number equivalence queries made involving correct
recursive literal.
Next assume Lr correct recursive literal. CT need subclause
BOTTOM [ Lr , response equivalence query may either positive
negative counterexample. positive counterexample e+ received ForceSim
called, result may FAILURE, may proper subclause H covers
e+ . Thus result choosing incorrect Lr (possibly empty) sequence
positive counterexamples followed either negative counterexample FAILURE. Since
equivalence queries involving correct recursive literal answered either
positive counterexample \yes"4, negative counterexample FAILURE
obtained, must Lr incorrect.
0

0

0

0

4. Recall answer \yes" equivalence query means hypothesis correct.

523

fiCohen

number variables BOTTOM bounded aj BOTTOM (Dec )j ,
closed recursive literal completely defined a0-tuple variables, number
possible closed recursive literals Lr bounded

p = (aj BOTTOM (Dec )j )a

0

Since j BOTTOM (Dec )j polynomial j Dec j , p polynomial j Dec j . means
polynomial number incorrect Lr 's need discarded. since
successive hypothesis using single incorrect Lr proper subclause previous hypothesis, polynomial number equivalence queries needed discard incorrect
Lr . Thus polynomial number equivalence queries made involving incorrect
recursive literals.
Thus Force1 needs polynomial number queries identify Ct.
Eciency. ForceSim runs time polynomial arguments H , f , Dec, DB [
h. ForceSim called Force1, h always polynomial ne j DB j ,
H always larger j BOTTOM d(Dec)j + 1, turn polynomial size
Dec . Hence every invocation ForceSim requires time polynomial ne , Dec , DB ,
hence Force1 processes query polynomial time.
completes proof.
result somewhat surprising, shows recursive clauses learned
even given adversarial choice training examples. contrast, implemented ILP
systems require well-choosen examples learn recursive clauses.
formal result strengthened number technical ways. One
interesting strengthenings consider variant Force1 maintains
fixed set positive negative examples, constructs set least general
clauses consistent examples: could done taking
clauses BOTTOM [ Lr1 , : : : , BOTTOM [ Lrp , forcibly simulating
positive examples turn, discarding clauses cover one negative
examples. set clauses could used tractably encode version space
consistent programs, using [S; N ] representation version spaces (Hirsh, 1992).

5. Extending Learning Algorithm

consider number ways result Theorem 5 extended.

5.1 Equality-Predicate Unique-Mode Assumptions
Theorem 5 shows language family

d-DepthLinRec[DB=; a-DetDEC =1]
identifiable equivalence queries. natural ask result extended
dropping assumptions equality predicate present declaration
contains unique legal mode predicate: is, result extended
language family
d-DepthLinRec[DB; a-DetDEC ]
524

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

extension fact straightforward. Given database DB declaration Dec =
(p; a0; R) satisfy equality-predicate unique-mode assumptions, one
modify follows.
1. every constant c appearing DB , add fact equal (c ; c ) DB .
2. every predicate q k valid modes qs1 , : : : , qsk R:
(a) remove mode declarations q , replace k mode strings
k new predicates qs1 , : : : , qsk , letting qsi si unique legal mode
predicate qsi ;
(b) remove every fact q (t1 ; : : :; ta) predicate q DB , replace
k facts qs1 (t1 ; : : :; ta ), : : : , qsk (t1 ; : : :; ta).
Note arity predicates bounded constant a, number modes
k predicate q bounded constant 2a , hence transformations
performed polynomial time, polynomial increase size Dec
DB .
Clearly target clause Ct 2 d-DepthLinRec[DB ; Dec ] equivalent clause
Ct0 2 d-DepthLinRec[DB 0; Dec0], DB 0 Dec 0 modified versions DB
Dec constructed above. Using Force1 possible identify Ct0 . (In learning Ct0, one
must perform steps 1 2b description part every counterexample
(f; D).) Finally, one convert Ct0 equivalent clause d-DepthLinRec[DB ; Dec]
repeatedly resolving clause equal(X,X) , replacing every predicate
symbol qsi q .
leads following strengthening Theorem 5:

Proposition 6 constants d, language family
d-DepthLinRec[DB; a-DetDEC ]
uniformly identifiable equivalence queries.

5.2 Datalog Assumption

far assumed target program contains function symbols,
background knowledge provided user database ground facts. convenient
formal analysis, assumptions relaxed.
Examination learning algorithm shows database DB used two
ways.

forcibly simulating hypothesis extended instance (f; D), necessary
find substitution 0 makes literal L true database DB [ D.
done algorithmically DB sets ground facts, plausible
assume user provided oracle answers polynomial time
mode-correct query L database DB . Specifically, answer oracle
either
525

fiCohen

{ (unique) most-general substitution 0 DB ^ ` L0 L0
ground;
{ \no" 0 exists.

oracle would presumably take form ecient theorem-prover DB .

calling ForceSim, top-level learning algorithm uses DB determine

depth bound length proof made using hypothesis program. Again,
reasonable assume user provide information directly,
form oracle. Specifically, oracle would provide fact f polynomial
upper bound depth proof f target program.

Finally note ecient (but non-ground) background knowledge allowed,
function symbols always removed via attening (Rouveirol, 1994). transformation preserves determinacy, although may increase depth|in general, depth
attened clause depends term depth original clause. Thus, assumption
target program Datalog replaced assumptions term depth
bounded constant, two oracles available: oracle answers queries
background knowledge, depth-bound oracle. types oracles
frequently assumed literature (Shapiro, 1982; Page & Frisch, 1992; Dzeroski et al.,
1992).

5.3 Learning k-ary Recursive Clauses

natural ask Theorem 5 extended clauses linear recursive.
One interesting case case closed k-ary recursive clauses constant k.
straightforward extend Force1 guess tuple k recursive literals Lr1 , : : : , Lrk ,
extend ForceSim recursively generalize hypothesis clause facts
Lr1 , : : : , Lrk . arguments Theorems 4 5 modified show
extension identify target clause polynomial number equivalence queries.
Unfortunately, however, longer case ForceSim runs polynomial time.
easily seen one considers tree recursive calls made ForceSim;
general, tree branching factor k polynomial depth, hence exponential
size. result unsurprising, implementation ForceSim described forcibly
simulates depth-bounded top-down interpreter, k-ary recursive program take
exponential time interpret interpreter.
least two possible solutions problem. One possible solution
retain simple top-down forced simulation procedure, require user provide
depth bound tighter (aj Dj + aj DB j )a , maximal possible depth tree.
example, learning 2-ary recursive sort quicksort, user might specify logarithmic depth bound, thus guaranteeing ForceSim polynomial-time. requires
additional input user, would easy implement. advantage
(not shared approach described below) hypothesized program executed using simple depth-bounded Prolog interpreter, always shallow proof
trees. seems plausible bias impose learning k-ary recursive Prolog
programs, many tend shallow proof trees.
0

526

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

second solution possible high cost forced simulation k-ary recursive
programs forcibly simulate \smarter" type interpreter|one execute
k-ary recursive program polynomial time.5 One sound complete theorem-prover
closed k-ary recursive programs implemented follows.
Construct top-down proof tree usual fashion, i.e., using depth-first left-to-right
strategy, maintain list ancestors current subgoal, list VISITED
records, previously visited node tree, subgoal associated
node. Now, suppose course constructing proof tree one generates subgoal
f VISITED list. Since traversal tree depth-first left-to-right,
node associated f either ancestor current node, descendant
left sibling ancestor current node. former case, proof tree contains
loop, cannot produce successful proof; case theorem-prover exit
failure. latter case, proof must already exist f 0 , hence nodes
current node tree need visited; instead theorem prover simply assume
f true.
top-down interpreter easily extended forced simulation procedure:
one simply traverses tree order, generalizing current hypothesis H
needed justify inference step tree. additional point note
one performing forced simulation revisits previously proved subgoal f node
n, current clause H need generalized order prove f , hence
permissible simply skip portion tree n. thus following
result.

Theorem 7 Let d-Depth-k-Rec set k-ary closed recursive clauses depth d.
constants a, d, k language family
d-Depth-k-Rec[DB; a-DetDEC]
uniformly identifiable equivalence queries.

Proof: Omitted, following informal argument made above.
Note give result without restrictions database contains
equality relation declaration unique-mode, since tricks used relax
restrictions Proposition 6 still applicable.

5.4 Learning Recursive Base Cases Simultaneously

far, analyzed problem learning single clauses: first single nonrecursive
clause, single recursive clause. However, every useful recursive program contains
least two clauses: recursive clause, nonrecursive base case. natural ask
possible learn complete recursive program simultaneously learning
recursive clause, associated nonrecursive base case.
general, possible, demonstrated elsewhere (Cohen, 1995). However,
several cases positive result extended two-clause programs.
5. Note plausible believe theorem-prover exists, polynomial
number possible theorem-proving goals|namely, (aj Dj + aj DB j )a possible recursive subgoals.
0

527

fiCohen

begin algorithm Force2 (d ; Dec; DB ):
let Lr1 ; : : :; Lrp possible recursive literals BOTTOM d(Dec)
choose unmarked recursive literal Lri
let HR BOTTOM d(Dec) [ fLri g
let HB BOTTOM d(Dec)
let P = (HR; Hb)

repeat

Ans answer query \Is HR ; HB correct?"
Ans =\yes" return HR ; HB
elseif Ans negative example e,
P FAILURE
elseif Ans positive example e+
let (f; D) components e+
P ForceSim2 (HR; HB ; f ; Dec; (DB [ ); (a j Dj + j DBj )a )
0

endif
P = FAILURE
recursive literals Lrj marked
return \no consistent hypothesis"
else

mark Lri
choose unmarked recursive literal Lrj
let HR BOTTOM d(Dec) [ fLrj g
let HB BOTTOM (Dec)
let P = (HR ; HB )

end

endif
endif
endrepeat

Figure 6: learning algorithm two-clause recursive programs

528

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

begin subroutine ForceSim2 (HR; HB ; f ; Dec; DB ; h ):

% \forcibly simulate" program HR ; HB f
h < 1 return FAILURE
% check see f covered HB
elseif BASECASE (f )
return current Hr generalized HB
return (HR; ForceSimNR(HB ; f ; Dec; DB ))
elseif head HR f cannot unified
return FAILURE

else

let Lr recursive literal HR
let H 0 H , fLrg
let head H 0
let mgu e
literal L body H 0
substitution 0 L0 2 DB
0, 0 general substitution
else
delete L body H 0 , together
literals L0 supported (directly indirectly) L

endif
endfor

% generalize H 0; HB recursive subgoal Lr
Lr ground
% continue simulation program
return ForceSim2(H 0 [ fLr g; HB; Lr; Dec; DB ; h , 1)
else return FAILURE

end

endif
endif

Figure 7: Forced simulation two-clause recursive programs

529

fiCohen

section, first discuss learning recursive clause base clause simultaneously, assuming determinate base clause possible, assuming
additional \hint" available, form special \basecase" oracle.
discuss various alternative types \hints".
Let P target program base clause CB recursive clause CR. basecase
oracle P takes input extended instance (f; D) returns \yes" CB ^ DB ^ ` f ,
\no" otherwise. words, oracle determines f covered nonrecursive
base clause alone. example, append program, basecase oracle return
\yes" instance append(Xs,Ys,Zs) Xs empty list, \no" otherwise.
Given existence basecase oracle, learning algorithm extended
follows. before, possible recursive literals Lri clause BOTTOM generated;
however, case, learner test two clause hypotheses initially
form (BOTTOM [ Lri ; BOTTOM ). forcibly simulate hypothesis fact f ,
following procedure used. checking usual termination conditions, forced
simulator checks see BASECASE(f) true. so, calls ForceSimNR (with appropriate
arguments) generalize current hypothesis base case. BASECASE(f)
false, recursive clause Hr forcibly simulated f , subgoal Lr generated
before, generalized program recursively forcibly simulated subgoal.
Figures 6 7 present learning algorithm Force2 two clause programs consisting
one linear recursive clause CR one nonrecursive clause CB , assumption
equivalence basecase oracles available.
straightforward extend arguments Theorem 5 case, leading
following result.

Theorem 8 Let d-Depth-2-Clause set 2-clause programs consisting one

clause d-DepthLinRec one clause d-DepthNonRec. constants
language family

d-Depth-2-Clause[DB; a-DetDEC ]
uniformly identifiable equivalence basecase queries.

Proof: Omitted.
companion paper (Cohen, 1995) shows something basecase oracle
necessary: particular, without \hints" base clause, learning two-clause
linear recursive program hard learning boolean DNF. However, several
situations basecase oracle dispensed with.
Case 1. basecase oracle replaced polynomial-sized set possible base
clauses. learning algorithm case enumerate pairs base clauses CBi
\starting clauses" BOTTOM [ Lrj , generalize starting clause forced
simulation, mark pair incorrect overgeneralization detected.
Case 2. basecase oracle replaced fixed rule determines base
clause applicable. example, consider rule says base clause
applicable atom p(X1; : : :; Xa) Xi non-null list. Adopting
530

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

rule leads immediately learning procedure pac-learns exactly
two-clause linear recursive programs rule correct.
Case 3. basecase oracle replaced polynomial-sized set rules
determining base clause applicable. learning algorithm case
pick unmarked decision rule run Force2 using rule basecase oracle.
Force2 returns \no consistent hypothesis" decision rule marked incorrect,
new one choosen. algorithm learn two-clause linear recursive
programs given decision rules correct.
Even though general problem determining basecase decision rule arbitrary
Datalog program may dicult, may small number decision procedures
apply large number common Prolog programs. example, recursion
list-manipulation programs halts argument reduced null list
singleton list. Thus Case 3 seems likely cover large fraction automatic
logic programming programs practical interest.
note heuristics proposed finding basecase decision rules
automatically using typing restrictions (Stahl, Tausend, & Wirth, 1993).

5.5 Combining Results

Finally, note extensions described compatible. means
let kd-MaxRecLang language two-clause programs consisting one
clause CR k-ary closed recursive depth-d determinate, one clause CB
nonrecursive depth-d determinate, following holds.

Proposition 9 constants a, k language family
kd-MaxRecLang[DB; a-DetDEC ]
uniformly identifiable equivalence basecase queries.
5.5.1 Extensions
notation kd-MaxRecLang may seem point unjustified; although

expressive language recursive clauses proven learnable,
numerous extensions may eciently learnable. example, one might generalize
language allow arbitrary number recursive clauses, include clauses
determinate. generalizations might well pac-learnable|given results
presented far.
However, companion paper (Cohen, 1995) presents series negative results showing
natural generalizations kd-MaxRecLang eciently learnable,
kd-MaxRecLang eciently learnable without basecase oracle. Specifically, companion paper shows eliminating basecase oracle leads
problem hard learning boolean DNF, open problem computational
learning theory. Similarly, learning two linear recursive clauses simultaneously hard
learning DNF, even base case known. Finally, following learning problems
hard breaking certain (presumably) secure cryptographic codes: learning n
531

fiCohen

linear recursive determinate clauses, learning one n-ary recursive determinate clause,
learning one linear recursive \k-local" clause. negative results hold
model identification equivalence queries, weaker models
pac-learnability pac-predictability.

6. Related Work
discussing related work concentrate previous formal analyses employ
learning model similar considered here: namely, models (a) require computation polynomial natural parameters problem, (b) assume either neutral
source adversarial source examples, equivalence queries stochastically presented examples. note, however, much previous formal work exists relies
different assumptions. instance, much work member subset
queries allowed (Shapiro, 1982; De Raedt & Bruynooghe, 1992), examples
choosen non-random manner helpful learner (Ling, 1992; De Raedt
& Dzeroski, 1994). work eciency requirements
imposed pac-learnability model relaxed (Nienhuys-Cheng & Polman, 1994).
requirement eciency relaxed far enough, general positive results obtained using simple learning algorithms. example, model learnability
limit (Gold, 1967), language recursively enumerable decidable (which
includes Datalog) learned simple enumeration procedure; model
U-learnability (Muggleton & Page, 1994) language polynomially enumerable
polynomially decidable learned enumeration.
similar previous work Frazier Page (1993a, 1993b). analyze
learnability equivalence queries recursive programs function symbols
without background knowledge. positive results provide program classes
satisfy following property: given set positive examples + requires
clauses target program prove instances + , polynomial number
recursive clauses possible; base clause must certain highly constrained
form. Thus concept class \almost" bounded size polynomial. learning
algorithm program class interleave series equivalence queries
test every possible target program. contrast, positive results exponentially
large classes recursive clauses. Frazier Page present series negative results
suggesting learnable languages analyzed dicult generalize without
sacrificing ecient learnability.
Previous results exist pac-learnability nonrecursive constant-depth determinate programs, pac-learnability recursive constant-depth determinate
programs model allows membership subset queries (Dzeroski et al.,
1992).
basis intelligent search used learning algorithms technique
forced simulation . method finds least implicant clause C covers
extended instance e. Although developed method believed
original, subsequently discovered case|an identical technique
previously proposed Ling (1991). Since extended instance e converted
(via saturation) ground Horn clause, close connection forced
532

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

simulation recent work \inverting implication" \recursive anti-unification";
instance, Muggleton (1994) describes nondeterministic procedure finding clauses
imply clause C , Idestam-Almquist (1993) describes means constraining
implicant-generating procedure produce least common implicant two clauses.
However, techniques obvious applications learning,
extremely expensive worst case.
CRUSTACEAN system (Aha et al., 1994) uses inverting implication constrained
settings learn certain restricted classes recursive programs. class programs
eciently learned system formally well-understood, appears
similar classes analyzed Frazier Page. Experimental results show
systems perform well inferring recursive programs use function symbols certain
restricted ways. system cannot, however, make use background knowledge.
Finally, wish direct reader several pieces research relevant. noted above, companion paper exists presents negative learnability results
several natural generalizations language kd-MaxRecLang (Cohen, 1995). Another related paper investigates learnability non-recursive Prolog programs (Cohen,
1993b); paper contains number negative results strongly motivate
restriction constant-depth determinacy. final prior paper may interest
presents experimental results Prolog implementation variant Force2
algorithm (Cohen, 1993a). paper shows forced simulation basis
learning program outperforms state-of-the art heuristic methods FOIL (Quinlan, 1990; Quinlan & Cameron-Jones, 1993) learning randomly chosen examples.

7. Conclusions
often desirable guarantees correctness program, many
plausible contexts would highly desirable automatic programming system
offer formal guarantees correctness. topic paper learnability
recursive logic programs using formally well-justified algorithms. specifically,
concerned development algorithms provably sound ecient
learning recursive logic programs equivalence queries. showed one constantdepth determinate closed k-ary recursive clause identifiable equivalent queries;
implies immediately language learnable Valiant's (1984) model paclearnability. showed program consisting one recursive clause
one constant-depth determinate nonrecursive clause identifiable equivalence queries
given additional \basecase oracle", determines positive example covered
non-recursive base clause target program alone.
obtaining results, introduced several new formal techniques analyzing learnability recursive programs. shown soundness
eciency several instances generalization forced simulation . method may
applications practical learning systems. Force2 algorithm compares quite well experimentally modern ILP systems learning problems restricted class
identify (Cohen, 1993a); thus sound learning methods Force2 might useful
filter general ILP system FOIL (Quinlan, 1990; Quinlan & CameronJones, 1993). Alternatively, forced simulation could used heuristic programs.
533

fiCohen

example, although forced simulation programs many recursive clauses nondeterministic hence potentially inecient, one could introduce heuristics would make
forced simulation ecient, cost completeness.
companion paper (Cohen, 1995) shows positive results paper
likely improved: either eliminating basecase oracle language
learning two recursive clauses simultaneously hard learning DNF, learning n
linear recursive determinate clauses, one n-ary recursive determinate clause, one linear
recursive \k-local" clause hard breaking certain cryptographic codes. positive results paper, negative results establish boundaries learnability
recursive programs function-free pac-learnability model. results thus
give prescription building formally justified system learning recursive programs;
taken together, provide upper bounds one hope achieve
ecient, formally justified system learns recursive programs random examples
alone.

Appendix A. Additional Proofs

Theorem 1 states: Let Dec = (p; a0; R) declaration 2 a-DetDEC = , let nr = j Rj , let
X1; : : :; Xa distinct variables, define clause BOTTOM follows:
0

BOTTOM (Dec ) CONSTRAIN Dec (DEEPEN dDec (p(X1; : : :; Xa ) ))
0

constants a, following true:

size BOTTOM d(Dec) polynomial nr ;
every depth-d clause satisfies Dec equivalent subclause
BOTTOM (Dec ).

Proof: Let us first establish polynomial bound size BOTTOM d. Let C
clause size n. number variables C bounded an, size set LD

bounded

Thus clause C
similar argument

nr
|{z}

(|an{z)a,1}

(# modes) (# tuples input variables)

j DEEPEN Dec (C )j n + (an)a,1nr

(1)

j CONSTRAIN Dec (C )j n + (an)anr

(2)

Since functions DEEPEN Dec CONSTRAIN Dec give outputs polynomially larger size inputs, follows composing functions constant
number times, done computing BOTTOM constant d, produce
polynomial increase size.
Next, wish show every depth-d determinate clause C satisfies Dec
equivalent subclause BOTTOM . Let C depth-d determinate clause,
534

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

without loss generality let us assume pair literals Li Lj body
C mode, predicate symbol, sequence input variables.6
Given C , let us define substitution C follows:
1. Initially set
C fX1 = X1; : : :; Xa = Xa g
X1; : : :; Xa arguments head BOTTOM X1 ; : : :; Xa
arguments head C .
Notice variables head BOTTOM distinct, mapping
well-defined.
2. Next, examine literals body C left-to-right order.
literal L, let variables T1; : : :Tk input variables. literal L
body BOTTOM mode predicate symbol whose input variables
T1; : : :; Tk 8i : 1 r; TjC = Tj , modify C follows:
0

0

0

0

C [ fU1 = U1 ; : : :; Ul = Ul g
U1 ; : : :; Ul output variables L U1; : : :; Ul output variables
L .
Notice assume C contains one literal L given predC

icate symbol sequence input variables, output variables
literals L BOTTOM distinct, mapping well-defined.
easy verify (by induction length C ) executing procedure
variable BOTTOM always mapped input variable Ti , least
one L meeting requirements exists. Thus mapping C onto
variables appearing C .7
Let head BOTTOM , consider clause C 0 defined follows:
head C 0 A.
body C 0 contains literals L body BOTTOM either
{ LC body C
{ L literal equal (Xi; Xj) XiC = XjC .
claim C 0 subclause BOTTOM equivalent C . Certainly C 0
subclause BOTTOM . One way see equivalent C consider
clause C^ substitution ^C generated follows. Initially, let C^ = C 0
let ^C = C . Then, every literal L = equal (Xi; Xj) body C^ , delete L
^ ij replace ^C (^C )ij , ij
C^ , finally replace C^ C
substitution fXi = Xij ; Xj = Xij g Xij new variable previously appearing
6. assumption made without loss generality since determinate clause C , output
variables Li Lj necessarily bound values, hence Li Lj could unified
together one deleted without changing semantics C .
7. Recall function f : X onto range 8y 2 9x 2 X : f (x) = y.

535

fiCohen

C^ . (Note: (^C )ij refer substitution formed replacing every occurrence
Xi Xj appearing ^C Xij .) C^ semantically equivalent C 0
operation described equivalent simply resolving possible L body
C 0 clause \equal(X,X) ".
following straightforward verify:
^C one-to-one mapping.
see true, notice every pair assignments Xi = Xj =
C must literal equal (Xi; Xj) C 0. Hence following process
described assignments Xi = Xj = ^C would eventually
replaced Xij = Xij = .

^C onto variables C .
Notice C onto variables C , every assignment Xi = C
assignment ^C right-hand side (and assignment
either form Xi = Xij = ). Thus ^C onto variables C .
literal L^ body C^ iff L^^C body C .
follows definition C 0 fact every literal L
C 0 form equal (Xi; Xj) corresponding literal C^ .
Thus C^ alphabetic variant C , hence equivalent C . Since C^ equivalent
C 0, must C 0 equivalent C , proves claim.

Acknowledgements
author wishes thank three anonymous JAIR reviewers number useful suggestions presentation technical content.

References
Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication small
training sets. Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture
Notes Computer Science # 784.
Angluin, D. (1988). Queries concept learning. Machine Learning, 2 (4).
Angluin, D. (1989). Equivalence queries approximate fingerprints. Proceedings
1989 Workshop Computational Learning Theory Santa Cruz, California.
Bergadano, F., & Gunetti, D. (1993). interactive system learn functional logic programs. Proceedings 13th International Joint Conference Artificial Intelligence Chambery, France.
536

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Biermann, A. (1978). inference regular lisp programs examples. IEEE Transactions Systems, Man Cybernetics, 8 (8).
Cohen, W. W. (1993a). pac-learning algorithm restricted class recursive logic
programs. Proceedings Tenth National Conference Artificial Intelligence
Washington, D.C.
Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. appear Artificial
Intelligence.
Cohen, W. W. (1993c). Rapid prototyping ILP systems using explicit bias. Proceedings
1993 IJCAI Workshop Inductive Logic Programming Chambery, France.
Cohen, W. W. (1994). Pac-learning nondeterminate clauses. Proceedings Eleventh
National Conference Artificial Intelligence Seattle, WA.
Cohen, W. W. (1995). Pac-learning recursive logic programs: negative results. Journal
AI Research, 2, 541{573.
De Raedt, L., & Bruynooghe, M. (1992). Interactive concept-learning constructive
induction analogy. Machine Learning, 8 (2).
De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories PAC-learnable.
Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive
Logic Programming Bad Honnef/Bonn, Germany.
De Raedt, L., Lavrac, N., & Dzeroski, S. (1993). Multiple predicate learning. Proceedings
Third International Workshop Inductive Logic Programming Bled, Slovenia.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability determinate logic
programs. Proceedings 1992 Workshop Computational Learning Theory
Pittsburgh, Pennsylvania.
Frazier, M., & Page, C. D. (1993a). Learnability inductive logic programming:
basic results techniques. Proceedings Tenth National Conference
Artificial Intelligence Washington, D.C.
Frazier, M., & Page, C. D. (1993b). Learnability recursive, non-determinate theories:
basic results techniques. Proceedings Third International Workshop
Inductive Logic Programming Bled, Slovenia.
Gold, M. (1967). Language identification limit. Information Control, 10.
Hirsh, H. (1992). Polynomial-time learning version spaces. Proceedings Tenth
National Conference Artificial Intelligence San Jose, California. MIT Press.
Idestam-Almquist, P. (1993). Generalization implication recursive anti-unification.
Proceedings Ninth International Conference Machine Learning Amherst,
Massachusetts. Morgan Kaufmann.
537

fiCohen

King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug design
machine learning: use inductive logic programming model structureactivity relationships trimethoprim analogues binding dihydrofolate reductase.
Proceedings National Academy Science, 89.
Lavrac, N., & Dzeroski, S. (1992). Background knowledge declarative bias inductive
concept learning. Jantke, K. P. (Ed.), Analogical Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. Lectures
Artificial Intelligence Series #642.
Ling, C. (1991). Inventing necessary theoretical terms scientific discovery inductive
logic programming. Tech. rep. 301, University Western Ontario.
Ling, C. (1992). Logic program synthesis good examples. Inductive Logic Programming. Academic Press.
Lloyd, J. W. (1987). Foundations Logic Programming: Second Edition. Springer-Verlag.
Muggleton, S. (1994). Inverting implication. appear Artificial Intelligence.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.
Journal Logic Programming, 19/20 (7), 629{679.
Muggleton, S., & Feng, C. (1992). Ecient induction logic programs. Inductive Logic
Programming. Academic Press.
Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structure
prediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S., & Page, C. D. (1994). learnability model universal representations.
Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive
Logic Programming Bad Honnef/Bonn, Germany.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.
Nienhuys-Cheng, S., & Polman, M. (1994). Sample pac-learnability model inference.
Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture notes
Computer Science # 784.
Page, C. D., & Frisch, A. M. (1992). Generalization learnability: study constrained
atoms. Inductive Logic Programming. Academic Press.
Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. Machine
Learning, 9 (1).
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Brazdil, P. B.
(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notes
Computer Science # 667.
Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5 (3).
538

fiPac-Learning Recursive Logic Programs: Efficient Algorithms

Quinlan, J. R. (1991). Determinate literals inductive logic programming. Proceedings
Eighth International Workshop Machine Learning Ithaca, New York. Morgan
Kaufmann.
Rouveirol, C. (1994). Flattening saturation: two representation changes generalization. Machine Learning, 14 (2).
Shapiro, E. (1982). Algorithmic Program Debugging. MIT Press.
Srinivasan, A., Muggleton, S. H., King, R. D., & Sternberg, M. J. E. (1994). Mutagenesis:
ILP experiments non-determinate biological domain. Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive Logic Programming Bad
Honnef/Bonn, Germany.
Stahl, I., Tausend, B., & Wirth, R. (1993). Two methods improving inductive logic
programming. Proceedings 1993 European Conference Machine Learning
Vienna, Austria.
Summers, P. D. (1977). methodology LISP program construction examples.
Journal Association Computing Machinery, 24 (1), 161{175.
Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11).
Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers treebanks:
machine learning approach. Proceedings Twelfth National Conference
Artificial Intelligence Seattle, Washington. MIT Press.

539


