Journal Artificial Intelligence Research 2 (1995) 575-609

Submitted 12/94; published 5/95

Provably Bounded-Optimal Agents
Stuart J. Russell

Computer Science Division, University California
Berkeley, CA 94720, USA

Devika Subramanian

Computer Science Department, Cornell University
Ithaca, NY 14853, USA

russell@cs.berkeley.edu
devika@cs.cornell.edu

Abstract

Since inception, artificial intelligence relied upon theoretical foundation centred around perfect rationality desired property intelligent systems. argue,
others done, foundation inadequate imposes fundamentally
unsatisfiable requirements. result, arisen wide gap theory
practice AI, hindering progress field. propose instead property called bounded
optimality. Roughly speaking, agent bounded-optimal program solution
constrained optimization problem presented architecture task environment. show construct agents property simple class machine
architectures broad class real-time environments. illustrate results using
simple model automated mail sorting facility. define weaker property,
asymptotic bounded optimality (ABO), generalizes notion optimality classical
complexity theory. construct universal ABO programs, i.e., programs
ABO matter real-time constraints applied. Universal ABO programs
used building blocks complex systems. conclude discussion
prospects bounded optimality theoretical basis AI, relate similar trends
philosophy, economics, game theory.

1. Introduction

Since beginning artificial intelligence, philosophers, control theorists
economists looked satisfactory definition rational behaviour. needed
underpin theories ethics, inductive learning, reasoning, optimal control, decision-making,
economic modelling. Doyle (1983) proposed AI defined computational study rational behaviour|effectively equating rational behaviour intelligence. role definitions AI ensure theory practice correctly
aligned. define property P , hope able design system
provably possesses property P . Theory meets practice systems exhibit P reality. Furthermore, exhibit P reality something actually care
about. sense, choice P study determines nature field.
number possible choices P :
Perfect rationality: classical notion rationality economics philosophy.
perfectly rational agent acts every instant way maximize
expected utility, given information acquired environment. Since
action selection requires computation, computation takes time, perfectly rational
agents exist non-trivial environments.
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiRussell & Subramanian

Calculative rationality: notion rationality studied AI. calculatively rational

agent eventually returns would rational choice beginning
deliberation. exist systems uence diagram evaluators exhibit
property decision-theoretic definition rational choice, systems
nonlinear planners exhibit logical definition rational choice.
assumed interesting property system exhibit since constitutes
\in-principle" capacity right thing. Calculative rationality limited
value practice, actual behaviour exhibited systems absurdly
far rational; example, calculatively rational chess program choose
right move, may take 1050 times long so. result, AI systembuilders often ignore theoretical developments, forced rely trial-and-error
engineering achieve goals. Even simple domains chess, little
theory designing analysing high-performance programs.
Metalevel rationality: natural response problems calculative rationality.
metalevel rational system optimizes object-level computations performed service selecting actions. words, decision finds
optimal combination computation-sequence-plus-action, constraint
action must selected computation. Full metalevel rationality
seldom useful metalevel computations take time, metalevel decision problem often dicult object-level problem. Simple
approximations metalevel rationality proved useful practice|for example, metalevel policies limit lookahead chess programs|but engineering
expedients merely serve illustrate lack theoretical basis agent design.
Bounded optimality: bounded optimal agent behaves well possible given
computational resources. Bounded optimality specifies optimal programs rather
optimal actions optimal computation sequences. former approach
avoid placing constraints intelligent agents cannot met
program. Actions computations are, all, generated programs,
programs designers control.
make three claims:
1. system exhibits bounded optimality desirable reality.
2. possible construct provably bounded optimal programs.
3. Artificial intelligence usefully characterized study bounded optimality,
particularly context complex task environments reasonably powerful
computing devices.
first claim unlikely controversial. paper supports second claim
detail. third claim may, may not, stand test time.
begin section 2 necessarily brief discussion relationship
bounded optimality earlier notions rationality. note particular important distinctions missed without precise definitions terms. Thus section 3
provide formal definitions agents, programs, behaviour rationality.
576

fiProvably bounded-optimal agents

Together formal descriptions task environments, elements allow us prove
given agent exhibits bounded optimality. Section 4 examines class agent architectures problem generating bounded optimal configurations eciently
soluble. solution involves class interesting practically relevant optimization
problems appear addressed scheduling literature. illustrate results showing throughput automated mail-sorting facility
might improved. Section 5 initiates discussion bounded optimal configurations
might learned experience environment. section 6, define weaker property, asymptotic bounded optimality (ABO), may robust tractable
strict version bounded optimality. particular, construct universal ABO
programs. program universally ABO ABO regardless specific form
time dependence utility function.1 Universal ABO programs therefore used
building blocks complex systems. conclude assessment prospects
development approach artificial intelligence.

2. Historical Perspective
classical idea perfect rationality, developed Aristotle's theories ethics,
work Arnauld others choice uncertainty, Mill's utilitarianism, put
formal footing decision theory Ramsey (1931) vonNeumann Morgernstern
(1947). stipulates rational agent always act maximize expected utility.
expectation taken according agent's beliefs; thus, perfect rationality
require omniscience.
artificial intelligence, logical definition rationality, known philosophy
\practical syllogism", put forward McCarthy (1958), reiterated strongly
Newell (1981). definition, agent take action believes
guaranteed achieve goals. AI said theoretical foundation, definition rationality provided it. McCarthy believed, probably
correctly, early stages field important concentrate \epistemological adequacy" \heuristic adequacy" | is, capability principle rather
practice. methodology resulted involves designing programs exhibit
calculative rationality, using various speedup techniques approximations
hope getting close possible perfect rationality. belief, albeit unproven,
simple agent designs fulfill specification calculative rationality may
provide good starting points approach bounded optimality. Moreover, theoretical foundation based calculative rationality cannot provide necessary guidance
search.
clear AI would embarked quest calculative rationality
operating halcyon days formal intractability results discovered.
One response spectre complexity rule bounds. Levesque
Brachman (1987) suggest limiting complexity environment calculative
perfect rationality coincide. Doyle Patil (1991) argue strongly position.
1. usage term \universal" derives use scheduling randomized algorithms
Luby, Sinclair Zuckerman (1993).

577

fiRussell & Subramanian

Economists used perfect rationality abstract model economic entities,
purposes economic forecasting designing market mechanisms. makes
possible prove theorems properties markets equilibrium. Unfortunately,
Simon (1982) pointed out, real economic entities limited time limited powers
deliberation. proposed study bounded rationality, investigating \: : : shape
system effectiveness computation one important weapons
survival." Simon's work focussed mainly satisficing designs, deliberate
reaching solution satisfying preset \aspiration level." results descriptive value modelling various actual entities policies, general prescriptive
framework bounded rationality developed. Although proved possible calculate
optimal aspiration levels certain problems, structural variation allowed
agent design.
theory games, bounds complexity players become topic
intense interest. example, troubling fact defection equilibrium
strategy unbounded agents playing fixed number rounds Prisoners' Dilemma
game. Neyman's theorem (Neyman, 1985), recently proved Papadimitriou Yannakakis (1994), shows essentially cooperative equilibrium exists agent
finite automaton number states less exponential number
rounds. essentially bounded optimality result, bound space rather
speed computation. type result made possible shift
problem selecting actions problem selecting programs.
I. J. Good (1971) distinguished perfect \type I" rationality, metalevel
\type II" rationality. defines \the maximization expected utility taking
account deliberation costs." Simon (1976) says: \The global optimization problem
find least-cost best-return decision, net computational costs." Although type II
rationality seems step right direction, entirely clear whether
made precise way respects desirable intuition computation important.
try one interpretation, although may others.2 key issue space
\maximization" \optimization" occurs. Good Simon seem
referring space possible deliberations associated particular decision.
Conceptually, \object-level machine" executes sequence computations
control \meta-level machine." outcome sequence selection
external action. agent exhibits type II rationality end deliberation
subsequent action, utility maximized compared possible deliberate/act pairs
could engaged. example, Good discusses one possible application type
II rationality chess programs. case, object-level steps node expansions
game tree, followed backing leaf node evaluations show best move.
simplicity assume per-move time limit. type II rational agent execute
whichever sequence node expansions chooses best move, finish
2. example, conceivable Good Simon really intended refer finding agent design
minimizes deliberation costs general. discussions, however, seem couched terms
finding right deliberation decision. Thus, type II metalevel rationality coincides
bounded optimality bounded optimal agent designed single decision single
situation.

578

fiProvably bounded-optimal agents

time limit.3 Unfortunately, computations required \metalevel machine"
select object-level deliberation may extremely expensive. Good actually proposes
fairly simple (and nearly practical) metalevel decision procedure chess, far
optimal. hard see type II rational agent could justify executing suboptimal
object-level computation sequence limit scope optimization problem
single decision. diculty resolved thinking design
agent program, generates unbounded set possible deliberations response
unbounded set circumstances may arise life agent.
Philosophy seen gradual evolution definition rationality.
shift consideration act utilitarianism | rationality individual acts |
rule utilitarianism, rationality general policies acting. shift caused
diculties individual versus societal rationality, rather consideration
diculty computing rational acts. consideration given recently
tractability general moral policies, view making understandable
usable persons average intelligence (Brandt, 1953). Cherniak (1986) suggested
definition \minimal rationality", specifying lower bounds reasoning powers
rational agent, instead upper bounds. philosophical proposal generally consistent
notion bounded optimality found Dennett's \Moral First Aid Manual"
(1986). Dennett explicitly discusses idea reaching equilibrium within space
decision procedures. uses example PhD admissions procedure philosophy
department. concludes, we, best procedure may neither elegant
illuminating. existence procedure, process reaching it,
main points interest.
Many researchers AI, whose work discussed below, worked
problem designing agents limited computational resources. 1989 AAAI Symposium AI Limited Rationality (Fehling & Russell, 1989) contains interesting
variety work topic. Much work concerned metalevel rationality.
Metareasoning | reasoning reasoning | important technique area,
since enables agent control deliberations according costs benefits.
Combined idea anytime (Dean & Boddy, 1988) exible algorithms (Horvitz,
1987), return better results time goes by, simple form metareasoning allows
agent behave well real-time environment. simple example provided
iterative-deepening algorithms used game-playing. Breese Fehling (1990) apply similar ideas controlling multiple decision procedures. Russell Wefald (1989) give
general method precompiling certain aspects metareasoning system eciently estimate effects individual computations intentions, giving fine-grained
control reasoning. techniques seen approximating metalevel rationality; provide useful insights general problem control reasoning,
reason suppose approximations used optimal sense.
intuitive notion bounded optimality seems become current AI
community mid-1980's. Horvitz (1987) uses term bounded optimality refer
\the optimization computational utility given set assumptions expected
3. One would imagine cases move selected move selected Type
agent, sense \accidental" deliberation might cause program
abandon it.

579

fiRussell & Subramanian

problems constraints reasoning resources." Russell Wefald (1991) say
agent exhibits bounded optimality given task environment \if program solution
constrained optimization problem presented architecture." Recent work
Etzioni (1989) Russell Zilberstein (1991) seen optimizing welldefined set agent designs, thereby making notion bounded optimality precise.
next section, build suitable set general definitions ground up,
begin demonstrate examples provably bounded optimal agents.

3. Agents, Architectures Programs
Intuitively, agent physical entity wish view terms perceptions
actions. counts first instance does, necessarily thinks,
even whether thinks all. initial refusal consider constraints
internal workings agent (such reason logically, example) helps
three ways: first, allows us view \cognitive faculties" planning reasoning
occurring service finding right thing do; second, makes room
among us (Agre & Chapman, 1987; Brooks, 1986) take position systems
right thing without cognitive faculties; third, allows freedom consider
various specifications, boundaries interconnections subsystems.
begin defining agents environments terms actions percepts
exchange, sequence states go through. agent described
agent function percept sequences actions. treatment fairly standard
(see, e.g., Genesereth & Nilsson, 1987). go \inside" agent look agent
program generates actions, define \implementation" relationship
program corresponding agent function. consider performance measures
agents, problem designing agents optimize performance measure.

3.1 Specifying agents environments

agent described abstractly mapping (the agent function) percept
sequences actions. Let set percepts agent receive instant,
set possible actions agent carry external world. Since
interested behaviour agent time, introduce set time points
instants, T. set totally ordered < relation unique least element.
Without loss generality, let set non-negative integers.
percept history agent sequence percepts indexed time. define
set percept histories OT = fOT : ! Og. prefix history OT 2 OT
till time denoted Ot projection OT [0::t]. define set
percept history prefixes Ot = fOt j 2 T; OT 2 OTg. Similarly, define set
action histories = fAT : ! Ag. set action history prefixes At, defined
set projections histories 2 .

Definition 1 Agent function: mapping
f : Ot !
580

fiProvably bounded-optimal agents



AT(t) = f (Ot)
Note agent function entirely abstract entity, unlike agent program
implements it. Note \output" agent function given percept sequence
may null action, example agent still thinking do. agent
function specifies agent time step. crucial distinction
perfect rationality calculative rationality.
Agents live environments. states environment E drawn set X.
set possible state trajectories defined XT = fX : ! Xg. agent
necessarily full access current state X T(t), percept received agent
depend current state perceptual filtering function fp. effects
agent's actions represented environment's transition function fe,
specifies next state given current state agent's action. environment
therefore defined follows:

Definition 2 Environment E : set states X distinguished initial state X0,
transition function fe perceptual filter function fp

X (0) = X0
X T(t + 1) = fe(AT (t); X T(t))
OT(t) = fp(X T(t))
state history X thus determined environment agent function.
use notation effects(f; E ) denote state history generated agent function
f operating environment E . use notation [E; ] denote
state history generated applying action sequence starting initial state
environment E .
Notice environment discrete deterministic formulation.
extend definitions cover non-deterministic continuous environments,
cost additional complexity exposition. None results depend significant
way discreteness determinism.

3.2 Specifying agent implementations

consider physical agent consisting architecture program.
architecture responsible interfacing program environment,
running program itself. architecture , associate finite programming
language LM , set programs runnable architecture. agent
program program l 2 LM takes percept input internal state drawn
set initial state i0. (The initial internal state depends program l,
usually suppress argument.) set possible internal state histories
= : ! Ig. prefix internal state history 2 till time denoted
projection [0::t].
581

fiRussell & Subramanian

Definition 3 architecture fixed interpreter agent program runs

program single time step, updating internal state generating action:
: LM !

hI T(t + 1); AT(t)i = (l; T(t); OT(t))
Thus, architecture generates stream actions according dictates program.
physical properties architecture, running program single
time step results execution finite number instructions. program may
often fail reach \decision" time step, result action produced
architecture may null (or previous action, depending program
design).

3.3 Relating agent specifications implementations

relate agent programs corresponding agent functions. say
agent program l running machine implements agent function Agent(l; ).
agent function constructed following definition specifying action sequences
produced l running possible percept sequences. Note importance
\Markovian" construction using internal state agent ensure actions
based past, future.

Definition 4 program l running implements agent function f = Agent(l; ),
defined follows. environment E = (X; fe; fp), f (Ot) = (t)
hI T(t + 1); AT(t)i = (l; T(t); OT(t))
OT (t)
X T(t + 1)
X T(0)
T(0)

=
=
=
=

fp(X T(t))
fe(AT(t); X T(t))
X0
i0

Although every program l induces corresponding agent function Agent(l; ),
action follows given percept necessarily agent's \response" percept;
delay incurred deliberation, may ect percepts occurring much
earlier sequence. Furthermore, possible map every agent function
implementation l 2 LM . define subset set agent functions f
implementable given architecture language LM :
Feasible(M ) = j 9l 2 LM ; f = Agent(l; )g
Feasibility related to, clearly distinct from, notion computability. Computability refers existence program eventually returns output specified
function, whereas feasibility refers production output appropriate
point time. set feasible agent functions therefore much smaller set
computable agent functions.
582

fiProvably bounded-optimal agents

3.4 Performance measures agents

evaluate agent's performance world, define real-valued utility function U
state histories:
U : XT ! <
utility function seen external agent environment. defines
problem solved designer agent. agent designs may incorporate
explicit representation utility function, means required.
use term task environment denote combination environment utility
function.
Recall agent's actions drive environment E particular sequence
states accordance function effects(f; E ). define value agent
function f environment E utility state history generates:
V (f; E ) = U (effects(f; E ))
designer set E environments probability distribution p them,
instead single environment E , value agent E defined
expected value elements E. slight abuse notation,

V (f; E) =

X

E2E

p(E )V (f; E )

assign value V (l; M; E ) program l executed architecture
environment E simply looking effect agent function implemented
program:
V (l; M; E ) = V (Agent(l; ); E ) = U (effects(Agent(l; ); E ))
above, extend set possible environments follows:

V (l; M; E) =

X

E2E

p(E )V (l; M; E )

3.5 Perfect rationality bounded optimality

discussed Section 2, perfectly rational agent selects action maximizes
expected utility, given percepts far. framework, amounts agent
function maximizes V (f; E) possible agent functions.
Definition 5 perfectly rational agent set E environments agent function
fopt
fopt = argmaxf (V (f; E))
definition persuasive specification optimal agent function given
set environments, underlies several recent projects intelligent agent design (Dean
& Wellman,1991; Doyle, 1988; Hansson & Mayer, 1989). direct implementation
specification, ignores delay incurred deliberation, yield reasonable
583

fiRussell & Subramanian

solution problem { calculation expected utilities takes time real agent.
terms simple formal description agents introduced above, easy see
diculty arisen. designing agent program, logicists decision theorists
concentrated specifying optimal agent function fopt order guarantee
selection best action history. function fopt independent architecture .
Unfortunately, real program LM implements function non-trivial environment,
optimal actions cannot usually computed next percept arrives.
is, quite frequently, fopt 62 Feasible(M ).
Suppose environment consists games chess tournament rules
population human grandmasters, suppose standard personal computer.
fopt describes agent always plays way maximize total
expected points opposition, maximization moves makes.
claim possible program play way. quite possible, using depth-first
alpha-beta search termination, execute program chooses (say) optimal
minimax move situation, agent function induced program
fopt. particular, ignores percepts dropping ag indicating
loss time.
trouble perfect rationality definition arose unconstrained optimization space f 's determination fopt , without regard feasibility.
(Similarly, metalevel rationality assumes unconstrained optimization space deliberations.) escape quandary, propose machine-dependent standard rationality, maximize V implementable set agent functions Feasible(M ).
is, impose optimality constraints programs rather agent functions
deliberations.

Definition 6 bounded-optimal agent architecture set E environments
agent program lopt
lopt = argmaxl2LM V (l; M; E)

see immediately specification avoids obvious problems
Type Type II rationality. Consider chess example,
suppose computer
26
2
total program memory 8 megabytes. 2 possible programs
represented machine, much smaller number play legal chess.
tournament conditions, one programs best expected performance. suitable candidate lopt. Thus bounded optimality is, definition,
feasible specification; moreover, program achieves highly desirable.
yet ready announce identity lopt chess eight-megabyte PC,
begin restricted problem.

4. Provably Bounded-Optimal Agents

order construct provably bounded optimal agent, must carry following
steps:
Specify properties environment actions taken,
utility function behaviours.
584

fiProvably bounded-optimal agents

Specify class machines programs run.
Propose construction method.
Prove construction method succeeds building bounded optimal agents.
methodology similar formal analysis used field optimal control,
studies design controllers (agents) plants (environments). optimal control
theory, controller viewed essentially instantaneous implementation optimal
agent function. contrast, focus computation time required agent,
relation computation time dynamics environment.

4.1 Episodic, real-time task environments

section, consider restricted class task environments call episodic
environments. episodic task environment, state history generated actions
agent considered divided series episodes, terminated
action. Let A? distinguished set actions terminate episode.
utility complete history given sum utilities episode,
determined turn state sequence. 2 A?, environment
\resets" state chosen random stationary probability distribution Pinit .
order include effects choice utility episode, notionally
divide environment state \configuration" part \value" part,
configuration part determines state transitions value part determines
utility state sequence. Actions A? reset configuration part, \value"
recorded value part. restrictions mean episode treated
separate decision problem, translate following property: agent program l1
higher expected utility individual episodes agent l2, higher expected
utility corresponding episodic task environment.
real-time task environment one utility action depends
time executed. Usually, dependence suciently strong make
calculative rationality unacceptably bad approximation perfect rationality.
automated mail sorter4 provides illustrative example episodic task environment (see Figure 1). machine scans handwritten printed addresses (zipcodes)
mail pieces dispatches appropriate bins. episode starts arrival
new mail piece terminates execution physical action recommended
sorter: routing piece specific bin. \configuration part" environment corresponds letter feeder side, provides new, randomly selected letter
previous letter sorted. \value part" state corresponds state
receiving bins, determines utility process. aim maximize
accuracy sorting minimizing reject percentage avoiding jams. jam occurs
current piece routed appropriate bin, rejected, arrival
next piece.
provide formal definitions three varieties real-time task environments:
fixed deadlines, fixed time cost stochastic deadlines.
4. See (Sackinger et al. 1992; Boser et al. 1992) details actual system. application
suggested us Bernhard Boser early presentation work 1992 NEC Symposium.

585

fiRussell & Subramanian

camera
sacks mail

zipcode
buckets

reject

Figure 1: automated mail-sorting facility provides simple example episodic,
real-time task environment.
4.1.1 Fixed deadlines

simplest commonly studied kind real-time task environment contains
deadline known time. work real-time systems, deadlines described
informally systems built meet deadline. Here, need formal specification
order connect description deadline properties agents running
deadline task environments. One might think deadlines part environment
description, fact mainly realized constraints utility function. One
see considering opposite deadline | \starter's pistol." two
distinguished differing constraints utilities acting specific
time.
Definition 7 Fixed deadline: task environment hE; U fixed deadline time td
following conditions hold.
Taking action A? time deadline results utility:

U ([E; At1]) = U ([E; A(2td ,1) AT1 (t)])
\" denotes sequence concatenation, td , AT1 (t) 2 A? , A(1t,1) A(2td ,1)
contain action A?.
Actions taken td effect utility:

U ([E; At1]) U ([E; At2]) U ([E; At1d ]) U ([E; At2d ]) td
4.1.2 Fixed time cost

Task environments approximately fixed time cost common. Examples
include consultations lawyers, keeping taxi waiting, dithering invest
one's money. define task environment fixed time cost c comparing
utilities actions taken different times.
586

fiProvably bounded-optimal agents

Definition 8 Fixed time cost:
task environment hE; U fixed time cost if,


action history prefixes A11 A22 satisfying
(1) AT1 (t1) 2 A? AT2 (t2) = AT1 (t1)
(2) A(1t1 ,1) A(2t2 ,1) contain action A?
utilities differ difference time cost:
U ([E; At22 ]) = U ([E; At11 ]) , c(t2 , t1)

Strictly speaking, task environments fixed time cost. Utility values
finite range, one cannot continue incurring time costs indefinitely. reasonably short
times reasonably small costs, linear utility penalty useful approximation.
4.1.3 Stochastic deadlines

fixed-deadline fixed-cost task environments occur frequently design
real-time systems, uncertainty time-dependence utility function
common. turns interesting, see below.
stochastic deadline represented uncertainty concerning time occurrence
fixed deadline. words, agent probability distributionPpd deadline
time td . assume deadline must come eventually, t2T pd (t) = 1.
define cumulative deadline distribution Pd .
deadline occur known time, need distinguish
two cases:
agent receives percept, called herald (Dean & Boddy, 1988), announces
impending deadline. model using distinguished percept Od :

OT(td ) = Od
agent responds immediately, \meets deadline."
percept available, case agent walking blindfolded towards
utility cliff. deliberating further, agent risks missing deadline may
improve decision quality. example familiar readers deciding
whether publish paper current form, embellish risk
\scooped." treat case current paper.
Formally, stochastic deadline case similar fixed deadline case, except td
drawn distribution pd . utility executing action history prefix E
expectation utilities state history prefix possible deadline times.
Definition 9 Stochastic deadline: task environment class hE; U fixed-deadline task
environments stochastic deadline distributed according pd if, action history
prefix ,
X
U ([E; ]) = pd (t0)U ([Et ; ])
2T

0

0

hEt ; U task environment hE; U fixed deadline t0.
0

587

fiRussell & Subramanian

mail sorter example well described stochastic deadline. time
arrival mail pieces image processing station distributed according density
function pd , usually Poisson.

4.2 Agent programs agent architecture

consider simple agent programs episodic task environments, constructed elements set R = fr1 ; : : : ; rn g decision procedures rules. decision procedure
recommends (but execute) action Ai 2 A?, agent program fixed
sequence decision procedures. purposes, decision procedure black box
two parameters:

run time ti 0, integer represents time taken procedure
compute action.

quality qi 0, real number. gives expected reward resulting
executing action Ai start episode:

qi = U ([E; Ai])

(1)

Let MJ denote agent architecture executes decision procedures language J .
Let tM denote maximum runtime decision procedures accommodated
. example, runtime feedforward neural network proportional
size, tM runtime largest neural network fits .
architecture executes agent program = s1 : : : sm running decision
procedure turn, providing input obtained initial percept.
deadline arrives (at fixed time td , heralded percept Od ),
entire sequence completed, agent selects action recommended
highest-quality procedure executed:

(s; T(td); OT(td)) = hi0; action(I T(td ))i
(s; T(ts); OT(ts)) = hi0; action(I T(ts))i ts = Psi2s ti
(s; T(t); Od) = hi0; action(I T(t))i

(2)

updates agent's internal state history T(t) action(I T(t))
action recommended completed decision procedure highest quality.
action executed, internal state agent re-initialized i0 . agent
design works three task environment categories described above.
Next derive value V (s; M; E ) agent program environment E running
three real-time regimes show construct bounded optimal agents
task environments.

4.3 Bounded optimality fixed deadlines

Equation 2, know agent picks action A? recommended
decision procedure r highest quality executed deadline td arrives.
588

fiProvably bounded-optimal agents

P

Let s1 : : : sj longest prefix program ji=1 ti td . Definition 7
Equation 1, follows
V (s; M; E ) = Qj
(3)
Qi = maxfq1; : : : ; qi g. Given expression value agent program,
easily show following:
Theorem 1 Let r = arg maxri 2 R;titd qi . singleton sequence r bounded optimal
program episodic task environment known deadline td.
is, best program single decision procedure maximum quality whose runtime
less deadline.

4.4 Bounded optimality fixed time cost

Equation 2, know agent picks action A? recommended best
decision procedure sequence, since runs entire sequence = s1 : : : sm
deadline. Definition 8 Equation 1,

V (s; M; E ) = Qm , c


X
i=1

ti

(4)

Given expression value agent program, easily show following:
Theorem 2 Let r = arg maxri 2 R qi , cti . singleton sequence r bounded optimal
program episodic task environment fixed time cost c.
is, optimal program single decision procedure whose quality, net time
cost, highest.

4.5 Bounded optimality stochastic deadlines

stochastic deadline distributed according pd, value agent program
: : sm expectation. Definition 9, calculate
P ps (t=)V s(s;1 :M;
Et), hEt; U task environment fixed deadline t. Aft2T
ter substituting V (s; M; Et) Equation 3, expression simplifies summation,
procedures sequence, probability interruption ith procedure
sequence multiplied quality best completed decision procedure:

X
Pi
V (s) V (s; M; E) = [Pd(Pij+1
(5)
=1 tj ) , Pd ( j =1 tj )]Qi
i=1
Rt
Pd (t) = ,1
pd(t0)dt0 Pd (t) = 1 Pmi=1 ti.
simple example serves illustrate value function. Consider R = fr1 ; r2 ; r3g.
rule r1 quality 0.2 needs 2 seconds run: represent r1 = (0:2; 2).
rules r2 = (0:5; 5); r3 = (0:7; 7). deadline distribution function pd
uniform distribution 0 10 seconds. value sequence r1 r2r3
V (r1 r2r3 ) = [:7 , :2]:2 + [1 , :7]:5 + [1 , 1]:7 = :25
geometric intuition given notion performance profile, shown Figure 2.

589

fiRussell & Subramanian

q

0.7
0.5
0.2

p(t)


2

5

7

Figure 2: Performance profile r1r2 r3, pd superimposed.

Definition 10 Performance profile: sequence s, performance profile Qs (t) gives
quality action returned agent interrupted t:

Qs(t) = maxfqi :


X

j =1

tj tg

uniform deadline density function, value sequence proportional
area performance profile last possible interrupt time. Note
height profile interval length ti rule running quality
best previous rules.
Definition 10, following obvious property:
Lemma 1 performance profile sequence monotonically nondecreasing.
case sequence higher quality decisions times better
sequence:
Lemma 2 8t Qs1 (t) Qs2 (t), V (s1) V (s2 ).
case say Qs1 dominates Qs2 .
use idea performance profiles establish useful properties optimal
sequences.
Lemma 3 exists optimal sequence sorted increasing order q's.

P

Without Lemma 3, ni=1 i! possible sequences consider. ordering constraint eliminates 2n sequences. means proofs properties sequences, need consider ordered sequences. addition, replace Qi
Equation 5 qi .
following lemma establishes sequence always improved addition
better rule end:
Lemma 4 every sequence = s1 : : : sm sorted increasing order quality, single
step z qz qsm , V (sz ) V (s).
590

fiProvably bounded-optimal agents

Corollary 1 exists optimal sequence ending highest-quality rule R.
following lemma ects obvious intuition one get better result
less time, there's point spending time get worse result:
Lemma 5 exists optimal sequence whose rules nondecreasing order ti .
apply preparatory results derive algorithms construct bounded
optimal programs various deadline distributions.
4.5.1 General distributions

general deadline distribution, dynamic programming method used obtain
optimal sequence decision rules pseudo-polynomial time. construct optimal
sequence using definition V (s; M; E ) Equation 5. Optimal sequences generated
methods ordered qi, accordance Lemma 3.
construct table (i; t), entry table highest value
sequence ends rule ri time t. assume rule indices arranged

P
increasing order quality, ranges start time 0 end time L = ri 2R ti .
update rule is:

(i; t) = maxk2[0:::i,1][S (k; , ti ) + (qi , qk )[1 , Pd (t)]]
boundary condition
(i; 0) = 0 rule (0; t) = 0 time
Corollary 1, read best sequence highest value row n
matrix .
Theorem 3 DP algorithm computes optimal sequence time O(n2L) n
number decision procedures R.

dependence L time complexity DP algorithm means algorithm polynomial input size. Using standard rounding scaling methods,
however, fully polynomial approximation scheme constructed. Although
hardness proof problem, John Binder (1994) shown deadline
distribution used constant-time oracle finding values P (t), algorithm
require exponential number calls oracle worst case.
4.5.2 Long uniform distributions

deadline uniformly distributed time interval greater sum
running times rules, call distribution long uniform distribution. Consider
rule sequence = s1 : : : sm drawn rule set R. long uniform distribution,
probability deadline arrives rule si sequence independent
time si starts. permits simpler form Equation 5:

V (s; M; E) = Pmi=1,1 Pd (ti+1)qi + qm (1 , Pmi=1 Pd (ti))
591

(6)

fiRussell & Subramanian

derive optimal sequence long uniform distribution, obtain recursive
specification value sequence 2 R = s1 : : : sm sequence
R.

V (as; M; E) = V (s; M; E) + qaPd (t1) , qmPd (ta)

(7)

allows us define dynamic programming scheme calculating optimal sequence
using state function (i; j ) denoting highest value rule sequence starts
rule ends rule j . Lemma 3 Equation 7, update rule is:

(i; j ) = maxi<kj [S (k; j ) + Pd (tk )qi , Pd (ti)qj ]

(8)

boundary condition

(i; i) = (1 , Pd (ti))qi

(9)

Corollary 1, know optimal sequence long uniform distribution ends
rn , rule highest quality R. Thus, need examine (i; n); 1
n. entry requires O(n) computation, n entries compute. Thus,
optimal sequence long uniform case calculated O(n2 ).

Theorem 4 optimal sequence decision procedures long uniform deadline distribution determined O(n2) time n number decision procedures
R.
4.5.3 Short uniform distributions

P

ni=1 Pd (ti) > 1, uniform deadline distribution Pd, call short. means
sequences longer last possible deadline time, therefore rules
sequences possibility executing deadline. sequences,
cannot use Equation 7 calculate V (s). However, sequence truncated
removing rules would complete execution last possible deadline.
value sequence unaffected truncation, truncated sequences use
Equation 7 justified. Furthermore, optimal sequence truncated
sequence.
Since update rule 8 correctly computes (i; j ) truncated sequences, use
short uniform distributions provided add check ensure sequences
considered truncated. Unlike long uniform case, however, identity last rule
optimal sequence unknown, need compute n2 entries (i; j ) table.
entry computation takes O(n) time, thus time compute optimal sequence
O(n3).

Theorem 5 optimal sequence decision procedures short uniform deadline distribution determined O(n3) time n number decision procedures
R.
592

fiProvably bounded-optimal agents

4.5.4 Exponential distributions

exponential distribution, Pd(t) = 1,e,fit . Exponential distributions allow optimal
sequence computed polynomial time. Let pi stand probability rule
interrupted, assuming starts 0. pi = Pd(ti ) = 1 , e,fiti : exponential
distribution, V (s; M; E) simplifies as:

V (s; M; E) =



h
ij =1(1 , pj ) pi+1qi + mj=1(1 , pj ) qm

mX
,1 h
i=1

yields simple recursive specification value V (as; M; E) sequence
begins rule a:

V (as; M; E) = (1 , pa )p1qa + (1 , pa)V (s; M; E)
use state function (i; j ) represents highest value rule sequence
starting ending j .

(i; j ) = maxi<kj [(1 , pi )pk qi + (1 , pi)S (k; j )]
boundary condition (i; i) = qi(1 , pi). given j , (i; j ) calculated
O(n2). Corollary 1, know optimal sequence whose last element
highest-valued rule R.

Theorem 6 optimal sequence decision procedures exponentially distributed
stochastic deadline determined O(n2) time n number decision
procedures R.

proof similar long uniform distribution case.

4.6 Simulation results mail-sorter

preceding results provide set algorithms optimizing construction agent
program variety general task environment classes. section, illustrate
results possible gains realized specific task environment, namely,
simulated mail-sorter.
First, let us precise utility function U episodes. four
possible outcomes; utility outcome ui.
1. zipcode successfully read letter sent correct bin delivery.
2. zipcode misread letter goes wrong bin.
3. letter sent reject bin.
4. next letter arrives recognizer finished, jam. Since
letter arrival heralded, jams cannot occur machine architecture given
Equation 2.
593

fiRussell & Subramanian

1

1
mu=9

0.8

0.8

0.6

0.6
P(t)

Accuracy

lambda=0.9

0.4

0.4

0.2

0.2

0

0
0

2

4
6
Computation Time (sec)

8

10

0

2

4
6
Time (sec)

8

10

Figure 3: (a) Accuracy profile (1 , e,x ), = 0:9. (b) Poisson arrival distribution,
mean = 9 sec
Without loss generality, set u1 = 1:0 u2 = 0:0. probability rule
recommending correct destination bin pi, qi = piu1 + (1 , pi)u2 = pi . assume
u2 u3, hence threshold probability letter sent
reject bin instead. therefore include rule set R rule rreject
zero runtime recommends rejection. sequence construction algorithm
automatically exclude rules quality lower qreject = u3. overall utility
episode chosen linear combination quality sorting (qi ), probability
rejection rejection rate (given P (t1), t1 runtime first non-reject
rule executed), speed sorting (measured arrival time mean).
agent program (Boser et al. 1992) uses single neural network chip.
show variety conditions optimized sequence networks
significantly better single network terms throughput accuracy. examine
following experimental conditions:
assume network executes time recognition accuracy p
depends t. consider p = 1,e,t . particular choice irrelevant
scale chosen arbitrary. choose = 0:9, convenience (Figure 3(a)).
include rreject qreject = u3 treject = 0.
consider arrival time distributions Poisson varying means. Figure 3(b) shows three example distributions, means 1, 5, 9 seconds.
create optimized sequences sets 40 networks execution times taken
equal intervals = 1 40.
compare
(a) BO sequence: bounded optimal sequence;
(b) Best singleton: best single rule;
(c) 50% rule: rule whose execution time mean distribution (i.e.,
complete 50% cases);
594

fiProvably bounded-optimal agents

1
BO Sequence
Best Singleton
50% Rule
90% Rule

Average utility per second

0.8

0.6

0.4

0.2

0
5

10

15

20
25
Mean arrival time

30

35

40

Figure 4: Graph showing achievable utility per second function average time
per letter, four program types. = 0:9.
(d) 90% rule: rule whose execution time guarantees complete 90%
cases.
last three cases, add rreject initial step; BO sequence include
automatically.
measure utility per second function mean arrival rate (Figure 4).
shows optimal setting sorting machinery 6 letters per
minute (inter-arrival time = 10 seconds) bounded optimal program, given
fixed 0.9.
Finally, investigate effect variance arrival time relative
performance four program types. purpose, use uniform distribution
centered around 20 seconds different widths vary variance without
affecting mean (Figure 5).
notice several interesting things results:
policy choosing rule 90% probability completion performs poorly
rapid arrival rates ( 3), catches performance best single
rule slower arrival rates ( > 4). artifact exponential accuracy
profile > 0:5, difference quality rules run times
greater 6 seconds quite small.
policy choosing rule 50% probability completion fares well
best single rule high arrival rates ( 2), rapidly diverges
thereafter, performing far worse arrival time means greater 5 seconds.
595

fiRussell & Subramanian

1
BO Sequence
Best Singleton
50% Rule
90% Rule

Average utility per second

0.8

0.6

0.4

0.2

0
0

20

40

60
80
Variance arrival time

100

120

Figure 5: Graphs showing utility gain per second function arrival time
variance, four program types uniform distribution mean
20 seconds.

best sequence best single rule give best overall performance
arrival rate around 6 letters per minute. performance advantage
optimal sequence best single rule 7% arrival rate.
noted significant performance advantage obtainable
extra computational resources. slower arrival rates ( 7), difference
performance best rule best sequence arises decreased
rejection rate best sequence. exponential accuracy profile ( 0:5)
advantage running rule shorter completion time ahead longer rule
ability reduce probability rejecting letter. high arrival rates
(inter-arrival times 1 4 seconds), useful short rules instead
longer single rule.

Figure 5 shows best sequence performs better best single rule

variance arrival time increases.5 performance optimal sequence
appears largely unaffected variance. exactly behaviour expect
observe | ability run sequence rules instead committing single
one gives robustness face increasing variance. Since realistic environments
involve unexpected demands many kinds, possession variety default
behaviours graded sophistication would seem optimal design choice
bounded agent.

5. performance 50% rule uniform distributions used experiment
fixed mean symmetric, 50% rule always rule runs 20 seconds.
90% rule changes variance, curve exhibits discretization effects. could
eliminated using finer-grained set rules.

596

fiProvably bounded-optimal agents

5. Learning Approximately Bounded-Optimal Programs

derivations assume suitable rule set R available ab initio, correct
qualities qi runtimes ti , deadline distribution known. section,
study ways information learned, implications
bounded optimality resulting system. concentrate learning rules
qualities, leaving runtimes deadline distributions future work.
basic idea learning algorithms converge, time, set
optimal components | accurate rules accurate quality estimates
them. happens, value agent constructed rules, using quality
estimates, converges value lopt. Thus two sources suboptimality
learned agent:
rules R may best possible rules | may recommend actions
lower utility would recommended rules.
may errors estimating expected utility rule. cause
algorithms given construct suboptimal sequences, even best rules
available.
notional method constructing bounded optimal agents (1) learns sets individual decision procedures episodic interactions, (2) arranges sequence
using one algorithms described earlier performance agent using
sequence least good agent. assume parameterized learning algorithm LJ ;k used learn one rule possible runtime
k 2 f1; : : : ; tM g. Since never need include two rules runtime
R, obviates need consider entire rule language J optimization
process.
setting places somewhat unusual requirements learning algorithm.
learning algorithms, LJ ;k works observing collection training episodes E,
including utility obtained episode. not, however, make assumptions
form correct decision rule. Instead, make assumptions
hypotheses, namely come finite language Jk , set programs
J complexity k. setting called agnostic learning setting
Kearns, Schapire Sellie (1992), assumptions made environment
all. shown (Theorems 4 5 Kearns, Schapire Sellie, 1992) that,
languages J , error learned approximation bounded within
best rule Jk fits examples, probability 1 , . sample size needed
guarantee bounds polynomial complexity parameter k, well 1 1 .
addition constructing decision procedures, LJ ;k outputs estimates
quality qi . Standard Chernoff-Hoeffding bounds used limit error quality
estimate within q probability 1 , q . sample size estimation quality
polynomial 1q 1q .
Thus error agnostically learned rule bounded within best rule
complexity class probability 1 , . error quality estimation
rules bounded q probability 1 , q . bounds, calculate bound
utility deficit agent program construct, comparison lopt :
597

fiRussell & Subramanian

Theorem 7 Assume architecture MJ executes sequences decision procedures
agnostically learnable language J whose runtimes range [1::tM ]. real time task

environments fixed time cost, fixed deadline, stochastic deadline, construct
program l
V (lopt ; M; E) , V (l; M; E) + 2q
probability greater 1 , m( + q ), number decision procedures
lopt .

Proof: prove theorem stochastic deadline regime, bounded

optimal program sequence decision procedures. proofs fixed cost
fixed deadline regimes, bounded optimal program singleton, follow
special case. Let best decision procedures E set R = fr1 ; : : : ; rn g,
let lopt = s1 : : : sm optimal sequence constructed R. Let R = fr1 ; : : : rng
set decision procedures returned learning algorithm. probability greater
1 , m, qi , qi i, qi refers true quality ri . error
estimated quality q^i decision procedure ri bounded: probability greater
1 , mq , jq^i , qi j q i.
Let = s1 : : : sm rules R come runtime classes
rules s1 : : : sm R . Then, Equation 5,
V (lopt ; M; E) , V (s; M; E)
error V weighted average errors individual qi . Similarly,

jV^ (s; M; E) , V (s; M; E)j q
suppose sequence construction algorithm applied R produces sequence
l = s1 : : : sl . definition, sequence appears optimal according estimated
value function V^ . Hence
V^ (l; M; E) V^ (s; M; E)
before, bound error estimated value:
jV^ (l; M; E) , V (l; M; E)j q
Combining inequalities,
V (lopt ; M; E) , V (l; M; E) + 2q
0

0

2

Although theorem practical applications, mainly intended illustration
learning procedure converge bounded optimal configuration.
additional work, general error bounds derived case rule
execution times ti real-time utility variation (time cost, fixed deadline, deadline
distribution) estimated training episodes. obtain error bounds
case rule language J divided smaller number coarser
runtime classes, rather potentially huge number currently use.
598

fiProvably bounded-optimal agents

6. Asymptotic Bounded Optimality

strict notion bounded optimality may useful philosophical landmark
explore artificial intelligence, may strong allow many interesting, general
results obtained. observation made ordinary complexity theory:
although absolute eciency aim, asymptotic eciency game. sorting
algorithm O(n log n) rather O(n2) considered significant, replacing \multiply
2" \shift-left 1 bit" considered real advance. slack allowed
definitions complexity classes essential building earlier results, obtaining robust
results restricted specific implementations, analysing complexity
algorithms use algorithms subroutines. section, begin reviewing
classical complexity. propose definitions asymptotic bounded optimality
advantages, show classical optimality special case
asymptotic bounded optimality. Lastly, report preliminary investigations
use asymptotic bounded optimality theoretical tool constructing universal
real-time systems.

6.1 Classical complexity

problem, classical sense, defined pair predicates output
z solution input x (x) (x; z ) hold. problem instance
input satisfying , algorithm problem class always terminates output
z satisfying (x; z ) given input x satisfying (x). Asymptotic complexity describes
growth rate worst-case runtime algorithm function input size.
define formally follows. Let Ta (x) runtime algorithm input x,
let Ta (n) maximum runtime input size n. algorithm
complexity O(f (n))
9k; n0 8n n > n0 ) Ta (n) kf (n)
Intuitively, classically optimal algorithm one lowest possible complexity.
purposes constructing asymptotic notion bounded optimality,
useful definition classical optimality mention complexity
directly. done follows:
Definition 11 Classically optimal algorithm: algorithm classically optimal

9k; n0 8a0; n n > n0 ) Ta (n) kTa (n)
relate classical complexity framework, need define special case task
environments traditional programs appropriate. task environments,
input provided program initial percept, utility function
environment histories obeys following constraint:
Definition 12 Classical task environment: hEP ; U classical task environment
problem P
(
l outputs correct solution P
V (l; M; EP ) = u0 (T (l; M; EP )) ifotherwise
0

599

fiRussell & Subramanian

(l; M; EP ) running time l EP , universal Turing machine,
u positive decreasing function.
notion problem class classical complexity theory thus corresponds class
classical task environments unbounded complexity. example, Traveling Salesperson Problem contains instances arbitrarily large numbers cities.

6.2 Varieties asymptotic bounded optimality

first thing need complexity measure environments. Let n(E ) suitable
measure complexity environment. assume existence environment
classes unbounded complexity. Then, analogy definition classical
optimality, define worst-case notion asymptotic bounded optimality (ABO).
Letting V (l; M; n; E) minimum value V (l; M; E ) E E complexity n,

Definition 13 Worst-case asymptotic bounded optimality: agent program l timewise
(or spacewise) worst-case asymptotically bounded optimal E iff
9k; n0 8l0; n n > n0 ) V (l; kM; n; E) V (l0; M; n; E)
kM denotes version machine speeded factor k (or k times
memory).
English, means program basically along right lines needs
faster (larger) machine worst-case behaviour good program
environments.
probability distribution associated environment class E, use
expected value V (l; M; E) define average-case notion ABO:
Definition 14 Average-case asymptotic bounded optimality: agent program l timewise
(or spacewise) average-case asymptotically bounded optimal E iff
9k 8l0 V (l; kM; E) V (l0 ; M; E)
worst-case average-case definitions ABO, would happy
program ABO nontrivial environment nontrivial architecture , unless
k enormous.6 rest paper, use worst-case definition ABO.
Almost identical results obtained using average-case definition.
first observation made ABO programs classically optimal
programs special case ABO programs:7
6. classical definitions allow optimality constant factor k runtime algorithms.
One might wonder chose use constant factor expand machine capabilities, rather
increase time available program. context ordinary complexity theory,
two alternatives exactly equivalent, context general time-dependent utilities,
former appropriate. would possible simply \let l run k times longer," programs
wish consider control execution time, trading solution quality. One could
imagine slowing entire environment factor k, merely less realistic version
propose.
7. connection suggested Bart Selman.

600

fiProvably bounded-optimal agents

Theorem 8 program classically optimal given problem P
timewise worst-case ABO corresponding classical task environment class hEP ; U i.
observation follows directly Definitions 11, 12, 13.
summary, notion ABO provide degree theoretical robustness
machine-independence study bounded systems asymptotic complexity
classical programs. set basic framework, begin exercise
definitions.

6.3 Universal asymptotic bounded optimality

Asymptotic bounded optimality defined respect specific value function V .
constructing real-time systems, would prefer certain degree independence
temporal variation value function. achieve defining family V value
functions, differing temporal variation. mean value function
preserves preference ordering external actions time, value functions
family preference ordering.8
example, fixed-cost regime vary time cost c generate family
value functions; stochastic deadline case, vary deadline distribution Pd
generate another family. Also, since three regimes uses quality measure
actions, union three corresponding families family.
show single program, call universal program, asymptotically
bounded-optimal regardless value function chosen within particular family.
Definition 15 Universal asymptotic bounded optimality (UABO): agent program l
UABO environment class E family value functions V iff l ABO E
every Vi 2 V .
UABO program must compete ABO programs every individual value function
family. UABO program therefore universal real-time solution given task.
UABO programs exist? so, construct them?
turns use scheduling construction (Russell & Zilberstein,
1991) design UABO programs. construction designed reduce task environments unknown interrupt times case known deadlines, insight
applies here. construction requires architecture provide program concatenation (e.g., LISP prog construct), conditional-return construct, null program
. universal program lU form concatenation individual programs
increasing runtime, appropriate termination test each. written
lU = [l0 l1 lj ]
lj consists program termination test. program part lj
program LM ABO E value function Vj corresponds fixed deadline
td = 2j , time increment smaller execution time non-null
program LM .
8. value function must therefore separable (Russell & Wefald, 1989), since preservation rank
order allows separate time cost defined. See chapter 9 (Keeney & Raiffa, 1976) thorough
discussion time-dependent utility.

601

fiRussell & Subramanian

q

l U 4M

0.7

l opt

0.5
0.2

p(t)


0

10

Figure 6: Performance profiles lU running 4M , lopt running
proceeding statement lU indeed UABO, let us look example.
Consider simple, sequential machine architecture described earlier. Suppose
select rules three-rule set r1 = (0:2; 2), r2 = (0:5; 5) r3 = (0:7; 7). Since
shortest runtime rules 2 seconds, let = 1. look optimal
programs l0 ; l1; l2; l3 ; : : : fixed-deadline task environments td = 1; 2; 4; 8; : : :.
are:

l0 = ; l1 = r1; l2 = r1; l3 = r3 ; : : :
Hence sequence programs lU [; r1; r1 ; r3; : : :].
consider task environment class value function Vi specifies stochastic
deadline uniformly distributed range [0: : : 10]. class, lopt = r1 r2
bounded optimal sequence.9 turns lU higher utility lopt provided
run machine four times faster. see plotting two performance
profiles: QU lU 4M Qopt lopt . QU dominates Qopt, shown Figure 6.
establish lU construction yields UABO programs general, need
define notion worst-case performance profile. Let Q (t; l; M; n; E) minimum
value obtained interrupting l t, E E complexity n. know
lj lU satisfies following:

8l0; n n > nj ) Vj(lj ; kj M; n; E) Vj(l0 ; M; n; E)
constants kj , nj . aim prove

8Vi 2 V 9k; n0 8l0; n n > n0 ) Vi(lU ; kM; n; E) Vi(l0; M; n; E)
Given definition worst-case performance profile, fairly easy show following
lemma (the proof essentially identical proof Theorem 1 Russell Zilberstein,
1991):
9. Notice that, simple model, output quality rule depends execution time
input complexity. means worst-case average-case behaviour same.

602

fiProvably bounded-optimal agents

1
BO Sequence
ABO sequence

Average utility per second

0.8

0.6

0.4

0.2

0
5

10

15

20
25
Mean arrival time

30

35

40

, function mean
Figure 7: Throughput accuracy improvement lU lopt
arrival time, = 0.2, Poisson arrivals.

Lemma 6 lU universal program E V , li ABO E Vi 2 V ,
Q(t; lU ; kM; n; E) dominates Q(t; li ; M; n; E) k 4 maxj kj , n > maxj nj .
lemma establishes that, small constant penalty, ignore specific realtime nature task environment constructing bounded optimal programs. However,
still need deal issue termination. possible general lU
terminate appropriate time without access information concerning timedependence utility function. example, fixed-time-cost task environment,
appropriate termination time depends value time cost c.
general case deterministic time-dependence, help lU supplying, Vi , \aspiration level" Qi (ti; li ; M; n; E), ti time
li acts. lU terminates completed lj qj Qi (ti ; li; M; n; E).
construction, happen later ti Lemma 6.

Theorem 9 task environments deterministic time-dependence, lU suitable
aspiration level UABO E .
deadline heralds, termination test somewhat simpler require
additional input lU .
Theorem 10 task environment stochastic deadlines, lU UABO E
terminates herald arrives.
Returning mail-sorting example, fairly easy see lU (which consists
sequence networks, optimal programs stochastic deadline case)
ABO fixed-deadline regime. obvious ABO particular
603

fiRussell & Subramanian

stochastic deadline case | recall regimes considered single family.
programmed constructor function universal programs, applied
mail-sorter environment class. Varying letter arrival distribution gives us different value
functions Vi 2 V . Figure 7 shows lU (on 4M ) higher throughput accuracy
across entire range arrival distributions.
lopt
Given existence UABO programs, possible consider behaviour compositions thereof. simplest form composition functional composition,
output one program used input another. complex, nested compositional structures entertained, including loops conditionals (Zilberstein, 1993).
main issue constructing UABO compositions allocate time among
components. Provided solve time allocation problem know
total runtime allowed, use construction technique used generate composite UABO programs, optimality among possible compositions
components. Zilberstein Russell (1993), show allocation problem
solved linear time size composite system, provided composition tree
bounded degree.

7. Conclusions Work
examined three possible formal bases artificial intelligence, concluded
bounded optimality provides appropriate goal constructing intelligent systems.
noted similar notions arisen philosophy game theory
less reason: mismatch classically optimal actions
called feasible behaviours|those generated agent program running
computing device finite speed size.
showed careful specification task environment computing
device one design provably bounded-optimal agents. exhibited simple
agents, likely bounded optimality strict sense dicult goal
achieve larger space agent programs considered. relaxed notions
asymptotic bounded optimality (ABO) may provide theoretically robust tools
progress. particular, ABO promises yield useful results composite agent
designs, allowing us separate problem designing complex ABO agents discrete
structural problem continuous temporal optimization problem tractable
many cases. Hence, reason optimistic artificial intelligence
usefully characterized study bounded optimality. may speculate provided
computing device neither small (so small changes speed size cause
significant changes optimal program design) powerful (so classically
optimal decisions computed feasibly), ABO designs stable reasonably
wide variations machine speed size environmental complexity. details
optimal designs may rather arcane, learning processes play large part
discovery; expect focus type research questions
convergence optimality various structural classes end result itself.
Perhaps important implication, beyond conceptual foundations field
itself, research bounded optimality applies, design, practice artificial
intelligence way idealized, infinite-resource models may not. given,
604

fiProvably bounded-optimal agents

way illustrating definition, bounded optimal agent: design simple system
consisting sequences decision procedures provably better program
class. theorem exhibits bounded optimal design translates, definition,
agent whose actual behaviour desirable.
appear plenty worthwhile directions continue exploration
bounded optimality. foundational point view, one interesting
questions concept applies agents incorporate learning component.
(Note section 5, learning algorithm external agent.)
case, necessarily largely stable bounded optimal configuration
agent program large enough; instead, agent adapt shorter-term
horizon rewrite becomes obsolete.
results preservation ABO composition, start examine
much interesting architectures simple production system studied above.
example, look optimal search algorithms, algorithm constrained
apply metalevel decision procedure step decide node expand,
(Russell & Wefald, 1989). extend work asymptotic bounded optimality
provide utility-based analogue \big-O" notation describing performance
agent designs, including suboptimal.
context computational learning theory, obvious stationarity
requirement environment, necessary satisfy preconditions PAC
results, restrictive. fact agent learns may effect
distribution future episodes, little known learning cases (Aldous &
Vazirani, 1990). could relax deterministic episodic requirement allow
non-immediate rewards, thereby making connections current research reinforcement
learning.
computation scheduling problem examined interesting itself,
appear studied operations research combinatorial optimization literature. Scheduling algorithms usually deal physical rather computational tasks,
hence objective function usually involves summation outputs rather picking
best. would resolve formal question tractability general case,
look cases solution qualities individual processes interdependent
(such one use results another). Practical extensions include computation
scheduling parallel machines multiple agents, scheduling combinations computational physical (e.g., job-shop ow-shop) processes, objective functions
combination summation maximization. latter extension broadens scope
applications considerably. industrial process, designing manufacturing
car, consists computational steps (design, logistics, factory scheduling, inspection
etc.) physical processes (stamping, assembling, painting etc.). One easily imagine
many applications real-time financial, industrial, military contexts.
may turn bounded optimality found wanting theoretical framework.
case, hope refuted interesting way, better framework
created process.
605

fiRussell & Subramanian

Appendix: Additional Proofs
appendix contains formal proofs three subsidiary lemmata main body
paper.

Lemma 3 exists optimal sequence sorted increasing order q's.
Proof: Suppose case, optimal sequence. must

two adjacent rules i, + 1 qi > qi+1 (see Figure 8). Removal rule + 1 yields
sequence s0 Qs (t) Qs (t), Lemma 1 fact ti+2 ti+1 + ti+2 .
Lemma 2, s0 must optimal. repeat removal process s0 ordered
qi , proving theorem reductio ad absurdum.2
0

Lemma 4 every sequence = s1 : : : sm sorted increasing order quality, single
step z qz qsm , V (sz ) V (s).
Proof: calculate V (sz ) , V (s) using Equation 5 show non-negative:
V (sz ) , V (s) = qz [1 , Pd ((Pmj=1 tj ) + tz )] , qm[1 , Pd ((Pmj=1 tj ) + tz )]
P
= (qz , qm )[1 , Pd (( mj=1 tj ) + tz )]

non-negative since qz qm .2
q

i+2

qi+2
qi
qi-1
qi+1



ti

i+1

Figure 8: Proof ordering qi; lower dotted line indicates original profile; upper dotted
line indicates profile removal rule + 1.

Lemma 5 exists optimal sequence whose rules nondecreasing order ti .
Proof: Suppose case, optimal sequence. must
two adjacent rules i, + 1 qi qi+1 ti > ti+1 (see Figure 9). Removal rule
yields sequence s0 Qs (t) Qs (t), Lemma 1. Lemma 2, s0 must
0

optimal. repeat removal process s0 ordered ti, proving theorem
reductio ad absurdum.2
606

fiProvably bounded-optimal agents

q

qi+1
qi

i+1

qi-1



i+1

ti

Figure 9: Proof ordering ti ; dotted line indicates profile removal rule i.

Acknowledgements

would acknowledge stimulating discussions Michael Fehling, Michael Genesereth, Russ Greiner, Eric Horvitz, Henry Kautz, Daphne Koller, Bart Selman
subject bounded optimality; Dorit Hochbaum, Nimrod Megiddo, Kevin Glazebrook subject dynamic programming scheduling problems; Nick
Littlestone Michael Kearns subject agnostic learning. would
thank reviewers many constructive suggestions. Many early ideas
work based arose discussions late Eric Wefald. Thanks
Ron Parr work uniform-distribution case, Rhonda Righter extending
results exponential distribution, Patrick Zieske help implementing dynamic programming algorithm. first author supported NSF grants IRI-8903146,
IRI-9211512 IRI-9058427, visiting fellowship SERC sabbatical
UK, NEC Research Institute. second author supported NSF
grant IRI-8902721.

References

Agre, P., & Chapman, D. (1987). Pengi: implementation theory activity.
Proc. 6th National Conference Artificial Intelligence, Seattle, WA. Morghan Kaufmann.
Aldous, D., & Vazirani, U. (1990). markovian extension valiant's learning model.
Proc. 31st Annual Symposium Foundations Computer Science, St. Louis, MO.
IEEE Comput. Soc. Press.
Binder, J. (1994). complexity deliberation scheduling stochastic deadlines..
Boser, B. E., Sackinger, E., Bromley, J., & LeCun, Y. (1992). Hardware requirements
neural network pattern classifiers | case study implementation. IEEE Micro,
12, 32{40.
Brandt, R. (1953). search credible form rule utilitarianism. Nakhnikian, G., &
Castaneda, H. (Eds.), Morality Language Conduct.
607

fiRussell & Subramanian

Breese, J. S., & Fehling, M. R. (1990). Control problem-solving: Principles architecture. Shachter, R. D., Levitt, T., Kanal, L., & Lemmer, J. (Eds.), Uncertainty
Artificial Intelligence 4. North Holland: Amsterdam.
Brooks, R. A. (1986). robust, layered control system mobile robot. IEEE Journal
Robotics Automation, 2, 14{23.
Cherniak, C. (1986). Minimal rationality. MIT Press: Cambridge.
Dean, T., & Boddy, M. (1988). analysis time-dependent planning. Proc. AAAI88, pp. 49{54.
Dean, T. L., & Wellman, M. P. (1991). Planning control. Morgan Kaufmann: San
Mateo, CA.
Dennett, D. (1986). moral first aid manual. Tanner lectures human values, University
Michigan.
Doyle, J. (1983). rational psychology? toward modern mental philosophy. AI
Magazine, 4, 50{53.
Doyle, J. (1988). Artificial intelligence rational self-government. Tech. rep.. Technical
report CMU-CS-88-124.
Doyle, J., & Patil, R. (1991). Two theses knowledge representation: language restrictions, taxonomic classification, utility representation services. Artificial
intelligence, 48, 261{297.
Etzioni, O. (1989). Tractable decision-analytic control. Proc. 1st International Conference Knowledge Representation Reasoning, pp. 114{125.
Fehling, M., & Russell, S. J. (1989). Proceedings AAAI Spring Symposium Limited
Rationality. AAAI.
Genesereth, M. R., & Nilsson, N. J. (1987). Logical Foundations Artificial Intelligence.
Morgan Kaufmann: Mateo, CA.
Good, I. J. (1971). Twenty-seven principles rationality. Godambe, V. P., & Sprott,
D. A. (Eds.), Foundations Statistical Inference, pp. 108{141. Holt, Rinehart, Winston.: Toronto.
Hansson, O., & Mayer, A. (1989). Heuristic search evidential reasoning. Proceedings
Fifth Workshop Uncertainty Artificial Intelligence, Windsor, Ontario.
Horvitz, E. J. (1988). Reasoning beliefs actions computational resource
constraints. Levitt, T., Lemmer, J., & Kanal, L. (Eds.), Uncertainty Artificial
Intelligence 3. North Holland: Amsterdam.
Kearns, M., Schapire, R., & Sellie, L. (1992). Toward ecient agnostic learning. Proc. 5th
Ann. Workshop Computational Learning Theory, Pittsburgh, PA. Morgan Kaufmann.
608

fiProvably bounded-optimal agents

Keeney, R., & Raiffa, H. (1976). Decisions multiple objectives: Preferences value
tradeoffs. Wiley: New York.
Levesque, H., & Brachman, R. (1987). Expressiveness tractability knowledge representation reasoning. Computational Intelligence, 3, 78{93.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup las vegas algorithms.
Information Processing Letters, 47, 173{80.
McCarthy, J. (1958). Programs common sense. Proceedings Symposium
Mechanization Thought Processes, Teddington, England: HMSO.
Newell, A. (1981). knowledge level. AI Magazine, 2, 1{20.
Neyman, A. (1985). Bounded complexity justifies cooperation finitely repeated prisoners' dilemma. Economics Letters, 19, 227{229.
Papadimitriou, C., & Yannakakis, M. (1994). complexity bounded rationality.
Proc. ACM Symposium Theory Computation.
Ramsey, F. P. (1931). Truth probability. Braithwaite, R. (Ed.), foundations
mathematics logical essays. Harcourt Brace Jovanovich: New York.
Russell, S. J., & Wefald, E. H. (1989a). optimal game tree search using rational metareasoning. Proc. IJCAI-89.
Russell, S. J., & Wefald, E. H. (1989b). Principles metareasoning. Proc. KR-89.
Russell, S. J., & Wefald, E. H. (1991). right thing: Studies limited rationality.
MIT Press: Cambridge, MA.
Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Proc. IJCAI-91,
Sydney.
Sackinger, E., Boser, B. E., Bromley, J., & LeCun, Y. (1992). Application anna
neural network chip high-speed character recognition. IEEE Transactions Neural
Networks, 3, 498{505.
Simon, H. A. (1976). decide do. Models bounded rationality,
Volume 2.
Simon, H. A. (1982). Models bounded rationality, Volume 2. MIT Press: Cambridge.
von Neumann, J., & Morgenstern, O. (1947). Theory games economic behavior.
Princeton University Press: Princeton.
Zilberstein, S. (1993). Operational Rationality Compilation Anytime Algorithms.
Ph.D. thesis, Computer Science Division, University California, Berkeley.
Zilberstein, S., & Russell, S. (1993). Optimal composition real-time systems. Submitted
Artificial Intelligence.
609


