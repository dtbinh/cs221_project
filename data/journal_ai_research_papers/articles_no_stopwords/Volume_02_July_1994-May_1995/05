Journal Artificial Intelligence Research 2 (1995) 541-573

Submitted 9/94; published 5/95

Pac-learning Recursive Logic Programs:
Negative Results
William W. Cohen

AT&T Bell Laboratories
600 Mountain Avenue, Murray Hill, NJ 07974 USA

wcohen@research.att.com

Abstract

companion paper shown class constant-depth determinate k-ary
recursive clauses eciently learnable. paper present negative results showing
natural generalization class hard learn Valiant's model paclearnability. particular, show following program classes cryptographically
hard learn: programs unbounded number constant-depth linear recursive
clauses; programs one constant-depth determinate clause containing unbounded
number recursive calls; programs one linear recursive clause constant locality.
results immediately imply non-learnability general class programs.
show learning constant-depth determinate program either two linear
recursive clauses one linear recursive clause one non-recursive clause hard
learning boolean DNF. Together positive results companion paper,
negative results establish boundary ecient learnability recursive function-free
clauses.

1. Introduction
Inductive logic programming (ILP) (Muggleton, 1992; Muggleton & De Raedt, 1994)
active area machine learning research hypotheses learning system
expressed logic programming language. many different learning problems
considered ILP, including great practical interest (Muggleton, King,
& Sternberg, 1992; King, Muggleton, Lewis, & Sternberg, 1992; Zelle & Mooney, 1994;
Cohen, 1994b), class problems frequently considered reconstruct simple
list-processing arithmetic functions examples. prototypical problem sort
might learning append two lists. Often, sort task attempted using
randomly-selected positive negative examples target concept.
Based similarity problems studied field automatic programming
examples (Summers, 1977; Biermann, 1978), (informally) call class
learning tasks automatic logic programming problems. number experimental
systems built (Quinlan & Cameron-Jones, 1993; Aha, Lapointe, Ling, & Matwin,
1994), experimental success automatic logic programming systems limited.
One common property automatic logic programming problems presence recursion . goal paper explore analytic methods computational limitations
learning recursive programs Valiant's model pac-learnability (1984). (In brief,
model requires accurate approximation target concept found polynomial time using polynomial-sized set labeled examples, chosen stochastically.)
surprise nobody limitations exist, far obvious previous
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCohen

research limits lie: provably fast methods learning recursive
logic programs, even fewer meaningful negative results.
starting point investigation series positive learnability results appearing companion paper (Cohen, 1995). results show single constant-depth
determinate clause constant number \closed" recursive calls pac-learnable.
show two-clause constant-depth determinate program consisting one nonrecursive clause one recursive clause type described pac-learnable,
additional \hints" target concept provided.
paper, analyze number generalizations learnable languages.
show relaxing restrictions leads dicult learning problems: particular, learning problems either hard learning DNF (an open problem
computational learning theory), hard cracking certain presumably secure cryptographic schemes. main contribution paper, therefore, delineation
boundaries learnability recursive logic programs.
paper organized follows. Section 2 define classes logic programs
learnability models used paper. Section 3 present cryptographic
hardness results two classes constant-depth determinate recursive programs: programs
n linear recursive clauses, programs one n-ary recursive clause.
analyze learnability clauses constant locality, another class clauses paclearnable nonrecursive case, show even single linearly recursive local
clause cryptographically hard learn. turn, Section 4, analysis
even restricted classes recursive programs. show two different classes
constant-depth determinate programs prediction-equivalent boolean DNF: class
programs containing single linear recursive clause single nonrecursive clause,
class programs containing two linearly recursive clauses. Finally, summarize
results paper companion, discuss related work, conclude.
Although paper read independently companion paper suggest
readers planning read papers begin companion paper (Cohen, 1995).

2. Background

completeness, present technical background needed state results;
however, aside Sections 2.2 2.3, introduce polynomial predictability
prediction-preserving reducibilities, respectively, background closely follows presented companion paper (Cohen, 1995). Readers encouraged skip section
already familiar material.

2.1 Logic Programs

assume reader familiarity logic programming (such
obtained reading one standard texts (Lloyd, 1987).) treatment logic
programs differs usually consider body clause ordered
set literals. consider logic programs without function symbols|i.e.,
programs written Datalog.
semantics Datalog program P defined relative database , DB ,
set ground atomic facts. (When convenient, think DB
542

fiPac-Learning Recursive Logic Programs: Negative Results

conjunction ground unit clauses). particular, interpret P DB subset
set extended instances . extended instance pair (f; D)
instance fact f ground fact, description set ground unit clauses.
extended instance (f; D) covered (P; DB ) iff
DB ^ ^ P ` f
extended instances allowed, function-free programs encode many computations usually represented function symbols. example, function-free
program tests see list append two lists written follows:

Program P :

append(Xs,Ys,Ys)
null(Xs).
append(Xs,Ys,Zs)
components(Xs,X,Xs1) ^
components(Zs,X,Zs1) ^
append(Xs1,Ys,Zs1).

Database DB :

null(nil).
predicate components(A,B,C) means list head B tail C; thus
extended instance equivalent append([1,2],[3],[1,2,3]) would instance fact
f = append (list12 ; list3 ; list123 ) description containing atoms:
components(list12,1,list2), components(list2,2,nil),
components(list123,1,list23), components(list23,2,list3),
components(list3,3,nil)
use extended instances function-free programs closely related \ attening"
(Rouveirol, 1994; De Raedt & Dzeroski, 1994); experimental learning systems
impose similar restriction (Quinlan, 1990; Pazzani & Kibler, 1992). Another motivation
using extended instances technical. (sometimes quite severe) syntactic
restrictions considered paper, often polynomial number possible
ground facts|i.e., Herbrand base polynomial. Hence programs interpreted
usual model-theoretic way would possible learn program equivalent
given target simply memorizing appropriate subset Herbrand base. However,
programs interpreted sets extended instances, trivial learning algorithms
become impossible; even extremely restricted program classes still exponential number extended instances size n. discussion found
companion paper (Cohen, 1995).
define terminology logic programs used
paper.
2.1.1 Input/Output Variables

B1 ^ : : : ^ Br (ordered) definite clause, input variables literal Bi
variables appear clause B1 ^ : : : ^ Bi,1 ; variables
appearing Bi called output variables .
543

fiCohen

2.1.2 Types Recursion

literal body clause recursive literal predicate symbol
arity head clause. every clause program one recursive
literal, program linear recursive . every clause program k recursive
literals, program k-ary recursive . every recursive literal program contains
output variables, program closed recursive.
2.1.3 Depth

depth variable appearing (ordered) clause B1 ^ : : : ^ Br defined follows.
Variables appearing head clause depth zero. Otherwise, let Bi first
literal containing variable V , let maximal depth input variables
Bi ; depth V +1. depth clause maximal depth variable
clause.
2.1.4 Determinacy

literal Bi clause B1 ^ : : : ^ Br determinate iff every possible substitution
unifies fact e
DB ` B1 ^ : : : ^ Bi,1

one maximal substitution DB ` Bi . clause determinate
literals determinate. Informally, determinate clauses
evaluated without backtracking Prolog interpreter.
term ij -determinate (Muggleton & Feng, 1992) sometimes used programs
depth i, determinate, contain literals arity j less. number experimental systems exploit restrictions associated limited depth determinacy (Muggleton
& Feng, 1992; Quinlan, 1991; Lavrac & Dzeroski, 1992; Cohen, 1993c). learnability constant-depth determinate clauses received formal study (Dzeroski,
Muggleton, & Russell, 1992; Cohen, 1993a).
2.1.5 Mode Constraints Declarations

Mode declarations commonly used analyzing Prolog code describing Prolog code;
instance, mode declaration \components (+; ,; ,)" indicates predicate components used first argument input second third arguments
outputs. Formally, define mode literal L appearing clause C
string initial character predicate symbol L, j > 1
j -th character \+" (j , 1)-th argument L input variable
\," (j , 1)-th argument L output variable. (This definition assumes
arguments head clause inputs; justified since considering
clauses behave classifying extended instances, ground.) mode constraint
set mode strings R = fs1 ; : : :; sk g, clause C said satisfy mode constraint
R p every literal L body C , mode L R.
define declaration tuple (p; a0; R) p predicate symbol, a0
integer, R mode constraint. say clause C satisfies declaration
544

fiPac-Learning Recursive Logic Programs: Negative Results

head C arity a0 predicate symbol p, every literal L body
C mode L appears R.
2.1.6 Determinate Modes

typical setting, facts database DB extended instances arbitrary:
instead, representative \real" predicate, may obey certain restrictions. Let us assume database extended-instance facts drawn
(possibly infinite) set F . Informally, mode determinate input positions
facts F functionally determine output positions. Formally, f = p(t1 ; : : :; tk )
fact predicate symbol p pff mode, define inputs (f; pff) hti1 ; : : :; ti i,
i1 , : : : , ik indices containing \+", define outputs (f; pff)
htj1 ; : : :; tj i, j1, : : : , jl indices containing \,". define mode
string pff predicate p determinate F iff
k

l

fhinputs (f; pff); outputs (f; pff)i : f 2 Fg
function. clause satisfies declaration Dec 2 DetDEC must determinate.
set declarations containing modes determinate F denoted
DetDEC F . Since paper set F assumed fixed, generally omit
subscript.
2.1.7 Bounds Predicate Arity

use notation a-DB set databases contain facts arity
less, a-DEC set declarations (p; a0; R) every string 2 R
length + 1 less.
2.1.8 Size Measures

learning models presented following section require learner use resources polynomial size inputs. Assuming predicates arity
less constant allows simple size measures used. paper,
measure size database DB cardinality; size extended instance (f; D)
cardinality D; size declaration (p; a0; R) cardinality R;
size clause B1 ^ : : : ^ Br number literals body.

2.2 Model Learnability
2.2.1 Preliminaries

Let X set. call X domain , call elements X instances . Define
concept C X representation subset X , define language Lang
set concepts. paper, rather casual distinction
concept set represents; risk confusion refer
set represented concept C extension C . Two sets C1 C2
extension said equivalent . Define example C pair (e; b) b = 1
e 2 C b = 0 otherwise. probability distribution function, sample C
545

fiCohen

X drawn according pair multisets + ; , drawn domain X according
D, + containing positive examples C , , containing negative ones.
Associated X Lang two size complexity measures , use
following notation:

size complexity concept C 2 Lang written j C j .
size complexity instance e 2 X written j ej .
set, Sn stands set elements size complexity greater
n. instance, Xn = fe 2 X : j ej ng Langn = fC 2 Lang : j C j ng.
assume size measures polynomially related number bits needed
represent C e; holds, example, size measures logic programs
databases defined above.
2.2.2 Polynomial Predictability

define polynomial predictability follows. language Lang polynomially
predictable iff algorithm PacPredict polynomial function m( 1 ; 1 ; ne ; nt)
every nt > 0, every ne > 0, every C 2 Langn , every : 0 < < 1, every
: 0 < < 1, every probability distribution function D, PacPredict following
behavior:


1. given sample + ; , C Xn drawn according containing least
m( 1 ; 1 ; ne ; nt) examples, PacPredict outputs hypothesis H
e

Prob(D(H , C ) + D(C , H ) > ) <
probability taken possible samples + , (if PacPredict
randomized algorithm) coin ips made PacPredict;
2. PacPredict runs time polynomial 1 , 1 , ne , nt , number examples;

3. hypothesis H evaluated polynomial time.
algorithm PacPredict called prediction algorithm Lang, function m( 1 ; 1 ; ne ; nt ) called sample complexity PacPredict. sometimes
abbreviate \polynomial predictability" \predictability".
first condition definition merely states error rate hypothesis
must (usually) low, measured probability distribution
training examples drawn. second condition, together stipulation
sample size polynomial, ensures total running time learner polynomial.
final condition simply requires hypothesis usable weak sense
used make predictions polynomial time. Notice worst case
learning model, definition allows adversarial choice inputs learner.
546

fiPac-Learning Recursive Logic Programs: Negative Results

2.2.3 Relation Models

model polynomial predictability well-studied (Pitt & Warmuth, 1990),
weaker version Valiant's (1984) criterion pac-learnability . language Lang
pac-learnable iff algorithm PacLearn
1. PacLearn satisfies requirements definition polynomial predictability,

2. inputs + , , PacLearn always outputs hypothesis H 2 Lang.
Thus language pac-learnable predictable.
companion paper (Cohen, 1995), positive results expressed model
identifiability equivalence queries, strictly stronger pac-learnability;
is, anything learnable equivalence queries necessarily pac-learnable.1
Since paper contains negative results, use relatively weak model
predictability. Negative results model immediately translate negative results
stronger models; language predictable, cannot pac-learnable,
identifiable equivalence queries.
2.2.4 Background Knowledge Learning

typical ILP system, setting slightly different, user usually provides clues
target concept addition examples, form database DB
\background knowledge" set declarations. account additional inputs
necessary extend framework described setting learner accepts
inputs training examples. Following formalization used companion
paper (Cohen, 1995), adopt notion \language family".
Lang set clauses, DB database Dec declaration, define
Lang[DB ; Dec] set pairs (C; DB ) C 2 Lang C satisfies Dec .
Semantically, pair denote set extended instances (f; D) covered
(C; DB ). Next, DB set databases DEC set declarations, define
Lang[DB ; DEC ] = fLang[DB ; Dec ] : DB

2 DB Dec 2 DECg

set languages called language family .
extend definition predictability queries language families follows.
language family Lang[DB; DEC ] polynomially predictable iff every language set
predictable. language family Lang[DB; DEC ] polynomially predictable iff
single algorithm Identify(DB ; Dec ) predicts every Lang[DB ; Dec] family
given DB Dec .
usual model polynomial predictability worst-case choices target
concept distribution examples. notion polynomial predictability
language family extends model natural way; extended model worstcase possible choices database DB 2 DB Dec 2 DEC . worst-case
1. equivalence query question form \is H equivalent target concept?" answered
either \yes" counterexample. Identification equivalence queries essentially means
target concept exactly identified polynomial time using polynomial queries.

547

fiCohen

model may seem unintuitive, since one typically assumes database DB provided
helpful user, rather adversary. However, worst-case model reasonable
learning allowed take time polynomial size smallest target concept
set Lang[DB ; Dec ]; means database given user
target concept cannot encoded succinctly (or all) learning allowed take
time.
Notice language family Lang[DB ; Dec] polynomially predictable, every
language family must polynomially predictable. Thus show family
polynomially predictable sucient construct one language family
learning hard. proofs paper form.

2.3 Prediction-Preserving Reducibilities

principle technical tool used negative results notion prediction-preserving
reducibility , introduced Pitt Warmuth (1990). Prediction-preserving reducibilities
method showing one language harder predict another. Formally,
let Lang1 language domain X1 Lang2 language domain X2.
say predicting Lang1 reduces predicting Lang2, denoted Lang1 Lang2 ,
function : X1 ! X2 , henceforth called instance mapping , function
fc : Lang1 ! Lang2 , henceforth called concept mapping , following hold:
1. x 2 C (x) 2 fc (C ) | i.e., concept membership preserved
mappings;
2. size complexity fc (C ) polynomial size complexity C |i.e., size
concept representations preserved within polynomial factor;
3. (x) computed polynomial time.
Note fc need computable; also, since computed polynomial time,
fi(x) must preserve size within polynomial factor.
Intuitively, fc (C1) returns concept C2 2 Lang2 \emulate" C1|i.e., make
decisions concept membership|on examples \preprocessed"
function . predicting Lang1 reduces predicting Lang2 learning
algorithm Lang2 exists, one possible scheme learning concepts Lang1
would following. First, convert examples unknown concept C1
domain X1 examples domain X2 using instance mapping .
conditions definition hold, since C1 consistent original examples,
concept fc (C1) consistent image ; thus running learning
algorithm Lang2 produce hypothesis H good approximation
fc (C1). course, may possible map H back original language Lang1,
computing fc ,1 may dicult impossible. However, H still used predict
membership C1: given example x original domain X1, one simply predict
x 2 C1 true whenever (x) 2 H .
Pitt Warmuth (1988) give rigorous argument approach leads
prediction algorithm Lang1 , leading following theorem.
548

fiPac-Learning Recursive Logic Programs: Negative Results

Theorem 1 (Pitt Warmuth) Assume Lang1 Lang2. Lang1 polynomially predictable, Lang2 polynomially predictable.

3. Cryptographic Limitations Learning Recursive Programs

Theorem 1 allows one transfer hardness results one language another.
useful number languages, known prediction hard breaking
cryptographic schemes widely assumed secure. example, known
predicting class languages accepted deterministic finite state automata
\cryptographically hard", class languages accepted log-space bounded Turing
machines.
section make use Theorem 1 previous cryptographic hardness
results show certain restricted classes recursive logic programs hard learn.

3.1 Programs n Linear Recursive Clauses

companion paper (Cohen, 1995) showed single linear closed recursive clause
identifiable equivalence queries. section show program
polynomial number clauses identifiable equivalence queries, even
polynomially predictable.
Specifically, let us extend notion \family languages" slightly, let
DLog[n; s] represent language log-space bounded deterministic Turing machines
states accepting inputs size n less, usual semantics complexity
measure.2 let d-DepthLinRecProg denote family logic programs containing
depth-d linear closed recursive clauses, containing number clauses.
following result:
Theorem 2 every n s, exists database DB n;s 2 1-DB declaration
Dec n;s 2 1-DetDEC sizes polynomial n
DLog[n; s] 1-DepthLinRecProg[DB n;s ; Dec n;s ]
Hence 1 1, d-DepthLinRecProg[DB; a-DetDEC ] uniformly polynomially predictable cryptographic assumptions.3
Proof: Recall log-space bounded Turing machine (TM) input tape length
n, work tape length log2 n initially contains zeros, finite state control
state set Q. simplify proof, assume without loss generality tape
input alphabets binary, single accepting state qf 2 Q,
machine always erase work tape position work tape head far left
decides accept input.
time step, machine read tape squares input tape head
work tape head, based values current state q ,
2. I.e., machine represents set inputs accepts, complexity number states.
3. Specifically, language uniformly polynomially predictable unless following cryptographic problems solved polynomial time: solving quadratic residue problem, inverting
RSA encryption function, factoring Blum integers. result holds cryptographic problems reduced learning DLOG Turing machines (Kearns & Valiant, 1989).

549

fiCohen






write either 1 0 work tape,
shift input tape head left right,
shift work tape head left right,
transition new internal state q 0
deterministic machine thus specified transition function

: f0; 1g f0; 1g Q ,! f0; 1g fL; Rg fL; Rg Q
Let us define internal configuration TM consist string symbols
written worktape, position tape heads, internal state q
machine: thus configuration element set
CON f0; 1glog2 n f1; : : :; log2 ng f1; : : :; ng Q

simplified specification machine transition function

0 : f0; 1g CON ! CON
component f0; 1g represents contents input tape square
input tape head.
Notice machine whose worktape size bounded log n, cardinality
CON p = jQjn2 log2 n, polynomial n = jQj. use fact
constructions.
background database DB n;s follows. First, = 0; : : :; p, atom
form coni(ci ) present. constant ci represent different internal configuration
Turing machine. arbitrarily select c1 represent (unique) accepting
configuration, add DB n;s atom accepting(c1). Thus
DB n;s fcon (ci)gpi=1 [ faccepting (c1)g

Next, define instance mapping. instance Turing machine's domain
binary string X = b1 : : :bn ; mapped extended instance (f; D)

f accepting (c0 )
ftruei gb 2X :b =1 [ ffalsei gb 2X :b =0








description atoms effect defining predicate truei true iff i-th
bit X \1", defining predicate falsei true iff i-th bit X
\0". constant c0 represent start configuration Turing machine,
predicate accepting(C) defined true iff Turing machine accepts input
X starting state C.
let Dec n;s = (accepting ; 1; R) R contains modes coni (+) coni (,),
= 1; : : :; p; truej falsej j = 1; : : :; n.
Finally, concept mapping fc , let us assume arbitrary one-to-one mapping
internal configurations Turing machine predicate names
550

fiPac-Learning Recursive Logic Programs: Negative Results

con0,: : : ,conp,1 start configuration (0log2 n ; 1; q0) maps con0 accepting configuration (0log2 n ; 1; qf ) maps con1. construct program fc (M )
follows. transition 0(1; c) ! c0 0, c c0 CON , construct
clause form

accepting(C)

conj (C) ^ truei ^ conj 0 (C1) ^ accepting(C1).

position input tape head encoded c, con j = (c),
con j 0 = (c0). transition 0(0; c) ! (c0) 0 construct analogous clause,
truei replaced falsei.
Now, claim program P , machine accept started
configuration ci iff
DB n;s ^ ^ P ` accepting (ci )
hence construction preserves concept membership. perhaps easiest see considering action top-down theorem prover given goal
accepting (C ): sequence subgoals accepting (ci ), accepting (ci +1 ), : : : generated
theorem-prover precisely parallel sequence configurations ci , : : : entered Turing
machine.
easily verified size program polynomial n s,
clauses linear recursive, determinate, depth one, completing proof.
number ways result strengthened. Precisely
construction used used reduce class nondeterministic log-space
bounded Turing machines constant-depth determinate linear recursive programs.
Further, slight modification construction used reduce class log-space
bounded alternating Turing machines (Chandra, Kozen, & Stockmeyer, 1981) constantdepth determinate 2-ary recursive programs. modification emulate configurations
corresponding universal states Turing machine clauses form
accepting(C)
conj (C) ^ truei ^
conj 10 (C1) ^ accepting(C1) ^
conj 20 (C2) ^ accepting(C2).
conj1 0 conj2 0 two successors universal configuration conj .
strong result, since log-space bounded alternating Turing machines known
able perform every polynomial-time computation.

3.2 Programs One n-ary Recursive Clause

consider learning single recursive clause arbitrary closed recursion.
Again, key result section observation expressive power:
background database allows every log-space deterministic Turing machine
emulated single recursive constant-depth determinate clause. leads
following negative predictability result.
551

fiCohen

Theorem 3 every n s, exists database DB n;s 2 3-DB declaration
Dec n;s 2 3-DetDEC sizes polynomial n
DLog[n; s] 3-DepthRec[DB n;s ; Dec n;s ]
Hence 3 3, d-DepthRec[DB n ; a-DetDEC ] uniformly polynomially
predictable cryptographic assumptions.

Proof: Consider DLOG machine . proof Theorem 2, assume without
loss generality tape alphabet f0; 1g, unique starting configura-

tion c0, unique accepting configuration c1. assume without
loss generality unique \failing" configuration cf ail; exactly
one transition form
0(b; cj) ! c0j
every combination 2 f1; : : :; ng, b 2 f0; 1g, cj 2 CON , fc1; cf ailg. Thus
input X = x1 : : :xn machine starts CONFIG=c0 , executes transitions
reaches CONFIG=c1 CONFIG=cf ail, point X accepted rejected
(respectively). use p number configurations. (Recall p polynomial
n s.)
emulate , convert example X = b1 : : :bn extended instance
fi(X ) = (f; D)

f accepting (c0 )
fbit (bi)gni=1
Thus predicate bit (X ) binds X i-th bit TM's input tape.
define following predicates background database DB n;s .

every possible b 2 f0; 1g j : 1 j p(n), predicate statusb;j (B,C,Y)
defined given bindings variables B C , statusb;j (B,C,Y) fail
C = cf ail; otherwise succeed, binding active B = b C = cj
binding inactive otherwise.
j : 1 j p(n), predicate nextj (Y,C) succeed iff bound
either active inactive. = , C bound cj ; otherwise, C
bound accepting configuration c1.
database contains fact accepting (c1 ).
easy show size database polynomial n s.
declaration Dec n;s defined (accepting ; 1; R) R includes modes
status bj (+; +; ,), next j (+; ,), bit (,) b 2 f0; 1g, j = 1; : : :; p, = 1; : : :; n.
Now, consider transition rule 0(b; cj ) ! c0j , corresponding conjunction
TRANSibj biti (Bibj ) ^ statusb;j (C,Bibj ,Yibj ) ^ nextj 0 (Yibj ,C1ibj ) ^ accepting(C1ibj )
552

fiPac-Learning Recursive Logic Programs: Negative Results

Given DB n;s D, assuming C bound configuration c, conjunction
fail c = cf ail. succeed xi 6= b c 6= cj ; case Yibj bound
inactive, C1ibj bound c1, recursive call succeeds accepting(c1)
DB n;s . Finally, xi = b c = cj , TRANSibj succeed atom accepting(cj 0 )
provable; case, Yibj bound active C1ibj bound cj 0 .
clear clause fc (M )
^
accepting(C)
TRANSibj


2f1;:::;ng; b2f0;1g
j 2f1;:::;pg

correctly emulate machine examples preprocessed
function described above. Hence construction preserves concept membership.
easily verified size program polynomial n s,
clause determinate depth three.

3.3 One k-Local Linear Closed Recursive Clause

far considered one class extensions positive result given
companion paper (Cohen, 1995)|namely, relaxing restrictions imposed recursive
structure target program. Another reasonable question ask linear closed
recursive programs learned without restriction constant-depth determinacy.
earlier papers (Cohen, 1993a, 1994a, 1993b) studied conditions
constant-depth determinacy restriction relaxed still allowing learnability nonrecursive clauses. turns generalizations constant-depth
determinate clauses predictable, even without recursion. However, language
nonrecursive clauses constant locality pac-learnable generalization constant-depth
determinate clauses. Below, define language, summarize relevant previous
results, address question learnability recursive local clauses.
Define variable V appearing clause C free appears body C
head C . Let V1 V2 two free variables appearing clause. V1 touches V2
appear literal, V1 uences V2 either touches V2, touches
variable V3 uences V2. locale free variable V set literals
either contain V , contain free variable uenced V . Informally, variable
V1 uences variable V2 choice binding V1 affect possible choices
bindings V2.
locality clause size largest locale. Let k-LocalNonRec denote
language nonrecursive clauses locality k less. (That is, k-LocalNonRec
set logic programs containing single nonrecursive k-local clause.) following facts
known (Cohen, 1993b):
fixed k a, language family k-LocalNonRec[a-DB; a-DEC] uniformly
pac-learnable.
every constant d, every constant a, every database DB 2 a-DB, every declaration
Dec 2 a-DetDEC , every clause C 2 d-DepthNonRec[DB ; Dec ],
553

fiCohen

equivalent clause C 0 k-LocalNonRec[DB ; Dec] size bounded kj C j , k
function (and hence constant constants.)
Hence
k-LocalNonRec[DB; a-DEC]
pac-learnable generalization

d-DepthNonRec[DB; a-DetDEC ]
thus plausible ask recursive programs k-local clauses pac-learnable.
facts learnability k-local programs follow immediately previous results.
example, immediate consequence construction Theorem 2 programs
polynomial number linear recursive k-local clauses predictable k 2.
Similarly, Theorem 3 shows single recursive k-local clause predictable k 4.
still reasonable ask, however, positive result bounded-depth determinate
recursive clauses (Cohen, 1995) extended k-ary closed recursive k-local clauses.
Unfortunately, following negative result, shows even linear closed
recursive clauses learnable.

Theorem 4 Let Dfa[s] denote language deterministic finite automata states,

let k-LocalLinRec set linear closed recursive k-local clauses. constant exists database DB 2 3-DB declaration Dec 2 3-DEC , size
polynomial s,
Dfa[s] 3-LocalLinRec[DB ; Dec ]

Hence k 3 3, k-LocalLinRec[a-DB ; Dec] uniformly polynomially
predictable cryptographic assumptions.

Proof: Following Hopcroft Ullman (1979) represent DFA alphabet

tuple (q0; Q; F; ) q0 initial state, Q set states, F set
accepting states, : Q ! Q transition function (which sometimes
think subset Q Q). prove theorem, need construct database
DB size polynomial every s-state DFA emulated linear
recursive k-local clause DB .
Rather directly emulating , convenient emulate instead modification . Let M^ DFA state set Q^ Q [ fq(,1); qe ; qf g, q(,1) , qe qf
new states found Q. initial state M^ q(,1) . final state M^
qf . transition function M^
[
^ [ f(q(,1); a; q0); (qe; c; qf )g [
f(qi; b; qe)g
2

qi F

a, b, c new letters . Note M^ DFA alphabet
[ fa; b; cg, and, described, need complete DFA alphabet. (That
is, may pairs (qi ; a) ^(qi ; a) undefined.) However, M^ easily
554

fiPac-Learning Recursive Logic Programs: Negative Results



1


q


0

?

0

M^

1



q



- ?

1

0

1

1







q
q
q
q
q


,1

0

?



-

0

M0




-

0

?

1

b,c,0,1








1







b

-

a,b,c
1

-

c

e


q

?

r

a,b,c

-

f

a,b,c,0,1
a,b,c,0,1


6

a,b,
0,1







q
q
q
q
q









,1





0

?

-



0
0

-

?

1





,
,

,
, b -



e

c

-

f

Figure 1: DFA modified emulation local clause

555

fiCohen

made complete introducing additional rejecting state qr , making every undefined
transition lead qr . precisely, let 0 defined
0 ^ [ f(qi; x; qr) j qi 2 Q^ ^ x 2 [ fa; b; cg ^ (6 9qj : (qi ; x; qj ) 2 ^)g
Thus 0 = (q(,1); Q^ [fqr g; fqf g; 0) \completed" version M^ , Q0 = Q^ [fqr g.
use 0 construction below; let Q0 = Q^ [ fqr g 0 = [ fa; b; cg.
Examples , M^ 0 shown Figure 1. Notice aside arcs
rejecting state qr , state diagram 0 nearly identical .
differences 0 new initial state q(,1) single outgoing arc
labeled old initial state q0 ; every final state 0 outgoing arc
labeled b new state qe , turn single outgoing arc labeled c final
state qf . easy show

x 2 L(M ) iff axbc 2 L(M 0)
Now, given set states Q0 define database DB contains following
predicates:
arcq ;;q (S,X,T) true 2 Q0, 2 Q0, X 2 0, unless = qi,
X = , 6= qj .
state(S) true 2 Q0.
accept(c,nil,qe,qf ) true.
motivation arc predicates, observe emulating 0 clearly useful
able represent transition function 0. usefulness arc predicates
transition function 0 represented using conjunction arc literals. particular,
conjunction
^
arc q ;;q (S; X; )


j



(q ;;q )20


j

j

succeeds 0 (S; X ) = , fails otherwise.
Let us define instance mapping (x) = (f; D)

f = accept (a; xbc; q(,1); q0)
set facts defines components relation list corresponds
string xbc. words, x = 1 : : :n , set facts
components(1 : : :n bc; 1; 2 : : :n bc)
components(2 : : :n bc; 2; 3 : : :n bc)
..
.
components(c,c,nil)
declaration Dec n Dec n = (accept ; 4; R) R contains modes
components (+; ,; ,), state (,), arc q ;;q (+; +; +) qi , qj Q0 , 2 0 .
Finally, define concept mapping fc (M ) machine clause


j

556

fiPac-Learning Recursive Logic Programs: Negative Results

accept(X,Ys,S,T)
V
(q ;;q )20 arcq ;;q (S,X,T)
^ components(Ys,X1,Ys1) ^ state(U) ^ accept(X1,Ys1,T,U).
0 transition function corresponding machine 0 defined above.
easy show construction polynomial.
clause X letter 0, Ys list letters, states
Q0 . intent construction predicate accept succeed exactly
(a) string XYs accepted 0 0 started state , (b) first action
taken 0 string XYs go state state .
Since initial transitions 0 q(,1) q0 input a,
predicate accept claimed behavior, clearly proposed mapping satisfies requirements Theorem 1. complete proof, therefore, must verify
predicate accept succeeds iff XYs accepted 0 state initial transition
T.
definition DFAs string XYs accepted 0 state initial
transition iff one following two conditions holds.
0(S; X ) = , Ys empty string final state 0, or;
0(S; X ) = , Ys nonempty string (and hence head X 1 tail
Ys1) Ys1 accepted 0 state , initial transition.
base fact accept(c,nil,qe,qf ) succeeds precisely first case holds, since
0 transition one final state. second case, conjunction
arc conditions fc (M ) clause succeeds exactly (S; X ) = (as noted above).
second conjunction clause succeeds Ys nonempty string
head X 1 tail Ys1 X1Ys1 accepted 0 state initial transition
state U , corresponds exactly second case above.
Thus concept membership preserved mapping. completes proof.


j



j

4. DNF-Hardness Results Recursive Programs

summarize previous results determinate clauses, shown single
k-ary closed recursive depth-d clause pac-learnable (Cohen, 1995), set n linear closed
recursive depth-d clauses not; further, even single n-ary closed recursive depth-d clauses
pac-learnable. still large gap positive negative results,
however: particular, learnability recursive programs containing constant number
k-ary recursive clauses yet established.
section investigate learnability classes programs.
show programs either two linear closed recursive clauses one linear closed recursive clause one base case hard learn boolean functions disjunctive
normal form (DNF). pac-learnability DNF long-standing open problem computational learning theory; import results, therefore, establishing
learnability classes require substantial advance computational learning
theory.
557

fiCohen

4.1 Linear Recursive Clause Plus Base Clause

Previous work established two-clause constant-depth determinate programs consisting one linear recursive clause one nonrecursive clause identified, given
two types oracles: standard equivalence-query oracle, \basecase oracle' (Cohen,
1995). (The basecase oracle determines example covered nonrecursive clause
alone.) section show absence basecase oracle, learning
problem hard learning boolean DNF.
discussion below, Dnf[n; r] denotes language r-term boolean functions
disjunctive normal form n variables.

Theorem 5 Let d-Depth-2-Clause set 2-clause programs consisting one

clause d-DepthLinRec one clause d-DepthNonRec. n
r exists database DB n;r 2 2-DB declaration Dec n;r 2 2-DEC , sizes
polynomial n r,
Dnf[n; r] 1-Depth-2-Clause[DB n;r ; Dec n;r ]

Hence 2 1 language family d-Depth-2-Clause[DB; a-DetDEC ]
uniformly polynomially predictable DNF polynomially predictable.

Proof: produce DB n;r 2 DB Dec n;r 2 2-DetDEC predicting
DNF reduced predicting 1-Depth-2-Clause[DB n;r ; Dec n;r ]. construction
makes use trick first used Theorem 3 (Cohen, 1993a), DNF formula
emulated conjunction containing single variable existentially quantified
restricted range.
begin instance mapping . assignment = b1 : : :bn converted
extended instance (f; D)
f p(1)
fbit (bi)gni=1
Next, define database DB n;r contain binary predicates true1 , false1, : : : , truer ,
falser behave follows:

truei(X,Y) succeeds X = 1, 2 f1; : : :; rg , fig.
falsei(X,Y) succeeds X = 0, 2 f1; : : :; rg , fig.
Further, DB n;r contains facts define predicate succ(Y,Z) true whenever
Z = + 1, Z numbers 1 r. Clearly size DB n;r
polynomial r.
Let Dec n;r = (p; 1; R) R contains modes bit (,), = 1; : : :; n; true j (+; +)
false j (+; +), j = 1; : : :; r, succ (+; ,).
let r-term DNF formula = _ri=1 ^sj =1 lij variables v1 ; : : :; vn.
may assume without loss generality contains exactly r terms, since DNF
formula fewer r terms padded exactly r terms adding terms


558

fiPac-Learning Recursive Logic Programs: Negative Results

Background database:

= 1; : : :; r
truei (b; ) b; : b = 1 2 f1; : : :; rg 6=
falsei (b; ) b; : b = 0 2 f1; : : :; rg 6=
succ(y,z)
z = + 1 2 f1; : : :; rg z 2 f1; : : :; rg

DNF formula: (v1 ^ v3 ^ v4) _ (v2 ^ v3) _ (v1 ^ v4)
Equivalent program:
p(Y) succ(Y,Z)^p(Z).
p(Y) bit1 (X1 ) ^ bit2 (X2 ) ^ bit3 (X3 ) ^ bit4 (X4 ) ^
true1 (X1,Y) ^ false1 (X3 ,Y) ^ true1(X4 ,Y) ^
false2 (X2,Y) ^ false2 (X3,Y)^
true3 (X1,Y) ^ false3 (X4 ,Y).
Instance mapping: fi(1011) = (p(1); fbit1(1); bit 2(0); bit3(1); bit4(1)g)
Figure 2: Reducing DNF recursive program
form v1 v1. define concept mapping fc () program CR; CB CR
linear recursive depth 1 determinate clause

p(Y ) succ(Y; Z ) ^ p(Z )
CB nonrecursive depth 1 determinate clause

n
^
^r ^
p(Y )
bit k (Xk ) ^
Bij


i=1 j =1

k =1

Bij defined follows:

Bij

(

truei (Xk ,Y) lij = vk
falsei (Xk ,Y) lij = vk

example construction shown Figure 2; suggest reader refer
figure point. basic idea behind construction first, clause
CB succeed variable bound i-th term succeeds (the
definitions truei falsei designed ensure property holds); second,
recursive clause CR constructed program fc () succeeds iff CB succeeds
bound one values 1; : : :; n.
argue rigorously correctness construction. Clearly, ( )
fc () size respectively. Since DB n;r polynomial
size, reduction polynomial.
Figure 3 shows possible proofs constructed program fc ();
notice program fc () succeeds exactly clause CB succeeds value
559

fiCohen

p(1)

A@
A@
AA @
@
B(1)





succ(1,2) p(2)


@



A@


AA @
@
B(2)

succ(2,3) p(3)



@


A@


AA @
@
B(3)
:::

p(n-1)

B (i) V bit (X ) ^ V V B




ij



@


A@


AA @
B(n-1)
@

succ(n-1,n) p(n)
B(n)

Figure 3: Space proofs possible program fc ()
Vs l must true;

1

r
.
Now,



true


term

=
j =1 ij
V
V
s0

case j =1 Bij succeeds bound value j =1 Bi0 j every i0 6=
succeeds bound i. hand, false assignment, Ti
fails, hence every possible binding generated repeated use recursive
clause CR base clause CB fail. Thus concept membership preserved
mapping.
concludes proof.






4.2 Two Linear Recursive Clauses

Recall single linear closed recursive clause identifiable equivalence
queries (Cohen, 1995). construction similar used Theorem 5 used
show result cannot extended programs two linear recursive clauses.
Theorem 6 Let d-Depth-2-Clause0 set 2-clause programs consisting two
clauses d-DepthLinRec. (Thus assume base case recursion given
background knowledge.) constants n r exists database DB n;r 2
2-DB declaration Dec n;r 2 2-DEC , sizes polynomial n,
Dnf[n; r] 1-Depth-2-Clause0[DB n;r ; Dec n;r ]
Hence constants 2 1 language family
d-Depth-2-Clause0 [DB; a-DetDEC ]
560

fiPac-Learning Recursive Logic Programs: Negative Results

uniformly polynomially predictable DNF polynomially predictable.

Proof: before, proof makes use prediction-preserving reducibility DNF

d-Depth-2-Clause0[DB ; Dec ] specific DB Dec . Let us assume DNF
r terms, assume r = 2k . (Again, assumption made without
loss generality, since number terms increased padding vacuous
terms.) consider complete binary tree depth k + 1. k-th level tree
exactly r nodes; let us label nodes 1, : : : , r, give nodes arbitrary labels.

construct database DB n;r Theorem 5, except following changes:
predicates truei (b,y) falsei(b,y) succeed label node
level k.
Rather predicate succ, database contains two predicates leftson
rightson encode relationship nodes binary tree.
database includes facts p(!1), : : : , p(!2r), !1, : : : , !2r leaves
binary tree. used base cases recursive program
learned.
Let label root binary tree. define instance mapping

(b1 : : :b1) (p(); fbit1 (b1); : : :; bit n (bn )g)
Note except use rather 1, identical instance mapping
used Theorem 5. let Dec n;r = (p; 1; R) R contains modes bit (,), =
1; : : :; n; true j (+; +) false j (+; +), j = 1; : : :; r; leftson (+; ,); rightson (+; ,).
concept mapping fc () pair clauses R1; R2, R1 clause

n
^
^r ^
p(Y )
bit k (Xk ) ^
Bij ^ leftson(Y; Z ) ^ p(Z )


k =1

R2 clause

p(Y )

n
^
k =1

bit k (Xk ) ^

i=1 j =1


^r ^


i=1 j =1

Bij ^ rightson (Y; Z ) ^ p(Z )

Note clause linear recursive, determinate, depth 1. Also,
construction clearly polynomial. remains show membership preserved.
Figure 4 shows space proofs
V constructed
V V program fc ();
Figure 3, B (i) abbreviates conjunction bit (Xi) ^ Bij . Notice program
succeed recursive calls manage finally recurse one base cases
p(!1), : : : , p(!2r ), correspond leaves binary tree. clauses
succeed first k , 1 levels tree. However, reach base cases
recursion leaves tree, recursion must pass k-th level tree;
is, one clauses must succeed node binary tree,
k-th level tree, hence label number 1 r.
program thus succeeds ( ) precisely number 1
561

fiCohen

p()

"
, b
@b
"

H

"H
,
@bb
"
" ,
@ b
" ,
@ bb
"
"
b
,
@
"
b
"
,
@
b
"
b
,
@
B() p(L)
B()
p(R)
Z
Z
`
` \
Z
X \\
Z

X
\

Z

Z

\ Z

\ Z


\ Z
\
\ Z


E
X
X
X
EX


E


E


E

:::

:::

:::

B
B
B




B
B

:::

:::

B(1) p(LL: : : L) B(1) p(LL: : : R)

p(!1 )

:::



J
J

J


J

J

B
B
B

B

B

B(n) p(RR: : : LR) B(n) p(RR: : : R)

p(!2 )

p(!2 ,1 )
r

p(!2 )
r

Figure 4: Proofs possible program fc ()

r conjunction B(i) succeeds, (by argument given Theorem 5)
happen satisfied assignment . Thus, mappings preserve

concept membership. completes proof.

Notice programs fc () used proof property depth
every proof logarithmic size instances. means hardness
result holds even one additionally restricts class programs logarithmic
depth bound.

4.3 Upper Bounds Diculty Learning

previous sections showed several highly restricted classes recursive programs
least hard predict DNF. section show restricted
classes harder predict DNF.
wish restrict depth proof constructed target program. Thus, let
h(n) function; use Langh(n) set programs class Lang
proofs extended instance (f; D) depth bounded h(j Dj ).
562

fiPac-Learning Recursive Logic Programs: Negative Results

Theorem 7 Let Dnf[n; ] language DNF boolean functions (with number

terms), recall d-Depth-2-Clause language 2-clause programs consisting one clause d-DepthLinRec one clause d-DepthNonRec,
d-Depth-2-Clause0 language 2-clause programs consisting two clauses
d-DepthLinRec.
constants a, databases DB 2 DB declarations Dec 2 a-DetDEC ,
polynomial function poly (n)

d-Depth-2-Clause[DB ; Dec] Dnf[poly (j DB j ); ]
d-Depth-2-Clause0h(n) [DB ; Dec] Dnf[poly (j DB j ); ] h(n) bounded c log n
constant c.
Hence either language families uniformly polynomially predictable, Dnf[n; ]
polynomially predictable.

Proof: proof relies several facts established companion paper (Cohen, 1995).
every declaration Dec, clause BOTTOM d(Dec) every nonrecursive depth-d determinate clause C equivalent subclause BOTTOM .
Further, size BOTTOM polynomial Dec . means language subclauses BOTTOM normal form nonrecursive constant-depth
determinate clauses.

Every linear closed recursive clause CR constant-depth determinate equivalent subclause BOTTOM plus recursive literal Lr ; further,
polynomial number possible recursive literals Lr .
constants a, a0, d, database DB 2 a-DB, declaration Dec =
(p; a0; R), database DB 2 a-DB , program P d-Depth-2-Clause[DB ; Dec ],
depth terminating proof constructing using P hmax,
hmax polynomial size DB Dec .
assumed without loss generality database DB decsriptions
contain equality predicate , equality predicate simply predicate
equal(X,Y) true exactly X = .
idea proof contruct prediction-preserving reduction two
classes recursive programs listed DNF. begin two lemmas.

Lemma 8 Let Dec 2 a-DetDEC , let C nonrecursive depth-d determinate clause

consistent Dec. Let SubclauseC denote language subclauses C , let
Monomial[u] denote language monomials u variables. polynomial poly 1 database DB 2 DB,
SubclauseC [DB ; Dec] Monomial[poly 1(j DB j )]

563

fiCohen

Proof lemma: Follows immediately construction used Theorem 1

Dzeroski, Muggleton, Russell (Dzeroski et al., 1992). (The basic idea construction introduce propositional variable representing \success" connected
chain literals C . subclause C represented conjunction
propositions.)
lemma extended follows.

Lemma 9 Let Dec 2 a-DetDEC , let = fC1; : : :; Crg set r nonrecursive depth-

determinate clauses consistent Dec, length n less. Let SubclauseS denote
set programs form P = (D1; : : :; Ds) Di subclause
Cj 2 .
polynomial poly 2 database DB 2 DB,
SubclauseS [DB ; Dec] Dnf[poly 2 (j DB j ; r); ]

Proof lemma: Lemma 8, Ci 2 , set variables Vi size
polynomial j DB j
every clause SubclauseC emulated monomial
Sr

V
V
.
Clearly,
jV j polynomial n r, every clause
. Let V =

i=1





SubclauseC emulated monomial V . Further, every disjunction

r clauses represented disjunction monomials.
Since Ci 's satisfy single declaration Dec = (p; a; R), heads
principle function arity; further, may assume (without loss generality, since
equality predicate assumed) variables appearing heads clauses
distinct. Since Ci's nonrecursive, every program
P 2 SubclauseS

represented disjunction D1 _ : : : _ Dr i, Di 2 ( SubclauseC ). Hence
every P 2 SubclauseS represented r-term DNF set variables V .




Let us introduce additional notation. C clauses, use
C u denote result resolving C together, C denote result
resolving C times. Note C u unique C linear recursive C
predicate heads (since one pair complementary
literals.)
Now, consider target program

P = (CR; CB ) 2 d-Depth-2-Clause[DB ; Dec]
CR recursive clause CB base. proof extended instance
(f; D) must use clause CR repeatedly h times use clause CB resolve away
final subgoal. Hence nonrecursive clause CRh u CB could used cover
instance (f; D).
Since depth proof class programs bounded number hmax
polynomial j DB j ne , nonrecursive program

P 0 = fCRh u CB : 0 h hmax g
564

fiPac-Learning Recursive Logic Programs: Negative Results

equivalent P extended instances size ne less.
Finally, recall assume CB subclause BOTTOM ; also,
polynomial-sized set LR = Lr1 ; : : :; Lr closed recursive literals
Lr 2 LR , clause CR subclause BOTTOM [ Lr . means let
polynomial-sized set
S1 = f(BOTTOM [ Lr )h u BOTTOM j 0 h hmax Lr 2 LR g
P 0 2 SubclauseS1 . Thus Lemma 9, d-Depth-2-Clause Dnf. concludes
proof first statement theorem.
show
d-Depth-2-Clause0h(n) [DB ; Dec] Dnf[poly (j DB j ; ]
similar argument applies. Let us introduce notation, define
MESHh;n (CR1 ; CR2 ) set clauses form
p









CR 1 u CR 2 u : : : u CR 0
j , CR = CR1 CR = CR2 , h0 h(n). Notice functions
h(n) c log n number clauses polynomial n.
let p predicate appearing heads CR1 CR2 , let C^ (respectively
^ ) version C (DB ) every instance predicate p replaced
DB
new predicate p^. P recursive program P = fCR1 ; CR2 g d-Depth-2-Clause0
^ ,
database DB , P ^ DB equivalent4 nonrecursive program P 0 ^ DB
i;

ij



i;

i;h

ij

P 0 = fC^ j C 2 MESHh;n (CR1 ; CR2 )g
e

recall polynomial number recursive literals Lr , hence
polynomial number pairs recursive literals Lr ; Lr . means set clauses
[
S2 =
fC^ j C 2 MESHh;n (BOTTOM [ Lr ; BOTTOM [ Lr )g




(L

ri

e

2

;Lrj ) LR LR

j



j

polynomial-sized; furthermore, program P language d-Depth-2-Clause,
P 0 2 SubclauseS2 . second part theorem follows application Lemma 9.
immediate corollary result Theorems 6 5 strengthened
follows.
Corollary 10 constants 1 2, language family
d-Depth-2-Clause[DB; a-DetDEC ]
uniformly polynomially predictable DNF polynomially predictable.
constants 1 2, language family
d-Depth-2-Clause0 [DB; a-DetDEC ]
uniformly polynomially predictable DNF polynomially predictable.
4. extended instances size n less.
e

565

fiCohen

Thus important sense learning problems equivalent learning boolean
DNF. resolve questions learnability languages,
show learnability dicult formal problem: predictability boolean DNF
long-standing open problem computational learning theory.

5. Related Work
work described paper differs previous formal work learning logic programs simultaneously allowing background knowledge, function-free programs, recursion. focused exclusively computational limitations ecient learnability
associated recursion, considered languages known paclearnable nonrecursive case. Since results paper negative,
concentrated model polynomial predictability; negative results model immediately imply negative result stronger model pac-learnability, imply
negative results strictly expressive languages.
Among closely related prior results negative results previously
obtained certain classes nonrecursive function-free logic programs (Cohen, 1993b).
results similar character results described here, apply nonrecursive
languages. Similar cryptographic results obtained Frazier Page (1993)
certain classes programs (both recursive nonrecursive) contain function symbols
disallow background knowledge.
prior negative results obtained learnability firstorder languages using proof technique consistency hardness (Pitt & Valiant, 1988).
Haussler (1989) showed language \existential conjunction concepts" paclearnable showing hard find concept language consistent
given set examples. Similar results obtained two restricted languages
Horn clauses (Kietz, 1993); simple description logic (Cohen & Hirsh, 1994);
language sorted first-order terms (Page & Frisch, 1992). results, however,
specific model pac-learnability, none easily extended polynomial
predictability model considered here. results extend languages
expressive specific constrained languages. Finally, none languages allow
recursion.
knowledge, negative learnability results first-order languages. discussion prior positive learnability results first-order languages
found companion paper (Cohen, 1995).

6. Summary
paper companion (Cohen, 1995) considered large number different
subsets Datalog. aim comprehensive, systematic: particular, wished find precisely boundaries learnability lie various syntactic
restrictions imposed relaxed. Since easy reader \miss forest
trees", brie summarize results contained paper, together
positive results companion paper (Cohen, 1995).
566

fiPac-Learning Recursive Logic Programs: Negative Results

Local
Clauses

Constant-Depth Determinate
Clauses

nCR,

nCR,

nCR jCB,

nCR ; CB,

k nCR,

n nCR,

kCR,

kCR+

kCRjCB+

kCR; CBDNF

k k0CRDNF

n kCR,

1CR,

1CR+

1CRjCB+

1CR; CB=DNF

2 1CR=DNF

n 1CR,

Table 1: summary learnability results
Throughout papers, assumed polynomial amount background
knowledge exists; programs learned contain function symbols;
literals body clause small arity. assumed recursion
closed , meaning output variables appear recursive clause; however, believe
restriction relaxed without fundamentally changing results paper.
companion paper (Cohen, 1995) showed single nonrecursive constantdepth determinate clause learnable strong model identification equivalence
queries . learning model, one given access oracle counterexamples|that
is, oracle find, unit time, example current hypothesis
incorrect|and must reconstruct target program exactly polynomial number
counterexamples. result implies single nonrecursive constant-depth determinate clause pac-learnable (as counterexample oracle emulated drawing
random examples pac setting). result novel (Dzeroski et al., 1992); however
proof given independent, independent interest. Notably, somewhat
rigorous earlier proofs, proves result directly, rather via reduction propositional learning problem. proof introduces simple version
forced simulation technique, variants used positive results.
showed learning algorithm nonrecursive clauses extended
case single linear recursive constant-depth determinate clause, leading
result restricted class recursive programs identifiable equivalence
queries. bit effort, algorithm extended learn single
k-ary recursive constant-depth determinate clause.
considered extended learning algorithm learn recursive programs consisting one constant-depth determinate clauses. interesting extension
simultaneously learn recursive clause CR base clause CB , using equivalence
queries \basecase oracle" indicates counterexamples covered
base clause CB . model, possible simultaneously learn recursive clause
nonrecursive base case situations recursive clause learned
567

fiCohen

Language Family
d-DepthNonRec[a-DB; a-DetDEC]
d-DepthLinRec[a-DB; a-DetDEC]
d-Depth-k-Rec[a-DB; a-DetDEC]
d-Depth-2-Clause[a-DB; a-DetDEC]
kd-MaxRecLang[a-DB; a-DetDEC ]
d-Depth-2-Clause[a-DB; a-DetDEC]
d-Depth-2-Clause [a-DB; a-DetDEC ]
d-DepthLinRecProg[a-DB; a-DetDEC ]
d-DepthRec[a-DB; a-DetDEC ]
k-LocalLinRec[a-DB; a-DEC ]
0

B
1
0
0
1
1
1
0
0
0
0

R
0
1
1
1
1
1
2

L/R Oracles
, EQ
1
EQ
k
EQ
1
EQ,BASE
k
EQ,BASE
1
EQ
1
EQ
n 1
EQ
1 n
EQ
1 1
EQ

Notation Learnable
CB
yes
1CR
yes
kCR
yes
1CRjCB yes
kCRjCB
yes
1CR; CB =DNF
2 1CR =DNF
n 1CR
nCR

1CR


Table 2: Summary language learnability results. Column B indicates number
base (nonrecursive) clauses allowed program; column R indicates number recursive clauses; L/R indicates number recursive literals allowed
single recursive clause; EQ indicates oracle equivalence queries BASE
indicates basecase oracle. languages except k-LocalLinRec, clauses
must determinate depth d.
alone; instance, one learn k-ary recursive clause together nonrecursive
base case. strongest positive result.
results summarized Tables 1 2. Table 1, program one rary recursive clause denoted rCR, program one r-ary recursive clause one
nonrecursive basecase denoted rCR; CB , rCRjCB \basecase" oracle,
program different r-ary recursive clauses denoted rCR . boxed results
associated one theorems paper, companion paper,
unmarked results corollaries results. \+" program class indicates
identifiable equivalence queries; thus positive results described
summarized four \+" entries lower left-hand corner section table
concerned constant-depth determinate clauses.
Table 2 presents information slightly different format, relates
notation Table 1 terminology used elsewhere paper.
paper considered learnability various natural generalizations
languages shown learnable companion paper. Consider moment single
clauses. companion paper showed fixed k single k-ary recursive constantdepth determinate clause learnable. showed restrictions
necessary. particular, program n constant-depth linear recursive clauses
polynomially predictable; hence restriction single clause necessary. Also, single
clause n recursive calls hard learn; hence restriction k-ary recursion
necessary. showed restriction constant-depth determinate clauses
necessary, considering learnability constant locality clauses . Constant locality
clauses known generalization constant-depth determinate clauses
pac-learnable nonrecursive case. However, showed recursion allowed,
568

fiPac-Learning Recursive Logic Programs: Negative Results

language learnable: even single linear recursive clause polynomially
predictable.
Again, results summarized Table 1; \," program class means
polynomially predictable, cryptographic assumptions, hence neither
pac-learnable identifiable equivalence queries.
negative results based cryptographic hardness give upper bound expressiveness learnable recursive languages, still leave open learnability programs
constant number k-ary recursive clauses absence basecase oracle.
final section paper, showed following problems are, model
polynomial predictability, equivalent predicting boolean DNF:
predicting two-clause constant-depth determinate recursive programs containing one
linear recursive clause one base case;
predicting two-clause recursive constant-depth determinate programs containing two
linear recursive clauses, even base case known.
note program classes nearly simplest classes multi-clause
recursive programs one imagine, pac-learnability DNF longstanding open problem computational learning theory. results suggest, therefore,
pac-learning multi-clause recursive logic programs dicult; least,
show finding provably correct pac-learning algorithm require substantial advances
computational learning theory. Table 1, \= Dnf" (respectively Dnf) means
corresponding language prediction-equivalent DNF (respectively least hard
DNF).
summarize Table 1: sort recursion, programs containing
constant-depth determinate clauses learnable. constant-depth determinate
recursive programs learnable contain single k-ary recursive clause
(in standard equivalence query model) single k-ary recursive clause plus base
case (if \basecase oracle" allowed). classes recursive programs either
cryptographically hard, hard boolean DNF.

7. Conclusions

Inductive logic programming active area research, one broad class learning
problems considered area class \automatic logic programming" problems.
Prototypical examples genre problems learning append two lists,
multiply two numbers. target concepts automatic logic programming recursive
programs, often, training data learning system simply examples
target concept, together suitable background knowledge.
topic paper pac-learnability recursive logic programs random
examples background knowledge; specifically, wished establish computational
limitations inherit performing task. began positive results established
companion paper. results show one constant-depth determinate closed k-ary
recursive clause pac-learnable, further, program consisting one recursive
clause one constant-depth determinate nonrecursive clause pac-learnable given
additional \basecase oracle".
569

fiCohen

paper showed positive results likely improved.
particular, showed either eliminating basecase oracle learning two recursive clauses simultaneously prediction-equivalent learning DNF, even case
linear recursion. showed following problems hard breaking (presumably) secure cryptographic codes: pac-learning n linear recursive determinate clauses,
pac-learning one n-ary recursive determinate clause, pac-learning one linear recursive
k-local clause.
results contribute machine learning several ways. point view
computational learning theory, several results technically interesting. One
prediction-equivalence several classes restricted logic programs boolean DNF;
result, together others (Cohen, 1993b), reinforces importance learnability problem DNF. paper gives dramatic example adding recursion
widely differing effects learnability: constant-depth determinate clauses
remain pac-learnable linear recursion added, constant-locality clauses become cryptographically hard.
negative results show systems apparently learn larger class recursive
programs must taking advantage either special properties target concepts
learn, distribution examples provided with. believe
likely opportunity obtaining positive formal results area
identify analyze special properties. example, many examples
FOIL learned recursive logic programs, made use \complete example sets"|
datasets containing examples certain size, rather sets randomly
selected examples (Quinlan & Cameron-Jones, 1993). possible complete datasets
allow expressive class programs learned random datasets; fact,
progress recently made toward formalizing conjecture (De Raedt & Dzeroski,
1994).
Finally, importantly, paper established boundaries learnability
determinate recursive programs pac-learnability model. many plausible automatic programming contexts would highly desirable system offered
formal guarantees correctness. results paper provide upper bounds
one hope achieve ecient, formally justified system learns recursive
programs random examples alone.

Acknowledgements
author wishes thank three anonymous JAIR reviewers number useful suggestions presentation technical content.

References
Aha, D., Lapointe, S., Ling, C. X., & Matwin, S. (1994). Inverting implication small
training sets. Machine Learning: ECML-94 Catania, Italy. Springer-Verlag. Lecture
Notes Computer Science # 784.
570

fiPac-Learning Recursive Logic Programs: Negative Results

Biermann, A. (1978). inference regular lisp programs examples. IEEE Transactions Systems, Man Cybernetics, 8 (8).
Chandra, A. K., Kozen, D. C., & Stockmeyer, L. J. (1981). Alternation. Journal
ACM, 28, 114{113.
Cohen, W. W. (1993a). Cryptographic limitations learning one-clause logic programs.
Proceedings Tenth National Conference Artificial Intelligence Washington,
D.C.
Cohen, W. W. (1993b). Pac-learning non-recursive Prolog clauses. appear Artificial
Intelligence.
Cohen, W. W. (1993c). Rapid prototyping ILP systems using explicit bias. Proceedings
1993 IJCAI Workshop Inductive Logic Programming Chambery, France.
Cohen, W. W. (1994a). Pac-learning nondeterminate clauses. Proceedings Eleventh
National Conference Artificial Intelligence Seattle, WA.
Cohen, W. W. (1994b). Recovering software specifications inductive logic programming. Proceedings Eleventh National Conference Artificial Intelligence
Seattle, WA.
Cohen, W. W. (1995). Pac-learning recursive logic programs: ecient algorithms. Journal
AI Research, 2, 501{539.
Cohen, W. W., & Hirsh, H. (1994). learnability description logics equality
constraints. Machine Learning, 17 (2/3).
De Raedt, L., & Dzeroski, S. (1994). First-order jk-clausal theories PAC-learnable.
Wrobel, S. (Ed.), Proceedings Fourth International Workshop Inductive
Logic Programming Bad Honnef/Bonn, Germany.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability determinate logic
programs. Proceedings 1992 Workshop Computational Learning Theory
Pittsburgh, Pennsylvania.
Frazier, M., & Page, C. D. (1993). Learnability recursive, non-determinate theories:
basic results techniques. Proceedings Third International Workshop
Inductive Logic Programming Bled, Slovenia.
Haussler, D. (1989). Learning conjunctive concepts structural domains. Machine Learning, 4 (1).
Hopcroft, J. E., & Ullman, J. D. (1979). Introduction Automata Theory, Languages,
Computation. Addison-Wesley.
Kearns, M., & Valiant, L. (1989). Cryptographic limitations learning Boolean formulae
finite automata. 21th Annual Symposium Theory Computing. ACM
Press.
571

fiCohen

Kietz, J.-U. (1993). computational lower bounds computational complexity
inductive logic programming. Proceedings 1993 European Conference
Machine Learning Vienna, Austria.
King, R. D., Muggleton, S., Lewis, R. A., & Sternberg, M. J. E. (1992). Drug design
machine learning: use inductive logic programming model structureactivity relationships trimethoprim analogues binding dihydrofolate reductase.
Proceedings National Academy Science, 89.
Lavrac, N., & Dzeroski, S. (1992). Background knowledge declarative bias inductive
concept learning. Jantke, K. P. (Ed.), Analogical Inductive Inference: International Workshop AII'92. Springer Verlag, Daghstuhl Castle, Germany. Lectures
Artificial Intelligence Series #642.
Lloyd, J. W. (1987). Foundations Logic Programming: Second Edition. Springer-Verlag.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.
Journal Logic Programming, 19/20 (7), 629{679.
Muggleton, S., & Feng, C. (1992). Ecient induction logic programs. Inductive Logic
Programming. Academic Press.
Muggleton, S., King, R. D., & Sternberg, M. J. E. (1992). Protein secondary structure
prediction using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press.
Page, C. D., & Frisch, A. M. (1992). Generalization learnability: study constrained
atoms. Inductive Logic Programming. Academic Press.
Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. Machine
Learning, 9 (1).
Pitt, L., & Warmuth, M. K. (1988). Reductions among prediction problems: difficulty predicting automata. Proceedings 3rd Annual IEEE Conference
Structure Complexity Theory Washington, D.C. Computer Society Press
IEEE.
Pitt, L., & Valiant, L. (1988). Computational limitations learning examples. Journal
ACM, 35 (4), 965{984.
Pitt, L., & Warmuth, M. (1990). Prediction-preserving reducibility. Journal Computer
System Sciences, 41, 430{467.
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Brazdil, P. B.
(Ed.), Machine Learning: ECML-93 Vienna, Austria. Springer-Verlag. Lecture notes
Computer Science # 667.
Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5 (3).
572

fiPac-Learning Recursive Logic Programs: Negative Results

Quinlan, J. R. (1991). Determinate literals inductive logic programming. Proceedings
Eighth International Workshop Machine Learning Ithaca, New York. Morgan
Kaufmann.
Rouveirol, C. (1994). Flattening saturation: two representation changes generalization. Machine Learning, 14 (2).
Summers, P. D. (1977). methodology LISP program construction examples.
Journal Association Computing Machinery, 24 (1), 161{175.
Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11).
Zelle, J. M., & Mooney, R. J. (1994). Inducing deterministic Prolog parsers treebanks:
machine learning approach. Proceedings Twelfth National Conference
Artificial Intelligence Seattle, Washington. MIT Press.

573


