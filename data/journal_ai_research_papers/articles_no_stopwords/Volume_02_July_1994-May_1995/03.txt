Journal Artificial Intelligence Research 2 (1995) 411-446

Submitted 11/94; published 4/95

Rerepresenting Restructuring Domain Theories:
Constructive Induction Approach
Steven K. Donoho
Larry A. Rendell

Department Computer Science, Univeristy Illinois
405 N. Mathews Ave., Urbana, IL 61801 USA

donoho@cs.uiuc.edu
rendell@cs.uiuc.edu

Abstract

Theory revision integrates inductive learning background knowledge combining
training examples coarse domain theory produce accurate theory.
two challenges theory revision theory-guided systems face. First,
representation language appropriate initial theory may inappropriate
improved theory. original representation may concisely express initial theory,
accurate theory forced use representation may bulky, cumbersome,
dicult reach. Second, theory structure suitable coarse domain theory may
insucient fine-tuned theory. Systems produce small, local changes
theory limited value accomplishing complex structural alterations may
required.
Consequently, advanced theory-guided learning systems require exible representation
exible structure. analysis various theory revision systems theory-guided
learning systems reveals specific strengths weaknesses terms two desired
properties. Designed capture underlying qualities system, new system uses
theory-guided constructive induction. Experiments three domains show improvement
previous theory-guided systems. leads study behavior, limitations,
potential theory-guided constructive induction.

1. Introduction
Inductive learners normally use training examples, use background knowledge. Effectively integrating knowledge induction widely studied research problem. work date area theory revision
knowledge given coarse, perhaps incomplete incorrect, theory problem domain,
training examples used shape initial theory refined, accurate
theory (Ourston & Mooney, 1990; Thompson, Langley, & Iba, 1991; Cohen, 1992; Pazzani
& Kibler, 1992; Baffes & Mooney, 1993; Mooney, 1993). develop exible
robust approach problem learning data theory knowledge
addressing two following desirable qualities:

Flexible Representation. theory-guided system utilize knowledge contained initial domain theory without adhere closely initial
theory's representation language.

Flexible Structure. theory-guided system unnecessarily restricted
structure initial domain theory.

c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDonoho & Rendell

giving precise definitions terms, motivate work intuitively.

1.1 Intuitive Motivation
first desirable quality, exibility representation, arises theory representation appropriate describing coarse, initial domain theory may inadequate
final, revised theory. initial domain theory may compact concise
one representation, accurate theory may quite bulky cumbersome representation. Furthermore, representation best expressing initial theory may
best carrying refinements. helpful refinement step may clumsy
make initial representation yet carried quite simply another representation.
simple example, coarse domain theory may expressed logical conjunction
N conditions met. accurate theory, though, one
N conditions holds. Expressing accurate theory DNF representation
used describe initial theory would cumbersome unwieldy (Murphy & Pazzani,
1991). Furthermore, arriving final theory using refinement operators suitable
DNF (drop-condition, add-condition, modify-condition) would cumbersome task.
M-of-N representation adopted, refinement simply involves empirically
finding appropriate M, final theory expressed concisely (Baffes & Mooney,
1993).
Similarly, second desirable quality, exibility structure, arises theory
structure suitable coarse domain theory may insucient fine-tuned
theory. order achieve desired accuracy, restructuring initial theory may
necessary. Many theory revision systems act making series local changes,
lead behavior two extremes. first extreme rigidly retain backbone
structure initial domain theory, allowing small, local changes. Figure 1 illustrates
situation. Minor revisions made { conditions added, dropped,
modified { refined theory trapped backbone structure initial
theory. local changes needed, techniques proven useful (Ourston
& Mooney, 1990), often required. required, systems often
move extreme; drop entire rules groups rules build entire
new rules groups rules scratch replace them. Thus restructure,
forfeit valuable knowledge process. ideal theory revision system would glean
knowledge theory substructures cannot fixed small, local changes
use restructured theory.
intuitive illustration, consider piece software \almost works." Sometimes
made useful local operations: fixing couple bugs, adding
needed subroutine, on. cases, though, piece software \almost
works" fact far full working order. may need redesigned restructured.
mistake one extreme try fix program making series patches
original code. mistake extreme discard original program without
learning anything start scratch. best approach would examine
original program see learned design use knowledge
redesign. Likewise, attempting improve coarse domain theory series
local changes may yield little improvement theory trapped initial
412

fiRerepresenting Restructuring Domain Theories

Initial Theory



b

c

j

e

h

Refined Theory

n

k l





b

c

p q r

j

w e

f g



n

k l

u



p

v

r

f g

Figure 1: Typical theory revision allows limited structural exibility. Although conditions added, dropped, modified, revised theory much
constrained structure initial theory.
structure. render original domain theory useless; careful analysis
initial domain theory give valuable guidance design best final theory.
illustrated Figure 2 many substructures taken initial
theory adapted use refined theory. Information initial theory
used, structure revised theory restricted structure
initial theory.
Initial Theory



b

c

n

j

e

h

Refined Theory

k l

k

p q r

l

f



g



e

h







p

u

v

f g q r

f g

Figure 2: exible structural modification. revised theory taken many substructures initial theory adapted recombined use,
structure revised theory restricted structure
initial theory.

413

fiDonoho & Rendell

1.2 Terminology

paper, training data consist examples classified vectors feature/value pairs. assume initial theory set conditions combined using
operators AND, OR, indicating one classes. unreasonable
believe theories always form, covers much existing theory revision
research.
work intended informal exploration exible representation exible
structure. Flexible representation means allowing theory revised using representation language initial theory. example exible representation
introduction new operator combining features | operator used
initial theory. Section 1.1 example given introducing M-of-N operator
represent theory originally expressed DNF. Flexible structure means limiting
revision theory series small, incremental modifications. example
breaking theory components using building blocks
construction new theory.
Constructive induction process whereby training examples redescribed using
new set features. new features combinations original features. Bias
knowledge may used construction new features. subtle point
speak exible representation, referring representation
domain theory, training data. Although phrase \change representation"
often applied constructive induction, refers change data. paper,
term exible representation reserved change theory representation. Thus
system performing constructive induction (changing feature language
data) without exhibiting exible representation (changing representation theory).

1.3 Overview

Theory revision constructive induction embody complementary aspects machine
learning research community's ultimate goals. Theory revision uses data improve
theory; constructive induction use theory improve data facilitate learning.
paper present theory-guided constructive induction approach addresses
two desirable qualities discussed Section 1.1. initial theory analyzed, new
features constructed based components theory. constructed features
need expressed representational language initial theory
refined better match training examples. Finally, standard inductive learning
algorithm, C4.5 (Quinlan, 1993), applied redescribed examples.
begin analyzing landmark theory revision learning systems exhibited exibility handling domain theory part played performance. analysis, extract guidelines system design apply
design limited system. effort integrate learning theory data,
borrow heavily theory revision, multistrategy learning, constructive induction communities, guidelines system design fall closest classical constructive
induction methods. central focus paper presentation \another new
system" rather study exible representation structure, manifestation
previous work, guidance future design.
414

fiRerepresenting Restructuring Domain Theories

Section 2 gives context work analyzing previous research uence work. Section 3 explores Promoter Recognition domain demonstrates
related theory revision systems behave domain. Section 4, guidelines
theory-guided constructive induction presented. guidelines synthesis
positive aspects related research, address two desirable qualities, exibility
representation exibility structure. Section 4 presents specific theory-guided
constructive induction algorithm instantiation guidelines set forth earlier
section. Results experiments three domains given Section 5 followed
discussion strengths theory-guided constructive induction Section 6. Section 7 presents experimental analysis limits applicability simple algorithm
followed discussion limitations future directions work Section 8.

2. Context Related Work

Although work bears resemblance form objective many papers constructive induction (Michalski, 1983; Fu & Buchanan, 1985; Utgoff, 1986; Schlimmer, 1987;
Drastal & Raatz, 1989; Matheus & Rendell, 1989; Pagallo & Haussler, 1990; Ragavan &
Rendell, 1993; Hirsh & Noordewier, 1994), theory revision (Ourston & Mooney, 1990; Feldman, Serge, & Koppel, 1991; Thompson et al., 1991; Cohen, 1992; Pazzani & Kibler, 1992;
Baffes & Mooney, 1993), multistrategy approaches (Flann & Dietterich, 1989; Towell,
Shavlik, & Noordeweir, 1990; Dzerisko & Lavrac, 1991; Bloedorn, Michalski, & Wnek, 1993;
Clark & Matwin, 1993; Towell & Shavlik, 1994), focus upon handful systems, significant, underlying similarities work. section
analyze Miro, Either, Focl, LabyrinthK , Kbann, Neither-MofN, Grendel
discuss related underlying contributions relationship perspective.

2.1

Miro

(Drastal & Raatz, 1989) seminal work knowledge-guided constructive induction. takes knowledge low-level features interact uses knowledge
construct high-level features training examples. standard learning algorithm
run examples described using new features. domain theory used
shift bias induction problem (Utgoff, 1986). Empirical results showed
describing examples high-level, abstract terms improved learning accuracy.
Miro approach provides means utilizing knowledge domain theory without
restricted structure theory. Substructures domain theory
used construct high-level features standard induction algorithm arrange
concept. constructed features used are, others ignored,
others combined low-level features, still others may used differently
multiple contexts. end result knowledge domain theory utilized,
structure final theory restricted structure initial theory.
Miro provides exible structure.
Another benefit Miro-like techniques applied even partial
domain theory exists, i.e., domain theory specifies high-level features
link together domain theory specifies high-level features
others. One Miro's shortcomings provided means making minor changes
Miro

415

fiDonoho & Rendell

domain theory rather constructed features exactly domain theory
specified. representation Miro's constructed features primitive | either
example met conditions high-level feature not. example Miro's
behavior given Section 3.2.

2.2

,

, LabyrinthK

Either Focl



Either

(Ourston & Mooney, 1990), LabyrinthK (Thompson et al., 1991),
Focl (Pazzani & Kibler, 1992) systems represent broad spectrum theory revision
work. make steps toward effective integration background knowledge inductive
learning. Although systems many superficial differences regard supervised/unsupervised learning, concept description language, etc., share underlying
principle incrementally revising initial domain theory series local changes.
discuss Either representative class systems. Either's theory
revision operators include: removing unwanted conditions rule, adding needed conditions rule, removing rules, adding totally new rules. Either first classifies
training examples according current theory. misclassified, seeks repair
theory applying theory revision operator result correct classification
previously misclassified examples without losing correct examples. Thus
series local changes made allow improvement accuracy training
set without losing examples previously classified correctly.
Either-type methods provide simple yet powerful tools repairing many important
common faults domain theories, fail meet qualities exible representation exible structure. theory revision operators make small, local
modifications existing domain theory, final theory constrained similar
structure initial theory. accurate theory significantly different structure initial theory, systems forced one two extremes discussed
Section 1. first extreme become trapped local maximum similar
initial theory unable reach global maximum local changes made.
extreme drop entire rules groups rules replace new
rules built scratch thus forfeiting knowledge contained domain theory.
Also, Either carries theory revision steps representation initial
theory. Consequently, representation final theory initial
theory. Another representation may appropriate revised theory
one initial theory comes, facilities provided accommodate this.
advanced theory revision system would combine locally acting strengths Eithertype systems exibility structure exibility representation. example
Either's behavior given Section 3.3.

2.3

Kbann

Neither-MofN

Kbann system (Towell et al., 1990; Towell & Shavlik, 1994) makes unique contributions theory revision work. Kbann takes initial domain theory described symbolically
logic creates neural network whose structure initial weights encode theory.
Backpropagation (Rumelhart, Hinton, & McClelland, 1986) applied refinement tool fine-tuning network weights. Kbann empirically shown give
416

fiRerepresenting Restructuring Domain Theories

significant improvement many theory revision systems widely-used Promoter
Recognition domain. Although work different implementation Kbann,
abstract ideologies similar.
One Kbann's important contributions takes domain theory one representation (propositional logic) translates less restricting representation (neural
network). logic appropriate representation initial domain theory
promoter problem, neural network representation convenient refining
theory expressing best revised theory. change representation
Kbann's real source power. Much attention given fact Kbann combines symbolic knowledge subsymbolic learner, combination viewed
generally means implementing important change representation. may
change representation gives Kbann power, necessarily specific
symbolic/subsymbolic implementation. Thus Kbann system embodies higher-level
principle allowing refinement occur appropriate representation.
alternative representation Kbann's source power, question must raised
whether actual Kbann implementation always best means achieving
goal. neural network representation may expressive required. Accordingly, backpropagation often refinement power needed. Thus Kbann may
carry excess baggage translating neural net representation, performing expensive
backpropagation, extracting symbolic rules refined network. Although full
extent Kbann's power may needed problems, many important problems may
solvable applying Kbann's principles symbolic level using less expensive tools.
Neither-MofN (Baffes & Mooney, 1993), descendant Either, second example
system allows theory revised representation
initial theory. domain theory input Neither-MofN expressed propositional
logic AND/OR tree. Neither-MofN interprets theory less rigidly |
rule true time N conditions true. Initially set equal N (all
conditions must true rule true), one theory refinement operator
lower particular rule. end result examples close enough
partial match initial theory accepted. Neither-MofN, since built upon
Either framework, includes Either-like theory revision operators: add-condition,
drop-condition, etc.
Thus Neither-MofN allows revision take place representation appropriate
revision appropriate concisely expressing best refined theory. NeitherMofN achieved results comparable Kbann Promoter Recognition domain,
suggests change representation two systems share
give power rather particular implementation. Neither-MofN
demonstrates small amount representational exibility sometimes enough.
M-of-N representation employs big change original representation
neural net representation Kbann employs yet achieves similar results
arrives much quickly Kbann (Baffes & Mooney, 1993).
shortcoming Neither-MofN since acts making local changes
initial theory, still become trapped structure initial theory. advanced
theory revision system would incorporate Neither-MofN's Kbann's exibility
417

fiDonoho & Rendell

representation allow knowledge-guided theory restructuring. Examples
Neither-MofN's behavior given Sections 3.4 3.5.

2.4

's

Kbann

Grendel

Cohen (1992) analyzes class theory revision systems draws insightful conclusions. One \generality [in theory interpretation] comes expense power."
draws principle fact system Either Focl treats every
domain theory therefore must treat every domain theory general way. argues rather applying general refinement strategy
every problem, small set refinement strategies available narrow
enough gain leverage yet narrow apply single problem. Cohen
presents Grendel, toolbox translators transforms domain theory
explicit bias. translator interprets domain theory different way,
appropriate interpretation applied given problem.
apply Cohen's principle representation domain theories. domain
theories translated representation, general, adaptable representation used order accommodate general case. comes
expense higher computational costs possibly lower accuracy due overfit
stemming unbridled adaptability. neural net representation Kbann
translates domain theories allows 1) measure partial match domain theory 2) different parts domain theory weighted differently 3) conditions added
dropped domain theory. options adaptability probably
necessary problems may even detrimental. options Kbann
require computationally expensive backpropagation method.
representation used Neither-MofN adaptable Kbann's |
allow individual parts domain theory weighted differently. NeitherMofN runs quickly Kbann small problems probably matches even
surpasses Kbann's accuracy many domains | domains fine-grained weighting
unfruitful even detrimental. toolbox theory rerepresentation translators analogous
Grendel would allow domain theory translated representation
appropriate forms adaptability.

2.5 Outlook Summary

summary, brie reexamine exible representation exible structure, two
desirable qualities set forth Section 1. consider various systems exemplify
subset desirable qualities.
Kbann Neither-MofN interpreted theory exibly original
representation allowed revised theory adaptable representation.
final, refined theory often many exceptions rule; may tolerate partial
matches missing pieces evidence; may weight evidence heavily
evidence. Kbann's Neither-MofN's new representation may
concise, appropriate representation initial theory, new
representation allows concise expression otherwise cumbersome final theory.
cases principle exible representation.
418

fiRerepresenting Restructuring Domain Theories

Standard induction programs quite successful building concise theories
high predictive accuracy target concept concisely expressed using
original set features. can't, constructive induction means creating
new features target concept concisely expressed. Miro uses
constructive induction take advantage strengths domain theory
standard induction. Knowledge theory guides construction appropriate
new features, standard induction structures concise description
concept. Thus Miro-like construction coupled standard induction provides
ready powerful means exibly restructuring knowledge contained
initial domain theory. case principle exible structure.

following section introduce DNA Promoter Recognition domain order
illustrate tangibly systems discussed integrate knowledge
induction.

3. Demonstrations Related Work
section introduces Promoter Recognition domain (Harley, Reynolds, & Noordewier,
1990) brie illustrates Miro-like system, Either, Kbann, NeitherMofN behave domain. implemented Miro-like system promoter domain; versions Either Neither-MofN available Ray Mooney's group;
Kbann's behavior described analyzing (Towell & Shavlik, 1994). chose promoter domain non-trivial, real-world problem number theory
revision researchers used test work (Ourston & Mooney, 1990; Thompson
et al., 1991; Wogulis, 1991; Cohen, 1992; Pazzani & Kibler, 1992; Baffes & Mooney, 1993;
Towell & Shavlik, 1994). promoter domain one three domains evaluate
work, theory-guided constructive induction, Section 5.

3.1 Promoter Recognition Domain

promoter sequence region DNA marks beginning gene. example promoter recognition domain region DNA classified either promoter
non-promoter. illustrated Figure 3, examples consist 57 features representing sequence 57 DNA nucleotides. feature take values A,G,C,
representing adenine, guanine, cytosine, thymine corresponding DNA position.
features labeled according position p-50 p+7 (there zero
position). notation \p-N " denotes nucleotide N positions upstream
beginning gene. goal predict whether sequence promoter
nucleotides. total 106 examples available: 53 promoters 53 non-promoters.
promoter recognition problem comes initial domain theory shown Figure 4 (quoted almost verbatim Towell Shavlik's entry UCI Machine Learning
Repository). theory states promoter sequences must two regions make
contact protein must acceptable conformation pattern.
four possibilities contact region minus 35 (35 nucleotides upstream beginning gene). match four possibilities satisfy minus 35
contact condition, thus joined disjunction. Similarly, four possibilities
419

fiDonoho & Rendell

DNA Sequence

p-50

p+7

C G C
Figure 3: instance promoter domain consists sequence 57 nucleotides
labeled p-50 p+7. nucleotide take values A,G,C,
representing adenine, guanine, cytosine, thymine.
contact region minus 10 four acceptable conformation patterns. Figure 5
gives pictorial presentation portions theory. 106 examples
dataset, none matched domain theory exactly, yielding accuracy 50%.

3.2

Miro

Promoter Domain

Miro-like system promoter domain would use rules Figure 4 construct new high-level features DNA segment. Figure 6 shows example this.
DNA segment shown position p-38 position p-30. minus 35 rules
theory shown, four new features (feat minus35 feat minus35 D)
constructed DNA segment, one minus 35 rule. new features feat minus35 feat minus35 value 1 DNA fragment
matches first fourth minus 35 rules. Likewise, feat minus35 B feat minus35 C
value 0 DNA fragment match second third
rules. Furthermore, since four minus 35 rules joined disjunction, new feature,
feat minus35 all, created group would value 1 least one
minus 35 rules matches.
New features would similarly created minus 10 rules conformation
rules, standard induction algorithm could applied. implemented Mirolike system; Figure 7 gives example theory created it. (Drastal's original Miro used
candidate elimination algorithm (Mitchell, 1977) underlying induction algorithm.
used C4.5 (Quinlan, 1993).) opposed theory revision systems incrementally
modify domain theory, Miro broken theory components
fashioned components new theory using standard induction program. Thus
Miro exhibited exible structure principle domain { restricted
way structure initial theory. Rather, Miro exploited strengths
standard induction concisely characterize training examples using new features.
420

fiRerepresenting Restructuring Domain Theories

Promoters region protein (RNA polymerase) must make contact
helical DNA sequence must valid conformation two pieces
contact region spatially align. Prolog notation used.
promoter :- contact, conformation.
two regions "upstream" beginning gene
RNA polymerase makes contact.
contact

:- minus_35, minus_10.

following rules describe compositions possible contact regions.
minus_35
minus_35
minus_35
minus_35

:- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
:p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
:p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

minus_10
minus_10
minus_10
minus_10

:- p-14=t, p-13=a, p-12=t, p-11=a, p-10=a, p-9=t.
:p-13=t, p-12=a,
p-10=a,
p-8=t.
:p-13=t, p-12=a, p-11=t, p-10=a, p-9=a, p-8=t.
:p-12=t, p-11=a,
p-7=t.

following rules describe sequences produce acceptable conformations.
conformation :- p-47=c,
p-18=t,
p-1=c.
conformation :- p-45=a,
conformation :- p-49=a,
conformation :- p-45=a,
p-15=t,

p-46=a, p-45=a, p-43=t, p-42=t, p-40=a, p-39=c, p-22=g,
p-16=c, p-8=g, p-7=c, p-6=g, p-5=c, p-4=c, p-2=c,
p-44=a, p-41=a.
p-44=t, p-27=t, p-22=a, p-18=t, p-16=t, p-15=g, p-1=a.
p-41=a, p-28=t, p-27=t, p-23=t, p-21=a, p-20=a, p-17=t,
p-4=t.

Figure 4: initial domain theory recognizing promoters (from Towell Shavlik).
weakness Miro displays example allows exibility representation
theory. representation features constructed Miro basically
all-or-none representation initial theory; either DNA segment matched rule,
not.

3.3

Either

Promoter Domain

Either-like system refines initial promoter theory dropping adding
conditions rules. simulated Either turning M-of-N option Neither
ran promoter domain. Figure 8 shows refined theory produced using
randomly selected training set size 80. initial promoter domain theory
lend revision small, local changes, Either limited success.
421

fiDonoho & Rendell

DNA Sequence

p-50

p+7

Contact minus_35

Contact minus_10

-37 -36 -35 -34 -33 -32 -31

C

G C

*

-14 -13 -12 -11 -10 -9

-8 -7



* *









* G * C

*





*



* G C

*





* G C

* *


*



*

*

*

*

*

*

Figure 5: contact portion theory. four possibilities
minus 35 minus 10 portions theory. \*" matches nucleotide.
conformation portion theory spread display pictorially.
run, program exhibited second behavioral extreme discussed Section 1;
entirely removed groups rules tried build new rules replace
lost. minus 10 conformation rules essentially removed, new rules
added minus 35 group. new minus 35 rules contain condition
p-12=t previously found minus 10 group condition p-44=a previously found
conformation group.
Either's behavior example direct result lack exibility representation exibility structure. dicult transform minus 10 conformation
rules something useful initial representation using Either's locally-acting operators. Either handles dropping sets rules, losing knowledge,
attempting rediscover lost knowledge empirically. end result loss
knowledge lower optimal accuracy shown later Section 5.

3.4

Kbann

Promoter Domain

Figure 9, modeled figure Towell Shavlik (1994), shows setup Kbann
network promoter theory. slot along bottom represents one nucleotide
DNA sequence. node first level bottom embodies single
domain rule, higher levels encode groups rules final concept top.
links shown figure ones initially high-weighted. net next filled
fully connected low-weight links. Backpropagation applied refine
network's weights.
422

fiRerepresenting Restructuring Domain Theories

DNA segment fragment:
:::

p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=t, p-30=t

:::

minus 35 group rules corresponding constructed features:
minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
minus 35 :p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

feat minus35 = 1
feat minus35 B = 0
feat minus35 C = 0
feat minus35 = 1

feat minus35 = (feat minus35 _ feat minus35 B _ feat minus35 C _ feat minus35 D) = 1

Figure 6: example feature construction Miro-like system. constructed
features first fourth rules minus 35 group true (value = 1)
DNA segment matches rules. constructed feature
entire group, feat minus35 all, true four minus 35 rules joined
disjunction.
feat_minus10_all
0

1
promoter

feat_conf_B
0

1

feat_minus35_D
0
non-promoter

promoter
1

promoter

Figure 7: example theory created Miro-like system. DNA segment recognized
promoter matches minus 10 rules, second conformation
rule, fourth minus 35 rule.
neural net representation appropriate domain propositional
logic representation initial theory. allows measurement partial match
weighting links way subset rule's conditions enough surpass
node's threshold. allows variable weightings different parts theory; therefore, predictive nucleotides weighted heavily, slightly predictive
nucleotides weighted less heavily. Kbann limited exibility structure.
refined network result series incremental modifications
initial network, fundamental restructuring theory embodies unlikely. Kbann
423

fiDonoho & Rendell

promoter :- contact, conformation.
contact

:- minus_35, minus_10.

minus_35
minus_35
minus_35
minus_35
minus_35
minus_35

::::::-

p-35=t,
p-36=t,
p-36=t,
p-34=g,
p-34=g,
p-35=t,

p-34=g.
p-33=a, p-32=c.
p-32=c, p-50=c.
p-12=t.
p-44=a.
p-47=g.

minus_10 :- true.
conformation :- true.

Figure 8: revised theory produce Either.
promoter

contact

conformation
minus_35

p-50

minus_10

DNA Sequence

p+7

Figure 9: setup Kbann network promoter theory.
limited finding best network fundamental structure imposed
initial theory.
One Kbann's advantages uses standard learning algorithm foundation. Backpropagation widely used consequently improved previous
researchers. Theory refinement tools built ground use standard
tool tangentially suffer invent methods handling standard
problems overfit, noisy data, etc. wealth neural net experience resources
available Kbann user; neural net technology advances, Kbann technology
passively advance it.
424

fiRerepresenting Restructuring Domain Theories

3.5

Neither-MofN

Promoter Domain

refines initial promoter theory dropping adding conditions rules allowing conjunctive rules true subset
conditions true. ran Neither-MofN randomly selected training set size
80, Figure 10 shows refined promoter theory produced. theory expressed
9 M-of-N rules would require 30 rules using propositional logic, initial theory's
representation. importantly, unclear system using initial representation would reach 30-rule theory initial theory. Thus M-of-N representation
adopted allows concise expression final theory facilitates
refinement process.
Neither-MofN

promoter :- 2 ( contact, conformation ).
contact

:- 2 ( minus_35, minus_10 ).

minus_35 :- 2 ( p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a ).
minus_35 :- 5 ( p-36=t, p-35=t, p-34=g, p-33=a, p-32=c
).
minus_10
minus_10
minus_10
minus_10

::::-

2
2
6
2






(
(
p-13=t,
( p-14=t, p-13=a,
(
p-13=t,

p-12=t, p-11=a,
p-12=a,
p-10=a,
p-12=t, p-11=a, p-10=a, p-9=t
p-12=a,
p-10=a,

p-7=t
p-8=t

).
).
).
p-34=g ).

conformation :- true.

Figure 10: revised theory produced Neither-MofN.
Neither-MofN displays exibility representation allowing M-of-N interpretation original propositional logic, allow fine-grained refinement
Kbann. allow measure partial match, Kbann could weight
predictive features heavily. example, minus 35 rules, perhaps p-36=t
predictive DNA segment promoter p-34=g therefore
weighted heavily. Neither-MofN simply counts number true conditions
rule; therefore, every condition weighted equally. Kbann's fine-grained weighting may
needed domains others. may actually detrimental domains.
advanced theory revision system offer range representations.
Kbann, Neither-MofN limited exibility structure. refined
theory reached series small, incremental modifications initial theory
precluding fundamental restructuring. Neither-MofN therefore limited finding
best theory fundamental structure initial theory.

4. Theory-Guided Constructive Induction

first half section present guidelines theory-guided constructive induction
summarize work discussed Sections 2 3. remainder section
425

fiDonoho & Rendell

presents algorithm instantiates guidelines. evaluate algorithm
Section 5.

4.1 Guidelines

following guidelines synthesis strengths previously discussed related
work.
Miro, new features constructed using components domain
theory. new features combinations existing features, final theory
created applying standard induction algorithm training examples described
using new features. allows knowledge gleaned initial theory
without forcing final theory conform initial theory's backbone structure.
takes full advantage domain theory building high-level features
original low-level features. takes advantage strength standard induction
| building concise theories high predictive accuracy target concept
concisely expressed using given features.
Either, constructed features modifiable various operators
act locally, adding dropping conjuncts constructed feature.
Kbann Neither-MofN, representation constructed features
need exact representation initial theory given. example,
initial theory may given set rules written propositional logic.
new feature constructed rule, need boolean feature
telling whether conditions met; example may count
many conditions rule met. allows final theory formed
expressed representation suitable representation
initial theory.
Grendel, complete system offer library interpreters allowing
domain theory translated range representations differing adaptability. One interpreter might emulate Miro strictly translating domain theory
boolean constructed features. Another interpreter might construct features
count number satisfied conditions corresponding component domain theory thus providing measure partial match. Still another interpreter
might construct features weighted sums satisfied conditions.
weights could refined empirically examining set training examples. Thus
appropriate amount expressive power applied given problem
without incurring unnecessary expense.

4.2 Specific Interpreter

section describes algorithm limited instantiation guidelines
described. algorithm intended demonstration distillation synthesis
principles embodied previous landmark systems. contains main module,
Tgci described Figure 12, specific interpreter, Tgci1 described Figure 11.
main module Tgci redescribes training testing examples calling Tgci1
426

fiRerepresenting Restructuring Domain Theories

applies C4.5 redescribed examples (just Miro applied candidate
elimination algorithm examples redescribing them). Tgci1 viewed
single interpreter potential Grendel-like toolbox. takes input single example
domain theory expressed AND/OR tree one shown Figure 13.
returns new vector features example measure partial match
example theory. Thus creates new features components domain theory
Miro, measures partial match, allows exibility representing
information contained initial theory Kbann Neither-MofN. One
aspect guidelines 4.1 appear algorithm Either's locally
acting operators adding dropping conditions portion theory.
following two paragraphs explain detail workings Tgci1 Tgci
respectively.
Given: example E domain theory root node R. domain
theory AND/OR/NOT tree leaves conditions
tested true false.
Return: pair P = (F; F ) F top feature measuring partial
match E whole domain theory, F vector new features measuring partial match E various parts subparts domain theory.

1. R directly testable condition, return P=(1,<>) R true E
P=(-1,<>) R false E .
2. n = number children R
3. child Rj R, call Tgci1(Rj ,E ) store respective results
Pj = (Fj ; Fj ).
4. major operator R OR, Fnew = MAX (Fj ).
Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).
P
5. major operator R AND, Fnew = ( nj=1 Fj )=n.
Return P = (Fnew ; concatenate(<Fnew >; F1; F2; :::; Fn)).
6. major operator R NOT, Fnew = ,1 F1 .
Return P = (Fnew ; F1).
Figure 11: Tgci1 algorithm
Tgci1 algorithm, given Figure 11, recursive. inputs example E
domain theory root node R. ultimately returns redescription E form
vector new features F . returns value F called top feature used
intermediate calculations described below. base case occurs domain theory
single leaf node (i.e., R simple condition). case (Line 1), Tgci1 returns
top feature 1 condition true -1 condition false. new features
returned base case would simply duplicate existing features.
427

fiDonoho & Rendell

domain theory single leaf node, Tgci1 recursively calls R's children
(Line 3). child R, Rj , processed, returns vector new features Fj (which
measures partial match example j th child R various subparts).
returns top feature Fj included Fj marked special
measures partial match example whole j th child R. n
children, result Line 3 n vectors new features, F1 Fn , n top features, F1
Fn . operator node R (Line 4), Fnew , new feature created
node, maximum Fj . Thus Fnew measures closely best R's children come
conditions met example. vector new features returned
case concatenation Fnew new features R's children. operator
node R (Line 5), Fnew average Fj . Thus Fnew measures closely
R's children group come conditions met example.
vector new features returned case concatenation Fnew new
features R's children. operator node R (Line 6), R
one child, Fnew F1 negated. Thus Fnew measures extent conditions
R's child met example.
Given: set training examples Etrain , set testing examples Etest ,
domain theory root node R.
Return: Learned concept accuracy testing examples.
1. example Ei 2 Etrain , call Tgci1(R,Ei) returns Pi =
(Fi ; Fi). Etrain,new = fFig.
2. example Ei 2 Etest, call Tgci1(R,Ei). returns Pi =
(Fi ; Fi). Etest,new = fFi g.
3. Call C4.5 training examples Etrain,new testing examples
Etest,new . Return decision tree accuracy Etest,new .
Figure 12: Tgci algorithm
Tgci1 called twice two different examples domain theory,
two vectors new features size. Furthermore, corresponding features
measure match corresponding parts domain theory. Tgci main module
Figure 12 takes advantage creating redescribed example sets input
example sets. Line 1 redescribes example training set producing new training
set. Line 2 testing set. Line 3 runs standard induction program
C4.5 redescribed example sets. returned decision tree easily interpreted
examining new features used part domain theory
correspond to.

4.3

Tgci1

Examples

example Tgci1 interpreter works, consider toy theory shown
Figure 13. Tgci1 redescribes input example constructing new feature node
428

fiRerepresenting Restructuring Domain Theories

input theory. Consider situation input example matches conditions A,
B, C E. Tgci1 evaluates children Node 6, gets values
F1 = 1, F2 = 1, F3 = ,1, F4 = 1, F5 = ,1. Since operator Node 6 AND, Fnew
average values received children, 0.20 ((1 + 1 + (,1) + 1 + (,1))=5 =
0:20). Likewise, condition G matchs F H, Fnew Node 5 value
0.33 (,1 ((1 + (,1) + (,1))=3)) two three matching conditions Node 7 give
value ,0:33, negated Node 5. Since Node 2 disjunction,
new feature measures best partial match two children value 0.33
(MAX(0.20,0.33)), on.
1
3


2
4
5


6

B C E

7

F G H

8

J K

9

L N

Figure 13: example theory form AND/OR tree might used
interpreter generate constructed features.
Figure 14 shows Tgci1 redescribes particular DNA segment using minus 35
rules promoter theory. partial DNA segment shown along four minus 35
rules new feature constructed rule (We given new features names
simplify illustration). first rule, four six nucleotides match; therefore, DNA segment feat minus35 value 0.33 ((1+1+1+1+(,1)+(,1))=6).
second rule, four five nucleotides match; therefore, feat minus35 B
value 0.60. two minus 35 rules joined disjunction original domain theory, feat minus35 all, new feature constructed
group, takes maximum value four children; therefore, feat minus35
value 0.60 feat minus35 B value 0.60, highest group. Intuitively, feat minus35 represents best partial match grouping | extent
disjunction partially satisfied. results running Tgci1 DNA
sequence set redescribed training examples. redescribed example value
feat minus35 feat minus35 D, feat minus35 all, nodes promoter domain theory. training set essentially redescribed using new feature vector
derived information contained domain theory. form, off-the-shelf
induction program applied new example set.
Anomalous situations created Tgci1 gives \good score" seemingly
bad example bad score good example. Situations created
logically equivalent theories give different scores single example. occur
429

fiDonoho & Rendell

DNA segment fragment:
:::

p-38=g, p-37=c, p-36=t, p-35=t, p-34=g, p-33=c, p-32=a, p-31=a, p-30=t

:::

minus 35 group rules corresponding constructed features:
minus 35 :- p-37=c, p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.
minus 35 :p-36=t, p-35=t, p-34=g,
p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c, p-31=a.
minus 35 :p-36=t, p-35=t, p-34=g, p-33=a, p-32=c.

feat minus35 = 0.33
feat minus35 B = 0.60
feat minus35 C = 0.33
feat minus35 = 0.20

feat minus35 = max(feat minus35 A, feat minus35 B, feat minus35 C, feat minus35 D) = 0.60

Figure 14: example Tgci1 generates constructed features portion
promoter domain theory DNA segment. Four conditions first
minus 35 rule match DNA segment; therefore, constructed feature
rule value 0.33 ((1 + 1 + 1 + 1 + (,1) + (,1))=6). Feat minus35 all,
new feature entire minus 35 group takes maximum value
children thus embodying best partial match group.
biased favor situations matched conditions desirable,
matched conditions necessarily better. Eliminating anomalies
would remove bias.
Tgci1

5. Experiments Analysis
section presents results applying theory-guided constructive induction three
domains: promoter domain (Harley et al., 1990), primate splice-junction domain (Noordewier, Shavlik, & Towell, 1992), gene identification domain (Craven & Shavlik,
1995). case Tgci1 interpreter applied domain's theory examples
order redescribe examples using new features. C4.5 (Quinlan, 1993)
applied redescribed examples.

5.1 Promoter Domain
Figure 15 shows learning curve theory-guided constructive induction promoter
domain accompanied curves Either, LabyrinthK , Kbann, Neither-MofN.
Following methodology described Towell Shavlik [1994], set 106 examples
randomly divided training set size 80 testing set size 26. learning
curve created training subsets training set size 8, 16, 24, : : : 72, 80,
using 26 examples testing. curves Either, LabyrinthK , Kbann
taken Ourston Mooney (1990), Thompson, Langley, Iba (1991), Towell
430

fiRerepresenting Restructuring Domain Theories

Shavlik (1994) respectively obtained similar methodology1 . curve
forTgci average 50 independent random data partitions given along 95%
confidence ranges. Neither-MofN program obtained Ray Mooney's group
used generating Neither-MofN curve using 50 data partitions
used Tgci2.
42.5
40

EITHER
Labyrinth-k
NEITHER-MofN
KBANN
TGCI
95% confidence NEITHER-MofN
95% confidence TGCI

37.5
35
32.5
30
27.5
% Error

25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0
0

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size Training Sample

Figure 15: Learning curves theory-guided constructive induction systems
promoter domain.
Tgci showed improvement Either LabyrinthK portions curve
performed better Kbann Neither-MofN except smallest training sets. Confidence intervals available Either, LabyrinthK ,

1.

used testing set size 25 use conformation portion domain theory.
testing set LabyrinthK always consisted 13 promoters 13 non-promoters.
2. Baffes Mooney (1993) report slightly better learning curve Neither-MofN obtained,
communication Paul Baffes, think difference caused random selection
data partitions.
Either

431

fiDonoho & Rendell

Kbann, pairwise comparison Neither-MofN, improvement Tgci
significant 0.0005 level confidence training sets size 48 larger.

Structure Initial Promoter Theory

100%
first
conf.
rule

100%
first
minus_35
rule

100%
second
minus_35
rule

100%
third
minus_35
rule

100%
fourth
minus_35
rule

100%
first
minus_10
rule

100%
second
minus_10
rule

100%
third
minus_10
rule

100%
second
conf.
rule

100%
third
conf.
rule

100%
fourth
conf.
rule

100%
fourth
minus_10
rule

Structure Revised Promoter Theory

>20%
second
minus_35
rule

>33%
first
minus_10
rule

>33%
second
minus_10
rule

>33%
third
minus_10
rule

>33%
fourth
minus_10
rule

Figure 16: revised theory produced theory-guided constructive induction borrowed substructures initial theory, whole restricted structure.
Figure 16 compares initial promoter theory theory created Tgci. Reasons
Tgci's improvement inferred figure. Tgci extracted components original theory helpful restructured
concise theory. Neither Kbann Neither-MofN facilitates radical extraction
restructuring. seen leaf nodes, new theory measures partial match
example components original theory. aspect similar Kbann
Neither-MofN.
Part Tgci's improvement Kbann Neither-MofN may due knowledge/bias con ict latter two systems, situation revision biases con ict
knowledge way undo knowledge's benefits. occur
whenever detailed knowledge opened revision using set examples. revision
guided examples rather examples interpreted set
432

fiRerepresenting Restructuring Domain Theories

algorithmic biases. Biases useful absence knowledge may undo good
knowledge improperly applied. Yet biases developed perfected pure induction often unquestioningly applied revision theories. biases governing
dropping conditions Neither-MofN reweighting conditions Kbann may
neutralizing promoter theory's potential. speculate conducted
experiments allowed bias-guided dropping adding conditions within Tgci.
found techniques actually reduced accuracy domain.
45
42.5
40
37.5
c4.5
backpropagation
KBANN
TGCI
95% confidence TGCI
domain theory

35
32.5
% Error

30
27.5
25
22.5
20
17.5
15
12.5
10
7.5
0

20

40

60

80

100

120

140

160

180

200

Size Training Sample

Figure 17: Learning curves Tgci systems primate splice-junction domain.

5.2 Primate Splice-junction Domain

primate splice-junction domain (Noordewier et al., 1992) involves analyzing DNA
sequence identifying boundaries introns exons. Exons parts
DNA sequence kept splicing; introns spliced out. task involves placing
433

fiDonoho & Rendell

given boundary one three classes: intron/exon boundary, exon/intron boundary, neither. imperfect domain theory available 39.0% error rate
entire set available examples.
Figure 17 shows learning curves C4.5, backpropagation, Kbann, Tgci
primate splice-junction domain. results Kbann backpropagation taken
Towell Shavlik (1994). curves plain C4.5 Tgci algorithm
created training sets size 10,20,30,...90,100,120,...200 testing set size
800. curves C4.5 Tgci average 40 independent data partitions.
comparison made Neither-MofN implementation obtained
could handle two-class concepts. training sets larger 200, Kbann, Tgci,
backpropagation performed similarly.
accuracy Tgci appears slightly worse Kbann perhaps significantly. Kbann's advantage Tgci ability assign fine-grained weightings
individual parts domain theory. Tgci's advantage Kbann ability
easily restructure information contained domain theory. speculate
Kbann's capability assign fine-grained weights outweighted somewhat rigid structuring domain theory. Theory-guided constructive induction advantage
speed Kbann C4.5, underlying learner, runs much quickly
backpropagation, Kbann's underlying learning algorithm.

5.3 Gene Identification Domain
gene identification domain (Craven & Shavlik, 1995) involves classifying given DNA
segment coding sequence (one codes protein) non-coding sequence.
domain theory available gene identification domain; therefore, created
artificial domain theory using information organisms may favor certain nucleotide
triplets others gene coding. domain theory embodies knowledge DNA
segment likely gene coding segment triplets coding-favoring triplets
triplets noncoding-favoring triplets. decision triplets codingfavoring, noncoding-favoring, favored neither, made empirically
analyzing makeup 2500 coding 2500 noncoding sequences. specific artificial domain theory used described Online Appendix 1.
Figure 18 shows learning curves C4.5 Tgci gene identification domain.
original domain theory yields 31.5% error. curves created training
example sets size 50,200,400,...2000 testing separate example set size 1000.
curves average 40 independent data partitions.
partial curve given Neither-MofN became prohibitively slow
larger training sets. promoter domain training sets smaller 100,
Tgci Neither-MofN ran comparable speeds (approximately 10 seconds Sun4
workstation). domain Tgci ran approximately 2 minutes larger training sets.
Neither-MofN took 21 times long Tgci training sets size 400, 69 times
long size 800, 144 times long size 1200. Consequently, Neither-MofN's
curve extends 1200 represents five randomly selected data partitions.
reasons, solid comparison Neither-MofN Tgci cannot made
curves, appears Tgci's accuracy slightly better. speculate Neither434

fiRerepresenting Restructuring Domain Theories

45
42.5
40

TGCI
95% confidence TGCI
C4.5
NEITHER-MofN
domain theory

37.5

% Error

35
32.5
30
27.5
25
22.5
20
0

200

400

600

800

1000 1200 1400 1600 1800 2000

Number training examples

Figure 18: Learning curves Tgci systems gene identification domain.
's slightly lower accuracy partially due fact revises theory
correctly classify training examples. result theory likely overfits
training examples. Tgci need explicitly avoid overfit handled
underlying learner.
MofN

5.4 Summary Experiments
goal paper present new technique rather understand
behavior landmark systems, distill strengths, synthesize simple
system, Tgci. evaluation algorithm shows accuracy roughly matches
exceeds predecessors. promoter domain, Tgci showed sizable improvement
many published results. splice-junction domain, Tgci narrowly falls short
Kbann's accuracy. gene identification domain, Tgci outperforms Neither-MofN.
domains Tgci greatly improves original theory alone C4.5 alone.
435

fiDonoho & Rendell

faster closest competitors. Tgci runs much 100 times faster
large datasets. strict quantitative comparison speeds Tgci
Kbann made 1) backpropagation known much slower
decision trees (Mooney, Shavlik, Towell, & Gove, 1989), 2) Kbann uses multiple hidden
layers makes training time even longer (Towell & Shavlik, 1994), 3) Towell
Shavlik (1994) point run Kbann must made multiple times
different initial random weights, whereas single run Tgci sucient.
Overall, experiments support two claims paper: First, accuracy Tgci
substantiates delineation system strengths terms exible theory representation
exible theory structure, since characterization basis algorithm's
design. Second, Tgci's combination speed accuracy suggest unnecessary computational complexity avoided synthesizing strengths landmark systems.
following section take closer look strengths theory-guided constructive
induction.
Tgci

Neither-MofN

6. Discussion Strengths
number strengths theory-guided constructive induction discussed within
context Tgci algorithm used experiments.

6.1 Flexible Representation

discussed Section 1, many domains representation appropriate
initial theory may appropriate refined theory. theory-guided constructive induction allows translation initial theory different representation,
accommodate domains. experiments paper representation
needed allowed measurement partial match domain theory. Tgci1
accomplished simply counting matching features propagating information theory appropriately. Either LabyrinthK easily afford
measure partial match therefore appropriate problems best
representation final theory initial theory. Kbann allows
finer-grained measurement partial match Neither-MofN work,
price paid computational complexity. theory-guided constructive induction framework allows variety potential tools varying degrees granularity
partial match, although one tool used experiments.

6.2 Flexible Structure

discussed Section 2.5, strength existing induction programs fashioning concise
highly predictive description concept target concept concisely
described given features. Consequently, value domain theory lies
overall structure. feature language sucient, induction program build
good overall theory structure. Instead, value domain theory lies information
contains redescribe examples using high-level features. high-level
features facilitate concise description target concept. Systems Either
Neither-MofN reach final theory series modifications initial
436

fiRerepresenting Restructuring Domain Theories

theory hope gain something keeping theory's overall structure intact. initial
theory suciently close accurate theory, method works, often clinging
structure hinders full exploitation domain theory. Theory-guided constructive
induction provides means fully exploiting information domain theory
strengths existing induction programs. Figure 16 Section 5.1 gives comparison
structure initial promoter theory structure revised theory produced
theory-guided constructive induction. Substructures borrowed, revised
theory whole restructured.

6.3 Use Standard Induction Underlying Learner

theory-guided constructive induction uses standard induction program
underlying learner, need reinvent solutions overfit avoidance, multi-class
concepts, noisy data, etc. Overfit avoidance widely studied standard induction,
many standard techniques exist. system modifies theory accommodate
set training examples must address issue overfit training examples.
many theory revision systems existing overfit avoidance techniques cannot easily adapted,
problem must addressed scratch. Theory-guided constructive induction
take advantage full range previous work overfit avoidance standard induction.
multiple theory parts available multi-class concepts, interpreter
run multiple theory parts, resulting new feature sets combined.
primate splice-junction domain presented Section 5.2 three classes: intron/exon
boundaries, exon/intron boundaries, neither. Theories given intron/exon
exon/intron. theories used create new features, new features
concatenated together learning. Interpreters Tgci1 trivially handle
negation domain theory.

6.4 Use Theory Fragments

Theory-guided constructive induction limited using full domain theories.
part theory available, used. demonstrate this, three experiments
run fragments promoter domain theory used. first
experiment, four minus 35 rules used. Five features constructed | one
feature rule additional feature group. Similar experiments
run minus 10 group conformation group.
Figure 19 gives learning curves three experiments along curves entire theory theory (C4.5 using original features). Although conformation
portion theory gives significant improvement C4.5, minus 35
minus 10 portions theory give significant improvements performance. Thus even
partial theories theory fragments used theory-guided constructive induction
yield sizable performance improvements.
use theory fragments explored means evaluating contribution
different parts theory. Figure 19, conformation portion theory shown
yield improvement. could signal knowledge engineer knowledge
conveyed portion theory useful learner
present form.
437

fiDonoho & Rendell

45
C4.5
conformation portion theory
minus_10 portion theory
minus_35 portion theory
whole theory

42.5
40
37.5
35
32.5
30

% Error

27.5
25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0
0

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size Training Sample

Figure 19: Learning curves theory-guided constructive induction fragments
promoter domain theory. minus 35 portion theory, minus 10
portion theory, conformation portion theory used
separately feature construction. Curves given full theory
C4.5 alone comparison.

6.5 Use Multiple Theories

Theory-guided constructive induction use multiple competing even incompatible
domain theories. multiple theories exist, theory-guided constructive induction provides
natural means integrating way extract best theories.
Tgci1 would called input theory producing new features. Next, new
features simply pooled together induction program selects among
fashioning final theory. seen small scale promoter domain.
438

fiRerepresenting Restructuring Domain Theories

% Error

Figure 4 minus 35 rules subsume minus 35 rules. According entry
UCI Database, \the biological evidence inconclusive respect
correct specificity." handled simply using four possibilities, selection
useful knowledge left induction program.
Tgci could used evaluate contributions competing theories
used evaluate theory fragments above. knowledge engineer could use evaluation
guide revision synthesis competing theories.
25
22.5
20
17.5
15
12.5
10
7.5
5
2.5
0

TGCI using C4.5
TGCI using LFC
95% confidence LFC

5

10 15 20 25 30 35 40 45 50 55 60 65 70 75 80
Size Training Sample

Figure 20: Theory-guided constructive induction Lfc C4.5 underlying
learning system. Theory-guided constructive induction use inductive
learner underlying learning component. Therefore, sophisticated
underlying induction programs improve accuracy.

6.6 Easy Adoption New Techniques

Since theory-guided constructive induction use standard induction method
underlying learner, improvements made standard induction, theory-guided constructive induction passively improves. demonstrate this, tests run Lfc
(Ragavan & Rendell, 1993) underlying induction program. Lfc decision tree
learner performs example-based constructive induction looking ahead combinations features. Characteristically, Lfc improves accuracy moderate number
examples. Figure 20 shows resulting learning curve along C4.5 Tgci curve.
curves average 50 separate runs data partitions used
program. pairwise comparison improvement Lfc C4.5 significant
0.025 level confidence training sets size 72 80. sophisticated underlying
induction programs improve accuracy.
439

fiDonoho & Rendell

7. Testing Limits Tgci
purpose section explore performance theory-guided constructive
induction theory revision problems ranging easy dicult. easy problems
underlying concept embodied training testing examples matches domain
theory fairly closely; therefore, examples match domain theory fairly
closely. dicult problems underlying concept embodied examples
match domain theory well examples either. Although many
factors determine diculty individual problem, aspect important component worth exploring. experiment section intended relate ranges
diculty amount improvement produced Tgci.
Since number factors affect problem diculty chose theory revision
problems experiment variations single problem.
able hold factors constant vary closeness match domain
theory. wanted avoid totally artificial domains, chose start
promoter domain create \new" domains perverting example set.
\new" domains created perverting examples original promoter
problem either closely match promoter domain theory less closely match
promoter domain theory. positive examples altered. example, one domain
created 30% fewer matches domain theory original promoter
domain follows: feature value given example examined see matched
part theory. so, 30% probability, randomly reassigned new value
set possible values feature. end result set examples 30%
fewer matches domain theory original example set3. experiment
new domains created 10%, 30%, 60%, 90% fewer matches.
features, multiple values may match theory different disjuncts
theory specify different values single feature. example, referring back
Figure 4, feature p-12 matches two minus 10 rules value another
two rules value t. single feature might accidentally match one part
theory fact example whole closely matches another part theory.
cases these, true matches separated accidental matches examining
part theory clearly matched example whole expecting
match part theory.
New domains closely matched theory created similar manner.
example, domain created 30% fewer mismatches domain theory
original promoter domain follows: feature value given example examined
see matched corresponding part theory. not, 30% probability,
reassigned value matched theory. end result set examples
30% mismatches domain theory eliminated. experiment
new domains created 30%, 60%, 90% fewer mismatches.
Ten different example sets created level closeness domain theory:
10%, 30%, 60%, 90% fewer matches, 30%, 60%, 90% fewer mismatches. total, forty
example sets created matched original theory less closely original
3. precisely, would slightly matches 30% fewer matches features
would randomly reassigned back original matching value.

440

fiRerepresenting Restructuring Domain Theories

55
50
45
40

% Error

35

C4.5
TGCI

30
25
20
15
10
5
0
-100

-80

-60

-40

-20

0

20

40

60

80

100

Closeness theory

Figure 21: Seven altered promoter domains created, three closely matched
theory original domains four less closely matched.
100 x-axis indicates domain positive examples match
domain theory 100%. negative 100 indicates domain match
positive examples domain theory purely chance. accuracy
C4.5 Tgci plotted different levels proximity domain
theory.

example set, thirty example sets created matched original theory
closely original example set. example sets tested using leaveone-out methodology using C4.5 Tgci algorithm. results summarized
Figure 21. x-axis measure theory proximity { closeness example set
domain theory. \0" x-axis indicates change original promoter examples.
\100" x-axis means positive example exactly matches domain theory.
\-100" x-axis means match feature value positive example
441

fiDonoho & Rendell

domain theory totally chance4 . datapoint Figure 21 result averaging
accuracies ten example sets level theory proximity (except
point zero accuracy exact original promoter examples).
One notable portion Figure 21 section 0 60 x-axis. Domains
region greater trivial level mismatch domain theory
moderate mismatch. region Tgci's best performance.
domains, Tgci achieves high accuracy standard learner, C4.5, using original
feature set gives mediocre performance. second region examine -60 0
x-axis level mismatch ranges moderate extreme. region
Tgci's performance falls improvement original feature set remains high
shown Figure 22 plots improvement Tgci C4.5. final two
regions notice greater 60 less -60 x-axis. level
mismatch theory examples becomes trivially small (x-axis greater 60),
C4.5 able pick theory's patterns leading high accuracy approaches
Tgci's. level mismatch becomes extreme (x-axis less -60) theory gives
little help problem-solving resulting similarly poor accuracy methods.
summary, shown Figure 22 variants promoter problem wide range
theory proximity (centered around real promoter problem) theory-guided
constructive induction yields sizable improvement standard learners.
20
17.5

error difference

% Error

15
12.5
10
7.5
5
2.5
0
-100

-80

-60

-40

-20

0

20

40

60

80

100

Closeness theory

Figure 22: difference error C4.5 Tgci different levels proximity
example set domain theory.

4. scale 0 -100 left half graph may directly comparable scale 0 100
right half graph since equal number matches mismatches
original examples.

442

fiRerepresenting Restructuring Domain Theories

8. Conclusion
goal paper present another new system, rather
study two qualities exible representation exible structure. capabilities
intended frame reference analyzing theory-guided systems. two principles
provide guidelines purposeful design. distilled essence systems
Miro, Kbann, Neither-MofN, theory-guided constructive induction
natural synthesis strengths. experiments demonstrated even
simple application two principles effectively integrate theory knowledge
training examples. Yet much room improvement; two principles could
quantified made precise, implementations proceed
explored refined.
Quantifying representational exibility one step. Section 4 gave three degrees
exibility: one measured exact match theory, one counted number matching
conditions, one allowed weighted sum matching conditions. amount
exibility quantified, finer-grained degrees exibility explored.
accuracy assorted domains evaluated function representational
exibility.
Finer-grained structural exibility would advantageous. presented systems
make small, incremental modifications theory lacking structural exibility. Yet
theory-guided constructive induction falls extreme, perhaps allowing excessive
structural exibility. Fortunately, existing induction tools capable fashioning simple
yet highly predictive theory structures problem features suitably high-level.
Nevertheless, approaches explored take advantage structure
initial theory without unduly restricted it.
strength discussed Section 6.5 given attention. Although
promoter domain gives small example synthesizing competing theories,
explored domain entire competing, inconsistent theories available
synthesizing knowledge given multiple experts. point made Section 6.4
Tgci use theory fragments evaluate contribution different parts
theory. explored further.
exploration bias standard induction, Utgoff (1986) refers biases ranging
weak strong incorrect correct. strong bias restricts concepts
represented weak bias thus providing guidance learning.
bias becomes stronger, may become incorrect ruling useful concept descriptions. similar situation arises theory revision | theory representation language
inappropriately rigid may impose strong, incorrect bias revision. language
allows adaptability along many dimensions may provide weak bias. Grendellike toolbox would allow theory translated range representations
varying dimensions adaptability. Utgoff advocates starting strong, possibly incorrect bias shifting appropriately weak correct bias. Similarly, theory could
translated successively adaptable representations appropriate bias
found. implemented single tool; many open problems remain along line
research.
443

fiDonoho & Rendell

converse relationship theory revision constructive induction warrants
examination | theory revision uses data improve theory; constructive induction
use theory improve data facilitate learning. Since long-term goal machine
learning use data, inference, theory improve them, believe
consideration related methods beneficial, particularly
research area strengths lacks.
analysis landmark theory revision theory-guided learning systems led
two principles exible representation exible structure. theory-guided
constructive induction based upon high-level principles, simple yet achieves
good accuracy. principles provide guidelines future work, yet discussed above,
principles imprecise call exploration.

Acknowledgements
would thank Geoff Towell, Kevin Thompson, Ray Mooney, Jeff Mahoney
assistance getting datapoints Kbann, LabyrinthK , Either. would
thank Paul Baffes making Neither program available advice
setting program's parameters. thank anonymous reviewers constructive
criticism earlier draft paper. gratefully acknowledge support
work DoD Graduate Fellowship NSF grant IRI-92-04473.

References
Baffes, P., & Mooney, R. (1993). Symbolic revision theories M-of-N rules.
Proceedings 1993 IJCAI.
Bloedorn, E., Michalski, R., & Wnek, J. (1993). Multistrategy constructive induction:
AQ17-MCI. Proceeding second international workshop multistrategy learning.
Clark, P., & Matwin, S. (1993). Using qualitative models guide inductive learning.
Proceedings 1993 International Conference Machine Learning.
Cohen, W. (1992). Compiling prior knowledge explicit bias. Proceedings
1992 International Conference Machine Learning.
Craven, M. W., & Shavlik, J. W. (1995). Investigating value good input representation. Computational Learning Theory Natural Learning Systems, 3. Forthcoming.
Drastal, G., & Raatz, S. (1989). Empirical results learning abstraction space.
Proceedings 1989 IJCAI.
Dzerisko, S., & Lavrac, N. (1991). Learning relations noisy examples: empirical
comparison LINUS FOIL. Proceedings 1991 International Conference
Machine Learning.
444

fiRerepresenting Restructuring Domain Theories

Feldman, R., Serge, A., & Koppel, M. (1991). Incremental refinement approximate
domain theories. Proceedings 1991 International Conference Machine
Learning.
Flann, N., & Dietterich, T. (1989). study explanation-based methods inductive
learning. Machine Learning, 4, 187{226.
Fu, L. M., & Buchanan, B. G. (1985). Learning intermediate concepts constructing
hierarchical knowledge base. Proceedings 1985 IJCAI.
Harley, C., Reynolds, R., & Noordewier, M. (1990). Creators original promoter dataset.
Hirsh, H., & Noordewier, M. (1994). Using background knowledge improve inductive
learning DNA sequences. Tenth IEEE Conference AI Applications San
Antonio, TX.
Matheus, C. J., & Rendell, L. A. (1989). Constructive induction decision trees.
Proceedings 1989 IJCAI.
Michalski, R. S. (1983). theory methodology inductive learning. Artificial Intelligence, 20 (2), 111{161.
Mitchell, T. (1977). Version spaces: candidate elimination approach rule learning.
Proceedings 1977 IJCAI.
Mooney, R. J. (1993). Induction unexplained: Using overly-general domain theories
aid concept learning. Machine Learning, 10 (1), 79{110.
Mooney, R. J., Shavlik, J. W., Towell, G. G., & Gove, A. (1989). experimental comparison symbolic connectionist learning algorithms. Proceedings 1989
IJCAI.
Murphy, P., & Pazzani, M. (1991). ID2-of-3: Constructive induction M-of-N concepts
discriminators decision trees. Proceedings 1991 International Conference
Machine Learning.
Noordewier, M., Shavlik, J., & Towell, G. (1992). Donors original primate splice-junction
dataset.
Ourston, D., & Mooney, R. (1990). Changing rules: comprehensive approach theory
refinement. Proceedings 1990 National Conference Artificial Intelligence.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. Machine
Learning, 5 (1), 71{99.
Pazzani, M., & Kibler, D. (1992). utility knowledge inductive learning. Machine
Learning, 9 (1), 57{94.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. San Mateo, CA: Morgan
Kaufmann.
445

fiDonoho & Rendell

Ragavan, H., & Rendell, L. (1993). Lookahead feature construction learning hard concepts. Proceedings 1993 International Conference Machine Learning.
Rumelhart, D. E., Hinton, G. E., & McClelland, J. L. (1986). general framework
parallel distributed processing. Rumelhart, D. E., & McClelland, J. L. (Eds.),
Parallel Distributed Processing: Explorations Microarchitecture Cognition,
Volume I. Cambridge, MA: MIT Press.
Schlimmer, J. C. (1987). Learning representation change. Kaufmann, M. (Ed.),
Proceedings 1987 National Conference Artificial Intelligence.
Thompson, K., Langley, P., & Iba, W. (1991). Using background knowledge concept
formation. Proceedings 1991 International Conference Machine Learning.
Towell, G., & Shavlik, J. (1994). Knowledge-based artificial neural networks. Artificial
Intelligence, 70, 119{165.
Towell, G., Shavlik, J., & Noordeweir, M. (1990). Refinement approximately correct
domain theories knowledge-based neural networks. Proceedings 1990
National Conference Artificial Intelligence.
Utgoff, P. E. (1986). Shift bias inductive concept learning. Michalski, Carbonell,
& Mitchell (Eds.), Machine Learning, Vol. 2, chap. 5, pp. 107{148. San Mateo, CA:
Morgan Kaufmann.
Wogulis, J. (1991). Revising relational domain theories. Proceedings 1991 International Conference Machine Learning.

446


