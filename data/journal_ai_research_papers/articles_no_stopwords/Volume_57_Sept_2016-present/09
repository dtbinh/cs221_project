journal artificial intelligence

submitted published

embarrassingly parallel search constraint programming
arnaud malapert
jean charles regin
mohamed rezgui

arnaud malapert unice fr
jean charles regin unice fr
rezgui unice fr

universite cote dazur cnrs france

abstract
introduce embarrassingly parallel search eps method solving constraint
parallel method matches even outperforms state ofthe art number computing infrastructures eps
simple method master decomposes many disjoint subproblems solved independently workers three advantages
efficient method involves almost communication synchronization
workers implementation made easy master workers rely
underlying constraint solver require modify describes
method applications constraint satisfaction enumeration
optimization method adapted different underlying solvers
gecode choco tools different computing infrastructures multi core data centers cloud computing experiments cover unsatisfiable enumeration optimization
cover first solution search makes hard analyze variability observed optimization lesser
extent optimality proof required eps offers good average performance
matches outperforms available parallel implementations gecode well
solvers portfolios moreover perform depth analysis factors
make efficient well anomalies occur last
decomposition key component efficiency load balancing

introduction
second half th century frequency processors doubled every months
clear years period free lunch put sutter
larus behind us outlined bordeaux hamadi samulowitz
available computational power keep increasing exponentially increase
terms number available processors terms frequency per unit multi core
processors norm raises significant challenges software development
data centers high performance computing readily accessible many academia
industry cloud computing amazon microsoft azure google offers massive
infrastructures rent computing storage used demand
facilities anyone gain access super computing facilities moderate cost
distributed computing offers possibilities put computational resources common
effectively obtains massive capabilities examples include seti home anderson cobb
korpela lebofsky werthimer distributed net distributed computing technologies inc sharcnet bauer main challenge therefore scale e
cope growth
c

ai access foundation rights reserved

fimalapert regin rezgui

constraint programming cp appealing technology variety combinatorial
grown steadily last three decades strengths cp
use constraint propagation combined efficient search constraint
propagation aims removing combinations values variable domains cannot
appear solution number years possible gains offered parallel
computing attracted attention
parallel computing form computation many calculations carried
simultaneously almasi gottlieb operating principle large
often divided smaller ones solved parallel different forms
parallel computing exist bit level instruction level data task parallelism task
parallelism common parallel branch bound b b mattson sanders massingill achieved processor executes different
thread process different data parallel computer programs difficult write sequential ones concurrency introduces several classes
potential software bugs race conditions common example
memory shared several tasks modify data
time could render program incorrect mutual exclusion allows worker lock
certain resources obtain exclusive access create starvation
workers must wait worker frees resources moreover indeterminism
parallel programs makes behaviour execution unpredictable e
different program runs may differ communication synchronization among different
sub tasks address issue typically greatest obstacles good
performance another central bottleneck load balancing e keeping processors busy
much possible
wilkinson allen introduced embarrassingly parallel paradigm assumes computation divided number completely independent parts
part executed separate processor introduce
embarrassingly parallel search eps method constraint
method often outperforms state art parallel b b number
computing infrastructures master decomposes
many disjoint subproblems solved independently workers since constraint program trivially embarrassingly parallel decomposition procedure must
carefully designed three advantages efficient method
involves almost communication synchronization mutual exclusion workers
implementation simple master workers rely underlying
constraint solver require modify additionally deterministic
certain restrictions
integrates series publications regin rezgui malapert
rezgui regin malapert however includes novel contributions implementations implementation eps top
java library choco choco uses decomposition procedure
given implementations top c library gecode schulte
tools perron nikolaj vincent types instances
tested eps compared parallelizations gecode several static


fiembarrassingly parallel search cp

solvers portfolios perform depth analysis components especially decomposition procedures well anomalies occur
organized follows section presents constraint programming background amdahls law related work parallel constraint solving section gives
detailed description embarrassingly parallel search method section gives extensive experimental implementations gecode choco tools
different computing infrastructures multi core data center cloud computing well
comparisons state art parallel implementations static solver portfolios

related work
present constraint programming background two important parallelization
measures related amdahls law related work parallel constraint solving
constraint programming background
constraint programming cp attracted high attention among experts many areas
potential solving hard real life extensive review
constraint programming refer reader handbook rossi van beek
walsh constraint satisfaction csp consists set x variables
defined corresponding set possible values domains set c constraints
constraint relation subset variables restricts possible values
variables take simultaneously important feature constraints declarative
manner e specify relationship must hold current domain x
variable x x non strict subset initial domain partial assignment
represents case domains variables reduced singleton
namely variable assigned value solution csp assignment
value variable constraints simultaneously satisfied
solutions found searching systematically possible assignments
values variables backtracking scheme incrementally extends partial assignment
specifies consistent values variables toward complete solution
repeatedly choosing value another variable variables labeled given value
sequentially node search tree uninstantiated variable selected
node extended resulting branches node represent alternative
choices may examined order solution branching strategy
determines next variable instantiated order values
domain selected partial assignment violates constraints backtracking
performed recently assigned variable still alternative values available
domain clearly whenever partial assignment violates constraint backtracking
able eliminate subspace cartesian product variable domains
filtering associated constraint removes inconsistent values
domains variables e assignments cannot belong solution
constraint constraints handled constraint propagation mechanism
allows reduction domains variables global fixpoint reached
domain reductions possible fact constraint specifies relationship must hold
filtering computational procedure enforces relationship


fimalapert regin rezgui

generally consistency techniques complete e remove inconsistent
values domains variables
backtracking scheme consistency techniques used alone completely
solve csp combination allows search space explored complete
efficient way propagation mechanism allows reduction variable
domains pruning search tree whereas branching strategy improve
detection solutions failures unsatisfiable
consider complete standard backtracking scheme depth first traversal
search tree combined following variable selection strategies note different
variable selection strategies used although one time lex selects variable
according lexicographic ordering dom selects variable smallest remaining domain haralick elliott ddeg selects variable largest dynamic degree beck
prosser wallace variable constrained largest number
unassigned variables boussemart hemery lecoutre sais proposed conflictdirected variable ordering heuristics every time constraint causes failure
search weight incremented one variable weighted degree
sum weights constraints variable occurs wdeg selects
variable largest weighted degree current domain variable incorporated give dom ddeg dom wdeg selects variable minimum ratio
current domain size dynamic weighted degree boussemart et al
beck et al dom bwdeg variant follows binary labeling scheme impact
selects variable value pair strongest impact e leads strongest
search space reduction refalo
optimization consider standard top maintains
lower bound lb upper bound ub objective value ub lb subtree
pruned cannot contain better solution
parallelization measures amdahls law
two important parallelization measures speedup efficiency let c wallclock time parallel c number cores let
wall clock time sequential speedup su c c measure
indicating many times parallel performs faster due parallelization
efficiency eff c su c c normalized version speedup speedup
value divided number cores maximum possible speedup single program
parallelization known amdahls law amdahl states
small portion program cannot parallelized limit overall speedup
available parallelization let b fraction strictly
sequential time c
takes finish executed c cores
corresponds c b c b therefore theoretical speedup su c

su c


b b

c



fiembarrassingly parallel search cp

according amdahls law speedup never exceed number cores e linear
speedup terms efficiency measure means efficiency less

note sequential parallel b b explore
search space therefore super linear speedups parallel b b contradiction amdahls law processors access high quality solutions early
iterations turn brought reduction search tree size
parallel constraint solving
designing developing parallel programs manual process programmer responsible identifying implementing parallelism barney livermore section discuss parallel constraint solving parallel
logic programming refer reader surveys de kergommeaux codognet
gupta pontelli ali carlsson hermenegildo parallel integer
programming refer reader surveys crainic le cun roucairol
bader hart phillips gendron crainic
main approaches parallel constraint solving roughly divided following main categories search space shared memory search space splitting portfolio splitting approaches require communication synchronization
important issue load balancing refers practice distributing
approximately equal amounts work among tasks processors kept busy
time
search space shared memory
methods implemented many cores sharing list open nodes
search tree nodes least one children still unvisited
starved processors pick promising node list expand
defining different node evaluation functions one implement different strategies dfs
bfs others perron proposed comprehensive framework tested
processors vidal bordeaux hamadi reported good performance parallel
best first search processors although kind mechanism intrinsically provides
excellent load balancing known scale beyond certain number processors
beyond point performance starts decrease indeed shared memory system
threads must contend communicating memory
exacerbated cache consistency transactions
search space splitting
search space splitting strategies exploring parallelism provided search space
common approaches branching done different branches explored
parallel pruul nemhauser rushmeier one challenge load balancing
branches search tree typically extremely imbalanced require non negligible
overhead communication work stealing lai sahni
work stealing method originally proposed burton sleep first
implemented lisp parallel machines halstead search space dynamically


fimalapert regin rezgui

split resolution worker finished explore subproblem asks
workers another subproblem another worker agrees demand splits
dynamically current subproblem two disjoint subproblems sends one subproblem
starving worker starving worker steals work busy one note
form locking necessary avoid several starving workers steal
subproblems starving worker asks workers turn receives
subproblem termination work stealing method must carefully designed reduce
overhead almost workers starving almost work remains recent works
zoeteweij arbab jaffar santosa yap
zhu michel see hentenryck chu schulte stuckey
work stealing uses communication synchronization computation time
cannot easily scaled thousands processors address issues xie
davenport allocated specific processors coordination tasks allowing increase
number processors linear scaling processors used
parallel supercomputer performance starts decline
machado pedro abreu proposed hierarchical work stealing scheme correlated cluster physical infrastructure order reduce communication overhead
worker first tries steal local node considering remote nodes starting
closest remote node achieved good scalability cores
n queens quadratic assignment constraint optimization
maintaining best solution worker would require large communication
synchronization overhead machado et al observed scalability lowered
lazy dissemination far best solution e workers use
obsolete best solution
general purpose programming languages designed multi threaded parallel computing
charm kale krishnan cilk leiserson budiu delling
werneck ease implementation work stealing approaches otherwise
work stealing framework bobpp galea le cun le cun menouer vanderswalmen provides interface solvers parallel computers bobpp
work shared via global priority queue search tree decomposed allocated
different cores demand search execution periodically worker
tests starving workers exist case worker stops search path
root node highest right open node saved inserted global priority
queue worker continues search left open node otherwise
starving worker exists worker continues search locally solver starving
workers notified insertions global priority queue one picks
node starts search tools underlying solver menouer le cun
menouer le cun observed good speedups golomb ruler
marks workers queens
workers experiments investigate exploration overhead caused
bordeaux et al proposed another promising search space
splitting mechanism work stealing use hashing function
allocating implicitly leaves processors processor applies search
strategy allocated search space well designed hashing constraints address
load balancing issue gives linear speedup processors


fiembarrassingly parallel search cp

n queens speedups stagnate processors however
got moderate industrial sat instances
presented earlier works embarrassingly parallel search method
search space splitting loose communications regin et al rezgui et al

fischetti monaci salvagnin proposed another paradigm called selfsplit
worker able autonomously determine without communication
workers job parts process selfsplit decomposed three phases
enumeration tree initially built workers sampling enough open nodes
generated sampling phase ends worker applies deterministic rule
identify solve nodes belong solving single worker gathers
others merging selfsplit exhibited linear speedups processors good
speedups processors five benchmark instances selfsplit assumes sampling
bottleneck overall computation whereas happen practice regin
et al
sometimes complex applications good domain specific strategies
known parallel exploit domain specific strategy moisan gaudreault quimper moisan quimper gaudreault proposed
parallel implementation classic backtracking limited discrepancy search
lds known efficient centralized context good variable value
selection heuristic provided harvey ginsberg xie davenport proposed processor locally uses lds search trees allocated
tree splitting work stealing global system replicate lds
strategy
cube conquer heule kullmann wieringa biere parallelizing sat solvers cube conjunction literals dnf formula disjunction
cubes sat split several disjoint subproblems dnf formulas
solved independently workers cube conquer conflictdriven clause learning cdcl solver lingeling outperforms parallel sat solvers
instances sat benchmarks outperformed many
instances thus concurrent cube conquer van der tak heule biere tries
predict instances works well abort parallel search seconds
favor sequential cdcl solver
las vegas portfolios
explore parallelism provided different viewpoints
instance different parameter tuning idea exploited
non parallel context gomes selman communication required
excellent level load balancing achieved workers visit search space even
causes high level redundancy processors shows really good
performance greatly improved randomized restarts luby sinclair
zuckerman worker executes restart strategy recently cire
kadioglu sellmann executed luby restart strategy whole parallel
proved achieves asymptotic linear speedups practice often obtained


fimalapert regin rezgui

linear speedups besides authors proposed allow processors share information
learned search hamadi jabbour sais
one challenge scalable source diverse viewpoints provide orthogonal
performance therefore complementary interest distinguish
two aspects parallel portfolios assumptions made number available
processors possible handpick set solvers settings complement
optimally want face arbitrarily high number processors
need automated methods generate portfolio size demand bordeaux et al
portfolio designers became interested feature selection gomes selman
kautz horvitz ruan gomes selman features characterize
instances number variables domain sizes number constraints constraints arities
many portfolios select best candidate solvers pool static features
learning dynamic behaviour solvers sat portfolio isac amadini gabbrielli
mauro cp portfolio cphydra omahony hebrard holland nugent
osullivan use feature selection choose solvers yield best performance
additionally cphydra exploits knowledge coming resolution training set
instances candidate solver given instance cphydra determines k
similar instances training set determines time limit candidate
solver constraint program maximizing number solved instances within
global time limit minutes briefly cphydra determines switching policy
solvers choco abscon mistral
many recent sat solvers portfolio manysat hamadi et al
satzilla xu hutter hoos leyton brown sartagnan stephan michael
hydra xu hoos leyton brown pminisat chu stuckey harwood
minisat een sorensson combine portfolio
selection automatic configuration different underlying solvers
example satzilla xu et al exploits per instance variation among solvers
learned runtime
general main advantage portfolio many strategies automatically tried time useful defining good
search strategies difficult task
splitting
splitting another idea relates parallelism split
pieces solved processor typically becomes difficult
solve centralized case processor complete view
reconciling partial solutions subproblem becomes challenging
splitting typically relates distributed csps framework introduced yokoo ishida
kuwabara naturally split among agents privacy
reasons distributed csp frameworks proposed hirayama
yokoo chong hamadi ezzahir bessiere belaissaoui bouyakhf
leaute ottens szymanek wahbi ezzahir bessiere bouyakhf



fiembarrassingly parallel search cp

parallel constraint propagation
approaches thought typically parallelization one key solver instance constraint propagation nguyen deville hamadi
rolf kuchcinski however parallelizing propagation challenging kasif
scalability limited amdahls law approaches focus
particular topologies make assumptions
concluding remarks
note oldest approaches scalability issues still investigated
small number processors typically around processors one major
issue approaches may must resort communication communication
parallel agents costly general shared memory multi core
typically means access shared data structure one cannot avoid
form locking cost message passing cross cpu even significantly higher communication additionally makes difficult get insights solving process since
executions highly inter dependent understanding parallel executions notoriously
complex
parallel b b explore leaves search tree different order
would single processor system could pity situations
know really good search strategy entirely exploited parallel
many approaches experiments parallel programming involve great deal nondeterminism running twice instance identical number
threads parameters may different solutions sometimes different
runtimes

embarrassingly parallel search
section present details embarrassingly parallel search first section
introduces key concepts guided design choices section introduces
several search space splitting strategies implemented via top bottom decomposition procedures presented section section gives details architecture
communication section explains manage queue subproblems
order obtain deterministic parallel section gives details
implementation
key concepts
introduce key concepts guided design choices massive static decomposition
loose communication non intrusive implementation toward deterministic
massive static decomposition
master decomposes p subproblems
solved parallel independently workers solving process equivalent
real time scheduling p jobs w parallel identical machines known p cmax korf


fimalapert regin rezgui

schreiber efficient exists p cmax even simple list scheduling
priority rules w approximation desirable properties
defined section ensure low precision processing times makes
easier hold precision number workers fixed increase number
subproblems get harder perfect schedules appear get
easier case number p subproblems range one three
orders magnitude larger number workers w low chance
finding perfect schedules therefore obtain good speedups low large
decomposition takes longer becomes difficult conditions met
unlikely worker assigned work therefore
decomposition statistically balanced beside reach good speedups practice
total solving time subproblems must close sequential solving time

advantage master workers independent use different
filtering branching strategies even underlying solvers decomposition
crucial step bottleneck computation quality greatly
impacts parallelization efficiency
loose communication
p subproblems solved parallel independently w workers load balancing
must statistically obtained decomposition allow work stealing
order drastically reduce communication course communication still needed
dispatch subproblems gather possibly exchange useful additional
information objective bound values loose communication allows use star network
without risk congestion central node foreman connected nodes master
workers
non intrusive implementation
sake laziness efficiency rely much possible underlying solver
computing infrastructure consequently modify little possible underlying solver consider nogoods clauses exchanges techniques
intrusive increase communication overhead additionally logging fault
tolerance respectively delegated underlying solver infrastructure
toward determinism
deterministic given particular input produce output underlying machine passing sequence states determinism already challenging sequential b b
due complexity randomization restarts learning optimization still
difficult parallel b b
guarantee reproducibility real time assignment subproblems workers stored reproducibility means possible replay
solving process restrictions detailed later parallel made
deterministic additional cost moreover parallel able


fiembarrassingly parallel search cp

mimic sequential e produce identical solutions requires
parallel visits tree leaves order sequential
generally would useful debugging performance evaluation incremental
solving parallel may produce identical solutions matter many
workers present computing infrastructure used
conversely real time scheduling applied subproblems would
allow improve diversification randomization exploit past information
provided solving process experiments use fifo scheduling
subproblems scheduling policy would change shape size
search tree therefore reduces relevance speedups unlike eps work stealing
approaches deterministic offer control subproblem scheduling
search space splitting strategies
extend search space splitting proposed bordeaux et al
called splitting hashing let us recall c set constraints
split search space p parts one assign subproblem
p extended set constraints c hi hi hashing constraint
constrains subproblem particular subset search space hashing constraints
must necessarily sound effective nontrivial statistically balanced
sound hashing constraints must partition search space pi hi must cover entire
initial search space completeness mutual intersections hi hj j p
preferably empty non overlapping
effective addition hashing constraints effectively allow worker
efficiently skip portions search space assigned current subproblem
subproblem must significantly easier original causes overhead
refer recomputation overhead
nontrivial addition hashing constraints lead immediate
failure underlying solver thus generating trivial subproblems might paid
exploration overhead many would discarded propagation
mechanism sequential
statistically balanced workers given amount work
decomposition appropriate number p subproblems significantly larger
number w workers thus unlikely given worker would assigned
significantly work worker real time scheduling however possible solving one subproblem requires significantly work another
subproblem
bordeaux et al defined hashing constraints selecting subset x variables
p
stating hi p follows xx x mod p effectively
decomposes p p within reasonable limits p imposes
parity constraints sum variables splitting repeated scale
arbitrary number processors splitting obviously sound less effective


fimalapert regin rezgui

cp solvers sat solvers study assignment splitting node splitting
generate given number p subproblems
assignment splitting
let us consider non empty subset x x ordered variables x x xd vector v vd tuple x vj xj j let h dj xj vj
hashing constraints restrict search space solutions extending tuple
q
total decomposition x splits initial di xi subproblems e one
subproblem per tuple total decomposition clearly sound effective efficient
practice indeed regin et al showed number trivial subproblems
grow exponentially
table decomposition subproblem defined set tuples allows reach
exactly number p subproblems let ordered list ofj tuples
x
k


p first subproblem defined first k p tuples second
subproblem defined following k tuples subproblems defined
number tuples possibly exception last
tuple solver consistent propagation extended set constraints c
h underlying solver detect unsatisfiability order obtain nontrivial
decompositions total table decompositions restricted solver consistent tuples
node splitting
node splitting allows parallel exploit domain specific strategies
decomposition good strategy known let us recall concepts search
trees perron basis decomposition procedures introduced later
decompose one needs able map individual parts search tree
hashing constraints parts called open nodes open nodes defined
present search tree decomposed set open nodes
open nodes node expansion search tree partitioned three sets open
nodes closed nodes unexplored nodes make assumption
arity search tree e maximal number children nodes subsets
following properties
ancestors open node closed nodes
unexplored node exactly one open node ancestor
closed node open node ancestor
set open nodes called search frontier illustrated figure search
active
path
closed

open

frontier

unexplored

figure node status search frontier search tree


fiembarrassingly parallel search cp

frontier evolves simply process known node expansion removes open node
frontier transforms removed node closed node adds unexplored
children frontier node expansion operation happens
search corresponds branch operation b b
point search search frontier sound nontrivial decomposition
original open node associated subproblem decomposition effective branching strategy effective let us remark
assignment splitting seen special case node splitting static ordering
used variables values
active path jumps search tree expanding one node another may
require changing state least variables domains search process
first node second worker charge exploring open node must reconstruct
state visits done active path jumping operation
going search tree search process builds active path
list ancestors current open node illustrated figure worker
moves one node another jump search tree make jump
simply recomputes every move root gets target node causes
overhead refer recomputation overhead recomputation change
search frontier expand node
decomposition procedures
decomposition challenge depth search frontier contains
approximately p nodes assignment splitting strategy implemented top
procedure starts root node incrementally visits next levels whereas
node splitting strategy implemented bottom procedure starts form
level deep enough climbs back previous levels
top decomposition
challenge top decomposition ordered variables produce
approximately p solver consistent tuples realizes solver consistent table
decomposition iterated depth bounded depth first searches early removals inconsistent assignments regin et al
starts root node
empty list tuples line computes list p tuples solver consistent
table decomposition iterativly increasing decomposition depth let us assume
exists static order variables iteration determines lower
bound line decomposition depth e number variables involved
decomposition lower bound uses cartesian product current domains
next variables xd xd depth bounded depth first search extends
decomposition depth updates list tuples line current tuples
added constraints model search line order reduce
redundant work search extended tuples propagated line reduce
domains improve next lower bound decomposition depth tuple
solver consistent proven infeasible last search process repeated
number tuples greater equal p end tuples aggregated


fimalapert regin rezgui

top decomposition


















data csp x c number subproblems p
list tuples


simulate breadth first search iterated depth bounded dfss
repeat
determine
decomposition
depth

n lower bound
ql


min l max xi p



extend current decomposition variables
depthboundeddfs x c h x xd
break
propagate tuples without failure
propagate x c h
p
aggregate tuples generate exactly p subproblems
aggregatetuples subproblems become simultaneously available

foreach sendsubproblem x c h

generate exactly p subproblems practice consecutive tuples aggregated
subproblems become simultaneously available aggregation
sometimes sequential decomposition bottleneck amdahls law
parallel decomposition procedure increases scalability regin et al
two steps differ first instead starting depth empty list
tuples line first list quickly generated least five tuples per
worker
n




ql
min l xi w
qd
xi

second iteration tuple extended parallel instead extending sequentially tuples line parallel decomposition change ordering
compared sequential one subproblems become available
end decomposition




run parallel
foreach
extend tuple parallel
depthboundeddfs x c h x xd









top procedures assume variable ordering used decomposition static next decomposition procedure bypasses limitation handles
branching strategy


fiembarrassingly parallel search cp

bottom decomposition













data csp x c decomposition depth subproblem limit p
p
generate subproblems visiting top real tree
node callback decomposition node
depth node
sendsubproblem node
p p
p p
decrease dynamically depth
max
p p
backtrack
dfs x c

bottom decomposition
bottom decomposition explores search frontier depth approximately p nodes simplest form decomposition depth provided
user good knowledge explores search frontier depth
depth first search illustrated figure search callback identifies
node level line sends immediately active path defines subproblem
subproblem solved worker decomposition depth dynamic
reduced number subproblems becomes large line aims
compensate poor choice decomposition depth practice depth reduced
one unit current number subproblems exceeds given limit p limit
initially set p p doubled time reached contrary
depth static p never changes whatever number subproblems
practice common user provides decomposition depth
automated procedure without users intervention needed aims
identifying topmost search frontier approximately p open nodes sampling
estimation procedure divided three phases build partial tree sampling

final depth

search frontier

dynamic



p nodes

static
p nodes

initial depth

p nodes

decomposition

b estimation

figure bottom decomposition estimation


fimalapert regin rezgui

bottom estimation















data csp x c number subproblems p
data time limit node limit n maximum depth large enough
decomposition depth
set counters width levels
foreach width
build partial tree sampling
node callback estimation node
depth node

width width
width p
else backtrack
hasfinished n break
dfs x c
estimate level widths tree decomposition depth
width estimatewidths width
estimatedepth width p



top real search tree estimate level widths real tree determine
decomposition depth greedy heuristic
since need explore top search tree upper bound decomposition depth fixed maximum decomposition depth must chosen according
number workers expected number subproblems per worker
small decomposition could generate subproblems large
sampling time increases decomposition quality could decrease
sampling phase builds partial tree p open nodes level
callback depth first search number open nodes level partial
tree counted callback maximum depth reduced time p nodes
opened given level line sampling ends within limits top
tree entirely visited estimation needed otherwise line one needs
estimate widths topmost levels tree depending partial tree
estimation straightforward adaptation one proposed cornuejols karamanov
li deal n ary search tree line practice main issue
higher arity lower precision estimation therefore greedy heuristic
determines decomposition depth estimated number nodes per level
number nodes partial tree line heuristics minimizes
absolute deviation estimated number nodes expected number p
several levels identical absolute deviation lowest level estimated
number subproblems greater equal p selected
architecture communication
describe messages exchanged actors depending type
typical use case illustrates solving process optimization briefly


fiembarrassingly parallel search cp

communication network star network foreman acts pipe transmit
messages master workers
actors messages
total number messages depends linearly number workers w
number subproblems p messages synchronous sake simplicity
means work must wait communications completed barney livermore
interleaving computation communication single greatest benefit
asynchronous communications since work done communications taking
place however asynchronous communications complicate architecture instance
message requests answer
master control unit decomposes collects final
sends following messages create foreman give subproblem foreman
wait foreman gather destroy foreman master deals
foreman decomposition time elapsed time create
wait messages workers time elapsed time first give destroy
messages wall clock time elapsed time creation destruction
master
foreman central node star network queuing system stores subproblems received master dispatches workers gathers
collected workers foreman allows master concentrate
decomposition performance bottleneck handling communications
workers sends following messages create worker give subproblem worker
collect send final master destroy worker foreman
detects search ended sends collect message containing final
master
workers search engines send following messages subproblem
foreman must answer give message collect send foreman
contain essential information solution solving process workers
know foreman worker acquires work receives give message
foreman acquired subproblem recomputed causes recomputation overhead
work stealing context schulte noticed higher node search
tree smaller recomputation overhead construction topmost nodes
used
types
discuss specificities first solution solution best solution searches
first solution search search complete soon solution found
workers must immediately terminated well decomposition procedure
solution search search complete subproblems solved


fimalapert regin rezgui

best solution search main design issue best solution search maintain
far best solution sequential b b knows far best solution
difficult achieve concurrent setting several workers maintaining best
solution worker could lead large communication synchronization overheads
instead prefer solution foreman workers maintain far best
solution follows default give collect messages foreman
workers carry objective information additionally worker send better messages
foreman intermediate solution foreman send best solution
workers instance worker finds solution informs foreman
sending better message solution accepted threshold function similarly
foreman receives solution collect better message checks
whether solution really better solution accepted threshold function
foreman sends another better message workers architecture sketched
entails worker might know far best solution consequence
parts search tree explored pruned away worker
exact knowledge thus loose coupling might paid exploration
overhead

master

foreman

worker

worker

opt

allocate resources
create
create
create
give


give
opt

better

best solution search

give

give

give
collect

wait

give
collect
better

opt

better

best solution search

collect

collect
opt
release resources
destroy
destroy

destroy

master

foreman
master

worker
master

worker
master

figure sequence diagram solving process two workers


fiembarrassingly parallel search cp

use case
figure sequence diagram illustrating solving process optimization
two workers shows actors operate chronological order
first horizontal frame resource allocation master creates foreman
foreman creates workers immediately creation master worker
load original foreman transparently manages concurrent queue subproblems produced master consumed workers workers
jumps search tree
foreman creation master starts decomposition original
p subproblems soon subproblem generated master gives
foreman give messages interleaved node splitting
decomposition proposed section assignment splitting decomposition proposed
section would produce unique give message subproblems
decomposition finished master sends wait message foreman waits
collect response containing final last collect message triggers
resource deallocation
time worker starving asks foreman subproblem waits
first subproblem assigned first worker second worker waits
second subproblem best solution search frames correspond specific messages
optimization first worker quickly finds good solution sends
foreman via better message second subproblem generated master
given foreman turn foreman gives second subproblem updated
objective information second worker second quickly solved
second worker sends collect message foreman collect message
stands message third last subproblem assigned second
worker
foreman broadcasts better message good quality solution
received first worker note message useless first worker
foreman detects termination solving process sends collect message
master three following conditions met master waiting subproblems
queue empty workers starving last horizontal frame resource
deallocation
queuing determinism
foreman plays role queuing system receives subproblems master
dispatches workers section eps modified
return solution sequential useful several
scenarios debugging performance evaluation generally queuing policy
applied select next subproblem solve
let us assume subproblems p p pp sent foreman fixed
order case sequential top procedure bottom procedure
otherwise fixed order subproblems obtained sorting subproblems
first solution found sequential belongs satisfiable subproblem
pi smallest index e leftmost solution let us assume parallel


fimalapert regin rezgui

finds first solution subproblem pj j
necessary solve pk k j one must wait
pk k j determine leftmost solution satisfiable subproblem
smallest index
easily extended optimization slightly modifying cutting
constraints usually cutting constraint stated solution found
allows strictly improving solution contrary constraints cutting
constraint propagated backtracking solution found solving
subproblem pj cutting constraint allows strictly improving solution
subproblems k j allows equivalent solution subproblems k j
parallel returns solution sequential one
subproblem visited order moreover solution returned parallel
depend number workers decomposition
experiments queuing policy fifo policy ensures subproblems
solved order speedups relevant however guaranty
sequential parallel return solution

experimental
describe experiments eps carry detailed data analysis aim
answer following questions eps efficient different number workers
different solvers different computing platforms compared parallel
approaches influence different components decomposition procedures
search strategies constraint eps robust flexible anomalies
occur
section presents benchmark instances execution environments parameters settings different implementations first section analyze evaluate
top bottom decomposition procedures well importance search
strategy especially decomposition evaluate efficiency scalability
parallel solvers multi core machine section data center section
cloud platform section sections compare implementations
eps work stealing approaches whenever possible section analyze efficiency parallel solver depending search strategy section
transform reasonable effort parallel solver distributed parallel solver
batch scheduler provided data center anomalies parallel solver
explained resolved distributed equivalent last section discusses
performance parallel solvers compared static portfolios built underlying
sequential solvers data center
experimental protocol
section introduce benchmark instances execution environments metrics
notations give details implementations


fiembarrassingly parallel search cp

benchmark instances
lot benchmark instances available literature aim select difficult
instances represent tackled cp ideally instance
difficult none solvers solve quickly indeed parallel solving relevant
shortens long wall clock time consider unsatisfiable enumeration
optimization instances ignore finding first feasible
solution parallel speedup completely uncorrelated number
workers making hard analyze consider optimization
variability observed lesser extent optimality
proof required variability unsatisfiable enumeration instances lowered
therefore often used test bed parallel computing besides unsatisfiable
instances practical importance instance software testing enumeration
important users compare solutions
first set called fzn selection instances selected
instances repository maintained kjellerstrand directly
minizinc distribution written flatzinc language nicta optimisation
group instance solved seconds less hour
gecode selection composed unsatisfiable enumeration optimization
instances
set xcsp composed instances categories acad real xcsp
roussel lecoutre consists difficult instances solved within
hours choco malapert lecoutre first subset called xcsp composed
unsatisfiable enumeration instances whereas second subset called xcsp
composed unsatisfiable enumeration instances set xcsp composed
instances easier solve xcsp
besides consider two classical n queens golomb ruler
widely used literature gent walsh
implementation details
implemented eps method top three solvers choco written java gecode
tools rev written c use two parallelism implementation
technologies threads mueller et al kleiman shah smaalders
mpi lester gropp lusk typical difference
threads process run shared memory space mpi standardized
portable message passing system exchange information processes running
separate memory spaces therefore thread technology handle multiple nodes
cluster whereas mpi
c use threads implemented pthreads posix library mueller et al
kleiman et al used unix systems java use standard java thread
technology hyde
many implementations mpi openmpi gabriel fagg bosilca angskun
dongarra squyres sahay kambadur barrett lumsdaine et al intel mpi intel
corporation mpi ch mpi ch team ms mpi krishna balaji lusk
thakur tiller lantz mpi standard api characteristics


fimalapert regin rezgui

machine never taken account machine providers bull ibm intel
provide mpi implementation according specifications delivered machine thus cluster provided bull custom intel mpi library openmpi
installed microsoft azure supports ms mpi library
tools uses sequential top decomposition c threads gecode uses
parallel top decomposition c threads mpi technologies fact gecode
use c pthread multi core computer openmpi data center
ms mpi cloud platform gecode tools use lex variable selection
heuristic top decomposition requires fixed variable ordering choco
uses bottom decomposition java threads every case foreman schedules
jobs fifo mimic much possible sequential speedups
relevant needed master workers read model file
take value selection heuristic selects smallest value whatever
variable selection heuristic
execution environments
use three execution environments representative computing platforms available nowadays
multi core dell computer gb ram intel e ghz processors running scientific linux processor cores
data center centre de calcul interactif hosted universite nice sophia
antipolis provides cluster composed nodes cores running centos
node gb ram intel e ghz processors cores
cluster managed oar capit da costa georgiou huard martin mounie neyron
richard e versatile resource task manager thread technology
limited single node cluster choco use physical cores whereas gecode
use number nodes thanks mpi
cloud computing cloud platform managed microsoft company microsoft
azure enables deploy applications windows server technology li
node gb ram intel xeon e e ghz processors physical cores
allowed simultaneously use nodes cores managed microsoft hpc
cluster microsoft corporation
computing infrastructures provide hyper threading technologies hyper threading
improves parallelization computations multiple tasks core
physically present operating system addresses two logical cores shares
workload among possible multi core computer provides hyper threading
whereas deactivated cluster available cloud
setting parameters
time limit solving instance set hours whatever solver
number workers strictly less number cores w c
unused cores usually one chooses w c workers work simultaneously
multi core computer use two workers per physical core w c hyperthreading efficient experimentally demonstrated appendix target number


fiembarrassingly parallel search cp

p subproblems depends linearly number w workers p w allows
statistical balance workload without increasing much total overhead regin
et al
experiments network ram memory loads low regards
capacities computing infrastructures indeed total number messages depends
linearly number workers number subproblems ram pre allocated
computing infrastructure allows last workers almost produce input output
disk access
metrics notations
let solving time seconds let su speedup parallel
tables row gives obtained different given
instance row best solving times speedups indicated bold dashes
indicate instance solved question marks indicate
speedup cannot computed sequential solver solve instance
within time limit arithmetic means abbreviated computed solving times
whereas geometrical means abbreviated gm computed speedups efficiency
missing values e dashes question marks ignored computing statistics
use scoring procedure borda count voting system brams
fishburn benchmark instance treated voter ranks solvers
solver scores points related number solvers beats precisely
solver scores points p comparing performance solver
follows
gives better answer scores point
else answer gives worse answer scores point
else scoring execution time comparison give indistinguishable
answers
let respectively denote wall clock times solvers given
instance case indistinguishable answers scores f according borda system
used minizinc challenge function f capture users preferences
well indeed solver solves n seconds n others seconds
whereas solver solves first n seconds n others
seconds solvers obtain score n whereas users would certainly
prefer use another scoring function g g interpreted
utility function solving instance within seconds function g
strictly decreasing toward remaining points shared function f

f




g g g g f

g


loga

function g previous example solvers respectively
scored n n points


fimalapert regin rezgui

analysis decomposition
section compare quality performance top bottom
decomposition procedures introduced section
decomposition quality
top decomposition returns target number p w subproblems
whereas guaranteed bottom decomposition figure boxplot
number subproblems per worker p w bottom decomposition
choco depending number workers boxplots display differences among populations without making assumptions underlying statistical distribution
non parametric box boxplot spans range values first quartile
third quartile whiskers extend end box range equal
times interquartile range points lie outside range whiskers
considered outliers drawn individual circles
number workers w decompositions xcsp instances
one variable selection heuristic among lex dom dom ddeg dom wdeg dom bwdeg
impact combined minval considered bottom decomposition obtains
satisfying average performance mostly subproblems per worker
respecting much possible branching strategy however anomalies occur
first decomposition sensitive shape search tree sometimes model
contains variables large domains forbid accurate decomposition
instance first second levels knights search tree respectively contain
nodes significant underestimation
tree size especially branching high arity instance width second
level fapp estimated around nodes contains
nodes contrary underestimation occur top nodes eliminated
search tree low arity apart underestimation decomposition accurate
search trees low arity
top decomposition accurate requires fixed variable ordering whereas
bottom decomposition less accurate handles branching strategy





instances

subproblems per worker





















workers

choco w
choco w
gecode w
gecode w








time

number subproblems per worker

b decomposition time

figure analysis decomposition procedures w




fiembarrassingly parallel search cp

decomposition time
figure b gives percentage decompositions done within given time choco
times reported variable selection heuristics xcsp instances gecode times
reported lex xcsp fzn instances
implementation differences times reported choco gecode
slightly different indeed decomposition time alone given gecode choco
times take account estimation time time taken foreman fill
queue subproblems time taken workers empty queue let us
remind subproblems become available top decomposition
complete whereas become available fly bottom decomposition
cases reported time lower bound solving time
top decomposition faster bottom decomposition
parallelism fact gecode decomposition often faster estimation time alone
one compelling example instance knights highest time around
seconds well poor quality structure unsuited
bottom decomposition variables large domains
values almost domain reduction top tree
propagation long
conclude parallel top decomposition gecode fast accurate
bottom decomposition offers greater flexibility less robustness
influence search strategy
analyze influence search strategies decomposition resolution
apply variable selection heuristic decomposition master another one
resolution workers table gives solving times combinations lex
dom solving instances xcsp reported significant
differences among solving times choice variable selection heuristic critical
decomposition resolution indeed initial choices made branching
least informed important lead largest subtrees
search hardly recover early mistakes master workers
use variable selection heuristic

instances

worker
master

costasarray
latinsquare dg
lemma mod
pigeons
quasigroup
queenattacking
squares

lex

dom

lex

dom

lex

dom

































table solving times different search strategies choco multi core w c


fimalapert regin rezgui

multi core
section use parallel solvers thread technologies solve instances
xcsp n queens multi core computer let us recall
two worker per physical core hyper threading activated w c
eps frequently gives linear speedups outperforms work stealing
proposed schulte nielsen
performance analysis
table gives solving times speedups parallel solvers workers
xcsp instances choco tested lex dom whereas gecode tools
use lex compared work stealing denoted gecode ws schulte
nielsen first implementations eps faster efficient
work stealing eps often reaches linear speedups number cores whereas never
happens work stealing even worse three instances solved within
hours time limit work stealing whereas sequential solver
choco dom efficient parallel lex remains slightly slower
average decomposition key bad performance instances knights
lemma mod outlined decomposition knights takes
seconds generates much subproblems forbids speedup issue
lessened sequential decomposition tools resolved parallel
top decomposition gecode note sequential solving times tools
gecode respectively times higher similarly long decomposition time
choco lemma mod leads low speedup however moderate efficiency
choco gecode squares caused decomposition
gecode tools often efficient faster choco solvers
different behaviors even variable selection heuristic
instances

costasarray
knights
latinsquare dg
lemma mod
ortholatin
pigeons
quasigroup
queenattacking
series
squares
gm su
borda score rank

choco lex

choco dom

gecode

tools

gecode ws



su



su



su



su



su













































































































































table solving times speedups multi core w c gecode tools use
lex heuristic



fiembarrassingly parallel search cp

propagation mechanisms decompositions differ furthermore parallel top
decomposition gecode preserve ordering subproblems regard
sequential
variations n queens
verify effectiveness eps classic csp settings consider four
well known n queens n n queens puzzle placing
n chess queens n n chessboard two queens threaten
enumerate solutions heuristics lex dom reasonable choices
alldifferent global constraints enforce arc consistency ac alldifferent constraints enforce bound consistency bc arithmetic inequalities constraints neq
dedicated global constraint jc milano trick ch
table gives solving times speedups choco workers
decomposition depth striking splitting
technique gives excellent linear speedup processors
exception jc model unfortunate since jc model clearly best model
sequential solver dom better choice lex number
subproblems dom whatever model whereas total number nodes
changes indicates filtering weak top search tree
works report good often linear speedups n queens bordeaux et al reported linear speedups cores queens
improvement cores whereas machado et al scales workers hierarchical work stealing menouer le cun reported
speedups around cores queens pedro abreu pedro abreu
reported speedups around cores zoeteweij arbab reported
linear speedups cores queens pedro et al reported speedup
cores eps efficiency slightly average similar
observed queens
previous experimental setting favor eps exploring search
space exhaustively highly symmetric indeed variance subproblems solving time low especially higher levels consistency note
lower speedups jc model probably caused load balancing issues
subproblems neq model greater mean variance

model

lex

dom



bc
ac
neq
jc









su



su



su



su









































table variations queens choco multi core w c


fimalapert regin rezgui

instances

lex


dom
su



dom ddeg
su



dom bwdeg

su



su

dom wdeg


cc




costasarray



crossword







crossword c

fapp






knights



knights


knights


langford





langford
langford



latinsquare dg




lemma mod







ortholatin
pigeons




quasigroup
queenattacking


queensknights






ruler






ruler
scen f







series





squares
squaresunsat






gm su
borda score rank



























crossword words
crossword c words vg ext
queensknights mul squaresunsat



su

impact


su























































latinsquare dg

table detailed speedups solving times depending variable selection heuristics
choco data center w





speedup











lex

dom

ddeg

bwdeg

wdeg

impact

variable selection

figure speedups variable selection heuristics choco data center w



fiembarrassingly parallel search cp

data center
section study influence search strategy solving times
speedups scalability workers compare eps work stealing
influence search strategy
study performance choco workers solving xcsp instances
variable selection heuristics presented section figure boxplot
speedups variable selection heuristic first speedups lower dom bwdeg
decomposition effective binary branching states constraint x
left branch x right branch workload left right
branches imbalanced case positive decisions left branches
taken account second without learning lex dom parallel
efficient robust terms speedup learning dom bwdeg dom wdeg
impact parallel may explore different search tree sequential
one indeed master explores top tree changes learning
possibly branching decisions worker learns subproblems
whole search tree frequently causes exploration overhead solving
queensknights mul twelve times nodes dom wdeg sometimes gives
super linear speedup solving quasigroup three times less nodes impact
last low speedups occur variable selection heuristics
table gives solving times speedups obtained different variable selection
heuristics borda scores computed choco table gecode table first
variable selection heuristics strictly dominates others sequential parallel
however dom wdeg robust outlined borda scores fact variability solving times different heuristics reduced parallelization
remains important second spite low speedups dom bwdeg remains second
best variable selection heuristic parallel solving best one sequential
average advanced variable selection heuristics dom bwdeg dom wdeg
impact gives lower solving times lex dom spite lower speedups highlights
fact decomposition procedures handle branching strategy section
investigate low speedups instance crossword c words vg
caused variable selection heuristics
scalability workers
table compares gecode implementations eps work stealing ws solving
xcsp instances workers eps faster efficient work
stealing workers work stealing ranked last borda score
workers eps average almost times faster work stealing
efficient parallelize sequential solver multi core
machine gecode faster choco instances xcsp performance
gecode mitigated outlined borda scores five instances
solved within time limit gecode reported table six instances
solved workers whereas twelve instances solved sequential
solver way comparison five instances solved choco lex


fimalapert regin rezgui

w

instances

w

eps

cc
costasarray
crossword c
crossword
knights
knights
knights
langford
langford
langford
latinsquare dg
lemma mod
ortholatin
pigeons
quasigroup
queenattacking
ruler
ruler
series
squares

eps

ws



su



su



su



su

























































































































































































gm su
borda score rank


ws



crossword words





crossword c words vg ext

table speedups solving times xcsp gecode lex data center w
heuristics whereas instances solved sequential parallel dom wdeg
dom bwdeg highlights importance search strategy
figure boxplot speedups different numbers workers solving fzn
instances median speedups around w average dispersion remains
low



speedup su





















workers w

figure scalability workers gecode lex data center


fiembarrassingly parallel search cp

instance

eps

market split
market split
market split u
pop stress
nmseq
pop stress
fillomino
steiner triples
nmseq
golombruler
cc base mzn rnd test
ghoulomb
still life free x
bacp
depot placement st
open stacks wbp
bacp
still life still life
talent scheduling alt film
gm su

ws



su



su





























































































table solving times speedups fzn gecode lex cloud w

cloud computing
eps deployed microsoft azure cloud platform available computing
infrastructure organized follows cluster nodes computes application one head
node manages cluster nodes proxy nodes load balances communication
cluster nodes contrary data center cluster nodes may far
communication time may take longer proxy nodes requires cores managed
service provider nodes cores gb ram memory provide
workers cluster nodes managed mpi
table compares gecode implementations eps work stealing solving
fzn instances workers briefly eps faster work stealing
therefore efficient parallelize sequential solver work
stealing suffers higher communication overhead cloud data center
furthermore architecture computing infrastructure location cluster
nodes mostly unknown forbid improvements work stealing
proposed machado et al xie davenport
embarrassingly distributed search
section transform reasonable effort parallel solver eps distributed
parallel solver edps batch scheduler oar capit et al provided


fimalapert regin rezgui

data center fact batch scheduler oar plays foreman parallel choco
solver modified workers write subproblems files instead solving
script submits jobs subproblems oar batch scheduler waits
termination gathers oar schedules jobs cluster
priority fifo backfilling fair share priorities backfilling allows start
lower priority jobs without delaying highest priority jobs whereas fair share means
user application preferred way main drawback worker must
created subproblem worker process allocated oar predefined
resources worker sequential eds parallel solver edps
offers practical advantage resource reservation data center
indeed asking mpi process one wait enough resources available
process starts resources cores nodes nibbled soon
become available drastically reduce waiting time furthermore bypasses
limitations threads technology allowing use multiple nodes data center
however clearly increases recomputation overhead worker solves single
subproblem instead multiple subproblems model creation initial propagation
realized often introduces non negligible submission overhead
time taken create submit jobs oar batch scheduler
anomaly crossword c words vg ext
investigate low speedups solving instance crossword c words vg
variable selection heuristic see table compare parallel
eps w distributed eds sequential worker different decomposition depths table gives solving times speedups efficiencies
number distinct cores used distributed bad estimator computing efficiencies used short period time therefore
number c cores used compute efficiency eds edps estimated
ratio total runtime wall clock time
first parallel slower sequential one however
speedups distributed significant even decrease quickly
decomposition depth increases fall efficiency shows eds scalable
sequential workers indeed recomputation especially submission overhead
become important number subproblems increases
second bad performance parallel caused statistically
imbalanced decomposition would observe similar performance distributed
profiling parallel particular instances suggests bad
eds


p









eps w



su

eff



su

eff

























table eds eps crossword instance choco dom data center


fiembarrassingly parallel search cp

performance comes underlying solver indeed number instructions
similar sequential parallel whereas numbers context switches
cache references cache misses increase considerably fact parallel
spent half time internal methods extensional constraints e
relation constraint specified listing satisfying tuples issue occurred
computing infrastructure different java virtual machines note instances
use extensional constraints impose fewer consequences issue would
happen mpi implementation shared memory advocates
implementations eps mpi rather thread technology
variations golomb ruler
golomb ruler set marks integer positions along imaginary ruler
two pairs marks distance apart number marks ruler
order largest distance two marks length enumerate
optimal rulers minimal length specific number marks simple constraint
model inspired one galinier jaumard morales pesant
heuristics lex dom reasonable choice table gives solving times speedups
efficiencies parallel w distributed sequential
workers w distributed parallel workers w
worker decomposition depth dw different master decomposition depths
first eps obtains almost linear speedup decomposition depth large enough
without surprise speedups lower enough subproblems second
distributed eds sequential workers efficient number subproblems remains low otherwise still give speedups dom wastes
resources since efficiency low fact submitting many jobs batch
scheduler lex lead high submission overhead around minutes globally degrades performance finally distributed parallel workers offer
good trade speedups efficiencies allows use many resources
submitting jobs thus reducing submission recomputation overheads note eds tested roughly equivalent eps
workers edps tested submission overhead becomes
important

edps w dw

eds


p



su

eff



su

lex





























dom





























eps w


su

eff

































eff

table eds eps golomb ruler marks choco data center


fimalapert regin rezgui

parallel approaches reported good performance golomb ruler instance michel et al chu et al respectively reported linear
speedups workers eds efficient work stealing proposed
menouer le cun workers ruler marks efficient
selfsplit fischetti et al workers ruler marks
last enumerated optimal golomb rulers marks edps
master workers use lex heuristic master decomposition depth equal
generates around hundreds subproblems parallel workers
decomposition depth dw equal settings used cores
data center solving process bypasses limitations number
cores used mpi imposed administrator furthermore solving process starts
immediately cores grabbed soon become available whereas mpi
process waits enough cores becomes simultaneously available enumerating optimal
rulers marks respectively took seconds knowledge
first time constraint solver finds rulers furthermore reasonable amount time however optimal rulers discovered via exhaustive
computer search shearer recently distributed computing technologies inc
found optimum rulers marks beside plane construction atkinson hassenklover allows larger optimal rulers
comparison portfolios
portfolio approaches exploit variability performance observed several
solvers several parameter settings solver use portfolios portfolio
cphydra omahony et al uses features selection top solvers mistral
gecode choco cphydra uses case reasoning determine solve
unseen instance exploiting case base solving experience aims
feasible solution within minutes handle optimization solution time limit hard coded static fixed size portfolios
choco cag tools use different variable selection heuristics see section well
randomization restarts details choco cag found malapert
lecoutre cag portfolio extends choco portfolio solvers
abscon gecode cag produces better choco tools
portfolio gold medal minizinc challenge seem unfair
compare parallel solvers portfolios different numbers workers designing
scalable portfolio workers difficult task almost implementation
publicly available
table gives solving times eps portfolios solving xcsp instances
data center first cphydra workers solves among unsatisfiable instances
cc pigeons less seconds whereas difficult
approaches tools second less efficient solves fewer
often takes longer confirmed low borda score parallel choco
dom wdeg better average choco portfolio even portfolio solves
instances much faster scen f queensknights mul case
diversification provided portfolio outperforms speedups offered parallel


fiembarrassingly parallel search cp

instances

eps
choco

cc
costasarray
crossword words
crossword c words vg ext
fapp
knights
knights
knights
langford
langford
langford
latinsquare dg
lemma mod
ortholatin
pigeons
quasigroup
queenattacking
queensknights mul
ruler
ruler
scen f
series
squares
squaresunsat
arithmetic mean
borda score rank

portfolio

gecode

choco

cag

tools

w

w

w

w

w

w















































































































































































table solving times eps portfolio data center
b b emphasized cag portfolio solves instances
obtains several best solving times parallel gecode workers often slower
less robust portfolios choco cag however increasing number
workers clearly makes fastest solver still less robust five instances
solved within time limit
conclude choco cag portfolios robust thanks inherent diversification solving times vary one instance another workers
implementations eps outperform cphydra tools portfolio competitive
choco portfolio slightly dominated cag portfolio fact
good scaling eps key beat portfolios

conclusion
introduced embarrassingly parallel search eps method solving constraint
satisfaction constraint optimization several
advantages first efficient method matches even outperforms state

fimalapert regin rezgui

art number computing infrastructures second
involves almost communication synchronization mostly relies underlying
sequential solver implementation debugging made easier last
simplicity method allows propose many variants adapted specific applications
computing infrastructures moreover certain restrictions parallel
deterministic even mimic sequential important
practice production debugging
several interesting perspectives around eps first modified order
provide diversification learn useful information solving subproblems
instance easily combined portfolio subproblems
solved several search strategies second thanks simplicity simplest variants
eps could implemented meta searches rendl guns stuckey tack
would offer convenient way parallelize applications satisfactory efficiency last
another perspective predict solution time large combinatorial
known solution times small set subproblems statistical machine
learning approaches

acknowledgments
would thank much christophe lecoutre laurent perron youssef hamadi
carine fedele bertrand lecun tarek menouer comments advices
helped improve work supported cnrs oseo
bpi france within isi project pajero work granted access hpc
visualization resources centre de calcul interactif hosted universite nice sophia
antipolis microsoft azure cloud wish thank anonymous
referees comments

appendix efficiency hyper threading
section hyper threading technology improves efficiency eps
solving instances xcsp multi core computer figure boxplot
speedups provided hyper threading parallel solver among choco gecode
tools speedups indicate many times parallel solver workers
w c faster one workers w c maximum speedup according
amdahls law
choco tested lex dom whereas gecode tools use lex
compared work stealing proposed schulte denoted
gecode ws hyper threading clearly improves parallel efficiency eps whereas
performance work stealing roughly remains unchanged interesting
eps high cpu demand resources physical core shared
two logical cores indeed performance hyper threading known
application dependent exception lemma mod squares choco
tools faster workers lemma mod choco decomposition
workers takes longer generates many subproblems instance solved


fiembarrassingly parallel search cp

hyperthreading speedup







choco lex

choco dom

gecode

tools

gecode ws

figure speedups provided hyper threading multi core w
easily tools less two seconds becomes difficult improve efficiency squares decomposition changes according number workers
cannot explain hyper threading improve eps parallel efficiency
gecode reduced multiple instances interest hyper threading less obvious
choco tools conclude hyper threading globally improves efficiency
eps limited interest work stealing

references
almasi g gottlieb highly parallel computing benjamin cummings
publishing co inc redwood city ca usa
amadini r gabbrielli mauro j empirical evaluation portfolios
approaches solving csps gomes c sellmann eds integration
ai techniques constraint programming combinatorial optimization
vol lecture notes computer science pp springer
berlin heidelberg
amdahl g validity single processor achieving large scale
computing capabilities proceedings april spring joint computer conference afips pp york ny usa acm
anderson p cobb j korpela e lebofsky werthimer seti home
experiment public resource computing commun acm
atkinson hassenklover sets integers distinct differences tech
rep scs tr school computer science carlton university ottawa ontario
canada
bader hart w phillips c parallel design branch
bound g h ed tutorials emerging methodologies applications operations vol international series operations management
science pp springer york


fimalapert regin rezgui

barney b livermore l introduction parallel computing
computing llnl gov tutorials parallel comp

https

bauer high performance computing software challenges proceedings
international workshop parallel symbolic computation pasco
pp york ny usa acm
beck c prosser p wallace r trying fail first recent advances
constraints pp springer berlin heidelberg
bordeaux l hamadi samulowitz h experiments massively parallel
constraint solving boutilier boutilier pp
boussemart f hemery f lecoutre c sais l boosting systematic search
weighting constraints proceedings th eureopean conference artificial intelligence ecai including prestigious applicants intelligent systems
pais pp
boutilier c ed ijcai proceedings st international joint conference
artificial intelligence pasadena california usa july
brams j fishburn p c voting procedures arrow k j sen k
suzumura k eds handbook social choice welfare vol handbook
social choice welfare chap pp elsevier
budiu delling werneck r dryadopt branch bound distributed
data parallel execution engines parallel distributed processing symposium
ipdps ieee international pp ieee
burton f w sleep r executing functional programs virtual tree
processors proceedings conference functional programming
languages computer architecture fpca pp york ny usa
acm
capit n da costa g georgiou huard g martin c mounie g neyron p
richard batch scheduler high level components proceedings
fifth ieee international symposium cluster computing grid ccgrid ccgrid pp washington dc usa
ieee computer society
choco choco open source java constraint programming library ecole des
mines de nantes report
chong l hamadi distributed log reconciliation proceedings
conference ecai th european conference artificial intelligence august september riva del garda italy pp amsterdam
netherlands netherlands ios press
chu g schulte c stuckey p j confidence work stealing parallel constraint programming gent p ed cp vol lecture notes
computer science pp springer
chu g stuckey p j harwood pminisat parallelization minisat
tech rep nicta national ict australia


fiembarrassingly parallel search cp

cire kadioglu sellmann parallel restarted search proceedings
twenty eighth aaai conference artificial intelligence aaai pp
aaai press
cornuejols g karamanov li early estimates size branchand bound trees informs journal computing
crainic g le cun b roucairol c parallel branch bound
parallel combinatorial optimization
de kergommeaux j c codognet p parallel logic programming systems acm
computing surveys csur
distributed computing technologies inc distributed net home page http
www distributed net
een n sorensson n minisat sat solver conflict clause minimization
sat
ezzahir r bessiere c belaissaoui bouyakhf e h dischoco platform
distributed constraint programming dcr eighth international workshop
distributed constraint reasoning conjunction ijcai pp hyderabad india
fischetti monaci salvagnin self splitting workload parallel
computation simonis h ed integration ai techniques constraint
programming th international conference cpaior cork ireland may proceedings pp cham springer international publishing
gabriel e fagg g bosilca g angskun dongarra j squyres j sahay v kambadur p barrett b lumsdaine et al open mpi goals concept
design next generation mpi implementation recent advances parallel
virtual machine message passing interface pp springer
galea fran c le cun b bob framework exact combinatorial
optimization methods parallel machines international conference high performance computing simulation hpcs conjunction st
european conference modeling simulation ecms pp
galinier p jaumard b morales r pesant g constraint
golomb ruler rd international workshop integration ai
techniques
gendron b crainic g parallel branch bound survey
synthesis operations
gent walsh csplib benchmark library constraints proceedings th international conference principles practice constraint
programming cp pp
gomes c selman b portfolio design theory vs practice
proceedings thirteenth conference uncertainty artificial intelligence pp



fimalapert regin rezgui

gomes c selman b search strategies hybrid search spaces tools
artificial intelligence proceedings th ieee international conference pp
ieee
gomes c selman b hybrid search strategies heterogeneous search spaces
international journal artificial intelligence tools
gomes c selman b portfolios artificial intelligence
gropp w lusk e mpi communication library design portable
implementation scalable parallel libraries conference proceedings
pp ieee
gupta g pontelli e ali k carlsson hermenegildo v parallel
execution prolog programs survey acm transactions programming languages
systems toplas
halstead r implementation multilisp lisp multiprocessor proceedings
acm symposium lisp functional programming lfp pp
york ny usa acm
hamadi optimal distributed arc consistency constraints
hamadi jabbour sais l manysat parallel sat solver journal
satisfiability boolean modeling computation
haralick r elliott g increasing tree search efficiency constraint satisfaction artificial intelligence
harvey w ginsberg l limited discrepancy search proceedings
fourteenth international joint conference artificial intelligence ijcai
montreal quebec canada august volumes pp
heule j kullmann wieringa biere cube conquer guiding
cdcl sat solvers lookaheads hardware software verification testing
pp springer
hirayama k yokoo distributed partial constraint satisfaction
principles practice constraint programming cp pp springer
hyde p java thread programming vol sams
intel corporation intel mpi library https software intel com en us intel
mpi library
jaffar j santosa e yap r h c zhu k q scalable distributed depthfirst search greedy work stealing th ieee international conference
tools artificial intelligence pp ieee computer society
kale l krishnan charm portable concurrent object oriented system
c vol acm
kasif parallel complexity discrete relaxation constraint satisfaction networks artificial intelligence
kautz h horvitz e ruan gomes c selman b dynamic restart policies
th national conference artificial intelligence aaai iaai


fiembarrassingly parallel search cp

kjellerstrand h hakan kjellerstrands blog http www hakank org
kleiman shah smaalders b programming threads sun soft press
korf r e schreiber e l optimally scheduling small numbers identical
parallel machines borrajo kambhampati oddi fratini eds
icaps aaai
krishna j balaji p lusk e thakur r tiller f implementing mpi
windows comparison common approaches unix recent advances
message passing interface vol lecture notes computer science pp
springer berlin heidelberg
lai h sahni anomalies parallel branch bound commun acm
lantz e windows hpc server microsoft message passing interface msmpi
le cun b menouer vander swalmen p bobpp http forge prism
uvsq fr projects bobpp
leaute ottens b szymanek r frodo open source framework
distributed constraint optimization boutilier boutilier pp
leiserson c e cilk concurrency platform journal supercomputing

lester b art parallel programming prentice hall englewood cliffs nj
li h introducing windows azure apress berkely ca usa
luby sinclair zuckerman optimal speedup las vegas
inf process lett
machado r pedro v abreu scalability constraint programming
hierarchical multiprocessor systems icpp pp ieee
malapert lecoutre c propos de la bibliotheque de modeles xcsp
emes journees francophones de programmation par contraintes jfpc
angers france
mattson sanders b massingill b patterns parallel programming first
ed addison wesley professional
menouer le cun b anticipated dynamic load balancing strategy parallelize constraint programming search ieee th international symposium
parallel distributed processing workshops phd forum pp
menouer le cun b adaptive n p portfolio solving constraint
programming top parallel bobpp framework ieee th
international symposium parallel distributed processing workshops phd
forum
michel l see hentenryck p v transparent parallelization constraint
programming informs journal computing


fimalapert regin rezgui

microsoft corporation microsoft hpc pack r hpc pack http
technet microsoft com en us library jj aspx
milano trick constraint integer programming toward unified
methodology springer us boston
moisan gaudreault j quimper c g parallel discrepancy search
principles practice constraint programming vol lecture notes
computer science pp springer berlin heidelberg
moisan quimper c g gaudreault j parallel depth bounded discrepancy
search simonis h ed integration ai techniques constraint programming th international conference cpaior cork ireland may
proceedings pp cham springer international publishing
mpi ch team high performance portable mpi http www mpich org
mueller f et al library implementation posix threads unix
usenix winter pp
nguyen deville distributed arc consistency science
computer programming concurrent constraint programming
nicta optimisation group minizinc flatzinc http www g
csse unimelb edu au minizinc
nielsen parallel search gecode masters thesis kth royal institute
technology
omahony e hebrard e holland nugent c osullivan b casebased reasoning portfolio constraint solving irish conference
artificial intelligence cognitive science pp
pedro v abreu pedro v abreu distributed work stealing constraint solving corr abs
perron l search procedures parallelism constraint programming principles practice constraint programming cp th international conference
cp alexandria va usa october proceedings pp berlin
heidelberg springer berlin heidelberg
perron l nikolaj v vincent f tools tech rep google
pruul e nemhauser g rushmeier r branch bound parallel computation historical note operations letters
refalo p impact search strategies constraint programming wallace
ed principles practice constraint programming th international conference cp toronto canada vol lecture notes computer science
pp springer
regin j c rezgui malapert embarrassingly parallel search principles practice constraint programming th international conference cp
uppsala sweden september proceedings pp springer
berlin heidelberg berlin heidelberg


fiembarrassingly parallel search cp

regin j c rezgui malapert improvement embarrassingly parallel search data centers osullivan b ed principles practice constraint
programming th international conference cp lyon france september proceedings vol lecture notes computer science pp
springer international publishing cham
rendl guns stuckey p tack g minisearch solver independent
meta search language minizinc pesant g pesant g pesant g eds
principles practice constraint programming st international conference
cp cork ireland august september proceedings vol
lecture notes computer science pp springer international publishing
cham
rezgui regin j c malapert cloud computing solving
constraint programming first workshop cloud computing optimization conference workshop cp lyon france
rolf c c kuchcinski k parallel consistency constraint programming
pdpta international conference parallel distributed processing techniques applications
rossi f van beek p walsh eds handbook constraint programming
elsevier
roussel lecoutre c xml representation constraint networks format
http www cril univ artois fr cpai xcsp competition pdf
schulte c parallel search made simple proceedings trics techniques
implementing constraint programming systems post conference workshop
cp pp singapore
schulte c gecode generic constraint development environment http www
gecode org
shearer j b optimum golomb rulers ieee trans inf theor

stephan k michael k sartagnan parallel portfolio sat solver
lockless physical clause sharing pragmatics sat
sutter h larus j free lunch fundamental turn toward toward
concurrency dr dobbs journal
van der tak p heule j biere concurrent cube conquer theory
applications satisfiability testingsat pp springer
vidal v bordeaux l hamadi adaptive k parallel best first search
simple efficient multi core domain independent
proceedings third international symposium combinatorial search aaai
press
wahbi ezzahir r bessiere c bouyakhf e h dischoco platform
distributed constraint reasoning proceedings ijcai workshop
distributed constraint reasoning dcr pp barcelona catalonia spain


fimalapert regin rezgui

wilkinson b allen parallel programming techniques application
networked workstations parallel computers nd ed prentice hall inc
xie f davenport massively parallel constraint programming supercomputers challenges initial integration ai techniques
constraint programming combinatorial optimization th international conference cpaior bologna italy june proceedings vol
lecture notes computer science pp berlin heidelberg springer
berlin heidelberg
xu l hoos h leyton brown k hydra automatically configuring portfolio selection aaai conference artificial intelligence
vol pp
xu l hutter f hoos h leyton brown k satzilla portfolio selection sat journal artificial intelligence
yokoo ishida kuwabara k distributed constraint satisfaction dai
proceedings distributed ai workshop bandara tx
zoeteweij p arbab f component parallel constraint solver de
nicola r ferrari g l meredith g eds coordination vol lecture
notes computer science pp springer




