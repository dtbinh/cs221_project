journal artificial intelligence

submitted published

primer neural network
natural language processing
yoav goldberg

yoav goldberg gmail com

computer science department
bar ilan university israel

abstract
past years neural networks emerged powerful machine learning
yielding state art fields image recognition speech
processing recently neural network started applied textual
natural language signals promising tutorial surveys neural
network perspective natural language processing attempt
bring natural language researchers speed neural techniques tutorial
covers input encoding natural language tasks feed forward networks convolutional
networks recurrent networks recursive networks well computation graph
abstraction automatic gradient computation

introduction
decade core nlp techniques dominated machine learning approaches
used linear support vector machines logistic regression trained
high dimensional yet sparse feature vectors
recently field seen success switching linear
sparse inputs non linear neural network dense inputs
neural network techniques easy apply sometimes almost drop replacements
old linear classifiers many cases strong barrier entry tutorial
attempt provide nlp practitioners well newcomers basic background
jargon tools methodology allow understand principles behind
neural network apply work tutorial expected
self contained presenting different approaches unified notation
framework repeats lot material available elsewhere points
external sources advanced topics appropriate
primer intended comprehensive resource go
develop next advances neural network machinery though may serve good entry
point rather aimed readers interested taking existing useful
technology applying useful creative ways favourite nlp
depth general discussion neural networks theory behind advanced
optimization methods advanced topics reader referred existing
resources particular book bengio goodfellow courville highly
recommended
c

ai access foundation rights reserved

figoldberg

scope
focus applications neural networks language processing tasks however
subareas language processing neural networks deliberately left
scope tutorial include vast literature language modeling acoustic
modeling use neural networks machine translation multi modal applications
combining language signals images videos e g caption generation
caching methods efficient runtime performance methods efficient training large
output vocabularies attention discussed word embeddings
discussed extent needed understand order use inputs
unsupervised approaches including autoencoders recursive
autoencoders fall scope applications neural networks language
modeling machine translation mentioned text treatment means
comprehensive
note terminology
word feature used refer concrete linguistic input word suffix
part speech tag example first order part speech tagger features might
current word previous word next word previous part speech term input
vector used refer actual input fed neural network classifier
similarly input vector entry refers specific value input contrast
lot neural networks literature word feature overloaded
two uses used primarily refer input vector entry
mathematical notation
use bold upper case letters represent matrices x z bold lower case letters
represent vectors b series related matrices vectors example
matrix corresponds different layer network superscript indices
used w w rare cases want indicate power matrix
vector pair brackets added around item exponentiated w w
unless otherwise stated vectors assumed row vectors use v v denote
vector concatenation
choice use row vectors right multiplied matrices xw b
somewhat non standard lot neural networks literature use column vectors
left multiplied matrices wx b trust reader able adapt
column vectors notation reading literature

choice use row vectors notation inspired following benefits matches way
input vectors network diagrams often drawn literature makes hierarchical layered
structure network transparent puts input left variable rather
nested fully connected layer dimensions din dout rather dout din maps
better way networks implemented code matrix libraries numpy



fia primer neural networks nlp

neural network architectures
neural networks powerful learning discuss two kinds neural network
architectures mixed matched feed forward networks recurrent
recursive networks feed forward networks include networks fully connected layers
multi layer perceptron well networks convolutional pooling
layers networks act classifiers different strengths
fully connected feed forward neural networks section non linear learners
part used drop replacement wherever linear learner used
includes binary multiclass classification well complex structured prediction section non linearity network well
ability easily integrate pre trained word embeddings often lead superior classification accuracy series works managed obtain improved syntactic parsing
simply replacing linear model parser fully connected feed forward network straight forward applications feed forward network classifier replacement
usually coupled use pre trained word vectors provide benefits ccg
supertagging dialog state tracking pre ordering statistical machine translation
language modeling iyyer manjunatha boyd graber daume iii demonstrate
multi layer feed forward networks provide competitive sentiment classification factoid question answering
networks convolutional pooling layers section useful classification
tasks expect strong local clues regarding class membership
clues appear different places input example document classification
task single key phrase ngram help determining topic document
johnson zhang would learn certain sequences words good
indicators topic necessarily care appear document
convolutional pooling layers allow model learn local indicators
regardless position convolutional pooling architecture promising
many tasks including document classification short text categorization sentiment
classification relation type classification entities event detection paraphrase
identification semantic role labeling question answering predicting box office rev

chen manning weiss alberti collins petrov pei ge chang
durrett klein
lewis steedman
henderson thomson young
de gispert iglesias byrne
bengio ducharme vincent janvin vaswani zhao fossum chiang
johnson zhang
wang xu xu liu zhang wang hao
kalchbrenner grefenstette blunsom kim
zeng liu lai zhou zhao dos santos xiang zhou
chen xu liu zeng zhao nguyen grishman
yin schutze
collobert weston bottou karlen kavukcuoglu kuksa
dong wei zhou xu



figoldberg

enues movies critic reviews modeling text interestingness modeling
relation character sequences part speech tags
natural language often work structured data arbitrary sizes
sequences trees would able capture regularities structures
model similarities structures many cases means encoding
structure fixed width vector pass another statistical
learner processing convolutional pooling architectures allow us
encode arbitrary large items fixed size vectors capturing salient features
sacrificing structural information recurrent section
recursive section architectures hand allow us work sequences
trees preserving lot structural information recurrent networks elman
designed model sequences recursive networks goller kuchler
generalizations recurrent networks handle trees discuss
extension recurrent networks allow model stacks dyer ballesteros ling
matthews smith watanabe sumita
recurrent shown produce strong language modeling well sequence tagging machine translation dependency parsing
sentiment analysis noisy text normalization dialog state tracking response generation modeling relation character sequences part speech tags
recursive shown produce state art near state art
constituency dependency parse ranking discourse parsing semantic relation
classification political ideology detection parse trees sentiment classification
target dependent sentiment classification question answering























bitvai cohn
gao pantel gamon deng
dos santos zadrozny
notable works mikolov karafiat burget cernocky khudanpur mikolov
kombrink lukas burget cernocky khudanpur mikolov duh neubig sudoh
tsukada adel vu schultz auli galley quirk zweig auli gao

irsoy cardie xu auli clark ling dyer black trancoso fermandez amir
marujo luis b
sundermeyer alkhouli wuebker ney tamura watanabe sumita sutskever
vinyals le cho van merrienboer gulcehre bahdanau bougares schwenk bengio
b
dyer et al watanabe sumita
wang liu sun wang wang b
chrupala
mrksic seaghdha thomson gasic su vandyke wen young
sordoni galley auli brockett ji mitchell nie gao dolan
ling et al b
socher bauer manning ng
le zuidema zhu qiu chen huang
li li hovy
hashimoto miwa tsuruoka chikayama liu wei li ji zhou wang
iyyer enns boyd graber resnik b
socher perelygin wu chuang manning ng potts hermann blunsom
dong wei tan tang zhou xu
iyyer boyd graber claudino socher daume iii



fia primer neural networks nlp

feature representation
discussing network structure depth important pay attention
features represented think feed forward neural network
function nn x takes input din dimensional vector x produces dout
dimensional output vector function often used classifier assigning input x
degree membership one dout classes function complex
almost non linear common structures function discussed section
focus input x dealing natural language input x encodes
features words part speech tags linguistic information perhaps
biggest conceptual jump moving sparse input linear neural network
stop representing feature unique dimension called
one hot representation representing instead dense vectors core
feature embedded dimensional space represented vector space
embeddings vector representation core feature trained
parameter function nn figure shows two approaches feature
representation
feature embeddings values vector entries feature treated
model parameters need trained together components
network methods training obtaining feature embeddings discussed later
consider feature embeddings given
general structure nlp classification system feed forward neural
network thus
extract set core linguistic features f fk relevant predicting
output class
feature interest retrieve corresponding vector v
combine vectors concatenation summation combination
input vector x
feed x non linear classifier feed forward neural network
biggest change input move sparse representations
feature dimension dense representation feature mapped
vector another difference extract core features feature combinations elaborate changes briefly
dense vectors vs one hot representations
benefits representing features vectors instead unique ids
represent features dense vectors lets consider two kinds
representations

different feature types may embedded different spaces example one may represent word
features dimensions part speech features dimensions



figoldberg

figure sparse vs dense feature representations two encodings information current word dog previous word previous pos tag det
sparse feature vector dimension represents feature feature combinations receive dimensions feature values binary dimensionality
high b dense embeddings feature vector core feature
represented vector feature corresponds several input vector entries explicit encoding feature combinations dimensionality low
feature vector mappings come embedding table



fia primer neural networks nlp

one hot feature dimension
dimensionality one hot vector number distinct features

features completely independent one another feature word
dog dis similar word thinking word cat
dense feature dimensional vector
dimensionality vector

model training cause similar features similar vectors information
shared similar features
one benefit dense low dimensional vectors computational majority
neural network toolkits play well high dimensional sparse vectors
however technical obstacle resolved engineering
effort
main benefit dense representations generalization power believe
features may provide similar clues worthwhile provide representation
able capture similarities example assume observed word dog
many times training observed word cat handful times
words associated dimension occurrences dog
tell us anything occurrences cat however dense vectors representation
learned vector dog may similar learned vector cat allowing
model share statistical strength two events argument assumes
good vectors somehow given us section describes ways obtaining vector
representations
cases relatively distinct features category believe
correlations different features may use one hot representation however believe going correlations different features
group example part speech tags may believe different verb
inflections vb vbz may behave similarly far task concerned may
worthwhile let network figure correlations gain statistical strength
sharing parameters may case circumstances
feature space relatively small training data plentiful wish
share statistical information distinct words gains made
one hot representations however still open question
strong evidence side majority work pioneered collobert weston
collobert et al chen manning advocate use dense trainable
embedding vectors features work neural network architecture sparse
vector encodings see work johnson zhang
finally important note representing features dense vectors integral
part neural network framework consequentially differences
sparse dense feature representations subtler may appear first
fact sparse one hot vectors input training neural network amounts
dedicating first layer network learning dense embedding vector
feature training data touch section


figoldberg

variable number features continuous bag words
feed forward networks assume fixed dimensional input easily accommodate
case feature extraction function extracts fixed number features feature
represented vector vectors concatenated way region
resulting input vector corresponds different feature however cases number
features known advance example document classification common
word sentence feature thus need represent unbounded
number features fixed size vector one way achieving socalled continuous bag words cbow representation mikolov chen corrado dean
cbow similar traditional bag words representation
discard order information works summing averaging embedding
vectors corresponding features
cbow f fk

k
x
v
k





simple variation cbow representation weighted cbow different
vectors receive different weights

wcbow f fk pk

ai

k
x

ai v





feature associated weight ai indicating relative importance
feature example document classification task feature may correspond
word document associated weight ai could words tf idf score
distance position features
linear distance two words sentence may serve informative feature
example event extraction task may given trigger word candidate
argument word asked predict argument word indeed argument
trigger distance relative position trigger argument strong
signal prediction task traditional nlp setup distances usually encoded
binning distances several groups e associating
bin one hot vector neural architecture input vector composed
binary indicator features may seem natural allocate single input entry
distance feature numeric value entry distance however
taken practice instead distance features encoded similarly
note v one hot vectors rather dense feature representations cbow eq
wcbow eq would reduce traditional weighted bag words representations
turn equivalent sparse feature vector representation binary indicator feature
corresponds unique word
event extraction task involves identification events predefined set event types
example identification purchase events terror attack events event type triggered
triggering words commonly verbs several slots arguments needs filled
e purchased purchased amount



fia primer neural networks nlp

feature types bin associated dimensional vector distanceembedding vectors trained regular parameters network zeng et al
dos santos et al zhu et al nguyen grishman
feature combinations
note feature extraction stage neural network settings deals extraction core features contrast traditional linear model nlp systems
feature designer manually specify core features interest
interactions e g introducing feature stating word
x feature stating tag combined feature stating word x tag
sometimes even word x tag previous word z combination
features crucial linear introduce dimensions input
transforming space data points closer linearly separable
hand space possible combinations large feature designer
spend lot time coming effective set feature combinations one
promises non linear neural network one needs define
core features non linearity classifier defined network structure
expected take care finding indicative feature combinations alleviating need
feature combination engineering
kernel methods shawe taylor cristianini particular polynomial kernels
kudo matsumoto allow feature designer specify core features
leaving feature combination aspect learning contrast neuralnetwork kernels methods convex admitting exact solutions optimization
however computational complexity classification kernel methods scales
linearly size training data making slow practical purposes
suitable training large datasets hand computational
complexity classification neural networks scales linearly size network
regardless training data size
dimensionality
many dimensions allocate feature unfortunately theoretical bounds even established best practices space clearly dimensionality
grow number members class probably want assign
dimensions word embeddings part speech embeddings much
enough current dimensionality word embedding vectors range
hundreds extreme cases thousands since dimensionality vectors direct effect memory requirements processing time good
rule thumb would experiment different sizes choose good trade
speed task accuracy
vector sharing
consider case features share vocabulary example
assigning part speech given word may set features considering


figoldberg

previous word set features considering next word building input
classifier concatenate vector representation previous word
vector representation next word classifier able distinguish two
different indicators treat differently two features share
vectors vector dog previous word vector dog nextword assign two distinct vectors mostly empirical
question believe words behave differently appear different positions
e g word x behaves word previous position x behaves z
next position may good idea use two different vocabularies assign
different set vectors feature type however believe words behave
similarly locations something may gained shared vocabulary
feature types
networks output
multi class classification k classes networks output k dimensional
vector every dimension represents strength particular output class
output remains traditional linear scalar scores items discrete
set however see section k matrix associated output
layer columns matrix thought dimensional embeddings
output classes vector similarities vector representations k classes
indicate learned similarities output classes
historical note
representing words dense vectors input neural network popularized bengio
et al context neural language modeling introduced nlp tasks
pioneering work collobert weston colleagues embeddings
representing words arbitrary features popularized following chen
manning

feed forward neural networks
section introduces feed forward neural networks starts popular brain
inspired metaphor triggered quickly switches back mathematical
notation discuss structure feed forward neural networks representation
power common non linearities loss functions
brain inspired metaphor
name suggests neural networks inspired brains computation mechanism
consists computation units called neurons metaphor neuron computational unit scalar inputs outputs input associated weight
work bengio collobert weston colleagues popularized approaches
first use earlier authors use dense continuous space vectors representing word inputs
neural networks include lee et al forcada neco similarly continuous space
language used machine translation already schwenk et al



fia primer neural networks nlp

neuron multiplies input weight sums applies non linear
function passes output neurons connected
forming network output neuron may feed inputs one neurons
networks shown capable computational devices weights set
correctly neural network enough neurons non linear activation function
approximate wide range mathematical functions precise
later
output
layer

hidden
layer

hidden
layer

input layer

r

r







r

r

r

r

r

r

r

r

x

x

x

x

r

figure feed forward neural network two hidden layers
typical feed forward neural network may drawn figure circle
neuron incoming arrows neurons inputs outgoing arrows neurons outputs arrow carries weight reflecting importance shown neurons
arranged layers reflecting flow information bottom layer incoming
arrows input network top layer outgoing arrows
output network layers considered hidden sigmoid shape
inside neurons middle layers represent non linear function e logistic
function exa applied neurons value passing output
figure neuron connected neurons next layer called
fully connected layer affine layer
brain metaphor sexy intriguing distracting cumbersome
manipulate mathematically therefore switch concise mathematical
notation values row neurons network thought vector
figure input layer dimensional vector x layer dimensional vector h fully connected layer thought linear transformation
summing common operation functions max possible



figoldberg

dimensions dimensions fully connected layer implements vector matrix
multiplication h xw weight connection ith neuron
input row jth neuron output row wij values h transformed non linear function g applied value passed
next input whole computation input output written g xw w
w weights first layer w weights second one
mathematical notation
point abandon brain metaphor describe networks exclusively
terms vector matrix operations
simplest neural network perceptron linear function inputs
nnperceptron x xw b



x rdin w rdin dout b rdout
w weight matrix b bias term order go beyond linear functions
introduce non linear hidden layer network figure two layers resulting
multi layer perceptron one hidden layer mlp feed forward neural network
one hidden layer form
nnmlp x g xw b w b



x rdin w rdin b rd w rd b rd
w b matrix bias term first linear transformation
input g non linear function applied element wise called non linearity
activation function w b matrix bias term second linear
transform
breaking xw b linear transformation input x din dimensions
dimensions g applied dimensions matrix w together
bias vector b used transform dimensional output
vector non linear activation function g crucial role networks ability
represent complex functions without non linearity g neural network
represent linear transformations input
add additional linear transformations non linearities resulting mlp
two hidden layers network figure form
nnmlp x g g xw b w b w



perhaps clearer write deeper networks intermediary variables
see p
case denote weight ith input jth neuron h wij value
hj hj xi wij
network figure include bias terms bias term added layer adding
additional neuron incoming connections whose value
see consider sequence linear transformations still linear transformation



fia primer neural networks nlp

nnmlp x
h g xw b
h g h w b



h w
vector resulting linear transform referred layer outer
linear transform output layer linear transforms hidden
layers hidden layer followed non linear activation cases
last layer example bias vectors forced dropped
layers resulting linear transformations often referred fully connected
affine types architectures exist particular image recognition benefit
convolutional pooling layers layers uses language processing
discussed section networks several hidden layers said deep
networks hence name deep learning
describing neural network one specify dimensions layers
input layer expect din dimensional vector input transform
dout dimensional vector dimensionality layer taken dimensionality
output fully connected layer l x xw b input dimensionality din
output dimensionality dout dimensions x din w din dout b
dout
output network dout dimensional vector case dout networks
output scalar networks used regression scoring considering
value output binary classification consulting sign output
networks dout k used k class classification associating
dimension class looking dimension maximal value similarly
output vector entries positive sum one output interpreted
distribution class assignments output normalization typically achieved
applying softmax transformation output layer see section
matrices bias terms define linear transformations parameters network common refer collection parameters together
input parameters determine networks output training
responsible setting values networks predictions correct training
discussed section
representation power
terms representation power shown hornik stinchcombe white
cybenko mlp universal approximator approximate
desired non zero amount error family functions include continuous
functions closed bounded subset rn function mapping finite
specifically feed forward network linear output layer least one hidden layer squashing activation function approximate borel measurable function one finite dimensional space
another



figoldberg

dimensional discrete space another may suggest reason go beyond
mlp complex architectures however theoretical discuss
learnability neural network states representation exists say
easy hard set parameters training data specific learning
guarantee training correct function
generating training data finally state large hidden layer
indeed telgarsky exist neural networks many layers
bounded size cannot approximated networks fewer layers unless layers
exponentially large
practice train neural networks relatively small amounts data local
search methods variants stochastic gradient descent use hidden layers
relatively modest sizes several thousands universal approximation theorem
give guarantees non ideal real world conditions definitely
benefit trying complex architectures mlp many cases
however mlp indeed provide strong discussion representation power feed forward neural networks see book bengio et al section


common non linearities
non linearity g take many forms currently good theory
non linearity apply conditions choosing correct non linearity
given task part empirical question go common nonlinearities literature sigmoid tanh hard tanh rectified linear unit
relu nlp researchers experimented forms non linearities
cube tanh cube

sigmoid
sigmoid activation function x ex called logistic function
shaped function transforming value x range sigmoid
canonical non linearity neural networks since inception currently considered
deprecated use internal layers neural networks choices listed
prove work much better empirically

hyperbolic tangent tanh
x

hyperbolic tangent tanh x ee x
activation function shaped function trans
forming values x range


fia primer neural networks nlp

hard tanh
hard tanh activation function approximation tanh function faster
compute take derivatives


x
hardtanh x

x


x
otherwise
rectifier relu
rectifier activation function glorot bordes bengio known
rectified linear unit simple activation function easy work
shown many times produce excellent relu unit clips value x
despite simplicity performs well many tasks especially combined
dropout regularization technique see section


relu x max x
x

x
otherwise



rule thumb relu units work better tanh tanh works better
sigmoid
output transformations
many cases output layer vector transformed common transformation
softmax
x x xk
e xi
softmax xi pk
xj
j e



technical advantages relu sigmoid tanh activation functions
involve expensive compute functions importantly saturate sigmoid
tanh activation capped gradients region functions near zero
driving entire gradient near zero relu activation making
especially suitable networks multiple layers susceptible vanishing gradients
trained saturating units
addition activation functions recent works nlp community experiment
reported success forms non linearities cube activation function g x x
suggested chen manning found effective non linearities
feed forward network used predict actions greedy transition dependency
parser tanh cube activation function g x tanh x x proposed pei et al
found effective non linearities feed forward network used
component structured prediction graph dependency parser
cube tanh cube activation functions motivated desire better capture interactions different features activation functions reported improve performance
certain situations general applicability still determined



figoldberg

vector non negative real numbers sum one making discrete
probability distribution k possible outcomes
softmax output transformation used interested modeling probability distribution possible output classes effective used
conjunction probabilistic training objective cross entropy see section

softmax transformation applied output network without hidden
layer well known multinomial logistic regression model known
maximum entropy classifier
embedding layers
discussion ignored source x treating arbitrary vector
nlp application x usually composed embeddings vectors
explicit source x include networks definition introduce c
function core features input vector
common c extract embedding vector associated feature
concatenate
x c f f f v f v f v f
nnmlp x nnmlp c f f f
nnmlp v f v f v f



g v f v f v f w b w b
another common choice c sum embedding vectors assumes embedding vectors share dimensionality
x c f f f v f v f v f
nnmlp x nnmlp c f f f
nnmlp v f v f v f



g v f v f v f w b w b
form c essential part networks design many papers common
refer c part network likewise treat word embeddings v resulting
embedding layer lookup layer consider vocabulary v words
embedded dimensional vector collection vectors thought
v embedding matrix e row corresponds embedded feature let
v dimensional vector zeros except one index corresponding
value ith feature value called one hot vector
multiplication e select corresponding row e thus v defined
terms e
v e




fia primer neural networks nlp

similarly
cbow f fk

k
x

e



k
x

e





input network considered collection one hot vectors
elegant well defined mathematically efficient implementation typically involves
hash data structure mapping features corresponding embedding vectors
without going one hot representation
tutorial take c separate network architecture networks
inputs dense real valued input vectors c applied input passed
network similar feature function familiar linear terminology however training network input vector x remember constructed
propagate error gradients back component embedding vectors appropriate
error propagation discussed section
note notation
describing network layers get concatenated vectors x z input
authors use explicit concatenation x z w b others use affine transformation
xu yv zw b weight matrices u v w affine transformation
different one another two notations equivalent
note sparse vs dense features
consider network uses traditional sparse representation input vectors
embedding layer assuming set available features v k
features f fk v networks input
x

k
x

v

x n







first layer ignoring non linear activation
k
x
xw b
w





w r v b rd
layer selects rows w corresponding input features x sums
adding bias term similar embedding layer produces cbow
representation features matrix w acts embedding matrix
main difference introduction bias vector b fact embedding
layer typically undergo non linear activation rather passed directly
first layer another difference scenario forces feature receive separate
vector row w embedding layer provides flexibility allowing example
features next word dog previous word dog share vector


figoldberg

however differences small subtle comes multi layer feed forward
networks difference dense sparse inputs smaller may seem
first sight
loss functions
training neural network training section much
training linear classifier one defines loss function l stating loss predicting
true output training objective minimize loss across
different training examples loss l assigns numerical score scalar
networks output given true expected output loss function
bounded minimum attained cases networks output
correct
parameters network matrices wi biases bi commonly embeddings e set order minimize loss l training examples usually
sum losses different training examples minimized
loss arbitrary function mapping two vectors scalar practical
purposes optimization restrict functions easily compute
gradients sub gradients cases sufficient advisable rely common
loss function rather defining detailed discussion loss functions
neural networks see work lecun chopra hadsell ranzato huang lecun
huang bengio et al discuss loss functions
commonly used neural networks nlp
hinge binary
binary classification networks output single scalar intended
output classification rule sign classification considered
correct meaning share sign hinge loss known
margin loss svm loss defined
lhinge binary max



loss share sign otherwise loss linear
words binary hinge loss attempts achieve correct classification
margin least
hinge multiclass
hinge loss extended multiclass setting crammer singer let
yn networks output vector one hot vector correct
output class
classification rule defined selecting class highest score
prediction arg max yi




notation output expected output vectors many cases
natural think expected output scalar class assignment cases simply
corresponding one hot vector



fia primer neural networks nlp

denote arg maxi yi correct class k arg maxi yi highest scoring
class k multiclass hinge loss defined
lhinge multiclass max yt yk



multiclass hinge loss attempts score correct class classes
margin least
binary multiclass hinge losses intended used linear output
layer hinge losses useful whenever require hard decision rule
attempt model class membership probability
log loss
log loss common variation hinge loss seen soft version
hinge loss infinite margin lecun et al
llog log exp yt yk



categorical cross entropy loss
categorical cross entropy loss referred negative log likelihood used
probabilistic interpretation scores desired
let yn vector representing true multinomial distribution
labels n let yn networks output transformed
softmax activation function represent class membership conditional distribution
yi p x categorical cross entropy loss measures dissimilarity
true label distribution predicted label distribution defined cross
entropy
lcross entropy

x

yi log yi





hard classification training example single correct
class assignment one hot vector representing true class cases cross
entropy simplified
lcross entropy hard classification log yt



correct class assignment attempts set probability mass assigned
correct class scores transformed softmax
function represent conditional distribution increasing mass assigned correct
class means decreasing mass assigned classes
cross entropy loss common neural networks literature produces
multi class classifier predict one best class label predicts
distribution possible labels cross entropy loss assumed
networks output transformed softmax transformation


figoldberg

ranking losses
settings given supervision term labels rather pairs
correct incorrect items x x goal score correct items incorrect
ones training situations arise positive examples generate
negative examples corrupting positive example useful loss scenarios
margin ranking loss defined pair correct incorrect examples
lranking margin x x max nn x nn x



nn x score assigned network input vector x objective
score rank correct inputs incorrect ones margin least
common variation use log version ranking loss
lranking log x x log exp nn x nn x



examples ranking hinge loss language tasks include training auxiliary tasks used deriving pre trained word embeddings see section
given correct word sequence corrupted word sequence goal score
correct sequence corrupt one collobert weston similarly van
de cruys used ranking loss selectional preferences task network trained rank correct verb object pairs incorrect automatically derived
ones weston bordes yakhnenko usunier trained model score correct
head relation trail triplets corrupted ones information extraction setting
example ranking log loss found work gao et al
variation ranking log loss allowing different margin negative positive
class given work dos santos et al

word embeddings
main component neural network use embeddings representing
feature vector low dimensional space vectors come
section survey common approaches
random initialization
enough supervised training data available one treat feature embeddings
model parameters initialize embedding vectors random values
let network training procedure tune good vectors
care taken way random initialization performed method
used effective word vec implementation mikolov et al mikolov sutskever
chen corrado dean initialize word vectors uniformly sampled random

numbers range
number dimensions another option
use xavier
see section initialize uniformly sampled values
h initialization






fia primer neural networks nlp

practice one often use random initialization initialize embedding vectors commonly occurring features part speech tags individual
letters form supervised unsupervised pre training initialize
potentially rare features features individual words pre trained vectors
treated fixed network training process commonly
treated randomly initialized vectors tuned task hand
supervised task specific pre training
interested task limited amount labeled data
example syntactic parsing auxiliary task b say part speech tagging
much labeled data may want pre train word vectors
perform well predictors task b use trained vectors training
task way utilize larger amounts labeled data task b
training task treat pre trained vectors fixed tune
task another option train jointly objectives see section
details
unsupervised pre training
common case auxiliary task large enough amounts
annotated data maybe want help bootstrap auxiliary task training better
vectors cases resort unsupervised methods trained huge
amounts unannotated text
techniques training word vectors essentially supervised learning
instead supervision task care instead create practically
unlimited number supervised training instances raw text hoping tasks
created match close enough final task care
key idea behind unsupervised approaches one would embedding
vectors similar words similar vectors word similarity hard define
usually task dependent current approaches derive distributional
hypothesis harris stating words similar appear similar contexts
different methods create supervised training instances goal
predict word context predict context word
important benefit training word embeddings large amounts unannotated
data provides vector representations words appear supervised training set ideally representations words similar
related words appear training set allowing model generalize better
unseen events thus desired similarity word vectors learned unsupervised captures aspects similarity useful performing
intended task network
interpretation creating auxiliary raw text inspired ando zhang
ando zhang b



figoldberg

common unsupervised word embedding include word vec mikolov et al
glove pennington socher manning collobert weston
embeddings inspired neural networks
stochastic gradient training however deeply connected another
family evolved nlp ir communities
matrix factorization discussion see levy goldberg b levy et al
arguably choice auxiliary predicted kind
context affects resulting vectors much learning method
used train thus focus different choices auxiliary
available skim details training methods several software packages
deriving word vectors available including word vec gensim implementing
word vec word windows contexts word vecf modified
version word vec allowing use arbitrary contexts glove implementing
glove model many pre trained word vectors available download web
beyond scope tutorial worth noting word embeddings
derived unsupervised training wide range applications nlp
beyond initializing word embeddings layer neural network model
training objectives
given word w context c different formulate different auxiliary tasks
cases word represented dimensional vector initialized
random value training model perform auxiliary tasks well good
word embeddings relating words contexts turn
embedding vectors similar words similar
language modeling inspired approaches taken mikolov et al
mnih kavukcuoglu well glove pennington et al use auxiliary tasks
goal predict word given context posed probabilistic
setup trying model conditional probability p w c
approaches reduce binary classification addition
set observed word context pairs set created random words
context pairings binary classification given w c pair
come approaches differ set constructed
structure classifier objective optimized collobert
weston take margin binary ranking training feed forward
neural network score correct w c pairs incorrect ones mikolov et al
take instead probabilistic version training log bilinear model predict probability
p w c w c pair come corpus rather random sample
often treated single word vec actually software package including
training objectives optimization methods hyperparameters see work rong
levy goldberg dagan discussion
https code google com p word vec
https radimrehurek com gensim
https bitbucket org yoavgo word vecf
http nlp stanford edu projects glove



fia primer neural networks nlp

choice contexts
cases contexts word taken words appear
surrounding short window around within sentence paragraph
document cases text automatically parsed syntactic parser
contexts derived syntactic neighbourhood induced automatic parse
trees sometimes definitions words context change include parts words
prefixes suffixes
neural word embeddings originated world language modeling
network trained predict next word sequence preceding words bengio
et al text used create auxiliary tasks aim predict
word context k previous words training language modeling
auxiliary prediction indeed produce useful embeddings needlessly
restricted constraints language modeling task one allowed look
previous words care language modeling
resulting embeddings may better ignoring constraint taking context
symmetric window around focus word
window
common sliding window auxiliary tasks
created looking sequence k words middle word callled focus word
k words side contexts single task created
goal predict focus word context words represented
cbow see mikolov et al vector concatenation see collobert weston
k distinct tasks created pairing focus word different context
word k tasks popularized mikolov et al referred
skip gram model skip gram approaches shown robust efficient train
mikolov et al pennington et al often produce state art
effect window size size sliding window strong effect resulting vector similarities larger windows tend produce topical similarities e
dog bark leash grouped together well walked run walking smaller windows tend produce functional syntactic similarities e
poodle pitbull rottweiler walking running approaching
positional windows cbow skip gram context representations
different context words within window treated equally distinction
context words close focus words farther
likewise distinction context words appear focus
words context words appear information easily factored
positional contexts indicating context word relative position
focus words e instead context word becomes indicating
word appears two positions right focus word use positional context
together smaller windows tend produce similarities syntactic
strong tendency grouping together words share part speech well
functionally similar terms semantics positional vectors shown ling


figoldberg

dyer black trancoso effective window vectors
used initialize networks part speech tagging syntactic dependency parsing
variants many variants window possible one may lemmatize words
learning apply text normalization filter short long sentences remove
capitalization see e g pre processing steps described dos santos gatti
one may sub sample part corpus skipping probability creation tasks
windows common rare focus words window size may
dynamic different window size turn one may weigh different positions
window differently focusing trying predict correctly close word context
pairs away ones choices effect resulting vectors
hyperparameters others discussed levy et al
sentences paragraphs documents
skip grams cbow one consider contexts word
words appear sentence paragraph document
equivalent large window sizes expected word vectors
capture topical similarity words topic e words one would expect
appear document likely receive similar vectors
syntactic window
work replace linear context within sentence syntactic one levy
goldberg bansal gimpel livescu text automatically parsed
dependency parser context word taken words
proximity parse tree together syntactic relation
connected approaches produce highly functional similarities grouping together words
fill role sentence e g colors names schools verbs movement
grouping syntactic grouping together words share inflection levy
goldberg
multilingual
another option multilingual translation contexts hermann blunsom
faruqui dyer example given large amount sentence aligned parallel
text one run bilingual alignment model ibm model model e
giza software use produced alignments derive word contexts
context word instance foreign language words aligned
alignments tend synonym words receiving similar vectors authors
work instead sentence alignment level without relying word alignments gouws
bengio corrado train end end machine translation neural network
use resulting word embeddings hill cho jean devin bengio appealing
method mix monolingual window multilingual
creating kinds auxiliary tasks likely produce vectors similar
window reducing somewhat undesired effect window

fia primer neural networks nlp

antonyms e g hot cold high low tend receive similar
vectors faruqui dyer
character sub word representations
interesting line work attempts derive vector representation word
characters compose approaches likely particularly useful tasks
syntactic nature character patterns within words strongly related
syntactic function approaches benefit producing small
model sizes one vector character alphabet together handful
small matrices needs stored able provide embedding vector every
word may encountered dos santos gatti dos santos zadrozny
kim et al model embedding word convolutional network
see section characters ling et al b model embedding word
concatenation final states two rnn lstm encoders section one
reading characters left right right left produce
strong part speech tagging work ballesteros et al
two lstms encoding ling et al b beneficial representing words
dependency parsing morphologically rich languages
deriving representations words representations characters motivated unknown words encounter word
embedding vector working level characters alleviates
large extent vocabulary possible characters much smaller
vocabulary possible words however working character level
challenging relationship form characters function syntax semantics
language quite loose restricting oneself stay character level may
unnecessarily hard constraint researchers propose middle ground word
represented combination vector word vectors sub word
units comprise sub word embeddings help sharing information
different words similar forms well allowing back subword level
word observed time forced rely solely
form enough observations word available botha blunsom suggest model embedding vector word sum word specific vector
vector available vectors different morphological components comprise
components derived morfessor creutz lagus unsupervised
morphological segmentation method gao et al suggest core features
word form unique feature hence unique embedding vector
letter trigrams word

neural network training
neural network training done trying minimize loss function training set
gradient method roughly speaking training methods work repeatedly
computing estimate error dataset computing gradient respect
error moving parameters opposite direction gradient
differ error estimate computed moving opposite


figoldberg

direction gradient defined describe basic stochastic gradient
descent sgd briefly mention approaches pointers
reading gradient calculation central gradients efficiently
automatically computed reverse mode differentiation computation graph
general algorithmic framework automatically computing gradient network
loss function discussed section
stochastic gradient training
common training neural networks stochastic gradient descent
sgd bottou lecun bottou orr muller variant
sgd general optimization receives function f parameterized
loss function desired input output pairs attempts set parameters
loss f respect training examples small works
follows
online stochastic gradient descent training
input function f x parameterized parameters
input training set inputs x xn desired outputs yn
input loss function l
stopping criteria met

sample training example xi yi

compute loss l f xi yi

g gradients l f xi yi w r

g
return
pnthe goal set parameters minimize total loss
l f xi yi training set works repeatedly sampling training example computing gradient error example respect parameters
line input expected output assumed fixed loss treated
function parameters parameters updated opposite
direction gradient scaled learning rate line learning rate
fixed throughout training process decay function time step
discussion setting learning rate see section
note error calculated line single training example thus
rough estimate corpus wide loss aiming minimize noise
loss computation may inaccurate gradients common way reducing
noise estimate error gradients sample examples
gives rise minibatch sgd
lines estimates gradient corpus loss
minibatch loop g contains gradient estimate parameters
updated toward g minibatch size vary size n higher
values provide better estimates corpus wide gradients smaller values allow
learning rate decay required order prove convergence sgd



fia primer neural networks nlp

minibatch stochastic gradient descent training
input function f x parameterized parameters
input training set inputs x xn desired outputs yn
input loss function l
stopping criteria met

sample minibatch examples x xm ym

g



compute loss l f xi yi


g g gradients
l f xi yi w r
g
return


updates turn faster convergence besides improved accuracy gradients
estimation minibatch provides opportunities improved training efficiency
modest sizes computing architectures e gpus allow efficient parallel
implementation computation lines properly decreasing learning rate
sgd guaranteed converge global optimum function convex however
used optimize non convex functions neural network
longer guarantees finding global optimum proved robust
performs well practice
training neural network parameterized function f neural network
parameters linear transformation matrices bias terms embedding matrices
gradient computation key step sgd well
neural network training question compute
gradients networks error respect parameters fortunately
easy solution form backpropagation rumelhart hinton williams
lecun bottou bengio haffner b backpropagation fancy
name methodically computing derivatives complex expression chainrule caching intermediary generally backpropagation
special case reverse mode automatic differentiation neidinger
section baydin pearlmutter radul siskind bengio following
section describes reverse mode automatic differentiation context computation
graph abstraction
beyond sgd
sgd often produce good advanced available sgd momentum polyak nesterov momentum
sutskever martens dahl hinton nesterov variants
sgd previous gradients accumulated affect current update adap recent work neural networks literature argue non convexity networks manifested proliferation saddle points rather local minima dauphin pascanu gulcehre cho
ganguli bengio may explain success training neural networks despite
local search techniques



figoldberg

tive learning rate including adagrad duchi hazan singer adadelta
zeiler rmsprop tieleman hinton adam kingma ba
designed select learning rate minibatch sometimes per coordinate basis
potentially alleviating need fiddling learning rate scheduling details
see original papers book bengio et al sections
many neural network software frameworks provide implementations
easy sometimes worthwhile try different variants
computation graph abstraction
one compute gradients parameters network hand
implement code procedure cumbersome error prone purposes preferable use automatic tools gradient computation bengio
computation graph abstraction allows us easily construct arbitrary networks evaluate
predictions given inputs forward pass compute gradients parameters
respect arbitrary scalar losses backward pass
computation graph representation arbitrary mathematical computation
graph directed acyclic graph dag nodes correspond mathematical
operations bound variables edges correspond flow intermediary values
nodes graph structure defines order computation terms
dependencies different components graph dag tree
one operation input several continuations consider example
graph computation b b










b



computation b shared restrict case computation
graph connected
since neural network essentially mathematical expression represented
computation graph
example figure presents computation graph mlp one hiddenlayer softmax output transformation notation oval nodes represent mathematical operations functions shaded rectangle nodes represent parameters bound
variables network inputs treated constants drawn without surrounding node
input parameter nodes incoming arcs output nodes outgoing arcs
output node matrix dimensionality indicated
node
graph incomplete without specifying inputs cannot compute output
figure b shows complete graph mlp takes three words inputs predicts
distribution part speech tags third word graph used
prediction training output vector scalar graph
take account correct answer loss term finally graph c shows


fia primer neural networks nlp


neg

log



b

c

pick







softmax

softmax

softmax







add

add

add







mul

mul

mul


tanh


w




b


w

tanh




b





add

add

add







mul

mul

mul


w




b

concat


w









w

tanh




x






b

concat

w













lookup

lookup

lookup

lookup

lookup

lookup



black

dog



black

dog

v
e

b

b

v
e

figure computation graph mlp graph unbound input b graph
concrete input c graph concrete input expected output loss
node

computation graph specific training example inputs embeddings
words black dog expected output noun whose index
pick node implements indexing operation receiving vector index
case returning corresponding entry vector
graph built straightforward run forward computation compute computation backward computation computing gradients
constructing graphs may look daunting actually easy
dedicated software libraries apis
forward computation
forward pass computes outputs nodes graph since nodes output
depends incoming edges trivial compute outputs
nodes traversing nodes topological order computing output
node given already computed outputs predecessors


figoldberg

formally graph n nodes associate node index according
topological ordering let function computed node e g multiplication
addition let parent nodes node j j
children nodes node arguments denote v output node
application output values arguments variable
input nodes constant function empty forward
computes values v n
computation graph forward pass
n

let

v v v

backward computation derivatives backprop
backward pass begins designating node n scalar output loss node
running forward computation node backward computation computes
n
gradients respect nodes value denote quantity


backpropagation used compute values nodes
backward pass fills table follows
computation graph backward pass backpropagation
n
n
p
fj

j j


fj
partial derivative fj j w r argument j

value depends function fj values v v
j arguments computed forward pass

quantity

thus order define kind node one need define two methods one
calculating forward value v nodes inputs another calculating

x
x
information automatic differentiation see work neidinger
section baydin et al depth discussion backpropagation
computation graphs called flow graphs see work bengio et al
section lecun et al b bengio popular yet technical
presentation see online post olah


fia primer neural networks nlp

software
several software packages implement computation graph model including theano
chainer penne cnn pycnn packages support essential components node types defining wide range neural network architectures covering
structures described tutorial graph creation made almost transparent
use operator overloading framework defines type representing graph nodes
commonly called expressions methods constructing nodes inputs parameters
set functions mathematical operations take expressions input
complex expressions example python code creating computation
graph figure c pycnn framework
import pycnn pc
model initialization
model pc model
pw model add parameters
pb model add parameters
pw model add parameters
pb model add parameters
words model add lookup parameters
building computation graph
pc renew cg create graph
wrap model parameters graph nodes
w pc parameter pw
b pc parameter pb
w pc parameter pw
b pc parameter pb
def get index x return place holder
generate embeddings layer
vthe
pc lookup words get index
vblack pc lookup words get index black
vdog
pc lookup words get index dog
connect leaf nodes complete graph
x pc concatenate vthe vblack vdog
output pc softmax w pc tanh w x b b
loss pc log pc pick output
loss value loss forward
loss backward gradient computed
stored corresponding
parameters

code involves initializations first block defines model parameters
shared different computation graphs recall graph corresponds
specific training example second block turns model parameters graphnode expression types third block retrieves expressions embeddings





http deeplearning net software theano
http chainer org
https bitbucket org ndnlp penne
https github com clab cnn



figoldberg

input words finally fourth block graph created note transparent
graph creation almost one one correspondence creating
graph describing mathematically last block shows forward backward
pass software frameworks follow similar patterns
theano involves optimizing compiler computation graphs blessing
curse one hand compiled large graphs run efficiently
cpu gpu making ideal large graphs fixed structure
inputs change instances however compilation step costly
makes interface bit cumbersome work contrast packages focus
building large dynamic computation graphs executing fly without
compilation step execution speed may suffer respect theanos optimized
version packages especially convenient working recurrent
recursive networks described sections well structured prediction settings
described section
implementation recipe
computation graph abstraction pseudo code network training
given
neural network training computation graph abstraction minibatches size
define network parameters
iteration n

training example xi yi dataset

loss node build computation graph xi yi parameters

loss node forward

gradients loss node backward

parameters update parameters parameters gradients
return parameters
build computation graph user defined function builds computation
graph given input output network structure returning single loss node
update parameters optimizer specific update rule recipe specifies
graph created training example accommodates cases network
structure varies training example recurrent recursive neural networks
discussed sections networks fixed structures mlps
may efficient create one base computation graph vary inputs
expected outputs examples
network composition
long networks output vector k matrix trivial compose networks
making output one network input another creating arbitrary networks
computation graph abstractions makes ability explicit node computation
graph computation graph designated output node one


fia primer neural networks nlp

design arbitrarily deep complex networks able easily evaluate train
thanks automatic forward gradient computation makes easy define
train networks structured outputs multi objective training discuss
section well complex recurrent recursive networks discussed sections

optimization issues
gradient computation taken care network trained sgd another
gradient optimization function optimized convex
long time training neural networks considered black art done
selected indeed many parameters affect optimization process care
taken tune parameters tutorial intended comprehensive
guide successfully training neural networks list prominent issues
discussion optimization techniques neural networks refer
book bengio et al ch theoretical discussion analysis refer
work glorot bengio practical tips recommendations
see work lecun et al bottou
initialization
non convexity loss function means optimization procedure may get stuck
local minimum saddle point starting different initial points e g
different random values parameters may different thus
advised run several restarts training starting different random initializations
choosing best one development set amount variance
different different network formulations datasets cannot predicted
advance
magnitude random values important effect success training
effective scheme due glorot bengio called xavier initialization
glorots first name suggests initializing weight matrix w rdin dout






w u

din dout
din dout




u b uniformly sampled random value range b suggestion
properties tanh activation function works well many occasions
preferred default initialization method many
analysis et al suggests relu non linearities weights
initialized
sampling zero mean gaussian distribution whose standard
q

deviation
din initialization found et al work better xavier
initialization image classification task especially deep networks involved
debugging reproducibility advised used fixed random seed



figoldberg

vanishing exploding gradients
deep networks common error gradients vanish become exceedingly
close explode become exceedingly high propagate back computation graph becomes severe deeper networks especially
recursive recurrent networks pascanu mikolov bengio dealing
vanishing gradients still open question solutions include making
networks shallower step wise training first train first layers auxiliary
output signal fix train upper layers complete network
real task signal performing batch normalization ioffe szegedy every
minibatch normalizing inputs network layers zero mean unit
variance specialized architectures designed assist gradient flow e g
lstm gru architectures recurrent networks discussed section dealing
exploding gradients simple effective solution clipping gradients
norm exceeds given threshold let g gradients parameters
network kgk l norm pascanu et al suggest set g threshold
kgk g
kgk threshold
saturation dead neurons
layers tanh sigmoid activations become saturated resulting output values
layer close one upper limit activation function saturated
neurons small gradients avoided layers relu activation
cannot saturated die values negative thus clipped zero
inputs resulting gradient zero layer network train
well advisable monitor network layers many saturated dead neurons
saturated neurons caused large values entering layer may controlled
changing initialization scaling range input values changing
learning rate dead neurons caused signals entering layer negative
example happen large gradient update reducing learning rate
help situation saturated layers another option normalize values
saturated layer activation e instead g h tanh h g h k tanh h
tanh h k
layer normalization effective measure countering saturation expensive
terms gradient computation related technique batch normalization due ioffe
szegedy activations layer normalized
mean variance across mini batch batch normalization techniques
became key component effective training deep networks computer vision
writing less popular natural language applications
shuffling
order training examples presented network important
sgd formulation specifies selecting random example turn practice
implementations go training example order advised shuffle training
examples pass data


fia primer neural networks nlp

learning rate
selection learning rate important large learning rates prevent network
converging effective solution small learning rates take long time
converge rule thumb one experiment range initial learning rates
range e g monitor networks loss time decrease
learning rate loss stops improving learning rate scheduling decreases rate
function number observed minibatches common schedule dividing initial
learning rate iteration number leon bottou recommends learning
rate form initial learning rate learning
rate use tth training example additional hyperparameter
recommends determining good value small sample data prior
running entire dataset
minibatches
parameter updates occur every training example minibatches size every k
training examples benefit training larger minibatch sizes
terms computation graph abstraction one create computation graph
k training examples connecting k loss nodes averaging node
whose output loss minibatch large minibatched training
beneficial terms computation efficiency specialized computing architectures
gpus replacing vector matrix operations matrix matrix operations beyond
scope tutorial
regularization
neural network many parameters overfitting easily occur overfitting
alleviated extent regularization common regularization method
l regularization placing squared penalty parameters large values adding
additive kk term objective function minimized set
model parameters k k squared l norm sum squares values
hyperparameter controlling amount regularization
recently proposed alternative regularization method dropout hinton srivastava
krizhevsky sutskever salakhutdinov dropout method designed prevent
network learning rely specific weights works randomly dropping
setting half neurons network specific layer training
example work wager et al establishes strong connection dropout
method l regularization
dropout technique one key factors contributing strong
neural network methods image classification tasks krizhevsky sutskever hinton
especially combined relu activation units dahl sainath hinton
dropout technique effective nlp applications neural networks


figoldberg

cascading multi task learning
combination online training methods automatic gradient computations
computation graph abstraction allows easy implementation model cascading
parameter sharing multi task learning
model cascading
powerful technique large networks built composing smaller
component networks example may feed forward network predicting
part speech word neighbouring words characters compose
pipeline would use network predicting parts speech
feed predictions input features neural network syntactic chunking
parsing instead could think hidden layers network encoding
captures relevant information predicting part speech cascading
take hidden layers network connect part
speech prediction inputs syntactic network
larger network takes input sequences words characters outputs
syntactic structure computation graph abstraction allows us easily propagate
error gradients syntactic task loss way back characters
combat vanishing gradient deep networks well make better
use available training material individual component networks parameters
bootstrapped training separately relevant task plugging
larger network tuning example part speech predicting network
trained accurately predict parts speech relatively large annotated corpus
plugging hidden layer syntactic parsing network less training
data available case training data provide direct supervision tasks
make use training creating network two outputs one task
computing separate loss output summing losses single node
backpropagate error gradients
model cascading common convolutional recursive recurrent
neural networks example recurrent network used encode sentence
fixed sized vector used input another network supervision
signal recurrent network comes primarily upper network consumes
recurrent networks output inputs
multi task learning
used related prediction tasks necessarily feed one another
believe information useful one type prediction useful
tasks example chunking named entity recognition ner
language modeling examples synergistic tasks information predicting chunk
boundaries named entity boundaries next word sentence rely
shared underlying syntactic semantic representation instead training separate network
task create single network several outputs common
multi layer feed forward network whose final hidden layer concatenation


fia primer neural networks nlp

hidden layers passed different output layers way parameters
network shared different tasks useful information learned one
task help disambiguate tasks computation graph abstraction
makes easy construct networks compute gradients
computing separate loss available supervision signal summing
losses single loss used computing gradients case several
corpora different kind supervision signal e g one corpus ner
another chunking training procedure shuffle available training
example performing gradient computation updates respect different loss
every turn multi task learning context language processing introduced
discussed work collobert et al examples cascaded multi task
learning feed forward network see work zhang weiss context
recurrent neural networks see work luong le sutskever vinyals kaiser
sgaard goldberg

structured output prediction
many nlp involve structured outputs cases desired output
class label distribution class labels structured object sequence
tree graph canonical examples sequence tagging e g part speech tagging
sequence segmentation chunking ner syntactic parsing section discuss
feed forward neural network used structured tasks later sections
discuss specialized neural network dealing sequences section
trees section
greedy structured prediction
greedy structured prediction decompose structure prediction
sequence local prediction training classifier perform
local decision test time trained classifier used greedy manner examples
left right tagging gimenez marquez greedy
transition parsing nivre approaches easily adapted use neural
networks simply replacing local classifier linear classifier svm
logistic regression model neural network demonstrated chen manning
lewis steedman
greedy approaches suffer error propagation mistakes early decisions
carry influence later decisions overall higher accuracy achievable nonlinear neural network classifiers helps offsetting extent addition
training techniques proposed mitigating error propagation
attempting take easier predictions harder ones easy first goldberg elhadad making training conditions similar testing conditions
exposing training procedure inputs likely mistakes hal daume iii
langford marcu goldberg nivre effective training
greedy neural network demonstrated zhang zhu easy first
tagger ballesteros goldberg dyer smith dynamic oracle training
greedy dependency parsing


figoldberg

search structured prediction
common predicting natural language structures search indepth discussion search structure prediction nlp see book smith
techniques easily adapted use neural network neural networks
literature discussed framework energy learning lecun
et al section presented setup terminology familiar
nlp community
search structured prediction formulated search possible structures
predict x arg max score x



yy x

x input structure output x typical example x sentence
tag assignment parse tree sentence x set valid
structures x looking output maximize score
x pair
scoring function defined linear model
score x w x



feature extraction function w weight vector
order make search optimal tractable structure decomposed
parts feature function defined terms parts p part local
feature extraction function
x
x
p

pparts x

part scored separately structure score sum component
parts scores

score x w x w

x

p

py

x
py

w p

x

score p



py

p shorthand p parts x decomposition parts
exists inference allows efficient search best scoring
structure given scores individual parts
one trivially replace linear scoring function parts neuralnetwork

score x

x

score p

py

x

nn c p

py

c p maps part p din dimensional vector
case one hidden layer feed forward network




fia primer neural networks nlp

score x

x

nnmlp c p

x
g c p w b w



py

py

c p rdin w rdin b rd w rd common objective structured
prediction making gold structure score higher structure leading
following generalized perceptron loss

max
score x score x





terms implementation means create computation graph cgp
possible parts calculate score run inference scored parts
best scoring structure connect output nodes computation graphs
corresponding parts gold predicted structure summing node cgy
cg connect cgy cg minus node cgl compute gradients
argued lecun et al section generalized perceptron loss may
good loss function training structured prediction neural networks
margin margin hinge loss preferred

max max
score x score x





trivial modify implementation work hinge loss
note cases lose nice properties linear model particular
model longer convex expected even simplest non linear neural
network already non convex nonetheless could still use standard neural network
optimization techniques train structured model
training inference slower evaluate neural network take
gradients parts x times
structured prediction vast field beyond scope tutorial loss
functions regularizers methods described e g smith cost augmented
decoding easily applied adapted neural network framework
probabilistic objective crf
probabilistic framework conditional random fields crf treat parts
scores clique potential see discussions smith lafferty mccallum
pereira define score structure
one keep mind resulting objectives longer convex lack formal guarantees bounds associated convex optimization similarly theory learning bounds
guarantees associated automatically transfer neural versions



figoldberg

p
exp py score p
p
scorecrf x p x p
x exp py score p
p
exp py nn p
p
p
x exp py nn p



scoring function defines conditional distribution p x
p wish set parameters network corpus conditional log likelihood xi yi training log p yi xi
maximized
loss given training example x log scorecrf x taking
gradient respect loss involved building associated computation
graph tricky part denominator partition function requires summing
potentially exponentially many structures however
dynamic programming exists efficiently solving summation polynomial
time e forward backward viterbi recurrences sequences cky insideoutside recurrences tree structures exists adapted
create polynomial size computation graph
efficient enough computing partition function available
approximate methods used example one may use beam search inference
partition function sum structures remaining beam instead
exponentially large x
sequence level crfs neural network clique potentials discussed peng bo
xu arti others applied sequence labeling
biological data ocr data speech signals wang manning
apply traditional natural language tagging tasks chunking ner hinge
used pei et al arc factored dependency parsing
probabilistic durrett klein crf constituency parser
approximate beam partition function effectively used zhou et al
transition parser
reranking
searching possible structures intractable inefficient hard integrate
model reranking methods often used reranking framework charniak
johnson collins koo base model used produce list kbest scoring structures complex model trained score candidates
k best list best structure respect gold one scored highest
search performed k items rather exponential space
complex model condition extract features arbitrary aspects scored
structure reranking methods natural candidates structured prediction neuralnetwork allow modeler focus feature extraction network
structure removing need integrate neural network scoring decoder
indeed reranking methods often used experimenting neural
straightforward integrate decoder convolutional recurrent recursive
networks discussed later sections works reranking


fia primer neural networks nlp

include schwenk et al socher et al auli et al le
zuidema zhu et al
memm hybrid approaches
formulations course possible example memm mccallum
freitag pereira trivially adapted neural network world replacing
logistic regression maximum entropy component mlp
hybrid approaches neural networks linear explored
particular weiss et al report strong transition dependency parsing
two stage model first stage static feed forward neural network mlp
trained perform well individual decisions structured
isolation second stage neural network model held fixed different layers
output well hidden layer vectors input concatenated used
input features linear structured perceptron model collins trained
perform beam search best resulting structure clear training
regime effective training single structured prediction neural network use
two simpler isolated allowed researchers perform much extensive
hyper parameter search e g tuning layer sizes activation functions learning rates
model feasible complicated networks

convolutional layers
sometimes interested making predictions ordered sets items e g
sequence words sentence sequence sentences document
consider example predicting sentiment positive negative neutral sentence
sentence words informative sentiment words less
informative good approximation informative clue informative regardless
position sentence would feed sentence words
learner let training process figure important clues one possible solution
feeding cbow representation fully connected network mlp however
downside cbow ignores ordering information completely
assigning sentences good actually quite bad bad
actually quite good exact representation global position
indicators good bad matter classification task
local ordering words word appears right word bad
important naive would suggest embedding word pairs bi grams rather
words building cbow embedded bigrams architecture
could effective huge embedding matrices scale longer ngrams suffer data sparsity share statistical strength
different n grams embedding quite good good completely
independent one another learner saw one training
able deduce anything component words
convolution pooling called convolutional neural networks cnns architecture
elegant robust solution modeling convolutional neural network
designed identify indicative local predictors large structure combine


figoldberg

produce fixed size vector representation structure capturing local aspects
informative prediction task hand
convolution pooling architectures lecun bengio evolved neural
networks vision community showed great success object detectors recognizing object predefined category cat bicycles regardless position
image krizhevsky et al applied images architecture
dimensional grid convolutions applied text mainly concerned
sequence convolutions convolutional networks introduced nlp community pioneering work collobert weston colleagues used
semantic role labeling later kalchbrenner et al kim used
sentiment question type classification
basic convolution pooling
main idea behind convolution pooling architecture language tasks apply
non linear learned function instantiation k word sliding window
sentence function called filter transforms window k words
dimensional vector captures important properties words window
dimension sometimes referred literature channel pooling
operation used combine vectors resulting different windows single
dimensional vector taking max average value observed
channels different windows intention focus important
features sentence regardless location dimensional vector
fed network used prediction gradients propagated
back networks loss training process used tune parameters
filter function highlight aspects data important task
network trained intuitively sliding window run sequence
filter function learns identify informative k grams
formally consider sequence words x x xn corresponding demb dimensional word embedding v xi convolution layer width k works
moving sliding window size k sentence applying filter
window sequence v xi v xi v xi k filter function usually
linear transformation followed non linear activation function
let concatenated vector ith window wi v xi v xi v xi k
wi rkdemb depending whether pad sentence k words side
may get n k narrow convolution n k windows wide
convolution kalchbrenner et al convolution layer vectors
p pm pi rdconv
pi g wi w b



g non linear activation function applied element wise w rkdemb dconv
b rdconv parameters network pi dconv dimensional vector encoding
refers convolution operating dimensional inputs sequences opposed
convolutions applied images



fia primer neural networks nlp


w

max

quick brown fox jumped lazy dog
quick brown

mul tanh

quick brown fox

mul tanh

brown fox jumped

mul tanh

fox jumped

mul tanh

jumped

mul tanh

lazy

mul tanh

lazy dog

mul tanh

convolution

pooling

figure convolution pooling sentence quick brown fox jumped
lazy dog narrow convolution padding added sentence
window size word translated dim embedding vector
shown embedding vectors concatenated resulting dim
window representations seven windows transfered
filter linear transformation followed element wise tanh resulting seven
dimensional filtered representations max pooling operation applied
taking max dimension resulting final dimensional pooled
vector

information wi ideally dimension captures different kind indicative information vectors combined max pooling layer resulting single
dconv dimensional vector c
cj max pi j
im



pi j denotes jth component pi effect max pooling operation get
salient information across window positions ideally dimension specialize
particular sort predictors max operation pick important
predictor type
figure provides illustration process
resulting vector c representation sentence dimension
reflects salient information respect prediction task c fed
downstream network layers perhaps parallel vectors culminating
output layer used prediction training procedure network calculates
loss respect prediction task error gradients propagated
way back pooling convolution layers well embedding layers
besides useful prediction product training procedure set parameters w b
embeddings v used convolution pooling architecture encode arbitrary length



figoldberg

max pooling common pooling operation text applications
pooling operations possible second common operation average
pooling taking average value index instead max
dynamic hierarchical k max pooling
rather performing single pooling operation entire sequence may want
retain positional information domain understanding prediction
hand end split vectors pi distinct groups apply
pooling separately group concatenate resulting dconv dimensional
vectors c c division pi groups performed domain knowledge example may conjecture words appearing early sentence
indicative words appearing late split sequence equally
sized regions applying separate max pooling region example johnson
zhang found classifying documents topics useful
average pooling regions clearly separating initial sentences topic usually
introduced later ones sentiment classification task single max pooling
operation entire sentence optimal suggesting one two strong
signals enough determine sentiment regardless position sentence
similarly relation extraction kind task may given two words asked
determine relation could argue words first word
words second word words provide three different kinds
information chen et al thus split pi vectors accordingly pooling
separately windows resulting group
another variation hierarchy convolutional layers succession convolution pooling layers stage applies convolution sequence
pools every k neighboring vectors performs convolution resulting pooled sequence
applies another convolution architecture allows sensitivity increasingly
larger structures
finally kalchbrenner et al introduced k max pooling operation
top k values dimension retained instead best one preserving
order appeared text example consider following matrix


























max pooling column vectors max pooling



following matrix
whose rows concatenated




sentences fixed size vectors sentences share kind predictive information
close



fia primer neural networks nlp

k max pooling operation makes possible pool k active indicators
may number positions apart preserves order features insensitive
specific positions discern finely number times feature
highly activated kalchbrenner et al
variations
rather single convolutional layer several convolutional layers may applied
parallel example may four different convolutional layers different
window size range capturing n gram sequences varying lengths
convolutional layer pooled resulting vectors concatenated
fed processing kim
convolutional architecture need restricted linear ordering sentence example et al generalize convolution operation work
syntactic dependency trees window around node syntactic tree
pooling performed different nodes similarly liu et al apply
convolutional architecture top dependency paths extracted dependency trees le
zuidema propose perform max pooling vectors representing different
derivations leading chart item chart parser

recurrent neural networks modeling sequences stacks
dealing language data common work sequences words
sequences letters sentences sequences words documents saw feedforward networks accommodate arbitrary feature functions sequences
use vector concatenation vector addition cbow particular cbow representations allows encode arbitrary length sequences fixed sized vectors however
cbow representation quite limited forces one disregard order features convolutional networks allow encoding sequence fixed size vector
representations derived convolutional networks improvement
cbow representation offer sensitivity word order order sensitivity
restricted mostly local patterns disregards order patterns far apart
sequence
recurrent neural networks rnns elman allow representing arbitrarily sized
structured inputs fixed size vector paying attention structured properties
input
rnn abstraction
use xi j denote sequence vectors xi xj rnn abstraction takes
input ordered list input vectors x xn together initial state vector
returns ordered list state vectors sn well ordered list output
vectors yn output vector yi function corresponding state vector
si input vectors xi presented rnn sequential fashion state
vector si output vector yi represent state rnn observing inputs
x output vector yi used prediction example model


figoldberg

predicting conditional probability event e given sequence defined
p e j x softmax yi w b j jth element output vector resulting
softmax operation rnn model provides framework conditioning
entire history x xi without resorting markov assumption traditionally
used modeling sequences indeed rnn language good
perplexity scores compared n gram
mathematically recursively defined function r takes input state
vector si input vector xi state vector si additional
function used map state vector si output vector yi constructing
rnn much constructing feed forward network one specify dimension
inputs xi well dimensions outputs yi dimensions states
si function output dimension
rnn x n n n
si r si xi



yi si
xi rdin yi rdout si rf dout
functions r across sequence positions rnn keeps
track states computation state vector kept passed
invocations r
graphically rnn traditionally presented figure
yi

si

r



xi

si

figure graphical representation rnn recursive
kth order markov assumption states observation time independent observations
times k j j given observations times k assumption
basis many sequence modeling technique n gram hidden markov
function somewhat non standard used order unify different rnn
presented next section simple rnn elman rnn gru architectures
identity mapping lstm architecture selects fixed subset state
rnn architectures state dimension independent output dimension
possible current popular architectures including simple rnn lstm gru
follow flexibility



fia primer neural networks nlp

presentation follows recursive definition correct arbitrary long sequences
however finite sized input sequence input sequences deal finite
one unroll recursion resulting structure figure




r

x







r



r

x





x

r

x





r



x



figure graphical representation rnn unrolled

usually shown visualization include parameters order
highlight fact parameters shared across time steps different
instantiations r different network structures exhibit different
properties terms running times ability trained effectively
gradient methods however adhere abstract interface
provide details concrete instantiations r simple rnn lstm
gru section lets consider modeling rnn abstraction
first note value si entire input x xi example
expanding recursion get

r x


z
r r x x


z
r r r x x x





z
r r r r x x x x
thus sn well yn could thought encoding entire input sequence
encoding useful depends definition usefulness job network
training set parameters r state conveys useful information
task tying solve
note unless r specifically designed likely later elements input
sequence stronger effect sn earlier ones



figoldberg

rnn training
viewed figure easy see unrolled rnn deep neural
network rather large computation graph somewhat complex nodes
parameters shared across many parts computation train
rnn network need create unrolled computation graph
given input sequence add loss node unrolled graph use backward
backpropagation compute gradients respect loss
procedure referred rnn literature backpropagation time bptt
werbos ways supervision signal applied
acceptor
one option base supervision signal final output vector yn viewed
way rnn acceptor observe final state decide outcome
example consider training rnn read characters word one one
use final state predict part speech word inspired ling
et al b rnn reads sentence final state decides
conveys positive negative sentiment inspired wang et al b rnn
reads sequence words decides whether valid noun phrase loss
cases defined terms function yn sn error gradients
backpropagate rest sequence see figure loss take
familiar form cross entropy hinge margin etc
encoder
similar acceptor case encoder supervision uses final output vector yn
however unlike acceptor prediction made solely basis final
vector final vector treated encoding information sequence
used additional information together signals example extractive
document summarization system may first run document rnn resulting
variants bptt include unrolling rnn fixed number input symbols
time first unroll rnn inputs x k resulting k compute loss backpropagate
error network k steps back unroll inputs xk k time sk
initial state backpropagate error k steps strategy
observations simple rnn variant gradients k steps tend vanish large enough
k omitting negligible procedure allows training arbitrarily long sequences
rnn variants lstm gru designed specifically mitigate vanishing
gradients fixed size unrolling less motivated yet still used example
language modeling book without breaking sentences similar variant unrolls
network entire sequence forward step propagates gradients back k steps
position
terminology borrowed finite state acceptors however rnn potentially infinite
number states making necessary rely function lookup table mapping states
decisions
kind supervision signal may hard train long sequences especially simplernn vanishing gradients generally hard learning task
tell process parts input focus



fia primer neural networks nlp

loss
predict
calc loss



r



x

r



x

r



x

r



x

r

x

figure acceptor rnn training graph
vector yn summarizing entire document yn used together
features order select sentences included summarization
transducer
another option treat rnn transducer producing output input
reads modeled way compute local loss signal llocal yi yi
outputs yp
true label yi loss unrolled sequence
l n
n ni llocal yi yi another combination rather sum
average weighted average see figure one example transducer
sequence tagger take xi n feature representations n words
sentence yi input predicting tag assignment word
words ccg super tagger architecture provides state art
ccg super tagging xu et al
loss

sum

predict
calc loss

predict
calc loss




r

x

predict
calc loss




r

x

predict
calc loss




r

x

predict
calc loss




r

x




r

x

figure transducer rnn training graph
natural use case transduction setup language modeling
sequence words x used predict distribution th word rnn
language shown provide better perplexities traditional language
mikolov et al sundermeyer schluter ney mikolov jozefowicz
vinyals schuster shazeer wu
rnns transducers allows us relax markov assumption traditionally taken language hmm taggers condition entire prediction


figoldberg

history power ability condition arbitrarily long histories demonstrated
generative character level rnn text generated character character character conditioning previous ones sutskever martens hinton
generated texts sensitivity properties captured n gram language
including line lengths nested parenthesis balancing good demonstration
analysis properties rnn character level language see work
karpathy johnson li
encoder decoder
finally important special case encoder scenario encoder decoder framework
cho van merrienboer bahdanau bengio sutskever et al rnn
used encode sequence vector representation yn vector representation
used auxiliary input another rnn used decoder example
machine translation setup first rnn encodes source sentence vector
representation yn state vector fed separate decoder rnn
trained predict transducer language modeling objective words
target language sentence previously predicted words well yn
supervision happens decoder rnn gradients propagated
way back encoder rnn see figure
loss

sum

predict
calc loss

predict
calc loss


sd

rd od


sd

oe

x

se

sd

rd od

x

oe

x

se

predict
calc loss



rd od

x

se

predict
calc loss


sd

rd od

x

oe

x

se

predict
calc loss

sd

rd od

x

oe

x

se

x

oe

se

x

figure encoder decoder rnn training graph
shown surprisingly effective machine translation sutskever
et al lstm rnns order technique work sutskever et al found
effective input source sentence reverse xn corresponds first


fia primer neural networks nlp

word sentence way easier second rnn establish relation
first word source sentence first word target sentence
another use case encoder decoder framework sequence transduction
order generate tags tn encoder rnn first used encode sentence
x n fixed sized vector vector fed initial state vector another
transducer rnn used together x n predict label ti position
used filippova alfonseca colmenares kaiser vinyals
model sentence compression deletion
multi layer stacked rnns
rnns stacked layers forming grid hihi bengio consider k rnns
j
rnn rnnk jth rnn states sj n outputs n
input
first rnn x n input jth rnn j outputs rnn
j
k
n
output entire formation output last rnn n
layered architectures often called deep rnns visual representation layer
rnn given figure







r







r

r

x

r

r

r

r

r

x

r

x

r


































r

r

x









r

r






r



x

figure layer deep rnn architecture
theoretically clear additional power gained deeper
architecture observed empirically deep rnns work better shallower ones
tasks particular sutskever et al report layers deep architecture crucial achieving good machine translation performance encoder decoder
framework irsoy cardie report improved moving onelayer birnn architecture several layers many works report
layered rnn architectures explicitly compare layer rnns
bidirectional rnns birnn
useful elaboration rnn bidirectional rnn birnn commonly referred
birnn schuster paliwal graves consider task sequence
tagging sentence x xn rnn allows us compute function ith word
used specific rnn architecture lstm model called bilstm



figoldberg

xi past words x including however following words
xi n may useful prediction evident common sliding window
focus word categorized window k words surrounding much
rnn relaxes markov assumption allows looking arbitrarily back
past birnn relaxes fixed window size assumption allowing look arbitrarily far
past future
consider input sequence x n birnn works maintaining two separate states
f
si sbi input position forward state sfi x x xi
backward state sbi xn xn xi forward backward states
generated two different rnns first rnn rf fed input sequence x n
second rnn rb ob fed input sequence reverse state
representation si composed forward backward states
output position concatenation two output vectors
yi yif yib sfi ob sbi taking account past future
vector yi used directly prediction fed part input
complex network two rnns run independently error gradients position flow forward backward two rnns visual
representation birnn architecture given figure
ythe

ybrown

concat

concat

sb

rb ob
f

sf

rf

xthe

concat
b

b
sb

rb ob

sb

rf

rb ob
f

sf

rf

xbrown

xfox



concat
b

f
sf

yjumped

yfox

concat
b

sb

rb ob
f

sf

rf

xjumped

b
sb

sb

rb ob
f

sf

sf

rf

x

figure birnn sentence brown fox jumped
use birnns sequence tagging introduced nlp community irsoy
cardie
rnns representing stacks
language processing including transition parsing nivre
require performing feature extraction stack instead confined
looking k top elements stack rnn framework used provide
fixed sized vector encoding entire stack
main intuition stack essentially sequence stack state
represented taking stack elements feeding order rnn resulting
final encoding entire stack order computation efficiently without


fia primer neural networks nlp

performing n stack encoding operation time stack changes rnn state
maintained together stack state stack push would
trivial whenever element x pushed stack corresponding vector x
used together rnn state si order obtain state si dealing
pop operation challenging solved persistent stack
data structure okasaki goldberg zhao huang persistent immutable
data structures keep old versions intact modified persistent stack
construction represents stack pointer head linked list empty stack
empty list push operation appends element list returning head
pop operation returns parent head keeping original list intact
point view someone held pointer previous head stack
change subsequent push operation add child node applying
procedure throughout lifetime stack tree root
empty stack path node root represents intermediary stack state
figure provides example tree process applied
computation graph construction creating rnn tree structure instead chain
structure backpropagating error given node affect elements
participated stack node created order figure shows
computation graph stack rnn corresponding last state figure
modeling proposed independently dyer et al watanabe
sumita transition dependency parsing
head

head


head







push

head




b

push b

b



head

c





push c

b

c





pop

push

head







b
head

pop

c







b

c





b

c

b

head

e

e





c





b

f

c

head
pop

push e

push f

figure immutable stack construction sequence operations push push b
push c pop push pop pop push e push f

note reading literature
unfortunately often case inferring exact model form reading
description quite challenging many aspects


figoldberg

ya e

r

ya e f

sa e

ya b xe

sa

ya



r

xa

ya b

sa

r

xb

sa b

ya c

r

sa e f

xf

sa b

r

sa b

r

xd

sa c

xc

figure stack rnn corresponding final state figure

yet standardized different researchers use terms refer slightly
different things list examples inputs rnn one hot vectors
case embedding matrix internal rnn embedded representations
input sequence padded start sequence end sequence symbols
output rnn usually assumed vector expected
fed additional layers followed softmax prediction case
presentation tutorial papers assume softmax part rnn
multi layer rnn state vector output top layer
concatenation outputs layers encoder decoder framework
conditioning output encoder interpreted different ways
top lstm architecture described next section many small
variants referred common name lstm choices
made explicit papers require careful reading others still even
mentioned hidden behind ambiguous figures phrasing
reader aware issues reading interpret model descriptions
writer aware issues well fully specify model mathematical
notation refer different source model fully specified source
available default implementation software package without knowing
details explicit fact specify software package use case
dont rely solely figures natural language text describing model
often ambiguous


fia primer neural networks nlp

concrete rnn architectures
turn present three different instantiations abstract rn n architecture
discussed previous section providing concrete definitions functions r
simple rnn srnn long short term memory lstm gated
recurrent unit gru
simple rnn
simplest rnn formulation known elman network simple rnn rnn
proposed elman explored use language modeling mikolov
rnn takes following form
si rsrnn si xi g xi wx si ws b
yi osrnn si si



si yi rds xi rdx wx rdx ds ws rds ds b rds
state position linear combination input position
previous state passed non linear activation commonly tanh relu
output position hidden state position
spite simplicity simple rnn provides strong sequence tagging
xu et al well language modeling comprehensive discussion
simple rnns language modeling see phd thesis mikolov
lstm
rnn hard train effectively vanishing gradients pascanu
et al error signals gradients later steps sequence diminish quickly
back propagation process reach earlier input signals making hard
rnn capture long range dependencies long short term memory lstm
architecture hochreiter schmidhuber designed solve vanishing gradients
main idea behind lstm introduce part state representation
memory cells vector preserve gradients across time access
memory cells controlled gating components smooth mathematical functions
simulate logical gates input state gate used decide much
input written memory cell much current content
memory cell forgotten concretely gate g n vector values
range multiplied component wise another vector v rn
added another vector values g designed close e
sigmoid function indices v corresponding near one values g allowed
pass corresponding near zero values blocked
authors treat output position complicated function state e g linear
transformation mlp presentation transformation output
considered part rnn separate computations applied rnns output



figoldberg

mathematically lstm architecture defined

sj rlstm sj xj cj hj
cj cj f g

hj tanh cj

xj wxi hj whi

f xj wxf hj whf
xj w

xo

hj w

ho





g tanh xj wxg hj whg
yj olstm sj hj

sj r dh xi rdx cj hj f g rdh wx rdx dh wh rdh dh
symbol used denote component wise product state time j composed two vectors cj hj cj memory component hj hidden
state component three gates f controlling input f orget output
gate values computed linear combinations current input xj
previous state hj passed sigmoid activation function update candidate g
computed linear combination xj hj passed tanh activation function memory cj updated forget gate controls much previous
memory keep cj f input gate controls much proposed update
keep g finally value hj output yj determined
content memory cj passed tanh non linearity controlled
output gate gating mechanisms allow gradients related memory part cj
stay high across long time ranges
discussion lstm architecture see phd thesis alex graves
well online post olah b analysis behavior
lstm used character level language model see work karpathy et al

explanation motivation behind gating mechanism lstm
gru relation solving vanishing gradient recurrent neural
networks see sections detailed course notes cho
lstms currently successful type rnn architecture responsible many state art sequence modeling main competitor
lstm rnn gru discussed next
many variants lstm architecture presented example forget gates
part original proposal hochreiter schmidhuber shown important
part architecture variants include peephole connections gate tying overview
comprehensive empirical comparison lstm architectures see work greff srivastava
koutnk steunebrink schmidhuber



fia primer neural networks nlp

practical considerations
training lstm networks jozefowicz et al strongly recommend
initialize bias term forget gate close one applying dropout
rnn lstm zaremba et al found crucial apply dropout
non recurrent connection e apply layers
sequence positions
gru
lstm architecture effective quite complicated complexity
system makes hard analyze computationally expensive work
gated recurrent unit gru recently introduced cho et al b alternative
lstm subsequently shown chung et al perform comparably
lstm several non textual datasets
lstm gru gating mechanism substantially
fewer gates without separate memory component
sj rgru sj xj z sj z sj
z xj wxz sj wsz
r xj wxr sj wsr
sj tanh xj wxs sj r wsg



yj ogru sj sj
sj sj rds xi rdx z r rds wx rdx ds ws rds ds
one gate r used control access previous state sj compute proposed update sj updated state sj serves output yj determined
interpolation previous state sj proposal sj proportions
interpolation controlled gate z
gru shown effective language modeling machine translation
however jury still gru lstm possible alternative rnn
architectures subject actively researched empirical exploration
gru lstm architectures see work jozefowicz et al
variants
gated architectures lstm gru help alleviating vanishing gradients simple rnn allow rnns capture dependencies span
long time ranges researchers explore simpler architectures lstm
gru achieving similar benefits
mikolov et al observed matrix multiplication si ws coupled
nonlinearity g update rule r simple rnn causes state vector si undergo
states often called h gru literature



figoldberg

large changes time step prohibiting remembering information long
time periods propose split state vector si slow changing component ci
context units fast changing component hi slow changing component ci
updated according linear interpolation input previous component ci
xi wx ci update allows ci accumulate previous
inputs fast changing component hi updated similarly simple rnn update
rule changed take ci account well hi xi wx hi wh ci wc
finally output yi concatenation slow fast changing parts
state yi ci hi mikolov et al demonstrate architecture provides competitive
perplexities much complex lstm language modeling tasks
mikolov et al interpreted constraining block
matrix ws rnn corresponding ci multiply identity matrix see
mikolov et al details le jaitly hinton propose even simpler
set activation function rnn relu initialize biases b
zeroes matrix ws identify matrix causes untrained rnn copy
previous state current state add effect current input xi set
negative values zero setting initial bias towards state copying training
procedure allows ws change freely le et al demonstrate simple modification
makes rnn comparable lstm number parameters several
tasks including language modeling

modeling trees recursive neural networks
rnn useful modeling sequences language processing often natural
desirable work tree structures trees syntactic trees discourse trees
even trees representing sentiment expressed parts sentence socher
et al may want predict values specific tree nodes predict values
root nodes assign quality score complete tree part tree
cases may care tree structure directly rather reason spans
sentence cases tree merely used backbone structure helps
guide encoding process sequence fixed size vector
recursive neural network recnn abstraction pollack popularized nlp
richard socher colleagues socher manning ng socher lin ng manning socher et al socher generalization rnn sequences
binary trees
much rnn encodes sentence prefix state vector recnn encodes
tree node state vector rd use state vectors predict
values corresponding nodes assign quality values node semantic
representation spans rooted nodes
depart notation mikolov et al reuse symbols used lstm description
update rule diverges rnn update rule fixing non linearity sigmoid
function bias term however changes discussed central
proposal
presented terms binary parse trees concepts easily transfer general recursively defined
data structures major technical challenge definition effective form r
combination function



fia primer neural networks nlp

main intuition behind recursive neural networks subtree represented dimensional vector representation node p children c c
function representation nodes vec p f vec c vec c f
composition function taking two dimensional vectors returning single dimensional
vector much rnn state si used encode entire sequence x recnn
state associated tree node p encodes entire subtree rooted p see figure
illustration


combine

n p

vp

combine

n p

v

figure illustration recursive neural network representations v np
combined form representation vp representations vp
np combined form representation

formal definition
consider binary parse tree n word sentence reminder ordered
unlabeled tree string x xn represented unique set triplets k j
k j triplet indicates node spanning words xi j parent
nodes spanning xi k xk j triplets form correspond terminal symbols
tree leaves words xi moving unlabeled case labeled one
represent tree set tuples b c k j whereas k j indicate spans
b c node labels nodes spanning xi j xi k xk j
respectively leaf nodes form pre terminal
symbol refer tuples production rules example consider syntactic
tree sentence boy saw duck


figoldberg


vp

np

np

det noun verb


boy

saw

det noun


duck

corresponding unlabeled labeled representations
unlabeled










labeled
det det det
noun noun noun
verb verb verb
det det det
noun noun noun
np det noun
vp verb np
np det noun
np vp

corresponding span
x
x boy
saw

duck
duck
saw duck
boy
boy saw duck


set production rules uniquely converted set tree nodes qi j
indicating node symbol span xi j simply ignoring elements
b c k production rule position define recursive neural
network
recursive neural network recnn function takes input parse tree
n word sentence x xn sentences words represented dimensional
vector xi tree represented set production rules b c j k
recnn returns output corresponding set
denote nodes qi j


inside state vectors si j inside state vector sa
j r represents corresponding
encodes entire structure rooted node sequence rnn
tree node qi j
tree shaped recnn defined recursively function r inside vector
given node defined function inside vectors direct children formally



recnn x xn sa
j r qi j

sa
v xi

b
c
sa
j r b c si k sk j


b
c
qk j

qi k

le zuidema extend recnn definition node addition inside
state vector outside state vector representing entire structure around subtree rooted
node formulation recursive computation classic inside outside
thought birnn counterpart tree recnn details see work
le zuidema



fia primer neural networks nlp

function r usually takes form simple linear transformation may
may followed non linear activation function g
c
b
c
r b c sb
k sk j g si k sk j w



formulation r ignores tree labels matrix w r dd
combinations may useful formulation case node labels exist e g
tree represent syntactic structure clearly defined labels
unreliable however labels available generally useful include
composition function one would introduce label embeddings v
mapping non terminal symbol dnt dimensional vector change r include
embedded symbols combination function
c
b
c
r b c sb
k sk j g si k sk j v v b w



w r dnt taken qian tian huang liu zhu
zhu alternative due socher et al untie weights
according non terminals different composition matrix b c pair
symbols
bc
c
b
c

r b c sb
k sk j g si k sk j w



formulation useful number non terminal symbols number
possible symbol combinations relatively small usually case phrase structure
parse trees similar model used hashimoto et al encode subtrees
semantic relation classification task
extensions variations
definitions r suffer vanishing gradients
simple rnn several authors sought replace functions inspired long shortterm memory lstm gated architecture resulting tree shaped lstms tai socher
manning zhu sobhani guo b question optimal tree representation
still much open question vast space possible combination
functions r yet explored proposed variants tree structured rnns includes
recursive matrix vector model socher huval manning ng recursive neural
tensor network socher et al first variant word represented
combination vector matrix vector defines words static semantic
content matrix acts learned operator word allowing
subtle semantic compositions addition weighted averaging implied
concatenation followed linear transformation function second variant words
associated vectors usual composition function becomes expressive
basing tensor instead matrix operations
explored literature trivial extension would condition transformation matrix




figoldberg

training recursive neural networks
training procedure recursive neural network follows recipe training
forms networks define loss spell computation graph compute gradients
backpropagation train parameters sgd
regard loss function similar sequence rnn one associate loss
root tree given node set nodes case
individual nodes losses combined usually summation loss function
labeled training data associates label quantity different tree
nodes
additionally one treat recnn encoder whereas inside vector associated node taken encoding tree rooted node encoding
potentially sensitive arbitrary properties structure vector
passed input another network
discussion recursive neural networks use natural language
tasks refer phd thesis richard socher

conclusions
neural networks powerful learners providing opportunities ranging non linear
classification non markovian modeling sequences trees hope exposition helps nlp researchers incorporate neural network work take
advantage power

references
adel h vu n schultz combination recurrent neural networks
factored language code switching language modeling proceedings
st annual meeting association computational linguistics short papers pp sofia bulgaria association computational
linguistics
ando r zhang high performance semi supervised learning method
text chunking proceedings rd annual meeting association
computational linguistics acl pp ann arbor michigan association
computational linguistics
ando r k zhang b framework learning predictive structures
multiple tasks unlabeled data journal machine learning

auli galley quirk c zweig g joint language translation modeling recurrent neural networks proceedings conference
empirical methods natural language processing pp seattle washington usa association computational linguistics
introduction computation graph abstraction specific backpropagation procedure
computing gradients recnn defined referred back propagation
structure bpts goller kuchler



fia primer neural networks nlp

auli gao j decoder integration expected bleu training recurrent
neural network language proceedings nd annual meeting
association computational linguistics short papers pp
baltimore maryland association computational linguistics
ballesteros dyer c smith n improved transition parsing
modeling characters instead words lstms proceedings conference empirical methods natural language processing pp lisbon
portugal association computational linguistics
ballesteros goldberg dyer c smith n training exploration
improves greedy stack lstm parser arxiv cs
bansal gimpel k livescu k tailoring continuous word representations
dependency parsing proceedings nd annual meeting association
computational linguistics short papers pp baltimore
maryland association computational linguistics
baydin g pearlmutter b radul siskind j automatic
differentiation machine learning survey arxiv cs
bengio practical recommendations gradient training deep architectures arxiv cs
bengio ducharme r vincent p janvin c neural probabilistic language model j mach learn res
bengio goodfellow j courville deep learning book preparation
mit press
bitvai z cohn non linear text regression deep convolutional
neural network proceedings rd annual meeting association
computational linguistics th international joint conference natural language processing short papers pp beijing china association
computational linguistics
botha j blunsom p compositional morphology word representations
language modelling proceedings st international conference
machine learning icml beijing china award best application
bottou l stochastic gradient descent tricks neural networks tricks
trade pp springer
charniak e johnson coarse fine n best parsing maxent discriminative reranking proceedings rd annual meeting association
computational linguistics acl pp ann arbor michigan association
computational linguistics
chen manning c fast accurate dependency parser neural
networks proceedings conference empirical methods natural
language processing emnlp pp doha qatar association computational linguistics


figoldberg

chen xu l liu k zeng zhao j event extraction via dynamic
multi pooling convolutional neural networks proceedings rd annual
meeting association computational linguistics th international
joint conference natural language processing long papers pp
beijing china association computational linguistics
cho k natural language understanding distributed representation
arxiv cs stat
cho k van merrienboer b bahdanau bengio properties
neural machine translation encoderdecoder approaches proceedings ssst eighth workshop syntax semantics structure statistical translation
pp doha qatar association computational linguistics
cho k van merrienboer b gulcehre c bahdanau bougares f schwenk h
bengio b learning phrase representations rnn encoderdecoder
statistical machine translation proceedings conference empirical
methods natural language processing emnlp pp doha qatar
association computational linguistics
chrupala g normalizing tweets edit scripts recurrent neural embeddings
proceedings nd annual meeting association computational linguistics short papers pp baltimore maryland association
computational linguistics
chung j gulcehre c cho k bengio empirical evaluation gated
recurrent neural networks sequence modeling arxiv cs
collins discriminative training methods hidden markov theory
experiments perceptron proceedings conference empirical methods natural language processing pp association
computational linguistics
collins koo discriminative reranking natural language parsing
computational linguistics
collobert r weston j unified architecture natural language processing
deep neural networks multitask learning proceedings th international
conference machine learning pp acm
collobert r weston j bottou l karlen kavukcuoglu k kuksa p
natural language processing almost scratch journal machine learning

crammer k singer algorithmic implementation multiclass kernelbased vector machines journal machine learning
creutz lagus k unsupervised morpheme segmentation
morphology learning acm trans speech lang process
cybenko g approximation superpositions sigmoidal function mathematics
control signals systems


fia primer neural networks nlp

dahl g sainath hinton g improving deep neural networks lvcsr
rectified linear units dropout ieee international conference
acoustics speech signal processing icassp pp
dauphin n pascanu r gulcehre c cho k ganguli bengio
identifying attacking saddle point high dimensional non convex
optimization ghahramani z welling cortes c lawrence n weinberger k q eds advances neural information processing systems pp
curran associates inc
de gispert iglesias g byrne b fast accurate preordering smt
neural networks proceedings conference north american
chapter association computational linguistics human language technologies pp denver colorado association computational linguistics
arti others neural conditional random fields international
conference artificial intelligence statistics pp
dong l wei f tan c tang zhou xu k adaptive recursive neural
network target dependent twitter sentiment classification proceedings
nd annual meeting association computational linguistics
short papers pp baltimore maryland association computational
linguistics
dong l wei f zhou xu k question answering freebase
multi column convolutional neural networks proceedings rd annual
meeting association computational linguistics th international
joint conference natural language processing long papers pp
beijing china association computational linguistics
dos santos c gatti deep convolutional neural networks sentiment
analysis short texts proceedings coling th international conference computational linguistics technical papers pp dublin ireland
dublin city university association computational linguistics
dos santos c xiang b zhou b classifying relations ranking
convolutional neural networks proceedings rd annual meeting
association computational linguistics th international joint conference natural language processing long papers pp beijing
china association computational linguistics
dos santos c zadrozny b learning character level representations partof speech tagging proceedings st international conference machine
learning icml pp
duchi j hazan e singer adaptive subgradient methods online learning
stochastic optimization journal machine learning

duh k neubig g sudoh k tsukada h adaptation data selection neural language experiments machine translation proceedings


figoldberg

st annual meeting association computational linguistics short papers pp sofia bulgaria association computational
linguistics
durrett g klein neural crf parsing proceedings rd annual
meeting association computational linguistics th international
joint conference natural language processing long papers pp
beijing china association computational linguistics
dyer c ballesteros ling w matthews smith n transitionbased dependency parsing stack long short term memory proceedings
rd annual meeting association computational linguistics
th international joint conference natural language processing long
papers pp beijing china association computational linguistics
elman j l finding structure time cognitive science
faruqui dyer c improving vector space word representations multilingual correlation proceedings th conference european chapter
association computational linguistics pp gothenburg sweden
association computational linguistics
filippova k alfonseca e colmenares c kaiser l vinyals sentence
compression deletion lstms proceedings conference
empirical methods natural language processing pp lisbon portugal
association computational linguistics
forcada l neco r p recursive hetero associative memories translation
biological artificial computation neuroscience technology pp
springer
gao j pantel p gamon x deng l modeling interestingness
deep neural networks proceedings conference empirical methods
natural language processing emnlp pp doha qatar association
computational linguistics
gimenez j marquez l svmtool general pos tagger generator
support vector machines proceedings th lrec lisbon portugal
glorot x bengio understanding difficulty training deep feedforward
neural networks international conference artificial intelligence statistics
pp
glorot x bordes bengio deep sparse rectifier neural networks
international conference artificial intelligence statistics pp
goldberg elhadad efficient easy first non directional
dependency parsing human language technologies annual conference
north american chapter association computational linguistics pp
los angeles california association computational linguistics
goldberg levy word vec explained deriving mikolov et al negativesampling word embedding method arxiv cs stat


fia primer neural networks nlp

goldberg nivre j training deterministic parsers non deterministic
oracles transactions association computational linguistics

goldberg zhao k huang l efficient implementation beam search
incremental parsers proceedings st annual meeting association
computational linguistics short papers pp sofia bulgaria
association computational linguistics
goller c kuchler learning task dependent distributed representations
backpropagation structure proc icnn pp
ieee
gouws bengio corrado g bilbowa fast bilingual distributed representations without word alignments proceedings nd international
conference machine learning pp
graves supervised sequence labelling recurrent neural networks ph
thesis technische universitat munchen
greff k srivastava r k koutnk j steunebrink b r schmidhuber j
lstm search space odyssey arxiv cs
hal daume iii langford j marcu search structured prediction
machine learning journal mlj
harris z distributional structure word
hashimoto k miwa tsuruoka chikayama simple customization
recursive neural networks semantic relation classification proceedings
conference empirical methods natural language processing pp
seattle washington usa association computational linguistics
k zhang x ren sun j delving deep rectifiers surpassing
human level performance imagenet classification arxiv cs
henderson thomson b young deep neural network
dialog state tracking challenge proceedings sigdial conference
pp metz france association computational linguistics
hermann k blunsom p role syntax vector space
compositional semantics proceedings st annual meeting association computational linguistics long papers pp sofia
bulgaria association computational linguistics
hermann k blunsom p multilingual compositional distributed
semantics proceedings nd annual meeting association computational linguistics long papers pp baltimore maryland
association computational linguistics
hihi e bengio hierarchical recurrent neural networks long term
dependencies touretzky mozer c hasselmo e eds advances
neural information processing systems pp mit press


figoldberg

hill f cho k jean devin c bengio embedding word similarity
neural machine translation arxiv cs
hinton g e srivastava n krizhevsky sutskever salakhutdinov r r
improving neural networks preventing co adaptation feature detectors
arxiv cs
hochreiter schmidhuber j long short term memory neural computation

hornik k stinchcombe white h multilayer feedforward networks
universal approximators neural networks
ioffe szegedy c batch normalization accelerating deep network training
reducing internal covariate shift arxiv cs
irsoy cardie c opinion mining deep recurrent neural networks
proceedings conference empirical methods natural language
processing emnlp pp doha qatar association computational linguistics
iyyer boyd graber j claudino l socher r daume iii h neural
network factoid question answering paragraphs proceedings
conference empirical methods natural language processing emnlp pp
doha qatar association computational linguistics
iyyer enns p boyd graber j resnik p b political ideology detection
recursive neural networks proceedings nd annual meeting
association computational linguistics long papers pp
baltimore maryland association computational linguistics
iyyer manjunatha v boyd graber j daume iii h deep unordered
composition rivals syntactic methods text classification proceedings
rd annual meeting association computational linguistics th
international joint conference natural language processing long papers pp beijing china association computational linguistics
johnson r zhang effective use word order text categorization
convolutional neural networks proceedings conference north
american chapter association computational linguistics human language technologies pp denver colorado association computational
linguistics
jozefowicz r vinyals schuster shazeer n wu exploring
limits language modeling arxiv cs
jozefowicz r zaremba w sutskever empirical exploration recurrent network architectures proceedings nd international conference
machine learning icml pp
kalchbrenner n grefenstette e blunsom p convolutional neural network
modelling sentences proceedings nd annual meeting association computational linguistics long papers pp baltimore
maryland association computational linguistics


fia primer neural networks nlp

karpathy johnson j li f f visualizing understanding recurrent
networks arxiv cs
kim convolutional neural networks sentence classification proceedings conference empirical methods natural language processing
emnlp pp doha qatar association computational linguistics
kim jernite sontag rush character aware neural language
arxiv cs stat
kingma ba j
arxiv cs

adam method stochastic optimization

krizhevsky sutskever hinton g e imagenet classification deep
convolutional neural networks pereira f burges c j c bottou l weinberger k q eds advances neural information processing systems pp
curran associates inc
kudo matsumoto fast methods kernel text analysis
proceedings st annual meeting association computational linguistics acl pp stroudsburg pa usa association computational
linguistics
lafferty j mccallum pereira f c conditional random fields probabilistic
segmenting labeling sequence data proceedings icml
le p zuidema w inside outside recursive neural network model
dependency parsing proceedings conference empirical methods
natural language processing emnlp pp doha qatar association
computational linguistics
le p zuidema w forest convolutional network compositional distributional semantics neural chart without binarization proceedings
conference empirical methods natural language processing pp
lisbon portugal association computational linguistics
le q v jaitly n hinton g e simple way initialize recurrent networks
rectified linear units arxiv cs
lecun bengio convolutional networks images speech timeseries arbib ed handbook brain theory neural networks
mit press
lecun bottou l orr g muller k efficient backprop orr g
k eds neural networks tricks trade springer
lecun bottou l bengio haffner p b gradient learning applied
pattern recognition proceedings ieee
lecun chopra hadsell r ranzato huang f tutorial energybased learning predicting structured data
lecun huang f loss functions discriminative training energybased
proceedings aistats aistats


figoldberg

lee g flowers dyer g learning distributed representations conceptual knowledge application script story processing connectionist
natural language processing pp springer
levy goldberg dependency word embeddings proceedings
nd annual meeting association computational linguistics
short papers pp baltimore maryland association computational
linguistics
levy goldberg b neural word embedding implicit matrix factorization ghahramani z welling cortes c lawrence n weinberger
k q eds advances neural information processing systems pp
curran associates inc
levy goldberg dagan improving distributional similarity
lessons learned word embeddings transactions association computational linguistics
lewis steedman improved ccg parsing semi supervised supertagging transactions association computational linguistics
li j li r hovy e recursive deep discourse parsing proceedings conference empirical methods natural language processing
emnlp pp doha qatar association computational linguistics
ling w dyer c black w trancoso two simple adaptations
word vec syntax proceedings conference north
american chapter association computational linguistics human language technologies pp denver colorado association computational
linguistics
ling w dyer c black w trancoso fermandez r amir marujo l
luis b finding function form compositional character
open vocabulary word representation proceedings conference
empirical methods natural language processing pp lisbon portugal
association computational linguistics
liu wei f li ji h zhou wang h dependency neural
network relation classification proceedings rd annual meeting
association computational linguistics th international joint conference natural language processing short papers pp beijing
china association computational linguistics
luong le q v sutskever vinyals kaiser l multi task sequence
sequence learning arxiv cs stat
j zhang zhu j tagging web building robust web tagger
neural network proceedings nd annual meeting association computational linguistics long papers pp baltimore
maryland association computational linguistics
huang l zhou b xiang b dependency convolutional neural
networks sentence embedding proceedings rd annual meeting


fia primer neural networks nlp

association computational linguistics th international joint conference natural language processing short papers pp beijing
china association computational linguistics
mccallum freitag pereira f c maximum entropy markov
information extraction segmentation icml vol pp
mikolov chen k corrado g dean j efficient estimation word
representations vector space arxiv cs
mikolov joulin chopra mathieu ranzato learning longer
memory recurrent neural networks arxiv cs
mikolov karafiat burget l cernocky j khudanpur recurrent
neural network language model interspeech th annual conference international speech communication association makuhari chiba
japan september pp
mikolov kombrink lukas burget cernocky j h khudanpur extensions recurrent neural network language model acoustics speech signal
processing icassp ieee international conference pp ieee
mikolov sutskever chen k corrado g dean j distributed representations words phrases compositionality burges c j c
bottou l welling ghahramani z weinberger k q eds advances
neural information processing systems pp curran associates inc
mikolov statistical language neural networks ph thesis ph
thesis brno university technology
mnih kavukcuoglu k learning word embeddings efficiently noisecontrastive estimation burges c j c bottou l welling ghahramani z
weinberger k q eds advances neural information processing systems
pp curran associates inc
mrksic n seaghdha thomson b gasic su p h vandyke wen h
young multi domain dialog state tracking recurrent neural
networks proceedings rd annual meeting association computational linguistics th international joint conference natural language
processing short papers pp beijing china association
computational linguistics
neidinger r introduction automatic differentiation matlab objectoriented programming siam review
nesterov method solving convex programming convergence
rate k soviet mathematics doklady vol pp
nesterov introductory lectures convex optimization kluwer academic publishers
nguyen h grishman r event detection domain adaptation
convolutional neural networks proceedings rd annual meeting


figoldberg

association computational linguistics th international joint conference natural language processing short papers pp beijing
china association computational linguistics
nivre j deterministic incremental dependency parsing computational linguistics
okasaki c purely functional data structures cambridge university press cambridge u k york
olah c calculus computational graphs backpropagation retrieved
http colah github io posts backprop
olah c b understanding lstm networks retrieved http colah
github io posts understanding lstms
pascanu r mikolov bengio difficulty training recurrent
neural networks arxiv cs
pei w ge chang b effective neural network model graph
dependency parsing proceedings rd annual meeting association
computational linguistics th international joint conference natural
language processing long papers pp beijing china association computational linguistics
peng j bo l xu j conditional neural fields bengio schuurmans
lafferty j williams c k culotta eds advances neural
information processing systems pp curran associates inc
pennington j socher r manning c glove global vectors word representation proceedings conference empirical methods natural
language processing emnlp pp doha qatar association computational linguistics
pollack j b recursive distributed representations artificial intelligence

polyak b methods speeding convergence iteration methods
ussr computational mathematics mathematical physics
qian q tian b huang liu zhu x zhu x learning tag embeddings
tag specific composition functions recursive neural network proceedings
rd annual meeting association computational linguistics
th international joint conference natural language processing long
papers pp beijing china association computational linguistics
rong x word vec parameter learning explained arxiv cs
rumelhart e hinton g e williams r j learning representations
back propagating errors nature
schuster paliwal k k bidirectional recurrent neural networks ieee
transactions signal processing


fia primer neural networks nlp

schwenk h dchelotte gauvain j l continuous space language
statistical machine translation proceedings coling acl main
conference poster sessions pp association computational linguistics
shawe taylor j cristianini n kernel methods pattern analysis cambridge
university press
smith n linguistic structure prediction synthesis lectures human language technologies morgan claypool
socher r recursive deep learning natural language processing computer
vision ph thesis stanford university
socher r bauer j manning c ng parsing compositional
vector grammars proceedings st annual meeting association
computational linguistics long papers pp sofia bulgaria
association computational linguistics
socher r huval b manning c ng semantic compositionality
recursive matrix vector spaces proceedings joint conference
empirical methods natural language processing computational natural
language learning pp jeju island korea association computational
linguistics
socher r lin c c ng manning c parsing natural scenes
natural language recursive neural networks getoor l scheffer
eds proceedings th international conference machine learning icml
bellevue washington usa june july pp omnipress
socher r manning c ng learning continuous phrase representations
syntactic parsing recursive neural networks proceedings deep
learning unsupervised feature learning workshop nips pp

socher r perelygin wu j chuang j manning c ng potts c
recursive deep semantic compositionality sentiment treebank
proceedings conference empirical methods natural language
processing pp seattle washington usa association computational
linguistics
sgaard goldberg deep multi task learning low level tasks supervised
lower layers proceedings th annual meeting association
computational linguistics short papers pp association
computational linguistics
sordoni galley auli brockett c ji mitchell nie j gao j
dolan b neural network context sensitive generation
conversational responses proceedings conference north
american chapter association computational linguistics human language technologies pp denver colorado association computational
linguistics
sundermeyer alkhouli wuebker j ney h translation modeling
bidirectional recurrent neural networks proceedings conference


figoldberg

empirical methods natural language processing emnlp pp doha
qatar association computational linguistics
sundermeyer schluter r ney h lstm neural networks language
modeling interspeech
sutskever martens j dahl g hinton g importance initialization
momentum deep learning proceedings th international conference
machine learning icml pp
sutskever martens j hinton g e generating text recurrent neural
networks proceedings th international conference machine learning
icml pp
sutskever vinyals le q v v sequence sequence learning
neural networks ghahramani z welling cortes c lawrence n
weinberger k q eds advances neural information processing systems pp
curran associates inc
tai k socher r manning c improved semantic representations
tree structured long short term memory networks proceedings rd annual meeting association computational linguistics th international joint conference natural language processing long papers
pp beijing china association computational linguistics
tamura watanabe sumita e recurrent neural networks word
alignment model proceedings nd annual meeting association
computational linguistics long papers pp baltimore
maryland association computational linguistics
telgarsky benefits depth neural networks arxiv cs stat
tieleman hinton g lecture rmsprop divide gradient running
average recent magnitude coursera neural networks machine learning
van de cruys neural network selectional preference acquisition proceedings conference empirical methods natural language processing emnlp pp doha qatar association computational
linguistics
vaswani zhao fossum v chiang decoding large scale neural language improves translation proceedings conference
empirical methods natural language processing pp seattle washington usa association computational linguistics
wager wang liang p dropout training adaptive regularization
burges c j c bottou l welling ghahramani z weinberger k q
eds advances neural information processing systems pp curran
associates inc
wang manning c effect non linear deep architecture sequence
labeling ijcnlp pp


fia primer neural networks nlp

wang p xu j xu b liu c zhang h wang f hao h semantic clustering convolutional neural network short text categorization proceedings
rd annual meeting association computational linguistics
th international joint conference natural language processing short
papers pp beijing china association computational linguistics
wang x liu sun c wang b wang x b predicting polarities tweets
composing word embeddings long short term memory proceedings
rd annual meeting association computational linguistics
th international joint conference natural language processing long
papers pp beijing china association computational linguistics
watanabe sumita e transition neural constituent parsing proceedings rd annual meeting association computational linguistics
th international joint conference natural language processing
long papers pp beijing china association computational linguistics
weiss alberti c collins petrov structured training neural
network transition parsing proceedings rd annual meeting
association computational linguistics th international joint conference natural language processing long papers pp beijing
china association computational linguistics
werbos p j backpropagation time
proceedings ieee
weston j bordes yakhnenko usunier n connecting language
knowledge bases embedding relation extraction proceedings
conference empirical methods natural language processing pp
seattle washington usa association computational linguistics
xu w auli clark ccg supertagging recurrent neural network
proceedings rd annual meeting association computational linguistics th international joint conference natural language processing
short papers pp beijing china association computational
linguistics
yin w schutze h convolutional neural network paraphrase identification
proceedings conference north american chapter association computational linguistics human language technologies pp
denver colorado association computational linguistics
zaremba w sutskever vinyals recurrent neural network regularization
arxiv cs
zeiler adadelta adaptive learning rate method arxiv
cs
zeng liu k lai zhou g zhao j relation classification via convolutional deep neural network proceedings coling th international


figoldberg

conference computational linguistics technical papers pp dublin
ireland dublin city university association computational linguistics
zhang weiss stack propagation improved representation learning syntax proceedings th annual meeting association computational
linguistics long papers pp association computational
linguistics
zhou h zhang huang chen j neural probabilistic structuredprediction model transition dependency parsing proceedings
rd annual meeting association computational linguistics th
international joint conference natural language processing long papers pp beijing china association computational linguistics
zhu c qiu x chen x huang x ranking model dependency
parser recursive convolutional neural network proceedings rd
annual meeting association computational linguistics th international joint conference natural language processing long papers
pp beijing china association computational linguistics
zhu x sobhani p guo h b long short term memory tree structures
arxiv cs




