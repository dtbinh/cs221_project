Journal Artificial Intelligence Research 57 (2016) 421-464

Submitted 06/16; published 11/16

Embarrassingly Parallel Search Constraint Programming
Arnaud Malapert
Jean-Charles Regin
Mohamed Rezgui

arnaud.malapert@unice.fr
jean-charles.regin@unice.fr
rezgui@i3s.unice.fr

Universite Cote dAzur, CNRS, I3S, France

Abstract
introduce Embarrassingly Parallel Search (EPS) method solving constraint
problems parallel, show method matches even outperforms state-ofthe-art algorithms number problems using various computing infrastructures. EPS
simple method master decomposes problem many disjoint subproblems solved independently workers. approach three advantages:
efficient method; involves almost communication synchronization
workers; implementation made easy master workers rely
underlying constraint solver, require modify it. paper describes
method, applications various constraint problems (satisfaction, enumeration,
optimization). show method adapted different underlying solvers
(Gecode, Choco2, OR-tools) different computing infrastructures (multi-core, data centers, cloud computing). experiments cover unsatisfiable, enumeration optimization
problems, cover first solution search makes results hard analyze. variability observed optimization problems, lesser
extent optimality proof required. EPS offers good average performance,
matches outperforms available parallel implementations Gecode well
solvers portfolios. Moreover, perform in-depth analysis various factors
make approach efficient well anomalies occur. Last, show
decomposition key component efficiency load balancing.

1. Introduction
second half 20th century, frequency processors doubled every 18 months
so. clear years period free lunch, put Sutter
Larus (2005), behind us. outlined Bordeaux, Hamadi, Samulowitz (2009),
available computational power keep increasing exponentially, increase
terms number available processors, terms frequency per unit. Multi-core
processors norm raises significant challenges software development.
Data centers high-performance computing readily accessible many academia
industry. Cloud computing (Amazon, Microsoft Azure, Google, . . . ) offers massive
infrastructures rent computing storage used demand.
facilities anyone gain access super-computing facilities moderate cost.
Distributed Computing offers possibilities put computational resources common
effectively obtains massive capabilities. Examples include Seti@home (Anderson, Cobb,
Korpela, Lebofsky, & Werthimer, 2002), Distributed.net (Distributed Computing Technologies Inc, 20) Sharcnet (Bauer, 2007). main challenge therefore scale, i.e.,
cope growth.
c
2016
AI Access Foundation. rights reserved.

fiMalapert, Regin, & Rezgui

Constraint programming (CP) appealing technology variety combinatorial
problems grown steadily last three decades. strengths CP
use constraint propagation combined efficient search algorithms. Constraint
propagation aims removing combinations values variable domains cannot
appear solution. number years, possible gains offered parallel
computing attracted attention.
Parallel computing form computation many calculations carried
simultaneously (Almasi & Gottlieb, 1989) operating principle large problems
often divided smaller ones, solved parallel. Different forms
parallel computing exist: bit-level, instruction level, data task parallelism. Task
parallelism common approach parallel branch-and-bound (B&B) algorithms (Mattson, Sanders, & Massingill, 2004) achieved processor executes different
thread (or process) different data. Parallel computer programs difficult write sequential ones, concurrency introduces several new classes
potential software bugs, race conditions common. example,
memory shared, several tasks algorithm modify data
time. could render program incorrect. Mutual exclusion allows worker lock
certain resources obtain exclusive access, create starvation
workers must wait worker frees resources. Moreover, indeterminism
parallel programs makes behaviour execution unpredictable, i.e. results
different program runs may differ. So, communication synchronization among different
sub-tasks address issue, typically greatest obstacles good
performance. Another central bottleneck load balancing, i.e. keeping processors busy
much possible.
Wilkinson Allen (2005) introduced Embarrassingly Parallel paradigm assumes computation divided number completely independent parts
part executed separate processor. paper, introduce
Embarrassingly Parallel Search (EPS) method constraint problems show
method often outperforms state-of-the-art parallel B&B algorithms number
problems various computing infrastructures. master decomposes problem
many disjoint subproblems solved independently workers. Since constraint program trivially embarrassingly parallel, decomposition procedure must
carefully designed. approach three advantages: efficient method;
involves almost communication, synchronization, mutual exclusion workers;
implementation simple master workers rely underlying
constraint solver require modify it. Additionally, deterministic
certain restrictions.
paper integrates results series publications (Regin, Rezgui, & Malapert,
2013, 2014; Rezgui, Regin, & Malapert, 2014). However, paper includes novel contributions, implementations, results. new implementation EPS top
Java library Choco2 (Choco, 2010) uses new decomposition procedure. New results
given implementations top C++ library Gecode (Schulte, 2006)
OR-tools (Perron, Nikolaj, & Vincent, 2012), problems types instances
tested. EPS compared parallelizations Gecode several static
422

fiEmbarrassingly Parallel Search CP

solvers portfolios, perform in-depth analysis various components, especially decomposition procedures, well anomalies occur.
paper organized follows. Section 2 presents constraint programming background, Amdahls law, related work parallel constraint solving. Section 3 gives
detailed description embarrassingly parallel search method. Section 4 gives extensive experimental results various implementations (Gecode, Choco2, OR-tools)
different computing infrastructures (multi-core, data center, cloud computing) well
comparisons state-of-the-art parallel implementations static solver portfolios.

2. Related Work
Here, present constraint programming background, two important parallelization
measures related Amdahls law, related work parallel constraint solving.
2.1 Constraint Programming Background
Constraint programming (CP) attracted high attention among experts many areas
potential solving hard real-life problems. extensive review
constraint programming, refer reader handbook Rossi, Van Beek,
Walsh (2006). constraint satisfaction problem (CSP) consists set X variables
defined corresponding set possible values (the domains D) set C constraints.
constraint relation subset variables restricts possible values
variables take simultaneously. important feature constraints declarative
manner, i.e. specify relationship must hold. current domain D(x)
variable x X always (non-strict) subset initial domain. partial assignment
represents case domains variables reduced singleton
(namely variable assigned value). solution CSP assignment
value variable constraints simultaneously satisfied.
Solutions found searching systematically possible assignments
values variables. backtracking scheme incrementally extends partial assignment
specifies consistent values variables, toward complete solution,
repeatedly choosing value another variable. variables labeled (given value)
sequentially. node search tree, uninstantiated variable selected
node extended resulting new branches node represent alternative
choices may examined order find solution. branching strategy
determines next variable instantiated, order values
domain selected. partial assignment violates constraints, backtracking
performed recently assigned variable still alternative values available
domain. Clearly, whenever partial assignment violates constraint, backtracking
able eliminate subspace Cartesian product variable domains.
filtering algorithm associated constraint removes inconsistent values
domains variables, i.e. assignments cannot belong solution
constraint. Constraints handled constraint propagation mechanism
allows reduction domains variables global fixpoint reached (no
domain reductions possible). fact, constraint specifies relationship must hold
filtering algorithm computational procedure enforces relationship.
423

fiMalapert, Regin, & Rezgui

Generally, consistency techniques complete, i.e. remove inconsistent
values domains variables.
backtracking scheme consistency techniques used alone completely
solve CSP, combination allows search space explored complete
efficient way. propagation mechanism allows reduction variable
domains pruning search tree whereas branching strategy improve
detection solutions (or failures unsatisfiable problems).
Here, consider complete standard backtracking scheme depth-first traversal
search tree combined following variable selection strategies. Note different
variable selection strategies used although one time. lex selects variable
according lexicographic ordering. dom selects variable smallest remaining domain (Haralick & Elliott, 1980). ddeg selects variable largest dynamic degree (Beck,
Prosser, & Wallace, 2005), is, variable constrained largest number
unassigned variables. Boussemart, Hemery, Lecoutre, Sais (2004) proposed conflictdirected variable ordering heuristics every time constraint causes failure
search, weight incremented one. variable weighted degree,
sum weights constraints variable occurs. wdeg selects
variable largest weighted degree. current domain variable incorporated give dom/ddeg dom/wdeg selects variable minimum ratio
current domain size dynamic weighted degree (Boussemart et al., 2004;
Beck et al., 2005). dom/bwdeg variant follows binary labeling scheme. impact
selects variable/value pair strongest impact, i.e. leads strongest
search space reduction (Refalo, 2004).
optimization problems, consider standard top-down algorithm maintains
lower bound, lb, upper bound, ub, objective value. ub lb, subtree
pruned cannot contain better solution.
2.2 Parallelization Measures Amdahls Law
Two important parallelization measures speedup efficiency. Let t(c) wallclock time parallel algorithm c number cores let t(1)
wall-clock time sequential algorithm. speedup su(c) = t(1) / t(c) measure
indicating many times parallel algorithm performs faster due parallelization.
efficiency eff (c) = su(c) / c normalized version speedup, speedup
value divided number cores. maximum possible speedup single program
result parallelization known Amdahls law (Amdahl, 1967). states
small portion program cannot parallelized limit overall speedup
available parallelization. Let B [0, 1] fraction algorithm strictly
sequential, time t(c)
algorithm takes finish executed c cores
corresponds to: t(c) = t(1) B + 1c (1 B) . Therefore, theoretical speedup su(c) is:

su(c) =

1
B + (1 B)
1
c

424

fiEmbarrassingly Parallel Search CP

According Amdahls law, speedup never exceed number cores, i.e. linear
speedup. This, terms efficiency measure, means efficiency always less
1.
Note sequential parallel B&B algorithms always explore
search space. Therefore, super-linear speedups parallel B&B algorithms contradiction Amdahls law processors access high quality solutions early
iterations, turn brought reduction search tree problem size.
2.3 Parallel Constraint Solving
Designing developing parallel programs manual process programmer responsible identifying implementing parallelism (Barney & Livermore, 2016). section, discuss parallel constraint solving. parallel
logic programming, refer reader surveys De Kergommeaux Codognet
(1994), Gupta, Pontelli, Ali, Carlsson, Hermenegildo (2001). parallel integer
programming, refer reader surveys Crainic, Le Cun, Roucairol (2006),
Bader, Hart, Phillips (2005), Gendron Crainic (1994).
main approaches parallel constraint solving roughly divided following main categories: search space shared memory; search space splitting; portfolio algorithms; problem splitting. approaches require communication synchronization,
important issue load balancing refers practice distributing
approximately equal amounts work among tasks processors kept busy
time.
2.3.1 Search Space Shared Memory
methods implemented many cores sharing list open nodes
search tree (nodes least one children still unvisited).
Starved processors pick promising node list expand it.
defining different node evaluation functions, one implement different strategies (DFS,
BFS others). Perron (1999) proposed comprehensive framework tested
4 processors. Vidal, Bordeaux, Hamadi (2010) reported good performance parallel
best-first search 64 processors. Although kind mechanism intrinsically provides
excellent load balancing, known scale beyond certain number processors;
beyond point, performance starts decrease. Indeed, shared memory system,
threads must contend communicating memory problem
exacerbated cache consistency transactions.
2.3.2 Search Space Splitting
Search Space Splitting strategies exploring parallelism provided search space
common approaches: branching done, different branches explored
parallel (Pruul, Nemhauser, & Rushmeier, 1988). One challenge load balancing:
branches search tree typically extremely imbalanced require non-negligible
overhead communication work stealing (Lai & Sahni, 1984).
work stealing method originally proposed Burton Sleep (1981) first
implemented Lisp parallel machines (Halstead, 1984). search space dynamically
425

fiMalapert, Regin, & Rezgui

split resolution. worker finished explore subproblem, asks
workers another subproblem. another worker agrees demand, splits
dynamically current subproblem two disjoint subproblems sends one subproblem
starving worker. starving worker steals work busy one. Note
form locking necessary avoid several starving workers steal
subproblems. starving worker asks workers turn receives new
subproblem. Termination work stealing method must carefully designed reduce
overhead almost workers starving, almost work remains. Recent works
based approach Zoeteweij Arbab (2004), Jaffar, Santosa, Yap,
Zhu (2004), Michel, See, Hentenryck (2009), Chu, Schulte, Stuckey (2009).
work stealing uses communication, synchronization computation time,
cannot easily scaled thousands processors. address issues, Xie
Davenport (2010) allocated specific processors coordination tasks, allowing increase
number processors (linear scaling 256 processors) used
parallel supercomputer performance starts decline.
Machado, Pedro, Abreu (2013) proposed hierarchical work stealing scheme correlated cluster physical infrastructure, order reduce communication overhead.
worker first tries steal local node, considering remote nodes (starting
closest remote node). approach achieved good scalability 512 cores
n-queens quadratic assignment problems. constraint optimization problems,
maintaining best solution worker would require large communication
synchronization overhead. But, Machado et al. observed scalability lowered
lazy dissemination so-far best solution, i.e. workers use
obsolete best solution.
General-purpose programming languages designed multi-threaded parallel computing
Charm++ (Kale & Krishnan, 1993) Cilk++ (Leiserson, 2010; Budiu, Delling, &
Werneck, 2011) ease implementation work stealing approaches. Otherwise,
work stealing framework Bobpp (Galea & Le Cun, 2007; Le Cun, Menouer, & VanderSwalmen, 2007) provides interface solvers parallel computers. Bobpp,
work shared via global priority queue search tree decomposed allocated
different cores demand search algorithm execution. Periodically, worker
tests starving workers exist. case, worker stops search path
root node highest right open node saved inserted global priority
queue. Then, worker continues search left open node. Otherwise,
starving worker exists, worker continues search locally using solver. starving
workers notified insertions global priority queue, one picks
node starts search. Using OR-tools underlying solver, Menouer Le Cun
(2013), Menouer Le Cun (2014) observed good speedups Golomb Ruler
problem 13 marks (41.3 48 workers) 16-queens problem (8.63 12
workers). experiments investigate exploration overhead caused approach.
Bordeaux et al. (2009) proposed another promising approach based search space
splitting mechanism based work stealing approach. use hashing function
allocating implicitly leaves processors. processor applies search
strategy allocated search space. Well-designed hashing constraints address
load balancing issue. approach gives linear speedup 30 processors
426

fiEmbarrassingly Parallel Search CP

n-queens problem, speedups stagnate 30 64 processors. However,
got moderate results 100 industrial SAT instances.
presented earlier works Embarrassingly Parallel Search method based
search space splitting loose communications (Regin et al., 2013, 2014; Rezgui et al.,
2014).
Fischetti, Monaci, Salvagnin (2014) proposed another paradigm called SelfSplit
worker able autonomously determine, without communication
workers, job parts process. SelfSplit decomposed three phases:
enumeration tree initially built workers (sampling); enough open nodes
generated, sampling phase ends worker applies deterministic rule
identify solve nodes belong (solving); single worker gathers results
others (merging). SelfSplit exhibited linear speedups 16 processors good
speedups 64 processors five benchmark instances. SelfSplit assumes sampling
bottleneck overall computation whereas happen practice (Regin
et al., 2014).
Sometimes, complex applications good domain specific strategies
known, parallel algorithm exploit domain-specific strategy. Moisan, Gaudreault, Quimper (2013), Moisan, Quimper, Gaudreault (2014) proposed
parallel implementation classic backtracking algorithm, Limited Discrepancy Search
(LDS), known efficient centralized context good variable/value
selection heuristic provided (Harvey & Ginsberg, 1995). Xie Davenport (2010) proposed processor locally uses LDS search trees allocated (by
tree splitting work stealing algorithm) global system replicate LDS
strategy.
Cube-and-Conquer (Heule, Kullmann, Wieringa, & Biere, 2012) approach parallelizing SAT solvers. cube conjunction literals DNF formula disjunction
cubes. SAT problem split several disjoint subproblems DNF formulas
solved independently workers. Cube-and-Conquer using ConflictDriven Clause Learning (CDCL) solver Lingeling outperforms parallel SAT solvers
instances SAT 2009 benchmarks, outperformed many
instances. Thus, Concurrent Cube-and-Conquer (Van Der Tak, Heule, & Biere, 2012) tries
predict instances works well abort parallel search seconds
favor sequential CDCL solver not.
2.3.3 Las Vegas Algorithms / Portfolios
explore parallelism provided different viewpoints problem,
instance using different algorithms parameter tuning. idea exploited
non-parallel context (Gomes & Selman, 2000). communication required
excellent level load balancing achieved (all workers visit search space). Even
approach causes high level redundancy processors, shows really good
performance. greatly improved using randomized restarts (Luby, Sinclair, &
Zuckerman, 1993) worker executes restart strategy. recently, Cire,
Kadioglu, Sellmann (2014) executed Luby restart strategy, whole, parallel.
proved achieves asymptotic linear speedups and, practice, often obtained
427

fiMalapert, Regin, & Rezgui

linear speedups. Besides, authors proposed allow processors share information
learned search (Hamadi, Jabbour, & Sais, 2008).
One challenge find scalable source diverse viewpoints provide orthogonal
performance therefore complementary interest. distinguish
two aspects parallel portfolios: assumptions made number available
processors possible handpick set solvers settings complement
optimally. want face arbitrarily high number processors,
need automated methods generate portfolio size demand (Bordeaux et al.,
2009). So, portfolio designers became interested feature selection (Gomes & Selman, 1997,
1999, 2001; Kautz, Horvitz, Ruan, Gomes, & Selman, 2002). Features characterize problem
instances number variables, domain sizes, number constraints, constraints arities.
Many portfolios select best candidate solvers pool based static features
learning dynamic behaviour solvers. SAT portfolio iSAC (Amadini, Gabbrielli,
& Mauro, 2013) CP portfolio CPHydra (OMahony, Hebrard, Holland, Nugent, &
OSullivan, 2008) use feature selection choose solvers yield best performance.
Additionally, CPHydra exploits knowledge coming resolution training set
instances candidate solver. Then, given instance, CPHydra determines k
similar instances training set determines time limit candidate
solver based constraint program maximizing number solved instances within
global time limit 30 minutes. Briefly, CPHydra determines switching policy
solvers (Choco2, AbsCon, Mistral).
Many recent SAT solvers based portfolio ManySAT (Hamadi et al.,
2008), SATzilla (Xu, Hutter, Hoos, & Leyton-Brown, 2008), SArTagnan (Stephan & Michael,
2011), Hydra (Xu, Hoos, & Leyton-Brown, 2010), Pminisat (Chu, Stuckey, & Harwood,
2008) based Minisat (Een & Sorensson, 2005). combine portfolio-based
algorithm selection automatic algorithm configuration using different underlying solvers.
example, SATzilla (Xu et al., 2008) exploits per-instance variation among solvers
using learned runtime models.
general, main advantage algorithms portfolio approach many strategies automatically tried time. useful defining good
search strategies difficult task.
2.3.4 Problem Splitting
Problem Splitting another idea relates parallelism, problem split
pieces solved processor. problem typically becomes difficult
solve centralized case processor complete view problem.
So, reconciling partial solutions subproblem becomes challenging. Problem
splitting typically relates distributed CSPs, framework introduced Yokoo, Ishida,
Kuwabara (1990) problem naturally split among agents, privacy
reasons. distributed CSP frameworks proposed Hirayama
Yokoo (1997), Chong Hamadi (2006), Ezzahir, Bessiere, Belaissaoui, Bouyakhf
(2007), Leaute, Ottens, Szymanek (2009), Wahbi, Ezzahir, Bessiere, Bouyakhf
(2011).
428

fiEmbarrassingly Parallel Search CP

2.3.5 Parallel Constraint Propagation
approaches thought of, typically based parallelization one key algorithm solver, instance constraint propagation (Nguyen & Deville, 1998; Hamadi,
2002; Rolf & Kuchcinski, 2009). However, parallelizing propagation challenging (Kasif,
1990) scalability limited Amdahls law. approaches focus
particular topologies make assumptions problem.
2.3.6 Concluding Remarks
Note oldest approaches, scalability issues still investigated
small number processors, typically around 16 64 processors. One major
issue approaches may (and must) resort communication. Communication
parallel agents costly general: shared-memory models multi-core,
typically means access shared data structure one cannot avoid
form locking; cost message-passing cross-CPU even significantly higher. Communication additionally makes difficult get insights solving process since
executions highly inter-dependent understanding parallel executions notoriously
complex.
parallel B&B algorithms explore leaves search tree different order
would single-processor system. could pity situations
know really good search strategy, entirely exploited parallel algorithm.
many approaches, experiments parallel programming involve great deal nondeterminism: running algorithm twice instance, identical number
threads parameters, may result different solutions, sometimes different
runtimes.

3. Embarrassingly Parallel Search
section, present details embarrassingly parallel search. First, Section 3.1
introduces key concepts guided design choices. Then, Section 3.2 introduces
several search space splitting strategies implemented via top-down bottom-up decomposition procedures presented Section 3.3. Section 3.4 gives details architecture
communication. Section 3.5 explains manage queue subproblems
order obtain deterministic parallel algorithm. Section 4.1 gives details
implementation.
3.1 Key Concepts
introduce key concepts guided design choices: massive static decomposition;
loose communication; non-intrusive implementation; toward deterministic algorithm.
3.1.1 Massive Static Decomposition
master decomposes problem p subproblems
solved parallel independently workers. So, solving process equivalent
real-time scheduling p jobs w parallel identical machines known P ||Cmax (Korf &
429

fiMalapert, Regin, & Rezgui

Schreiber, 2013). Efficient algorithms exists P ||Cmax even simple list scheduling
algorithms (based priority rules) (2 w1 )-approximation. desirable properties
defined Section 3.2 ensure low precision processing times makes problems
easier. hold precision number workers fixed, increase number
subproblems, problems get harder perfect schedules appear, get
easier. case, number p subproblems range one three
orders magnitude larger number workers w. low, chance
finding perfect schedules, therefore obtain good speedups, low. large,
decomposition takes longer becomes difficult. conditions met,
unlikely worker assigned work other, therefore,
decomposition statistically balanced. Beside, reach good speedups practice,
total solving time subproblems must close sequential solving time
problem.
advantage master workers independent. use different
filtering algorithms, branching strategies, even underlying solvers. decomposition
crucial step, bottleneck computation quality greatly
impacts parallelization efficiency.
3.1.2 Loose Communication
p subproblems solved parallel independently w workers. load balancing
must statistically obtained decomposition, allow work stealing
order drastically reduce communication. course, communication still needed
dispatch subproblems, gather results possibly exchange useful additional
information, objective bound values. Loose communication allows use star network
without risk congestion. central node (foreman) connected nodes (master
workers).
3.1.3 Non-intrusive Implementation
sake laziness efficiency, rely much possible underlying solver(s)
computing infrastructure. Consequently, modify little possible underlying solver. consider nogoods clauses exchanges techniques
intrusive increase communication overhead. Additionally, logging fault
tolerance respectively delegated underlying solver infrastructure.
3.1.4 Toward Determinism
deterministic algorithm algorithm which, given particular input, always produce output, underlying machine always passing sequence states. determinism already challenging sequential B&B algorithms
due complexity (randomization, restarts, learning, optimization), still
difficult parallel B&B algorithms.
Here, always guarantee reproducibility real-time assignment subproblems workers stored. Reproducibility means always possible replay
solving process. restrictions detailed later, parallel algorithm made
deterministic additional cost. Moreover, parallel algorithm able
430

fiEmbarrassingly Parallel Search CP

mimic sequential algorithm, i.e. produce identical solutions. requires
parallel algorithm visits tree leaves order sequential algorithm.
generally, would useful debugging, performance evaluation, incremental problem
solving parallel algorithm may produce identical solutions matter many
workers present computing infrastructure used.
Conversely, real-time scheduling algorithm applied subproblems. would
allow improve diversification using randomization, exploit past information
provided solving process. experiments, use FIFO scheduling
subproblems, scheduling policy would change shape size
search tree and, therefore, reduces relevance speedups. Unlike EPS, work stealing
approaches deterministic offer control subproblem scheduling.
3.2 Search-Space Splitting Strategies
Here, extend approach search-space splitting proposed Bordeaux et al. (2009),
called splitting hashing. Let us recall C set constraints problem.
split search space problem p parts, one approach assign subproblem
(1 p) extended set constraints C Hi Hi hashing constraint,
constrains subproblem particular subset search space. Hashing constraints
must necessarily sound effective, nontrivial, statistically balanced.
Sound Hashing constraints must partition search space: pi=1 Hi must cover entire
initial search space (completeness), mutual intersections Hi Hj (1 < j p)
preferably empty (non-overlapping).
Effective addition hashing constraints effectively allow worker
efficiently skip portions search space assigned current subproblem.
subproblem must significantly easier original problem. causes overhead,
refer recomputation overhead.
Nontrivial addition hashing constraints lead immediate
failure underlying solver. Thus, generating trivial subproblems might paid
exploration overhead, many would discarded propagation
mechanism sequential algorithm.
Statistically Balanced workers given amount work.
decomposition appropriate, number p subproblems significantly larger
number w workers. thus unlikely given worker would assigned
significantly work worker real-time scheduling algorithm. However, possible solving one subproblem requires significantly work another
subproblem.
Bordeaux et al. (2009) defined hashing constraints selecting subset X variables
P
problem stating Hi (1 p) follows: xX x mod p. effectively
decomposes problem p problems p within reasonable limits. p = 2, imposes
parity constraints sum variables. Splitting repeated scale-up
arbitrary number processors. splitting obviously sound, less effective
431

fiMalapert, Regin, & Rezgui

CP solvers SAT solvers. Here, study assignment splitting node splitting
generate given number p? subproblems.
3.2.1 Assignment Splitting
Let us consider non empty subset X X ordered variables: X = (x1 , . . . , xd ). vector = (v1 , . . . , vd ) tuple X vj D(xj ) (j = 1, . . . , d). Let H( ) = dj=1 (xj = vj )
hashing constraints restrict search space solutions extending tuple .
Q
total decomposition X splits initial problem di=1 D(xi ) subproblems, i.e. one
subproblem per tuple. total decomposition clearly sound effective, efficient
practice. Indeed, Regin et al. (2013) showed number trivial subproblems
grow exponentially.
table decomposition, subproblem defined set tuples allows reach
exactly number p? subproblems. Let ordered list ofj tuples
X
k
|T |
?
|T | > p . Then, first subproblem defined first k = p? tuples, second
subproblem defined following k tuples, on. So, subproblems defined
number tuples possibly exception last.
tuple solver-consistent propagation extended set constraints C
H( ) underlying solver detect unsatisfiability. order obtain nontrivial
decompositions, total table decompositions restricted solver-consistent tuples.
3.2.2 Node Splitting
Node splitting allows parallel algorithm exploit domain-specific strategies
decomposition good strategy known. Let us recall concepts search
trees (Perron, 1999) basis decomposition procedures introduced later.
decompose problems, one needs able map individual parts search tree
hashing constraints. parts called open nodes. open nodes defined,
present search tree decomposed set open nodes.
Open Nodes Node Expansion search tree partitioned three sets, open
nodes, closed nodes, unexplored nodes. Here, make assumption
arity search tree, i.e. maximal number children nodes. subsets
following properties.
ancestors open node closed nodes.
unexplored node exactly one open node ancestor.
closed node open node ancestor.
set open nodes called search frontier illustrated Figure 1. search
active
path
closed

open

Frontier

unexplored

Figure 1: Node status search frontier search tree.
432

fiEmbarrassingly Parallel Search CP

frontier evolves simply process known node expansion. removes open node
frontier, transforms removed node closed node, adds unexplored
children frontier. Node expansion operation happens
search. corresponds branch operation B&B algorithm.
point search, search frontier sound nontrivial decomposition
original problem open node associated subproblem. decomposition effective branching strategy effective. Let us remark
assignment splitting seen special case node splitting static ordering
used variables values.
Active Path Jumps Search Tree Expanding one node another may
require changing state (at least variables domains) search process
first node second. So, worker charge exploring open node must reconstruct
state visits. done using active path jumping operation.
going search tree, search process builds active path,
list ancestors current open node, illustrated Figure 1. worker
moves one node another, jump search tree. make jump,
simply recomputes every move root gets target node. causes
overhead, refer recomputation overhead. Recomputation change
search frontier expand node.
3.3 Decomposition Procedures
decomposition challenge find depth search frontier contains
approximately p? nodes. assignment splitting strategy implemented top-down
procedure starts root node incrementally visits next levels, whereas
node splitting strategy implemented bottom-up procedure starts form
level deep enough climbs back previous levels.
3.3.1 Top-Down Decomposition
challenge top-down decomposition find ordered variables produce
approximately p? solver-consistent tuples. Algorithm 1 realizes solver-consistent table
decomposition iterated depth-bounded depth-first searches early removals inconsistent assignments (Regin et al., 2013).
algorithm starts root node
empty list tuples (line 1). computes list p? tuples solver-consistent
table decomposition iterativly increasing decomposition depth. Let us assume
exists static order variables. iteration, determines new lower
bound (line 4) decomposition depth d, i.e. number variables involved
decomposition. lower bound uses Cartesian product current domains
next variables xd+1 , xd+2 , . . . Then, depth-bounded depth-first search extends
decomposition new depth updates list tuples (line 5). current tuples
added constraints model search (line 5) order reduce
redundant work. search, extended tuples propagated (line 7) reduce
domains, improve next lower bound decomposition depth. tuple
solver-consistent (not proven infeasible) last search. process repeated
number |T | tuples greater equal p? . end, tuples aggregated
433

fiMalapert, Regin, & Rezgui

Algorithm 1: Top-down decomposition.

1
2

3

4

5
6

7
8

9

10

Data: CSP (X , D, C) number subproblems p?
Result: list tuples
0;
;
/* Simulate breadth-first search: iterated depth bounded DFSs.
repeat
/* Determine
decomposition
depth. */

n lower bound
Ql

?
min l max(1, |T |) i=d+1 |D(xi )| p ;

*/

/* Extend current decomposition new variables. */
depthBoundedDFS (X , C { H( )}, D, {x1 , . . . , xd });
== break;
/* Propagate tuples (without failure). */
propagate (X , C { H( )}, D);
|T | < p? ;
/* Aggregate tuples generate exactly p? subproblems */
aggregateTuples(T ) /* subproblems become simultaneously available.
*/
foreach sendSubProblem (X , C H( ), D);

generate exactly p? subproblems. practice, consecutive tuples aggregated.
subproblems become simultaneously available aggregation.
Sometimes, sequential decomposition bottleneck Amdahls law. So,
parallel decomposition procedure increases scalability (Regin et al., 2014).
two steps differ Algorithm 1. First, instead starting depth 0 empty list
tuples (line 1 Algorithm 1), first list quickly generated least five tuples per
worker.
n

1
2

Ql
min l i=1 |D(xi )| 5 w ;
Qd
i=1 D(xi );

Second, iteration, tuple extended parallel instead extending sequentially tuples (line 5 Algorithm 1). parallel decomposition change ordering
compared sequential one. Again, subproblems become available
end decomposition.

8

0 ;
run parallel
foreach
/* extend tuple parallel */
0 0 depthBoundedDFS (X , C H( ), D, {x1 , . . . , xd });

9

0;

5
6
7

top-down procedures assume variable ordering used decomposition static. next decomposition procedure bypasses limitation handles
branching strategy.
434

fiEmbarrassingly Parallel Search CP

Algorithm 2: Bottom-up decomposition.
1

2
3
4
5
6

7
8
9
10

Data: CSP (X , D, C), decomposition depth d? , subproblem limit P .
p 0;
/* Generate subproblems visiting top real tree. */
Node Callback decomposition(node)
depth(node) d?
sendSubProblem (node);
p p + 1;
p P
/* Decrease dynamically depth. */
d? max(1, d? 1);
P 2 P;
backtrack;
DFS (X ,C,D);

3.3.2 Bottom-Up Decomposition
bottom-up decomposition explores search frontier depth d? approximately p? nodes. simplest form, decomposition depth d? provided
user good knowledge problem. Algorithm 2 explores search frontier depth
d? using depth-first search illustrated Figure 2(a). search callback identifies
node level d? (line 2), sends immediately active path, defines subproblem,
subproblem solved worker. decomposition depth dynamic,
reduced number subproblems becomes large (line 6). aims
compensate poor choice decomposition depth d? . practice, depth reduced
one unit current number subproblems exceeds given limit P . limit
initially set P = 2 p? doubled time reached. contrary,
depth static (P = +) never changes whatever number subproblems.
practice, common user provides decomposition depth,
automated procedure without users intervention needed. Algorithm 3 aims
identifying topmost search frontier approximately p? open nodes sampling
estimation. procedure divided three phases: build partial tree sampling

Final depth

Search Frontier

Dynamic



p nodes

Static
P nodes

Initial depth

2P nodes

(a) Decomposition.

(b) Estimation.

Figure 2: Bottom-up decomposition estimation.
435

fiMalapert, Regin, & Rezgui

Algorithm 3: Bottom-up estimation.

1

2
3
4
5
6
7
8
9

10
11

Data: CSP (X , D, C) number subproblems p? .
Data: time limit t, node limit n, maximum depth large enough
Result: decomposition depth d?
/* Set counters width levels */
foreach [1, D] width[d] 0;
/* Build partial tree sampling. */
Node Callback estimation(node)
depth(node);

width[d] width[d] + 1;
width[d] p? 1 ;
else backtrack;
hasFinished (t,n) break;
DFS (X ,C,D);
/* Estimate level widths tree decomposition depth.
width estimateWidths(width);
d? estimateDepth(width, p? );

*/

top real search tree; estimate level widths real tree; determine
decomposition depth d? greedy heuristic.
Since need explore top search tree, upper bound decomposition depth fixed. maximum decomposition depth must chosen according
number workers expected number subproblems per worker.
small, decomposition could generate subproblems. large,
sampling time increases decomposition quality could decrease.
sampling phase builds partial tree p? open nodes level using
callback depth-first search. number open nodes level partial
tree counted callback. maximum depth reduced time p? nodes
opened given level (line 6). sampling ends within limits, top
tree entirely visited estimation needed. Otherwise (line 8), one needs
estimate widths topmost levels tree depending partial tree.
estimation straightforward adaptation one proposed Cornuejols, Karamanov,
Li (2006) deal n-ary search tree (line 10). practice, main issue
higher arity is, lower precision estimation. Therefore, greedy heuristic
determines decomposition depth based estimated number nodes per level,
number nodes partial tree (line 11). heuristics minimizes
absolute deviation estimated number nodes expected number p? .
several levels identical absolute deviation, lowest level estimated
number subproblems greater equal p? selected.
3.4 Architecture Communication
describe messages exchanged actors depending problems type. Then,
typical use case illustrates solving process optimization problem. Briefly,
436

fiEmbarrassingly Parallel Search CP

communication network star network foreman acts pipe transmit
messages master workers.
3.4.1 Actors Messages
total number messages depends linearly number workers (w)
number subproblems (p). messages synchronous sake simplicity
means work must wait communications completed (Barney & Livermore,
2016). Interleaving computation communication single greatest benefit using
asynchronous communications since work done communications taking
place. However, asynchronous communications complicate architecture, instance
message requests answer.
Master control unit decomposes problem collects final results.
sends following messages: create foreman; give subproblem foreman;
wait foreman gather results; destroy foreman. master deals
foreman. decomposition time elapsed time create
wait messages. workers time elapsed time first give destroy
messages. wall-clock time elapsed time creation destruction
master.
Foreman central node star network. queuing system stores subproblems received master dispatches workers. gathers results
collected workers. foreman allows master concentrate problems
decomposition, performance bottleneck, handling communications
workers. sends following messages: create worker; give subproblem worker;
collect (send) final results master; destroy worker. foreman
detects search ended, sends collect-message containing final results
master.
Workers search engines. send following messages: find subproblem (the
foreman must answer give-message); collect (send) results foreman.
results contain essential information solution(s) solving process. Workers
know foreman. worker acquires new work (receives give-message
foreman), acquired subproblem recomputed causes recomputation overhead.
work stealing context, Schulte (2000) noticed higher node search
tree, smaller recomputation overhead. construction, topmost nodes
used here.
3.4.2 Problems Types
discuss specificities first solution, solution, best solution searches.
First Solution Search search complete soon solution found.
workers must immediately terminated well decomposition procedure.
Solution Search search complete subproblems solved.
437

fiMalapert, Regin, & Rezgui

Best Solution Search main design issue best-solution search maintain
so-far best solution. sequential B&B algorithm always knows so-far best solution.
difficult achieve concurrent setting several workers. Maintaining best
solution worker could lead large communication synchronization overheads.
Instead prefer solution foreman workers maintain so-far best
solution follows. default, give collect messages foreman
workers carry objective information. Additionally, worker send better messages
foreman intermediate solution, foreman send best solution
workers. instance, worker finds new solution, informs foreman
sending better message solution accepted threshold function. Similarly,
foreman receives new solution collect better message, checks
whether solution really better. solution accepted threshold function,
foreman sends another better message workers. architecture sketched
entails worker might always know so-far best solution. consequence,
parts search tree explored, pruned away worker
exact knowledge. Thus, loose coupling might paid exploration
overhead.

Master

Foreman

Worker 1

Worker 2

opt

[Allocate Resources]
<< create >>
<< create >>
<< create >>
give

find
give
opt

better

[Best Solution Search]

give

give

give
collect
find
wait

give
collect
better

opt

better

[Best Solution Search]

collect

collect
opt
[Release Resources]
<< destroy >>
<< destroy >>

<< destroy >>

Master

Foreman
Master

Worker
Master1

Worker
Master2

Figure 3: Sequence diagram solving process two workers.
438

fiEmbarrassingly Parallel Search CP

3.4.3 Use Case
Figure 3 sequence diagram illustrating solving process optimization problem
two workers. shows actors operate chronological order.
first horizontal frame resource allocation. master creates foreman.
foreman creates workers. Immediately creation, master worker
load original problem. foreman transparently manages concurrent queue subproblems produced master consumed workers. that, workers
jumps search tree.
foreman creation, master starts decomposition original problem
p = 3 subproblems. soon subproblem generated, master gives
foreman. Here, give find messages interleaved node splitting
decomposition proposed Section 3.3.2. assignment splitting decomposition proposed
Section 3.3.1 would produce unique give message subproblems.
decomposition finished, master sends wait message foreman waits
collect response containing final result. last collect message triggers
resource deallocation.
time worker starving, asks foreman subproblem waits it.
Here, first subproblem assigned first worker second worker waits
second subproblem. Best Solution Search frames correspond specific messages
optimization problems. first worker quickly finds good solution sends
foreman via better message. second subproblem generated master
given foreman. turn, foreman gives second subproblem updated
objective information second worker. second problem quickly solved
second worker sends collect message foreman. collect message
stands find message. Then, third, last, subproblem assigned second
worker.
foreman broadcasts better message good quality solution
received first worker. Note message useless first worker.
foreman detects termination solving process sends collect message
master three following conditions met: master waiting; subproblems
queue empty; workers starving. last horizontal frame resource
deallocation.
3.5 Queuing Determinism
foreman plays role queuing system receives subproblems master
dispatches workers. section, show EPS modified
return solution sequential algorithm useful several
scenarios debugging performance evaluation. Generally, queuing policy
applied select next subproblem solve.
Let us assume subproblems P1 , P2 , . . . , Pp sent foreman fixed
order case sequential top-down procedure bottom-up procedure.
Otherwise, fixed order subproblems obtained sorting subproblems.
first solution found sequential algorithm belongs satisfiable subproblem
Pi smallest index, i.e. leftmost solution. Let us assume parallel
439

fiMalapert, Regin, & Rezgui

algorithm finds first solution subproblem Pj j > i. Then,
necessary solve problems Pk k > j one must wait problem
Pk k < j determine leftmost solution, satisfiable subproblem
smallest index.
easily extended optimization problems slightly modifying cutting
constraints. Usually, cutting constraint stated new solution found
allows strictly improving solution. contrary constraints, cutting
constraint always propagated backtracking. Here, solution found solving
subproblem Pj , cutting constraint allows strictly improving solution
subproblems k j, allows equivalent solution subproblems k < j.
So, parallel algorithm returns solution sequential one
subproblem visited order. Moreover, solution returned parallel
algorithm depend number workers, decomposition.
experiments, queuing policy FIFO policy ensures subproblems
solved order speedups relevant. However, guaranty
sequential parallel algorithms return solution.

4. Experimental Results
Here, describe experiments EPS carry detailed data analysis. aim
answer following questions. EPS efficient? different number workers?
different solvers? different computing platforms? Compared parallel
approaches? influence different components (decomposition procedures,
search strategies, constraint models)? EPS robust flexible? anomalies
occur?
Section 4.1 presents benchmark instances, execution environments, parameters settings, different implementations. First, Section 4.2, analyze evaluate
top-down bottom-up decomposition procedures well importance search
strategy, especially decomposition. Then, evaluate efficiency scalability
parallel solvers multi-core machine (Section 4.3), data center (Section 4.4),
cloud platform (Section 4.5). sections, compare implementations
EPS work stealing approaches whenever possible. Section 4.4, analyze efficiency parallel solver depending search strategy. Section 4.6,
transform reasonable effort parallel solver distributed parallel solver
using batch scheduler provided data center. anomalies parallel solver
explained resolved distributed equivalent. Last, Section 4.7 discusses
performance parallel solvers compared static portfolios built underlying
sequential solvers data center.
4.1 Experimental Protocol
section, introduce benchmark instances, execution environments, metrics
notations. give details implementations.
440

fiEmbarrassingly Parallel Search CP

4.1.1 Benchmark Instances
lot benchmark instances available literature. aim select difficult
instances various models represent problems tackled CP. Ideally, instance
difficult none solvers solve quickly. Indeed, parallel solving relevant
shortens long wall-clock time. Here, consider unsatisfiable, enumeration
optimization problems instances. ignore problem finding first feasible
solution parallel speedup completely uncorrelated number
workers, making results hard analyze. consider optimization problems
variability observed, lesser extent optimality
proof required. variability unsatisfiable enumeration instances lowered,
therefore, often used test bed parallel computing. Besides, unsatisfiable
instances practical importance, instance software testing, enumeration
important users compare various solutions.
first set called fzn selection 18 instances selected 5000
instances either repository maintained Kjellerstrand (2014) directly
Minizinc 1.6 distribution written FlatZinc language (NICTA Optimisation Research
Group, 2012). instance solved 500 seconds less 1 hour
Gecode. selection composed 1 unsatisfiable, 6 enumeration, 11 optimization
instances.
set xcsp composed instances categories ACAD REAL XCSP
2.1 (Roussel & Lecoutre, 2008). consists difficult instances solved within
24 hours Choco2 (Malapert & Lecoutre, 2014). first subset called xcsp1 composed
5 unsatisfiable 5 enumeration instances whereas second subset called xcsp2
composed 11 unsatisfiable 3 enumeration instances. set xcsp1 composed
instances easier solve xcsp2.
Besides, consider two classical problems, n-queens Golomb ruler
problems widely used literature (Gent & Walsh, 1999).
4.1.2 Implementation Details
implemented EPS method top three solvers: Choco2 2.1.5 written Java, Gecode
4.2.1 OR-tools rev. 3163 written C++. use two parallelism implementation
technologies: Threads (Mueller et al., 1993; Kleiman, Shah, & Smaalders, 1996)
MPI (Lester, 1993; Gropp & Lusk, 1993). typical difference
threads (of process) run shared memory space, MPI standardized
portable message-passing system exchange information processes running
separate memory spaces. Therefore, Thread technology handle multiple nodes
cluster whereas MPI does.
C++, use Threads implemented pthreads, POSIX library (Mueller et al.,
1993; Kleiman et al., 1996) used Unix systems. Java, use standard Java Thread
technology (Hyde, 1999).
many implementations MPI OpenMPI (Gabriel, Fagg, Bosilca, Angskun,
Dongarra, Squyres, Sahay, Kambadur, Barrett, Lumsdaine, et al., 2004), Intel MPI (Intel
Corporation, 2015), MPI-CH (MPI-CH Team, 2015) MS-MPI (Krishna, Balaji, Lusk,
Thakur, & Tiller, 2010; Lantz, 2008). MPI standard API, characteristics
441

fiMalapert, Regin, & Rezgui

machine never taken account. So, machine providers Bull, IBM Intel
provide MPI implementation according specifications delivered machine. Thus, cluster provided Bull custom Intel MPI 4.0 library, OpenMPI
1.6.4 installed, Microsoft Azure supports MS-MPI 7 library.
OR-tools uses sequential top-down decomposition C++ Threads. Gecode uses
parallel top-down decomposition C++ Threads MPI technologies. fact, Gecode
use C++ pthread multi-core computer, OpenMPI data center,
MS-MPI cloud platform. Gecode OR-tools use lex variable selection
heuristic top-down decomposition requires fixed variable ordering. Choco2
uses bottom-up decomposition Java Threads. every case, foreman schedules
jobs FIFO mimic much possible sequential algorithm speedups
relevant. needed, master workers read model file.
always take value selection heuristic selects smallest value whatever
variable selection heuristic.
4.1.3 Execution Environments
use three execution environments representative computing platforms available nowadays.
Multi-core Dell computer 256 GB RAM 4 Intel E7-4870 2.40 GHz processors running Scientific Linux 6.0 (each processor 10 cores).
Data Center Centre de Calcul Interactif hosted Universite Nice Sophia
Antipolis provides cluster composed 72 nodes (1152 cores) running CentOS
6.3, node 64 GB RAM 2 Intel E5-2670 2.60 GHz processors (8 cores).
cluster managed OAR (Capit, Da Costa, Georgiou, Huard, Martin, Mounie, Neyron,
& Richard, 2005), i.e., versatile resource task manager. Thread technology
limited single node cluster, Choco2 use 16 physical cores whereas Gecode
use number nodes thanks MPI.
Cloud Computing cloud platform managed Microsoft company (Microsoft
Azure) enables deploy applications Windows Server technology (Li, 2009).
node 56 GB RAM Intel Xeon E5-2690E 2.6 GHz processors (8 physical cores)
allowed simultaneously use 3 nodes (24 cores) managed Microsoft HPC
Cluster 2012 (Microsoft Corporation, 2015).
computing infrastructures provide hyper-threading technologies. Hyper-threading
improves parallelization computations (doing multiple tasks once). core
physically present, operating system addresses two logical cores, shares
workload among possible. multi-core computer provides hyper-threading,
whereas deactivated cluster, available cloud.
4.1.4 Setting Parameters
time limit solving instance set 12 hours whatever solver.
number workers strictly less number cores (w < c), always
unused cores. Usually, one chooses w = c, workers work simultaneously.
multi-core computer, use two workers per physical core (w = 2c) hyperthreading efficient experimentally demonstrated Appendix A. target number
442

fiEmbarrassingly Parallel Search CP

p? subproblems depends linearly number w workers (p? = 30 w) allows
statistical balance workload without increasing much total overhead (Regin
et al., 2013).
experiments, network RAM memory loads low regards
capacities computing infrastructures. Indeed, total number messages depends
linearly number workers number subproblems. RAM pre-allocated
computing infrastructure allows it. Last, workers almost produce input/output
disk access.
4.1.5 Metrics Notations
Let solving time (in seconds) algorithm let su speedup parallel
algorithm. tables, row gives results obtained different algorithms given
instance. row, best solving times speedups indicated bold. Dashes
indicate instance solved algorithm. Question marks indicate
speedup cannot computed sequential solver solve instance
within time limit. Arithmetic means, abbreviated AM, computed solving times,
whereas geometrical means, abbreviated GM, computed speedups efficiency.
Missing values, i.e. dashes question marks, ignored computing statistics.
use scoring procedure based Borda count voting system (Brams &
Fishburn, 2002). benchmark instance treated voter ranks solvers.
solver scores points related number solvers beats. precisely,
solver scores points problem P comparing performance solver s0
follows:
gives better answer s0 , scores 1 point;
else answer gives worse answer s0 , scores 0 point;
else scoring based execution time comparison (s s0 give indistinguishable
answers).
Let t0 respectively denote wall-clock times solvers s0 given problems
instance. case indistinguishable answers, scores f (t, t0 ) according Borda system
used Minizinc challenge. But, function f capture users preferences
well. Indeed, solver solves n problems 0.1 seconds n others 1000 seconds
whereas solver s0 solves first n problems 0.2 seconds n others 500
seconds, solvers obtain score n whereas users would certainly
prefer s0 . So, use another scoring function g(t, t0 ) g(t) interpreted
utility function solving problems instance within seconds. function g(t)
strictly decreasing 0.5 toward 0. remaining points shared using function f .

f (t, t0 ) =

t0
+ t0

g(t, t0 ) = g(t)+(1g(t)g(t0 ))f (t, t0 )

g(t) =

1
2 (loga (t + 1) + 1)

Using function g (a = 10) previous example, solvers s0 respectively
scored 0.81 n 1.19 n points.
443

fiMalapert, Regin, & Rezgui

4.2 Analysis Decomposition
section, compare quality performance top-down bottom-up
decomposition procedures introduced Section 3.3.
4.2.1 Decomposition Quality
top-down decomposition always returns target number p? = 30 w subproblems
whereas guaranteed bottom-up decomposition. Figure 4(a) boxplot
number subproblems per worker (p / w) bottom-up decomposition
Choco2 depending number workers. Boxplots display differences among populations without making assumptions underlying statistical distribution:
non-parametric. box boxplot spans range values first quartile
third quartile. whiskers extend end box range equal
1.5 times interquartile range. points lie outside range whiskers
considered outliers: drawn individual circles.
number workers w {16, 80, 512}, decompositions xcsp instances
using one variable selection heuristic among lex, dom, dom/ddeg,dom/wdeg, dom/bwdeg,
impact, combined minVal, considered. bottom-up decomposition obtains
satisfying average performance (mostly 10 100 subproblems per worker)
respecting much possible branching strategy. However, anomalies occur.
First, decomposition sensitive shape search tree. Sometimes, model
contains variables large domains forbid accurate decomposition.
instance, first second levels knights-80-5 search tree respectively contain
6000 50000 nodes. significant underestimation
tree size, especially branching high arity. instance, width second
level fapp07-0600-7 estimated around 950 nodes contains 6000
nodes. contrary, underestimation occur top nodes eliminated
search tree low arity. Apart underestimation, decomposition accurate
search trees low arity.
top-down decomposition accurate, requires fixed variable ordering, whereas
bottom-up decomposition less accurate, handles branching strategy.
1

0.8

100
instances (%)

subproblems per worker

1000

10

0.6

0.4

1
0.2

0.1
16

80

0

512

workers

Choco2 w=80
Choco2 w=512
Gecode w=80
Gecode w=512
0.1

1

10

100

time (s)

(a) Number subproblems per worker.

(b) Decomposition time.

Figure 4: Analysis decomposition procedures (w = 16, 80, 512).
444

1000

fiEmbarrassingly Parallel Search CP

4.2.2 Decomposition Time
Figure 4(b) gives percentage decompositions done within given time. Choco2
times reported variable selection heuristics xcsp instances. Gecode times
reported lex xcsp fzn instances.
implementation differences, times reported Choco2 Gecode
slightly different. Indeed, decomposition time alone given Gecode. Choco2
times take account estimation time, time taken foreman fill
queue subproblems, time taken workers empty queue. Let us
remind subproblems become available top-down decomposition
complete whereas become available fly bottom-up decomposition.
cases, reported time lower bound solving time.
top-down decomposition faster bottom-up decomposition
parallelism. fact, Gecode decomposition often faster estimation time alone.
One compelling example instance knights-80-5 highest time (around
800 seconds) well poor quality structure problem unsuited
bottom-up decomposition: variables large domains (more
6000 values); almost domain reduction top tree;
propagation long.
conclude, parallel top-down decomposition Gecode fast accurate
bottom-up decomposition offers greater flexibility, less robustness.
4.2.3 Influence Search Strategy
analyze influence search strategies decomposition resolution,
apply variable selection heuristic decomposition (master) another one
resolution (workers). Table 1 gives solving times combinations lex
dom solving instances xcsp1. Results reported significant
differences among solving times. choice variable selection heuristic critical
decomposition resolution. Indeed, initial choices made branching
least informed important, lead largest subtrees
search hardly recover early mistakes. on, master workers
use variable selection heuristic.

Instances

Worker
Master

costasArray-14
latinSquare-dg-8
lemma-100-9-mod
pigeons-14
quasigroup5-10
queenAttacking-6
squares-9-9

lex

dom

lex

dom

lex

dom

191.2
479.4
109.7
1003.8
182.2
872.4
126.8

240.9
323.8
125.9
956.3
125.3
598.3
1206.5

191.4
470.6
101.8
953.2
188.5
867.8
127.8

240.0
328.1
123.4
899.1
123.5
622.5
1213.0

Table 1: Solving times different search strategies (Choco2, multi-core, w = 2c = 80).
445

fiMalapert, Regin, & Rezgui

4.3 Multi-core
section, use parallel solvers based Thread technologies solve instances
xcsp1 n-queens problem using multi-core computer. Let us recall
two worker per physical core hyper-threading activated (w = 2c = 80). show
EPS frequently gives linear speedups, outperforms work stealing approach
proposed Schulte (2000), Nielsen (2006).
4.3.1 Performance Analysis
Table 2 gives solving times speedups parallel solvers using 80 workers
xcsp1 instances. Choco2 tested lex dom whereas Gecode OR-tools
use lex. compared work stealing approach denoted Gecode-WS (Schulte,
2000; Nielsen, 2006). First, implementations EPS faster efficient
work stealing. EPS often reaches linear speedups number cores whereas never
happens work stealing. Even worse, three instances solved within 12
hours time limit using work stealing whereas using sequential solver.
Choco2, dom efficient parallel lex remains slightly slower
average. Decomposition key bad performance instances knights-80-5
lemma-100-9-mod. outlined before, decomposition knights-80-5 takes
1100 seconds generates much subproblems, forbids speedup. issue
lessened using sequential decomposition OR-tools resolved parallel
top-down decomposition Gecode. Note sequential solving times OR-tools
Gecode respectively 20 40 times higher. Similarly, long decomposition time
Choco2 lemma-100-9-mod leads low speedup. However, moderate efficiency
Choco2 Gecode squares-9-9 caused decomposition.
Gecode OR-tools often efficient faster Choco2. solvers show
different behaviors even using variable selection heuristic
Instances

costasArray-14
knights-80-5
latinSquare-dg-8
lemma-100-9-mod
ortholatin-5
pigeons-14
quasigroup5-10
queenAttacking-6
series-14
squares-9-9
(t) GM (su)
Borda score (rank)

Choco2-lex

Choco2-dom

Gecode

OR-tools

Gecode-WS



su



su



su



su



su

191.2
1138.3
479.4
109.7
248.7
1003.8
182.2
872.4
39.3
126.8

31.4
1.2
39.0
4.0
30.0
13.8
30.7
23.4
29.9
19.0

240.0
1133.1
328.1
123.4
249.9
899.1
123.5
622.5
39.3
1213.0

38.8
1.5
39.2
4.1
36.0
15.5
32.5
28.5
32.9
16.1

62.3
548.7
251.7
6.7
421.7
211.8
18.6
15899.1
11.3
17.9

19.1
37.6
42.0
10.1
13.5
39.1
26.4
?
34.2
18.4

50.9
2173.9
166.6
1.8
167.7
730.3
17.0

16.2
81.4

33.4
18.5
35.2
22.9
38.1
18.5
36.9

28.7
35.0

594.0

4488.5
3.0
2044.6

22.8

552.3
427.8

2.0

2.4
22.3
2.8

21.5

0.7
0.8

439.2

15.9

497.2

17.4

1745.0

24.0

378.4

28.7

1161.9

3.3

20.6 (3)

19.7 (4)

26.1 (1)

22.8 (2)

9.8 (5)

Table 2: Solving times speedups (multi-core, w = 2c = 80). Gecode OR-tools use
lex heuristic.

446

fiEmbarrassingly Parallel Search CP

propagation mechanisms decompositions differ. Furthermore, parallel top-down
decomposition Gecode preserve ordering subproblems regard
sequential algorithm.
4.3.2 Variations N-Queens Problem
Here, verify effectiveness EPS classic CSP settings. consider four models
well-known n-queens problem (n = 17). n-queens puzzle problem placing
n chess queens n n chessboard two queens threaten other. Here,
enumerate solutions heuristics lex dom reasonable choices. models are:
allDifferent global constraints enforce arc-consistency (AC); allDifferent constraints enforce bound-consistency (BC); arithmetic inequalities constraints (NEQ);
dedicated global constraint (JC) (Milano & Trick, 2004, ch. 3).
Table 3 gives solving times speedups Choco2 80 workers
decomposition depth either 3 4. striking result splitting
technique gives excellent results, linear speedup 40 processors
exception JC model. unfortunate since JC model clearly best model
sequential solver. Here, dom always better choice lex. number
subproblems dom whatever model whereas total number nodes
changes. indicates filtering weak top search tree.
works report good results, often linear speedups n-queens problem. Bordeaux et al. (2009) reported linear speedups 30 cores 17 queens,
improvement 64 cores, whereas Machado et al. (2013) scales 512 workers using hierarchical work stealing approach. Menouer Le Cun (2014) reported
speedups around 8 using 12 cores 16 queens, Pedro, Abreu, Pedro, Abreu
(2010) reported speedups around 20 using 24 cores. Zoeteweij Arbab (2004) reported
linear speedups 16 cores 15 queens, Pedro et al. (2010) reported speedup
20 using 24 cores, So, EPS efficiency slightly average, similar
results observed 15 16 queens.
previous experimental setting favor EPS exploring search
space exhaustively, problem highly symmetric. Indeed, variance subproblems solving time low, especially higher levels consistency. Note
lower speedups JC model probably caused load balancing issues
subproblems NEQ model greater mean variance.

Model

lex

dom

d=3

BC
AC
NEQ
JC

d=4

d=3

d=4



su



su



su



su

838.8
3070.2
280.7
202.4

38.3
38.8
31.8
20.4

835.1
3038.9
241.9
196.9

38.5
39.3
36.9
21.0

640.4
2336.2
188.8
140.6

38.7
38.8
36.4
24.2

635.5
2314.7
181.1
148.8

39.0
39.2
37.9
22.9

Table 3: Variations 17 queens problem (Choco2, multi-core, w = 2c = 80).
447

fiMalapert, Regin, & Rezgui

Instances

lex


dom
su



dom/ddeg
su



dom/bwdeg

su



su

dom/wdeg


cc-15-15-2
1947.1 5.2 25701.7
?

1524.9 4.6 2192.1
costasArray-14
500.4 12.4
641.9 12.1 895.3 8.6 4445.0 2.4
649.9
crossword-m11
506.1 4.4




492.0 1.9
204.6
crossword-m1c2
2376.9 0.6 1173.9 0.6 1316.2 0.5 1471.3 0.7 1611.9
fapp07-0600-7





1069.5 2.1 2295.7
knights-20-9
359.3 17.2
353.9 17.3 357.4 14.9 5337.5 1.3
491.3
knights-25-9
855.3 17.8
840.6 18.0 986.1 13.3 13264.8 1.3 1645.2
knights-80-5
708.5 2.0
726.9 2.0 716.4 2.1 1829.5 0.9 1395.6
langford-3-17
38462.7
? 708.3 12.5 5701.6 2.5 6397.5 1.9 3062.2
40465.2
? 148.2 14.4 1541.9 2.2 1307.1 2.1
538.3
langford-4-18
langford-4-19

747.2 16.9
0.0 7280.1 2.3 2735.3
latinSquare-dg3
1161.7 14.3
903.2 12.2 812.0 14.4
416.9 4.2
294.8
lemma-100-9-mod
110.5 4.1
117.6 3.7 180.4 3.7
154.4 3.5
145.3
572.6 13.5
558.9 13.5 475.5 11.5
453.1 11.6 362.4
ortholatin-5
pigeons-14
1330.1 9.8 1492.6 8.3 1471.6 11.8 6331.1 2.6 2993.3
397.2 12.6 277.3 12.9 1156.6 3.6
733.5 5.2
451.5
quasigroup5-10
queenAttacking-6
2596.7 7.3 1411.8 10.6 4789.7 4.2 2891.0 1.9
706.4
queensKnights4





1517.8 0.2 5209.5
ruler-70-12-a3
137.4 16.8 2410.6 17.5


51.5 2.4
42.8
6832.0 3.9 4021.1 4.7 7549.2 2.2 1412.0 0.9 1331.3
ruler-70-12-a4
scen11-f5





38698.7 0.0

series-14
77.8 14.8
89.1 12.4 9828.6 3.4 1232.2 2.6
338.9
220.7 10.5 1987.4 7.2 129.7 9.4
697.2 2.2 115.9
squares-9-9
squaresUnsat5





3766.1 1.2 3039.8
(t) GM (su)
Borda score (rank)
1
4

5243.1

7.5

65.1 (6)

2332.2

8.6 2369.3

72.8 (5)

4.8

4282.3

49.6 (8)

1.6 1385.0

91.2 (2)

2

crossword-m1-words-05-06
crossword-m1c-words-vg7-7 ext
queensKnights-20-5-mul 5 squaresUnsat-19-19

3

su

impact


su

2.1 31596.1
?
11.4
652.2 10.5
5.1 179.5 2.9
0.6 4689.6 1.9
1.8


17.5 215.4 16.5
14.1 550.8 16.9
3.4
896.3 2.5
3.7 5995.6
?
4.8 1041.5 10.0
5.6 4778.9
?
11.3
28.7 5.0
3.5
226.8 2.5
13.7
641.7 10.6
5.1 3637.2 4.2
7.9
308.4 27.8
5.4 427.1 6.2
1.0


6.7
24.8 12.1
2.3 102.9 24.4
0.0


9.9
346.5 8.5
11.0
138.9 10.8
2.9


4.9

100.3 (1)

2823.9

7.7

81.4 (3)

latinSquare-dg-8

Table 4: Detailed speedups solving times depending variable selection heuristics
(Choco2, data center, w = 16).

30

25

speedup

20

15

10

5

0

lex

dom

ddeg

bwdeg

wdeg

impact

variable selection

Figure 5: Speedups variable selection heuristics (Choco2, data center, w = 16).

448

fiEmbarrassingly Parallel Search CP

4.4 Data Center
section, study influence search strategy solving times
speedups, scalability 512 workers, compare EPS work stealing approach.
4.4.1 Influence Search Strategy
study performance Choco2 using 16 workers solving xcsp instances using
variable selection heuristics presented Section 2.1. Figure 5 boxplot
speedups variable selection heuristic. First, speedups lower dom/bwdeg
decomposition effective. binary branching states constraint x =
left branch x 6= right branch. So, workload left right
branches imbalanced. case, positive decisions left branches
taken account. Second, without learning (lex dom), parallel algorithm
efficient robust terms speedup. learning (dom/bwdeg, dom/wdeg,
impact), parallel algorithm may explore different search tree sequential
one. Indeed, master explores top tree changes learning,
possibly branching decisions. worker learns subproblems,
whole search tree. frequently causes exploration overhead solving
queensKnights-20-5-mul (twelve times nodes using dom/wdeg) or, sometimes gives
super-linear speedup solving quasigroup5-10 (three times less nodes using impact).
Last, low speedups occur variable selection heuristics.
Table 4 gives solving times speedups obtained different variable selection
heuristics. Borda scores computed Choco2 (Table 4) Gecode (Table 5). First,
variable selection heuristics strictly dominates others either sequential parallel.
However, dom/wdeg robust outlined Borda scores. fact, variability solving times different heuristics reduced parallelization,
remains important. Second, spite low speedups, dom/bwdeg remains second
best variable selection heuristic parallel solving best one sequential.
average, using advanced variable selection heuristics dom/bwdeg, dom/wdeg,
impact gives lower solving times lex dom spite lower speedups. highlights
fact decomposition procedures handle branching strategy. Section 4.6.1,
investigate low speedups instance crossword-m1c-words-vg7-7
caused variable selection heuristics.
4.4.2 Scalability 512 Workers
Table 5 compares Gecode implementations EPS work stealing (WS) solving
xcsp instances using 16 512 workers. EPS faster efficient work
stealing. 16 workers, work stealing ranked last using Borda score.
512 workers, EPS average almost 10 times faster work stealing.
efficient parallelize sequential solver. multi-core
machine, Gecode faster Choco2 instances xcsp1. Here, performance
Gecode mitigated outlined Borda scores. Five instances
solved within time limit Gecode reported Table 5. Six instances
solved 16 workers whereas twelve instances solved sequential
solver. way comparison, five instances solved Choco2 using lex
449

fiMalapert, Regin, & Rezgui

w = 16

Instances

w = 512

EPS

cc-15-15-2
costasArray-14
crossword-m1c1
crossword-m12
knights-20-9
knights-25-9
knights-80-5
langford-3-17
langford-4-18
langford-4-19
latinSquare-dg-8
lemma-100-9-mod
ortholatin-5
pigeons-14
quasigroup5-10
queenAttacking-6
ruler-70-12-a3
ruler-70-12-a4
series-14
squares-9-9

EPS

WS



su



su



su



su


64.4
240.6
171.7
5190.7
7462.3
1413.7
24351.5
3203.2
26871.2
613.5
3.4
309.5
383.3
27.1
42514.8
96.6
178.9
22.5
22.8


13.6
13.1
14.5
?
?
11.5
?
?
?
13.1
14.7
14.1
14.5
13.5
?
15.1
14.4
13.4
11.1


69.3
482.1
178.5
38347.4

8329.2
21252.3
25721.2

621.2
5.8
335.8
6128.9
33.7
37446.1
105.5
185.2
56.9
44.3


12.7
6.6
13.9
?

2.0
?
?

13.0
8.6
13.0
0.9
10.8
?
13.8
13.9
5.3
5.7


3.6
18.7
13.3
153.4
214.9
49.3
713.5
94.6
782.5
23.6
1.0
10.4
15.3
1.7
1283.9
3.7
6.0
1.1
1.3


243.8
168.6
187.3
?
?
329.8
?
?
?
341.7
51.4
422.0
363.1
211.7
?
389.3
429.5
264.0
191.7


17.7
83.1
57.8
3312.4

282.6
7443.5
5643.1

124.4
2.5
71.7
2320.2
9.8
9151.5
67.7
34.1
8.2
7.6


49.8
38.0
43.0
?

57.5
?
?

64.7
19.7
61.0
2.4
37.3
?
21.5
75.5
36.9
33.7

5954.8

13.5

8196.7

7.4

178.53

246.2

1684.6

33.5

(t) GM (su)
Borda score (rank)
1

WS

76.9 (4)

crossword-m1-words-05-06

2

60.3 (7)

crossword-m1c-words-vg7-7 ext

Table 5: Speedups solving times xcsp (Gecode, lex, data center, w = 16 512).
heuristics whereas instances solved sequential parallel using dom/wdeg
dom/bwdeg. again, highlights importance search strategy.
Figure 6 boxplot speedups different numbers workers solving fzn
instances. median speedups around w2 average dispersion remains
low.
512
256

speedup (su)

128
64
32
16
8
4
2

16

32

64

128

256

512

workers (w)

Figure 6: Scalability 512 workers (Gecode, lex, data center).
450

fiEmbarrassingly Parallel Search CP

Instance

EPS

market split s5-02
market split s5-06
market split u5-09
pop stress 0600
nmseq 400
pop stress 0500
fillomino 18
steiner-triples 09
nmseq 300
golombruler 13
cc base mzn rnd test.11
ghoulomb 3-7-20
still life free 8x8
bacp-6
depot placement st70 6
open stacks 01 wbp 20 20 1
bacp-27
still life still life 9
talent scheduling alt film117
(t) GM (su)

WS



su



su

467.1
452.7
468.1
874.8
342.4
433.2
160.2
108.8
114.5

24.3
24.4
24.4
10.8
8.5
10.1
13.9
17.2
6.6

658.6
650.7
609.2
2195.7
943.2
811.0
184.6
242.4
313.1

17.3
17.0
18.7
4.3
3.1
5.4
12.1
7.7
2.4

154.0
1143.6
618.2
931.2
400.8
433.9
302.7
260.2
189.0
22.7

20.6
7.3
6.8
9.6
16.4
18.3
17.6
16.4
16.9
74.0

210.4
2261.3
3366.0
1199.4
831.0
1172.5
374.1
548.4
196.8
110.5

15.1
3.7
1.2
7.5
7.9
6.8
14.3
7.8
16.2
15.2

414.7

15.1

888.4

7.7

Table 6: Solving times speedups fzn (Gecode, lex, cloud, w = 24).

4.5 Cloud Computing
EPS deployed Microsoft Azure cloud platform. available computing
infrastructure organized follows: cluster nodes computes application; one head
node manages cluster nodes; proxy nodes load-balances communication
cluster nodes. contrary data center, cluster nodes may far
communication time may take longer. Proxy nodes requires 2 cores managed
service provider. Here, 3 nodes 8 cores 56 GB RAM memory provide 24
workers (cluster nodes) managed MPI.
Table 6 compares Gecode implementations EPS work stealing solving
fzn instances 24 workers. Briefly, EPS always faster work stealing,
therefore, efficient parallelize sequential solver. work
stealing suffers higher communication overhead cloud data center.
Furthermore, architecture computing infrastructure location cluster
nodes mostly unknown forbid improvements work stealing
proposed Machado et al. (2013), Xie Davenport (2010).
4.6 Embarrassingly Distributed Search
section, transform reasonable effort parallel solver (EPS) distributed
parallel solver (EDPS) using batch scheduler OAR (Capit et al., 2005) provided
451

fiMalapert, Regin, & Rezgui

data center. fact, batch scheduler OAR plays foreman. parallel Choco2
solver modified workers write subproblems files instead solving
them. Then, script submits jobs/subproblems OAR batch scheduler, waits
termination, gathers results. OAR schedules jobs cluster using
priority FIFO backfilling fair-share based priorities. Backfilling allows start
lower priority jobs without delaying highest priority jobs whereas fair-share means
user/application preferred way. main drawback new worker must
created subproblem. worker process allocated OAR predefined
resources. worker either sequential (EDS) parallel solver (EDPS).
approach offers practical advantage resource reservation data center.
Indeed, asking MPI process, one wait enough resources available
process starts. Here, resources (cores nodes) nibbled soon
become available drastically reduce waiting time. Furthermore, bypasses
limitations Threads Technology allowing use multiple nodes data center.
However, clearly increases recomputation overhead because, worker solves single
subproblem instead multiple subproblems. So, model creation initial propagation
realized often. introduces non-negligible submission overhead
time taken create submit jobs OAR batch scheduler.
4.6.1 Anomaly crossword-m1c-words-vg7-7 ext
investigate low speedups solving instance crossword-m1c-words-vg7-7
variable selection heuristic (see Table 4). compare results parallel
(EPS, w = 16) distributed (EDS sequential worker) algorithms different decomposition depths (d = 1, 2, 3). Table 7 gives solving times, speedups, efficiencies.
number distinct cores used distributed algorithm bad estimator computing efficiencies, used short period time. Therefore,
number c cores used compute efficiency EDS EDPS estimated
ratio total runtime wall-clock time.
First, parallel algorithm always slower sequential one. However,
speedups distributed algorithms significant even decrease quickly
decomposition depth increases. fall efficiency shows EDS scalable
sequential workers. Indeed, recomputation, especially submission overhead
become important number subproblems increases.
Second, bad performance parallel algorithms caused statistically
imbalanced decomposition would observe similar performance distributed
algorithm. Profiling parallel algorithm particular instances suggests bad
EDS


p

2
3
4

186
827
2935

EPS (w = 16)



su

eff



su

eff

73.0
229.0
797.0

10.2
3.3
1.1

0.435
0.128
0.039

1069.9
1074.2
1091.8

0.7
0.7
0.7

0.044
0.044
0.044

Table 7: EDS EPS crossword instance (Choco2, dom, data center).
452

fiEmbarrassingly Parallel Search CP

performance comes underlying solver itself. Indeed, number instructions
similar sequential parallel algorithms whereas numbers context switches,
cache references cache misses increase considerably. fact, parallel algorithms
spent half time internal methods extensional constraints, i.e.
relation constraint specified listing satisfying tuples. issue occurred
computing infrastructure different Java virtual machines. Note instances
use extensional constraints, impose fewer consequences. issue would
happen MPI implementation shared memory. So, advocates
implementations EPS based MPI rather Thread Technology.
4.6.2 Variations Golomb Ruler Problem
Golomb ruler set marks integer positions along imaginary ruler
two pairs marks distance apart. number marks ruler
order, largest distance two marks length. Here, enumerate
optimal rulers (minimal length specific number marks) simple constraint
model inspired one Galinier, Jaumard, Morales, Pesant (2001)
heuristics lex dom reasonable choice. Table 8 gives solving times, speedups,
efficiencies parallel algorithm (w = 16), distributed algorithm sequential
workers (w = 1), distributed algorithms parallel workers (w = 16
worker decomposition depth dw = 2) different master decomposition depths d.
First, EPS obtains almost linear speedup decomposition depth large enough.
Without surprise, speedups lower enough subproblems. Second,
distributed algorithm EDS sequential workers efficient number subproblems remains low. Otherwise, still give speedups (dom), wastes
resources since efficiency low. fact, submitting many jobs batch
scheduler (lex) lead high submission overhead (around 13 minutes) globally degrades performance. Finally, distributed algorithms parallel workers offer
good trade-off speedups efficiencies allows use many resources
submitting jobs thus reducing submission recomputation overheads. Note EDS = 1 tested roughly equivalent EPS
16 workers, EDPS = 3 tested submission overhead becomes
important.

EDPS (w = 16, dw = 2)

EDS


p



su

eff



su

lex

1
2
3

20
575
14223


769.0
17880.0


66.8
2.9


0.846
0.005

572.0
497.0


89.7
103.3


dom

1
2
3

20
222
5333


2394.0
3018.0


50,5
40,0


0,989
0,146

1538.0
366.0


78,6
330,2


EPS (w = 16)


su

eff

0.968
0.232


11141.7
4084.2
3502.6

4.6
12.6
14.7

0.288
0.786
0.916

0,935
0,742


28299,9
9703,6
8266,6

4,3
12,4
14,6

0,267
0,778
0,914

eff

Table 8: EDS EPS Golomb Ruler 14 marks (Choco2, data center).
453

fiMalapert, Regin, & Rezgui

parallel approaches reported good performance Golomb ruler problem. instance, Michel et al. (2009), Chu et al. (2009) respectively reported linear
speedups 4 8 workers. EDS efficient work stealing proposed
Menouer Le Cun (2014) using 48 workers ruler 13 marks efficient
selfsplit Fischetti et al. (2014) using 64 workers ruler 14 marks.
Last, enumerated optimal Golomb Rulers 15 16 marks using EDPS.
Master workers use lex heuristic. master decomposition depth equal
2 generates around 800 hundreds subproblems. 16 parallel workers
decomposition depth dw equal 2. settings, used 700 cores
data center solving process. So, bypasses limitations number
cores used MPI imposed administrator. Furthermore, solving process starts
immediately cores grabbed soon become available whereas MPI
process waits enough cores becomes simultaneously available. Enumerating optimal
rulers 15 16 marks respectively took 1422 5246 seconds. knowledge,
first time constraint solver finds rulers, furthermore reasonable amount time. However, optimal rulers discovered via exhaustive
computer search (Shearer, 1990). recently, Distributed Computing Technologies Inc
(20) found optimum rulers 26 marks. Beside, plane construction (Atkinson & Hassenklover, 1984) allows find larger optimal rulers.
4.7 Comparison Portfolios
Portfolio approaches exploit variability performance observed several
solvers, several parameter settings solver. use 4 portfolios. portfolio
CPHydra (OMahony et al., 2008) uses features selection top solvers Mistral,
Gecode, Choco2. CPHydra uses case-based reasoning determine solve
unseen problem instance exploiting case base problem solving experience. aims
find feasible solution within 30 minutes, handle optimization solution problems time limit hard-coded. static fixed-size portfolios
(Choco2, CAG, OR-tools) use different variable selection heuristics (see Section 2.1) well
randomization restarts. Details Choco2 CAG found (Malapert &
Lecoutre, 2014). CAG portfolio extends Choco2 portfolio using solvers
AbsCon Gecode. So, CAG always produces better results Choco2. OR-tools
portfolio gold medal Minizinc challenge 2013 2014. seem unfair
compare parallel solvers portfolios using different numbers workers, designing
scalable portfolio (up 512 workers) difficult task almost implementation
publicly available.
Table 9 gives solving times EPS portfolios solving xcsp instances
data center. First, CPHydra 16 workers solves 2 among 16 unsatisfiable instances
(cc-15-15-2 pigeons-14), less 2 seconds whereas difficult
approaches. OR-tools second less efficient approach solves fewer
problems often takes longer confirmed low Borda score. parallel Choco2
using dom/wdeg better average Choco2 portfolio even portfolio solves
instances much faster scen11-f5 queensKnights-20-5-mul. case,
diversification provided portfolio outperforms speedups offered parallel
454

fiEmbarrassingly Parallel Search CP

Instances

EPS
Choco2

cc-15-15-2
costasArray-14
crossword-m1-words-05-06
crossword-m1c-words-vg7-7 ext
fapp07-0600-7
knights-20-9
knights-25-9
knights-80-5
langford-3-17
langford-4-18
langford-4-19
latinSquare-dg-8
lemma-100-9-mod
ortholatin-5
pigeons-14
quasigroup5-10
queenAttacking-6
queensKnights-20-5-mul
ruler-70-12-a3
ruler-70-12-a4
scen11-f5
series-14
squares-9-9
squaresUnsat-19-19
Arithmetic mean
Borda score (rank)

Portfolio

Gecode

Choco2

CAG

OR-tools

w = 16

w = 16

w = 512

w = 14

w = 23

w = 16

2192.1
649.9
204.6
1611.9
2295.7
491.3
1645.2
1395.6
3062.2
538.3
2735.3
294.8
145.3
362.4
2993.3
451.5
706.4
5209.5
42.8
1331.3

338.9
115.9
3039.8


64.4
240.6
171.7

5190.7
7462.3
1413.7
24351.5
3203.2
26871.2
613.5
3.4
309.5
383.3
27.1
42514.8

96.6
178.9

22.5
22.8



3.6
18.7
13.3

153.4
214.9
49.3
713.5
94.6
782.5
23.6
1.0
10.4
15.3
1.7
1283.9

3.7
6.0

1.1
1.3


1102.6
6180.8
512.3
721.2
37.9
3553.9
9324.8
1451.5
8884.7
2126.0
12640.2
65.1
435.3
4881.2
12336.9
3545.8
2644.5
235.3
123.5
1250.2
45.3
1108.3
1223.7
4621.1

3.5
879.4
512.3
721.2
3.2
0.8
1.1
301.6
8884.7
2126.0
12640.2
36.4
50.1
4371.0
5564.5
364.3
2644.5
1.0
123.5
1250.2
8.5
302.1
254.3
4621.1

1070.0
1368.8
22678.1
13157.2



32602.6



4599.8
38.2
4438.7
12279.6
546.0


8763.1


416.2
138.3


1385.0

5954.8

178.5

3293.8

1902.7

7853.6

65.0 (3)

52.2 (5)

77.1 (1)

57.0 (4)

72.8 (2)

20.0 (6)

Table 9: Solving times EPS portfolio (data center).
B&B algorithm. emphasized CAG portfolio solves instances
obtains several best solving times. parallel Gecode 16 workers often slower
less robust portfolios Choco2 CAG. However, increasing number
workers 512 clearly makes fastest solver, still less robust five instances
solved within time limit.
conclude, Choco2 CAG portfolios robust thanks inherent diversification, solving times vary one instance another. 16 workers,
implementations EPS outperform CPHydra OR-tools portfolio, competitive
Choco2 portfolio, slightly dominated CAG portfolio. fact,
good scaling EPS key beat portfolios.

5. Conclusion
introduced Embarrassingly Parallel Search (EPS) method solving constraint
satisfaction problems constraint optimization problems. approach several
advantages. First, efficient method matches even outperforms state-of-the455

fiMalapert, Regin, & Rezgui

art algorithms number problems using various computing infrastructures. Second,
involves almost communication synchronization mostly relies underlying
sequential solver implementation debugging made easier. Last,
simplicity method allows propose many variants adapted specific applications
computing infrastructures. Moreover, certain restrictions, parallel algorithm
deterministic, even mimic sequential algorithm important
practice either production debugging.
several interesting perspectives around EPS. First, modified order
provide diversification learn useful information solving subproblems.
instance, easily combined portfolio approach subproblems
solved several search strategies. Second, thanks simplicity, simplest variants
EPS could implemented meta-searches (Rendl, Guns, Stuckey, & Tack, 2015),
would offer convenient way parallelize applications satisfactory efficiency. Last,
another perspective predict solution time large combinatorial problem, based
known solution times small set subproblems based statistical machine
learning approaches.

Acknowledgments
would thank much Christophe Lecoutre, Laurent Perron, Youssef Hamadi,
Carine Fedele, Bertrand Lecun Tarek Menouer comments advices
helped improve paper. work supported CNRS OSEO
(BPI France) within ISI project Pajero. work granted access HPC
visualization resources Centre de Calcul Interactif hosted Universite Nice Sophia
Antipolis, Microsoft Azure Cloud. wish thank anonymous
referees comments.

Appendix A. Efficiency Hyper-Threading
section, show hyper-threading technology improves efficiency EPS
solving instances xcsp1 multi-core computer. Figure 7 boxplot
speedups provided hyper-threading parallel solver among Choco2, Gecode,
OR-tools. Here, speedups indicate many times parallel solver using 80 workers
(w = 2c) faster one using 40 workers (w = c). maximum speedup according
Amdahls law 2.
Choco2 tested lex dom whereas Gecode OR-tools use lex.
compared work stealing approach proposed Schulte (2000) denoted
Gecode-WS. Hyper-threading clearly improves parallel efficiency EPS whereas
performance work stealing roughly remains unchanged. interesting
EPS high CPU demand resources physical core shared
two logical cores. Indeed, performance hyper-threading known
application-dependent. exception lemma-100-9-mod squares-9-9, Choco2
OR-tools faster 80 workers. lemma-100-9-mod, Choco2 decomposition
80 workers takes longer generates many subproblems. instance solved
456

fiEmbarrassingly Parallel Search CP

hyperthreading speedup

2

1

0.5

Choco2-lex

Choco2-dom

Gecode

OR-tools

Gecode-WS

Figure 7: Speedups provided hyper-threading (multi-core, w = 40, 80).
easily OR-tools (less two seconds) becomes difficult improve efficiency. squares-9-9, decomposition changes according number workers,
cannot explain hyper-threading improve EPS. parallel efficiency
Gecode reduced multiple instances interest hyper-threading less obvious
Choco2 OR-tools. conclude, hyper-threading globally improves efficiency
EPS limited interest work stealing.

References
Almasi, G. S., & Gottlieb, A. (1989). Highly Parallel Computing. Benjamin-Cummings
Publishing Co., Inc., Redwood City, CA, USA.
Amadini, R., Gabbrielli, M., & Mauro, J. (2013). Empirical Evaluation Portfolios
Approaches Solving CSPs Gomes, C., & Sellmann, M.Eds., Integration
AI Techniques Constraint Programming Combinatorial Optimization
Problems, Vol. 7874 Lecture Notes Computer Science, pp. 316324. Springer
Berlin Heidelberg.
Amdahl, G. (1967). Validity Single Processor Approach Achieving Large Scale
Computing Capabilities Proceedings April 18-20, 1967, Spring Joint Computer Conference, AFIPS 67, pp. 483485, New York, NY, USA. ACM.
Anderson, D. P., Cobb, J., Korpela, E., Lebofsky, M., & Werthimer, D. (2002). Seti@home:
experiment public-resource computing Commun. ACM, 45 (11), 5661.
Atkinson, M. D., & Hassenklover, A. (1984). Sets Integers Distinct Differences Tech.
Rep. SCS-TR-63, School Computer Science, Carlton University, Ottawa Ontario,
Canada.
Bader, D., Hart, W., & Phillips, C. (2005). Parallel Algorithm Design Branch
Bound G, H.Ed., Tutorials Emerging Methodologies Applications Operations Research, Vol. 76 International Series Operations Research & Management
Science, pp. 51544. Springer New York.
457

fiMalapert, Regin, & Rezgui

Barney, B., & Livermore, L. (2016). Introduction Parallel Computing
computing.llnl.gov/tutorials/parallel comp/.

https://

Bauer, M. A. (2007). High performance computing: software challenges Proceedings
2007 International Workshop Parallel Symbolic Computation, PASCO 07,
pp. 1112, New York, NY, USA. ACM.
Beck, C., Prosser, P., & Wallace, R. (2005). Trying Fail-First Recent Advances
Constraints, pp. 4155. Springer Berlin Heidelberg.
Bordeaux, L., Hamadi, Y., & Samulowitz, H. (2009). Experiments Massively Parallel
Constraint Solving. Boutilier (Boutilier, 2009), pp. 443448.
Boussemart, F., Hemery, F., Lecoutre, C., & Sais, L. (2004). Boosting Systematic Search
Weighting Constraints Proceedings 16th Eureopean Conference Artificial Intelligence, ECAI2004, including Prestigious Applicants Intelligent Systems,
PAIS, pp. 146150.
Boutilier, C.Ed.. (2009). IJCAI 2009, Proceedings 21st International Joint Conference
Artificial Intelligence, Pasadena, California, USA, July 11-17.
Brams, S. J., & Fishburn, P. C. (2002). Voting procedures Arrow, K. J., Sen, A. K.,
& Suzumura, K.Eds., Handbook Social Choice Welfare, Vol. 1 Handbook
Social Choice Welfare, chap. 4, pp. 173236. Elsevier.
Budiu, M., Delling, D., & Werneck, R. (2011). DryadOpt: Branch-and-bound distributed
data-parallel execution engines Parallel Distributed Processing Symposium
(IPDPS), 2011 IEEE International, pp. 12781289. IEEE.
Burton, F. W., & Sleep, M. R. (1981). Executing Functional Programs Virtual Tree
Processors Proceedings 1981 Conference Functional Programming
Languages Computer Architecture, FPCA 81, pp. 187194, New York, NY, USA.
ACM.
Capit, N., Da Costa, G., Georgiou, Y., Huard, G., Martin, C., Mounie, G., Neyron, P., &
Richard, O. (2005). Batch Scheduler High Level Components Proceedings
Fifth IEEE International Symposium Cluster Computing Grid (CCGrid05) - Volume 2 - Volume 02, CCGRID 05, pp. 776783, Washington, DC, USA.
IEEE Computer Society.
Choco, T. (2010). Choco: open source java constraint programming library Ecole des
Mines de Nantes, Research report, 1, 1002.
Chong, Y. L., & Hamadi, Y. (2006). Distributed Log-Based Reconciliation Proceedings
2006 Conference ECAI 2006: 17th European Conference Artificial Intelligence August 29 September 1, 2006, Riva Del Garda, Italy, pp. 108112, Amsterdam,
Netherlands, Netherlands. IOS Press.
Chu, G., Schulte, C., & Stuckey, P. J. (2009). Confidence-Based Work Stealing Parallel Constraint Programming Gent, I. P.Ed., CP, Vol. 5732 Lecture Notes
Computer Science, pp. 226241. Springer.
Chu, G., Stuckey, P. J., & Harwood, A. (2008). PMiniSAT: Parallelization MiniSAT
2.0 Tech. Rep., NICTA : National ICT Australia.
458

fiEmbarrassingly Parallel Search CP

Cire, A. A., Kadioglu, S., & Sellmann, M. (2014). Parallel Restarted Search Proceedings
Twenty-Eighth AAAI Conference Artificial Intelligence, AAAI14, pp. 842
848. AAAI Press.
Cornuejols, G., Karamanov, M., & Li, Y. (2006). Early Estimates Size Branchand-Bound Trees INFORMS Journal Computing, 18, 8696.
Crainic, T. G., Le Cun, B., & Roucairol, C. (2006). Parallel branch-and-bound algorithms
Parallel combinatorial optimization, 1, 128.
De Kergommeaux, J. C., & Codognet, P. (1994). Parallel logic programming systems ACM
Computing Surveys (CSUR), 26 (3), 295336.
Distributed Computing Technologies Inc (20). Distributed.net home page http://
www.distributed.net/.
Een, N., & Sorensson, N. (2005). MiniSat: SAT solver conflict-clause minimization
Sat, 5, 1.
Ezzahir, R., Bessiere, C., Belaissaoui, M., & Bouyakhf, E. H. (2007). DisChoco: platform
distributed constraint programming DCR07: Eighth International Workshop
Distributed Constraint Reasoning - conjunction IJCAI07, pp. 1621, Hyderabad, India.
Fischetti, M., Monaci, M., & Salvagnin, D. (2014). Self-splitting workload parallel
computation Simonis, H.Ed., Integration AI Techniques Constraint
Programming: 11th International Conference, CPAIOR 2014, Cork, Ireland, May 1923, 2014. Proceedings, pp. 394404, Cham. Springer International Publishing.
Gabriel, E., Fagg, G., Bosilca, G., Angskun, T., Dongarra, J., Squyres, J., Sahay, V., Kambadur, P., Barrett, B., Lumsdaine, A., et al. (2004). Open MPI: Goals, Concept,
Design next generation MPI implementation Recent Advances Parallel
Virtual Machine Message Passing Interface, pp. 97104. Springer.
Galea, Fran c., & Le Cun, B. (2007). Bob++ : Framework Exact Combinatorial
Optimization Methods Parallel Machines International Conference High Performance Computing & Simulation 2007 (HPCS07) conjunction 21st
European Conference Modeling Simulation (ECMS 2007), pp. 779785.
Galinier, P., Jaumard, B., Morales, R., & Pesant, G. (2001). Constraint-Based Approach
Golomb Ruler Problem 3rd International Workshop integration AI
techniques.
Gendron, B., & Crainic, T. G. (1994). Parallel branch-and-bound algorithms: Survey
synthesis Operations research, 42 (6), 10421066.
Gent, I., & Walsh, T. (1999). CSPLIB: Benchmark Library Constraints Proceedings 5th International Conference Principles Practice Constraint
Programming, CP 99, pp. 480481.
Gomes, C., & Selman, B. (1997). Algorithm Portfolio Design: Theory vs. Practice
Proceedings Thirteenth conference Uncertainty artificial intelligence, pp.
190197.
459

fiMalapert, Regin, & Rezgui

Gomes, C., & Selman, B. (1999). Search strategies hybrid search spaces Tools
Artificial Intelligence, 1999. Proceedings. 11th IEEE International Conference, pp.
359364. IEEE.
Gomes, C., & Selman, B. (2000). Hybrid Search Strategies Heterogeneous Search Spaces
International Journal Artificial Intelligence Tools, 09, 4557.
Gomes, C., & Selman, B. (2001). Algorithm Portfolios Artificial Intelligence, 126, 4362.
Gropp, W., & Lusk, E. (1993). MPI communication library: design portable
implementation Scalable Parallel Libraries Conference, 1993., Proceedings the,
pp. 160165. IEEE.
Gupta, G., Pontelli, E., Ali, K. A., Carlsson, M., & Hermenegildo, M. V. (2001). Parallel
execution prolog programs: survey ACM Transactions Programming Languages
Systems (TOPLAS), 23 (4), 472602.
Halstead, R. (1984). Implementation Multilisp: Lisp Multiprocessor Proceedings
1984 ACM Symposium LISP Functional Programming, LFP 84, pp.
917, New York, NY, USA. ACM.
Hamadi, Y. (2002). Optimal Distributed Arc-Consistency Constraints, 7, 367385.
Hamadi, Y., Jabbour, S., & Sais, L. (2008). ManySAT: Parallel SAT Solver. Journal
Satisfiability, Boolean Modeling Computation, 6 (4), 245262.
Haralick, R., & Elliott, G. (1980). Increasing Tree Search Efficiency Constraint Satisfaction Problems Artificial intelligence, 14 (3), 263313.
Harvey, W. D., & Ginsberg, M. L. (1995). Limited Discrepancy Search Proceedings
Fourteenth International Joint Conference Artificial Intelligence, IJCAI 95,
Montreal Quebec, Canada, August 20-25 1995, 2 Volumes, pp. 607615.
Heule, M. J., Kullmann, O., Wieringa, S., & Biere, A. (2012). Cube conquer: Guiding
CDCL SAT solvers lookaheads Hardware Software: Verification Testing,
pp. 5065. Springer.
Hirayama, K., & Yokoo, M. (1997). Distributed Partial Constraint Satisfaction Problem
Principles Practice Constraint Programming-CP97, pp. 222236. Springer.
Hyde, P. (1999). Java thread programming, Vol. 1. Sams.
Intel Corporation (2015). Intel MPI Library https://software.intel.com/en-us/intel
-mpi-library.
Jaffar, J., Santosa, A. E., Yap, R. H. C., & Zhu, K. Q. (2004). Scalable Distributed DepthFirst Search Greedy Work Stealing 16th IEEE International Conference
Tools Artificial Intelligence, pp. 98103. IEEE Computer Society.
Kale, L., & Krishnan, S. (1993). CHARM++: portable concurrent object oriented system
based C++, Vol. 28. ACM.
Kasif, S. (1990). Parallel Complexity Discrete Relaxation Constraint Satisfaction networks Artificial Intelligence, 45, 275286.
Kautz, H., Horvitz, E., Ruan, Y., Gomes, C., & Selman, B. (2002). Dynamic Restart Policies
18th National Conference Artificial Intelligence AAAI/IAAI, 97, 674681.
460

fiEmbarrassingly Parallel Search CP

Kjellerstrand, H. (2014). Hakan Kjellerstrands Blog http://www.hakank.org/.
Kleiman, S., Shah, D., & Smaalders, B. (1996). Programming threads. Sun Soft Press.
Korf, R. E., & Schreiber, E. L. (2013). Optimally Scheduling Small Numbers Identical
Parallel Machines Borrajo, D., Kambhampati, S., Oddi, A., & Fratini, S.Eds.,
ICAPS. AAAI.
Krishna, J., Balaji, P., Lusk, E., Thakur, R., & Tiller, F. (2010). Implementing MPI
Windows: Comparison Common Approaches Unix Recent Advances
Message Passing Interface, Vol. 6305 Lecture Notes Computer Science, pp.
160169. Springer Berlin Heidelberg.
Lai, T.-H., & Sahni, S. (1984). Anomalies Parallel Branch-and-bound Algorithms Commun. ACM, 27 (6), 594602.
Lantz, E. (2008). Windows HPC Server : Using Microsoft Message Passing Interface (MSMPI).
Le Cun, B., Menouer, T., & Vander-Swalmen, P. (2007). Bobpp http://forge.prism
.uvsq.fr/projects/bobpp.
Leaute, T., Ottens, B., & Szymanek, R. (2009). FRODO 2.0: open-source framework
distributed constraint optimization. Boutilier (Boutilier, 2009), pp. 160164.
Leiserson, C. E. (2010). Cilk++ concurrency platform Journal Supercomputing,
51 (3), 244257.
Lester, B. (1993). art parallel programming. Prentice Hall Englewood Cliffs, NJ.
Li, H. (2009). Introducing Windows Azure. Apress, Berkely, CA, USA.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal Speedup Las Vegas Algorithms
Inf. Process. Lett., 47, 173180.
Machado, R., Pedro, V., & Abreu, S. (2013). Scalability Constraint Programming
Hierarchical Multiprocessor Systems ICPP, pp. 530535. IEEE.
Malapert, A., & Lecoutre, C. (2014). propos de la bibliotheque de modeles XCSP
10emes Journees Francophones de Programmation par Contraintes(JFPC15),
Angers, France.
Mattson, T., Sanders, B., & Massingill, B. (2004). Patterns Parallel Programming (First
ed.). Addison-Wesley Professional.
Menouer, T., & Le Cun, B. (2013). Anticipated Dynamic Load Balancing Strategy Parallelize Constraint Programming Search 2013 IEEE 27th International Symposium
Parallel Distributed Processing Workshops PhD Forum, pp. 17711777.
Menouer, T., & Le Cun, B. (2014). Adaptive N P Portfolio Solving Constraint
Programming Problems Top Parallel Bobpp Framework 2014 IEEE 28th
International Symposium Parallel Distributed Processing Workshops PhD
Forum.
Michel, L., See, A., & Hentenryck, P. V. (2009). Transparent Parallelization Constraint
Programming INFORMS Journal Computing, 21, 363382.
461

fiMalapert, Regin, & Rezgui

Microsoft Corporation (2015). Microsoft HPC Pack 2012 R2 HPC Pack 2012 http://
technet.microsoft.com/en-us/library/jj899572.aspx.
Milano, M., & Trick, M. (2004). Constraint Integer Programming: Toward Unified
Methodology. Springer US, Boston, MA.
Moisan, T., Gaudreault, J., & Quimper, C.-G. (2013). Parallel Discrepancy-Based Search
Principles Practice Constraint Programming, Vol. 8124 Lecture Notes
Computer Science, pp. 3046. Springer Berlin Heidelberg.
Moisan, T., Quimper, C.-G., & Gaudreault, J. (2014). Parallel Depth-bounded Discrepancy
Search Simonis, H.Ed., Integration AI Techniques Constraint Programming: 11th International Conference, CPAIOR 2014, Cork, Ireland, May 19-23,
2014. Proceedings, pp. 377393, Cham. Springer International Publishing.
MPI-CH Team (2015). High-Performance Portable MPI http://www.mpich.org/.
Mueller, F., et al. (1993). Library Implementation POSIX Threads UNIX.
USENIX Winter, pp. 2942.
Nguyen, T., & Deville, Y. (1998). distributed arc-consistency algorithm Science
Computer Programming, 30 (12), 227 250. Concurrent Constraint Programming.
NICTA Optimisation Research Group (2012). MiniZinc FlatZinc http://www.g12
.csse.unimelb.edu.au/minizinc/.
Nielsen, M. (2006). Parallel Search Gecode Masters thesis, KTH Royal Institute
Technology.
OMahony, E., Hebrard, E., Holland, A., Nugent, C., & OSullivan, B. (2008). Using casebased reasoning algorithm portfolio constraint solving Irish Conference
Artificial Intelligence Cognitive Science, pp. 210216.
Pedro, V., Abreu, S., Pedro, V., & Abreu, S. (2010). Distributed Work Stealing Constraint Solving CoRR, abs/1009.3800, 118.
Perron, L. (1999). Search Procedures Parallelism Constraint Programming Principles Practice Constraint Programming CP99: 5th International Conference,
CP99, Alexandria, VA, USA, October 11-14, 1999. Proceedings, pp. 346360, Berlin,
Heidelberg. Springer Berlin Heidelberg.
Perron, L., Nikolaj, V. O., & Vincent, F. (2012). Or-Tools Tech. Rep., Google.
Pruul, E., Nemhauser, G., & Rushmeier, R. (1988). Branch-and-bound Parallel Computation: historical note Operations Research Letters, 7, 6569.
Refalo, P. (2004). Impact-Based Search Strategies Constraint Programming Wallace,
M.Ed., Principles Practice Constraint Programming, 10th International Conference, CP 2004, Toronto, Canada, Vol. 3258 Lecture Notes Computer Science,
pp. 557571. Springer.
Regin, J.-C., Rezgui, M., & Malapert, A. (2013). Embarrassingly Parallel Search Principles Practice Constraint Programming: 19th International Conference, CP
2013, Uppsala, Sweden, September 16-20, 2013. Proceedings, pp. 596610. Springer
Berlin Heidelberg, Berlin, Heidelberg.
462

fiEmbarrassingly Parallel Search CP

Regin, J.-C., Rezgui, M., & Malapert, A. (2014). Improvement Embarrassingly Parallel Search Data Centers OSullivan, B.Ed., Principles Practice Constraint
Programming: 20th International Conference, CP 2014, Lyon, France, September 812, 2014. Proceedings, Vol. 8656 Lecture Notes Computer Science, pp. 622635.
Springer International Publishing, Cham.
Rendl, A., Guns, T., Stuckey, P., & Tack, G. (2015). MiniSearch: Solver-Independent
Meta-Search Language MiniZinc Pesant, G., Pesant, G., & Pesant, G.Eds.,
Principles Practice Constraint Programming: 21st International Conference,
CP 2015, Cork, Ireland, August 31 September 4, 2015, Proceedings, Vol. 9255
Lecture Notes Computer Science, pp. 376392. Springer International Publishing,
Cham.
Rezgui, M., Regin, J.-C., & Malapert, A. (2014). Using Cloud Computing Solving
Constraint Programming Problems First Workshop Cloud Computing Optimization, conference workshop CP 2014, Lyon, France.
Rolf, C. C., & Kuchcinski, K. (2009). Parallel Consistency Constraint Programming
PDPTA 09: 2009 International Conference Parallel Distributed Processing Techniques Applications, 2, 638644.
Rossi, F., Van Beek, P., & Walsh, T.Eds.. (2006). Handbook Constraint Programming.
Elsevier.
Roussel, O., & Lecoutre, C. (2008). Xml representation constraint networks format
http://www.cril.univ-artois.fr/CPAI08/XCSP2 1Competition.pdf.
Schulte, C. (2000). Parallel Search Made Simple Proceedings TRICS: Techniques
Implementing Constraint programming Systems, post-conference workshop
CP 2000, pp. 4157, Singapore.
Schulte, C. (2006). Gecode: Generic Constraint Development Environment http://www
.gecode.org/.
Shearer, J. B. (1990). New Optimum Golomb Rulers IEEE Trans. Inf. Theor., 36 (1),
183184.
Stephan, K., & Michael, K. (2011). SArTagnan - parallel portfolio SAT solver
lockless physical clause sharing Pragmatics SAT.
Sutter, H., & Larus, J. (2005). free lunch over: fundamental turn toward toward
Concurrency Dr. Dobbs Journal, 30, 202210.
Van Der Tak, P., Heule, M. J., & Biere, A. (2012). Concurrent cube-and-conquer Theory
Applications Satisfiability TestingSAT 2012, pp. 475476. Springer.
Vidal, V., Bordeaux, L., & Hamadi, Y. (2010). Adaptive K-Parallel Best-First Search:
Simple Efficient Algorithm Multi-Core Domain-Independent Planning
Proceedings Third International Symposium Combinatorial Search. AAAI
Press.
Wahbi, M., Ezzahir, R., Bessiere, C., & Bouyakhf, E.-H. (2011). DisChoco 2: Platform
Distributed Constraint Reasoning Proceedings IJCAI11 workshop
Distributed Constraint Reasoning, DCR11, pp. 112121, Barcelona, Catalonia, Spain.
463

fiMalapert, Regin, & Rezgui

Wilkinson, B., & Allen, M. (2005). Parallel Programming: Techniques Application
Using Networked Workstations Parallel Computers (2nd ed.). Prentice-Hall Inc.
Xie, F., & Davenport, A. (2010). Massively Parallel Constraint Programming Supercomputers: Challenges Initial Results Integration AI Techniques
Constraint Programming Combinatorial Optimization Problems: 7th International Conference, CPAIOR 2010, Bologna, Italy, June 14-18, 2010. Proceedings, Vol.
6140 Lecture Notes Computer Science, pp. 334338, Berlin, Heidelberg. Springer
Berlin Heidelberg.
Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically Configuring Algorithms Portfolio-Based Selection AAAI Conference Artificial Intelligence,
Vol. 10, pp. 210216.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based Algorithm Selection SAT Journal Artificial Intelligence Research, 32, 565606.
Yokoo, M., Ishida, T., & Kuwabara, K. (1990). Distributed Constraint Satisfaction DAI
Problems Proceedings 1990 Distributed AI Workshop, Bandara, TX.
Zoeteweij, P., & Arbab, F. (2004). Component-Based Parallel Constraint Solver De
Nicola, R., Ferrari, G. L., & Meredith, G.Eds., Coordination, Vol. 2949 Lecture
Notes Computer Science, pp. 307322. Springer.

464


