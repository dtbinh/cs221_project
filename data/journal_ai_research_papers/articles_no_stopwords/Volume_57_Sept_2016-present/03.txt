Journal Artificial Intelligence Research 57 (2016) 151-185

Submitted 05/16; published 10/16

Lightweight Random Indexing
Polylingual Text Classification
Alejandro Moreo Fernandez
Andrea Esuli

alejandro.moreo@isti.cnr.it
andrea.esuli@isti.cnr.it

Istituto di Scienza e Tecnologie dellInformazione
Consiglio Nazionale delle Ricerche
56124 Pisa,

Fabrizio Sebastiani

fsebastiani@qf.org.qa

Qatar Computing Research Institute
Hamad bin Khalifa University
PO Box 5825, Doha, QA

Abstract
Multilingual Text Classification (MLTC) text classification task documents
written one among set L natural languages, documents must
classified classification scheme, irrespective language. two main
variants MLTC, namely Cross-Lingual Text Classification (CLTC) Polylingual Text
Classification (PLTC). PLTC, focus paper, assume (differently
CLTC) language L representative set training documents;
PLTC consists improving accuracy |L| monolingual classifiers
leveraging training documents written (|L| 1) languages.
obvious solution, consisting generating single polylingual classifier juxtaposed
monolingual vector spaces, usually infeasible, since dimensionality resulting
vector space roughly |L| times monolingual one, thus often unmanageable.
response, use machine translation tools multilingual dictionaries
proposed. However, resources always available, always free use.
One machine-translation-free dictionary-free method that, best knowledge, never applied PLTC before, Random Indexing (RI). analyse RI
terms space time efficiency, propose particular configuration (that
dub Lightweight Random Indexing LRI). running experiments two well known public benchmarks, Reuters RCV1/RCV2 (a comparable corpus) JRC-Acquis (a parallel
one), show LRI outperform (both terms effectiveness efficiency) number
previously proposed machine-translation-free dictionary-free PLTC methods
use baselines.

1. Introduction
rapid growth multicultural multilingual information accessible Internet, properly classify texts written different languages become problem
relevant practical interest. Multilingual Text Classification (MLTC) text classification task documents written one among set L = {l1 , . . . , l|L| }
natural languages, documents must classified classification scheme, irrespective language. two main variants MLTC, namely
Cross-Lingual Text Classification (CLTC) Polylingual Text Classification (PLTC).
c
2016
AI Access Foundation. rights reserved.

fiMoreo, Esuli, & Sebastiani

CLTC task characterized fact that, languages subset LT
L, training documents; task thus consists classifying unlabelled
documents written languages LT (i.e., target languages) leveraging
training documents expressed languages LS = L\LT (i.e., source languages).
CLTC thus transfer learning problem (Pan & Yang, 2010), one needs transfer
knowledge acquired learning training data LS , task classifying
documents LT . previous work MLTC indeed focuses CLTC, fewer efforts
devoted PLTC, instead focus paper.
PLTC, representative set training documents languages L assumed
available. Therefore, straightforward solution may consist training |L| independent
monolingual classifiers, one language. However, solution suboptimal,
classifier obtained disregarding additional supervision could obtained
using training documents written (|L| 1) languages. PLTC thus
consists leveraging training documents written languages L improve
classification accuracy could obtained simply training |L| independent,
monolingual classifiers.
However, PLTC entails number obstacles work detriment efficient
representation. see this, assume generate single polylingual vector space (hereafter,
juxtaposed vector space) juxtaposing monolingual vector spaces. vector
space monolingual dataset usually consists tens even hundreds thousands
features; juxtaposed vector space polylingual dataset, dimensionality gets
roughly multiplied number distinct languages consideration. substantial increase feature space would degrade performance many classification
algorithms, so-called curse dimensionality, would bring
severe degradation efficiency. Additionally, co-occurrence-based techniques tend lose
power representations polylingual, since terms belonging different languages
rarely co-occur, (a problem usually referred feature disjointness).
response, authors proposed use machine translation (MT) tools
device simultaneously cope high dimensionality feature disjointness
PLTC. idea reduce problem monolingual case (typically English).
is, non-English training documents automatically translated English, added
English training set, monolingual (English) classifier trained. classification
time, non-English unlabelled documents translated English classified.
(Of course, idea used CLTC; case, training documents
translate.) However, MT-based PLTC (and CLTC) techniques suffer number
drawbacks (Wei, Yang, Lee, Shi, & Yang, 2014): (i) automatically translated texts usually
present different statistical properties respect human translations; (ii) MT systems
always available language pairs; (iii) training statistical MT system
free toolkits available requires collecting large corpora parallel text
domain interest, always easy.
Thesaurus-based dictionary-based methods, side, represent lighter
approach MLTC. multilingual dictionary thesaurus encompasses different languages available, kind unification vector representation may
attempted. customarily done replacing non-English words English equivalents dictionary, replacing terms thesaurus codes invariant
152

fiLightweight Random Indexing Polylingual Text Classification

across languages (e.g., BabelNet synsets Ehrmann, Cecconi, Vannella, McCrae, Cimiano,
& Navigli, 2014). However, bilingual dictionaries thesauri available language pairs, automatically constructing domain-dependent bilingual resource requires
suitable parallel corpus sentence-level alignment.
1.1 Distributional Representations
classification purposes, textual document usually represented vector vector
space according bag-of-words (BoW) model, i.e., distinct term corresponds
dimension vector space. juxtaposed vector space, columns
document-by-term matrix thus informative one languages.
Since distinct term corresponds dimension vector space, BoW model
agnostic respect semantic similarities among terms. is, dimension
term governor orthogonal dimension related term president,
dimension unrelated term transport. semantic relations among terms
uncovered detecting co-occurrences, i.e., contexts words tend
used together. idea rests distributional hypothesis, according words
similar meanings tend co-occur contexts (Harris, 1968). detecting
co-occurrences, possible establish parallelism term meaning geometrical properties vector space. Distributed Semantic Models (DSMs sometimes
called word space models Sahlgren, 2006) aim learning continuous compact distributed term representations, recently called word embeddings (Mikolov,
Sutskever, Chen, Corrado, & Dean, 2013b). DSMs gained lot attention
machine learning community, delivering improved results many natural language processing tasks (Bengio, Schwenk, Senecal, Morin, & Gauvain, 2006; Bullinaria & Levy, 2007;
Collobert, Weston, Bottou, Karlen, Kavukcuoglu, & Kuksa, 2011). DSM-based methods
categorised (see Pennington, Socher, & Manning, 2014; Baroni, Dinu, & Kruszewski,
2014) belonging (a) class context-counting models, often based
matrix factorization, e.g., Latent Semantic Analysis (LSA Deerwester, Dumais, Furnas,
Landauer, & Harshman, 1990; Osterlund, Odling, & Sahlgren, 2015), (b) class
context-predicting models, e.g., methods based deep learning architectures (Bengio,
2009; Mikolov et al., 2013b).
However, multilingual contexts huge quantities plain text language
processed order learn meaningful word representations, incurs high computational costs. Trying find representations large multilingual vocabulary
thus become computationally prohibitive. attempts recently made
direction, leveraging multilingual external resources Wikipedia articles
(Al-Rfou, Perozzi, & Skiena, 2013), bilingual dictionaries (Gouws & Sgaard, 2015),
word-aligned parallel corpora (Klementiev, Titov, & Bhattarai, 2012), sentence-aligned
parallel corpora (Zou, Socher, Cer, & Manning, 2013; Hermann & Blunsom, 2014; Lauly,
Boulanger, & Larochelle, 2014; Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, &
Saha, 2014), document-aligned parallel corpora (Vulic & Moens, 2015). However,
external resources may always available language combinations and,
available (e.g., Wikipedia articles), may uneven quality quantity
languages English. Alternatively, approaches require computationally
153

fiMoreo, Esuli, & Sebastiani

expensive post-processing step align word representations across languages (Mikolov, Le,
& Sutskever, 2013a; Faruqui & Dyer, 2014).
article discuss efficient representation mechanisms PLTC (i)
MT-free, (ii) require external resources, (iii) incur high computational
costs. particular, investigate suitability Random Indexing (RI Kanerva,
Kristofersson, & Holst, 2000; Sahlgren, 2005) effective representation mechanism
original co-occurrence matrix PLTC. RI context-counting model belonging
family random projections methods (Kaski, 1998; Papadimitriou, Raghavan, Tamaki, &
Vempala, 1998), produces linear projections nearly-orthogonal reduced space
original distances vectors approximately preserved (Hecht-Nielsen,
1994; Johnson, Lindenstrauss, & Schechtman, 1986). RI expected deliver fast
semantically meaningful representations reduced space, viewed cheaper
approximation LSA (Sahlgren, 2005). RI column polylingual
matrix produced depend single specific language (as instead
BoW representation). hypothesize could advantageous PLTC, since
entire new space becomes potentially informative languages once, thus making
problem easily separable enough dimensions considered. RI already
applied bilingual scenarios (Gorman & Curran, 2006; Sahlgren & Karlgren, 2005),
best knowledge tested PLTC case far. monolingual
TC, RI found competitive, superior, BoW (Sahlgren & Coster, 2004).
article demonstrate RI outperforms BoW model PLTC.
method present article, dub Lightweight Random Indexing (LRI),
inspired works Achlioptas (2001) Li, Hastie, Church (2006)
sparse random projections, goes one step pushing sparsity limit. LRI
designed orthogonality projection base maximized, causes sparsity
preserved projection. empirically show LRI helps Support Vector
Machines (SVMs) deliver better classification accuracies PLTC respect many
popular alternative vector space models (including main random projection variants,
LSA-based approaches, polylingual topic models), requiring substantially
less computation effort.
contribution work twofold. First, conduct comparative empirical
study several PLTC approaches two representative scenarios: first
training corpus comparable topic-level (i.e., documents direct translations
other, simply similar topics; exemplified RCV1/RCV2
dataset), second training corpus parallel document-level (i.e.,
text available languages thanks intervention human translators;
scenario exemplified JRC-Acquis dataset). show LRI yields best results
settings, terms effectiveness efficiency. second contribution,
present analytical study useful better understand nature random
mapping methods.
rest paper organized follows. Section 2 discuss related work.
Section 3 present problem statement, describe Random Indexing method
detail, present proposal. Section 4 reports results experiments
conducted. Section 5 presents analytical study computational efficiency, Section
6 concludes.
154

fiLightweight Random Indexing Polylingual Text Classification

2. Related Work
section gives overview main approaches PLTC emerged
literature. distinguish three groups methods, according whether problem approached (i) leveraging external resources, (ii) combining outcome independent
monolingual classifiers, (iii) reducing dimensionality resulting multilingual
feature space. discussion includes references CLTC techniques
consider relevant PLTC approach.
2.1 Exploiting External Multilingual Resources
Multilingual text classification relatively recent area research, previous
efforts within devoted CLTC subtask. CLTC labelled
information languages, previous approaches typically relied automatic translation
mechanisms means fill gap source target languages.
main difference CLTC PLTC lies fact PLTC exploits labelled
documents belonging different languages learning. Despite this, two tasks
close-knit relation, since cross-lingual adaptation generally carried
means external resources, parallel corpora, bilingual dictionaries,
statistical thesauri.
suitable (unlabelled) multilingual corpus containing short aligned pieces texts
available, correlations among groups words two languages could explored.
Cross-Lingual Kernel Canonical Correlation Analysis (CL-KCCA) proposed Vinokourov, Shawe-Taylor, Cristianini (2002) means obtain semantic cross-lingual
representation, investigating correlations aligned text fragments. CL-KCCA
takes advantage kernel functions order map aligned texts high-dimensional
space manner correlations mapped aligned texts jointly
maximized. cross-lingual representation could used classification, retrieval,
clustering tasks. CL-KCCA investigated combination Support Vector Machines (SVMs) applied cross-lingual patent classification Li Shawe-Taylor
(2007). method, called SVM 2k, learns two SVM-based classifiers searching two
linear projections original feature space language distance
projections (instead correlation projections) two aligned texts minimized.
similar vein, polylingual topic models (Mimno, Wallach, Naradowsky, Smith, &
McCallum, 2009) proposed extension Latent Dirichlet Allocation (LDA
Blei, Ng, & Jordan, 2003) polylingual case. LDA generative model
assigns probability distributions documents latent topics, latent topics
terms. distributions viewed compact representations documents
latent space. Since topics discovered Polylingual LDA (PLDA) aligned across
languages, documents represented common vector space regardless language
written in. However, PLDA (which use baseline experimental
section) requires parallel collection documents aligned sentence level.
Bilingual dictionaries used straightforward manner carry word-byword translation feature space. However, dictionary-based translations suffer
several deficiencies, e.g., context-unaware translations might perform poorly handling
polysemic words; dictionaries might suffer substantial lack coverage novel terms
155

fiMoreo, Esuli, & Sebastiani

domain-dependent terminology; dictionaries might available language
pairs, free use. response drawbacks, automatic acquisition
statistical bilingual dictionaries proposed. Wei et al. (2014) explored cooccurrence-based method measure polylingual statistical strength correlation
among words parallel corpus. correlations taken account reinforce weight feature order select important (highly weighted) ones.
Gliozzo Strapparava (2006) experimented bilingual dictionaries and, interestingly, provided means automatically obtain Multilingual Domain Model (MDM),
natural extension domain models multiple languages, additional multilingual resources available. domain model defines soft relations words
domain topics. absence multilingual dictionary, MDM could automatically
obtained comparable corpus performing Latent Semantic Analysis (explained
detail below).
argued words shared across languages play important role
searching semantic latent space. Accordingly, Steinberger, Pouliquen, Ignat
(2004) exploit language-independent tokens shared across languages,
propose simple method link documents existing external resources thesauri,
nomenclatures, gazetteers. Finally, de Melo Siersdorfer (2007) use ontologies
map original features onto synset-like identifiers, documents translated
language-independent feature space.
MT tools, side, provide elaborated translations texts, represent promising research field multilingual tasks. Unfortunately, above-mentioned
problems regarding availability, accessibility, performance still hold case.
effect different translation strategies CLTC investigated Bel, Koster,
Villegas (2003), Rigutini, Maggini, Liu (2005), Wei, Lin, Yang (2011).
Even available, MT tools may expensive resources. reason,
experiments Prettenhofer Stein (2010) restrict use MT tool limited budget
calls. Structural Correspondence Learning (SCL) method, initially proposed
domain adaptation, indeed applied CLTC. key idea method consists
discovering cross-lingual correspondences pairs terms (dubbed pivot features)
later used bridge across two languages. Pivot features play important
role bilingual tasks, since establish pairs words behave similarly source
target languages, allowing one find cross-language structural correspondences. One
special type pivot features obviously words shared across languages,
proper nouns, technical terms, yet lexicalized terms, stemmed forms etymologically
related terms. Nastase Strapparava (2013) found etymological ancestors words
actually add useful information, allowing transcend cross-lingual boundaries.
method however depends availability etymological thesauri (such Wikipedias
Wiktionary, Etymological WordNet), remains restricted historically interrelated
languages.
sum, applicability multilingual methods discussed section usually
constrained availability external resources. aim overcoming limitations, restrict investigations dictionary-free, MT-free multilingual methods.
156

fiLightweight Random Indexing Polylingual Text Classification

2.2 Monolingual Classifiers Multiview Learning
Given availability representative set labelled documents language,
simple baseline, known nave polylingual classifier, could obtained delegating
classification process individual monolingual classifiers, built upon separate
monolingual data. solution sub-optimal, classifier exploit labelled
information languages, type information might provide insights
different perspectives semantics classes.
Garca Adeva, Calvo, Lopez de Ipina (2005) compared different nave strategies,
considering one single polylingual classifier, i.e., classifier works juxtaposed
representation (1C), vs. various monolingual ones (NC), one language-independent
preprocessor (1P) vs. various language-specific ones (NP), using various learning methods
bilingual Spanish/Basque benchmark. experimentation combinations NPNC NP-1C, consider baselines, yielded best results terms
running time, memory usage, accuracy.
Even though training separate language-specific classifiers simple way approach
PLTC task, strategies could improve final accuracy better
merging outcomes classifier. Multiview learning (Xu, Tao, & Xu, 2013) TC
deals parallel texts, i.e., case document available languages,
language considered separate source. shown Amini, Usunier,
Goutte (2009) multiview majority voting algorithm, returns label output
highest number language-specific classifiers, outperforms nave polylingual
classifier multiview Gibbs classifier, bases predictions mean prediction
language-specific classifier. Amini Goutte (2010) proposed co-regularization
approach multiview text classification minimizes joint loss function takes
account language-specific classifier loss. However, availability parallel
corpus containing documents views strong restriction, usually
alleviated leveraging machine translation tools automatically generate missing
documents views.
2.3 Dimensionality Reduction Multilingual Classification
One main challenges juxtaposed vector space approach PLTC concerns
relevant increase number features represent documents, i.e., dimensionality vector space (Rigutini et al., 2005). Feature selection methods attempt
select reduced subset informative features original set F size
subset much smaller |F | reduced set yields high classification
effectiveness. TC problem usually tackled via filtering approach, relies
mathematical function meant measure contribution feature classification task. Yang Pedersen (1997) showed filtering approaches may improve
performance classification, even aggressive reduction ratios (e.g., removal 90%
features).
Another important dimensionality reduction technique Latent Semantic Analysis
(LSA aka Latent Semantic Indexing), originated information retrieval
community (Deerwester et al., 1990), later applied cross-lingual classification (Gliozzo & Strapparava, 2006; Xiao & Guo, 2013) cross-lingual problems general
157

fiMoreo, Esuli, & Sebastiani

(Dumais, Letsche, Littman, & Landauer, 1997). LSA maps original document-term matrix lower dimensional latent semantic space attempts capture (linear)
relations among original features documents. mapping carried
means singular value decomposition (SVD) original document-term matrix .
SVD decomposes = V U , diagonal matrix containing eigenvalues . approximation Mk = Vk k UkT original matrix computed
taking k largest eigenvalues setting remaining ones 0; Mk said
rank-k optimal terms Frobenius norm. Vk Uk orthogonal matrices
explain relations among pairs terms pairs documents, respectively.
Although LSA successfully used discover hidden relations indirectly
correlated features, case terms belonging different languages, suffers
high computational costs. Random mappings arise alternative LSA,
perform comparably different machine learning tasks preserving important
characteristics LSA, bringing about, time, significant savings
terms computational cost (Fradkin & Madigan, 2003). Random Projections (RPs
Papadimitriou et al., 1998) Random Mappings (RMs Kaski, 1998) two equivalent
formulations deriving Johnson-Lindenstrauss lemma (Johnson et al., 1986),
states distances Euclidean space approximately preserved projected onto
lower-dimensional random space. formulations based fundamental
result Hecht-Nielsen (1994), proved many nearly orthogonal
truly orthogonal directions high-dimensional spaces.
RP-like methods formalized terms projection original documentterm matrix means random matrix , i.e., M|D|n = M|D||F | |F |n ,
approximates identity matrix, |D| |F | indicate number documents
terms collection, n stands reduced dimensionality, typically
chosen advance. definition random-projection matrix fundamental
aspect method; Achlioptas (2001) demonstrated random distribution
zero mean unit variance satisfies Johnson-Lindenstrauss lemma, proposed two
simple distributions definition elements ij = {ij } random projection
matrix, setting parameter distribution Equation 1 either = 2 = 3:

1
+1 probability 2s

0 probability 1 1s
ij =

(1)

1
1 probability 2s
Achlioptas proved configuration = 3 used speed computation, since case 1/3 data non-zero (sparse random projection),pand
therefore 2/3 computations skipped. Similarly, Li et al. (2006) set = |F |
= |F |/ log |F | (very sparse random projections) significantly speed computation still preserving inner distances.
Random Indexing (RI), first proposed Kanerva et al. (2000), equivalent formulation RPs accommodates Achlioptas theory. Sahlgren (2001) defines RI
approximate alternative LSA semantic representation. RI maintains dictionary
random index vectors feature original space. random index vector consists n-dimensional sparse vector k non-zero values, randomly distributed across
+1 1 (the method explained detail Section 3). work Gorman
158

fiLightweight Random Indexing Polylingual Text Classification

Curran (2006) different weighting criteria random index vectors dictionary
proven useful improving matrix representation. RI tested different tasks,
search (Rangan, 2011), query expansion (Sahlgren, Karlgren, Coster, & Jarvinen,
2002), image text compression (Bingham & Mannila, 2001), event detection (Jurgens & Stevens, 2009). Fradkin Madigan (2003) showed that, since RI distances
approximately preserved, distance-based learners k-Nearest Neighbours (k-NN)
SVMs preferable learning randomly indexed instances. Accordingly,
Sahlgren Coster (2004) applied RI (monolingual) text classification using SVMs,
suggested random indexing representation (there dubbed Bag Concepts
BoCs Sahlgren & Coster, 2004) performed comparably BoW representation.
performance RI tested Sahlgren Karlgren (2005) Gorman
Curran (2006) realm automatic bilingual lexicon acquisition.
above-discussed works indicate RI promising dimensionality reduction technique representing polylingual data. proposal inspired works Achlioptas
(2001) Li et al. (2006) sparse projections taking level sparsity extreme, extends application RI TC (Sahlgren & Coster, 2004) PLTC, which,
best knowledge, never done far. following section first
describe method detail, propose particular setting aimed overcoming
certain obstacles could arise polylingual setting.

3. Lightweight Random Indexing Polylingual Text Classification
Text Classification (TC) formalized task approximating unknown target
function : C {1, +1}, indicates documents ought classified,
means function : C {1, +1}, called classifier, coincide
much possible terms given evaluation metric. denotes domain
documents, C = {c1 , c2 , ..., c|C| } set predefined classes, values +1 1
indicate membership non-membership document class, respectively.
consider multilabel classification, is, setting document
could belong zero, one, several classes time; consider flat
version problem, hierarchical relations among classes exist. adopt
1 vs. strategy, according multilabel classification problem solved
|C| independent binary classification problems.
document collection represented via matrix M|D||F |
~

d1
w11
w12 w1|F |
d~2 w21
w22 w2|F |



= . = .
..
..
..
.. ..
.
.
.
w|D|1 w|D|2 w|D||F |
d~|D|

(2)

|D| |F | number documents features collection, real
values wij represent weight feature fj document di , usually determined
function frequency feature document collection.
Polylingual Text Classification adds one fundamental aspect TC, i.e., different documents may belong different languages. Let : L return language
159

fiMoreo, Esuli, & Sebastiani

given document written, L = {l1 , l2 , . . . , l|L| } pool languages, |L| > 1.
S|L|
Let F = i=1 denote vocabulary collection, expressed
union language-specific vocabularies . polylingual setting assumes
distribution P ((d) = li ) across training set approximately uniform, is,
representative quantity labelled documents language.
usually small amount shared features across languages (e.g., proper
nouns)1 , implies hd~0 , d~00 0 (d0 ) 6= (d00 ), h, denotes dot
product. (Incidentally, means direct similarity comparison among documents
expressed different languages, e.g., using cosine-similarity, would doomed fail.)
thus possible, language li , perform reordering rows columns

M1 M2 0
matrix allows polylingual matrix expressed =
,
0 M3 4
[M1 ;
2 ]
|{d : (d) = li }| |Fi | monolingual matrix representation
M2
language li ,
|D| matrix containing words shared across two
M3
languages, 0 denotes all-zero matrices.
3.1 Random Indexing
Random Indexing maps observable problem feature random vector vector
space number dimensions determined number different unique
features want map, instead fixed advance. Originally, RI proposed
performing semantic comparisons terms. document thus mapped
random index vector accumulated (via vector addition) terms row
term-document matrix time term occurred document. case,
instead interested performing semantic comparisons documents, terms.
Thus, term assigned n-dimensional random index vector, accumulated
j-th row document-term matrix every time term found document dj .
Random index vectors nearly-orthogonal, comply conditions spelled
Achlioptas (2001) (see Section 2.3), i.e., zero-mean distribution unit variance,
satisfy Johnson-Lindenstrauss lemma. random index vector created randomly
setting k n non-zero values, equally distributed +1 1, n-dimensional
vector n typically order thousands. n fixed, recommended
choice k literature k = n/100. dub configuration RI1% , use
comparative experiments. vectors RI1% sparse, using sparse data structure
representations could bring memory savings. M|D|n = M|D||F | |F |n matrix
multiplication (see Section 2.3) completely skipped, building M|D|n on-the-fly
scanning document accumulating corresponding random index vectors
term read. avoids need allocate entire matrix M|D||F | memory.
According Sahlgren (2005), main advantages RI summarized follows:
method (i) incremental, provides intermediate results data read
1. Note formulations polylingual problem, e.g., ones Amini et al. (2009)
Prettenhofer Stein (2010), actually impose 6= j Fj = . means
shared words across languages, proper nouns, given multiple representations languagespecific features.

160

fiLightweight Random Indexing Polylingual Text Classification

in; (ii) avoids so-called huge matrix step (i.e., allocating entire M|D||F | matrix
memory), (iii) scalable, since adding new elements data increase
dimensionality space (e.g., new features represented via new random index,
via new dimension).
BoW matrices typically weighted normalized better represent importance
word document avoid giving long documents priori importance,
respectively. Weighting schemes could incorporated RI formalism
simple manner; e.g., time random index added document row, first
multiplied weight term document. brings improved
accuracy shown Gorman Curran (2006); however, work
shown incremental nature algorithm sacrificed non-linear weights
taken account. experiments, weighting criterion use well-known
tfidf method, expressed
tfidf (di , fj ) = tf (di , fj ) log

|D|
|d : tf (d, fj ) > 0|

(3)

tf (di , fj ) counts number occurrences feature fj document di ; weights
normalized via cosine normalization,
wij = qP

tfidf (di , fj )

fk F

(4)

tfidf (di , fk )2

3.2 Lightweight Random Indexing
preliminary experiments application RI method dimensionality
reduction, observed SVMs required time train training set
processed RI, original high-dimensional vector space (see Section
5.2). observed correlation training times choice k,
choice n smaller impact efficiency.
Optimizing choice k RI though means achieve two main goals:
(i) able encode large number different features reduced space, (ii)
increasing chance two random index vectors orthogonal.
respect (i), easy show that, want assign different n-dimensional
index vector
k non-zero values original feature, RI could encode maximum
C(n, k) = nk 2k features (representation capacity). C(n, k) grows rapidly function
either n k; example, C(5000, 50) 2.5 10135 . huge capacity clearly
exceeds representation requirements imposed current future dataset. However,
even small values k capacity becomes large enough encode reasonable
dataset, e.g., C(5000,2)=49,990,000 distinct features.
respect (ii), random-projection-based algorithms rely Hecht-Nielsen
(1994) lemma find nearly orthogonal directions reduced space. Two vectors ~u
~v inPan inner product space said orthogonal whenever h~u, ~v = 0,
h~u, ~v = ui vi dot product. Random indexes chosen sparse order
increase probability dot product equals zero, non-zero products evenly
distributed +1 1, leaving expected value outcome close zero.
161

fiMoreo, Esuli, & Sebastiani

Figure 1: Probability orthogonality two random index vectors function k
n.

means Monte Carlo algorithm, estimated probability orthogonality
two randomly generated vectors grid sample values n k. results,
plotted Figure 1, reveal smaller values k main factor favouring
orthogonality two random index vectors, n smaller impact.
many random index vectors lack orthogonality, information conveyed original distinct features, predominantly pair-wise semantically unrelated, gets mixed
up, causing learner difficulty learning meaningful separation patterns
them. orthogonality random index vectors plays even important role
features shared across languages. shown work Gliozzo Strapparava (2005), shared words play relevant role bringing useful information across
languages. corresponding random index vectors orthogonal respect
vectors, information contribute process maximized, instead
diluted less informative features.
Following observations above, propose use Random Indexing fixed
k = 2; dub configuration Lightweight Random Indexing (LRI). hypothesis
setting could advantageous mechanism reduce dimensionality (so
mitigate problem feature disjointness PLTC), since sufficient order
represent large feature vocabularies preserving vector orthogonality. Note
choosing k = 1, n = |F |, would equivalent performing random permutation
162

fiLightweight Random Indexing Polylingual Text Classification

1

2

3

4

5

6

7
8

Output: Dictionary;
// Generate random index vector feature
= 0 (|F | 1)
// choose 1st dimension sequentially
dim1 (i mod n) + 1 ;
// choose 2nd dimension uniformly random
// dimensions chosen Line 2
dim2 rand({1, ..., n)} \ {dim1 }) ;
// assign 1st non-zero value uniformly random
+1
val1 rand({
, 12 }) ;
2
// 2nd non-zero value
+1
val2 rand({
, 12 }) ;
2
// create sparse random index vector
random index vector [(dim1 , val1 ), (dim2 , val2 )] ;
// build feature-vector mapping
Dictionary.map(fi+1 , random index vector) ;
end
Algorithm 1: Feature Dictionary Lightweight Random Indexing.

feature indexes BoW representation; k = 2 minimum value actual
RI performed.
Algorithm 1 formalizes process creating dictionary, is, creating mapping
consisting one random vector original feature; mapping created training
time used classifying unlabelled documents(this means that, Line 1, F
set features present training set). value 1/ 2 used instead 1 order
obtain vectors length one. Note two dimensions selected different
manner, step Line 2 ensuring latent dimensions used approximately
number times, step Line 3 ensuring dimension chosen
previous step chosen twice.
proposal presents following advantages respect standard RI1% and,
general, respect RI k > 2:
index vector two non-zero values. mapping allocated
memory number original features, projection performed
quickly;
Given fixed value n, higher probability instantiation
RI generating truly pairwise orthogonal random vectors;
Parameter k becomes constant needs tuning.

163

fiMoreo, Esuli, & Sebastiani

4. Experiments
section experimentally compare Lightweight Random Indexing (LRI) method
representation approaches proposed literature.
4.1 Baselines Implementation Details
baselines compare LRI chosen following methods,
group three categories according common characteristics:
Orthogonal Mappings: methods using canonical basis co-occurrence matrix:
PolyBow: classifier operates juxtaposed BoW representation (PolyBow
corresponds NP-1C setup Garca Adeva et al., 2005).
FS: Feature Selection PolyBoW using Information Gain term scoring function Round Robin (Forman, 2004) term selection policy.
Majority Voting: multiview voting algorithm returns label output
highest number language-specific classifiers (Amini et al., 2009).
MonoBoW: lower bound baseline uses set nave monolingual classifiers
(MonoBoW corresponds NP-NC setup Garca Adeva et al., 2005).
MT: upper bound baseline based statistical machine translation, translates non-English training test documents English.
Random Mappings: dimensionality reduction methods relying random projections:
RI1% : Random Indexing k = n/100 (Sahlgren & Coster, 2004).
ACH: Achlioptas mapping ternary distribution obtained setting = 3
Equation 1 (Achlioptas, 2001).
Non-Random Mappings: dimensionality reduction methods relying mappings
random:
CL-LSA: Cross-Lingual Latent Semantic Analysis (Dumais et al., 1997).
MDM: Multilingual Domain Models (Gliozzo & Strapparava, 2005).
PLDA: Polylingual Latent Dirichlet Allocation (Mimno et al., 2009).
assume language labels available advance2 training testing
documents. Note RI methods PolyBoW represent documents
feature space, irrespective language label. Conversely, MonoBoW keeps separate
language-specific classifier language; class label test document
decided classifier associated documents language label. test PLDA
Majority Voting JRC-Acquis parallel corpus, since documents
require separate view languages available. Majority Voting maintains
separate classifier distinct language (5 experiments); test document
thus classified using 5 classification decisions voting, one language-specific
2. assumption fair, current language identification models deliver accuracies close 100%

164

fiLightweight Random Indexing Polylingual Text Classification

view. singular value decomposition used Rohde (2011) package.
used Haddow, Hoang, Bertoldi, Bojar, Heafield (2016) implementation generate
set statistical translation systems trained sentence-aligned parallel data provided
Europarl data release (Koehn, 2005). Note that, since used method described
Gliozzo Strapparava (2005) automatically obtain bilingual model MDM,
MT method using external knowledge. PLDA used Richardson
(2008) implementation, uses Gibbs sampling; adhere common practice
fixing budget iterations 1,000. implemented LRI method
baseline methods part Esuli, Fagni, Moreo (2016) framework.
used Support Vector Machines (SVMs) learning device cases, since
consistently delivered state-of-the-art results TC far; used well-known
Joachims (2009) implementation Joachims (2005), default parameters.
4.2 Evaluation Measures
effectiveness measure use well-known F1 , harmonic mean precision
() recall () defined F1 = (2)/( + ) = (T P )/(2T P + F P + F N ) P ,
F P , F N stand numbers true positives, false positives, false negatives,
respectively. take F1 = 1 P = F P = F N = 0, since classifier correctly
classified examples negative.
compute micro-averaged F1 (denoted F1 ) macro-averaged F1 (denoted
F1M ). F1 obtained (i) computing class-specific values Pr , F Pr , F Nr , (ii)
obtaining P summation Pr (same F P F N ), applying
F1 formula. F1M obtained first computing class-specific F1 values
averaging across classes. fact F1M attributes equal importance
classes means low-frequency classes important high-frequency ones
determining F1M scores; F1 instead influenced high-frequency classes
low-frequency ones. High values F1M thus tend indicate classifier performs well
low-prevalence classes, high values F1 may indicate classifier
performs well high-prevalence classes.
4.3 Datasets
performed experiments two publicly available corpora, RCV1/RCV2 (a
comparable corpus) JRC-Acquis (a parallel corpus).
4.3.1 RCV1/RCV2
RCV1 publicly available collection consisting 804,414 English news stories generated Reuters 20 Aug 1996 19 Aug 1997 (Lewis, Yang, Rose, & Li, 2004). RCV2
instead polylingual collection, containing 487,000 news stories generated
timeframe thirteen languages English (Dutch, French, German, Chinese,
Japanese, Russian, Portuguese, Spanish, LatinoAmerican Spanish, Italian, Danish, Norwegian, Swedish). union RCV1 RCV2 (hereafter referred RCV1/RCV2)
corpus comparable topic-level, news stories direct translations
simply refer related events different languages. Since cor165

fiMoreo, Esuli, & Sebastiani

pus parallel, training document given language general
counterpart languages.
RCV1/RCV2 randomly selected 8,000 news stories 5 languages (English,
Italian, Spanish, French, German) pertaining last 4 months (from 1997-04-19
1997-08-19), performed 70%/30% train/test split, thus obtaining training set
28,000 documents (5,600 language) test set 12,000 documents (2,400
language)3 . experiments restricted attention 67 classes
(out 103) least one positive training example five languages.
average number classes per document 2.92, ranging minimum 1
maximum 11; number positive examples per class/language combination ranges
minimum 1 maximum 4,182.
preprocessed corpus removing stop words stemming terms using
Porter stemmer English, Snowball stemmer languages.
resulted total 123,258 stemmed terms, distributed across languages shown Table
1.

English
Italian
Spanish
French
German

English
40,483

Italian
3,420
14,762

Spanish
6,559
3,752
30,077

French
6,370
3,300
6,139
26,961

German
3,921
1,929
3,014
3,441
38,232

Appearing
1 languages
2 languages
3 languages
4 languages
5 languages

#
106,182
10,474
3,851
1,923
828

Table 1: Feature distribution across languages RCV1/RCV2 comparable corpus.
leftmost part table, cell row column j represents
number features shared across i-specific j-specific sections
dataset. (The table symmetric, better clarity entries
diagonal omitted.) rightmost part table indicates many
features shared across x language-specific sections dataset.

4.3.2 JRC-Acquis
JRC-Acquis corpus (version 3.0) version Acquis Communautaire collection
parallel legislative texts European Union law written 1950s 2006
(Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, & Varga, 2006). JRC-Acquis
publicly available research purposes, covers 22 official European languages.
corpus parallel sentence-level, i.e., document exists 22 languages,
sentence-by-sentence translation. corpus labelled according ontology-based
EuroVoc thesaurus, consists 6,000 classes; experiments
restricted attention 21 classes top level EuroVoc hierarchy.
3. information required replicate experiments, e.g., IDs selected documents, assigned
labels, etc., publicly available (Moreo, 2016). source code used experiments accessible
part Esuli et al. (2016) framework

166

fiLightweight Random Indexing Polylingual Text Classification

English
Italian
Spanish
French
German

English
150,866

Italian
77,878
150,838

Spanish
80,220
95,515
143,712

French
89,573
90,522
88,561
147,077

German
98,740
78,919
85,434
86,905
228,834

Appearing
1 languages
2 languages
3 languages
4 languages
5 languages

#
249,216
42,566
33,305
22,171
59,676

Table 2: Feature distribution across languages JRC-Acquis parallel corpus;
meaning cells Table 1. Note high number features (59,676) appear five languages; due presence
proper names, languages. Note high number
features (228,834) unique German language: due
presence word compounds, phenomenon present German language
four languages.

selected 7,235 texts 2006 5 languages (English, Italian, Spanish,
French, German) removed documents without labels, thus obtaining 6,980 documents per language. taken first 70% documents training (24,430, i.e.,
4,886 language) remaining 30% (10,470, i.e., 2,094 language)
testing. average number classes per document 3.5, ranging minimum
1 maximum 10; number positive examples per class/language combination
ranges minimum 47 maximum 2,011.
preprocessing RCV1/RCV2 carried dataset, obtaining 406,934 distinct features distributed across languages shown Table 2. Since
JRC-Acquis corpus parallel, language-specific document guaranteed
counterpart languages, results relatively large number
terms (e.g., proper nouns) appearing several languages. Note that, despite fact
dataset parallel sentence level, interested indexing entire documents
whole, thus disregard sentence order; thus consider corpus parallel
document level.
use JRC-Acquis corpus order test performance LRI cases
co-occurrence matrix compacted, defined work Dumais et al.
(1997). precisely, compact representation |L| translation-equivalent documents
vector consisting concatenation |L| vectors represent one (monolingual) document. different juxtaposed representation used
previous chapters, vector corresponding one monolingual document
zeros positions corresponding features languages. compact
matrix thus obtained matrix resulting juxtaposed representations
compressing |L| rows single (compact) row storing sum.
167

fiMoreo, Esuli, & Sebastiani

4.4 Results
section present results experiments. first compare LRI set
monolingual classifiers (Section 4.4.1), explore dimensionality reduction
aspect polylingual problem (Section 4.4.2).
4.4.1 Polylingual Information
first case study, investigate much addition polylingual information
affects accuracy monolingual classifier. scenario, compare LRI
PolyBoW, train documents languages, lower bound MonoBoW,
trains documents language test documents, upper
bound MT, first translates training test documents English. Note
MT baseline tested JRC-Acquis corpus documents
already available direct translation languages. experiment vector space
reduced, i.e., set n = |F | LRI vector spaces PolyBoW
LRI number dimensions. Values LRI averaged 10 runs.
results illustrated Figure 2 show simple addition examples different languages (PolyBoW) brings improvement accuracy respect
monolingual solution (MonoBoW). improvement likely achieved thanks words
shared across languages. However, LRI clearly outperforms PolyBoW. improvements
PolyBoW MonoBoW range -0.4% +29.7%, LRI achieves improvements ranging +9.7% +41.1%; LRI obtains smallest improvement
MonoBoW terms F1M (on Italian, +9.7%), PolyBoW performs slightly worse
MonoBoW (-0.4%). improvements marked F1M F1 , indicating
improvements especially take place infrequent classes,
substantial impact F1M F1 .
general, training documents coming languages (PolyBoW, LRI, MT)
seems preferable training language-specific documents (MonoBoW).
particularly MT baseline, obtained best results cases
sole exception English, LRI obtained best result. exception might
explained fact automatically translated documents tend exhibit different
statistical properties respect documents written humans, means
English test documents (which translations) might tune training
documents (which mostly result automatic translation).
language-specific classification performance much homogeneous JRCAcquis RCV1/RCV2. explained fact JRC-Acquis parallel
corpus, therefore language benefits information.
significant difference performance among different languages, means
effects due different difficulty various languages minor. Instead, differences
RCV1/RCV2 explained different amount information training
sets carry corresponding test sets. example, Spanish classifier worst
performer, one obtains best benefit (with respect MonoBoW
baseline) addition polylingual information (as PolyBoW, LRI, MT).
168

fiLightweight Random Indexing Polylingual Text Classification

Figure 2: Monolingual classification RCV1/RCV2 (top) JRC-Acquis (bottom), using F1M (left) F1 (right) evaluation measure.

Note experiment matrices PolyBoW LRI feed learning
algorithm size. difference two methods, likely
cause difference effectiveness, PolyBoW useful dimensions
specific language packed specific portion vector space, LRI spreads
across entire vector space, causing dimensions become potentially useful
languages. Note substantial increase number useful dimensions
available language allows model create easily separable representations.
discuss aspect Section 5.3.
4.4.2 Dimensionality Reduction
PolyBoW setup dimensionality vector space substantially increased
languages considered training. following experiments explore
dimensionality reduction aspect problem, address realistic polylingual
scenario, training test data contain examples language.
169

fiMoreo, Esuli, & Sebastiani

n
MonoBoW
PolyBoW
MT
CL-LSA
MDM
ACH
RI1%
LRI

500
0.273
0.365
0.472
0.366
0.426
0.375

1,000
0.353
0.399
0.493
0.389
0.483
0.464

F1M
5,000
0.444
0.513
0.539
0.547

10,000
0.472
0.530
0.543
0.554

full
0.473
0.498
0.509
0.570

500
0.668
0.765
0.769
0.621
0.683
0.679

1,000
0.736
0.777
0.771
0.610
0.705
0.736

F1
5,000
0.786
0.736
0.756
0.792

10,000
0.795
0.755
0.775
0.802

full
0.802
0.804
0.808
0.811

Figure 3: Effects dimensionality reduction RCV1/RCV2 (English Italian). Dotted
lines indicate reference values, e.g., green red lines represent performance
LRI PolyBoW, respectively, dimensionality reduced. Values
bold highlights best performing method dimension.

first run sample bilingual experiment RCV1/RCV2 (as language
English picked Italian). total amount features dataset
51,828. Restricting experiment two languages allows us compare LRI (i)
methods proposed bilingual representations (MDM), (ii) methods
would computationally expensive considering languages (such ACH,
see below). explore effect dimensionality reduction, number selected
features ranging 500 10,000 (Figure 3). adhere common practice
establishes number dimensions ranging 500 1000 LSA MDM. Results
random projection methods (ACH, RI1% , LRI) averaged 10 runs.
LRI obtains good results macro- micro-averaged F1 , methods exhibit alternating performance two measures. RI1% obtains comparable results
terms F1M performs poorly F1 ; contrast, PolyBoW performs comparably
terms F1 worse terms F1M . two-tailed t-test paired examples reveals
difference terms F1M LRI RI1% statistically significant,
170

fiLightweight Random Indexing Polylingual Text Classification

n
MonoBoW
PolyBoW
MT
CL-LSA
RI1%
LRI

500
0.254
0.351
0.300
0.270

1,000
0.308
0.384
0.402
0.376

F1M
5,000
0.420
0.482
0.491

10,000
0.445
0.501
0.511

full
0.415
0.483
0.521
0.528

500
0.606
0.728
0.580
0.573

1,000
0.657
0.746
0.649
0.659

F1
5,000
0.747
0.696
0.749

10,000
0.764
0.733
0.766

full
0.753
0.781
0.793
0.786

Figure 4: Accuracy different PLTC methods RCV1/RCV2 5 languages, different
levels dimensionality reduction.

LRI significantly outperforms RI1% F1 rest dimensionality reduction
methods evaluation measures, p < 0.001. Surprisingly, CL-LSA MDM
perform worse nave classifier (MonoBoW) features. However,
remarked outperform baselines 500 1000 dimensions.
seen Section 5, apart drastic dimensionality reduction, methods affected large computational costs negatively impact run times
memory resources needed. Consistently previous observations (see Figure 2), LRI,
PolyBoW, MonoBoW, MT comparable terms F1 , LRI outperforms
tested algorithms terms F1M .
test scalability method several languages involved, extend
experiment five languages (English, Italian, Spanish, French, German) RCV1/RCV2
(Figure 4). Note case algorithms able complete execution
due memory constraints, hence incomplete plots table; concretely, ACH
last iterations RI1% overflowed memory resources trying allocate 28, 000
123, 258 matrix. insights space time complexity reported Section 5.
Results RI1% LRI average 10 runs use different random seeds.
results confirm previous observations. RI1% behaves similarly LRI terms
F1M (i.e., statistically significant difference) worse terms F1 (p <0.001),
171

fiMoreo, Esuli, & Sebastiani

n
PolyBoW
CL-LSA
PLDA
Majority Vote
RI1%
LRI

500
0.365
0.570
0.456
0.543
0.524

1,000
0.416
0.593
0.463
0.581
0.581

F1M
5,000
0.534
0.655
0.659

10,000
0.570
0.680
0.672

full
0.640
0.656
0.688

500
0.560
0.725
0.644
0.656
0.660

1,000
0.606
0.739
0.650
0.676
0.702

F1
5,000
0.697
0.743
0.764

10,000
0.723
0.770
0.776

full
0.768
0.759
0.789

Figure 5: Accuracy different PLTC methods JRC-Acquis 5 languages, different
levels dimensionality reduction.

PolyBoW behaves opposite way, i.e., performs worse LRI terms
F1M (p < 0.001) comparably terms F1 . dimensionality reduction method,
LRI thus outperforms methods considering F1M F1 ;
dimensionality reduction applied, upper bound MT comparable LRI
F1M F1 .
Finally, used JRC-Acquis reproduce one last polylingual scenario, namely, one
texts aligned document level. Even situation common
practice (exceptions include, say, proceedings official events), scenario interesting
since dataset may serve test bed multiview learning methods (Amini et al.,
2009). Since documents JRC-Acquis translated humans, results affected
noise MT tools might introduce. Figure 5 shows results obtained considering
compacted matrix JRC-Acquis (a 4, 886 406, 934 matrix), tested Majority Voting, combines classification decisions five independently trained
MonoBoW classifiers parallel versions documents, PLDA, first defines
generative model based polylingual topics trains tests probability distributions topics assigned document. set number polylingual
latent topics 500 1000, respectively.
LRI clearly superior PolyBoW case. difference performance
LRI RI1% seems lower case, especially terms F1M ; t-test revealed
172

fiLightweight Random Indexing Polylingual Text Classification

however LRI superior RI1% statistically significant sense (p <0.001). However,
considered LRI delivers best performance without reducing dimensionality polylingual matrix, RI1% able accomplish projection due
memory restrictions; something expand following section. PLDA,
turn, succeeded discovering polylingual topics aligned across languages,
proved less effective terms classification performance.

5. Analysis
experiments observed substantial differences terms efficiency among
compared methods, particularly ACH, RI1% , LRI. example, RI1%
exhausted memory resources n 10, 000, LRI able represent even fullsized |D||F | matrix (see Figure 4). Given strong relationship two methods,
would expected delivered similar performance. anomaly prompted us
investigate issue depth. section presents analytical study terms
efficiency methods discussed previous section.
5.1 Space Efficiency
Data samples ML usually represented co-occurrence matrix. TC matrix
suffers high-dimensionality, luckily enough sparse. sparse, low-density
matrix suggests use non-exhaustive data-structure, zero values
stored explicitly.
random projection direct impact sparsity. feature contained
document, k non-zero values placed projected matrix. ACH situation
worse, since feature mapped, average, n/3 non-zero values. example,
n = 5, 000 feature mapped 50 1,666 non-zero values RI1%
ACH, respectively.
example, rerun RCV1/RCV2 experiments English Italian
languages, examined matrix density (percentage non-zero values
total matrix size) memory footprint (absolute number non-zero values).
results displayed Figure 6.
LRI requires double space respect standard BoW, succeeds preserving
sparsity, RI1% drastically increases matrix density produces large memory
footprint. MDM, LSA, ACH operate dense matrices. However, since MDM
LSA produce extreme dimensionality reduction, overall memory footprint remains
much lower RI1% and, especially, ACH. n = |F |, LRI must allocate
1, 844103 values (this indicated LRI (full) Figure 6), RI1% (n = 5000)
must allocate 28, 463 103 values (requiring 15.42 times space); ACH (n = 5000)
must allocate 55, 998 103 values (30.35 times space). Note even though MDM
LSA reduce significantly dimensionality (e.g., 51,828 500, 1,000),
need allocate values memory LRI (full).
example, let us suppose non-zero value represented double
(typically: 8 bytes modern programming languages); means roughly need
428MB ACH 218MB RI1% , whereas LRI requires 15MB. Although
difference substantial, (even taking account actual memory needed higher
173

fiMoreo, Esuli, & Sebastiani

Figure 6: Matrix density (left) memory footprint (right) RCV1/RCV2 EnglishItalian run (11, 200 51, 828 full training matrix size).

values indexed hash table) still represent real problem
terms space modern computers. However, note matrix
data structure need allocate memory. mapping dictionary, i.e., data
structure linking original feature random index vector, allocated
memory. dictionary queried many times terms document
want classify. dictionary small enough (which LRI), may able
allocate cache order significantly speed indexing new documents.
Assuming sparse representation, random index vector described list k
pairs (di , vi ), di indicates latent dimension vi encodes value. example,
k = 2 random vector (0, 0, +1, 0, 1, 0, ...) could represented [(3, 1), (5, 0)],
bit set 1 encodes +1 bit set 0 encodes 1. Equation 5,
space occupation dictionary random indexing method depends (i) |F |,
number indexes; (ii) k, number non-zero values index; (iii)
number bits needed indicate one latent position encode possible non-trivial
values; is,
Cost(RIk ) = O(|F | k (log2 n + log2 2))

(5)

turns that, given expected number non-zero values ACH n/3, using
dense representation index cheaper. position thus indicates one
three possible values index. cost terms space ACH index dictionary
described
Cost(ACH) = O(|F | n log2 3)
174

(6)

fiLightweight Random Indexing Polylingual Text Classification

Method
LRI
RI1%
ACH

Index type
sparse
sparse
dense

Index size
2
100
10,000

Index cell
log2 n + log2 2 bits encode dimi vali , resp.
log2 n + log2 2 bits encode dimi vali , resp.
log2 3 bits encode ij

Memory required
1.39MB
69.31MB
768.87MB

Table 3: Memory occupation feature dictionary different random mapping methods JRC-Acquis dataset (|F | = 406, 934). meanings dimi vali
Algorithm 1. meaning ij Equation 1.

Assuming reduced dimensionality set fixed percentage original dimensionality, i.e., n = |F | 0 < 1, following hold:
Cost(RI) = O(|F |2 log2 |F |) >
Cost(ACH) = O(|F |2 ) >

(7)

Cost(LRI) = O(|F | log2 |F |)
However, hidden constants play key role practice. example, computed
total amount memory required method storing index dictionaries
n = 10, 000 JRC-Acquis, |F | = 406, 934; resulting values reported Table
3. observed, index dictionary ACH requires 769MB, space
required RI-based versions one three orders magnitude smaller.
words, index dictionary LRI could easily fit current cache memories, RI1%
ACH need resort higher-capacity, thus slower, storage devices.
5.2 Time Efficiency
usually case sparsity benefits space occupation, execution
time. example, computational cost SVD O(|F |2 |D|) document-by-term
matrix; however, implementation SVDLIBC specifically optimized sparse matrices
requires O(c|F ||D|) steps, c average number non-zero values vector.
Figure 7 plot run times experiments bilingual (English-Italian)
RCV1/RCV2 experiment paying attention time required (i) obtaining
transformed index training set, (ii) training learning algorithm (SVM), (iii)
obtaining transformed index test set, (iv) classifying test documents.
experiments run Intel i7 64bit processor 12 cores, running
1,600MHz, 24GBs RAM memory.
results show takes 3.5 minutes generate test classifier
uses BoW representation. Time slightly reduced 3 minutes
5000 features selected. total time LRI roughly higher factor 2,
7.3 (full) 6.6 (n = 5000) minutes, respectively. Notwithstanding this, figures
still low compared methods: training testing times grow
substantially RI ACH. Regarding latent methods, pointed
time required preparing matrices grow substantially, due large
175

fiMoreo, Esuli, & Sebastiani

Figure 7: Run times RCV1/RCV2 (English Italian setting).

computational cost inherent SVD matrix multiplication, case random
indexing methods times negligible.
comparing overall memory footprint (Figure 6, right) execution times (Figure 7) seems clear strong correlation them. investigated
dependency experiments computing Pearson correlation them.
Pearson correlation quantifies degree linear dependence two variables,
ranges 1, meaning perfect negative correlation, +1, meaning perfect positive
correlation, whereas 0 means linear dependency. found linear
Pearson correlation +0.988 +0.998 number non-zero values
matrix times required training testing, respectively, brings additional
support observation: preserving sparsity projection favours execution
times PLTC.
5.3 Effect k Random Indexing
Previous work RI (see, e.g., Sahlgren & Karlgren, 2005; Sahlgren & Coster, 2004) tend
set k 1% dimension vector; smaller values k (about k = 0.1%)
explored (Karlgren, Holst, & Sahlgren, 2008). works related random
projections (see, e.g., Achlioptas, 2001; Li et al., 2006) noticed sparse projection
matrices help speed computation.
Besides run times, sparsity projection matrix affects orthogonality
random projection, turn impact preservation relative
distances. Two random vectors ri rj said orthogonal angle
90 degrees. Although probability two randomly picked vectors
orthogonal increases dimensionality vector space grows (Karlgren et al., 2008),
random projection approaches choose sparse random vectors, maximize
probability.
176

fiLightweight Random Indexing Polylingual Text Classification

Figure 8: Probability distribution angle two arbitrary vectors highdimensional space (left), excess kurtosis function non-zero values
projection matrix 10,000 dimensions (right).

could thus establish parallelism degree orthogonality projection matrix probability distribution angle two random vectors.
probability distribution skewed towards 90 degrees, closer orthogonal
projection base is, better distances preserved. propose quantify
orthogonality means excess kurtosis distribution angle4 . aim,
studied kurtosis angle distributions (as estimated via Monte Carlo
algorithm) varies function matrix sparsity k 10,000-dimension projection
matrix (Figure 8, right).
Figure 8 shows orthogonality projection, fixed dimensionality,
rapidly degrades density increases. LRI thus expected produce nearly
orthogonal indexing, followed RI ACH.
investigated relation orthogonality PLTC accuracy.
aim, run series experiments bilingual version RCV1/RCV2,
varying (from 2 100) number k non-zero values (from 1,000 10,000)
reduced dimensionality n. Figure 9 shows contour lines (equally valued points
3-dimensional representation) classification performance (here measured terms
F1 ), execution time, probability pairwise orthogonality (i.e., probability
hri , rj = 0 two randomly chosen random index vectors).
following trends directly observed results: (i) accuracy improves
n increases k decreases; (ii) run times tend grow n k increase,
(iii) higher dimensionality n smaller parameter k, higher
probability finding two orthogonal random indexes.
4. excess kurtosis random variable X typically defined fourth standardized moment minus
3, i.e., EKurt[X] = 44 3.

177

fiMoreo, Esuli, & Sebastiani

Figure 9: Impact dimensionality n (on x axis) number k non-zero values (on
axis) classification accuracy (left), execution time (center), probability
finding orthogonal pair random indexes (right). Darker regions represent
lower values.

Figure 9, behaviour LRI method propose described green
horizontal line bottom plot, RIs behaviour described blue
diagonal line coordinates (n = 1, 000, k = 10) (n = 10, 000, k = 100). performance RI improves cost space time efficiency, gradually disrupting
orthogonality base. contrary, following desirable features LRI
evident: dimensionality increases (i) accuracy improves without penalizing execution
times, due preservation sparsity, (ii) orthogonality base improved.

6. Conclusions
compared several techniques polylingual text classification, checking suitability dimensionality reduction techniques techniques generation alternative representations co-occurrence matrix, two PLTC benchmarks (one parallel
one comparable). investigation indicates reducing dimensionality
data sufficient reasonable efficiency (in terms time space) required.
Based observation proposed variant Random Indexing, method originated within IR community that, best knowledge, never tested
PLTC date. proposal, Lightweight Random Indexing, yielded best results
terms (both time space) efficiency, terms classification accuracy,
Lightweight Random Indexing obtained best results terms macroand micro-averaged F1 . Lightweight Random Indexing preserves matrix sparsity,
means memory footprint training time penalized. example,
Figures 6 7 may see Lightweight Random Indexing (in full configuration
is, random vectors dimensionality original space)
improved Latent Semantic Analysis (in n = 1, 000 configuration is,
178

fiLightweight Random Indexing Polylingual Text Classification

dimensionality reduced space 1,000) margin +4.37% terms F1
89.69% reduction execution time 82.60% reduction memory footprint.
Even though Lightweight Random Indexing works well dimensionality reduction method, achieves best performance projection reduce
original dimensionality. Apparently, BoW representation might expected
preferable case, truly orthogonal. However, polylingual BoW
representation features informative restricted set data; e.g.,
German term entire dimension reserved vector space model,
dimension useful documents written German. Random projections instead
map feature space space shared among languages once. effect
dimension space becomes informative represent documents regardless
language originally written in. configuration, projection
space larger actual number different features single language, reminiscent kernel-trick effect, informative space language enlarged
thus becomes easily separable.
light experiments, Lightweight Random Indexing important advantages
respect previous PLTC approaches. First, method machine translationfree, dictionary-free, require sort additional resources apart
labelled collection. projected matrix preserves sparsity, direct effect
reducing running time total memory usage. respect original random
indexing technique, Lightweight Random Indexing presents following advantages: (i)
probability finding pair truly orthogonal indexes higher; (ii) requires less memory
allocate index dictionary; (iii) avoids need tuning k parameter.
LRI proven effective PLTC, conjecture could bring similar
benefits related tasks, CLTC, cross-lingual information retrieval, well
tackling problems dealing sparse heterogeneous sources data general.
discussed above, one reasons k = 2 safe configuration still preserves
representation capacity. However, might hold circumstances; e.g.,
processing huge streams dynamic data (e.g., streams tweets), certain
point representation capacity might saturate dimensionality space
chosen carefully. cases, opting configurations k > 2 might mitigate
problem.
Another fact emerges experiments dimensionality reduction
necessarily synonym computational efficiency. reason modern secondary
storage data structures optimized operate sparse data, dimensionality drastically reduced, matrix density may increase, net effect may decrease
efficiency. true benefit thus achieved extent trade-off sparsity
separability preserved; dimension, LRI proved extremely effective.
Although results encouraging, investigations still needed shed
light foundations random projection methods. first question whether
criterion better choose random index vectors; given current criterion
random, seems might room better motivated strategies, possibly leveraging
class labels taking account document language labels. Considering
Random Indexing originally proposed context IR community, wonder
whether proposed approach could produce similar improvements IR tasks
179

fiMoreo, Esuli, & Sebastiani

query expansion bilingual lexicon acquisition. Finally, could interesting combine
Lightweight Random Indexing Reflexive Random Indexing (Cohen, Schvaneveldt, &
Widdows, 2010; Rangan, 2011), recent formulation model iteratively
alternates row indexing column indexing original co-occurrence matrix.

Acknowledgements
Fabrizio Sebastiani leave Consiglio Nazionale delle Ricerche, Italy.

References
Achlioptas, D. (2001). Database-friendly random projections. Proceedings 20th
ACM Symposium Principles Database Systems (PODS 2001), pp. 274281,
Santa Barbara, US.
Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representations
multilingual NLP. Proceedings 17th Conference Computational Natural
Language Learning (CoNLL 2013), pp. 183192, Sofia, BL.
Amini, M.-R., & Goutte, C. (2010). co-classification approach learning multilingual corpora. Machine Learning, 79 (1/2), 105121.
Amini, M.-R., Usunier, N., & Goutte, C. (2009). Learning multiple partially observed
views; application multilingual text categorization. Proceedings 23rd
Annual Conference Neural Information Processing Systems (NIPS 2009), pp. 28
36, Vancouver, CA.
Baroni, M., Dinu, G., & Kruszewski, G. (2014). Dont count, predict! systematic comparison context-counting vs. context-predicting semantic vectors. Proceedings
52nd Annual Meeting Association Computational Linguistics (ACL
2014), pp. 238247, Baltomore, US.
Bel, N., Koster, C. H., & Villegas, M. (2003). Cross-lingual text categorization. Proceedings 7th European Conference Research Advanced Technology
Digital Libraries (ECDL 2003), pp. 126139, Trondheim, NO.
Bengio, Y. (2009). Learning deep architectures AI. Foundations Trends Machine
Learning, 2 (1), 1127.
Bengio, Y., Schwenk, H., Senecal, J.-S., Morin, F., & Gauvain, J.-L. (2006). Neural probabilistic language models. Innovations Machine Learning, pp. 137186. Springer,
Heidelberg, DE.
Bingham, E., & Mannila, H. (2001). Random projection dimensionality reduction: applications image text data. Proceedings 7th ACM International
Conference Knowledge Discovery Data Mining (KDD 2001), pp. 245250, San
Francisco, US.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
180

fiLightweight Random Indexing Polylingual Text Classification

Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations word
co-occurrence statistics: computational study. Behavior Research Methods, 39 (3),
510526.
Chandar, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B., Raykar, V. C., & Saha,
A. (2014). autoencoder approach learning bilingual word representations.
Proceedings 28th Annual Conference Neural Information Processing Systems
(NIPS 2014), pp. 18531861, Montreal, CA.
Cohen, T., Schvaneveldt, R., & Widdows, D. (2010). Reflective random indexing indirect inference: scalable method discovery implicit connections. Journal
Biomedical Informatics, 43 (2), 240256.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural language processing (almost) scratch. Journal Machine Learning
Research, 12, 24932537.
de Melo, G., & Siersdorfer, S. (2007). Multilingual text classification using ontologies.
Proceedings 29th European Conference Information Retrieval (ECIR 2007),
pp. 541548, Roma, IT.
Deerwester, S. C., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R. A.
(1990). Indexing latent semantic analysis. Journal American Society
Information Science, 41 (6), 391407.
Dumais, S. T., Letsche, T. A., Littman, M. L., & Landauer, T. K. (1997). Automatic crosslanguage retrieval using latent semantic indexing. Working Notes AAAI
Spring Symposium Cross-language Text Speech Retrieval, pp. 1824, Stanford,
US.
Ehrmann, M., Cecconi, F., Vannella, D., McCrae, J. P., Cimiano, P., & Navigli, R. (2014).
Representing multilingual data linked data: case BabelNet 2.0. Proceedings 9th Conference Language Resources Evaluation (LREC 2014), pp.
401408, Reykjavik, IS.
Esuli, A., Fagni, T., & Moreo, A. (2016). JaTeCS (Java Text Categorization System).
Github. Retrieved September 11, 2016, https://github.com/jatecs/jatecs.
Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. Proceedings 14th Conference European Chapter
Association Computational Linguistics (EACL 2014), pp. 462471, Gothenburg,
SE.
Forman, G. (2004). pitfall solution multi-class feature selection text classification. Proceedings 21st International Conference Machine Learning
(ICML 2004), pp. 3845, Banff, CA.
Fradkin, D., & Madigan, D. (2003). Experiments random projections machine
learning. Proceedings 9th ACM International Conference Knowledge
Discovery Data Mining (KDD 2003), pp. 517522, Washington, US.
Garca Adeva, J. J., Calvo, R. A., & Lopez de Ipina, D. (2005). Multilingual approaches
text categorisation. European Journal Informatics Professional, 6 (3), 4351.
181

fiMoreo, Esuli, & Sebastiani

Gliozzo, A., & Strapparava, C. (2005). Cross-language text categorization acquiring
multilingual domain models comparable corpora. Proceedings ACL
Workshop Building Using Parallel Texts, pp. 916, Ann Arbor, US.
Gliozzo, A., & Strapparava, C. (2006). Exploiting comparable corpora bilingual dictionaries cross-language text categorization. Proceedings 44th Annual
Meeting Association Computational Linguistics (ACL 2006), pp. 553560,
Sydney, AU.
Gorman, J., & Curran, J. R. (2006). Random indexing using statistical weight functions.
Proceedings 4th Conference Empirical Methods Natural Language Processing (EMNLP 2006), pp. 457464, Sydney, AU.
Gouws, S., & Sgaard, A. (2015). Simple task-specific bilingual word embeddings.
Proceedings North American Chapter Association Computational
Linguistics Human Language Technologies Conference (NAACL-HLT 2015), pp.
13861390.
Haddow, B., Hoang, H., Bertoldi, N., Bojar, O., & Heafield, K. (2016). MOSES statistical
machine translation system. Moses website. Retrieved September 11, 2016,
http://www.statmt.org/moses/.
Harris, Z. S. (1968). Mathematical structures language. Wiley, New York, US.
Hecht-Nielsen, R. (1994). Context vectors: General-purpose approximate meaning representations self-organized raw data. Computational Intelligence: Imitating Life,
pp. 4356. IEEE Press.
Hermann, K. M., & Blunsom, P. (2014). Multilingual models compositional distributed
semantics. Proceedings 52nd Annual Meeting Association Computational Linguistics (ACL 2014), pp. 5868, Baltimore, US.
Joachims, T. (2009). SVMperf: Support Vector Machine multivariate performance
measures. Cornell University website. Retrieved September 11, 2016,
http://www.cs.cornell.edu/people/tj/svm_light/svm_perf.html.
Joachims, T. (2005). support vector method multivariate performance measures.
Proceedings 22nd International Conference Machine Learning (ICML 2005),
pp. 377384, Bonn, DE.
Johnson, W. B., Lindenstrauss, J., & Schechtman, G. (1986). Extensions Lipschitz maps
Banach spaces. Israel Journal Mathematics, 54 (2), 129138.
Jurgens, D., & Stevens, K. (2009). Event detection blogs using temporal random indexing.
Proceedings Workshop Events Emerging Text Types, pp. 916, Borovets,
BG.
Kanerva, P., Kristofersson, J., & Holst, A. (2000). Random indexing text samples latent semantic analysis. Proceedings 22nd Annual Conference Cognitive
Science Society (CogSci 2000), p. 1036, Philadelphia, US.
Karlgren, J., Holst, A., & Sahlgren, M. (2008). Filaments meaning word space.
Proceedings 30th European Conference Information Retrieval (ECIR 2008),
pp. 531538, Glasgow, UK.
182

fiLightweight Random Indexing Polylingual Text Classification

Kaski, S. (1998). Dimensionality reduction random mapping: Fast similarity computation
clustering. Proceedings IEEE International Joint Conference Neural
Networks (IJCNN 1998), pp. 413418, Anchorage, US.
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations words. Proceedings 24th International Conference Computational Linguistics (COLING 2012), pp. 14591474, Mumbai, IN.
Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. MT
summit, Vol. 5, pp. 7986. Publicly available http://www.statmt.org/europarl/.
Lauly, S., Boulanger, A., & Larochelle, H. (2014). Learning Multilingual Word Representations using Bag-of-Words Autoencoder. ArXiv e-prints, arXiv:1401.1803 [cs.CL].
Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). Rcv1: new benchmark collection
text categorization research. Journal machine learning research, 5 (Apr), 361397.
Publicly available http://www.jmlr.org/papers/volume5/lewis04a/lyrl2004_
rcv1v2_README.htm.
Li, P., Hastie, T. J., & Church, K. W. (2006). sparse random projections. Proceedings
12th ACM SIGKDD International Conference Knowledge Discovery
Data Mining (KDD 2006), pp. 287296, Philadelphia, US.
Li, Y., & Shawe-Taylor, J. (2007). Advanced learning algorithms cross-language patent
retrieval classification. Information Processing Management, 43 (5), 1183
1199.
Mikolov, T., Le, Q. V., & Sutskever, I. (2013a). Exploiting Similarities among Languages
Machine Translation. ArXiv e-prints, arXiv:1309.4168 [cs.CL].
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013b). Distributed
representations words phrases compositionality. Proceedings
27th Annual Conference Neural Information Processing Systems (NIPS 2013), pp.
31113119, Lake Tahoe, US.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual topic models. Proceedings 2009 Conference Empirical Methods
Natural Language Processing (EMNLP 2009), pp. 880889, Singapore, SN.
Moreo, A. (2016). Data resources reproducing experiments polylingual text classification. Human Language Technologies (HLT) group website. Retrieved September
11, 2016, http://hlt.isti.cnr.it/pltc.
Nastase, V., & Strapparava, C. (2013). Bridging languages etymology: case
cross-language text categorization. Proceedings 51st Annual Meeting
Association Computational Linguistics (ACL 2013), pp. 651659, Sofia, BL.
Osterlund, A., Odling, D., & Sahlgren, M. (2015). Factorization latent variables distributional semantic models. Proceedings Conference Empirical Methods
Natural Language Processing (EMNLP 2015), pp. 227231, Lisbon, PT.
Pan, S. J., & Yang, Q. (2010). survey transfer learning. IEEE Transactions
Knowledge Data Engineering, 22 (10), 13451359.
183

fiMoreo, Esuli, & Sebastiani

Papadimitriou, C. H., Raghavan, P., Tamaki, H., & Vempala, S. (1998). Latent semantic
indexing: probabilistic analysis. Proceedings 17th ACM Symposium
Principles Database Systems (PODS 1998), pp. 159168, Seattle, US.
Pennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors word
representation. Proceedings Conference Empirical Methods Natural
Language Processing (EMNLP 2014), pp. 15321543, Doha, QA.
Prettenhofer, P., & Stein, B. (2010). Cross-language text classification using structural
correspondence learning. Proceedings 48th Annual Meeting Association
Computational Linguistics (ACL 2010), pp. 11181127, Uppsala, SE.
Rangan, V. (2011). Discovery related terms corpus using reflective random indexing. Proceedings ICAIL 2011 Workshop Setting Standards Searching
Electronically Stored Information, Pittsburgh, US.
Richardson, J. (2008). PolyLDA++. Atlassian Bitbucket. Retrieved September 11, 2016,
https://bitbucket.org/trickytoforget/polylda.
Rigutini, L., Maggini, M., & Liu, B. (2005). EM-based training algorithm crosslanguage text categorization. Proceedings 3rd IEEE/WIC/ACM International Conference Web Intelligence (WI 2005), pp. 529535, Compiegne, FR.
Rohde, D. (2011). C library computing singular value decompositions. SVDLIBC.
Retrieved September 11, 2016, http://tedlab.mit.edu/~dr/SVDLIBC/.
Sahlgren, M. (2001). Vector-based semantic analysis: Representing word meanings based
random labels. Proceedings ESSLLI Workshop Semantic Knowledge
Acquistion Categorization, Helsinki, FI.
Sahlgren, M. (2005). introduction random indexing. Proceedings Workshop
Methods Applications Semantic Indexing, Copenhagen, DK.
Sahlgren, M. (2006). Word-Space Model: Using distributional analysis represent syntagmatic paradigmatic relations words high-dimensional vector spaces.
Ph.D. thesis, Swedish Institute Computer Science, University Stockholm, Stockholm, SE.
Sahlgren, M., & Coster, R. (2004). Using bag-of-concepts improve performance
support vector machines text categorization. Proceedings 20th International Conference Computational Linguistics (COLING 2004), Geneva, CH.
Sahlgren, M., & Karlgren, J. (2005). Automatic bilingual lexicon acquisition using random
indexing parallel corpora. Natural Language Engineering, 11 (3), 327341.
Sahlgren, M., Karlgren, J., Coster, R., & Jarvinen, T. (2002). SICS CLEF 2002: Automatic query expansion using random indexing. Working Notes CrossLanguage Evaluation Forum Workshop (CLEF 2002), pp. 311320, Roma, IT.
Steinberger, R., Pouliquen, B., & Ignat, C. (2004). Exploiting multilingual nomenclatures
language-independent text features interlingua cross-lingual text analysis
applications. Proceedings 4th Slovenian Language Technology Conference,
Ljubljana, SL.
184

fiLightweight Random Indexing Polylingual Text Classification

Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., & Varga,
D. (2006). JRC-Acquis: multilingual aligned parallel corpus 20+ languages. Proceedings 5th International Conference Language Resources
Evaluation (LREC 2006), pp. 21422147, Genova, IT. Publicly available
https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis.
Vinokourov, A., Shawe-Taylor, J., & Cristianini, N. (2002). Inferring semantic representation text via cross-language correlation analysis. Proceedings 16th Annual
Conference Neural Information Processing Systems (NIPS 2002), pp. 14731480,
Vancouver, CA.
Vulic, I., & Moens, M.-F. (2015). Monolingual cross-lingual information retrieval models
based (bilingual) word embeddings. Proceedings 38th International ACM
SIGIR Conference Research Development Information Retrieval (SIGIR
2015), pp. 363372, Santiago, CL.
Wei, C.-P., Lin, Y.-T., & Yang, C. C. (2011). Cross-lingual text categorization: Conquering
language boundaries globalized environments. Information Processing Management, 47 (5), 786804.
Wei, C.-P., Yang, C.-S., Lee, C.-H., Shi, H., & Yang, C. C. (2014). Exploiting poly-lingual
documents improving text categorization effectiveness. Decision Support Systems,
57, 6476.
Xiao, M., & Guo, Y. (2013). novel two-step method cross-language representation
learning. Proceedings 27th Annual Conference Neural Information Processing Systems (NIPS 2013), pp. 12591267, Lake Tahoe, US.
Xu, C., Tao, D., & Xu, C. (2013). survey multi-view learning. ArXiv e-prints,
arXiv:1304.5634 [cs.LG].
Yang, Y., & Pedersen, J. O. (1997). comparative study feature selection text categorization. Proceedings 14th International Conference Machine Learning
(ICML 1997), pp. 412420, Nashville, US.
Zou, W. Y., Socher, R., Cer, D. M., & Manning, C. D. (2013). Bilingual word embeddings
phrase-based machine translation. Proceedings Conference Empirical
Methods Natural Language Processing (EMNLP 2013), pp. 13931398, Melbourne,
AU.

185


