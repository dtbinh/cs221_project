Journal Artificial Intelligence Research 57 (2016) 229271

Submitted 03/16; published 10/16

Goal Probability Analysis MDP Probabilistic Planning:
Exploring Enhancing State Art
Marcel Steinmetz
Jorg Hoffmann

STEINMETZ @ CS . UNI - SAARLAND . DE
HOFFMANN @ CS . UNI - SAARLAND . DE

Saarland University,
Saarland Informatics Campus,
Saarbrucken, Germany

Olivier Buffet

OLIVIER . BUFFET @ LORIA . FR

INRIA / Universite de Lorraine / CNRS,
Nancy, France

Abstract
Unavoidable dead-ends common many probabilistic planning problems, e.g. actions may fail operating resource constraints. important objective settings
MaxProb, determining maximal probability goal reached, policy
achieving probability. Yet algorithms MaxProb probabilistic planning severely underexplored, extent scant evidence empirical state art actually is.
close gap comprehensive empirical analysis. design explore large space
heuristic search algorithms, systematizing known algorithms contributing several new algorithm variants. consider MaxProb, well weaker objectives baptize AtLeastProb
(requiring achieve given goal probabilty threshold) ApproxProb (requiring compute
maximum goal probability given accuracy). explore general case
may 0-reward cycles, practically relevant special case acyclic planning,
planning limited action-cost budget. design suitable termination criteria, search algorithm variants, dead-end pruning methods using classical planning heuristics, node selection
strategies. design benchmark suite comprising 1000 instances adapted
IPPC, resource-constrained planning, simulated penetration testing. evaluation clarifies
state art, characterizes behavior wide range heuristic search algorithms,
demonstrates significant benefits new algorithm variants.

1. Introduction
Many probabilistic planning problems contain unavoidable dead-ends (e.g. Kolobov, Mausam,
Weld, & Geffner, 2011; Teichteil-Konigsbuch, Vidal, & Infantes, 2011; Kolobov, Mausam, & Weld,
2012; Teichteil-Konigsbuch, 2012), i.e., policy guarantees eventually, circumstances,
attain goal. Examples planning resource constraints limited budget, situations
actions may fail eventually run options. One important objective
MaxProb, determining maximal probability goal reached (and identifying
policy achieving probability). MaxProb partly underlies International Probabilistic Planning Competition (IPPC) (Younes, Littman, Weissman, & Asmuth, 2005; Bryce & Buffet,
2008; Coles, Coles, Garca Olaya, Jimenez, Linares Lopez, Sanner, & Yoon, 2012), planners evaluated often reach goal online policy execution. (The time limit
IPPC setting mixes MaxProb bias towards policies reaching goal quickly.
c
2016
AI Access Foundation. rights reserved.

fiS TEINMETZ & H OFFMANN & B UFFET

relates proposals Kolobov et al., 2012 Teichteil-Konigsbuch, 2012, asking
cheapest policy among maximizing goal probability, proposal Chatterjee,
Chmelik, Gupta, & Kanodia, 2015, 2016, asking cheapest policy ensuring target state
reached almost surely partially observable setting.)
consider MDP-based probabilistic planning, factored models (probabilistic extensions
STRIPS) whose state spaces may large build explicitly. focus optimal offline
setting, i.e., solving MaxProb exactly. setup objective certainly relevant,
little work towards developing solvers. main effort made Kolobov et al. (2011),
discuss detail below. Hou, Yeoh, Varakantham (2014) consider several variants
topological VI (Dai, Mausam, Weld, & Goldsmith, 2011), solving MaxProb necessitating
build entire reachable state space. works addressing goal probability maximization
aim guaranteeing optimality (e.g. Teichteil-Konigsbuch, Kuter, & Infantes, 2010; Camacho,
Muise, & McIlraith, 2016).
MDP heuristic search (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001; Bonet
& Geffner, 2003b; McMahan, Likhachev, & Gordon, 2005; Smith & Simmons, 2006; Bonet &
Geffner, 2006) potential find optimal policies without building entire state space,
Kolobov et al. (2011) authors addressing optimal MaxProb heuristic search.
Part reason lack research heuristic search MaxProb following two major obstacles. First, MDP heuristic search successful expected-cost minimization,
suffers lack admissible (upper-bounding) heuristic estimators goal probability.
best known possibility detect dead-ends set initial heuristic estimate 0, using
trivial upper bound 1 elsewhere. Second, MaxProb fit stochastic shortest path (SSP)
framework (Bertsekas, 1995), due 0-reward cycles. pointed Kolobov et al. (2011),
MaxProb equivalent non-discounted reward maximization problem, non-goal cycles
receive 0 reward thus improper policies accumulate reward .
address second problem, Kolobov et al. (2011) devised FRET (find, revise, eliminate traps) framework, admits heuristic search, yet requires several iterations complete
searches. heuristic search iterations, FRET eliminates 0-reward cycles (traps). FRET iterates cycles persist. Kolobov et al.s contribution mainly theoretical considering MaxProb much larger class generalized SSPs empirical evaluation
serves merely proof concept. experiment single domain (ExplodingBlocks),
run one configuration search (LRTDP, Bonet & Geffner, 2003b), one possibility
dead-end detection thus non-trivial initial heuristic estimates (SixthSense, Kolobov, Mausam,
& Weld, 2010). outperform value iteration (VI), dead-end detection used
VI, remains unclear extent improvement due actual heuristic search,
rather state pruning itself.
summary, heuristic search MaxProb challenging, addressed
Kolobov et al. (2011), limited experiments. Given this:
(i) actually empirical state art heuristic search MaxProb?
known algorithms, variants thereof, work better?
explore large design space algorithms, show that, indeed, variants work
much better.
(ii) simpler yet still relevant special cases, weaker objectives, may easier
solve?
230

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

indeed practically relevant cases necessitate FRET, weaker objectives
enable refer early termination.
elaborate first (ii): state space planning task hand acyclic, clearly FRET
needed: cycles particular 0-reward cycles state space
finite, execution end (goal non-goal) absorbing state; implies
within realm SSPs. special case is, however, still practically relevant. illustration,
acyclic state spaces occur even standard IPPC benchmarks, namely TriangleTireworld
domain moves made one direction. importantly, planning limited
action-cost budget, limited-budget planning, acyclic state spaces action costs non-0,
strictly decreasing remaining budget. similar class scenarios every action consumes non-0 amount non-replenishable resource. Another example recently proposed
models simulated penetration testing, per Hoffmann (2015). MDP models network
intrusion point view attacker. state space acyclic exploit
attempted (trying exploit network configuration would
yield outcome). States thus need remember remaining action set, every action
application strictly reduces set.
Regarding weaker objectives: alternatives MaxProb, reasonable ask whether
maximum goal probability exceeds given threshold , require computing maximum goal
probability given accuracy . refer objectives AtLeastProb ApproxProb
respectively.1 example, penetration testing, AtLeastProb naturally assesses level network security: attacker reach target host probability greater given security
margin? E.g., customer data server compromised probability greater 0.01?
AtLeastProb ApproxProb allow early termination based maintaining both, lower (pessimistic) bound V L upper (admissible/optimistic) bound V U . especially promising
AtLeastProb, terminate lower bound already good enough (V L ),
upper bound already proves infeasibility (V U < ). Good anytime behavior, either
bounds, translates early termination.
Let us elaborate (i), exploring state art beyond. design algorithm
space characterized by:
(a) Search algorithm. design variants AO (Nilsson, 1971), LRTDP (Bonet & Geffner,
2003b), depth-first oriented heuristic searches (Bonet & Geffner, 2003a, 2006), maintaining
upper lower bounds early termination.
(b) FRET. design new variant FRET better suited problems uninformative initial
upper bounds.
(c) Bisimulation reduction. design new probabilistic-state-space reduction method, via bisimulation relative all-outcomes determinization (e.g. Bonet & Geffner, 2003b; Yoon, Fern,
& Givan, 2007; Little & Thiebaux, 2007).
1. AtLeastProb relates MDP model-checking, one typically wants validate given PCTL (Probabilistic
Computation Tree Logic) formula valid probability (Baier, Groer, Leucker, Bollig, & Ciesinski, 2004;
Kwiatkowska, Parker, & Qu, 2011a; Kwiatkowska, Norman, & Parker, 2011b). relates Constrained MDPs
(Altman, 1999), enforcing minimum success probability could expressed constraint particular
quantity. Chance-Constrained POMDPs (Santana, Thibaux, & Williams, 2016) different AtLeastProb
constraint probability remain safe states, reach goal states.

231

fiS TEINMETZ & H OFFMANN & B UFFET

(d) Dead-end pruning method. employ classical-planning heuristic functions dead-end detection probabilistic planning, via all-outcomes determinization, previously done
Teichteil-Konigsbuch et al. (2011). especially promising limited-budget planning,
prune state admissible classical-planning estimate exceeds remaining
budget s.
(e) Node selection strategy. design comprehensive arsenal simple strategies, biasing tie
breaking action state selection manners targeted fostering early termination.
implemented techniques within Fast Downward (FD) (Helmert, 2006), thus contributing, side effect work, ideal implementation basis exploiting classical-planning
heuristic search techniques MDP heuristic search.2
algorithm dimensions (a) (e) orthogonal (excepting dependencies, particular
bisimulation reduction subsumes dead-end pruning). explore behavior resulting
design space large benchmark suite design purpose. suite includes domains
IPPC, resource-constrained planning, penetration testing, limitedbudget version unlimited-budget version. suite comprises 1089 benchmark instances
total.3 Amongst things, observe:
Heuristic search yields substantial benefits, even trivial admissible heuristic setting
initial estimate 1 everywhere (+9% total coverage across benchmarks),
admissible heuristics based dead-end detection (+12%).
Early termination yields substantial benefits (e.g. AtleastProb +8% = 0.2
+7% = 0.9).
FRET variant yields dramatic benefits (+32% total coverage cyclic benchmarks).
Bisimulation reduction yields optimal MaxProb solver excells TriangleTireworld,
even surpassing Prob-PRP (Muise, McIlraith, & Beck, 2012; Camacho et al., 2016)
standard version goal achieved certainty hence
Prob-PRP optimal, limited-budget version so.
side, discover landmarks compilation per Domshlak Mirkis (2015), employed
dead-end pruning oversubscription planning setting, actually, own, equivalent
pruning remaining budget standard admissible landmark heuristic.
relevant work because, otherwise, compilation would canonical candidate
dead-end pruning setting (indeed started investigation).
paper organized follows. Section 2 describes model syntax semantics, goal
probability analysis without action-cost budget limit. Section 3 specifies search
algorithm (a) FRET variants (b). Section 4 describes bisimulation reduction method (c).
Section 5 describes dead-end pruning methods (d), Section 6 describes node selection
strategies (e). present experiments Section 7, conclude Section 8.
two appendices giving additional technical details sketch main text, Appendix B
2. source code available http://fai.cs.uni-saarland.de/downloads/fd-prob.tar.bz2
3.
benchmark
suite

available

http://fai.cs.uni-saarland.de/downloads/
ppddl-benchmarks-acyclic.tar.bz2 (acyclic cases) http://fai.cs.uni-saarland.
de/downloads/ppddl-benchmarks-cyclic.tar.bz2 (cyclic cases).

232

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

regarding Domshlak Mirkis (2015) landmarks compilation, Appendix regarding depthfirst oriented heuristic searches. 4

2. MDP Models
consider PPDDL-style models (Younes et al., 2005), precisely probabilistic extensions
STRIPS. employ two formalism variants, without limited action-cost budget.
specify first unlimited-budget version. Planning tasks tuples = (F, A, I, G) consisting
finite set F facts, finite set actions, initial state F , goal G F .
pair (pre(a), O(a)) pre(a) F precondition, O(a) finite set
outcomes o. O(a) tuple (p(o), add (o), del (o))
P outcome probability p(o), add list
add (o) F , delete list del (o) F . require oO(a) p(o) = 1.
Given task , state space probabilistic transition system (S, P, I, S> ). Here,
set states, associated set F (s) true facts. initial state .
set goal states S> contains G F (s). Transitions, transition
probability function P : 7 [0, 1], defined follows. Action applicable state
pre(a) F (s) 6 S> (goal states absorbing, see below). A[s] denote
set actions applicable s. Given s, A[s], outcome O(a), sJoK denote
result outcome s, i.e., F (sJoK) := (F (s) add (o)) \ del (o). define P (s, a, t) := p(o)
applicable = sJoK.5 Otherwise, define P (s, a, t) := 0 (there transition).
Absorbing states outgoing transitions (no applicable actions). set non-goal
absorbing states lost states denoted .
limited-budget planning, extend follows. limited-budget task tuple
= (F, A, I, G, b), including budget b R+
0 , associating
.

addition


true
facts
F
(s),
states
action outcome cost c(o) R+
0
associated remaining budget b(s) R. States negative remaining budget b(s) < 0
legal may occur, lost, , due following definitions goal states,
action applicability, transitions. goal states S> G F (s)
b(s) 0, i.e., must reach goal 0 remaining budget. actions applicable
pre(a) F (s) least one outcome fits within remaining budget, i.e.,
exists O(a) c(o) b(s). outcome states sJoK, outcomes cost deduced
remaining budget, i.e., b(sJoK) := b(s) c(o).
notes order regarding limited-budget planning. c(o) > 0 o, state
space viewed directed graph arc (s, t) whenever action mapping
non-0 probability acyclic every transition strictly reduces remaining budget.
state space infinite due continuous state variable b(s), reachable part (which
algorithms consider) finite. Note remaining budget local state.
states policy violate budget, parts policy (even outcomes
action) still continue trying reach goal. differs constrained MDPs (Altman,
1999), budget bound applied globally expected cost policy. note
4. paper extension previous conference paper (Steinmetz, Hoffmann, & Buffet, 2016). cover larger
space algorithms (now including depth-first oriented heuristic searches), provide comprehensive explanations
discussions, present experiments detail.
5. assume O(a) leads different outcome state. simplify notation (our
implementation make assumption).

233

fiS TEINMETZ & H OFFMANN & B UFFET

that, single budget considered sake simplicity, framework results
straightforwardly extend models multiple budget variables.
Limited-budget planning explored deterministic oversubscription setting, objective maximize reward achieved (soft) goals subject budget (Domshlak
& Mirkis, 2015). classical-planning variant would relate resource-constrained planning (e.g.
Haslum & Geffner, 2001; Nakhost, Hoffmann, & Muller, 2012; Coles, Coles, Fox, & Long, 2013)
single consumed resource. probabilistic variant previously considered
Hou et al. (2014). Prior work probabilistic planning resources (e.g. Marecki &
Tambe, 2008; Meuleau, Benazera, Brafman, Hansen, & Mausam, 2009; Coles, 2012) often
assumed limited budgets non-0 consumption, dealt uncertain-continuous resource
consumption, contrast discrete fixed budget consumed action costs.
Though relatively restricted, limited-budget probabilistic planning quite natural. Decision
making often constrained finite budget. Furthermore, non-0 costs often reasonable
assume. applies to, example, penetration testing. Problems asking achieve goal within
given number steps, e.g. finite-horizon goal probability maximization, special case.
Let us define solutions planning tasks, well objectives wish
achieve. policy partial function : \ (S> ) 7 {}, mapping non-absorbing
state within domain either action applicable s, dont care symbol .
symbol used (only) policies already achieve sufficient goal probability elsewhere,
need elaborate act descendants. is, still require closed
policies (see below), use explicitly indicate special cases actions may chosen
arbitrarily. Formally, (s) = extends domain picking, every 6 S> reachable
(t) undefined, arbitrary action applicable setting (t) := a.
policy closed state if, every state 6 S> reachable , (t)
defined. closed closed initial state I. proper if, every state
defined, eventually reaches absorbing state probability 1.6
Following Kolobov et al. (2011), formulate goal probability maximal non-discounted
expected reward reaching goal gives reward 1 rewards 0. value
V (s) policy closed state is:

S>
1

V (s) = 0P
(1)


P (s, (s), t)V (t) otherwise
optimal value state
V (s) =

max

: closed

V (s)

(2)

Observe that, difference Kolobov et al. consider problems general MaxProb, dont need exclude improper maximization.
negative rewards, i.e., policies cannot gain anything infinite cycles.
Given value function V (any function mapping states R), Bellman update operator
defined, usual, maximization actions relative current values given V :
6. Keep mind absorbing states setting S> , i.e., goal states lost states. SSP
policy considered valid executions end goal state finding shortest path
implies path exists MaxProb policy valid executions end absorbing (goal non-goal)
state executions may fail, need always terminate.

234

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING


S>
1

V (s) := 0
P

maxaA[s] P (s, a, t)V (t) otherwise

(3)

difference V (s) prior update, updated value according right-hand
side, called Bellman residual.
greedy policy value function V selects non-absorbing state action obtaining maximum right-hand side equation (note greedy policy unique
tie-breaking). refer state space subgraph induced states reachable
using greedy policy -greedy graph. V -greedy graph, refer
state space subgraph induced states reachable greedy policy V , i.e.,
allowing state choose action greedy V .
acyclic state spaces, every run ends absorbing state finite number steps,
facing SSP problem (subject definition absorbing states, cf. above)
Bellman update operator unique fixed point V , converges initial V .
cyclic state spaces, pointed Kolobov et al. (2011), Bellman update operator may
multiple sub-optimal fixed points, updates optimistic (upper-bound) initialization
V guaranteed converge optimum V . One either use pessimistic (lowerbound) initialization V , updates guaranteed converge V ; one use
Kolobov et al.s FRET method described earlier.
consider three different objectives (algorithmic problems) goal probability analysis:
MaxProb: Find optimal policy, i.e., closed s.t. V (I) = V (I).
AtLeastProb: Find policy guaranteeing user-defined goal probability threshold [0, 1], i.e.,
closed s.t. V (I) . (Or prove exist.)
ApproxProb: Find policy optimal user-defined goal probability accuracy [0, 1], i.e.,
closed s.t. V (I) V (I) .
define algorithm family addressing problems. cover search algorithms, bisimulation reduction, dead-end pruning, node selection strategies, order.

3. Search Algorithms
use value iteration (VI) baseline. design variants AO LRTDP, well family
depth-first oriented heuristic searches, systematizing algorithm parameters underlying improved
LAO (here: ILAO ) (Hansen & Zilberstein, 2001), heuristic dynamic programming (Bonet &
Geffner, 2003a), learning depth-first search (Bonet & Geffner, 2006). furthermore design
variant FRET better suited problems uninformative initial upper bounds.
3.1 VI
pre-process VI, make one forward pass building reachable state space (actually
pruned subgraph, see Section 5). initialize value function pessimistically, simply 0
everywhere. acyclic cases, perform single backward pass Bellman updates, starting
absorbing states updating children parents, thus computing optimal value function
updating every state exactly once.
235

fiS TEINMETZ & H OFFMANN & B UFFET

procedure GoalProb-AO
initialize consist I; Initialize(I)
loop
[MaxProb: V L (I) = 1]
[AtLeastProb:V L (I) ]
[ApproxProb: V L (I) 1 V U (I) V L (I) ]
return L endif /* early termination (positive) */
[AtLeastProb: V U (I) < ]
return impossible endif /* early termination (negative) */
ex. leaf state 6 S> reachable using U
select state
else return U endif /* regular termination */
P (s, a, t) > 0
already contained
insert child ; Initialize(t)
else insert new parent
endif
endfor
BackwardsUpdate(s)
endloop
procedure
Initialize(s):
0
U
V (s) :=
1 otherwise
1 S>
L
V (s) :=
0 otherwise
6 S> L (s) := endif

Figure 1: AO* search MaxProb, AtLeastProb, ApproxProb (as indicated), acyclic state
spaces. U current greedy policy V U , L current greedy policy V L .
BackwardsUpdate(s) procedure updates V U , U , V L , L . states may
several parents , first make backwards sweep collect sub-graph |s ending
(to update V U U , greedy sub-graph V U suffices). update |s
reverse topological order.
general/cyclic case, assume convergence parameter (likewise algorithms addressing case), compute -consistent value function, Bellman
residual every state . efficient value iteration, employ topological VI per
Dai et al. (2011): find strongly connected components (SCC) state space, handle
SCC individually, children SCCs parent SCCs. VI SCC stops every state
-consistent.
Dai et al. (2011) introduce focused topological VI, eliminates sub-optimal actions
pre-process obtain smaller SCCs. much runtime-effective, still
requires building entire state space. experiments, runtime/memory exhaustion
process, i.e., building state space, reason VI failures.
consider focused topological VI here.
3.2 AO
AO , restrict acyclic case, overhead repeated value iteration
fixed points, inherent LAO (Hansen & Zilberstein, 2001), disappears. (The ILAO variant,
236

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

issue addressed depth-first orientation, covered part
depth-first oriented heuristic search family introduced Section 3.4 below.)
Figure 1 shows pseudo-code GoalProb-AO variant. algorithm incrementally constructs subgraph state space. handling duplicates simple, identifying search
nodes states, state space acyclic. reason, simple backward updating
suffices maintain value function. Adopting ideas prior work (e.g. McMahan et al., 2005;
Little, Aberdeen, & Thiebaux, 2005; Smith & Simmons, 2006; Kuter & Hu, 2007), maintain
two value functions, namely upper bound V U lower bound V L goal probability.
lack heuristic estimators goal probability, value functions initialized trivially,
1 V U 0 V L , except absorbing states exact value known. (Dead-end
detection, simple non-trivial V U initialization, discussed Section 5.) Nevertheless,
bounds useful search, early termination (V L V U ), detecting
sub-optimal parts state space (V U ). observe latter, note that, refute action a,
may suffice reduce V U one outcomes. Hence, even trivial initialization, V U
may allow disregard parts search space, usual way admissible heuristic functions.
shall see, kind behavior occurs frequently practice (as reflected benchmarks).
Regarding early termination, lower bound enables positive early termination
already guarantee sufficient goal probability, namely 1 (MaxProb), (AtLeastProb), 1 (ApproxProb). upper bound enables negative early termination AtLeastProb, V U (I) < .
ApproxProb, clearly terminate V U (I) V L (I) . relevant observation
V L (I) = 1 (MaxProb) V L (I) 1 (ApproxProb) criteria redundant
maintaining upper bound, i.e., heuristic search: V L (I) 1 , trivially
V U (I) V L (I) . V L (I) = 1, search branch achieving goal certainty,
V U (I) = 1 well search terminates regularly. configurations maintaining V U ,
however, criteria useful reduce search.
correctness GoalProb-AO easy establish. standard properties Bellman
updates, point time execution algorithm, state ,
V L (s) V (s) V U (s), i.e., V L V U lower respectively upper bounds goal
probability. Indeed, bounds monotone (Bertsekas & Tsitsiklis,
1996), precisely, V L
P
U
L
V exact absorbing states, satisfy V (s) maxaA[s] P (s, a, t)V L (t) respectively
P
V U (s) maxaA[s] P (s, a, t)V U (t) non-absorbing ones. V L V U
initialized functions trivially satisfying properties, properties invariant
Bellman updates non-absorbing states (given monotonicity, V L grow, V U
decrease). Thanks monotonicity, arguments given LAO (Hansen &
Zilberstein, 2001), get V U converges V finite time U -greedy graph.
Finally, need prove that, case early termination returning L , greedy policy L
V L actually achieves want, i.e., (1) L closed (2) L provides sufficient goal
L
probability, i.e., V (I) V L (I). (1), L always closed policy, applies
dont care symbol non-absorbing leaf states . (Note applied L
L
states.) (2), show that, states s, V (s) V L (s).
claim trivial states L (s) = , never updated V L (s) = 0.
states s, claim follows simple inductive reasoning maximal distance
L
absorbing state L -greedy graph. absorbing states s, V (s) = V L (s) = V (s),
P
L
L
claim trivially satisfied. induction step, V (s) = P (s, L (s), t)V (t)
L
L
definition V , while, induction hypothesis, V (t) V L (t) states
237

fiS TEINMETZ & H OFFMANN & B UFFET

procedure GoalProb-LRTDP
:= {I}; Initialize(I)
loop
[early termination criteria exactly GoalProb-AO ]
labeled solved
LRTDP-Trial(I)
else return U endif /* regular termination */
endloop
procedure LRTDP-Trial(s):
P := empty stack
labeled solved
push onto P
S> break endif
[cyclic: -consistent break endif]
P (s, a, t) > 0
6 Initialize(t) endif
endfor
update V U (s), U (s), V L (s), L (s)
:= sample according P (s, U (s), t)
endwhile
P empty
pop P
[acyclic: CheckSolved(s, 0) break endif]
[cyclic: CheckSolved(s, ) break endif]
endwhile

Figure 2: LRTDP MaxProb, AtLeastProb, ApproxProb, acyclic general (cyclic) state
spaces. U current greedy policy V U , L current greedy policy V L .
CheckSolved(s, ) procedure exactly specified Bonet Geffner (2003b).
visits states reachable using U , initializing previously visited, stopping
-consistent. performs updates bottom-up, labeling solved iff
descendants -consistent. change update V L L along V U
U .
P
L
P (s, L (s), t) > 0, words V (s) P (s, L (s), t)V L (t). plugging
definition L (s), using monotonicity property, easy conclude
L
V (s) V L (s), desired.
3.3 LRTDP
Figure 2 shows pseudo-code GoalProb-LRTDP variant, applicable general case (cyclic
well acyclic problems). assume that, cyclic cases, algorithm run within FRET
framework. main change original version LRTDP consists maintaining lower
bound addition upper (optimistic) bound, adding early termination criteria
GoalProb-AO . Correctness early termination follows arguments before, i.e.,
V L (s) V U (s) monotone lower respectively upper bounds, L always closed policy.
Note true even general/cyclic case, i.e., early termination applies,
terminate overall FRET process.
change make additional stopping criterion trials cyclic case,
namely current state -consistent. Kolobov et al. (2011) use criterion keep trials
238

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

procedure GoalProb-DFHS
:= {I}
loop
[early termination criteria exactly GoalProb-AO ]
(Label labeled solved)
(VI U changed running VI U -greedy graph)
DFHS-Exploration(I)
clean visited-markers
else return U endif /* regular termination */
endloop
procedure DFHS-Exploration(s):
6 Initialize(s) endif
S> labeled solved
label solved
return
endif
f lag :=
FW
V U (s) -consistent f lag := > endif
update V U (s), U (s), V L (s), L (s)
Consist f lag return > endif
endif
mark visited
foreach P (s, U (s), t) > 0
visited f lag := DFHS-Exploration(t) f lag endif
done
f lag FW
V U (s) -consistent f lag := > endif
update V U (s), U (s), V L (s), L (s)
endif
Label f lag label solved endif
return f lag

Figure 3: Depth-First Heuristic Search (DFHS) acyclic MaxProb, AtLeastProb, ApproxProb. cyclic version shown Appendix uses Tarjans SCC procedure
instead depth-first search. VI, Label, FW, Consist Boolean algorithm parameters (see text). Recall U -greedy graph set states reachable using
current greedy policy U . f lag returned DFHS-Exploration used inside
recursion (it ignored top-level calls), decide whether backward-update
state forward-updates use.

getting trapped 0-reward (non-goal) cycles. criterion preserves property that, upon
regular termination, states reachable using U -consistent.7

cyclic case, V U fixed point found LRTDP may sub-optimal, use
FRET. acyclic case, use = 0, single call LRTDP suffices.
239

fiS TEINMETZ & H OFFMANN & B UFFET

3.4 Depth-First Heuristic Search
finally consider systematic heuristic searches (not based trials LRTDP) strong
depth-first orientation. Intuitively, orientation especially beneficial context
likely lead absorbing states, thus states non-trivial heuristic function initialization,
quickly. refer algorithms Depth-First Heuristic Search (DFHS). Known instances
ILAO (Hansen & Zilberstein, 2001),8 heuristic dynamic programming (HDP) (Bonet & Geffner,
2003a), learning depth-first search (LDFS) (Bonet & Geffner, 2006). commonality lies
conducting depth-first searches (DFS) state-space subgraph defined actions greedy
current upper bound V U , updated backwards DFS, termination criterion applies. algorithms differ depth-first branches terminated, overall algorithm
terminated, whether updates performed forward direction. Here, systematize parameters, obtaining DFHS algorithm family containing previous algorithms
family members.
Figure 3 gives pseudo-code description DFHS algorithm family. simplicity,
figure considers acyclic problems only. cyclic problems, instead DFS algorithms use
Tarjans depth-first SCC algorithm (Tarjan, 1972), order detect SCCs time
exploration updates, suggested Bonet Geffner (2003a). (Knowing
SCCs required correct solved-labeling general case.) pseudo-code description
DFHS algorithm family general (cyclic) case given Appendix A.
algorithms search U -greedy graph. variant would instead search V U greedy graph. variant, employed LDFS, effective goal probability analysis
V U 1 everywhere initially, V U -greedy graph entire (dead-end pruned) reachable
state space. hence omit option, therewith LDFS, DFHS family (matters may
change better admissible heuristic functions identified future work, cf. Section 8).
algorithms update values backward direction, leaving state. FW algorithm
parameter true, value updates done forward direction, entering state.
consistently yields (small) advantages empirically, switch FW true algorithm
configurations, except one corresponding known algorithm ILAO use
technique. Detecting whether optimal solution found done two ways: (1)
Label, maintaining solved-labels DFS; (2) VI, running value iteration U greedy graph DFS terminated. (1), U optimal initial state labeled solved.
(2), one terminate greedy policy change VI. use forward updates,
(as already check Bellman residual anyway) additional option Consist
stop search -inconsistent states, opposed stopping absorbing states. Overall,
run 5 different parameter settings DFHS, overviewed Table 1.
Correctness early termination follows arguments before.
correctness regular termination, need show fixed point policy obtained, i.e., upon
regular termination, (*) U -greedy graph contains -inconsistent states. holds
algorithm variants fit Bonet Geffners (2003a) Find-and-Revise schema finite state
space monotone optimistic bound, (1) search iteration find update
7. updates trials are, difference original LRTDP formulation, related trial-stopping guarantee
goal probability maximization. turn consistently yield (small) advantages empirically, keep
here.
8. brief description ILAO Hansen Zilberstein (2001) thus depth-first orientation subject
interpretation. design follows Bonet Geffner (2005) mGPT tool.

240

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

Acronym
DFHSVI
DFHSFwd
VI
DFHSFwdCons
VI
DFHSFwd
Lab
DFHSFwdCons
Lab

Termination
VI
VI
VI
Label
Label

FW?

yes
yes
yes
yes

Cons?


yes

yes

Known?
yes: ILAO (Hansen & Zilberstein, 2001)
no: new variant
no: new variant
no: new variant
yes: HDP (Bonet & Geffner, 2003a)


Table 1: Depth-First Heuristic Search (DFHS) family overview. include LDFS (Bonet
& Geffner, 2006) as, due considering V U -greedy graph rather U -greedy
graph, LDFS work well MaxProb (see text).
least one -inconsistent state, (2) condition (*) met. Given depth-first search (respectively
Tarjans algorithm, general case) clear (1) holds true. Regarding (2), obvious
VI termination option used;9 holds Label termination option state
labeled solved descendant states U -greedy graph -consistent.
done Table 1, usually omit GoalProb- algorithm names. Keep mind
though algorithms differ original ones, particular terms early termination
depends objective MaxProb, AtLeastProb, ApproxProb. study termination
benefits lower vs. upper bound, switch bound individually.
X denotes one search algorithms, denote X|U X|L variants X maintaining
V U respectively V L . sometimes write X|LU make explicit bounds
used. Early termination criteria involving non-maintained bound disabled. X|U ,
leaves negative criterion V U (I) < AtLeastProb; X|L still positive criteria.
test version X|L X=AO , canonical representative (non-VI) blind search.
AO |L , non-absorbing leaf states open (rather reachable using U ),
case regular termination return L .
3.5 FRET
previously hinted, Kolobov et al.s (2011) FRET performs iteration complete searches.
starts upper-bound approximation V U V , continuously updated throughout
FRET process. Within FRET iteration, heuristic search algorithm runs termination,
i.e., finding fixed point policy. iterations, FRET runs trap elimination
step, finds traps V U -greedy graph. FRET forces next search iteration
include traps. FRET terminates V U -greedy graph contain trap.
trap elimination step works follows. trap subset non-absorbing states
greedy policy remain indefinitely, i.e., outgoing transitions V U -greedy
graph lead another trap state . trap removed collapsing states
single state sT . incoming transitions sT incoming state ,
outgoing transitions transitions -states exiting (note transitions are,
construction, contained V U -greedy graph).
transformation obviously prevents occuring later iterations. preserves

V trap states identical V values: trap states non-absorbing reach
other, states reach 0-reward transitions (note holds regardless
9. Note that, acyclic case, full VI actually needed algorithm could simplified. leave way
here, used ILAO general case, simplicity presentation.

241

fiS TEINMETZ & H OFFMANN & B UFFET

V U , i.e., holds parts state space V U yet converged).
finite number possible traps state space, FRET eventually finds V U whose V U greedy graph contain trap. graph, V U -greedy policy extracted,
contain traps, hence proper trap-collapsed state space, hence optimal
state space. optimal policy original task constructed acting, within
collapsed traps, way exit taken eventually reached certainty. (This
correctness argument given Kolobov, 2013.)
new variant FRET differs original version terms state space
subgraph considered: instead V U -greedy graph, use U -greedy graph, i.e., consider
actions selected current greedy policy (cf. discussion DFHS above).
refer design FRET- U , refer Kolobov et al.s (2011) design FRET-V U .
easy see FRET- U still correct. arguments remain intact stated.
FRET-V U potentially eliminates traps iteration, may hence require fewer iterations. Yet traps may actually need eliminated (we might eventually find optimal
policy entering them), trap elimination step may much costly. particular,
goal probability analysis, FRET-V U typically ineffective because, similarly discussed
DFHS, first FRET step V U often 1 almost everywhere, V U -greedy graph
almost entire reachable state space. shall see, FRET- U clearly outperforms FRET-V U .

4. State-Space Reduction via Determinized Bisimulation
Bisimulation known method reduce state space size MDPs/probabilistic planning (e.g.
Dean & Givan, 1997). idea essentially group equivalent sets states together block
states, solve smaller MDP block states. Here, observe approach fruitfully combined state-of-the-art classical planning techniques, namely mergeand-shrink heuristics (Drager, Finkbeiner, & Podelski, 2009; Helmert, Haslum, Hoffmann, & Nissim, 2014), allow effectively compute bisimulation determinized state space.
Determinized-bisimilar states bisimilar probabilistic state space well, identifies practical special case probabilistic bisimulation given factored (STRIPS-like) problem
specification.
Let us spell little detail. Given task (with without budget limit),
probabilistic bisimulation partitioning P = {B1 , . . . , Bn } state set that,
every Bi Bj , every action a, every s, Bi , following two properties satisfied
(Dean & Givan, 1997):
(i) applicable iff applicable t;
P
P
(ii) applicable t, oO(a),sJoKBj p(o) = oO(a),tJoKBj p(o).
Dean Givan show optimal solution bisimulation MDP induces optimal
solution MDP itself. words, suffices work block states Bi .
Now, denote det all-outcomes determinization (e.g. Yoon et al., 2007; Little
& Thiebaux, 2007), separate action adet
every O(a), inheriting preo
condition os adds, deletes, cost. determinized bisimulation partitioning
P = {B1 , . . . , Bn } states that, every Bi Bj , every determinized action adet
,
every s, Bi , following two properties satisfied (Milner, 1990; Helmert et al., 2014):
det
(a) adet
applicable iff ao applicable t;

242

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

det
det
(b) adet
applicable t, sJao K Bj iff tJao K Bj .

easy see {B1 , . . . , Bn } probabilistic bisimulation . Since action
adet
applicable state iff corresponding action original MDP applicable s,
(a) directly implies (i). (b), know every action applicable s, t,
det
outcome O(a), sJadet
K Bj iff tJao K Bj . obviously implies (ii);
restrictive needed insists subset outcomes sides, rather
summed-up probability same.
compute determinized bisimulation ? nave solution build state
space front computing determinized bisimulation it. One potentially much
better though, using merge-and-shrink widely employed shrinking strategies based
bisimulation (Nissim, Hoffmann, & Helmert, 2011; Katz, Hoffmann, & Helmert, 2012; Helmert
et al., 2014). nutshell, algorithm framework constructs abstraction starting
collection abstractions considering single state variable only, iteratively merging
two abstractions (replacing synchronized product) single abstraction
left, shrinking abstractions bisimulation thereof every merging step.
shall see experiments, often still incurs prohibitive overhead, feasible,
lead substantial state space size reductions. cases, results tremendous performance
improvements.

5. Dead-End Pruning
refer states V (s) = 0, i.e., goal cannot reached s, dead-ends.
one detects via dead-end detection technique, one treat exactly lost
state (except setting L (s) := need act non-absorbing states). constitutes
pruning method itself, useful search algorithm, state space needs
longer explored. Apart pruning itself, heuristic search algorithms, dead-end
detection provides non-trivial initialization V U , initialize V U (s) = 0 instead
V U (s) = 1 detected dead-end. informed initial upper bound typically
leads additional search reductions.
detect dead-ends? Kolobov et al. (2011) employ SixthSense (Kolobov et al., 2010),
learns dead-end detection rules generalizing information obtained using classical planner. instead exploit power classical-planning heuristic functions readily
available FD implementation framework run all-outcomes determinization.
especially promising limited-budget planning, use lower bounds determinized
remaining cost detect states insufficient remaining budget. Observe natural
effective using admissible remaining-cost estimators, yet would impractical using actual classical planner (which would need optimal thus prohibitively slow). unlimited-budget
case, use heuristic function able detect dead-ends (returning ), applies
known heuristics. Indeed, merge-and-shrink heuristics recently shown extremely competitive dead-end detectors (Hoffmann, Kissmann, & Torralba, 2014).
make concrete, consider state task , denote det alloutcomes determinization . Let h classical-planning heuristic function. h guarantees
return dead-ends, h(s) = det , exists sequence action
outcomes achieving goal s, V (s) = 0. limited-budget task, h admissible,
h(s) > b(s), cannot achieve goal within budget, thus V (s) = 0.
243

fiS TEINMETZ & H OFFMANN & B UFFET

experiment state-of-the-art heuristic functions, namely (a) admissible landmark
heuristic per Karpas Domshlak (2009), (b) LM-cut (Helmert & Domshlak, 2009), (c) several
variants merge-and-shrink heuristics, (d) hmax (Bonet & Geffner, 2001) simple
canonical option. (a) turned perform consistently worse (b), report
(b) (d).
limited-budget planning, considered adopting problem reformulation Domshlak Mirkis (2015) oversubscription planning, reduces budget b using landmarks
exchange allows traversing yet unused landmarks reduced cost search. turns
out, however, pruning states whose reformulated budget < 0 equivalent much simpler method pruning states whose heuristic (a) exceeds (original/not reformulated) remaining
budget. added value Domshlak Mirkis reformulation thus lies, pruning per se,
compilation planning language resulting combinability heuristics.
give full details Appendix B. get intuition Domshlak Mirkis reformulation
is, per se, equivalent (a), assume simplicity L set disjoint disjunctive action landmarks initial state, assume actions unit costs. Say prune reduced budget, b0 (s), < 0. reduced initial budget b0 := b|L|. reduced costs allow applying member actions yet non-used landmarks 0 cost, non-used landmarks given search
path l L touched path. Consider state reached path ~a. Denote
non-used landmarks L0 . cost saved ~a thanks reformulation exactly
used landmarks, |L\L0 |. Hence b0 (s) = b0 (|~a||L\L0 |) = (b|L|)|~a|+|L\L0 | = b|~a||L0 |.
pruned reformulation, b0 (s) < 0, iff b |~a| |L0 | < 0 iff b |~a| < |L0 |. latter
condition, however, exactly pruning condition using simple method (a) instead.

6. Node Selection Strategies
algorithms, good anytime behavior V L and/or V U may translate early termination.
explore potential fostering via (1) biasing tie-breaking selection best
actions U greedy respect V U , (2) biasing, respectively, outcome-state sampling
trials (LRTDP) choice expanded leaf states (AO ). precise regarding
latter: usual, maintain state open flags AO , true state open descendants within
U -greedy graph. select leaf state expand going forward using U ,
action one open outcome state t, select best according bias (2). Note
(2) relevant DFHS, every iteration DFS explores outcomes anyhow.
Hence, DFHS, use U tie-breaking criteria (1) explained follows.
experimented variety strategies. follows, strategy specifies one
(1) (2) only, setting default strategy. strategy corresponds
commonly used settings. uses arbitrary tie-breaking (1), fixed manner, changing
U (s) action becomes strictly better s, suggested Bonet Geffner
(2003b) LRTDP. bias (2) outcome states AO (an open outcome state selected
arbitrarily). Bias (2) LRTDP outcome probability. tried most-prob-outcome
bias strategy AO , likely open outcome state selected.
h-bias strategy prefers states smaller h value, heuristic h one
used dead-end pruning.10 Specifically, action selection tie-breaking (1), actions
10. experimented strategy using merge-and-shrink determinized action costs set negated
logarithm outcome probability (compare e.g. Jimenez, Coles, & Smith, 2006). compelling theory

244

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

P
U
maximizing optimistic expected
Pgoal probability P (s, a, t)V (t), select minimizing expected heuristic value P (s, a, t)h(t). outcome-state bias (2) obtained
1
renormalizing weighed probabilities h(t)
P (s, a, t), prefer high probability outcomes
small h value.
Inspired BRTDP (McMahan et al., 2005), experiment gap-bias strategy, biasing
U
L
search towards states large
Precisely, (1) break ties favor actions
PV V gaps.
maximizing expected gap P (s, a, t)[V U (t)V L (t)], (2) renormalize weighed
probabilities [V U (t) V L (t)] P (s, a, t).
Inspired common methods classical planning (e.g. Hoffmann & Nebel, 2001; Helmert,
2006; Richter & Helmert, 2009), experiment preferred actions strategy, (1)
U (s) action participating delete-relaxed determinized plan s,
prefers set P
maximizing P (s, a, t)V U (t) exists.
AO |L special case, maintain upper bound thus selection
(1) actions U greedy respect V U . apply node selection strategies (2) directly
set (all) leaf states current search graph . default strategy depth-first,
rationale try reach absorbing states quickly. h-bias strategy selects deepest leaf
minimal h value, preferred actions strategy selects deepest open leaf reachable using
preferred actions. furthermore experiment breadth-first strategy, comparison.

7. Experiments
implemented algorithms Fast Downward (FD) (Helmert, 2006), ran experiments
extensive suite benchmarks.11 evaluation first summarize results acyclic
benchmarks (where FRET needed), ones cyclic benchmarks (where FRET
needed).
7.1 Experiments Setup
start giving details implementation describing benchmark suite used
experiments.
7.1.1 MPLEMENTATION
model pertains goal-directed MDPs limited number (explicitly listed) outcomes
per action, naturally use PPDDL (Younes et al., 2005), rather RDDL (Sanner, 2010; Coles
et al., 2012), surface-level language. FDs pre-processes extended handle PPDDL,
added support specifying (numeric) budget limit.
Given FD implementation framework contrast previous works optimal probabilistic
planning, implemented algorithms scratch. FRET, closely followed original
implementation, details specified Kolobov et al. (2011), based personal communication Andrey Kolobov. (Kolobovs original source code available anymore,
plays role state-of-the-art comparison, see next.)
because, then, bisimulation-based heuristic corresponds exact goal probability best outcome sequence
state. Yet, already pointed out, computing heuristic often infeasible.
11. source code available online appendix, downloaded http://fai.cs.unisaarland.de/downloads/fd-prob.tar.bz2

245

fiS TEINMETZ & H OFFMANN & B UFFET

Given scant prior work optimal goal probability analysis (cf. Section 1), state art
represented topological VI, LRTDP|U dead-end pruning acyclic problems,
FRET-V U using LRTDP|U dead-end pruning cyclic problems. configurations
particular points space configurations explore, comparison state art
part comparison across configurations. thing missing particular form
dead-end detection, SixthSense prior work, Kolobov et al. (2011).
SixthSense complex method advanced dead-end pruning via heuristic functions readily
available framework, re-implement SixthSense. discussion cyclic problems
Section 7.3 includes detailed comparison results Kolobov et al.,
IPPC ExplodingBlocks domain Kolobov et al. considered.
Note providing quality guarantees important property study. reason,
sake clarity, compare unbounded suboptimal approaches,
using algorithm discounted criterion assigning large finite penalties dead-ends
(Teichteil-Konigsbuch et al., 2011; Kolobov et al., 2012).
Furthermore, AtLeastProb special case MDP model checking, one may wonder
probabilistic model checking tools, e.g. PRISM (Kwiatkowska et al., 2011b), would fare
problem planning benchmarks. investigate question here, would entail
translation PPDDL model checking language, non-trivial makes direct
comparison algorithms taking different inputs problematic. One may speculate that, given
focus blind searches, model checking tools inferior heuristic search approaches
fare well; remains question future work.
7.1.2 B ENCHMARK UITE
aim comprehensively explore relevant problem space, designed broad suite
benchmarks, 1089 instances total, based domains IPPC, resource-constrained
planning, penetration testing (pentesting).
IPPC, selected PDDL domains STRIPS format, moderate nonSTRIPS constructs easily compilable STRIPS. resulted 10 domains IPPC04
IPPC08; selected recent benchmark suite these.
resource-constrained planning, adopted NoMystery, Rovers, TPP benchmarks
Nakhost et al. (2012), precisely suites single consumed resource (fuel, energy, money), correspond limited-budget planning.12 created probabilistic versions
adding uncertainty underlying road map, akin Canadian Traveler scenario,
road segment present given probability (this encoded separate, probabilistic, action attempting segment first time). simplicity, set probability 0.8
throughout.
pentesting, general objective using exploits compromise computers network, one another, specific targets reached (or action available). modified
POMDP generator Sarraute, Buffet, Hoffmann (2012), based test scenario used Core Security (http://www.coresecurity.com/) output PPDDL encodings
Hoffmanns (2015) attack-asset MDP pentesting models. models, network configura12. make benchmarks feasible optimal probabilistic planning, reduce size parameters (number
locations etc). scaled parameters number < 1, chosen get instances borderline
feasibility VI.

246

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

tion known fixed, exploit callable succeeds (or fails) probability. generator uses network consisting exposed part, sensitive part, user part.
allows scale numbers H hosts E exploits. Sarraute et al.s POMDP model
solver (SARSOP, see Kurniawati, Hsu, & Lee, 2008, guarantee optimality) scale
H = 6, E = 10.13 benchmarks, fixed H = E simplicity (and obtain number
instances similar benchmark domains). scaled instances 6 . . . 20 without
budget limit, 10 . . . 24 budget limit.
benchmark tasks (except pentesting ones already
generated separate limited-budget version anyway), obtained several limited-budget benchmarks, follows. set outcome costs 1 otherwise specified. determined
minimum budget, bmin , required achieve non-0 goal probability. resource-constrained
benchmarks, bmin determined generator itself, minimum amount resource required
reach goal deterministic domain version. benchmarks, ran FD
LM-cut all-outcomes determinization . failed, skipped , otherwise
read bmin cost optimal plan created several limited-budget tasks [C], differing
constrainedness level C. Namely, following Nakhost et al. (2012), set global budget b
[C] b := C bmin , C factor available budget exceeds minimum
needed (to able reach goal all). let C range {1.0, 1.2, . . . , 2.0}.
AtleastProb, let range {0.1, 0.2, . . . , 1.0} ( = 0 pointless). ApproxProb,
let range {0.0, 0.1, . . . , 0.9} ( = 1 pointless). cyclic problems, convergence
parameter set 0.00005 (the value used Kolobov et al., 2011). experiments
run cluster Intel E5-2660 machines running 2.20 GHz, time/memory cut-offs
30 minutes/4 GB.
7.2 Acyclic Planning
consider first acyclic planning. pertains budget-limited benchmarks, pentesting
without budget limit, well IPPC TriangleTireworld (moves made
one direction state space acyclic). consider 3 objectives MaxProb, AtLeastProb,
ApproxProb. run 16 search algorithm variants (VI, AO , LRTDP, 5 DFHS variants,
subsets bounds applicable), 5 node selection strategies explained. deadend pruning, run LM-cut, well merge-and-shrink (M&S) state-of-the-art shrinking
strategies based bisimulation abstraction-size bound N ; show data N =
N = 100k (we tried N {10k, 50k, 200k} resulted similar behavior). run
variants without dead-end pruning. use deterministic-bisimulation (DB) reduced state space
VI: (and if) bisimulation successfully computed, block-state MDP easily
solved simplest algorithm. Given DB, require dead-end pruning
dead-ends already removed reduced state space.
Overall, yields 577 different possible algorithm configurations. actually test
configurations, course, interesting, needed make essential
observations. instead organize experiment terms three parts (1)(3), focusing
particular issue interest. Consider Table 2, gives overview configurations
considered experiment. design experiments follows:
13. modeling/solving entire network, is. domain-dependent decomposition algorithm 4AL,
trading accuracy performance, Sarraute et al. scale much further.

247

fiS TEINMETZ & H OFFMANN & B UFFET

Experiment

Search Algorithm


Pruning

Node selection

# Configs



AO |U ,
MaxProb search & prun- VI, AO |L ,
default
LRTDP|
,
DFHS|
U
U (5), (4)
ing
VI DB
VI, AO |L ,
AO |U ,

AtLeastProb & Approx- AO |LU ,
LRTDP|U ,
(2)
LM-cut
default
Prob parameters
LRTDP|LU ,
HDP|U ,
HDP|LU , VI DB
VI, AO |L ,
AO |U ,
(1, 4, 4, 5, 3,

LRTDP|U ,
AtLeastProb & Approx- AO |LU ,
(3)
LM-cut 4, 3, 4, 1 respecLRTDP|LU ,
HDP|U ,
Prob node selection
tively)
HDP|LU , VI DB
(1)

37

18

58

Table 2: Overview algorithms tested acyclic problems, Section 7.2. Numbers brackets give
number options number obvious. (2) (3), note total
number configurations gets multiplied 2 AtLeastProb vs. ApproxProb result
different algorithm configurations (using different termination criteria). HDP
member DFHS family, corresponding Bonet Geffners (2003a)
DFHSFwdCons
Lab
HDP algorithm.
(1) first evaluate different search algorithms dead-end pruning methods MaxProb, fixing
node selection strategy default.
omit X|LU variants, because, explained earlier, MaxProb heuristic search,
maintaining V L redundant (early termination dominated regular termination).
Using default node selection strategy makes sense node selection strategies
relevant anytime performance, i.e., early termination. plays minor role
MaxProb, whose early termination possibility exceptional case initial state
lower bound becomes V L (I) = 1.
(2) next fix best-performing dead-end pruning method, analyze search algorithm performance AtLeastProb ApproxProb function parameter respectively .
fix node selection strategy default here, leaving examination experiment
(3).
(3) finally let node selection strategies range, keeping otherwise setting experiment
(2).
conclude discussion (4) additional data illustrating typical anytime behavior.
part experiment described separate sub-section follows.
7.2.1 (1) EARCH LGORITHMS & P RUNING ETHODS AX P ROB
Table 3 shows coverage data, i.e., number benchmark tasks MaxProb solved
within given time/memory limits.
pruning methods, LM-cut clearly stands out. every search algorithm, yields
far best overall coverage. M&S substantial advantages RectangleTireworld
NoMystery-b. Note that, N = , overall coverage worse using pruning
all. due prohibitive overhead, domains, computing bisimulation
determinized state space. And, invested effort, pays use bisimulation
248

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

Domain

#

TriaTire

10

Blocksw-b
Boxworl-b
Drive-b
Elevator-b
ExpBloc-b
Random-b
RecTire-b
Tirewor-b
TriaTire-b
Zenotra-b

66
18
90
90
84
60
36
90
60
36

NoMystery-b 60
Rovers-b
60
TPP-b
60
Pentest-b
Pentest
P

90
15
925

Domain

#

TriaTire

10

Blocksw-b
Boxworl-b
Drive-b
Elevator-b
ExpBloc-b
Random-b
RecTire-b
Tirewor-b
TriaTire-b
Zenotra-b

66
18
90
90
84
60
36
90
60
36

NoMystery-b 60
Rovers-b
60
TPP-b
60
Pentest-b
Pentest
P

90
15
925

DFHSFwd
DFHSFwdCons
|U
DFHSFwd
VI |U
VI
Lab |U
LM M&S
LM M&S
LM M&S

N
N
N
IPPC Benchmarks
9 10 10 10
9 8 8 8 10 10 10 10
9 8 8 8 10
IPPC Benchmarks Budget Limit
24 28 24 24 24 28 24 24 24 28 24 24 24 28 24 24 24
0 3 0 0
0 3 0 0
0 3 0 0
0 3 0 0
0
90 90 90 52 90 90 90 52 90 90 90 52 90 90 90 52 90
78 86 79 33 79 86 79 33 78 86 79 33 79 86 79 33 78
37 60 39 37 37 60 39 37 36 66 39 37 37 60 39 37 36
36 44 36 33 36 44 36 33 36 44 36 33 36 44 36 33 36
28 31 36 36 30 31 36 36 30 31 36 36 30 31 36 36 30
90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90
46 55 55 55 46 55 55 54 46 55 55 55 46 55 55 54 46
15 17 17 18 15 17 17 18 12 15 14 15 15 17 17 18 14
Probabilistic Resource-Constrained Benchmarks Budget Limit
12 40 50 50 12 41 50 50 12 42 50 50 12 41 50 50 12
25 46 36 46 25 46 36 46 25 46 36 47 25 46 36 46 25
19 38 27 25 19 38 27 25 20 39 27 25 19 38 27 25 20
Pentesting Benchmarks
57 63 62 37 57 63 62 37 57 63 62 37 57 63 62 37 57
9 9 9 8
9 9 9 8
8 8 8 8
9 9 9 8
9
575 710 660 554 578 709 658 551 574 716 656 552 578 709 658 551 577
DFHSVI |U
LM M&S
N

HDP|U
LM M&S
N
10 10 10
28
3
90
86
66
44
31
90
55
16

24
0
90
79
39
36
36
90
55
16

42 50 50
46 36 47
39 27 25
63 62 37
9 9 8
718 659 553

AO |L
AO |U
LRTDP|U
HDP|U
LM M&S
LM M&S
LM M&S
LM M&S
N
N
N
N
IPPC Benchmarks
4 4 4 4
4 4 4 4 10 10 10 10 10 10 10 10 10 10 10 10
IPPC Benchmarks Budget Limit
24 28 24 24 24 28 24 24 24 28 24 24 24 28 24 24 24 28 24 24
0 3 0 0
0 3 0 0
0 3 0 0
0 3 0 0
0 3 0 0
90 90 90 52 90 90 90 52 90 90 90 52 90 90 90 52 90 90 90 52
71 82 72 33 74 84 76 33 65 77 67 33 79 86 79 33 78 86 79 33
32 46 38 37 32 46 38 37 39 57 39 37 38 65 39 37 36 66 39 37
27 33 35 33 39 34 36 33 35 44 36 33 36 44 36 33 36 44 36 33
30 31 36 36 30 31 36 36 30 31 36 36 30 31 36 36 30 31 36 36
90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90
45 52 52 52 45 52 52 52 46 55 55 55 47 57 57 57 46 55 55 55
15 16 16 18 15 16 16 18 14 16 16 17 15 17 16 17 14 16 16 16
Probabilistic Resource-Constrained Benchmarks Budget Limit
11 37 43 44 11 36 42 43 12 39 47 47 12 41 50 50 12 42 50 50
23 39 31 40 23 38 31 40 23 44 33 45 25 46 35 46 25 46 36 47
18 35 25 25 16 35 24 24 15 37 26 22 19 38 27 25 20 39 27 25
Pentesting Benchmarks
57 63 62 37 57 63 62 37 57 63 63 37 57 63 63 37 57 63 62 37
9 9 9 8
9 9 9 8
9 9 9 8
9 9 9 8
9 9 9 8
546 658 627 533 559 659 630 531 559 693 641 546 581 718 661 555 577 718 659 553
VI
LM M&S
N

24
0
52
33
37
33
36
90
55
16

VI

DB
10
24
0
52
33
37
33
36
90
60
17
51
50
26
37
8
564

Table 3: Acyclic planning. MaxProb coverage (number tasks solved within time & memory
limits). Best values, within table, boldface. Top: DFHS variants (recall HDP
DFHSFwdCons
member DFHS family; DFHSVI ILAO ). Bottom: remaining
Lab
search algorithms, including overall best DFHS variant. Domains -b modified
budget limit. #: number instances. : pruning; else pruning,
remaining budget -b domains, based h = domains. LM: LM-cut;
M&S: merge-and-shrink, N size bound N = 100k, size bound. VI DB:
VI run reduced (deterministic-bisimulated) state space. Default node selection.

249

fiS TEINMETZ & H OFFMANN & B UFFET

107

107

106

106

LRTDP|U (LM-cut)

LRTDP|U

reduced MDP state space (VI DB), rather dead-end pruning. extreme
example latter TriangleTireworld. Far beyond standard benchmarks Table 3 (triangleside length 20), VI DB scales side length 74 original domain limited-budget
version. comparison, hitherto best solver far Prob-PRP (Camacho et al., 2016),
scales side length 70 original domain, optimal goal probability 1, i.e.,
presence strong cyclic plans holds original domain limited-budget
version. (We could actually run Prob-PRP limited-budget domain version, Prob-PRP
natively support budget, hard-coding budget PPDDL resulted encodings
large pre-process.)
Comparing different DFHS|U variants, configuration clearly stands out.
Overall, perform equally well, though FwdCons variants (cutting exploration
inconsistent states rather absorbing states) slight edge. difference mainly comes
TriangleTireworld, ExplodingBlocks, TPP-b, FwdCons configurations solve
instances, Zenotravel-b FwdCons configurations perform slightly worse counterparts. termination parameter (VI vs. Label) almost effect coverage. Due
gives best coverage results,
similarity DFHS configurations, DFHSFwdCons
Lab


representative


DFHS
family

remaining discussion.
use DFHSFwdCons
Lab
FwdCons
corresponds HDP, simplicity refer name.
DFHSLab
AO |L better VI case early termination V L = 1, full-certainty
policy found visiting entire state space. happens rarely here, AO |L
dominated VI (this changes AtLeastProb, see Figures 5a 7 below). failures VI
due memory runtime exhaustion building reachable state space. LRTDP|U clearly
outperforms AO |U , presumably tends find absorbing states quickly. LRTDP|U
HDP|U par; LM-cut solve exact number instances (though
exactly instances), otherwise HDP|U solves slightly fewer tasks LRTDP|U .
gauge efficiency heuristic search vs. blind search MaxProb, compare LRTDP|U vs.
VI Table 3. Contrary intuition good initial goal probability estimator required
heuristic search useful, LRTDP|U clearly superior. advantage grow quality
initialization; LM-cut yields largest coverage increase far. However, even without
dead-end pruning, i.e., trivial initialization V U , LRTDP|U dominates VI throughout,
improves coverage 8 16 domains.

105
104
103

104
103
102

102
101 1
10

105

102

103

104
VI

105

106

101 1
10

107

102

103

104
105
VI (LM-cut)

106

107

Figure 4: Acyclic planning. Number states visited, VI (x) vs. LRTDP|U (y), pruning
(left) respectively LM-cut pruning (right). Default node selection.
next shed additional light comparing search space sizes runtime values.
Tables 4 5 provide aggregate data, Figure 4 gives scatter plot canonical comparison
250

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

AO |U
LRTDP|U
LM
M&S

LM
M&S
N

N

IPPC Benchmarks
843.1 843.1 843.1 843.1
0.2 0.2 0.2 0.2
0.8 0.7 0.7 0.7
0.4 0.4 0.4 0.4
3.5 1.7 1.7 1.7
IPPC Benchmarks Budget Limit
12.7 5.8 2.8 2.8
12 5.2 2.5 2.5 12.3 5.3 2.5 2.5
4.2 2.4 1.8 1.2
4.2 2.2 1.6
1
4 2.1 1.5
1
12.8 3.9 7.2
3
3.3 0.3 0.4 0.1
3.6 0.3 0.4 0.1
1.1K 41.9 92.1 33.2 112.2 1.2 12.2 0.8 117.8 1.4 12.5 1.2
4.1K 213.2 859.9 179 603.1 3.6 133.6 2.6 588
4 106.8 3.2
2.6K 1.5 17.2 1.1 3.1K 2.1 21.3 1.6
4.5 1.7 1.7 1.7
1.9 0.8 0.8 0.8
2 0.8 0.8 0.8
1.0K 130 127.4 127.4 42.6 6.4 6.4 6.4 43.7 6.4 6.4 6.4
2.7K 15.9 2.2 2.2 2.9K 15.9 2.2 2.2
50.6 5.6 1.5 1.5 49.8 5.1 1.2 1.2 50.4 5.1 1.3 1.3
81.9 8.9 2.4 2.4 81.5 8.2 1.9 1.9 81.6 8.3
2
2
1.4K 6.4 6.4 6.4 898.6
3
3
3 896.2 2.9 2.9 2.9
4.1K 229.4 229.4 229.4 1.8K 52.5 52.5 52.5 1.6K 44.5 44.5 44.5
7.4K 610.3 610.3 610.3 4.6K 303.3 303.3 303.3
491.5 30.2 35.8 18.2 491.3 29.9 35.2 17.9 288.2 23.6 27.1 14.1
967.4 104.4 164.6
64 967.1 102.9 161.1 62.3 478.1 75.3 114.9 45.8
Probabilistic Resource-Constrained Benchmarks Budget Limit
2.8K 6.9 0.5 0.5 2.6K 6.6 0.4 0.4 2.6K 6.4 0.4 0.4
12.4K 122.4 14.1 14.1 12.7K 122.3 16.4 16.4
1.1K 51.8 91.5 22.6 702.9 36.1 58.6 12.4 873.7 38.5 70.8 14.3
2.2K 290.1 512.6 137.6 1.1K 176 281.4 65.9 1.6K 190 366.2 76.3
4.7K 287.1 709.7 205.2 8.3K 338.3 1.0K 242.1
1.1K 49.6 265.4 10.9 660.9
33 183.3 5.7 897.2 38.5 220.7 7.6
3.0K 178.7 894.6 36.6 1.5K 100.4 549.5 14.3 2.1K 120.1 701.9 21.5
Pentesting Benchmarks
19.7 6.3 7.8 6.3 19.5 6.3 7.7 6.3 19.7 6.2 7.7 6.2
238.1 165.1 169.2 165.1 237.2 165.1 169 165.1 238.1 165.1 169.1 165.1
74.3 66.3 66.4 66.3 74.3 66.3 66.4 66.3 74.3 66.3 66.4 66.3
194.3 173.4 173.8 173.4 194.3 173.4 173.8 173.4 194.3 173.4 173.8 173.4


Domain

#

TriaTire
ONLY-H

1
4

Blocksw-b
18
Drive-b
20
Elevator-b
12
ExpBloc-b
18
NON-TRIVIAL 7
ONLY-H
3
Random-b
21
NON-TRIVIAL 4
ONLY-H
2
RecTire-b
18
NON-TRIVIAL 12
TriaTire-b
17
NON-TRIVIAL 6
ONLY-H
1
Zenotra-b
14
NON-TRIVIAL 10
NoMystery-b 11
ONLY-H
1
Rovers-b
21
NON-TRIVIAL 13
ONLY-H
2
TPP-b
9
NON-TRIVIAL 5
Pentest-b
28
NON-TRIVIAL 5
Pentest
3
NON-TRIVIAL 1

VI
LM
M&S
N






HDP|U
LM
M&S
N


2.2 1.8 1.8 1.8
160.3 835.4 835.4 835.4
11.5 4.8 2.3 2.3
4 2.1 1.5 0.9
3.4 0.3 0.4 0.1
150.7 1.2 13.6 0.8
780.4 3.4 125.6 2.2
5.2K 1.9 23.4 1.1
1.9 0.8 0.8 0.8
34.5 5.4 5.4 5.4
2.9K 15.9 2.2 2.2
50.4
5 1.2 1.2
81.5 8.1 1.9 1.9
954 3.4 3.4 3.4
1.8K 67.4 67.4 67.4
6.2K 634.8 634.8 634.8
285.1 23.6 27.4 14.2
468.6 75.3 115.9 46.3
2.7K 6.5 0.4 0.4
12.7K 117.3 14.2 14.2
782.7 35.8 63.2 12.4
1.3K 173.8 318.4 65.9
5.9K 265.5 741 189.5
765.4 31.3 188.1 5.6
1.8K 91.3 561.8
14
19.7 6.2 7.7 6.2
238.1 165.1 169.1 165.1
74.3 66.3 66.4 66.3
194.3 173.4 173.8 173.4

Table 4: Acyclic planning. MaxProb geometric mean search space size (number states visited)
multiples 1000. # gives size instance basis, namely instances solved
shown configurations, skipping instances solved 1 second configurations. NON-TRIVIAL uses instances solved VI < 1 second.
ONLY-H uses instances commonly solved AO |U , LRTDP|U , HDP|U ,
solved VI. Rows empty instance basis skipped. Default node selection.
VI LRTDP|U . Data AO |L shown coverage dominated VI (cf.
Table 3), goes runtime search space. include NON-TRIVIAL rows
tables show behavior interesting instances, averages skewed
many small instances domains. include ONLY-H rows elucidate
behavior challenging instances beyond reach VI.
clear message Table 4 Figure 4 heuristic search algorithms, apart
exceptions, visit much fewer states VI does, even trivial upper bound initialization search spaces reduced domains except RectangleTireworld Pentest.
instance, using LRTDP|U instead VI results gain around 1 order magnitude many
instances, larger gains (up 3 orders magnitude) occur rare cases. giving
heuristic search algorithms additional information earlier dead end detection, differences become even larger.
251

fiS TEINMETZ & H OFFMANN & B UFFET

Domain

#

TriaTire
ONLY-H

1
4

Blocksw-b
18
Drive-b
20
Elevator-b
12
ExpBloc-b
18
NON-TRIVIAL 7
ONLY-H
3
Random-b
21
NON-TRIVIAL 4
ONLY-H
2
RecTire-b
18
NON-TRIVIAL 12
TriaTire-b
17
NON-TRIVIAL 6
ONLY-H
1
Zenotra-b
14
NON-TRIVIAL 10
NoMystery-b 11
ONLY-H
1
Rovers-b
21
NON-TRIVIAL 13
ONLY-H
2
TPP-b
9
NON-TRIVIAL 5
Pentest-b
28
NON-TRIVIAL 5
Pentest
3
NON-TRIVIAL 1

AO |U
LRTDP|U
HDP|U
LM M&S
LM M&S
LM M&S
N

N

N

IPPC Benchmarks
3.5 7.4 4.1
4
0
0 0.1 0.1
0 0 0
0
0 0 0.1 0.1
0.1 0.1 0.5 0.5 0.1 0.3 0.6 0.6
1.4 23.9 12.1 12.1
IPPC Benchmarks Budget Limit
0.1 0.6 2.5 2.3
1.8 0.8
3 2.8 0.2 0.6 2.3 2.4
0.2 0.5
2 2.1
0 0.2 6.9 14
0.1 0.2
8 15.4
0 0.2 7.1 14.2
0 0.2 6.1 12.3
0.1 0.1 1.8 4.1
0
0 2.2 4.5
0 0 1.8 4.1
0 0 1.7 3.8
6 1 15.5 7.5
1.6
0 16
8 0.8 0.1 14.2 7.1
0.9
0 13.4 6.6
25.3 4.8 36.2 45.7
11.5 0.1 33 45.9
4 0.1 29.7 42.1
5 0.1 27.2 39.6
29.3 0.1 40.9 40.4 21.4 0.1 30.7 34.9 35.1 0.1 30.2 32.5
0.5 0.6 4.8 4.8
0.3 0.3 5.2 5.2 0.3 0.3 4.7 4.8
0.2 0.3 4.4 4.4
13.9 10.1 39.2 43.2
3 0.9 44.4 49.9 1.4 0.8 36.3 43.2
0.8 0.7 35.3 38.2
27.8 11.3 38.1 42.9 30.3 11.1 35.4 37.4 29.8 10.8 34.8 35.2
9 19.4 1.2 1.2
73.6 20.2 1.3 1.3 43.1 17.6 1.3 1.3 131.2 16.4 1.2 1.2
20.4 57.3 2.3 2.3 178.9 61.8 2.4 2.4 106.8 51.4 2.3 2.3 330.1 46.5 2.3 2.3
10.5 0.5 0.6 0.6
14.5 0.4 0.5 0.5 9.5 0.3 0.4 0.4
8.4 0.4 0.4 0.4
27.7 5.9 3.2 3.3
31.3 2.4 2.1
2 14.7
2 1.7 1.6 13.7 2.4 1.8 1.7
153.2 25.4 13.6 13.6 41.5 11.9 5.1 4.4
42 18.2 6.9 7.1
2.7 4.9 15
9
56 5.7 18.9 11.8
13 4.3 15.9 9.2 52.5
5 16.8 9.3
5.6 16.5 27 13.5 163.4 19.3 37.2 18.7 25.3 13.6 30.1 14.5 118.3 16.8 35.2 15.1
Probabilistic Resource-Constrained Benchmarks Budget Limit
15.6 0.4 0.3 0.3 242.6 0.4 0.3 0.3 27.8 0.4 0.3 0.3 26.2 0.4 0.3 0.3
1623.4 8.9 0.6 0.6 158.8 7.8 0.5 0.4 137.2 7.3 0.4 0.4
9.7 2.3 11.8 17
96.2 2.3 16.5 21.7 12.8
2 11.6 16.1 10.8 1.8 9.9 14.9
20.9 12.9 20.2 33.7 236.6 12 33.3 45.1 24.9 9.7 19.5 30.3 19.2 8.8 15.9 29.1
751.2 19.2 76.9 151.4 127.6 18.6 44.8 126.9 85.4 14.8 34.3 105.1
8.5 1.6 14.9 69.8
63.2 1.3 24.6 70.2 12.7 1.3 16 69.1
9.6 1.1 14.3 65.1
22.4 5.7 18.5 76.2 203.5 4.5 37.3 78.2 31.3 4.2 20.1 76.3 23.2 3.2 17.7 72.4
Pentesting Benchmarks
0 0 6.6 16.5
0.5
0 8.2 19.8
0 0 7.3 18.2
0.3
0 6.5 16.6
3.2 2.2 10.1 92.7
16 4.5 15 108.9
8.5 5.2 15.2 107.6
6.4 4.4 12.7 100
0.9 0.7 3.2 4.4
5.9 2.3 5.7 6.5 3.8
3 5.8 6.3
2.4 2.7 5.1 6.1
2.7
2 6.5 17.2
23 8.1 20.6 23.8
15 10.4 16.6 24.4
9.3 10.6 15.2 24.4
VI
LM M&S
N



Table 5: Acyclic planning. MaxProb geometric mean runtime (in CPU seconds). setup
presentation Table 4.
previously hinted, observations made clarity before.
Kolobov et al. (2011) report LRTDP beat VI MaxProb, consider single domain; experiment trivially initialized V U ; use dead-end pruning
VI, LRTDP already benefits smaller state space, impact heuristic search
remains unclear.
Even though search space heuristic search algorithms many cases small
fraction whole (dead-end pruned) state space, necessarily reflected runtime.
instances solved VI, typically fast, often faster heuristic search rarely
outperformed significantly. despite larger search spaces, i.e., heuristic search
visit less states suffers updates (recall VI updates
visited state exactly once). Significant runtime advantages VI (in NON-TRVIAL rows)
obtained heuristic search ExplodingBlocksb, Randomb, TriangleTireworldb.
Comparing heuristic search algorithms, conclusions fine-grained overall
similar concluded coverage above. LRTDP|U dominates AO |U almost throughout.
Note that, even though search space size AO |U LRTDP|U almost always similar, AO |U
requires lot time LRTDP|U . performs updates. Across nontrivial commonly solved instances tables, geometric mean number updates done
252

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

AO |U 4 times higher LRTDP|U . LRTDP|U HDP|U non-trivial
value initialization give similar results, terms coverage, terms
runtime search space size. LRTDP|U is, however, effective domains (e.g.,
RectangleTireworld Zenotravel) additional dead-end detection method used.
hand, HDP|U slight edge probabilistic resource-constrained domains. One notable
case LRTDP|U consistently outperforms HDP|U TriangleTireworld.
impact dead-end pruning VI typically moderate. gains heuristic search
much pronounced, thanks stronger heuristic function initialization. Especially AO |U
benefits lot. LRTDP|U HDP|U benefit well, smaller extent, partly
already effective first place. Comparing across different dead-end pruning methods,
although M&S N = clearly yields largest search space reductions, necessarily
recognizes dead-ends, overhead bisimulation computation outweighs search space
reduction cases. terms pruning power, M&S N = 100k LM-cut
heuristic overall roughly similar, yet LM-cut edge runtime.
7.2.2 (2) L EAST P ROB PPROX P ROB PARAMETER NALYSIS
turn weaker objectives, AtLeastProb ApproxProb. fix LM-cut (almost
always effective) dead-end pruning. examine power early termination different
search algorithms node selection strategies. best viewed function goal probability threshold AtLeastProb, desired goal probability accuracy ApproxProb. VI
forms baseline independent (). Consider Figure 5.
VI
LRTDP|U

850

VI

LRTDP|LU

AO |LU

HDP|U

HDP|LU

800

850
825
# solved instances

# solved instances

825

AO |L

AO |U

775
750
725
700
675

AO |L

AO |U

LRTDP|LU

AO |LU

HDP|U

HDP|LU

800
775
750
725
700
675

650

650

625

625
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


LRTDP|U

0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0


(a)
(b)
Figure 5: Acyclic planning. Total coverage AtLeastProb function (a), ApproxProb function (b). configurations use default node selection LM-cut
dead-end pruning.
AtLeastProb (Figure 5a), interesting region benchmark instances feasible
VI yet sometimes feasible search algorithms, one clear feature superiority
LRTDP AO HDP. one see smaller values , LRTDP able
update V L much effectively HDP, resulting larger coverage LRTDP region
253

fiS TEINMETZ & H OFFMANN & B UFFET

smaller values. AO |L exhibits strikingly strong behavior small values , approaching
(and one case, surpassing) performance LRTDP|U . Evidently, depth-first expansion
strategy quite effective anytime behavior V L thus termination via V L (I) .
way effective heuristic search AO |LU . shall see (Figure 7),
often effective LRTDP. general, algorithms, using V L clear advantage
small . larger , maintaining V L become burden, yet V U advantage due early
termination V U (I) < . Algorithms using bounds exhibit easy-hard-easy pattern.
spike left-hand side Figure 5 (a), i.e., significantly worse performance = 0.1
= 0.2, outlier due Pentest domains without domains, AO |LU ,
LRTDP|LU HDP|LU exhibit strict easy-hard-easy pattern. because, contrast typical probabilistic planning scenarios, penetration testing goal probability chance
successful attack typically small, indeed benchmarks. Searches using
upper bound quickly obtain V U (I) < 0.2, terminating early based V U (I) < = 0.2.
takes long time obtain V U (I) < 0.1.
ApproxProb (Figure 5b), smaller values consistently result worse performance.
see superiority LRTDP AO HDP, (relatively, compared AO |LU )
strong behavior AO |L regions allowing aggressive early termination. Again, key
LRTDP beating HDP clearly due LRTDP updating V L much effectively. HDP|LU
improve HDP|U small margin. Nonetheless, see superiority algorithms
using bounds dont.
7.2.3 (3) N ODE ELECTION TRATEGIES
Figure 6 shows different node selection strategies AtLeastProb (the relative performance node
selection strategies ApproxProb, include separate figure that).
LRTDP|LU (def)
AO |L (BFS)

850

# solved instances

825

AO |LU (def)
AO |LU (h)

AO |L (DFS)
AO |L (h)

AO |LU (o-prob)
HDP|LU (def)

VI

800
775
750
725
700
675
650
625
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


Figure 6: Acyclic planning. Total coverage AtLeastProb function , varying node
selection strategy. configurations use LM-cut dead-end pruning.
readability, show competitive base algorithms, AO |L , AO |LU , LRTDP|LU ,
HDP|LU (as well VI baseline). LRTDP HDP, show default node selection, consistently works basically well alternatives. AO |L , see
254

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

depth-first strategy important (and way beyond breadth-first, worse VI).
h-bias strategy marginally, consistently, better depth-first. AO |LU , h-bias
most-prob-outcome bias helpful, substantially improving default strategy.
h-bias consistently improves bit default AO . gap-bias preferred-actions strategies
shown consistently slightly worse (apparently, gap-bias leads
breadth-first style behavior, preferred actions mainly cause runtime overhead).
7.2.4 (4) N LLUSTRATION YPICAL NYTIME B EHAVIOR
conclude discussion acyclic planning, Figure 7 exemplifies typical anytime behavior, i.e.,
development V L (I) V U (I) bounds initial state value, function runtime,
LRTDP|LU AO |L .
1

LRTDP V U LM-cut

LRTDP V L

LRTDP V L LM-cut

AO |

AO |

L

1

L LM-cut

0.8
Probability

Probability

0.8

LRTDP V U

0.6
0.4
0.2

0.6
0.4
0.2

0

0
0

100

200
300
Time (s)

400

500

0

100

(a)

LRTDP V U

LRTDP V U LM-cut

LRTDP V L

LRTDP V L LM-cut

AO |L

AO |L LM-cut

200
300
Time (s)

400

500

(b)

Figure 7: Acyclic planning. Anytime behavior LRTDP|LU (V V L ) AO |L (V L only),
function runtime. Elevators instance 11, without pruning LM-cut pruning,
constrainedness level C = 1.4 (a) respectively C = 1.8 (b). Default node selection.
U

benefit LM-cut pruning evident. Observe AO |L way effective
LRTDP quickly improving lower bound. Indeed, runs shown find optimal policy
quickly. Across benchmarks solved AO |L LRTDP, omitting
took < 1 second, 56% cases AO |L finds optimal policy faster LRTDP. (geometric) average, AO |L takes 66% time taken LRTDP purpose. downside,
unless V (I) , AO |L must explore entire state space. runs Figure 7 exhaust memory
MaxProb. summary, heuristic search much stronger proving maximum goal
probability found, often distracting improving V L quickly.
parts Figure 7 use base instance different constrainedness levels C,
draw conclusions effect surplus budget. budget, actions
applied reaching absorbing states. adversely affects upper bound (consistently
across experiments), takes much longer time decrease. lower bound,
hand, often increases quickly higher C easier find goal states.
255

fiS TEINMETZ & H OFFMANN & B UFFET

7.3 Cyclic Planning FRET
consider cyclic planning, pertaining standard IPPC benchmarks, probabilistic
NoMystery, Rovers, TPP without budget (nor resource-) limit. run LRTDP DFHS,
AO restricted acyclic state spaces. use two different variants FRET described earlier:
FRET-V U per Kolobov et al. (2011), new variant FRET- U . consider 3 objectives,
4 dead-end pruning methods (as LM-cut returns iff cheaper heuristic hmax does,
use hmax here). vary node selection strategies because, seen before,
LRTDP DFHS bring notable advantage default strategy. use
deterministic-bisimulation (DB) reduced state space base algorithm, differences
emerge (in difference acyclic case) VI algorithms, need
run FRET. Again, given DB require dead-end pruning.
Overall, yields 305 different possible algorithm configurations. before,
interesting, instead organize experiment terms parts focusing issues
interest. Specifically, parts (1) MaxProb (2) AtLeastProb/ApproxProb before.
node selection strategies relevant here, previous part (3) considering
these. integrate data illustrating anytime behavior discussion (2). Table 6 gives
overview tested configurations.
Experiment
FRET variant
Search Algorithm
Pruning
# Configs
MaxProb search & prun(1)
65
, FRET-V U , FRET- U VI, LRTDP|U , DFHS (5) (4), DB
ing
(2)

VI,
AtLeastProb & ApproxU
U LRTDP| ,
,
FRET-V
,
FRET-
LU
Prob parameters
HDP|LU

LRTDP|U ,
HDP|U ,

hmax

18

Table 6: Overview algorithms tested cyclic problems, Section 7.3. Note VI require, hence combined with, FRET; denote (not using FRET all)
. (2), note number configurations gets multiplied 2 AtLeastProb
vs. ApproxProb result different algorithm configurations (using different termination
criteria). configurations tested use default node selection.
7.3.1 (1) EARCH LGORITHMS & P RUNING ETHODS AX P ROB
Table 7 shows coverage data. before, DFHS family shown top, remaining
search algorithms, including competitive DFHS algorithm HDP,
shown bottom. vary FRET variant top space reasons, as,
FRET-V U , coverage differences across DFHS family members.
Similarly acyclic case, DFHS configurations stopping exploration -inconsistent
states give slightly better results stopping absorbing states. termination
parameter almost effect coverage: HDP (i.e., DFHSFwdCons
) solves one task
Lab
ExplodingBlocks DFHSFwdCons
,

otherwise

coverage


same. akin
VI
acyclic case, LRTDP HDP perform equally well, though HDP slight edge
combination FRET- U .
Running search deterministic-bismulation state space less effective cyclic
benchmarks acyclic ones. gives clear advantage Rovers.
256

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING


Domain

#

Blocksworld
Boxworld
Drive
Elevators
ExplodingBlocks
Random
RectangleTireworld
Tireworld
Zenotravel
NoMystery
Rovers
TPP
P

Domain
Blocksworld
Boxworld
Drive
Elevators
ExplodingBlocks
Random
RectangleTireworld
Tireworld
Zenotravel
NoMystery
Rovers
TPP
P

15 4
15 0
15 4
15 15
15 5
15 6
14 14
15 15
15 3
10 4
10 9
10 8
164 87

#

FRET- U
DFHSVI |U
DFHSFwdCons
|U
DFHSFwd
HDP|U
VI
Lab |U
hmax M&S
hmax M&S hmax M&S hmax M&S
N BS
N BS
N BS
N BS
N
IPPC Benchmarks
4 4 4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0
15 15 6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6
15 15 5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5
12 5 4 4 5 12 5 4 4 5 14 5 4 4 5 12 5 4 4 5 15 5 4
6 1 0 0 6
6 2 0 0 6
6 1 0 0 6
6 2 0 0 6
6 1 0
14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14
15 15 12 11 15 15 15 12 11 15 15 15 12 11 15 15 15 12 11 15 15 15 12
3 3 1 1 3
3 3 1 1 3
3 3 1 1 3
3 3 1 1 3
3 3 1
Probabilistic Resource-Constrained Benchmarks
4 4 4 0 4
4 4 4 0 4
4 4 4 0 4
4 4 4 1 4
4 4 4
9 9 8 9 9
9 9 8 9 9
9 9 8 9 9
9 9 8 9 9
9 9 8
8 8 6 6 8
8 8 6 6 8
8 8 6 6 8
8 8 6 6 8
8 8 6
105 93 64 60 98 105 94 64 60 98 107 93 64 60 98 105 94 64 61 98 108 93 64
DFHSFwd
VI |U
hmax M&S

4
0
15
15
4
0
14
10
3

4
0
15
15
6
0
14
10
3

4
0
15
15
4
0
14
10
3

10 5
10 5
10 6
164 81

5
5
6
83

5
5
6
81

15
15
15
15
15
15
14
15
15

FRET-V U
FRET- U
LRTDP|U
HDP|U
LRTDP|U
HDP|U
hmax M&S hmax M&S hmax M&S hmax M&S
N DB
N DB
N DB
N
IPPC Benchmarks
4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4 4 4
4 4 4
0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0
6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6 6 15 15 15 6
5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5 5 15 15 15 5
4 4 4
6 4 4 4 4
6 4 4 4 5 14 5 4 4 5 15 5 4
0 0 0
0 0 0 0 0
0 0 0 0 4
4 0 0 0 6
6 1 0
14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14 14
10 11 10 11 10 10 11 10 10 10 10 11 15 15 15 12 11 15 15 15 12
1 0 3
3 3 1 1 3
3 3 1 1 3
3 3 1 1 3
3 3 1
Probabilistic Resource-Constrained Benchmarks
5 5 5
5 5 5 5 5
5 5 5 5 4
4 4 4 1 4
4 4 4
5 9 5
5 5 5 9 5
5 5 5 9 9
9 9 8 9 9
9 9 8
6 6 6
6 6 6 6 6
6 6 6 6 8
8 8 6 6 8
8 8 6
60 64 81 84 81 60 65 81 83 81 60 65 96 105 92 64 61 98 108 93 64

VI
hmax M&S
N DB


BS
4
0
6
5
4
0
14
11
1
1
9
6
61


DB
4
0
6
5
4
0
14
11
1
1
9
6
61

Table 7: Cyclic planning. MaxProb coverage. Best values, within table, boldface. FRETV U per Kolobov et al. (2011), FRET- U modified version. Top: DFHS variants
member DFHS family; DFHSVI ILAO );
(recall HDP DFHSFwdCons
Lab
showing dominating FRET version, FRET- U . Bottom: remaining search algorithms, varying FRET version, including overall best DFHS variant.
Dead-end pruning variants: none, else based heuristic value , hmax respectively merge-and-shrink (N size bound N = 100k, size bound). DB: run
reduced (deterministic-bisimulated) state space. Default node selection.
striking result far FRET- U outperforms VI FRET-V U substantially. Note that, domains except ExplodingBlocks Rovers, advantage VI
obtained even without dead-end pruning, i.e., trivial initialization V U . strongly confirms
power heuristic search even absence good admissible goal probability estimators.
before, shed additional light coverage results search space size runtime
data. Figure 8 compares search space sizes VI vs. FRET- U . non-trivial initialization
using hmax useful, gains 3 orders magnitude possible even without it.
Table 8 provides aggregate search space size runtime data. data shown configuration using FRET-V U HDP, data almost identical FRET-V U LRTDP:
257

fi107

107

106

106
FRET- U (hmax )

FRET- U

TEINMETZ & H OFFMANN & B UFFET

105
104
103

105
104
103
102

102
101 1
10

102

103

104
VI

105

106

101 1
10

107

102

103

104
105
VI (hmax )

106

107

Figure 8: Cyclic planning. Number states visited, VI (x) vs. FRET- U using LRTDP|U (y),
pruning (left) respectively hmax pruning (right).
FRET-V U
FRET- U
LRTDP|U
LRTDP|U
M&S
hmax
M&S
hmax
M&S

N

N

N

IPPC Benchmarks
2.8
2.7
0 0.1
2.7
2.8
0.1
0.1
3 2.8
0.1
6 42.8
0
0 5.8 33.1
0
0 6.1 42.8
0
2.2
1.7
0
0 2.2
1.8
0
0 2.2
1.9
0
19.3 18.1 15.9 0.4 31.5 17.7 15.7
0 28.8 17.5
3.5
45.1 110.4 82.2 0.8 112.4 102.6 72.9
0 104.1 110.1 14.3
4.7
4.7
3.9
4 4.7
4.7 19.8
4.1
4.7
4.7 20.4
9.1
9
7.6 7.9
9.1
9
53 8.1
9.1
9.2 55.5
14
35
60 55.3 60.2 86.4
0
0 3.8 24.7
0
19.5 55.7 92.1 84.5 89.8 136
0
0 4.7 39.8
0
96.3 283.8
7 12.6 54.3 241.1
0.2
0.2 43.5 227.4
0.2
Probabilistic Resource-Constrained Benchmarks
29.1 69.4 133.9 127 141.4 166.7 627.3 582.4 676.4 618.7 632.1
39.1 42.7 439.8 420.3 435.2 425.1
1.8
1.3
6.2
8.3
1.8
20.1 44.8 140.1 125.8 136.6 156.3 32.9 18.3 63.2 71.3 32.9
31.6 83.5 259.1 241 253.2 299.1 52.5 31.8 118.3 138.8 52.9
IPPC Benchmarks
1.1
1.1
1.1 1.1
1.1
1.1
1.1
1.1
1.1
1.1
1.1
0.3
0.3
0.3 0.2
0.2
0.2
0.3
0.2
0.2
0.2
0.3
0.9
0.9
0.9 0.9
0.9
0.9
0.2
0.2
0.2
0.2
0.2
252.3 39.9 408.5 20.1 242.2 16.9 44.3
0.2
14 0.1 44.4
2.0K 142.4 2.0K 34.6 2.0K 32.4 133.6
0.2 133.6
0.2 133.7
0
0
0.7 0.2
0
0
0.7
0.2
0
0
0.7
0
0
1 0.3
0
0
1 0.3
0
0
1
1.2K 1.2K 1.2K 974.7 974.7 974.7
0.5
0.2
0.2
0.2
2.4
1.7K 1.7K 1.7K 1.4K 1.4K 1.4K
0.4
0.2
0.2
0.2
2.7
309.3 309.3 309.3 309.3 309.3 309.3
2.7
2.7
2.7
2.7
2.7
Probabilistic Resource-Constrained Benchmarks
2.6K 2.6K 2.6K 2.6K 2.6K 2.6K 433 430.8 433 430.8 432.6
2.8K 2.8K 2.8K 2.8K 2.8K 2.8K 15.2 14.8 15.1 14.8 15.2
1.3K 1.3K 1.3K 1.3K 1.3K 1.3K 112.6 89.2 95.7 89.2 112.6
2.3K 2.3K 2.3K 2.3K 2.3K 2.3K 149.2 127.2 138.5 127.2 149.2

VI
hmax
Domain

#

Blocksworld
Drive
Elevators
ExplodingBlocks
NON-TRIVIAL
RectangleTireworld
NON-TRIVIAL
Tireworld
NON-TRIVIAL
Zenotravel

4
1
5
4
2
6
4
8
7
1

0
0
0
2.6
14.2
3.7
7.2
7.3
11
55.7

0
0
0
0.7
2.7
4
7.9
10.9
16.5
49.3

NoMystery
Rovers
TPP
NON-TRIVIAL

4
5
6
5

21.5
33.1
11.8
21.6

29.8
40.2
14.4
26.2

Blocksworld
Drive
Elevators
ExplodingBlocks
NON-TRIVIAL
RectangleTireworld
NON-TRIVIAL
Tireworld
NON-TRIVIAL
Zenotravel

4
1.1
1.1
1
0.3
0.3
5
0.9
0.9
4 408.5 46.5
2 2.0K 152.6
6
0.7
0.2
4
1
0.3
8 1.2K 1.2K
7 1.7K 1.7K
1 309.3 309.3

NoMystery
Rovers
TPP
NON-TRIVIAL

4
5
6
5

2.6K
2.8K
1.3K
2.3K

2.6K
2.8K
1.3K
2.3K

HDP|U
hmax
M&S
N

0.1
3.1
2.9
0 7.2 33.5
0 2.2
1.8
0 18.9 18.3
0 44.4 107.1
4.1
4.7
4.7
8.1
9.2
9.1
0 3.8 24.9
0 4.6 39.9
0.2
51 233.9
580.4 634.2 628.5
1.3
6.1
8.3
18.5 64.1 70.3
32.3 120.1 137.1
1.1
1.1
0.2
0.2
0.2
0.2
0.2
14
0.2 133.7
0.2
0
0.3
0
0.5
0.5
0.5
0.5
2.7
2.7

1.1
0.2
0.2
0.1
0.2
0
0
0.5
0.5
2.7

418.8 432.6 418.8
14.9 15.1 14.9
89.2 95.7 89.2
127.2 138.5 127.2

Table 8: Cyclic planning. Top: MaxProb geometric mean runtime (in CPU seconds). Bottom:
MaxProb geometric mean search space size (number states visited) multiples 1000.
Similar setup presentation Table 4: # gives size instance basis.
default commonly solved instances, skipping trivial ones. NON-TRIVIAL uses
instances solved VI < 1 second. (ONLY-H shown, see text.)

258

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

search space sizes exactly same, runtimes differ seconds. difference
Tables 4 5, include ONLY-H rows, would interesting here:
FRET-V U hardly solves instances VI, would excluded rows;
then, data would compare LRTDP vs. HDP, perform similarly anyway.
striking Table 8 consistency which, extent which, FRET- U visits
less states competitors (for LRTDP HDP). advantage typically yields better
runtimes well, notable exception NoMystery, larger number FRET iterations results substantial slow-down, despite much smaller search space: FRET-V U
LRTDP requires 11 FRET iterations average, NoMystery instances commonly
solved FRET- U LRTDP, latter configuration requires 20000 iterations average. Similarly using HDP.
impact dead-end pruning notably smaller acyclic case: search spaces
reduced substantially single domain, ExplodingBlocks. domains, either
reduction, minor/moderate one only.
VI (Kolobov)
VI
VI (hmax )
FRET-V U (Kolobov)
FRET-V U (hmax )
FRET- U (hmax )

104
103

106

Time (s)

States visited

108

105

104

102
101
100

102
1

2

3
4
Problem #

5

101

6

1

2

3
4
Problem #

5

6

(a)
(b)
Figure 9: Cyclic planning. Results ExplodingBlocks, shown Kolobov et al. (2011): FRET
vs VI, (a) number states visited, (b) runtime CPU seconds, function
IPPC instance index. Different variants included comparison. data Kolobov
et al. taken paper (as code available anymore), hence runtime
comparison modulo different computational platforms, treated
care. shown FRET configurations use LRTDP|U , default node selection.
ExplodingBlocks happens single domain Kolobov et al. (2011) experimented with.
Figure 9 provides detailed comparison Kolobov et al.s data, state art
measure provided previous work. use exact runtime/search space size data reported
Kolobov et al.; recall source code available anymore.
Kolobov et al. (2011) ran VI pruning vs. FRET-V U using LRTDP pruning based
SixthSense (Kolobov et al., 2010). observed coverage 4 former 6
latter, identical results VI vs. FRET-V U using LRTDP hmax . give
259

fiS TEINMETZ & H OFFMANN & B UFFET

detail, Figure 9 shows number states visited, total runtime, terms plots IPPC
instance index done Kolobov et al (2011).
Consider first Figure 9 (a), search space size. difference VI (Kolobov)
VI different task/state representation resulting respective implementation
framework, FD framework somewhat effective. substantially better performance
VI hmax dead-end pruning shows omission Kolobov et al.s (2011) study, using
dead-end pruning FRET VI, indeed obfuscates possible conclusions regarding
effect heuristic search vs. effect state pruning itself: hmax pruning, VI almost
effective FRET-V U using pruning. Kolobov et al.s FRET-V U close
this, except exploring significantly less states large instances. latter shows, especially
given effective representation FD, SixthSense stronger dead-end detector
hmax . hardly surprising, considering information sources SixthSense outcomes
(determinized) classical planning guidance, h2 (Graphplan) based validity tests.
hand, SixthSenses information sources much time-intensive hmax ,
presumably reason runtime picture Figure 9 (b). latter qualitatively
similar (a), except FRET-V U (Kolobov) significantly worse, rather better,
largest instance. last conclusion taken grain salt though, given different
computational environments.
Certainly, given clarity FRET- U advantage search space size runtime, one
conclude variant FRET substantially improves previous state art.
7.3.2 (2) L EAST P ROB PPROX P ROB PARAMETER NALYSIS
weaker objectives AtLeastProb ApproxProb, examine coverage function respectively . Figure 10 shows data.
FRET-V U , behavior Figure 10 similar acyclic case Figure 5.
particular, maintaining upper lower bound, FRET-V U exhibits easy-hardeasy pattern due advantages early termination.
FRET- U , though, curves flat , observation small advantage
using V L addition V U . due scaling benchmarks, combined extreme
performance loss point scaling: domain, instance number x
that, x, FRET- U solve instances completely (i.e., solving MaxProb), x
neither V L (I) V U (I) improved all, remaining 0 respectively 1 time/memory
limit. smaller instances, get expected anytime behavior. Figure 11 exemplifies this.
easy-hard-easy pattern would thus emerge smaller runtime/memory limits.14

8. Conclusion
Optimal goal probability analysis probabilistic planning notoriously hard problem,
extent amount work addressing limited. investigation contributes comprehensive design space known adapted algorithms addressing problem, designing several new
algorithm variants along way, establishing FD implementation basis supporting tight
integration MDP heuristic search classical planning techniques. experiments clarify
14. Figure 11 (b) considers largest instance feasible using hmax pruning. Figure 11 (a) considers secondlargest instance feasible without pruning: largest one feasible without pruning, namely instance 05, maximum goal probability 1 anytime curve V U interesting.

260

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

FRET-V U LRTDP|U

FRET- U LRTDP|U

FRET-V U LRTDP|U

FRET- U LRTDP|U

FRET-V U LRTDP|LU
FRET-V U HDP|U
FRET-V U HDP|LU
VI

FRET- U LRTDP|LU
FRET- U HDP|U
FRET- U HDP|LU

FRET-V U LRTDP|LU
FRET-V U HDP|U
FRET-V U HDP|LU
VI

FRET- U LRTDP|LU
FRET- U HDP|U
FRET- U HDP|LU

110
# solved instances

# solved instances

110

100

90

100

90

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1


0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0


(a)

(b)

1

1

0.8

0.8
Probability

Probability

Figure 10: Cyclic planning. Total coverage AtLeastProb function (a), ApproxProb function (b). configurations use default node selection hmax
dead-end pruning.

0.6
0.4

0.6
0.4

0.2

0.2

0

0
0

20

40
Time (s)

0

60

(a)

100

200
300
Time (s)

400

(b)
FRET- U

Figure 11: Cyclic planning. Anytime behavior
LRTDP|LU HDP|LU ,
default node selection, (a) without pruning ExplodingBlocks instance 04, (b)
hmax pruning instance 15.
empirical state art, exhibit substantial improvements thanks new techniques technique combinations. furthermore showcase opportunities arising naturally acyclic
problems, early termination criteria weaker maximum goal probability.
hope encouraging results new implementation basis inspire renewed
interest research important problem. many promising future directions,
would emphasize:
Advanced admissible goal probability estimators. could obtained, e.g. abstractions interpreted bounded-parameter MDPs (Givan, Leach, & Dean, 2000). promis261

fiS TEINMETZ & H OFFMANN & B UFFET

ing approach extend state-of-the-art classical-planning abstraction techniques pattern
databases (Edelkamp, 2001; Haslum, Botea, Helmert, Bonet, & Koenig, 2007), merge-andshrink (Helmert et al., 2014), Cartesian abstractions (Seipp & Helmert, 2013, 2014)
probabilistic setting.
Hybrids heuristic search Monte-Carlo tree search. appears promising option
improve anytime behavior, respect upper and/or lower bound, thus foster
early termination. Inspiration could taken existing hybrids, geared toward
purposes (Keller & Eyerich, 2012; Bonet & Geffner, 2012; Keller & Helmert, 2013).
Exploiting dominance relations. Goal probability higher dominating states,
raising opportunity prune dominated regions and/or transfer upper/lower bounds across
states. State domination ubiquitous limited-budget planning (and resource-constrained
planning). general domination relations shown exist many
classical planning problems (Torralba & Hoffmann, 2015), transfer techniques probabilistic case, via all-outcomes determinization, straightforward.
Last least, simulated penetration testing application worth algorithms research
right. basic idea exploit particular structure models, specifically
partially delete-relaxed behavior. characterizing property simulated penetration testing
action, applicable, remains applicable first executed (once attacker gets
position enabling exploit, exploit remains enabled). Hence, delete-relaxed planning,
find optimal solution, navely branch action every state ever after.
combat this, least three interesting directions. Following Pommerening Helmerts
(2012) methods computing h+ , different branching schemes might apply, challenge
maintain value function correctness. Following Gefen Brafmans (2012) methods computing h+ , partial-order reduction could adapted, challenge deal action
interference entailed shared budget. Finally, methods specific probabilistic setting may
apply: intuitively, preserve optimality, certain actions need attempted alternate
goal path failed. suggests identify, branch at, particular critical points along
search path.

Acknowledgments
work partially supported German Research Foundation (DFG), grant HO
2169/5-1, Critically Constrained Planning via Partial Delete Relaxation, well Federal Ministry Education Research (BMBF) funding Center IT-Security,
Privacy Accountability (CISPA) grant 16KIS0656. thank Christian Muise
Probabilistic-PDDL extension FD parser. thank Andrey Kolobov discussions.
thank anonymous reviewers, whose comments helped improve paper.

Appendix A. Depth-First Heuristic Search Cyclic Problems
pseudo-code family depth-first heuristic search algorithms (DFHS) general (cyclic)
probabilistic planning problems shown Figure 12.
262

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

procedure GoalProb-DFHS
:= {I};
loop
[early termination criteria exactly GoalProb-AO ]
(Label labeled solved)
(VI U changed running VI U -greedy graph)
index := 0
DFHS-Exploration(I)
set IDX visited states
clean stack visited
else return U endif /* regular termination */
endloop
procedure DFHS-Exploration(s):
6 Initialize(s) endif
S> labeled solved
label solved
return
endif
f lag :=
FW
V U (s) consistent f lag := > endif
update V U (s), U (s), V L (s), L (s)
Consist f lag return > endif
endif
s.IDX := index; s.lowlink := index
push onto stack; mark visited
index := index + 1
foreach P (s, U (s), t) > 0
t.IDX =
f lag := DFHS-Exploration(s) f lag
t.IDX < t.lowlink < s.lowlink s.lowlink := t.lowlink endif
else stack t.IDX < s.lowlink s.lowlink := t.IDX endif
done
f lag FW
V U (s) -consistent f lag := > endif
update V U (s), U (s), V L (s), L (s)
endif
Label f lag s.IDX = s.lowlink
forever
:= stack.pop()
label solved
= break endif
done
endif
return f lag

Figure 12: Depth-First Heuristic Search (DFHS) general (cyclic) MaxProb, AtLeastProb,
ApproxProb.

Appendix B. Landmarks Pruning: Admissible Heuristic vs. Budget Reduction
stated, Domshlak Mirkis (2015) problem reformulation, pruning states based global
budget reduced using disjunctive action landmarks, equivalent, regarding states pruned
method own, much simpler method using landmarks pruning
263

fiS TEINMETZ & H OFFMANN & B UFFET

remaining original budget. give argument, previously made unit costs
pairwise disjoint landmarks, general setting. assume classical planning setup
simplicity. arguments probabilistic oversubscription setups essentially same.
Assume STRIPS planning task = (F, A, I, G), action costs c(a) global
budget b. use notation following admissible landmark heuristics per Karpas Domshlak
(2009). Let L set disjunctive action landmarks I, i.e., every l L every
action sequence ~a leading goal, ~a touches l (there exists l used ~a). Let
furthermore
cp : L 7 R+
0 cost partitioning, i.e., function satisfying, A,
P
c(a, l)P
c(a). Denote h(l) := minal cp(a, l), subset L0 L landmarks
denote h(L0 ) := lL0 h(l). Intuitively, landmark l L assigned weight h(l) via cp,
admissible heuristic value h(L) obtained summing weights.
describe Domshlak Mirkis (2015) pruning technique terms. Domshlak
Mirkis formulation terms compilation planning language, complicated, equivalent formulation far pruning concerned.
Domshlak Mirkis technique maintains non-used landmarks part states. Namely,
state reached path ~a, l L non-used iff ~a touch l. denote set nonused landmarks L(s). Obviously, l L(s) landmarks s. Note that, L(s)
part state, even two search paths lead end state use different landmarks,
end states considered different. restriction arises compilation approach,
book-keeping landmarks must happen inside language, i.e., inside states. One
could formulate pruning technique without restriction; get back below.
pruning technique arises interplay reduced global budget reduced
action costs depending non-used landmarks. Define reduced global budget b0 := b h(L).
action a, denote L(a) set landmarks participates in, i.e., L(a) := {l | l L,
l}. state search, applicable action a, transition t[[a]]
reduced cost, namely cost c(a) h(L(a) L(t)). words, reduce cost
(summed-up) weight non-used landmarks participates in.
Consider state search. Denote remaining reduced budget b0 (s).
Say prune iff b0 (s) < 0.15 Consider path ~a ending s. non-used landmarks
part state, paths must touch
P subset landmarks L, namely
L \ L(s). Denote actual cost ~a c(~a) := a~a c(a). Relative cost, cost saved
thanks cost reduction exactly h(L \ L(s)), weight touched landmarks. Therefore,
b0 (s) = b0 (c(~
a) h(L \ L(s))) =P(b h(L)) c(~a) + h(L \ L(s)). P
definition h,
P
equals (b h(l)) c(~a) + lL\L(s) h(l), equals b c(~a) lL(s) h(l) =
b c(~a) h(L(s)). Thus, pruned, b0 (s) < 0, iff b c(~a) < h(L(s)). latter condition
b(s) < h(L(s)), exactly pruning condition resulting using h(L(s))
admissible heuristic function pruning remaining budget.
non-compilation setting, one could, indeed customary admissible landmark heuristics, handle landmarks path-dependent manner. is, non-used landmarks maintained
15. Domshlak Mirkis (2015) maintain remaining budget part state, instead prune g(s) >
b0 . is, obviously, equivalent, except duplicate detection powerful compares states based
facts F (s) only. purpose discussion here, make difference. Note that,
probabilistic setting, distinguish states based F (s) b(s), goal probability depends
maintaining best way reaching F (s) suffice compute exact goal probability
initial state.

264

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

annotations states rather part them, multiple search paths may end state
use different landmarks. set remaining landmarks L(s) union
individual path; is, l L non-used iff exists least one path
touch l. still suffices show l landmark s. landmark heuristic approach per Karpas Domshlak (2009) kind book-keeping, uses admissible
heuristic value h(L(s)).
one apply Domshlak Mirkis (2015) reformulation technique without maintaining
landmarks part state, notion transition-cost reduction would become
complicated (lest one loses information). because, reached a~1 reduced
cost due touching landmark l1 , later find another path a~2 touch l1 ,
l1 actually still valid landmark s, therefore need reduce cost
a~1 . account this, would revise path costs posthoc, every time new path
becomes available. revisions, cost reduction path ~a exactly
h(L \ L(s)): weight non-used landmarks L(s) longer subtracted, weight
landmarks L \ L(s) subtracted every ~a because, definition, every ~a touches every
l L \ L(s). cost saved every path ~a s, relative ~a, exactly h(L \ L(s)),
point arguments apply show pruning equivalent pruning
via b(s) < h(L(s)). (This stronger pruning method would get without posthoc
path cost revision.)
summary, based reduced remaining budget b0 (s) < 0 equivalent pruning based
original remaining budget vs. landmark heuristic b(s) < h(L(s)). noted, though,
pruning benefit Domshlak Mirkis (2015) reformulation technique.
technique allows compute another, complementary, admissible heuristic h reformulated task 0 (and Domshlak Mirkis point part motivation,
practice). perspective here, landmark heuristic h used additively
admissible pruning remaining budget, additivity achieved method
generalizing cost partitionings: 0 , cost-reduced variant action applied
once. h abstract away constraint, h uses action twice, employs
reduced cost once, yet pays full cost second time. Exploring kind generalized
cost partitioning detail interesting research line future work.

References
Altman, E. (1999). Constrained Markov Decision Processes. CRC Press.
Baier, C., Groer, M., Leucker, M., Bollig, B., & Ciesinski, F. (2004). Controller Synthesis
Probabilistic Systems (Extended Abstract), pp. 493506. Springer US, Boston, MA.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.
Bertsekas, D. (1995). Dynamic Programming Optimal Control, (2 Volumes). Athena Scientific.
Bertsekas, D., & Tsitsiklis, J. (1996). Neurodynamic Programming. Athena Scientific.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),
533.
265

fiS TEINMETZ & H OFFMANN & B UFFET

Bonet, B., & Geffner, H. (2003a). Faster heuristic search algorithms planning uncertainty
full feedback. Gottlob, G. (Ed.), Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI03), pp. 12331238, Acapulco, Mexico. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2003b). Labeled RTDP: Improving convergence real-time dynamic
programming. Giunchiglia, E., Muscettola, N., & Nau, D. (Eds.), Proceedings 13th
International Conference Automated Planning Scheduling (ICAPS03), pp. 1221,
Trento, Italy. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2005). mgpt: probabilistic planner based heuristic search. Journal
Artificial Intelligence Research, 24, 933944.
Bonet, B., & Geffner, H. (2006). Learning depth-first search: unified approach heuristic search
deterministic non-deterministic settings, application MDPs. Long, D., &
Smith, S. (Eds.), Proceedings 16th International Conference Automated Planning
Scheduling (ICAPS06), pp. 142151, Ambleside, UK. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2012). Action selection MDPs: Anytime AO* versus UCT. Hoffmann, J., & Selman, B. (Eds.), Proceedings 26th AAAI Conference Artificial Intelligence (AAAI12), Toronto, ON, Canada. AAAI Press.
Bryce, D., & Buffet, O. (2008). 6th international planning competition: Uncertainty part. Proceedings 6th International Planning Competition (IPC08).
Camacho, A., Muise, C., & McIlraith, S. A. (2016). FOND robust probabilistic planning: Computing compact policies bypass avoidable deadends. Coles, A., Coles, A.,
Edelkamp, S., Magazzeni, D., & Sanner, S. (Eds.), Proceedings 26th International
Conference Automated Planning Scheduling (ICAPS16). AAAI Press.
Chatterjee, K., Chmelik, M., Gupta, R., & Kanodia, A. (2015). Optimal cost almost-sure reachability POMDPs. Bonet, B., & Koenig, S. (Eds.), Proceedings 29th AAAI Conference
Artificial Intelligence (AAAI15), pp. 34963502. AAAI Press.
Chatterjee, K., Chmelik, M., Gupta, R., & Kanodia, A. (2016). Optimal cost almost-sure reachability POMDPs. Artificial Intelligence, 234, 2648.
Coles, A. J. (2012). Opportunistic branched plans maximise utility presence resource
uncertainty. Raedt, L. D. (Ed.), Proceedings 20th European Conference Artificial
Intelligence (ECAI12), pp. 252257, Montpellier, France. IOS Press.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2013). hybrid LP-RPG heuristic modelling
numeric resource flows planning. Journal Artificial Intelligence Research, 46, 343412.
Coles, A. J., Coles, A., Garca Olaya, A., Jimenez, S., Linares Lopez, C., Sanner, S., & Yoon, S.
(2012). survey seventh international planning competition. AI Magazine, 33(1).
Dai, P., Mausam, Weld, D. S., & Goldsmith, J. (2011). Topological value iteration algorithms.
Journal Artificial Intelligence Research, 42, 181209.
Dean, T. L., & Givan, R. (1997). Model minimization markov decision processes. Kuipers,
B. J., & Webber, B. (Eds.), Proceedings 14th National Conference American
Association Artificial Intelligence (AAAI97), pp. 106111, Portland, OR. MIT Press.
266

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

Domshlak, C., & Mirkis, V. (2015). Deterministic oversubscription planning heuristic search:
Abstractions reformulations. Journal Artificial Intelligence Research, 52, 97169.
Drager, K., Finkbeiner, B., & Podelski, A. (2009). Directed model checking distancepreserving abstractions. International Journal Software Tools Technology Transfer,
11(1), 2737.
Edelkamp, S. (2001). Planning pattern databases. Cesta, A., & Borrajo, D. (Eds.), Proceedings 6th European Conference Planning (ECP01), pp. 1324. Springer-Verlag.
Gefen, A., & Brafman, R. I. (2012). Pruning methods optimal delete-free planning. Bonet,
B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings 22nd International
Conference Automated Planning Scheduling (ICAPS12). AAAI Press.
Givan, R., Leach, S. M., & Dean, T. (2000). Bounded-parameter Markov decision processes. Artificial Intelligence, 122(1-2), 71109.
Hansen, E. A., & Zilberstein, S. (2001). LAO* : heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129(1-2), 3562.
Haslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent construction pattern database heuristics cost-optimal planning. Howe, A., & Holte,
R. C. (Eds.), Proceedings 22nd National Conference American Association
Artificial Intelligence (AAAI07), pp. 10071012, Vancouver, BC, Canada. AAAI Press.
Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. Cesta, A., &
Borrajo, D. (Eds.), Proceedings 6th European Conference Planning (ECP01), pp.
121132. Springer-Verlag.
Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence Research, 26, 191246.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats difference anyway?. Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings
19th International Conference Automated Planning Scheduling (ICAPS09), pp.
162169. AAAI Press.
Helmert, M., Haslum, P., Hoffmann, J., & Nissim, R. (2014). Merge & shrink abstraction: method
generating lower bounds factored state spaces. Journal Association Computing Machinery, 61(3).
Hoffmann, J. (2015). Simulated penetration testing: Dijkstra Turing Test++. Brafman, R., Domshlak, C., Haslum, P., & Zilberstein, S. (Eds.), Proceedings 25th International Conference Automated Planning Scheduling (ICAPS15). AAAI Press.
Hoffmann, J., Kissmann, P., & Torralba, A. (2014). Distance? Cares? Tailoring merge-andshrink heuristics detect unsolvability. Schaub, T. (Ed.), Proceedings 21st European
Conference Artificial Intelligence (ECAI14), Prague, Czech Republic. IOS Press.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Hou, P., Yeoh, W., & Varakantham, P. (2014). Revisiting risk-sensitive MDPs: New algorithms
results. Chien, S., Do, M., Fern, A., & Ruml, W. (Eds.), Proceedings 24th
International Conference Automated Planning Scheduling (ICAPS14). AAAI Press.
267

fiS TEINMETZ & H OFFMANN & B UFFET

Jimenez, S., Coles, A., & Smith, A. (2006). Planning probabilistic domains using deterministic
numeric planner. Proceedings 25th Workshop UK Planning Scheduling
Special Interest Group (PlanSig06).
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning landmarks. Boutilier, C. (Ed.),
Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI09),
pp. 17281733, Pasadena, California, USA. Morgan Kaufmann.
Katz, M., Hoffmann, J., & Helmert, M. (2012). relax bisimulation?. Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings 22nd International Conference Automated Planning Scheduling (ICAPS12), pp. 101109. AAAI Press.
Keller, T., & Eyerich, P. (2012). PROST: Probabilistic planning based UCT. Bonet, B.,
McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings 22nd International
Conference Automated Planning Scheduling (ICAPS12). AAAI Press.
Keller, T., & Helmert, M. (2013). Trial-based heuristic tree search finite horizon MDPs.
Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings 23rd International Conference Automated Planning Scheduling (ICAPS13), Rome, Italy. AAAI
Press.
Kolobov, A. (2013). Scalable Methods Expressive Models Planning Uncertainty.
Ph.D. thesis, University Washington.
Kolobov, A., Mausam, & Weld, D. S. (2010). Sixthsense: Fast reliable recognition dead ends
MDPs. Fox, M., & Poole, D. (Eds.), Proceedings 24th National Conference
American Association Artificial Intelligence (AAAI10), Atlanta, GA, USA. AAAI Press.
Kolobov, A., Mausam, & Weld, D. S. (2012). theory goal-oriented MDPs dead ends.
de Freitas, N., & Murphy, K. P. (Eds.), Proceedings 28th Conference Uncertainty
Artificial Intelligence (UAI12), pp. 438447, Catalina Island, CA, USA. AUAI Press.
Kolobov, A., Mausam, Weld, D. S., & Geffner, H. (2011). Heuristic search generalized stochastic
shortest path MDPs. Bacchus, F., Domshlak, C., Edelkamp, S., & Helmert, M. (Eds.),
Proceedings 21st International Conference Automated Planning Scheduling
(ICAPS11). AAAI Press.
Kurniawati, H., Hsu, D., & Lee, W. S. (2008). SARSOP: Efficient point-based POMDP planning
approximating optimally reachable belief spaces. Robotics: Science Systems IV.
Kuter, U., & Hu, J. (2007). Computing using lower upper bounds action elimination
MDP planning. Miguel, I., & Ruml, W. (Eds.), Proceedings 7th International Symposium Abstraction, Reformulation, Approximation (SARA-07), Vol. 4612 Lecture
Notes Computer Science, Whistler, Canada. Springer-Verlag.
Kwiatkowska, M., Parker, D., & Qu, H. (2011a). Incremental quantitative verification markov
decision processes. 2011 IEEE/IFIP 41st International Conference Dependable Systems
Networks (DSN), pp. 359370.
Kwiatkowska, M. Z., Norman, G., & Parker, D. (2011b). Prism 4.0: Verification probabilistic
real-time systems. Gopalakrishnan, G., & Qadeer, S. (Eds.), Proceedings 23rd International Conference Computer Aided Verification (CAV11), Vol. 6806 Lecture Notes
Computer Science, pp. 585591. Springer.
268

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

Little, I., Aberdeen, D., & Thiebaux, S. (2005). Prottle: probabilistic temporal planner. Veloso,
M. M., & Kambhampati, S. (Eds.), Proceedings 20th National Conference American Association Artificial Intelligence (AAAI05), pp. 11811186, Pittsburgh, Pennsylvania, USA. AAAI Press.
Little, I., & Thiebaux, S. (2007). Probabilistic planning vs replanning. ICAPS Workshop
International Planning Competition: Past, Present Future.
Marecki, J., & Tambe, M. (2008). Towards faster planning continuous resources stochastic
domains. Fox, D., & Gomes, C. (Eds.), Proceedings 23rd National Conference
American Association Artificial Intelligence (AAAI08), pp. 10491055, Chicago, Illinois,
USA. AAAI Press.
McMahan, H. B., Likhachev, M., & Gordon, G. J. (2005). Bounded real-time dynamic programming: RTDP monotone upper bounds performance guarantees. Proceedings
22nd International Conference Machine Learning (ICML-05).
Meuleau, N., Benazera, E., Brafman, R. I., Hansen, E. A., & Mausam, M. (2009). heuristic search
approach planning continuous resources stochastic domains. Journal Artificial
Intelligence Research, 34(1), 2759.
Milner, R. (1990). Operational algebraic semantics concurrent processes. van Leeuwen, J.
(Ed.), Handbook Theoretical Computer Science, Volume B: Formal Models Sematics,
pp. 12011242. Elsevier MIT Press.
Muise, C. J., McIlraith, S. A., & Beck, J. C. (2012). Improved non-deterministic planning
exploiting state relevance. Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.),
Proceedings 22nd International Conference Automated Planning Scheduling
(ICAPS12). AAAI Press.
Nakhost, H., Hoffmann, J., & Muller, M. (2012). Resource-constrained planning: monte carlo
random walk approach. Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.),
Proceedings 22nd International Conference Automated Planning Scheduling
(ICAPS12), pp. 181189. AAAI Press.
Nilsson, N. J. (1971). Problem Solving Methods Artificial Intelligence. McGraw-Hill.
Nissim, R., Hoffmann, J., & Helmert, M. (2011). Computing perfect heuristics polynomial time:
bisimulation merge-and-shrink abstraction optimal planning. Walsh, T. (Ed.),
Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI11),
pp. 19831990. AAAI Press/IJCAI.
Pommerening, F., & Helmert, M. (2012). Optimal planning delete-free tasks incremental
lm-cut. Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings
22nd International Conference Automated Planning Scheduling (ICAPS12). AAAI
Press.
Richter, S., & Helmert, M. (2009). Preferred operators deferred evaluation satisficing planning. Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings 19th
International Conference Automated Planning Scheduling (ICAPS09), pp. 273280.
AAAI Press.
269

fiS TEINMETZ & H OFFMANN & B UFFET

Sanner, S. (2010). Relational dynamic influence diagram language (rddl): Language description.
Available http://users.cecs.anu.edu.au/ssanner/IPPC_2011/RDDL.
pdf.
Santana, P., Thibaux, S., & Williams, B. (2016). RAO*: algorithm chance-constrained
POMDPs. Schuurmans, D., & Wellman, M. (Eds.), Proceedings 30th AAAI Conference Artificial Intelligence (AAAI16), pp. 33083314. AAAI Press.
Sarraute, C., Buffet, O., & Hoffmann, J. (2012). POMDPs make better hackers: Accounting
uncertainty penetration testing. Hoffmann, J., & Selman, B. (Eds.), Proceedings
26th AAAI Conference Artificial Intelligence (AAAI12), pp. 18161824, Toronto, ON,
Canada. AAAI Press.
Seipp, J., & Helmert, M. (2013). Counterexample-guided Cartesian abstraction refinement.
Borrajo, D., Fratini, S., Kambhampati, S., & Oddi, A. (Eds.), Proceedings 23rd International Conference Automated Planning Scheduling (ICAPS13), pp. 347351,
Rome, Italy. AAAI Press.
Seipp, J., & Helmert, M. (2014). Diverse additive cartesian abstraction heuristics. Chien, S.,
Do, M., Fern, A., & Ruml, W. (Eds.), Proceedings 24th International Conference
Automated Planning Scheduling (ICAPS14). AAAI Press.
Smith, T., & Simmons, R. G. (2006). Focused real-time dynamic programming MDPs: Squeezing heuristic. Gil, Y., & Mooney, R. J. (Eds.), Proceedings 21st
National Conference American Association Artificial Intelligence (AAAI06), pp.
12271232, Boston, Massachusetts, USA. AAAI Press.
Steinmetz, M., Hoffmann, J., & Buffet, O. (2016). Revisiting goal probability analysis probabilistic planning. Coles, A., Coles, A., Edelkamp, S., Magazzeni, D., & Sanner, S. (Eds.),
Proceedings 26th International Conference Automated Planning Scheduling
(ICAPS16). AAAI Press.
Tarjan, R. E. (1972). Depth first search linear graph algorithms. SIAM Journal Computing,
1(2), 146160.
Teichteil-Konigsbuch, F. (2012). Stochastic safest shortest path problems. Hoffmann, J.,
& Selman, B. (Eds.), Proceedings 26th AAAI Conference Artificial Intelligence
(AAAI12), Toronto, ON, Canada. AAAI Press.
Teichteil-Konigsbuch, F., Kuter, U., & Infantes, G. (2010). Incremental plan aggregation generating policies MDPs. van der Hoek, W., Kaminka, G. A., Lesperance, Y., Luck, M., &
Sen, S. (Eds.), Proceedings 9th International Conference Autonomous Agents
Multiagent Systems (AAMAS10), pp. 12311238. IFAAMAS.
Teichteil-Konigsbuch, F., Vidal, V., & Infantes, G. (2011). Extending classical planning heuristics
probabilistic planning dead-ends. Burgard, W., & Roth, D. (Eds.), Proceedings
25th National Conference American Association Artificial Intelligence (AAAI11),
San Francisco, CA, USA. AAAI Press.
Torralba, A., & Hoffmann, J. (2015). Simulation-based admissible dominance pruning. Yang,
Q. (Ed.), Proceedings 24th International Joint Conference Artificial Intelligence
(IJCAI15), pp. 16891695. AAAI Press/IJCAI.
270

fiG OAL P ROBABILITY NALYSIS P ROBABILISTIC P LANNING

Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning.
Boddy, M., Fox, M., & Thiebaux, S. (Eds.), Proceedings 17th International Conference
Automated Planning Scheduling (ICAPS07), pp. 352359, Providence, Rhode Island,
USA. Morgan Kaufmann.
Younes, H. L. S., Littman, M. L., Weissman, D., & Asmuth, J. (2005). first probabilistic track
international planning competition. Journal Artificial Intelligence Research, 24,
851887.

271


