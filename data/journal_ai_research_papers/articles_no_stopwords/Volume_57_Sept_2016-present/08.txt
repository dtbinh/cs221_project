Journal Artificial Intelligence Research 57 (2016) 345420

Submitted 9/15; published 11/16

Primer Neural Network Models
Natural Language Processing
Yoav Goldberg

yoav.goldberg@gmail.com

Computer Science Department
Bar-Ilan University, Israel

Abstract
past years, neural networks re-emerged powerful machine-learning
models, yielding state-of-the-art results fields image recognition speech
processing. recently, neural network models started applied textual
natural language signals, promising results. tutorial surveys neural
network models perspective natural language processing research, attempt
bring natural-language researchers speed neural techniques. tutorial
covers input encoding natural language tasks, feed-forward networks, convolutional
networks, recurrent networks recursive networks, well computation graph
abstraction automatic gradient computation.

1. Introduction
decade, core NLP techniques dominated machine-learning approaches
used linear models support vector machines logistic regression, trained
high dimensional yet sparse feature vectors.
Recently, field seen success switching linear models
sparse inputs non-linear neural-network models dense inputs.
neural network techniques easy apply, sometimes almost drop-in replacements
old linear classifiers, many cases strong barrier entry. tutorial
attempt provide NLP practitioners (as well newcomers) basic background,
jargon, tools methodology allow understand principles behind
neural network models apply work. tutorial expected
self-contained, presenting different approaches unified notation
framework. repeats lot material available elsewhere. points
external sources advanced topics appropriate.
primer intended comprehensive resource go
develop next advances neural-network machinery (though may serve good entry
point). Rather, aimed readers interested taking existing, useful
technology applying useful creative ways favourite NLP problems.
in-depth, general discussion neural networks, theory behind them, advanced
optimization methods advanced topics, reader referred existing
resources. particular, book Bengio, Goodfellow, Courville (2015) highly
recommended.
c
2016
AI Access Foundation. rights reserved.

fiGoldberg

1.1 Scope
focus applications neural networks language processing tasks. However,
subareas language processing neural networks deliberately left
scope tutorial. include vast literature language modeling acoustic
modeling, use neural networks machine translation, multi-modal applications
combining language signals images videos (e.g. caption generation).
Caching methods efficient runtime performance, methods efficient training large
output vocabularies attention models discussed. Word embeddings
discussed extent needed understand order use inputs
models. unsupervised approaches, including autoencoders recursive
autoencoders, fall scope. applications neural networks language
modeling machine translation mentioned text, treatment means
comprehensive.
1.2 Note Terminology
word feature used refer concrete, linguistic input word, suffix,
part-of-speech tag. example, first-order part-of-speech tagger, features might
current word, previous word, next word, previous part speech. term input
vector used refer actual input fed neural-network classifier.
Similarly, input vector entry refers specific value input. contrast
lot neural networks literature word feature overloaded
two uses, used primarily refer input-vector entry.
1.3 Mathematical Notation
use bold upper case letters represent matrices (X, Y, Z), bold lower-case letters
represent vectors (b). series related matrices vectors (for example,
matrix corresponds different layer network), superscript indices
used (W1 , W2 ). rare cases want indicate power matrix
vector, pair brackets added around item exponentiated: (W)2 , (W3 )2 .
Unless otherwise stated, vectors assumed row vectors. use [v1 ; v2 ] denote
vector concatenation.
choice use row vectors, right multiplied matrices (xW + b)
somewhat non standard lot neural networks literature use column vectors
left multiplied matrices (Wx + b). trust reader able adapt
column vectors notation reading literature.1

1. choice use row vectors notation inspired following benefits: matches way
input vectors network diagrams often drawn literature; makes hierarchical/layered
structure network transparent puts input left-most variable rather
nested; results fully-connected layer dimensions din dout rather dout din ; maps
better way networks implemented code using matrix libraries numpy.

346

fiA Primer Neural Networks NLP

2. Neural Network Architectures
Neural networks powerful learning models. discuss two kinds neural network
architectures, mixed matched feed-forward networks recurrent /
recursive networks. Feed-forward networks include networks fully connected layers,
multi-layer perceptron, well networks convolutional pooling
layers. networks act classifiers, different strengths.
Fully connected feed-forward neural networks (Section 4) non-linear learners
can, part, used drop-in replacement wherever linear learner used.
includes binary multiclass classification problems, well complex structured prediction problems (Section 8). non-linearity network, well
ability easily integrate pre-trained word embeddings, often lead superior classification accuracy. series works2 managed obtain improved syntactic parsing results
simply replacing linear model parser fully connected feed-forward network. Straight-forward applications feed-forward network classifier replacement
(usually coupled use pre-trained word vectors) provide benefits CCG
supertagging,3 dialog state tracking,4 pre-ordering statistical machine translation5
language modeling.6 Iyyer, Manjunatha, Boyd-Graber, Daume III (2015) demonstrate
multi-layer feed-forward networks provide competitive results sentiment classification factoid question answering.
Networks convolutional pooling layers (Section 9) useful classification
tasks expect find strong local clues regarding class membership,
clues appear different places input. example, document classification
task, single key phrase (or ngram) help determining topic document
(Johnson & Zhang, 2015). would learn certain sequences words good
indicators topic, necessarily care appear document.
Convolutional pooling layers allow model learn find local indicators,
regardless position. Convolutional pooling architecture show promising results
many tasks, including document classification,7 short-text categorization,8 sentiment
classification,9 relation type classification entities,10 event detection,11 paraphrase
identification,12 semantic role labeling,13 question answering,14 predicting box-office rev-

2. Chen Manning (2014), Weiss, Alberti, Collins, Petrov (2015) Pei, Ge, Chang (2015)
Durrett Klein (2015)
3. Lewis Steedman (2014)
4. Henderson, Thomson, Young (2013)
5. de Gispert, Iglesias, Byrne (2015)
6. Bengio, Ducharme, Vincent, Janvin (2003) Vaswani, Zhao, Fossum, Chiang (2013)
7. Johnson Zhang (2015)
8. Wang, Xu, Xu, Liu, Zhang, Wang, Hao (2015a)
9. Kalchbrenner, Grefenstette, Blunsom (2014) Kim (2014)
10. Zeng, Liu, Lai, Zhou, Zhao (2014), dos Santos, Xiang, Zhou (2015)
11. Chen, Xu, Liu, Zeng, Zhao (2015), Nguyen Grishman (2015)
12. Yin Schutze (2015)
13. Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa (2011)
14. Dong, Wei, Zhou, Xu (2015)

347

fiGoldberg

enues movies based critic reviews,15 modeling text interestingness,16 modeling
relation character-sequences part-of-speech tags.17
natural language often work structured data arbitrary sizes,
sequences trees. would able capture regularities structures,
model similarities structures. many cases, means encoding
structure fixed width vector, pass another statistical
learner processing. convolutional pooling architectures allow us
encode arbitrary large items fixed size vectors capturing salient features,
sacrificing structural information. Recurrent (Section 10)
recursive (Section 12) architectures, hand, allow us work sequences
trees preserving lot structural information. Recurrent networks (Elman,
1990) designed model sequences, recursive networks (Goller & Kuchler, 1996)
generalizations recurrent networks handle trees. discuss
extension recurrent networks allow model stacks (Dyer, Ballesteros, Ling,
Matthews, & Smith, 2015; Watanabe & Sumita, 2015).
Recurrent models shown produce strong results language modeling,18 ; well sequence tagging,19 machine translation,20 dependency parsing,21
sentiment analysis,22 noisy text normalization,23 dialog state tracking,24 response generation,25 modeling relation character sequences part-of-speech tags.26
Recursive models shown produce state-of-the-art near state-of-the-art results
constituency27 dependency28 parse re-ranking, discourse parsing,29 semantic relation
classification,30 political ideology detection based parse trees,31 sentiment classification,32
target-dependent sentiment classification33 question answering.34
15.
16.
17.
18.

19.
20.

21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.

Bitvai Cohn (2015)
Gao, Pantel, Gamon, He, Deng (2014)
dos Santos Zadrozny (2014)
notable works Mikolov, Karafiat, Burget, Cernocky, Khudanpur (2010), Mikolov,
Kombrink, Lukas Burget, Cernocky, Khudanpur (2011), Mikolov (2012), Duh, Neubig, Sudoh,
Tsukada (2013), Adel, Vu, Schultz (2013), Auli, Galley, Quirk, Zweig (2013) Auli Gao
(2014)
Irsoy Cardie (2014), Xu, Auli, Clark (2015), Ling, Dyer, Black, Trancoso, Fermandez, Amir,
Marujo, Luis (2015b)
Sundermeyer, Alkhouli, Wuebker, Ney (2014), Tamura, Watanabe, Sumita (2014), Sutskever,
Vinyals, Le (2014) Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares, Schwenk, Bengio
(2014b)
Dyer et al. (2015), Watanabe Sumita (2015)
Wang, Liu, Sun, Wang, Wang (2015b)
Chrupala (2014)
Mrksic, Seaghdha, Thomson, Gasic, Su, Vandyke, Wen, Young (2015)
Sordoni, Galley, Auli, Brockett, Ji, Mitchell, Nie, Gao, Dolan (2015)
Ling et al. (2015b)
Socher, Bauer, Manning, Ng (2013)
Le Zuidema (2014), Zhu, Qiu, Chen, Huang (2015a)
Li, Li, Hovy (2014)
Hashimoto, Miwa, Tsuruoka, Chikayama (2013), Liu, Wei, Li, Ji, Zhou, Wang (2015)
Iyyer, Enns, Boyd-Graber, Resnik (2014b)
Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts (2013), Hermann Blunsom (2013)
Dong, Wei, Tan, Tang, Zhou, Xu (2014)
Iyyer, Boyd-Graber, Claudino, Socher, Daume III (2014a)

348

fiA Primer Neural Networks NLP

3. Feature Representation
discussing network structure depth, important pay attention
features represented. now, think feed-forward neural network
function NN(x) takes input din dimensional vector x produces dout
dimensional output vector. function often used classifier, assigning input x
degree membership one dout classes. function complex,
almost always non-linear. Common structures function discussed Section 4.
Here, focus input, x. dealing natural language, input x encodes
features words, part-of-speech tags linguistic information. Perhaps
biggest conceptual jump moving sparse-input linear models neural-network
based models stop representing feature unique dimension (the called
one-hot representation) representing instead dense vectors. is, core
feature embedded dimensional space, represented vector space.35
embeddings (the vector representation core feature) trained
parameter function NN. Figure 1 shows two approaches feature
representation.
feature embeddings (the values vector entries feature) treated
model parameters need trained together components
network. Methods training (or obtaining) feature embeddings discussed later.
now, consider feature embeddings given.
general structure NLP classification system based feed-forward neural
network thus:
1. Extract set core linguistic features f1 , . . . , fk relevant predicting
output class.
2. feature interest, retrieve corresponding vector v(fi ).
3. Combine vectors (either concatenation, summation combination both)
input vector x.
4. Feed x non-linear classifier (feed-forward neural network).
biggest change input, then, move sparse representations
feature dimension, dense representation feature mapped
vector. Another difference extract core features feature combinations. elaborate changes briefly.
3.1 Dense Vectors vs. One-Hot Representations
benefits representing features vectors instead unique IDs?
always represent features dense vectors? Lets consider two kinds
representations:

35. Different feature types may embedded different spaces. example, one may represent word
features using 100 dimensions, part-of-speech features using 20 dimensions.

349

fiGoldberg

Figure 1: Sparse vs. dense feature representations. Two encodings information: current word dog; previous word the; previous pos-tag DET.
(a) Sparse feature vector. dimension represents feature. Feature combinations receive dimensions. Feature values binary. Dimensionality
high. (b) Dense, embeddings-based feature vector. core feature
represented vector. feature corresponds several input vector entries. explicit encoding feature combinations. Dimensionality low.
feature-to-vector mappings come embedding table.

350

fiA Primer Neural Networks NLP

One Hot feature dimension.
Dimensionality one-hot vector number distinct features.

Features completely independent one another. feature word
dog dis-similar word thinking word cat .
Dense feature d-dimensional vector.
Dimensionality vector d.

Model training cause similar features similar vectors information
shared similar features.
One benefit using dense low-dimensional vectors computational: majority
neural network toolkits play well high-dimensional, sparse vectors.
However, technical obstacle, resolved engineering
effort.
main benefit dense representations generalization power: believe
features may provide similar clues, worthwhile provide representation
able capture similarities. example, assume observed word dog
many times training, observed word cat handful times,
all. words associated dimension, occurrences dog
tell us anything occurrences cat. However, dense vectors representation
learned vector dog may similar learned vector cat, allowing
model share statistical strength two events. argument assumes
good vectors somehow given us. Section 5 describes ways obtaining vector
representations.
cases relatively distinct features category, believe
correlations different features, may use one-hot representation. However, believe going correlations different features
group (for example, part-of-speech tags, may believe different verb
inflections VB VBZ may behave similarly far task concerned) may
worthwhile let network figure correlations gain statistical strength
sharing parameters. may case circumstances,
feature space relatively small training data plentiful, wish
share statistical information distinct words, gains made using
one-hot representations. However, still open research question,
strong evidence either side. majority work (pioneered Collobert & Weston,
2008; Collobert et al. 2011; Chen & Manning, 2014) advocate use dense, trainable
embedding vectors features. work using neural network architecture sparse
vector encodings see work Johnson Zhang (2015).
Finally, important note representing features dense vectors integral
part neural network framework, consequentially differences
using sparse dense feature representations subtler may appear first.
fact, using sparse, one-hot vectors input training neural network amounts
dedicating first layer network learning dense embedding vector
feature based training data. touch Section 4.6.
351

fiGoldberg

3.2 Variable Number Features: Continuous Bag Words
Feed-forward networks assume fixed dimensional input. easily accommodate
case feature-extraction function extracts fixed number features: feature
represented vector, vectors concatenated. way, region
resulting input vector corresponds different feature. However, cases number
features known advance (for example, document classification common
word sentence feature). thus need represent unbounded
number features using fixed size vector. One way achieving socalled continuous bag words (CBOW) representation (Mikolov, Chen, Corrado, & Dean,
2013). CBOW similar traditional bag-of-words representation
discard order information, works either summing averaging embedding
vectors corresponding features:36
CBOW(f1 , ..., fk ) =

k
1X
v(fi )
k

(1)

i=1

simple variation CBOW representation weighted CBOW, different
vectors receive different weights:
1
WCBOW(f1 , ..., fk ) = Pk

i=1 ai

k
X

ai v(fi )

(2)

i=1

Here, feature associated weight ai , indicating relative importance
feature. example, document classification task, feature may correspond
word document, associated weight ai could words TF-IDF score.
3.3 Distance Position Features
linear distance two words sentence may serve informative feature.
example, event extraction task37 may given trigger word candidate
argument word, asked predict argument word indeed argument
trigger. distance (or relative position) trigger argument strong
signal prediction task. traditional NLP setup, distances usually encoded
binning distances several groups (i.e. 1, 2, 3, 4, 510, 10+) associating
bin one-hot vector. neural architecture, input vector composed
binary indicator features, may seem natural allocate single input entry
distance feature, numeric value entry distance. However,
approach taken practice. Instead, distance features encoded similarly
36. Note v(fi )s one-hot vectors rather dense feature representations, CBOW (eq
1) WCBOW (eq 2) would reduce traditional (weighted) bag-of-words representations,
turn equivalent sparse feature-vector representation binary indicator feature
corresponds unique word.
37. event extraction task involves identification events predefined set event types.
example identification purchase events terror-attack events. event type triggered
various triggering words (commonly verbs), several slots (arguments) needs filled
(i.e. purchased? purchased? amount?).

352

fiA Primer Neural Networks NLP

feature types: bin associated d-dimensional vector, distanceembedding vectors trained regular parameters network (Zeng et al., 2014;
dos Santos et al., 2015; Zhu et al., 2015a; Nguyen & Grishman, 2015).
3.4 Feature Combinations
Note feature extraction stage neural-network settings deals extraction core features. contrast traditional linear-model-based NLP systems
feature designer manually specify core features interest
interactions (e.g., introducing feature stating word
X feature stating tag combined feature stating word X tag
sometimes even word X, tag previous word Z). combination
features crucial linear models introduce dimensions input,
transforming space data-points closer linearly separable.
hand, space possible combinations large, feature designer
spend lot time coming effective set feature combinations. One
promises non-linear neural network models one needs define
core features. non-linearity classifier, defined network structure,
expected take care finding indicative feature combinations, alleviating need
feature combination engineering.
Kernel methods (Shawe-Taylor & Cristianini, 2004), particular polynomial kernels
(Kudo & Matsumoto, 2003), allow feature designer specify core features,
leaving feature combination aspect learning algorithm. contrast neuralnetwork models, kernels methods convex, admitting exact solutions optimization
problem. However, computational complexity classification kernel methods scales
linearly size training data, making slow practical purposes,
suitable training large datasets. hand, computational
complexity classification using neural networks scales linearly size network,
regardless training data size.
3.5 Dimensionality
many dimensions allocate feature? Unfortunately, theoretical bounds even established best-practices space. Clearly, dimensionality
grow number members class (you probably want assign
dimensions word embeddings part-of-speech embeddings) much
enough? current research, dimensionality word-embedding vectors range
50 hundreds, and, extreme cases, thousands. Since dimensionality vectors direct effect memory requirements processing time, good
rule thumb would experiment different sizes, choose good trade-off
speed task accuracy.
3.6 Vector Sharing
Consider case features share vocabulary. example,
assigning part-of-speech given word, may set features considering
353

fiGoldberg

previous word, set features considering next word. building input
classifier, concatenate vector representation previous word
vector representation next word. classifier able distinguish two
different indicators, treat differently. two features share
vectors? vector dog:previous-word vector dog:nextword? assign two distinct vectors? This, again, mostly empirical
question. believe words behave differently appear different positions
(e.g., word X behaves word previous position, X behaves Z
next position) may good idea use two different vocabularies assign
different set vectors feature type. However, believe words behave
similarly locations, something may gained using shared vocabulary
feature types.
3.7 Networks Output
multi-class classification problems k classes, networks output k-dimensional
vector every dimension represents strength particular output class.
is, output remains traditional linear models scalar scores items discrete
set. However, see Section 4, k matrix associated output
layer. columns matrix thought dimensional embeddings
output classes. vector similarities vector representations k classes
indicate models learned similarities output classes.
3.8 Historical Note
Representing words dense vectors input neural network popularized Bengio
et al. (2003) context neural language modeling. introduced NLP tasks
pioneering work Collobert, Weston colleagues (2008, 2011).38 Using embeddings
representing words arbitrary features popularized following Chen
Manning (2014).

4. Feed-Forward Neural Networks
section introduces feed-forward neural networks. starts popular brain
inspired metaphor triggered them, quickly switches back using mathematical
notation. discuss structure feed forward neural networks, representation
power, common non-linearities loss functions.
4.1 Brain-Inspired Metaphor
name suggests, neural-networks inspired brains computation mechanism,
consists computation units called neurons. metaphor, neuron computational unit scalar inputs outputs. input associated weight.
38. work Bengio, Collobert, Weston colleagues popularized approaches,
first use them. Earlier authors use dense continuous-space vectors representing word inputs
neural networks include Lee et al. (1992) Forcada Neco (1997). Similarly, continuous-space
language models used machine-translation already Schwenk et al. (2006).

354

fiA Primer Neural Networks NLP

neuron multiplies input weight, sums39 them, applies non-linear
function result, passes output. neurons connected other,
forming network: output neuron may feed inputs one neurons.
networks shown capable computational devices. weights set
correctly, neural network enough neurons non-linear activation function
approximate wide range mathematical functions (we precise
later).
Output
layer

Hidden
layer

Hidden
layer

Input layer

R

R

y1

y2

y3

R

R

R

R

R

R

R

R

x1

x2

x3

x4

R

Figure 2: Feed-forward neural network two hidden layers.
typical feed-forward neural network may drawn Figure 2. circle
neuron, incoming arrows neurons inputs outgoing arrows neurons outputs. arrow carries weight, reflecting importance (not shown). Neurons
arranged layers, reflecting flow information. bottom layer incoming
arrows, input network. top-most layer outgoing arrows,
output network. layers considered hidden. sigmoid shape
inside neurons middle layers represent non-linear function (i.e., logistic
function 1/(1 + exa )) applied neurons value passing output.
figure, neuron connected neurons next layer called
fully-connected layer affine layer.
brain metaphor sexy intriguing, distracting cumbersome
manipulate mathematically. therefore switch using concise mathematical
notation. values row neurons network thought vector.
Figure 2 input layer 4 dimensional vector (x), layer 6 dimensional vector (h1 ). fully connected layer thought linear transformation
39. summing common operation, functions, max, possible

355

fiGoldberg

4 dimensions 6 dimensions. fully-connected layer implements vector-matrix
multiplication, h = xW weight connection ith neuron
input row jth neuron output row Wij .40 values h transformed non-linear function g applied value passed
next input. whole computation input output written as: (g(xW1 ))W2
W1 weights first layer W2 weights second one.
4.2 Mathematical Notation
point on, abandon brain metaphor describe networks exclusively
terms vector-matrix operations.
simplest neural network perceptron, linear function inputs:
NNPerceptron (x) = xW + b

(3)

x Rdin , W Rdin dout , b Rdout
W weight matrix, b bias term.41 order go beyond linear functions,
introduce non-linear hidden layer (the network Figure 2 two layers), resulting
Multi Layer Perceptron one hidden-layer (MLP1). feed-forward neural network
one hidden-layer form:
NNMLP1 (x) = g(xW1 + b1 )W2 + b2

(4)

x Rdin , W1 Rdin d1 , b1 Rd1 , W2 Rd1 d2 , b2 Rd2
W1 b1 matrix bias term first linear transformation
input, g non-linear function applied element-wise (also called non-linearity
activation function), W2 b2 matrix bias term second linear
transform.
Breaking down, xW1 +b1 linear transformation input x din dimensions
d1 dimensions. g applied d1 dimensions, matrix W2 together
bias vector b2 used transform result d2 dimensional output
vector. non-linear activation function g crucial role networks ability
represent complex functions. Without non-linearity g, neural network
represent linear transformations input.42
add additional linear-transformations non-linearities, resulting MLP
two hidden-layers (the network Figure 2 form):
NNMLP2 (x) = (g 2 (g 1 (xW1 + b1 )W2 + b2 ))W3

(5)

perhaps clearer write deeper networks using intermediary variables:
40. see P
case, denote weight ith input jth neuron h wij . value
hj hj = 4i=1 xi wij .
41. network figure 2 include bias terms. bias term added layer adding
additional neuron incoming connections, whose value always 1.
42. see why, consider sequence linear transformations still linear transformation.

356

fiA Primer Neural Networks NLP

NNMLP2 (x) =y
h1 =g 1 (xW1 + b1 )
h2 =g 2 (h1 W2 + b2 )

(6)

=h2 W3
vector resulting linear transform referred layer. outer-most
linear transform results output layer linear transforms result hidden
layers. hidden layer followed non-linear activation. cases,
last layer example, bias vectors forced 0 (dropped).
Layers resulting linear transformations often referred fully connected,
affine. types architectures exist. particular, image recognition problems benefit
convolutional pooling layers. layers uses language processing,
discussed Section 9. Networks several hidden layers said deep
networks, hence name deep learning.
describing neural network, one specify dimensions layers
input. layer expect din dimensional vector input, transform
dout dimensional vector. dimensionality layer taken dimensionality
output. fully connected layer l(x) = xW + b input dimensionality din
output dimensionality dout , dimensions x 1 din , W din dout b
1 dout .
output network dout dimensional vector. case dout = 1, networks
output scalar. networks used regression (or scoring) considering
value output, binary classification consulting sign output.
Networks dout = k > 1 used k-class classification, associating
dimension class, looking dimension maximal value. Similarly,
output vector entries positive sum one, output interpreted
distribution class assignments (such output normalization typically achieved
applying softmax transformation output layer, see Section 4.5).
matrices bias terms define linear transformations parameters network. common refer collection parameters . Together
input, parameters determine networks output. training algorithm
responsible setting values networks predictions correct. Training
discussed Section 6.
4.3 Representation Power
terms representation power, shown Hornik, Stinchcombe, White (1989)
Cybenko (1989) MLP1 universal approximator approximate
desired non-zero amount error family functions43 include continuous
functions closed bounded subset Rn , function mapping finite
43. Specifically, feed-forward network linear output layer least one hidden layer squashing activation function approximate Borel measurable function one finite dimensional space
another.

357

fiGoldberg

dimensional discrete space another. may suggest reason go beyond
MLP1 complex architectures. However, theoretical result discuss
learnability neural network (it states representation exists, say
easy hard set parameters based training data specific learning
algorithm). guarantee training algorithm find correct function
generating training data. Finally, state large hidden layer
be. Indeed, Telgarsky (2016) show exist neural networks many layers
bounded size cannot approximated networks fewer layers unless layers
exponentially large.
practice, train neural networks relatively small amounts data using local
search methods variants stochastic gradient descent, use hidden layers
relatively modest sizes (up several thousands). universal approximation theorem
give guarantees non-ideal, real-world conditions, definitely
benefit trying complex architectures MLP1. many cases,
however, MLP1 indeed provide strong results. discussion representation power feed-forward neural networks, see book Bengio et al. (2015, Section
6.5).

4.4 Common Non-linearities
non-linearity g take many forms. currently good theory
non-linearity apply conditions, choosing correct non-linearity
given task part empirical question. go common nonlinearities literature: sigmoid, tanh, hard tanh rectified linear unit
(ReLU). NLP researchers experimented forms non-linearities
cube tanh-cube.

4.4.1 Sigmoid
sigmoid activation function (x) = 1/(1 + ex ), called logistic function,
S-shaped function, transforming value x range [0, 1]. sigmoid
canonical non-linearity neural networks since inception, currently considered
deprecated use internal layers neural networks, choices listed
prove work much better empirically.

4.4.2 Hyperbolic Tangent (tanh)
2x

hyperbolic tangent tanh(x) = ee2x 1
activation function S-shaped function, trans+1
forming values x range [1, 1].
358

fiA Primer Neural Networks NLP

4.4.3 Hard tanh
hard-tanh activation function approximation tanh function faster
compute take derivatives of:


1 x < 1
hardtanh(x) = 1
(7)
x>1


x
otherwise
4.4.4 Rectifier (ReLU)
Rectifier activation function (Glorot, Bordes, & Bengio, 2011), known
rectified linear unit simple activation function easy work
shown many times produce excellent results.44 ReLU unit clips value x < 0
0. Despite simplicity, performs well many tasks, especially combined
dropout regularization technique (see Section 6.4).
(
0
ReLU(x) = max(0, x) =
x

x<0
otherwise

(8)

rule thumb, ReLU units work better tanh, tanh works better
sigmoid.45
4.5 Output Transformations
many cases, output layer vector transformed. common transformation
softmax :
x =x1 , . . . , xk
e xi
softmax(xi ) = Pk
xj
j=1 e

(9)

44. technical advantages ReLU sigmoid tanh activation functions
involve expensive-to-compute functions, importantly saturate. sigmoid
tanh activation capped 1, gradients region functions near zero,
driving entire gradient near zero. ReLU activation problem, making
especially suitable networks multiple layers, susceptible vanishing gradients
problem trained saturating units.
45. addition activation functions, recent works NLP community experiment
reported success forms non-linearities. Cube activation function, g(x) = (x)3 ,
suggested Chen Manning (2014), found effective non-linearities
feed-forward network used predict actions greedy transition-based dependency
parser. tanh cube activation function g(x) = tanh((x)3 + x) proposed Pei et al. (2015),
found effective non-linearities feed-forward network used
component structured-prediction graph-based dependency parser.
cube tanh-cube activation functions motivated desire better capture interactions different features. activation functions reported improve performance
certain situations, general applicability still determined.

359

fiGoldberg

result vector non-negative real numbers sum one, making discrete
probability distribution k possible outcomes.
softmax output transformation used interested modeling probability distribution possible output classes. effective, used
conjunction probabilistic training objective cross-entropy (see Section 4.7.4
below).
softmax transformation applied output network without hidden
layer, result well known multinomial logistic regression model, known
maximum-entropy classifier.
4.6 Embedding Layers
now, discussion ignored source x, treating arbitrary vector.
NLP application, x usually composed various embeddings vectors.
explicit source x, include networks definition. introduce c(),
function core features input vector.
common c extract embedding vector associated feature,
concatenate them:
x = c(f1 , f2 , f3 ) =[v(f1 ); v(f2 ); v(f3 )]
NNMLP1 (x) =NNMLP1 (c(f1 , f2 , f3 ))
=NNMLP1 ([v(f1 ); v(f2 ); v(f3 )])

(10)

=(g([v(f1 ); v(f2 ); v(f3 )]W1 + b1 ))W2 + b2
Another common choice c sum embedding vectors (this assumes embedding vectors share dimensionality):
x = c(f1 , f2 , f3 ) =v(f1 ) + v(f2 ) + v(f3 )
NNMLP1 (x) =NNMLP1 (c(f1 , f2 , f3 ))
=NNMLP1 (v(f1 ) + v(f2 ) + v(f3 ))

(11)

=(g((v(f1 ) + v(f2 ) + v(f3 ))W1 + b1 ))W2 + b2
form c essential part networks design. many papers, common
refer c part network, likewise treat word embeddings v(fi ) resulting
embedding layer lookup layer. Consider vocabulary |V | words,
embedded dimensional vector. collection vectors thought
|V | embedding matrix E row corresponds embedded feature. Let
|V |-dimensional vector, zeros except one index, corresponding
value ith feature, value 1 (this called one-hot vector).
multiplication E select corresponding row E. Thus, v(fi ) defined
terms E :
v(fi ) = E
360

(12)

fiA Primer Neural Networks NLP

similarly:
CBOW(f1 , ..., fk ) =

k
X

(fi E) = (

i=1

k
X

)E

(13)

i=1

input network considered collection one-hot vectors.
elegant well defined mathematically, efficient implementation typically involves
hash-based data structure mapping features corresponding embedding vectors,
without going one-hot representation.
tutorial, take c separate network architecture: networks
inputs always dense real-valued input vectors, c applied input passed
network, similar feature function familiar linear-models terminology. However, training network, input vector x remember constructed,
propagate error gradients back component embedding vectors, appropriate
(error propagation discussed section 6).
4.6.1 Note Notation
describing network layers get concatenated vectors x, z input,
authors use explicit concatenation ([x; y; z]W +b) others use affine transformation
(xU + yV + zW + b). weight matrices U, V, W affine transformation
different one another, two notations equivalent.
4.6.2 Note Sparse vs. Dense Features
Consider network uses traditional sparse representation input vectors,
embedding layer. Assuming set available features V k
features f1 , . . . , fk , V , networks input is:
x=

k
X

|V |

x N+



i=1

(14)

first layer (ignoring non-linear activation) is:
k
X
xW + b = (
)W

(15)

i=1

W R|V |d , b Rd
layer selects rows W corresponding input features x sums them,
adding bias term. similar embedding layer produces CBOW
representation features, matrix W acts embedding matrix.
main difference introduction bias vector b, fact embedding
layer typically undergo non-linear activation rather passed directly
first layer. Another difference scenario forces feature receive separate
vector (row W) embedding layer provides flexibility, allowing example
features next word dog previous word dog share vector.
361

fiGoldberg

However, differences small subtle. comes multi-layer feed-forward
networks, difference dense sparse inputs smaller may seem
first sight.
4.7 Loss Functions
training neural network (more training Section 6 below), much
training linear classifier, one defines loss function L(y, y), stating loss predicting
true output y. training objective minimize loss across
different training examples. loss L(y, y) assigns numerical score (a scalar)
networks output given true expected output y.46 loss function
bounded below, minimum attained cases networks output
correct.
parameters network (the matrices Wi , biases bi commonly embeddings E) set order minimize loss L training examples (usually,
sum losses different training examples minimized).
loss arbitrary function mapping two vectors scalar. practical
purposes optimization, restrict functions easily compute
gradients (or sub-gradients). cases, sufficient advisable rely common
loss function rather defining own. detailed discussion loss functions
neural networks see work LeCun, Chopra, Hadsell, Ranzato, Huang (2006), LeCun
Huang (2005) Bengio et al. (2015). discuss loss functions
commonly used neural networks NLP.
4.7.1 Hinge (binary)
binary classification problems, networks output single scalar intended
output {+1, 1}. classification rule sign(y), classification considered
correct > 0, meaning share sign. hinge loss, known
margin loss SVM loss, defined as:
Lhinge(binary) (y, y) = max(0, 1 y)

(16)

loss 0 share sign |y| 1. Otherwise, loss linear.
words, binary hinge loss attempts achieve correct classification,
margin least 1.
4.7.2 Hinge (multiclass)
hinge loss extended multiclass setting Crammer Singer (2002). Let
= y1 , . . . , yn networks output vector, one-hot vector correct
output class.
classification rule defined selecting class highest score:
prediction = arg max yi


(17)

46. notation, models output expected output vectors, many cases
natural think expected output scalar (class assignment). cases, simply
corresponding one-hot vector.

362

fiA Primer Neural Networks NLP

Denote = arg maxi yi correct class, k = arg maxi6=t yi highest scoring
class k 6= t. multiclass hinge loss defined as:
Lhinge(multiclass) (y, y) = max(0, 1 (yt yk ))

(18)

multiclass hinge loss attempts score correct class classes
margin least 1.
binary multiclass hinge losses intended used linear output
layer. hinge losses useful whenever require hard decision rule,
attempt model class membership probability.
4.7.3 Log Loss
log loss common variation hinge loss, seen soft version
hinge loss infinite margin (LeCun et al., 2006).
Llog (y, y) = log(1 + exp((yt yk ))

(19)

4.7.4 Categorical Cross-Entropy Loss
categorical cross-entropy loss (also referred negative log likelihood ) used
probabilistic interpretation scores desired.
Let = y1 , . . . , yn vector representing true multinomial distribution
labels 1, . . . , n, let = y1 , . . . , yn networks output, transformed
softmax activation function, represent class membership conditional distribution
yi = P (y = i|x). categorical cross entropy loss measures dissimilarity
true label distribution predicted label distribution y, defined cross
entropy:
Lcross-entropy (y, y) =

X

yi log(yi )

(20)



hard classification problems training example single correct
class assignment, one-hot vector representing true class. cases, cross
entropy simplified to:
Lcross-entropy(hard classification) (y, y) = log(yt )

(21)

correct class assignment. attempts set probability mass assigned
correct class 1. scores transformed using softmax
function represent conditional distribution, increasing mass assigned correct
class means decreasing mass assigned classes.
cross-entropy loss common neural networks literature, produces
multi-class classifier predict one-best class label predicts
distribution possible labels. using cross-entropy loss, assumed
networks output transformed using softmax transformation.
363

fiGoldberg

4.7.5 Ranking Losses
settings, given supervision term labels, rather pairs
correct incorrect items x x0 , goal score correct items incorrect
ones. training situations arise positive examples, generate
negative examples corrupting positive example. useful loss scenarios
margin-based ranking loss, defined pair correct incorrect examples:
Lranking(margin) (x, x0 ) = max(0, 1 (NN(x) NN(x0 )))

(22)

NN(x) score assigned network input vector x. objective
score (rank) correct inputs incorrect ones margin least 1.
common variation use log version ranking loss:
Lranking(log) (x, x0 ) = log(1 + exp((NN(x) NN(x0 ))))

(23)

Examples using ranking hinge loss language tasks include training auxiliary tasks used deriving pre-trained word embeddings (see section 5),
given correct word sequence corrupted word sequence, goal score
correct sequence corrupt one (Collobert & Weston, 2008). Similarly, Van
de Cruys (2014) used ranking loss selectional-preferences task, network trained rank correct verb-object pairs incorrect, automatically derived
ones, Weston, Bordes, Yakhnenko, Usunier (2013) trained model score correct
(head,relation,trail) triplets corrupted ones information-extraction setting.
example using ranking log loss found work Gao et al. (2014).
variation ranking log loss allowing different margin negative positive
class given work dos Santos et al. (2015).

5. Word Embeddings
main component neural-network approach use embeddings representing
feature vector low dimensional space. vectors come from?
section survey common approaches.
5.1 Random Initialization
enough supervised training data available, one treat feature embeddings
model parameters: initialize embedding vectors random values,
let network-training procedure tune good vectors.
care taken way random initialization performed. method
used effective word2vec implementation (Mikolov et al., 2013; Mikolov, Sutskever,
Chen, Corrado, & Dean, 2013) initialize word vectors uniformly sampled random
1 1
numbers range [ 2d
, 2d ] number dimensions. Another option
use xavier
(see Section 6.3.1) initialize uniformly sampled values
h initialization

6 6

, .
364

fiA Primer Neural Networks NLP

practice, one often use random initialization approach initialize embedding vectors commonly occurring features, part-of-speech tags individual
letters, using form supervised unsupervised pre-training initialize
potentially rare features, features individual words. pre-trained vectors
either treated fixed network training process, or, commonly,
treated randomly-initialized vectors tuned task hand.
5.2 Supervised Task-Specific Pre-training
interested task A, limited amount labeled data (for
example, syntactic parsing), auxiliary task B (say, part-of-speech tagging)
much labeled data, may want pre-train word vectors
perform well predictors task B, use trained vectors training
task A. way, utilize larger amounts labeled data task B.
training task either treat pre-trained vectors fixed, tune
task A. Another option train jointly objectives, see Section 7
details.
5.3 Unsupervised Pre-training
common case auxiliary task large enough amounts
annotated data (or maybe want help bootstrap auxiliary task training better
vectors). cases, resort unsupervised methods, trained huge
amounts unannotated text.
techniques training word vectors essentially supervised learning,
instead supervision task care about, instead create practically
unlimited number supervised training instances raw text, hoping tasks
created match (or close enough to) final task care about.47
key idea behind unsupervised approaches one would embedding
vectors similar words similar vectors. word similarity hard define
usually task-dependent, current approaches derive distributional
hypothesis (Harris, 1954), stating words similar appear similar contexts.
different methods create supervised training instances goal either
predict word context, predict context word.
important benefit training word embeddings large amounts unannotated
data provides vector representations words appear supervised training set. Ideally, representations words similar
related words appear training set, allowing model generalize better
unseen events. thus desired similarity word vectors learned unsupervised algorithm captures aspects similarity useful performing
intended task network.
47. interpretation creating auxiliary problems raw text inspired Ando Zhang (2005a)
Ando Zhang (2005b).

365

fiGoldberg

Common unsupervised word-embedding algorithms include word2vec 48 (Mikolov et al.,
2013, 2013), GloVe (Pennington, Socher, & Manning, 2014) Collobert Weston
(2008, 2011) embeddings algorithm. models inspired neural networks
based stochastic gradient training. However, deeply connected another
family algorithms evolved NLP IR communities, based
matrix factorization (for discussion see Levy & Goldberg, 2014b; Levy et al., 2015).
Arguably, choice auxiliary problem (what predicted, based kind
context) affects resulting vectors much learning method
used train them. thus focus different choices auxiliary problems
available, skim details training methods. Several software packages
deriving word vectors available, including word2vec49 Gensim50 implementing
word2vec models word-windows based contexts, word2vecf51 modified
version word2vec allowing use arbitrary contexts, GloVe52 implementing
GloVe model. Many pre-trained word vectors available download web.
beyond scope tutorial, worth noting word embeddings
derived unsupervised training algorithms wide range applications NLP
beyond using initializing word-embeddings layer neural-network model.
5.4 Training Objectives
Given word w context c, different algorithms formulate different auxiliary tasks.
cases, word represented d-dimensional vector initialized
random value. Training model perform auxiliary tasks well result good
word embeddings relating words contexts, turn result
embedding vectors similar words similar other.
Language-modeling inspired approaches taken Mikolov et al. (2013),
Mnih Kavukcuoglu (2013) well GloVe (Pennington et al., 2014) use auxiliary tasks
goal predict word given context. posed probabilistic
setup, trying model conditional probability P (w|c).
approaches reduce problem binary classification. addition
set observed word-context pairs, set created random words
context pairings. binary classification problem then: given (w, c) pair
come not? approaches differ set constructed,
structure classifier, objective optimized. Collobert
Weston (2008, 2011) take margin-based binary ranking approach, training feed-forward
neural network score correct (w, c) pairs incorrect ones. Mikolov et al. (2013, 2014)
take instead probabilistic version, training log-bilinear model predict probability
P ((w, c) D|w, c) pair come corpus rather random sample.
48. often treated single algorithm, word2vec actually software package including various
training objectives, optimization methods hyperparameters. See work Rong (2014)
Levy, Goldberg, Dagan (2015) discussion.
49. https://code.google.com/p/word2vec/
50. https://radimrehurek.com/gensim/
51. https://bitbucket.org/yoavgo/word2vecf
52. http://nlp.stanford.edu/projects/glove/

366

fiA Primer Neural Networks NLP

5.5 Choice Contexts
cases, contexts word taken words appear
surrounding, either short window around it, within sentence, paragraph
document. cases text automatically parsed syntactic parser,
contexts derived syntactic neighbourhood induced automatic parse
trees. Sometimes, definitions words context change include parts words,
prefixes suffixes.
Neural word embeddings originated world language modeling,
network trained predict next word based sequence preceding words (Bengio
et al., 2003). There, text used create auxiliary tasks aim predict
word based context k previous words. training language modeling
auxiliary prediction problems indeed produce useful embeddings, approach needlessly
restricted constraints language modeling task, one allowed look
previous words. care language modeling
resulting embeddings, may better ignoring constraint taking context
symmetric window around focus word.
5.5.1 Window Approach
common approach sliding window approach, auxiliary tasks
created looking sequence 2k + 1 words. middle word callled focus word
k words side contexts. Then, either single task created
goal predict focus word based context words (represented either
using CBOW, see Mikolov et al., 2013 vector concatenation, see Collobert & Weston,
2008), 2k distinct tasks created, pairing focus word different context
word. 2k tasks approach, popularized Mikolov et al. (2013) referred
skip-gram model. Skip-gram based approaches shown robust efficient train
(Mikolov et al., 2013; Pennington et al., 2014), often produce state art results.
Effect Window Size size sliding window strong effect resulting vector similarities. Larger windows tend produce topical similarities (i.e.
dog, bark leash grouped together, well walked, run walking), smaller windows tend produce functional syntactic similarities (i.e.
Poodle, Pitbull, Rottweiler, walking,running,approaching).
Positional Windows using CBOW skip-gram context representations,
different context words within window treated equally. distinction
context words close focus words farther
it, likewise distinction context words appear focus
words context words appear it. information easily factored
using positional contexts: indicating context word relative position
focus words (i.e. instead context word becomes the:+2, indicating
word appears two positions right focus word). use positional context
together smaller windows tend produce similarities syntactic,
strong tendency grouping together words share part speech, well
functionally similar terms semantics. Positional vectors shown Ling,
367

fiGoldberg

Dyer, Black, Trancoso (2015a) effective window-based vectors
used initialize networks part-of-speech tagging syntactic dependency parsing.
Variants Many variants window approach possible. One may lemmatize words
learning, apply text normalization, filter short long sentences, remove
capitalization (see, e.g., pre-processing steps described dos Santos & Gatti, 2014).
One may sub-sample part corpus, skipping probability creation tasks
windows common rare focus words. window size may
dynamic, using different window size turn. One may weigh different positions
window differently, focusing trying predict correctly close word-context
pairs away ones. choices effect resulting vectors.
hyperparameters (and others) discussed Levy et al. (2015).
5.5.2 Sentences, Paragraphs Documents
Using skip-grams (or CBOW) approach, one consider contexts word
words appear sentence, paragraph document.
equivalent using large window sizes, expected result word vectors
capture topical similarity (words topic, i.e. words one would expect
appear document, likely receive similar vectors).
5.5.3 Syntactic Window
work replace linear context within sentence syntactic one (Levy &
Goldberg, 2014a; Bansal, Gimpel, & Livescu, 2014). text automatically parsed
using dependency parser, context word taken words
proximity parse tree, together syntactic relation
connected. approaches produce highly functional similarities, grouping together words
fill role sentence (e.g. colors, names schools, verbs movement).
grouping syntactic, grouping together words share inflection (Levy &
Goldberg, 2014a).
5.5.4 Multilingual
Another option using multilingual, translation based contexts (Hermann & Blunsom,
2014; Faruqui & Dyer, 2014). example, given large amount sentence-aligned parallel
text, one run bilingual alignment model IBM model 1 model 2 (i.e.
using GIZA++ software), use produced alignments derive word contexts.
Here, context word instance foreign language words aligned it.
alignments tend result synonym words receiving similar vectors. authors
work instead sentence alignment level, without relying word alignments (Gouws,
Bengio, & Corrado, 2015) train end-to-end machine-translation neural network
use resulting word embeddings (Hill, Cho, Jean, Devin, & Bengio, 2014). appealing
method mix monolingual window-based approach multilingual approach,
creating kinds auxiliary tasks. likely produce vectors similar
window-based approach, reducing somewhat undesired effect window368

fiA Primer Neural Networks NLP

based approach antonyms (e.g. hot cold, high low) tend receive similar
vectors (Faruqui & Dyer, 2014).
5.5.5 Character-Based Sub-word Representations
interesting line work attempts derive vector representation word
characters compose it. approaches likely particularly useful tasks
syntactic nature, character patterns within words strongly related
syntactic function. approaches benefit producing small
model sizes (only one vector character alphabet together handful
small matrices needs stored), able provide embedding vector every
word may encountered. Dos Santos Gatti (2014), dos Santos Zadrozny
(2014) Kim et al. (2015) model embedding word using convolutional network
(see Section 9) characters. Ling et al. (2015b) model embedding word
using concatenation final states two RNN (LSTM) encoders (Section 10), one
reading characters left right, right left. produce
strong results part-of-speech tagging. work Ballesteros et al. (2015) show
two-LSTMs encoding Ling et al. (2015b) beneficial representing words
dependency parsing morphologically rich languages.
Deriving representations words representations characters motivated unknown words problem encounter word
embedding vector? Working level characters alleviates
problem large extent, vocabulary possible characters much smaller
vocabulary possible words. However, working character level
challenging, relationship form (characters) function (syntax, semantics)
language quite loose. Restricting oneself stay character level may
unnecessarily hard constraint. researchers propose middle-ground, word
represented combination vector word vectors sub-word
units comprise it. sub-word embeddings help sharing information
different words similar forms, well allowing back-off subword level
word observed. time, models forced rely solely
form enough observations word available. Botha Blunsom (2014) suggest model embedding vector word sum word-specific vector
vector available, vectors different morphological components comprise
(the components derived using Morfessor (Creutz & Lagus, 2007), unsupervised
morphological segmentation method). Gao et al. (2014) suggest using core features
word form unique feature (hence unique embedding vector)
letter-trigrams word.

6. Neural Network Training
Neural network training done trying minimize loss function training set,
using gradient-based method. Roughly speaking, training methods work repeatedly
computing estimate error dataset, computing gradient respect
error, moving parameters opposite direction gradient.
Models differ error estimate computed, moving opposite
369

fiGoldberg

direction gradient defined. describe basic algorithm, stochastic gradient
descent (SGD), briefly mention approaches pointers
reading. Gradient calculation central approach. Gradients efficiently
automatically computed using reverse mode differentiation computation graph
general algorithmic framework automatically computing gradient network
loss function, discussed Section 6.2.
6.1 Stochastic Gradient Training
common approach training neural networks using stochastic gradient descent
(SGD) algorithm (Bottou, 2012; LeCun, Bottou, Orr, & Muller, 1998a) variant it.
SGD general optimization algorithm. receives function f parameterized ,
loss function, desired input output pairs. attempts set parameters
loss f respect training examples small. algorithm works
follows:
Algorithm 1 Online Stochastic Gradient Descent Training
1: Input: Function f (x; ) parameterized parameters .
2: Input: Training set inputs x1 , . . . , xn desired outputs y1 , . . . , yn .
3: Input: Loss function L.
4: stopping criteria met
5:
Sample training example xi , yi
6:
Compute loss L(f (xi ; ), yi )
7:
g gradients L(f (xi ; ), yi ) w.r.t
8:
g
9: return
PnThe goal algorithm set parameters minimize total loss
i=1 L(f (xi ; ), yi ) training set. works repeatedly sampling training example computing gradient error example respect parameters
(line 7) input expected output assumed fixed, loss treated
function parameters . parameters updated opposite
direction gradient, scaled learning rate (line 8). learning rate either
fixed throughout training process, decay function time step t.53
discussion setting learning rate, see Section 6.3.
Note error calculated line 6 based single training example, thus
rough estimate corpus-wide loss aiming minimize. noise
loss computation may result inaccurate gradients. common way reducing
noise estimate error gradients based sample examples.
gives rise minibatch SGD algorithm:
lines 6 9 algorithm estimates gradient corpus loss based
minibatch. loop, g contains gradient estimate, parameters
updated toward g. minibatch size vary size = 1 = n. Higher
values provide better estimates corpus-wide gradients, smaller values allow
53. Learning rate decay required order prove convergence SGD.

370

fiA Primer Neural Networks NLP

Algorithm 2 Minibatch Stochastic Gradient Descent Training
1: Input: Function f (x; ) parameterized parameters .
2: Input: Training set inputs x1 , . . . , xn desired outputs y1 , . . . , yn .
3: Input: Loss function L.
4: stopping criteria met
5:
Sample minibatch examples {(x1 , y1 ), . . . , (xm , ym )}
6:
g 0
7:
= 1
8:
Compute loss L(f (xi ; ), yi )
1
9:
g g + gradients
L(f (xi ; ), yi ) w.r.t
g
11: return
10:

updates turn faster convergence. Besides improved accuracy gradients
estimation, minibatch algorithm provides opportunities improved training efficiency.
modest sizes m, computing architectures (i.e. GPUs) allow efficient parallel
implementation computation lines 69. properly decreasing learning rate,
SGD guaranteed converge global optimum function convex. However,
used optimize non-convex functions neural-network.
longer guarantees finding global optimum, algorithm proved robust
performs well practice.54
training neural network, parameterized function f neural network,
parameters linear-transformation matrices, bias terms, embedding matrices
on. gradient computation key step SGD algorithm, well
neural network training algorithms. question is, then, compute
gradients networks error respect parameters. Fortunately,
easy solution form backpropagation algorithm (Rumelhart, Hinton, & Williams,
1986; LeCun, Bottou, Bengio, & Haffner, 1998b). backpropagation algorithm fancy
name methodically computing derivatives complex expression using chainrule, caching intermediary results. generally, backpropagation algorithm
special case reverse-mode automatic differentiation algorithm (Neidinger, 2010,
Section 7; Baydin, Pearlmutter, Radul, & Siskind, 2015; Bengio, 2012). following
section describes reverse mode automatic differentiation context computation
graph abstraction.
6.1.1 Beyond SGD
SGD algorithm often produce good results, advanced algorithms available. SGD+Momentum (Polyak, 1964) Nesterov Momentum
(Sutskever, Martens, Dahl, & Hinton, 2013; Nesterov, 1983, 2004) algorithms variants
SGD previous gradients accumulated affect current update. Adap54. Recent work neural-networks literature argue non-convexity networks manifested proliferation saddle points rather local minima (Dauphin, Pascanu, Gulcehre, Cho,
Ganguli, & Bengio, 2014). may explain success training neural networks despite
using local search techniques.

371

fiGoldberg

tive learning rate algorithms including AdaGrad (Duchi, Hazan, & Singer, 2011), AdaDelta
(Zeiler, 2012), RMSProp (Tieleman & Hinton, 2012) Adam (Kingma & Ba, 2014)
designed select learning rate minibatch, sometimes per-coordinate basis,
potentially alleviating need fiddling learning rate scheduling. details
algorithms, see original papers book Bengio et al. (2015, Sections 8.3, 8.4).
many neural-network software frameworks provide implementations algorithms,
easy sometimes worthwhile try different variants.
6.2 Computation Graph Abstraction
one compute gradients various parameters network hand
implement code, procedure cumbersome error prone. purposes, preferable use automatic tools gradient computation (Bengio, 2012).
computation-graph abstraction allows us easily construct arbitrary networks, evaluate
predictions given inputs (forward pass), compute gradients parameters
respect arbitrary scalar losses (backward pass).
computation graph representation arbitrary mathematical computation
graph. directed acyclic graph (DAG) nodes correspond mathematical
operations (bound) variables edges correspond flow intermediary values
nodes. graph structure defines order computation terms
dependencies different components. graph DAG tree,
result one operation input several continuations. Consider example
graph computation (a b + 1) (a b + 2):
*
+

+
*

1



b

2

computation b shared. restrict case computation
graph connected.
Since neural network essentially mathematical expression, represented
computation graph.
example, Figure 3a presents computation graph MLP one hiddenlayer softmax output transformation. notation, oval nodes represent mathematical operations functions, shaded rectangle nodes represent parameters (bound
variables). Network inputs treated constants, drawn without surrounding node.
Input parameter nodes incoming arcs, output nodes outgoing arcs.
output node matrix, dimensionality indicated
node.
graph incomplete: without specifying inputs, cannot compute output.
Figure 3b shows complete graph MLP takes three words inputs, predicts
distribution part-of-speech tags third word. graph used
prediction, training, output vector (not scalar) graph
take account correct answer loss term. Finally, graph 3c shows
372

fiA Primer Neural Networks NLP

11
neg
11
log
11
(a)

(b)

(c)

pick

1 17

1 17

1 17

softmax

softmax

softmax

1 17

1 17

1 17

ADD

ADD

ADD

1 17

1 17

1 17

MUL

MUL

MUL

1 20
tanh

20 17
W2

1 20

1 17
b2

20 17
W2

tanh

1 20

1 17
b2

1 20

1 20

ADD

ADD

ADD

1 20

1 20

1 20

MUL

MUL

MUL

150 20
W1

1 150

1 20
b1

concat

150 20
W1

20 17

1 17

150 20

1 20

W2

tanh

1 20

1 150
x

5

1 150

1 20
b1

concat

W1

1 50

1 50

1 50

1 50

1 50

1 50

lookup

lookup

lookup

lookup

lookup

lookup



black

dog



black

dog

|V | 50
E

b2

b1

|V | 50
E

Figure 3: Computation Graph MLP1. (a) Graph unbound input. (b) Graph
concrete input. (c) Graph concrete input, expected output, loss
node.

computation graph specific training example, inputs (embeddings
of) words the, black, dog, expected output NOUN (whose index
5). pick node implements indexing operation, receiving vector index (in
case, 5) returning corresponding entry vector.
graph built, straightforward run either forward computation (compute result computation) backward computation (computing gradients),
show below. Constructing graphs may look daunting, actually easy
using dedicated software libraries APIs.
6.2.1 Forward Computation
forward pass computes outputs nodes graph. Since nodes output
depends incoming edges, trivial compute outputs
nodes traversing nodes topological order computing output
node given already computed outputs predecessors.
373

fiGoldberg

formally, graph N nodes, associate node index according
topological ordering. Let function computed node (e.g. multiplication.
addition, . . . ). Let (i) parent nodes node i, 1 (i) = {j | (j)}
children nodes node (these arguments ). Denote v(i) output node
i, is, application output values arguments 1 (i). variable
input nodes, constant function 1 (i) empty. Forward algorithm
computes values v(i) [1, N ].
Algorithm 3 Computation Graph Forward Pass
1: = 1 N
2:
Let a1 , . . . , = 1 (i)
3:
v(i) (v(a1 ), . . . , v(am ))

6.2.2 Backward Computation (Derivatives, Backprop)
backward pass begins designating node N scalar (11) output loss-node,
running forward computation node. backward computation computes
N
gradients respect nodes value. Denote d(i) quantity
.

backpropagation algorithm used compute values d(i) nodes i.
backward pass fills table d(i) follows:
Algorithm 4 Computation Graph Backward Pass (Backpropagation)
1: d(N ) 1
2: = N-1 1
P
fj
3:
d(i) j(i) d(j)


fj
partial derivative fj ( 1 (j)) w.r.t argument 1 (j).

value depends function fj values v(a1 ), . . . , v(am ) (where a1 , . . . , =
1 (j)) arguments, computed forward pass.

quantity

Thus, order define new kind node, one need define two methods: one
calculating forward value v(i) based nodes inputs, another calculating

x 1 (i).
x
information automatic differentiation see work Neidinger (2010,
Section 7) Baydin et al. (2015). depth discussion backpropagation
algorithm computation graphs (also called flow graphs) see work Bengio et al.
(2015, Section 6.4), LeCun et al. (1998b) Bengio (2012). popular yet technical
presentation, see online post Olah (2015a).
374

fiA Primer Neural Networks NLP

6.2.3 Software
Several software packages implement computation-graph model, including Theano55 ,
Chainer56 , penne57 CNN/pyCNN58 . packages support essential components (node types) defining wide range neural network architectures, covering
structures described tutorial more. Graph creation made almost transparent
use operator overloading. framework defines type representing graph nodes
(commonly called expressions), methods constructing nodes inputs parameters,
set functions mathematical operations take expressions input result
complex expressions. example, python code creating computation
graph Figure (3c) using pyCNN framework is:
import pycnn pc
# model initialization.
model = pc.Model()
pW1 = model.add_parameters((20,150))
pb1 = model.add_parameters(20)
pW2 = model.add_parameters((17,20))
pb2 = model.add_parameters(17)
words = model.add_lookup_parameters((100, 50))
# Building computation graph:
pc.renew_cg() # create new graph.
# Wrap model parameters graph-nodes.
W1 = pc.parameter(pW1)
b1 = pc.parameter(pb1)
W2 = pc.parameter(pW2)
b2 = pc.parameter(pb2)
def get_index(x): return 1 # place holder
# Generate embeddings layer.
vthe
= pc.lookup(words, get_index("the"))
vblack = pc.lookup(words, get_index("black"))
vdog
= pc.lookup(words, get_index("dog"))
# Connect leaf nodes complete graph.
x = pc.concatenate([vthe, vblack, vdog])
output = pc.softmax(W2*(pc.tanh(W1*x)+b1)+b2)
loss = -pc.log(pc.pick(output, 5))
loss_value = loss.forward()
loss.backward() # gradient computed
# stored corresponding
# parameters.

code involves various initializations: first block defines model parameters
shared different computation graphs (recall graph corresponds
specific training example). second block turns model parameters graphnode (Expression) types. third block retrieves Expressions embeddings
55.
56.
57.
58.

http://deeplearning.net/software/theano/
http://chainer.org
https://bitbucket.org/ndnlp/penne
https://github.com/clab/cnn

375

fiGoldberg

input words. Finally, fourth block graph created. Note transparent
graph creation almost one-to-one correspondence creating
graph describing mathematically. last block shows forward backward
pass. software frameworks follow similar patterns.
Theano involves optimizing compiler computation graphs, blessing
curse. one hand, compiled, large graphs run efficiently either
CPU GPU, making ideal large graphs fixed structure,
inputs change instances. However, compilation step costly,
makes interface bit cumbersome work with. contrast, packages focus
building large dynamic computation graphs executing fly without
compilation step. execution speed may suffer respect Theanos optimized
version, packages especially convenient working recurrent
recursive networks described Sections 10, 12 well structured prediction settings
described Section 8.
6.2.4 Implementation Recipe
Using computation graph abstraction, pseudo-code network training algorithm
given Algorithm 5.
Algorithm 5 Neural Network Training Computation Graph Abstraction (using minibatches size 1)
1: Define network parameters.
2: iteration = 1 N
3:
Training example xi , yi dataset
4:
loss node build computation graph(xi , yi , parameters)
5:
loss node.forward()
6:
gradients loss node().backward()
7:
parameters update parameters(parameters, gradients)
8: return parameters.
Here, build computation graph user-defined function builds computation
graph given input, output network structure, returning single loss node.
update parameters optimizer specific update rule. recipe specifies new
graph created training example. accommodates cases network
structure varies training example, recurrent recursive neural networks,
discussed Sections 10 12. networks fixed structures, MLPs,
may efficient create one base computation graph vary inputs
expected outputs examples.
6.2.5 Network Composition
long networks output vector (1 k matrix), trivial compose networks
making output one network input another, creating arbitrary networks.
computation graph abstractions makes ability explicit: node computation
graph computation graph designated output node. One
376

fiA Primer Neural Networks NLP

design arbitrarily deep complex networks, able easily evaluate train
thanks automatic forward gradient computation. makes easy define
train networks structured outputs multi-objective training, discuss
Section 7, well complex recurrent recursive networks, discussed Sections
1012.
6.3 Optimization Issues
gradient computation taken care of, network trained using SGD another
gradient-based optimization algorithm. function optimized convex,
long time training neural networks considered black art done
selected few. Indeed, many parameters affect optimization process, care
taken tune parameters. tutorial intended comprehensive
guide successfully training neural networks, list prominent issues.
discussion optimization techniques algorithms neural networks, refer
book Bengio et al. (2015, ch. 8). theoretical discussion analysis, refer
work Glorot Bengio (2010). various practical tips recommendations,
see work LeCun et al. (1998a) Bottou (2012).
6.3.1 Initialization
non-convexity loss function means optimization procedure may get stuck
local minimum saddle point, starting different initial points (e.g.
different random values parameters) may result different results. Thus,
advised run several restarts training starting different random initializations,
choosing best one based development set.59 amount variance
results different different network formulations datasets, cannot predicted
advance.
magnitude random values important effect success training.
effective scheme due Glorot Bengio (2010), called xavier initialization
Glorots first name, suggests initializing weight matrix W Rdin dout as:


#

6
6
W U
, +
din + dout
din + dout
"

(24)

U [a, b] uniformly sampled random value range [a, b]. suggestion
based properties tanh activation function, works well many occasions,
preferred default initialization method many.
Analysis et al. (2015) suggests using ReLU non-linearities, weights
initialized
sampling zero-mean Gaussian distribution whose standard
q
2
deviation
din . initialization found et al work better xavier
initialization image classification task, especially deep networks involved.
59. debugging, reproducibility results, advised used fixed random seed.

377

fiGoldberg

6.3.2 Vanishing Exploding Gradients
deep networks, common error gradients either vanish (become exceedingly
close 0) explode (become exceedingly high) propagate back computation graph. problem becomes severe deeper networks, especially
recursive recurrent networks (Pascanu, Mikolov, & Bengio, 2012). Dealing
vanishing gradients problem still open research question. Solutions include making
networks shallower, step-wise training (first train first layers based auxiliary
output signal, fix train upper layers complete network based
real task signal), performing batch-normalization (Ioffe & Szegedy, 2015) (for every
minibatch, normalizing inputs network layers zero mean unit
variance) using specialized architectures designed assist gradient flow (e.g.,
LSTM GRU architectures recurrent networks, discussed Section 11). Dealing
exploding gradients simple effective solution: clipping gradients
norm exceeds given threshold. Let g gradients parameters
network, kgk L2 norm. Pascanu et al. (2012) suggest set: g threshold
kgk g
kgk > threshold.
6.3.3 Saturation Dead Neurons
Layers tanh sigmoid activations become saturated resulting output values
layer close one, upper-limit activation function. Saturated
neurons small gradients, avoided. Layers ReLU activation
cannot saturated, die values negative thus clipped zero
inputs, resulting gradient zero layer. network train
well, advisable monitor network layers many saturated dead neurons.
Saturated neurons caused large values entering layer. may controlled
changing initialization, scaling range input values, changing
learning rate. Dead neurons caused signals entering layer negative (for
example happen large gradient update). Reducing learning rate
help situation. saturated layers, another option normalize values
saturated layer activation, i.e. instead g(h) = tanh(h) using g(h) = k tanh(h)
tanh(h)k .
Layer normalization effective measure countering saturation, expensive
terms gradient computation. related technique batch normalization, due Ioffe
Szegedy (2015), activations layer normalized
mean 0 variance 1 across mini-batch. batch-normalization techniques
became key component effective training deep networks computer vision.
writing, less popular natural language applications.
6.3.4 Shuffling
order training examples presented network important.
SGD formulation specifies selecting random example turn. practice,
implementations go training example order. advised shuffle training
examples pass data.
378

fiA Primer Neural Networks NLP

6.3.5 Learning Rate
Selection learning rate important. large learning rates prevent network
converging effective solution. small learning rates take long time
converge. rule thumb, one experiment range initial learning rates
range [0, 1], e.g. 0.001, 0.01, 0.1, 1. Monitor networks loss time, decrease
learning rate loss stops improving. Learning rate scheduling decreases rate
function number observed minibatches. common schedule dividing initial
learning rate iteration number. Leon Bottou (2012) recommends using learning
rate form = 0 (1 + 0 t)1 0 initial learning rate, learning
rate use tth training example, additional hyperparameter.
recommends determining good value 0 based small sample data prior
running entire dataset.
6.3.6 Minibatches
Parameter updates occur either every training example (minibatches size 1) every k
training examples. problems benefit training larger minibatch sizes.
terms computation graph abstraction, one create computation graph
k training examples, connecting k loss nodes averaging node,
whose output loss minibatch. Large minibatched training
beneficial terms computation efficiency specialized computing architectures
GPUs, replacing vector-matrix operations matrix-matrix operations. beyond
scope tutorial.
6.4 Regularization
Neural network models many parameters, overfitting easily occur. Overfitting
alleviated extent regularization. common regularization method
L2 regularization, placing squared penalty parameters large values adding
additive 2 kk2 term objective function minimized, set
model parameters, k k2 squared L2 norm (sum squares values),
hyperparameter controlling amount regularization.
recently proposed alternative regularization method dropout (Hinton, Srivastava,
Krizhevsky, Sutskever, & Salakhutdinov, 2012). dropout method designed prevent
network learning rely specific weights. works randomly dropping
(setting 0) half neurons network (or specific layer) training
example. Work Wager et al. (2013) establishes strong connection dropout
method L2 regularization.
dropout technique one key factors contributing strong results
neural-network methods image classification tasks (Krizhevsky, Sutskever, & Hinton,
2012), especially combined ReLU activation units (Dahl, Sainath, & Hinton,
2013). dropout technique effective NLP applications neural networks.
379

fiGoldberg

7. Cascading Multi-task Learning
combination online training methods automatic gradient computations using
computation graph abstraction allows easy implementation model cascading,
parameter sharing multi-task learning.
7.1 Model Cascading
powerful technique large networks built composing smaller
component networks. example, may feed-forward network predicting
part speech word based neighbouring words and/or characters compose
it. pipeline approach, would use network predicting parts speech,
feed predictions input features neural network syntactic chunking
parsing. Instead, could think hidden layers network encoding
captures relevant information predicting part speech. cascading
approach, take hidden layers network connect (and part
speech prediction themselves) inputs syntactic network.
larger network takes input sequences words characters, outputs
syntactic structure. computation graph abstraction allows us easily propagate
error gradients syntactic task loss way back characters.
combat vanishing gradient problem deep networks, well make better
use available training material, individual component networks parameters
bootstrapped training separately relevant task, plugging
larger network tuning. example, part-of-speech predicting network
trained accurately predict parts-of-speech relatively large annotated corpus,
plugging hidden layer syntactic parsing network less training
data available. case training data provide direct supervision tasks,
make use training creating network two outputs, one task,
computing separate loss output, summing losses single node
backpropagate error gradients.
Model cascading common using convolutional, recursive recurrent
neural networks, where, example, recurrent network used encode sentence
fixed sized vector, used input another network. supervision
signal recurrent network comes primarily upper network consumes
recurrent networks output inputs.
7.2 Multi-task Learning
used related prediction tasks necessarily feed one another,
believe information useful one type prediction useful
tasks. example, chunking, named entity recognition (NER)
language modeling examples synergistic tasks. Information predicting chunk
boundaries, named-entity boundaries next word sentence rely
shared underlying syntactic-semantic representation. Instead training separate network
task, create single network several outputs. common approach
multi-layer feed-forward network, whose final hidden layer (or concatenation
380

fiA Primer Neural Networks NLP

hidden layers) passed different output layers. way, parameters
network shared different tasks. Useful information learned one
task help disambiguate tasks. Again, computation graph abstraction
makes easy construct networks compute gradients them,
computing separate loss available supervision signal, summing
losses single loss used computing gradients. case several
corpora, different kind supervision signal (e.g. one corpus NER
another chunking), training procedure shuffle available training
example, performing gradient computation updates respect different loss
every turn. Multi-task learning context language-processing introduced
discussed work Collobert et al. (2011). examples cascaded Multi-task
learning feed forward network, see work Zhang Weiss (2016). context
recurrent neural networks, see work Luong, Le, Sutskever, Vinyals, Kaiser
(2015) Sgaard Goldberg (2016).

8. Structured Output Prediction
Many problems NLP involve structured outputs: cases desired output
class label distribution class labels, structured object sequence,
tree graph. Canonical examples sequence tagging (e.g. part-of-speech tagging)
sequence segmentation (chunking, NER), syntactic parsing. section, discuss
feed-forward neural network models used structured tasks. later sections
discuss specialized neural network models dealing sequences (Section 10)
trees (Section 12).
8.1 Greedy Structured Prediction
greedy approach structured prediction decompose structure prediction
problem sequence local prediction problems training classifier perform
local decision. test time, trained classifier used greedy manner. Examples
approach left-to-right tagging models (Gimenez & Marquez, 2004) greedy
transition-based parsing (Nivre, 2008). approaches easily adapted use neural
networks simply replacing local classifier linear classifier SVM
logistic regression model neural network, demonstrated Chen Manning
(2014) Lewis Steedman (2014).
greedy approaches suffer error propagation, mistakes early decisions
carry influence later decisions. overall higher accuracy achievable nonlinear neural network classifiers helps offsetting problem extent. addition,
training techniques proposed mitigating error propagation problem either
attempting take easier predictions harder ones (the easy-first approach Goldberg & Elhadad, 2010) making training conditions similar testing conditions
exposing training procedure inputs result likely mistakes (Hal Daume III,
Langford, & Marcu, 2009; Goldberg & Nivre, 2013). effective training
greedy neural network models, demonstrated Ma, Zhang, Zhu (2014) (easy-first
tagger) Ballesteros, Goldberg, Dyer, Smith (2016) (dynamic oracle training
greedy dependency parsing).
381

fiGoldberg

8.2 Search Based Structured Prediction
common approach predicting natural language structures search based. indepth discussion search-based structure prediction NLP, see book Smith (2011).
techniques easily adapted use neural-network. neural-networks
literature, models discussed framework energy based learning (LeCun
et al., 2006, Section 7). presented using setup terminology familiar
NLP community.
Search-based structured prediction formulated search problem possible structures:
predict(x) = arg max score(x, y)

(25)

yY(x)

x input structure, output x (in typical example x sentence
tag-assignment parse-tree sentence), Y(x) set valid
structures x, looking output maximize score
x, pair.
scoring function defined linear model:
score(x, y) = w (x, y)

(26)

feature extraction function w weight vector.
order make search optimal tractable, structure decomposed
parts, feature function defined terms parts, (p) part-local
feature extraction function:
X
(x, y) =
(p)
(27)
pparts(x,y)

part scored separately, structure score sum component
parts scores:

score(x, y) =w (x, y) = w

X

(p) =

py

X
py

w (p) =

X

score(p)

(28)

py

p shorthand p parts(x, y). decomposition parts
exists inference algorithm allows efficient search best scoring
structure given scores individual parts.
One trivially replace linear scoring function parts neuralnetwork:

score(x, y) =

X

score(p) =

py

X

NN(c(p))

py

c(p) maps part p din dimensional vector.
case one hidden-layer feed-forward network:
382

(29)

fiA Primer Neural Networks NLP

score(x, y) =

X

NNMLP1 (c(p)) =

X
(g(c(p)W1 + b1 ))w

(30)

py

py

c(p) Rdin , W1 Rdin d1 , b1 Rd1 , w Rd1 . common objective structured
prediction making gold structure score higher structure 0 , leading
following (generalized perceptron) loss:

max
score(x, 0 ) score(x, y)
0


(31)

terms implementation, means: create computation graph CGp
possible parts, calculate score. Then, run inference scored parts
find best scoring structure 0 . Connect output nodes computation graphs
corresponding parts gold (predicted) structure (y 0 ) summing node CGy
(CG0y ). Connect CGy CG0y using minus node, CGl , compute gradients.
argued LeCun et al. (2006, Section 5), generalized perceptron loss may
good loss function training structured prediction neural networks
margin, margin-based hinge loss preferred:

max(0, + max
score(x, 0 ) score(x, y))
0
6=y

(32)

trivial modify implementation work hinge loss.
Note cases lose nice properties linear model. particular,
model longer convex. expected, even simplest non-linear neural
network already non-convex. Nonetheless, could still use standard neural-network
optimization techniques train structured model.
Training inference slower, evaluate neural network (and take
gradients) |parts(x, y)| times.
Structured prediction vast field beyond scope tutorial, loss
functions, regularizers methods described by, e.g., Smith (2011), cost-augmented
decoding, easily applied adapted neural-network framework.60
8.2.1 Probabilistic Objective (CRF)
probabilistic framework (conditional random fields, CRF), treat parts
scores clique potential (see discussions Smith, 2011 Lafferty, McCallum, &
Pereira, 2001) define score structure be:
60. One keep mind resulting objectives longer convex, lack formal guarantees bounds associated convex optimization problems. Similarly, theory, learning bounds
guarantees associated algorithms automatically transfer neural versions.

383

fiGoldberg

P
exp( py score(p))
P
scorecrf (x, y) = P (y|x) = P
0 Y(x) exp( py 0 score(p))
P
exp( py NN((p)))
P
=P
0 Y(x) exp( py 0 NN((p)))

(33)

scoring function defines conditional distribution P (y|x),
P wish set parameters network corpus conditional log likelihood (xi ,yi )training log P (yi |xi )
maximized.
loss given training example (x, y) then: log scorecrf (x, y). Taking
gradient respect loss involved building associated computation
graph. tricky part denominator (the partition function) requires summing
potentially exponentially many structures Y. However, problems,
dynamic programming algorithm exists efficiently solving summation polynomial
time (i.e. forward-backward viterbi recurrences sequences CKY insideoutside recurrences tree structures). algorithm exists, adapted
create polynomial-size computation graph.
efficient enough algorithm computing partition function available,
approximate methods used. example, one may use beam search inference,
partition function sum structures remaining beam instead
exponentially large Y(x).
Sequence-level CRFs neural-network clique potentials discussed Peng, Bo,
Xu (2009) Do, Arti, others (2010), applied sequence labeling
biological data, OCR data speech signals, Wang Manning (2013)
apply traditional natural language tagging tasks (chunking NER). hinge
based approach used Pei et al. (2015) arc-factored dependency parsing,
probabilistic approach Durrett Klein (2015) CRF constituency parser.
approximate beam-based partition function effectively used Zhou et al. (2015)
transition based parser.
8.2.2 Reranking
searching possible structures intractable, inefficient hard integrate
model, reranking methods often used. reranking framework (Charniak
& Johnson, 2005; Collins & Koo, 2005) base model used produce list kbest scoring structures. complex model trained score candidates
k-best list best structure respect gold one scored highest.
search performed k items rather exponential space,
complex model condition (extract features from) arbitrary aspects scored
structure. Reranking methods natural candidates structured prediction using neuralnetwork models, allow modeler focus feature extraction network
structure, removing need integrate neural network scoring decoder.
Indeed, reranking methods often used experimenting neural models
straightforward integrate decoder, convolutional, recurrent recursive
networks, discussed later sections. Works using reranking approach
384

fiA Primer Neural Networks NLP

include Schwenk et al. (2006), Socher et al. (2013), Auli et al. (2013), Le
Zuidema (2014) Zhu et al. (2015a).
8.2.3 MEMM Hybrid Approaches
formulations are, course, possible. example, MEMM (McCallum,
Freitag, & Pereira, 2000) trivially adapted neural network world replacing
logistic regression (Maximum Entropy) component MLP.
Hybrid approaches neural networks linear models explored.
particular, Weiss et al. (2015) report strong results transition-based dependency parsing
two-stage model. first stage, static feed-forward neural network (MLP2)
trained perform well individual decisions structured problem
isolation. second stage, neural network model held fixed, different layers
(output well hidden layer vectors) input concatenated used
input features linear structured perceptron model (Collins, 2002) trained
perform beam-search best resulting structure. clear training
regime effective training single structured-prediction neural network, use
two simpler, isolated models allowed researchers perform much extensive
hyper-parameter search (e.g. tuning layer sizes, activation functions, learning rates
on) model feasible complicated networks.

9. Convolutional Layers
Sometimes interested making predictions based ordered sets items (e.g.
sequence words sentence, sequence sentences document on).
Consider example predicting sentiment (positive, negative neutral) sentence.
sentence words informative sentiment, words less
informative, good approximation, informative clue informative regardless
position sentence. would feed sentence words
learner, let training process figure important clues. One possible solution
feeding CBOW representation fully connected network MLP. However,
downside CBOW approach ignores ordering information completely,
assigning sentences good, actually quite bad bad,
actually quite good exact representation. global position
indicators good bad matter classification task,
local ordering words (that word appears right word bad)
important. naive approach would suggest embedding word-pairs (bi-grams) rather
words, building CBOW embedded bigrams. architecture
could effective, result huge embedding matrices, scale longer ngrams, suffer data sparsity problems share statistical strength
different n-grams (the embedding quite good good completely
independent one another, learner saw one training,
able deduce anything based component words).
convolution-and-pooling (also called convolutional neural networks, CNNs) architecture
elegant robust solution modeling problem. convolutional neural network
designed identify indicative local predictors large structure, combine
385

fiGoldberg

produce fixed size vector representation structure, capturing local aspects
informative prediction task hand.
Convolution-and-pooling architectures (LeCun & Bengio, 1995) evolved neural
networks vision community, showed great success object detectors recognizing object predefined category (cat, bicycles) regardless position
image (Krizhevsky et al., 2012). applied images, architecture using
2-dimensional (grid) convolutions. applied text, mainly concerned
1-d (sequence) convolutions. Convolutional networks introduced NLP community pioneering work Collobert, Weston colleagues (2011) used
semantic-role labeling, later Kalchbrenner et al. (2014) Kim (2014) used
sentiment question-type classification.
9.1 Basic Convolution + Pooling
main idea behind convolution pooling architecture language tasks apply
non-linear (learned) function instantiation k-word sliding window
sentence. function (also called filter) transforms window k words
dimensional vector captures important properties words window (each
dimension sometimes referred literature channel). Then, pooling
operation used combine vectors resulting different windows single
d-dimensional vector, taking max average value observed
channels different windows. intention focus important
features sentence, regardless location. d-dimensional vector
fed network used prediction. gradients propagated
back networks loss training process used tune parameters
filter function highlight aspects data important task
network trained for. Intuitively, sliding window run sequence,
filter function learns identify informative k-grams.
formally, consider sequence words x = x1 , . . . , xn , corresponding demb dimensional word embedding v(xi ). 1d convolution layer61 width k works
moving sliding window size k sentence, applying filter
window sequence [v(xi ); v(xi+1 ); . . . ; v(xi+k1 )]. filter function usually
linear transformation followed non-linear activation function.
Let concatenated vector ith window wi = [v(xi ); v(xi+1 ); . . . ; v(xi+k1 )],
wi Rkdemb . Depending whether pad sentence k 1 words side,
may get either = n k + 1 (narrow convolution) = n + k + 1 windows (wide
convolution) (Kalchbrenner et al., 2014). result convolution layer vectors
p1 , . . . , pm , pi Rdconv where:
pi = g(wi W + b)

(34)

g non-linear activation function applied element-wise, W Rkdemb dconv
b Rdconv parameters network. pi dconv dimensional vector, encoding
61. 1d refers convolution operating 1-dimensional inputs sequences, opposed 2d
convolutions applied images.

386

fiA Primer Neural Networks NLP

63
W

max

quick brown fox jumped lazy dog
quick brown

MUL+tanh

quick brown fox

MUL+tanh

brown fox jumped

MUL+tanh

fox jumped

MUL+tanh

jumped

MUL+tanh

lazy

MUL+tanh

lazy dog

MUL+tanh

convolution

pooling

Figure 4: 1d convolution+pooling sentence quick brown fox jumped
lazy dog. narrow convolution (no padding added sentence)
window size 3. word translated 2-dim embedding vector
(not shown). embedding vectors concatenated, resulting 6-dim
window representations. seven windows transfered 6 3
filter (linear transformation followed element-wise tanh), resulting seven
3-dimensional filtered representations. Then, max-pooling operation applied,
taking max dimension, resulting final 3-dimensional pooled
vector.

information wi . Ideally, dimension captures different kind indicative information. vectors combined using max pooling layer, resulting single
dconv dimensional vector c.
cj = max pi [j]
1<im

(35)

pi [j] denotes jth component pi . effect max-pooling operation get
salient information across window positions. Ideally, dimension specialize
particular sort predictors, max operation pick important
predictor type.
Figure 4 provides illustration process.
resulting vector c representation sentence dimension
reflects salient information respect prediction task. c fed
downstream network layers, perhaps parallel vectors, culminating
output layer used prediction. training procedure network calculates
loss respect prediction task, error gradients propagated
way back pooling convolution layers, well embedding layers. 62
62. Besides useful prediction, by-product training procedure set parameters W, B
embeddings v() used convolution pooling architecture encode arbitrary length

387

fiGoldberg

max-pooling common pooling operation text applications,
pooling operations possible, second common operation average
pooling, taking average value index instead max.
9.2 Dynamic, Hierarchical k-max Pooling
Rather performing single pooling operation entire sequence, may want
retain positional information based domain understanding prediction
problem hand. end, split vectors pi ` distinct groups, apply
pooling separately group, concatenate ` resulting dconv -dimensional
vectors c1 , . . . , c` . division pi groups performed based domain knowledge. example, may conjecture words appearing early sentence
indicative words appearing late. split sequence ` equally
sized regions, applying separate max-pooling region. example, Johnson
Zhang (2015) found classifying documents topics, useful 20
average-pooling regions, clearly separating initial sentences (where topic usually
introduced) later ones, sentiment classification task single max-pooling
operation entire sentence optimal (suggesting one two strong
signals enough determine sentiment, regardless position sentence).
Similarly, relation extraction kind task may given two words asked
determine relation them. could argue words first word,
words second word, words provide three different kinds
information (Chen et al., 2015). thus split pi vectors accordingly, pooling
separately windows resulting group.
Another variation using hierarchy convolutional layers, succession convolution pooling layers, stage applies convolution sequence,
pools every k neighboring vectors, performs convolution resulting pooled sequence,
applies another convolution on. architecture allows sensitivity increasingly
larger structures.
Finally, Kalchbrenner et al. (2014) introduced k-max pooling operation,
top k values dimension retained instead best one, preserving
order appeared text. example a, consider following matrix:

1
9

2

7
3

2
6
3
8
4


3
5

1

1
1



1-max pooling column vectors result 9 8 5 , 2-max pooling


9 6 3
result following matrix:
whose rows concatenated
7 8 5


9 6 3 7 8 5
sentences fixed-size vectors, sentences share kind predictive information
close other.

388

fiA Primer Neural Networks NLP

k-max pooling operation makes possible pool k active indicators
may number positions apart; preserves order features, insensitive
specific positions. discern finely number times feature
highly activated (Kalchbrenner et al., 2014).
9.3 Variations
Rather single convolutional layer, several convolutional layers may applied
parallel. example, may four different convolutional layers, different
window size range 25, capturing n-gram sequences varying lengths. result
convolutional layer pooled, resulting vectors concatenated
fed processing (Kim, 2014).
convolutional architecture need restricted linear ordering sentence. example, et al. (2015) generalize convolution operation work
syntactic dependency trees. There, window around node syntactic tree,
pooling performed different nodes. Similarly, Liu et al. (2015) apply
convolutional architecture top dependency paths extracted dependency trees. Le
Zuidema (2015) propose perform max pooling vectors representing different
derivations leading chart item chart parser.

10. Recurrent Neural Networks Modeling Sequences Stacks
dealing language data, common work sequences, words
(sequences letters), sentences (sequences words) documents. saw feedforward networks accommodate arbitrary feature functions sequences
use vector concatenation vector addition (CBOW). particular, CBOW representations allows encode arbitrary length sequences fixed sized vectors. However,
CBOW representation quite limited, forces one disregard order features. convolutional networks allow encoding sequence fixed size vector.
representations derived convolutional networks improvement
CBOW representation offer sensitivity word order, order sensitivity
restricted mostly local patterns, disregards order patterns far apart
sequence.
Recurrent neural networks (RNNs) (Elman, 1990) allow representing arbitrarily sized
structured inputs fixed-size vector, paying attention structured properties
input.
10.1 RNN Abstraction
use xi:j denote sequence vectors xi , . . . , xj . RNN abstraction takes
input ordered list input vectors x1 , ..., xn together initial state vector s0 ,
returns ordered list state vectors s1 , ..., sn , well ordered list output
vectors y1 , ..., yn . output vector yi function corresponding state vector
si . input vectors xi presented RNN sequential fashion, state
vector si output vector yi represent state RNN observing inputs
x1:i . output vector yi used prediction. example, model
389

fiGoldberg

predicting conditional probability event e given sequence m1:i defined
p(e = j|x1:i ) = softmax(yi W + b)[j], jth element output vector resulting
softmax operation. RNN model provides framework conditioning
entire history x1 , . . . , xi without resorting Markov assumption traditionally
used modeling sequences.63 Indeed, RNN-based language models result good
perplexity scores compared n-gram based models.
Mathematically, recursively defined function R takes input state
vector si input vector xi+1 , results new state vector si+1 . additional
function used map state vector si output vector yi .64 constructing
RNN, much constructing feed-forward network, one specify dimension
inputs xi well dimensions outputs yi . dimensions states
si function output dimension.65
RNN(s0 , x1:n ) =s1:n , y1:n
si = R(si1 , xi )

(36)

yi = O(si )
xi Rdin , yi Rdout , si Rf (dout )
functions R across sequence positions, RNN keeps
track states computation state vector kept passed
invocations R.
Graphically, RNN traditionally presented Figure 5.
yi

si1

R,O



xi

si

Figure 5: Graphical representation RNN (recursive).
63. kth-order Markov assumption states observation time independent observations
times (k + j) j > 0 given observations times 1, , k. assumption
basis many sequence modeling technique n-gram models hidden markov models.
64. Using function somewhat non-standard, used order unify different RNN models
presented next section. Simple RNN (Elman RNN) GRU architectures,
identity mapping, LSTM architecture selects fixed subset state.
65. RNN architectures state dimension independent output dimension
possible, current popular architectures, including Simple RNN, LSTM GRU
follow flexibility.

390

fiA Primer Neural Networks NLP

presentation follows recursive definition, correct arbitrary long sequences.
However, finite sized input sequence (and input sequences deal finite)
one unroll recursion, resulting structure Figure 6.
y1

s0

R,O

x1

y3

y2

s1

R,O

s2

R,O

x2

y4

s3

x3

R,O

x4

y5

s4

R,O

s5

x5



Figure 6: Graphical representation RNN (unrolled).

usually shown visualization, include parameters order
highlight fact parameters shared across time steps. Different
instantiations R result different network structures, exhibit different
properties terms running times ability trained effectively using
gradient-based methods. However, adhere abstract interface.
provide details concrete instantiations R Simple RNN, LSTM
GRU Section 11. that, lets consider modeling RNN abstraction.
First, note value si based entire input x1 , ..., xi . example,
expanding recursion = 4 get:

s4 =R(s3 , x4 )


z }|3 {
=R(R(s2 , x3 ), x4 )


z }|2 {
=R(R(R(s1 , x2 ), x3 ), x4 )

(37)



z }|1 {
=R(R(R(R(s0 , x1 ), x2 ), x3 ), x4 )
Thus, sn (as well yn ) could thought encoding entire input sequence.66
encoding useful? depends definition usefulness. job network
training set parameters R state conveys useful information
task tying solve.
66. Note that, unless R specifically designed this, likely later elements input
sequence stronger effect sn earlier ones.

391

fiGoldberg

10.2 RNN Training
Viewed Figure 6 easy see unrolled RNN deep neural
network (or rather, large computation graph somewhat complex nodes),
parameters shared across many parts computation. train
RNN network, then, need create unrolled computation graph
given input sequence, add loss node unrolled graph, use backward
(backpropagation) algorithm compute gradients respect loss.
procedure referred RNN literature backpropagation time, BPTT
(Werbos, 1990).67 various ways supervision signal applied.
10.2.1 Acceptor
One option base supervision signal final output vector, yn . Viewed
way, RNN acceptor. observe final state, decide outcome.68
example, consider training RNN read characters word one one
use final state predict part-of-speech word (this inspired Ling
et al., 2015b), RNN reads sentence and, based final state decides
conveys positive negative sentiment (this inspired Wang et al., 2015b) RNN
reads sequence words decides whether valid noun-phrase. loss
cases defined terms function yn = O(sn ), error gradients
backpropagate rest sequence (see Figure 7).69 loss take
familiar form cross entropy, hinge, margin, etc.
10.2.2 Encoder
Similar acceptor case, encoder supervision uses final output vector, yn .
However, unlike acceptor, prediction made solely basis final
vector, final vector treated encoding information sequence,
used additional information together signals. example, extractive
document summarization system may first run document RNN, resulting
67. Variants BPTT algorithm include unrolling RNN fixed number input symbols
time: first unroll RNN inputs x1:k , resulting s1:k . Compute loss, backpropagate
error network (k steps back). Then, unroll inputs xk+1:2k , time using sk
initial state, backpropagate error k steps, on. strategy based
observations Simple-RNN variant, gradients k steps tend vanish (for large enough
k), omitting negligible. procedure allows training arbitrarily long sequences.
RNN variants LSTM GRU designed specifically mitigate vanishing
gradients problem, fixed size unrolling less motivated, yet still used, example
language modeling book without breaking sentences. similar variant unrolls
network entire sequence forward step, propagates gradients back k steps
position.
68. terminology borrowed Finite-State Acceptors. However, RNN potentially infinite
number states, making necessary rely function lookup table mapping states
decisions.
69. kind supervision signal may hard train long sequences, especially SimpleRNN, vanishing gradients problem. generally hard learning task,
tell process parts input focus.

392

fiA Primer Neural Networks NLP

loss
predict &
calc loss
y5
s0

R,O

s1

x1

R,O

s2

x2

R,O

s3

x3

R,O

s4

x4

R,O

x5

Figure 7: Acceptor RNN Training Graph.
vector yn summarizing entire document. Then, yn used together
features order select sentences included summarization.
10.2.3 Transducer
Another option treat RNN transducer, producing output input
reads in. Modeled way, compute local loss signal Llocal (yi , yi )
outputs yP
based true label yi . loss unrolled sequence be:
L(y1:n
, y1:n ) = ni=1 Llocal (yi , yi ), using another combination rather sum
average weighted average (see Figure 8). One example transducer
sequence tagger, take xi:n feature representations n words
sentence, yi input predicting tag assignment word based
words 1:i. CCG super-tagger based architecture provides state-of-the art
CCG super-tagging results (Xu et al., 2015).
loss

sum

predict &
calc loss

predict &
calc loss

y1
s0

R,O

x1

predict &
calc loss

y2
s1

R,O

x2

predict &
calc loss

y3
s2

R,O

x3

predict &
calc loss

y4
s3

R,O

x4

y5
s4

R,O

x5

Figure 8: Transducer RNN Training Graph.
natural use-case transduction setup language modeling,
sequence words x1:i used predict distribution (i + 1)th word. RNN based
language models shown provide better perplexities traditional language models
(Mikolov et al., 2010; Sundermeyer, Schluter, & Ney, 2012; Mikolov, 2012; Jozefowicz,
Vinyals, Schuster, Shazeer, & Wu, 2016).
Using RNNs transducers allows us relax Markov assumption traditionally taken language models HMM taggers, condition entire prediction
393

fiGoldberg

history. power ability condition arbitrarily long histories demonstrated
generative character-level RNN models, text generated character character, character conditioning previous ones (Sutskever, Martens, & Hinton, 2011).
generated texts show sensitivity properties captured n-gram language
models, including line lengths nested parenthesis balancing. good demonstration
analysis properties RNN-based character level language models, see work
Karpathy, Johnson, Li (2015).
10.2.4 Encoder - Decoder
Finally, important special case encoder scenario Encoder-Decoder framework
(Cho, van Merrienboer, Bahdanau, & Bengio, 2014a; Sutskever et al., 2014). RNN
used encode sequence vector representation yn , vector representation
used auxiliary input another RNN used decoder. example,
machine-translation setup first RNN encodes source sentence vector
representation yn , state vector fed separate (decoder) RNN
trained predict (using transducer-like language modeling objective) words
target language sentence based previously predicted words well yn .
supervision happens decoder RNN, gradients propagated
way back encoder RNN (see Figure 9).
loss

sum

predict &
calc loss

predict &
calc loss

y1
sd0

RD ,OD

y2
sd1

,OE

x1

se1

sd2

RD ,OD

x2

,OE

x2

se2

predict &
calc loss

y3

RD ,OD

x1

se0

predict &
calc loss

y4
sd3

RD ,OD

x3

,OE

x3

se3

predict &
calc loss
y5
sd4

RD ,OD

x4

,OE

x4

se4

x5

,OE

se5

x5

Figure 9: Encoder-Decoder RNN Training Graph.
approach shown surprisingly effective Machine Translation (Sutskever
et al., 2014) using LSTM RNNs. order technique work, Sutskever et al. found
effective input source sentence reverse, xn corresponds first
394

fiA Primer Neural Networks NLP

word sentence. way, easier second RNN establish relation
first word source sentence first word target sentence.
Another use-case encoder-decoder framework sequence transduction. Here,
order generate tags t1 , . . . , tn , encoder RNN first used encode sentence
x1:n fixed sized vector. vector fed initial state vector another
(transducer) RNN, used together x1:n predict label ti position
i. approach used Filippova, Alfonseca, Colmenares, Kaiser, Vinyals (2015)
model sentence compression deletion.
10.3 Multi-layer (Stacked) RNNs
RNNs stacked layers, forming grid (Hihi & Bengio, 1996). Consider k RNNs,
j
RNN1 , . . . , RNNk , jth RNN states sj1:n outputs y1:n
. input
first RNN x1:n , input jth RNN (j 2) outputs RNN
j1
k .
it, y1:n
. output entire formation output last RNN, y1:n
layered architectures often called deep RNNs. visual representation 3-layer
RNN given Figure 10.
y1

y2

y13
s30

R3 ,O3

y23
s31

y12
s20

R2 ,O2

R1 ,O1

x1

R3 ,O3

R2 ,O2

R3 ,O3

R2 ,O2

R1 ,O1

x2

R1 ,O1

x3

R3 ,O3

y53
s34

y42
s23

y31
s12

y5

y43
s33

y32
s22

y21
s11

y4

y33
s32

y22
s21

y11
s10

y3

R2 ,O2

R1 ,O1

x4

s35

y52
s24

y41
s13

R3 ,O3

R2 ,O2

s25

y51
s14

R1 ,O1

s15

x5

Figure 10: 3-layer (deep) RNN architecture.
theoretically clear additional power gained deeper
architecture, observed empirically deep RNNs work better shallower ones
tasks. particular, Sutskever et al. (2014) report 4-layers deep architecture crucial achieving good machine-translation performance encoder-decoder
framework. Irsoy Cardie (2014) report improved results moving onelayer biRNN architecture several layers. Many works report result using
layered RNN architectures, explicitly compare 1-layer RNNs.
10.4 Bidirectional RNNs (biRNN)
useful elaboration RNN bidirectional-RNN (biRNN, commonly referred
biRNN) (Schuster & Paliwal, 1997; Graves, 2008).70 Consider task sequence
tagging sentence x1 , . . . , xn . RNN allows us compute function ith word
70. used specific RNN architecture LSTM, model called biLSTM.

395

fiGoldberg

xi based past words x1:i including it. However, following words
xi:n may useful prediction, evident common sliding-window approach
focus word categorized based window k words surrounding it. Much
RNN relaxes Markov assumption allows looking arbitrarily back
past, biRNN relaxes fixed window size assumption, allowing look arbitrarily far
past future.
Consider input sequence x1:n . biRNN works maintaining two separate states,
f
si sbi input position i. forward state sfi based x1 , x2 , . . . , xi ,
backward state sbi based xn , xn1 , . . . , xi . forward backward states
generated two different RNNs. first RNN (Rf , ) fed input sequence x1:n
is, second RNN (Rb , Ob ) fed input sequence reverse. state
representation si composed forward backward states.
output position based concatenation two output vectors
yi = [yif ; yib ] = [Of (sfi ); Ob (sbi )], taking account past future.
vector yi used directly prediction, fed part input
complex network. two RNNs run independently other, error gradients position flow forward backward two RNNs. visual
representation biRNN architecture given Figure 11.
ythe

ybrown

concat

concat

sb5

Rb ,Ob
y1f

sf0

Rf ,Of

xthe

concat
y4b

y5b
sb44

Rb ,Ob

sb33

Rf ,Of

Rb ,Ob
y3f

sf2

Rf ,Of

xbrown

xfox



concat
y3b

y2f
sf1

yjumped

yfox

concat
y2b

sb22

Rb ,Ob
y4f

sf3

Rf ,Of

xjumped

y1b
sb11

sb00

Rb ,Ob
y5f

sf4

sf5

Rf ,Of

x

Figure 11: biRNN sentence brown fox jumped ..
use biRNNs sequence tagging introduced NLP community Irsoy
Cardie (2014).
10.5 RNNs Representing Stacks
algorithms language processing, including transition-based parsing (Nivre,
2008), require performing feature extraction stack. Instead confined
looking k top-most elements stack, RNN framework used provide
fixed-sized vector encoding entire stack.
main intuition stack essentially sequence, stack state
represented taking stack elements feeding order RNN, resulting
final encoding entire stack. order computation efficiently (without
396

fiA Primer Neural Networks NLP

performing O(n) stack encoding operation time stack changes), RNN state
maintained together stack state. stack push-only, would
trivial: whenever new element x pushed stack, corresponding vector x
used together RNN state si order obtain new state si+1 . Dealing
pop operation challenging, solved using persistent-stack
data-structure (Okasaki, 1999; Goldberg, Zhao, & Huang, 2013). Persistent, immutable,
data-structures keep old versions intact modified. persistent stack
construction represents stack pointer head linked list. empty stack
empty list. push operation appends element list, returning new head.
pop operation returns parent head, keeping original list intact.
point view someone held pointer previous head, stack
change. subsequent push operation add new child node. Applying
procedure throughout lifetime stack results tree, root
empty stack path node root represents intermediary stack state.
Figure 12 provides example tree. process applied
computation graph construction, creating RNN tree structure instead chain
structure. Backpropagating error given node affect elements
participated stack node created, order. Figure 13 shows
computation graph stack-RNN corresponding last state Figure 12.
modeling approach proposed independently Dyer et al. (2015) Watanabe
Sumita (2015) transition-based dependency parsing.
head

head


head







(1) push

head




b

(2) push b

b



head

c





(3) push c

b

c





(4) pop

(5) push

head







b
head

(6) pop

c







b

c





b

c

b

head

e

e





c





b

f

c

head
(7) pop

(8) push e

(9) push f

Figure 12: immutable stack construction sequence operations push a; push b;
push c; pop; push d; pop; pop; push e; push f.

10.6 Note Reading Literature
Unfortunately, often case inferring exact model form reading
description research paper quite challenging. Many aspects models
397

fiGoldberg

ya,e

R,O

ya,e,f

sa,e

ya,b,d xe

sa

ya



R,O

xa

ya:b

sa

R,O

xb

sa:b

ya:c

R,O

sa,e,f

xf

sa,b,d

R,O

sa:b

R,O

xd

sa:c

xc

Figure 13: stack-RNN corresponding final state Figure 12.

yet standardized, different researchers use terms refer slightly
different things. list examples, inputs RNN either one-hot vectors
(in case embedding matrix internal RNN) embedded representations;
input sequence padded start-of-sequence and/or end-of-sequence symbols,
not; output RNN usually assumed vector expected
fed additional layers followed softmax prediction (as case
presentation tutorial), papers assume softmax part RNN itself;
multi-layer RNN, state vector either output top-most layer,
concatenation outputs layers; using encoder-decoder framework,
conditioning output encoder interpreted various different ways;
on. top that, LSTM architecture described next section many small
variants, referred common name LSTM. choices
made explicit papers, require careful reading, others still even
mentioned, hidden behind ambiguous figures phrasing.
reader, aware issues reading interpret model descriptions.
writer, aware issues well: either fully specify model mathematical
notation, refer different source model fully specified, source
available. using default implementation software package without knowing
details, explicit fact specify software package use. case,
dont rely solely figures natural language text describing model,
often ambiguous.
398

fiA Primer Neural Networks NLP

11. Concrete RNN Architectures
turn present three different instantiations abstract RN N architecture
discussed previous section, providing concrete definitions functions R O.
Simple RNN (SRNN), Long Short-Term Memory (LSTM) Gated
Recurrent Unit (GRU).
11.1 Simple RNN
simplest RNN formulation, known Elman Network Simple-RNN (S-RNN),
proposed Elman (1990) explored use language modeling Mikolov (2012).
S-RNN takes following form:
si =Rsrnn (si1 , xi ) = g(xi Wx + si1 Ws + b)
yi =Osrnn (si ) = si

(38)

si , yi Rds , xi Rdx , Wx Rdx ds , Ws Rds ds , b Rds
is, state position linear combination input position
previous state, passed non-linear activation (commonly tanh ReLU).
output position hidden state position.71
spite simplicity, Simple RNN provides strong results sequence tagging
(Xu et al., 2015) well language modeling. comprehensive discussion using
Simple RNNs language modeling, see PhD thesis Mikolov (2012).
11.2 LSTM
S-RNN hard train effectively vanishing gradients problem (Pascanu
et al., 2012). Error signals (gradients) later steps sequence diminish quickly
back-propagation process, reach earlier input signals, making hard
S-RNN capture long-range dependencies. Long Short-Term Memory (LSTM)
architecture (Hochreiter & Schmidhuber, 1997) designed solve vanishing gradients
problem. main idea behind LSTM introduce part state representation
memory cells (a vector) preserve gradients across time. Access
memory cells controlled gating components smooth mathematical functions
simulate logical gates. input state, gate used decide much new
input written memory cell, much current content
memory cell forgotten. Concretely, gate g [0, 1]n vector values
range [0, 1] multiplied component-wise another vector v Rn , result
added another vector. values g designed close either 0 1, i.e.
using sigmoid function. Indices v corresponding near-one values g allowed
pass, corresponding near-zero values blocked.
71. authors treat output position complicated function state, e.g. linear
transformation, MLP. presentation, transformation output
considered part RNN, separate computations applied RNNs output.

399

fiGoldberg

Mathematically, LSTM architecture defined as:72

sj = Rlstm (sj1 , xj ) =[cj ; hj ]
cj =cj1 f + g

hj = tanh(cj )

=(xj Wxi + hj1 Whi )

f =(xj Wxf + hj1 Whf )
=(xj W

xo

+ hj1 W

ho

(39)

)

g = tanh(xj Wxg + hj1 Whg )
yj = Olstm (sj ) =hj

sj R2dh , xi Rdx , cj , hj , i, f , o, g Rdh , Wx Rdx dh , Wh Rdh dh ,
symbol used denote component-wise product. state time j composed two vectors, cj hj , cj memory component hj hidden
state component. three gates, i, f o, controlling input, f orget output.
gate values computed based linear combinations current input xj
previous state hj1 , passed sigmoid activation function. update candidate g
computed linear combination xj hj1 , passed tanh activation function. memory cj updated: forget gate controls much previous
memory keep (cj1 f ), input gate controls much proposed update
keep (g i). Finally, value hj (which output yj ) determined based
content memory cj , passed tanh non-linearity controlled
output gate. gating mechanisms allow gradients related memory part cj
stay high across long time ranges.
discussion LSTM architecture see PhD thesis Alex Graves
(2008), well online-post Olah (2015b). analysis behavior
LSTM used character-level language model, see work Karpathy et al.
(2015).
explanation motivation behind gating mechanism LSTM
(and GRU) relation solving vanishing gradient problem recurrent neural
networks, see Sections 4.2 4.3 detailed course notes Cho (2015).
LSTMs currently successful type RNN architecture, responsible many state-of-the-art sequence modeling results. main competitor
LSTM-RNN GRU, discussed next.
72. many variants LSTM architecture presented here. example, forget gates
part original proposal Hochreiter Schmidhuber (1997), shown important
part architecture. variants include peephole connections gate-tying. overview
comprehensive empirical comparison various LSTM architectures see work Greff, Srivastava,
Koutnk, Steunebrink, Schmidhuber (2015).

400

fiA Primer Neural Networks NLP

11.2.1 Practical Considerations
training LSTM networks, Jozefowicz et al. (2015) strongly recommend always
initialize bias term forget gate close one. applying dropout
RNN LSTM, Zaremba et al. (2014) found crucial apply dropout
non-recurrent connection, i.e. apply layers
sequence positions.
11.3 GRU
LSTM architecture effective, quite complicated. complexity
system makes hard analyze, computationally expensive work with.
gated recurrent unit (GRU) recently introduced Cho et al. (2014b) alternative
LSTM. subsequently shown Chung et al. (2014) perform comparably
LSTM several (non textual) datasets.
LSTM, GRU based gating mechanism, substantially
fewer gates without separate memory component.
sj = RGRU (sj1 , xj ) =(1 z) sj1 + z sj
z =(xj Wxz + sj1 Wsz )
r =(xj Wxr + sj1 Wsr )
sj = tanh(xj Wxs + (sj1 r)Wsg )

(40)

yj = OGRU (sj ) =sj
sj , sj Rds , xi Rdx , z, r Rds , Wx Rdx ds , Ws Rds ds ,
One gate (r) used control access previous state sj1 compute proposed update sj . updated state sj (which serves output yj ) determined based
interpolation previous state sj1 proposal sj , proportions
interpolation controlled using gate z.73
GRU shown effective language modeling machine translation.
However, jury still GRU, LSTM possible alternative RNN
architectures, subject actively researched. empirical exploration
GRU LSTM architectures, see work Jozefowicz et al. (2015).
11.4 Variants
gated architectures LSTM GRU help alleviating vanishing gradients problem Simple RNN, allow RNNs capture dependencies span
long time ranges. researchers explore simpler architectures LSTM
GRU achieving similar benefits.
Mikolov et al. (2014) observed matrix multiplication si1 Ws coupled
nonlinearity g update rule R Simple RNN causes state vector si undergo
73. states often called h GRU literature.

401

fiGoldberg

large changes time step, prohibiting remembering information long
time periods. propose split state vector si slow changing component ci
(context units) fast changing component hi .74 slow changing component ci
updated according linear interpolation input previous component: ci =
(1 )xi Wx1 + ci1 , (0, 1). update allows ci accumulate previous
inputs. fast changing component hi updated similarly Simple RNN update
rule, changed take ci account well:75 hi = (xi Wx2 + hi1 Wh + ci Wc ).
Finally, output yi concatenation slow fast changing parts
state: yi = [ci ; hi ]. Mikolov et al. demonstrate architecture provides competitive
perplexities much complex LSTM language modeling tasks.
approach Mikolov et al. interpreted constraining block
matrix Ws S-RNN corresponding ci multiply identity matrix (see
Mikolov et al. (2014) details). Le, Jaitly, Hinton (2015) propose even simpler
approach: set activation function S-RNN ReLU, initialize biases b
zeroes matrix Ws identify matrix. causes untrained RNN copy
previous state current state, add effect current input xi set
negative values zero. setting initial bias towards state copying, training
procedure allows Ws change freely. Le et al. demonstrate simple modification
makes S-RNN comparable LSTM number parameters several
tasks, including language modeling.

12. Modeling Trees Recursive Neural Networks
RNN useful modeling sequences. language processing, often natural
desirable work tree structures. trees syntactic trees, discourse trees,
even trees representing sentiment expressed various parts sentence (Socher
et al., 2013). may want predict values based specific tree nodes, predict values
based root nodes, assign quality score complete tree part tree.
cases, may care tree structure directly rather reason spans
sentence. cases, tree merely used backbone structure helps
guide encoding process sequence fixed size vector.
recursive neural network (RecNN) abstraction (Pollack, 1990), popularized NLP
Richard Socher colleagues (Socher, Manning, & Ng, 2010; Socher, Lin, Ng, & Manning, 2011; Socher et al., 2013; Socher, 2014) generalization RNN sequences
(binary) trees.76
Much RNN encodes sentence prefix state vector, RecNN encodes
tree-node state vector Rd . use state vectors either predict
values corresponding nodes, assign quality values node, semantic
representation spans rooted nodes.
74. depart notation Mikolov et al. (2014) reuse symbols used LSTM description.
75. update rule diverges S-RNN update rule fixing non-linearity sigmoid
function, using bias term. However, changes discussed central
proposal.
76. presented terms binary parse trees, concepts easily transfer general recursively-defined
data structures, major technical challenge definition effective form R,
combination function.

402

fiA Primer Neural Networks NLP

main intuition behind recursive neural networks subtree represented dimensional vector, representation node p children c1 c2
function representation nodes: vec(p) = f (vec(c1 ), vec(c2 )), f
composition function taking two d-dimensional vectors returning single d-dimensional
vector. Much RNN state si used encode entire sequence x1 : i, RecNN
state associated tree node p encodes entire subtree rooted p. See Figure 14
illustration.

S=
combine

N P2 =

VP =

combine

N P1 =

V =

Figure 14: Illustration recursive neural network. representations V NP1
combined form representation VP. representations VP
NP2 combined form representation S.

12.1 Formal Definition
Consider binary parse tree n-word sentence. reminder, ordered,
unlabeled tree string x1 , . . . , xn represented unique set triplets (i, k, j),
s.t. k j. triplet indicates node spanning words xi:j parent
nodes spanning xi:k xk+1:j . Triplets form (i, i, i) correspond terminal symbols
tree leaves (the words xi ). Moving unlabeled case labeled one,
represent tree set 6-tuples (A B, C, i, k, j), whereas i, k j indicate spans
before, A, B C node labels nodes spanning xi:j , xi:k xk+1:j
respectively. Here, leaf nodes form (A A, A, i, i, i), pre-terminal
symbol. refer tuples production rules. example, consider syntactic
tree sentence boy saw duck.
403

fiGoldberg


VP

NP

NP

Det Noun Verb


boy

saw

Det Noun


duck

corresponding unlabeled labeled representations :
Unlabeled
(1,1,1)
(2,2,2)
(3,3,3)
(4,4,4)
(5,5,5)
(4,4,5)
(3,3,5)
(1,1,2)
(1,2,5)

Labeled
(Det, Det, Det, 1, 1, 1)
(Noun, Noun, Noun, 2, 2, 2)
(Verb, Verb, Verb, 3, 3, 3)
(Det, Det, Det, 4, 4, 4)
(Noun, Noun, Noun, 5, 5, 5)
(NP, Det, Noun, 4, 4, 5)
(VP, Verb, NP, 3, 3, 5)
(NP, Det, Noun, 1, 1, 2)
(S, NP, VP, 1, 2, 5)

Corresponding Span
x1:1
x2:2 boy
saw

duck
duck
saw duck
boy
boy saw duck


set production rules uniquely converted set tree nodes qi:j
(indicating node symbol span xi:j ) simply ignoring elements
(B, C, k) production rule. position define recursive neural
network.
recursive neural network (RecNN) function takes input parse tree
n-word sentence x1 , . . . , xn . sentences words represented d-dimensional
vector xi , tree represented set production rules (A B, C, i, j, k).
. RecNN returns output corresponding set
Denote nodes qi:j


inside state vectors si:j , inside state vector sA
i:j R represents corresponding
, encodes entire structure rooted node. sequence RNN,
tree node qi:j
tree shaped RecNN defined recursively using function R, inside vector
given node defined function inside vectors direct children.77 Formally:



RecNN(x1 , . . . , xn , ) ={sA
i:j R | qi:j }

sA
i:i =v(xi )

B
C
sA
i:j =R(A, B, C, si:k , sk+1:j )

(41)
B
C
, qk+1:j

qi:k

77. Le Zuidema (2014) extend RecNN definition node has, addition inside
state vector, outside state vector representing entire structure around subtree rooted
node. formulation based recursive computation classic inside-outside
algorithm, thought biRNN counterpart tree RecNN. details, see work
Le Zuidema.

404

fiA Primer Neural Networks NLP

function R usually takes form simple linear transformation, may
may followed non-linear activation function g:
C
B
C
R(A, B, C, sB
i:k , sk+1:j ) = g([si:k ; sk+1:j ]W)

(42)

formulation R ignores tree labels, using matrix W R2dd
combinations. may useful formulation case node labels exist (e.g.
tree represent syntactic structure clearly defined labels)
unreliable. However, labels available, generally useful include
composition function. One approach would introduce label embeddings v(A)
mapping non-terminal symbol dnt dimensional vector, change R include
embedded symbols combination function:
C
B
C
R(A, B, C, sB
i:k , sk+1:j ) = g([si:k ; sk+1:j ; v(A); v(B)]W)

(43)

(here, W R2d+2dnt ). approach taken Qian, Tian, Huang, Liu, Zhu,
Zhu (2015). alternative approach, due Socher et al. (2013) untie weights
according non-terminals, using different composition matrix B, C pair
symbols:78
BC
C
B
C
)
R(A, B, C, sB
i:k , sk+1:j ) = g([si:k ; sk+1:j ]W

(44)

formulation useful number non-terminal symbols (or number
possible symbol combinations) relatively small, usually case phrase-structure
parse trees. similar model used Hashimoto et al. (2013) encode subtrees
semantic-relation classification task.
12.2 Extensions Variations
definitions R suffer vanishing gradients problem
Simple RNN, several authors sought replace functions inspired Long ShortTerm Memory (LSTM) gated architecture, resulting Tree-shaped LSTMs (Tai, Socher, &
Manning, 2015; Zhu, Sobhani, & Guo, 2015b). question optimal tree representation
still much open research question, vast space possible combination
functions R yet explored. proposed variants tree-structured RNNs includes
recursive matrix-vector model (Socher, Huval, Manning, & Ng, 2012) recursive neural
tensor network (Socher et al., 2013). first variant, word represented
combination vector matrix, vector defines words static semantic
content before, matrix acts learned operator word, allowing
subtle semantic compositions addition weighted averaging implied
concatenation followed linear transformation function. second variant, words
associated vectors usual, composition function becomes expressive
basing tensor instead matrix operations.
78. explored literature, trivial extension would condition transformation matrix
A.

405

fiGoldberg

12.3 Training Recursive Neural Networks
training procedure recursive neural network follows recipe training
forms networks: define loss, spell computation graph, compute gradients
using backpropagation79 , train parameters using SGD.
regard loss function, similar sequence RNN one associate loss
either root tree, given node, set nodes, case
individual nodes losses combined, usually summation. loss function based
labeled training data associates label quantity different tree
nodes.
Additionally, one treat RecNN Encoder, whereas inside-vector associated node taken encoding tree rooted node. encoding
potentially sensitive arbitrary properties structure. vector
passed input another network.
discussion recursive neural networks use natural language
tasks, refer PhD thesis Richard Socher (2014).

13. Conclusions
Neural networks powerful learners, providing opportunities ranging non-linear
classification non-Markovian modeling sequences trees. hope exposition helps NLP researchers incorporate neural network models work take
advantage power.

References
Adel, H., Vu, N. T., & Schultz, T. (2013). Combination Recurrent Neural Networks
Factored Language Models Code-Switching Language Modeling. Proceedings
51st Annual Meeting Association Computational Linguistics (Volume 2: Short Papers), pp. 206211, Sofia, Bulgaria. Association Computational
Linguistics.
Ando, R., & Zhang, T. (2005a). High-Performance Semi-Supervised Learning Method
Text Chunking. Proceedings 43rd Annual Meeting Association
Computational Linguistics (ACL05), pp. 19, Ann Arbor, Michigan. Association
Computational Linguistics.
Ando, R. K., & Zhang, T. (2005b). framework learning predictive structures
multiple tasks unlabeled data. Journal Machine Learning Research, 6,
18171853.
Auli, M., Galley, M., Quirk, C., & Zweig, G. (2013). Joint Language Translation Modeling Recurrent Neural Networks. Proceedings 2013 Conference
Empirical Methods Natural Language Processing, pp. 10441054, Seattle, Washington, USA. Association Computational Linguistics.
79. introduction computation graph abstraction, specific backpropagation procedure
computing gradients RecNN defined referred Back-propagation
Structure (BPTS) algorithm (Goller & Kuchler, 1996).

406

fiA Primer Neural Networks NLP

Auli, M., & Gao, J. (2014). Decoder Integration Expected BLEU Training Recurrent
Neural Network Language Models. Proceedings 52nd Annual Meeting
Association Computational Linguistics (Volume 2: Short Papers), pp. 136142,
Baltimore, Maryland. Association Computational Linguistics.
Ballesteros, M., Dyer, C., & Smith, N. A. (2015). Improved Transition-based Parsing
Modeling Characters instead Words LSTMs. Proceedings 2015 Conference Empirical Methods Natural Language Processing, pp. 349359, Lisbon,
Portugal. Association Computational Linguistics.
Ballesteros, M., Goldberg, Y., Dyer, C., & Smith, N. A. (2016). Training Exploration
Improves Greedy Stack-LSTM Parser. arXiv:1603.03793 [cs].
Bansal, M., Gimpel, K., & Livescu, K. (2014). Tailoring Continuous Word Representations
Dependency Parsing. Proceedings 52nd Annual Meeting Association
Computational Linguistics (Volume 2: Short Papers), pp. 809815, Baltimore,
Maryland. Association Computational Linguistics.
Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2015). Automatic
differentiation machine learning: survey. arXiv:1502.05767 [cs].
Bengio, Y. (2012). Practical recommendations gradient-based training deep architectures. arXiv:1206.5533 [cs].
Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). Neural Probabilistic Language Model. J. Mach. Learn. Res., 3, 11371155.
Bengio, Y., Goodfellow, I. J., & Courville, A. (2015). Deep Learning. Book preparation
MIT Press.
Bitvai, Z., & Cohn, T. (2015). Non-Linear Text Regression Deep Convolutional
Neural Network. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 2: Short Papers), pp. 180185, Beijing, China. Association
Computational Linguistics.
Botha, J. A., & Blunsom, P. (2014). Compositional Morphology Word Representations
Language Modelling. Proceedings 31st International Conference
Machine Learning (ICML), Beijing, China. *Award best application paper*.
Bottou, L. (2012). Stochastic gradient descent tricks. Neural Networks: Tricks
Trade, pp. 421436. Springer.
Charniak, E., & Johnson, M. (2005). Coarse-to-Fine n-Best Parsing MaxEnt Discriminative Reranking. Proceedings 43rd Annual Meeting Association
Computational Linguistics (ACL05), pp. 173180, Ann Arbor, Michigan. Association
Computational Linguistics.
Chen, D., & Manning, C. (2014). Fast Accurate Dependency Parser using Neural
Networks. Proceedings 2014 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 740750, Doha, Qatar. Association Computational Linguistics.
407

fiGoldberg

Chen, Y., Xu, L., Liu, K., Zeng, D., & Zhao, J. (2015). Event Extraction via Dynamic
Multi-Pooling Convolutional Neural Networks. Proceedings 53rd Annual
Meeting Association Computational Linguistics 7th International
Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 167
176, Beijing, China. Association Computational Linguistics.
Cho, K. (2015). Natural Language Understanding Distributed Representation.
arXiv:1511.07916 [cs, stat].
Cho, K., van Merrienboer, B., Bahdanau, D., & Bengio, Y. (2014a). Properties
Neural Machine Translation: EncoderDecoder Approaches. Proceedings SSST8, Eighth Workshop Syntax, Semantics Structure Statistical Translation,
pp. 103111, Doha, Qatar. Association Computational Linguistics.
Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &
Bengio, Y. (2014b). Learning Phrase Representations using RNN EncoderDecoder
Statistical Machine Translation. Proceedings 2014 Conference Empirical
Methods Natural Language Processing (EMNLP), pp. 17241734, Doha, Qatar.
Association Computational Linguistics.
Chrupala, G. (2014). Normalizing tweets edit scripts recurrent neural embeddings.
Proceedings 52nd Annual Meeting Association Computational Linguistics (Volume 2: Short Papers), pp. 680686, Baltimore, Maryland. Association
Computational Linguistics.
Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical Evaluation Gated
Recurrent Neural Networks Sequence Modeling. arXiv:1412.3555 [cs].
Collins, M. (2002). Discriminative Training Methods Hidden Markov Models: Theory
Experiments Perceptron Algorithms. Proceedings 2002 Conference Empirical Methods Natural Language Processing, pp. 18. Association
Computational Linguistics.
Collins, M., & Koo, T. (2005). Discriminative Reranking Natural Language Parsing.
Computational Linguistics, 31 (1), 2570.
Collobert, R., & Weston, J. (2008). unified architecture natural language processing:
Deep neural networks multitask learning. Proceedings 25th international
conference Machine learning, pp. 160167. ACM.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural language processing (almost) scratch. Journal Machine Learning
Research, 12, 24932537.
Crammer, K., & Singer, Y. (2002). algorithmic implementation multiclass kernelbased vector machines. Journal Machine Learning Research, 2, 265292.
Creutz, M., & Lagus, K. (2007). Unsupervised Models Morpheme Segmentation
Morphology Learning. ACM Trans. Speech Lang. Process., 4 (1), 3:13:34.
Cybenko, G. (1989). Approximation superpositions sigmoidal function. Mathematics
Control, Signals Systems, 2 (4), 303314.
408

fiA Primer Neural Networks NLP

Dahl, G., Sainath, T., & Hinton, G. (2013). Improving deep neural networks LVCSR
using rectified linear units dropout. 2013 IEEE International Conference
Acoustics, Speech Signal Processing (ICASSP), pp. 86098613.
Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014).
Identifying attacking saddle point problem high-dimensional non-convex
optimization. Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., & Weinberger, K. Q. (Eds.), Advances Neural Information Processing Systems 27, pp.
29332941. Curran Associates, Inc.
de Gispert, A., Iglesias, G., & Byrne, B. (2015). Fast Accurate Preordering SMT
using Neural Networks. Proceedings 2015 Conference North American
Chapter Association Computational Linguistics: Human Language Technologies, pp. 10121017, Denver, Colorado. Association Computational Linguistics.
Do, T., Arti, T., & others (2010). Neural conditional random fields. International
Conference Artificial Intelligence Statistics, pp. 177184.
Dong, L., Wei, F., Tan, C., Tang, D., Zhou, M., & Xu, K. (2014). Adaptive Recursive Neural
Network Target-dependent Twitter Sentiment Classification. Proceedings
52nd Annual Meeting Association Computational Linguistics (Volume
2: Short Papers), pp. 4954, Baltimore, Maryland. Association Computational
Linguistics.
Dong, L., Wei, F., Zhou, M., & Xu, K. (2015). Question Answering Freebase
Multi-Column Convolutional Neural Networks. Proceedings 53rd Annual
Meeting Association Computational Linguistics 7th International
Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 260
269, Beijing, China. Association Computational Linguistics.
dos Santos, C., & Gatti, M. (2014). Deep Convolutional Neural Networks Sentiment
Analysis Short Texts. Proceedings COLING 2014, 25th International Conference Computational Linguistics: Technical Papers, pp. 6978, Dublin, Ireland.
Dublin City University Association Computational Linguistics.
dos Santos, C., Xiang, B., & Zhou, B. (2015). Classifying Relations Ranking
Convolutional Neural Networks. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 626634, Beijing,
China. Association Computational Linguistics.
dos Santos, C., & Zadrozny, B. (2014). Learning Character-level Representations Partof-Speech Tagging. Proceedings 31st International Conference Machine
Learning (ICML), pp. 18181826.
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods online learning
stochastic optimization. Journal Machine Learning Research, 12, 2121
2159.
Duh, K., Neubig, G., Sudoh, K., & Tsukada, H. (2013). Adaptation Data Selection using Neural Language Models: Experiments Machine Translation. Proceedings
409

fiGoldberg

51st Annual Meeting Association Computational Linguistics (Volume 2: Short Papers), pp. 678683, Sofia, Bulgaria. Association Computational
Linguistics.
Durrett, G., & Klein, D. (2015). Neural CRF Parsing. Proceedings 53rd Annual
Meeting Association Computational Linguistics 7th International
Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 302
312, Beijing, China. Association Computational Linguistics.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). TransitionBased Dependency Parsing Stack Long Short-Term Memory. Proceedings
53rd Annual Meeting Association Computational Linguistics
7th International Joint Conference Natural Language Processing (Volume 1: Long
Papers), pp. 334343, Beijing, China. Association Computational Linguistics.
Elman, J. L. (1990). Finding Structure Time. Cognitive Science, 14 (2), 179211.
Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using Multilingual Correlation. Proceedings 14th Conference European Chapter
Association Computational Linguistics, pp. 462471, Gothenburg, Sweden.
Association Computational Linguistics.
Filippova, K., Alfonseca, E., Colmenares, C. A., Kaiser, L., & Vinyals, O. (2015). Sentence
Compression Deletion LSTMs. Proceedings 2015 Conference
Empirical Methods Natural Language Processing, pp. 360368, Lisbon, Portugal.
Association Computational Linguistics.
Forcada, M. L., & Neco, R. P. (1997). Recursive hetero-associative memories translation.
Biological Artificial Computation: Neuroscience Technology, pp. 453
462. Springer.
Gao, J., Pantel, P., Gamon, M., He, X., & Deng, L. (2014). Modeling Interestingness
Deep Neural Networks. Proceedings 2014 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 213, Doha, Qatar. Association
Computational Linguistics.
Gimenez, J., & Marquez, L. (2004). SVMTool: general POS tagger generator based
Support Vector Machines. Proceedings 4th LREC, Lisbon, Portugal.
Glorot, X., & Bengio, Y. (2010). Understanding difficulty training deep feedforward
neural networks. International conference artificial intelligence statistics,
pp. 249256.
Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep sparse rectifier neural networks.
International Conference Artificial Intelligence Statistics, pp. 315323.
Goldberg, Y., & Elhadad, M. (2010). Efficient Algorithm Easy-First Non-Directional
Dependency Parsing. Human Language Technologies: 2010 Annual Conference
North American Chapter Association Computational Linguistics, pp.
742750, Los Angeles, California. Association Computational Linguistics.
Goldberg, Y., & Levy, O. (2014). word2vec Explained: deriving Mikolov et al.s negativesampling word-embedding method. arXiv:1402.3722 [cs, stat].
410

fiA Primer Neural Networks NLP

Goldberg, Y., & Nivre, J. (2013). Training Deterministic Parsers Non-Deterministic
Oracles. Transactions Association Computational Linguistics, 1 (0), 403
414.
Goldberg, Y., Zhao, K., & Huang, L. (2013). Efficient Implementation Beam-Search
Incremental Parsers. Proceedings 51st Annual Meeting Association
Computational Linguistics (Volume 2: Short Papers), pp. 628633, Sofia, Bulgaria.
Association Computational Linguistics.
Goller, C., & Kuchler, A. (1996). Learning Task-Dependent Distributed Representations
Backpropagation Structure. Proc. ICNN-96, pp. 347352.
IEEE.
Gouws, S., Bengio, Y., & Corrado, G. (2015). BilBOWA: Fast Bilingual Distributed Representations without Word Alignments. Proceedings 32nd International
Conference Machine Learning, pp. 748756.
Graves, A. (2008). Supervised sequence labelling recurrent neural networks. Ph.D.
thesis, Technische Universitat Munchen.
Greff, K., Srivastava, R. K., Koutnk, J., Steunebrink, B. R., & Schmidhuber, J. (2015).
LSTM: Search Space Odyssey. arXiv:1503.04069 [cs].
Hal Daume III, Langford, J., & Marcu, D. (2009). Search-based Structured Prediction.
Machine Learning Journal (MLJ).
Harris, Z. (1954). Distributional Structure. Word, 10 (23), 146162.
Hashimoto, K., Miwa, M., Tsuruoka, Y., & Chikayama, T. (2013). Simple Customization
Recursive Neural Networks Semantic Relation Classification. Proceedings
2013 Conference Empirical Methods Natural Language Processing, pp.
13721376, Seattle, Washington, USA. Association Computational Linguistics.
He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep Rectifiers: Surpassing
Human-Level Performance ImageNet Classification. arXiv:1502.01852 [cs].
Henderson, M., Thomson, B., & Young, S. (2013). Deep Neural Network Approach
Dialog State Tracking Challenge. Proceedings SIGDIAL 2013 Conference,
pp. 467471, Metz, France. Association Computational Linguistics.
Hermann, K. M., & Blunsom, P. (2013). Role Syntax Vector Space Models
Compositional Semantics. Proceedings 51st Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 894904, Sofia,
Bulgaria. Association Computational Linguistics.
Hermann, K. M., & Blunsom, P. (2014). Multilingual Models Compositional Distributed
Semantics. Proceedings 52nd Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 5868, Baltimore, Maryland.
Association Computational Linguistics.
Hihi, S. E., & Bengio, Y. (1996). Hierarchical Recurrent Neural Networks Long-Term
Dependencies. Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), Advances
Neural Information Processing Systems 8, pp. 493499. MIT Press.
411

fiGoldberg

Hill, F., Cho, K., Jean, S., Devin, C., & Bengio, Y. (2014). Embedding Word Similarity
Neural Machine Translation. arXiv:1412.6448 [cs].
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R.
(2012). Improving neural networks preventing co-adaptation feature detectors.
arXiv:1207.0580 [cs].
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural computation,
9 (8), 17351780.
Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks
universal approximators. Neural Networks, 2 (5), 359366.
Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training
Reducing Internal Covariate Shift. arXiv:1502.03167 [cs].
Irsoy, O., & Cardie, C. (2014). Opinion Mining Deep Recurrent Neural Networks.
Proceedings 2014 Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 720728, Doha, Qatar. Association Computational Linguistics.
Iyyer, M., Boyd-Graber, J., Claudino, L., Socher, R., & Daume III, H. (2014a). Neural
Network Factoid Question Answering Paragraphs. Proceedings 2014
Conference Empirical Methods Natural Language Processing (EMNLP), pp.
633644, Doha, Qatar. Association Computational Linguistics.
Iyyer, M., Enns, P., Boyd-Graber, J., & Resnik, P. (2014b). Political Ideology Detection
Using Recursive Neural Networks. Proceedings 52nd Annual Meeting
Association Computational Linguistics (Volume 1: Long Papers), pp. 11131122,
Baltimore, Maryland. Association Computational Linguistics.
Iyyer, M., Manjunatha, V., Boyd-Graber, J., & Daume III, H. (2015). Deep Unordered
Composition Rivals Syntactic Methods Text Classification. Proceedings
53rd Annual Meeting Association Computational Linguistics 7th
International Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 16811691, Beijing, China. Association Computational Linguistics.
Johnson, R., & Zhang, T. (2015). Effective Use Word Order Text Categorization
Convolutional Neural Networks. Proceedings 2015 Conference North
American Chapter Association Computational Linguistics: Human Language Technologies, pp. 103112, Denver, Colorado. Association Computational
Linguistics.
Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., & Wu, Y. (2016). Exploring
Limits Language Modeling. arXiv:1602.02410 [cs].
Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015). Empirical Exploration Recurrent Network Architectures. Proceedings 32nd International Conference
Machine Learning (ICML-15), pp. 23422350.
Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). Convolutional Neural Network
Modelling Sentences. Proceedings 52nd Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 655665, Baltimore,
Maryland. Association Computational Linguistics.
412

fiA Primer Neural Networks NLP

Karpathy, A., Johnson, J., & Li, F.-F. (2015). Visualizing Understanding Recurrent
Networks. arXiv:1506.02078 [cs].
Kim, Y. (2014). Convolutional Neural Networks Sentence Classification. Proceedings 2014 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 17461751, Doha, Qatar. Association Computational Linguistics.
Kim, Y., Jernite, Y., Sontag, D., & Rush, A. M. (2015). Character-Aware Neural Language
Models. arXiv:1508.06615 [cs, stat].
Kingma, D., & Ba, J. (2014).
arXiv:1412.6980 [cs].

Adam: Method Stochastic Optimization.

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet Classification Deep
Convolutional Neural Networks. Pereira, F., Burges, C. J. C., Bottou, L., & Weinberger, K. Q. (Eds.), Advances Neural Information Processing Systems 25, pp.
10971105. Curran Associates, Inc.
Kudo, T., & Matsumoto, Y. (2003). Fast Methods Kernel-based Text Analysis.
Proceedings 41st Annual Meeting Association Computational Linguistics Volume 1, ACL 03, pp. 2431, Stroudsburg, PA, USA. Association Computational
Linguistics.
Lafferty, J., McCallum, A., & Pereira, F. C. (2001). Conditional random fields: Probabilistic
models segmenting labeling sequence data. Proceedings ICML.
Le, P., & Zuidema, W. (2014). Inside-Outside Recursive Neural Network model
Dependency Parsing. Proceedings 2014 Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 729739, Doha, Qatar. Association
Computational Linguistics.
Le, P., & Zuidema, W. (2015). Forest Convolutional Network: Compositional Distributional Semantics Neural Chart without Binarization. Proceedings
2015 Conference Empirical Methods Natural Language Processing, pp.
11551164, Lisbon, Portugal. Association Computational Linguistics.
Le, Q. V., Jaitly, N., & Hinton, G. E. (2015). Simple Way Initialize Recurrent Networks
Rectified Linear Units. arXiv:1504.00941 [cs].
LeCun, Y., & Bengio, Y. (1995). Convolutional Networks Images, Speech, TimeSeries. Arbib, M. A. (Ed.), Handbook Brain Theory Neural Networks.
MIT Press.
LeCun, Y., Bottou, L., Orr, G., & Muller, K. (1998a). Efficient BackProp. Orr, G., &
K, M. (Eds.), Neural Networks: Tricks trade. Springer.
LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998b). Gradient Based Learning Applied
Pattern Recognition. Proceedings IEEE, 86 (11), 22782324.
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). tutorial energybased learning. Predicting structured data, 1, 0.
LeCun, Y., & Huang, F. (2005). Loss functions discriminative training energybased
models. Proceedings AISTATS. AIStats.
413

fiGoldberg

Lee, G., Flowers, M., & Dyer, M. G. (1992). Learning distributed representations conceptual knowledge application script-based story processing. Connectionist
Natural Language Processing, pp. 215247. Springer.
Levy, O., & Goldberg, Y. (2014a). Dependency-Based Word Embeddings. Proceedings
52nd Annual Meeting Association Computational Linguistics (Volume
2: Short Papers), pp. 302308, Baltimore, Maryland. Association Computational
Linguistics.
Levy, O., & Goldberg, Y. (2014b). Neural Word Embedding Implicit Matrix Factorization. Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., & Weinberger,
K. Q. (Eds.), Advances Neural Information Processing Systems 27, pp. 21772185.
Curran Associates, Inc.
Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving Distributional Similarity
Lessons Learned Word Embeddings. Transactions Association Computational Linguistics, 3 (0), 211225.
Lewis, M., & Steedman, M. (2014). Improved CCG Parsing Semi-supervised Supertagging. Transactions Association Computational Linguistics, 2 (0), 327338.
Li, J., Li, R., & Hovy, E. (2014). Recursive Deep Models Discourse Parsing. Proceedings 2014 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 20612069, Doha, Qatar. Association Computational Linguistics.
Ling, W., Dyer, C., Black, A. W., & Trancoso, I. (2015a). Two/Too Simple Adaptations
Word2Vec Syntax Problems. Proceedings 2015 Conference North
American Chapter Association Computational Linguistics: Human Language Technologies, pp. 12991304, Denver, Colorado. Association Computational
Linguistics.
Ling, W., Dyer, C., Black, A. W., Trancoso, I., Fermandez, R., Amir, S., Marujo, L., &
Luis, T. (2015b). Finding Function Form: Compositional Character Models
Open Vocabulary Word Representation. Proceedings 2015 Conference
Empirical Methods Natural Language Processing, pp. 15201530, Lisbon, Portugal.
Association Computational Linguistics.
Liu, Y., Wei, F., Li, S., Ji, H., Zhou, M., & Wang, H. (2015). Dependency-Based Neural
Network Relation Classification. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 2: Short Papers), pp. 285290, Beijing,
China. Association Computational Linguistics.
Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., & Kaiser, L. (2015). Multi-task Sequence
Sequence Learning. arXiv:1511.06114 [cs, stat].
Ma, J., Zhang, Y., & Zhu, J. (2014). Tagging Web: Building Robust Web Tagger
Neural Network. Proceedings 52nd Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 144154, Baltimore,
Maryland. Association Computational Linguistics.
Ma, M., Huang, L., Zhou, B., & Xiang, B. (2015). Dependency-based Convolutional Neural
Networks Sentence Embedding. Proceedings 53rd Annual Meeting
414

fiA Primer Neural Networks NLP

Association Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 2: Short Papers), pp. 174179, Beijing,
China. Association Computational Linguistics.
McCallum, A., Freitag, D., & Pereira, F. C. (2000). Maximum Entropy Markov Models
Information Extraction Segmentation.. ICML, Vol. 17, pp. 591598.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation Word
Representations Vector Space. arXiv:1301.3781 [cs].
Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., & Ranzato, M. (2014). Learning Longer
Memory Recurrent Neural Networks. arXiv:1412.7753 [cs].
Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., & Khudanpur, S. (2010). Recurrent
neural network based language model.. INTERSPEECH 2010, 11th Annual Conference International Speech Communication Association, Makuhari, Chiba,
Japan, September 26-30, 2010, pp. 10451048.
Mikolov, T., Kombrink, S., Lukas Burget, Cernocky, J. H., & Khudanpur, S. (2011). Extensions recurrent neural network language model. Acoustics, Speech Signal
Processing (ICASSP), 2011 IEEE International Conference on, pp. 55285531. IEEE.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed Representations Words Phrases Compositionality. Burges, C. J. C.,
Bottou, L., Welling, M., Ghahramani, Z., & Weinberger, K. Q. (Eds.), Advances
Neural Information Processing Systems 26, pp. 31113119. Curran Associates, Inc.
Mikolov, T. (2012). Statistical language models based neural networks. Ph.D. thesis, Ph.
D. thesis, Brno University Technology.
Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently noisecontrastive estimation. Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z.,
& Weinberger, K. Q. (Eds.), Advances Neural Information Processing Systems 26,
pp. 22652273. Curran Associates, Inc.
Mrksic, N., Seaghdha, D., Thomson, B., Gasic, M., Su, P.-H., Vandyke, D., Wen, T.-H.,
& Young, S. (2015). Multi-domain Dialog State Tracking using Recurrent Neural
Networks. Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language
Processing (Volume 2: Short Papers), pp. 794799, Beijing, China. Association
Computational Linguistics.
Neidinger, R. (2010). Introduction Automatic Differentiation MATLAB ObjectOriented Programming. SIAM Review, 52 (3), 545563.
Nesterov, Y. (1983). method solving convex programming problem convergence
rate (1/k2). Soviet Mathematics Doklady, Vol. 27, pp. 372376.
Nesterov, Y. (2004). Introductory lectures convex optimization. Kluwer Academic Publishers.
Nguyen, T. H., & Grishman, R. (2015). Event Detection Domain Adaptation
Convolutional Neural Networks. Proceedings 53rd Annual Meeting
415

fiGoldberg

Association Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 2: Short Papers), pp. 365371, Beijing,
China. Association Computational Linguistics.
Nivre, J. (2008). Algorithms Deterministic Incremental Dependency Parsing. Computational Linguistics, 34 (4), 513553.
Okasaki, C. (1999). Purely Functional Data Structures. Cambridge University Press, Cambridge, U.K.; New York.
Olah, C. (2015a). Calculus Computational Graphs: Backpropagation. Retrieved
http://colah.github.io/posts/2015-08-Backprop/.
Olah, C. (2015b). Understanding LSTM Networks. Retrieved http://colah.
github.io/posts/2015-08-Understanding-LSTMs/.
Pascanu, R., Mikolov, T., & Bengio, Y. (2012). difficulty training Recurrent
Neural Networks. arXiv:1211.5063 [cs].
Pei, W., Ge, T., & Chang, B. (2015). Effective Neural Network Model Graph-based
Dependency Parsing. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural
Language Processing (Volume 1: Long Papers), pp. 313322, Beijing, China. Association Computational Linguistics.
Peng, J., Bo, L., & Xu, J. (2009). Conditional Neural Fields. Bengio, Y., Schuurmans,
D., Lafferty, J. D., Williams, C. K. I., & Culotta, A. (Eds.), Advances Neural
Information Processing Systems 22, pp. 14191427. Curran Associates, Inc.
Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global Vectors Word Representation. Proceedings 2014 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 15321543, Doha, Qatar. Association Computational Linguistics.
Pollack, J. B. (1990). Recursive Distributed Representations. Artificial Intelligence, 46,
77105.
Polyak, B. T. (1964). methods speeding convergence iteration methods.
USSR Computational Mathematics Mathematical Physics, 4 (5), 1 17.
Qian, Q., Tian, B., Huang, M., Liu, Y., Zhu, X., & Zhu, X. (2015). Learning Tag Embeddings
Tag-specific Composition Functions Recursive Neural Network. Proceedings
53rd Annual Meeting Association Computational Linguistics
7th International Joint Conference Natural Language Processing (Volume 1: Long
Papers), pp. 13651374, Beijing, China. Association Computational Linguistics.
Rong, X. (2014). word2vec Parameter Learning Explained. arXiv:1411.2738 [cs].
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations
back-propagating errors. Nature, 323 (6088), 533536.
Schuster, M., & Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE
Transactions Signal Processing, 45 (11), 26732681.
416

fiA Primer Neural Networks NLP

Schwenk, H., Dchelotte, D., & Gauvain, J.-L. (2006). Continuous space language models
statistical machine translation. Proceedings COLING/ACL Main
conference poster sessions, pp. 723730. Association Computational Linguistics.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods Pattern Analysis. Cambridge
University Press.
Smith, N. A. (2011). Linguistic Structure Prediction. Synthesis Lectures Human Language Technologies. Morgan Claypool.
Socher, R. (2014). Recursive Deep Learning Natural Language Processing Computer
Vision. Ph.D. thesis, Stanford University.
Socher, R., Bauer, J., Manning, C. D., & Ng, A. Y. (2013). Parsing Compositional
Vector Grammars. Proceedings 51st Annual Meeting Association
Computational Linguistics (Volume 1: Long Papers), pp. 455465, Sofia, Bulgaria.
Association Computational Linguistics.
Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic Compositionality
Recursive Matrix-Vector Spaces. Proceedings 2012 Joint Conference
Empirical Methods Natural Language Processing Computational Natural
Language Learning, pp. 12011211, Jeju Island, Korea. Association Computational
Linguistics.
Socher, R., Lin, C. C.-Y., Ng, A. Y., & Manning, C. D. (2011). Parsing Natural Scenes
Natural Language Recursive Neural Networks. Getoor, L., & Scheffer, T.
(Eds.), Proceedings 28th International Conference Machine Learning, ICML
2011, Bellevue, Washington, USA, June 28 - July 2, 2011, pp. 129136. Omnipress.
Socher, R., Manning, C., & Ng, A. (2010). Learning Continuous Phrase Representations
Syntactic Parsing Recursive Neural Networks. Proceedings Deep
Learning Unsupervised Feature Learning Workshop {NIPS} 2010, pp. 19.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., & Potts, C. (2013).
Recursive Deep Models Semantic Compositionality Sentiment Treebank.
Proceedings 2013 Conference Empirical Methods Natural Language
Processing, pp. 16311642, Seattle, Washington, USA. Association Computational
Linguistics.
Sgaard, A., & Goldberg, Y. (2016). Deep multi-task learning low level tasks supervised
lower layers. Proceedings 54th Annual Meeting Association
Computational Linguistics (Volume 2: Short Papers), pp. 231235. Association
Computational Linguistics.
Sordoni, A., Galley, M., Auli, M., Brockett, C., Ji, Y., Mitchell, M., Nie, J.-Y., Gao, J.,
& Dolan, B. (2015). Neural Network Approach Context-Sensitive Generation
Conversational Responses. Proceedings 2015 Conference North
American Chapter Association Computational Linguistics: Human Language Technologies, pp. 196205, Denver, Colorado. Association Computational
Linguistics.
Sundermeyer, M., Alkhouli, T., Wuebker, J., & Ney, H. (2014). Translation Modeling
Bidirectional Recurrent Neural Networks. Proceedings 2014 Conference
417

fiGoldberg

Empirical Methods Natural Language Processing (EMNLP), pp. 1425, Doha,
Qatar. Association Computational Linguistics.
Sundermeyer, M., Schluter, R., & Ney, H. (2012). LSTM Neural Networks Language
Modeling.. INTERSPEECH.
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). importance initialization
momentum deep learning. Proceedings 30th international conference
machine learning (ICML-13), pp. 11391147.
Sutskever, I., Martens, J., & Hinton, G. E. (2011). Generating text recurrent neural
networks. Proceedings 28th International Conference Machine Learning
(ICML-11), pp. 10171024.
Sutskever, I., Vinyals, O., & Le, Q. V. V. (2014). Sequence Sequence Learning
Neural Networks. Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., &
Weinberger, K. Q. (Eds.), Advances Neural Information Processing Systems 27, pp.
31043112. Curran Associates, Inc.
Tai, K. S., Socher, R., & Manning, C. D. (2015). Improved Semantic Representations
Tree-Structured Long Short-Term Memory Networks. Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 1: Long Papers),
pp. 15561566, Beijing, China. Association Computational Linguistics.
Tamura, A., Watanabe, T., & Sumita, E. (2014). Recurrent Neural Networks Word
Alignment Model. Proceedings 52nd Annual Meeting Association
Computational Linguistics (Volume 1: Long Papers), pp. 14701480, Baltimore,
Maryland. Association Computational Linguistics.
Telgarsky, M. (2016). Benefits depth neural networks. arXiv:1602.04485 [cs, stat].
Tieleman, T., & Hinton, G. (2012). Lecture 6.5RmsProp: Divide gradient running
average recent magnitude. COURSERA: Neural Networks Machine Learning.
Van de Cruys, T. (2014). Neural Network Approach Selectional Preference Acquisition. Proceedings 2014 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 2635, Doha, Qatar. Association Computational
Linguistics.
Vaswani, A., Zhao, Y., Fossum, V., & Chiang, D. (2013). Decoding Large-Scale Neural Language Models Improves Translation. Proceedings 2013 Conference
Empirical Methods Natural Language Processing, pp. 13871392, Seattle, Washington, USA. Association Computational Linguistics.
Wager, S., Wang, S., & Liang, P. S. (2013). Dropout Training Adaptive Regularization.
Burges, C. J. C., Bottou, L., Welling, M., Ghahramani, Z., & Weinberger, K. Q.
(Eds.), Advances Neural Information Processing Systems 26, pp. 351359. Curran
Associates, Inc.
Wang, M., & Manning, C. D. (2013). Effect Non-linear Deep Architecture Sequence
Labeling.. IJCNLP, pp. 12851291.
418

fiA Primer Neural Networks NLP

Wang, P., Xu, J., Xu, B., Liu, C., Zhang, H., Wang, F., & Hao, H. (2015a). Semantic Clustering Convolutional Neural Network Short Text Categorization. Proceedings
53rd Annual Meeting Association Computational Linguistics
7th International Joint Conference Natural Language Processing (Volume 2: Short
Papers), pp. 352357, Beijing, China. Association Computational Linguistics.
Wang, X., Liu, Y., Sun, C., Wang, B., & Wang, X. (2015b). Predicting Polarities Tweets
Composing Word Embeddings Long Short-Term Memory. Proceedings
53rd Annual Meeting Association Computational Linguistics
7th International Joint Conference Natural Language Processing (Volume 1: Long
Papers), pp. 13431353, Beijing, China. Association Computational Linguistics.
Watanabe, T., & Sumita, E. (2015). Transition-based Neural Constituent Parsing. Proceedings 53rd Annual Meeting Association Computational Linguistics
7th International Joint Conference Natural Language Processing (Volume
1: Long Papers), pp. 11691179, Beijing, China. Association Computational Linguistics.
Weiss, D., Alberti, C., Collins, M., & Petrov, S. (2015). Structured Training Neural
Network Transition-Based Parsing. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 323333, Beijing,
China. Association Computational Linguistics.
Werbos, P. J. (1990). Backpropagation time: it..
Proceedings IEEE, 78 (10), 1550 1560.
Weston, J., Bordes, A., Yakhnenko, O., & Usunier, N. (2013). Connecting Language
Knowledge Bases Embedding Models Relation Extraction. Proceedings
2013 Conference Empirical Methods Natural Language Processing, pp.
13661371, Seattle, Washington, USA. Association Computational Linguistics.
Xu, W., Auli, M., & Clark, S. (2015). CCG Supertagging Recurrent Neural Network.
Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing
(Volume 2: Short Papers), pp. 250255, Beijing, China. Association Computational
Linguistics.
Yin, W., & Schutze, H. (2015). Convolutional Neural Network Paraphrase Identification.
Proceedings 2015 Conference North American Chapter Association Computational Linguistics: Human Language Technologies, pp. 901911,
Denver, Colorado. Association Computational Linguistics.
Zaremba, W., Sutskever, I., & Vinyals, O. (2014). Recurrent Neural Network Regularization.
arXiv:1409.2329 [cs].
Zeiler, M. D. (2012). ADADELTA: Adaptive Learning Rate Method. arXiv:1212.5701
[cs].
Zeng, D., Liu, K., Lai, S., Zhou, G., & Zhao, J. (2014). Relation Classification via Convolutional Deep Neural Network. Proceedings COLING 2014, 25th International
419

fiGoldberg

Conference Computational Linguistics: Technical Papers, pp. 23352344, Dublin,
Ireland. Dublin City University Association Computational Linguistics.
Zhang, Y., & Weiss, D. (2016). Stack-propagation: Improved representation learning syntax. Proceedings 54th Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 15571566. Association Computational
Linguistics.
Zhou, H., Zhang, Y., Huang, S., & Chen, J. (2015). Neural Probabilistic StructuredPrediction Model Transition-Based Dependency Parsing. Proceedings
53rd Annual Meeting Association Computational Linguistics 7th
International Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 12131222, Beijing, China. Association Computational Linguistics.
Zhu, C., Qiu, X., Chen, X., & Huang, X. (2015a). Re-ranking Model Dependency
Parser Recursive Convolutional Neural Network. Proceedings 53rd
Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 1: Long Papers),
pp. 11591168, Beijing, China. Association Computational Linguistics.
Zhu, X., Sobhani, P., & Guo, H. (2015b). Long Short-Term Memory Tree Structures.
arXiv:1503.04881 [cs].

420


