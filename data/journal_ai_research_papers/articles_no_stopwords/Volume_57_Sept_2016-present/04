Journal Artificial Intelligence Research 57 (2016) 187-227

Submitted 9/15; published 10/16

Multi-objective Reinforcement Learning
Continuous Pareto Manifold Approximation
Simone Parisi

parisi@ias.tu-darmstadt.de

Technische Universitat Darmstadt
Hochschulstr. 10, 64289 Darmstadt, Germany

Matteo Pirotta
Marcello Restelli

matteo.pirotta@polimi.it
marcello.restelli@polimi.it

Politecnico di Milano
Piazza Leonardo da Vinci 32, 20133 Milano, Italy

Abstract
Many real-world control applications, economics robotics, characterized
presence multiple conflicting objectives. problems, standard concept
optimality replaced Paretooptimality goal find Pareto frontier,
set solutions representing different compromises among objectives. Despite recent advances multiobjective optimization, achieving accurate representation
Pareto frontier still important challenge. paper, propose reinforcement
learning policy gradient approach learn continuous approximation Pareto frontier multiobjective Markov Decision Problems (MOMDPs). Differently previous
policy gradient algorithms, n optimization routines executed n solutions,
approach performs single gradient ascent run, generating step improved
continuous approximation Pareto frontier. idea optimize parameters
function defining manifold policy parameters space, corresponding
image objectives space gets close possible true Pareto frontier. Besides
deriving compute estimate gradient, discuss nontrivial
issue defining metric assess quality candidate Pareto frontiers. Finally,
properties proposed approach empirically evaluated two problems,
linear-quadratic Gaussian regulator water reservoir control task.

1. Introduction
Multiobjective sequential decision problems characterized presence multiple
conflicting objectives found many real-world scenarios, economic
systems (Shelton, 2001), medical treatment (Lizotte, Bowling, & Murphy, 2012), control
water reservoirs (Castelletti, Pianosi, & Restelli, 2013), elevators (Crites & Barto, 1998)
robots (Nojima, Kojima, & Kubota, 2003; Ahmadzadeh, Kormushev, & Caldwell, 2014),
mention few. problems often modeled Multiobjective Markov Decision
Processes (MOMDPs), concept optimality typical MDPs replaced
one Pareto optimality, defines compromise among different objectives.
last decades, Reinforcement Learning (RL) (Sutton & Barto, 1998) established
effective theoretically grounded framework allows solve singleobjective
MDPs whenever either (or little) prior knowledge available system dynamics
dimensionality system controlled high classical optimal control
2016 AI Access Foundation. rights reserved.

fiParisi, Pirotta, & Restelli

methods. Multiobjective Reinforcement Learning (MORL), instead, concerns MOMDPs
tries solve sequential decision problems two conflicting objectives.
Despite successful development RL theory high demand multiobjective
control applications, MORL still relatively young unexplored research topic.
MORL approaches divided two categories, based number policies
learn (Vamplew, Dazeley, Berry, Issabekov, & Dekker, 2011): single multiple
policy. Although MORL approaches belong former category, present
multiplepolicy approach, able learn set policies approximating Pareto frontier.
representation complete Pareto frontier, fact, allows posteriori selection
solution encapsulates trade-offs among objectives, giving better insights
relationships among objectives. Among multiplepolicy algorithms possible
identify two classes: valuebased (Lizotte et al., 2012; Castelletti et al., 2013; Van Moffaert
& Nowe, 2014), search optimal solutions value functions space, policy gradient approaches (Shelton, 2001; Parisi, Pirotta, Smacchia, Bascetta, & Restelli, 2014),
search policy space. practice, approach different advantages. Value
based methods usually stronger guarantees convergence, preferred domains lowdimensional state-action spaces prone suffer curse
dimensionality (Sutton & Barto, 1998). hand, policy gradient methods
favorable many domains robotics allow taskappropriate
prestructured policies integrated straightforwardly (Deisenroth, Neumann, & Peters,
2013) experts knowledge incorporated ease. selecting suitable policy
parametrization, learning problem simplified stability well robustness
frequently ensured (Bertsekas, 2005). Nonetheless, approaches lack guarantees uniform covering true Pareto frontier quality approximate
frontier, terms accuracy (distance true frontier) covering (its extent),
related metric used measure discrepancy true Pareto frontier.
However, nowadays definition metric open problem MOO literature.
paper, overcome limitations proposing novel gradientbased MORL
approach alternative quality measures approximate frontiers. algorithm, namely
ParetoManifold Gradient Algorithm (PMGA), exploiting continuous approximation
locally Paretooptimal manifold policy space, able generate arbitrarily
dense approximate frontier. article extension preliminary work presented
Pirotta, Parisi, Restelli (2015) main contributions are: derivation
gradient approach general case, i.e., independent metric used measure
quality current solution (Section 3), estimate gradient samples
(Section 4), discussion frontier quality measures effectively integrated
proposed approach (Section 5), thorough empirical evaluation proposed
algorithm metrics performance multiobjective discrete-time Linear-Quadratic
Gaussian regulator water reservoir management domain (Sections 6 7).

2. Preliminaries
section, first briefly summarize terminology used paper discuss
state-of-the-art approaches MORL. Subsequently, focus describing policy
gradient techniques introduce notation used remainder paper.
188

fiMORL Continuous Pareto Manifold Approximation

2.1 Problem Formulation
discretetime continuous Markov Decision Process (MDP) mathematical framework
modeling decision making. described tuple hS, A, P, R, , Di, Rn
continuous state space, Rm continuous action space, P Markovian
transition model P(s0 |s, a) defines transition density state s0
action a, R : R reward function, [0, 1) discount factor,
distribution initial state drawn. context, behavior
agent defined policy, i.e., density distribution (a|s) specifies probability
taking action state s. Given initial state distribution D, possible define
expected return J associated policy
"T 1
#
X


J =
E
R(st , , st+1 )|s0 ,
st P,at

t=0

R(st , , st+1 ) immediate reward obtained state st+1 reached executing
action state st , finite infinite time horizon. goal agent
maximize return.
Multiobjective Markov Decision Processes (MOMDPs) extension MDPs
several pairs reward functions discount factors defined, one
objective. Formally, MOMDP described tuple hS, A, P, R, , Di, R =
[R1 , . . . , Rq ]T = [1 , . . . , q ]T qdimensional column vectors reward functions
Ri : R discount factors [0, 1), respectively.
MOMDPs, policy




associated q expected returns J = J1 , . . . , Jq ,
"T 1
#
X
Ji =
E
Ri (st , , st+1 )|s0 .
st P,at

t=0

Unlike happens MDPs, MOMDPs single policy dominating others
usually exist, conflicting objectives considered, policy simultaneously maximize them. reason, Multiobjective Optimization (MOO)
concept Pareto dominance used. Policy strongly dominates policy 0 , denoted
0 , superior objectives, i.e.,
0

0 {1, . . . , q} , Ji > Ji .
Similarly, policy weakly dominates policy 0 , denoted 0 , worse
objectives, i.e.,
0

0

0 {1, . . . , q} , Ji Ji {1, . . . , q} , Ji = Ji .
policy 0 0 , policy Paretooptimal. speak
locally Paretooptimal policies, definition above, except
restrict dominance neighborhood . general, multiple
(locally) Paretooptimal policies. Solving
MOMDP

equivalent determine set
= | @ 0 , 0 , maps socalled Pareto
Paretooptimal
policies



frontier F = J | .1
1. done Harada, Sakuma, Kobayashi (2006), assume locally Paretooptimal solutions
Paretooptimal exist.

189

fiParisi, Pirotta, & Restelli

2.2 Related Work
Multiobjective Optimization (MOO) field, two common solution concepts:
multiobjective singleobjective strategy Pareto strategy. former approach
derives scalar objective multiple objectives and, then, uses standard Single
objective Optimization (SOO) techniques: weighted sum (Athan & Papalambros, 1996),
normbased (Yu & Leitmann, 1974; Koski & Silvennoinen, 1987), sequential (Romero,
2001), constrained (Waltz, 1967), physical programming (Messac & Ismail-Yahaya, 2002)
min-max methods (Steuer & Choo, 1983). latter strategy based concept
Pareto dominance considers Paretooptimal solutions non-inferior solutions among
candidate solutions. main exponent class convex hull method (Das &
Dennis, 1998; Messac, Ismail-Yahaya, & Mattson, 2003).
Similar MOO, current MORL approaches divided two categories based
number policies learn (Vamplew et al., 2011). Singlepolicy methods aim
finding best policy satisfies preference among objectives. majority
MORL approaches belong category differ way preferences
expressed. easy implement, require priori decision type
solution suffer instability, small changes preferences may result
significant variations solution (Vamplew et al., 2011). straightforward
common singlepolicy approach scalarization function applied
reward vector order produce scalar signal. Usually, linear combination weighted
sum rewards performed weights used express preferences
multiple objective (Castelletti, Corani, Rizzolli, Soncinie-Sessa, & Weber, 2002; Natarajan
& Tadepalli, 2005; Van Moffaert, Drugan, & Nowe, 2013). Less common use non
linear mappings (Tesauro, Das, Chan, Kephart, Levine, Rawson, & Lefurgy, 2008).
main advantage scalarization simplicity. However, linear scalarization presents
limitations: able find solutions lie concave linear region
Pareto frontier (Athan & Papalambros, 1996) uniform distribution weights may
produce accurate evenly distributed points Pareto frontier (Das & Dennis,
1997). addition, even frontier convex, solutions cannot achieved
scalarization loss one objective may compensated increment
another one (Perny & Weng, 2010). Different singlepolicy approaches based
thresholds lexicographic ordering (Gabor, Kalmar, & Szepesvari, 1998) different
kinds preferences objective space (Mannor & Shimkin, 2002, 2004).
Multiplepolicy approaches, contrary, aim learning multiple policies order
approximate Pareto frontier. Building exact frontier generally impractical
real-world problems, thus, goal build approximation frontier contains
solutions accurate, evenly distributed along frontier range similar
Pareto one (Zitzler, Thiele, Laumanns, Fonseca, & da Fonseca, 2003). many
reasons behind superiority multiplepolicy methods: permit posteriori
selection solution encapsulate trade-offs among multiple objectives.
addition, graphical representation frontier give better insights relationships among objectives useful understanding problem
choice solution. However, benefits come higher computational cost,
prevent learning online scenarios. common approach approximate
190

fiMORL Continuous Pareto Manifold Approximation

Pareto frontier perform multiple runs singlepolicy algorithm varying
preferences among objectives (Castelletti et al., 2002; Van Moffaert et al., 2013).
simple approach suffers disadvantages singlepolicy method used.
Besides this, examples multiplepolicy algorithms found literature.
Barrett Narayanan (2008) proposed algorithm learns deterministic policies defining convex hull Pareto frontier single learning process. Recent
works focused extension fitted Q-iteration multiobjective scenario.
Lizotte, Bowling, Murphy (2010), Lizotte et al. (2012) focused
linear approximation value function, Castelletti, Pianosi, Restelli (2012) able
learn control policy linear combinations preferences among objectives single learning process. Finally, Wang Sebag (2013) proposed MonteCarlo
Tree Search algorithm able learn solutions lying concave region frontier.
Nevertheless, classic approaches exploit deterministic policies result
scattered Pareto frontiers, stochastic policies give continuous range compromises
among objectives (Roijers, Vamplew, Whiteson, & Dazeley, 2013; Parisi et al., 2014). Shelton (2001, Section 4.2.1) pioneer use stochastic mixture policies
gradient ascent MORL. achieved two well known goals MORL: simultaneous
conditional objectives maximization. former, agent must maintain goals
time. algorithm starts mixture policies obtained applying standard
RL techniques independent objective. policy subsequently improved following
convex combination gradients policy space nonnegative w.r.t.
objectives. objective i, gradient gi expected return w.r.t. policy
computed vector vi highest dot product gi simultaneously
satisfying nonnegativity condition returns used improving direction
i-th reward. vectors vi combined convex form obtain direction
parameter improvement. result policy belongs Pareto frontier.
approximation Pareto frontier obtained performing repeated searches
different weights reward gradients vi . hand, conditional optimization
consists maximizing objective maintaining certain level performance
others. resulting algorithm gradient search reduced policy space
value constrained objectives greater desired performance.
studies followed work Shelton (2001) regard policy gradient
algorithms applied MOMDPs. Recently Parisi et al. (2014) proposed two policy gradient
based MORL approaches that, starting initial policies, perform gradient ascent
policy parameters space order determine set nondominated policies.
first approach (called Radial ), given number p Pareto solutions required
approximating Pareto frontier, p gradient ascent searches performed, one
following different (uniformly spaced) direction within ascent simplex defined
convex combination singleobjective gradients. second approach (called Pareto
Following) starts performing singleobjective optimization moves along
Pareto frontier using two-step iterative process: updating policy parameters following
gradient ascent direction, applying correction procedure move
new solution onto Pareto frontier. Although methods exploit stochastic policies
proved effective several scenarios, still return scattered solutions
guaranteed uniformly cover Pareto frontier. best knowledge, nowadays
191

fiParisi, Pirotta, & Restelli

MORL algorithm returning continuous approximation Pareto frontier2 .
following sections present first approach able that: ParetoManifold
Gradient Algorithm (PMGA).
2.3 Policy Parametrization PolicyGradient Approaches
singleobjective
MDPs,

policygradient approaches consider parameterized policies
= : Rd , compact notation (a|s, ) policy
parameters space. Given policy parametrization , assume policy performance
J : F Rq least class C 2 .3 F called objectives space J defined
expected reward space possible trajectories
Z
p ( |) r( )d,
J () =


trajectory drawn density distribution p( |) reward vector
r( )
accumulated expected discounted reward trajectory , i.e.,
Prepresents
1
Ri (st , , st+1 ). Examples parametrized policies used context
ri ( ) = Tt=0
Guassian policies Gibbs policies. MOMDPs, q gradient directions defined
policy parameter (Peters & Schaal, 2008b), i.e.,
Z


Ji () =
p ( |) ri ( )d = E ln p ( |) ri ( )

"
#

1
X
b Ji (),
E ri ( )
ln (at |st , ) =
(1)
t=0

direction Ji associated particular discount factorreward function
b Ji () sample-based estimate. shown Equation (1),
pair < , Ri >
differentiability expected return connected differentiability policy
ln p ( |) =


1
X

ln (at |st , ).

t=0

remark notation. following use symbol DX F denote
derivative generic function F : Rmn Rpq w.r.t. matrix X.4 Notice
following relationship holds scalar functions vector variable: x f = (Dx f )T . Finally,
symbol Ix used denote x x identity matrix.

3. Gradient Ascent Policy Manifold Continuous Pareto Frontier
Approximation
section first provide general definition optimization problem want
solve explain solve MOMDP scenario using gradient
based approach. novel contributes section summarized Lemma 3.1
2. notable exception MOO approach Calandra, Peters, Deisenrothy (2014) Gaussian
Processes used obtain continuous approximation Pareto frontier.
3. function class C 2 continuous, twice differentiable derivatives continuous.
4. derivative operator well defined matrices, vectors scalar functions. Refer work
Magnus Neudecker (1999) details.

192

fiMORL Continuous Pareto Manifold Approximation

objective function gradient described. particular, provide solution
problem evaluating performance continuous approximation Pareto
frontier w.r.t. indicator function. problem non trivial MORL
direct access Pareto frontier manipulate policy
parameters. provide step-by-step derivation results leveraging manifold
theory matrix calculus.
3.1 Continuous Pareto Frontier Approximation Multiobjective
Optimization
shown locally Paretooptimal solutions locally forms (q 1)dimensional
manifold, assuming > q (Harada, Sakuma, Kobayashi, & Ono, 2007). follows
2objective problems, Paretooptimal solutions described curves policy
parameters objective spaces. idea behind work parametrize locally
Paretooptimal solution curve objectives space, order produce continuous
representation Pareto frontier.
Let generative space open set Rb b q. analogous high
dimensional function parameterized curve smooth map : Rq class
C l (l 1), P Rk free variables parameters,
respectively. set F = (T ) together map constitute parametrized
manifold dimension b, denoted F (T ) (Munkres, 1997). manifold represents
approximation Pareto frontier. goal find best approximation, i.e.,
parameters minimize distance real frontier
= arg max (F (T )) ,

(2)

P

: Rq R indicator function measuring quality F (T ) w.r.t.
true Pareto frontier. Notice Equation (2) interpreted special projection
operator (refer Figure 1a graphical representation). However, since requires
knowledge true Pareto frontier, different indicator function needed.
definition metric open problem literature. Recently, several metrics
defined, candidate presents intrinsic flaws prevent definition
unique superior metric (Vamplew et al., 2011). Furthermore, see
remainder section, proposed approach needs metric differentiable w.r.t.
policy parameters. investigate topic Section 5.
general, MOO algorithms compute value frontier sum value
points composing discrete approximation. scenario, continuous
approximate frontier available, maps integration Pareto manifold
Z
IdV,
(3)
L () =
F (T )

L () manifold value, dV denotes integral w.r.t. volume manifold
: F (T ) R indicator function measuring Pareto optimality point
F (T ). Assuming continuous, integral given (Munkres, 1997)
Z
Z
L () =
IdV
(I ) V ol (Dt (t)) dt,
F (T )



193

fiParisi, Pirotta, & Restelli

(a)

(b)

Figure 1: Transformation maps generic MOO setting (Figure (a)) MORL (Figure (b)). MOO possible consider parametrized solutions Figure (b), MORL necessary, mapping known
closed form determined (discounted) sum rewards.
1
provided integral exists V ol (X) = det X X 2 . standard way maximize
previous equation performing gradient ascent, updating parameters according
gradient manifold value w.r.t. parameters , i.e., + L () .
3.2 Continuous Pareto Frontier Approximation Multiobjective
Reinforcement Learning
standard multiobjective optimization function free designed,
MORL must satisfy conditions. first thing notice direct map
parameters space objective space unknown, easily
defined reparameterization involving policy space , shown Figure 1b.
previous section mentioned tight relationship (local)
manifold objective space (local) manifold policy parameters space.
mapping well known defined performance function J() defining
utility policy . means that, given set policy parameterizations, define
associated points objective space. consequence, optimization problem
reformulated search best approximation Pareto manifold
policy parameter space, i.e., search manifold policy parameter space
best describes optimal Pareto frontier.
Formally, let : smooth map class C l (l 1) defined
domain . think map parameterization subset (T ) :
choice point gives rise point (t) (T ) . means
subset (T ) space spanned map , i.e., (T ) bdimensional
parametrized manifold policy parameters space, i.e.,
(T ) = { : = (t), } ,
and, consequence, associated parameterized Pareto frontier bdimensional
open set defined
F (T ) = {J () : (T )} .
194

fiMORL Continuous Pareto Manifold Approximation

3.3 Gradient Ascent Manifold Space
point introduced notation needed derive gradient L ().
Lemma 3.1. (Pirotta et al., 2015) Let open set Rb , let F (T ) manifold
parametrized smooth map expressed composition maps J , (i.e., =
J : Rq ). Given continuous function defined point F (T ),
integral w.r.t. volume given
Z
Z
L () =
IdV =
(I (J )) V ol (D J()Dt (t)) dt,
F (T )



provided integral exists. associated gradient w.r.t. parameters given
Z

L ()
=
(I (J )) V ol (T) dt





Z





(I (J )) V ol (T) vec
+
Nb Ib Di Tdt, (4)


= J()Dt (t), Kronecker product, Nb = 12 (Ib2 + Kbb ) symmetric
(b2 b2 ) idempotent matrix rank 21 b(b + 1) Kbb permutation matrix (Magnus
& Neudecker, 1999). Finally,


Di = Dt (t)T Iq (D J()) Di (t) + (Ib J()) Di (Dt (t)) .
Proof. equation manifold value L () follows directly definition
volume integral manifold (Munkres, 1997) definition function composition.
following, provide detailed derivation i-th component gradient.
Let = J( )Dt (t),
Z
L ()

=
(I (J )) V ol (T) dt



Z
det TT
1
+
(I (J ))
dt.
2V ol (T)


indicator derivative determinant derivative respectively expanded

(I (J )) = DJ I(Jt ) J( ) Di (t),



det TT
det TT vec TT
=
,



(vec
T)
(vec
T)

|
{z
} |
{z
} | {z } |{z}
11

1b2

b2 qb

qb1



det TT
(vec T)T
TT
(vec T)T





,
= det
vec






= 2Nb Ib TT ,

195

fiParisi, Pirotta, & Restelli

Kronecker product, Nb = 12 (Ib2 + Kbb ) symmetric (b2 b2 ) idempotent
matrix rank 21 b(b + 1) Kbb permutation matrix (Magnus & Neudecker, 1999).
(T)
last term expanded Di vec
. start basic property
differential, i.e.,
(D J()Dt (t)) = d(D J())Dt (t) + J() d(Dt (t))
then, applying vector operator,
dvec (D J()Dt (t)) = vec (d(D J())Dt (t)) + vec (D J() d(Dt (t)))


= Dt (t)T Iq dvec (D J()) + (Ib J()) dvec (Dt (t)) .
{z
} |
{z
}|
{z
}
{z
}|
|
dq1

bqdq

bqbd

bd1

Finally, derivative given

vec J() (t)
vec Dt (t)


Di = Dt (t)T Iq
+ (Ib J())




|
{z
} | {zi }
|
{z
}
dqd



d1

bd1





= Dt (t) Iq (D J()) Di (t) + (Ib J()) Di (Dt (t)) .

interesting notice gradient manifold value L () requires
compute second derivatives policy performance J(). However, (D J()) =
vec J()
denote Hessian matrix transformation

(m,n)
H
Ji

=

2
Dn,m
Ji ()


=
n



Ji




= Dp,n (D J()) ,

p = + q(m 1) q (number objectives) number rows Jacobian
matrix. Recall Hessian
matrixis defined derivative transpose
Jacobian, i.e., H J() = J()T .
now, little research done second-order methods5 particular
Hessian formulations. first analysis performed Kakade (2001), provided
formulation based policy gradient theorem (Sutton, McAllester, Singh, & Mansour,
2000). Recently, extended comparison Newton method, EM algorithm
natural gradient presented Furmston Barber (2012). sake clarity,
report Hessian formulation provided Furmston Barber (2012) using notation
introduce optimal baseline (in terms variance reduction) formulation.
Lemma 3.2. MOMDP, Hessian H J() expected discounted reward J
w.r.t. policy parameters qd matrix obtained stacking Hessian
5. Notable exceptions natural gradient approaches that, although explicitly require
compute second-order derivatives, usually considered second-order methods.

196

fiMORL Continuous Pareto Manifold Approximation

component
H J() =


vec




Ji ()






H J1 ()


..
=
,
.
H Jq ()


Z
H Ji () =

!
p ( |) (ri ( ) bi ) ln p ( |) ln p ( |)T + H ln p ( |) d,

(5)




ln p ( |) =


1
X

ln (at |st , ),

H ln p ( |) =

t=0


1
X

H ln (at |st , ).

t=0
(m,n)

optimal baseline Hessian estimate H
Ji provided Equation (5)
computed done Greensmith, Bartlett, Baxter (2004) order reduce
variance gradient estimate. given component-wise


2
(m,n)
E p(|) Ri ( ) G
( )
(m,n)

bi
=
2 ,
(m,n)
E p(|) G
( )
(m,n)

(m,n)

n
G
( ) =
ln p ( |) ln p ( |)+H
Appendix A.

ln p ( |). derivation, refer

4. Manifold Gradient Estimation Sample Trajectories
MORL, prior knowledge reward function state transition
model, need estimate gradient L () trajectory samples. section
aims provide guide estimation manifold gradient. particular, review
results related estimation standard RL components (expected discounted return
gradient) provide finite-sample analysis Hessian estimate.
formulation gradient L () provided Lemma 3.1 composed terms
related parameterization manifold policy space terms related
MDP. Since map free designed, associated terms (e.g., Dt (t))
computed exactly. hand, terms related MDP (J (), J()
H J()) need estimated. estimate expected discounted reward
associated gradient old topic RL literature several results
proposed (Kakade, 2001; Pirotta, Restelli, & Bascetta, 2013), literature lacks explicit
analysis Hessian estimate. Recently, simultaneous perturbation stochastic approximation technique exploited estimate Hessian (Fonteneau & Prashanth, 2014).
However, rely formulation provided Furmston Barber (2012)
Hessian estimated trajectory samples obtained current policy, removing
necessity generating policy perturbations.
197

fiParisi, Pirotta, & Restelli

Algorithm 1 ParetoManifold Gradient Algorithm
Define policy , parametric function , indicator learning rate
Initialize parameters
Repeat terminal condition reached
Collect n = 1 . . . N trajectories
Sample free variable t[n] generative space

Sample policy parameters [n] = t[n]
n

[n] [n] [n]
Execute trajectory collect data st , , rt,
t=1

b Ji () according Equation (1)
Compute gradients
b Ji () according Equation (6)
Compute Hessians H
Compute manifold value derivative L () according Equation (4)
Update parameters + L ()
Since p ( |) unknown, expectation approximated empirical average.
Assuming access N trajectories, Hessian estimate
!
N

1
X
X
1
b Ji () =
H
rnt,i b
N
n=1
t=0
!T 1
!

1

1
X
X
X

ln ant ,snt
ln ant ,snt
+
H ln ant ,snt ,
(6)
t=0




n
[n] [n] [n]
st , , rt,

t=1

t=0

t=0

denotes n-th trajectory. formulation resembles def-

inition REINFORCE estimate given Williams (1992) gradient J().
estimates, known likelihood ratio methods, overcome problem determining perturbation parameters occurring finite-difference methods. Algorithm 1 describes
complete PMGA procedure.
order simplify theoretical analysis Hessian estimate, make following assumptions.
Assumption 4.1 (Uniform boundedness). reward function, log-Jacobian
log-Hessian policy uniformly bounded: = 1, . . . , q, = 1, . . . , d, n =
1, . . . , d, (s, a, s0 ) ,







(m)

(m,n)

0
ln (a|s, )fi G.
fiRi (s, a, )fi Ri ,
fiD ln (a|s, )fi D,
fiH
Lemma 4.2. Given parametrized policy (a|s, ), Assumption 4.1, i-th component log-Hessian expected return bounded
kH Ji ()kmax


Ri
2
TD + G ,
1

max norm matrix defined kAkmax = maxi,j {aij }.
198

fiMORL Continuous Pareto Manifold Approximation

Proof. Consider definition Hessian Equation (5). assumption 4.1,
Hessian components bounded (m, n)

"

1

1

fiZ
X
X


(m,n)

ln (at |st , )
ln (aj |sj , )
Ji ()fi = p ( |) ri ( )
fiH


n
t=0
j=0
#fi

2

+
ln (at |st , )

n



1

1

1

X
X
X
Ri
2

Ri
l1
+ G =
TD + G .
1
l=0

t=0

j=0

previous result used derive bound sample complexity
Hessian estimate.
Theorem 4.3. Given parametrized policy (a|s, ), Assumption 4.1, using
following number -step trajectories

2 2
1
Ri
2
N 2
TD + G
ln

2i (1 )
b Ji () generated Equation (6) probability 1
gradient estimate H


b

.
H Ji () H Ji ()
max

Proof. Hoeffdings inequality implies m, n
N 2 2



PN
b (m,n)

(m,n)
(bi ai )2
i=1
=.
P fiH
Ji () H
Ji () 2e

Solving equation N noticing Lemma 4.2 provides bound sample,
obtain

2 2
1
Ri
2
N= 2
TD + G
ln .

2i (1 )

integral estimate computed using standard MonteCarlo techniques. Several
statistical bounds proposed literature, refer Robert Casella (2004)
survey MonteCarlo methods.
point paper, reader may expect analysis convergence (or
convergence rate) optimal parametrization. Although consider analysis theoretically challenging interesting, provide result related topic.
analysis hard (or even impossible) provide general settings since objective
function nonlinear nonconcave. Moreover, analysis simplified scenario (if
possible) almost useless real applications.
199

fiParisi, Pirotta, & Restelli

5. Metrics Multiobjective Optimization
section, review indicator functions proposed literature, underlining advantages drawbacks, propose alternatives. Recently, MOO focused
use indicators turn multiobjective optimization problem singleobjective
one optimizing indicator itself. indicator function used assign every
point given frontier scalar measure gives rough idea discrepancy candidate frontier Pareto one. Since instead optimizing objective
functions directly indicatorbased algorithms aim finding solution set maximizes
indicator metric, natural question arises correctness change
optimization procedure properties indicator functions enjoy. instance,
hypervolume indicator weighted version among widespread metrics
literature. metrics gained popularity refinements
Pareto dominance relation (Zitzler, Thiele, & Bader, 2010). Recently, several works
proposed order theoretically investigate properties hypervolume indicator (e.g., Friedrich, Horoba, & Neumann, 2009). Nevertheless, argued
hypervolume indicator may introduce bias search. Furthermore another important
issue dealing hypervolume indicator choice reference point.
perspective, main issues metric high computational complexity (the
computation hypervolume indicator #Phard problem, see Friedrich et al., 2009)
and, all, non differentiability. Several metrics defined field
MOO, refer work Okabe, Jin, Sendhoff (2003) survey. However,
MOO literature able provide superior metric among candidates
one suited scenario. Again, main issues non differentiability,
capability evaluating discrete representations Pareto frontier intrinsic
nature metrics. example, generational distance, another widespread measure
based minimum distance reference frontier, available settings.
overcome issues, mixed different indicator concepts novel differentiable
metrics. insights guided metrics definition related MOO
desiderata. Recall goal MOO compute approximation frontier
including solutions accurate, evenly distributed covering range similar
actual one (Zitzler et al., 2003). Note uniformity frontier intrinsically guaranteed continuity approximation introduced. concepts
mind, need induce accuracy extension indicator function.
stressed clear definition want indicator
maximized real Pareto frontier. must ensure indicator function
induces partial ordering frontiers: manifold F2 solutions (weakly) dominated
manifold F1 ones, F1 manifold value must better F2 one.
Definition 5.1 (Consistent Indicator Function). Let F set (q 1)dimensional
manifolds associated MOMDP q objectives. Let k manifold
policy parameters
space mapping Fk F F true Pareto frontier. Let
R
LI (F) = F IdV manifold value. indicator function consistent
Fk 6= Fh , LI (Fh ) > LI (Fk ) Fh F ,



h , k , k , j h , j = LI (Fh ) > LI (Fk ).
200

fiMORL Continuous Pareto Manifold Approximation

5.1 Accuracy Metrics
Given reference point p, simple indicator obtained computing distance
every point frontier F reference point, i.e.,
= kJ pk22 .
mentioned hypervolume indicator, choice reference point may
critical. However, natural choice utopia (ideal) point (pU ), i.e., point
optimizes objectives. case goal minimization indicator
function, denoted IU (utopia indicator ). Since dominated policy farther
utopia least one Paretooptimal solution, accuracy easily guaranteed.
hand, since minimized, measure forces solution collapse
single point, thus consistent. Note problem mitigated
(but solved) forcing transformation pass singleobjective
optima. Although trick helpful, discuss Section 6, requires
find singleobjective optimal policies order constrain parameters. However,
information required properly set utopia.
Concerning accuracy frontier, theoretical perspective, possible
define another metric using definition Pareto optimality. point Paretooptimal
(Brown & Smith, 2005)
l(, ) =

q
X

Ji () = 0,

q
X

i=1

= 1,

0,

i=1

is, possible identify ascent direction simultaneously improves
objectives. consequence, Paretoascent direction l point Pareto
frontier null. Formally, metric respects Paretooptimality defined
follows:
q
X
= minq kl(, )k22 ,
= 1, 0.
R

i=1

denote indicator IPN (Pareto norm indicator ). utopiabased metric,
extent frontier taken account without constraint optimal
solution collapses single point frontier.
5.2 Covering Metrics
extension frontier primary concern, maximizing distance
antiutopia (pAU ) results metric grows frontier dimension. However,
contrary utopia point, antiutopia located half space
reached solutions MOO problems. means considering
antiutopiabased metric maximization problem could become unbounded moving
solutions arbitrary far Pareto frontier antiutopia point. Therefore
measure, denoted IAU (antiutopia indicator ), provide guarantee
accuracy.
201

fiParisi, Pirotta, & Restelli

5.3 Mixed Metrics
mentioned indicators provide one desiderata. consequence,
resulting approximate frontier might arbitrary far actual one. order
consider desiderata mix previous concepts following indicator:
= IAU w
w penalization function, i.e., monotonic function decreases
accuracy input increases, e.g., w = 1 IPN w = 1 IU . metrics, denoted
respectively I,PN I,U , take advantage expansive behavior antiutopia
based indicator accuracy optimalitybased indicator. way
desiderata met single scalar measure, C l (l 1) differentiable.
Another solution mix utopia antiutopiabased indicators different way.
want solutions simultaneously far antiutopia close utopia,
consider following metric (to maximized):
= 1

IAU
2 ,
IU

1 2 free parameters.
next section, show proposed mixed metrics effective driving
PMGA close Pareto frontier exact approximate scenarios. However,
want make clear consistency guaranteed strongly depends
free parameters , 1 2 . insights discussed Section 7.

6. Experiments
section, evaluate algorithm two problems, Linear-Quadratic Gaussian
regulator water reservoir control task. PMGA compared state-of-the-art methods
(Peters, Mulling, & Altun, 2010; Castelletti et al., 2013; Parisi et al., 2014; Beume, Naujoks,
& Emmerich, 2007) using hypervolume (Vamplew et al., 2011) extension
previously defined performance index (Pianosi, Castelletti, & Restelli, 2013), named loss,
measuring distance approximate Pareto front reference one. 2objective
problems, hypervolume exactly computed. 3objective problems, given high
computational complexity, hypervolume approximated MonteCarlo estimate
percentage points dominated frontier cube defined utopia
antiutopia points. estimate one million points used.
}
idea loss index compare true Pareto frontier FW = {Jw
wW

space weights W frontier JW = {Jbw }wW returned algorithm
weights (Jw denotes discounted return new singleobjective MDP defined
linear combination objectives w). Formally loss function l defined
l(J



Jw maxM Jbw

Z

J

, F, W, p) =
wW

Jw

p(dw),

(7)

p() probability density simplex W Jw = w J normalization factor, i-th component J difference best
202

fiMORL Continuous Pareto Manifold Approximation

worst value i-th objective Pareto frontier, i.e., Ji = max(Ji ) min(Ji ).
M.
means that, weight, policy minimizes loss function chosen JW
true Pareto frontier F known, reference one used.
Since PMGA returns continuous frontiers two scores designed discrete
ones, evaluation frontiers discretized. Also, figures presented
section show discretized frontiers order allow better representation. Besides
hypervolume loss function, report number solutions returned
algorithm number rollouts (i.e., total number episodes simulated
learning process). data collected simulation results averaged
ten trials6 . experiments, PMGA learning rate


,
(8)
=

L () 1 L ()
positive definite, symmetric matrix userdefined parameter.
stepsize rule comes formulation gradient ascent constrained problem
predefined distance metric (Peters, 2007) underlies derivation natural
gradient approaches. However, since algorithm exploits vanilla gradient (i.e.,
consider Euclidean space) metric identity matrix I.
remainder section organized follows. start studying behavior
metrics proposed Section 5 effects parametrization (t) LQG.
Subsequently, focus attention sample complexity, meant number rollouts
needed approximate Pareto front. Finally, analyze quality algorithm
water reservoir control task, complex real world scenario, compare
state-of-the-art multiobjective techniques. case study, domains first
presented results reported discussed.
6.1 Linear-Quadratic Gaussian Regulator (LQG)
first case study discrete-time Linear-Quadratic Gaussian regulator (LQG)
multi-dimensional continuous state action spaces (Peters & Schaal, 2008b).
LQG problem defined following dynamics
st+1 = Ast + Bat ,

N (K st , )

R(st , ) = st Qst Rat
st n-dimensional column vectors, A, B, Q, R Rnn , Q symmetric
semidefinite matrix, R symmetric positive definite matrix. Dynamics
coupled, i.e., B identity matrices. policy Gaussian parameters
= vec(K), K Rnn . Finally, constant covariance matrix = used.
LQG easily extended account multiple conflicting objectives.
particular, problem minimizing distance origin w.r.t. i-th axis
taken account, considering cost action axes
X
Ri (st , ) = s2t,i
a2t,j .
i6=j

6. Source code available https://github.com/sparisi/mips.

203

fiParisi, Pirotta, & Restelli

Since maximization i-th objective requires null action axes,
objectives conflicting. reward formulation violates positiveness matrix
Ri , change adding sufficiently small -perturbation




X
X
Ri (st , ) = (1 ) s2t,i +
a2t,j
s2t,j + a2t,i .
i6=j

j6=i

parameters used experiments following: = 0.9, = 0.1 initial
state s0 = [10, 10]T s0 = [10, 10, 10]T 2 3objective case, respectively.
following sections compare performance proposed metrics several settings.
made use tables summarize results end set experiments.
6.1.1 2objective Case Results
LQG scenario particular instructive since terms involved definition returns, gradients Hessians computed exactly. therefore focus studying
different policy manifold parametrizations (t) metrics I.
Unconstrained Parametrization. domain problematic since defined
control actions range [1, 0] controls outside range lead divergence
system. primary concern therefore related boundedness control
actions, leading following parametrization manifold policy space:


(1 + exp(1 + 2 t))1
,
[0, 1].
= (t) =
(1 + exp(3 + 4 t))1
Utopia antiutopia points [150, 150] [310, 310], respectively, metrics IAU
IU normalized order 1 reference point.7 learning step parameter
Equation (8) = 1.
case, exploiting nonmixed metrics, PMGA able learn good approximation Pareto frontier terms accuracy covering. Using utopiabased
indicator, learned frontier collapses one point knee front.
behavior occurs using IPN . Using antiutopia point reference point solutions
dominated approximate frontier gets wider, diverging true frontier
expanding opposite half space. behaviors surprising, considering
definition indicator functions, explained Section 5.
contrary, shown Figure 2, mixed metrics able achieve
accuracy covering. starting 0 set [1, 2, 0, 3]T , algorithm
able learn even starting different random parameters. free metric parameters
set = 1.5 I,PN , = 1 I,U 1 = 3, 2 = 1 .8 Although
shown figure, I,U behaved similarly I,PN . notice cases
first accuracy obtained pushing parametrization onto Pareto frontier,
frontier expanded toward extrema order attain covering.
7. Recall initially defined = kJ pk22 . slightly modify normalizing policy
performance w.r.t. reference point: = kJ/p 1k22 , / component-wise operator.
8. Section 7 study sensitivity proposed metrics parameters .

204

fiMORL Continuous Pareto Manifold Approximation

Table 1: Summary 2dimensional LQG (unconstrained)
Metrics
Nonmixed
Issues:

Accuracy
Covering
7
7
IU , IPN : frontier collapses one point
IAU : diverging behavior dominated solutions found
3
3

Mixed

Partial solution
Final approximation
True Pareto frontier

300

250

16
L ()

23

J2

100

20

200

50

1

21

end

0

150
150

200

250

300

0

50

J1

100

Iterations

(a) Learning process mixed metric I,PN .

300

15

250
10

L ()

J2

1,000

1

200

500

0

5
end
150
150

200

250

500

300

0

J1

50

100

Iterations

(b) Learning process mixed metric .

Figure 2: Learning processes 2objective LQG without constraint
parametrization. Numbers denote iteration, end denotes frontier obtained
terminal condition reached. left, approximated Pareto frontiers,
right corresponding L (). Using I,PN (Figure (a)) (Figure (b)) approximated frontier overlaps true one. However, using , PMGA converges faster.

205

fiParisi, Pirotta, & Restelli

Constrained Parametrization. alternative approach consists forcing policy
manifold pass extreme points true front knowing parameterizations singleobjective optimal policies. general, requires additional
optimizations collection additional trajectories must accounted
results. However, extreme points required set utopia antiutopia. Moreover, case optimal singleobjective policies available literature.
reasons, count additional samples report total number rollouts.
Using constrained parameterization, two improvements easily obtained. First,
number free parameters decreases and, consequence, learning process
simplified. Second, approximate frontier forced sufficiently large area
cover extrema. Thus, problem covering shown nonmixed indicators
alleviated or, cases, completely eliminated. 2dimensional LQG,
parametrization forced pass extrema frontier following:


(1 + exp(2.18708 1 t2 + (3.33837 + 1 )t))1
= (t) =
,
[0, 1].
(1 + exp(1.15129 2 t2 + (3.33837 + 2 )t))1
initial parameter vector 0 = [2, 2]T . constraint able correct diverging
behavior IU IPN , returned accurate wide approximation Pareto
frontier, shown Figure 2a. notice much faster convergence, since algorithm required learn fewer parameters (two instead four). However, IAU still shows
diverging behavior initial parameters 0 (in Figure 2b, 0 = [6, 6]T ).
contrary, solutions obtained metrics independent initial 0 ,
algorithm converges close true frontier even starting parametrization
generating initial frontier far away true one.
6.1.2 3objective Case Results
Unconstrained Parametrization.


(1 + exp(1 + 2 t1 + 3 t2 ))1
= (t) = (1 + exp(4 + 5 t1 + 6 t2 ))1 ,
(1 + exp(7 + 8 t1 + 9 t2 ))1

simplex([0, 1]2 ).

Utopia antiutopia points [195, 195, 195] [360, 360, 360], respectively, metrics
IAU , IU normalized. initial parameters drawn uniform distribution 0
U nif ((0, 0.001)) (0 = 0 causes numerical issues) learning rate parameter = 1.
2objective scenario, frontiers learned IU IPN collapse single
point, IAU divergent trend (Figure 3a). However, unlike 2objective LQR,
I,PN failed correctly approximate Pareto frontier. reason tuning
difficult, given difference magnitude IPN IAU contrary,
I,U = 1.5 1 = 3, 2 = 1 returned high quality approximate frontier.
latter shown Figure 3b. Although small areas true Pareto frontier
covered approximate one, stress fact policies found
Paretooptimal. strength metrics found normalization
utopia antiutopiabased indicators. expedient, indeed, allows easier tuning
free metric parameters, magnitude single components similar.
insights tuning mixed metrics parameters discussed Section 7.
206

fiMORL Continuous Pareto Manifold Approximation

Table 2: Summary 2dimensional LQG (constrained)
Metrics
Nonmixed: IU , IPN
Nonmixed: IAU
Issues:
Mixed

Accuracy
Covering
3
3
7
7
IAU : diverging behavior dominated solutions found
3
3

Partial solution
Final approximation
True Pareto frontier

300

1

120
L ()

J2

250

2

200

3

130

end
150
150

200

250

140

300

0

5

J1

10

15

20

25

100

120

Iterations

(a) Learning process utopiabased metric IU .

300

23

250

600

J2

7

200

L ()

3
1

400
200

150

0

150

200

250

300

0

J1

20

40

60

80

Iterations

(b) Learning process antiutopiabased metric IAU .

Figure 3: Learning process 2objective LQG parametrization forced pass
extreme points frontier. constraints able correct behavior
IU (Figure (a)) convergence faster previous parametrization. However,
IAU still diverges (Figure (b)) returned frontier includes dominated solutions, since
metric considers covering frontier accuracy.

207

fiParisi, Pirotta, & Restelli

Table 3: Summary 3dimensional LQG (unconstrained)
Metrics
Nonmixed
Issues:

Accuracy
Covering
7
7
IU , IPN : frontier collapses one point
IAU : diverging behavior dominated solutions found
7
7
I,PN : difficult tuning
3
3

Mixed: I,PN
Issues:
Mixed: I,U ,

True Pareto frontier
Approximate frontier

J3

1,000

500

500

500

1,000

J1

1,000

500

1,000

500

J1

J2

1,000

J2

(a) Frontier approximated antiutopiabased metric IAU .

J3

True Pareto frontier
Approximate frontier

300
350

200
300

200
200

200
J2

300

300

J1

250
250

300
200

J1

J2

350

(b) Frontier approximated mixed metric .

Figure 4: Resulting frontiers 3objective LQG using unconstrained parametrization. Frontiers discretized better representation. IAU learning
diverges (Figure (a)) correctly approximates Pareto frontier (Figure (b)).

208

fiMORL Continuous Pareto Manifold Approximation

Constrained Parametrization.


(1 + exp(a + 1 t1 (b 2 )t2 1 t21 2 t22 3 t2 t1 ))1
,
(1 + exp(a (b 4 )t1 + 5 t2 4 t21 5 t22 6 t1 t2 ))1
= (t) =
2
2
1
(1 + exp(c + (7 + b)t1 + (8 + b)t2 7 t1 8 t2 9 t1 t2 ))
= 1.151035476,

b = 3.338299811,

simplex([0, 1]2 ).

c = 2.187264336,

initial parameters 0 = 0. Numerical results reported Table 4,
hypervolume computed normalizing objective w.r.t. antiutopia. Figure 5
shows frontiers obtained using utopia antiutopiabased indicators. clearly
see that, unlike 2objective case, even constrained parametrization metrics
lead poor solutions, failing providing MO desiderata. Figure 5a, using IU
frontier still tends collapse towards center true one, order minimize
distance utopia point (only constraint prevents that). Although shown
figures, similar slightly broader frontier returned using IPN . However,
stress solutions belong Pareto frontier, i.e., nondominated solutions
found. Figure 5b shows frontier obtained IAU . expected, algorithm tries
produce frontier wide possible, order increase distance antiutopia
point. behavior leads dominated solutions learning process diverges.
contrary, using mixed metrics I,PN ( = 30), I,U ( = 1.4) (1 =
2.5, 2 = 1) PMGA able completely accurately cover Pareto frontier, shown
Figures 6a 6b. worth notice different magnitude free parameter
I,PN compared 2objective case, 1.5. already discussed, due
substantial difference magnitude IAU IPN . contrary, tuning
mixed metrics easier, similar parameters used unconstrained
parametrization proved effective. come back topic Section 7.
Finally, shown Table 4, I,U achieve best numerical results, first
attains highest hypervolume lowest loss, latter attains fastest
convergence. superiority resides easy differentiability tuning, especially compared I,PN . reasons, chosen empirical analysis
sample complexity comparison state-of-the-art algorithms
real-world MO problem, discussed next sections.
Table 4: Performance comparison different metrics 3objective LQG
constrained parametrization. reference frontier hypervolume 0.7297.
Metric

Hypervolume

Loss

#Iterations

IU

0.6252

2.9012e-02

59

IAU

0





IPN

0.7167

1.9012e-02

133

I,PN

0.7187

5.2720e-04

47

I,U

0.7212

4.9656e-04

33



0.7204

5.0679e-04

15

209

fiParisi, Pirotta, & Restelli

Table 5: Summary 3dimensional LQG (constrained)
Metrics
Nonmixed
Issues:
Mixed

Accuracy
Covering
7
7
IU , IPN : frontier collapses one point
IAU : diverging behavior dominated solutions found
3
3

J3

True Pareto frontier
Approximate frontier

300
350

200
300

200
200

200
J2

300

300

J1

250
250

300
200

J1

J2

350

(a) Frontier approximated utopiabased metric IU .

J3

True Pareto frontier
Approximate frontier

500
200

350
250

300
200

200
300
J2

J1

300
250

300

J2

350
J1

200

(b) Frontier approximated antiutopiabased metric IAU .

Figure 5: Results parametrization forced pass extreme points
frontier. Using IU (Figure (a)) frontier shrinks much allowed parametrization. constraint therefore able solve issues metric 2
objective scenario. contrary, using IAU frontier gets wider diverges
true one (in Figure (b) intermediate frontier shown).

210

fiMORL Continuous Pareto Manifold Approximation

J3

True Pareto frontier
Approximate frontier

300

3

200
200

200
J2

300

300

0.4
0.6
0.8

J1

0.4

0.8

0.6
1

(a) Frontier objectives space.

0.4

0.6
2
0.8

(b) Frontier policy parameters space.

Figure 6: Results using constrained parametrization. shown Figure (a),
approximate frontier perfectly overlaps true one, despite small discrepancies
policy parameters space learned parameters optimal ones (Figure (b)).
Similar frontiers obtainable I,PN I,U .

6.1.3 Empirical Sample Complexity Analysis
section, provide empirical analysis sample complexity PMGA, meant
number rollouts needed approximate Pareto frontier. goal identify
relevant parameter estimate MDP terms J(), J() HJ().
analysis performed 2dimensional LQG domain varying number
policies used estimate integral per iteration PMGA number episodes
policy evaluation. steps episode fixed 50. first used
parametrization forced pass extreme points frontier 0 = [3, 7]T ,
produces initial approximate frontier far true one. parameter
learning rate Equation (8) set = 0.5 parameter I,U set
= 1. performance criterion, choose total number rollouts required reach
loss smaller 5 104 hypervolume larger 99.5% reference one.
criteria used conditions convergence (both satisfied).
evaluation, MDP terms computed closed form. terminal condition must
reached 100, 000 episodes otherwise algorithm forced end. symbol used
represent latter case.
Table 6a results relevant parameter number episodes
used estimate MDP terms. parameter controls variance estimate,
i.e., accuracy estimate L (). increasing number episodes,
estimation process less prone generate misleading directions, happens, instance,
oneepisode case parameters move towards wrong direction. contrary,
number points used estimate integral (denoted table #t) seems
significant impact final performance algorithm, influences
number model evaluations needed reach prescribed accuracy. best behavior,
211

fiParisi, Pirotta, & Restelli

Table 6: Total number episodes needed converge varying number points #t
approximate integral number episodes #ep per point. symbol
used terminal condition reached.
(a) parametrization constrained pass extreme points frontier, one
point sufficient move whole frontier towards right direction.

#ep

1

5

10

25

50

1



695 578

560 172

1, 850 757

1, 790 673

5



2, 550 1, 509

3, 440 2, 060

5, 175 3, 432

8, 250 2, 479

10



4, 780 4, 623

6, 820 3, 083

10, 500 3, 365

11, 800 1, 503

25



7, 525 2, 980

15, 100 9, 500

18, 375 6, 028

24, 250 7, 097

50



8, 700 5, 719

18, 000 6, 978

26, 750 7, 483

50, 000 1, 474

#t

(b) contrary, using unconstrained parametrization, PMGA needs sufficient number
episodes enough points correct update step.

#ep

1

5

10

25

50

1











5









29, 350 7, 310

10







44, 100 9, 466

64, 500 1, 359

25







60, 500 1, 000

83, 500 8, 923

50





47, 875 18, 558

84, 250 1, 457



#t

samplebased perspective, obtained exploiting one point
integral estimate. Although surprising, simple explanation exists. forcing
parameterization pass singleobjective optima, correct estimation
gradient direction single point enough move entire frontier toward
true one, i.e., move parameters towards optimal ones.
contrary, unconstrained parametrization used, one point sufficient
anymore, shown Table 6b. case, initial parameter vector set 0 =
[1, 1, 0, 0]T , learning rate parameter = 0.1 terminal condition requires
frontier loss smaller 103 hypervolume larger 99% reference
frontier. Without constraint, algorithm needs accuracy evaluation
single points i.e., sufficient number episodes enough points move whole
frontier towards right direction. accuracy gradient estimate L () therefore
depends number points number episodes, PMGA requires
much rollouts converge. best behavior, samplebased perspective,
obtained exploiting five points integral estimate 50 episodes
policy evaluation.
212

fiMORL Continuous Pareto Manifold Approximation

6.2 Water Reservoir
water reservoir modeled MOMDP continuous state variable representing water volume stored reservoir, continuous action controlling
water release, state-transition model depending stochastic reservoir inflow ,
set conflicting objectives. domain proposed Pianosi et al. (2013).
Formally, state-transition function described mass balance equation
st+1 = st + t+1 max(at , min(at , )) st reservoir storage time t; t+1
reservoir inflow time + 1, generated white noise normal distribution
t+1 N (40, 100); release decision; minimum maximum
releases associated storage st according relations = st = max(st 100, 0).
work consider three objectives: flooding along lake shores, irrigation
supply hydro-power supply. immediate rewards defined
R1 (st , , st+1 ) = max(ht+1 h, 0),
R2 (st , , st+1 ) = max( , 0),
R3 (st , , st+1 ) = max(e et+1 , 0),
ht+1 = st+1 /S reservoir level (in following experiments = 1), h
flooding threshold (h = 50), = max(at , min(at , )) release reservoir,
water demand ( = 50), e electricity demand (e = 4.36) et+1 electricity
production
et+1 = g H2 0 ht+1 ,
= 106 /3.6 dimensional conversion coefficient, g = 9.81 gravitational
acceleration, = 1 turbine efficiency H2 0 = 1, 000 water density. R1 denotes
negative cost due flooding excess level, R2 negative deficit
water supply R3 negative deficit hydro-power production.
original work, discount factor set 1 objectives
initial state drawn finite set. However, different settings used learning
evaluation phases. Given intrinsic stochasticity problem, policies
evaluated 1,000 episodes 100 steps, learning phase requires different
number episodes 30 steps, depending algorithm. discuss details
results section.
Since problem continuous exploit Gaussian policy model


(a|s, ) = N + (s)T , 2 ,
: Rd basis functions, = || = {, , }. optimal policies
objectives linear state variable, use radial basis approximation


(s) = e

ksci k2
wi

.

used four centers ci uniformly placed interval [20, 190] widths wi 60,
total six policy parameters.
213

fiParisi, Pirotta, & Restelli

6.2.1 Results
evaluate effectiveness algorithm analyzed performance
frontiers found weighted sum Stochastic Dynamic Programming (Pianosi et al., 2013),
Multi-objective FQI (Pianosi et al., 2013), episodic version Relative Entropy Policy
Search (Peters et al., 2010; Deisenroth et al., 2013), SMS-EMOA (Beume et al., 2007),
two recent policy gradient approaches, i.e., Radial Algorithm ParetoFollowing
Algorithm (Parisi et al., 2014). Since optimal Pareto front available, one
found SDP chosen reference one loss computation. MOFQI learns
deterministic policies (i.e., standard deviation Gaussian set zero)
trained using 10, 000 samples dataset 50, 000 tuples 2objective
problem 20, 000 samples dataset 500, 000 tuples 3objective problem.
remaining competing algorithms learn stochastic policies. number episodes
required policy update step 25 REPS, 100 PFA RA, 50 SMS-EMOA.
Given episodic formulation, REPS draws parameters upper distribution
(|) = N (, ) ,
diagonal covariance matrix, set zero. However, since algorithm
learns parameters = {, }, overall learned policy still stochastic. SMS-EMOA
maximum population size 100 500 2 3objective case, respectively.
crossover uniform mutation, chance 80% occur, adds white
noise random chromosomes. iteration, top 10% individuals kept
next generation guarantee solution quality decrease. Finally, MOFQI
scalarizes objectives using weights SDP, i.e., 11 25 weights 2
3objective case, respectively. REPS uses instead 50 500 linearly spaced weights. RA
follows 50 500 linearly spaced directions and, along PFA, exploits natural
gradient (Peters & Schaal, 2008a) adaptive learning step Equation (8), = 4
= F , F Fisher information matrix. Concerning parametrization
PMGA, used complete first degree polynomial 2objective case


66 1 t2 + (1 16)t
105 2 t2 + (2 + 20)t


18 3 t2 + (3 16)t

,
[0, 1].
= (t) =
2

23 4 + (4 + 53)t
39 5 t2 + (5 + 121)t
0.01 6 t2 + (6 + 0.1)t
Similarly, 3objective case complete second degree polynomial used


36 + (15 1 )t2 + (1 + 1)t1 t2 + 30t21 + (1 1)t22
57 (27 + 2 )t2 + (2 + 1)t1 t2 48t21 + (2 1)t22


13 + (7 23 )t1 + (3 + 1)t1 t2 + (23 2)t21 11t22
2

= (t) =
30 + (9 24 )t1 + (4 + 1)t1 t2 + (24 2)t2 + 60t2 , simplex([0, 1] ).
1
2

104 + (57 5 )t2 + (5 + 1)t1 t2 65t2 + (5 1)t2
1

0.05 + (1 6 )t2 + (6 + 1)t1 t2 + (6 1)t22

2

parameterizations forced pass near extreme points Pareto frontier,
computed singleobjective policy search. cases starting parameter
214

fiMORL Continuous Pareto Manifold Approximation

103
9.5

L ()

J2 (Water Demand)

0

1

2

10

SDP
PMGA(0 )
PMGA(end )

10.5
11
11.5

50

100 150 200
Iterations

250

4

(a)

3.5

3

2.5 2 1.5
J1 (Flooding)

1

(b)

Figure 7: Results 2objective water reservoir. Even starting arbitrary poor
initial parametrization, PMGA able approach true Pareto frontier (Figure (b)).
Figure (a), trend manifold metric L () averaged ten trials.

vector 0 = [0, 0, 0, 0, 0, 50]T . last parameter set 50 order guarantee
generation sufficiently explorative policies, 6 responsible variance
Gaussian distribution. However, fair comparison, competing algorithms
take advantage information, mean initial policies calculated accordingly behavior optimal ones described Castelletti et al. (2012), i.e.,
= [50, 50, 0, 0, 50]T . initial standard deviation set = 20 guarantee sufficient exploration. parametrization avoids completely random poor quality initial
policies. Utopia antiutopia points set [0.5, 9] [2.5, 11] 2
objective case, [0.5, 9, 0.001] [65, 12, 0.7] 3objective one.
According results presented Section 6.1.3, integral estimate PMGA
performed using MonteCarlo algorithm fed one random point. instance variable t, 50 trajectories 30 steps used estimate gradient
Hessian policy. Regarding learning rate, adaptive one described Equation (8) used = 2. evaluation, 1,000 2,000 points used
integral estimate 2 3objective case, respectively. already discussed, given
results obtained LQG problem order show capability approximate algorithm, decided consider indicator (1 = 1 2 = 1).
main reasons efficiency (in Table 4 attained fastest convergence)
easy differentiability. Finally, recall results averaged ten trials.
Figure 7b reports initial final frontiers first two objectives
considered. Even starting far true Pareto frontier, PMGA able approach
it, increasing covering accuracy approximate frontier. Also, shown Figure 7a, despite low number exploited samples, algorithm presents almost
monotonic trend learning process, converges iterations.
215

fiParisi, Pirotta, & Restelli

J2 (Water Demand)

9.5

10

10.5

SDP
PFA
RA
MOFQI
REPS
SMS-EMOA
PMGA
2.6

2.4

2.2

2

1.8

1.6

J1 (Flooding)

1.4

1.2

1

0.8

Figure 8: Visual comparison 2objective water reservoir. PMGA frontier comparable ones obtained state-of-the-art algorithms terms accuracy covering.
However, continuous one, others scattered.
Table 7: Numerical algorithm comparison 2objective water reservoir. SDP
reference frontier hypervolume 0.0721 nine solutions.
Algorithm

Hypervolume

Loss

#Rollouts

#Solutions

0.0620 0.0010

0.0772 0.0045

16, 250 1, 072



PFA

0.0601 0.0012

0.0861 0.0083

27, 761 4, 849

51.1 10.9

RA

0.0480 0.0005

0.1214 0.0043

59, 253 3, 542

16.1 2.9

-

0.1870 0.0090

10, 000

-

REPS

0.0540 0.0009

0.1181 0.0030

37, 525 2, 235

17.0 4.1

SMS-EMOA

0.0581 0.0022

0.0884 0.0019

149, 825 35, 460

14.2 2.4

PMGA

MOFQI

Figure 8 offers visual comparison Pareto points Tables 7 8 report
numerical evaluation, including hypervolume loss achieved algorithms
w.r.t. SDP approximation9 . PMGA attains best performance 2 3
objective cases, followed PFA. SMS-EMOA returns good approximation,
slowest, requiring ten times amount samples used PMGA. MOFQI
outperforms PMGA sample complexity, loss highest. Finally, Figure 9
shows hypervolume trend PMGA comparison sample complexity
2objective case. PMGA substantially sample efficient algorithms,
attaining larger hypervolume much fewer rollouts. example, capable
generating frontier hypervolume RA one tenth rollouts,
outperforms PFA half samples needed latter.
9. Results regarding MOFQI include loss number rollouts hypervolume
number solutions available original paper.

216

fiMORL Continuous Pareto Manifold Approximation

0.065

PMGA

PFA (27,761)
SMS-EMOA (149,825)

Hypervolume

0.06

REPS (37,525)

0.055

RA (59,253)

0.05
0.045
0.04
2,000

4,000

6,000

8,000

10,000

12,000

14,000

16,000

#Rollouts
Figure 9: Comparison sample complexity 2objective case using hypervolume
evaluation score. brackets number rollouts needed algorithm produce
best frontier. PMGA clearly outperforms competing algorithms, requires
much fewer samples generate frontiers better hypervolume.
Table 8: Numerical algorithm comparison 3objective water reservoir. SDP
reference frontier hypervolume 0.7192 25 solutions.
Algorithm

Hypervolume

Loss

#Rollouts

#Solutions

0.6701 0.0036

0.0116 0.0022

62, 640 7, 963



PFA

0.6521 0.0029

0.0210 0.0012

343, 742 12, 749

595 32.3

RA

0.6510 0.0047

0.0207 0.0016

626, 441 35, 852

137.3 25.4

-

0.0540 0.0061

20, 000

-

REPS

0.6139 0.0003

0.0235 0.0014

187, 565 8, 642

86 9.7

SMS-EMOA

0.6534 0.0007

0.0235 0.0020

507, 211 56, 823

355.6 13.9

PMGA

MOFQI

7. Metrics Tuning
section want examine deeply tuning mixed metric parameters,
order provide reader better insights correct use metrics. performance PMGA strongly depends indicator used and, thereby, configuration
critical. precise, mixed metrics, obtained best approximate Pareto
frontiers experiments conducted Section 6, include trade-off accuracy
covering, expressed parameters. following, analyze fundamental
concepts behind metrics study performance influenced changes
parameters.
217

fiParisi, Pirotta, & Restelli

Approximate frontier
1,000

True Pareto frontier

300

300

250

250

200

200

500

150
500

(a) = 1

1,000

150
150

200

250

300

(b) = 1.5

150

200

250

300

(c) = 2

Figure 10: Approximate frontiers 2objective LQG learned PMGA using I,PN
varying . Figure (a) indicator penalize enough dominated solutions,
Figure (c) frontier wide enough. contrary, Figure (b)
algorithm achieves accuracy covering.

7.1 Tuning
first indicator (to maximized) analyze
= IAU w,
w penalization term. previous sections proposed w = 1 IPN
w = 1 IU , order take advantage expansive behavior antiutopiabased
indicator accuracy optimalitybased indicator. section study
performance mixed metric changing , proposing simple tuning process.
idea set initial value increase (or decrease) approximate
frontier contains dominated solutions (or wide enough). Figure 10 shows different
approximate frontiers obtained different values exact 2objective LQG
50 iterations using w = 1 IPN . Starting = 1 indicator behaves mostly
IAU , meaning small (Figure 10a). Increasing 2 (Figure 10c)
algorithm converges, approximate frontier completely cover true one,
i.e., IPN mostly condition behavior metric. Finally, = 1.5 (Figure 10b)
approximate frontier perfectly matches true one metric correctly mixes two
single indicators.
However, already discussed Section 6, use w = 1 IPN problematic
difference magnitude IAU IPN make tuning hard
point metric becomes ineffective. drawback solved using w = 1 IU
normalizing reference point indicators (i.e., IU IAU ) I(J, p) = kJ/p 1k22 ,
normalization bounds utopia antiutopiabased metrics similar intervals,
i.e., (0, ) [0, ), respectively.10
10. ratio two vectors a/b component-wise operation.

218

fiMORL Continuous Pareto Manifold Approximation

J2

J2

U

1

10

J1

AU

1
(a)

J2

U

J1

AU

1
(b)

U

1

J1

AU

1
(c)

Figure 11: Examples Pareto frontiers. Figures (a) (b) frontiers convex,
latter objectives normalized. Figure (c) frontier concave.

7.2 Tuning
second mixed indicator (to maximized) takes advantage expansive behavior antiutopiabased indicator accuracy utopiabased one.
defined
IAU
= 1
2 ,
IU
1 2 free parameters.
better understand insights guided metric definition, consider
different scenarios according shape Pareto frontier. Figure 11a frontier
convex normalized objectives. case point closer
antiutopia utopia is, sure, dominated solution. ratio IAU /IU
point frontier always greater 1 hence reasonable set 1
2 1. Therefore, need know exactly antiutopia point
drawback antiutopiabased metric IAU disappears, since take account
distance utopia point. Nevertheless, setting points critical,
magnitude strongly affect PMGA performance. example shown Figure 11b,
frontier normalized objectives different magnitude.
case, setting 1 2 1, indicator evaluated extrema frontier
(J1 = [1, 0]T J2 = [0, 10]T ) equal 0.99 99, respectively. first value
negative, approximate frontier includes points true Pareto frontier,
J1 would perform better true Pareto frontier.
contrary, frontier concave (Figure 11c) true point
closer antiutopia utopia dominated solution, ratio IAU /IU
point frontier (with exception, eventually, ends) always
smaller one. Keeping 1 = 1 2 = 1, PMGA would try collapse frontier
single point, order maximize indicator. Therefore, parameters need
changed accordingly trial-and-error. instance, returned frontier
achieve accuracy, possible solution decrease 1 increase 2 .
219

fiParisi, Pirotta, & Restelli

8. Conclusion
paper proposed novel gradientbased approach, namely ParetoManifold
Gradient Algorithm (PMGA), learn continuous approximation Pareto frontier
MOMDPs. idea define parametric function describes manifold
policy parameters space, maps manifold objectives space. Given metric
measuring quality manifold objectives space (i.e., candidate frontier),
shown compute (and estimate trajectory samples) gradient w.r.t.
parameters . Updating parameters along gradient direction generates new
policy manifold associated improved (w.r.t. chosen metric) continuous frontier
objectives space. Although provided derivation independent
parametric function metric used measure quality candidate solutions,
terms strongly influence final result. Regarding former, achieved
high quality results forcing parameterization pass singleobjective
optima. However, trick might require domain expertise additional samples
therefore could always applicable. Regarding latter, presented different
alternative metrics, examined pros cons one, shown properties
empirical analysis discussed general tuning process promising ones.
evaluation included sample complexity analysis investigate performance
PMGA, comparison state-of-the-art algorithms MORL. results,
approach outperforms competing algorithms quality frontier sample
complexity. would interesting study properties theoretical perspective
order provide support empirical evidence. leave open problems
investigation convergence rate approximation error true Pareto
frontier. However, think hard provide analysis general setting.
Future research address study metrics parametric functions
produce good results general case. particular, investigate problems
many objectives (i.e., three) highdimensional policies. Since complexity manifold parameterization grows number objectives policy
parameters, polynomial parameterization could effective complex problems alternative parameterizations found. Another interesting direction
research concerns importance sampling techniques reducing sample complexity
gradient estimate. Since frontier composed continuum policies, likely
trajectory generated specific policy partially used estimation
quantities related similar policies, thus decreasing number samples needed
MonteCarlo estimate integral. Moreover, would interesting investigate automatic techniques tuning metric parameters applicability
PMGA multi-agent scenario (e.g., Roijers, Whiteson, & Oliehoek, 2015).

220

fiMORL Continuous Pareto Manifold Approximation

Appendix A. Optimal Baseline
Theorem A.1 (Componentdependent baseline). optimal baseline (i, j)-component
(i,j)
Hessian estimate HRF, JD () given Equation (6)

(i,j)
bH,



2
(i,j)
G ( )



E R( )

=
2
(i,j)
E G ( )

,


(i,j)

G

(i,j)

( ) = ln p ( |) j ln p ( |) + H

ln p ( |) .

Given baseline b, variance reduction obtained optimal baseline bH,
Var (HRF, JD (, b)) Var (HRF, J (, bH, )) =


(i,j) 2

(i,j)
2
b
bH,
(i,j)
E
G ( )
.

N
(i,j)

Proof. Let G

( ) (i, j)-th component G ( )
(i,j)

G

(i,j)

( ) = ln p ( |) j ln p ( |) + H

ln p ( |) .

(i,j)

variance HRF, JD () given by11
Var



(i,j)
HRF, JD



i2
2
2 h

(i,j)
(i,j)
(i,j)
G ( )
E R( ) b(i,j) G ( )
() = E R( ) b





2

2
(i,j)
(i,j)
2
(i,j) 2
+E b
G ( )
= E R( ) G ( )





2
h
i2
(i,j)
(i,j)
2b(i,j) E R( ) G ( )
E R( )G ( )
.




Minimizing previous equation w.r.t. b(i,j) get

(i,j)

bH,



2
(i,j)
E R( ) G ( )

=
2 .
(i,j)
E G ( )

11. use compact notation E [] denote E [].

221

fiParisi, Pirotta, & Restelli

excess variance given




(i,j)
(i,j)
(i,j)
Var G ( )(R( ) b(i,j) ) Var G ( )(R( ) bH, )




2
2
2

2
(i,j)
(i,j)
(i,j)
(i,j)
(i,j)
2
2b
E R( ) G ( )
+E b
G ( )
= E R( ) G ( )





h

2

2
i2
(i,j)
(i,j) 2
(i,j)
(i,j)
2
E bH,
E R( )G ( )
E R( ) G ( )
G ( )





2 h
i2
(i,j)
(i,j)
(i,j)
+ E R( )G ( )
+ 2bH, E R( ) G ( )




2
2

2
(i,j)
(i,j)
(i,j)
(i,j)
= b
E G ( )
2b
E R( ) G ( )




2

2
2
(i,j)
(i,j)
(i,j)
(i,j)
bH, E G ( )
+ 2bH, E R( ) G ( )






2
2

2

(i,j)
(i,j)
(i,j)
2
(i,j)
E G ( )
2b
E R( ) G ( )
= b








2 2



(i,j)

2
E R( ) G ( )

(i,j)




G ( )
2 E

(i,j)
E G ( )


2
(i,j)


2

E R( ) G ( )
(i,j)
2




R( ) G ( )
+ 2

2
E

(i,j)
E G ( )

2

2
2

(i,j)
(i,j)
(i,j)
(i,j)
2b
E R( ) G ( )
E G ( )
= b






2 2
(i,j)
E R( ) G ( )

+
2
(i,j)
E G ( )




2
(i,j)
G ( )



E R( )


(i,j) 2
(i,j)

= b
2b
2

(i,j)
E G ( )
E







(i,j)

= b

(i,j)

G


( )

2


(i,j) 2
bH, E




2
(i,j)
G ( )


.

222



2 2
(i,j)

E R( ) G ( )



+




2

(i,j)
E G ( )


fiMORL Continuous Pareto Manifold Approximation

References
Ahmadzadeh, S., Kormushev, P., & Caldwell, D. (2014). Multi-objective reinforcement
learning auv thruster failure recovery. Adaptive Dynamic Programming
Reinforcement Learning (ADPRL), 2014 IEEE Symposium on, pp. 18.
Athan, T. W., & Papalambros, P. Y. (1996). note weighted criteria methods compromise solutions multi-objective optimization. Engineering Optimization, 27 (2),
155176.
Barrett, L., & Narayanan, S. (2008). Learning optimal policies multiple criteria.
Proceedings 25th International Conference Machine Learning, ICML 08,
pp. 4147, New York, NY, USA. ACM.
Bertsekas, D. P. (2005). Dynamic programming suboptimal control: survey
ADP MPC*. European Journal Control, 11 (4-5), 310 334.
Beume, N., Naujoks, B., & Emmerich, M. (2007). Sms-emoa: Multiobjective selection based
dominated hypervolume. European Journal Operational Research, 181 (3), 1653
1669.
Brown, M., & Smith, R. E. (2005). Directed multi-objective optimization. International
Journal Computers, Systems, Signals, 6 (1), 317.
Calandra, R., Peters, J., & Deisenrothy, M. (2014). Pareto front modeling sensitivity
analysis multi-objective bayesian optimization. NIPS Workshop Bayesian
Optimization, Vol. 5.
Castelletti, A., Corani, G., Rizzolli, A., Soncinie-Sessa, R., & Weber, E. (2002). Reinforcement learning operational management water system. IFAC Workshop
Modeling Control Environmental Issues, Keio University, Yokohama, Japan,
pp. 325330.
Castelletti, A., Pianosi, F., & Restelli, M. (2012). Tree-based fitted q-iteration multiobjective markov decision problems. Neural Networks (IJCNN), 2012 International Joint Conference on, pp. 18.
Castelletti, A., Pianosi, F., & Restelli, M. (2013). multiobjective reinforcement learning
approach water resources systems operation: Pareto frontier approximation
single run. Water Resources Research, 49 (6), 34763486.
Crites, R. H., & Barto, A. G. (1998). Elevator group control using multiple reinforcement
learning agents. Machine Learning, 33 (2-3), 235262.
Das, I., & Dennis, J. (1997). closer look drawbacks minimizing weighted sums
objectives pareto set generation multicriteria optimization problems. Structural
optimization, 14 (1), 6369.
Das, I., & Dennis, J. E. (1998). Normal-boundary intersection: new method generating
pareto surface nonlinear multicriteria optimization problems. SIAM Journal
Optimization, 8 (3), 631657.
Deisenroth, M. P., Neumann, G., & Peters, J. (2013). survey policy search robotics.
Foundations Trends Robotics, 2 (1-2), 1142.
223

fiParisi, Pirotta, & Restelli

Fonteneau, R., & Prashanth, L. A. (2014). Simultaneous perturbation algorithms batch
off-policy search. 53rd IEEE Conference Decision Control, CDC 2014, Los
Angeles, CA, USA, December 15-17, 2014, pp. 26222627. IEEE.
Friedrich, T., Horoba, C., & Neumann, F. (2009). Multiplicative approximations
hypervolume indicator. Proceedings 11th Annual Conference Genetic
Evolutionary Computation, GECCO 09, pp. 571578, New York, NY, USA. ACM.
Furmston, T., & Barber, D. (2012). unifying perspective parametric policy search
methods markov decision processes. Pereira, F., Burges, C., Bottou, L., &
Weinberger, K. (Eds.), Advances Neural Information Processing Systems 25, pp.
27172725. Curran Associates, Inc.
Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning.
Shavlik, J. W. (Ed.), Proceedings Fifteenth International Conference
Machine Learning (ICML 1998), Madison, Wisconsin, USA, July 24-27, 1998, pp.
197205. Morgan Kaufmann.
Greensmith, E., Bartlett, P. L., & Baxter, J. (2004). Variance reduction techniques
gradient estimates reinforcement learning. Journal Machine Learning Research,
5, 14711530.
Harada, K., Sakuma, J., & Kobayashi, S. (2006). Local search multiobjective function
optimization: Pareto descent method. Proceedings 8th Annual Conference
Genetic Evolutionary Computation, GECCO 06, pp. 659666, New York, NY,
USA. ACM.
Harada, K., Sakuma, J., Kobayashi, S., & Ono, I. (2007). Uniform sampling local paretooptimal solution curves pareto path following applications multi-objective
GA. Lipson, H. (Ed.), Genetic Evolutionary Computation Conference, GECCO
2007, Proceedings, London, England, UK, July 7-11, 2007, pp. 813820. ACM.
Kakade, S. (2001). Optimizing average reward using discounted rewards. Helmbold, D. P.,
& Williamson, R. C. (Eds.), Computational Learning Theory, 14th Annual Conference
Computational Learning Theory, COLT 2001 5th European Conference
Computational Learning Theory, EuroCOLT 2001, Amsterdam, Netherlands, July
16-19, 2001, Proceedings, Vol. 2111 Lecture Notes Computer Science, pp. 605
615. Springer.
Koski, J., & Silvennoinen, R. (1987). Norm methods partial weighting multicriterion optimization structures. International Journal Numerical Methods
Engineering, 24 (6), 11011121.
Lizotte, D. J., Bowling, M., & Murphy, S. A. (2012). Linear fitted-q iteration multiple
reward functions. Journal Machine Learning Research, 13, 32533295.
Lizotte, D. J., Bowling, M. H., & Murphy, S. A. (2010). Efficient reinforcement learning
multiple reward functions randomized controlled trial analysis. Furnkranz, J.,
& Joachims, T. (Eds.), Proceedings 27th International Conference Machine
Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 695702. Omnipress.
224

fiMORL Continuous Pareto Manifold Approximation

Magnus, J. R., & Neudecker, H. (1999). Matrix Differential Calculus Applications
Statistics Econometrics. Wiley Ser. Probab. Statist.: Texts References
Section. Wiley.
Mannor, S., & Shimkin, N. (2002). steering approach multi-criteria reinforcement
learning. Dietterich, T., Becker, S., & Ghahramani, Z. (Eds.), Advances Neural
Information Processing Systems 14, pp. 15631570. MIT Press.
Mannor, S., & Shimkin, N. (2004). geometric approach multi-criterion reinforcement
learning. J. Mach. Learn. Res., 5, 325360.
Messac, A., & Ismail-Yahaya, A. (2002). Multiobjective robust design using physical programming. Structural Multidisciplinary Optimization, 23 (5), 357371.
Messac, A., Ismail-Yahaya, A., & Mattson, C. A. (2003). normalized normal constraint method generating pareto frontier. Structural multidisciplinary
optimization, 25 (2), 8698.
Munkres, J. R. (1997). Analysis Manifolds. Adv. Books Classics Series. Westview Press.
Natarajan, S., & Tadepalli, P. (2005). Dynamic preferences multi-criteria reinforcement
learning. Raedt, L. D., & Wrobel, S. (Eds.), Machine Learning, Proceedings
Twenty-Second International Conference (ICML 2005), Bonn, Germany, August
7-11, 2005, Vol. 119 ACM International Conference Proceeding Series, pp. 601608.
ACM.
Nojima, Y., Kojima, F., & Kubota, N. (2003). Local episode-based learning multiobjective behavior coordination mobile robot dynamic environments. Fuzzy
Systems, 2003. FUZZ 03. 12th IEEE International Conference on, Vol. 1, pp.
307312 vol.1.
Okabe, T., Jin, Y., & Sendhoff, B. (2003). critical survey performance indices
multi-objective optimisation. Evolutionary Computation, 2003. CEC 03. 2003
Congress on, Vol. 2, pp. 878885 Vol.2.
Parisi, S., Pirotta, M., Smacchia, N., Bascetta, L., & Restelli, M. (2014). Policy gradient
approaches multi-objective sequential decision making. 2014 International Joint
Conference Neural Networks, IJCNN 2014, Beijing, China, July 6-11, 2014, pp.
23232330. IEEE.
Perny, P., & Weng, P. (2010). finding compromise solutions multiobjective markov
decision processes. Coelho, H., Studer, R., & Wooldridge, M. (Eds.), ECAI 2010 19th European Conference Artificial Intelligence, Lisbon, Portugal, August 16-20,
2010, Proceedings, Vol. 215 Frontiers Artificial Intelligence Applications, pp.
969970. IOS Press.
Peters, J. (2007). Machine Learning Motor Skills Robotics. Ph.D. thesis, University
Southern California.
Peters, J., Mulling, K., & Altun, Y. (2010). Relative entropy policy search. Fox, M.,
& Poole, D. (Eds.), Proceedings Twenty-Fourth AAAI Conference Artificial
Intelligence (AAAI 2010), pp. 16071612. AAAI Press.
225

fiParisi, Pirotta, & Restelli

Peters, J., & Schaal, S. (2008a). Natural actor-critic. Neurocomputing, 71 (7-9), 1180 1190.
Progress Modeling, Theory, Application Computational Intelligenc 15th
European Symposium Artificial Neural Networks 2007 15th European Symposium
Artificial Neural Networks 2007.
Peters, J., & Schaal, S. (2008b). Reinforcement learning motor skills policy gradients.
Neural Networks, 21 (4), 682 697. Robotics Neuroscience.
Pianosi, F., Castelletti, A., & Restelli, M. (2013). Tree-based fitted q-iteration multiobjective markov decision processes water resource management. Journal Hydroinformatics, 15 (2), 258270.
Pirotta, M., Parisi, S., & Restelli, M. (2015). Multi-objective reinforcement learning
continuous pareto frontier approximation. Bonet, B., & Koenig, S. (Eds.), Proceedings Twenty-Ninth AAAI Conference Artificial Intelligence, January 25-30,
2015, Austin, Texas, USA., pp. 29282934. AAAI Press.
Pirotta, M., Restelli, M., & Bascetta, L. (2013). Adaptive step-size policy gradient
methods. Burges, C. J. C., Bottou, L., Ghahramani, Z., & Weinberger, K. Q. (Eds.),
Advances Neural Information Processing Systems 26: 27th Annual Conference
Neural Information Processing Systems 2013. Proceedings meeting held December
5-8, 2013, Lake Tahoe, Nevada, United States., pp. 13941402.
Robert, C., & Casella, G. (2004). Monte Carlo Statistical Methods. Springer Texts
Statistics. Springer-Verlag New York.
Roijers, D. M., Vamplew, P., Whiteson, S., & Dazeley, R. (2013). survey multi-objective
sequential decision-making. Journal Artificial Intelligence Research, 48, 67113.
Roijers, D. M., Whiteson, S., & Oliehoek, F. A. (2015). Computing convex coverage sets
faster multi-objective coordination. Journal Artificial Intelligence Research, 52,
399443.
Romero, C. (2001). Extended lexicographic goal programming: unifying approach. Omega,
29 (1), 6371.
Shelton, C. R. (2001). Importance Sampling Reinforcement Learning Multiple
Objectives. Ph.D. thesis, Massachusetts Institute Technology.
Steuer, R. E., & Choo, E.-U. (1983). interactive weighted tchebycheff procedure
multiple objective programming. Mathematical Programming, 26 (3), 326344.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. Bradford
book. Bradford Book.
Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y. (2000). Policy gradient
methods reinforcement learning function approximation. Solla, S., Leen,
T., & Muller, K. (Eds.), Advances Neural Information Processing Systems 12, pp.
10571063. MIT Press.
Tesauro, G., Das, R., Chan, H., Kephart, J., Levine, D., Rawson, F., & Lefurgy, C. (2008).
Managing power consumption performance computing systems using reinforcement learning. Platt, J., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances
Neural Information Processing Systems 20, pp. 14971504. Curran Associates, Inc.
226

fiMORL Continuous Pareto Manifold Approximation

Vamplew, P., Dazeley, R., Berry, A., Issabekov, R., & Dekker, E. (2011). Empirical evaluation methods multiobjective reinforcement learning algorithms. Machine Learning,
84 (1-2), 5180.
Van Moffaert, K., Drugan, M. M., & Nowe, A. (2013). Scalarized multi-objective reinforcement learning: Novel design techniques. Adaptive Dynamic Programming
Reinforcement Learning (ADPRL), 2013 IEEE Symposium on, pp. 191199.
Van Moffaert, K., & Nowe, A. (2014). Multi-objective reinforcement learning using sets
pareto dominating policies. Journal Machine Learning Research, 15, 34833512.
Waltz, F. M. (1967). engineering approach: Hierarchical optimization criteria. Automatic
Control, IEEE Transactions on, 12 (2), 179180.
Wang, W., & Sebag, M. (2013). Hypervolume indicator dominance reward based multiobjective monte-carlo tree search. Machine Learning, 92 (2-3), 403429.
Williams, R. (1992). Simple statistical gradient-following algorithms connectionist reinforcement learning. Machine Learning, 8 (3-4), 229256.
Yu, P., & Leitmann, G. (1974). Compromise solutions, domination structures, salukvadzes solution. Journal Optimization Theory Applications, 13 (3), 362378.
Zitzler, E., Thiele, L., & Bader, J. (2010). set-based multiobjective optimization. Evolutionary Computation, IEEE Transactions on, 14 (1), 5879.
Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., & da Fonseca, V. G. (2003). Performance assessment multiobjective optimizers: analysis review. Evolutionary
Computation, IEEE Transactions on, 7 (2), 117132.

227


