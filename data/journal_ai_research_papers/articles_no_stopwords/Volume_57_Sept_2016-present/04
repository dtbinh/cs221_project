journal artificial intelligence

submitted published

multi objective reinforcement learning
continuous pareto manifold approximation
simone parisi

parisi ias tu darmstadt de

technische universitat darmstadt
hochschulstr darmstadt germany

matteo pirotta
marcello restelli

matteo pirotta polimi
marcello restelli polimi

politecnico di milano
piazza leonardo da vinci milano italy

abstract
many real world control applications economics robotics characterized
presence multiple conflicting objectives standard concept
optimality replaced paretooptimality goal pareto frontier
set solutions representing different compromises among objectives despite recent advances multiobjective optimization achieving accurate representation
pareto frontier still important challenge propose reinforcement
learning policy gradient learn continuous approximation pareto frontier multiobjective markov decision momdps differently previous
policy gradient n optimization routines executed n solutions
performs single gradient ascent run generating step improved
continuous approximation pareto frontier idea optimize parameters
function defining manifold policy parameters space corresponding
image objectives space gets close possible true pareto frontier besides
deriving compute estimate gradient discuss nontrivial
issue defining metric assess quality candidate pareto frontiers finally
properties proposed empirically evaluated two
linear quadratic gaussian regulator water reservoir control task

introduction
multiobjective sequential decision characterized presence multiple
conflicting objectives found many real world scenarios economic
systems shelton medical treatment lizotte bowling murphy control
water reservoirs castelletti pianosi restelli elevators crites barto
robots nojima kojima kubota ahmadzadeh kormushev caldwell
mention often modeled multiobjective markov decision
processes momdps concept optimality typical mdps replaced
one pareto optimality defines compromise among different objectives
last decades reinforcement learning rl sutton barto established
effective theoretically grounded framework allows solve singleobjective
mdps whenever little prior knowledge available system dynamics
dimensionality system controlled high classical optimal control
ai access foundation rights reserved

fiparisi pirotta restelli

methods multiobjective reinforcement learning morl instead concerns momdps
tries solve sequential decision two conflicting objectives
despite successful development rl theory high demand multiobjective
control applications morl still relatively young unexplored topic
morl approaches divided two categories number policies
learn vamplew dazeley berry issabekov dekker single multiple
policy although morl approaches belong former category present
multiplepolicy able learn set policies approximating pareto frontier
representation complete pareto frontier fact allows posteriori selection
solution encapsulates trade offs among objectives giving better insights
relationships among objectives among multiplepolicy possible
identify two classes valuebased lizotte et al castelletti et al van moffaert
nowe search optimal solutions value functions space policy gradient approaches shelton parisi pirotta smacchia bascetta restelli
search policy space practice different advantages value
methods usually stronger guarantees convergence preferred domains lowdimensional state action spaces prone suffer curse
dimensionality sutton barto hand policy gradient methods
favorable many domains robotics allow taskappropriate
prestructured policies integrated straightforwardly deisenroth neumann peters
experts knowledge incorporated ease selecting suitable policy
parametrization learning simplified stability well robustness
frequently ensured bertsekas nonetheless approaches lack guarantees uniform covering true pareto frontier quality approximate
frontier terms accuracy distance true frontier covering extent
related metric used measure discrepancy true pareto frontier
however nowadays definition metric open moo literature
overcome limitations proposing novel gradientbased morl
alternative quality measures approximate frontiers namely
paretomanifold gradient pmga exploiting continuous approximation
locally paretooptimal manifold policy space able generate arbitrarily
dense approximate frontier article extension preliminary work presented
pirotta parisi restelli main contributions derivation
gradient general case e independent metric used measure
quality current solution section estimate gradient samples
section discussion frontier quality measures effectively integrated
proposed section thorough empirical evaluation proposed
metrics performance multiobjective discrete time linear quadratic
gaussian regulator water reservoir management domain sections

preliminaries
section first briefly summarize terminology used discuss
state art approaches morl subsequently focus describing policy
gradient techniques introduce notation used remainder


fimorl continuous pareto manifold approximation

formulation
discretetime continuous markov decision process mdp mathematical framework
modeling decision making described tuple hs p r di rn
continuous state space rm continuous action space p markovian
transition model p defines transition density state
action r r reward function discount factor
distribution initial state drawn context behavior
agent defined policy e density distribution specifies probability
taking action state given initial state distribution possible define
expected return j associated policy


x


j
e
r st st
st p



r st st immediate reward obtained state st reached executing
action state st finite infinite time horizon goal agent
maximize return
multiobjective markov decision processes momdps extension mdps
several pairs reward functions discount factors defined one
objective formally momdp described tuple hs p r di r
r rq q qdimensional column vectors reward functions
ri r discount factors respectively
momdps policy




associated q expected returns j j jq


x
ji
e
ri st st
st p



unlike happens mdps momdps single policy dominating others
usually exist conflicting objectives considered policy simultaneously maximize reason multiobjective optimization moo
concept pareto dominance used policy strongly dominates policy denoted
superior objectives e


q ji ji
similarly policy weakly dominates policy denoted worse
objectives e




q ji ji q ji ji
policy policy paretooptimal speak
locally paretooptimal policies definition except
restrict dominance neighborhood general multiple
locally paretooptimal policies solving
momdp

equivalent determine set
maps socalled pareto
paretooptimal
policies



frontier f j
done harada sakuma kobayashi assume locally paretooptimal solutions
paretooptimal exist



fiparisi pirotta restelli

related work
multiobjective optimization moo field two common solution concepts
multiobjective singleobjective strategy pareto strategy former
derives scalar objective multiple objectives uses standard single
objective optimization soo techniques weighted sum athan papalambros
normbased yu leitmann koski silvennoinen sequential romero
constrained waltz physical programming messac ismail yahaya
min max methods steuer choo latter strategy concept
pareto dominance considers paretooptimal solutions non inferior solutions among
candidate solutions main exponent class convex hull method das
dennis messac ismail yahaya mattson
similar moo current morl approaches divided two categories
number policies learn vamplew et al singlepolicy methods aim
finding best policy satisfies preference among objectives majority
morl approaches belong category differ way preferences
expressed easy implement require priori decision type
solution suffer instability small changes preferences may
significant variations solution vamplew et al straightforward
common singlepolicy scalarization function applied
reward vector order produce scalar signal usually linear combination weighted
sum rewards performed weights used express preferences
multiple objective castelletti corani rizzolli soncinie sessa weber natarajan
tadepalli van moffaert drugan nowe less common use non
linear mappings tesauro das chan kephart levine rawson lefurgy
main advantage scalarization simplicity however linear scalarization presents
limitations able solutions lie concave linear region
pareto frontier athan papalambros uniform distribution weights may
produce accurate evenly distributed points pareto frontier das dennis
addition even frontier convex solutions cannot achieved
scalarization loss one objective may compensated increment
another one perny weng different singlepolicy approaches
thresholds lexicographic ordering gabor kalmar szepesvari different
kinds preferences objective space mannor shimkin
multiplepolicy approaches contrary aim learning multiple policies order
approximate pareto frontier building exact frontier generally impractical
real world thus goal build approximation frontier contains
solutions accurate evenly distributed along frontier range similar
pareto one zitzler thiele laumanns fonseca da fonseca many
reasons behind superiority multiplepolicy methods permit posteriori
selection solution encapsulate trade offs among multiple objectives
addition graphical representation frontier give better insights relationships among objectives useful understanding
choice solution however benefits come higher computational cost
prevent learning online scenarios common approximate


fimorl continuous pareto manifold approximation

pareto frontier perform multiple runs singlepolicy varying
preferences among objectives castelletti et al van moffaert et al
simple suffers disadvantages singlepolicy method used
besides examples multiplepolicy found literature
barrett narayanan proposed learns deterministic policies defining convex hull pareto frontier single learning process recent
works focused extension fitted q iteration multiobjective scenario
lizotte bowling murphy lizotte et al focused
linear approximation value function castelletti pianosi restelli able
learn control policy linear combinations preferences among objectives single learning process finally wang sebag proposed montecarlo
tree search able learn solutions lying concave region frontier
nevertheless classic approaches exploit deterministic policies
scattered pareto frontiers stochastic policies give continuous range compromises
among objectives roijers vamplew whiteson dazeley parisi et al shelton section pioneer use stochastic mixture policies
gradient ascent morl achieved two well known goals morl simultaneous
conditional objectives maximization former agent must maintain goals
time starts mixture policies obtained applying standard
rl techniques independent objective policy subsequently improved following
convex combination gradients policy space nonnegative w r
objectives objective gradient gi expected return w r policy
computed vector vi highest dot product gi simultaneously
satisfying nonnegativity condition returns used improving direction
th reward vectors vi combined convex form obtain direction
parameter improvement policy belongs pareto frontier
approximation pareto frontier obtained performing repeated searches
different weights reward gradients vi hand conditional optimization
consists maximizing objective maintaining certain level performance
others resulting gradient search reduced policy space
value constrained objectives greater desired performance
studies followed work shelton regard policy gradient
applied momdps recently parisi et al proposed two policy gradient
morl approaches starting initial policies perform gradient ascent
policy parameters space order determine set nondominated policies
first called radial given number p pareto solutions required
approximating pareto frontier p gradient ascent searches performed one
following different uniformly spaced direction within ascent simplex defined
convex combination singleobjective gradients second called pareto
following starts performing singleobjective optimization moves along
pareto frontier two step iterative process updating policy parameters following
gradient ascent direction applying correction procedure move
solution onto pareto frontier although methods exploit stochastic policies
proved effective several scenarios still return scattered solutions
guaranteed uniformly cover pareto frontier best knowledge nowadays


fiparisi pirotta restelli

morl returning continuous approximation pareto frontier
following sections present first able paretomanifold
gradient pmga
policy parametrization policygradient approaches
singleobjective
mdps

policygradient approaches consider parameterized policies
rd compact notation policy
parameters space given policy parametrization assume policy performance
j f rq least class c f called objectives space j defined
expected reward space possible trajectories
z
p r
j


trajectory drawn density distribution p reward vector
r
accumulated expected discounted reward trajectory e
prepresents

ri st st examples parametrized policies used context
ri tt
guassian policies gibbs policies momdps q gradient directions defined
policy parameter peters schaal b e
z


ji
p ri e ln p ri





x
b ji
e ri
ln st



direction ji associated particular discount factorreward function
b ji sample estimate shown equation
pair ri
differentiability expected return connected differentiability policy
ln p



x

ln st



remark notation following use symbol dx f denote
derivative generic function f rmn rpq w r matrix x notice
following relationship holds scalar functions vector variable x f dx f finally
symbol ix used denote x x identity matrix

gradient ascent policy manifold continuous pareto frontier
approximation
section first provide general definition optimization want
solve explain solve momdp scenario gradient
novel contributes section summarized lemma
notable exception moo calandra peters deisenrothy gaussian
processes used obtain continuous approximation pareto frontier
function class c continuous twice differentiable derivatives continuous
derivative operator well defined matrices vectors scalar functions refer work
magnus neudecker details



fimorl continuous pareto manifold approximation

objective function gradient described particular provide solution
evaluating performance continuous approximation pareto
frontier w r indicator function non trivial morl
direct access pareto frontier manipulate policy
parameters provide step step derivation leveraging manifold
theory matrix calculus
continuous pareto frontier approximation multiobjective
optimization
shown locally paretooptimal solutions locally forms q dimensional
manifold assuming q harada sakuma kobayashi ono follows
objective paretooptimal solutions described curves policy
parameters objective spaces idea behind work parametrize locally
paretooptimal solution curve objectives space order produce continuous
representation pareto frontier
let generative space open set rb b q analogous high
dimensional function parameterized curve smooth map rq class
c l l p rk free variables parameters
respectively set f together map constitute parametrized
manifold dimension b denoted f munkres manifold represents
approximation pareto frontier goal best approximation e
parameters minimize distance real frontier
arg max f



p

rq r indicator function measuring quality f w r
true pareto frontier notice equation interpreted special projection
operator refer figure graphical representation however since requires
knowledge true pareto frontier different indicator function needed
definition metric open literature recently several metrics
defined candidate presents intrinsic flaws prevent definition
unique superior metric vamplew et al furthermore see
remainder section proposed needs metric differentiable w r
policy parameters investigate topic section
general moo compute value frontier sum value
points composing discrete approximation scenario continuous
approximate frontier available maps integration pareto manifold
z
idv

l
f

l manifold value dv denotes integral w r manifold
f r indicator function measuring pareto optimality point
f assuming continuous integral given munkres
z
z
l
idv
v ol dt dt
f





fiparisi pirotta restelli



b

figure transformation maps generic moo setting figure morl figure b moo possible consider parametrized solutions figure b morl necessary mapping known
closed form determined discounted sum rewards

provided integral exists v ol x det x x standard way maximize
previous equation performing gradient ascent updating parameters according
gradient manifold value w r parameters e l
continuous pareto frontier approximation multiobjective
reinforcement learning
standard multiobjective optimization function free designed
morl must satisfy conditions first thing notice direct map
parameters space objective space unknown easily
defined reparameterization involving policy space shown figure b
previous section mentioned tight relationship local
manifold objective space local manifold policy parameters space
mapping well known defined performance function j defining
utility policy means given set policy parameterizations define
associated points objective space consequence optimization
reformulated search best approximation pareto manifold
policy parameter space e search manifold policy parameter space
best describes optimal pareto frontier
formally let smooth map class c l l defined
domain think map parameterization subset
choice point gives rise point means
subset space spanned map e bdimensional
parametrized manifold policy parameters space e

consequence associated parameterized pareto frontier bdimensional
open set defined
f j


fimorl continuous pareto manifold approximation

gradient ascent manifold space
point introduced notation needed derive gradient l
lemma pirotta et al let open set rb let f manifold
parametrized smooth map expressed composition maps j e
j rq given continuous function defined point f
integral w r given
z
z
l
idv
j v ol j dt dt
f



provided integral exists associated gradient w r parameters given
z

l

j v ol dt





z





j v ol vec

nb ib di tdt


j dt kronecker product nb ib kbb symmetric
b b idempotent matrix rank b b kbb permutation matrix magnus
neudecker finally


di dt iq j di ib j di dt
proof equation manifold value l follows directly definition
integral manifold munkres definition function composition
following provide detailed derivation th component gradient
let j dt
z
l


j v ol dt



z
det tt


j
dt
v ol


indicator derivative determinant derivative respectively expanded

j dj jt j di



det tt
det tt vec tt





vec

vec



z

z
z z


b

b qb

qb



det tt
vec
tt
vec






det
vec






nb ib tt



fiparisi pirotta restelli

kronecker product nb ib kbb symmetric b b idempotent
matrix rank b b kbb permutation matrix magnus neudecker

last term expanded di vec
start basic property
differential e
j dt j dt j dt
applying vector operator
dvec j dt vec j dt vec j dt


dt iq dvec j ib j dvec dt
z

z

z

z


dq

bqdq

bqbd

bd

finally derivative given

vec j
vec dt


di dt iq
ib j





z
zi

z

dqd





bd





dt iq j di ib j di dt

interesting notice gradient manifold value l requires
compute second derivatives policy performance j however j
vec j
denote hessian matrix transformation

n
h
ji




dn
ji



n



ji




dp n j

p q q number objectives number rows jacobian
matrix recall hessian
matrixis defined derivative transpose
jacobian e h j j
little done second order methods particular
hessian formulations first analysis performed kakade provided
formulation policy gradient theorem sutton mcallester singh mansour
recently extended comparison newton method em
natural gradient presented furmston barber sake clarity
report hessian formulation provided furmston barber notation
introduce optimal baseline terms variance reduction formulation
lemma momdp hessian h j expected discounted reward j
w r policy parameters qd matrix obtained stacking hessian
notable exceptions natural gradient approaches although explicitly require
compute second order derivatives usually considered second order methods



fimorl continuous pareto manifold approximation

component
h j


vec




ji






h j






h jq


z
h ji


p ri bi ln p ln p h ln p






ln p



x

ln st

h ln p





x

h ln st


n

optimal baseline hessian estimate h
ji provided equation
computed done greensmith bartlett baxter order reduce
variance gradient estimate given component wise



n
e p ri g

n

bi


n
e p g

n

n

n
g

ln p ln p h
appendix

ln p derivation refer

manifold gradient estimation sample trajectories
morl prior knowledge reward function state transition
model need estimate gradient l trajectory samples section
aims provide guide estimation manifold gradient particular review
related estimation standard rl components expected discounted return
gradient provide finite sample analysis hessian estimate
formulation gradient l provided lemma composed terms
related parameterization manifold policy space terms related
mdp since map free designed associated terms e g dt
computed exactly hand terms related mdp j j
h j need estimated estimate expected discounted reward
associated gradient old topic rl literature several
proposed kakade pirotta restelli bascetta literature lacks explicit
analysis hessian estimate recently simultaneous perturbation stochastic approximation technique exploited estimate hessian fonteneau prashanth
however rely formulation provided furmston barber
hessian estimated trajectory samples obtained current policy removing
necessity generating policy perturbations


fiparisi pirotta restelli

paretomanifold gradient
define policy parametric function indicator learning rate
initialize parameters
repeat terminal condition reached
collect n n trajectories
sample free variable n generative space

sample policy parameters n n
n

n n n
execute trajectory collect data st rt


b ji according equation
compute gradients
b ji according equation
compute hessians h
compute manifold value derivative l according equation
update parameters l
since p unknown expectation approximated empirical average
assuming access n trajectories hessian estimate

n


x
x

b ji
h
rnt b
n
n







x
x
x

ln ant snt
ln ant snt

h ln ant snt






n
n n n
st rt







denotes n th trajectory formulation resembles def

inition reinforce estimate given williams gradient j
estimates known likelihood ratio methods overcome determining perturbation parameters occurring finite difference methods describes
complete pmga procedure
order simplify theoretical analysis hessian estimate make following assumptions
assumption uniform boundedness reward function log jacobian
log hessian policy uniformly bounded q n










n


ln g
firi ri
fid ln
fih
lemma given parametrized policy assumption th component log hessian expected return bounded
kh ji kmax


ri

td g


max norm matrix defined kakmax maxi j aij


fimorl continuous pareto manifold approximation

proof consider definition hessian equation assumption
hessian components bounded n







fiz
x
x


n

ln st
ln aj sj
ji p ri
fih


n

j





ln st

n









x
x
x
ri


ri
l
g
td g

l



j

previous used derive bound sample complexity
hessian estimate
theorem given parametrized policy assumption
following number step trajectories



ri

n
td g
ln


b ji generated equation probability
gradient estimate h


b


h ji h ji
max

proof hoeffdings inequality implies n
n



pn
b n

n
bi ai


p fih
ji h
ji e

solving equation n noticing lemma provides bound sample
obtain



ri

n
td g
ln



integral estimate computed standard montecarlo techniques several
statistical bounds proposed literature refer robert casella
survey montecarlo methods
point reader may expect analysis convergence
convergence rate optimal parametrization although consider analysis theoretically challenging interesting provide related topic
analysis hard even impossible provide general settings since objective
function nonlinear nonconcave moreover analysis simplified scenario
possible almost useless real applications


fiparisi pirotta restelli

metrics multiobjective optimization
section review indicator functions proposed literature underlining advantages drawbacks propose alternatives recently moo focused
use indicators turn multiobjective optimization singleobjective
one optimizing indicator indicator function used assign every
point given frontier scalar measure gives rough idea discrepancy candidate frontier pareto one since instead optimizing objective
functions directly indicatorbased aim finding solution set maximizes
indicator metric natural question arises correctness change
optimization procedure properties indicator functions enjoy instance
hypervolume indicator weighted version among widespread metrics
literature metrics gained popularity refinements
pareto dominance relation zitzler thiele bader recently several works
proposed order theoretically investigate properties hypervolume indicator e g friedrich horoba neumann nevertheless argued
hypervolume indicator may introduce bias search furthermore another important
issue dealing hypervolume indicator choice reference point
perspective main issues metric high computational complexity
computation hypervolume indicator phard see friedrich et al
non differentiability several metrics defined field
moo refer work okabe jin sendhoff survey however
moo literature able provide superior metric among candidates
one suited scenario main issues non differentiability
capability evaluating discrete representations pareto frontier intrinsic
nature metrics example generational distance another widespread measure
minimum distance reference frontier available settings
overcome issues mixed different indicator concepts novel differentiable
metrics insights guided metrics definition related moo
desiderata recall goal moo compute approximation frontier
including solutions accurate evenly distributed covering range similar
actual one zitzler et al note uniformity frontier intrinsically guaranteed continuity approximation introduced concepts
mind need induce accuracy extension indicator function
stressed clear definition want indicator
maximized real pareto frontier must ensure indicator function
induces partial ordering frontiers manifold f solutions weakly dominated
manifold f ones f manifold value must better f one
definition consistent indicator function let f set q dimensional
manifolds associated momdp q objectives let k manifold
policy parameters
space mapping fk f f true pareto frontier let
r
li f f idv manifold value indicator function consistent
fk fh li fh li fk fh f



h k k j h j li fh li fk


fimorl continuous pareto manifold approximation

accuracy metrics
given reference point p simple indicator obtained computing distance
every point frontier f reference point e
kj pk
mentioned hypervolume indicator choice reference point may
critical however natural choice utopia ideal point pu e point
optimizes objectives case goal minimization indicator
function denoted iu utopia indicator since dominated policy farther
utopia least one paretooptimal solution accuracy easily guaranteed
hand since minimized measure forces solution collapse
single point thus consistent note mitigated
solved forcing transformation pass singleobjective
optima although trick helpful discuss section requires
singleobjective optimal policies order constrain parameters however
information required properly set utopia
concerning accuracy frontier theoretical perspective possible
define another metric definition pareto optimality point paretooptimal
brown smith
l

q
x

ji

q
x









possible identify ascent direction simultaneously improves
objectives consequence paretoascent direction l point pareto
frontier null formally metric respects paretooptimality defined
follows
q
x
minq kl k

r



denote indicator ipn pareto norm indicator utopiabased metric
extent frontier taken account without constraint optimal
solution collapses single point frontier
covering metrics
extension frontier primary concern maximizing distance
antiutopia pau metric grows frontier dimension however
contrary utopia point antiutopia located half space
reached solutions moo means considering
antiutopiabased metric maximization could become unbounded moving
solutions arbitrary far pareto frontier antiutopia point therefore
measure denoted iau antiutopia indicator provide guarantee
accuracy


fiparisi pirotta restelli

mixed metrics
mentioned indicators provide one desiderata consequence
resulting approximate frontier might arbitrary far actual one order
consider desiderata mix previous concepts following indicator
iau w
w penalization function e monotonic function decreases
accuracy input increases e g w ipn w iu metrics denoted
respectively pn u take advantage expansive behavior antiutopia
indicator accuracy optimalitybased indicator way
desiderata met single scalar measure c l l differentiable
another solution mix utopia antiutopiabased indicators different way
want solutions simultaneously far antiutopia close utopia
consider following metric maximized


iau

iu

free parameters
next section proposed mixed metrics effective driving
pmga close pareto frontier exact approximate scenarios however
want make clear consistency guaranteed strongly depends
free parameters insights discussed section

experiments
section evaluate two linear quadratic gaussian
regulator water reservoir control task pmga compared state art methods
peters mulling altun castelletti et al parisi et al beume naujoks
emmerich hypervolume vamplew et al extension
previously defined performance index pianosi castelletti restelli named loss
measuring distance approximate pareto front reference one objective
hypervolume exactly computed objective given high
computational complexity hypervolume approximated montecarlo estimate
percentage points dominated frontier cube defined utopia
antiutopia points estimate one million points used

idea loss index compare true pareto frontier fw jw
ww

space weights w frontier jw jbw ww returned
weights jw denotes discounted return singleobjective mdp defined
linear combination objectives w formally loss function l defined
l j



jw maxm jbw

z

j

f w p
ww

jw

p dw



p probability density simplex w jw w j normalization factor th component j difference best


fimorl continuous pareto manifold approximation

worst value th objective pareto frontier e ji max ji min ji

means weight policy minimizes loss function chosen jw
true pareto frontier f known reference one used
since pmga returns continuous frontiers two scores designed discrete
ones evaluation frontiers discretized figures presented
section discretized frontiers order allow better representation besides
hypervolume loss function report number solutions returned
number rollouts e total number episodes simulated
learning process data collected simulation averaged
ten trials experiments pmga learning rate






l l
positive definite symmetric matrix userdefined parameter
stepsize rule comes formulation gradient ascent constrained
predefined distance metric peters underlies derivation natural
gradient approaches however since exploits vanilla gradient e
consider euclidean space metric identity matrix
remainder section organized follows start studying behavior
metrics proposed section effects parametrization lqg
subsequently focus attention sample complexity meant number rollouts
needed approximate pareto front finally analyze quality
water reservoir control task complex real world scenario compare
state art multiobjective techniques case study domains first
presented reported discussed
linear quadratic gaussian regulator lqg
first case study discrete time linear quadratic gaussian regulator lqg
multi dimensional continuous state action spaces peters schaal b
lqg defined following dynamics
st ast bat

n k st

r st st qst rat
st n dimensional column vectors b q r rnn q symmetric
semidefinite matrix r symmetric positive definite matrix dynamics
coupled e b identity matrices policy gaussian parameters
vec k k rnn finally constant covariance matrix used
lqg easily extended account multiple conflicting objectives
particular minimizing distance origin w r th axis
taken account considering cost action axes
x
ri st
j
j

source code available https github com sparisi mips



fiparisi pirotta restelli

since maximization th objective requires null action axes
objectives conflicting reward formulation violates positiveness matrix
ri change adding sufficiently small perturbation




x
x
ri st
j
j
j

j

parameters used experiments following initial
state objective case respectively
following sections compare performance proposed metrics several settings
made use tables summarize end set experiments
objective case
lqg scenario particular instructive since terms involved definition returns gradients hessians computed exactly therefore focus studying
different policy manifold parametrizations metrics
unconstrained parametrization domain problematic since defined
control actions range controls outside range lead divergence
system primary concern therefore related boundedness control
actions leading following parametrization manifold policy space


exp



exp
utopia antiutopia points respectively metrics iau
iu normalized order reference point learning step parameter
equation
case exploiting nonmixed metrics pmga able learn good approximation pareto frontier terms accuracy covering utopiabased
indicator learned frontier collapses one point knee front
behavior occurs ipn antiutopia point reference point solutions
dominated approximate frontier gets wider diverging true frontier
expanding opposite half space behaviors surprising considering
definition indicator functions explained section
contrary shown figure mixed metrics able achieve
accuracy covering starting set
able learn even starting different random parameters free metric parameters
set pn u although
shown figure u behaved similarly pn notice cases
first accuracy obtained pushing parametrization onto pareto frontier
frontier expanded toward extrema order attain covering
recall initially defined kj pk slightly modify normalizing policy
performance w r reference point kj p k component wise operator
section study sensitivity proposed metrics parameters



fimorl continuous pareto manifold approximation

table summary dimensional lqg unconstrained
metrics
nonmixed
issues

accuracy
covering


iu ipn frontier collapses one point
iau diverging behavior dominated solutions found



mixed

partial solution
final approximation
true pareto frontier






l



j













end
















j



iterations

learning process mixed metric pn








l

j












end













j





iterations

b learning process mixed metric

figure learning processes objective lqg without constraint
parametrization numbers denote iteration end denotes frontier obtained
terminal condition reached left approximated pareto frontiers
right corresponding l pn figure figure b approximated frontier overlaps true one however pmga converges faster



fiparisi pirotta restelli

constrained parametrization alternative consists forcing policy
manifold pass extreme points true front knowing parameterizations singleobjective optimal policies general requires additional
optimizations collection additional trajectories must accounted
however extreme points required set utopia antiutopia moreover case optimal singleobjective policies available literature
reasons count additional samples report total number rollouts
constrained parameterization two improvements easily obtained first
number free parameters decreases consequence learning process
simplified second approximate frontier forced sufficiently large area
cover extrema thus covering shown nonmixed indicators
alleviated cases completely eliminated dimensional lqg
parametrization forced pass extrema frontier following


exp



exp
initial parameter vector constraint able correct diverging
behavior iu ipn returned accurate wide approximation pareto
frontier shown figure notice much faster convergence since required learn fewer parameters two instead four however iau still shows
diverging behavior initial parameters figure b
contrary solutions obtained metrics independent initial
converges close true frontier even starting parametrization
generating initial frontier far away true one
objective case
unconstrained parametrization


exp
exp
exp

simplex

utopia antiutopia points respectively metrics
iau iu normalized initial parameters drawn uniform distribution
u nif causes numerical issues learning rate parameter
objective scenario frontiers learned iu ipn collapse single
point iau divergent trend figure however unlike objective lqr
pn failed correctly approximate pareto frontier reason tuning
difficult given difference magnitude ipn iau contrary
u returned high quality approximate frontier
latter shown figure b although small areas true pareto frontier
covered approximate one stress fact policies found
paretooptimal strength metrics found normalization
utopia antiutopiabased indicators expedient indeed allows easier tuning
free metric parameters magnitude single components similar
insights tuning mixed metrics parameters discussed section


fimorl continuous pareto manifold approximation

table summary dimensional lqg constrained
metrics
nonmixed iu ipn
nonmixed iau
issues
mixed

accuracy
covering




iau diverging behavior dominated solutions found



partial solution
final approximation
true pareto frontier






l

j











end















j













iterations

learning process utopiabased metric iu









j





l





















j









iterations

b learning process antiutopiabased metric iau

figure learning process objective lqg parametrization forced pass
extreme points frontier constraints able correct behavior
iu figure convergence faster previous parametrization however
iau still diverges figure b returned frontier includes dominated solutions since
metric considers covering frontier accuracy



fiparisi pirotta restelli

table summary dimensional lqg unconstrained
metrics
nonmixed
issues

accuracy
covering


iu ipn frontier collapses one point
iau diverging behavior dominated solutions found


pn difficult tuning



mixed pn
issues
mixed u

true pareto frontier
approximate frontier

j











j









j

j



j

frontier approximated antiutopiabased metric iau

j

true pareto frontier
approximate frontier











j





j







j

j



b frontier approximated mixed metric

figure resulting frontiers objective lqg unconstrained parametrization frontiers discretized better representation iau learning
diverges figure correctly approximates pareto frontier figure b



fimorl continuous pareto manifold approximation

constrained parametrization


exp b

exp b




exp c b b


b

simplex

c

initial parameters numerical reported table
hypervolume computed normalizing objective w r antiutopia figure
shows frontiers obtained utopia antiutopiabased indicators clearly
see unlike objective case even constrained parametrization metrics
lead poor solutions failing providing mo desiderata figure iu
frontier still tends collapse towards center true one order minimize
distance utopia point constraint prevents although shown
figures similar slightly broader frontier returned ipn however
stress solutions belong pareto frontier e nondominated solutions
found figure b shows frontier obtained iau expected tries
produce frontier wide possible order increase distance antiutopia
point behavior leads dominated solutions learning process diverges
contrary mixed metrics pn u
pmga able completely accurately cover pareto frontier shown
figures b worth notice different magnitude free parameter
pn compared objective case already discussed due
substantial difference magnitude iau ipn contrary tuning
mixed metrics easier similar parameters used unconstrained
parametrization proved effective come back topic section
finally shown table u achieve best numerical first
attains highest hypervolume lowest loss latter attains fastest
convergence superiority resides easy differentiability tuning especially compared pn reasons chosen empirical analysis
sample complexity comparison state art
real world mo discussed next sections
table performance comparison different metrics objective lqg
constrained parametrization reference frontier hypervolume
metric

hypervolume

loss

iterations

iu



e



iau







ipn



e



pn



e



u



e







e





fiparisi pirotta restelli

table summary dimensional lqg constrained
metrics
nonmixed
issues
mixed

accuracy
covering


iu ipn frontier collapses one point
iau diverging behavior dominated solutions found



j

true pareto frontier
approximate frontier











j





j







j

j



frontier approximated utopiabased metric iu

j

true pareto frontier
approximate frontier












j

j






j


j



b frontier approximated antiutopiabased metric iau

figure parametrization forced pass extreme points
frontier iu figure frontier shrinks much allowed parametrization constraint therefore able solve issues metric
objective scenario contrary iau frontier gets wider diverges
true one figure b intermediate frontier shown



fimorl continuous pareto manifold approximation

j

true pareto frontier
approximate frontier









j









j








frontier objectives space







b frontier policy parameters space

figure constrained parametrization shown figure
approximate frontier perfectly overlaps true one despite small discrepancies
policy parameters space learned parameters optimal ones figure b
similar frontiers obtainable pn u

empirical sample complexity analysis
section provide empirical analysis sample complexity pmga meant
number rollouts needed approximate pareto frontier goal identify
relevant parameter estimate mdp terms j j hj
analysis performed dimensional lqg domain varying number
policies used estimate integral per iteration pmga number episodes
policy evaluation steps episode fixed first used
parametrization forced pass extreme points frontier
produces initial approximate frontier far true one parameter
learning rate equation set parameter u set
performance criterion choose total number rollouts required reach
loss smaller hypervolume larger reference one
criteria used conditions convergence satisfied
evaluation mdp terms computed closed form terminal condition must
reached episodes otherwise forced end symbol used
represent latter case
table relevant parameter number episodes
used estimate mdp terms parameter controls variance estimate
e accuracy estimate l increasing number episodes
estimation process less prone generate misleading directions happens instance
oneepisode case parameters move towards wrong direction contrary
number points used estimate integral denoted table seems
significant impact final performance influences
number model evaluations needed reach prescribed accuracy best behavior


fiparisi pirotta restelli

table total number episodes needed converge varying number points
approximate integral number episodes ep per point symbol
used terminal condition reached
parametrization constrained pass extreme points frontier one
point sufficient move whole frontier towards right direction

ep









































































b contrary unconstrained parametrization pmga needs sufficient number
episodes enough points correct update step

ep









































































samplebased perspective obtained exploiting one point
integral estimate although surprising simple explanation exists forcing
parameterization pass singleobjective optima correct estimation
gradient direction single point enough move entire frontier toward
true one e move parameters towards optimal ones
contrary unconstrained parametrization used one point sufficient
anymore shown table b case initial parameter vector set
learning rate parameter terminal condition requires
frontier loss smaller hypervolume larger reference
frontier without constraint needs accuracy evaluation
single points e sufficient number episodes enough points move whole
frontier towards right direction accuracy gradient estimate l therefore
depends number points number episodes pmga requires
much rollouts converge best behavior samplebased perspective
obtained exploiting five points integral estimate episodes
policy evaluation


fimorl continuous pareto manifold approximation

water reservoir
water reservoir modeled momdp continuous state variable representing water stored reservoir continuous action controlling
water release state transition model depending stochastic reservoir inflow
set conflicting objectives domain proposed pianosi et al
formally state transition function described mass balance equation
st st max min st reservoir storage time
reservoir inflow time generated white noise normal distribution
n release decision minimum maximum
releases associated storage st according relations st max st
work consider three objectives flooding along lake shores irrigation
supply hydro power supply immediate rewards defined
r st st max ht h
r st st max
r st st max e et
ht st reservoir level following experiments h
flooding threshold h max min release reservoir
water demand e electricity demand e et electricity
production
et g h ht
dimensional conversion coefficient g gravitational
acceleration turbine efficiency h water density r denotes
negative cost due flooding excess level r negative deficit
water supply r negative deficit hydro power production
original work discount factor set objectives
initial state drawn finite set however different settings used learning
evaluation phases given intrinsic stochasticity policies
evaluated episodes steps learning phase requires different
number episodes steps depending discuss details
section
since continuous exploit gaussian policy model


n
rd basis functions optimal policies
objectives linear state variable use radial basis approximation


e

ksci k
wi



used four centers ci uniformly placed interval widths wi
total six policy parameters


fiparisi pirotta restelli


evaluate effectiveness analyzed performance
frontiers found weighted sum stochastic dynamic programming pianosi et al
multi objective fqi pianosi et al episodic version relative entropy policy
search peters et al deisenroth et al sms emoa beume et al
two recent policy gradient approaches e radial paretofollowing
parisi et al since optimal pareto front available one
found sdp chosen reference one loss computation mofqi learns
deterministic policies e standard deviation gaussian set zero
trained samples dataset tuples objective
samples dataset tuples objective
remaining competing learn stochastic policies number episodes
required policy update step reps pfa ra sms emoa
given episodic formulation reps draws parameters upper distribution
n
diagonal covariance matrix set zero however since
learns parameters overall learned policy still stochastic sms emoa
maximum population size objective case respectively
crossover uniform mutation chance occur adds white
noise random chromosomes iteration top individuals kept
next generation guarantee solution quality decrease finally mofqi
scalarizes objectives weights sdp e weights
objective case respectively reps uses instead linearly spaced weights ra
follows linearly spaced directions along pfa exploits natural
gradient peters schaal adaptive learning step equation
f f fisher information matrix concerning parametrization
pmga used complete first degree polynomial objective case
















similarly objective case complete second degree polynomial used










simplex










parameterizations forced pass near extreme points pareto frontier
computed singleobjective policy search cases starting parameter


fimorl continuous pareto manifold approximation




l

j water demand









sdp
pmga
pmga end








iterations












j flooding



b

figure objective water reservoir even starting arbitrary poor
initial parametrization pmga able true pareto frontier figure b
figure trend manifold metric l averaged ten trials

vector last parameter set order guarantee
generation sufficiently explorative policies responsible variance
gaussian distribution however fair comparison competing
take advantage information mean initial policies calculated accordingly behavior optimal ones described castelletti et al e
initial standard deviation set guarantee sufficient exploration parametrization avoids completely random poor quality initial
policies utopia antiutopia points set
objective case objective one
according presented section integral estimate pmga
performed montecarlo fed one random point instance variable trajectories steps used estimate gradient
hessian policy regarding learning rate adaptive one described equation used evaluation points used
integral estimate objective case respectively already discussed given
obtained lqg order capability approximate decided consider indicator
main reasons efficiency table attained fastest convergence
easy differentiability finally recall averaged ten trials
figure b reports initial final frontiers first two objectives
considered even starting far true pareto frontier pmga able
increasing covering accuracy approximate frontier shown figure despite low number exploited samples presents almost
monotonic trend learning process converges iterations


fiparisi pirotta restelli

j water demand







sdp
pfa
ra
mofqi
reps
sms emoa
pmga












j flooding









figure visual comparison objective water reservoir pmga frontier comparable ones obtained state art terms accuracy covering
however continuous one others scattered
table numerical comparison objective water reservoir sdp
reference frontier hypervolume nine solutions


hypervolume

loss

rollouts

solutions









pfa









ra

















reps









sms emoa









pmga

mofqi

figure offers visual comparison pareto points tables report
numerical evaluation including hypervolume loss achieved
w r sdp approximation pmga attains best performance
objective cases followed pfa sms emoa returns good approximation
slowest requiring ten times amount samples used pmga mofqi
outperforms pmga sample complexity loss highest finally figure
shows hypervolume trend pmga comparison sample complexity
objective case pmga substantially sample efficient
attaining larger hypervolume much fewer rollouts example capable
generating frontier hypervolume ra one tenth rollouts
outperforms pfa half samples needed latter
regarding mofqi include loss number rollouts hypervolume
number solutions available original



fimorl continuous pareto manifold approximation



pmga

pfa
sms emoa

hypervolume



reps



ra




















rollouts
figure comparison sample complexity objective case hypervolume
evaluation score brackets number rollouts needed produce
best frontier pmga clearly outperforms competing requires
much fewer samples generate frontiers better hypervolume
table numerical comparison objective water reservoir sdp
reference frontier hypervolume solutions


hypervolume

loss

rollouts

solutions









pfa









ra

















reps









sms emoa









pmga

mofqi

metrics tuning
section want examine deeply tuning mixed metric parameters
order provide reader better insights correct use metrics performance pmga strongly depends indicator used thereby configuration
critical precise mixed metrics obtained best approximate pareto
frontiers experiments conducted section include trade accuracy
covering expressed parameters following analyze fundamental
concepts behind metrics study performance influenced changes
parameters


fiparisi pirotta restelli

approximate frontier


true pareto frontier































b









c

figure approximate frontiers objective lqg learned pmga pn
varying figure indicator penalize enough dominated solutions
figure c frontier wide enough contrary figure b
achieves accuracy covering

tuning
first indicator maximized analyze
iau w
w penalization term previous sections proposed w ipn
w iu order take advantage expansive behavior antiutopiabased
indicator accuracy optimalitybased indicator section study
performance mixed metric changing proposing simple tuning process
idea set initial value increase decrease approximate
frontier contains dominated solutions wide enough figure shows different
approximate frontiers obtained different values exact objective lqg
iterations w ipn starting indicator behaves mostly
iau meaning small figure increasing figure c
converges approximate frontier completely cover true one
e ipn mostly condition behavior metric finally figure b
approximate frontier perfectly matches true one metric correctly mixes two
single indicators
however already discussed section use w ipn problematic
difference magnitude iau ipn make tuning hard
point metric becomes ineffective drawback solved w iu
normalizing reference point indicators e iu iau j p kj p k
normalization bounds utopia antiutopiabased metrics similar intervals
e respectively
ratio two vectors b component wise operation



fimorl continuous pareto manifold approximation

j

j

u





j

au




j

u

j

au


b

u



j

au


c

figure examples pareto frontiers figures b frontiers convex
latter objectives normalized figure c frontier concave

tuning
second mixed indicator maximized takes advantage expansive behavior antiutopiabased indicator accuracy utopiabased one
defined
iau


iu
free parameters
better understand insights guided metric definition consider
different scenarios according shape pareto frontier figure frontier
convex normalized objectives case point closer
antiutopia utopia sure dominated solution ratio iau iu
point frontier greater hence reasonable set
therefore need know exactly antiutopia point
drawback antiutopiabased metric iau disappears since take account
distance utopia point nevertheless setting points critical
magnitude strongly affect pmga performance example shown figure b
frontier normalized objectives different magnitude
case setting indicator evaluated extrema frontier
j j equal respectively first value
negative approximate frontier includes points true pareto frontier
j would perform better true pareto frontier
contrary frontier concave figure c true point
closer antiutopia utopia dominated solution ratio iau iu
point frontier exception eventually ends
smaller one keeping pmga would try collapse frontier
single point order maximize indicator therefore parameters need
changed accordingly trial error instance returned frontier
achieve accuracy possible solution decrease increase


fiparisi pirotta restelli

conclusion
proposed novel gradientbased namely paretomanifold
gradient pmga learn continuous approximation pareto frontier
momdps idea define parametric function describes manifold
policy parameters space maps manifold objectives space given metric
measuring quality manifold objectives space e candidate frontier
shown compute estimate trajectory samples gradient w r
parameters updating parameters along gradient direction generates
policy manifold associated improved w r chosen metric continuous frontier
objectives space although provided derivation independent
parametric function metric used measure quality candidate solutions
terms strongly influence final regarding former achieved
high quality forcing parameterization pass singleobjective
optima however trick might require domain expertise additional samples
therefore could applicable regarding latter presented different
alternative metrics examined pros cons one shown properties
empirical analysis discussed general tuning process promising ones
evaluation included sample complexity analysis investigate performance
pmga comparison state art morl
outperforms competing quality frontier sample
complexity would interesting study properties theoretical perspective
order provide support empirical evidence leave open
investigation convergence rate approximation error true pareto
frontier however think hard provide analysis general setting
future address study metrics parametric functions
produce good general case particular investigate
many objectives e three highdimensional policies since complexity manifold parameterization grows number objectives policy
parameters polynomial parameterization could effective complex alternative parameterizations found another interesting direction
concerns importance sampling techniques reducing sample complexity
gradient estimate since frontier composed continuum policies likely
trajectory generated specific policy partially used estimation
quantities related similar policies thus decreasing number samples needed
montecarlo estimate integral moreover would interesting investigate automatic techniques tuning metric parameters applicability
pmga multi agent scenario e g roijers whiteson oliehoek



fimorl continuous pareto manifold approximation

appendix optimal baseline
theorem componentdependent baseline optimal baseline j component
j
hessian estimate hrf jd given equation

j
bh




j
g



e r



j
e g




j

g

j

ln p j ln p h

ln p

given baseline b variance reduction obtained optimal baseline bh
var hrf jd b var hrf j bh


j

j

b
bh
j
e
g


n
j

proof let g

j th component g
j

g

j

ln p j ln p h

ln p

j

variance hrf jd given
var



j
hrf jd





h

j
j
j
g
e r b j g
e r b








j
j

j
e b
g
e r g






h

j
j
b j e r g
e r g





minimizing previous equation w r b j get

j

bh




j
e r g



j
e g

use compact notation e denote e



fiparisi pirotta restelli

excess variance given




j
j
j
var g r b j var g r bh









j
j
j
j
j

b
e r g
e b
g
e r g





h





j
j
j
j

e bh
e r g
e r g
g





h

j
j
j
e r g
bh e r g








j
j
j
j
b
e g
b
e r g








j
j
j
j
bh e g
bh e r g











j
j
j

j
e g
b
e r g
b












j


e r g

j




g
e

j
e g



j




e r g
j





r g



e

j
e g






j
j
j
j
b
e r g
e g
b







j
e r g



j
e g





j
g



e r


j
j

b
b


j
e g
e







j

b

j

g







j
bh e





j
g









j

e r g










j
e g


fimorl continuous pareto manifold approximation

references
ahmadzadeh kormushev p caldwell multi objective reinforcement
learning auv thruster failure recovery adaptive dynamic programming
reinforcement learning adprl ieee symposium pp
athan w papalambros p note weighted criteria methods compromise solutions multi objective optimization engineering optimization

barrett l narayanan learning optimal policies multiple criteria
proceedings th international conference machine learning icml
pp york ny usa acm
bertsekas p dynamic programming suboptimal control survey
adp mpc european journal control
beume n naujoks b emmerich sms emoa multiobjective selection
dominated hypervolume european journal operational

brown smith r e directed multi objective optimization international
journal computers systems signals
calandra r peters j deisenrothy pareto front modeling sensitivity
analysis multi objective bayesian optimization nips workshop bayesian
optimization vol
castelletti corani g rizzolli soncinie sessa r weber e reinforcement learning operational management water system ifac workshop
modeling control environmental issues keio university yokohama japan
pp
castelletti pianosi f restelli tree fitted q iteration multiobjective markov decision neural networks ijcnn international joint conference pp
castelletti pianosi f restelli multiobjective reinforcement learning
water resources systems operation pareto frontier approximation
single run water resources
crites r h barto g elevator group control multiple reinforcement
learning agents machine learning
das dennis j closer look drawbacks minimizing weighted sums
objectives pareto set generation multicriteria optimization structural
optimization
das dennis j e normal boundary intersection method generating
pareto surface nonlinear multicriteria optimization siam journal
optimization
deisenroth p neumann g peters j survey policy search robotics
foundations trends robotics


fiparisi pirotta restelli

fonteneau r prashanth l simultaneous perturbation batch
policy search rd ieee conference decision control cdc los
angeles ca usa december pp ieee
friedrich horoba c neumann f multiplicative approximations
hypervolume indicator proceedings th annual conference genetic
evolutionary computation gecco pp york ny usa acm
furmston barber unifying perspective parametric policy search
methods markov decision processes pereira f burges c bottou l
weinberger k eds advances neural information processing systems pp
curran associates inc
gabor z kalmar z szepesvari c multi criteria reinforcement learning
shavlik j w ed proceedings fifteenth international conference
machine learning icml madison wisconsin usa july pp
morgan kaufmann
greensmith e bartlett p l baxter j variance reduction techniques
gradient estimates reinforcement learning journal machine learning

harada k sakuma j kobayashi local search multiobjective function
optimization pareto descent method proceedings th annual conference
genetic evolutionary computation gecco pp york ny
usa acm
harada k sakuma j kobayashi ono uniform sampling local paretooptimal solution curves pareto path following applications multi objective
ga lipson h ed genetic evolutionary computation conference gecco
proceedings london england uk july pp acm
kakade optimizing average reward discounted rewards helmbold p
williamson r c eds computational learning theory th annual conference
computational learning theory colt th european conference
computational learning theory eurocolt amsterdam netherlands july
proceedings vol lecture notes computer science pp
springer
koski j silvennoinen r norm methods partial weighting multicriterion optimization structures international journal numerical methods
engineering
lizotte j bowling murphy linear fitted q iteration multiple
reward functions journal machine learning
lizotte j bowling h murphy efficient reinforcement learning
multiple reward functions randomized controlled trial analysis furnkranz j
joachims eds proceedings th international conference machine
learning icml june haifa israel pp omnipress


fimorl continuous pareto manifold approximation

magnus j r neudecker h matrix differential calculus applications
statistics econometrics wiley ser probab statist texts references
section wiley
mannor shimkin n steering multi criteria reinforcement
learning dietterich becker ghahramani z eds advances neural
information processing systems pp mit press
mannor shimkin n geometric multi criterion reinforcement
learning j mach learn res
messac ismail yahaya multiobjective robust design physical programming structural multidisciplinary optimization
messac ismail yahaya mattson c normalized normal constraint method generating pareto frontier structural multidisciplinary
optimization
munkres j r analysis manifolds adv books classics series westview press
natarajan tadepalli p dynamic preferences multi criteria reinforcement
learning raedt l wrobel eds machine learning proceedings
twenty second international conference icml bonn germany august
vol acm international conference proceeding series pp
acm
nojima kojima f kubota n local episode learning multiobjective behavior coordination mobile robot dynamic environments fuzzy
systems fuzz th ieee international conference vol pp
vol
okabe jin sendhoff b critical survey performance indices
multi objective optimisation evolutionary computation cec
congress vol pp vol
parisi pirotta smacchia n bascetta l restelli policy gradient
approaches multi objective sequential decision making international joint
conference neural networks ijcnn beijing china july pp
ieee
perny p weng p finding compromise solutions multiobjective markov
decision processes coelho h studer r wooldridge eds ecai th european conference artificial intelligence lisbon portugal august
proceedings vol frontiers artificial intelligence applications pp
ios press
peters j machine learning motor skills robotics ph thesis university
southern california
peters j mulling k altun relative entropy policy search fox
poole eds proceedings twenty fourth aaai conference artificial
intelligence aaai pp aaai press


fiparisi pirotta restelli

peters j schaal natural actor critic neurocomputing
progress modeling theory application computational intelligenc th
european symposium artificial neural networks th european symposium
artificial neural networks
peters j schaal b reinforcement learning motor skills policy gradients
neural networks robotics neuroscience
pianosi f castelletti restelli tree fitted q iteration multiobjective markov decision processes water resource management journal hydroinformatics
pirotta parisi restelli multi objective reinforcement learning
continuous pareto frontier approximation bonet b koenig eds proceedings twenty ninth aaai conference artificial intelligence january
austin texas usa pp aaai press
pirotta restelli bascetta l adaptive step size policy gradient
methods burges c j c bottou l ghahramani z weinberger k q eds
advances neural information processing systems th annual conference
neural information processing systems proceedings meeting held december
lake tahoe nevada united states pp
robert c casella g monte carlo statistical methods springer texts
statistics springer verlag york
roijers vamplew p whiteson dazeley r survey multi objective
sequential decision making journal artificial intelligence
roijers whiteson oliehoek f computing convex coverage sets
faster multi objective coordination journal artificial intelligence

romero c extended lexicographic goal programming unifying omega

shelton c r importance sampling reinforcement learning multiple
objectives ph thesis massachusetts institute technology
steuer r e choo e u interactive weighted tchebycheff procedure
multiple objective programming mathematical programming
sutton r barto g reinforcement learning introduction bradford
book bradford book
sutton r mcallester singh p mansour policy gradient
methods reinforcement learning function approximation solla leen
muller k eds advances neural information processing systems pp
mit press
tesauro g das r chan h kephart j levine rawson f lefurgy c
managing power consumption performance computing systems reinforcement learning platt j koller singer roweis eds advances
neural information processing systems pp curran associates inc


fimorl continuous pareto manifold approximation

vamplew p dazeley r berry issabekov r dekker e empirical evaluation methods multiobjective reinforcement learning machine learning

van moffaert k drugan nowe scalarized multi objective reinforcement learning novel design techniques adaptive dynamic programming
reinforcement learning adprl ieee symposium pp
van moffaert k nowe multi objective reinforcement learning sets
pareto dominating policies journal machine learning
waltz f engineering hierarchical optimization criteria automatic
control ieee transactions
wang w sebag hypervolume indicator dominance reward multiobjective monte carlo tree search machine learning
williams r simple statistical gradient following connectionist reinforcement learning machine learning
yu p leitmann g compromise solutions domination structures salukvadzes solution journal optimization theory applications
zitzler e thiele l bader j set multiobjective optimization evolutionary computation ieee transactions
zitzler e thiele l laumanns fonseca c da fonseca v g performance assessment multiobjective optimizers analysis review evolutionary
computation ieee transactions




