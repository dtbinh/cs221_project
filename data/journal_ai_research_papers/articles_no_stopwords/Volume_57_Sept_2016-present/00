Journal Artificial Intelligence Research 57 (2016) 1-37

Submitted 03/16; published 09/16

Learning Continuous Time Bayesian Networks
Non-stationary Domains
Simone Villa
Fabio Stella

villa@disco.unimib.it
stella@disco.unimib.it

Department Informatics, Systems Communication
University Milano-Bicocca
Viale Sarca 336, 20126 Milan, Italy

Abstract
Non-stationary continuous time Bayesian networks introduced. allow
parents set node change continuous time. Three settings developed
learning non-stationary continuous time Bayesian networks data: known transition
times, known number epochs unknown number epochs. score function
setting derived corresponding learning algorithm developed. set numerical
experiments synthetic data used compare effectiveness non-stationary continuous time Bayesian networks non-stationary dynamic Bayesian networks. Furthermore, performance achieved non-stationary continuous time Bayesian networks
compared achieved state-of-the-art algorithms four real-world datasets,
namely drosophila, saccharomyces cerevisiae, songbird macroeconomics.

1. Introduction
identification relationships statistical dependencies components multivariate time-series, ability reasoning whether dependencies
change time crucial many research domains biology, economics, finance,
traffic engineering neurology, mention few. biology, example, knowing
gene regulatory network allows understand complex biological mechanisms ruling
cell. context, Bayesian networks (BNs) (Pearl, 1989; Segal, Peer, Regev, Koller,
& Friedman, 2005; Scutari & Denis, 2014), dynamic Bayesian networks (DBNs) (Dean
& Kanazawa, 1989; Zou & Conzen, 2005; Vinh, Chetty, Coppel, & Wangikar, 2012)
continuous time Bayesian networks (CTBNs) (Nodelman, Shelton, & Koller, 2002; Acerbi,
Zelante, Narang, & Stella, 2014) used reconstruct transcriptional regulatory
networks gene expression data. effectiveness discrete DBNs investigated identify functional correlations among neuroanatomical regions interest (Burge,
Lane, Link, Qiu, & Clark, 2009), useful primer BNs functional magnetic resonance imaging data analysis made available (Mumford & Ramsey, 2014). However,
mentioned applications require time-series generated stationary distribution, i.e. one change time. stationarity reasonable
assumption many situations, cases data generating process clearly
non-stationary. Indeed, last years, researchers different disciplines, ranging
economics computational biology, sociology medicine become interested
representing relationships dependencies change time.
c
2016
AI Access Foundation. rights reserved.

fiVilla & Stella

Specifically, researchers interested analyzing temporal evolution
genetic networks (Lebre, Becq, Devaux, Stumpf, & Lelandais, 2010), flow neural
information networks (Smith, Yu, Smulders, Hartemink, & Jarvis, 2006), heart failure (Liu,
Hommersom, van der Heijden, & Lucas, 2016), complications type 1 diabetes (Marini,
Trifoglio, Barbarini, Sambo, Camillo, Malovini, Manfrini, Cobelli, & Bellazzi, 2015)
dependence structure among financial markets crisis (Durante & Dunson, 2014).
According specialized literature evolution models (Robinson & Hartemink, 2010),
divided two main categories: structurally non-stationary, i.e. models
allowed change structure time, parametrically non-stationary,
i.e. models allow parameters values change time.
paper, structurally non-stationary continuous time Bayesian network model
(nsCTBN) introduced. nsCTBN consists sequence CTBNs improves expressiveness single CTBN. Indeed, nsCTBN allows parents set node
change time specific transition times thus allows model non-stationary systems. learn nsCTBN, Bayesian score learning CTBNs extended (Nodelman,
Shelton, & Koller, 2003). nsCTBN version Bayesian score still decomposable
variable depends knowledge setting be: known transition times,
transition times known, known number epochs, number transition times known, unknown number epochs, number transition times
unknown. learning algorithm knowledge setting designed developed.
Experiments non-stationary dynamic Bayesian networks (nsDBNs) (Robinson &
Hartemink, 2010), i.e. discrete time counterparts nsCTBNs, performed.
main contributions paper following:
definition structurally non-stationary continuous time Bayesian network model;
derivation Bayesian score decomposition knowledge setting;
design algorithms learning nsCTBNs different knowledge settings.
novel dynamic programming algorithm learning nsCTBNs known
transition times setting described, learning nsCTBNs others settings
performed simulated annealing, exploiting dynamic programming algorithm;
performance comparison nsCTBNs nsDBNs knowledge settings
rich set synthetic data generated nsCTBNs nsDBNs;
performance comparison nsCTBNs state-of-the-art algorithms realworld datasets, namely drosophila, saccharomyces cerevisiae songbird;
nsCTBN learned macroeconomics dataset consisting variables evolving
different time granularities spanning 1st January 1986 31st March 2015.
rest paper organized follows. Section 2 continuous time Bayesian networks introduced together learning problem complete data. Section 3
introduces non-stationary continuous time Bayesian networks, presents three learning settings derives corresponding Bayesian score functions. Algorithms learning
nsCTBNs different learning settings described Section 4. Numerical experiments synthetic real-world datasets presented Section 5. Section 6 closes
paper making conclusions indicating directions research activities.
2

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

2. Continuous Time Bayesian Networks
Continuous time Bayesian networks combine Bayesian networks homogeneous Markov
processes together efficiently model discrete state continuous time dynamical systems
(Nodelman et al., 2002). particularly useful modeling domains variables evolve different time granularities, model presence people
computers (Nodelman & Horvitz, 2003), study reliability dynamical systems (Boudali
& Dugan, 2006), model failures server farms (Herbrich, Graepel, & Murphy, 2007),
detect network intrusion (Xu & Shelton, 2008), analyze social networks (Fan & Shelton,
2009), model cardiogenic heart failure (Gatti, Luciani, & Stella, 2011) reconstruct
gene regulatory networks (Acerbi & Stella, 2014; Acerbi, Vigano, Poidinger, Mortellaro,
Zelante, & Stella, 2016). Recently, complexity inference continuous time Bayesian
networks studied (Sturlaugson & Sheppard, 2014).
2.1 Basics
representation ability continuous time Bayesian networks inherent factorization system dynamics local continuous time Markov processes depend
limited set states. continuous time Bayesian network model defined follows:
Definition 1. Continuous time Bayesian network (Nodelman et al., 2002). Let X
set random variables X = {X1 , X2 , . . . , XN }. X finite domain values
V al(X) = {x1 , x2 , . . . , xI }. continuous time Bayesian network X consists two
0 , specified Bayesian network X,
components: first initial distribution PX
second continuous time transition model specified as: directed (possibly cyclic)
P a(X)
graph G whose nodes X1 , X2 , . . . , XN ; conditional intensity matrix (CIM), QX
,
variable X X, P a(X) denotes set parents X graph G.
P a(X)

conditional intensity matrix QX

consists set intensity matrices

qxpa1 u
qxpa2 xu1
=

.
qxpaI xu1


u
Qpa
X

.
.
.
.


qxpa1 xuI
qxpa2 xuI
,

.
pau
qxI


pau ranges possible configurations parents set P a(X), qxpai u =
P
pau
pau
pau
xj 6=xi qxi xj . Off-diagonal elements QX , i.e. qxi xj , proportional probability
variable X transitions state xi state xj given parents state pau .
pau
u
intensity matrix Qpa
X equivalently summarized two independent sets: q X =
pau
{qxi : 1 I}, i.e. set intensities parameterizing exponential distributions
pau
pau
pau
u
next transition occurs, pa
X = {xi xj = qxi xj /qxi : 1 i, j I, j 6= i},
i.e. set probabilities parameterizing multinomial distributions
state transitions. Note CTBN model assumes one single variable
change state specific instant, transition dynamics specified parents
via CIM, independent variables given Markov Blanket.
3

fiVilla & Stella

2.2 Structural Learning
Given fully observed dataset D, i.e. dataset consisting multiple trajectories1 whose
states transition times fully known, problem learning structure CTBN
addressed problem selecting graph G maximizes Bayesian
score computed dataset (Nodelman et al., 2003):
BS (G : D) = ln P (G) + ln P (D|G).

(1)

P (G) prior graph G P (D|G) marginal likelihood.
prior P (G) graph G, allows us prefer CTBNs structures
others, usually assumed satisfy structure modularity property (Friedman &
Koller, 2000), i.e. decompose following product terms:

P (G) =
P (P a(X) = P aG (X)),
(2)
XX

term parents set P aG (X) graph G. uniform prior G often used.
marginal likelihood P (D|G) depends prior parameters P (q G , G |G)
usually assumed satisfy global parameter independence, local parameter independence parameter modularity properties, outlined below.
Global parameter independence (Spiegelhalter & Lauritzen, 1990) states paramP (X)
P (X)
eters q X G
X G
associated variable X graph G independent,
thus prior parameters decomposes variable follows:

P (X) P (X)
P (q G , G |G) =
P (q X G , X G |G).
(3)
XX

Local parameter independence (Spiegelhalter & Lauritzen, 1990) asserts parameters associated configuration pau parents P aG (X) variable X
independent. Therefore, parameters associated variable X decomposable
parent configuration pau follows:
YY
P (X) P (X)
u
(4)
P (q X G , X G |G) =
P (qxpai u , pa
xi |G).
pau xi

Parameter modularity (Geiger & Heckerman, 1997) asserts variable X
parents P aG (X) = P aG 0 (X) two distinct graphs G G 0 , probability density
functions parameters associated X must identical:
P aG (X)

P (q X

P aG (X)

, X

P aG 0 (X)

|G) = P (q X

P aG 0 (X)

, X

|G 0 ).

(5)

Furthermore, assume sets parameters characterizing exponential distributions independent sets parameters characterizing multinomial
distributions:
P (q G , G |G) = P (q G |G)P ( G |G).
(6)
1. trajectory defined sequence pairs (t, X(t)), transition time [0, ]
associated state X(t) random variables corresponding nodes CTBN.

4

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

Dirichlet distribution selected prior parameters associated multinomial distribution, gamma distribution selected prior parameters
associated exponential distribution, i.e.

P (qxpai u ) Gamma xpai u , xpai u ,
(7)

pau
pau
pau
P ( xi ) Dir xi x1 , . . . , xi xI ,
(8)
xpai u , xpai u , xpai xu1 , . . . , xpai xuI priors hyperparameters. particular, hyperparameters represent pseudocounts number transitions state state,
parameter represents imaginary amount time spent state
data observed. Note hyperparameter xpai u inversely proportional
number joint states parents X. Conditioning dataset D, obtain
following posteriors parameters:

u
,
(9)
P (qxpai u |D) Gamma xpai u + Mxpai u , xpai u + Txpa


pau
pau
pau
pau
pau
(10)
P ( xi |D) Dir xi x1 + Mxi x1 , . . . , xi xI + Mxi xI ,
u
Mxpai xuj sufficient statistics CTBN (Nodelman et al., 2003).
Txpa

u
particular, Txpa
amount time spent variable X state xi

parents P a(X) state pau , Mxpai xuj number times variable X
transitions state xi state xj parents P a(X) state pau 2 .
Bayesian score (1) term P (G) grow size dataset D.
Thus, significant term marginal likelihood P (D|G). case complete data,
exploiting parameters independence (6) global parameter independence
property (3), marginal likelihood written follows:

P (X)
P (X)
P (D|G) =
L(q X G |D) L( X G |D),
(11)

XX
P aG (X)

L(q X

|D) marginal likelihood q derived follows:
YY
pau xi

P aG (X)

L( X

(xpai u + Mxpai u + 1) (xpai u )
u
(xpai u + 1) (xpai u + Txpa
)

u
(pa
xi +1)

pau
u
(pa
xi +Mxi +1)

,

|D) marginal likelihood derived follows:


xpai xuj + Mxpai xuj
(xpai u )
,
pau
pau
pau

(
+

)


x
x
x


xj
pa x =x
x 6=x
u



j



(12)

(13)

j

Bayesian-Dirichlet equivalent (BDe) metric version CTBNs (Nodelman, 2007).
case, BDe metric uses priors (7) (8), parameter modularity (5),
well global (3) local (4) parameter independence properties assumed
satisfied.
2. Please note number times P
variable X leaves state xi parents P a(X)
pau
u
state pau computed follows Mxpa
=
xj 6=xi Mxi xj .


5

fiVilla & Stella

conclusion, Bayesian score (1) computed closed form assuming
structure modularity property (2) satisfied, using BDe metric follows:
X
P (X)
P (X)
ln P (P a(X) = P aG (X)) + ln L(q X G |D) + ln L( X G |D). (14)
BS(G : D) =
XX

Since graph G CTBN acyclicity constraints, possible maximize
Bayesian score (14) separately optimizing parents set P a(X) variable
X. worthwhile mention maximum number parents set,
search optimal value Bayesian score (14) performed polynomial time.
search performed enumerating possible parents set using greedy
hill-climbing procedure operators add, delete reverse edges graph G.

3. Non-stationary Continuous Time Bayesian Networks
Continuous time Bayesian networks structurally stationary, graph
change time, parametrically stationary, conditional intensity matrices
change time. stationarity assumptions reasonable many situations,
cases data generating process intrinsically non-stationary
thus CTBNs longer used. Therefore, section, extend CTBNs become
structurally non-stationary. i.e. allow CTBNs structure change continuous
time.
3.1 Definition
non-stationary continuous time Bayesian network model, graph CTBN
replaced graphs sequence G = (G1 , G2 , . . . , GE ), graph Ge represents
causal dependency structure model epoch e {1, 2, . . . , E}3 . model
structurally non-stationary introduction graphs sequence
handle transition times common whole network and/or node-specific.
Following notations definitions used non-stationary dynamic Bayesian networks, let = (t1 , . . . , tE1 ) transition times sequence, i.e. times
causal dependency structure Ge , active epoch e, replaced causal dependency
structure Ge+1 , becomes active epoch e + 1. epoch defined period
time two consecutive transitions, i.e. epoch e active period
time starting te1 ending te . graph Ge+1 , active epoch
e + 1, differs graph Ge , active epoch e, set edges
call set edge changes Ge .
Figure 1 shows graphs sequence G = (G1 , G2 , G3 , G4 ) consisting four epochs (E = 4)
transition times = (t1 , t2 , t3 ). epoch associated set edge changes.
Specifically, graph G2 differs graph G1 following set edge changes
G1 = {X3 X2 , X2 6 X3 , X1 6 X2 }, graph G3 differs graph G2
following set edge changes G2 = {X2 X1 } graph G4 differs graph
G3 following set edge changes G3 = {X3 X4 , X4 X1 , X1 6 X4 , X4 6 X3 }.
3. worthwhile mention first epoch, i.e. epoch starting time 0 ending time t1
associated graph G1 , last epoch, i.e. epoch starting time tE1 ending
time (the supremum considered time interval, i.e. [0,T]) associated graph GE .

6

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

X1

X2

X1

X2

X1

X2

X1

X2

X4

X3

X4

X3

X4

X3

X4

X3

0

3

2

1

t2

t1

4

t3



Figure 1: Graphs sequence G = (G1 , G2 , G3 , G4 ) nsCTBN four epochs, E = 4,
three transition times, = (t1 , t2 , t3 ), edges gained lost time.
Non-stationary continuous time Bayesian networks allow node
sequence parents sets, parents set active given epoch. Therefore,
introduce concept homogeneous interval H(X) = (h1 , . . . , hM ) associated node
X, defined union consecutive epochs parents set
P a(X) active node X. Note epoch associated different
parents set, equal E.
non-stationary continuous time Bayesian network defined follows.
Definition 2. (Structurally) non-stationary continuous time Bayesian network. Let X
set random variables X1 , . . . , XN . X finite domain values V al(X) =
{x1 , . . . , xI }. (structurally) non-stationary continuous time Bayesian network Nns =
(B, Mns ) X consists two components:
0 , specified Bayesian network B X,
initial distribution PX

non-stationary continuous time transition model Mns specified as:
sequence directed (possibly cyclic) graphs G = (Ge )E
e=1 whose nodes
X1 , . . . , XN , E represents number epochs;
P (X)

G
conditional intensity matrix, QX,H(X)
, X X, P aG (X) denotes
parents sets X G, H(X) denotes intervals associated X.

P (X)

G
conditional intensity matrix QX,H(X)
consists set intensity matrices
u
qxpa1 ,h

pa
q u
x2 x1 ,hm
=

.
pau
qxI x1 ,hm



u
Qpa
X,hm

.
.
.
.


qxpa1 xuI ,hm
qxpa2 xuI ,hm
,

.
pau
qxI ,hm

one configuration pau parents set P a(X) P aG (X) active
interval hm H(X).4
u
4. Note following equation qxpai ,h
=


P

xj 6=xi

7

qxpai xuj ,hm still holds.

fiVilla & Stella

3.2 Learning Framework
Learning nsCTBN fully observed dataset done using Bayesian learning
framework taking account entire graphs sequence G. nsCTBNs case, must
specify prior probability graphs sequence G and, possible sequence,
density measure possible values parameters q G G . prior P (G)
likelihood P (q G , G |G) given, marginal likelihood P (D|G) computed
Bayesian score evaluated. important note focused
recovering graphs sequence G detecting possible changes parameters.
fact, identify non-stationarity parameters model, i.e. entries
conditional intensity matrices, significant enough result structural changes
graph. Others changes assumed small enough alter graph structure.
3.2.1 Prior Probability Graphs
Given transition times , thus number epochs E, assume prior
nsCTBNs structure G written follows:
P (G|T ) = P (G1 , ..., GE |T ) = P (G1 , G1 , ..., GE1 |T ) = P (G1 )P (G1 , ..., GE1 |T ).
(15)
Equation (15) justified assume probability distribution edge
changes function number changes performed, defined
independently initial graph G1 . knowledge particular edges
overall topology available initial network, use informative prior
P (G1 ) otherwise resort uniform distribution. CTBNs, P (G1 ) must satisfy structure modularity assumption (2), prior set edge changes
P (G1 , . . . , GE1 |T ) defines way edges change adjacent epochs.
3.2.2 Prior Probability Parameters
prior parameters P (q G , G |G, ) selected satisfy following assumptions:
independence sets parameters characterizing exponential multinomial distributions (6), parameter modularity (5) parameter independence. latter
assumption divided three components nsCTBNs: global parameter independence,
interval parameter independence local parameter independence.
Global parameter independence asserts parameters associated node
nsCTBNs graphs sequence independent, prior parameters decomposes
variable X follows:

P aG (X)
P aG (X)
P (q G , G |G, ) =
P (q X,H(X)
, X,H(X)
|G, ).
(16)
XX

Interval parameter independence states parameters associated interval
active parents node independent, parameters associated
X parents sets P aG (X) decomposable interval hm H(X) follows:
P (X)

P (X)

G
G
P (q X,H(X)
, X,H(X)
|G, ) =


hm

8

P (X)

P (X)

P (q X,hGm , X,hGm |G, ).

(17)

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

Local parameter independence states parameters associated state
variable given interval independent, thus parameters associated X
interval hm H(X) decomposable parent configuration pau follows:
YY
P (X) P (X)
u
u
(18)
, pa
P (qxpai ,h
P (q X,hGm , X,hGm |G, ) =
xi ,hm |G, ).

pau xi

CTBNs case, Dirichlet distribution used prior parameters
multinomial distribution gamma distribution used prior parameters
u

exponential distribution. sufficient statistics modified follows: Txpa
,hm
amount time spent state X = xi P a(X) = pau interval H(X) = hm ,
Mxpai xuj ,hm number transitions state X = xi state X = xj P a(X) = pau
P
interval H(X) = hm . let Mxpai ,hu = xj 6=xi Mxpai xuj ,hm number times
X leaves state xi parents P a(X) state pau interval H(X) = hm .
3.2.3 Marginal Likelihood
Given graphs sequence G, transition times , marginal likelihood P (D|G, )
dataset computed closed form using priors sufficient statistics
previously defined. derive Bayesian-Dirichlet equivalent metric nsCTBNs,
make assumptions CTBNs. case, parameter independence
assumption divided global (16), interval (17) local (18) parameter independence.
Therefore, marginal likelihood becomes:

P aG (X)
P aG (X)
P (D|G, ) =
L(q X,H(X)
|D) L( X,H(X)
|D).
(19)
XX

marginal likelihood q equation (19) calculated follows:
(pau +1)


xi ,hm
pau
pau
u
+
1

+

xpai ,h



xi ,hm
xi ,hm

P aG (X)
L(q X,H(X) |D) =
(pau +M pau +1) ,


xi ,hm
xi ,hm
pau
pau
pa
hm pau xi u + 1
xi ,hm + Txi ,hm
xi ,hm

(20)

marginal likelihood equation (19) calculated follows:




pau
pau
u
xpai ,h


+





xi xj ,hm
xi xj ,hm

P aG (X)




L( X,H(X)
|D) =
.
pau
pau
pau
xi xj ,hm
hm pau xi =xj xi ,hm + Mxi ,hm
xi 6=xj
(21)
important note nsCTBNs, pseudocounts well imaginary
amount time associated interval. aspect requires careful choice
order biased towards values small intervals analyzed.
possible correction weight CTBNs hyperparameters quantity proportional time interval width (hm hm1 ), hM denotes total time. Thus,
nsCTBNs hyperparameters could defined follows:
xpai xuj ,hm
xpai ,hu

(hm hm1 )
,
hM
(hm hm1 )
= xpai u
.
hM
= xpai xuj

9

(22)
(23)

fiVilla & Stella

want control parameter priors using two hyperparameters ,
use uniform BDe nsCTBNs (BDeu). case, hyperparameters
defined (22) (23) divided number U possible configurations
parents P a(X) node X times cardinality domain X, follows:
xpai xuj ,hm

=

xpai ,hu

=

(hm hm1 )
,
UI
hM
(hm hm1 )
.
UI
hM

(24)
(25)

Equations (22) (23) rescale hyperparameters way biased
respect epochs length, equations (24) (25) based uniform
distribution used performing numerical experiments.
3.3 Bayesian Score Decomposition
Bayesian score decomposed variable based information available
transition times. regard, three knowledge settings used derive
Bayesian score, namely: known transition times (KTT), known number epochs (KNE)
unknown number epochs (UNE).
3.3.1 Known Transition Times
setting, transition times known. Thus, prior probability
graphs sequence P (G|T ) decomposes equation (15), marginal likelihood
decomposes variable X according equation (19).
Therefore, Bayesian score BS(G : D, ) written follows:
BS(G : D, ) = ln P (G1 ) + ln P (G1 , . . . , GE1 |T )
P (X)

P (X)

G
G
+ ln L(q X,H(X)
|D) + ln L( X,H(X)
|D).

(26)

setting structural learning problem non-stationary continuous time
Bayesian network consists finding graph G1 active first epoch (e = 1)
E 1 sets edge changes G1 , . . . , GE1 together corresponding parameters
values, maximize Bayesian score defined equation (26).
graphs G2 , . . . , GE selected making assumptions ways
edges change continuous time. common approach (Robinson & Hartemink, 2010)
consists assuming graphs sequence G = (G1 , . . . , GE ) depends parameter
controls number edge changes continuous time. approach uses
truncated geometric distribution, parameter p = 1 exp(c ), model number
parents changes occurring transition time te+1 :
X
ce =
|Ge (X)|.
(27)
XX

variable ce counts number edge changes two consecutive graphs Ge
Ge+1 , parameter c controls impact number edge changes ce
score function (26).
10

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

edge changes Ge assumed mutually independent, probability
edge changes subsequent epochs written follows:
P (G1 , . . . , GE1 |T ) =

E1

e=1

E1

(1 exp(c ))(exp(c ))ce

(exp(c ))ce ,
1 (exp(c ))cmax +1

(28)

e=1

cmax truncation term. Therefore, assume truncated geometric distribution number parents changes occurring transition times equation
(28) holds, Bayesian score (26) decomposes variable X follows:
BS(G : D, ) =

X

ln P (P a(X) = P aG1 (X)) c

XX
P (X)

G
+ ln L(q X,H(X)
|D) +

E1
X

ce
e=1
P aG (X)
ln L( X,H(X)
|D).

(29)

worthwhile notice number parents changes ce epoch e
penalizes Bayesian score, thus discourages sudden variations parents set
consecutive epochs, parameter c controls impact changes
score function (26).
3.3.2 Known Number Epochs
transition times unknown, Bayesian score written follows:
BS(G, : D) = ln P (G, ) + ln P (D|G, ).

(30)

Assuming P (G, ) = P (G)P (T ) Bayesian score (30) becomes:
BS(G, : D) = ln P (G) + ln P (T ) + ln P (D|G, ).

(31)

number epochs E known, prior probability P (G) graphs
sequence G decomposes equation (15), truncated geometric distribution
used number parents changes occurring transition time,
known transition times setting.
choice P (T ) made include prior knowledge set transition
times. However, information available, uniform prior P (T ) used, implying
possible values transition times equally likely given number epochs
E. Thus, Bayesian score (31) decomposed variable X follows:
BS(G, : D) = ln P (T ) +
+

X

ln P (P a(X) = P aG1 (X)) c

XX
P aG (X)
ln L(q X,H(X) |D)

E1
X

ce

e=1
P (X)

G
+ ln L( X,H(X)
|D),

(32)

ce counts number edge changes two consecutive parents sets, c
controls impacts BS(G, : D) edge changes, happens KTT
setting.
11

fiVilla & Stella

3.3.3 Unknown Number Epochs
number epochs E unknown, transition times unknown well.
setting, learn nsCTBN exploiting introduced KTT
KNE settings. assume structure non-stationary continuous time
Bayesian network evolve different speeds continuous time. assumption
incorporated using truncated geometric distribution parameter p = 1exp(e )
number epochs. general, large values e encode strong prior belief
structure nsCTBN changes slowly (i.e. epochs exist).
Following presented KTT setting, Bayesian score obtained
subtracting parameter e times number epochs E. Therefore, Bayesian
score BS(G, : D) decomposes variable X follows:
BS(G, : D) = ln P (T ) e E +

X

ln P (P a(X) = P aG1 (X)) c

ce

e=1

XX
P (X)

E1
X

P (X)

G
G
+ ln L(q X,H(X)
|D) + ln L( X,H(X)
|D).

(33)

Note Bayesian score (33) contains two parameters, namely c e ,
encode prior belief structure nsCTBN. Specifically, parameter c
regulates prior belief smoothness edge changes (e.g. encouraging
discouraging edge changes per epoch), parameter e regulates prior belief
number epochs (e.g. encouraging discouraging creation epochs).

4. Structural Learning
optimal structure nsCTBNs found separately maximizing components
Bayesian score associated node. achieved using exact optimization algorithm based dynamic programming transition times
given. contrast, number epochs known information
transition times available, resort approximate techniques based Monte
Carlo simulated annealing. present exact algorithm solving structural
learning problem KTT setting. Then, briefly outline stochastic algorithms
solve structural learning problem KNE setting UNE setting.
4.1 Known Transition Times
setting Bayesian score decomposes according equation (29). Thus,
optimal graphs sequence G found separately searching optimal parents sequence G X node X. solve problem finding optimal parents sequence
G X node X consider sequence consisting intervals H(X) = (h1 , . . . , hM )
possible parents, Z = 2S possible parents sets. find optimal parents
sequence G X must compute Z marginal likelihood terms associated q ,
one marginal likelihood term possible parents set P az (X) interval hm .
Then, optimization algorithm used find maximum component
Bayesian score associated node X.
12

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

exhaustive search would prohibitive, would require evaluating Z scores,
one possible parents sequence G X . Unfortunately, greedy search strategy
computes parents set maximizes Bayesian score interval
viable. fact, function counts parents changes ce (29) binds choice
subsequent parents set, i.e. binds Ge Ge+1 .
However, relation score variable X associated parents set
P a(X)
P a(X) interval hm , denoted BSX,hm , score associated parents set
P a(X)

P a(X) interval hm1 , denoted BSX,hm1 , defined recursion follows:
n

P a(X)
P (X)
P a(X) P a(X)
BSX,hm = max BSX,hzm1 c cX,e + ln L(q X,hm , X,hm |D) ,
(34)
P az

cX,e = |Ge (X)|, marginal likelihoods q grouped together.
P a(X)
score BSX,hm , associated parents set P a(X) node X interval hm ,
introduced clarify recursion used Algorithm 1. Note score depends
components score hm . particular, marginal likelihoods
component involved, term cX,e , counts parents changes, included
binds choice subsequent parents sets. Equation (34) exploited dynamic
programming select optimal parents sequence G X node X.
Algorithm 1 takes input marginal likelihoods q interval
parents set, prior probability initial parents set, number parents
changes, parameter c . Algorithm 1 ensures optimal parents sequence G X
node X corresponding optimal Bayesian score. core computation
Z score matrix, denoted SC, dynamic programming recursion.
dynamic programming recursion interval h1 (m = 1) defined follows:
P (X)

SC1z = ln L(q X,hz1

P (X)

, X,hz1

|D) + ln P (P az (X) = P aGh1 (X)),

(35)

1 z Z, while, intervals hm (m = 2, . . . , ), recursion is:
n

P (X) P (X)
z
u
SCm
= max SCm1
+ ln L(q X,hzm , X,hzm |D) c cX,e .
1uZ

filling score matrix SC, value maxz {SC[M, z]} optimal Bayesian score,
optimal parents sequence reconstructed backwards 1 using
index matrix . cost computing dynamic programming recursion O(M Z 2 ),
polynomial fixed maximum number parents S.
problem selecting optimal parents sequence interesting graph representation. Indeed, possible create graph whose nodes associated marginal
likelihoods q interval hm parents set P az (X), node associated interval hm linked nodes associated interval hm+1 .
arc associated weight computed difference marginal likelihoods interval hm parents set P az (X) cost switching
parents set interval hm1 parents set interval hm . Two special nodes
added represent start end optimal parents sequence. graph
cycles, thus selection optimal parents sequence node
reduced longest path problem start node end node directed
acyclic graph, thus solved using either dynamic linear programming.
13

fiVilla & Stella

Algorithm 1 LearnKTTX
Require: matrix containing marginal likelihoods q LX[M, Z], vector containing prior probability initial parents set P R[Z], matrix containing
number parents changes C[Z, Z] parameter parents changes c .
Ensure: score matrix SC[M, Z] index matrix [M, Z].
1: Initialize SC[m, z] , [m, z] 0.
2: 1, . . . ,
3:
z 1, . . . , Z
4:
(m = 1)
5:
SC[m, z] ln LX[m, z] + ln P R[z]
6:
else
7:
w 1, . . . , Z
8:
score SC[m 1, w] + ln LX[m, z] c C[w, z]
9:
(score > SC[m, z])
10:
SC[m, z] score
11:
[m, z] w
12:
end
13:
end
14:
end
15:
end
16: end
Learning nsCTBN done following following four steps procedure: i) use
u
dataset compute variable X sufficient statistics Txpa
Mxpai xuj ,hm
,hm
according given transition times ; ii) compute marginal likelihoods (20) (21),
fill LX matrix; iii) run Algorithm 1 node X get corresponding
optimal parents sequence; iv) collect optimal parents sequence node X
compute corresponding CIMs using sufficient statistics already computed step i).
allow intervals differ transition times, i.e. obtained
one possible unions transition times; repeat learning
procedure E (E 1)/2 cases. possible speed computation
sufficient statistics aggregated intervals. way, read
dataset once, precomputed marginal likelihoods stored reused
intervals. Moreover, computations performed parallel node.
4.2 Known Number Epochs
setting, know number epochs, transition times given,
cannot directly apply Algorithm 1. However, tentative allocation transition
times given, apply Algorithm 1 obtain optimal nsCTBNs structure,
assumption different true transition times . find
optimal tentative allocation , i.e. allocation close possible , apply
simulated annealing (SA) algorithm (Kirkpatrick, Gelatt, & Vecchi, 1983).
14

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

Simulated annealing iterative algorithm attempts find global optimum
x given function f (x) stochastic search feasible region. iteration
k, SA algorithm assumed state xk , samples proposal state x0
according proposal distribution x0 P 0 (|xk ). Then, SA algorithm computes
quantity = exp ((f (x) f (x0 ))/CT ), CT computational temperature.
SA algorithm accepts proposal state x0 probability equal min{1, }. Concisely,
SA always accepts proposal state x0 f (x0 ) > f (x) setting xk+1 = x0 ,
accepts proposal state x0 f (x0 ) < f (x) probability setting xk+1 = x0
probability xk+1 = xk probability (1 ), i.e. case state
SA algorithm change. computational temperature reduces iterations
according cooling schedule. shown one cools sufficiently slowly,
algorithm probably find global optimum (Kirkpatrick et al., 1983). design
cooling schedule important part SA algorithm (Bertsimas & Tsitsiklis,
1993). possible approach use exponential cooling schedule defined follows:
CTk = CT0 k , CT0 represents initial temperature, typically set 1.0,
cooling rate, usually set close 0.8, k current iteration (Murphy, 2012).
nsCTBNs case, state SA algorithm x associated tentative
allocation , function f (x) Bayesian score (32). Algorithm 2 takes input
sufficient statistics, parameters used run Algorithm 1 parameters
SA algorithm. solves structural learning problem KNE setting given
variable X ensuring optimal tentative allocation corresponding score.
Algorithm 2 LearnKNEX
Require: sufficient statistics SuffStatsX, prior probability P R[], number parents
changes C[, ], parameter c , tentative allocation , initial temperature CT0 , cooling
rate , number iterations Iters, truncation parameter z standard deviation .
Ensure: optimal tentative allocation best Bayesian score bestSC.
1: Initialize k 0, .
2: LX GetMLX(SuffStatsX , )
3: bestSC LearnKTTX(M LX, P R[], C[, ], c )
4: (k < Iters)
5:
TentativeAllocation(T , z, )
6:
LX GetMLX(SuffStatsX , )
7:
tentSC LearnKTTX(M LX, P R[], C[, ], c )
8:
CT CT0 kn


9:
accP rob min 1, exp (bestSCtentSC)
CT
10:
ur UniRand()
11:
(ur accP rob)
12:

13:
currSC tentSC
14:
end
15:
k k+1
16: end
17: bestSC currSC
15

fiVilla & Stella

simulated annealing parameters used include tentative allocation ,
initial temperature CT0 , cooling rate number iterations Iters
exponential cooling schedule. Moreover, truncation parameter z standard deviation
used selection new tentative allocation 0 according random
procedure shown Algorithm 3. procedure selects transition time discrete
uniform distribution, UniRandDiscr(T ), perturbs according truncated normal
distribution, StdNormRand(), standard deviation equal , addition
point masses z z, z represents truncation parameter.
Algorithm 3 TentativeAllocation
Require: tentative allocation , truncation parameter z standard deviation .
Ensure: new tentative allocation 0 .
1: UniRandDiscr(T )
2: 0 \
3: nr StdNormRand()
4: (nr < z)
5:
nr z
6: end
7: (nr > z)
8:
nr z
9: end
10: + nr
11: 0

4.3 Unknown Number Epochs
setting number epochs unknown; thus structural learning algorithm
must able move across different number epochs, well corresponding
transition times. case, used simulated annealing algorithm state
x tentative allocation function optimized f (x) Bayesian score
shown equation (33). cooling schedule set one used
KNE setting. proposal distribution differs one used KNE setting
uses two additional operators, namely split merge operators. split
operator allows split given interval [tm ; tm+1 ) two subintervals [tm ; t) [t; tm+1 )
tm , tm+1 . merge operator allows merge contiguous intervals [tm1 ; tm )
[tm ; tm+1 ) form wider interval [tm1 ; tm+1 ) tm1 , tm , tm+1 .
new state obtained sampling number epochs changes ec multinoulli distribution parameters (p1 , p2 , p3 ), p1 represents probability
number epochs next iteration |T | decreased one; p3 represents probability
number epochs next iteration |T | increased one, p2 represents
probability number epochs next iteration |T | change respect
current one. ec equal 2, Algorithm 2 invoked, ec equal 1,
merge operator applied invoking Algorithm 2, ec equal 3,
split operator applied invoking Algorithm 2.
16

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

Algorithm 4 solves structural learning problem nsCTBN UNE setting
given node X ensuring optimal tentative allocation corresponding
Bayesian score. algorithm similar one used KNE settings,
uses Algorithm 5 apply split merge operators. left(t) function Algorithm
5 returns transition time comes immediately transition time t.
Algorithm 4 LearnUNEX
Require: sufficient statistics SuffStatsX, prior probability P R[], number parents
changes C[, ], parameter c , parameter e , tentative allocation , initial temperature
CT0 , cooling rate , number iterations Iters, truncation parameter z, standard deviation , split probability sp merge probability mp.
Ensure: optimal tentative allocation best Bayesian score bestSC.
1: Initialize k 0, .
2: bestSC LearnKTTX(GetMLX(SuffStatsX, ), P R[], C[, ], c ) e |T |
3: (k < Iters)
4:
SplitMerge(T , sp, mp)
5:
TentativeAllocation(T , z, )
6:
tentSC LearnKTTX(GetMLX(SuffStatsX, ), P R[], C[, ], c ) e |T |
7:
CT CT0 kn


8:
accP rob min 1, exp (bestSCtentSC)
CT
9:
ur UniRand()
10:
(ur accP rob)
11:

12:
currSC tentSC
13:
end
14:
k k+1
15: end
16: bestSC currSC
Algorithm 5 SplitMerge
Require: tentative allocation , split probability sp merge probability mp.
Ensure: new tentative allocation 0 .
1: 0
2: p UniRand()
3: (p < mp)
4:
UniRandDiscr(T )
5:
0 \
6: else
7:
(p < (mp + sp))
8:
UniRandDiscr(T )
9:
nt left(t) + tleft(t)
2
10:
0 nt
11:
end
12: end
17

fiVilla & Stella

5. Numerical Experiments
Numerical experiments performed synthetic real-world datasets. Synthetic
datasets used compare nsCTBNs nsDBNs KTT, KNE UNE knowledge settings terms accuracy, precision, recall F1 measure. following real-world
datasets: drosophila, saccharomyces cerevisiae songbird, used compare nsCTBNs
state-of-the-art algorithms, i.e. TSNI (a method based ordinary differential equations),
nsDBN (Robinson & Hartemink, 2010) non-homogeneous dynamic Bayesian networks
Bayesian regularization (TVDBN) (Dondelinger, Lebre, & Husmeier, 2013),
UNE knowledge setting. Drosophila, saccharomyces cerevisiae songbird datasets
collected fixed time intervals, thus analyzed additional real-world dataset, consisting financial/economic variables evolving different time granularities, exploit
expressiveness nsCTBNs events occur asynchronously. Note performance comparison using synthetic datasets benefits knowledge ground
truth, apply performance comparison using real-world datasets
ground truth available. cases, comparison exploits partial
meta-knowledge available specialized literature.
5.1 Synthetic Datasets
Artificially generated datasets include data sampled rich set nsDBN models,
i.e. nsDBN generated datasets, rich set nsCTBN models, i.e. nsCTBN generated
datasets. nsDBN nsCTBN models consist five nodes associated binary
ternary variables. Numerical experiments concern learning parents sets, transition
times number epochs single node. choice motivated fact
structural learning nsCTBN performed single node independently
remaining ones. However, transition times unknown, multiple parents
sets changes could make easier correctly identify times change.
5.1.1 nsDBN Generated Datasets
nsDBN generated datasets sampled nsDBN models5 associated following
number epochs E {2, 3, 4, 5}. particular, number epochs E, 10 different
nsDBN instances sampled obtain number datasets equal 10, one consisting single trajectory. Thus, 40 synthetic datasets used learn structure
nsDBN nsCTBN (number models =2) KTT, KNE UNE settings.
Structural learning experiments performed c = {1, 2, 4} e = {5, 10, 15}
nsCTBN = {1, 2, 4} = {10, 50, 100} nsDBN6 . overall number
1,200 experiments performed. particular, performed number epochs
number datasets number c number models = 41032 = 240 experiments
KTT setting, 240 KNE setting, number epochs number
datasets number c number e number models = 410332 = 720
experiments performed UNE setting.
5. Inter-slice arcs allowed, intra-slice arcs allowed. holds true nsDBN models
sampled obtain nsDBN generated datasets.
6. worthwhile mention parameters nsDBN counterparts c
e parameters nsCTBN.

18

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

nsdbn jar executable7 (Robinson & Hartemink, 2010) used structural learning nsDBN, set maximum number proposed networks 500,000
burn-in period 50,000 nsDBN. nsCTBN learned using following parameters setting: Iters = 1,000, CT0 = 1,000, = 0.8, z = 3, = 1, sp = 0.3, mp = 0.3,
= 1 = 0.1 using BDeu metric. Furthermore, nsDBN nsCTBN set
maximum number parents 4. arcs occurred 90 percent
samples8 belong inferred nsDBN nsCTBN models. Accuracy (Acc), precision (P rc), recall (Rec) F1 measure (F1 ) achieved nsDBN nsCTBN learned
KTT, KNE UNE settings reported Table 1, 2 3 respectively.
worthwhile mention KNE UNE settings, nsDBNs nsCTBNs
almost always identified correct number epochs location associated
transition times. Accuracy, precision, recall F1 measure computed two
different ways. Firstly, included arcs true network epoch. Secondly,
excluded self-reference arcs, i.e. arcs connecting node two consecutive
time-slices true network epoch. fact, node nsCTBN
self-reference arc default, happen nsDBNs. means
first case nsDBN required learn arcs nsCTBN required do. Therefore,
ensure fair comparison nsCTBN nsDBN adopted second case. Tables 1,
2 3 report performance measure values computed excluding self-reference arcs
set arcs true networks epoch.
Table 1: nsCTBN compared nsDBN KTT setting nsDBN generated data.
Average, min (subscript) max (superscript) performance values 10 networks
c nsCTBN nsDBN.
Number epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.961.00
0.93
0.901.00
0.67
0.771.00
0.40
0.801.00
0.57

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.93
0.791.00
0.50
0.670.83
0.43
0.710.91
0.47

1.00
0.920.75
1.00
1.001.00
1.00
0.860.63
1.00
0.920.77

0.950.99
0.89
0.871.00
0.40
0.650.90
0.25
0.730.95
0.31

1.00
0.820.63
1.00
0.960.75
1.00
0.700.38
1.00
0.800.50

0.940.97
0.89
0.851.00
0.50
0.580.80
0.25
0.690.86
0.35

0.820.95
0.55
0.991.00
0.80
0.710.91
0.33
0.820.95
0.47

According Tables 1, 2 3, nsDBNs consistently achieve greater accuracy values
achieved nsCTBNs three settings. Furthermore, nsDBNs
accuracy stable respect number epochs E happen
nsCTBNs. Indeed, number epochs E greater 3, nsCTBNs achieve
accuracy values significantly smaller achieved number
epochs E equal 2 3. happen nsDBNs accuracy
robust respect number epochs E.
7. acknowledge precious help Alex Hartemink let us use nsdbn jar executable program
learning nsDBN models. Furthermore, provided drosophila songbird datasets.
8. Samples obtained parameters values.

19

fiVilla & Stella

Table 2: nsCTBN compared nsDBN KNE setting nsDBN generated data.
Average, min (subscript) max (superscript) performance values 10 networks
c nsCTBN nsDBN.
Number epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.941.00
0.88
0.911.00
0.69
0.761.00
0.39
0.801.00
0.58

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.92
0.791.00
0.50
0.680.84
0.35
0.720.91
0.51

1.00
0.920.75
1.00
1.000.95
1.00
0.860.63
1.00
0.920.77

0.940.99
0.89
0.860.97
0.55
0.650.91
0.25
0.740.95
0.38

1.00
0.810.63
1.00
0.950.75
1.00
0.710.38
1.00
0.810.50

0.930.96
0.87
0.850.96
0.55
0.590.78
0.24
0.700.74
0.35

0.820.95
0.55
0.981.00
0.80
0.700.91
0.33
0.810.95
0.47

Table 3: nsCTBN compared nsDBN UNE setting nsDBN generated data.
Average, min (subscript) max (superscript) performance values 10 networks
c , e nsCTBN , nsDBN.
Number epochs E
3
4

2
Acc
P rec
Rec
F1

5

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

nsDBN

nsCTBN

0.951.00
0.88
0.910.98
0.70
0.750.98
0.38
0.790.96
0.55

0.921.00
0.75
1.001.00
1.00
0.851.00
0.50
0.911.00
0.67

0.950.98
0.93
0.801.00
0.52
0.660.81
0.41
0.700.89
0.48

1.00
0.920.75
1.00
1.001.00
1.00
0.860.63
1.00
0.920.77

0.930.98
0.89
0.870.95
0.67
0.650.87
0.26
0.740.87
0.41

0.99
0.810.61
1.00
0.950.73
0.99
0.700.36
0.99
0.800.48

0.920.96
0.89
0.840.90
0.71
0.570.78
0.23
0.680.85
0.34

0.810.93
0.55
0.981.00
0.80
0.690.87
0.33
0.810.93
0.47

different picture emerges focusing task discover positive arcs. Indeed,
case nsCTBNs achieve values precision, recall F1 measure, always
greater achieved nsDBNs. nsCTBNs achieve precision values robust
respect knowledge settings number epochs E.
hold true recall performance measure. Indeed, nsCTBNs achieve robust recall
respect knowledge settings (KTT, KNE UNE), recall achieved
nsCTBNs significantly degrades moving 2 3 epochs knowledge
settings. happens F1 measure achieved nsCTBNs. results
numerical experiments suggest nsCTBNs effective nsDBNs discover
positive arcs, even datasets generated using nsDBNs. possible explanation
behavior learning nsDBNs difficult learning nsCTBNs.
particular, nsDBNs must learn self-reference arcs nsCTBNs not. Furthermore,
node, nsCTBNs learn locally sequence parents sets
happen nsDBNs. fact, nsDBNs learn globally sequence parents sets
nodes, i.e. globally learn sequence networks, thus solve learning
problem difficult one solved nsCTBNs.
20

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

5.1.2 nsCTBN Generated Datasets
generated 40 synthetic datasets E {2, 3, 4, 5}, datasets used
learn structure nsCTBN three knowledge settings. parameters
setting used one used nsCTBN learning nsDBN generated datasets (for
nsCTBN, used = 1, = 0.1 BDeu metric), while, case,
perform structural learning experiments nsDBN models9 . graphical structures
nsCTBN models sampled obtain datasets sampled
obtain nsDBN datasets. goal experiments analyze performance
nsCTBN structural learning algorithms three knowledge settings.
analysis data reported Tables 4, 5 6 brings us conclude
nsCTBN structural learning algorithms work well three settings according
considered performance measures. Accuracy, recall F1 measure decrease slightly
number epochs increases 2 5. particular, recall measure suffers
greatest decrease 1 0.95 number epochs increases 2 5.
Accuracy F1 measure robust respect number epochs,
precision robust performance measure respect different datasets
different values number epochs knowledge settings.
Table 4: nsCTBN KTT setting nsCTBN generated data. Average, min
(subscript) max (superscript) performance values 10 networks c .

Acc
P rec
Rec
F1

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Number
3
0.991.00
0.92
1.001.00
1.00
0.991.00
0.86
0.991.00
0.92

epochs E
4
0.991.00
0.94
1.001.00
1.00
0.991.00
0.89
0.991.00
0.94

5
0.981.00
0.90
1.001.00
1.00
0.961.00
0.86
0.981.00
0.92

Table 5: nsCTBN KNE setting nsCTBN generated data. Average, min
(subscript) max (superscript) performance values 10 networks c .

Acc
P rec
Rec
F1

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Number
3
0.981.00
0.92
0.991.00
0.90
0.971.00
0.86
0.981.00
0.88

epochs E
4
0.991.00
0.94
1.001.00
0.99
0.981.00
0.89
0.991.00
0.94

5
0.971.00
0.90
1.001.00
0.97
0.961.00
0.85
0.981.00
0.90

9. nsCTBN generated data asynchronous involving different time granularities, thus nsDBN cannot
directly applied. option preprocess datasets adapt nsDBNs. Given
would strongly arbitrary penalizing nsDBNs, decided learn nsCTBN models.

21

fiVilla & Stella

Table 6: nsCTBN UNE setting nsCTBN generated data. Average, min
(subscript) max (superscript) performance values 10 networks c e .

2
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00
1.00
1.001.00

Acc
P rec
Rec
F1

Number
3
0.991.00
0.92
1.001.00
1.00
0.991.00
0.86
0.991.00
0.92

epochs E
4
0.991.00
0.94
1.001.00
1.00
0.981.00
0.89
0.991.00
0.94

5
0.971.00
0.90
1.001.00
0.98
0.951.00
0.81
0.971.00
0.89

best worst values accuracy E = 5 reported Table 6 belong
experiments performed synthetic dataset number 3 number 9 respectively.
results illustrated hereafter. Figure 2(a) shows graphs sequence true nsCTBN
synthetic datasets number 3, Figure 2(b) displays posterior distribution
epochs (right), together distribution corresponding transition times
(left)10 learned nsCTBN UNE case. Figure 3 shows information
depicted Figure 2, synthetic dataset number 9. latter case,
distribution epochs slightly favor correct number epochs.

(a) True nsCTBN model.
Distribution transition times

Distribution number epochs

1

1
True
Retrieved
0.8
Posterior probability

Probability transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

5

10

15

20

25

30

35 40
Time

45

50

55

60

65

70

0

5
Number epochs

(b) Learned nsCTBN model results.

Figure 2: nsCTBN generated dataset number 3: (a) true graphs sequence E=5 epochs
(b) distribution transition times (left) posterior epochs (right) associated
nsCTBN inferred UNE setting.

10. Transition times whose distance less 0.1 aggregated.

22

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

(a) True nsCTBN model.
Distribution transition times

Distribution number epochs

1

1
True
Retrieved
0.8
Posterior probability

Probability transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

5

10

15

20

25

30

35

40 45
Time

50

55

60

65

70

75

80

0

4
5
Number epochs

(b) Learned nsCTBN model results.

Figure 3: nsCTBN generated dataset number 9: (a) true graphs sequence E=5 epochs
(b) distribution transition times (left) posterior epochs (right) associated
nsCTBN inferred UNE setting.

5.2 Real-world Datasets
difficult find real-world datasets corresponding ground truth model
completely known and/or uniform consensus domain experts reached.
Therefore, decided use following three well-known datasets: drosophila, saccharomyces cerevisiae songbird compare performance nsCTBNs nsDBNs
state-of-the-art algorithms, i.e. TSNI TVDBN. datasets publicly
available, clearly described rich detailed discussion likely ground truth
models given specialized literature. Furthermore, macroeconomics dataset introduced analyzed. dataset consists 17 financial/economic variables collected
different time granularity spanning 1st January 1986 31st March 2015.
5.2.1 Drosophila
drosophila dataset includes mRNA expression levels 4,028 genes 67 successive time-points spanning four stages Drosophila melanogaster life cycle (Lebre
et al., 2010): embryonic (31 time-points), larval (10 time-points) pupal stage (18
time-points) first 30 days adulthood (8 time-points). comparative purposes
(Dondelinger et al., 2013), analyzed reduced drosophila dataset consisting
gene expression time-series 11 genes involved wing muscle development. Given
nsCTBNs based discrete variables, binarized expression level 11 genes
reduced drosophila dataset done literature (Zhao, Serpedin, & Dougherty,
2006; Guo, Hanneke, Fu, & Xing, 2007; Robinson & Hartemink, 2010).
23

fiVilla & Stella

Firstly, network inference task embryonic, larval, pupal adulthood morphogenic stages performed KTT setting (Robinson & Hartemink, 2010; Dondelinger et al., 2013). nsCTBN structural learning performed using following
parameter values c = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} setting maximum number parents 4. nsCTBN learned different c values combined,
arcs occurred 20 percent samples included
inferred non-stationary continuous time Bayesian network. techniques predict
non-stationary directed networks (Robinson & Hartemink, 2010), precision, recall
F1 measure, computed respect networks inferred Zhao et al. (2006) Guo et
al. (2007), reported Table 7 nsDBN, nsCTBN TVDBN (Dondelinger et al.,
2013). networks associated four epochs, inferred nsCTBN
reduced drosophila dataset KTT setting, depicted Figure 4.
Table 7: Precision (Prec), recall (Rec) F1 measure (F1 ) achieved nsCTBN, nsDBN,
TVDBN drosophila dataset computed respect networks inferred
Zhao et al. (2006) Guo et al. (2007). Average values (Average) precision, recall
F1 measure achieved Zhao et al. (2006) Guo et al. (2007) reported.

nsDBN
nsCTBN
TVDBN

Zhao
Prec
0.58
0.33
0.17

et al. (2006)
Rec
F1
0.38 0.46
0.37 0.35
0.27 0.21

Guo et al. (2007)
Prec Rec
F1
0.47 0.34 0.39
0.41 0.43 0.42
0.36 0.61 0.45

Prec
0.52
0.37
0.27

Average
Rec
F1
0.36 0.42
0.40 0.39
0.44 0.33

According Table 7, optimal algorithm exists reduced drosophila dataset.
network retrieved Zhao et al. (2006) used ground truth, nsDBN
best model, network retrieved Guo et al. (2007) network used ground
truth, TVDBN optimal one far F1 measure concerned. average
performance computed, nsDBN best model TVDBN worst;
nsCTBN achieves F1 value close one achieved nsDBN.
Secondly, investigated whether transition times inferred structural learning
nsCTBN UNE setting correspond known transitions stages (Lebre
et al., 2010; Dondelinger et al., 2013). network inference task performed learning
nsCTBN UNE setting following parameter values c = {0.2, 0.4, 1, 2}
e = {0.5, 1, 2, 5}. Furthermore, set maximum number parents 2,
number iterations 1,000 number runs 100.
Figure 5 shows distribution transition times11 (left) posterior
number epochs (right). number epochs correctly detected 4 even
probability close 0.1 associated 5 epochs. However, transition times
correctly identified. embryonic stage correctly identified, larval stage
correctly discovered start time-point 31, inferred end time-point 38
11. stem represents posterior probability corresponding time-point starts new epoch.
Therefore, stem time-point means epoch ends time-point 1, next epoch
starts time-point t.

24

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

instead 40. nsCTBN identify pupal adulthood stages, identified
two additional transition times (17 51). behavior observed nsDBNs,
TVDBNs capable correctly identify pupal adulthood stages. However, TVDBN-0, TVDBN-Exp TVDBN-Bino inferred networks (Dondelinger et al.,
2013) consist number epochs ranging 6 7.

mhc

mhc
gfl

gfl

mlc1

mlc1

eve

eve

msp300

msp300

actn

actn

myo61f

myo61f





prm

prm
twi

twi

sls

sls

(a) Embryonic (epoch 0 30).

(b) Larval (epoch 31 41).

mhc

mhc
gfl

gfl

mlc1

mlc1

eve

eve

msp300

msp300

actn

actn

myo61f

myo61f





prm

prm
twi

twi

sls

sls

(c) Pupal (epoch 42 59).

(d) Adulthood (epoch 60 66).

Figure 4: Networks inferred nsCTBN KKT setting reduced drosophila
dataset. arcs occurred 20 percent networks associated
different c values included inferred nsCTBN model.

25

fiVilla & Stella

Distribution transition times

Distribution number epochs

1

1
Retrieved
0.8
Posterior probability

Probability transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 66
Time

0

4
5
Number epochs

Figure 5: Transition time graph (left) posterior probability histogram number
epochs E (right) associated nsCTBN model learned drosophila reduced
dataset UNE setting c = 0.2 e = 2.

5.2.2 Saccharomyces Cerevisiae
saccharomyces cerevisiae dataset obtained synthetic regulatory network
5 genes saccharomyces cerevisiae (Cantone, Marucci, Iorio, Ricci, Belcastro, Bansal,
Santini, di Bernardo, di Bernardo, & Cosma, 2009). obtained measuring gene
expression time-series RT-PCR (reverse transcription polymerase chain reaction)
16 21 time-points two conditions related carbon source: galactose (switch
experimental condition) glucose (switch experimental condition). merged
time-series two experimental conditions exclusion boundary point
done literature (Dondelinger et al., 2013). obtained time-series binarized
way 1 indicates gene expression level greater equal
sample mean, 0 indicates gene expression level smaller sample mean.
obtained dataset used infer saccharomyces cerevisiae networks associated
switch switch experimental conditions.
network inference task performed learning nsCTBN UNE setting
following parameter values c = {0.2, 0.4, 1, 2} e = {0.2, 0.4, 1, 2}. Furthermore, set maximum number parents 4, number iterations 1,000
number runs 100. arcs occurred 50 percent runs
included inferred nsCTBN model. Precision, recall F1 measure values achieved
nsCTBN compared achieved state-of-the-art algorithms (i.e. TSNI,
nsDBN TVDBN) Table 8.
result performed numerical experiment shows nsCTBN competitive respect state-of-the-art algorithms, achieves non-optimal results
precision associated switch experimental condition. condition,
5
nsCTBN achieves precision equal 0.5 ( 10
), optimal value achieved TSNI
4
TVDBN 0.8 ( 5 ). contrary, nsCTBN achieves best recall value,
equal 0.63 ( 85 ). switch experimental condition, nsCTBN achieves best
value precision, equal 0.67 ( 69 ), recall, equal 0.75 ( 68 ).
26

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

computed overall performance structural learning algorithms.
case, focusing attention F1 measure, conclude nsCTBN (0.63)
comparable TVDBN (0.60), considered state-of-the-art algorithm
structural learning task applied saccharomyces cerevisiae dataset. networks
inferred nsCTBN model switch switch experimental conditions,
using c = 0.2 c = 2, depicted Figure 6.
Table 8: nsCTBN compared TSNI, nsDBN, TVDBN learning saccharomyces cerevisiae dataset. nsCTBN learned UNE setting (c = 0.2, e = 2);
time-point 17 used transition time switch switch experimental conditions. TSNI, nsDBN TVDBN networks described specialized
literature. Precision, recall F1 measure reported switch switch
experimental conditions. number true positive arcs (superscript) sum
true false positive arcs (subscript) reported precision, number true
positive arcs (superscript) sum true positive false negative arcs (subscript)
reported recall. Performance values achieved aggregating inferred networks
two epochs reported.

TSNI
nsDBN
TVDBN
nsCTBN

Switch
P rec
Rec
0.8045 0.5048
0.3326 0.2528
0.8045 0.5048
0.50510 0.6358

F1
0.62
0.29
0.62
0.56

Switch
P rec Rec
F1
0.6035 0.3838 0.46
0.6035 0.3838 0.46
0.5659 0.6358 0.59
0.6769 0.7568 0.71

GAL4

F1
0.54
0.37
0.60
0.63

GAL4

GAL80

CBF1

SWI5

Aggregated
P rec
Rec
0.70710 0.44716
0.45511 0.31516
0.64914 0.56916
0.5811
0.6911
19
16

GAL80

ASH1

SWI5

(a) Switch network.

CBF1

ASH1

(b) Switch network.

Figure 6: Switch (a) switch (b) networks inferred nsCTBN saccharomyces cerevisiae dataset UNE setting c = 0.2, e = 2. two pictures
report positive arcs (black continuous), false negative arcs (red dashed)
false positive arcs (green dotted) inferred networks.
27

fiVilla & Stella

Figure 7 shows posterior distribution number epochs (left) together
distribution transition times (right) nsCTBN learned c = 0.2 e = 2.
transition switch switch experimental conditions known
occur time-point 17 (i.e. switch epoch starts time-point 18). worthwhile
notice small number arcs, associated synthetic regulatory network
saccharomyces cerevisiae, suggests one careful evaluating
result performed numerical experiment. particular, think overstatements
effectiveness and/or superiority different structural learning algorithms
learning task saccharomyces cerevisiae dataset avoided.
Distribution transition times

Distribution number epochs

1

1
Retrieved
0.8
Posterior probability

Probability transition

0.8

0.6

0.4

0.2

0

0.6

0.4

0.2

0

2

4

6

8

10 12 14 16 18 20 22 24 26 28 30 32 34 36
Time

0

2
Number epochs

Figure 7: Transition time graph (left) posterior probability number epochs
(right) associated nsCTBN inferred saccharomyces cerevisiae dataset
UNE setting c = 0.2 e = 2. maximum aposteriori estimate
number epochs associated E = 2 epochs: epoch 1 starts time-point 1
ends time-point 17, epoch 2 starts time-point 18 ends time-point 36.

5.2.3 Songbird
songbird dataset collected eight electrodes placed vocal nuclei six
female zebra finches (Smith et al., 2006). Voltage changes recorded populations
neurons birds provided four different two-second auditory stimuli,
presented 18 20 times. Voltages post-processed root mean square
transformation binned 5 ms (Robinson & Hartemink, 2010).
songbird dataset used learn neural information flow networks, i.e. networks
represent transmission information different regions songbird
brain. neural information flow network represents dynamic utilization potential
pathways along information travel. identification neural information
flow networks songbirds auditory stimuli allows understand sounds
stored processed songbirds brain. songbird dataset consists data
8 variables recorded electrodes two seconds pre-stimulus, two seconds
stimulus two seconds post-stimulus six birds. stimuli hear-song, i.e.
bird hears another bird singing, white-noise, i.e. bird hears white noise stimulus.
28

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

show results nsCTBN learned two six birds songbird
dataset, namely bird 648 bird 841. results obtained four birds
similar. Given nsCTBNs based discrete variables, values 8 variables
discretized three bins using uniform quantiles (0, 13 , 23 , 1) according literature
(Robinson & Hartemink, 2010). inference task neural information flow networks
performed learning nsCTBN UNE setting following parameter
values c = {0.25, 0.5, 1, 2, 5, 10} e = {0.25, 0.5, 1, 2, 5, 10}. set maximum
number parents 3, number iterations 500 number runs 10.
Figure 8 (a) (b) show probability transition (left) posterior probability
number epochs (right) bird 648 bird 841 white-noise stimulus.
Figure 9 (a) (b) show probability transition (left) posterior probability
number epochs (right) bird 648 bird 841 hear-song stimulus.
Distribution transition times

Distribution number epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

0

6

3
4
Number epochs

(a) white-noise stimulus bird 648: learned model results.
Distribution transition times

Distribution number epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

6

0

3
4
Number epochs

(b) white-noise stimulus bird 841: learned model results.

Figure 8: Distribution transition times posterior distribution epochs
nsCTBN UNE setting songbird dataset white-noise stimulus.
29

fiVilla & Stella

location transition time-points white-noise stimulus hearsong stimulus accurately inferred bird 648 bird 841. posterior distribution
number epochs birds 648 841 white-noise stimulus nearly
equally split 3 4 epochs, hear-song stimulus peaked 3
epochs. Therefore, number epochs location transition time-points
reliably recovered nsCTBN learned UNE setting. Unfortunately,
able find additional information validate learned nsCTBNs
dataset. Moreover, comparison across different birds eventually develop consensus
network possible due songbird data collection settings. Indeed, six
birds characterized electrodes, make difficult obtain correspondence
map across different birds.

Distribution transition times

Distribution number epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

0

6

3
4
Number epochs

(a) hear-song stimulus bird 648: learned model results.
Distribution transition times

Distribution number epochs

0.5

1
Retrieved
0.8
Posterior probability

Probability transition

0.4

0.3

0.2

0.1

0

0.6

0.4

0.2

0

0.5

1

1.5

2

2.5

3
Time

3.5

4

4.5

5

5.5

6

0

3
4
Number epochs

(b) hear-song stimulus bird 841: learned model results.

Figure 9: Distribution transition times posterior distribution epochs
nsCTBN UNE setting songbird dataset hear-song stimulus.

30

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

5.2.4 Macroeconomics
macroeconomics dataset consists 17 financial/economic time-series pertaining
economy United States. Time-series different time granularity span
1st January 1986 31st March 2015. specifically, five time-series daily granularity, namely Crude oil (OIL), USD EUR spot exchange rate (USDEUR), Gold (GOLD),
S&P500 equity index (S&P500) 10-years treasury bond yield rate (US10yrsNote).
Eleven time-series monthly granularity, namely production total industry (PTI),
real manufacturing trade industries sales (RMTIS), personal income (PI), unemployment (UN), consumer price index (CPI), federal funds rate (RATE), producer price index
(PPI), non-farm payrolls (NFP), new one-family houses sold (NHSold), new houses sale
(NHSale) new private house permits (NHPermit). Finally, gross domestic product
(GDP) time-series quarterly granularity.
goal study discover financial economic environment evolves
time. particular, focused attention detect business cycles12
associated change relationships among financial economic variables. Given
duration business cycle highly variable, ability identify turning point
cycle (i.e. recession starts) considerable importance policymakers, financial
companies well individuals. substantial literature available business
cycle turning points detection generally relying Markov-switching models (Hamilton &
Raj, 2005). However, models able represent important features
dependence structure among variables business cycle.
order use nsCTBN model context, applied binary discretization
variable associated time-series. Discretization performed using lookback period 1 year, i.e. current value greater past one, binary
variable set 1 otherwise, set 0. approach looking back past
widely used finance (Moskowitz, Ooi, & Pedersen, 2012). nsCTBNs learning
performed UNE setting using following parameter values: c = {0.5, 1, 2},
e = {0.1, 1, 10}, 2 maximum parents per node, 300 iterations 10 runs.
Figure 10 shows probability transition (left side, left axis) versus S&P500
equity index used reference (left side, right axis) posterior probability
number epochs (right side). nsCTBN consists three epochs transition times
close end July 2000 end November 2007. compare dates
turning points US business cycle reported National Bureau Economic
Research13 , see far turning point March 2001
close one December 2007, missed turning point occurred
July 1990, probably limited length dataset.
Figure 11 shows structure nsCTBN model corresponding probable
number epochs, i.e. E = 3. arc included nsCTBN model occurs
75% performed runs epoch. retrieved networks correspond
following time periods: January 1986 July 2000 (epoch 1), August 2000
November 2007 (epoch 2) December 2007 March 2015 (epoch 3).
12. Business cycles fluctuations aggregate economic activity, recurrent (i.e. possible
identify expansion-recession cycles), persistent periodic (i.e. differ length severity).
13. official business cycle turning points dates available http://www.nber.org/cycles.html

31

fiVilla & Stella

Distribution transition times vs S&P500

Distribution number epochs
2500

1

0.8

2000

0.8

0.6

1500

0.4

1000

0.2

500

1

Posterior probability

Value

Probability transition

Retrieved (left)
S&P500 (right)

0
1985

1990

1995

2000
2005
Time

2010

0.4

0.2

0
2020

2015

0.6

0

2

3
4
5
Number epochs

Figure 10: Distribution transition times S&P500 behavior time (left). Posterior probability epochs (right) learned nsCTBN UNE setting.
USDEUR

OIL
GOLD

USDEUR
OIL

UN

GOLD

US10YRS

US10YRS

PPI

NHPer

CPI

PPI

NHPer

PTI

SP500

PTI

SP500

PI
RMTIS

NHSale

RATE

PI

CPI

RMTIS

RATE
NFP

GDP
NHSold
NFP

GDP
UN

NHSale

(a) Epoch 1 (Jan 1986 - Jul 2000).

NHSold

(b) Epoch 2 (Aug 2000 - Nov 2007).
USDEUR
OIL

PPI

US10YRS

GOLD
NHSale

SP500
PTI

CPI

RATE

NHPer
PI
RMTIS
NFP
UN
GDP

NHSold

(c) Epoch 3 (Dec 2007 - Mar 2015).

Figure 11: nsCTBN learned macroeconomics dataset UNE setting.
nsCTBN corresponds probable number epochs (E = 3). arc included
nsCTBN model occurs 75% runs epoch.

32

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

novelty approach economic analysis opens door many considerations new speculations economic variables business cycles.
paper, highlight two patterns emerging learned nsCTBN model: well
known relevant role personal income (PI) relation unemployment (UN)
(Mankiw, 2014) less known relation non-farm payrolls (NFP) S&P500
equity index (S&P500) (Miao, Ramchander, & Zumwalt, 2014).

6. Conclusions
introduced non-stationary continuous time Bayesian networks developed three
structural learning algorithms used different knowledge settings (i.e. KTT,
KNE UNE) problem analyzed. structural learning algorithm
known transition times case exact exploits graph theory infer optimal
nsCTBNs structure. polynomial time complexity assumption
maximum number parents node fixed. nsCTBNs structural learning algorithms competitive state-of-the-art algorithms synthetic real-world
datasets considered. statement proved rich set numerical experiments.
nsCTBNs adapted use different score metrics, far considered score
metrics integrates non-structural parameters. nsCTBNs exploit interesting
property CTBNs offer possibility learn optimal nsCTBNs structure
single variable. could extremely useful case non-stationary
behavior analyzed system synchronous, thus may case
node changes parents independently nodes change parents set.
However, two main limitations exist nsCTBNs: i) variables assumed
discrete; specifically variable dataset must take value countable number
states ii) finding optimal value c e hyperparameters extremely
difficult (the true nsDBNs). Concerning i), problem discretizing continuous
variables studied long time robust solutions described
specialized literature. Discretizing continuous variables whose value measured time
studied intensively many issues still remain. problem ii) selecting
optimal value hyperparameters known specialized literature much
done experts provide valuable apriori knowledge. However, apriori
knowledge poor available all, selecting optimal hyperparameter values
extremely difficult. important note one strong limitations studying
comparing non-stationary models lack ground truth models.
Possible directions research include application nsCTBNs structural
learning algorithms datasets, arabidopsis thaliana dataset (Grzegorczyk,
Aderhold, & Husmeier, 2015) well financial datasets supported in-depth
economic analyses. Another interesting perspective study development
modeling approach, going towards direction allowing node change parents
set asynchronously. Furthermore, think increase applicability real-world
time-series data proposed nsCTBNs structural learning algorithms issue timeseries discretization must addressed. particular, think issue must
addressed integrated manner nsCTBNs structural learning algorithm.
33

fiVilla & Stella

Finally, could interesting apply framework nsCTBNs address
task classification objects streaming context using probabilistic graphical model based approach (Borchani, Martinez, Masegosa, Langseth, Nielsen, Salmeron,
Fernandez, Madsen, & Saez, 2015a; Borchani, Martnez, Masegosa, Langseth, Nielsen,
Salmeron, Fernandez, Madsen, & Saez, 2015b).

Acknowledgments
authors wish thank Alexander Hartemink kindly provided nsDBN
jar executable associated datasets. special thank goes Marco Grzegorczyk
providing arabidopsis thaliana dataset together fundamental information analyze
it. authors greatly indebted anonymous referees constructive comments
extremely helpful suggestions, contributed significantly improve
quality paper. special thank goes Associate Editor Manfred Jaeger.
Fabio Stella corresponding author article.

References
Acerbi, E., & Stella, F. (2014). Continuous time bayesian networks gene network reconstruction: comparative study time course data. 10th International
Symposium Bioinformatics Research Applications, Zhangjiajie, China, 2014,
10.
Acerbi, E., Vigano, E., Poidinger, M., Mortellaro, A., Zelante, T., & Stella, F. (2016).
Continuous time bayesian networks identify prdm1 negative regulator th17 cell
differentiation humans. Scientific Reports, 6, 23128.
Acerbi, E., Zelante, T., Narang, V., & Stella, F. (2014). Gene network inference using
continuous time bayesian networks: comparative study application th17 cell
differentiation. BMC Bioinformatics, 15 (1).
Ahmed, A., & Xing, E. P. (2009). Recovering time-varying networks dependencies social
biological studies. Proceedings National Academy Sciences, 106 (29),
1187811883.
Bertsimas, D., & Tsitsiklis, J. (1993). Simulated annealing. Statistical Science, 8 (1), 1015.
Borchani, H., Martinez, A. M., Masegosa, A., Langseth, H., Nielsen, T. D., Salmeron, A.,
Fernandez, A., Madsen, A. L., & Saez, R. (2015a). Dynamic Bayesian modeling
risk prediction credit operations. 13th Scandinavian Conference Artificial
Intelligence (SCAI 2015), Halmstad, Sweden.
Borchani, H., Martnez, A. M., Masegosa, A. R., Langseth, H., Nielsen, T. D., Salmeron,
A., Fernandez, A., Madsen, A. L., & Saez, R. (2015b). Modeling concept drift:
probabilistic graphical model based approach. 14th International Symposium
Intelligent Data Analysis (IDA 2015), Saint-Etienne, France.
Boudali, H., & Dugan, J. B. (2006). continuous-time bayesian network reliability modeling, analysis framework. IEEE Transactions Reliability, 55 (1), 8697.
34

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

Burge, J., Lane, T., Link, H., Qiu, S., & Clark, V. P. (2009). Discrete dynamic bayesian
network analysis fmri data. Human brain mapping, 30 (1), 122137.
Cantone, I., Marucci, L., Iorio, F., Ricci, M. A., Belcastro, V., Bansal, M., Santini, S.,
di Bernardo, M., di Bernardo, D., & Cosma, M. P. (2009). yeast synthetic network
vivo assessment reverse-engineering modeling approaches. Cell, 137 (1),
172 181.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Comput. Intell., 5 (3), 142150.
Dondelinger, F., Lebre, S., & Husmeier, D. (2013). Non-homogeneous dynamic bayesian
networks bayesian regularization inferring gene regulatory networks
gradually time-varying structure. Machine Learning, 90 (2), 191230.
Durante, D., & Dunson, D. B. (2014). Bayesian dynamic financial networks timevarying predictors. Statistics & Probability Letters, 93, 1926.
Fan, Y., & Shelton, C. R. (2009). Learning continuous-time social network dynamics.
25th Conference Uncertainty Artificial Intelligence (UAI 2009), Montreal,
Canada.
Friedman, N., & Koller, D. (2000). bayesian bayesian network structure:
bayesian approach structure discovery bayesian networks. Machine Learning,
50, 95125.
Gatti, E., Luciani, D., & Stella, F. (2011). continuous time bayesian network model
cardiogenic heart failure. Flexible Services Manufacturing Journal, 24 (2),
496515.
Geiger, D., & Heckerman, D. (1997). characterization dirchlet distributions
local global independence. Annals Statistics, 25, 13441368.
Grzegorczyk, M., Aderhold, A., & Husmeier, D. (2015). Inferring bi-directional interactions circadian clock genes metabolism model ensembles. Statistical
Applications Genetics Molecular Biology, 14 (2), 143167.
Guo, F., Hanneke, S., Fu, W., & Xing, E. P. (2007). Recovering temporally rewiring networks: model-based approach. Machine Learning, Proceedings 24th International Conference (ICML 2007), Corvallis, USA, June 20-24, 2007, pp. 321328.
Hamilton, J. D., & Raj, B. (Eds.). (2005). Advances Markov-Switching Models: Applications Business Cycle Research Finance. Studies Empirical Economics.
Springer-Verlag.
Herbrich, R., Graepel, T., & Murphy, B. (2007). Structure failure. 2nd USENIX
workshop Tackling computer systems problems machine learning techniques
(SYSML 07), Cambridge, USA, pp. 16.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing.
Science, 220 (4598), 671680.
Lebre, S., Becq, J., Devaux, F., Stumpf, M., & Lelandais, G. (2010). Statistical inference
time-varying structure gene regulation networks. BMC Systems Biology, 4 (1),
130+.
35

fiVilla & Stella

Liu, M., Hommersom, A., van der Heijden, M., & Lucas, P. J. (2016). Hybrid time bayesian
networks. International Journal Approximate Reasoning, .
Mankiw, N. G. (2014). Principles Macroeconomics (7th edition). South-Western College
Pub.
Marini, S., Trifoglio, E., Barbarini, N., Sambo, F., Camillo, B. D., Malovini, A., Manfrini,
M., Cobelli, C., & Bellazzi, R. (2015). dynamic bayesian network model longterm simulation clinical complications type 1 diabetes. Journal Biomedical
Informatics, 57, 369 376.
Miao, H., Ramchander, S., & Zumwalt, J. K. (2014). S&p 500 index-futures price jumps
macroeconomic news. Journal Futures Markets, 34 (10), 9801001.
Moskowitz, T. J., Ooi, Y. H., & Pedersen, L. H. (2012). Time series momentum. Journal
Financial Economics, 104 (2), 228250.
Mumford, J. A., & Ramsey, J. D. (2014). Bayesian networks fmri: primer. Neuroimage,
86, 573582.
Murphy, K. P. (2012). Machine Learning: Probabilistic Perspective. MIT Press.
Nodelman, U. (2007). Continuous Time Bayesian Networks. Ph.D. thesis, Stanford University.
Nodelman, U., & Horvitz, E. (2003). Continuous time bayesian networks inferring users
presence activities extensions modeling evaluation. Tech. rep. MSRTR-2003-97, Microsoft Research.
Nodelman, U., Shelton, C. R., & Koller, D. (2002). Continuous time bayesian networks.
18th Conference Uncertainty Artificial Intelligence (UAI 2002), Edmonton,
Canada, pp. 378387.
Nodelman, U., Shelton, C., & Koller, D. (2003). Learning continuous time bayesian networks. 19th Conference Uncertainty Artificial Intelligence (UAI 2003),
Acapulco, Mexico, pp. 451458.
Pearl, J. (1989). Probabilistic reasoning intelligent systems - networks plausible inference. Morgan Kaufmann series representation reasoning. Morgan Kaufmann.
Robinson, J. W., & Hartemink, A. J. (2010). Learning non-stationary dynamic bayesian
networks. Journal Machine Learning Research, 11, 36473680.
Scutari, M., & Denis, J.-B. (2014). Bayesian Networks Examples R. Chapman
Hall, Boca Raton. ISBN 978-1482225587.
Segal, E., Peer, D., Regev, A., Koller, D., & Friedman, N. (2005). Learning module networks. Journal Machine Learning Research, 6, 557588.
Smith, A. V., Yu, J., Smulders, T. V., Hartemink, A. J., & Jarvis, E. D. (2006). Computational Inference Neural Information Flow Networks. PLoS Computational Biology,
2 (11), e161+.
Spiegelhalter, D. J., & Lauritzen, S. L. (1990). Sequential updating conditional probabilities directed graphical structures. Networks, 20 (5), 579605.
36

fiLearning Continuous Time Bayesian Networks Non-stationary Domains

Sturlaugson, L., & Sheppard, J. W. (2014). Inference complexity continuous time bayesian
networks. 30th Conference Uncertainty Artificial Intelligence (UAI
2014), Quebec City, Canada, pp. 772779.
Vinh, N. X., Chetty, M., Coppel, R., & Wangikar, P. P. (2012). Gene regulatory network
modeling via global optimization high-order dynamic bayesian network. BMC
Bioinformatics, 13, 131.
Xu, J., & Shelton, C. R. (2008). Continuous time bayesian networks host level network
intrusion detection. European Conference Machine Learning Principles
Practice Knowledge Discovery Databases (ECML PKDD 2008), Antwerp,
Belgium, pp. 613627.
Zhao, W., Serpedin, E., & Dougherty, E. R. (2006). Inferring gene regulatory networks
time series data using minimum description length principle. Bioinformatics,
22 (17), 21292135.
Zou, M., & Conzen, S. D. (2005). new dynamic bayesian network (dbn) approach identifying gene regulatory networks time course microarray data. Bioinformatics,
21 (1), 7179.

37


