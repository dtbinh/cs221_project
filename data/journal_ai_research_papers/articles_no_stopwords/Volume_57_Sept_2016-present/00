journal artificial intelligence

submitted published

learning continuous time bayesian networks
non stationary domains
simone villa
fabio stella

villa disco unimib
stella disco unimib

department informatics systems communication
university milano bicocca
viale sarca milan italy

abstract
non stationary continuous time bayesian networks introduced allow
parents set node change continuous time three settings developed
learning non stationary continuous time bayesian networks data known transition
times known number epochs unknown number epochs score function
setting derived corresponding learning developed set numerical
experiments synthetic data used compare effectiveness non stationary continuous time bayesian networks non stationary dynamic bayesian networks furthermore performance achieved non stationary continuous time bayesian networks
compared achieved state art four real world datasets
namely drosophila saccharomyces cerevisiae songbird macroeconomics

introduction
identification relationships statistical dependencies components multivariate time series ability reasoning whether dependencies
change time crucial many domains biology economics finance
traffic engineering neurology mention biology example knowing
gene regulatory network allows understand complex biological mechanisms ruling
cell context bayesian networks bns pearl segal peer regev koller
friedman scutari denis dynamic bayesian networks dbns dean
kanazawa zou conzen vinh chetty coppel wangikar
continuous time bayesian networks ctbns nodelman shelton koller acerbi
zelante narang stella used reconstruct transcriptional regulatory
networks gene expression data effectiveness discrete dbns investigated identify functional correlations among neuroanatomical regions interest burge
lane link qiu clark useful primer bns functional magnetic resonance imaging data analysis made available mumford ramsey however
mentioned applications require time series generated stationary distribution e one change time stationarity reasonable
assumption many situations cases data generating process clearly
non stationary indeed last years researchers different disciplines ranging
economics computational biology sociology medicine become interested
representing relationships dependencies change time
c

ai access foundation rights reserved

fivilla stella

specifically researchers interested analyzing temporal evolution
genetic networks lebre becq devaux stumpf lelandais flow neural
information networks smith yu smulders hartemink jarvis heart failure liu
hommersom van der heijden lucas complications type diabetes marini
trifoglio barbarini sambo camillo malovini manfrini cobelli bellazzi
dependence structure among financial markets crisis durante dunson
according specialized literature evolution robinson hartemink
divided two main categories structurally non stationary e
allowed change structure time parametrically non stationary
e allow parameters values change time
structurally non stationary continuous time bayesian network model
nsctbn introduced nsctbn consists sequence ctbns improves expressiveness single ctbn indeed nsctbn allows parents set node
change time specific transition times thus allows model non stationary systems learn nsctbn bayesian score learning ctbns extended nodelman
shelton koller nsctbn version bayesian score still decomposable
variable depends knowledge setting known transition times
transition times known known number epochs number transition times known unknown number epochs number transition times
unknown learning knowledge setting designed developed
experiments non stationary dynamic bayesian networks nsdbns robinson
hartemink e discrete time counterparts nsctbns performed
main contributions following
definition structurally non stationary continuous time bayesian network model
derivation bayesian score decomposition knowledge setting
design learning nsctbns different knowledge settings
novel dynamic programming learning nsctbns known
transition times setting described learning nsctbns others settings
performed simulated annealing exploiting dynamic programming
performance comparison nsctbns nsdbns knowledge settings
rich set synthetic data generated nsctbns nsdbns
performance comparison nsctbns state art realworld datasets namely drosophila saccharomyces cerevisiae songbird
nsctbn learned macroeconomics dataset consisting variables evolving
different time granularities spanning st january st march
rest organized follows section continuous time bayesian networks introduced together learning complete data section
introduces non stationary continuous time bayesian networks presents three learning settings derives corresponding bayesian score functions learning
nsctbns different learning settings described section numerical experiments synthetic real world datasets presented section section closes
making conclusions indicating directions activities


filearning continuous time bayesian networks non stationary domains

continuous time bayesian networks
continuous time bayesian networks combine bayesian networks homogeneous markov
processes together efficiently model discrete state continuous time dynamical systems
nodelman et al particularly useful modeling domains variables evolve different time granularities model presence people
computers nodelman horvitz study reliability dynamical systems boudali
dugan model failures server farms herbrich graepel murphy
detect network intrusion xu shelton analyze social networks fan shelton
model cardiogenic heart failure gatti luciani stella reconstruct
gene regulatory networks acerbi stella acerbi vigano poidinger mortellaro
zelante stella recently complexity inference continuous time bayesian
networks studied sturlaugson sheppard
basics
representation ability continuous time bayesian networks inherent factorization system dynamics local continuous time markov processes depend
limited set states continuous time bayesian network model defined follows
definition continuous time bayesian network nodelman et al let x
set random variables x x x xn x finite domain values
v al x x x xi continuous time bayesian network x consists two
specified bayesian network x
components first initial distribution px
second continuous time transition model specified directed possibly cyclic
p x
graph g whose nodes x x xn conditional intensity matrix cim qx

variable x x p x denotes set parents x graph g
p x

conditional intensity matrix qx

consists set intensity matrices

qxpa u
qxpa xu



qxpai xu


u
qpa
x







qxpa xui
qxpa xui



pau
qxi


pau ranges possible configurations parents set p x qxpai u
p
pau
pau
pau
xj xi qxi xj diagonal elements qx e qxi xj proportional probability
variable x transitions state xi state xj given parents state pau
pau
u
intensity matrix qpa
x equivalently summarized two independent sets q x
pau
qxi e set intensities parameterizing exponential distributions
pau
pau
pau
u
next transition occurs pa
x xi xj qxi xj qxi j j
e set probabilities parameterizing multinomial distributions
state transitions note ctbn model assumes one single variable
change state specific instant transition dynamics specified parents
via cim independent variables given markov blanket


fivilla stella

structural learning
given fully observed dataset e dataset consisting multiple trajectories whose
states transition times fully known learning structure ctbn
addressed selecting graph g maximizes bayesian
score computed dataset nodelman et al
bs g ln p g ln p g



p g prior graph g p g marginal likelihood
prior p g graph g allows us prefer ctbns structures
others usually assumed satisfy structure modularity property friedman
koller e decompose following product terms

p g
p p x p ag x

xx

term parents set p ag x graph g uniform prior g often used
marginal likelihood p g depends prior parameters p q g g g
usually assumed satisfy global parameter independence local parameter independence parameter modularity properties outlined
global parameter independence spiegelhalter lauritzen states paramp x
p x
eters q x g
x g
associated variable x graph g independent
thus prior parameters decomposes variable follows

p x p x
p q g g g
p q x g x g g

xx

local parameter independence spiegelhalter lauritzen asserts parameters associated configuration pau parents p ag x variable x
independent therefore parameters associated variable x decomposable
parent configuration pau follows
yy
p x p x
u

p q x g x g g
p qxpai u pa
xi g
pau xi

parameter modularity geiger heckerman asserts variable x
parents p ag x p ag x two distinct graphs g g probability density
functions parameters associated x must identical
p ag x

p q x

p ag x

x

p ag x

g p q x

p ag x

x

g



furthermore assume sets parameters characterizing exponential distributions independent sets parameters characterizing multinomial
distributions
p q g g g p q g g p g g

trajectory defined sequence pairs x transition time
associated state x random variables corresponding nodes ctbn



filearning continuous time bayesian networks non stationary domains

dirichlet distribution selected prior parameters associated multinomial distribution gamma distribution selected prior parameters
associated exponential distribution e

p qxpai u gamma xpai u xpai u


pau
pau
pau
p xi dir xi x xi xi

xpai u xpai u xpai xu xpai xui priors hyperparameters particular hyperparameters represent pseudocounts number transitions state state
parameter represents imaginary amount time spent state
data observed note hyperparameter xpai u inversely proportional
number joint states parents x conditioning dataset obtain
following posteriors parameters

u


p qxpai u gamma xpai u mxpai u xpai u txpa


pau
pau
pau
pau
pau

p xi dir xi x mxi x xi xi mxi xi
u
mxpai xuj sufficient statistics ctbn nodelman et al
txpa

u
particular txpa
amount time spent variable x state xi

parents p x state pau mxpai xuj number times variable x
transitions state xi state xj parents p x state pau
bayesian score term p g grow size dataset
thus significant term marginal likelihood p g case complete data
exploiting parameters independence global parameter independence
property marginal likelihood written follows

p x
p x
p g
l q x g l x g


xx
p ag x

l q x

marginal likelihood q derived follows
yy
pau xi

p ag x

l x

xpai u mxpai u xpai u
u
xpai u xpai u txpa


u
pa
xi

pau
u
pa
xi mxi



marginal likelihood derived follows


xpai xuj mxpai xuj
xpai u

pau
pau
pau







x
x
x


xj
pa x x
x x
u



j







j

bayesian dirichlet equivalent bde metric version ctbns nodelman
case bde metric uses priors parameter modularity
well global local parameter independence properties assumed
satisfied
please note number times p
variable x leaves state xi parents p x
pau
u
state pau computed follows mxpa

xj xi mxi xj




fivilla stella

conclusion bayesian score computed closed form assuming
structure modularity property satisfied bde metric follows
x
p x
p x
ln p p x p ag x ln l q x g ln l x g
bs g
xx

since graph g ctbn acyclicity constraints possible maximize
bayesian score separately optimizing parents set p x variable
x worthwhile mention maximum number parents set
search optimal value bayesian score performed polynomial time
search performed enumerating possible parents set greedy
hill climbing procedure operators add delete reverse edges graph g

non stationary continuous time bayesian networks
continuous time bayesian networks structurally stationary graph
change time parametrically stationary conditional intensity matrices
change time stationarity assumptions reasonable many situations
cases data generating process intrinsically non stationary
thus ctbns longer used therefore section extend ctbns become
structurally non stationary e allow ctbns structure change continuous
time
definition
non stationary continuous time bayesian network model graph ctbn
replaced graphs sequence g g g ge graph ge represents
causal dependency structure model epoch e e model
structurally non stationary introduction graphs sequence
handle transition times common whole network node specific
following notations definitions used non stationary dynamic bayesian networks let te transition times sequence e times
causal dependency structure ge active epoch e replaced causal dependency
structure ge becomes active epoch e epoch defined period
time two consecutive transitions e epoch e active period
time starting te ending te graph ge active epoch
e differs graph ge active epoch e set edges
call set edge changes ge
figure shows graphs sequence g g g g g consisting four epochs e
transition times epoch associated set edge changes
specifically graph g differs graph g following set edge changes
g x x x x x x graph g differs graph g
following set edge changes g x x graph g differs graph
g following set edge changes g x x x x x x x x
worthwhile mention first epoch e epoch starting time ending time
associated graph g last epoch e epoch starting time te ending
time supremum considered time interval e associated graph ge



filearning continuous time bayesian networks non stationary domains

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x

x



















figure graphs sequence g g g g g nsctbn four epochs e
three transition times edges gained lost time
non stationary continuous time bayesian networks allow node
sequence parents sets parents set active given epoch therefore
introduce concept homogeneous interval h x h hm associated node
x defined union consecutive epochs parents set
p x active node x note epoch associated different
parents set equal e
non stationary continuous time bayesian network defined follows
definition structurally non stationary continuous time bayesian network let x
set random variables x xn x finite domain values v al x
x xi structurally non stationary continuous time bayesian network nns
b mns x consists two components
specified bayesian network b x
initial distribution px

non stationary continuous time transition model mns specified
sequence directed possibly cyclic graphs g ge e
e whose nodes
x xn e represents number epochs
p x

g
conditional intensity matrix qx h x
x x p ag x denotes
parents sets x g h x denotes intervals associated x

p x

g
conditional intensity matrix qx h x
consists set intensity matrices
u
qxpa h

pa
q u
x x hm



pau
qxi x hm



u
qpa
x hm







qxpa xui hm
qxpa xui hm



pau
qxi hm

one configuration pau parents set p x p ag x active
interval hm h x
u
note following equation qxpai h



p

xj xi



qxpai xuj hm still holds

fivilla stella

learning framework
learning nsctbn fully observed dataset done bayesian learning
framework taking account entire graphs sequence g nsctbns case must
specify prior probability graphs sequence g possible sequence
density measure possible values parameters q g g prior p g
likelihood p q g g g given marginal likelihood p g computed
bayesian score evaluated important note focused
recovering graphs sequence g detecting possible changes parameters
fact identify non stationarity parameters model e entries
conditional intensity matrices significant enough structural changes
graph others changes assumed small enough alter graph structure
prior probability graphs
given transition times thus number epochs e assume prior
nsctbns structure g written follows
p g p g ge p g g ge p g p g ge

equation justified assume probability distribution edge
changes function number changes performed defined
independently initial graph g knowledge particular edges
overall topology available initial network use informative prior
p g otherwise resort uniform distribution ctbns p g must satisfy structure modularity assumption prior set edge changes
p g ge defines way edges change adjacent epochs
prior probability parameters
prior parameters p q g g g selected satisfy following assumptions
independence sets parameters characterizing exponential multinomial distributions parameter modularity parameter independence latter
assumption divided three components nsctbns global parameter independence
interval parameter independence local parameter independence
global parameter independence asserts parameters associated node
nsctbns graphs sequence independent prior parameters decomposes
variable x follows

p ag x
p ag x
p q g g g
p q x h x
x h x
g

xx

interval parameter independence states parameters associated interval
active parents node independent parameters associated
x parents sets p ag x decomposable interval hm h x follows
p x

p x

g
g
p q x h x
x h x
g


hm



p x

p x

p q x hgm x hgm g



filearning continuous time bayesian networks non stationary domains

local parameter independence states parameters associated state
variable given interval independent thus parameters associated x
interval hm h x decomposable parent configuration pau follows
yy
p x p x
u
u

pa
p qxpai h
p q x hgm x hgm g
xi hm g

pau xi

ctbns case dirichlet distribution used prior parameters
multinomial distribution gamma distribution used prior parameters
u

exponential distribution sufficient statistics modified follows txpa
hm
amount time spent state x xi p x pau interval h x hm
mxpai xuj hm number transitions state x xi state x xj p x pau
p
interval h x hm let mxpai hu xj xi mxpai xuj hm number times
x leaves state xi parents p x state pau interval h x hm
marginal likelihood
given graphs sequence g transition times marginal likelihood p g
dataset computed closed form priors sufficient statistics
previously defined derive bayesian dirichlet equivalent metric nsctbns
make assumptions ctbns case parameter independence
assumption divided global interval local parameter independence
therefore marginal likelihood becomes

p ag x
p ag x
p g
l q x h x
l x h x


xx

marginal likelihood q equation calculated follows
pau


xi hm
pau
pau
u





xpai h



xi hm
xi hm

p ag x
l q x h x
pau pau


xi hm
xi hm
pau
pau
pa
hm pau xi u
xi hm txi hm
xi hm



marginal likelihood equation calculated follows




pau
pau
u
xpai h








xi xj hm
xi xj hm

p ag x




l x h x


pau
pau
pau
xi xj hm
hm pau xi xj xi hm mxi hm
xi xj

important note nsctbns pseudocounts well imaginary
amount time associated interval aspect requires careful choice
order biased towards values small intervals analyzed
possible correction weight ctbns hyperparameters quantity proportional time interval width hm hm hm denotes total time thus
nsctbns hyperparameters could defined follows
xpai xuj hm
xpai hu

hm hm

hm
hm hm
xpai u

hm
xpai xuj






fivilla stella

want control parameter priors two hyperparameters
use uniform bde nsctbns bdeu case hyperparameters
defined divided number u possible configurations
parents p x node x times cardinality domain x follows
xpai xuj hm



xpai hu



hm hm

ui
hm
hm hm

ui
hm




equations rescale hyperparameters way biased
respect epochs length equations uniform
distribution used performing numerical experiments
bayesian score decomposition
bayesian score decomposed variable information available
transition times regard three knowledge settings used derive
bayesian score namely known transition times ktt known number epochs kne
unknown number epochs une
known transition times
setting transition times known thus prior probability
graphs sequence p g decomposes equation marginal likelihood
decomposes variable x according equation
therefore bayesian score bs g written follows
bs g ln p g ln p g ge
p x

p x

g
g
ln l q x h x
ln l x h x




setting structural learning non stationary continuous time
bayesian network consists finding graph g active first epoch e
e sets edge changes g ge together corresponding parameters
values maximize bayesian score defined equation
graphs g ge selected making assumptions ways
edges change continuous time common robinson hartemink
consists assuming graphs sequence g g ge depends parameter
controls number edge changes continuous time uses
truncated geometric distribution parameter p exp c model number
parents changes occurring transition time te
x
ce
ge x

xx

variable ce counts number edge changes two consecutive graphs ge
ge parameter c controls impact number edge changes ce
score function


filearning continuous time bayesian networks non stationary domains

edge changes ge assumed mutually independent probability
edge changes subsequent epochs written follows
p g ge

e

e

e

exp c exp c ce

exp c ce
exp c cmax



e

cmax truncation term therefore assume truncated geometric distribution number parents changes occurring transition times equation
holds bayesian score decomposes variable x follows
bs g

x

ln p p x p ag x c

xx
p x

g
ln l q x h x


e
x

ce
e
p ag x
ln l x h x




worthwhile notice number parents changes ce epoch e
penalizes bayesian score thus discourages sudden variations parents set
consecutive epochs parameter c controls impact changes
score function
known number epochs
transition times unknown bayesian score written follows
bs g ln p g ln p g



assuming p g p g p bayesian score becomes
bs g ln p g ln p ln p g



number epochs e known prior probability p g graphs
sequence g decomposes equation truncated geometric distribution
used number parents changes occurring transition time
known transition times setting
choice p made include prior knowledge set transition
times however information available uniform prior p used implying
possible values transition times equally likely given number epochs
e thus bayesian score decomposed variable x follows
bs g ln p


x

ln p p x p ag x c

xx
p ag x
ln l q x h x

e
x

ce

e
p x

g
ln l x h x




ce counts number edge changes two consecutive parents sets c
controls impacts bs g edge changes happens ktt
setting


fivilla stella

unknown number epochs
number epochs e unknown transition times unknown well
setting learn nsctbn exploiting introduced ktt
kne settings assume structure non stationary continuous time
bayesian network evolve different speeds continuous time assumption
incorporated truncated geometric distribution parameter p exp e
number epochs general large values e encode strong prior belief
structure nsctbn changes slowly e epochs exist
following presented ktt setting bayesian score obtained
subtracting parameter e times number epochs e therefore bayesian
score bs g decomposes variable x follows
bs g ln p e e

x

ln p p x p ag x c

ce

e

xx
p x

e
x

p x

g
g
ln l q x h x
ln l x h x




note bayesian score contains two parameters namely c e
encode prior belief structure nsctbn specifically parameter c
regulates prior belief smoothness edge changes e g encouraging
discouraging edge changes per epoch parameter e regulates prior belief
number epochs e g encouraging discouraging creation epochs

structural learning
optimal structure nsctbns found separately maximizing components
bayesian score associated node achieved exact optimization dynamic programming transition times
given contrast number epochs known information
transition times available resort approximate techniques monte
carlo simulated annealing present exact solving structural
learning ktt setting briefly outline stochastic
solve structural learning kne setting une setting
known transition times
setting bayesian score decomposes according equation thus
optimal graphs sequence g found separately searching optimal parents sequence g x node x solve finding optimal parents sequence
g x node x consider sequence consisting intervals h x h hm
possible parents z possible parents sets optimal parents
sequence g x must compute z marginal likelihood terms associated q
one marginal likelihood term possible parents set p az x interval hm
optimization used maximum component
bayesian score associated node x


filearning continuous time bayesian networks non stationary domains

exhaustive search would prohibitive would require evaluating z scores
one possible parents sequence g x unfortunately greedy search strategy
computes parents set maximizes bayesian score interval
viable fact function counts parents changes ce binds choice
subsequent parents set e binds ge ge
however relation score variable x associated parents set
p x
p x interval hm denoted bsx hm score associated parents set
p x

p x interval hm denoted bsx hm defined recursion follows
n

p x
p x
p x p x
bsx hm max bsx hzm c cx e ln l q x hm x hm

p az

cx e ge x marginal likelihoods q grouped together
p x
score bsx hm associated parents set p x node x interval hm
introduced clarify recursion used note score depends
components score hm particular marginal likelihoods
component involved term cx e counts parents changes included
binds choice subsequent parents sets equation exploited dynamic
programming select optimal parents sequence g x node x
takes input marginal likelihoods q interval
parents set prior probability initial parents set number parents
changes parameter c ensures optimal parents sequence g x
node x corresponding optimal bayesian score core computation
z score matrix denoted sc dynamic programming recursion
dynamic programming recursion interval h defined follows
p x

sc z ln l q x hz

p x

x hz

ln p p az x p agh x



z z intervals hm recursion
n

p x p x
z
u
scm
max scm
ln l q x hzm x hzm c cx e
uz

filling score matrix sc value maxz sc z optimal bayesian score
optimal parents sequence reconstructed backwards
index matrix cost computing dynamic programming recursion z
polynomial fixed maximum number parents
selecting optimal parents sequence interesting graph representation indeed possible create graph whose nodes associated marginal
likelihoods q interval hm parents set p az x node associated interval hm linked nodes associated interval hm
arc associated weight computed difference marginal likelihoods interval hm parents set p az x cost switching
parents set interval hm parents set interval hm two special nodes
added represent start end optimal parents sequence graph
cycles thus selection optimal parents sequence node
reduced longest path start node end node directed
acyclic graph thus solved dynamic linear programming


fivilla stella

learnkttx
require matrix containing marginal likelihoods q lx z vector containing prior probability initial parents set p r z matrix containing
number parents changes c z z parameter parents changes c
ensure score matrix sc z index matrix z
initialize sc z z


z z



sc z ln lx z ln p r z

else

w z

score sc w ln lx z c c w z

score sc z

sc z score

z w

end

end

end

end
end
learning nsctbn done following following four steps procedure use
u
dataset compute variable x sufficient statistics txpa
mxpai xuj hm
hm
according given transition times ii compute marginal likelihoods
fill lx matrix iii run node x get corresponding
optimal parents sequence iv collect optimal parents sequence node x
compute corresponding cims sufficient statistics already computed step
allow intervals differ transition times e obtained
one possible unions transition times repeat learning
procedure e e cases possible speed computation
sufficient statistics aggregated intervals way read
dataset precomputed marginal likelihoods stored reused
intervals moreover computations performed parallel node
known number epochs
setting know number epochs transition times given
cannot directly apply however tentative allocation transition
times given apply obtain optimal nsctbns structure
assumption different true transition times
optimal tentative allocation e allocation close possible apply
simulated annealing sa kirkpatrick gelatt vecchi


filearning continuous time bayesian networks non stationary domains

simulated annealing iterative attempts global optimum
x given function f x stochastic search feasible region iteration
k sa assumed state xk samples proposal state x
according proposal distribution x p xk sa computes
quantity exp f x f x ct ct computational temperature
sa accepts proposal state x probability equal min concisely
sa accepts proposal state x f x f x setting xk x
accepts proposal state x f x f x probability setting xk x
probability xk xk probability e case state
sa change computational temperature reduces iterations
according cooling schedule shown one cools sufficiently slowly
probably global optimum kirkpatrick et al design
cooling schedule important part sa bertsimas tsitsiklis
possible use exponential cooling schedule defined follows
ctk ct k ct represents initial temperature typically set
cooling rate usually set close k current iteration murphy
nsctbns case state sa x associated tentative
allocation function f x bayesian score takes input
sufficient statistics parameters used run parameters
sa solves structural learning kne setting given
variable x ensuring optimal tentative allocation corresponding score
learnknex
require sufficient statistics suffstatsx prior probability p r number parents
changes c parameter c tentative allocation initial temperature ct cooling
rate number iterations iters truncation parameter z standard deviation
ensure optimal tentative allocation best bayesian score bestsc
initialize k
lx getmlx suffstatsx
bestsc learnkttx lx p r c c
k iters

tentativeallocation z

lx getmlx suffstatsx

tentsc learnkttx lx p r c c

ct ct kn



accp rob min exp bestsctentsc
ct

ur unirand

ur accp rob



currsc tentsc

end

k k
end
bestsc currsc


fivilla stella

simulated annealing parameters used include tentative allocation
initial temperature ct cooling rate number iterations iters
exponential cooling schedule moreover truncation parameter z standard deviation
used selection tentative allocation according random
procedure shown procedure selects transition time discrete
uniform distribution uniranddiscr perturbs according truncated normal
distribution stdnormrand standard deviation equal addition
point masses z z z represents truncation parameter
tentativeallocation
require tentative allocation truncation parameter z standard deviation
ensure tentative allocation
uniranddiscr

nr stdnormrand
nr z

nr z
end
nr z

nr z
end
nr


unknown number epochs
setting number epochs unknown thus structural learning
must able move across different number epochs well corresponding
transition times case used simulated annealing state
x tentative allocation function optimized f x bayesian score
shown equation cooling schedule set one used
kne setting proposal distribution differs one used kne setting
uses two additional operators namely split merge operators split
operator allows split given interval tm tm two subintervals tm tm
tm tm merge operator allows merge contiguous intervals tm tm
tm tm form wider interval tm tm tm tm tm
state obtained sampling number epochs changes ec multinoulli distribution parameters p p p p represents probability
number epochs next iteration decreased one p represents probability
number epochs next iteration increased one p represents
probability number epochs next iteration change respect
current one ec equal invoked ec equal
merge operator applied invoking ec equal
split operator applied invoking


filearning continuous time bayesian networks non stationary domains

solves structural learning nsctbn une setting
given node x ensuring optimal tentative allocation corresponding
bayesian score similar one used kne settings
uses apply split merge operators left function
returns transition time comes immediately transition time
learnunex
require sufficient statistics suffstatsx prior probability p r number parents
changes c parameter c parameter e tentative allocation initial temperature
ct cooling rate number iterations iters truncation parameter z standard deviation split probability sp merge probability mp
ensure optimal tentative allocation best bayesian score bestsc
initialize k
bestsc learnkttx getmlx suffstatsx p r c c e
k iters

splitmerge sp mp

tentativeallocation z

tentsc learnkttx getmlx suffstatsx p r c c e

ct ct kn



accp rob min exp bestsctentsc
ct

ur unirand

ur accp rob



currsc tentsc

end

k k
end
bestsc currsc
splitmerge
require tentative allocation split probability sp merge probability mp
ensure tentative allocation

p unirand
p mp

uniranddiscr


else

p mp sp

uniranddiscr

nt left tleft


nt

end
end


fivilla stella

numerical experiments
numerical experiments performed synthetic real world datasets synthetic
datasets used compare nsctbns nsdbns ktt kne une knowledge settings terms accuracy precision recall f measure following real world
datasets drosophila saccharomyces cerevisiae songbird used compare nsctbns
state art e tsni method ordinary differential equations
nsdbn robinson hartemink non homogeneous dynamic bayesian networks
bayesian regularization tvdbn dondelinger lebre husmeier
une knowledge setting drosophila saccharomyces cerevisiae songbird datasets
collected fixed time intervals thus analyzed additional real world dataset consisting financial economic variables evolving different time granularities exploit
expressiveness nsctbns events occur asynchronously note performance comparison synthetic datasets benefits knowledge ground
truth apply performance comparison real world datasets
ground truth available cases comparison exploits partial
meta knowledge available specialized literature
synthetic datasets
artificially generated datasets include data sampled rich set nsdbn
e nsdbn generated datasets rich set nsctbn e nsctbn generated
datasets nsdbn nsctbn consist five nodes associated binary
ternary variables numerical experiments concern learning parents sets transition
times number epochs single node choice motivated fact
structural learning nsctbn performed single node independently
remaining ones however transition times unknown multiple parents
sets changes could make easier correctly identify times change
nsdbn generated datasets
nsdbn generated datasets sampled nsdbn associated following
number epochs e particular number epochs e different
nsdbn instances sampled obtain number datasets equal one consisting single trajectory thus synthetic datasets used learn structure
nsdbn nsctbn number ktt kne une settings
structural learning experiments performed c e
nsctbn nsdbn overall number
experiments performed particular performed number epochs
number datasets number c number experiments
ktt setting kne setting number epochs number
datasets number c number e number
experiments performed une setting
inter slice arcs allowed intra slice arcs allowed holds true nsdbn
sampled obtain nsdbn generated datasets
worthwhile mention parameters nsdbn counterparts c
e parameters nsctbn



filearning continuous time bayesian networks non stationary domains

nsdbn jar executable robinson hartemink used structural learning nsdbn set maximum number proposed networks
burn period nsdbn nsctbn learned following parameters setting iters ct z sp mp
bdeu metric furthermore nsdbn nsctbn set
maximum number parents arcs occurred percent
samples belong inferred nsdbn nsctbn accuracy acc precision p rc recall rec f measure f achieved nsdbn nsctbn learned
ktt kne une settings reported table respectively
worthwhile mention kne une settings nsdbns nsctbns
almost identified correct number epochs location associated
transition times accuracy precision recall f measure computed two
different ways firstly included arcs true network epoch secondly
excluded self reference arcs e arcs connecting node two consecutive
time slices true network epoch fact node nsctbn
self reference arc default happen nsdbns means
first case nsdbn required learn arcs nsctbn required therefore
ensure fair comparison nsctbn nsdbn adopted second case tables
report performance measure values computed excluding self reference arcs
set arcs true networks epoch
table nsctbn compared nsdbn ktt setting nsdbn generated data
average min subscript max superscript performance values networks
c nsctbn nsdbn
number epochs e




acc
p rec
rec
f



nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn









































































according tables nsdbns consistently achieve greater accuracy values
achieved nsctbns three settings furthermore nsdbns
accuracy stable respect number epochs e happen
nsctbns indeed number epochs e greater nsctbns achieve
accuracy values significantly smaller achieved number
epochs e equal happen nsdbns accuracy
robust respect number epochs e
acknowledge precious help alex hartemink let us use nsdbn jar executable program
learning nsdbn furthermore provided drosophila songbird datasets
samples obtained parameters values



fivilla stella

table nsctbn compared nsdbn kne setting nsdbn generated data
average min subscript max superscript performance values networks
c nsctbn nsdbn
number epochs e




acc
p rec
rec
f



nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn









































































table nsctbn compared nsdbn une setting nsdbn generated data
average min subscript max superscript performance values networks
c e nsctbn nsdbn
number epochs e




acc
p rec
rec
f



nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn

nsdbn

nsctbn









































































different picture emerges focusing task discover positive arcs indeed
case nsctbns achieve values precision recall f measure
greater achieved nsdbns nsctbns achieve precision values robust
respect knowledge settings number epochs e
hold true recall performance measure indeed nsctbns achieve robust recall
respect knowledge settings ktt kne une recall achieved
nsctbns significantly degrades moving epochs knowledge
settings happens f measure achieved nsctbns
numerical experiments suggest nsctbns effective nsdbns discover
positive arcs even datasets generated nsdbns possible explanation
behavior learning nsdbns difficult learning nsctbns
particular nsdbns must learn self reference arcs nsctbns furthermore
node nsctbns learn locally sequence parents sets
happen nsdbns fact nsdbns learn globally sequence parents sets
nodes e globally learn sequence networks thus solve learning
difficult one solved nsctbns


filearning continuous time bayesian networks non stationary domains

nsctbn generated datasets
generated synthetic datasets e datasets used
learn structure nsctbn three knowledge settings parameters
setting used one used nsctbn learning nsdbn generated datasets
nsctbn used bdeu metric case
perform structural learning experiments nsdbn graphical structures
nsctbn sampled obtain datasets sampled
obtain nsdbn datasets goal experiments analyze performance
nsctbn structural learning three knowledge settings
analysis data reported tables brings us conclude
nsctbn structural learning work well three settings according
considered performance measures accuracy recall f measure decrease slightly
number epochs increases particular recall measure suffers
greatest decrease number epochs increases
accuracy f measure robust respect number epochs
precision robust performance measure respect different datasets
different values number epochs knowledge settings
table nsctbn ktt setting nsctbn generated data average min
subscript max superscript performance values networks c

acc
p rec
rec
f











number










epochs e




















table nsctbn kne setting nsctbn generated data average min
subscript max superscript performance values networks c

acc
p rec
rec
f











number










epochs e




















nsctbn generated data asynchronous involving different time granularities thus nsdbn cannot
directly applied option preprocess datasets adapt nsdbns given
would strongly arbitrary penalizing nsdbns decided learn nsctbn



fivilla stella

table nsctbn une setting nsctbn generated data average min
subscript max superscript performance values networks c e











acc
p rec
rec
f

number










epochs e




















best worst values accuracy e reported table belong
experiments performed synthetic dataset number number respectively
illustrated hereafter figure shows graphs sequence true nsctbn
synthetic datasets number figure b displays posterior distribution
epochs right together distribution corresponding transition times
left learned nsctbn une case figure shows information
depicted figure synthetic dataset number latter case
distribution epochs slightly favor correct number epochs

true nsctbn model
distribution transition times

distribution number epochs




true
retrieved

posterior probability

probability transition
































time
















number epochs

b learned nsctbn model

figure nsctbn generated dataset number true graphs sequence e epochs
b distribution transition times left posterior epochs right associated
nsctbn inferred une setting

transition times whose distance less aggregated



filearning continuous time bayesian networks non stationary domains

true nsctbn model
distribution transition times

distribution number epochs




true
retrieved

posterior probability

probability transition


































time



















number epochs

b learned nsctbn model

figure nsctbn generated dataset number true graphs sequence e epochs
b distribution transition times left posterior epochs right associated
nsctbn inferred une setting

real world datasets
difficult real world datasets corresponding ground truth model
completely known uniform consensus domain experts reached
therefore decided use following three well known datasets drosophila saccharomyces cerevisiae songbird compare performance nsctbns nsdbns
state art e tsni tvdbn datasets publicly
available clearly described rich detailed discussion likely ground truth
given specialized literature furthermore macroeconomics dataset introduced analyzed dataset consists financial economic variables collected
different time granularity spanning st january st march
drosophila
drosophila dataset includes mrna expression levels genes successive time points spanning four stages drosophila melanogaster life cycle lebre
et al embryonic time points larval time points pupal stage
time points first days adulthood time points comparative purposes
dondelinger et al analyzed reduced drosophila dataset consisting
gene expression time series genes involved wing muscle development given
nsctbns discrete variables binarized expression level genes
reduced drosophila dataset done literature zhao serpedin dougherty
guo hanneke fu xing robinson hartemink


fivilla stella

firstly network inference task embryonic larval pupal adulthood morphogenic stages performed ktt setting robinson hartemink dondelinger et al nsctbn structural learning performed following
parameter values c setting maximum number parents nsctbn learned different c values combined
arcs occurred percent samples included
inferred non stationary continuous time bayesian network techniques predict
non stationary directed networks robinson hartemink precision recall
f measure computed respect networks inferred zhao et al guo et
al reported table nsdbn nsctbn tvdbn dondelinger et al
networks associated four epochs inferred nsctbn
reduced drosophila dataset ktt setting depicted figure
table precision prec recall rec f measure f achieved nsctbn nsdbn
tvdbn drosophila dataset computed respect networks inferred
zhao et al guo et al average values average precision recall
f measure achieved zhao et al guo et al reported

nsdbn
nsctbn
tvdbn

zhao
prec




et al
rec
f




guo et al
prec rec
f




prec




average
rec
f




according table optimal exists reduced drosophila dataset
network retrieved zhao et al used ground truth nsdbn
best model network retrieved guo et al network used ground
truth tvdbn optimal one far f measure concerned average
performance computed nsdbn best model tvdbn worst
nsctbn achieves f value close one achieved nsdbn
secondly investigated whether transition times inferred structural learning
nsctbn une setting correspond known transitions stages lebre
et al dondelinger et al network inference task performed learning
nsctbn une setting following parameter values c
e furthermore set maximum number parents
number iterations number runs
figure shows distribution transition times left posterior
number epochs right number epochs correctly detected even
probability close associated epochs however transition times
correctly identified embryonic stage correctly identified larval stage
correctly discovered start time point inferred end time point
stem represents posterior probability corresponding time point starts epoch
therefore stem time point means epoch ends time point next epoch
starts time point



filearning continuous time bayesian networks non stationary domains

instead nsctbn identify pupal adulthood stages identified
two additional transition times behavior observed nsdbns
tvdbns capable correctly identify pupal adulthood stages however tvdbn tvdbn exp tvdbn bino inferred networks dondelinger et al
consist number epochs ranging

mhc

mhc
gfl

gfl

mlc

mlc

eve

eve

msp

msp

actn

actn

myo f

myo f





prm

prm
twi

twi

sls

sls

embryonic epoch

b larval epoch

mhc

mhc
gfl

gfl

mlc

mlc

eve

eve

msp

msp

actn

actn

myo f

myo f





prm

prm
twi

twi

sls

sls

c pupal epoch

adulthood epoch

figure networks inferred nsctbn kkt setting reduced drosophila
dataset arcs occurred percent networks associated
different c values included inferred nsctbn model



fivilla stella

distribution transition times

distribution number epochs




retrieved

posterior probability

probability transition


















time





number epochs

figure transition time graph left posterior probability histogram number
epochs e right associated nsctbn model learned drosophila reduced
dataset une setting c e

saccharomyces cerevisiae
saccharomyces cerevisiae dataset obtained synthetic regulatory network
genes saccharomyces cerevisiae cantone marucci iorio ricci belcastro bansal
santini di bernardo di bernardo cosma obtained measuring gene
expression time series rt pcr reverse transcription polymerase chain reaction
time points two conditions related carbon source galactose switch
experimental condition glucose switch experimental condition merged
time series two experimental conditions exclusion boundary point
done literature dondelinger et al obtained time series binarized
way indicates gene expression level greater equal
sample mean indicates gene expression level smaller sample mean
obtained dataset used infer saccharomyces cerevisiae networks associated
switch switch experimental conditions
network inference task performed learning nsctbn une setting
following parameter values c e furthermore set maximum number parents number iterations
number runs arcs occurred percent runs
included inferred nsctbn model precision recall f measure values achieved
nsctbn compared achieved state art e tsni
nsdbn tvdbn table
performed numerical experiment shows nsctbn competitive respect state art achieves non optimal
precision associated switch experimental condition condition

nsctbn achieves precision equal
optimal value achieved tsni

tvdbn contrary nsctbn achieves best recall value
equal switch experimental condition nsctbn achieves best
value precision equal recall equal


filearning continuous time bayesian networks non stationary domains

computed overall performance structural learning
case focusing attention f measure conclude nsctbn
comparable tvdbn considered state art
structural learning task applied saccharomyces cerevisiae dataset networks
inferred nsctbn model switch switch experimental conditions
c c depicted figure
table nsctbn compared tsni nsdbn tvdbn learning saccharomyces cerevisiae dataset nsctbn learned une setting c e
time point used transition time switch switch experimental conditions tsni nsdbn tvdbn networks described specialized
literature precision recall f measure reported switch switch
experimental conditions number true positive arcs superscript sum
true false positive arcs subscript reported precision number true
positive arcs superscript sum true positive false negative arcs subscript
reported recall performance values achieved aggregating inferred networks
two epochs reported

tsni
nsdbn
tvdbn
nsctbn

switch
p rec
rec





f





switch
p rec rec
f





gal

f





gal

gal

cbf

swi

aggregated
p rec
rec








gal

ash

swi

switch network

cbf

ash

b switch network

figure switch switch b networks inferred nsctbn saccharomyces cerevisiae dataset une setting c e two pictures
report positive arcs black continuous false negative arcs red dashed
false positive arcs green dotted inferred networks


fivilla stella

figure shows posterior distribution number epochs left together
distribution transition times right nsctbn learned c e
transition switch switch experimental conditions known
occur time point e switch epoch starts time point worthwhile
notice small number arcs associated synthetic regulatory network
saccharomyces cerevisiae suggests one careful evaluating
performed numerical experiment particular think overstatements
effectiveness superiority different structural learning
learning task saccharomyces cerevisiae dataset avoided
distribution transition times

distribution number epochs




retrieved

posterior probability

probability transition




























time




number epochs

figure transition time graph left posterior probability number epochs
right associated nsctbn inferred saccharomyces cerevisiae dataset
une setting c e maximum aposteriori estimate
number epochs associated e epochs epoch starts time point
ends time point epoch starts time point ends time point

songbird
songbird dataset collected eight electrodes placed vocal nuclei six
female zebra finches smith et al voltage changes recorded populations
neurons birds provided four different two second auditory stimuli
presented times voltages post processed root mean square
transformation binned ms robinson hartemink
songbird dataset used learn neural information flow networks e networks
represent transmission information different regions songbird
brain neural information flow network represents dynamic utilization potential
pathways along information travel identification neural information
flow networks songbirds auditory stimuli allows understand sounds
stored processed songbirds brain songbird dataset consists data
variables recorded electrodes two seconds pre stimulus two seconds
stimulus two seconds post stimulus six birds stimuli hear song e
bird hears another bird singing white noise e bird hears white noise stimulus


filearning continuous time bayesian networks non stationary domains

nsctbn learned two six birds songbird
dataset namely bird bird obtained four birds
similar given nsctbns discrete variables values variables
discretized three bins uniform quantiles according literature
robinson hartemink inference task neural information flow networks
performed learning nsctbn une setting following parameter
values c e set maximum
number parents number iterations number runs
figure b probability transition left posterior probability
number epochs right bird bird white noise stimulus
figure b probability transition left posterior probability
number epochs right bird bird hear song stimulus
distribution transition times

distribution number epochs




retrieved

posterior probability

probability transition






























time

















number epochs

white noise stimulus bird learned model
distribution transition times

distribution number epochs




retrieved

posterior probability

probability transition






























time

















number epochs

b white noise stimulus bird learned model

figure distribution transition times posterior distribution epochs
nsctbn une setting songbird dataset white noise stimulus


fivilla stella

location transition time points white noise stimulus hearsong stimulus accurately inferred bird bird posterior distribution
number epochs birds white noise stimulus nearly
equally split epochs hear song stimulus peaked
epochs therefore number epochs location transition time points
reliably recovered nsctbn learned une setting unfortunately
able additional information validate learned nsctbns
dataset moreover comparison across different birds eventually develop consensus
network possible due songbird data collection settings indeed six
birds characterized electrodes make difficult obtain correspondence
map across different birds

distribution transition times

distribution number epochs




retrieved

posterior probability

probability transition






























time

















number epochs

hear song stimulus bird learned model
distribution transition times

distribution number epochs




retrieved

posterior probability

probability transition






























time

















number epochs

b hear song stimulus bird learned model

figure distribution transition times posterior distribution epochs
nsctbn une setting songbird dataset hear song stimulus



filearning continuous time bayesian networks non stationary domains

macroeconomics
macroeconomics dataset consists financial economic time series pertaining
economy united states time series different time granularity span
st january st march specifically five time series daily granularity namely crude oil oil usd eur spot exchange rate usdeur gold gold
p equity index p years treasury bond yield rate us yrsnote
eleven time series monthly granularity namely production total industry pti
real manufacturing trade industries sales rmtis personal income pi unemployment un consumer price index cpi federal funds rate rate producer price index
ppi non farm payrolls nfp one family houses sold nhsold houses sale
nhsale private house permits nhpermit finally gross domestic product
gdp time series quarterly granularity
goal study discover financial economic environment evolves
time particular focused attention detect business cycles
associated change relationships among financial economic variables given
duration business cycle highly variable ability identify turning point
cycle e recession starts considerable importance policymakers financial
companies well individuals substantial literature available business
cycle turning points detection generally relying markov switching hamilton
raj however able represent important features
dependence structure among variables business cycle
order use nsctbn model context applied binary discretization
variable associated time series discretization performed lookback period year e current value greater past one binary
variable set otherwise set looking back past
widely used finance moskowitz ooi pedersen nsctbns learning
performed une setting following parameter values c
e maximum parents per node iterations runs
figure shows probability transition left side left axis versus p
equity index used reference left side right axis posterior probability
number epochs right side nsctbn consists three epochs transition times
close end july end november compare dates
turning points us business cycle reported national bureau economic
see far turning point march
close one december missed turning point occurred
july probably limited length dataset
figure shows structure nsctbn model corresponding probable
number epochs e e arc included nsctbn model occurs
performed runs epoch retrieved networks correspond
following time periods january july epoch august
november epoch december march epoch
business cycles fluctuations aggregate economic activity recurrent e possible
identify expansion recession cycles persistent periodic e differ length severity
official business cycle turning points dates available http www nber org cycles html



fivilla stella

distribution transition times vs p

distribution number epochs
























posterior probability

value

probability transition

retrieved left
p right










time





















number epochs

figure distribution transition times p behavior time left posterior probability epochs right learned nsctbn une setting
usdeur

oil
gold

usdeur
oil

un

gold

us yrs

us yrs

ppi

nhper

cpi

ppi

nhper

pti

sp

pti

sp

pi
rmtis

nhsale

rate

pi

cpi

rmtis

rate
nfp

gdp
nhsold
nfp

gdp
un

nhsale

epoch jan jul

nhsold

b epoch aug nov
usdeur
oil

ppi

us yrs

gold
nhsale

sp
pti

cpi

rate

nhper
pi
rmtis
nfp
un
gdp

nhsold

c epoch dec mar

figure nsctbn learned macroeconomics dataset une setting
nsctbn corresponds probable number epochs e arc included
nsctbn model occurs runs epoch



filearning continuous time bayesian networks non stationary domains

novelty economic analysis opens door many considerations speculations economic variables business cycles
highlight two patterns emerging learned nsctbn model well
known relevant role personal income pi relation unemployment un
mankiw less known relation non farm payrolls nfp p
equity index p miao ramchander zumwalt

conclusions
introduced non stationary continuous time bayesian networks developed three
structural learning used different knowledge settings e ktt
kne une analyzed structural learning
known transition times case exact exploits graph theory infer optimal
nsctbns structure polynomial time complexity assumption
maximum number parents node fixed nsctbns structural learning competitive state art synthetic real world
datasets considered statement proved rich set numerical experiments
nsctbns adapted use different score metrics far considered score
metrics integrates non structural parameters nsctbns exploit interesting
property ctbns offer possibility learn optimal nsctbns structure
single variable could extremely useful case non stationary
behavior analyzed system synchronous thus may case
node changes parents independently nodes change parents set
however two main limitations exist nsctbns variables assumed
discrete specifically variable dataset must take value countable number
states ii finding optimal value c e hyperparameters extremely
difficult true nsdbns concerning discretizing continuous
variables studied long time robust solutions described
specialized literature discretizing continuous variables whose value measured time
studied intensively many issues still remain ii selecting
optimal value hyperparameters known specialized literature much
done experts provide valuable apriori knowledge however apriori
knowledge poor available selecting optimal hyperparameter values
extremely difficult important note one strong limitations studying
comparing non stationary lack ground truth
possible directions include application nsctbns structural
learning datasets arabidopsis thaliana dataset grzegorczyk
aderhold husmeier well financial datasets supported depth
economic analyses another interesting perspective study development
modeling going towards direction allowing node change parents
set asynchronously furthermore think increase applicability real world
time series data proposed nsctbns structural learning issue timeseries discretization must addressed particular think issue must
addressed integrated manner nsctbns structural learning


fivilla stella

finally could interesting apply framework nsctbns address
task classification objects streaming context probabilistic graphical model borchani martinez masegosa langseth nielsen salmeron
fernandez madsen saez borchani martnez masegosa langseth nielsen
salmeron fernandez madsen saez b

acknowledgments
authors wish thank alexander hartemink kindly provided nsdbn
jar executable associated datasets special thank goes marco grzegorczyk
providing arabidopsis thaliana dataset together fundamental information analyze
authors greatly indebted anonymous referees constructive comments
extremely helpful suggestions contributed significantly improve
quality special thank goes associate editor manfred jaeger
fabio stella corresponding author article

references
acerbi e stella f continuous time bayesian networks gene network reconstruction comparative study time course data th international
symposium bioinformatics applications zhangjiajie china

acerbi e vigano e poidinger mortellaro zelante stella f
continuous time bayesian networks identify prdm negative regulator th cell
differentiation humans scientific reports
acerbi e zelante narang v stella f gene network inference
continuous time bayesian networks comparative study application th cell
differentiation bmc bioinformatics
ahmed xing e p recovering time varying networks dependencies social
biological studies proceedings national academy sciences

bertsimas tsitsiklis j simulated annealing statistical science
borchani h martinez masegosa langseth h nielsen salmeron
fernandez madsen l saez r dynamic bayesian modeling
risk prediction credit operations th scandinavian conference artificial
intelligence scai halmstad sweden
borchani h martnez masegosa r langseth h nielsen salmeron
fernandez madsen l saez r b modeling concept drift
probabilistic graphical model th international symposium
intelligent data analysis ida saint etienne france
boudali h dugan j b continuous time bayesian network reliability modeling analysis framework ieee transactions reliability


filearning continuous time bayesian networks non stationary domains

burge j lane link h qiu clark v p discrete dynamic bayesian
network analysis fmri data human brain mapping
cantone marucci l iorio f ricci belcastro v bansal santini
di bernardo di bernardo cosma p yeast synthetic network
vivo assessment reverse engineering modeling approaches cell

dean kanazawa k model reasoning persistence causation
comput intell
dondelinger f lebre husmeier non homogeneous dynamic bayesian
networks bayesian regularization inferring gene regulatory networks
gradually time varying structure machine learning
durante dunson b bayesian dynamic financial networks timevarying predictors statistics probability letters
fan shelton c r learning continuous time social network dynamics
th conference uncertainty artificial intelligence uai montreal
canada
friedman n koller bayesian bayesian network structure
bayesian structure discovery bayesian networks machine learning

gatti e luciani stella f continuous time bayesian network model
cardiogenic heart failure flexible services manufacturing journal

geiger heckerman characterization dirchlet distributions
local global independence annals statistics
grzegorczyk aderhold husmeier inferring bi directional interactions circadian clock genes metabolism model ensembles statistical
applications genetics molecular biology
guo f hanneke fu w xing e p recovering temporally rewiring networks model machine learning proceedings th international conference icml corvallis usa june pp
hamilton j raj b eds advances markov switching applications business cycle finance studies empirical economics
springer verlag
herbrich r graepel murphy b structure failure nd usenix
workshop tackling computer systems machine learning techniques
sysml cambridge usa pp
kirkpatrick gelatt c vecchi p optimization simulated annealing
science
lebre becq j devaux f stumpf lelandais g statistical inference
time varying structure gene regulation networks bmc systems biology



fivilla stella

liu hommersom van der heijden lucas p j hybrid time bayesian
networks international journal approximate reasoning
mankiw n g principles macroeconomics th edition south western college
pub
marini trifoglio e barbarini n sambo f camillo b malovini manfrini
cobelli c bellazzi r dynamic bayesian network model longterm simulation clinical complications type diabetes journal biomedical
informatics
miao h ramchander zumwalt j k p index futures price jumps
macroeconomic news journal futures markets
moskowitz j ooi h pedersen l h time series momentum journal
financial economics
mumford j ramsey j bayesian networks fmri primer neuroimage

murphy k p machine learning probabilistic perspective mit press
nodelman u continuous time bayesian networks ph thesis stanford university
nodelman u horvitz e continuous time bayesian networks inferring users
presence activities extensions modeling evaluation tech rep msrtr microsoft
nodelman u shelton c r koller continuous time bayesian networks
th conference uncertainty artificial intelligence uai edmonton
canada pp
nodelman u shelton c koller learning continuous time bayesian networks th conference uncertainty artificial intelligence uai
acapulco mexico pp
pearl j probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann series representation reasoning morgan kaufmann
robinson j w hartemink j learning non stationary dynamic bayesian
networks journal machine learning
scutari denis j b bayesian networks examples r chapman
hall boca raton isbn
segal e peer regev koller friedman n learning module networks journal machine learning
smith v yu j smulders v hartemink j jarvis e computational inference neural information flow networks plos computational biology
e
spiegelhalter j lauritzen l sequential updating conditional probabilities directed graphical structures networks


filearning continuous time bayesian networks non stationary domains

sturlaugson l sheppard j w inference complexity continuous time bayesian
networks th conference uncertainty artificial intelligence uai
quebec city canada pp
vinh n x chetty coppel r wangikar p p gene regulatory network
modeling via global optimization high order dynamic bayesian network bmc
bioinformatics
xu j shelton c r continuous time bayesian networks host level network
intrusion detection european conference machine learning principles
practice knowledge discovery databases ecml pkdd antwerp
belgium pp
zhao w serpedin e dougherty e r inferring gene regulatory networks
time series data minimum description length principle bioinformatics

zou conzen dynamic bayesian network dbn identifying gene regulatory networks time course microarray data bioinformatics





