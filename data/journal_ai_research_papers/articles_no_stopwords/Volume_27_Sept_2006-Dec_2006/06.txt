Journal Artificial Intelligence Research 27 (2006) 505549

Submitted 06/06; published 12/06

Resource Allocation Among Agents MDP-Induced
Preferences
Dmitri A. Dolgov

ddolgov@ai.stanford.edu

Technical Research Department (AI & Robotics Group)
Toyota Technical Center
2350 Green Road
Ann Arbor, MI 48105, USA

Edmund H. Durfee

durfee@umich.edu

Electrical Engineering Computer Science
University Michigan
2260 Hayward St.
Ann Arbor, MI 48109, USA

Abstract
Allocating scarce resources among agents maximize global utility is, general, computationally challenging. focus problems resources enable agents execute
actions stochastic environments, modeled Markov decision processes (MDPs),
value resource bundle defined expected value optimal MDP
policy realizable given resources. present algorithm simultaneously solves
resource-allocation policy-optimization problems. allows us avoid explicitly representing utilities exponentially many resource bundles, leading drastic
(often exponential) reductions computational complexity. use algorithm
context self-interested agents design combinatorial auction allocating resources. empirically demonstrate effectiveness approach showing
can, minutes, optimally solve problems straightforward combinatorial
resource-allocation technique would require agents enumerate 2100 resource
bundles auctioneer solve NP-complete problem input size.

1. Introduction
problem resource allocation ubiquitous many diverse research fields
economics, operations research, computer science, applications ranging decentralized scheduling (e.g., Wellman, Walsh, Wurman, & MacKie-Mason, 2001) network routing (e.g., Feldmann, Gairing, Lucking, Monien, & Rode, 2003) transportation
logistics (e.g., Sheffi, 2004; Song & Regan, 2002) bandwidth allocation (e.g., McMillan,
1994; McAfee & McMillan, 1996), name few. core question resource allocation distribute set scarce resources among set agents (either cooperative
self-interested) way maximizes measure global utility, social welfare
(sum agents utilities) one popular criteria.
many domains, agents utility obtaining set resources defined
agent accomplish using resources. example, value vehicle
delivery agent defined additional revenue agent obtain using
vehicle. However, figure best utilize resource (or set resources), agent
c
2006
AI Access Foundation. rights reserved.

fiDolgov & Durfee

Available
Resources

Available
Actions

Resource-Allocation
Problem

Planning Problem
(MDP)

Utility Function
Resources

Best Plan &
Payoff

Figure 1: Dependency Cycle: formulate planning problems, agents need
know resources get, utility functions, define
input resource-allocation problem, depend solutions planning
problems.

often must solve non-trivial planning problem actions might long-term, nondeterministic effects. Therefore, agents value set resources defined solution
planning problem, formulate planning problem agent needs know
resources obtain. leads cyclic dependencies (depicted Figure 1), wherein
input resource allocation problem depends solution planning problem,
vice versa. Unfortunately, anything simplest domains, neither resourceallocation planning problem solved closed form, making impossible
obtain parameterized solutions.
focus paper solving interdependent problems resource allocation
stochastic planning. main question consider allocate resources
way maximizes social welfare agents utility function agent
defined Markov decision process (Puterman, 1994) whose action set parameterized
resources. paper, specifically focus non-consumable resources (such
vehicles) enable actions, consumed action execution.
briefly mention case consumable resources Section 6, refer work
Dolgov (2006) detailed treatment.
assume agents MDPs weakly-coupled, meaning agents interact
resources, resources allocated, transition reward
functions MDPs independent. model weakly-coupled MDPs connected
via shared resources similar Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling,
Dean, Boutilier (1998) Benazera, Brafman, Meuleau, Hansen (2005),
differs assume resources allocated once, prior actions
taken. one-shot allocation assumption limits approach somewhat,
allows approach apply broadly non-cooperative settings (without
assumption, game-theoretic analysis agents interactions significantly
complex). importantly, allows us avoid state space explosion (due including
resource information MDP states), limits work finding
approximately optimal solutions non-trivial problems.
main result presented paper thus new algorithm that,
conditions, optimally solves resource-allocation policy-optimization problems
simultaneously. considering two problems together, sidesteps dependency cycle
506

fiResource Allocation Among Agents MDP-Induced Preferences

mentioned above, allows us avoid explicit representation utility functions
resource bundles, leading exponential reduction complexity combinatorial
resource allocation flat utility functions. empirically demonstrate resulting
algorithm scales well finding optimal solutions problems involving numerous agents
resources.
algorithm viewed contributing new approach dealing computational complexity resource allocation domains complex utility functions
linearly decomposable resources (due effects substitutability
complementarity). combinatorial allocation problems, finding optimal allocation NP-complete (often exponentially large) space resource bundles (Rothkopf,
Pekec, & Harstad, 1998). Previous approaches addressing complexity included
determining classes utility functions lead tractable problems (as surveyed
de Vries & Vohra, 2003), iterative algorithms resource allocation preference elicitation (as surveyed Sandholm & Boutilier, 2006), concise languages expressing
agents preferences (Sandholm, 1999; Nisan, 2000; Boutilier & Hoos, 2001; Boutilier, 2002).
novelty approach respect explicitly embraces underlying processes define agents utility functions, cases processes
modeled resource-parameterized MDPs. so, approach use
MDP-based models concise language agents utility functions, importantly, directly exploits structure models drastically reduce computational
complexity simultaneously solving planning resource-allocation problems.
context cooperative agents, approach viewed way solving
weakly-coupled multiagent MDPs, agents transition reward functions independent, space joint actions constrained, as, example, models used
Singh Cohn (1998) Meuleau et al. (1998). perspective, concept
resources viewed compact way representing interactions agents,
similarly model used Bererton, Gordon, Thrun (2003); however, work
differs number assumptions. Moreover, algorithms easily modified work
models constraints joint actions modeled directly (for example,
via SAT formulas).
non-cooperative agents, apply resource-allocation algorithm mechanismdesign problem (e.g., Mas-Colell, Whinston, & Green, 1995), goal allocate
resources among agents way maximizes social welfare, given
participating agent selfishly maximizing utility. domains self-interested
agents complex preferences exhibit combinatorial effects resources,
combinatorial auctions (e.g., de Vries & Vohra, 2003) often used resource-allocation.
Generalized Vickrey Auction (GVA) (MacKie-Mason & Varian, 1994), extension Vickrey-Clarke-Groves (VCG) mechanisms (Vickrey, 1961; Clarke, 1971; Groves,
1973) combinatorial auctions, particularly attractive nice analytical
properties (as described Section 4.1). develop variant VCG auction,
agents submit resource-parameterized MDPs bids, auctioneer simultaneously solves resource-allocation policy-optimization problems, thus retaining
compact representation agents preferences throughout process. describe extensions mechanism distributing computation encoding MDP information
reduce revelation private information.
507

fiDolgov & Durfee

remainder paper proceeds follows. brief review MDPs
Section 2, present (in Section 3) model decision-making agent: resourceparameterized MDP capacity constraints. analyze problem optimal policy
formulation resource-parameterized capacity-constrained MDP, study properties, present solution algorithm, based formulation (NP-complete)
problem mixed integer program.
building blocks, move multiagent setting present main
result, algorithm simultaneously allocating resources planning across agents
(Section 4). Based algorithm, design combinatorial auction allocating
resources among self-interested agents. describe distributed implementation
mechanism, discuss techniques preserving information privacy. Section 5,
analyze computational efficiency approach, empirically demonstrating exponential reductions computational complexity, compared straightforward combinatorial
resource-allocation algorithm flat utility functions. Finally, Section 6, conclude
discussion possible generalizations extensions approach. conciseness
better readability, proofs generalizations deferred appendices.

2. Markov Decision Processes
base model agents decision problems infinite-horizon fully-observable MDPs
total expected discounted reward optimization criterion (although results
applicable classes MDPs, MDPs average per-step rewards).
section introduces notation assumptions, serves brief overview
basic MDP results (see, example, text Puterman (1994) detailed discussion
material section).
classical single-agent, unconstrained, stationary, fully-observable MDP defined
4-tuple hS, A, p, ri, where:
finite set states agent in.
finite set actions agent execute.
p : 7 [0, 1] defines transition function. probability agent
goes state upon execution action state p(|s, a).

P assume that, action, corresponding transition matrix stochastic:
p(|s, a) = 1 S, A.
r : 7 R defines reward function. agent obtains reward r(s, a)
executes action state S. assume rewards bounded.
discrete-time fully-observable MDP, time step, agent observes current
state system chooses action according policy. policy said
Markovian (or history-independent) choice action depend history
states actions encountered past, rather current state
time. If, addition that, policy depend time, called stationary.
definition, stationary policy always Markovian. deterministic policy always prescribes
execution action state, randomized policy chooses actions
according probability distribution.
508

fiResource Allocation Among Agents MDP-Induced Preferences

Following standard notation (Puterman, 1994), refer different classes policies
xy , x = {H, M, S} specifies whether policy History-dependent, Markovian,
Stationary, = {R, D} specifies whether policy Randomized Deterministic
(e.g., class stationary deterministic policies labeled SD ). Obviously, Hy
Sy xR xD , history-dependent randomized policies HR stationary
deterministic policies SD least general, respectively.
stationary randomized policy thus mapping states probability distributions
actions: : 7 [0, 1], (s, a) defines probability action
executed state s. stationary deterministic policy viewed degenerate case
randomized policy one action state nonzero
probability executed.
unconstrained discounted MDP, goal find policy maximizes
total expected discounted reward infinite time horizon:1

hX

U (, ) = E
()t rt (, ) ,
(1)
t=0

[0, 1) discount factor (a unit reward time + 1 worth
agent reward time t), rt (random) reward agent receives time t,
whose distribution depends policy initial distribution state space
: 7 [0, 1].
One important results theory MDPs states that, unconstrained discounted MDP total expected reward optimization criterion, always exists optimal policy stationary, deterministic, uniformly optimal,
latter term means policy optimal distributions starting state.2
several commonly-used ways finding optimal policy, central
concept value function policy, v : 7 R, v (s) expected
cumulative value reward agent would receive started state behaved
according policy . given policy , value every state unique solution
following system |S| linear equations:
X
X
v (s) =
r(s, a)(s, a) +
p(|s, a)v (),
S.
(2)




find optimal policy, handy consider optimal value function v : 7 R,
v (s) represents value state s, given agent behaves optimally.
optimal value function satisfies following system |S| nonlinear equations:
h

X
v (s) = max r(s, a) +
p(|s, a)v () ,
S.
(3)




v,

Given optimal value function
optimal policy simply act greedily respect

v (with method tie-breaking case multiple optimal actions):
h

(
P
1 arg maxa r(s, a) + p(|s, a)v () ,
(s, a) =
(4)
0 otherwise.
1. Notation: (x)y exponent, xy superscript.
2. Uniform optimality policies reason included component textbook MDP.

509

fiDolgov & Durfee

One possible ways solving optimal value function formulate
nonlinear system (3) linear program (LP) |S| optimization variables v(s)
|S||A| constraints:
X
min
(s)v(s)


subject to:

(5)

v(s) r(s, a) +

X

p(|s, a)v(),

S, A,



arbitrary constant vector |S| positive components ((s) > 0 S).3
many problems (including ones focus paper), useful consider equivalent dual LP |S||A| optimization variables x(s, a) |S|
constraints:4
XX
r(s, a)x(s, a)
max
x





subject to:
X
XX
x(, a)
x(s, a)p(|s, a) = (),




(6)
S;



x(s, a) 0

S, A.

optimization variables x(s, a) often called visitation frequencies occupation measure policy. think initial probability distribution, x(s, a)
interpreted
P total expected number times action executed state s.
Then, x(s) = x(s, a) gives total expected flow state s, constraints
LP interpreted conservation flow states.
optimal policy computed solution dual LP as:
x(s, a)
,
(7)
(s, a) = P
x(s, a)
P
non-negativity guarantees x(s, a) > 0 S. general, appears
lead randomized policies. However, bounded LP n constraints always basic
feasible solution (e.g., Bertsimas & Tsitsiklis, 1997), definition
n non-zero components. strictly positive, basic feasible solution LP (7)
precisely |S| nonzero components (one state), guarantees existence
optimal deterministic policy. policy easily obtained LP solvers
(e.g., simplex always produce solutions map deterministic policies).
Furthermore, mentioned above, unconstrained discounted MDPs, always
exist policies uniformly optimal (optimal initial distributions).
dual LP (6) yields uniformly optimal policies strictly positive used. However,
3. overloading objective function coefficients initial probability distribution
MDP earlier intentional explained shortly.
4. Note authors (e.g., Altman, 1996) prefer opposite convention, (6) called dual,
(5), primal.

510

fiResource Allocation Among Agents MDP-Induced Preferences

solution (x) dual LP retains interpretation expected number times state
visited action executed initial probability distribution
used LP.
main benefit dual LP (6) manifested constrained MDPs (Altman, 1999;
Kallenberg, 1983; Heyman & Sobel, 1984), action, addition producing
reward r(s, a), incurs vector costs k (s, a) : 7 R k [1..K]. problem maximize expected reward, subject constraints expected costs.
Constrained models type arise many domains, telecommunication applications (e.g., Ross & Chen, 1988; Ross & Varadarajan, 1989), often desirable
maximize expected throughput, subject conditions average delay. problems,
constraints imposed expected costs, solved polynomial time
using linear programming simply augmenting dual LP (6) following linear
constraints:
XX
k (s, a)x(s, a) bk ,
k [1..K],
(8)




bk upper bound expected cost type k. resulting constrained MDP
differs standard unconstrained MDP: particular, deterministic policies
longer optimal, uniformly optimal policies not, general, exist problems
(Kallenberg, 1983).
reason easily augmentable constraints, dual LP (6)
forms basis approach. However, constraints arise resource-allocation
problems focus paper different linear constraints (8),
leading different optimization problems different properties requiring different
solution techniques (as described detail Section 3).
conclude background section introducing simple unconstrained MDP
serve basis running example, refer throughout rest
paper.
Example 1 Consider simple delivery domain, depicted Figure 2, agent
obtain rewards delivering furniture (action a1 ) delivering appliances (action a2 ).
Delivering appliances produces higher rewards (as shown diagram),
damage delivery vehicle. agent delivers furniture, damage vehicle
negligible, whereas agent delivers appliances, vehicle guaranteed function
reliably first year (state s1 ), (state s2 ) 10% probability failure,
per year. vehicle serviced (action a3 ), resetting condition, expense
lowering profits. truck break (state s3 ), repaired (action a4 ),
significant negative impact profits. assume discount factor = 0.9.
optimal value function v (s1 ) 95.3, v (s2 ) 94.7, v (s3 ) 86.7,
corresponding optimal occupation measure (assuming uniform ) following (listing
non-zero elements): x(s1 , a2 ) 4.9, x(s2 , a3 ) 4.8, x(s3 , a4 ) 0.3. maps
optimal policy dictates agent start delivering appliances (action a2
state s1 ), service vehicle first year (action a3 state s2 ), fix
vehicle ever gets broken (a4 s3 ) (the latter zero probability happening
policy agent starts state s1 s2 ).

511

fiDolgov & Durfee

a1: deliver furniture
p=1
r=5

s1
(initial)

a4: fix truck
p=1
r=1

a1: deliver furniture
p=1
r=5

a2: deliver appliances
p=1
r=10
a3: service truck
p=1
r=9

s2
(2nd year)
a2: deliver appliances
p=0.9
r=10
p=0.1
r=10

s3
(truck broken)

Figure 2: Unconstrained MDP example delivery domain. Transition probabilities (p)
rewards (r) shown diagram. Actions shown result transition
state reward. noop action a0 corresponds
nothing; change state produces zero reward.

3. Agent Model: Resource-Parameterized MDP
section, introduce model decision-making agent, describe
single-agent stochastic policy-optimization problem defines agents preferences
resources. show that, single-agent problem, formulated MDP whose
action set parameterized resources available agent, stationary deterministic
policies optimal, uniformly optimal policies not, general, exist. show
problem finding optimal policies NP-complete. Finally, present policyoptimization algorithm, based formulation problem mixed integer linear
program (MILP).
model agents resource-parameterized MDP follows. agent set
actions potentially executable, action requires certain combination
resources. capture local constraints sets resources agent use, use
concept capacities: resource capacity costs associated it,
agent capacity constraints. example, delivery company needs vehicles loading
equipment (resources allocated) make deliveries (execute actions). However,
equipment costs money requires manpower operate (the agents local capacity
costs). Therefore, amount equipment agent acquire successfully utilize
constrained factors budget limited manpower (agents local capacity
bounds). two-layer model capacities resources represented separately might
seem unnecessarily complex (why fold together impose constraints directly
resources?), separation becomes evident useful multiagent model
discussed Section 4. emphasize difference here: resources items
allocated among agents, capacities define inherent limitations individual
agent combinations resources usefully possess.
agents optimization problem choose subset available resources
violate capacity constraints, best policy feasible bundle
resources yields highest utility. words, single-agent problem analyzed
512

fiResource Allocation Among Agents MDP-Induced Preferences

section constraints total resource amounts (they introduced
multiagent problem next section), constraints due agents
capacity limits. Adding limited resource amounts single-agent model would
simple matter, since constraints handled simple pruning agents
action space. Further, note without capacity constraints, single-agent problem
would trivial, would always optimal agent simply take resources
potential use. However, presence capacity constraints, face problem
similar cyclic dependency Figure 1: resource-selection problem requires
knowing values resource bundles, defined planning problem,
planning problem ill-defined resource bundle chosen. section,
focus single-agent problem selecting optimal subset resources satisfies
agents capacity constraints assume agent value acquiring additional
resources exceed capacity bounds.
resources model outlined non-consumable, i.e., actions require
resources consume execution. mentioned Introduction,
work focus non-consumable resources, briefly outline case
consumable resources Section 6.
model agents optimization problem n-tuple hS, A, p, r, O, , C, ,
b, i:
hS, A, p, ri standard components MDP, defined earlier Section 2.
set resources (e.g., = {production equipment, vehicle, . . .}). use
refer resource type.
: 7 R function specifies resource requirements actions;
(a, o) defines much resource action needs executable (e.g.,
(a, vehicle) = 1 means action requires one vehicle).
C set capacities agent (e.g., C = {space, money, manpower, . . .}).
use c C refer capacity type.
: C 7 R function specifies capacity costs resources; (o, c) defines
much capacity c C unit resource requires (e.g., (vehicle, money) =
$50000 defines monetary cost vehicle, (vehicle, manpower) = 2 means
two people required operate vehicle).

b : C 7 R specifies upper bound capacities;
b(c) gives upper bound
capacity c C (e.g.,
b(money) = $1,000,000 defines budget constraint,

b(manpower) = 7 specifies size workforce).
: 7 R initial probability distribution; (s) probability agent
starts state s.
goal find policy yields highest expected reward, conditions resource requirements policy exceed capacity bounds
513

fiDolgov & Durfee

agent. words, solve following mathematical program:5
max U (, )


subject to:
n
X
X

(o, c) max (a, o)H
(s, a)
b(c),




(9)
c C,



H Heaviside step function nonnegative argument, defined as:
(
0 z = 0,
H(z) =
1 z > 0.
constraint (9) interpreted follows. argument H nonzero
theP
policy assigns nonzero probability using action least one state. Thus,
function
H( (s, a)) serves indicator

us whether agent plans use
P tells
action policy, max (a, o)H( (s, a)) tells us much resource
agent needs policy. take max respect a, resource
used different actions. Therefore, summed resources o, left-hand
side gives us total requirements policy terms capacity c,
greater bound
b(c).
following example illustrates single-agent model.
Example 2 Let us augment Example 1 follows. Suppose agents needs obtain
truck perform delivery actions (a1 a2 ). truck required service
repair actions (a3 a4 ). Further, deliver appliances, agent needs acquire
forklift, needs hire mechanic able repair vehicle (a4 ). noop
action a0 requires resources. maps model three resources (truck, forklift,
mechanic): = {ot , , om }, following action resource costs (listing
non-zero ones):
(a1 , ot ) = 1, (a2 , ot ) = 1, (a2 , ) = 1, (a3 , ot ) = 1, (a4 , ot ) = 1, (a4 , om ) = 1.
Moreover, suppose resources (truck ot , forklift , mechanic om )
following capacity costs (there one capacity type, money: C = {c1 })
(ot , c1 ) = 2, (of , c1 ) = 3, (om , c1 ) = 4,
agent limited budget
b = 8. can, therefore acquire two
three resources, means optimal solution unconstrained problem
Example 1 longer feasible.

Let us observe MDP-based model agents preferences presented
fully general discrete indivisible resources, i.e., non-decreasing utility function
resource bundles represented via resource-constrained MDP model described
above.
5. formulation assumes stationary policy, supported argument Section 3.1.

514

fiResource Allocation Among Agents MDP-Induced Preferences

Theorem 1 Consider finite set n indivisible resources = {oi } (i [1, n]),
N available units resource. Then, non-decreasing utility function
defined resource bundles f : [0, m]n 7 R, exists resource-constrained MDP
hS, A, p, r, O, , C, ,
b, (with resource set O) whose induced utility function
resource bundles f . words, every resource bundle z [0, m]n ,
value optimal policy among whose resource requirements exceed z
(call set (z)) f (z):
f : [0, m]n 7 R, hS, A, p, r, O, , C, ,
b, :
n
h

X


z [0, m]n , (z) = max (a, oi )H
(s, a) zi = max U (, ) = f (z).




(z)

Proof: See Appendix A.1.

Let us comment Theorem 1 establishes generality MDP-based preference model introduced section, construction used proof little practical interest, requires MDP exponentially large state action space. Indeed,
advocate mapping arbitrary unstructured utility functions exponentially-large
MDPs general solution technique. Rather, contention techniques apply domains utility functions induced stochastic decision-making process
(modeled MDP), thus resulting well-structured preferences resources
exploited drastically lower computational complexity resource-allocation
algorithms.
3.1 Properties Single-Agent Constrained MDP
section, analyze constrained policy-optimization problem (9). Namely,
show stationary deterministic policies optimal problem, meaning
necessary consider randomized, history-dependent policies. However, solutions
problem (9) not, general, uniformly optimal (optimal initial distribution).
Furthermore, show (9) NP-hard, unlike unconstrained MDPs,
solved polynomial time (Littman, Dean, & Kaelbling, 1995).
begin showing optimality stationary deterministic policies (9).
following, use HR refer class history-dependent randomized policies (the
general policies), SD HR refer class stationary deterministic
policies.
Theorem 2 Given MDP = hS, A, p, r, O, , C, ,
b, resource capacity
HR
constraints, exists policy
feasible solution , exists
stationary deterministic policy SD SD feasible, expected total reward
SD less :
HR , SD SD : U ( SD , ) U (, )
Proof: See Appendix A.2.

result Theorem 2 surprising: intuitively, stationary deterministic
policies optimal, history dependence increase utility policy,
515

fiDolgov & Durfee

using randomization increase resource costs. latter true including action policy incurs costs terms resources regardless
probability executing action (or expected number times action
executed). true dealing non-consumable resources; property hold MDPs consumable resources (as discuss detail
Section 6).
show uniformly optimal policies always exist constrained
problem. result well known another class constrained MDPs, constraints
imposed total expected costs proportional expected number
times corresponding actions executed (discussed earlier Section 2). MDPs
constraints arise, example, bounds imposed expected usage
consumable resources, mentioned Section 2, problems solved using
linear programming augmenting dual LP (6) linear constraints expected
costs (8). Below, establish result problems non-consumable resources
capacity constraints.
Observation 1 always exist uniformly optimal solutions (9).
words, exist two constrained MDPs differ initial conditions: =
hS, A, p, r, O, , C, ,
b, 0 = hS, A, p, r, O, , C, ,
b, 0 i, policy
optimal problems simultaneously, i.e., two policies 0
optimal solutions 0 , respectively, following holds:
U (, ) > U ( 0 , ),

U (, 0 ) < U ( 0 , 0 )

(10)

demonstrate observation example.
Example 3 Consider resource-constrained problem Example 2. easy see
initial conditions = [1, 0, 0] (the agent starts state s1 certainty),
optimal policy states s1 s2 Example 1 (s1 a2 s2 a3 ),
which, given initial conditions, results zero probability reaching state s3 (to
noop a0 assigned). policy requires truck forklift. However,
agent starts state s3 ( = [0, 0, 1]), optimal policy fix truck (execute a4
s3 ), resort furniture delivery (do a1 s1 assign noop ao s2 ,
never visited). policy requires mechanic truck. two policies
uniquely optimal corresponding initial conditions, suboptimal initial
conditions, demonstrates uniformly optimal policy exists example.
intuition behind fact uniformly optimal policies not, general, exist
constrained MDPs since resource information part MDP state
space, constraints imposed resource usage, principle Bellman optimality hold (optimal actions different states cannot chosen independently).
Given constrained MDP, possible construct equivalent unconstrained MDP
standard properties optimal solutions (by folding resource information
state space, modeling resource constraints via transition function), resulting
state space exponential number resources.
analyze computational complexity optimization problem (9).
516

fiResource Allocation Among Agents MDP-Induced Preferences

Theorem 3 following decision problem NP-complete. Given instance MDP
hS, A, p, r, O, , C, ,
b, resources capacity constraints, rational number ,
exist feasible policy , whose expected total reward, given , less ?
Proof: See Appendix A.3.

Note complexity result stems limited capacities agents
fact define resource requirements policy set resources
needed carry actions nonzero probability executed. If,
however, defined constraints expected resource requirements, actions
low probability executed would lower resource requirements, optimal policies
would randomized, problem would equivalent knapsack continuously
divisible items, solvable polynomial time via LP formulation MDPs
linear constraints (6,8).
3.2 MILP Solution
analyzed properties optimization problem (9), present
formulation (9) mixed integer linear program (MILP). Given established
NP-completeness (9) previous section, MILP (also NP-complete) reasonable
formulation allows us reap benefits vast selection efficient algorithms
tools (see, example, text Wolsey, 1998 references therein).
section rest paper assume resource requirements
actions binary, i.e., (a, o) = {0, 1}. make assumption simplify
discussion, limit generality results. briefly describe case
non-binary resource costs Appendix B completeness, refer work
Dolgov (2006) detailed discussion examples.
Let us rewrite (9) occupation measure coordinates x adding constraints
(9) standard LP occupancy coordinates (6). Noticing (for states
nonzero probability visited) (s, a) x(s, a) either zero nonzero simultaneously:
X
X


x(s, a) ,
A,
(s, a) = H
H




that, (a, o) = {0, 1}, total resource requirements policy simplified
follows:
X

n
X
X

max (a, o)H
x(s, a) = H
(a, o)
x(s, a) ,
O,
(11)








get following program x:
XX
max
x(s, a)r(s, a)
x





subject to:
X
XX
x(, a)
x(s, a)p(|s, a) = (),


X



(o, c)H



X


S;



(a, o)

X


x(s, a)
b(c),

c C;



x(s, a) 0,

S, A.
517

(12)

fiDolgov & Durfee

challenge solving mathematical program constraints nonlinear
due Heaviside function H.
linearize Heaviside function, augment original
variables
Poptimization
x
P
set |O| binary variables (o) {0, 1}, (o) = H
(a, o)
x(s, a) .
words, (o) indicator variable shows whether policy requires resource o.
Using (o), rewrite resource constraints (12) as:
X
(o, c)(o)
b(c),
c C,
(13)


linear . synchronize x via following linear inequalities:
X
X
1/X
(a, o)
x(s, a) (o),
O,
(14)




P
P
X maxo (a, o) x(s, a) normalization constant, upper bound argument H() used. bound X guaranteed
exist
P
1 max
(a,
o),
since

discounted
problems.

example,


use
X
=
(1

)


P
1
x valid occupation measure MDP
s,a x(s, a) = (1 )
6
discount factor .
Putting together, problem (9) finding optimal policies resource constraints formulated following MILP:
XX
max
x(s, a)r(s, a)
x,





subject to:
X
XX
x(, a)
x(s, a)p(|s, a) = (),


X



S;



(o, c)(o)
b(c),

c C;

X

O;

(15)



1/X

(a, o)



X

x(s, a) (o),



x(s, a) 0,

S, A;

(o) {0, 1},

O.

illustrate MILP construction example.
Example 4 Let us formulate MILP constrained problem Example 3. Recall problem three resources = {ot , , om } (truck, forklift,
mechanic), one capacity type C = {c1 } (money), actions following resource
requirements (again, listing nonzero ones):
(a1 , ot ) = 1, (a2 , ot ) = 1, (a2 , ) = 1, (a3 , ot ) = 1, (a4 , ot ) = 1, (a4 , om ) = 1
P
P
6. Instead using single X resources, different X(o) (a, o) x(s, a) used
every resource, leading uniform normalization potentially better numerical stability
MILP solver.

518

fiResource Allocation Among Agents MDP-Induced Preferences

resources following capacity costs:
(ot , c1 ) = 2,

(of , c1 ) = 3,

(om , c1 ) = 4,

agent limited budget, i.e., capacity bound,
b(c1 ) = 8.
compute optimal policy arbitrary , formulate problem
MILP described above. Using binary variables (o) = {(ot ), (of ), (om )},
express constraint capacity cost following inequality:
2(ot ) + 3(of ) + 4(om ) 8,
constraints synchronizeP
occupation measure x binary indicators (o),
set X = (1 )1 maxo (a, o) = 4(1 )1 . Combining
constraints (15), get MILP 12 continuous 4 binary variables,
|S| + |C| + |O| = 3 + 3 + 1 = 7 constraints (not counting last two sets range constraints).

mentioned earlier, even though solving programs is, general, NP-complete
task, wide variety efficient algorithms tools so. Therefore,
one benefits formulating optimization problem (9) MILP allows
us make use highly efficient existing tools.

4. Multiagent Resource Allocation
consider multiagent problem resource allocation several agents,
resource preferences agents defined constrained MDP model
described previous section. reiterate main assumptions problem:
1. Weak coupling. assume agents weakly-coupled (Meuleau et al., 1998),
i.e., interact shared resources, resources allocated, agents transitions rewards independent. assumption critical
results.7
2. One-shot resource allocation. resources distributed agents
start executing MDPs. reallocation resources MDP
phase. assumption critical results; allowing reallocation resources
would violate weak-coupling assumption.
3. Initial central control resources. assume beginning
resource-allocation phase, resources controlled single authority.
standard sell-auction setting. problems resources distributed
7. agents cooperative, assumption weak coupling relaxed (at expense
increase complexity), MILP-based algorithm simultaneously performing policy optimization resource allocation applied consider joint state spaces interacting
agents. self-interested agents, violation weakly-coupled assumption would mean
agents would playing Markov game (Shapley, 1953) resources allocated, would
significantly complicate strategic analysis agents bidding strategies initial resource
allocation.

519

fiDolgov & Durfee

among agents begin with, face problem designing computationallyefficient combinatorial exchange (Parkes, Kalagnanam, & Eso, 2001),
complicated problem outside scope work. However, many
ideas presented paper could potentially applicable domain well.
4. Binary resource costs. before, assume agents resource costs binary.
assumption limiting. case non-binary resources discussed
Appendix B.
Formally, input resource-allocation problem consists following:
set agents; use refer agent.
{hS, A, pm , rm , , ,
bm i} collection weakly-coupled single-agent MDPs,
defined single-agent model Section 3. simplicity, without loss
generality, assume agents state action spaces A,
transition reward functions pm rm , initial conditions ,
well resource requirements : 7 {0, 1} capacity bounds

bm : C 7 R. assume agents discount factor ,
assumption trivially relaxed.
C sets resources capacities, defined exactly single-agent
model Section 3.
: C 7 R specifies capacity costs resources, defined exactly
single-agent model Section 3.
b : 7 R specifies upper bound amounts shared resources (this
defines additional bound multiagent problem).
Given above, goal design mechanism allocating resources
agents economically efficient way, i.e., way maximizes social welfare
agents (one often-used criteria mechanism design). would
mechanism efficient computational standpoint.
Example 5 Suppose two delivery agents. MDP capacity constraints
first agent exactly defined previously Examples 1 2. MDP
second agent almost first agent, difference
gets slightly higher reward delivering appliances: r2 (s1 , a2 ) = 12 (whereas
r1 (s1 , a2 ) = 10 first agent). Suppose two trucks, one forklift, one
mechanic shared two agents. bounds specified follows:
b(ot ) = 2,

b(of ) = 1,

b(om ) = 1.

problem decide agent get forklift, get
mechanic (trucks plentiful example).

520

fiResource Allocation Among Agents MDP-Induced Preferences

4.1 Combinatorial Auctions
previously mentioned, problem finding optimal resource allocation among
self-interested agents complex valuations combinations resources arises
many different domains (e.g., Ferguson, Nikolaou, Sairamesh, & Yemini, 1996; Wellman
et al., 2001) often called combinatorial allocation problem. natural widely
used mechanism solving problems combinatorial auction (CA) (e.g., de Vries
& Vohra, 2003). CA, agent submits set bids resource bundles
auctioneer, decides resources agent get price.
Consider problem allocating among set agents set indivisible resources O, total quantity resource bounded b(o). earlier
simplifying assumption actions resource requirements binary implies agents
interested bundles contain one unit particular resource.
combinatorial auction, agent submits bid bm
w (specifying much
agent willing pay) every bundle w W value um
w > 0.
cases, possible express bids without enumerating bundles (for example, using
XOR bidding language (Sandholm, 1999) necessary consider bundles
strictly positive value, subset bundle value).
techniques often reduce complexity resource-allocation problem, not,
general, avoid exponential blow number bids. Therefore, describe
simplest combinatorial auction flat bids, noted many concise
bidding languages exist special cases reduce number explicit bids.
collecting bids, auctioneer solves winner-determination problem
(WDP), solution prescribes resources distributed among
, utility um q
agents prices. agent wins bundle w price qw
w
w
(we assuming risk-neutral agents quasi-linear utility functions). Thus, optimal
bidding strategy agent depends auctioneer allocates resources sets
prices.
Vickrey-Clarke-Groves (VCG) mechanisms (Vickrey, 1961; Clarke, 1971; Groves, 1973)
widely used family mechanisms certain attractive properties (discussed detail below). instantiation VCG mechanism context
combinatorial auctions Generalized Vickrey Auction (GVA) (MacKie-Mason & Varian, 1994), allocates resources sets prices follows. Given bids bm
w
agents, auctioneer chooses allocation maximizes sum agents bids.
problem NP-complete (Rothkopf et al., 1998) expressed following inm = {0, 1} indicator variables
teger program, optimization variables zw
show whether bundle w assigned agent m, nwo = {0, 1} specifies whether bundle w
contains o:8

8. related algorithms solving WDP (e.g., Sandholm, 2002), use
integer program (16) representative formulation class algorithms perform search
space binary decisions resource bundles.

521

fiDolgov & Durfee

X

max
z

X


zw
bw

mM wW

subject to:
X

zw
1,

M;

(16)

wW

X

X

mM

wW


zw
nwo b(o),

O.

first constraint (16) says agent receive one bundle,
second constraint ensures total amount resource assigned agents
exceed total amount available. Notice MILP (16) performs summation
exponentially large sets bundles w W . outlined above, auction XOR
bidding, sets would typically smaller, but, general, still exponentially large.
GVA assigns resources according optimal solution ze (16) sets payment
agent to:
X


m0 m0
qw
= Vm

zew
bw ,
(17)
m0 6=m

Vm
value (16) participate auction (the optimal
value submit bids), second term sum agents bids
solution ze WDP participating.
GVA number nice properties. strategy-proof, meaning dominant

strategy every agent bid true value every bundle: bm
w = uw . auction
economically efficient, meaning allocates resources maximize social
welfare agents (because, agents bid true values, objective function
(16) becomes social welfare). Finally, GVA satisfies participation constraint,
meaning agent decreases utility participating auction.
straightforward way implement GVA MDP-based problem following.
Let agent enumerate resource bundles W satisfy local capacity
constraints defined
bm (c) (this sufficient MDP model implies free disposal
resources agents, make assumption auctioneer).
bundle w W , agent would determine feasible action set A(w) formulate
MDP (w) = hS, A(w), pm (w), rm (w), i, pm (w) rm (w) transition
reward functions defined pruned action space A(w). Every agent would
solve (w) corresponding feasible bundle find optimal policy
em (w), whose



expected discounted reward would define value bundle w: uw = U (e
(w), ).
mechanism suffers two major complexity problems. First, agents
enumerate exponential number resource bundles compute value
solving corresponding (possibly large) MDP. Second, auctioneer solve
NP-complete winner-determination problem exponentially large input. following
sections devoted tackling complexity problems.

Example 6 Consider two-agent problem described Example 5, two trucks, one
forklift, services one mechanic auctioned off. Using straightforward
version combinatorial auction outlined above, agent would consider
522

fiResource Allocation Among Agents MDP-Induced Preferences

2|O| = 23 = 8 possible resource bundles (since resource requirements agents
binary, neither agent going bid bundle contains two trucks). every
resource bundle, agent formulate solve corresponding MDP
compute utility bundle.
example, assume agents start state s1 (different initial conditions
would result different expected rewards, thus different utility functions), value
null resource bundle agents would 0 (since action would able
execute noop a0 ). hand, value bundle [ot , , om ] = [1, 1, 1]
contains resources would 95.3 first agent 112.4 second one.
value bundle [1, 1, 0] agent would value [1, 1, 1] (since
optimal policies initial conditions put s1 require mechanic).
agents submit bids auctioneer, solve WDP via
integer program (16) |M|2|O| = 2(2)3 = 16 binary variables. Given above,
optimal way allocate resources would assign truck agents,
forklift second agent, mechanic either (or neither) two. Thus,
agents would receive bundles [1, 0, 0] [1, 1, 0], respectively, resulting social welfare
50 + 112.4 = 162.4. However, least one agents non-zero probability
starting state s3 , value resource bundles involving mechanic would change
drastically, would optimal resource allocation social value.

4.2 Avoiding Bundle Enumeration
avoid enumerating resource bundles non-zero value agent, two things
required: i) mechanism support concise bidding language allows
agent express preferences auctioneer compact manner, ii)
agents able find good representation preferences language.
simple way achieve model create auction agents submit
specifications resource-parameterized MDPs auctioneer bids: language
compact and, given assumption agent formulate planning problem
MDP, require additional computation agents. However,
changes communication protocol agents auctioneer, similarly
concise bidding languages (Sandholm, 1999; Nisan, 2000; Boutilier & Hoos, 2001;
Boutilier, 2002). such, simply moves burden solving valuation problem
agents auctioneer, lead gains computational
efficiency. mechanism implications information privacy issues,
agents reveal local MDPs auctioneer (which might want
do). Nevertheless, build idea increase efficiency solving
valuation winner-determination problems keeping agents
MDP information private. address ways maintaining information privacy next
section, moment focus improving computational complexity agents
valuation auctioneers winner-determination problems.
question pose section follows. Given bid agent consists MDP, resource information capacity bounds hS, A, pm , rm , , ,
bm i,
auctioneer formulate solve winner-determination problem efficiently
523

fiDolgov & Durfee

simply enumerating agents resource bundles solving standard integer
program (16) exponential number binary variables?
Therefore, goal auctioneer find joint policy (a collection single-agent
policies weak-coupling assumption) maximizes sum expected total
discounted rewards agents, conditions that: i) agent assigned set
resources violates capacity bound
bm (i.e., agent assigned resources
carry), ii) total amounts resources assigned agents exceed
global resource bounds b(o) (i.e., cannot allocate agents resources
available). problem expressed following mathematical program:
X
max
Um ( , )




subject to:
X
X

(o, c)H (a, o)
(s, a)
bm (c),


X

c C, M;

(18)




(a, o)H

X




(s, a) b(o),

O.





Obviously, decision version problem NP-complete, subsumes singleagent MDP capacity constraints, NP-completeness shown Section 3.1.
Moreover, problem remains NP-complete even absence single-agent capacity
constraints. Indeed, global constraint amounts shared resources sufficient
make problem NP-complete, shown straightforward reduction
KNAPSACK, similar one used single-agent case Section 3.1.
linearize (18) similarly single-agent problem Section 3.2, yielding
following MILP, simply multiagent version (15) (recall assumption
section resource requirements binary):
XXX
max
xm (s, a)rm (s, a)
x,







subject to:
X
XX
xm (, a)
xm (s, a)pm (|s, a) = (),


X



S, M;



(o, c) (o)
bm (c),

c C, M;
(19)



X



(o) b(o),

O;



1/X

X

(a, o)



X

xm (s, a) (o),

O, M;



xm (s, a) 0,

S, A, M;



(o) {0, 1},

O, M,

X maxo,m (a, o) xm (s, a) upper bound argument H(),
used normalization. single-agent case, bound guaranteed exist
discounted MDPs easy obtain.
P

P

524

fiResource Allocation Among Agents MDP-Induced Preferences

MILP (19) allows auctioneer solve WDP without enumerate
possible resource bundles. compared standard WDP formulation (16),
order |M|2|O| binary variables, (19) |M||O| binary variables.
exponential reduction attained exploiting knowledge agents MDP-based
valuations simultaneously solving policy-optimization resource-allocation problems. Given worst-case solution time MILPs exponential number
integer variables, reduction significant impact worst-case performance
mechanism. average-case running time reduced drastically, demonstrated
experiments, presented Section 5.
Example 7 apply mechanism discussed running example alternative straightforward combinatorial auction presented Example 6, winnerdetermination MILP (19) look follows. |M||S||A| = (2)(3)(5) = 30 continuous occupation-measure variables xm , |M||O| = (2)(3) = 6 binary variables (o).
|M||S| = (2)(3) = 6 conservation-of-flow constraints involve continuous
variables only, well |M||C| + |O| + |M||O| = (2)(1) + 3 + (2)(3) = 9 constraints
involve binary variables.
capacity constraints agents exactly single-agent case described
Example 4, global resource constraints be:
1 (ot ) + 2 (ot ) 2,

1 (of ) + 2 (of ) 1,

1 (om ) + 2 (om ) 1.

Notice example one binary decision variable per resource per agent
(yielding 6 variables simple problem). exponentially fewer
number binary variables straightforward CA formulation Example 6,
requires one binary variable per resource bundle per agent (yielding 16 variables
problem). Given MILPs NP-complete number integer variables,
reduction 16 6 variables noticeable even small problem one
lead drastic speedup larger domains.

mechanism described instantiation GVA, well-known
properties VCG mechanisms, auction strategy-proof (the agents incentive
lie auctioneer MDPs), attains socially optimal resource allocation,
agent decreases utility participating auction.
sum results section: agents submit MDP information auctioneer instead valuations resource bundles, essentially
removed computational burden agents time significantly simplified auctioneers winner-determination problem (the number integer variables
WDP reduced exponentially).
4.3 Distributing Winner-Determination Problem
Unlike straightforward combinatorial auction implementation discussed earlier Section 4.1, agents shared computational burden auctioneer,
mechanism Section 4.2, agents submit information auctioneer
idle waiting solution. suggests potential improvements
computational efficiency. Indeed, given complexity MILPs, would beneficial
525

fiDolgov & Durfee

exploit computational power agents offload computation
auctioneer back agents (we assume agents cost helping
would prefer outcome computed faster).9 Thus, would distribute
computation winner-determination problem (19), common objective distributed algorithmic mechanism design (Feigenbaum & Shenker, 2002; Parkes & Shneidman,
2004).10
concreteness, base algorithm section branch bound
method solving MILPs (Wolsey, 1998), exactly techniques work
MILP algorithms (e.g., cutting planes) perform search space LP
relaxations MILP. branch bound MILPs binary variables, LP relaxations created choosing binary variable setting either 0 1, relaxing
integrality constraints binary variables. solution LP relaxation happens integer-valued, provides lower bound value global solution.
non-integer solution provides upper bound current subproblem, (combined
lower bounds) used prune search space.
Thus, simple way auctioneer distribute branch bound algorithm
simply farm LP relaxations agents ask solve LPs. However,
easy see mechanism strategy-proof. Indeed, agent tasked
performing computation determining optimal resource allocation
associated payments could benefit lying outcome computation
auctioneer. common phenomenon distributed mechanism implementations:
whenever WDP calculations offloaded agent participating auction,
agent might able benefit sabotaging computation. several methods
ensuring strategy-proofness distributed implementation. approach best
suited problem based idea redundant computation (Parkes & Shneidman,
2004),11 multiple agents asked task disagreement
carefully punished discourage lying. rest section, demonstrate
easy implement case.
basic idea simple: let auctioneer distribute LP relaxations agents,
check solutions re-solve problems agents return incorrect solutions
(this would make truthful computation weakly-dominant strategy agents,
nonzero punishment used achieve strong dominance). strategy
auctioneer removes incentive agents lie yields exactly solution
centralized algorithm. However, order beneficial, complexity
checking solution must significantly lower complexity solving problem.
Fortunately, true LPs.
Suppose auctioneer solve following LP, written two
equivalent ways (let us refer one left primal, one right
9. observed Parkes Shneidman (2004), assumption bit controversial, since desire
efficient computation implies nonzero cost computation, agents cost helping
modeled. is, nonetheless, common assumption distributed mechanism implementations.
10. describe one simple way distributing mechanism, others possible.
11. Redundant computation discussed Parkes Shneidman (2004) context ex post Nash
equilibria, whereas interested dominant strategies, high-level idea similar.

526

fiResource Allocation Among Agents MDP-Induced Preferences

dual):
min v

max rT x

subject to:

(20)

subject to:



Ax = ;

v r.

x 0.

strong duality property, primal LP solution v , dual
solution x , v = rT x . Furthermore, given solution primal LP, easy
compute solution dual: complementary slackness, vT = rT B 1 x = B 1 ,
B square invertible matrix composed columns correspond basic
variables solution.
well-known properties used auctioneer quickly check optimality
solutions returned agents. Suppose agent returns v solution
primal LP. auctioneer calculate dual solution vT = rT B 1 check whether
rT x = v. Thus, expensive operation auctioneer perform
inversion B, done sub-cubic time. matter fact,
implementation perspective, would efficient ask agents return
primal dual solutions, since many popular algorithms compute process
solving LPs.
Thus, provided simple method allows us effectively distribute
winner-determination problem, maintaining strategy-proofness mechanism
negligible computation overhead auctioneer.
4.4 Preserving Information Privacy
mechanism discussed far drawback requires agents
reveal complete information MDPs auctioneer. problem
exacerbated distributed WDP algorithm previous section, since
agent reveal MDP information auctioneer, information
spread agents via LP relaxations global MILP. show
alleviate problem.
Let us note that, saying agents prefer reveal local information,
implicitly assuming external factor affects agents utilities
captured agents MDPs. sensible way measure value information
changes ones decision-making process outcomes. Since effect part
model (in fact, contradicts weak-coupling assumption), cannot domainindependent manner define constitutes useful information, bad
agent reveal much MDP. Modeling effects carefully analyzing
interesting research task, outside scope paper. Thus,
purposes section, content mechanism hides enough information
make impossible auctioneer agent uniquely determine transition
reward function agent (in fact, information revealed agent map
infinitely many MDPs agents).12 Many transformations possible;
present one illustrate concept.
12. stringent condition would require agents preferences resource bundles revealed
(Parkes, 2001), set lower bar here.

527

fiDolgov & Durfee

main idea approach modify previous mechanism agents
submit private information auctioneer encrypted form allows
auctioneer solve winner-determination problem, allow infer
agents original MDPs.
First, note that, instead passing MDP auctioneer, agent submit
equivalent LP (6). So, question becomes: agent transform LP
way auctioneer able solve it, able infer transition
reward functions originating MDP? words, problem reduces
following. Given LP L1 (created MDP = hS, A, p, r, via (6)), need
find transformation L1 L2 solution transformed LP L2 uniquely
map solution original LP L1 , L2 reveal transition reward
functions original MDP (p r). show simple change variables suffices.
Suppose agent m1 MDP-originated LP going ask agent m2 solve it.
order maintain linearity problem (to keep simple m2 solve), m1
limit linear transformations. Consider linear, invertible transformation
primal coordinates u = F v, linear, invertible transformation dual coordinates
= Dx. Then, LP (20) transformed (by applying F , switching
dual, applying D) equivalent LP new coordinates y:
max rT D1
subject to:
(F 1 )T AD1 = (F 1 )T ;

(21)

D1 0.
value optimal solution (21) value optimal solution
(20), given optimal solution (21), easy compute solution
original: x = D1 . Indeed, perspective dual, primal transformation F
equivalent linear transformation dual equality constraints Ax = , (given
F non-singular) effect solution objective function. Furthermore,
dual transformation equivalent change variables modifies solution
value objective function.
However, problem transformations gives away D1 . Indeed,
agent m2 able simply read (up set multiplicative constants) transformation constraints D1 0. Therefore, diagonal matrices positive
coefficients (which equivalent stretching coordinate system) trivially deduced m2 , since map 0. Choosing negative multiplier xi
(inverting axis) pointless, flips non-negativity constraints yi 0,
immediately revealing sign m2 .
Let us demonstrate that, given MDP corresponding LP L1 ,
choose F impossible m2 determine coefficients L1
(or equivalently original transition reward functions p r). agent m2
receives L2 (as (21)), knows L2 created MDP, columns
constraint matrix original LP L1 must sum constant:
X
X
Aji = 1
p(|s, a) = 1 .
(22)
j



528

fiResource Allocation Among Agents MDP-Induced Preferences

a1:
p=1
r=3

a1:
p=1
r=1

s2

s1

a2:
p=1
r=4

a2:
p=0.99
r=9.8

a2:
p=0.5
r=0.02

a1:
p=0.5
r=0.02
a2:
p=0.01
r=0.98

a1:
p=0.5
r=0.02

a1:
p=1
r=1

s2

s1

a2:
p=1
r=2

a2:
p=0.5
r=0.02

(a)

a1:
p=1
r=3

a1:
p=1
r=1

s2

s1

a2:
p=1
r=4

a2:
p=0.99
r=9.8

(b)

Figure 3: Preserving privacy example. Two different MDPs lead LP
constraint matrix.

gives m2 system |S| nonlinear equations diagonal arbitrary F ,
total |S||A| + |S|2 free parameters. everything degenerate
cases (which easily handled appropriate choice F ), equations
hugely under-constrained infinitely many solutions. matter fact,
sacrificing |S| free parameters, m1 choose F way
columns constraints L2 sum constant 0 (0, 1), would
effect transforming L1 L2 corresponds another valid MDP 2 . Therefore,
given L2 , infinitely many original MDPs transformations F
map LP L2 .
consider connection resource capacity costs agents occupation measures global WDP (19). two things auctioneer
able do: i) determine value agents policy (to able maximize
social welfare), ii) determine resource requirements policies (to check
resource constraints). So, question is, transformation affect these?
noted earlier, transformation change objective function, first
requirement holds. hand, change occupation measure xm (s, a)
arbitrary multipliers. However, multiplicative factor xm (s, a) effect usage
non-consumable resources, matters whether corresponding xm (s, a) zero
(step function H nullifies scaling effect). Thus, second condition holds.
Example 8 Consider two-state MDP depicted Figure 3a represents decisionmaking problem sales company, two states corresponding possible market
conditions, two actions two possible sales strategies. Market conditions state
s1 much favorable state s2 (the rewards actions higher).
transitions two states correspond probabilities market conditions changing
rewards reflect expected profitability two states. Obtaining numbers
realistic scenario would require performing costly time-consuming research,
company might want make information public.
Therefore, company participate resource-allocation mechanism described above, would want encrypt MDP submitting auctioneer.
529

a2:
p=0.5
r=0.02

fiDolgov & Durfee

MDP following reward function
r = (1, 19.622, 0.063, 0.084)T ,
following transition function:


1
0
p(a1 ) =
,
0.5 0.5


p(a2 ) =


0.986 0.014
.
0.5
0.5

(23)

(24)

Using = 0.8, corresponds following conservation flow constraint matrix:


0.2 0.212 0.4 0.4
A=
.
(25)
0 0.012 0.6
0.6
submitting LP auctioneer, agent applies following transformations:


2
0
,
(26)
= diag(1, 0.102, 47.619, 47.619), F =
0.084 0.126
yielding following new constraint matrix:


0.1
1
0
0
0
1
1
= (F ) AD =
.
0 0.9 0.1 0.1

(27)

However, constraint matrix A0 corresponds non-transformed conservation
flow constraint different MDP (shown Figure 3b) = 0.9, following
reward function:
r = (1, 2, 3, 4)T ,
(28)
following transition function:


1 0
p(a1 ) =
,
0 1


p(a2 ) =


0 1
.
0 1

(29)

Therefore, auctioneer receives constraint matrix A0 , way knowing whether agent MDP transition function (29) transformed
using (26) MDP transition function (24) transformed. Notice
dynamics two MDPs vary significantly: transition probabilities state
connectivity. second MDP reveal information originating MDP
corresponding market dynamics.

sum up, can, large extent, maintain information privacy mechanism
allowing agents apply linear transformations original LPs. information
revealed mechanism consists agents resource costs (a, o), capacity bounds

bm (c), sizes state action spaces (the latter hidden adding
dummy states actions MDP).
revealed information used infer agents preferences resource requirements. Further, numeric policies revealed, lack information transition
reward functions renders information worthless (as illustrated Example 8,
could multiple originating MDPs different properties).
530

fiResource Allocation Among Agents MDP-Induced Preferences

5. Experimental Results
section present empirical analysis computational complexity
resource-allocation mechanism described Section 4. report results computational complexity mechanism Section 4.2, agents submit
MDPs auctioneer, simultaneously solves resource-allocation policyoptimization problems. far additional speedup achieved distributing WDP,
described Section 4.3, report empirical results, since well-established
parallel programming literature parallel versions branch-and-bound MILP
solvers consistently achieve linear speedup (Eckstein, Phillips, & Hart, 2000). due
fact branch-and-bound algorithms require little inter-process communication.
experiments, implemented multiagent delivery problem, based
multiagent rover domain (Dolgov & Durfee, 2004). problem, agents operate
stochastic grid world delivery locations randomly placed throughout grid.
delivery task requires set resources, limited quantities resources.
random delivery locations grid, location set deliveries
accepts. resource size requirements (capacity cost), delivery agent
bounded space hold resources (limited capacity). agents participate
auction bid delivery resources. setting, value resource depends
resources agent acquires deliveries make. Given
bundle resources, agents policy optimization problem find optimal delivery
plan. exact parameters used experiments critical trends seen
results presented below, sake reproducibility domain described
detail Appendix C.13 resource costs experiments presented
binary.
Computational complexity constrained optimization problems vary greatly
constraints tightened relaxed. Therefore, first step analysis empirical
computational complexity mechanism, investigate running time depends
capacity constraints agent bounds total amounts
resources shared agents. common types constrained optimization
constraint-satisfaction problems, natural expect WDP MILP
easy solve problem over- under-constrained either capacity
resource bounds. empirically verify this, varied local capacity constraint levels 0
(meaning agents cannot use resources) 1 (meaning agent capacity
use enough resources execute optimal unconstrained policy), well global
constraint levels 0 meant resources available agents, 1
meant enough resources assign agent desired resource
bundle. experiments, part MILP solver played CPLEX 8.1
Pentium-4 machine 2GB RAM (RAM bottleneck due use
sparse matrix representations). typical running-time profile shown Figure 4.
problem easy over-constrained, becomes difficult constraints
relaxed abruptly becomes easy capacity resource levels start
approach utopia.
13. investigated other, randomly generated domains, results qualitatively same.

531

fiDolgov & Durfee

1
0.9
0.8
Local Constraint Level

6

t, sec

4

2

0
1

1

0.5
Local Constraints

0.5
0

0

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

Global Constraints

0.2

0.4
0.6
Global Constraint Level

0.8

1

Figure 4: Running time MDP-based winner-determination MILP (19) different levels global (b
) local (b
) constraints. constraint levels fractions
utopian levels needed implement optimal unconstrained policies.
Problems involved 10 agents, operating 5-by-5 grid, 10 shared
resource types. data point shown average ten runs randomlygenerated problems.

following experiments aim avoid easy regions constraint levels.
Therefore, given complexity profiles, set constraint levels 0.5 local
capacity global resource bounds. set discount factor = 0.95.
value chosen arbitrarily, investigations effect value
running time MILP revealed significant trends.
begin comparing performance MDP-based auction (Section 4.2)
performance straightforward CA flat preferences (as described Section 4.1).
results summarized Figure 5, compares time takes solve
standard winner-determination problem space resource bundles (16)
time needed solve combined MDP-WDP problem (19) used mechanism,
number resources increased (with 5 agents, 5-by-5 grid). Despite fact
algorithms exponential worst-case running time, number integer
variables (16) exponentially larger MILP (19), effect clearly
demonstrated Figure 5. Furthermore, comparison gives extremely optimistic
view performance standard CA, take account additional
complexity valuation problem, requires formulating solving large
number MDPs (one per resource bundle). hand, latter embedded
WDP mechanism (19), thus including time solving valuation problem
comparison would magnify effect. fact, experiments, time
required solve MDPs valuation problem significantly greater
time solving resulting WDP MILP. However, present quantitative results
effect here, difference implementation (iterating resource
bundles solving MDPs done via straightforward implementation Matlab,
532

fiResource Allocation Among Agents MDP-Induced Preferences

Figure 5: Gains computation efficiency: MDP-based WDP versus WDP straightforward CA implementation. latter include time solving
MDPs compute resource-bundle values. Error bars show standard
deviation ten runs.
n=5, |O| = 10
4

10

3

10

2

t, sec

10

1

10

0

10

1

10

5

10
15
Number Agents |M|

20

25

Figure 6: Scaling MDP-based winner-determination MILP (19) agents. Agents
operated 5-by-5 grids shared 10 types resources.

MILPs solved using highly-optimized CPLEX code). parallelization WDP
performed experiments either algorithm.
analyze performance algorithm larger problems infeasible
straightforward CA. Figure 6 illustrates scaling effect number agents
participating auction increased. below, point plot corresponds
single run experiment (with less ten runs performed every value
parameters), solid line mean. Recall size WDP scales linearly
533

fiDolgov & Durfee

n = 10, |M| = 5

n = 5, |M| = 10

2

10

2

10

1

1

10
t, sec

t, sec

10

0

10

0

10

1

10

1

10

0

20

40
60
80
Number Resource Types |O|

2

10

100

(a)

0

20

40
60
80
Number Resource Types |O|

100

(b)

n = 7, |M| = 10
3

10

2

t, sec

10

1

10

0

10

1

10

2

10

0

20

40
60
80
Number Resource Types |O|

100

(c)

(d)

Figure 7: (a)(c): scaling MDP-based winner-determination MILP (19)
number resources three sets problems different grid sizes (n)
different numbers agents (|M|); (d): linear-scale plot tail data
(c).

number agents. graph therefore reflects rather standard scaling effect
NP-complete problem. seen plot, problems 25 agents
10 resource types well within reach method, average taking around
30 minutes.
Next, analyze method scales number resource types. Figure 7
shows solution time function number resource types three different
sets problems. problems, number actions scaled linearly number
resource types, action required constant number resources, i.e., number
534

fiResource Allocation Among Agents MDP-Induced Preferences

n = 5, |M| = 3

80
70
60

t, sec

50
40
30
20
10
0
0

5

10
15
Resources Per Action

20

Figure 8: Complexity MDP-based winner-determination MILP (19) function
number actions resource requirements.

nonzero (a, o) per action constant (two) regardless total number resource
types. problems exhibit interesting trait wherein running time peaks
relatively low numbers resource types, falls quickly, increases much
slowly number resource types increases (as illustrated Figure 7d, uses
linear scale). due fact total number resource types
much higher number resources required action, less contention
particular resource among agents one agents actions. Therefore,
problems become relatively under-constrained solution time increases slowly.
better illustrate effect, ran set experiments inverse ones shown
Figure 7: kept total number resource types constant increased number
resource types required action. results shown Figure 8. running-time
profile similar observed earlier varied local global constraints:
total number resources per action low high, problem under- overconstrained relatively easy solve, complexity increases significantly
number resources required resource range 50-80% total
number resource types.
Based above, would expect actions resource requirements increased
total number resource types, problem would scale gracefully
Figure 7. example, Figure 9 illustrates running time problems number
resources required action scales linearly total number resources. There,
complexity increase significantly faster. However, unreasonable assume
many domains number actions not, fact, increase total
number resource types involved. Indeed, natural assume total number
resource types increases problem becomes complicated number
tasks agent perform increases. However, resource requirements
action increase well? delivery agent running example acquires ability
535

fiDolgov & Durfee

n= 5, |M| = 5
4

10

3

10

2

t, sec

10

1

10

0

10

1

10

2

10

0

10

20
30
40
Number Resource Types |O|

50

Figure 9: Complexity actions resource requirements grow proportionally total
number resource types. number resource types needed action
10% total number resource types |O|.

deliver pizza, might need new resources perform actions related new activity,
one would expect resource requirements delivering furniture appliances
change. Therefore, believe many real applications, method scale
gracefully total number resource types.
experiments illustrate point domains agents preferences defined underlying Markov decision processes, resource-allocation
mechanism developed paper lead significant computational advantages.
shown Figure 7, method successfully applied large problems that,
argue, well beyond reach combinatorial resource-allocation mechanisms flat
preferences. experiments show (Figure 5), even small problems, combinatorial
resource allocation mechanisms flat preferences time-consuming, attempts empirically evaluate simpler mechanisms larger problems proved futile.
instance, method takes one minute solve problem that, standard
CA, requires agents enumerate 2100 bundles auctioneer solve
NP-complete problem input size.

6. Generalizations, Extensions, Conclusions
many possible extensions generalizations work presented here,
briefly outline several below.
treatment paper focused problem resource allocation among
self-interested agents, algorithms apply cooperative MDPs weaklyinteracting agents. cooperative setting, concept resources viewed
compact way model inter-agent constraints inability include combinations joint actions policies. weakly-coupled MDPs, agents
536

fiResource Allocation Among Agents MDP-Induced Preferences

independent transition reward functions, certain combinations joint actions
feasible widely used model agents interactions (e.g., Singh & Cohn, 1998).
model resource-centric, direct models certainly possible. example, agents use SAT formulas describe valid combinations joint actions. case
easily handled via simple modifications single multiagent MILPs (15)
(19). Indeed, SAT formula expressed set linear inequalities
binary variables (a) (or (a) multiagent case), directly added
corresponding MILP (see case non-binary resources Appendix B MILP
defined indicators (a), instead (o) used binary case).
mentioned previously, work extended handle consumable resources
used whenever agents execute actions. fact, conditions, problem
considerably simplified domains kinds resources.
important change redefine value particular resource bundle
agent. difficulty that, given policy, total use consumable resources
uncertain, definition value resource bundle becomes ambiguous. One
possibility define value bundle payoff best policy whose expected
resource usage exceed amounts resources bundle. interpretation
(a, o) would change mean amount resource consumed action
every time executed. would make constraints (19) linear occupation
measure, would tremendously simplify WDP (making polynomial).
analogous models used constrained MDPs (Altman & Shwartz, 1991), briefly
described earlier Section 2. Information privacy handled similarly case
non-consumable resources. However, given transformation = Dx, resource cost
function scaled D1 (since total consumption consumable
resources proportional occupation measure). additional benefit
hiding resource cost functions (unlike case non-consumable resources
revealed). detailed treatment model consumable resources
presented work Dolgov (2006), including discussion risk-sensitive cases,
value resource bundle defined payoff best policy whose probability
exceeding resource amounts bounded.
work exploited structure agents preferences stems underlying
policy-optimization problems. However, latter modeled using flat MDPs
enumerate possible states actions. flat MDPs scale well due
curse dimensionality (Bellman, 1961). address this, WDP MILP modified
work factored MDPs (Boutilier, Dearden, & Goldszmidt, 1995) using factored
resource-allocation algorithm (Dolgov & Durfee, 2006), based dual ALP
method solving factored MDPs developed Guestrin (2003). method allows us
exploit types structure resource-allocation algorithms: structure agents
preferences induced underlying MDPs, well structure MDPs themselves.
resource-allocation mechanism discussed paper assumed one-shot allocation
resources static population agents. interesting extension work would
consider system agents resources arrive depart dynamically,
online mechanism design work (Parkes & Singh, 2003; Parkes, Singh, & Yanovsky, 2004).
Combining MDP-based model utility functions dynamics online problems
could valuable result thus appears worthwhile direction future work.
537

fiDolgov & Durfee

agent population static, periodic re-allocation resources allowed, techniques
phasing used solve resulting problem (Wu & Durfee, 2005).
summarize results paper, presented variant combinatorial auction resource allocation among self-interested agents whose valuations resource bundles
defined weakly-coupled constrained MDPs. problems, mechanism,
exploits knowledge structure agents MDP-based preferences, achieves
exponential reduction number integer decision variables, turn leads
tremendous speedup straightforward implementation, confirmed experimental results. mechanism implemented achieve reduction computational
complexity without sacrificing nice properties VCG mechanism (optimal outcomes, strategy-proofness, voluntary participation). discussed distributed
implementation mechanism retains strategy-proofness (using fact
LP solution easily verified), reveal agents private MDP information
(using transformation agents MDPs).
believe models solution algorithms described paper significantly
applicability combinatorial resource-allocation mechanisms practical problems, utility functions resource bundles defined sequential stochastic
decision-making problems.

7. Acknowledgments
thank anonymous reviewers helpful comments, well colleagues
Satinder Singh, Kang Shin, Michael Wellman, Demothenis Teneketsis, Jianhui Wu,
Jeffrey Cox valuable discussions related work.
material based part upon work supported Honeywell International,
DARPA IPTO COORDINATORs program Air Force Research Laboratory
Contract No. FA875005C0030. views conclusions contained document authors, interpreted representing official
policies, either expressed implied, Defense Advanced Research Projects Agency
U.S. Government.

Appendix A. Proofs
A.1 Proof Theorem 1
Theorem 1 Consider finite set n indivisible resources = {oi } (i [1, n]),
N available units resource. Then, non-decreasing utility function
defined resource bundles f : [0, m]n 7 R, exists resource-constrained MDP
hS, A, p, r, O, , C, ,
b, (with resource set O) whose induced utility function
resource bundles f . words, every resource bundle z [0, m]n ,
value optimal policy among whose resource requirements exceed z
(call set (z)) f (z):
f q : [0, m]n 7 R, hS, A, p, r, O, , C, ,
b, :
h


n
X


z [0, m]n , (z) = max (a, oi )H
(s, a) zi = max U (, ) = f (z).




538

(z)

fiResource Allocation Among Agents MDP-Induced Preferences

s000
a11
s100
0
a21

a11

a21

a0
r=f(0,0,0)

a31

s010
0
a31

s110 a0
a31

a11

s001
a31

s101 a0
a21

a21

a0
r=f(0,0,1)
a0
r=f(0,1,1)

s0

a0
r=0

s011

a11

a0
r=f(1,1,1)

s111

Figure 10: Creating MDP resources arbitrary non-decreasing utility function.
case shown three binary resources. transitions deterministic.

Proof: statement shown via straightforward construction MDP
exponential number (one per resource bundle) states actions. present
reduction linear number actions exponential number states. choice
due fact that, although reverse mapping requiring two states exponentially
many actions even straightforward, MDP feels somewhat unnatural.
Given arbitrary non-decreasing utility function f , corresponding MDP
constructed follows (illustrated Figure 10 n = 3 = 1). state space
MDP consists (m+1)n +1 states one state (sz ) every resource bundle z [0, m]n ,
plus sink state (s0 ).

action space MDP = a0 {aij }, [1, n], j [1, m] consists mn + 1
actions: actions per resource oi , [1, n], plus additional action a0 .
transition function p deterministic defined follows. Action a0 applicable every state leads sink state s0 . Every action aij applicable
states sz , zi = (j 1) leads certainty states zi = j:

0

1 = aij , = sz , = sz0 , zi = (j 1), zi = j,
p(|s, a) = 1 = a0 , = s0 ,


0 otherwise.
words, aij applies states j 1 units resource leads
state amount ith resource increases j.
reward function r defined follows. rewards state s0 ,
action a0 action produces rewards states:
(
f 0 (z) = ao , = sz , z [0, m]n
r(s, a) =
0
otherwise,
f 0 simple transformation f compensates effects discounting:
f 0 (z) = f (z)()
539

P

zi

.

fiDolgov & Durfee

P
words, takes zi transitions get state sz , contribution
total discounted reward exactly f (z).
resource requirements actions follows: action a0 require
resources, every action aij requires j units resource oi .
Finally, initial conditions (sz=0 ) = 1, meaning agent always starts
state corresponds empty resource bundle (state s000 Figure 10).
capacity costs limits
b used, set C = .
easy see MDP constructed above, given resource bundle z,
policy feasible set (z) zero probability reaching state sz0
z0 > z (for component i). Furthermore, optimal policy set (z)
transition state sz (since f (z) non-decreasing) use action a0 , thus obtaining
total discounted reward f (z).

A.2 Proof Theorem 2
Theorem 2 Given MDP = hS, A, p, r, O, , C, ,
b, resource capacity
constraints, exists policy HR feasible solution , exists
stationary deterministic policy SD SD feasible, expected total reward
SD less :
HR , SD SD : U ( SD , ) U (, )
Proof: Let us label A0 set actions non-zero probability
executed according , i.e.,
A0 = {a|s : (s, a) > 0}
Let us construct unconstrained MDP: 0 = hS, A0 , p0 , r0 i, p0 r0
restricted versions p r action domain limited A0 :
p0 : A0 7 [0, 1]
r0 : A0 7 R
p0 (|s, a) = p(|s, a), r0 (s, a) = r(s, a) S, S, A0
Due well-known property unconstrained infinite-horizon MDPs total
expected discounted reward optimization criterion, 0 guaranteed optimal
stationary deterministic solution (e.g., Theorem 6.2.10, Puterman, 1994), label
SD .
Consider SD potential solution . Clearly, SD feasible solution,
actions come set A0 includes actions uses non-zero probability,
means resource requirements (as (9)) SD greater
. Indeed:
n
n
X
X


max0 (a, o)H
(s, a) ,
(30)
SD (s, a) max0 (a, o) = max (a, o)H
aA



aA

aA



first inequality due fact H(z) 1 z, second equality
follows definition A0 .
540

fiResource Allocation Among Agents MDP-Induced Preferences

s1

a1:
r=v(u1),
a1,o1)=1
o1)=c(z1)

a2:
v(u2),

,o
)=1
2 2
o2)=c(z2)

:

r=

s2

a0:
r=0,
,.)=0


r=


...

s3

a0:
r=0,
,.)=0


sm


1-mv(u ),


am,om)=1
om)=c(zm)
a0:
r=0,
,.)=0


sm+1

a0:
r= ,
=0


Figure 11: Reduction KNAPSACK M-OPER-CMDP. transitions deterministic.

Furthermore, observe SD yields total reward 0 . Additionally, since SD uniformly optimal solution 0 , is, particular, optimal
initial conditions constrained MDP . Therefore, SD constitutes feasible
solution whose expected reward greater equal expected reward
feasible policy .

A.3 Proof Theorem 3
Theorem 3 following decision problem NP-complete. Given instance MDP
hS, A, p, r, O, , C, ,
b, resources capacity constraints, rational number ,
exist feasible policy , whose expected total reward, given , less ?
Proof: shown Theorem 2, always exists optimal policy (9)
stationary deterministic. Therefore, presence NP obvious, since can,
polynomial time, guess stationary deterministic policy, verify satisfies resource
constraints, calculate expected total reward (the latter done solving
standard system linear Markov equations (2) values states).
show NP-completeness problem, use reduction KNAPSACK (Garey
& Johnson, 1979). Recall KNAPSACK NP-complete problem, asks
whether, given set items z Z, cost c(z) value v(z),
exists subset Z 0 Z total value items Z 0 less

c, i.e.,
P constant vb,
Pthe total cost items greater another constant b
c(z)

b
c

v(z)

v
b
.

reduction

illustrated

Figure
11

proceeds
0
0
zZ
zZ
follows.
Given instance KNAPSACK |Z| = m, let us number items zi ,
[1, m] notational convenience. instance KNAPSACK, create
MDP + 1 states {s1 , s2 , . . . sm+1 }, + 1 actions {a0 , . . . }, types resources
= {o1 , . . . om }, single capacity C = {c1 }.
transition function states defined follows. Every state si , [1, m]
two transitions it, corresponding actions ai a0 . actions lead state
si+1 probability 1. State sm+1 absorbing transitions lead back
itself.
reward cost functions defined follows. want action ai , [1, m]
(which corresponds item zi KNAPSACK) contribute v(zi ) total discounted
541

fiDolgov & Durfee

reward. Hence, set immediate reward every action ai v(zi )()1i , which, given
transition function implies state si reached exactly step 1, ensures
action ai ever executed, contribution total discounted reward
v(zi )()1i ()i1 = v(zi ). Action a0 produces reward zero states.
resource requirements actions defined follows. Action ai , [1, m]
needs resource oi , i.e., (ai , oj ) = 1 = j. set cost resource oi
cost c(zi ) item KNAPSACK problem. null action a0 requires resources.
order complete construction, set initial distribution = [1, 0, . . .]
agent starts state s1 probability 1. define decision parameter = vb
upper bound single capacity
b=b
c.
Essentially, construction allows agent choose action ai a0 every state si .
Choosing action ai equivalent putting item zi knapsack, action a0
corresponds choice including zi knapsack. Therefore, exists
policy expected payoff less = vb uses
b = b
c
shared resource exists solution original instance
KNAPSACK.


Appendix B. Non-binary Resource Requirements
describe MILP formulation capacity-constrained single-agent optimization problem (9) arbitrary resource costs : 7 R, opposed binary costs
assumed main parts paper. corresponding multiagent winnerdetermination problem (the non-binary equivalent (19)) follows immediately
single-agent MILP.
arbitrary resource costs, obtain following non-binary equivalent optimization problem (12) occupation measure coordinates:
max
x

XX


x(s, a)r(s, a)



subject to:
X
XX
x(, a)
x(s, a)p(|s, a) = (),


X



S;



n
X

(o, c) max (a, o)H
x(s, a)
b(c),




c C;



x(s, a) 0,

S, A.

linearize sum max operators (31), let us observe inequality
n
X


g(ui ) max f (z, ui ) = g(u1 ) max f (z, u1 ) + . . . + g(un ) max f (z, un )
zZ

zZ

zZ

equivalent following system |Z|n linear inequalities:
g(u1 )f (z1 , u1 ) + g(u2 )f (z2 , u2 ) + . . . + g(un )f (zn , un ) a,
542

z1 , z2 , . . . zn Z.

(31)

fiResource Allocation Among Agents MDP-Induced Preferences

Applying constraints (31), express original system |C| nonlinear
constraints (each max):
X

n
X

(o, c) max (a, o)H
x(s, a)
b(c),




c C



following system |C||A||O| constraints max removed:
X

(o, c)(ao , o)H

X




x(s, a)
b(c),

c C, ao1 , ao2 , . . . A.

(32)



Notice way eliminating maximization exponentially increases number
constraints, expansion enumerates possible actions resource
(i.e., enumerates policies resource used action a1 , used
action a2 , action a3 , etc.) However, many problems resources used
actions. cases, constraints
Q become redundant, number
constraints reduced |C||A||O| |C| |Ao |, Ao number actions
use resource o.
linearize Heaviside function analogously case binary resource costs
Section 3.2: create binary indicator variable corresponds argument
H() tie occupation measure x via linear inequalities. difference
non-binary resource costs, instead using
P indicators resources, use indicators
actions: (a) {0, 1}, (a) = H( x(s, a)) indicator shows whether
action used policy. Using expanding max above, represent
optimization problem (9) following MILP:
max
x,

XX


x(s, a)r(s, a)



subject to:
X
XX
x(, a)
x(s, a)p(|s, a) = (),


X



S;



(o, c)(ao , o)(ao )
b(c),

c C, ao1 , ao2 , . . . A;

(33)



X

x(s, a)/X (a),

A;



x(s, a) 0,

S, A;

(a) {0, 1},

A,

P
X max x(s, a) constant finite upper bound expected number
times action used,
P exists discounted MDP. can, example, let
X = (1 )1 , since s,a x(s, a) = (1 )1 x valid occupation measure
MDP discount factor .
Example 9 Let us formulate MILP constrained problem Example 3. Recall
three resources = {ot , , om } (truck, forklift, mechanic), one capacity
543

fiDolgov & Durfee

type C = {c1 } (money), actions following resource requirements (listing
nonzero ones):
(a1 , ot ) = 1, (a2 , ot ) = 1, (a2 , ) = 1, (a3 , ot ) = 1, (a4 , ot ) = 1, (a4 , om ) = 1
resources following capacity costs:
(ot , c1 ) = 2, (of , c1 ) = 3, (om , c1 ) = 4,
agent limited budget, i.e., capacity bound
b(c1 ) = 8.
compute optimal policy arbitrary , formulate problem
MILP using techniques described above. Using binary variables {(ai )} = {i } =
{1 , 2 , 3 , 4 },14 express constraint capacity cost following system
|C||A||O| = 1(4)3 = 64 linear constraints:
(2)(1)1 + (3)(0)1 + (4)(0)1 8,
(2)(1)1 + (3)(0)1 + (4)(0)2 8,
(2)(1)1 + (3)(0)1 + (4)(0)3 8,
(2)(1)1 + (3)(0)1 + (4)(1)4 8,
(2)(1)1 + (3)(1)2 + (4)(0)1 8,
...
(2)(0)4 + (3)(0)4 + (4)(1)4 8.
easy see constraints redundant, fact action
requires small subset resources allows us prune many constraints.
fact, resource used byQmultiple actions ot . Therefore, accordance
earlier discussion, need |Ao | = 1 4 1 = 4 constraints:
(2)(1)1 + (3)(1)2 + (4)(1)4 8,
(2)(1)2 + (3)(1)2 + (4)(1)4 8,
(2)(1)3 + (3)(1)2 + (4)(1)4 8,
(2)(1)4 + (3)(1)2 + (4)(1)4 8,
four constraints corresponds case first resource (ot ) used
different action.
mentioned earlier, set X = (1 )1 constraints synchronize
occupation measure x binary indicators . Combining constraints
Q
(33), get MILP 12 continuous 4 binary variables, |S|+|C| |Ao |+
|A| = 3 + 4 + 3 = 10 constraints (not counting last two sets range constraints).

Finally, let us observe expanding resource action sets, problem
represented using binary resources only. domain contains mostly binary
requirements, may effective expand non-binary resource requirements
augmenting resource set O, use binary formulation Section 3.2 rather
directly applying more-general formulation described above.
14. create 0 noop action a0 , resource costs zero, drops
expressions.

544

fiResource Allocation Among Agents MDP-Induced Preferences

Appendix C. Experimental Setup
appendix details experimental domains constructed. delivery
domain |M| agents operating n-by-n grid sharing |O| resource types,
used following parameters.
resources enable agents carry delivery tasks. problem |O| resource
types, |O| delivery actions, performing action [1, |O|] requires random
subset resources (where number resources required action
important parameter, whose effect complexity discussed Section 5). probability
task [1, |O|] carried location 0.1+0.4(|O|i)/(|O|1), i.e., uniformly
distributed 0.1 0.5, function action ID (actions lower IDs
rewarding, per definition reward function below, executed
fewer locations).
n2 /5 possible delivery locations randomly placed grid. delivery
location assigned set delivery tasks executed (a single location
used multiple delivery tasks, single task carried several
locations). assignment tasks locations done randomly.
agent 4 + |O| actions: drive four perpendicular directions
execute one delivery tasks. drive actions result movement intended
direction probability 0.8 probability 0.2 produce change location.
movement actions incur negative reward, amount depends size
agent. problem |M| agents, movement penalty incurred agent
[1, |M|] 1 9(m 1)/(|M| 1), i.e., distributed uniformly [1, 10] function
agents ID.
Execution action corresponding delivery task [1, |O|] location
task assigned produces reward 100i/|O| moves agent new random
location grid. new location chosen randomly problem generation (thus
known agent), transition deterministic, induces topology nearby
remote locations. Attempting execution delivery task incorrect location
change state produces zero reward.
agents bid delivery resources |O| types. cglob |M| units
resource, cglob global constraint level (set 0.5 experiments,
described detail Section 5). one capacity type: size. size
requirements making deliveries type [1, |O|] i. capacity limit agent
cloc /2|O|(|O| + 1), cloc local constraint level (set 0.5
experiments, described detail Section 5).
initial location agent randomly selected uniform distribution.
discount factor = 0.95.

References
Altman, E. (1996). Constrained Markov decision processes total cost criteria: Occupation measures primal LP. Methods Models Operations Research, 43 (1),
4572.
545

fiDolgov & Durfee

Altman, E., & Shwartz, A. (1991). Adaptive control constrained Markov chains: Criteria policies. Annals Operations Research, special issue Markov Decision
Processes, 28, 101134.
Altman, E. (1999). Constrained Markov Decision Processes. Chapman HALL/CRC.
Bellman, R. (1961). Adaptive Control Processes: Guided Tour. Princeton University
Press.
Benazera, M. E., Brafman, R. I., Meuleau, N., & Hansen, E. (2005). Planning continuous resources stochastic domains. Proceedings Nineteenth International
Joint Conference Artificial Intelligence (IJCAI-05), pp. 12441251.
Bererton, C., Gordon, G., & Thrun, S. (2003). Auction mechanism design multi-robot
coordination. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Proceedings Conference
Neural Information Processing Systems (NIPS). MIT Press.
Bertsimas, D., & Tsitsiklis, J. N. (1997). Introduction Linear Optimization. Athena
Scientific.
Boutilier, C. (2002). Solving concisely expressed combinatorial auction problems. Proceedings Eighteenth National Conference Artificial Intelligence (AAAI-02),
pp. 359366.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proceedings Fourteenth International Joint Conference Artificial
Intelligence (IJCAI-95), pp. 11041111.
Boutilier, C., & Hoos, H. H. (2001). Bidding languages combinatorial auctions. Proceedings Seventeenth International Joint Conference Artificial Intelligence
(IJCAI-01), pp. 12111217.
Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 18, 1933.
de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: survey. INFORMS Journal
Computing, 15 (3), 284309.
Dolgov, D. (2006). Integrated Resource Allocation Planning Stochastic Multiagent
Environments. Ph.D. thesis, Computer Science Department, University Michigan.
Dolgov, D. A., & Durfee, E. H. (2004). Optimal resource allocation policy formulation loosely-coupled Markov decision processes. Proceedings Fourteenth
International Conference Automated Planning Scheduling (ICAPS-04), pp.
315324.
Dolgov, D. A., & Durfee, E. H. (2006). Resource allocation among agents preferences
induced factored MDPs. Proceedings Fifth International Joint Conference
Autonomous Agents Multiagent Systems (AAMAS-06), Hakodate, Japan.
Eckstein, J., Phillips, C., & Hart, W. (2000). Pico: object-oriented framework parallel
branch bound. Proceedings Workshop Inherently Parallel Algorithms
Optimization Feasibility Applications.
Feigenbaum, J., & Shenker, S. (2002). Distributed algorithmic mechanism design: Recent
results future directions. Proceedings Sixths International Workshop
546

fiResource Allocation Among Agents MDP-Induced Preferences

Discrete Algorithms Methods Mobile Computing Communications, pp.
113. ACM Press, New York.
Feldmann, R., Gairing, M., Lucking, T., Monien, B., & Rode, M. (2003). Selfish routing
non-cooperative networks: survey. Proceedings Twenty-Eights International
Symposium Mathematical Foundations Computer Science (MFCS-03), pp. 21
45. Springer-Verlag.
Ferguson, D., Nikolaou, C., Sairamesh, J., & Yemini, Y. (1996). Economic models allocating resources computer systems. Clearwater, S. (Ed.), Market-Based Control:
Paradigm Distributed Resource Allocation, pp. 156183, Hong Kong. World
Scientific.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. W. H. Freeman & Co.
Groves, T. (1973). Incentives teams. Econometrica, 41 (4), 617631.
Guestrin, C. (2003). Planning Uncertainty Complex Structured Environments.
Ph.D. thesis, Computer Science Department, Stanford University.
Heyman, D. P., & Sobel, M. J. (1984). Volume II: Stochastic Models Operations Research.
McGraw-Hill, New York.
Kallenberg, L. (1983). Linear Programming Finite Markovian Control Problems. Math.
Centrum, Amsterdam.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). complexity solving Markov
decision problems. Proceedings Eleventh Annual Conference Uncertainty
Artificial Intelligence (UAI95), pp. 394402, Montreal.
MacKie-Mason, J. K., & Varian, H. (1994). Generalized Vickrey auctions. Tech. rep.,
University Michigan.
Mas-Colell, A., Whinston, M. D., & Green, J. R. (1995). Microeconomic Theory. Oxford
University Press, New York.
McAfee, R. P., & McMillan, J. (1996). Analyzing airwaves auction. Journal Economic
Perspectives, 10 (1), 15975.
McMillan, J. (1994). Selling spectrum rights. Journal Economic Perspectives, 8 (3),
14562.
Meuleau, N., Hauskrecht, M., Kim, K.-E., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier,
C. (1998). Solving large weakly coupled Markov decision processes. Proceedings
Fifteenth National Conference Artificial Intelligence (AAAI-98), pp. 165
172.
Nisan, N. (2000). Bidding allocation combinatorial auctions. Electronic Commerce.
Parkes, D. (2001). Iterative Combinatorial Auctions: Achieving Economic Computational Efficiency. Ph.D. thesis, Department Computer Information Science,
University Pennsylvania.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance
Vickrey-based payment schemes exchanges. Proc. 17th International Joint Conference Artificial Intelligence (IJCAI-01), pp. 11611168.
547

fiDolgov & Durfee

Parkes, D. C., & Shneidman, J. (2004). Distributed implementations Vickrey-ClarkeGroves mechanisms. Proceedings Third International Joint Conference
Autonomous Agents Multi Agent Systems (AAMAS-04), pp. 261268.
Parkes, D. C., & Singh, S. (2003). MDP-based approach Online Mechanism Design.
Proceedings Seventeenths Annual Conference Neural Information Processing
Systems (NIPS-03).
Parkes, D. C., Singh, S., & Yanovsky, D. (2004). Approximately efficient online mechanism
design. Proceedings Eighteenths Annual Conference Neural Information
Processing Systems (NIPS-04).
Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons, New York.
Ross, K., & Chen, B. (1988). Optimal scheduling interactive non-interactive traffic
telecommunication systems. IEEE Transactions Automatic Control, 33, 261267.
Ross, K., & Varadarajan, R. (1989). Markov decision processes sample path constraints: communicating case. Operations Research, 37, 780790.
Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1998). Computationally manageable combinational auctions. Management Science, 44 (8), 11311147.
Sandholm, T., & Boutilier, C. (2006). Preference elicitation combinatorial auctions.
Cramton, Shoham, & Steinberg (Eds.), Combinatorial Auctions, chap. 10. MIT Press.
Sandholm, T. (1999). algorithm optimal winner determination combinatorial
auctions. Proceedings Sixteenth International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 542547, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Sandholm, T. (2002). Algorithm optimal winner determination combinatorial auctions. Artificial Intelligence, 135 (1-2), 154.
Shapley, L. S. (1953). Stochastic games. Proceedings National Academy Science, USA,
39, 10951100.
Sheffi, Y. (2004). Combinatorial auctions procurement transportation services.
Interfaces, 34 (4), 245252.
Singh, S., & Cohn, D. (1998). dynamically merge Markov decision processes.
Jordan, M. I., Kearns, M. J., & Solla, S. A. (Eds.), Advances Neural Information
Processing Systems, Vol. 10, pp. 10571063. MIT Press.
Song, J., & Regan, A. (2002). Combinatorial auctions transportation service procurement: carrier perspective. Transportation Research Record, 1833, 4046.
Vickrey, W. (1961). Counterspeculation, auctions competitive sealed tenders. Journal
Finance, 16, 837.
Wellman, M. P., Walsh, W. E., Wurman, P. R., & MacKie-Mason, J. K. (2001). Auction
protocols decentralized scheduling. Games Economic Behavior, 35, 271303.
Wolsey, L. (1998). Integer Programming. John Wiley & Sons.
548

fiResource Allocation Among Agents MDP-Induced Preferences

Wu, J., & Durfee, E. H. (2005). Automated resource-driven mission phasing techniques
constrained agents. Proceedings Fourth International Joint Conference
Autonomous Agents Multiagent Systems (AAMAS-05), pp. 331338.

549


