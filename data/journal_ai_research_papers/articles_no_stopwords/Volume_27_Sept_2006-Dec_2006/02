Journal Artificial Intelligence Research 27 (2006) 55-83

Submitted 10/05; published 09/06

Cognitive Principles Robust Multimodal Interpretation
Joyce Y. Chai
Zahar Prasov
Shaolin Qu

jchai@cse.msu.edu
prasovza@cse.msu.edu
qushaoli@cse.msu.edu

Department Computer Science Engineering
Michigan State University
East Lansing, MI 48824 USA

Abstract
Multimodal conversational interfaces provide natural means users communicate computer systems multiple modalities speech gesture.
build eective multimodal interfaces, automated interpretation user multimodal inputs
important. Inspired previous investigation cognitive status multimodal
human machine interaction, developed greedy algorithm interpreting user
referring expressions (i.e., multimodal reference resolution). algorithm incorporates
cognitive principles Conversational Implicature Givenness Hierarchy applies constraints various sources (e.g., temporal, semantic, contextual) resolve
references. empirical results shown advantage algorithm eciently
resolving variety user references. simplicity generality, approach
potential improve robustness multimodal input interpretation.

1. Introduction
Multimodal systems provide natural eective way users interact computers
multiple modalities speech, gesture, gaze. Since rst appearance
Put-That-There system (Bolt, 1980), number multimodal systems
built, among systems combine speech, pointing (Neal & Shapiro, 1991;
Stock, 1993), gaze (Koons, Sparrell, & Thorisson, 1993), systems integrate speech
pen inputs (e.g., drawn graphics) (Cohen, Johnston, McGee, Oviatt, Pittman, Smith,
Chen, & Clow, 1996; Wahlster, 1998), systems combine multimodal inputs outputs
(Cassell, Bickmore, Billinghurst, Campbell, Chang, Vilhjalmsson, & Yan, 1999), systems
mobile environments (Oviatt, 1999a), systems engage users intelligent
conversation (Gustafson, Bell, Beskow, Boye, Carlson, Edlund, Granstrom, House, & Wiren,
2000; Stent, Dowding, Gawron, Bratt, & Moore, 1999). Earlier studies shown
multimodal interfaces enable users interact computers naturally eectively
(Oviatt, 1996, 1999b).
One important aspect building multimodal systems multimodal interpretation,
process identies meanings user inputs. particular, key element
multimodal interpretation known reference resolution, process nds
proper referents referring expressions. referring expression phrase
given user inputs (most likely speech inputs) refer specic
entity entities. referent entity (e.g., specic object) user refers.
Suppose user points House 6 screen says much one.
c
2006
AI Access Foundation. rights reserved.

fiChai, Prasov, & Qu

case, reference resolution must infer referent House 6 assigned
referring expression one. paper particularly addresses problem reference
resolution multimodal interpretation.
multimodal conversation, way users communicate system depends
available interaction channels situated context (e.g., conversation focus, visual
feedback). dependencies form rich set constraints various aspects (e.g.,
semantic, temporal, contextual). correct interpretation attained
simultaneously considering constraints.
Previous studies shown user referring behavior multimodal conversation
occur randomly, rather follows certain linguistic cognitive principles.
human machine interaction, earlier work shown strong correlations
cognitive status Givenness Hierarchy form referring expressions (Kehler, 2000).
Inspired early work, developed greedy algorithm multimodal reference
resolution. algorithm incorporates principles Conversational Implicature
Givenness Hierarchy applies constraints various sources (e.g., gesture, conversation
context, visual display). empirical results shown promise algorithm
eciently resolving variety user references. One major advantage greedy
algorithm prior linguistic cognitive knowledge used guide
search prune search space constraint satisfaction. simplicity
generality, approach potential improve robustness interpretation
provide practical solution multimodal reference resolution (Chai, Prasov, Blaim,
& Jin, 2005).
following sections, rst demonstrate dierent types referring behavior
observed studies. briey introduce underlying cognitive principles
human-human communication describe principles used computational model eciently resolve multimodal references. Finally, present
experimental results.

2. Multimodal Reference Resolution
previous work (Chai, Hong, & Zhou, 2004b; Chai, Hong, Zhou, & Prasov, 2004),
multimodal conversational system developed users acquire real estate information1 .
Figure 1 snapshot graphical user interface. Users interact interface
speech gesture. Table 1 shows fragment conversation.
fragment, user exhibits dierent types referring behavior. example,
input U1 considered simple input. type simple input one
referring expression spoken utterance one accompanying gesture. Multimodal
fusion combines information speech gesture likely resolve
refers to. second user input (U2 ), accompanying gesture referring
expression explicitly used speech utterance. time, system needs
use conversation context infer object interest house mentioned
previous turn conversation. third user input, multiple referring
expressions multiple gestures. types inputs considered complex inputs.
1. first prototype system developed IBM T. J. Watson Research Center P. Hong,
M. Zhou, colleagues Intelligent Multimedia Interaction group.

56

fiMinimizing Conflicts: Heuristic Repair Method

Figure 1: snapshot multimodal conversational system.

U1
S1
U2
S2
U3
S3

Speech: much cost?
Gesture: point position screen
Speech: price 400K
Graphics: highlight house discussion
Speech: large?
Speech: 2500 square feet
Speech: Compare house one
Gesture: ....circle....cirle (put two consecutive circles screen)
Speech: comparison results
Graphics: show table comparison

Table 1: fragment demonstrating interaction dierent types referring behavior
Complex inputs dicult resolve. need consider temporal relations
referring expressions gestures, semantic constraints specied
referring expressions, contextual constraints prior conversation.
example, case U3 , system needs understand refers house
focus previous turn; house one aligned
two consecutive gestures. subtle variations constraints, including
temporal ordering, semantic compatibility, gesture recognition results lead
dierent interpretations.
example, see multimodal conversation, way user interacts system dependent available input channels (e.g., speech
gesture), upon his/her conversation goals, state conversation,
multimedia feedback system. words, rich context involves
57

fiChai, Prasov, & Qu

dependencies many dierent aspects established interaction. Interpreting
user inputs situated rich context. example, temporal relations
speech gesture important criteria determine information
two modalities combined. focus attention prior conversation
shapes users refer objects, thus, inuences interpretation referring
expressions. Therefore, need simultaneously consider temporal relations
referring expressions gestures, semantic constraints specied referring expressions, contextual constraints prior conversation. paper,
present ecient approach driven cognitive principles combine temporal,
semantic, contextual constraints multimodal reference resolution.

3. Related Work
Considerable eort devoted studying user multimodal behavior (Cohen, 1984;
Oviatt, 1999a) mechanisms interpret user multimodal inputs (Chai et al., 2004b;
Gustafson et al., 2000; Huls, Bos, & Classen, 1995; Johnston, Cohen, McGee, Oviatt,
Pittman, & Smith, 1997; Johnston, 1998; Johnston & Bangalore, 2000; Kehler, 2000; Koons
et al., 1993; Neal & Shapiro, 1991; Oviatt, DeAngeli, & Kuhn, 1997; Stent et al., 1999; Stock,
1993; Wahlster, 1998; Wu & Oviatt, 1999; Zancanaro, Stock, & Strapparava, 1997).
multimodal reference resolution, early work keeps track focus space
dialog (Grosz & Sidner, 1986) display model capture objects visible
graphical display (Neal, Thielman, Dobes, M., & Shapiro, 1998). checks semantic
constraints type candidate objects referenced properties
reference resolution. modied centering model multimodal reference resolution
introduced previous work (Zancanaro et al., 1997). idea based
centering movement turns, segments discourse constructed.
discourse entities appearing segment accessible current turn
used constrain referents referring expressions. Another approach introduced
use contextual factors multimodal reference resolution (Huls et al., 1995).
approach, salience value assigned instance based contextual factors.
determine referents multimodal referring expressions, approach retrieves
salient referent satises semantic restrictions referring expressions.
earlier approaches greedy nature, largely dependent semantic
constraints and/or constraints conversation context.
resolve multimodal references, two important issues. First mechanism combine information various sources modalities. second capability obtain best interpretation (among possible alternatives) given set
temporal, semantic, contextual constraints. section, give brief introduction
three recent approaches address issues.
3.1 Multimodal Fusion
Approaches multimodal fusion (Johnston, 1998; Johnston & Bangalore, 2000), although
focus dierent problem overall input interpretation, provide eective solutions
reference resolution. two major approaches multimodal fusion: unication58

fiMinimizing Conflicts: Heuristic Repair Method

based approaches (Johnston, 1998) nite state approaches (Johnston & Bangalore,
2000).
unication-based approach identies referents referring expressions unifying
feature structures generated speech utterances gestures using multimodal grammar (Johnston et al., 1997; Johnston, 1998). multimodal grammar combines
temporal spatial constraints. Temporal constraints encode absolute temporal relations speech gesture (Johnston, 1998),. grammar rules predened
based empirical studies multimodal interaction (Oviatt et al., 1997). example, one
rule indicates speech gesture combined speech either overlaps
gesture follows gesture within certain time frame. unication approach
process certain complex cases (as long satisfy predened multimodal
grammar) speech utterance accompanied one gesture dierent
types (Johnston, 1998). Using approach accommodate various situations
described Figure 1 require adding dierent rules cope situation.
specic user referring behavior exactly match existing integration rules
(e.g., temporal relations), unication would fail therefore references would
resolved.
nite state approach applies nite-state transducers multimodal parsing
understanding (Johnston & Bangalore, 2000). Unlike unication-based approach
chart parsing subject signicant computational complexity concerns (Johnston
& Bangalore, 2000), nite state approach provides ecient, tight-coupling
multimodal understanding speech recognition. approach, multimodal contextfree grammar dened transform syntax multimodal inputs semantic
meanings. domain-specic semantics directly encoded grammar. Based
grammars, multi-tape nite state automata constructed. automata
used identifying semantics combined inputs. Rather absolute temporal
constraints unication-based approach, approach relies temporal order
dierent modalities. parsing stage, gesture input gesture
tape (e.g., pointing particular person) combined speech expression
speech tape (e.g., person) considered referent expression.
problem approach multi-tape structure takes input speech
gesture incorporate conversation history consideration.
3.2 Decision List
identify potential referents, previous work investigated Givenness Hierarchy (to
introduced later) multimodal interaction (Kehler, 2000). Based data collected
Wizard Oz experiments, investigation suggests users tend tailor
expressions perceive systems beliefs concerning cognitive status
referents prominence (e.g., highlight) display. tailored referring
expressions resolved high accuracy based following decision list:
1. object gestured to, choose object.
2. Otherwise, currently selected object meets semantic type constraints imposed
referring expression, choose object.
59

fiChai, Prasov, & Qu

3. Otherwise, visible object semantically compatible, choose
object.
4. Otherwise, full NP (such proper name) used uniquely identify referent.
studies (Chai, Prasov, & Hong, 2004a), found decision list
following limitations:
Depending interface design, ambiguities (from systems perspective) could
occur. example, given interface one object (e.g., house) sometimes
created top another object (e.g., town), pointing gesture could result
multiple potential objects. Furthermore, given interface crowded objects,
nger point could result multiple objects dierent probabilities.
decision list able handle ambiguous cases.
User inputs always simple (consisting one referring expression
one gesture indicated decision list). fact, study (Chai et al.,
2004a), found user inputs complex, consisting multiple referring
expressions and/or multiple gestures. referents referring expressions
could come dierent sources, gesture inputs conversation context.
temporal alignment speech gesture important determining
correct referent given expression. decision list able handle
types complex inputs.
Nevertheless, previous ndings (Kehler, 2000) inspired work provided
basis algorithm described paper.
3.3 Optimization
Recently, probabilistic approach developed optimizing reference resolution based
graph matching (Chai et al., 2004b). graph-matching approach, information
gathered multiple input modalities conversation context represented
attributed relational graphs (ARGs) (Tsai & Fu, 1979). Specically, two graphs used.
One graph represents referring expressions speech utterances (i.e., called referring
graph). referring graph contains referring expressions used speech utterance
relations expressions. node corresponds one referring expression
consists semantic temporal information extracted expression.
edge represents semantic temporal relation two referring expressions.
resulting graph fully connected, undirected, graph. example, shown
Figure 2(a), speech input compare house, green house, brown one,
three nodes generated referring graph representing three referring expressions.
node contains semantic temporal features related corresponding referring
expression. include expressions semantic type (house, town, etc.), number
potential referents, type dependent features (size, price, etc.), syntactic category
expression, timestamp expression produced. edge contains
features describing semantic temporal relations pair nodes. semantic
features simply indicate whether two nodes share semantic type
60

fiMinimizing Conflicts: Heuristic Repair Method

Figure 2: Reference resolution probabilistic graph-matching

inferred utterance. Otherwise, semantic type relation deemed
unknown. temporal features indicate two expressions uttered rst.
Similarly, another graph represents potential referents gathered gestures, history, visual display (i.e., called referent graph). node referent graph
captures semantic temporal information potential referent, together
selection probability. selection probability particularly applied objects indicated gesture. gesture pointing circle potentially introduce
ambiguity terms intended referents, selection probability used indicate
likely object selected particular gesture. selection probability
derived function distance location entity focus point
recognized gesture display. referring graph, edge referent
graph captures semantic temporal relations two potential referents
whether two referents share semantic type temporal order
two referents introduced discourse. example, since gesture input
consists two pointings, referent graph (Figure 2b) consists potential referents
two pointings. objects rst dashed rectangle potential referents
selected rst pointing, second dashed rectangle correspond
second pointing. Furthermore, salient objects prior conversation included referent graph since could potential referents well (e.g.,
rightmost dashed rectangle Figure 2b).
Given graph representations, reference resolution problem becomes probabilistic graph-matching problem (Gold & Rangarajan, 1996). goal nd match
referring graph Gs referent graph Gc 2 achieves maximum
compatibility (i.e., maximizes Q(Gc , Gs )) described following equation:
2. subscription Gs refers speech referring expressions c Gc refers candidate referents.

61

fiChai, Prasov, & Qu

Q(Gc , Gs ) =


(x , )N odeSim(x , )
x
P




+

x





n P (x , )P (y , n )EdgeSim(xy , mn )

(1)

P (x , ) matching probability referent node x referring node
. overall compatibility Q(Gc , Gs ) depends node compatibility N odeSim
edge compatibility EdgeSim, dened temporal semantic
constraints (Chai et al., 2004). algorithm converges, P (x , ) gives matching
probabilities referent node x referring node maximizes overall
compatibility function. Using matching probabilities, system able identify
probable referent x referring node . Specically, referring expression
matches potential referent assigned referent probability match
exceeds empirically computed threshold. threshold met, referring
expression remains unresolved.
Theoretically, approach provides solution maximizes overall satisfaction
semantic, temporal, contextual constraints. However, many optimization
approaches, algorithm non-polynomial. relies expensive matching process,
attempts every possible assignment, order converge optimal interpretation
based constraints. However, previous linguistic cognitive studies indicate
user language behavior occur randomly, rather follows certain cognitive principles. Therefore, question arises whether knowledge cognitive principles
used guide matching process reduce complexity.

4. Cognitive Principles
Motivated previous work (Kehler, 2000), specically focus two principles: Conversational Implicature Givenness Hierarchy.
4.1 Conversational Implicature
Grices Conversational Implicature Theory indicates interpretation inference
utterance communication guided set four maxims (Grice, 1975). Among
four maxims, Maxim Quantity Maxim Manner particularly useful
purpose.
Maxim Quantity two components: (1) make contribution informative required (for current purposes exchange), (2) make
contribution informative required. context multimodal conversation,
maxim indicates users generally make unnecessary gestures speech
utterances. especially true pen-based gestures since usually require special
eort user. Therefore, pen-based gesture intentionally delivered user,
information conveyed often crucial component used interpretation.
Grices Maxim Manner four components: (1) avoid obscurity expression, (2)
avoid ambiguity, (3) brief, (4) orderly. maxim indicates users
intentionally make ambiguous references. use expressions (either speech
gesture) believe uniquely describe object interest listeners (in
case computer system) understand. expressions choose depend
62

fiMinimizing Conflicts: Heuristic Repair Method

Status
Expression Form
f ocus


Activated
that, this, N

F amiliar
N

U nique identif iable
N

Ref erential
indef inite N

Identif iable

Figure 3: Givenness Hierarchy

information mental models current state conversation. However,
information users mental model might dierent information system
possesses. information gap happens, dierent ambiguities could occur
system point view. fact, ambiguities intentionally caused
human speakers, rather systems incapability choosing among alternatives
given incomplete knowledge representation, limited capability contextual inference,
factors (e.g., interface design issues). Therefore, system anticipate
deliberate ambiguities users (e.g., user utters house refer particular
house screen), rather focus dealing types ambiguities
caused systems limitations (e.g., gesture ambiguity due interface design
speech ambiguity due incorrect recognition).
two maxims help positioning role gestures reference resolution.
particular, maxims put potential referents indicated gesture
important position, described Section 5.
4.2 Givenness Hierarchy
Givenness Hierarchy proposed Gundel et al. explains dierent determiners
pronominal forms signal dierent information memory attention state (i.e.,
cognitive status) (Gundel, Hedberg, & Zacharski, 1993). Figure 3, six
cognitive statuses hierarchy. example, focus indicates highest attentional
state likely continue topic. Activated indicates entities short term
memory. statuses associated forms referring expressions.
hierarchy, cognitive status implies statuses list. example, focus
implies Activated, Familiar, etc. use particular expression form signals
associated cognitive status met, signals lower statuses
met. words, given form used describe lower status used
refer higher status, vice versa. Cognitive statuses necessary conditions
63

fiChai, Prasov, & Qu

appropriate use dierent forms referring expressions. Gundel et al. found dierent
referring expressions almost exclusively correlate six statuses hierarchy.
Givenness Hierarchy investigated earlier algorithms resolving pronouns demonstratives spoken dialog systems (Eckert & Strube, 2000; Byron, 2002)
multimodal interaction (Kehler, 2000). particular, would extend previous work (Kehler, 2000) investigate whether Conversational Implicature Givenness Hierarchy used resolve variety references simple complex,
precise ambiguous. Furthermore, decision list used Kehler (2000) proposed based data analysis implemented evaluated real-time
system. Therefore, second goal design implement ecient algorithm
incorporating cognitive principles empirically compare performance
optimization approach (Chai et al., 2004), nite state approach (Johnston & Bangalore,
2000), decision list approach (Kehler, 2000).

5. Greedy Algorithm
greedy algorithm always makes choice looks best moment processing.
is, makes locally optimal choice hope choice lead globally optimal solution. Simple ecient greedy algorithms used approximate
many optimization problems. explore use Conversational Implicature
Givenness Hierarchy designing ecient greedy algorithm. particular, extend
decision list Kehler (2000) utilize concepts two cognitive principles
following way:
Corresponding Givenness Hierarchy, following hierarchy holds potential
referents: F ocus > V isible. hierarchy indicates objects focus higher
status terms attention states objects visual display. Focus
corresponds cognitive statuses focus Activated Givenness Hierarchy,
Visible corresponds statuses Familiar Uniquely identifiable. Note
Givenness Hierarchy ne grained terms dierent statuses. application
may able distinguish dierence statuses (e.g., focus
Activated) eectively use them. Therefore, Focus Visible introduced
group similar statuses (with respect application) together. Since
need dierentiate objects mentioned recently (e.g.,
focus activated) objects accessible either graph display
domain model (e.g., familiar unique identiable), assign
dierent modied statuses (e.g., Focus Visible).
Based Conversational Implicature, since pen-based gesture takes special effort deliver, must convey certain useful information. fact, objects indicated
gesture highest attentional state since deliberately singled
user. Therefore, combining (1) (2), derive modied hierarchy
Gesture > F ocus > V isible > Others. Others corresponds indenite cases
Givenness Hierarchy. modied hierarchy coincides processing order
Kehlers decision list (2000). modied hierarchy guide greedy
64

fiMinimizing Conflicts: Heuristic Repair Method

algorithm search solutions. Next, describe detail algorithm
related representations functions.
5.1 Representation
turn3 (i.e., receiving user input) conversation, use three vectors
represent rst three statuses modied hierarchy: objects selected gesture,
objects focus, objects visible display follows:
Gesture vector (g ) captures objects selected series gestures. element gi
object potentially selected gesture. elements gi gj < j,
gesture selects objects gi should: 1) temporally precede gesture selects
gj 2) gesture selects gj since one gesture could result
multiple objects.
Focus vector (f) captures objects focus selected
gesture. element represents object considered focus attention
previous turn conversation. temporal precedence relation
elements. consider corresponding objects simultaneously
accessible current turn conversation.
captures objects visible display neither
Display vector (d)
selected gesture (i.e., g) focus (f). temporal precedence relation elements. elements simultaneously accessible.
Based representations, object domain interest belongs either
one vectors Others. object vectors consists
following attributes:
Semantic type object. example, semantic type could House
Town.
attributes object. domain dependent feature. set attributes
associated semantic type. example, house object Price, Size,
Year Built, etc. attributes. Furthermore, object visual properties
reect appearance object display Color object icon.
identier object. object unique name.
selection probability. refers probability given object selected.
Depending interface design, gesture could result list potential referents.
use selection probability indicate likelihood object selected
gesture. calculation selection probability described later. objects
focus vector display vector, selection probabilities set 1/N
N total number objects respective vector.
3. Currently, user inactivity (i.e., 2 seconds input either speech gesture) used
boundary decide interaction turn.

65

fiChai, Prasov, & Qu

Temporal information. relative temporal ordering information corresponding gesture. Instead applying time stamps previous work (Chai et al.,
2004b), use index gestures according order occurrences. object selected rst gesture, temporal information
would 1.
addition vectors capture potential referents, user input, vector
represents referring expressions speech utterance (r) maintained.
element (i.e., referring expression) following information:
identier potential referent indicated referring expression.
example, identier potential referent expression house number eight
house object identier Eight.
semantic type potential referents indicated expression. example,
semantic type referring expression house House.
number potential referents indicated referring expression
utterance context. example, singular noun phrase refers one object.
phrase three houses provides exact number referents (i.e., 3).
Type dependent features. features associated potential referents,
Color Price, extracted referring expression.
temporal ordering information indicating order referring expressions
uttered. Again, instead specic time stamp, use
temporal ordering information. utterance consists N consecutive referring
expressions, temporal ordering information would 1, 2,
N .
syntactic categories referring expressions. Currently, referring
expression, assign one six syntactic categories (e.g., demonstrative
pronoun). Details explained later.
four vectors updated user turn conversation based current
user input system state (e.g., shown screen identied
focus previous turn conversation).
5.2 Algorithm
ow chart pseudo code algorithm shown Figure 4.
multimodal input particular turn conversation, algorithm takes inputs
vector (r) referring expressions size k, gesture vector (g ) size m, focus
size l. rst creates three matrices
vector (f ) size n, display vector (d)
G[i][j], F [i][j], D[i][j] capture scores matching referring expression
r object three vectors. Calculation matching score described later.
Note that, g ,f, empty, corresponding matrix (i.e., G, F ,
D) empty.
66

fiMinimizing Conflicts: Heuristic Repair Method

InitializeMatchMatrix (,,,){
(i = 1..m; j = 1..k) G[i][j] = Match(gi, rj)
(i = 1..n; j = 1..k) F[i][j] = Match(fi, rj)
(i = 1..l; j = 1..k) D[i][j] = Match(di, rj)
}

Yes

G empty


GreedySortingGesture {
index_max = 1; //index column
(i = 1..m) {
find j index_max, G[i][j] largest among elements row i.
add mark * G[i][j];
index_max = j; } //complete finding best match view object
AssignReferentsFromMatrix (G);
}

references resolved?



Yes

Yes
F empty



Return results

GreedySortingFocus{
(j = 1..k)
(rj resolved)
Cross column j F //only keep ones resolved
( = 1..n){
find j F[i][j] largest among elements row i.
mark * F[i][j]; }
AssignReferentsFromMatrix (F);
}

references resolved?



GreedySortingDisplay{
(j = 1..k)
(rj resolved)
Cross column j D;
( = 1..l){
find j D[i][j] largest among elements row i.
mark * D[i][j]; }
AssignReferentsFromMatrix (D);
}

Return results

AssignReferentsFromMatrix (Matrix X){
(i = 1..k) // i.e., expression ri column
(ri indicates specific number N N elements
ith column X *)
assign N largest elements * ri referents.
else assign elements * ri referents;
}

Figure 4: greedy algorithm multimodal reference resolution

67

Yes

Return results

fiChai, Prasov, & Qu

Based matching scores three matrices, algorithm applies greedy
search guided modied hierarchy described earlier. Since Gesture
highest status, algorithm rst searches Gesture Matrix (G) keeps track
matching scores referring expressions objects gestures. identies
highest (or multiple highest) matching scores assigns possible objects
gestures expressions (GreedySortingGesture).
referring expressions left resolved gestures processed,
algorithm looks objects Focus Matrix (F ) since Focus next highest cognitive status (GreedySortingFocus). still expressions resolved,
algorithm looks objects Display Matrix (D) (GreedySortingDisplay). Currently,
algorithm focuses three statuses. Certainly, still expressions
resolved steps, algorithm consult proper name resolution.
referring expressions resolved, system output results.
next multimodal input, system generate four new vectors apply greedy
algorithm again.
Note GreedySortingGesture, use index-max keep track column index
corresponds largest matching value. algorithm incrementally processes
row matrix, index-max incrementally increase. referring expressions gesture aligned according order occurrences.
Since objects Focus Matrix Display Matrix temporal precedence
relations, GreedySortingFocus GreedySortingDisplay use constraint.
reason call algorithm greedy always nds best assignment
referring expression given cognitive status hierarchy. words, algorithm
always makes best choice referring expression one time according
order occurrence utterance. One imagine mistaken assignment
made expression aect assignment following expressions. Therefore,
greedy algorithm may lead globally optimal solution. Nevertheless, general
user behavior following guiding principles makes greedy algorithm useful.
One major advantage greedy algorithm use modied hierarchy signicantly prune search space compared graph-matching approach.
Given referring expressions n potential referents various sources (e.g., gesture,
conversation context, visual display), algorithm nd solution O(mn).
Furthermore, algorithm goes beyond simple precise inputs illustrated
decision list Kehler (2000). scoring mechanism (described later) greedy
sorting process accommodate complex ambiguous user inputs.
5.3 Matching Functions
important component algorithm matching score object (o)
referring expression (e). use following equation calculate matching score:
atch(o, e) = [



P (o|S) P (S|e)] Compatibility(o, e)

(2)

S{G,F,D}

formula, represents possible associated status object o. could
three potential values: G (representing Gesture), F (Focus), (Display).
function determined three components:
68

fiMinimizing Conflicts: Heuristic Repair Method

rst, P (o|S), object selectivity component measures probability
object referent given status (S) object (i.e., gesture, focus,
visual display).
second, P (S|e), likelihood status component measures likelihood
status potential referent given particular type referring expression.
third, Compatibility(o, e), compatibility component measures
semantic temporal compatibility object referring expression
e.
Next explain three components detail.
5.3.1 Object Selectivity
calculate P (o|S = Gesture), use function takes consideration
distance object focus point gesture display (Chai et al.,
2004b).
Given object Focus (i.e., selected gesture), P (o|S = F ocus) = 1/N ,
N total number objects Focus vector. object neither
selected gesture, focus, visible screen, P (o|S = Display) =
1/M , total number objects Display vector. Currently,
applied simplest uniform distribution objects focus graphical
display. future, intend incorporate recency conversation discourse
model P (o|S = F ocus) use visual prominence (e.g., based visual characteristics)
model P (o|S = Display). Note that, discussed earlier Section 5.1, object
associated one three statuses. words, given object o,
one P (o|S = Gesture), P (o|S = F ocus), P (o|S = Display) non-zero.
5.3.2 Likelihood Status
Motivated Givenness Hierarchy earlier work (Kehler, 2000) form
referring expressions reect cognitive status referred entities users mental
model, use likelihood status measure probability reected status given
particular type referring expression. particular, use data reported Kehler
(2000) derive likelihood status potential referents given particular type
referring expression P (S|e). categorize referring expressions following six
categories:
Empty: referring expression used utterance.
Pronouns: it, they,
Locative adverbs:
Demonstratives: this, that, these,
Denite Noun Phrases: noun phrases denite article
Full noun phrases: types proper nouns.
69

fiChai, Prasov, & Qu

P (S|E)
Visible
Focus
Gesture
Sum

Empty
0
0.56
0.44
1

Pronoun
0
0.85
0.15
1

Locative
0
0.57
0.43
1

Demonstratives
0
0.33
0.67
1

Definite
0
0.07
0.67
1

Full
0
0.47
0.16
1

Table 2: Likelihood status referents given particular type expression
Table 2 shows estimated P (S|e). Note that, original data provided Kehler
(2000), zero count certain combination referring type referent status.
zero counts result zero probability table. use smoothing
techniques re-distribute probability mass. Furthermore, probability mass
assigned status Others.
5.3.3 Compatibility Measurement
term Compatibility(o, e) measures compatibility object referring
expression e. Similar compatibility measurement earlier work (Chai et al.,
2004), dened multiplication many factors following equation:
Compatibility(o, e) = Id(o, e) Sem(o, e)



Attrk (o, e) emp(o, e)

(3)

k

equation:
Id(o, e) captures compatibility identier (or name) identier
(or name) specied e. indicates identier potential referent,
expressed referring expression, match identier true referent.
particularly useful resolving proper nouns. example, referring
expression house number eight, correct referent identier
number eight. Id(o, e) = 0 identities e dierent. Id(o, e) = 1
identities e either one/both unknown.
Sem(o, e) captures semantic type compatibility e. indicates
semantic type potential referent expressed referring expression
match semantic type correct referent. Sem(o, e) = 0 semantic types
e dierent. Sem(o, e) = 1 unknown.
Attrk (o, e) captures type-specic constraint concerning particular semantic feature
(indicated subscript k). constraint indicates expected features
potential referent expressed referring expression compatible
features associated true referent. example, referring expression
Victorian house, style feature Victorian. Therefore, object
possible referent style object Victorian. Thus, dene following:
Attrk (o, e) = 0 e feature k values feature k
equal. Otherwise, Attrk (o, e) = 1.
70

fiMinimizing Conflicts: Heuristic Repair Method

(House 3
(House 9 (House 1
Town 1)
Town 2) Town 2)
Gesture input: ... ..i..i...i
Speech input: Compare houses.
Time

Figure 5: example complex input

emp(o, e) captures temporal compatibility e. consider temporal ordering speech gesture. Specically, temporal
compatibility dened following:
emp(o, e) = exp(|OrderIndex(o) OrderIndex(e)|)

(4)

order speech accompanying gestures occur important
deciding gestures aligned referring expressions.
order accompanying gestures introduced discourse
consistent order corresponding referring expressions
uttered. example, suppose user input consists three gestures g1 , g2 , g3
two referring expressions, s1 , s2 . possible g3 align s1
g2 align s2 . Note that, status object either Focus Visible,
emp(o, e) = 1. denition temporal compatibility dierent
function used previous work (Chai et al., 2004) takes real time stamps
consideration. Section 6.2 shows dierent performance results based dierent
temporal compatibility functions.
5.4 Example
Figure 5 shows example complex input involves multiple referring expressions
multiple gestures. interface displays house icons top town icons,
point (or circle) could result house town object. example, rst
gesture results House 3 Town 1. second gesture results House 9
Town 2, third results House 1 Town 2. Suppose input takes
place, House 8 highlighted screen previous turn conversation (i.e.,
House 8 focus). Furthermore, eight objects visible screen.
resolve referents expressions houses, greedy algorithm takes
following steps:
r created lengths 6, 1, 8, 2, respectively
1. four input vectors, g ,f, d,
represent six objects gesture vector, one object focus, eight objects
graphical display, two referring expressions used utterance.
2. Gesture Matrix G62 , Focus Matrix F12 , Display Matrix D82 created.
3. three matrixes initialized Equation 2. Figure 6 shows resulting
Gesture Matrix. probability values P (S|e) come Table 2. dierence
71

fiChai, Prasov, & Qu

Status
(G)

Referring Expression Match

Potential
Referent

j = 1:



j = 2: houses

1 0.15 1 = 0.15

= 1: House 3

1 0.67 0.37 = 0.25*

Gesture 1
= 2: Town 2

1 0.15 0 = 0

1 0.67 0 = 0

= 3: House 9

1 0.15 0.37 = 0.055

1 0.67 1 = 0.67*

= 4: Town 2

1 0.15 0 = 0

1 0.67 0 = 0

Gesture 2
= 5: House 1

1 0.15 0.14 = 0.02

1 0.67 0.37 = 0.25*

= 6: Town 2

1 0.15 0 = 0

1 0.67 0 = 0

Gesture 3

(a) Gesture Matrix
Status
(F)

Potential
Referent

Referring Expression Match

Focus

= 1: House 8

j = 1:



j = 2: houses

1 0.85 1= 0.85*

(b) Focus Matrix

Figure 6: Gesture Matrix (a) Focus Matrix (b) processing example Figure 5.
cell Referring Expression Match columns corresponds instantiation
matching function.

compatibility values house objects Gesture Matrix mainly due
temporal ordering compatibilities.
4. Next GreedySortingGesture procedure executed. row Gesture Matrix, algorithm nds largest legitimate value marks corresponding cell
*. legitimate means corresponding cell row + 1
either column column right corresponding cell
row i. values shown bold Figure 6(a). Next, starting
column, algorithm checks referring expression whether exists
corresponding column. so, objects assigned referring
expressions based number constraints. case, since specic number
given referring expression houses, three marked objects assigned
houses.
5. houses, still left resolved. algorithm continues
execute GreedySortingFocus. Focus Matrix prior executing GreedySortingFocus
shown Figure 6(b). Note since houses longer considered,
corresponding column deleted Focus Matrix. Similar previous step,
largest non-zero match value marked (shown bold Figure 6(b)) assigned
remaining referring expression it.
6. resulting Display Matrix shown point, referring expressions resolved.
72

fiMinimizing Conflicts: Heuristic Repair Method

s1 : the(adj) (N |N s)
s2 : (this|that)(adj )N
s3 : (these|those)(num+ )(adj )N
s4 : it|this|that|(this|that|the)adj one
s5 : (these|those)num+ adj ones|them
s6 : here|there
s7 : empty expression
s8 : proper nouns
s9 : multiple expressions
Total Num:

g1

gest.
2
4
0
3
0
1
1
1
1
13

g2
one
pt
8
43
0
8
0
1
1
5
0
66

g3
mult.
pts
0
3
0
0
0
0
0
3
4
10

g4
one
cir
2
33
31
10
2
5
1
3
11
98

g5
mult.
cirs
0
1
0
0
0
0
0
0
13
14

g6
pts &
cirs
1
7
5
0
0
0
0
3
2
18

Total
Num
13
91
36
21
2
7
3
15
31
219

Table 3: Detailed description user referring behavior

6. Evaluation
use data collected previous work (Chai et al., 2004) evaluate greedy
algorithm. questions addressed evaluation following:
impact temporal alignment speech gesture performance greedy algorithm?
role modeling cognitive status greedy algorithm?
eective greedy algorithm compared graph matching algorithm
(Section 3.3)?
error sources contribute failure real-time reference resolution?
greedy algorithm compared nite state approach (Section 3.1)
decision list approach (Section 3.2)?
6.1 Experiment Setup
evaluation data collected eleven subjects participated study.
subjects asked interact system using speech gestures
(e.g., pointing circle) accomplish tasks related real estate information seeking.
rst task nd least expensive house populated town. order
accomplish task, user would rst nd town highest
population nd least expensive house town. next task involved
obtaining description house located previous task. next task
compare house located rst task houses particular
town terms price. Additionally, least expensive house second town
determined. Another task nd expensive house particular town.
73

fiChai, Prasov, & Qu

S0 : referring expression
S1 : One referring expression
S2 : Multiple referring expressions
Total Num:

G0 :
Gesture
1 (a)
11 (a)
1 (c)
13

G1 : One
Gesture
2 (a)
151 (b)
11 (c)
164

G2 : MultiGesture
0 (c)
23 (c)
19 (c)
42

Total
Num
3
185
31
219

Table 4: Summary user referring behavior
last task involved comparing resulting houses previous four tasks.
last task, previous four tasks may completely partially repeated.
tasks designed users required explore interface acquire various
types information.
acoustic model subject trained individually minimize speech recognition errors. study session videotaped capture audio video
screen movement (including gestures system responses). IBM Viavoice speech
recognizer used process speech input.
Table 3 provides detailed description referring behavior observed study.
columns indicate whether gesture, one gesture (pointing circle), multiple gestures involved multimodal input. rows indicate type referring expressions
speech utterance. table entry shows number particular combination
speech gesture inputs.
Table 4 summarizes Table 3 terms whether gesture, one gesture, multiple
gestures (shown columns) whether referring expression, one referring expression,
multiple referring expressions (shown rows) involved input. Note
table intended input counted one input even input may split
turns system run time.
Based Table 4, categorize user inputs following three categories:
Simple Inputs One-Zero Alignment: inputs contain speech referring
expression gesture (i.e.,< S0 , G0 >), one referring expression zero gesture
(i.e.,< S1 , G0 >), referring expression one gesture (i.e., < S0 , G1 >).
types inputs require conversation context visual context resolve
references. One example type U2 Table 1. data, total
14 inputs belong category (marked (a) Table 4).
Simple Inputs One-One Alignment: inputs contain exactly one referring
expression one gesture (i.e., < S1 , G1 >). types inputs resolved
mostly combining gesture speech using multimodal fusion. total 151
inputs belong category (marked (b) Table 4).
Complex Inputs: inputs contain one referring expression and/or gesture. corresponds entry < S1 , G2 >, < S2 , G0 >,< S2 , G1 >,
< S2 , G2 > Table 4. One example type U3 Table 1. total 54
74

fiMinimizing Conflicts: Heuristic Repair Method

No. Correctly Resolved
Simple One-Zero Alignment
Simple One-One Alignment
Complex
Total
Accuracy

Ordering
5
104
24
133
60.7%

Absolute
5
104
19
128
58.4%

Combined
5
104
23
132
60.3%

Table 5: Performance comparison based dierent temporal compatibility functions
inputs belong category (marked (c) Table 4). types inputs
particularly challenging resolve.
section, focus dierent performance evaluations based three
types referring behaviors.
6.2 Temporal Alignment Speech Gesture
multimodal interpretation, align speech gesture based temporal
information important question. especially case complex inputs
multimodal input consists multiple referring expressions multiple gestures.
evaluated dierent temporal compatibility functions greedy approach. particular,
compared following three functions:
ordering temporal constraint Equation 4.
absolute temporal constraint dened following formula:
emp(o, e) = exp(|BeginT ime(o) BeginT ime(e)|)

(5)

Here, absolute timestamps potential referents (e.g., indicated gesture)
referring expressions used instead relative orders relevant entities
user input.
combined temporal constraint combines two aforementioned constraints,
giving equal weight determining compatibility score object
referring expression.
results shown Table 5. Dierent temporal constraints aect processing complex inputs. ordering temporal constraint worked slightly better
absolute temporal constraint. fact, temporal alignment speech gesture often one problems may aect interpretation results. Previous studies found
gestures tend occur corresponding speech unit takes place (Oviatt et al.,
1997). ndings suggest users tend tap screen rst start
speech utterance. behavior observed simple command based system (Oviatt
et al., 1997) speech unit corresponds single gesture (i.e., simple inputs
work).
75

fiChai, Prasov, & Qu

Non-overlap
Overlap
Total :

Speech First
7%
8%
15%

Gesture First
45%
40%
85%

Total
52%
48%
100%

Table 6: Overall temporal relations speech gesture

study, found temporal alignment gesture corresponding
speech units still issue needs investigated order improve
robustness multimodal interpretation. Table 6 shows percentage dierent
temporal relations observed study. rows indicate whether overlap
speech referring expressions accompanied gestures. columns indicate
whether speech (more precisely, referring expressions) gesture occurred rst.
Consistent previous ndings (Oviatt et al., 1997), cases (85% time),
gestures occurred referring expressions uttered. However, 15% cases
speech referring expressions uttered corresponding gesture occurred.
Among cases, 8% overlap referring expressions gesture
7% overlap.
Furthermore, although multimodal behaviors sequential (i.e., non-overlap)
simultaneous (e.g., overlap) integration quite consistent course interaction (Oviatt, Coulston, Tomko, Xiao, Bunsford, Wesson, & Carmichael, 2003),
exceptions. Figure 7 shows temporal alignments individual users study.
User 2 , User 6, User 8 maintained consistent behavior User 2s gesture always
happened overlapped corresponding speech referring expressions; User
6s gesture always occurred ahead speech expressions without overlapping; User
8s speech referring expressions always occurred corresponding gestures (without
overlap). users exhibited varied temporal alignment speech
gesture interaction. dicult system using pre-dened temporal
constraints anticipate accommodate dierent behaviors. Therefore,
desirable mechanism automatically learn user behavior alignment
automatically adjust behavior.
One potential approach introduce calibration process real human computer
interaction. calibration process, two tasks performed user. rst
task, user asked describe objects graph display speech
deictic gestures. second task, user asked respond system
questions using speech deictic gestures. reason users perform
two tasks identify whether dierence user initiated inputs
system initiated user responses. Based tasks, temporal relations
speech units corresponding gestures captured used real-time
interaction.
76

fiMinimizing Conflicts: Heuristic Repair Method

Percentage Occurance

Non-overlap Speech First
Overlap Speech First

Non-overlap Gesture First
Overlap Gesture First

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1

2

3

4

5

6

7

8

9

10

11

User Index

Figure 7: Temporal alignment behavior user study

No. Correctly Resolved
Simple One-Zero Alignment
Simple One-One Alignment
Complex
Total

Cognitive Principles
5
104
24
133

without Cognitive Principles
5
92
18
115

Table 7: role cognitive principles greedy algorithm

6.3 Role Cognitive Principles
examine role modeling cognitive status multimodal reference, compared two congurations greedy algorithm. rst conguration based
matching score dened Equation 2, incorporates cognitive principles described
earlier. second conguration uses matching score completely dependent compatibility referring expression gesture (i.e., Section 5.3.3)
without using cognitive principles (i.e., P (o|S) P (S|e) included Equation
2).
Table 7 shows comparison results terms two congurations. algorithm
using cognitive principles outperforms algorithm use cognitive
principles 15%. performance dierence applies simple inputs
one-one alignment complex inputs. results indicate modeling cognitive
status potentially improve reference resolution performance.
77

fiChai, Prasov, & Qu

Total Num
Total
Simple One-Zero Alignment
Simple One-One Alignment
Complex

219
14
151
54

Graph-matching
Num %
130
59.4%
7
50.0%
104
68.9%
19
35.2%

Greedy
Num %
133
60.7%
5
35.7%
104
68.9%
24
44.4%

Table 8: Performance comparison graph-matching algorithm greedy
algorithm

6.4 Greedy Algorithm versus Graph-matching Algorithm
compared greedy algorithm graph-matching algorithm terms
performance runtime. Table 8 shows performance comparison. Overall, greedy
algorithm performs comparably graph-matching algorithm.
compare runtime, ran algorithm user 10 times input
run 100 times. words, user input run 1000 times algorithm
get average runtime measurement. experiment done UltraSPARC-III
server 750MHz 64bit.
greedy algorithm graph-matching algorithm function
calls process speech inputs (e.g., parsing) gesture inputs (e.g., identify potentially
intended objects). dierence algorithms specic implementations
regarding graph creation matching graph-matching algorithm greedy
search greedy algorithm. result, average time greedy algorithm
process simple inputs complex inputs 17.3 milliseconds 21.2 milliseconds
respectively. average time graph matching algorithm process simple
complex inputs 22.3 milliseconds 24.8 milliseconds respectively. results show
average greedy algorithm runs slightly faster graph-matching algorithm
given current implementation, although worst case, graph-matching algorithm
asymptotically complex.
6.5 Real-time Error Analysis
understand bottleneck real-time multimodal reference resolution, examined
error cases algorithm failed provide correct referents.
spoken dialog systems, speech recognition major bottleneck. Although
trained users acoustic model individually, speech recognition rate still
low. 127 inputs correctly recognized referring expressions. Among
inputs, 103 resolved correct referents. Fusing inputs multiple
modalities together sometimes compensate recognition errors (Oviatt, 1996).
Among 92 inputs referring expressions incorrectly recognized, 29
correctly assigned referents due mutual disambiguation. mechanism reduce
78

fiMinimizing Conflicts: Heuristic Repair Method

recognition errors, especially utilizing information modalities,
important provide robust solution real time multimodal reference resolution.
second source errors comes another common problem spoken dialog
systems, namely out-of-vocabulary words. example, area vocabulary.
additional semantic constraint expressed area captured. Therefore,
system could identify whether house town referred user uttered
area. important system capability acquire knowledge (e.g.,
vocabulary) dynamically utilizing information modalities interaction
context. Furthermore, errors came lack understanding spatial relations
(as house close red one) superlatives (as expensive house).
Algorithms aligning visual features resolve spatial references desirable (Gorniak
& Roy, 2004).
addition two main sources, errors caused unsynchronized inputs.
Currently, use idle status (i.e., 2 seconds input either speech gesture)
boundary delimit interaction turn. Two types synchronization
observed. rst type unsynchronized inputs user (such big pause
speech gesture) comes underlying system implementation.
system captures speech inputs gesture inputs two dierent servers
TCP/IP protocol. communication delay sometimes split one synchronized input
two separate turns inputs (e.g., one turn speech input alone turn
gesture input alone). better engineering mechanism synchronizing inputs desired.
disuencies users accounted small number errors. current algorithm incapable distinguishing disuent cases normal cases. Fortunately,
disuent situations occur frequently study (only 6 inputs disuency). consistent previous ndings speech disuency rate lower
human machine conversation spontaneous speech (Brennan, 2000). humancomputer conversation, users tend speak carefully utterances tend short. Recent
ndings indicated gesture patterns could used additional source identify
dierent types speech disuencies human-human conversation (Chen, Harper, &
Quek, 2002). Based limited cases, found gesture patterns could indicators
speech disuencies occur. example, user says show red
house (point house A), green house (still point house A), behavior
pointing house dierent speech description usually indicates repair. Furthermore, gestures involve disuencies; example, repeatedly pointing object
gesture repetition. Failure identifying disuencies caused problems reference
resolution. ideal mechanism identify disuencies using
multimodal information.
6.6 Comparative Evaluation Two Approaches
examine greedy algorithm compared nite state approach
(Section 3.1) decision list approach (Section 3.2), conducted comparative evaluation. original nite state approach, N-best speech hypotheses maintained
speech tape. data here, best speech hypothesis speech
input. Therefore, manually updated incorrectly recognized words nite
79

fiChai, Prasov, & Qu

No. Correctly Resolved
Simple Inputs one-one alighment
Simple Inputs zero-one alighment
Complex Inputs
Total

Greedy
116
8
24
148

Finite State
115
0
13
128

Decision List
88
12
0
100

Table 9: Performance comparison two approaches
state approach would penalized lack N-best speech hypotheses 4 .
modied data used three approaches. Table 9 shows comparison results.
shown table, greedy algorithm correctly resolved inputs
nite state approach decision list approach. major problem nite state
approach incorporate conversation context nite state transducer.
problem contributes failure resolving simple inputs zero-one alignment
complex inputs. major problem decision list approach,
described earlier, lack capabilities process ambiguous gestures complex
inputs.
Note greedy algorithm algorithm obtain full semantic interpretation multimodal input. rather algorithm specically reference
resolution, uses information context gesture resolve speech referring expressions. regard, greedy algorithm dierent nite state approach
whose goal get full interpretation user inputs reference resolution
part process.

7. Conclusion
Motivated earlier investigation cognitive status human machine interaction,
paper describes greedy algorithm incorporates cognitive principles underlying human referring behavior resolve variety references human machine multimodal
interaction. particular, algorithm relies theories Conversation Implicature
Givenness Hierarchy eectively guide system searching potential referents. empirical studies shown modeling form referring experssions
implication cognitive status achieve better results algorithm
considers compatibility referring expressions potential referents.
greedy algorithm eciently achieve comparable performance previous optimization
approach based graph-matching. Furthermore, greedy algorithm handles
variety user inputs ranging precise ambiguous simple complex,
outperforms nite state approach decision list approach experiments.
simplicity generality, approach potential improve robustness multimodal interpretation. learned investigation prior
4. Note corrected inputs direct correspondence recognized
words transcribed words maintain consistency timestamps.

80

fiMinimizing Conflicts: Heuristic Repair Method

knowledge linguistic cognitive studies benecial designing ecient
practical algorithms enabling multimodal human machine communication.

Acknowledgments
work supported NSF CAREER award IIS-0347548. authors would
thank anonymous reviewers valuable comments suggestions.

References
Bolt, R. (1980). Put there: Voice gesture graphics interface. Computer
Graphics, 14 (3), 262270.
Brennan, S. (2000). Processes shape conversation implications computational linguistics. Proceedings 38th Annual Meeting ACL, pp. 18.
Byron, D. (2002). Resolving pronominal reference abstract entities. Proceedings
40th Annual Meeting ACL, pp. 8087.
Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., Chang, K., Vilhjalmsson, H., &
Yan, H. (1999). Embodiment conversational interfaces: Rea. Proceedings
CHI99, pp. 520527.
Chai, J., Hong, P., Zhou, M., & Prasov, Z. (2004). Optimization multimodal interpretation. Proceedings 42nd Annual Meeting Association Computational
Linguistics (ACL), pp. 18.
Chai, J., Prasov, Z., Blaim, J., & Jin, R. (2005). Linguistic theories ecient multimodal
reference resolution: empirical study. Proceedings 10th International
Conference Intelligent User Interfaces(IUI), pp. 4350.
Chai, J., Prasov, Z., & Hong, P. (2004a). Performance evaluation error analysis
multimodal reference resolution conversational system. Proceedings HLTNAACL 2004 (Companion Volumn), pp. 4144.
Chai, J. Y., Hong, P., & Zhou, M. X. (2004b). probabilistic approach reference resolution multimodal user interfaces. Proceedings 9th International Conference
Intelligent User Interfaces (IUI), pp. 7077.
Chen, L., Harper, M., & Quek, F. (2002). Gesture patterns speech repairs.
Proceedings International Conference Multimodal Interfaces (ICMI), pp. 155
160.
Cohen, P. (1984). pragmatics referring modality communication. Computational Linguistics, 10, 97146.
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., Smith, I., Chen, L., & Clow, J.
(1996). Quickset: Multimodal interaction distributed applications. Proceedings
ACM Multimedia, pp. 3140.
Eckert, M., & Strube, M. (2000). Dialogue acts, synchronising units anaphora resolution. Journal Semantics, Vol. 17(1), pp. 5189.
81

fiChai, Prasov, & Qu

Gold, S., & Rangarajan, A. (1996). graduated assignment algorithm graph-matching.
IEEE Trans. Pattern Analysis Machine Intelligence, 18 (4), 377388.
Gorniak, P., & Roy, D. (2004). Grounded semantic composition visual scenes. Journal
Artificial Intelligence Research, 21, 429470.
Grice, H. P. (1975). Logic conversation. Cole, P., & Morgan, J. (Eds.), Speech Acts,
pp. 4158. New York: Academic Press.
Grosz, B. J., & Sidner, C. (1986). Attention, intention, structure discourse.
Computational Linguistics, 12 (3), 175204.
Gundel, J. K., Hedberg, N., & Zacharski, R. (1993). Cognitive status form
referring expressions discourse. Language, 69 (2), 274307.
Gustafson, J., Bell, L., Beskow, J., Boye, J., Carlson, R., Edlund, J., Granstrom, B., House,
D., & Wiren, M. (2000). Adapt - multimodal conversational dialogue system
apartment domain. Proceedings 6th International Conference Spoken
Language Processing (ICSLP), Vol. 2, pp. 134137.
Huls, C., Bos, E., & Classen, W. (1995). Automatic referent resolution deictic
anaphoric expressions. Computational Linguistics, 21 (1), 5979.
Johnston, M. (1998). Unication-based multimodal parsing. Proceedings COLINGACL98, pp. 624630.
Johnston, M., & Bangalore, S. (2000). Finite-state multimodal parsing understanding.
Proceedings COLING00, pp. 369375.
Johnston, M., Cohen, P., McGee, D., Oviatt, S., Pittman, J., & Smith, I. (1997). Unicationbased multimodal integration. Proceedings ACL97, pp. 281288.
Kehler, A. (2000). Cognitive status form reference multimodal human-computer
interaction. Proceedings AAAI00, pp. 685689.
Koons, D. B., Sparrell, C. J., & Thorisson, K. R. (1993). Integrating simultaneous input
speech, gaze, hand gestures. Maybury, M. (Ed.), Intelligent Multimedia
Interfaces, pp. 257276. MIT Press.
Neal, J. G., & Shapiro, S. C. (1991). Intelligent multimedia interface technology. Sullivan,
J., & Tyler, S. (Eds.), Intelligent User Interfaces, pp. 4568. ACM: New York.
Neal, J. G., Thielman, C. Y., Dobes, Z. H., M., S., & Shapiro, S. C. (1998). Natural language
integrated deictic graphic gestures. Maybury, M., & Wahlster, W. (Eds.),
Intelligent User Interfaces, pp. 3851. CA: Morgan Kaufmann Press.
Oviatt, S., Coulston, R., Tomko, S., Xiao, B., Bunsford, R., Wesson, M., & Carmichael, L.
(2003). Toward theory organized multimodal integration patterns humancomputer interaction. Proceedings Fifth International Conference Multimodal
Interfaces, pp. 4451.
Oviatt, S., DeAngeli, A., & Kuhn, K. (1997). Integration synchronization input
modes multimodal human-computer interaction. Proceedings Conference
Human Factors Computing Systems: CHI97, pp. 415422.
82

fiMinimizing Conflicts: Heuristic Repair Method

Oviatt, S. L. (1996). Multimodal interfaces dynamic interactive maps. Proceedings
Conference Human Factors Computing Systems: CHI96, pp. 95102.
Oviatt, S. L. (1999a). Multimodal system processing mobile environments. Proceedings
Thirteenth Annual ACM Symposium User Interface Software Technology
(UIST2000), pp. 2130.
Oviatt, S. L. (1999b). Mutual disambiguation recognition errors multimodal architecture. Proceedings Conference Human Factors Computing Systems:
CHI99, pp. 576583.
Stent, A., Dowding, J., Gawron, J. M., Bratt, E. O., & Moore, R. (1999). commandtalk
spoken dialog system. Proceedings ACL99, pp. 183190.
Stock, O. (1993). Alfresco: Enjoying combination natural language processing
hypermedia information exploration. Maybury, M. (Ed.), Intelligent Multimedia
Interfaces, pp. 197224. MIT Press.
Tsai, W. H., & Fu, K. S. (1979). Error-correcting isomorphism attributed relational
graphs pattern analysis. IEEE Trans. Sys., Man Cyb., 9, 757768.
Wahlster, W. (1998). User discourse models multimodal communication. Maybury, M., & Wahlster, W. (Eds.), Intelligent User Interfaces, pp. 359370. ACM Press.
Wu, L., & Oviatt, S. (1999). Multimodal integration - statistical view. IEEE Transactions
Multimedia, 1 (4), 334341.
Zancanaro, M., Stock, O., & Strapparava, C. (1997). Multimodal interaction information
access: Exploiting cohesion. Computational Intelligence, 13 (7), 439464.

83


