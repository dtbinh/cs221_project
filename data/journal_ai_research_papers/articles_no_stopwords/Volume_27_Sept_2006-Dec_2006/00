Journal Artificial Intelligence Research 27 (2006) 1-23

Submitted 02/06; published 09/06

Variational Inference Procedure Allowing Internal
Structure Overlapping Clusters Deterministic
Constraints
Dan Geiger

dang@cs.technion.ac.il

Computer Science Dept., Technion,
Haifa, 32000, Israel

Christopher Meek

meek@microsoft.com

Microsoft Research, Microsoft Corporation,
Redmond, WA 98052, USA

Ydo Wexler

ywex@cs.technion.ac.il

Computer Science Dept., Technion,
Haifa, 32000, Israel

Abstract
develop novel algorithm, called VIP*, structured variational approximate
inference. algorithm extends known algorithms allow efficient multiple potential
updates overlapping clusters, overcomes difficulties imposed deterministic
constraints. algorithms convergence proven applicability demonstrated
genetic linkage analysis.

1. Introduction
Probabilistic graphical models elegant framework represent joint probability distributions compact manner. independence relationships random variables
nodes graph represented absence arcs model.
intuitively appealing presentation naturally enables design efficient generalpurpose algorithms computing marginal probabilities, called inference algorithms.
general inference problem NP-hard (Cooper, 1990; Dagum & Luby, 1993),
although many cases model small (or, precisely, small
treewidth) exact inference algorithms feasible, others time
space complexity makes use algorithms infeasible. cases fast yet
accurate approximations desired.
focus variational algorithms: powerful tool efficient approximate inference
offers guarantees form lower bound marginal probabilities.
family approaches aims minimize KL divergence distribution Q
target distribution P finding best distribution Q family distributions
inference feasible. particular, joint distribution P (X) set
discrete variables X goal compute marginal probability P (Y = y)
X. assume exact computation feasible. idea replace
P distribution Q used compute lower bound P (Y = y).
c
2006
AI Access Foundation. rights reserved.

fiGeiger, Meek & Wexler

let H = X \ . Then, using Jensens inequality get following bound:
log P (y) = log

X
h

Q(h)

P (y, h) X
P (y, h)

Q(h) log
= D(Q(H) || P (Y = y, H))
Q(h)
Q(h)
h

D( || ) denotes KL divergence two probability distributions. quantity D(Q || P ) often called free-energy P Q possibly un-normalized
distributions. Variational techniques aim choose distribution Q lower
bound high possible, equivalently, KL divergence Q(h)
P (h|Y = y) minimized.
Variational approaches mean field, generalized mean field, structured
mean field differ respect family approximating distributions
used, structural mean field approach subsuming remaining approaches
special cases. research several authors guided work: Saul & Jordan (1996),
Ghahramani & Jordan (1997), Wiegerinck (2000) Bishop & Winn (2003).
contributions paper threefold. First develop extension
algorithm Wiegerinck (2000), call vip? , allows set potentials
approximating distribution Q updated simultaneously even clusters Q overlap. Algorithm vip? N -fold faster Wiegerincks algorithm N N grid-like models
yields two orders magnitude improvement large graphs genetic linkage
analysis model large pedigrees. Note simultaneous updates first presented
phylogenic trees Jojic et al. (2004). Second, prove convergence vip? previous variational methods via novel proof method, using properties KL divergence.
Third, extend vip? allow deterministic constraints model demonstrate
applicability extension genetic linkage analysis.

2. Background
background section based primarily paper Weigerinck (2000),
turn builds pioneering works papers Saul & Jordan (1996) Ghahramani
& Jordan (1997). review provides new exposition material.
denote distributions P (x) Q(x) related un-normalized distributions
P (x) P (x) Q(x) Q(x). Let
X finite set variables x instantiation
1 Q
variables. Let P (x) = ZP (di ) di projection instantiation
x variables Di X non-negative function, commonly called
potential. constant ZP normalizes product potentials subsets {Di }Ii=1
allowed overlap. often suppress arguments potential distribution,
using instead (di ) P instead P (X).
goal find distribution Q minimizes Q
KL divergence Q P .
constrain Q form Q(x) = Z1Q j j (cj ) ZQ normalizing
constant C1 , . . . , CJ possibly overlapping subsets X, call clusters. Finding optimum Q, however, difficult. modest common goal
devising iterative converging algorithms iteration KL divergence
approximating distribution Q P decreases unless Q stationary point.
Throughout, define Q(w|u) = |W1\U | instantiations U = u Q(u) = 0.
Consequently, terms equality Q(w, u) = Q(u)Q(w|u) well defined even
2

fiA Variational Inference Procedure

P
Q(u) = 0. Moreover, convention maintains properties
W \U Q(w|u) = 1
1
1
1
Q(w, z|u) = Q(w|z, u)Q(z|u) = |W \{U Z}| |Z\U | = |{W Z}\U | . note
Q(x) log Q(x) = 0 whenever Q(x) = 0 thus KL divergence
D(Q || P ) =

X

Q(x) log

x

Q(x)
P (x)

finite P (x) = 0 Q(x) > 0 instance x.
starting point algorithm developed Wiegerinck (2000). algorithm finds
distribution Q follows: iterates clusters Cj instantiations cj
update potentials j (cj ) = ej (cj ) via following update equation:

j (cj )

X

X

Q(ck |cj ) log k (ck ) +

{k:gkj =1} Ck \Cj

X

X

Q(di |cj ) log (di )

(1)

{i:fij =1} Di \Cj

gkj fij two indicator functions defined via gkj = 0 Q(Ck |cj ) = Q(Ck )
every instance cj Cj 1 otherwise, fij = 0 Q(Di |cj ) = Q(Di ) every instance
cj Cj 1 otherwise. Wiegerinck (2000) proved convergence algorithm stationary point using Lagrangians. Throughout call iterative procedure, Wiegerincks
algorithm.
Wiegerincks algorithm relies step algorithm compute conditional
probabilities Q(ck |cj ) Q(di |cj ) un-normalized distribution Q represented
set potentials j (cj ). accomplished inference algorithm
bucket elimination algorithm sum-product algorithm described Dechter (1999)

Q
Kschischang, Frey & Loeliger (2001) . important note Q(x) = j j (cj )
computation conditionals affected multiplying j constant
.
Wiegerincks algorithm generalizes mean field (MF) algorithm generalized
mean field (GMF) algorithm (Xing, Jordan & Russell, 2003, 2004). mean field algorithm special case Wiegerincks algorithm Cj contains single
variable. Similarly, generalized mean field algorithm special case Cj
disjoint subsets variables. Cj disjoint clusters, formula j Eq. 1
simplifies GMF equations follows (first term drops out):
j (cj )

X

X

Q(di |cj ) log (di ).

(2)

{i:fij =1} Di \Cj

term Q(di |cj ) made explicit Cj disjoint clusters (Bishop & Winn
2003). particular, set Di \ Cj partitions Dik = (Di \ Cj ) Ck Q
k = 1, . . . , J
k 6= j. Note Dik = Di Ck . Using notation, Q(di |cj ) = k Q(dki )
Q(dki ) = 1 whenever Dik = . factorization simplifies formula j
follows:
X
X X
j (cj )
Q(d1i ) . . .
Q(dJi ) log (di ).
(3)
{i:fij =1} Di1

DiJ

3

fiGeiger, Meek & Wexler

simplification achieved automatically using bucket elimination computing
j . iterated sums Eq. 3 fact buckets formed bucket elimination
Cj disjoint.
Eq. 1 requires repeated computation quantities Q(ck |cj ) Q(di |cj ). repetition significant could many indices k Q(Ck |cj ) 6= Q(Ck ),
many indices Q(Di |cj ) 6= Q(Di ). computations share many subcomputations therefore reasonable add data structure facilitate efficient
implementation function calls. particular, possible save computations
sets C1 , . . . , CJ form junction tree.
set clusters C1 , . . . , CJ forms junction tree iff exists set trees JT one node, called Cj , cluster variables Cj , every two nodes Ci
Cj JT, connected path JT, node Ck path,
Ci Cj Ck holds. set trees mean undirected graph, necessarily
connected, cycles. Note definition allows junction tree disconnected graph.

Q
Q C1 , . . . , CJ form junction tree, Q(x) decomposable form
Q(x) = j j (cj )/ e e (se ), j marginals subsets Cj X,
e marginals intersections Se = Ci Cj , one two neighboring clusters
junction tree (Jensen 1996).
Wiegerinck (2000) enhanced basic algorithm maintains
consistent
junction
P
P
tree JT distribution Q(x). Consistency means Cj \Ck j = Ck \Cj k
every two clusters. consistent junction tree, potential j (Cj ) proportional
Q(Cj ). update potential algorithm may yield inconsistent junction
tree, however, consistency maintained applying DistributeEvidence(j ) (Jensen
1996) update potential. procedure DistributeEvidence(0j ) accepts
input consistent junction tree new cluster marginal 0j Cj , updates
potential every neighboring cluster Ck Cj via
Cj \Ck

0j (cj )

Cj \Ck

j (cj )

P
0k (ck )

k (ck ) P

(4)

neighboring cluster recursively propagates update applying Eq. 4
neighbors except one update came. output procedure
consistent junction tree, clusters, 0j (possibly un-normalized)
marginal probability Q Cj , conditional probability Q(X|Cj ) remains
unchanged (Jensen 1996, pp. 74).
Wiegerincks enhanced algorithm, uses junction tree, iteratively updates
potential cluster (node junction tree), using potentials clusters
separators. However, since junction tree may consistent update,
algorithm applies procedure DistributeEvidence(j ) junction tree,
update. Note description omits normalization step Wiegerinck (2000)
needed convergence.
time consuming computation variational algorithms computing conditional probabilities form Q(ck |cj ) Q(di |cj ). distinguish among conditional probabilities follows.

4

fiA Variational Inference Procedure

Definition: conditional probability Q(A|cj ) subsumed Q set target variables subset cluster Ck Q (i.e., (A \ Cj ) Ck ).
Wiegerincks enhanced algorithm substantial computational benefits conditional probabilities subsumed. cases needed quantities Eq. 1, Q(di |cj )
Q(ck |cj ), obtained mere lookup junction tree, one call
DistributeEvidence made update.
Weigerincks basic enhanced algorithms assume structure j , namely,
algorithms hold tables j explicit entry every instantiation Cj . Since
computations Q(ck |cj ) Q(di |cj ) grow exponentially size Di Ck ,
algorithms become infeasible large cliques clusters. simplification, additional
structure j suggested Wiegerinck (2000, Section 4) form,

j (cj ) =

nj


jl (cjl ),

(5)

l=1

sets Cjl , l = 1, . . . , nj , possibly overlapping subsets Cj , cjl
projection instantiation cj variables Cjl . Using structure sufficient
hold tables subsets Cjl considerably smaller. Note j
entry instantiation cj , nj = 1 j (cj ) = j1 (cj1 ). Weigerinck uses
structure potentials j following assumptions:
Definition [SelfQ
compatibility]: distribution Q clusters Cj subsets Cjl
1
form Q(x) = ZQ j j (cj ), clusters factor according Eq. 5, self compatible
every Cj Ck set indices Njk = {l : Q(Ck |cj ) = Q(Ck |cjl )} non-empty
regardless values potentials j , cj arbitrary instantiation Cj
cjl projection cj Cjl .
Definition [Compatibility
wrt P ]: distribution Q clusters Cj subsets Cjl
Q
form Q(x) = Z1Q j j (cj ), clusters factor according Eq. 5, compatible
Q
wrt distribution P sets Di form P (x) = Z1P (di ) every Di Cj
set indices Mij = {l : Q(Di |cj ) = Q(Di |cjl )} non-empty, cj arbitrary
instantiation Cj cjl projection cj Cjl .
Note self-compatibility compatibility wrt P depend form Q
particular realization potentials j .
assumptions Weigerinck states considerable simplifications deduced, provides examples statement.
note algorithms Bishop & Winn (2003) Jojic et al. (2004) use
stronger assumption clusters Cj approximating distribution Q disjoint
Q(Ck |cj ) = Q(Ck ). assumption, implies Q(Ck |cj ) = Q(Ck |cjl )
Q(Di |cj ) = Q(Di |cjl ) every index l, relaxed requiring equalities
hold single index l (but possibly multiple indices).
5

fiGeiger, Meek & Wexler

3. Multiple Potential Update using Overlapping Clusters
section develop new algorithm, called vip? , uses additional structure
potentials offered Eq. 5 speed computations. particular, rather updatnj
ing potential jl separately, offer way update set potentials {jl }l=1
simultaneously, saving considerable computations. Furthermore, simultaneous update
enhanced using junction tree, despite fact sets {Cjl } need form
junction tree, {Cj } form junction tree.
algorithm uses definitions self compatibility compatibility wrt P , defined
earlier, following definition indices.
Definition: Let indicator function gjk (l) equal 1 single fixed index l Njk
0 indices Njk Q(Ck |cj ) 6= Q(Ck ), equal 0 otherwise. Let
indicator function fij (l) equal 1 single fixed index l Mij 0 indices
Mij Q(Di |cj ) 6= Q(Di ), equal 0 otherwise.
Algorithm vip? given Figure 1. convergence proved Section 4. proof
requires Q self-compatible, compatible wrt P , addition, satisfy (P (x) = 0)
(Q(x) = 0). Note D(Q || P ) = distributions Q satisfy last
assumption.
main improvement algorithm efficient update potentials. potentials j factorize smaller potentials jl according Eq. 5, algorithm vip?
updates jl instead updating whole potential j , done Weigerincks algorithms.
update potentials jl done vip? equivalent updating j according
Eq. 1, irrelevant constant, require compute update equation
instance cluster Cj . proposed change considerably speeds previous
algorithms.
algorithm gets input target distribution P sets Di form P (x) =
1 Q
(di ) approximating distribution Q clusters Cj self-compatible,
ZP
compatible wrt P satisfies
condition (P (x) = 0) (Q(x) = 0). Distribution Q
Q
form Q(x) = Z1Q j j (cj ) potential every cluster Cj factors according
Qnj
jl (cjl ) clusters form consistent junction tree. algorithm
j (cj ) = l=1
iterates clusters, updating potential every instantiation subsets
Cjl according Eq. 6. apply update equation, quantities Q(di |cjl ) computed
via variable propagation (Jensen, pp 69-80) junction tree. quantities
subsumed, obtained mere lookup junction tree. Then, updating
potentials subsets Cjl cluster Cj , procedure DistributeEvidence applied
make junction tree consistent respect j . Since clusters Cj form
junction tree via subsets Cjl , Eq. 4 replaced Eq. 7. convergence,
algorithm vip? outputs approximating distribution Q revised potentials.
Example 1 target distribution P N N grid pairwise potentials (see Figure 2a)
approximating family defined single row set columns grid,
augmented edges middle vertex (see Figure 2b) C7 row
grid Ci (i = 1, . . . , N = 6) columns. Using notation Xi,j denote
vertex row column j grid, cluster C7 associated N 1 subsets
6

fiA Variational Inference Procedure

Algorithm VIP? (Q,P)
Q
Q
Input: Two probability distributions P (x) = Z1P (di ) Q(x) = Z1Q j j (cj )
Qnj
initial potentials j (cj ) = l=1
jl (cjl ) form consistent junction tree, Q
self-compatible, compatible wrt P , satisfies (P (x) = 0) (Q(x) = 0).
Output:
revised set potentials jl (cj ) defining probability distribution Q via Q(x)
Q

(c
j,l jl jl ) Q stationary point D(Q || P ).
Iterate clusters Cj convergence
Step 1.
l = 1, . . . , nj :
every instantiation cjl Cjl apply following update equation:
jl (cjl )

X

X

X

Q(ck |cjl ) log k (ck ) +

{k:gjk (l)=1} Ck \Cjl

X

Q(di |cjl ) log (di ) (6)

{i:fij (l)=1} Di \Cjl

jl (cjl ) ejl (cjl )
Note: Q(di |cjl ) computed via variable propagation (Jensen, pp 69-80) junction
tree JT. However, quantities subsumed, obtained mere lookup
JT.
Step 2. Make JT consistent respect j :

DistributeEvidence(JT, j )

DistributeEvidence(JT, 0j )
Input: junction tree JT nodes Ck potentials k (ck ) =
node Cj revised potential 0j .
Output: consistent junction tree.

Qnk

l=1 kl (ckl ).

starting

initialization source(j) 0; updated {j}
(updated6= )
first element updated; updated updated\{}
neighboring nodes Ck C JT k 6= source()
P
Qn 0
C
\C
l=1 l (cl )
0km (ckm ) km (ckm ) P k Qn
C \Ck
l=1 l (cl )
single subset Ck (C Ck ) Ckm
source(k)
updated updated{k}
Figure 1: Algorithm vip?
7

(7)

fiGeiger, Meek & Wexler

(a)

(b)

(c)

Figure 2: (a) Grid-like P distribution (b) & (c) Approximating distributions Q.
C7l = {X3,l , X3,l+1 }. column cluster Cj associated 2N-4=8 subsets Cjl
Cjl = {Xl,j , Xl+1,j } N-1 subsets (l = 1, . . . , 5), Cjl = {X1,j , X3,j } l = N ,
Cjl = {XlN +4,j , X3,j } additional N-4 subsets (l = 7, 8).
choice induces self-compatible approximating distribution Q; every column cluster
Cj independent another cluster given subset contains X3,j (such Cj2 ).
addition, row cluster C7 independent every column cluster Cj given C7j .
induced distribution compatible wrt P ; vertical edge Dv = {Xi,j , Xi+1,j }
P , distribution Q satisfies Q(Dv |ck ) = Q(Dv |ck2 ) column cluster Ck
k 6= j, Q(Dv |c7 ) = Q(Dv |c7j ). addition, horizontal edge Dh = {Xi,j , Xi,j+1 }
P , distribution Q satisfies Q(Dh |c7 ) = Q(Dh |c7j ), Q(Dh |ck ) = Q(Dh |ck2 ) k 6=
j, j + 1. Finally, edge Dh k = j, j + 1, approximating distribution satisfies
Q(Dh |ck ) = Q(Dh |ckl ) Ckl = {Xi,k , X3,k }, due additional N 3 edges added
column cluster.
Wiegerincks enhanced algorithm, algorithm vip? substantial computational
benefits conditional probabilities Q(di |cjl ) subsumed. cases needed
quantities, Q(di |cjl ) Q(ck |cjl ), obtained mere lookup junction tree
step 1 algorithm, one call DistributeEvidence made step 2,
demonstrated next paragraph. computational efficiency vip? achieved
even quantities Q(di |cjl ) subsumed factor subsumed probabilities.
Disjoint clusters one special case, quantities Q(di |cjl ) factor
subsumed probabilities Q(dki |cjl ), Dik = Di Ck , obtainable lookup
junction tree.
Consider Example 1 compare computational cost vip? versus Wiegerincks
basic enhanced algorithms. Assume Wiegerincks basic algorithm (Eq. 1), uses
distribution Q given Figure 2c, 35 clusters Cj0 60 sets Di . Therefore,
junction tree used, 4 (60 + 34) = 376 conditionals computed cluster Cj0
(edge) boundary grid, 94 four possible values edge
cluster Cj0 . Clearly, additional clusters introduced, shown example Figure 2b,
computational cost grows. using junction tree, done Wiegerincks enhanced algorithm, subsumed conditional probabilities, computed separately
Wiegerincks basic algorithm, computed single call DistributeEvidence.
computation covers subsets Figure 2c. conditionals sub8

fiA Variational Inference Procedure

(a)

(b)

(c)

Figure 3: target distribution P grid pairwise potentials (a). Two different
partitions grid clusters shown Figures (b) (c), contain
subsets.

sumed Q(di |cj ) horizontal edges Di contained single cluster, namely,
edges 2a 2c. factor two subsumed probabilities, one computed
single call described earlier requires second call DistributeEvidence.
example, let Di = {X1 , X2 } horizontal edge P overlap Cj0 ,
Q(x1 , x2 |c0j ) = Q(x1 |c0j , x2 )Q(x2 |c0j ). two conditionals subsumed, second
call DistributeEvidence needed obtain Q(x1 |c0j , x2 ). yields 25 calls DistributeEvidence. However, Example 1, one call DistributeEvidence sufficient
compute conditionals two adjacent horizontal edges, yielding need
15 calls. Therefore, since 4 15 = 60 calls DistributeEvidence, since
cost junction tree algorithm typically twice cost computing conditional
probabilities without using junction tree, yields 3-fold speedup Wiegerincks
enhanced algorithm versus Wiegerincks basic algorithm. edges boundary,
speedup factor less 3. size grid grows, smaller fraction edges
boundary, and, thus, speedup approaches 3-fold speedup.
significant speedup obtained using algorithm vip? clusters Cj subsets
described Figure 2b. Note additional subsets needed meet compatibility assumption vip? . Algorithm vip? makes one call DistributeEvidence
per cluster Cj non-subsumed conditional, rather every edge cluster Cj0 .
Since vip? uses N + 1 clusters Cj , speedup compared Wiegerincks enhanced
algorithm approaches N N N grid grows. O(N ) speedup confirmed
experiments section (Figure 9).
Another potential benefit vip? possibility alternating different
choices clusters contain identical subsets Cjl . simple example grid
Figure 3a, two choices illustrated Figures 3b 3c. two sets clusters update potentials j differently therefore yield better approximations
distance D(Q || P ) reduced every alternation. general, iterate
set choices clusters execute vip? one choice using initial potentials
potentials jl found earlier choice clusters. practical benefit option
added flexibility remains tested application.
9

fiGeiger, Meek & Wexler

4. Proof Convergence
develop several lemmas culminate proof convergence algorithm vip? .
Lemma 1 states two un-normalized probability distributions P (x) Q(x)
KL divergence minimized Q(x) proportional P (x). Lemma 2 rewrites
KL divergence D(Q || P ) terms potentials P
Q P using quantity j (cj )
which, according Lemma 3, differs j (cj ) = l jl (cjl ) constant. Finally,
Theorem 1 asserts KL divergence Q P decreases iteration
Algorithm vip? unless Q stationary point. proof exploits new form
D(Q || P ) provided Lemma 2, replaces term j (cj ) terms jl (cjl ) used
update equation vip? . final observation, uses Lemma 1, closes proof
showing potentials updated algorithm vip? , KL divergence
minimized wrt j .
first lemma provides variant well known property KL. Recall
every two probability distributions Q(x) P (x), KL divergence D(Q(x) || P (x)) 0
equality holds Q(x) = P (x) (Cover & Thomas 1991; Theorem 2.6.3).
similar result holds un-normalized probability distributions.
Lemma 1 Let Q(x) P (x) non-negative functions
let
Q(x) =
D(Q(x) || P (x))
P min
{Q|

x

P

x P (x)

= ZP > 0,

Q(x)=ZQ }

ZQ positive constant. Q(x) =

ZQ
ZP P (x).

Proof. observe
P (x)
D(Q(x) || P (x)) = ZQ D( Q(x)
ZQ || ZP ) + ZQ log

ZQ
ZP

implies, using cited result normalized distributions, minimum
P (x)
obtained Q(x)
ZQ = ZP , yielding desired claim.
next lemma rewrites KL divergence optimizing update equation
cluster Cj becomes readily available.
Lemma 2 Let P (x) =
tions. Then,

1
ZP

Q

(di )

D(Q || P ) =

X
Cj

Q(x) =

Q(cj ) log

1
ZQ

Q

j

j (cj ) two probability distribu-

j (cj )
+ log(ZP ) log(ZQ )
j (cj )

(8)

j (cj ) = ej (cj ) ,
j (cj ) =

X X

Q(ck |cj ) log k (ck ) +

X X


k Ck \Cj

10

Di \Cj

Q(di |cj ) log (di )

(9)

fiA Variational Inference Procedure

Proof: Recall
D(Q || P ) =

X

Q(x) log

X

Q(x)
= [H(Q) + EQ [log P (x)]]
P (x)

(10)

H(Q) denotes entropy Q(x) EQ denotes expectation respect Q.
entropy term written
X X
H(Q) =
Q(cj )Q(x|cj ) [log Q(cj ) + log Q(x|cj )]
Cj X\Cj

=

P

Cj

Q(cj ) log Q(cj )

P

Cj

Q(cj )

P

X\Cj

Q(x|cj ) log Q(x|cj ).

variation well known form H(Q) derived splitting
summation
P
X summation Cj X \ Cj , using fact Q(cj ) X\Cj Q(x|cj ) =
Q(cj ) holds every distribution. split sum X \ Cj Q(cj ) > 0 use
1 Q
X
k k (ck )
ZQ
=
log k (ck ) log Q(cj ) log(ZQ )
log Q(x|cj ) = log
Q(cj )
k

Thus,
X

Q(x|cj ) log Q(x|cj ) =

X\Cj

P P

P
Q(ck |cj )Q(x|ck , cj ) log k (ck ) X\Cj Q(x|cj ) [log Q(cj ) + log(ZQ )]
P
using Q(ck , cj ) X\{Ck Cj } Q(x|ck , cj ) = Q(ck , cj ) term rewritten
X
H(Q) =
Q(cj ) log Q(cj )
k

X\Cj

Cj



P

Cj Q(cj )

hP

k6=j

P

Ck \Cj Q(ck |cj ) log k (ck ) + log

j (cj )
Q(cj )



+ log(ZQ )

Note Q(cj ) = 0 bracketed term multiplied zero, due equality
0 log 0 = 0, product zero.

second term Eq. 10 similarly written
XX
X
EQ [log P (x)] =
Q(cj )
Q(x|cj ) log (di ) log(ZP )


=

X

Q(cj )

Cj

X X


Cj

(11)

X\Cj

Q(di |cj ) log (di ) log(ZP )

Di \Cj

Hence Eq. 10 rewritten
X
D(Q || P ) =
Q(cj ) log Q(cj )
Cj




X
Cj

Q(cj )


X X

Q(ck |cj ) log k (ck ) +

X X


k Ck \Cj

11

Di \Cj

Q(di |cj ) log (di )

fiGeiger, Meek & Wexler



X
Cj

Q(cj ) log

Q(cj )
+ log(ZP ) log(ZQ )
j (cj )

Denoting bracketed term j (cj ), letting j (cj ) = ej (cj ) , get
D(Q || P ) =

X
Cj

Q(cj ) log

j (cj )
+ log(ZP ) log(ZQ ).
j (cj )



P
next lemma shows j (cj ), defined Eq. 9, j (cj ) = l jl (cjl ), used
update potentials Q vip? , differ additive constant depend
cj . argued Theorem 1, fact difference constant enables vip?
use latter form, efficient representation.
Q
Q
Q
Lemma 3 Let P (x) = Z1P (di ) Q(x) = Z1Q j j (cj ) j (cj ) = l jl (cjl ),
two probability distributions Q self-compatible compatible wrt P . Let
X
X
X
X
jl (cjl ) =
Q(ck |cjl ) log k (ck ) +
Q(di |cjl ) log (di ) (12)
{k:gjk (l)=1} Ck \Cjl

{i:fij (l)=1} Di \Cjl

Then, difference j (cj ) defined Eq. 9 j (cj ) =
depend cj .

P

l

jl (cjl ) constant

P
Proof: first argue term form Ck \Cj Q(ck |cj ) log k (ck ) term
P
form Di \Cj Q(di |cj ) log (di ) Eq. 9 depends cj appears exactly
single subset Cjl Eq. 12. argue every term Eq. 12 appears
Eq. 9.
Since Q self-compatible, follows every cluster Ck depends Cj
function gkj (l) equalsPone single subset Cjl , namely Q(ck |cj ) = Q(ck |cjl ),
case expression
Ck \Cj Q(ck |cjl ) log k (ck ) appears Eq. 12. Similarly, since Q
compatible wrt P follows every set Di depends Cj function fij (l)
equals
one single subset Cjl , namely Q(di |cj ) = Q(di |cjl ), case expression
P
Di \Cj Q(di |cjl ) log (di ) appears second term Eq. 12.
remains show every term Eq. 12 appears Eq. 9. Since Q selfcompatible, implied Ck Cj Cjl thus summing Ck \ Cj equivalent
summing Ck \ Cjl . Therefore, every k gjk (l) = 1, first term
Eq. 12 appears Eq. 9. Similarly, since Q compatible wrt P , implied
Di Cj Cjl thus summing Di \ Cj equivalent summing Di \ Cjl
therefore every fij (l) = 1 second term Eq. 12 appears Eq. 9.


Theorem 1 (Convergence vip? ) Let initial approximating distribution Q selfcompatible compatible wrt given distribution P , assume (P (x) = 0)
(Q(x) = 0). Then, revised distribution Q retains properties, iteration
Algorithm vip? KL divergence Q P decreases unless Q stationary
point.
12

fiA Variational Inference Procedure

Q
Proof. Let Q(x) = Z1Q jl jl (cjl ) jl (cjl ) = ejl (cjl ) . need show
0
?
start
Q of0 iteration vip function Q defined revised potentials
0
j (cj ) = l jl (cjl ) probability distribution listed properties
closer P KL divergence Q start previous iteration.
First, show Q0 maintains properties listed theorem throughout
updates done vip? . properties self-compatibility compatibility wrt P
derived form Q thus affected updates done vip? .
property
(P (x) = 0) (Q(x) = 0), consider instance x P (x) = 0. Since
1 Q
P (x) = ZP (di ) exists potential P (di ) = 0, di
Q
projection x set Di . Since Q(x) = 0 Q(x) = Z1Q jl jl (cjl ) exists
subset Cjl Q(cjl ) = 0, cjl projection x Cjl . Algorithm vip?
1
convention,
updates jl (cjl ) log (di ) = Q(di |cjl ) = |Di \C
jl |
0
yielding Q (x) = 0, claimed.
Now, show additional zeroes introduced Q whenever (P (x) = 0)
(Q(x) = 0). Hence, normalizing constant ZQ > 0 therefore revised Q0
probability distribution. instances cjl Q(cjl ) > 0 terms Q(ck |cjl ) log (ck )
Q(di |cjl ) log (di ) finite long (P (x) = 0) (Q(x) = 0). implies jl (cjl )
updated positive value thus, additional zeroes introduced Q.
Using given form Q,


1 X
Q(cj ) =
k (ck ) j (cj ).
(13)
ZQ
X\Cj k6=j

denote bracketed coefficient j (cj ) Bj (cj ) note constant
sense depend quantity j optimized.
use Eq. 13 rewrite KL divergence justified Eq. 8 Lemma 2:


j (cj )Bj (cj )
1 X
D(Q || P ) =
j (cj )Bj (cj ) log
+ log(ZP ) log(ZQ ).
(14)
ZQ
j (cj )Bj (cj )
Cj

Due Lemma 3, distance D(Q || P
j (cj )
P) changes constant replacing

(c
)
?
j
j
j (cj ) = e
, j (cj ) = l jl (cjl ) computed via Eq. 6 vip . Note
j (cj ) depend (cj ) function Q(x) conditional
distribution X \ Cj given Cj (via Q(ck |cj )). Hence, Lemma 1 states minimum
D(Q || P ) wrt j achieved j (cj ) proportional j (cj ). potential j
held implicitly partial potentials jl , step 1 vip? updates j (cj ) via
Eq. 6 proportional j (cj ) setting potential jl (cjl ) proportional
jl (cjl ). proportionality constant matter j multiplied ,
arbitrary constraining constant ZQ multiplied , influences cancel
Eq. 14. simplicity, algorithm uses = 1 therefore j (cj ) ej (cj ) . Algorithm
vip? implicitly computes j (cj ) according formula hence decreases D(Q || P )
iteration improving j (cj ) holding cluster potentials fixed. Since
KL divergence lower bounded zero, vip? converges.

13

fiGeiger, Meek & Wexler

properties Q required Theorem 1 self-compatibility compatibility wrt
P derived form Q satisfied setting clusters Cj appropriately.
addition, condition (P (x) = 0) (Q(x) = 0) trivially satisfied strictly positive
distributions P .
Note difference j (cj ) defined Eq. 9 j (cj ) defined Eq. 1
constant depend cj . Consequently, convergence proof applies
Wiegerincks algorithm, algorithm special case vip? every cluster
Cj single subset Cj1 = Cj .

5. Handling Deterministic Potentials
distribution P strictly positive property (P (x) = 0) (Q(x) = 0)
must hold convergence proof vip? apply. section provide sufficient
condition Q retain property.
Definition: instantiation W = w feasible (wrt distribution P ) P (W = w) > 0.
Otherwise, instantiation infeasible.
Q
Definition: constraining set wrt distribution P (x) = Z1P (di ) sets Di
minimal set variables Di infeasible instantiation .
Q
Definition: distribution Q(x) = Z1Q j j (cj ) clusters Cj containable wrt
Q
distribution P (x) = Z1P (di ), every constraining set P exists least
one cluster Cj Cj .
Q
Q
Theorem 2 Let P (x) = Z1P (di ) Q(x) = Z1Q j j (cj ) two distributions
Q
j (cj ) = l jl (cjl ) Q containable compatible wrt P strictly positive. Then, vip? iterates clusters Cj , revised distribution Q satisfies
(P (x) = 0) (Q(x) = 0).
Proof: definition constraining set, every infeasible instantiation x,
exists infeasible instantiation constraining set projection x
. show vip? updates Q(x) = 0 instantiations. Since Q containable
wrt P exists cluster Cj contains . Furthermore, since Di Di
set P since Q compatible wrt P , exists subset Cjl contains .
every instantiation cjl projection Cjl expression jl (cjl ) updated
according Q
Eq. 6 vip? . true Q(di |cjl ) > 0 log (di ) = .
1
Since Q(x) = ZQ j,l jl (cjl ) update implies Q(x) = 0.
Whenever first two compatibility conditions Theorem 1 hold, follows vip?
converges containable distributions. Note since every iteration vip? decreases
KL divergence, following iterations change Q greater zero instantiation
infeasible wrt P , leads infinite distance. However, containability
implies stronger property stated next theorem.

14

fiA Variational Inference Procedure

Q
Q
Theorem 3 Let P (x) = Z1P (di ) Q(x) = Z1Q j j (cj ) two distributions
Q
j (cj ) = l jl (cjl ) Q containable wrt P (P (x) = 0) (Q(x) = 0). Then,
vip? iterates clusters Cj , revised distribution Q satisfies (Q(x) = 0)
(P (x) = 0).
Proof: Consider instantiation x P (x)
> 0. show Eq. 6 vip? upQ
dates Q(x) positive value. Since Q(x) = Z1Q j,l jl (cjl ), sufficient show
revised potential jl (cjl ) positive subset Cjl instance cjl
projection x Cjl . instances cjl value jl (cjl ) set Eq. 6
finite value since (di ) > 0 every instance di projection x set Di ,
k (ck ) = 0 implies Q(ck |cjl ) = 0. Therefore, jl (cjl ) = ejl (cjl ) > 0 Q(x) > 0.
consequence Theorem 3 Q(x) = 0 iff P (x) = 0. Conditions weaker
containability may sufficient ensure requirement needed convergence, however,
containability easily satisfiable applications variational techniques explicated
next section.

6. Genetic Linkage Analysis via Variational Algorithms
Genetic linkage analysis takes input family pedigree individuals
affected genetic disease, affection status members pedigree, marker readings
across genome, mode inheritance. output likelihood data function location disease gene given pedigree. Locations yielding maximum
close maximum likelihood singled suspect regions scrutiny.
exact computation likelihood often complex approximations needed.
Algorithm vip? developed facilitate likelihood computations. particular, vip? allows overlapping clusters minimizes loss valuable
information and, importantly, handle deterministic constraints common models. section, describe standard probabilistic model
genetic linkage, several approximate distributions use applying vip?
genetic linkage model, demonstrate vip? real-world data set large pedigree
115 individuals.
standard probabilistic model genetic linkage based pedigree contains several variables person location conditional probability tables
variable Xm given set variables called parents Xm denoted (Xm ).
distribution P (x) represents joint distribution variables pedigree
written using multiple indices; one set indices persons (i), one loci (j),
another type variable (t) follows:
P (x) =

YY
j

i,t
P (xi,t
j |(xj )) =



t{ps,ms,pg,mg,f }

1 YY
ZP
j



t{ps,ms,pg,mg,f }

15

i,t
i,t
i,t
j (xj |(xj ))

(15)

fiGeiger, Meek & Wexler

five possible types variables are: paternal selector (ps), maternal selector (ms),
paternal genotype (pg), maternal genotype (mg) phenotype (f). Thus, set Dji,t
equals {Xji,t , (Xji,t )}.
denote variables different types, ps, ms, pg, mg f, individual
i,m
locus j Sji,p , Sji,m , Gi,p
Fji respectively. notation possible potentials
j , Gj
a,p
a,m
i,p
i,m
b,p
b,m
i,m
i,p
i,p
i,m
i,m
P (Gi,p
j , Gj , Gj , Sj ), (Gj , Gj , Gj , Sj ), (Sj , Sj1 ), (Sj , Sj1 )
i,m
(Fji , Gi,p
j , Gj ) b father mother pedigree, respectively.
i,p
a,p
a,m
i,ms
i,m
exemplifying sets Dji,pg = {Gi,p
= {Sji,m , Sj1
}, Dji,f =
j , Sj , Gj , Gj }, Dj
i,m
{Fji , Gi,p
j , Gj }, father individual pedigree. note first
two types potentials possibly last one deterministic potentials equal
zero instantiations.
directed
Q acyclic graph along probability distribution R factors according
R(Z) = R(zi |(zi )) called Bayesian network. Bayesian network defined
Eq. 15, describes parents-offspring interaction simple genetic analysis problem
two siblings parents across 3 loci, given Figure 4. dashed boxes
contain variables describe variables single location. example
assume phenotype variable depends genotype single locus.
reflected fact edges single locus point phenotype variable.
full semantics variables details regarding conditional probability
tables found paper Fishelson & Geiger (2002); details needed
here.
use several choices cluster Bayesian network P (x) Q selfcompatible compatible wrt P . addition, since potentials P constrained
(e.g. i,pg
j ), choose clusters Q containable wrt P . According Theorem 2
choice ensures Q satisfies conditions necessary convergence vip? ,
particular (P (x) = 0) (Q(x) = 0).
Consider partition network slots, containing set consecutive loci.
simple case every slot single locus subsets Cji contains
variables related one individual genotypes parents slot. set

(i)
Cji = {Gj , Sji } Cj = {Cji } (i) denotes union parents.
illustration setting pedigree two siblings parents three
loci given Figure 5. setting, self-compatibility trivially satisfied
clusters Cj Q disjoint, Q containable wrt P since sets Dji,ps Dji,ms ,
potentials constrained, span across single locus. remains
show compatibility Q wrt P satisfied. sets Dji,t contained single subset

Cji trivial Q(Dji,t |cj ) = Q(Dji,t |cji ) = 1. Otherwise, equals ps ms without
i,p
i,p
i,p
i,p
loss generality, Q(Sj1
, Sji,p |cj ) = Q(Sj1
|cj ) = Q(Sj1
|cji ) = Q(Sj1
, Sji,p |cji ).
complex setup, similar Example 1, add cluster CJ+1
cuts across loci selector variables individual r, shown Figure 6.

(i)
subset Cji set Cji = {GjS , Sji , Sjr } clusters Cj = {Cji }, j = 1 . . . J.
r
addition, set CJ+1 = l {CJ+1,l } subsets CJ+1,l = {Sl,l+1
}, single
chosen individual r. verify Q still satisfies conditions Theorem 1. Selfcompatibility maintained since Q(Cj |cJ+1 ) = Q(Cj |cJ+1,j ), Q(CJ+1 |cj ) = Q(CJ+1 |cjr ),

16

fiA Variational Inference Procedure

Figure 4: Bayesian network representation pedigree two siblings parents
3-loci model. circles, squares diamond shapes represent genotype,
phenotype selector variables respectively.

Q(Ck |cj ) = Q(Ck |cjr ) every two clusters Cj , Ck j, k J. Sets Dji,t
6= ms, ps contained cluster Cj thus maintain independence given
subset. = ms, ps sets Dji,t connect two adjacent loci independent CJ+1
given CJ+1,j , independent clusters Cj given subset Cji , maintaining
compatibility Q wrt P . Finally, Q containable wrt P since clusters previous
option remain.
Immediate extensions clustering schemes allow every slot contain several
consecutive loci set possibly one individual R cut across loci.
maintain compatibility Q wrt P latter extension, subsets CjR set
(R)
CjR = {Gj , SjR }, (R) denotes union individuals R parents.
describe experiments performed using large pedigree 115 individuals
spanning across 21 locations, studied Narkis et al. (2004) locate area
contains gene causes fatal neurological disorder (LCCS type 2). First,
proximity disease gene markers tested two-point analysis
- model-selection method two loci considered simultaneously, one
disease locus. method, loci yield likelihood maxima suggest
probable locations disease locus. Two-point analysis abovementioned pedigree
took several days using exact inference software superlink v1.5 designed genetic
linkage analysis.
17

fiGeiger, Meek & Wexler

Figure 5: schematic division pedigree clusters, locus cluster.
cluster C3 variables every individual separate ellipse
number individual written. two clusters marked areas
C14 C23 .

log-likelihood probabilities obtained vip? using abovementioned extended
clustering scheme 5 clusters across loci cluster cutting across
loci, shown Figure 7. figure shows clearly exact approximate loglikelihood curves similar shape almost identical extremum points,
major difference absolute value.
Next, tested vip? three-point analysis problems pedigree,
two markers considered simultaneously along one disease locus. note
exact inference task specified pedigree hard single PC, taking several
weeks. Since exact location disease gene known high probability,
wished test whether lower-bounds found vip? indicate location.
considered two nearby markers (number 4 6) seven models differ
location speculated disease gene: first two, disease locus positioned
left marker 4 distances 0.01 0.02 centi-Morgan (cM), remaining five
positioned right marker 4 distances 0.01 0.05 cM 0.01 cM difference
locations. location disease gene 0.01cM right marker 4.
algorithm run three times model random initial potentials, taking
account maximum value obtained. results test plotted Figure 8
versus approximation found sampling software simwalk2 introduced Sobel,
Papp & Lange (2002) designed approximate pedigree analysis. shown,
probabilities found vip? higher approach location disease gene.
true probabilities found simwalk2. However, note vip?
18

fiA Variational Inference Procedure

ln-likelihood

Figure 6: pedigree partitioned clusters, locus cluster additional cluster C4 , striped area, contains variables one individuals.

0
-20
-40
-60
-80
-100
-120
-140
-160
-180
-200
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21

Exact

locus

VIP* - 5 clusters across loci
VIP* - disjoint clusters

Figure 7: Approximations log-likelihood two-point analysis using vip? .

much slower problem simwalk2, taking several hours run. addition,
19

fiGeiger, Meek & Wexler

note ln-likelihood probabilities Figures 8(a) (b) drawn different
scales due markedly different output two methods.

-182

-77
-78

-186

ln-likelihood

ln-likelihood

-184

-188
-190
-192
-194

-79
-80
-81
-82

-0.02

-0.01

0.01

0.02

0.03

0.04

0.05

-0.02

location (relative marker 4)

-0.01

0.01

0.02

0.03

0.04

0.05

location (relative marker 4)

(a) using vip?

(b) using simwalk2

Figure 8: Approximations log-likelihood three-point analysis.
compared convergence times vip? Wiegerincks algorithm various size
problems genetic linkage analysis. original network performed test
includes 332 variables represents pedigree 28 individuals four loci. create
various sized problems, subsets original pedigree increasing number individuals
considered. addition, fair comparison, clusters Wiegerincks algorithm
chosen subset subsets used vip? . Since number iterations
convergence vary significantly two algorithms, report ratio
iteration times tests theoretical speedup predicted Section 3 vip?
Wiegerincks algorithm. Figure 9 illustrates ratio time update iteration
two algorithms, evident ratio increases linearly problem size.
ratios indicated averaged 5 runs 10 iterations every problem size.
Finally examine convergence vip? Figure 10 using six representatives
original 21 two-point analysis runs described earlier, different network.
algorithm halted change lower-bound smaller 105
3 iterations. Although runs converge rate, seems obey
certain pattern convergence first iterations show significant improvements
lower-bound, followed slow convergence local maximum, another
moderate improvement better maximum point.

7. Discussion
paper present efficient algorithm called vip? structured variational approximate inference. algorithm, extends known algorithms, handle overlapping
clusters overcome difficulties imposed deterministic constraints. show
N N grid-like models, algorithm vip? N fold faster Wiegerincks algorithm,
20

fiA Variational Inference Procedure

20
18

ratio iteration times

16
14
12
10
8
6
4
2
0
10

12

14

16

18

20

22

24

26

28

number individuals

Figure 9: Speedup vip? Wiegerincks algorithm.

-70
-90

ln-likelihood

-110
-130
-150
-170
-190
-210
-230
1

3

5

7

9

11

13

15

17

19

21

23

iteration

Figure 10: Convergence vip? 6 runs two-point analysis.
junction tree used. addition, prove convergence vip? previous
variational methods via novel proof method, using properties KL divergence.
Finally, algorithm vip? tested Bayesian networks model genetic linkage analysis problems. graphs resemble grid-like models notoriously difficult
approximate due numerous deterministic constraints. results show linear
improvement speed vip? versus Wiegerincks algorithm, approximation
21

fiGeiger, Meek & Wexler

follows shape real likelihood probabilities. Nevertheless, Figure 7 shows variational methods Wiegerincks algorithm vip? still appropriate produce
accurate approximation likelihood genetic linkage analysis.

Acknowledgments
paper extension paper originally appeared 10th workshop
Artificial Intelligence Statistics (Geiger & Meek 2005). thank D. Heckerman, N.
Jojic V. Jojic helpful discussions. thank two anonymous reviewers
correcting several errors appeared early version well improving
presentation. Part work done first author visitor Microsoft
Research. work supported Israeli Science Foundation Israeli Science
Ministry.

References
Bishop, C. & Winn, J. (2003). Structured variational distributions VIBES. Artificial
Intelligence Statistics. Society Artificial Intelligence Statistics.
Cooper, G. (1990). Probabilistic inference using belief networks NP-hard. Artificial
Intelligence, 42, 393405.
Cover, T. M. & Thomas, J. A. (1991). Elements Information Theory. Wiley.
Dagum, P. & Luby, M. (1993). Approximating probabilistic inference Bayesian belief
networks NP-hard. Artificial Intelligence, 60 (1), 141153.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Fishelson, M. & Geiger, D. (2002). Exact genetic linkage computations general pedigrees.
Bioinformatics, 18, S189S198.
Geiger, D. & Meek, C. (2005). Structured variational inference procedures realizations. Proceedings Tenth International Workshop Artificial Intelligence
Statistics, Barbados. Society Artificial Intelligence Statistics.
Ghahramani, Z. & Jordan, M. I. (1997). Factorial hidden Markov models. Machine Learning, 29, 245273.
Jensen, F. V. (1996). Introduction Bayesian Networks. Springer.
Jojic, V., Jojic, N., Meek, C., Geiger, D., Siepel, A., Haussler, D., & Heckerman, D.
(2004). Efficient approximations learning phylogenetic HMM models data.
Bioinformatics, 20, 161168.
Kschischang, F. R., Frey, B. J., & Loeliger, H. A. (2001). Factor graphs sum-product
algorithm. IEEE Transactions information theory, 47 (2), 498519.
22

fiA Variational Inference Procedure

Narkis, G., Landau, D., Manor, E., Elbedour, K., Tzemach, A., Fishelson, M., Geiger, D.,
Ofir, R., Carmi, R., & Birk, O. (2004). Homozygosity mapping lethal congenital
contractural syndrome type 2 (LCCS2) 6 cM interval chromosome 12q13.
American Journal Medical Genetics, 130 (3), 272276.
Saul, L. & Jordan, M. I. (1996). Exploiting tractable substructures intractable networks.
Advances Neural Information Processing Systems (NIPS). MIT Press.
Sobel, E., Papp, J., & Lange, K. (2002). Detection integration genotyping errors
statistical genetics. American Journal Human Genetics, 70, 496508.
Wiegerinck, W. (2000). Variational approximations mean field theory junction tree algorithm. Uncertainty Artificial Intelligence, (pp. 626633). Morgan
Kaufmann.
Xing, E. P., Jordan, M. I., & Russell, S. (2003). generalized mean field algorithm
variational inference exponential families. Uncertainty Artificial Intelligence,
(pp. 583591). Morgan Kaufmann.
Xing, E. P., Jordan, M. I., & Russell, S. (2004). Graph partition strategies generalized
mean field inference. Uncertainty Artificial Intelligence, (pp. 602 610). Morgan
Kaufmann.

23


