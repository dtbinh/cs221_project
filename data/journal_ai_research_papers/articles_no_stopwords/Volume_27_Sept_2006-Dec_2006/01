journal artificial intelligence

submitted published

generative prior knowledge discriminative classification
arkady epshteyn
gerald dejong

aepshtey uiuc edu
dejong uiuc edu

department computer science
university illinois urbana champaign
n goodwin
urbana il usa

abstract
present novel framework integrating prior knowledge discriminative classifiers framework allows discriminative classifiers support vector machines
svms utilize prior knowledge specified generative setting dual objective
fitting data respecting prior knowledge formulated bilevel program
solved approximately via iterative application second order cone programming
test consider wordnet semantic database
english language improve low sample classification accuracy newsgroup categorization wordnet viewed approximate readily available source background
knowledge framework capable utilizing flexible way

introduction
svm vapnik classification accuracy many classification tasks often
competitive human subjects number training examples required
achieve accuracy prohibitively large domains intelligent user interfaces
example must adopt behavior individual user limited amount
interaction order useful medical systems diagnosing rare diseases generalize
well seeing examples natural language processing task performs
processing level n grams phrases frequent translation systems
cannot expect see sequence words sufficient number times even large
training corpora moreover supervised classification methods rely manually labeled
data expensive obtain thus important improve classification
performance small datasets classifiers competitive humans
ability generalize seeing examples techniques
proposed address active learning tong koller b campbell
cristianini smola hybrid generative discriminative classification raina shen
ng mccallum learning learn extracting common information related
learning tasks thrun baxter fink prior knowledge
work concentrate improving small sample classification accuracy
prior knowledge prior knowledge proven useful classification scholkopf
simard vapnik smola wu srihari fung mangasarian shavlik
epshteyn dejong sun dejong notoriously hard apply practice
mismatch form prior knowledge employed
classification prior probabilities explicit constraints hypothesis
c

ai access foundation rights reserved

fiepshteyn dejong

space classifier domain theories articulated human experts
unfortunate ontologies domain theories available abundance
considerable amount manual effort required incorporate existing prior knowledge
native learning bias chosen would take apply
existing domain theory automatically classification task specifically
designed work take first steps towards answering question
experiments domain theory exemplified wordnet linguistic
database semantic connections among english words miller apply wordnet standard benchmark task newsgroup categorization conceptually generative
model describes world works discriminative model inextricably linked
specific classification task thus reason believe generative interpretation
domain theory would seem natural generalize better across different
classification tasks section present empirical evidence indeed
case wordnet context newsgroup classification reason interpret
domain theory generative setting however many successful learning
support vector machines discriminative present framework allows
use generative prior discriminative classification setting
assumes generative distribution data given
bayesian framework p rob data model prior p rob model known however
instead performing bayesian model averaging assume single model
selected priori observed data manifestation model e
drawn according p rob data goal learning estimate
estimation performed two player sequential game full information
bottom generative player chooses bayes optimal discriminator function f
probability distribution p rob data model without taking training data
account given model model chosen top discriminative player
way prior probability occurring given p rob high forces
bottom player minimize training set error bayes optimal discriminator
f estimation procedure gives rise bilevel program
known np hard approximation solved efficiently iterative
application second order cone programming
remaining issue construct generative prior p rob model automatically domain theory describe solve section
argue generative setting appropriate capturing expert knowledge employing wordnet illustrative example section give necessary
preliminary information important known facts definitions framework incorporating generative prior discriminative classification described detail section
demonstrate efficacy experimentally presenting
wordnet newsgroup classification section theoretical explanation
improved generalization ability discriminative classifier constrained generative
prior knowledge appears section section describes related work section concludes
outlines directions future


figenerative prior knowledge discriminative classification

generative vs discriminative interpretation domain knowledge
wordnet viewed network nodes representing words links representing
relationships two words synonyms hypernyms meronyms partof etc important property wordnet semantic distance length
links shortest path two words semantic distance approximately
captures degree semantic relatedness two words set experiment
evaluate usefulness wordnet task newsgroup categorization posting
represented bag words binary feature representing presence
corresponding word evaluation done pairwise classification tasks
following two settings
generative framework assumes posting x x xn generated
distinct probability distribution newsgroup simplest version
linear discriminan analysis lda classifier posits x n
x n posting x given label r nn
identity matrix classification done assigning probable label
x x p rob x p rob x well known e g see duda hart
stork decision rule equivalent one given hyperplane
c
cn estimated via
x means bi



maximum likelihood training data x xm ym

discriminative svm classifier sets separating hyperplane directly minimize
number errors training data
c w
cn bb arg minw b kwk yi wt xi b
w
b w

experiment conducted learning learn framework thrun baxter
fink first stage classifier trained training data
training task e g classifying postings newsgroups atheism guns
second stage classifier generalized wordnets semantic information
third stage generalized classifier applied different test task e g classifying
postings newsgroups atheism vs mideast without seeing data
classification task way classifier generalize setting use
original sample acquire information wordnet exploit information
help label examples test sample learning perform task
system learns utilize classification knowledge implicit wordnet
describe second third stages two classifiers detail

intuitive interpret information embedded wordnet follows title
newsgroup guns words semantic distance
gun e g artillery shooter ordnance distance two provide
similar degree classification information quantify intuition let li train
j

n
li train
li train
li train
vector semantic distances wordnet
feature word j label training task newsgroup define
standard lda classifier assumes x n x n
estimates covariance matrix well means training data experiments
take



fiepshteyn dejong

train atheism vs guns
train atheism vs guns
train guns vs mideast
test atheism vs mideast
test guns vs mideast
test atheism vs mideast















































legend

generative
discriminative

figure test set accuracy percentage versus number training points
different classification experiments classification task random test
set chosen full set articles different ways error bars
confidence intervals

p

v

c
j
j
v
train
j
v
j li train
j l

denotes cardinality set compresses

information bi assumption words equidistant newsgroup
label equally likely appear posting newsgroup test
performance compressed classifier task semantic distances given
j
notice
li test generative distributions reconstructed via ji li test
classifier trained tested task applying function
equivalent averaging components means generative distribution
corresponding equivalence classes words equidistant label
classifier tested different classification task reconstruction process reassigns
averages semantic distances labels
less intuitive interpret wordnet discriminative setting one possible
interpretation coefficients w j separating hyperplane governed
semantic distances labels captured compression function v u
p

cj

w
j
j
u
v l
train
train
j
j
j l train
v l train
u
j l

j
j
reconstructed via w j l test
l test


note lda generative classifier svm discriminative classifier
hypothesis space separating hyperplanes resulting test set classification
accuracy classifier classification tasks newsgroup dataset


figenerative prior knowledge discriminative classification

blake merz presented figure x axis graph represents
size training task sample axis classifiers performance test
classification task generative classifier consistently outperforms discriminative
classifier converges much faster two three tasks discriminative classifier
able use prior knowledge nearly effectively generative classifier even
seeing available training data generative classifier
consistent performance note error bars much smaller
discriminative classifier clearly potential background
knowledge vehicle sharing information tasks effective sharing
contingent appropriate task decomposition supplied tuned generative
model
evidence figure seemingly contradicts conventional wisdom discriminative training outperforms generative sufficiently large training samples however
experiment evaluates two frameworks context ontology transfer
information learning tasks never done experiment demonstrates interpretation semantic distance wordnet intuitive
generative classification setting probably better reflects human intuitions
behind wordnet
however goal construct classifier performs well without seeing
examples test classification task want classifier improves
behavior sees labeled data test classification task presents us
one best performing classifiers certainly best text
classification task according study joachims svm discriminative
classifier therefore rest work focus incorporating generative prior
knowledge discriminative classification framework support vector machines

preliminaries
observed constraints probability measure half space
captured second order cone constraints gaussian distributions see e g tutorial
lobo vandenberghe boyd lebret allows efficient processing
constraints within framework second order cone programming socp intend
model prior knowledge elliptical distributions family probability distributions
generalizes gaussians follows give brief overview second order
cone programming relationship constraints imposed gaussian probability
distribution note possible extend argument presented lobo et
al elliptical distributions
second order cone program mathematical program form
min v x



x

kai x bi k cti x di n



x rn optimization variable v rn ai r ki xn bi rki ci rn
di r parameters kk represents usual l norm socps
solved efficiently interior point methods described lobo et al
tutorial contains excellent overview theory applications socp


fiepshteyn dejong

use elliptical distribution model distribution data priori elliptical
distributions distributions ellipsoidally shaped equiprobable contours density
function n variate elliptical distribution form f g x c det g x
x x rn random variable rn location parameter
r nxn positive definite n n matrix representing scale parameter function
g density generator c normalizing constant use notation x e g denote random variable x elliptical distribution
parameters g choosing appropriate density generator functions g gaussian
distribution student distribution cauchy distribution laplace distribution
logistic distribution seen special cases elliptical distribution elliptical distribution relaxes restrictive assumptions user make
imposing gaussian prior keeping many desirable properties gaussians
x e g r kn b rk ax b e b aat g
x e g e x
x e g v ar x g g constant depends
density generator g
following proposition shows elliptical distributions constraint p w x b
e probability x takes values half space w x b less
equivalent second order cone constraint
proposition
x e g p rob w x b equivalent w

b g w g constant depends g
proof proof identical one given lobo lanckriet et al
gaussian distributions provided completeness
assume p rob w x b



let u w x b let u denote mean u denote variance constraint
written
uu
u

p rob





properties elliptical distributions u w b g w uu





e g thus statement expressed p robxe g x w b
w
k
gk

equivalent w b
z p robxe g x z
w
k
gk

proposition follows g g

proposition monotonically decreasing g p robxe g x equivalent


x g c g c g
c constant depends
g c
proof follows directly definition p robxe g x


figenerative prior knowledge discriminative classification

generative prior via bilevel programming
deal binary classification task classifier function f x maps
instances x rn labels generative setting probability densities
p rob x p rob x parameterized provided
estimated data along prior probabilities class labels
bayes optimal decision rule given classifier
f x sign p rob x p rob x
sign x x otherwise lda instance parameters
means two gaussian distributions generating data given label
informally incorporating prior knowledge straightforward assume
two level hierarchical generative probability distribution model low level probability
distribution data given label p rob x parameterized turn
known probability distribution p rob goal classifier estimate
values parameter vector training set labeled points x xm ym
estimation performed two player sequential game full information
bottom generative player given selects bayes optimal decision rule f x
top discriminative player selects value high probability occurring
according p rob force bottom player select decision rule
minimizes discriminative error training set give formal
specification training formulate bilevel program
assumptions subsequently relaxed enforce tractability flexibility
use elliptical distribution e g model x another elliptical
distribution e g model x parameters known
bayes optimal decision rule restricted class linear classifiers form
fw b x sign w x b given f x minimizes probability error among
linear discriminants p rob error p rob w x b p rob w x b
p robxe g wt x b p robxe g wt x b
assuming equal prior probabilities classes model uncertainty
means elliptical distributions imposing elliptical prior distributions
locations means e ti g addition ensure optimization
well defined maximize margin hyperplane subject imposed
generative probability constraints
min kwk





yi wt xi b

p robi e ti g







w b solves min p robxe g w x b p robxe g w x b
w b


bilevel mathematical program e optimization
constraint region implicitly defined another optimization strongly
decision rule restricted class classifiers h optimal probability error larger
classifier h tong koller



fiepshteyn dejong

np hard even constraints objectives linear hansen jaumard
savard however possible solve reasonable approximation efficiently several iterations second order cone programming
first relax second level minimization breaking two constraints
p robxe g wt x b p robxe g wt x b thus instead looking bayes optimal decision boundary looks decision
boundary low probability error low error quantified choice
propositions enable us rewrite optimization resulting
relaxation follows
min kwk



w b

yi wt xi b





ti
p robi e ti g

w b

p robxe g wt x b

w

w b

p robxe g wt x b

w







notice form program depend generator function g
elliptical distribution constants depend defines far system
willing deviate prior choice generative model bounds
tail probabilities error type type ii system tolerate assuming
chosen generative model correct constants depend specific generator
g amount error user willing tolerate experiments select
values constants optimize performance unless user wants control
probability bounds constants sufficient assume priori
probability distributions prior hyper prior elliptical without making
commitments
solves repeating following two steps
fix top level optimization parameters step combines objectives maximizing margin classifier training data ensuring
decision boundary approximately bayes optimal respect given
generative probability densities specified
fix bottom level optimization parameters w b expand feasible region
program step function step fixes decision boundary
pushes means generative distribution far away boundary
constraint allow
steps repeated convergence practice convergence detected
optimization parameters change appreciably one iteration next
step formulated second order cone program


figenerative prior knowledge discriminative classification

step fix removing unnecessary constraints mathematical
program pushing objective constraints get following socp
min



w b

kwk



yi wt xi b



wt

b



w



w b



w



step fix w b expand span feasible region measured

w b

w


w b

w



removing unnecessary constraints get
w b w b


max


w
w




ti





behavior illustrated figure
following theorems state converges
theorem
suppose
n
produces sequence iterates





quality iterate evaluated margin w
w b

evaluation function converges








proof let values prior location parameters w b
minimum error hyperplane finds end th step end

st step w
b
still feasible region th step socp



b

w



true function f w




b

w



w



w b

w



w b

w

monotonically increasing one arguments argument fixed

fixing fixes exactly one argument solution

end


st step


fixing





w
b



w

f could increased



value beginning step ensures



w
b


w

contradicts observation f maximized end


second step contradiction reached



w
b



w

since

minimum error hyperplane previous
iteration feasible region start




must decrease monotonically one iteration
next iteration objective w
next since bounded zero converges


fiepshteyn dejong




















































































































figure steps iterative hard margin socp procedure
region hyperprior probability larger shaded prior
distribution covariance matrices represented equiprobable elliptical contours
example covariance matrices hyperprior prior distributions
multiples data points two different classes represented diamonds
squares
data prior hyperprior executed
hyperplane discriminator end step iteration
priors end step iteration
hyperplane discriminator end step iteration
converges end step step move
hyperplane
addition convergence objective function accumulation points
sequence iterates characterized following theorem
n


theorem accumulation points sequence w b e limiting
points convergent subsequences feasible descent directions original
optimization given
proof see appendix


figenerative prior knowledge discriminative classification

point feasible descent directions sufficiently small step along
directional vector increase objective function leave unchanged take
outside feasible region set points feasible descent directions
subset set local minima hence convergence point somewhat
weaker convergence local minimum
practice observed rapid convergence usually within iterations
finally may want relax strict assumptions correctness prior linear
separability data introducing slack variables optimization
following program
min

w b

kwk c


x

c c





yi wt xi b




ti


w b

w




w b

w














solved two step iterative socp procedure
imposing generative prior soft constraints ensures amount training
data increases data overwhelms prior converges maximummargin separating hyperplane

experiments
experiments designed demonstrate usefulness proposed
incorporation generative prior discriminative classification address
broader question showing possible use existing domain theory aid
classification task specifically designed order construct
generative prior generative lda classifier trained data training
classification task estimate gaussian location parameters bi described
section compression function v subsequently computed described
j
section used set hyperprior parameters via ji li test

order apply domain theory effectively task specifically
designed must able estimate confidence decomposition
domain theory respect learning task order model uncertainty
applicability wordnet newsgroup categorization system estimated confidence
homogeneity equivalence classes semantic distances computing variance


fiepshteyn dejong


bilevel gen discr















svm







figure performance bilevel discriminative classifier constrained generative
prior knowledge versus performance svm point represents unique
pair training test tasks test task data used training
averaged experiments

p

random variable v follows v

j l

c
ji v
j
train v
j
j li tran
v

hyperprior confidence

matrices reconstructed
respect test task semantic distances

j
li test
k j
identity matrices used
li test follows j k
k j
covariance matrices lower level prior rest parameters
set follows c c c constants
chosen manually optimize performance experiment training task atheism
vs guns test task guns vs mideast see figure without observing data
classification tasks
resulting classifier evaluated different experimental setups different
pairs newsgroups chosen training test tasks justify following
claims
bilevel generative discriminative classifier wordnet derived prior knowledge good low sample performance showing feasibility automatically
interpreting knowledge embedded wordnet efficacy proposed

bilevel classifiers performance improves increasing training sample size
integrating generative prior discriminative classification framework
better performance integrating prior directly generative
framework via bayes rule


figenerative prior knowledge discriminative classification

bilevel classifier outperforms state art discriminative multitask classifier
proposed evgeniou pontil taking advantage wordnet domain
theory
order evaluate low sample performance proposed classifier four newsgroups
newsgroup dataset selected experiments atheism guns middle east
auto categories thirty experimental setups created possible
ways assigning newsgroups training test tasks pair newsgroups assigned
task constraint training test pairs cannot identical
experiment compared following two classifiers
bilevel generative discriminative classifier knowledge transfer functions
v v learned labeled training data provided training task available data task resulting prior
subsequently introduced discriminative classification framework via approximate bilevel programming
vanilla svm classifier minimizes regularized empirical risk
min

w b


x


c kwk

yi wt xi b




classifiers trained available data test classification
task evaluated remaining test task data averaged
one hundred randomly selected datasets presented figure shows
plot accuracy bilevel generative discriminative classifier versus accuracy
svm classifier evaluated thirty experimental setups points
lie line indicating improvement performance due incorporation prior
knowledge via bilevel programming framework amount improvement ranges
improvements statistically significant
level
next experiment conducted evaluate effect increasing training data
test task performance system experiment selected
three newsgroups atheism guns middle east generated six experimental setups
possible ways splitting newsgroups unique training test pairs
addition classifiers following classifiers evaluated
state art multi task classifier designed evgeniou pontil
classifier learns set related classification functions ft x wtt x bt classification tasks training task test task given data points x xm ym
newsgroup articles preprocessed removing words could interpreted nouns
wordnet preprocessing ensured one part wordnet domain theory exercised
resulted virtually reduction classification accuracy
sedumi software sturm used solve iterative socp programs



fiepshteyn dejong

task minimizing regularized empirical risk
min

w wt bt

x
x






c x
kwt w k c kw k
c

yit wtt xit bt






regularization constraint captures tradeoff final w close
average model w large margin training data
training task data made available classifier constant c chosen
c selected set optimize
classifiers performance experiment training task atheism vs guns
test task guns vs mideast see figure observing test task data
addition training task data
lda classifier described section trained test task data since
classifier bottom level generative classifier used bilevel
performance gives upper bound performance bottomlevel classifier trained generative fashion
figure shows performance classifiers function size training
data test task evaluation done remaining test task data
averaged one hundred randomly selected datasets performance bilevel
classifier improves increasing training data discriminative portion
classifier aims minimize training error generative prior imposed
soft constraints expected performance curves classifiers converge
amount available training data increases even though constants used mathematical program selected single experimental setup classifiers performance
reasonable wide range data sets across different experimental setups
possible exception experiment training task guns vs mideast testing task atheism
vs mideast means constructed elliptical priors much closer
experiments thus prior imposed greater confidence
warranted adversely affecting classifiers performance
multi task classifier outperforms vanilla svm generalizing data points
across classification tasks however take advantage prior knowledge
classifier gain performance bilevel generative discriminative classifier
due fact relationship classification tasks captured much
better wordnet simple linear averaging weight vectors
constants involved bilevel classifier generative classifiers bayesian priors hard fair comparison classifiers constrained
generative priors two frameworks instead generatively trained classifier
gives empirical upper bound performance achievable bottom level classifier
trained generatively test task data accuracy classifier shown
horizontal plots figure since discriminative classification known
superior generative classification svm classifier outperforms


figenerative prior knowledge discriminative classification

train atheism vs guns

train atheism vs guns

train guns vs mideast

test atheism vs mideast

test guns vs mideast

test atheism vs guns
































































train guns vs mideast



















train atheism vs mideast























































































test atheism vs guns







train atheism vs mideast

test guns vs mideast

test atheism vs mideast
































legend

lda max performance
bilevel gen discr
svm
multitask svm

figure test set accuracy percentage versus number test task training points
two classifiers svm bilevel gen discr tested six different classification
tasks classification experiment data set split randomly
training test sets different ways error bars
confidence intervals

generative classifier given enough data four six experimental setups
interesting range training sample sizes bilevel classifier constrained
generative prior outperforms svm trained sample
generative classifier trained much larger sample four setups means
unless prior knowledge outweighs effect learning cannot enable lda classifier
compete bilevel classifier
finally set experiments performed determine effect varying mathematical program parameters generalization error parameter
varied set values rest parameters held fixed increased
maximum feasible value evaluation done setup experiment


fiepshteyn dejong

accuracy function

accuracy function
























































figure plots test set accuracy percentage versus mathematical program parameter
values classification task random training set size chosen
full set test task articles different ways error bars
confidence intervals experiments performed training
task atheism vs guns test task guns vs mideast

training task atheism vs guns test task guns vs mideast training set size
points presented figure increasing value equivalent
requiring hyperplane separator smaller error given prior decreasing
value equivalent increasing confidence hyperprior actions
tighten constraints e decrease feasible region good prior knowledge
effect improving generalization performance small training samples
since prior imposed higher confidence precisely observe
plots figure

generalization performance
generalize well low sample sizes section derive
theorem demonstrates convergence rate generalization error
constrained generative discriminative classifier depends parameters mathematical program margin would expected case large margin
classification without prior particular certainty generative prior knowledge increases upper bound generalization error classifier
constrained prior decreases increasing certainty prior mean
hyper prior becomes peaked e confidence locations
prior means increases desired upper bounds type type ii probabilities
error classifier decrease e requirement lower level discriminative
player choose restricted bayes optimal hyperplane strictly enforced
argument proceeds bounding fat shattering dimension classifier constrained prior knowledge fat shattering dimension large margin classifier
given following definition taylor bartlett
definition set points x xm shattered set functions f
mapping domain x r real numbers r rm
b function fb f b fb xi ri say


figenerative prior knowledge discriminative classification

r rm witness shattering fat shattering dimension f function
fatf maps cardinality largest shattered set
specifically consider class functions
f x w x kxk r kwk







wt
w

































w
w

following theorem bounds fat shattering dimension classifier

theorem let f class priori constrained functions defined
let min p max p denote minimum maximum eigenvalues matrix p




respectively set points shattered f r







k max
kt ktk

max min min
min min

k k
k k
max kt k
kt k max

kt k max kt k

assuming kti k kti k






proof see appendix b
following corollary follows directly taylor bartletts
theorem bounds classifiers generalization error fat shattering
dimension
corollary let g class real valued functions probability least
independently generated examples z classifier h sgn g sgn g

margin least examples z error h



log

log




f




g

f


class

functions
log em
g
g









defined df r g f usual class large margin
classifiers without prior taylor bartlett shows f
r




notice bounds depend r
however bound classifier constrained

generative prior depends term particular increases tightening constraints bound decreases ensuring expected
quicker convergence generalization error similarly decreasing tightens
constraints decreases upper bound generalization error
factor less upper bound fat shattering dimension df
tighter usual bound prior case df
since controls amount deviation decision boundary bayesoptimal hyperplane depends variance hyper prior distribution tightening
constraints corresponds increasing confidence prior note high
value represents high level user confidence generative elliptical model
note two ways increasing tightness hyperprior constraint
one user defined parameter automatically
estimated covariance matrices matrices estimate extent


fiepshteyn dejong

equivalence classes defined wordnet create appropriate decomposition domain
theory newsgroup categorization task thus tight constraint represents
high level user confidence means generative classification model estimated
wordnet good correspondence partition words imposed
semantic distance wordnet elliptical generative model data
approaches zero approaches highest feasible value solution bilevel
mathematical program reduces restricted bayes optimal decision boundary computed
solely generative prior distributions without data
hence shown prior imposed increasing level confidence
means elliptical generative model deemed good estimates
means good turn implies domain theory well suited
classification task hand convergence rate generalization error classifier
increases intuitively precisely desired effect increased confidence prior
since benefit derived training data outweighed benefit derived
prior knowledge low data samples improved accuracy assuming
domain theory good plots figure

related work
number approaches combining generative discriminative several focus deriving discriminative classifiers generative distributions tong
koller tipping learning parameters generative classifiers via
discriminative training methods greiner zhou roos wettig grunwald myllymaki tirri closest spirit maximum entropy
discrimination framework jebara jaakkola meila jebara performs
discriminative estimation parameters generative model taking account constraints fitting data respecting prior one important difference
framework estimating parameters maximum entropy discrimination minimizes distance generative model prior subject satisfying
discriminative constraint training data classified correctly given margin
framework hand maximizes margin training data subject
constraint generative model far prior emphasis
maximizing margin allows us derive priori bounds generalization error
classifier confidence prior yet available maximum entropy framework another difference performs classification
via single generative model maximum entropy discrimination averages set
generative weighted probabilities similar distinction
maximum posteriori bayesian estimation repercussions tractability maximum entropy discrimination however general framework sense
allowing richer set behaviors different priors
ng et al explore relative advantages discriminative generative
classification propose hybrid improves classification accuracy
low sample high sample scenarios collins proposes use viterbi
hmms inferencing generative assumptions combined
discriminative learning hmm parameter estimation


figenerative prior knowledge discriminative classification

directions orthogonal work since explicitly consider question
integration prior knowledge learning
context support vector classification forms prior knowledge
explored scholkopf et al demonstrate integrate prior knowledge
invariance transformations importance local structure kernel function
fung et al use domain knowledge form labeled polyhedral sets augment
training data wu srihari allow domain experts specify confidence
examples label varying effect example separating hyperplane
proportionately confidence epshteyn dejong explore effects rotational constraints normal separating hyperplane sun dejong
propose uses domain knowledge wordnet identify relevant
features examples incorporate resulting information form soft constraints
hypothesis space svm classifier mangasarian et al suggest use prior
knowledge support vector regression approaches prior knowledge takes
form explicit constraints hypothesis space large margin classifier
work emphasis generating constraints automatically domain knowledge
interpreted generative setting demonstrate wordnet application
generative interpretation background knowledge intuitive natural language
processing
second order cone constraints applied extensively model probability constraints robust convex optimization lobo et al bhattacharyya pannagadatta
smola constraints distribution data minimax machines lanckriet
et al huang king lyu chan work far know first one
prior knowledge constraints resulting optimization
connection bayes optimal classification different approaches
mentioned
work related empirical bayes estimation carlin louis empirical bayes estimation hyper prior parameters generative model estimated
statistical estimation methods usually maximum likelihood method moments
marginal distribution data learns parameters
discriminatively training data

conclusions future work
since many sources domain knowledge wordnet readily available believe
significant benefit achieved developing automatically applying
information classification argued generative paradigm interpreting background knowledge preferable discriminative
interpretation presented novel enables discriminative classifiers
utilize generative prior knowledge evaluated context complete system faced newsgroup classification task able estimate
parameters needed construct generative prior domain theory use
construction achieve improved performance newsgroup classification tasks
work restricted hypothesis class linear classifiers extending
form prior distribution distributions elliptical looking


fiepshteyn dejong

bayes optimal classifiers restricted expressive class linear separators
may improvement classification accuracy non linearly separable domains
however obvious approximate expressive form prior knowledge
convex constraints kernel trick may helpful handling nonlinear
assuming possible represent optimization exclusively terms
dot products data points constraints important issue requires
study
demonstrated interpreting domain theory generative setting
intuitive produces good empirical however usually multiple ways
interpreting domain theory wordnet instance semantic distance
words one measure information contained domain theory
complicated interpretations might example take account types links
path words hypernyms synonyms meronyms etc exploit commonsense observations wordnet words closer category label
likely informative words farther away comparing multiple ways
constructing generative prior domain theory ultimately selecting one
interpretations automatically fruitful direction

acknowledgments
authors thank anonymous reviewers valuable suggestions improving material upon work supported part national science foundation
award nsf iis part information processing technology office defense advanced projects agency award hr
opinions findings conclusions recommendations expressed publication
authors necessarily reflect views national science
foundation defense advanced projects agency

appendix convergence generative discriminative
let map h z z determine given point generates se
quence iterates iteration h iterative




section generates sequence iterates z applying following
map h
h h h

step h arg

min

w b u

kwk

set u defined constraints





yi w xi b



c w b


wt b

conic constraints cs w b
k wk



c w b





figenerative prior knowledge discriminative classification

step h w b arg

min

v

c w b c w b



set v given constraints











notice h h functions minima optimization
unique case step optimizes
strictly convex function convex set step optimizes linear non constant function
strictly convex set
convergence objective function min w b u kwk


shown theorem let denote set points map h
change value objective function e h
every accumulation point lies every point
augmented w b h point feasible descent
directions optimization equivalently expressed
min kwk v w b u

w b



order formally state need concepts duality theory
let constrained optimization given
min f x ci x k
x



following conditions known karush kuhn tucker kkt conditions necessary
x local minimum
proposition x local minimum k
p
f x ki ci x
k

ci x k
ci x k
k known lagrange multipliers constraints c ck
following well known states kkt conditions sufficient x
point feasible descent directions
proposition k following conditions satisfied x
p
f x ki ci x


fiepshteyn dejong

k
x feasible descent directions
proof sketch reproduce proof given textbook fletcher proposition true p
feasible direction vector st ci x x


k hence f x ki st ci x descent direction
following lemma characterizes points set

lemma let let w b h optimizer let
set lagrange multipliers corresponding
constraints solution w b define h let w b optimizer


proof consider case









since kw k kw k let set lagrange multipliers corresponding
constraints solution w b since w still feasible optimization
given argument theorem minimum
unique happen
w b w b

w b must satisfy kkt conditions implies
c w c w argument theorem means
kkt condition




therefore kkt condition w b w w b
b









c w b
c w b
kwk
x

x


w
c ww


c ww

b
kwk
b


yi
b

b



b

means kkt conditions optimization satisfied

point w b kkt condition satisfied feasibility w b
kkt condition satisfied condition observations

proofs two cases
analogous
following theorem states points kkt points e points
kkt conditions satisfied optimization given


figenerative prior knowledge discriminative classification

theorem let w b h w b kkt point
optimization given
proof let h lemma consider case




lemma



proofs two cases similar
kkt conditions h w b



c w b







kkt conditions h w b w b


kwk
w
kwk
b






x






yi x
yi









c w b
w
c w b
b
























w b w b


c w b
kwk


kwk


x
w


w
kwk
w
b


c
w


x

yi
b






kwk
b








c w b





kwk












c w b


w

c w b










b

















c w b





means kkt conditions optimization satisfied

point w b


satisfies kkt conditions assumption kkt conditions
h h
order prove convergence properties iterates use following theorem
due zangwill
theorem let map h z z determine iterative via
h let denote objective function let set points
map h change value objective function e h
suppose


fiepshteyn dejong

h uniformly compact z e compact subset z z
h z z
h strictly monotonic z e h
h closed z e wi w h wi h w
accumulation points sequence lie
following proposition shows minimization continuous function feasible
set continuous map functions argument forms closed function
proposition given
real valued continuous function f b
point set map u b continuous respect hausdorff metric
dist x max x x x maxxx minyy kx yk
define function f b
f arg min f b b f b f b b u
b u

assuming minimum exists unique function f closed
proof proof minor modification one given gunawardana byrne
let sequence
f b



function f closed f b suppose case e b f
arg minb u f b therefore
b arg min f b f b f b
b u



continuity f
f f f b



continuity u
dist u u b b b u



imply
k f f f b k



contradiction since assumption f arg minb u f b
b u
point set map u maps point set points u continuous respect distance
metric dist iff implies dist u u



figenerative prior knowledge discriminative classification

proposition function h defined closed
proof let sequence since iterates lie
closed feasible region bounded constraints boundary u
piecewise linear boundary u converges uniformly boundary u
implies hausdorff distance boundaries converges
zero since hausdorff distance convex sets equal hausdorff distance
boundaries dist u u converges zero hence proposition
implies h closed proposition implies h closed composition
closed functions closed hence h closed
prove main section
theorem let h function defined determines generative discriminative via h accumulation points
sequence augmented w b h feasible descent directions
original optimization given
proof proof verifying h satisfies properties theorem closedness
h shown proposition strict monotonicity shown theorem
since iterates closed feasible region bounded constraints h uniformly compact z since accumulation points lie
kkt points original optimization theorem therefore
feasible descent directions proposition

appendix b generalization generative discriminative classifier
need auxiliary proving theorem first proposition bounds
angle rotation two vectors w w distance angle
rotation vectors reference vector v sufficiently small
proposition b let kw k kw k kvk w v w v
w w
p
kw w k

proof

triangle inequality arccos w w arccos w v arccos w v arccos
since angle two vectors distance measure taking cosines
sides trigonometric equalities yields w w
expand kw w k kw k kw k w w w w since w w
part kw w k

next proposition bounds angle rotation two vectors
far away measured l norm distance


fiepshteyn dejong

proposition b let ktk k tk

tt
ktkkk






proof expanding k tk ktk kk tt k tk get
ktk
kk

kk
ktk



tt
ktkkk




ktkkk use triangle inequality ktk k tk kk
ktk k tk simplify

following proposition used bound angle rotation normal
w separating hyperplane mean vector hyper prior distribution
wt
kwkkk
ktk
ktk ktk

proposition b let
min

k tk ktk

wt
kwkktk



proof follows directly propositions b part b
prove theorem relies parts well known proof fatshattering dimension bound large margin classifiers derived taylor bartlett

theorem let f class priori constrained functions defined
let min p max p denote minimum maximum eigenvalues matrix p



respectively set points shattered f r








k max
kt ktk

min min
max min min

k k
k k
max kt k
kt k max

kt k max kt k

assuming kti k kti k








proof first use inequality min p kwk p w max p kwk relax
constraints
w
w


min
b

kwk
w





max
b
k k
min





wt

constraints imposed second prior relaxed

similar fashion produce

w

wt
min
kwk

b

k k max

b

assumptions made statement theorem hold



p
p
every subset satisfies k k r
assume shattered f argument used taylor bartlett
lemma shows definition fat shattering exists vector w

x
x
w


b


figenerative prior knowledge discriminative classification

similarly reversing labeling exists vector w
x
x
w

b
hence w w
implies

p



p

cauchy schwartz inequality


p
kw w k p
k k

b

constraints classifier represented b b imply proposition b
w
w


kw kkt k kw kkt k applying proposition b part
simplifying get
q
kw w k



applying analysis constraints b b get
q
kw w k
combining b b b get
x

x




p



b

b

b

defined statement theorem
taylor bartletts lemma proves probabilistic method
satisfies
x
p
x


b

r

combining b b yields

r



references
baxter j model inductive bias learning journal artificial intelligence

bhattacharyya c pannagadatta k smola second order cone programming formulation classifying missing data nips
blake
c

merz
c


newsgroups
http people csail mit edu people jrennie newsgroups

database

campbell c cristianini n smola query learning large margin classifiers proceedings seventeenth international conference machine learning
carlin b louis bayes empirical bayes methods data analysis
chapman hall
collins discriminative training methods hidden markov theory
experiments perceptron proceedings conference
empirical methods natural language processing


fiepshteyn dejong

duda r hart p stork pattern classification john wiley nd edition
epshteyn dejong g rotational prior knowledge svms proceedings
sixteenth european conference machine learning
evgeniou pontil regularized multi task learning proceedings
tenth acm sigkdd international conference knowledge discovery data
mining
fink object classification single example utilizing class relevance metrics
advances neural information processing systems
fletcher r practical methods optimization john wiley sons west sussex
england
fung g mangasarian shavlik j knowledge support vector machine
classifiers advances neural information processing systems
greiner r zhou w structural extension logistic regression discriminative
parameter learning belief net classifiers proceedings eighteenth national
conference artificial intelligence
gunawardana byrne w convergence theorems generalized alternating
minimization procedures journal machine learning
hansen p jaumard b savard g branch bound rules linear bilevel
programming siam journal scientific statistical computing
huang k king lyu r chan l minimum error minimax probability
machine journal machine learning
jaakkola meila jebara maximum entropy discrimination advances
neural information processing systems
jebara machine learning discriminative generative kluwer academic
publishers
joachims text categorization support vector machines learning many
relevant features proceedings tenth european conference machine learning
lanckriet g r g ghaoui l e bhattacharyya c jordan minimax
probability machine advances neural information processing systems
lobo vandenberghe l boyd lebret h applications second order
cone programming linear algebra applications
mangasarian shavlik j wild e knowledge kernel approximation
journal machine learning
miller g wordnet online lexical database international journal lexicography
ng jordan discriminative vs generative classifiers comparison
logistic regression naive bayes advances neural information processing
systems


figenerative prior knowledge discriminative classification

raina r shen ng mccallum classification hybrid generative discriminative advances neural information processing systems
roos wettig h grunwald p myllymaki p tirri h discriminative
bayesian network classifiers logistic regression machine learning
scholkopf b simard p vapnik v smola prior knowledge support
vector kernels advances kernel methods support vector learning
sturm j f sedumi matlab toolbox optimization symmetric cones optimization methods software
sun q dejong g explanation augmented svm incorporating
domain knowledge svm learning proceedings twenty second international conference machine learning
taylor j bartlett p generalization performance support vector machines
pattern classifiers advances kernel methods support vector learning
thrun learning n th thing easier learning first advances
neural information processing systems
tipping e sparse bayesian learning relevance vector machine journal
machine learning
tong koller restricted bayes optimal classifiers proceedings
seventeenth national conference artificial intelligence
tong koller b support vector machine active learning applications
text classification proceedings seventeenth international conference
machine learning
vapnik v nature statistical learning theory springer verlag
wu x srihari r incorporating prior knowledge weighted margin support
vector machines proceedings tenth acm sigkdd international conference
knowledge discovery data mining
zangwill w convergence conditions nonlinear programming management science




