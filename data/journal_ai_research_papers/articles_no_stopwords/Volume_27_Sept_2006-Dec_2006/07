Journal Artificial Intelligence Research 27 (2006) 419-439

Submitted 12/05; published 12/06

Engineering Note
FluCaP: Heuristic Search Planner First-Order MDPs
Steffen Holldobler
Eldar Karabaev
Olga Skvortsova

sh@iccl.tu-dresden.de
eldar@iccl.tu-dresden.de
skvortsova@iccl.tu-dresden.de

International Center Computational Logic
Technische Universitat Dresden, Dresden, Germany

Abstract
present heuristic search algorithm solving first-order Markov Decision Processes (FOMDPs). approach combines first-order state abstraction avoids evaluating states individually, heuristic search avoids evaluating states. Firstly,
contrast existing systems, start propositionalizing FOMDP
perform state abstraction propositionalized version apply state abstraction directly FOMDP avoiding propositionalization. kind abstraction referred
first-order state abstraction. Secondly, guided admissible heuristic, search
restricted states reachable initial state. demonstrate usefulness techniques solving FOMDPs system, referred FluCaP
(formerly, FCPlanner), entered probabilistic track 2004 International Planning Competition (IPC2004) demonstrated advantage planners
problems represented first-order terms.

1. Introduction
Markov decision processes (MDPs) adopted representational computational model decision-theoretic planning problems much recent work, e.g., Barto,
Bradtke, Singh (1995). basic solution techniques MDPs rely dynamic
programming (DP) principle (Boutilier, Dean, & Hanks, 1999). Unfortunately, classical dynamic programming algorithms require explicit enumeration state space grows
exponentially number variables relevant planning domain. Therefore,
algorithms scale complex AI planning problems.
However, several methods avoid explicit state enumeration developed
recently. One technique, referred state abstraction, exploits structure factored MDP representation solve problems efficiently, circumventing explicit state space
enumeration (Boutilier et al., 1999). Another technique, referred heuristic search,
restricts computation states reachable initial state, e.g., RTDP
Barto et al. (1995), envelope DP Dean, Kaelbling, Kirman, Nicholson (1995)
LAO Feng Hansen (2002). One existing approach combines techniques symbolic LAO algorithm Feng Hansen (2002) performs heuristic
search symbolically factored MDPs. exploits state abstraction, i.e., manipulates sets
states instead individual states. precisely, following SPUDD approach Hoey,
St-Aubin, Hu, Boutilier (1999), MDP components, value functions, policies,
admissible heuristic functions compactly represented using algebraic decision diagrams
c
2006
AI Access Foundation. rights reserved.

fiHolldobler, Karabaev & Skvortsova

(ADDs). allows computations LAO algorithm performed efficiently using
ADDs.
Following ideas symbolic LAO , given initial state, use admissible heuristic
restrict search states reachable initial state. Moreover,
exploit state abstraction order avoid evaluating states individually. Thus,
work much spirit symbolic LAO extends important way.
Whereas symbolic LAO algorithm starts propositionalization FOMDP,
performs state abstraction propositionalized version means
propositional ADDs, apply state abstraction directly structure FOMDP,
avoiding propositionalization. kind abstraction referred first-order state
abstraction.
Recently, following work Boutilier, Reiter, Price (2001), Holldobler Skvortsova
(2004) developed algorithm, referred first-order value iteration (FOVI)
exploits first-order state abstraction. dynamics MDP specified Probabilistic Fluent Calculus established Holldobler Schneeberger (1990),
first-order language reasoning states actions. precisely, FOVI produces
logical representation value functions policies constructing first-order formulae
partition state space clusters, referred abstract states. effect,
algorithm performs value iteration top clusters, obviating need explicit
state enumeration. allows problems represented first-order terms
solved without requiring explicit state enumeration propositionalization.
Indeed, propositionalizing FOMDPs impractical: number propositions grows considerably number domain objects relations.
dramatic impact complexity algorithms depends directly number propositions. Finally, systems solving FOMDPs rely propositionalizing
states propositionalize actions problematic first-order domains,
number ground actions grows dramatically domain size.
paper, address limitations proposing approach solving FOMDPs
combines first-order state abstraction heuristic search novel way, exploiting
power logical representations. algorithm viewed first-order generalization LAO , contribution show perform heuristic search
first-order MDPs, circumventing propositionalization. fact, show
improve performance symbolic LAO providing compact first-order MDP representation using Probabilistic Fluent Calculus instead propositional ADDs. Alternatively,
approach considered way improve efficiency FOVI algorithm
using heuristic search together symbolic dynamic programming.

2. First-order Representation MDPs
Recently, several representations propositionally-factored MDPs proposed,
including dynamic Bayesian networks Boutilier et al. (1999) ADDs Hoey et al.
(1999). instance, SPUDD algorithm Hoey et al. (1999) used solve
MDPs hundreds millions states optimally, producing logical descriptions value
functions involve hundreds distinct values. work demonstrates large
420

fiFluCaP: Heuristic Search Planner First-Order MDPs

MDPs, described logical fashion, often solved optimally exploiting logical
structure problem.
Meanwhile, many realistic planning domains best represented first-order terms.
However, existing implemented solutions first-order MDPs rely propositionalization, i.e., eliminate variables outset solution attempt instantiating terms
possible combinations domain objects. technique impractical
number propositions grows dramatically number domain objects
relations.
example, consider following goal statement taken colored Blocksworld
scenario, blocks, addition unique identifiers, associated colors.
G = X0 . . . X7 . red(X0 ) green(X1 ) blue(X2 ) red(X3 ) red(X4 )
red(X5 ) green(X6 ) green(X7 ) ower(X0 , . . . , X7 ) ,
ower(X0 , . . . , X7 ) represents fact eight blocks comprise one tower.
assume number blocks domain color distribution agrees
goal statement, namely eight blocks a, b, . . . , h domain,
four red, three green one blue. Then, full propositionalization
goal statement G results 4!3!1! = 144 different ground towers,
exactly many ways arranging four red, three green one blue block tower
eight blocks required color characteristics.
number ground combinations, hence, complexity reasoning propositional planner, depends dramatically number blocks and, importantly,
number colors domain. fewer colors domain contains, harder
solve propositional planner. example, goal statement G0 , G
above, eight blocks color, results 8! = 40320 ground towers,
grounded.
address limitations, propose concise representation FOMDPs within
Probabilistic Fluent Calculus logical approach modelling dynamically changing
systems based first-order logic. first, briefly describe basics theory
MDPs.
2.1 MDPs
Markov decision process (MDP), tuple (Z, A, P, R, C), Z finite set
states, finite set actions, P : Z Z [0, 1], written P(z 0 |z, a), specifies
transition probabilities. particular, P(z 0 |z, a) denotes probability ending
state z 0 given agent state z action executed. R : Z R realvalued reward function associating state z immediate utility R(z). C : R
real-valued cost function associating cost C(a) action a. sequential
decision problem consists MDP problem finding policy : Z
maximizes total expected discounted reward received executing policy
infinite (or indefinite) horizon.
value state z, starting z following policy afterwards,
computed following system linear equations:
X
V (z) = R(z) + C((z)) +
P(z 0 |z, (z))V (z 0 ),
z 0 Z

421

fiHolldobler, Karabaev & Skvortsova

0 1 discount factor. take equal 1 indefinite-horizon problems
only, i.e., goal reached system enters absorbing state
rewards costs accrued. optimal value function V satisfies:
X
V (z) = R(z) + max{C(a) +
P(z 0 |z, a)V (z 0 )} ,
aA

z 0 Z

z Z.
competition, expected total reward model used optimality criterion. Without discounting, care required design planning problems
ensure expected total reward bounded optimal policy. following
restrictions made problems used planning competition:
1. problem goal statement, identifying set absorbing goal states.
2. positive reward associated transitioning goal state.
3. cost associated action.
4. done action available states, could used end accumulation reward.
conditions ensure MDP model planning problem positive bounded
model described Puterman (1994). positive reward transitioning
goal state. Since goal states absorbing, is, outgoing transitions,
maximum value state bounded goal reward. Furthermore, done action
ensures action available state guarantees non-negative future
reward.
2.2 Probabilistic Fluent Calculus
Fluent Calculus (FC) Holldobler Schneeberger (1990) originally set
first-order logic program equality using SLDE-resolution sole inference rule.
Probabilistic Fluent Calculus (PFC) extension original FC expressing
planning domains actions probabilistic effects.
States
Formally, let denote set function symbols. distinguish two function symbols
, namely binary function symbol , associative, commutative, admits
unit element, constant 1. Let = \ {, 1}. Non-variable -terms
called fluents. function names fluents referred fluent names. example,
on(X, table) fluent meaning informally block X table,
fluent name. Fluent terms defined inductively follows: 1 fluent term;
fluent fluent term; F G fluent term, F G fluent terms. example,
on(b, table) holding(X) fluent term denoting informally block b table
block X robots gripper. words, freely occurring variables
assumed existentially quantified.
422

fiFluCaP: Heuristic Search Planner First-Order MDPs

assume fluent may occur state. Moreover, function
symbols, except binary operator, constant 1, fluent names constants,
disallowed. addition, binary function symbol allowed appear
outermost connective fluent term. denote set fluents F set fluent
terms LF , respectively. abstract state defined pair (P, N ), P LF
N LF . denote individual states z, z1 , z2 etc., abstract states Z, Z1 , Z2 etc.
set abstract states LP N .
interpretation F, denoted I, pair (, ), domain set
finite sets ground fluents F; interpretation function assigns
fluent term F set F abstract state Z = (P, N ) set Z
follows:
F = {d | .F d}
Z = {d | .P N N .
/ (N )I },
substitution. example, Figure 1 depicts interpretation abstract
state Z
Z = (on(X, a) on(a, table), {on(Y, X), holding(X 0 )})
informally read: exists block X block
table, block X exists block X 0
robot holds. Since Z contains finite sets ground fluents satisfy
P -part satisfy elements N -part, subtract sets ground
fluents belong Ni N set ground fluents correspond
P -part. Thus, bold area Figure 1 contains exactly sets ground
fluents (or, individual states) satisfy P -part Z none elements
N -part. example, individual state z1 = {on(b, a), on(a, table)} belongs Z ,
whereas z2 = {on(b, a), on(a, table), holding(c)} not. words, abstract states
characterized means conditions must hold ground instance thereof
and, thus, represent clusters individual states. way, abstract states embody
form state space abstraction. kind abstraction referred first-order state
abstraction.
Actions
Actions first-order terms starting action function symbol. example,
action picking block X another block might denoted pickup (X, ).
Formally, let Na denote set action names disjoint . action space tuple
= (A, Pre , Eff ), set terms form a(p1 , . . . , pn ), referred
actions, Na pi either variable, constant; Pre : LP N
precondition a; Eff : LP N effect a.
far, described deterministic actions only. actions PFC may
probabilistic effects well. Similar work Boutilier et al. (2001), decompose
stochastic action deterministic primitives natures control, referred natures
choices. use relation symbol choice/2 model natures choice. Consider action
pickup (X, ):
choice (pickup (X, ), A)
(A = pickupS (X, ) = pickupF (X, )) ,
423

fiHolldobler, Karabaev & Skvortsova

{on(b,a), on(a,table)}
{on(c,a), on(a,table), on(b,d)}
{on(b,a), on(a,table), holding(c)}
{on(c,a), on(a,table), on(b,c)}
{on(b,a), on(a,table)}
{on(c,a), on(a,table), on(b,d)}
{on(b,a), on(a,table), holding(c)}
{on(c,a), on(a,table), on(b,c)}

(b)

(a)
{on(b,a), on(a,table)}

{on(c,a), on(a,table), on(b,d)}
{on(b,a), on(a,table), holding(c)}
{on(c,a), on(a,table), on(b,c)}

(c)
Figure 1: (a) Interpretation fluent term F = on(X, a) on(a, table); (b) Bold area
interpretation abstract state Z 0 = (on(X, a) on(a, table), {on(Y, X)});
(c) Bold area interpretation abstract state Z = (on(X, a)
on(a, table), {on(Y, X), holding(X 0 )}).

424

fiFluCaP: Heuristic Search Planner First-Order MDPs

pickupS (X, ) pickupF (X, ) define two natures choices action pickup (X, ),
viz., succeeds fails. example, natures choice pickupS defined
follows:
Pre (pickupS (X, )) := (on(X, ) e, {on(W, X)})
Eff (pickupS (X, )) := (holding(X), {on(X, )}) ,
fluent e denotes empty robots gripper. simplicity, denote set
natures choices action Ch (a) := {aj |choice (a, aj )}. Please note nowhere
action descriptions restrict domain discourse pre-specified set
blocks.
natures choices aj associated action define probability
prob (aj , a, Z) denoting probability one natures choices aj chosen
state Z. example,
prob (pickupS (X, ), pickup (X, ), Z) = .75
states probability successful execution pickup action state Z
.75.
next step, define reward function state. example, might
want give reward 500 states block X block 0,
otherwise:
reward (Z) = 500 Z v (on(X, a), )
reward (Z) = 0 Z 6v (on(X, a), ) ,
v denotes subsumption relation, described detail Section 3.2.1.
One observe specified reward function without explicit state enumeration. Instead, state space divided two abstract states depending whether
not, block X block a. Likewise, value functions specified respect
abstract states only. contrast classical DP algorithms, states
explicitly enumerated. Action costs analogously defined follows:
cost(pickup (X, )) = 3
penalizing execution pickup -action value 3.
Inference Mechanism
Herein, show perform inferences, i.e., compute successors given abstract state,
action schemata directly, avoiding unnecessary grounding. note computation
predecessors performed similar way.
Let Z = (P, N ) abstract state, a(p1 , . . . , pn ) action parameters
p1 , . . . , pn , preconditions Pre (a) = (Pp , Np ) effects Eff (a) = (Pe , Ne ). Let
substitutions. action a(p1 , . . . , pn ) forward applicable, simply applicable, Z
, denoted forward (Z, a, , ), following conditions hold:
(f1) (Pp U1 ) =AC1 P
(f2) Np Np .N N .(P N U2 ) =AC1 (P Np ) ,
425

fiHolldobler, Karabaev & Skvortsova

U1 U2 new AC1-variables AC1 equational theory
represented following system associativity, commutativity, unit element
equations:
EAC1 = { (X, Y, Z) X (Y Z) = (X ) Z
(X, ) X = X
(X) X 1 = X
}.
words, conditions (f1) (f2) guarantee Z contains positive
negative preconditions action a. action forward applicable Z
Zsucc = (P 0 , N 0 ),
P 0 := (Pe U1 )
N 0 := N \ Np Ne

(1)

referred a-successor Z denoted succ(Z, a, , ).
example, consider action pickupS (X, ) defined above, take Z = (P, N ) =
(on(b, table) on(X1 , b) e, {on(X2 , X1 )}). action pickupS (X, ) forward applicable
Z = {X 7 X1 , 7 b, U1 7 on(b, table)} = {X2 7 W, U2 7 1}. Thus,
Zsucc = succ(Z, pickupS (X, ), , ) = (P 0 , N 0 )
P 0 = holding(X1 ) on(b, table) N 0 = {on(X1 , b)} .

3. First-Order LAO*
present generalization symbolic LAO algorithm Feng Hansen (2002),
referred first-order LAO (FOLAO ), solving FOMDPs. Symbolic LAO
heuristic search algorithm exploits state abstraction solving factored MDPs. Given
initial state, symbolic LAO uses admissible heuristic focus computation
parts state space reachable initial state. Moreover, specifies MDP
components, value functions, policies, admissible heuristics using propositional ADDs.
allows symbolic LAO manipulate sets states instead individual states.
Despite fact symbolic LAO shows advantageous behaviour comparison
classical non-symbolic LAO Hansen Zilberstein (2001) evaluates states
individually, suffers important drawback. solving FOMDPs, symbolic
LAO propositionalizes problem. approach impractical large FOMDPs.
intention show improve performance symbolic LAO providing
compact first-order representation MDPs heuristic search performed
without propositionalization. precisely, propose switch representational
formalism FOMDPs symbolic LAO propositional ADDs Probabilistic Fluent
Calculus. FOLAO algorithm presented Figure 2.
symbolic LAO , FOLAO two phases alternate complete solution
found, guaranteed optimal. First, expands best partial policy
evaluates states fringe using admissible heuristic function. performs
dynamic programming states visited best partial policy, update values
possibly revise current best partial policy. note focus partial policies
map subcollection states actions.
426

fiFluCaP: Heuristic Search Planner First-Order MDPs

policyExpansion(, 0 , G)
E := F :=
f rom := 0
repeat

{succ(Z, aj , , )},
:=
Zf rom aj Ch(a)

(a, , ) := (Z)
F := F (to G)
E := E f rom
f rom := G E
(f rom = )
E := E F
G := G F
return (E, F, G)
FOVI(E, A, prob, reward, cost, , V )
repeat
V 0 := V
loop Z E
loop
loop , forward (Z, a, , )
Q(Z, a, ,
P) := reward(Z) + cost(a)+

prob(aj , a, Z) V 0 (succ(Z, aj , , ))
aj Ch(a)

end loop
end loop
V (Z) := max Q(Z, a, , )
(a,,)

end loop
V := normalize(V )
r := kV V 0 k
stopping criterion
:= extractP olicy(V )
return (V, , r)
FOLAO (A, prob, reward, cost, , 0 , h, )
V := h
G :=
Z 0 , initialize arbitrary action
repeat
(E, F, G) := policyExpansion(, 0 , G)
(V, , r) := FOVI(E, A, prob, reward, cost, , V )
(F = ) r
return (, V )

Figure 2: First-order LAO algorithm.
policy expansion step, perform reachability analysis find set F states
yet expanded, reachable set 0 initial states
following partial policy . set states G contains states expanded
far. expanding partial policy mean defined larger set
states dynamic programming step. symbolic LAO , reachability analysis ADDs
performed means image operator symbolic model checking, computes
427

fiHolldobler, Karabaev & Skvortsova

set successor states following best current policy. Instead, FOLAO , apply
succ-operator, defined Equation 1. One observe since reachability
analysis FOLAO performed abstract states defined first-order entities,
reasoning successor states kept first-order level. contrast, symbolic
LAO would first instantiate 0 possible combinations objects, order
able perform computations using propositional ADDs later on.
contrast symbolic LAO , dynamic programming step performed using
modified version SPUDD, employ modified first-order value iteration algorithm
(FOVI). original FOVI Holldobler Skvortsova (2004) performs value iteration
entire state space. modify computes states reachable
initial states, precisely, set E states visited best current partial policy. way, improve efficiency original FOVI algorithm
using reachability analysis together symbolic dynamic programming. FOVI produces
PFC representation value functions policies constructing first-order formulae
partition state space abstract states. effect, performs value iteration
top abstract states, obviating need explicit state enumeration.
Given FOMDP value function represented PFC, FOVI returns best partial
value function V , best partial policy residual r. order update values
states Z E, assign values current value function successors
Z. compute successors respect natures choices aj . residual r
computed absolute value largest difference current newly
computed value functions V 0 V , respectively. note newly computed value
function V taken normalized form, i.e., result normalize procedure
described Section 3.2.1. Extraction best partial policy straightforward:
One simply needs extract maximizing actions best partial value function V .
symbolic LAO , FOLAO converges -optimal policy three conditions met: (1) current policy unexpanded states, (2) residual
r less predefined threshold , (3) value function initialized admissible heuristic. original convergence proofs LAO symbolic LAO Hansen
Zilberstein (2001) carry straightforward way FOLAO .
calling FOLAO , initialize value function admissible heuristic
function h focuses search subset reachable states. simple way create
admissible heuristic use dynamic programming compute approximate value
function. Therefore, order obtain admissible heuristic h FOLAO , perform
several iterations original FOVI. start algorithm initial value function
admissible. Since step FOVI preserves admissibility, resulting value
function admissible well. initial value function assigns goal reward
state thereby overestimating optimal value, since goal reward maximal possible
reward.
Since computations FOLAO performed abstract states instead individual
states, FOMDPs solved avoiding explicit state action enumeration propositionalization. first-order reasoning leads better performance FOLAO comparison
symbolic LAO , shown Section 4.
428

fiFluCaP: Heuristic Search Planner First-Order MDPs

= { Z 0 }
Z0

a11

a12

Z1
F,G
Z2

= { Z 1 , Z 2 }
F = {Z1,Z 2 }
E = { Z 0 , Z 1 , Z 2}
G = {Z1,Z 2 }

= { Z 0 }

Z1

Z0

G

FOVIA ({ Z 0 , Z 1 , Z 2})

a)

a21

Z2

a22

F
Z3

= { Z 2 }
Z0

= { Z 2 , Z 3 }
F = {Z3}
E = {Z0}

b)

= { Z 4 , Z 5 }
Z 1 F = { Z , Z ,Z }
4
5
3
,
,
{
Z
Z
Z
=
E
2
3 ,Z 4 ,Z 5 }
0
,
,
{
Z
=
Z
Z
G
G
3 ,Z 4 ,Z 5 }
2
1
a11

Z2
Z3

Z4
a12

F

c)

Z5

FOVIA ( { Z 0 , Z 2 , Z 3 , Z 4 , Z 5 } )

Figure 3: Policy Expansion.

3.1 Policy Expansion
policy expansion step FOLAO similar one symbolic LAO
algorithm. Therefore, illustrate expansion procedure means example. Assume start initial state Z0 two nondeterministic actions a1 a2
applicable Z0 , two outcomes a11 , a12 a21 , a22 , respectively. Without loss
generality, assume current best policy chooses a1 optimal action
state Z0 . construct successors Z1 Z2 Z0 respect outcomes a11
a12 action a1 .
fringe set F well set G states expanded far contain states Z1
Z2 only, whereas, set E states visited best current partial policy gets
state Z0 addition. See Figure 3a. next step, FOVI performed set E.
assume values updated way a2 becomes optimal action
Z0 . Thus, successors Z0 recomputed respect optimal action
a2 . See Figure 3b.
One observe one a2 -successors Z0 , namely Z2 , element
set G thus, contained already fringe F previous expansion
step. Hence, state Z2 expanded value recomputed. shown
Figure 3c, states Z4 Z5 a1 -successors Z2 , assumption a1
optimal action Z2 . result, fringe set F contains newly discovered
states Z3 , Z4 Z5 perform FOVI E = {Z0 , Z2 , Z3 , Z4 , Z5 }. state Z1
contained E, belong best current partial policy,
429

fiHolldobler, Karabaev & Skvortsova

dynamic programming step performed states visited best
current partial policy.
3.2 First-Order Value Iteration
FOLAO , first-order value iteration algorithm (FOVI) serves two purposes: First,
perform several iterations FOVI order create admissible heuristic h FOLAO .
Second, dynamic programming step FOLAO , apply FOVI states visited
best partial policy order update values possibly revise current
best partial policy.
original FOVI Holldobler Skvortsova (2004) takes finite state space
abstract states, finite set stochastic actions, real-valued reward cost functions,
initial value function input. produces first-order representation optimal
value function policy exploiting logical structure FOMDP. Thus, FOVI
seen first-order counterpart classical value iteration algorithm Bellman
(1957).
3.2.1 Normalization
Following ideas Boutilier et al. (2001), FOVI relies normalization state
space represents value function. normalization state space, mean
equivalence-preserving procedure reduces size state space. would
effect state space contains redundant entries, usually case symbolic
computations.
Although normalization considered important issue, done
hand far. best knowledge, preliminary implementation approach Boutilier et al. (2001) performs rudimentary logical simplifications
authors suggest using automated first-order theorem prover normalization task.
Holldobler Skvortsova (2004) developed automated normalization procedure
FOVI that, given state space, delivers equivalent one contains redundancy.
technique employs notion subsumption relation.
formally, let Z1 = (P1 , N1 ) Z2 = (P2 , N2 ) abstract states. Z1 said
subsumed Z2 , written Z1 v Z2 , exist substitutions
following conditions hold:
(s1) (P2 U1 ) =AC1 P1
(s2) N2 N2 .N1 N1 .(P1 N1 U2 ) =AC1 (P1 N2 ) ,
U1 U2 new AC1-variables. motivation notion subsumption
abstract states inherited notion -subsumption first-order clauses
Robinson (1965) difference abstract states contain complicated negative parts contrast first-order clauses.
example, consider two abstract states Z1 Z2 defined follows:
Z1 = (on(X1 , a) on(a, table), {red(Y1 )})
Z2 = (on(X2 , a), {red(X2 )}) ,
430

fiFluCaP: Heuristic Search Planner First-Order MDPs

N
0
1
2
3
4
5
6
7
8
9

Number states
Supdate
Snorm
9
6
24
14
94
23
129
33
328
39
361
48
604
52
627
54
795
56
811
59

Time, msec
Update Norm
144
1
393
3
884
12
1377
16
2079
46
2519
51
3268
107
3534
110
3873
157
4131
154

Runtime, msec

Runtime w/o norm, msec

145
396
896
1393
2125
2570
3375
3644
4030
4285

144
593
2219
13293
77514
805753
n/a
n/a
n/a
n/a

Table 1: Representative timing results first ten iterations FOVI.
Z1 informally asserts block X1 block table
blocks red. Whereas Z2 informally states block X2 block
X2 red. show Z1 v Z2 . relation holds since conditions (s1)
(s2) satisfied. Indeed,
(on(X2 , a) U1 ) =AC1 on(X1 , a) on(a, table)

(on(X1 , a) on(a, table) red(Y1 ) U2 ) = (on(X1 , a) on(a, table) red(X2 ))
= {X2 7 X1 , U1 7 on(a, table)} = {Y1 7 X1 , U2 7 1}.
One note subsumption language abstract states inherits complexity bounds -subsumption (Kapur & Narendran, 1986). Namely, deciding subsumption two abstract states NP-complete, general. However, Karabaev et al.
(2006) recently developed efficient algorithm delivers solutions subsumption problem case abstract states fluent terms.
purpose normalization, convenient represent value function
set pairs form hZ, i, Z abstract state real value. essence,
normalization algorithm seen exhaustive application following simplification rule value function V .
hZ1 , hZ2 ,
Z1 v Z2
hZ2 ,
Table 1 illustrates importance normalization algorithm providing representative timing results first ten iterations FOVI. experiments carried
problem taken colored Blocksworld scenario consisting ten blocks.
Even relatively simple problem FOVI normalization switched
scale beyond sixth iteration.
results Table 1 demonstrate normalization iteration
FOVI dramatically shrinks computational effort next iterations. columns
labelled Supdate Snorm show size state space performing value updates
431

fiHolldobler, Karabaev & Skvortsova

normalization, respectively. example, normalization factor, i.e., ratio
number Supdate states obtained performing one update step
number Snorm states obtained performing normalization step, seventh
iteration 11.6. means ninety percent state space contained
redundant information. fourth fifth columns Table 1 contain time Update
Norm spent performing value updates normalization, respectively.
total runtime Runtime, normalization switched on, given sixth column.
seventh column labelled Runtime w/o norm depicts total runtime FOVI
normalization switched off. would sum values seventh column
values sixth column sixth iteration inclusively, subtract latter
former divide result total time Norm needed performing normalization
first six iterations, would obtain normalization gain three
orders magnitude.

4. Experimental Evaluation
demonstrate advantages combining heuristic search together first-order
state abstraction system, referred FluCaP, successfully entered
probabilistic track 2004 International Planning Competition (IPC2004). experimental results obtained using RedHat Linux running 3.4GHz Pentium IV
machine 3GB RAM.
Table 2, present performance comparison FluCaP together symbolic
LAO examples taken colored Blocksworld (BW) scenario introduced
IPC2004.
main objective investigate whether first-order state abstraction using logic
could improve computational behaviour planning system solving FOMDPs.
colored BW problems main interest since ones represented
first-order terms hence ones allowed us make use first-order
state abstraction. Therefore, concentrated design domain-dependent
planning system tuned problems taken Blocksworld scenario.
colored BW problems differ classical BW ones that, along
unique identifier, block assigned specific color. goal formula, specified firstorder terms, provides arrangement colors instead arrangement blocks.
outset solving colored BW problem, symbolic LAO starts propositionalizing components, namely, goal statement actions. that, abstraction
using propositional ADDs applied. contrast, FluCaP performs first-order abstraction colored BW problem directly, avoiding unnecessary grounding. following,
show abstraction technique affects computation heuristic function.
create admissible heuristic, FluCaP performs twenty iterations FOVI symbolic
LAO performs twenty iterations approximate value iteration algorithm similar
APRICODD St-Aubin, Hoey, Boutilier (2000). columns labelled H.time
NAS show time needed computing heuristic function number abstract
states covers, respectively. comparison FluCaP, symbolic LAO needs evaluate
fewer abstract states heuristic function takes considerably time. One
432

fiFluCaP: Heuristic Search Planner First-Order MDPs

FluCaP

FluCaP

31.1
8.7
25.1
9.5
16.5
12.7
285.4
76.7
128.5 85.0
63.3
135.0
n/a
757.0
2813
718.3
443.6 1241
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a

LAO*

LAO*

FluCaP

FOVI

FluCaP

LAO*

FluCaP

494 22.3 22.0 23.4
496 23.1 17.8 22.7
495 27.3 11.7 15.7
493 137.6 78.5 261.6
492 150.5 33.0 119.1
496 221.3 16.6 56.4
491 1644 198.1 2776
494 1265 161.6 1809
494 2210 27.3 317.7
n/a n/a 1212 n/a
n/a n/a 598.5 n/a
n/a n/a 215.3 1908
n/a n/a 1809 n/a
n/a n/a 3548 n/a

4.2
1.3
0.3
21.0
9.3
1.2
171.3
143.6
12.3
804.1
301.2
153.2
1733
1751

35
34
32
68
82
46
143
112
101
n/a
n/a
n/a
n/a
n/a

410
172
55
1061
539
130
2953
2133
425
8328
3956
2019
7276
15225

1077
687
278
3847
1738
902
12014
7591
2109
n/a
n/a
7251
n/a
n/a

0.86
0.86
0.86
7.05
7.05
7.05
65.9
65.9
65.9
n/a
n/a
n/a
n/a
n/a

0.82
0.68
0.66
4.24
6.50
6.24
23.6
51.2
61.2
66.6
379.7
1121
1.2 107
2.5 107

2.7
2.1
1.9
3.1
2.3
2.0
3.5
2.4
2.0
4.1
3.0
2.3
5.7
6.1

Table 2: Performance comparison FluCaP (denoted FluCaP) symbolic LAO
(denoted LAO*), cells n/a denote fact planner
deliver solution within time limit one hour. NAS NGS number
abstract ground states, respectively.

conclude abstract states symbolic LAO enjoy complex structure
FluCaP.
note that, comparison FOVI, FluCaP restricts value iteration smaller
state space. Intuitively, value function, delivered FOVI, covers larger
state space, time allocated heuristic search FluCaP
used performing additional iterations FOVI. results column labelled %
justify harder problem (that is, colors contains), higher
percentage runtime spent normalization. Almost test problems, effort spent
normalization takes three percent total runtime average.
order compare heuristic accuracy, present column labelled NGS
number ground states heuristic assigns non-zero values to. One see
heuristics returned FluCaP symbolic LAO similar accuracy, FluCaP
takes much less time compute them. reflects advantage plain first-order
abstraction comparison marriage propositionalization abstraction using
propositional ADDs. examples, gain several orders magnitude H.time.
column labelled Total time presents time needed solve problem.
time, planner must execute 30 runs initial state goal state. one-hour block
allocated problem. note that, comparison FluCaP, time required
heuristic search symbolic LAO (i.e., difference Total time H.time) grows
considerably faster size problem. reflects potential employing
433

%

FOVI

15
17

494
495
495
493
493
495
491
494
494
n/a
n/a
n/a
n/a
n/a

NGS, 103

NAS
FluCaP

8

494
495
495
493
492
494
491
494
494
490
490
492
486
481

H.time, sec.
LAO*

7

494
496
496
493
493
495
492
494
494
n/a
n/a
n/a
n/a
n/a

Total time, sec.

FluCaP

6

FOVI

5

C
4
3
2
4
3
2
4
3
2
4
3
2
3
4

FluCaP

B

Total av. reward
LAO*

Problem

fiHolldobler, Karabaev & Skvortsova

B
20
22
24
26
28
30
32
34
36

Total av. reward, 500
489.0
487.4
492.0
482.8
493.0
491.2
476.0
475.6
n/a

Total time, sec.
137.5
293.8
757.3
817.0
2511.3
3580.4
3953.8
3954.1
n/a

H.time, sec.
56.8
110.2
409.8
117.2
823.3
1174.0
781.8
939.4
n/a

NAS
711
976
1676
1141
2832
4290
2811
3248
n/a

NGS 1021
1.7
1.1 103
1.0 106
4.6 108
8.6 1011
1.1 1015
7.4 1017
9.6 1020
n/a

Table 3: Performance FluCaP larger instances one-color Blocksworld problems,
cells n/a denote fact planner deliver solution within
time limit.

first-order abstraction instead abstraction based propositional ADDs heuristic
search.
average reward obtained 30 runs, shown column Total av. reward,
planners evaluation score. reward value close 500 (which maximum possible
reward) simply indicates planner found reasonably good policy. time
number blocks B increases 1, running time symbolic LAO increases roughly
10 times. Thus, could scale problems seven blocks.
contrast FluCaP could solve problems seventeen blocks. note
number colors C problem affects efficiency abstraction technique.
FluCaP, C decreases, abstraction rate increases which, turn, reflected
dramatic decrease runtime. opposite holds symbolic LAO .
addition, compare FluCaP two variants. first one, denoted FOVI,
performs heuristic search all, rather, employs FOVI compute -optimal
total value function policy extracted. second one, denoted FluCaP ,
performs trivial heuristic search starting initial value function admissible
heuristic. expected, FluCaP combines heuristic search FOVI demonstrates
advantage plain FOVI trivial heuristic search. results illustrate
significance heuristic search general (FluCaP vs. FOVI) importance heuristic
accuracy, particular (FluCaP vs. FluCaP ). FOVI FluCaP scale problems
seven blocks.
Table 3 presents performance results FluCaP larger instances one-color
BW problems number blocks varying twenty thirty four. believe
FluCaP scale problems larger size implementation yet
well optimized. general, believe FluCaP system sensitive
size problem propositional planners are.
experiments targeted one-color problems are,
one hand, simplest ones us and, hand, bottleneck propositional
planners. structure one-color problems allows us apply first-order state abstraction full power. example, 34-blocks problem FluCaP operates
3.3 thousand abstract states explode 9.6 1041 individual states proposition434

fiFluCaP: Heuristic Search Planner First-Order MDPs

Total av. reward, 500
UMass

Michigan

Purdue1

Purdue2

Purdue3

Caracas

Toulouse

C
3
3
5
0
0
0
0
0
0

Dresden

B
5
8
11
5
8
11
15
18
21

Canberra

Problem

494.6
486.5
479.7
494.6
489.7
479.1
467.5
351.8
285.7

496.4
492.8
486.3
494.6
489.9
n/a
n/a
n/a
n/a

n/a
n/a
n/a
494.8
n/a
n/a
n/a
n/a
n/a

n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a
n/a

496.5
486.6
481.3
494.1
488.7
480.3
469.4
462.4
455.7

496.5
486.4
481.5
494.6
490.3
479.7
467.7
-54.9
455.1

495.8
487.2
481.9
494.4
490
481.1
486.3
n/a
459

n/a
n/a
n/a
494.9
488.8
465.7
397.2
n/a
n/a

n/a
n/a
n/a
494.1
n/a
n/a
n/a
n/a
n/a

Table 4: Official competition results colored non-colored Blocksworld scenarios.
May, 2004. n/a-entries table indicate either planner
successful solving problem attempt solve it.

alization. propositional planner must highly optimized order cope
non-trivial state space.
note additional colors larger instances (more 20 blocks) BW problems
cause dramatic increase computational time, consider problems
unsolved. One observe number abstract states NAS increases
number blocks non-monotonically problems generated randomly.
example, 30-blocks problem happens harder 34-blocks one. Finally,
note results appear Tables 2 3 obtained using new version
evaluation software rely propositionalization contrast initial
version used competition.
Table 4 presents competition results IPC2004, FluCaP competitive
comparison planners colored BW problems. FluCaP perform
well non-colored BW problems problems propositional ones (that
is, goal statements initial states ground) FluCaP yet incorporate
optimization techniques applied modern propositional planners. contestants
indicated origin. example, Dresden - FluCaP, UMass - symbolic LAO etc.
pickup action cost 1, gain five points total reward means
plan contains ten fewer actions average. competition domains log files
available online appendix Younes, Littman, Weissman, Asmuth (2005).
Although empirical results presented work obtained
domain-dependent version FluCaP, recently developed (Karabaev et al.,
2006) efficient domain-independent inference mechanism core domainindependent version FluCaP.
435

fiHolldobler, Karabaev & Skvortsova

5. Related Work
follow symbolic DP (SDP) approach within Situation Calculus (SC) Boutilier
et al. (2001) using first-order state abstraction FOMDPs. One difference
representation language: use PFC instead SC. course symbolic value iteration, state space may contain redundant abstract states dramatically affect
algorithms efficiency. order achieve computational savings, normalization must performed remove redundancy. However, original work Boutilier et al. (2001)
done hand. best knowledge, preliminary implementation
SDP approach within SC uses human-provided rewrite rules logical simplification.
contrast, Holldobler Skvortsova (2004) developed automated normalization
procedure FOVI incorporated competition version FluCaP brings
computational gain several orders magnitude. Another crucial difference
algorithm uses heuristic search limit number states policy computed.
ReBel algorithm Kersting, van Otterlo, De Raedt (2004) relates FOLAO
uses representation language simpler Situation Calculus.
feature makes state space normalization computationally feasible.
motivation, approach closely connected Relational Envelope-based Planning
(REBP) Gardiol Kaelbling (2003) represents MDP dynamics compact set
relational rules extends envelope method Dean et al. (1995). However, REBP
propositionalizes actions first, afterwards employs abstraction using equivalenceclass sampling. contrast, FOLAO directly applies state action abstraction
first-order structure MDP. respect, REBP closer symbolic LAO
FOLAO . Moreover, contrast PFC, action descriptions REBP allow negation
appear preconditions effects. organization, FOLAO , symbolic LAO ,
similar real-time DP Barto et al. (1995) online search algorithm MDPs.
contrast, FOLAO works offline.
algorithms classified deductive approaches solving FOMDPs.
characterized following features: (1) model-based, (2)
aim exact solutions, (3) logical reasoning methods used compute abstractions.
note FOVI aims exact solution FOMDP, whereas FOLAO , due
heuristic search avoids evaluating states, seeks approximate solution.
Therefore, would appropriate classify FOLAO approximate deductive
approach FOMDPs.
another vein, research developing inductive approaches solving
FOMDPs, e.g., Fern, Yoon, Givan (2003). authors propose approximate
policy iteration (API) algorithm, replace use cost-function approximations
policy representations API direct, compact state-action mappings, use
standard relational learner learn mappings. effect, Fern et al. provide policylanguage biases enable solution large relational MDPs. inductive approaches
characterized following features: (1) model-free, (2) aim
approximate solutions, (3) abstract model used generate biased samples
underlying FOMDP abstract model altered based them.
recent approach Gretton Thiebaux (2004) proposes inductive policy construction algorithm strikes middle-ground deductive inductive tech436

fiFluCaP: Heuristic Search Planner First-Order MDPs

niques. idea use reasoning, particular first-order regression, automatically
generate hypothesis language, used input inductive solver.
approach Gretton Thiebaux related SDP approach sense
first-order domain specification language well logical reasoning employed.

6. Conclusions
proposed approach combines heuristic search first-order state abstraction solving FOMDPs efficiently. approach seen two-fold:
First, use dynamic programming compute approximate value function serves
admissible heuristic. heuristic search performed find exact solution
states reachable initial state. phases, exploit
power first-order state abstraction order avoid evaluating states individually.
experimental results show, approach breaks new ground exploring efficiency
first-order representations solving MDPs. comparison existing MDP planners
must propositionalize domain, e.g., symbolic LAO , solution scales better larger
FOMDPs.
However, plenty remaining done. example, interested
question extent optimization techniques applied modern propositional
planners combined first-order state abstraction. future competitions,
would face problems goal and/or initial states partially defined
underlying domain contains infinitely many objects.
current version FOLAO targeted problems allow efficient
first-order state abstraction. precisely, problems polynomially translated PFC. example colored BW domain, existentially-closed
goal descriptions linearly translated equivalent PFC representation. Whereas
universally-closed goal descriptions would require full propositionalization. Thus, current version PFC less first-order expressive than, e.g., Situation Calculus. future,
would interesting study extensions PFC language, particular, find
trade-off PFCs expressive power tractability solution methods
FOMDPs based PFC.

Acknowledgements
grateful anonymous reviewers thorough reading previous versions paper. thank Zhengzhu Feng fruitful discussions
providing us executable symbolic LAO planner. greatly appreciate
David E. Smith patience encouragement. valuable comments helped
us improve paper. Olga Skvortsova supported grant within Graduate Programme GRK 334 Specification discrete processes systems processes
operational models logics auspices Deutsche Forschungsgemeinschaft
(DFG).
437

fiHolldobler, Karabaev & Skvortsova

References
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72 (1-2), 81138.
Bellman, R. E. (1957). Dynamic programming. Princeton University Press, Princeton, NJ,
USA.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic Dynamic Programming FirstOrder MDPs. Nebel, B. (Ed.), Proceedings Seventeenth International Conference Artificial Intelligence (IJCAI2001), pp. 690700. Morgan Kaufmann.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning time constraints
stochastic domains. Artificial Intelligence, 76, 3574.
Feng, Z., & Hansen, E. (2002). Symbolic heuristic search factored Markov Decision Processes. Dechter, R., Kearns, M., & Sutton, R. (Eds.), Proceedings Eighteenth
National Conference Artificial Intelligence (AAAI2002), pp. 455460, Edmonton,
Canada. AAAI Press.
Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration policy language
bias. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Proceedings Seventeenth Annual Conference Neural Information Processing Systems (NIPS2003), Vancouver,
Canada. MIT Press.
Gardiol, N., & Kaelbling, L. (2003). Envelope-based planning relational MDPs. Thrun,
S., Saul, L., & Scholkopf, B. (Eds.), Proceedings Seventeenth Annual Conference
Neural Information Processing Systems (NIPS2003), Vancouver, Canada. MIT
Press.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression inductive policy
selection. Chickering, M., & Halpern, J. (Eds.), Proceedings Twentieth Conference Uncertainty Artificial Intelligence (UAI2004), Banff, Canada. Morgan
Kaufmann.
Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic Planning using
Decision Diagrams. Laskey, K. B., & Prade, H. (Eds.), Proceedings Fifteenth Conference Uncertainty Artificial Intelligence (UAI1999), pp. 279288,
Stockholm. Morgan Kaufmann.
Holldobler, S., & Schneeberger, J. (1990). new deductive approach planning. New
Generation Computing, 8, 225244.
Holldobler, S., & Skvortsova, O. (2004). Logic-Based Approach Dynamic Programming.
Proceedings Workshop Learning Planning Markov Processes
Advances Challenges Nineteenth National Conference Artificial Intelligence (AAAI04), pp. 3136, San Jose, CA. AAAI Press.
438

fiFluCaP: Heuristic Search Planner First-Order MDPs

Kapur, D., & Narendran, P. (1986). NP-completeness set unification matching
problems. Siekmann, J. H. (Ed.), Proceedings Eighth International Conference Automated Deduction (CADE1986), pp. 489495, Oxford, England. Springer
Verlag.
Karabaev, E., Ramme, G., & Skvortsova, O. (2006). Efficient symbolic reasoning firstorder MDPs. Proceedings Workshop Planning, Learning Monitoring
Uncertainty Dynamic Worlds Seventeenth European Conference
Artificial Intelligence (ECAI2006), Riva del Garda, Italy. appear.
Kersting, K., van Otterlo, M., & De Raedt, L. (2004). Bellman goes relational. Brodley,
C. E. (Ed.), Proceedings Twenty-First International Conference Machine
Learning (ICML2004), pp. 465472, Banff, Canada. ACM.
Puterman, M. L. (1994). Markov Decision Processes - Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY.
Robinson, J. (1965). machine-learning logic based resolution principle. Journal
Association Computing Machinery, 12 (1), 2341.
St-Aubin, R., Hoey, H., & Boutilier, C. (2000). APRICODD: Approximate policy construction using decision diagrams. Leen, T. K., Dietterich, T. G., & Tresp, V. (Eds.),
Proceedings Fourteenth Annual Conference Neural Information Processing
Systems (NIPS2000), pp. 10891095, Denver. MIT Press.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). first probabilistic track
International Planning Competition. Journal Artificial Intelligence Research,
24, 851887.

439


