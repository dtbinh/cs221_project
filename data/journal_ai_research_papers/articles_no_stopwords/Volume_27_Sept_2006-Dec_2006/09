journal artificial intelligence

submitted published

anytime point approximations large pomdps
joelle pineau

jpineau cs mcgill ca

school computer science
mcgill university
montreal qc h canada

geoffrey gordon

ggordon cs cmu edu

machine learning department
carnegie mellon university
pittsburgh pa usa

sebastian thrun

thrun stanford edu

computer science department
stanford university
stanford ca usa

abstract
partially observable markov decision process long recognized rich framework real world control especially robotics however exact solutions framework typically computationally intractable smallest
well known technique speeding pomdp solving involves performing value backups
specific belief points rather entire belief simplex efficiency
however depends greatly selection points presents set novel techniques
selecting informative belief points work well practice point selection procedure
combined point value backups form effective anytime pomdp called
point value iteration pbvi first aim introduce
present theoretical analysis justifying choice belief selection technique second aim
provide thorough empirical comparison pbvi state art
pomdp methods particular perseus effort highlight similarities
differences evaluation performed standard pomdp domains realistic robotic
tasks

introduction
concept long tradition ai literature fikes nilsson chapman
mcallester roseblitt penberthy weld blum furst classical
generally concerned agents operate environments fully observable
deterministic finite static discrete techniques able solve increasingly
large state space basic assumptions classical planningfull observability static
environment deterministic actionsmake unsuitable robotic applications
uncertainty aims improve robustness explicitly reasoning type
uncertainty arise partially observable markov decision process pomdp astrom
sondik monahan white lovejoy b kaelbling littman cassandra boutilier dean hanks emerged possibly general representation
single agent uncertainty pomdp supersedes frameworks terms
c

ai access foundation morgan kaufmann publishers rights reserved

fip ineau g ordon hrun

representational power simply combines essential features
uncertainty
first pomdps handle uncertainty action effects state observability whereas many
frameworks handle neither handle stochastic action effects handle partial state observability plans expressed information states instead world states
since latter ones directly observable space information states space
beliefs system might regarding world state information states easily calculated
measurements noisy imperfect sensors pomdps information states typically
represented probability distributions world states
second many pomdp form plans optimizing value function powerful plan optimization since allows one numerically trade alternative
ways satisfy goal compare actions different costs rewards well plan multiple
interacting goals value function optimization used approachesfor example markov decision processes mdps bellman pomdps unique expressing
value function information states rather world states
finally whereas classical conditional planners produce sequence actions pomdps
produce full policy action selection prescribes choice action possible
information state producing universal plan pomdps alleviate need
allow fast execution naturally main drawback optimizing universal plan computational complexity precisely seek alleviate work described

known exact pomdps operate optimizing value function
possible information states known beliefs run curse dimensionality dimensionality directly related
number states kaelbling et al suffer lesser known curse
history number belief contingent plans increases exponentially
horizon fact exact pomdp known pspace complete whereas propositional
np complete littman many pomdp domains
states actions sensor observations computationally intractable
commonly used technique speeding pomdp solving involves selecting finite set
belief points performing value backups set sondik cheng lovejoy
hauskrecht zhang zhang usefulness belief point updates
well acknowledged backups applied thoroughly
explored
describes class point value iteration pbvi pomdp approximations
value function estimated strictly point updates context
choice points integral part interleaves value backups
steps belief point selection one key contributions presentation
analysis set heuristics selecting informative belief points range naive
version combines point value updates random belief point selection sophisticated combines standard point value update estimate error
bound approximate exact solutions select belief points empirical theoretical evaluation techniques reveals importance taking distance points
consideration selecting belief points exhibits good perfor

fia nytime p oint pproximations l arge pomdp

mance belief points sometimes less number states thereby overcoming
curse history
pbvi class number important properties discussed
greater length
theoretical guarantees present bound error value function obtained
point approximation respect exact solution bound applies number
point approaches including pbvi perseus spaan vlassis
others
scalability able handle order states order magnitude larger solved traditional pomdp techniques
empirical performance evaluated extensively realistic robot tasks including search formissing person scenario
wide applicability makes assumptions nature structure
domain pbvi framework assume known discrete state action observation spaces
known model e state state transitions observation probabilities costs rewards
additional specific structure e g constrained policy class factored model
anytime performance anytime solution achieved gradually alternating phases
belief point selection phases point value updates allows effective
trade time solution quality
pbvi many important properties number recent pomdp approaches exhibit competitive performance braziunas boutilier poupart boutilier
smith simmons spaan vlassis provide overview techniques later part provide comparative evaluation
pbvi standard pomdp domains effort guide practitioners choice
one perseus spaan vlassis closely related pbvi
design performance therefore provide direct comparison two approaches
realistic robot task effort shed light comparative strengths weaknesses two approaches
organized follows section begins exploring basic concepts pomdp
solving including representation inference exact section presents general
anytime pbvi theoretical properties section discusses novel strategies select good belief points section presents empirical comparison pomdp
standard simulation section pursues empirical evaluation tackling complex robot
domains directly comparing pbvi perseus finally section surveys number existing
pomdp approaches closely related pbvi

review pomdps
partially observable markov decision processes provide general decision making
framework acting optimally partially observable domains well suited great
number real world decision making required despite prevalent uncertainty
generally assume complete correct world model stochastic state transitions imperfect state tracking reward structure given information goal action


fip ineau g ordon hrun

strategy maximizes expected reward gains section first establishes basic terminology essential concepts pertaining pomdps reviews optimal techniques pomdp

basic pomdp terminology
formally pomdp defined six distinct quantities denoted z r first three

states state world denoted finite set states denoted
state time denoted st discrete time index state
directly observable pomdps agent compute belief state
space
observations infer belief regarding worlds state agent take sensor measurements set measurements observations denoted z z z
observation time denoted zt observation zt usually incomplete projection
world state st contaminated sensor noise
actions act world agent given finite set actions denoted
actions stochastically affect state world choosing right action
function history core pomdps
throughout assume states actions observations discrete finite
mathematical convenience assume actions observations alternated
time
fully define pomdp specify probabilistic laws describe state transitions
observations laws given following distributions
state transition probability distribution
p r st st



probability transitioning state given agent state selects action since conditional probability distribution
p

notation suggests time invariant
observation probability distribution
z p r zt z st



probability agent perceive observation z upon executing action state
p
conditional probability defined z triplets zz z
probability function time invariant
finally objective pomdp optimize action selection agent given
reward function describing performance


fia nytime p oint pproximations l arge pomdp

reward function r assigns numerical value quantifying
utility performing action state assume reward bounded rmin
r rmax goal agent collect much reward possible time
precisely wants maximize sum
e


x

tt rt





rt reward time e mathematical expectation
discount factor ensures sum equation finite
items together states actions observations z reward r probability
distributions define probabilistic world model underlies pomdp
belief computation
pomdps instances markov processes implies current world state st sufficient predict future independent past st key characteristic
sets pomdps apart many probabilistic mdps fact state
st directly observable instead agent perceive observations z zt
convey incomplete information worlds state
given state directly observable agent instead maintain complete trace
observations actions ever executed use select actions action observation trace known history formally define
ht z zt zt



history time
history trace get long time goes well known fact history
need represented explicitly instead summarized via belief distribution astrom following posterior probability distribution
bt p r st zt zt b



course requires knowing initial state probability distribution
b p r



defines probability domain state time common
specify initial belief part model give runtime system tracks
beliefs selects actions work assume initial belief set possible
initial beliefs available planner
belief distribution bt sufficient statistic history suffices condition
selection actions bt instead ever growing sequence past observations
actions furthermore belief bt time calculated recursively belief one time
step earlier bt along recent action observation zt


fip ineau g ordon hrun

define belief update equation
bt zt bt
x



zt bt



p r zt bt



denominator normalizing constant
equation equivalent decades old bayes filter jazwinski commonly
applied context hidden markov rabiner known forward
continuous generalization forms basis kalman filters kalman
interesting consider nature belief distributions even finite state spaces
belief continuous quantity defined simplex describing space distributions
state space large state spaces calculating belief update eqn computationally challenging recent led efficient techniques belief state computation
exploit structure domain dean kanazawa boyen koller poupart
boutilier thrun fox burgard dellaert however far complex aspect pomdp generation policy action selection described next
example robotics calculating beliefs state spaces states easily done realtime burgard et al contrast calculating optimal action selection policies exactly appears
infeasible environments dozen states kaelbling et al
directly size state space complexity optimal policies
hence assume throughout belief computed accurately instead
focus finding good approximations optimal policy
optimal policy computation
central objective pomdp perspective compute policy selecting actions
policy form
b



b belief distribution action chosen policy
particular interest notion optimal policy policy maximizes expected future discounted cumulative reward


bt argmax e



x






tt rt fibt




two distinct interdependent reasons computing optimal policy challenging widely known reason called curse dimensionality
n physical states defined belief states n dimensional continuous space
less well known reason curse history pomdp solving many ways search
space possible pomdp histories starts searching short histories
select best short policies gradually considers increasingly long histories unfortunately number distinct possible action observation histories grows exponentially
horizon


fia nytime p oint pproximations l arge pomdp

two cursesdimensionality historyoften act independently complexity
grow exponentially horizon even states
large number physical states may still small number relevant histories curse
predominant depends hand solution technique example
belief point methods focus specifically target curse history leaving
vulnerable curse dimensionality exact hand typically
suffer far curse history goal therefore techniques offer best
balance
describe straightforward finding optimal policies sondik
overall idea apply multiple iterations dynamic programming compute increasingly
accurate values belief state b let v value function maps belief states values
beginning initial value function
v b max


x

r b



ss

th value function constructed th following recursive equation


vt b max



x

x

r b

ss

p r z b vt b z



zz

b z belief updating function defined equation value function update
maximizes expected sum possibly discounted future pay offs agent receives
next time steps belief state b thus produces policy optimal
horizon optimal policy directly extracted previous step value function




b

argmax


x

r b

x

p r z b vt b z



zz

ss

sondik showed value function finite horizon expressed set
vectors vector represents dimensional hyper plane
defines value function bounded region belief
x

vt b max


b



ss

addition vector associated action defining best immediate policy
assuming optimal behavior following steps defined respectively sets
vt v
horizon solution set computed follows first rewrite equation


vt b max
aa


x

ss

r b

x
zz

max



xx
ss

z b



notice representation vt b nonlinearity term p z b equation
cancels nonlinearity term b z leaving linear function b inside max
operator


fip ineau g ordon hrun

value vt b cannot computed directly belief b b since infinitely
many beliefs corresponding set generated sequence operations
set
z
first operation generate intermediate sets
z z step
r


z




ia z



x








z



ia z dimensional hyper plane
next create cross sum observations includes one z
z
step
z


z






finally take union sets step
aa



forms pieces backup solution horizon actual value function vt
extracted set described equation
bounded time pomdp finite state action observation
spaces solved exactly given choice horizon environment
agent might able bound horizon advance policy b approximation optimal one whose quality improves expectation horizon assuming

mentioned value function vt extracted directly set important aspect optimal finite horizon pomdp solutions
value function guaranteed piecewise linear convex continuous function belief sondik piecewise linearity continuous properties direct fact
vt composed finitely many linear vectors convexity property


maximization operator eqn worth pointing intermediate sets z

represent functions belief composed entirely linear segments property
holds intermediate representations incorporate expectation observation
probabilities eqn
worst case exact value update procedure described could require time doubly exponential horizon kaelbling et al better understand complexity
exact update let number states number actions z number
observations number vectors previous solution set step creates
z projections step generates z cross sums worst case
solution requires
z



symbol denotes cross sum operator cross sum operation defined two sets
b b b bn produces third set c b b bn
b b bn



fia nytime p oint pproximations l arge pomdp

vectors represent value function horizon computed time
z
often case vector completely dominated another vector
entire belief simplex
b j b b

similarly vector may fully dominated set vectors e g fig dominated combination vector pruned away without affecting
solution finding dominated vectors expensive checking whether single vector
dominated requires solving linear program variables constraints nonetheless
time effective apply pruning iteration prevent explosion solution
size practice often appears grow singly exponentially given clever mechanisms
pruning unnecessary linear functions enormous computational complexity long
key impediment toward applying pomdps practical

v

figure pomdp value function representation

point value backup
exact pomdp solving outlined optimizes value function beliefs many
approximate pomdp solutions including pbvi proposed gain computational advantage applying value updates specific belief points rather
beliefs cheng zhang zhang poon approaches differ significantly
great consequence select belief points set points selected
procedure updating value standard describe procedure updating
value function set known belief points
section value function update implemented sequence operations
set vectors assume interested updating value function fixed
set belief points b b b bq follows value function contain
one vector belief point point value function therefore represented
corresponding set q
given solution set simply modify exact backup operator eqn
one vector per belief point maintained point backup gives vector
valid region around b assumes belief points region
action choice lead facets vt point b key idea behind
presented reason large computational savings associated
class


fip ineau g ordon hrun

obtain solution set previous set begin generating intera z
mediate sets
z z exactly eqn step

r

z




ia z






x





z



next whereas performing exact value update requires cross sum operation eqn
operating finite set points instead use simple summation construct
step
ba


x

argmax

z
zz

x

b b b



ss

finally best action belief point step
b argmax

x


aa ss

b b b

bb b




operations preserve best vector belief point b b estimate
value function belief simplex including b
b extracted set

x

vt b max


b



ss

better understand complexity updating value set points b let
number states number actions z number observations number
vectors previous solution set exact update step creates z
projections time z steps reduce set b components
time z b thus full point value update takes polynomial time
even crucially size solution set remains constant every iteration
point value backup summarized table
note outlined table includes trivial pruning step lines
whereby refrain adding vector already included often
case b situation arises whenever multiple nearby belief points support
vector pruning step computed rapidly without solving linear programs clearly
advantageous terms reducing set
point value backup found many pomdp solvers general serves improve estimates value function integral part pbvi framework

anytime point value iteration
describe algorithmic framework class fast approximate pomdp called point value iteration pbvi pbvi class offer anytime solution
large scale discrete pomdp domains key achieving anytime solution interleave


fia nytime p oint pproximations l arge pomdp

backup b
action
observation z z
solution vector
p
ia z z
end
z
z

end
end

belief point hb b

p
p
p
b argmaxaa

b
ss r b
zz maxa z
ss

b

b
end
return


















table point value backup

two main components point update described table steps belief set selection approximate value function guaranteed bounded error compared
optimal discrete pomdp domain
current section focuses overall anytime theoretical properties independent belief point selection process section discusses detail novel
techniques belief point selection
overall pbvi framework simple start small initial set belief points
applied first series backup operations set belief points grown
series backup operations applied belief points old
satisfactory solution obtained interleaving value backup iterations expansions
belief set pbvi offers range solutions gradually trading computation time solution
quality
full presented table accepts input initial belief point
set binit initial value number desired expansions n horizon
common choice binit initial belief b alternately larger set could used
especially cases sample trajectories available initial value typically set
min
purposefully low e g r
pointbased solution lower bound exact solution lovejoy follows
simple observation failing compute vector lower value function
finite horizon run value backups expansion
belief set infinite horizon select horizon
rmax rmin
rmax maxs r rmin mins r




fip ineau g ordon hrun

complete terminates fixed number expansions n completed alternately could terminate value function approximation reaches
given performance criterion discussed
uses backup routine described table assume moment
expand subroutine line selects belief points random performs reasonably
well small easy achieve good coverage entire belief simplex
however scales poorly larger domains exponentially many points needed guarantee
good coverage belief simplex sophisticated approaches selecting belief points
presented section overall pbvi framework described offers simple yet flexible
solving large scale pomdps
pbvi main binit n
b binit

n expansions
iterations
backup b
end
bnew expand b
b b bnew
end
return













table point value iteration pbvi
belief set b horizon table produce estimate value
function denoted vtb error vtb optimal value function v
bounded bound depends densely b samples belief simplex denser
sampling vtb converges vt horizon optimal solution turn bounded error
respect v optimal solution cutting pbvi iterations sufficiently large
horizon difference vtb optimal infinite horizon v
large overall error pbvi bounded according triangle inequality
kvtb v k kvtb vt k kvt v k



second term bounded kv v k bertsekas tsitsiklis remainder
section states proves bound first term denote
begin assuming h denotes exact value backup h denotes pbvi backup
define b error introduced specific belief b performing one iteration
point backup
b hv b b hv b b
next define maximum total error introduced one iteration point backup
hv b hv b
max b
b



fia nytime p oint pproximations l arge pomdp

finally define density b set belief points b maximum distance belief
simplex belief set b precisely
b max
min kb b k

b bb



prove following lemma
lemma error introduced pbvi performing one iteration value backup b
instead bounded
rmax rmin b


proof let b point pbvi makes worst error value update b b
closest norm sampled belief b let vector maximal b
vector would maximal b failing include solution set pbvi makes error
b b hand since maximal b b b
b b






b b b b
b b b b
b b
k k kb bk
k k b



rmax rmin b


add zero
assume optimal b
arrange terms
holder inequality
definition b

last inequality holds vector represents reward achievable starting
state following sequence actions observations therefore sum rewards
min
max
must fall r
r

lemma states bound approximation error introduced one iteration point
value updates within pbvi framework look bound multiple value updates
theorem belief set b horizon error pbvi kvtb
vt k bounded
rmax rmin b


proof
vtb vt
b hv
hvt








definition h

b hv b hv b hv
hvt



rmax rmin b
b

hvt hvt

rmax rmin b
b v
vt



triangle inequality

rmax rmin b

rmax rmin b


definition



lemma
contraction exact value backup

sum geometric series


fip ineau g ordon hrun

bound described section depends densely b samples belief simplex
case beliefs reachable pbvi need sample densely
fig error bounds convergence
replace set reachable beliefs



hold simply need define b lemma
side note worth pointing pbvi makes assumption regarding
initial value function v b point solution v b guaranteed improve
addition belief points nonetheless theorem presented section shows bound
error vtb point solution v optimal solution guaranteed
decrease stay addition belief points cases vtb initialized
min
pessimistically e g v b r
suggested vtb improve stay
value backup addition belief points
section thus far skirted issue belief point selection however bound presented
section clearly argues favor dense sampling belief simplex randomly
selecting points according uniform distribution may eventually accomplish generally
inefficient particular high dimensional cases furthermore take advantage
fact error bound holds dense sampling reachable beliefs thus seek
efficient ways generate belief points random entire simplex issue
explored next section

belief point selection
section outlined prototypical pbvi conveniently avoiding question
belief points selected clear trade including fewer
beliefs would favor fast good performance versus including many beliefs
would slow ensure better bound performance brings
question many belief points included however number points
consideration likely collections belief points e g frequently encountered
likely produce good value function others brings question
beliefs included
number approaches proposed literature example exact value
function approaches use linear programs identify points value function needs
improved cheng littman zhang zhang however typically
expensive value function approximated learning value regular points
fixed resolution lovejoy variable resolution zhou hansen grid
less expensive solving lps scales poorly number states increases alternately one use heuristics generate grid points hauskrecht poon tends
scalable though significant experimentation required establish heuristics
useful
section presents five heuristic strategies selecting belief points fast naive
random sampling increasingly sophisticated stochastic simulation techniques
effective strategy propose one carefully selects points likely largest
impact reducing error bound theorem
strategies consider focus selecting reachable beliefs rather getting
uniform coverage entire belief simplex therefore useful begin discussion
looking reachability assessed


fia nytime p oint pproximations l arge pomdp

exact pomdp value iteration solutions optimal initial belief pbvi
related techniques assume known initial belief b shown figure use
initial belief build tree reachable beliefs representation path tree
corresponds sequence belief space increasing depth corresponds increasing plan
horizon selecting set belief points pbvi including reachable beliefs would guarantee optimal performance conditioned initial belief expense computational
grow exponentially horitractability since set reachable beliefs
sufficiently small computational
zon therefore best select subset b
tractability sufficiently large good value function approximation

b


ba z ba z

ba z

q

z



ba z

q



ba z










ba z ba z


ba z ba z
p

p

ba z

p q















ba z

ap zq

ba z

z

ba z

ap zq









figure set reachable beliefs
domains initial belief known unique still possible use reachability analysis sampling initial beliefs set known initial beliefs seed
multiple reachability trees
discuss five strategies selecting belief points used within
pbvi framework perform expansion belief set
random belief selection ra
first strategy simplest consists sampling belief points uniform distribution entire belief simplex sample simplex cannot simply sample
p
b independently would violate constraint b instead use
described table see devroye details including proof uniform
coverage
random point selection strategy unlike strategies presented focus
reachable beliefs reason necessarily advocate however
include obvious choice far simplest implement used
related work hauskrecht poon smaller domains e g states
strategies discussed assume belief point set b approximately doubles size belief
expansion ensures number rounds value iteration logarithmic final number belief
points needed alternately strategy could used little modification add fixed number
belief points may require many rounds value iteration since value iteration much expensive
belief computation seems appropriate double size b expansion



fip ineau g ordon hrun

bnew expandra b
bnew b
foreach b b
number states

btmp randuniform
end
sort btmp ascending order

bnew btmp btmp
end
bnew bnew bnew
end
return bnew
















table belief expansion random action selection

performs reasonably well since belief simplex relatively low dimensional large domains
e g states cannot provide good coverage belief simplex reasonable number
points therefore exhibits poor performance demonstrated experimental
presented section
remaining belief selection strategies make use belief tree figure focus
reachable beliefs rather trying cover entire belief simplex
stochastic simulation random action ssra
generate points along belief tree use technique called stochastic simulation involves
running single step forward trajectories belief points already b simulating single step
forward trajectory given b b requires selecting action observation pair z
computing belief b z bayesian update rule eqn case
stochastic simulation random action ssra action selected forward simulation
picked uniformly random full action set table summarizes belief expansion
procedure ssra first state drawn belief distribution b second action
drawn random full action set next posterior state drawn transition
model finally observation z drawn observation model z
triple b z calculate belief bnew b z according equation
add set belief points bnew
strategy better picking points random described restricts
bnew belief tree fig however belief tree still large especially
branching factor high due large numbers actions observations selective
paths belief tree explored one hope effectively restrict belief set



fia nytime p oint pproximations l arge pomdp

bnew expandssra b
bnew b
foreach b b
randmultinomial b
randuniform
randmultinomial
z randmultinomial
bnew b z see eqn
bnew bnew bnew
end
return bnew













table belief expansion random action selection

similar technique stochastic simulation discussed poon however belief set initialized differently b therefore stochastic simulations
restricted set reachable beliefs
stochastic simulation greedy action ssga
procedure generating points stochastic simulation greedy action ssga
well known greedy exploration strategy used reinforcement learning sutton
barto strategy similar ssra procedure except rather choosing
action randomly ssea choose greedy action e current best action given belief
b probability chose random action probability use
action selected perform single step forward simulation ssra yield belief
point table summarizes belief expansion procedure ssga
similar technique featuring stochastic simulation greedy actions outlined
hauskrecht however case belief set included extreme points belief
simplex stochastic simulation done extreme points rather initial
belief
stochastic simulation exploratory action ssea
error bound section suggests pbvi performs best belief set uniformly dense
set reachable beliefs belief point strategies proposed thus far ignore information
next propose gradually expands b greedily choosing reachable beliefs
improve worst case density
unlike ssra ssga select single action simulate forward trajectory
given b b stochastic sampling exploratory action ssea one step forward
simulation action thus producing beliefs ba ba however accept
beliefs ba ba rather calculates l distance ba closest
neighbor b keep point ba farthest away point already b


fip ineau g ordon hrun

bnew expandssga b
bnew b
foreach b b
randmultinomial b
randuniform
randuniform
else
p
argmax ss b
end
randmultinomial
z randmultinomial
bnew b z see eqn
bnew bnew bnew
end
return bnew

















table belief expansion greedy action selection

use l norm calculate distance belief points consistent error bound
theorem table summarizes ssea expansion procedure
bnew expandssea b
bnew b
foreach b b
foreach
randmultinomial b
randmultinomial
z randmultinomial
ba b z see eqn
end
p
bnew maxaa minb bnew ss ba b
bnew bnew bnew see eqn
end
return bnew















table belief expansion exploratory action selection

greedy error reduction ger
ssea strategy able improve worst case density reachable beliefs
directly minimize expected error would directly minimize


fia nytime p oint pproximations l arge pomdp

error measure bound error lemma therefore propose final strategy
greedily adds candidate beliefs effectively reduce error bound
empirical presented strategy successful one discovered
thus far
understand expand belief set ger strategy useful consider
belief tree reproduce figure node tree corresponds specific belief
divide nodes three sets set includes belief points already b
case b ba z set contains belief points immediate descendants points
b e nodes grey zone candidates select
points added b call set envelope denoted b set contains reachable
beliefs

b


ba z ba z

q

q












z





ba z




ba z



ba z ba z
p

p

ba z

p q





ba z ba z








ba z

ba z

ap zq





figure set reachable beliefs
need decide belief b removed envelope b added set
active belief points b every point added b improve estimate value
function point reduce error bounds defined section points
already b however error bound point might quite large means
largest error bound points b monotonically decrease however particular
point b initial belief b error bound decreasing
point reduce error bound look analysis
lemma lemma bounds amount additional error single point backup introduces write b belief considering adding write b belief
already b write value hyper plane b write b lemma
points
b b b
evaluating error need minimize b b since know
done backups b make conservative assumption choose
worst case value rmin rmax thus evaluate


b min
bb

x
ss

max
b b b b
r
rmin
b b b b





fip ineau g ordon hrun

one could simply pick candidate b b currently largest error bound
would ignore reachability considerations rather evaluate error b b
weighing error fringe nodes reachability probability
b

x

b max
aa

b z b z


x

max
aa



zz


xx


zz

z b b z

ss

noting b z b b z evaluated according equation
equation existing point b b largest error bound
directly reduce error adding set one descendants select next step belief
b z maximizes error bound reduction
b



b b z

b argmax


x

b z b z



bb aa zz

z argmax b z b z



zz

table summarizes ger belief point selection
bnew expandger b
bnew b
n b
n
p
b argmaxbb aa zz b z b z
z argmaxzz b z b z
bnew b z
bnew bnew bnew
end
return bnew












table belief expansion

complexity adding one points ger sazb states
actions z observations b beliefs already selected comparison value backup
one point azb point typically needs updated several times point
empirical belief selection even ger takes minimal time compared
value backup
concludes presentation belief selection techniques pbvi framework
summary three factors consider picking belief point likely
tried however perform well empirically suggest equation
consider probability reaching belief



fia nytime p oint pproximations l arge pomdp

occur far belief points already selected current approximate
value point simplest heuristic ra accounts none whereas
others ssra ssga ssea account one ger incorporates three factors
belief expansion example
consider simple example shown figure illustrate difference
belief expansion techniques outlined pomdp littman four states one
goal indicated star two actions left right expected
deterministic effect goal state fully observable observation goal three
states aliased observation none reward received goal state
otherwise reward zero assume discount factor initial distribution
uniform non goal states system resets distribution whenever goal reached

figure pomdp
belief set b initialized contain initial belief b figure shows part
belief tree including original belief set top node envelope leaf nodes
consider belief expansion method might
b
left

right



pr z none

b



pr z goal

pr z none

b

b

pr z goal

b

figure pomdp belief tree
random heuristic pick belief point equal probability entire belief
simplex directly expand branches belief tree eventually put samples
nearby
stochastic simulation random action chance picking action
regardless action picked theres chance seeing observation none
chance seeing observation goal ssra select p r bnew b
p r bnew b p r bnew b p r bnew b


fip ineau g ordon hrun

stochastic simulation greedy action first needs know policy b
iterations point updates section applied initial single point belief set reveal
b lef expansion belief greedily select action lef proba
bility
assuming action right selected belief

expansion probability
combining along observation probabilities
tell ssga expand follows p r bnew b p r bnew b
p r bnew b p r bnew b
predicting choice stochastic simulation exploratory action slightly complicated four cases occur depending outcomes random forward simulation b
action left goes b p r action right goes b p r b
selected b b whereas b b case occur
p r
action left goes b p r action right goes b p r b
selected b b case occur p r
action left goes b p r action right goes b p r b
selected b b case occur p r
action left goes b p r action right goes b p r
selected since equidistant b case b b p r
selected
told p r bnew b p r bnew b p r bnew b p r bnew b

looking belief expansion greedy error reduction need compute
error b z z consider equation since b one point b necessarily b b estimate apply multiple steps value backup b obtain
b estimate error candidate belief b b b b note b
one point dominating factor distance b next factor observation
probabilities eqns allows us determine lef z none
therefore select bnew b
summary note ssga ssea ger favor selecting b whereas ssra picks
option equal probability considering b b actually general
size reasonable expand entire belief tree techniques
discussed quickly except ra pick exact nodes belief
tree select equally good nearby beliefs example provided simply illustrate
different choices made strategy

review point approaches pomdp solving
previous section describes class point pomdp solving idea
point updates pomdps explored previously literature
may obvious reader follows directly repeated application equations



fia nytime p oint pproximations l arge pomdp

section summarize main approaches discussed procedure
updating value function given point remains unchanged outlined section
rather approaches mainly differentiated belief points selected
updates ordered
exact point
earlier exact pomdp techniques use point backups optimize value function limited regions belief simplex sondik cheng techniques
typically require solving multiple linear programs candidate belief points value
function sub optimal expensive operation furthermore guarantee exact solution found relevant beliefs must generated systematically meaning reachable
beliefs must considered methods typically cannot scale beyond handful
states actions observations
work zhang zhang point updates interleaved standard dynamic programming updates accelerate case points generated
systematically rather backups applied set witness points lp points
witness points identified standard dynamic programming updates whereas
lp points identified solving linear programs identify beliefs value
yet improved procedures significantly expensive belief selection heuristics presented limited domains dozen
states actions observations nonetheless guaranteed converge optimal solution
grid approximations
exists many approaches approximate value function finite set belief points
along values points often distributed according grid pattern belief
space thus name grid approximation interpolation extrapolation rule specifies
value non grid points function value neighboring grid points approaches
ignore convexity pomdp value function
performing value backups grid points relatively straightforward dynamic programming
updates specified equation adapted grid points simple polynomial time
given set grid points g value bg g defined

g

v b max



x

x

g

b r

ss

p r z b v b z



zz

b z part grid v b z defined value backups otherwise
v b z approximated interpolation rule
v b z

g
x

v bg






p g

produces convex combination grid points
two interesting questions respect grid approximations calculate
interpolation function select grid points


fip ineau g ordon hrun

general interpolation leads best value function approximation point
b requires solving following linear program
minimize

g
x

v bg






subject

b

g
x

bg





g
x







g



different approaches proposed select grid points lovejoy constructs
fixed resolution regular grid entire belief space benefit value interpolations
calculated quickly considering neighboring grid points disadvantage number
grid points grows exponentially dimensionality belief e number
states simpler would select random points belief space hauskrecht
requires slower interpolation estimating value points
methods less ideal beliefs encountered uniformly distributed
particular many characterized dense beliefs edges simplex e
probability mass focused states states zero probability low
belief density middle simplex distribution grid points better reflects
actual distribution belief points therefore preferable
alternately hauskrecht proposes corner points belief simplex e g
generating additional successor belief points
one step stochastic simulations eqn corner points proposes approximate
interpolation uses values critical points plus one non critical point
grid alternative brafman builds grid starting
critical points belief simplex uses heuristic estimate usefulness gradually
adding intermediate points e g bk bi bj pair points hauskrechts
brafmans methodsgenerally referred non regular grid approximationsrequire fewer
points lovejoys regular grid however interpolation rule used calculate
value non grid points typically expensive compute since involves searching
grid points rather neighboring sub simplex
zhou hansen propose grid approximation combines advantages
regular non regular grids idea sub sample regular fixed resolution grid
proposed lovejoy gives variable resolution grid since parts beliefs
densely sampled others restricting grid points lie fixed resolution grid
guarantee fast value interpolation non grid points nonetheless
often requires large number grid points achieve good performance
finally bonet proposes first grid pomdps optimality
requires thorough coverage belief space every point
within grid point value update grid point fast implement since
interpolation rule depends nearest neighbor one step successor belief
grid point pre computed main limitation fact coverage belief


fia nytime p oint pproximations l arge pomdp

space attained exponentially many grid points furthermore method
requires good coverage entire belief space opposed section
focus coverage reachable beliefs
approximate point
similar pbvi class approaches update value
gradient grid point lovejoy hauskrecht poon methods able
preserve piecewise linearity convexity value function define value function
entire belief simplex methods use random beliefs require inclusion large number fixed beliefs corners probability simplex contrast
pbvi class propose exception pbvi ra select reachable beliefs
particular belief points improve error bounds quickly possible idea
reachability analysis known stochastic simulation generate points explored earlier approaches hauskrecht poon however analysis
indicated stochastic simulation superior random point placements visit
question conclude otherwise empirical evaluation presented
recently technique closely related pbvi called perseus proposed vlassis
spaan spaan vlassis perseus uses point backups similar ones
used pbvi two approaches differ two ways first perseus uses randomly generated
trajectories belief space select set belief points contrast beliefpoint selection heuristics outlined pbvi second whereas pbvi systematically updates
value belief points every epoch value iteration perseus selects subset points
update every epoch method used select points following points randomly
sampled one time value updated continues value points
improved insight resides observing updating vector one point often
improves value estimate nearby points removed sampling set
conceptually simple empirically effective
hsvi smith simmons another point differs
pbvi picks belief points orders value updates maintains lower
upper bound value function approximation uses select belief points
updating upper bound requires solving linear programs generally expensive
step ordering value update follows whenever belief point expanded
belief tree hsvi updates value direct ancestors parents grand parents etc
way back initial belief head node contrast pbvi performs batch
belief point expansions followed batch value updates points respects
hsvi pbvi share many similarities offer anytime performance theoretical guarantees
scalability finally hsvi takes reachability account evaluate empirical
differences hsvi pbvi next section
finally rtbss paquet offers online version point
idea construct belief reachability tree similar figure current belief
top node terminating tree fixed depth value node
computed recursively finite horizon eliminate subtrees
calculating bound value comparing value computed subtrees
rtbss fact combined offline pbvi offline


fip ineau g ordon hrun

used pre compute lower bound exact value function used increase subtree
pruning thereby increasing depth online tree construction thus quality
solution online yield fast large pomdp domains however
overall solution quality achieve error guarantees offline approaches

experimental evaluation
section looks variety simulated pomdp domains evaluate empirical performance
pbvi first three domainstiger grid hallway hallway extracted established pomdp literature cassandra fourthtagwas introduced
earlier work challenge pomdp
first goal experiments establish scalability pbvi framework
accomplished showing pbvi type successfully solve excess
states demonstrate pbvi compare favorably alternative approximate
value iteration methods finally following example section study larger scale
impact belief selection strategy confirms superior performance ger
strategy
maze
exists set benchmark commonly used evaluate pomdp cassandra section presents demonstrating performance pbviclass benchmark relatively small
states actions observations compared robotics domains
useful analysis point view comparison previous work
initial performance analysis focuses three well known pomdp literature tiger grid known maze hallway hallway three maze navigation
sizes fully described littman cassandra kaelbling
parameterization available cassandra
figure presents tiger grid domain replicating earlier experiments brafman test runs terminate steps theres automatic reset every time goal
reached averaged runs
figures b c present hallway hallway domains respectively
case test runs terminated goal reached steps whichever occurs first
averaged runs consistent earlier experiments littman
cassandra kaelbling b
three figures compare performance three different
pbvi greedy error reduction ger belief point selection section
qmdp littman et al b
incremental pruning cassandra littman zhang
qmdp heuristic littman et al b takes account partial observability current step assumes full observability subsequent steps
qm dp b argmax
aa



x
ss

b qm dp



fia nytime p oint pproximations l arge pomdp

resulting policy ability resolve uncertainty cannot benefit long term
information gathering compare actions different information potential qmdp seen
providing good performance baseline three considered finds policy
extremely quickly policy clearly sub optimal
end spectrum incremental pruning zhang liu cassandra et al direct extension enumeration described principal insight pruning dominated vectors eqn interleaved directly
cross sum operator eqn resulting value function
efficient discards unnecessary vectors earlier incremental pruning
theoretically optimal policy three considered would take far
long fact iterations exact backups completed reasonable time three
resulting short horizon policy worse corresponding pbvi policy
shown figure pbvi ger provides much better time performance trade finds
policies better obtained qmdp matter seconds thereby
demonstrating suffer paralyzing complexity incremental pruning
take closer look may surprised see performance
pbvi actually decreases points e g dip fig c unexpected
important remember theoretical properties pbvi guarantee bound
estimate value function shown necessarily imply policy
needs improve monotonically nonetheless value function converges policy
albeit slower rate
tag
previous section establishes good performance pbvi well known simulation quite small fully demonstrate scalability
provide better understanding pbvis effectiveness large section presents
obtained applying pbvi tag robot version popular game
lasertag agent must navigate environment goal searching
tagging moving target rosencrantz gordon thrun real world versions
take many forms section present similar domain
interactive service robot must elderly patient roaming corridors nursing home
synthetic scenario considered order magnitude larger states
pomdp benchmarks literature cassandra formulated pomdp goal robot optimize policy allowing quickly person assuming
person moves stochastically according fixed policy spatial configuration
environment used throughout experiment illustrated figure
state space described cross product two position features robot
person sf ound start independently selected random
positions scenario finishes person sf ound robot select five actions
north south east west tag reward imposed motion action tag action
reward robot person cell otherwise throughout scenario robots position fully observable move action predictable
deterministic effect e g
p r robot robot n orth


fip ineau g ordon hrun




pbvi ger
qmdp
incprune

pbvi ger
qmdp
incprune







reward

reward


























time secs




















time secs

tiger grid









b hallway




pbvi ger
qmdp
incprune



reward



















time secs







c hallway

figure pbvi performance well known pomdp figure shows sum
discounted reward function computation time different domain



























































figure spatial configuration domain



fia nytime p oint pproximations l arge pomdp

adjacent cell direction position person hand
completely unobservable unless agents cell meanwhile step
person omniscient knowledge moves away robot p r stays place
p r e g
p r p erson p erson robot
p r p erson p erson robot
p r p erson p erson robot
figure shows performance pbvi greedy error reduction tag domain averaged runs different randomly chosen start positions run
qmdp approximation tested provide baseline comparison gradual improvement pbvis performance samples added shown data point represents
expansion belief set value backups confirms computation time directly related number belief points pbvi requires fewer belief points overcome
qmdp performance keeps improving points added performance appears
converging approximately belief points pbvi class
effectively tackle states

pbvi ger
qmdp


reward



















time secs









figure pbvi performance tag sum discounted reward function
computation time

far beyond reach incremental pruning single iteration
optimal value iteration size could produce vectors pruning
therefore applied
section describes one version tag used simulation purposes
work others braziunas boutilier poupart boutilier smith
simmons vlassis spaan fact formulated variety
ways accommodate different environments person motion observation
section discusses variations realistic robot person
presents validated onboard independently developed robot simulator


fip ineau g ordon hrun

empirical comparison pbvi class
establish good performance pbvi ger number consider
empirical different pbvi class allows us compare effects
belief expansion heuristics repeat experiments tiger grid hallway
hallway tag domains outlined case compare performance five
different pbvi class
pbvi ra pbvi belief points selected randomly belief simplex section
pbvi ssra pbvi belief points selected stochastic simulation random action section
pbvi ssga pbvi belief points selected stochastic simulation greedy action
section
pbvi ssea pbvi belief points selected stochastic simulation exploratory
action section
pbvi ger pbvi belief points selected greedy error reduction section
pbvi class converge optimal value function given sufficiently large
set belief points rate converge depends ability generally pick
useful points leave points containing less information since computation time
directly proportional number belief points best performance
generally one good solution fewest belief points
figure shows comparison performance five pbvi class
enumerated four domains pictures present performance
function computation time
seen smallest domaintiger gridpbvi ger similar performance random pbvi ra hallway domain pbvi ger reaches nearoptimal performance earlier hallway unclear five
best though ger seems converge earlier
larger tag domain situation interesting pbvi ger combination
clearly superior others reason believe pbvi ssea could match performance would require order twice many points nonetheless pbvi ssea
performs better pbvi ssra pbvi ssga random heuristic pbvi ra
reward improve regardless many belief points added therefore include presented figure suggest choice
belief points crucial dealing large general believe ger
ssea lesser degree superior heuristics solving domains large numbers
action observation pairs ability selectively chooses branches
reachability tree explore
side note surprised ssgas poor performance comparison ssra
tiger grid tag domains could due poorly tuned greedy bias
nearly identical graphs produced showing performance function number belief points
confirms complexity analysis showing computation time directly related number belief
points



fia nytime p oint pproximations l arge pomdp




ra
ssra
ssga
ssea
ger





reward

reward







ra
ssra
ssga
ssea
ger




















time secs
















tiger grid






time secs







b hallway




reward





ssra
ssga
ssea
ger


reward




ra
ssra
ssga
ssea
ger























time secs












c hallway











time secs









tag

figure belief expansion showing execution performance function computation
time

investigate length future investigations larger number actions may
shed better light issue
terms computational requirement ger expensive compute followed
ssea however cases time perform belief expansion step generally negligible
compared cost value update steps therefore seems best use
effective though expensive heuristic
pbvi framework accommodate wide variety strategies past described
example one could extract belief points directly sampled experimental traces
subject future investigations
comparative analysis
outlined pbvi type able handle wide spectrum
large scale pomdp domains sufficient compare performance pbvi


fip ineau g ordon hrun

qmdp incremental pruningthe two ends spectrumas done section fact
significant activity recent years development fast approximate pomdp
worthwhile spend time comparing pbvi framework
alternative approaches made easy fact many validated
set described
table summarizes performance large number recent pomdp approximation including pbvi four target domains tiger grid hallway hallway tag
listed selected availability comparable published available
code cases could implemented easily
compare empirical performance terms execution performance versus
set simulation domains however often case
single best solving therefore compile summary
attributes characteristics attempt tell may best
types table includes whenever possible goal completion rates sum
rewards policy computation time number required belief points policy size number
vectors number nodes finite state controllers number belief points policy
size often identical however latter smaller single vector best multiple
belief points
marked computed us ghz pentium likely
computed different platforms therefore time comparisons may approximate best
nonetheless number samples size final policy useful indicators
computation time reported pbvi correspond earliest data point figures pbvi ger achieves top performance
listed order performance starting achieving highest reward assume standard lookahead controller see hauskrecht
definition
overall indicate achieve sub par performance terms
expected reward case qmdp fundamental limitations
incremental pruning exact value directed compression theoretically reach optimal
performance would require longer computation time grid method see tiger grid
bpi see tiger grid hallway tag pbua see tag suffer
similar offer much graceful performance degradation worth noting none
approaches assumes known initial belief effect solving harder
bbsls sufficiently extensive comment length appears able
reasonable policies small controllers see tag
remaining algorithmshsvi perseus pbvi gerall offer comparable
performance relatively large pomdp domains hsvi seems offer good control performance full range tasks requires bigger controllers therefore probably slower
especially domains high stochasticity e g tiger grid hallway hallway trade offs
perseus pbvi ger less clear time controller size performance
quality quite comparable fact two approaches similar similarities
differences two approaches explored section



fia nytime p oint pproximations l arge pomdp

method
tiger grid maze
hsvi smith simmons
perseus vlassis spaan
pbua poon
pbvi ger
bpi poupart boutilier
grid brafman
qmdp littman et al b
incprune cassandra et al
exact vdc poupart boutilier
hallway
pbua poon
hsvi smith simmons
pbvi ger
perseus vlassis spaan
bpi poupart boutilier
qmdp littman et al b
exact vdc poupart boutilier
incprune cassandra et al
hallway
pbvi ger
perseus vlassis spaan
hsvi smith simmons
pbua poon
bpi poupart boutilier
grid brafman
qmdp littman et al b
exact vdc poupart boutilier
incprune cassandra et al
tag
hsvi smith simmons
pbvi ger
perseus vlassis spaan
bbsls braziunas boutilier
bpi poupart boutilier
qmdp littman et al b
pbua poon
incprune cassandra et al

goal

reward conf int

time

b



n
n
n
n
n
n
n
n
n
















n v

hrs
hrs

n v



n

n
n
n



n v


n

n v
n v




n v
n v



















hrs
hrs


n v


n
n
n
n

n v





n v
n v


n v


n v










n v









n v

hrs
hrs



n v

n

n
n
n




n v

n

n v
n v



n v
n v
n v



















hrs
hrs

n v


n
n
n

n







n v
n v

n applicable

n v available

computed us

table pbvi standard pomdp domains



fip ineau g ordon hrun

error estimates
presented thus far suggest pbvi framework performs best
greedy error reduction ger technique selecting belief points scheme decide belief points included estimate error bound set candidate points
pick one largest error estimate error bound estimated described
equation consider question estimate evolves points
added natural intuition first points error estimates large
density belief set increases error estimates become much smaller
figure reconsiders four target domains tiger grid hallway hallway tag
case present reward performance function number belief points top
row graphs error estimate point selected according order points
picked bottom row graphs addition bottom graphs dashed line trivial
rmin
bound error vt vt rmax
valid step value function arbitrary
policy expected bound typically tighter trivial bound tag occurs
number belief points exceeds number states surprising given
bound depends distance reachable beliefs states reachable beliefs
domain overall seems reasonably good correspondence improvement
performance decrease error estimates conclude figure even
though pbvi error quite loose fact informative guiding exploration belief
simplex
note significant variance error estimates one belief point next
illustrated non monotonic behavior curves bottom graphs figure
behavior attributed possibilities first fact error estimate
given belief approximate value function used calculate error estimate
approximate addition fact belief points selected
envelope reachable beliefs set reachable beliefs suggests ger could
improved maintaining deeper envelope candidate belief points currently envelope
contains points step forward simulations points already selected may
useful consider points steps ahead predict would reduce jaggedness seen
figure importantly reduce number points necessary good performance
course tradeoff time spent selecting points time spent would
evaluated light

robotic applications
overall motivation behind work described desire provide high quality
robust real world autonomous systems particular robots practical side search robust robot controller large part guided nursebot
project pineau montermerlo pollack roy thrun overall goal project
develop personalized robotic technology play active role providing improved care
services non institutionalized elderly people pearl shown figure main robotic
platform used project
many services nursing assistant robot could provide engelberger lacey
dawson howe much work date focused providing timely cognitive reminders e g medications take appointments attend etc elderly subjects pollack


fia nytime p oint pproximations l arge pomdp

tigergrid

hallway



reward



hallway

















tag






























belief points


















belief points













error









belief points









































belief points









belief points


















belief points













belief points





















belief points

figure sum discounted reward top graphs estimate bound error bottom
graphs function number selected belief points

figure pearl nursebot interacting elderly people nursing facility
important component task finding patient whenever time issue reminder
task shares many similarities tag presented section case
however robot generated map real physical environment used basis spatial
configuration domain map shown figure white areas correspond free
space black lines indicate walls obstacles dark gray areas visible
accessible robot one easily imagine patients room physiotherapy unit lying
end corridor common area shown upper middle section
overall goal robot traverse domain order missing patient
deliver message robot must systematically explore environment reasoning
spatial coverage human motion patterns order person


fip ineau g ordon hrun

figure map environment
pomdp modeling
domain represented jointly two state features robotposition personposition
feature expressed discretization environment experiments
assume discretization meters means discrete cells feature total
states
assumed person robot move freely throughout space robots
motion deterministically controlled choice action north south east west robot
fifth action delivermessage concludes scenario used appropriately e
robot person location
persons motion stochastic falls one two modes part time person
moves according brownian motion e g moves cardinal direction p r otherwise stays put times person moves directly away robot tag domain
section assumes person moves moves away robot realistic person cannot see robot current experiment instead assumes person
moves according brownian motion robot far away moves away robot
closer e g person policy designed way encourage robot
robust policy
terms state observability two components robot sense
position sense persons position first case assumption
robot knows position times may seem generous
optimistic assumption substantial experience domains size maps quality
demonstrated robust localization abilities thrun et al especially true
operates relatively coarse resolution meters compared localization precision
cm exact position information assumed domain execution
phase actually measure performance update belief full localization
information includes positional uncertainty whenever appropriate
regarding detection person assumption robot knowledge
persons position unless within range meters plausible given robots
sensors however even short range small probability p r robot
miss person therefore return false negative
general one could make sensible assumptions persons likely position e g
knowledge daily activities however currently information therefore assume uniform distribution initial positions persons subsequent movements


fia nytime p oint pproximations l arge pomdp

expressed motion model described e mix brownian motion purposeful avoidance
reward function straightforward r motion action r
robot decides delivermessage cell person r
robot decides delivermessage persons absence task terminates robot
successfully delivers message e deliverm essage srobot sperson assume
discount factor
assume known initial belief b consisting uniform distribution states
used selecting belief points subsequently executing testing
final policy
initial map fig domain collected mobile robot slightly cleaned
hand remove artifacts e g people walking assumed model parameters
described applied pbvi value updates belief point
expansions applied alternation simulation policy able person
trials trials terminated person found execution steps
final policy implemented tested onboard publicly available carmen robot simulator montemerlo roy thrun
comparative evaluation pbvi perseus
subtask described states beyond capabilities exact pomdp solvers
furthermore demonstrated mdp type approximations equipped handle
uncertainty type exhibited task main purpose analysis evaluate
effectiveness point described address
tag domain section hint fact pbvi may able
handle task realistic map modified motion model provide challenges
begin investigation directly comparing performance pbvi ger belief points selection perseus complex robot domain perseus
described section presented produced code provided authors perseus assume fixed pomdp model generated
robot simulator model stored solved offline
pbvi perseus parameters set pbvi requires number belief
points add expansion badd horizon expansion h perseus
requires number belief points generate random walk b maximum
time presented assume following parameter settings badd h
b fairly robust changes parameters
figure summarizes experiment suggest number observations
shown figure best solution similar time
pbvi ger better anytime performance perseus e g much better policy found
given sec
shown figure b require similar number vectors
shown figure c pbvi ger requires many fewer beliefs
change parameter value yielded sensibly similar terms reward number vectors
though course time memory number beliefs varied



fip ineau g ordon hrun





pbvi ger
ger
perseus
qmdp

pbvi ger
perseus



alpha vectors

reward




























time secs


















pbvi ger
perseus




beliefs

























time secs









b
memory requirement alphas beliefs states






time secs









c





pbvi ger
perseus



























time secs











figure comparison pbvi perseus robot simulation domain
requires fewer beliefs pbvi ger much lower memory requirements
quantified figure
suggest pbvi perseus similar performance objective
near optimal solution time memory constrained cases one willing
trade accuracy time pbvi may provide superior anytime performance cases
memory limited pbvis conservative respect belief point selection
advantageous properties suggest pbvi may scale better large domains
experimental robot simulator
presented assume pomdp model used testing e compute reward figure useful carry large number
experiments model however cannot entirely capture dynamics realistic robot system
therefore concern policy learned point methods perform
well realistic robot verify robustness final pbvi control policy
implemented tested onboard publicly available carmen robot simulator montemerlo
et al


fia nytime p oint pproximations l arge pomdp

resulting policy illustrated figure figure shows five snapshots obtained
single run particular scenario person starts far end left corridor
persons location shown figures since observable robot
figure instead shows belief person positions represented distribution point samples
grey dots fig point represents plausible hypothesis persons position
figure shows robot starting far right end corridor fig robot moves toward
left rooms entrance fig b proceeds check entire room fig c
relatively certain person nowhere found exits room fig
moves left branch corridor finally finds person end
corridor fig e
policy optimized start position person robot scenario
shown figure one longer execution traces since robot ends searching entire
environment finding person interesting compare choice action
snapshots b robot position practically identical yet b robot chooses
go room whereas robot chooses move toward left direct
beliefs rather states belief distribution person positions
clearly different two cases therefore policy specifies different course
action
figure looks policy obtained solving qmdp heuristic four snapshots offered different stages specific scenario assuming person
started far left side robot far right side fig proceeding
room entrance fig b robot continues corridor almost reaches end
fig c turns around comes back toward room entrance stations
fig scenario forcibly terminated robot cannot person
left edge corridor room whats runningaway behavior adopted subject even person starts elsewhere corridor
robot approaches person gradually retreat left similarly escape robot
even though qmdp explicitly plan beliefs generate different policy actions
cases state identical belief different seen comparing figure b robot identically located however belief person
positions different b probability mass left robot therefore travels direction probability mass distributed evenly three branches
left corridor room right corridor robot equally pulled directions therefore stops
scenario illustrates strength qmdp namely many cases
necessary explicitly reduce uncertainty however shows sophisticated
approaches needed handle cases
pbvi perform outside bounds simple maze domains
able handle realistic domains particular throughout evaluation robot
simulator way constrained behave described pomdp model sec
means robots actions often stochastic effects robots position
fully observable belief tracking performed asynchronously e
straightforward ordering actions observations despite misalignment model
assumed execution environment control policy optimized pbvi could
successfully used complete task



fip ineau g ordon hrun



b

c



e
figure example pbvi policy successfully finding person



fia nytime p oint pproximations l arge pomdp



b

c


figure example qmdp policy failing person



fip ineau g ordon hrun

discussion
describes class anytime point pomdp called pbvi combines point value updates strategic selection belief points solve large pomdps
extensions pbvi framework whereby value updates applied groups belief
points according spatial distribution described pineau gordon thrun
main contributions pertaining pbvi framework summarized
scalability pbvi framework important step towards truly scalable pomdp solutions
achieved bounding policy size selection small set belief points
anytime pbvi class alternates steps value updating steps
belief point selection points added solution improves expense increased
computational time trade controlled adjusting number points terminated satisfactory solution found time
elapsed
bounded error provide theoretical bound error approximation introduced
pbvi framework holds range belief point selection methods lead
directly development pbvi type pbvi ger estimates
error bound used directly select belief points furthermore bounds
useful assessing stop adding belief points
exploration proposed set point selection heuristics explore tree
reachable beliefs select useful belief points successful technique described greedy
error reduction ger uses estimate error bound candidate belief points select
useful points
improved empirical performance pbvi demonstrated ability reduce time
number well known pomdp including tiger grid hallway hallway
operating set discrete points pbvi perform polynomial time value updates
thereby overcoming curse history paralyzes exact ger technique used
select points allows us solve large fewer belief points alternative approaches
domain pbvi applied pomdp domain tag
generated approximate solution outperformed baseline qmdp incremental
pruning domain since adopted test case vlassis
spaan smith simmons braziunas boutilier poupart boutilier
fosters increased ease comparison techniques comparative analysis
provided section highlighting similarities differences pbvi perseus
demonstrated performance pbvi applied context robotic search rescue
type scenario mobile robot required search environment non stationary
individual pbvis performance evaluated realistic independently developed robot
simulator
significant portion dedicated thorough comparative analysis point
methods includes evaluating range point selection methods well evaluating
mechanisms ordering value updates comparison point selection techniques suggest ger method presented section superior naive techniques terms
ordering value updates randomized strategy used perseus appears
effective accelerate natural next step would combine ger belief selection
heuristic perseuss random value updates performed experiments along lines


fia nytime p oint pproximations l arge pomdp

achieve significant speed current performance pbvi perseus e g
reported figure likely belief points chosen carefully ger
points needs updated systematically therefore additional benefit
randomized value updates
looking towards future important remember demonstrated
ability solve large pomdp standards many real world domains far exceed
complex domains considered particular unusual
expressed number multi valued state features case number states grows
exponentially number features concern belief point
vector dimensionality number states dimensions updated
simultaneously important issue address improve scalability point value
approaches general
existing attempts overcoming curse dimensionality pomdps
thesee g belief compression techniques roy gordon cannot
incorporated within pbvi framework without compromising theoretical properties discussed section others particular exact compression poupart boutilier
combined pbvi however preliminary experiments direction
yielded little performance improvement reason believe approximate value compression would yield better expense forgoing pbvis theoretical properties challenge therefore devise function approximation techniques reduce
dimensionality effectively maintaining convexity properties solution
secondary less important issue concerning scalability pbvi pertains
number belief points necessary obtain good solution addressed thus far
usually solved belief points need true worse case number
belief points necessary may exponential plan length pbvi framework accommodate wide variety strategies generating belief points greedy error reduction
technique seems particularly effective however unlikely definitive answer belief
point selection general terms relates closely well known issue exploration
versus exploitation arises across wide array solving techniques
promising opportunities future aside pbvi framework already
pushed envelope pomdp solved existing computational resources
field pomdps matures finding ways computing policies efficiently likely continue
major bottleneck hope point pbvi play leading
role search efficient

acknowledgments
authors wish thank craig boutilier michael littman andrew moore matthew mason
many thoughtful comments discussions regarding work thank darius braziunas
pascal poupart trey smith nikos vlassis conversations regarding
contributions michael montemerlo nicholas roy conducting empirical
robot evaluations gratefully acknowledged finally thank three anonymous reviewers
one dedicated editor sridhar mahadevan whose feedback significantly improved
work funded darpa mars program nsfs itr program project robotic
assistants elderly pi j dunbar jacob


fip ineau g ordon hrun

references
astrom k j optimal control markov decision processes incomplete state estimation journal mathematical analysis applications
bellman r dynamic programming princeton university press
bertsekas p tsitsiklis j neuro dynamic programming athena scientific
blum l furst l fast graph analysis artificial
intelligence
bonet b epsilon optimal grid partially obserable markov decision
processes machine learning proceedings international conference icml
pp
boutilier c dean hanks decision theoretic structural assumptions
computational leverage journal artificial intelligence
boyen x koller tractable inference complex stochastic processes proceedings fourteenth conference uncertainty artificial intelligence uai pp
brafman r heuristic variable grid solution method pomdps proceedings
fourteenth national conference artificial intelligence aaai pp
braziunas boutilier c stochastic local search pomdp controllers proceedings nineteenth national conference artificial intelligence aaai pp
burgard w cremers b fox hahnel lakemeyer g schulz steiner w thrun
experiences interactive museum tour guide robot artificial intelligence

cassandra
tonys
ai pomdp code index html

pomdp

page

http www cs brown edu

cassandra littman l zhang n l incremental pruning simple fast exact
method partially observable markov decision processes proceedings thirteenth
conference uncertainty artificial intelligence uai pp
chapman conjunctive goals artificial intelligence
cheng h partially observable markov decision processes ph thesis
university british columbia
dean kanazawa k probabilistic temporal reasoning proceedings seventh
national conference artificial intelligence aaai pp
devroye l non uniform random variate generation springer verlag york
engelberger g handbook industrial robotics john wiley sons
fikes r e nilsson n j strips application theorem
proving solving artificial intelligence
hauskrecht incremental methods computing bounds partially observable markov
decision processes proceedings fourteenth national conference artificial intelligence aaai pp


fia nytime p oint pproximations l arge pomdp

hauskrecht value function approximations partially observable markov decision
processes journal artificial intelligence
jazwinski stochastic processes filtering theory academic york
kaelbling l p littman l cassandra r acting partially
observable stochastic domains artificial intelligence
kalman r e linear filtering prediction transactions
asme journal basic engineering
lacey g dawson howe k application robotics mobility aid
elderly blind robotics autonomous systems
littman l sequential decision making ph thesis brown university
littman l cassandra r kaelbling l p learning policies partially obsevable environments scaling tech rep cs brown university department
computer science
littman l cassandra r kaelbling l p b learning policies partially obsevable environments scaling proceedings twelfth international conference
machine learning pp
lovejoy w computationally feasible bounds partially observed markov decision
processes operations
lovejoy w b survey algorithmic methods partially observable markov decision
processes annals operations
mcallester roseblitt systematic nonlinear proceedings ninth
national conference artificial intelligence aaai pp
monahan g e survey partially observable markov decision processes theory management science
montemerlo roy n thrun perspectives standardization mobile robot
programming carnegie mellon navigation carmen toolkit proceedings
ieee rsj international conference intelligent robots systems iros vol pp pp

paquet distributed decision making task coordination dynamic uncertain
real time multiagent environments ph thesis universite laval
penberthy j weld ucpop sound complete partial order adl
proceedings third international conference knowledge representation reasoning pp
perseus http staff science uva nl mtjspaan software approx
pineau j gordon g thrun applying metric trees belief point pomdps
neural information processing systems nips vol
pineau j montermerlo pollack roy n thrun towards robotic assistants
nursing homes challenges robotics autonomous systems
pollack technology intelligent cognifitve orthotics proceedings
th international conference ai scheduling aips


fip ineau g ordon hrun

poon k fast heuristic decision theoretic masters thesis
hong kong university science technology
poupart p boutilier c value directed belief state approximation pomdps
proceedings sixteenth conference uncertainty artificial intelligence uai pp

poupart p boutilier c value directed compression pomdps advances neural
information processing systems nips vol
poupart p boutilier c bounded finite state controllers advances neural information processing systems nips vol
rabiner l r tutorial hidden markov selected applications speech
recognition proceedings ieee
rosencrantz gordon g thrun locating moving entities dynamic indoor
environments teams mobile robots second international joint conference
autonomous agents multiagent systems aamas pp
roy n gordon g exponential family pca belief compression pomdps
advances neural information processing systems nips vol pp
smith simmons r heuristic search value iteration pomdps proceedings
twentieth conference uncertainty artificial intelligence uai
sondik e j optimal control partially observable markov processes ph thesis
stanford university
spaan vlassis n perseus randomized point value iteration pomdps
journal artificial intelligence jair
sutton r barto g reinforcement learning introduction mit press
thrun fox burgard w dellaert f robust monte carlo localization mobile
robots artificial intelligence
vlassis n spaan j fast point pomdps proceedings
belgian dutch conference machine learning
white c c survey solution techniques partially observed markov decision
process annals operations
zhang n l liu w stochastic domains characteristics approximation tech rep hkust cs dept computer science hong kong university science technology
zhang n l zhang w speeding convergence value iteration partially
observable markov decision processes journal artificial intelligence

zhou r hansen e improved grid approximation pomdps
proceedings th international joint conference artificial intelligence ijcai
pp




