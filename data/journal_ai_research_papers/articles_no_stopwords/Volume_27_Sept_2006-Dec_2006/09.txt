Journal Artificial Intelligence Research 27 (2006) 335-380

Submitted 05/06; published 11/06

Anytime Point-Based Approximations Large POMDPs
Joelle Pineau

JPINEAU @ CS . MCGILL . CA

School Computer Science
McGill University
Montreal QC, H3A 2A7 CANADA

Geoffrey Gordon

GGORDON @ CS . CMU . EDU

Machine Learning Department
Carnegie Mellon University
Pittsburgh PA, 15232 USA

Sebastian Thrun

THRUN @ STANFORD . EDU

Computer Science Department
Stanford University
Stanford CA, 94305 USA

Abstract
Partially Observable Markov Decision Process long recognized rich framework real-world planning control problems, especially robotics. However exact solutions framework typically computationally intractable smallest problems.
well-known technique speeding POMDP solving involves performing value backups
specific belief points, rather entire belief simplex. efficiency approach,
however, depends greatly selection points. paper presents set novel techniques
selecting informative belief points work well practice. point selection procedure
combined point-based value backups form effective anytime POMDP algorithm called
Point-Based Value Iteration (PBVI). first aim paper introduce algorithm
present theoretical analysis justifying choice belief selection technique. second aim
paper provide thorough empirical comparison PBVI state-of-the-art
POMDP methods, particular Perseus algorithm, effort highlight similarities
differences. Evaluation performed using standard POMDP domains realistic robotic
tasks.

1. Introduction
concept planning long tradition AI literature (Fikes & Nilsson, 1971; Chapman,
1987; McAllester & Roseblitt, 1991; Penberthy & Weld, 1992; Blum & Furst, 1997). Classical
planning generally concerned agents operate environments fully observable,
deterministic, finite, static, discrete. techniques able solve increasingly
large state-space problems, basic assumptions classical planningfull observability, static
environment, deterministic actionsmake unsuitable robotic applications.
Planning uncertainty aims improve robustness explicitly reasoning type
uncertainty arise. Partially Observable Markov Decision Process (POMDP) (Astrom,
1965; Sondik, 1971; Monahan, 1982; White, 1991; Lovejoy, 1991b; Kaelbling, Littman, & Cassandra, 1998; Boutilier, Dean, & Hanks, 1999) emerged possibly general representation
(single-agent) planning uncertainty. POMDP supersedes frameworks terms
c
2006
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiP INEAU , G ORDON & HRUN

representational power simply combines essential features planning
uncertainty.
First, POMDPs handle uncertainty action effects state observability, whereas many
frameworks handle neither these, handle stochastic action effects. handle partial state observability, plans expressed information states, instead world states,
since latter ones directly observable. space information states space
beliefs system might regarding world state. Information states easily calculated
measurements noisy imperfect sensors. POMDPs, information states typically
represented probability distributions world states.
Second, many POMDP algorithms form plans optimizing value function. powerful approach plan optimization, since allows one numerically trade alternative
ways satisfy goal, compare actions different costs/rewards, well plan multiple
interacting goals. value function optimization used planning approachesfor example Markov Decision Processes (MDPs) (Bellman, 1957)POMDPs unique expressing
value function information states, rather world states.
Finally, whereas classical conditional planners produce sequence actions, POMDPs
produce full policy action selection, prescribes choice action possible
information state. producing universal plan, POMDPs alleviate need re-planning,
allow fast execution. Naturally, main drawback optimizing universal plan computational complexity so. precisely seek alleviate work described
paper
known algorithms exact planning POMDPs operate optimizing value function
possible information states (also known beliefs). algorithms run wellknown curse dimensionality, dimensionality planning problem directly related
number states (Kaelbling et al., 1998). suffer lesser known curse
history, number belief-contingent plans increases exponentially planning
horizon. fact, exact POMDP planning known PSPACE-complete, whereas propositional
planning NP-complete (Littman, 1996). result, many POMDP domains
states, actions sensor observations computationally intractable.
commonly used technique speeding POMDP solving involves selecting finite set
belief points performing value backups set (Sondik, 1971; Cheng, 1988; Lovejoy,
1991a; Hauskrecht, 2000; Zhang & Zhang, 2001). usefulness belief point updates
well acknowledged, backups applied thoroughly
explored.
paper describes class Point-Based Value Iteration (PBVI) POMDP approximations
value function estimated based strictly point-based updates. context,
choice points integral part algorithm, approach interleaves value backups
steps belief point selection. One key contributions paper presentation
analysis set heuristics selecting informative belief points. range naive
version combines point-based value updates random belief point selection, sophisticated algorithm combines standard point-based value update estimate error
bound approximate exact solutions select belief points. Empirical theoretical evaluation techniques reveals importance taking distance points
consideration selecting belief points. result approach exhibits good perfor336

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

mance belief points (sometimes less number states), thereby overcoming
curse history.
PBVI class algorithms number important properties, discussed
greater length paper:
Theoretical guarantees. present bound error value function obtained
point-based approximation, respect exact solution. bound applies number
point-based approaches, including PBVI, Perseus (Spaan & Vlassis, 2005),
others.
Scalability. able handle problems order 103 states, order magnitude larger problems solved traditional POMDP techniques.
empirical performance evaluated extensively realistic robot tasks, including search-formissing-person scenario.
Wide applicability. approach makes assumptions nature structure
domain. PBVI framework assume known discrete state/ action/observation spaces
known model (i.e., state-to-state transitions, observation probabilities, costs/rewards),
additional specific structure (e.g., constrained policy class, factored model).
Anytime performance. anytime solution achieved gradually alternating phases
belief point selection phases point-based value updates. allows effective
trade-off planning time solution quality.
PBVI many important properties, number recent POMDP approaches exhibit competitive performance (Braziunas & Boutilier, 2004; Poupart & Boutilier,
2004; Smith & Simmons, 2004; Spaan & Vlassis, 2005). provide overview techniques later part paper. provide comparative evaluation algorithms
PBVI using standard POMDP domains, effort guide practitioners choice
algorithm. One algorithms, Perseus (Spaan & Vlassis, 2005), closely related PBVI
design performance. therefore provide direct comparison two approaches
using realistic robot task, effort shed light comparative strengths weaknesses two approaches.
paper organized follows. Section 2 begins exploring basic concepts POMDP
solving, including representation, inference, exact planning. Section 3 presents general
anytime PBVI algorithm theoretical properties. Section 4 discusses novel strategies select good belief points. Section 6 presents empirical comparison POMDP algorithms using
standard simulation problems. Section 7 pursues empirical evaluation tackling complex robot
domains directly comparing PBVI Perseus. Finally, Section 5 surveys number existing
POMDP approaches closely related PBVI.

2. Review POMDPs
Partially Observable Markov Decision Processes provide general planning decision-making
framework acting optimally partially observable domains. well-suited great
number real-world problems decision-making required despite prevalent uncertainty.
generally assume complete correct world model, stochastic state transitions, imperfect state tracking, reward structure. Given information, goal find action
337

fiP INEAU , G ORDON & HRUN

strategy maximizes expected reward gains. section first establishes basic terminology essential concepts pertaining POMDPs, reviews optimal techniques POMDP
planning.
2.1 Basic POMDP Terminology
Formally, POMDP defined six distinct quantities, denoted {S, A, Z, T, O, R}. first three
are:
States. state world denoted s, finite set states denoted =
{s0 , s1 , . . .}. state time denoted st , discrete time index. state
directly observable POMDPs, agent compute belief state
space S.
Observations. infer belief regarding worlds state s, agent take sensor measurements. set measurements, observations, denoted Z = {z0 , z1 , . . .}.
observation time denoted zt . Observation zt usually incomplete projection
world state st , contaminated sensor noise.
Actions. act world, agent given finite set actions, denoted =
{a0 , a1 , . . .}. Actions stochastically affect state world. Choosing right action
function history core problem POMDPs.
Throughout paper, assume states, actions observations discrete finite.
mathematical convenience, assume actions observations alternated
time.
fully define POMDP, specify probabilistic laws describe state transitions
observations. laws given following distributions:
state transition probability distribution,
(s, a, s0 ) := P r(st = s0 | st1 = s, at1 = a) t,

(1)

probability transitioning state s0 , given agent state selects action a, (s, a, s0 ). Since conditional probability distribution,
P
0
s0 (s, a, ) = 1, (s, a). notation suggests, time-invariant.
observation probability distribution,
O(s, a, z) := P r(zt = z | st1 = s, at1 = a) t,

(2)

probability agent perceive observation z upon executing action state s.
P
conditional probability defined (s, a, z) triplets, zZ O(s, a, z) =
1, (s, a). probability function time-invariant.
Finally, objective POMDP planning optimize action selection, agent given
reward function describing performance:
338

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

reward function. R(s, a) : <, assigns numerical value quantifying
utility performing action state s. assume reward bounded, Rmin <
R < Rmax . goal agent collect much reward possible time.
precisely, wants maximize sum:
E[


X

tt0 rt ],

(3)

t=t0

rt reward time t, E[ ] mathematical expectation, 0 < 1
discount factor, ensures sum Equation 3 finite.
items together, states S, actions A, observations Z, reward R, probability
distributions, O, define probabilistic world model underlies POMDP.
2.2 Belief Computation
POMDPs instances Markov processes, implies current world state, st , sufficient predict future, independent past {s0 , s1 , ..., st1 }. key characteristic
sets POMDPs apart many probabilistic models (such MDPs) fact state
st directly observable. Instead, agent perceive observations {z1 , . . . , zt },
convey incomplete information worlds state.
Given state directly observable, agent instead maintain complete trace
observations actions ever executed, use select actions. action/observation trace known history. formally define
ht := {a0 , z1 , . . . , zt1 , at1 , zt }

(4)

history time t.
history trace get long time goes on. well-known fact history
need represented explicitly, instead summarized via belief distribution (Astrom, 1965), following posterior probability distribution:
bt (s) := P r(st = | zt , at1 , zt1 , . . . , a0 , b0 ).

(5)

course requires knowing initial state probability distribution:
b0 (s) := P r(s0 = s),

(6)

defines probability domain state time = 0. common either
specify initial belief part model, give runtime system tracks
beliefs selects actions. work, assume initial belief (or set possible
initial beliefs) available planner.
belief distribution bt sufficient statistic history, suffices condition
selection actions bt , instead ever-growing sequence past observations
actions. Furthermore, belief bt time calculated recursively, using belief one time
step earlier, bt1 , along recent action at1 observation zt .
339

fiP INEAU , G ORDON & HRUN

define belief update equation, (), as:
(bt1 , at1 , zt ) = bt (s0 )
X

=

O(s0 , at1 , zt ) (s, at1 , s0 ) bt1 (s)

s0

P r(zt |bt1 , at1 )

(7)

denominator normalizing constant.
equation equivalent decades-old Bayes filter (Jazwinski, 1970), commonly
applied context hidden Markov models (Rabiner, 1989), known forward
algorithm. continuous generalization forms basis Kalman filters (Kalman, 1960).
interesting consider nature belief distributions. Even finite state spaces,
belief continuous quantity. defined simplex describing space distributions
state space S. large state spaces, calculating belief update (Eqn 7) computationally challenging. Recent research led efficient techniques belief state computation
exploit structure domain (Dean & Kanazawa, 1988; Boyen & Koller, 1998; Poupart &
Boutilier, 2000; Thrun, Fox, Burgard, & Dellaert, 2000). However, far complex aspect POMDP planning generation policy action selection, described next.
example robotics, calculating beliefs state spaces 106 states easily done realtime (Burgard et al., 1999). contrast, calculating optimal action selection policies exactly appears
infeasible environments dozen states (Kaelbling et al., 1998),
directly size state space, complexity optimal policies.
Hence assume throughout paper belief computed accurately, instead
focus problem finding good approximations optimal policy.
2.3 Optimal Policy Computation
central objective POMDP perspective compute policy selecting actions.
policy form:
(b) a,

(8)

b belief distribution action chosen policy .
particular interest notion optimal policy, policy maximizes expected future discounted cumulative reward:


(bt0 ) = argmax E



X

t=t0




tt0 rt fibt0 .


(9)

two distinct interdependent reasons computing optimal policy challenging. widely-known reason so-called curse dimensionality: problem
n physical states, defined belief states (n 1)-dimensional continuous space.
less-well-known reason curse history: POMDP solving many ways search
space possible POMDP histories. starts searching short histories (through
select best short policies), gradually considers increasingly long histories. Unfortunately number distinct possible action-observation histories grows exponentially
planning horizon.
340

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

two cursesdimensionality historyoften act independently: planning complexity
grow exponentially horizon even problems states, problems
large number physical states may still small number relevant histories. curse
predominant depends problem hand, solution technique. example,
belief point methods focus paper specifically target curse history, leaving
vulnerable curse dimensionality. Exact algorithms hand typically
suffer far curse history. goal therefore find techniques offer best
balance both.
describe straightforward approach finding optimal policies Sondik (1971).
overall idea apply multiple iterations dynamic programming, compute increasingly
accurate values belief state b. Let V value function maps belief states values
<. Beginning initial value function:
V0 (b) = max


X

R(s, a)b(s),

(10)

sS

t-th value function constructed (t 1)-th following recursive equation:
"

Vt (b) = max


#
X

X

R(s, a)b(s) +

sS

P r(z | a, b)Vt1 ( (b, a, z)) ,

(11)

zZ

(b, a, z) belief updating function defined Equation 7. value function update
maximizes expected sum (possibly discounted) future pay-offs agent receives
next time steps, belief state b. Thus, produces policy optimal planning
horizon t. optimal policy directly extracted previous-step value function:
#

"

(b)

= argmax


X

R(s, a)b(s) +

X

P r(z | a, b)Vt1 ( (b, a, z)) .

(12)

zZ

sS

Sondik (1971) showed value function finite horizon expressed set
vectors: = {0 , 1 , . . . , }. -vector represents |S|-dimensional hyper-plane,
defines value function bounded region belief:
X

Vt (b) = max


(s)b(s).

(13)

sS

addition, -vector associated action, defining best immediate policy
assuming optimal behavior following (t 1) steps (as defined respectively sets
{Vt1 , ..., V0 }).
t-horizon solution set, , computed follows. First, rewrite Equation 11 as:


Vt (b) = max
aA


X

sS

R(s, a)b(s) +

X
zZ

max

t1

XX
sS

(s, a, s0 )O(s0 , a, z)(s0 )b(s) . (14)

s0

Notice representation Vt (b), nonlinearity term P (z|a, b) Equation 11
cancels nonlinearity term (b, a, z), leaving linear function b(s) inside max
operator.
341

fiP INEAU , G ORDON & HRUN

value Vt (b) cannot computed directly belief b B (since infinitely
many beliefs), corresponding set generated sequence operations
set t1 .
a,z
first operation generate intermediate sets a,
, A, z Z (Step 1):
a, (s) = R(s, a)
a,

a,z




ia,z (s)

=

X

(15)
0

0

0

(s, a, )O(s , a, z)i (s ), t1

s0

a, ia,z |S|-dimensional hyper-plane.
Next create (a A), cross-sum observations1 , includes one a,z
a,z
(Step 2):
a,z1
2
...
a,z
= a,

+

(16)

Finally take union sets (Step 3):
= aA .

(17)

forms pieces backup solution horizon t. actual value function Vt
extracted set described Equation 13.
Using approach, bounded-time POMDP problems finite state, action, observation
spaces solved exactly given choice horizon . environment
agent might able bound planning horizon advance, policy (b) approximation optimal one whose quality improves expectation planning horizon (assuming
0 < 1).
mentioned above, value function Vt extracted directly set . important aspect algorithm (and optimal finite-horizon POMDP solutions)
value function guaranteed piecewise linear, convex, continuous function belief (Sondik, 1971). piecewise-linearity continuous properties direct result fact
Vt composed finitely many linear -vectors. convexity property result
a,

maximization operator (Eqn 13). worth pointing intermediate sets a,z
,
represent functions belief composed entirely linear segments. property
holds intermediate representations incorporate expectation observation
probabilities (Eqn 15).
worst case, exact value update procedure described could require time doubly exponential planning horizon (Kaelbling et al., 1998). better understand complexity
exact update, let |S| number states, |A| number actions, |Z| number
observations, |t1 | number -vectors previous solution set. Step 1 creates
|A| |Z| |t1 | projections Step 2 generates |A| |t1 ||Z| cross-sums. So, worst case,
new solution requires:
|t | = O(|A||t1 ||Z| )

(18)

1. symbol denotes cross-sum operator. cross-sum operation defined two sets, =
{a1 , a2 , . . . , } B = {b1 , b2 , . . . , bn }, produces third set, C = {a1 + b1 , a1 + b2 , . . . , a1 + bn , a2 +
b1 , a2 + b2 , . . . , . . . , + bn }.

342

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

-vectors represent value function horizon t; computed time
O(|S|2 |A| |t1 ||Z| ).
often case vector completely dominated another vector
entire belief simplex:
b < j b, b.
(19)
Similarly, vector may fully dominated set vectors (e.g., 2 Fig. 1 dominated combination 1 3 ). vector pruned away without affecting
solution. Finding dominated vectors expensive. Checking whether single vector
dominated requires solving linear program |S| variables |t | constraints. Nonetheless
time-effective apply pruning iteration prevent explosion solution
size. practice, |t | often appears grow singly exponentially t, given clever mechanisms
pruning unnecessary linear functions. enormous computational complexity long
key impediment toward applying POMDPs practical problems.

V={ 0 , 1 , 2 , 3 }

Figure 1: POMDP value function representation

2.4 Point-Based Value Backup
Exact POMDP solving, outlined above, optimizes value function beliefs. Many
approximate POMDP solutions, including PBVI approach proposed paper, gain computational advantage applying value updates specific (and few) belief points, rather
beliefs (Cheng, 1988; Zhang & Zhang, 2001; Poon, 2001). approaches differ significantly
(and great consequence) select belief points, set points selected,
procedure updating value standard. describe procedure updating
value function set known belief points.
Section 2.3, value function update implemented sequence operations
set -vectors. assume interested updating value function fixed
set belief points, B = {b0 , b1 , ..., bq }, follows value function contain
one -vector belief point. point-based value function therefore represented
corresponding set {0 , 1 , . . . , q }.
Given solution set t1 , simply modify exact backup operator (Eqn 14)
one -vector per belief point maintained. point-based backup gives -vector
valid region around b. assumes belief points region
action choice lead facets Vt1 point b. key idea behind
algorithms presented paper, reason large computational savings associated
class algorithms.
343

fiP INEAU , G ORDON & HRUN

obtain solution set previous set t1 , begin generating intera,z
mediate sets a,
, A, z Z (exactly Eqn 15) (Step 1):
a,
a, (s) = R(s, a)

a,z




ia,z (s)

=

(20)
0

X

0

0

(s, a, )O(s , a, z)i (s ), t1 .

s0

Next, whereas performing exact value update requires cross-sum operation (Eqn 16),
operating finite set points, instead use simple summation. construct ,
(Step 2):
ba = a,
+

X

argmax(

a,z
zZ

X

(s)b(s)), b B.

(21)

sS

Finally, find best action belief point (Step 3):
b = argmax(

X


,aA sS

(s)b(s)), b B.

= bB b

(22)
(23)

operations preserve best -vector belief point b B, estimate
value function belief simplex (including b
/ B) extracted set
before:
X

Vt (b) = max


(s)b(s).

(24)

sS

better understand complexity updating value set points B, let |S|
number states, |A| number actions, |Z| number observations, |t1 | number
-vectors previous solution set. exact update, Step 1 creates |A| |Z| |t1 |
projections (in time |S|2 |A| |Z| |t1 |). Steps 2 3 reduce set |B| components
(in time |S| |A| |t1 | |Z| |B|). Thus, full point-based value update takes polynomial time,
even crucially, size solution set remains constant every iteration.
point-based value backup algorithm summarized Table 1.
Note algorithm outlined Table 1 includes trivial pruning step (lines 13-14),
whereby refrain adding vector already included it. result, often
case |t | |B|. situation arises whenever multiple nearby belief points support
vector. pruning step computed rapidly (without solving linear programs) clearly
advantageous terms reducing set .
point-based value backup found many POMDP solvers, general serves improve estimates value function. integral part PBVI framework.

3. Anytime Point-Based Value Iteration
describe algorithmic framework new class fast approximate POMDP algorithms called Point-Based Value Iteration (PBVI). PBVI-class algorithms offer anytime solution
large-scale discrete POMDP domains. key achieving anytime solution interleave
344

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

=BACKUP(B, t1 )
action
observation z Z
solution vector t1
P
ia,z (s) = s0 (s, a, s0 )O(s0 , a, z)i (s0 ),
End
a,z
a,z
=
End
End
=
belief point hb B

P
P
P
b = argmaxaA
[
(s)b(s)]
sS R(s, a)b(s) +
zZ maxa,z
sS

If(b
/ )
= b
End
Return

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Table 1: Point-based value backup

two main components: point-based update described Table 1 steps belief set selection. approximate value function find guaranteed bounded error (compared
optimal) discrete POMDP domain.
current section focuses overall anytime algorithm theoretical properties, independent belief point selection process. Section 4 discusses detail various novel
techniques belief point selection.
overall PBVI framework simple. start (small) initial set belief points
applied first series backup operations. set belief points grown,
new series backup operations applied belief points (old new), on,
satisfactory solution obtained. interleaving value backup iterations expansions
belief set, PBVI offers range solutions, gradually trading computation time solution
quality.
full algorithm presented Table 2. algorithm accepts input initial belief point
set (BInit ), initial value (0 ), number desired expansions (N ), planning horizon
(T ). common choice BInit initial belief b0 ; alternately, larger set could used,
especially cases sample trajectories available. initial value, 0 , typically set
min
purposefully low (e.g., 0 (s) = R1
, S). this, show pointbased solution always lower-bound exact solution (Lovejoy, 1991a). follows
simple observation failing compute -vector lower value function.
problems finite horizon, run value backups expansion
belief set. infinite-horizon problems, select horizon
[Rmax Rmin ] < ,
Rmax = maxs,a R(s, a) Rmin = mins,a R(s, a).
345

(25)

fiP INEAU , G ORDON & HRUN

complete algorithm terminates fixed number expansions (N ) completed. Alternately, algorithm could terminate value function approximation reaches
given performance criterion. discussed below.
algorithm uses BACKUP routine described Table 1. assume moment
EXPAND subroutine (line 8) selects belief points random. performs reasonably
well small problems easy achieve good coverage entire belief simplex.
However scales poorly larger domains exponentially many points needed guarantee
good coverage belief simplex. sophisticated approaches selecting belief points
presented Section 4. Overall, PBVI framework described offers simple yet flexible
approach solving large-scale POMDPs.
=PBVI-MAIN(BInit , 0 , N , )
B=BInit
= 0
N expansions
iterations
=BACKUP(B,)
End
Bnew =EXPAND(B,)
B = B Bnew
End
Return

1
2
3
4
5
6
7
8
9
10
11

Table 2: Algorithm Point-Based Value Iteration (PBVI)
belief set B horizon t, algorithm Table 2 produce estimate value
function, denoted VtB . show error VtB optimal value function V
bounded. bound depends densely B samples belief simplex ; denser
sampling, VtB converges Vt , t-horizon optimal solution, turn bounded error
respect V , optimal solution. cutting PBVI iterations sufficiently large
horizon, show difference VtB optimal infinite-horizon V
large. overall error PBVI bounded, according triangle inequality, by:
kVtB V k kVtB Vt k + kVt V k .

(26)

second term bounded kV0 V k (Bertsekas & Tsitsiklis, 1996). remainder
section states proves bound first term, denote .
Begin assuming H denotes exact value backup, H denotes PBVI backup.
define (b) error introduced specific belief b performing one iteration
point-based backup:
(b) = |HV B (b) HV B (b)| .
Next define maximum total error introduced one iteration point-based backup:
= |HV B HV B |
= max (b).
b

346

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

Finally define density B set belief points B maximum distance belief
simplex belief set B. precisely:
B = max
min kb b0 k1 .
0
b bB

(27)

prove following lemma:
Lemma 1. error introduced PBVI performing one iteration value backup B,
instead , bounded
(Rmax Rmin )B

1
Proof: Let b0 point PBVI makes worst error value update, b B
closest (1-norm) sampled belief b0 . Let vector maximal b, 0
vector would maximal b0 . failing include 0 solution set, PBVI makes error
0 b0 b0 . hand, since maximal b, 0 b b. So,
0 b 0 b0
=

=



0 b0 b0 + (0 b 0 b)
0 b0 b 0 + b 0 b
(0 ) (b0 b)
k0 k kb0 bk1
k0 k B



(Rmax Rmin )B
1

Add zero
Assume optimal b
Re-arrange terms
Holder inequality
definition B

last inequality holds -vector represents reward achievable starting
state following sequence actions observations. Therefore sum rewards
min
max
must fall R1
R1
.
Lemma 1 states bound approximation error introduced one iteration point-based
value updates within PBVI framework. look bound multiple value updates.
Theorem 3.1. belief set B horizon t, error PBVI algorithm = kVtB
Vt k bounded
(Rmax Rmin )B

(1 )2
Proof:
= ||VtB Vt ||
B HV ||
= ||HVt1
t1




=


definition H

B HV B || + ||HV B HV ||
||HVt1
t1
t1
t1
(Rmax Rmin )B
B

+ ||HVt1 HVt1 ||
1
(Rmax Rmin )B
B V ||
+ ||Vt1
t1
1

triangle inequality

(Rmax Rmin )B
1
(Rmax Rmin )B
(1)2

definition t1

+ t1

lemma 1
contraction exact value backup

sum geometric series
347

fiP INEAU , G ORDON & HRUN

bound described section depends densely B samples belief simplex .
case beliefs reachable, PBVI need sample densely,
(Fig. 2). error bounds convergence results
replace set reachable beliefs
0


hold . simply need re-define b lemma 1.
side note, worth pointing PBVI makes assumption regarding
initial value function V0B , point-based solution V B guaranteed improve
addition belief points. Nonetheless, theorem presented section shows bound
error VtB (the point-based solution) V (the optimal solution) guaranteed
decrease (or stay same) addition belief points. cases VtB initialized
min
pessimistically (e.g., V0B (s) = R1
, S, suggested above), VtB improve (or stay
same) value backup addition belief points.
section thus far skirted issue belief point selection, however bound presented
section clearly argues favor dense sampling belief simplex. randomly
selecting points according uniform distribution may eventually accomplish this, generally
inefficient, particular high dimensional cases. Furthermore, take advantage
fact error bound holds dense sampling reachable beliefs. Thus seek
efficient ways generate belief points random entire simplex. issue
explored next section.

4. Belief Point Selection
section 3, outlined prototypical PBVI algorithm, conveniently avoiding question
belief points selected. clear trade-off including fewer
beliefs (which would favor fast planning good performance), versus including many beliefs
(which would slow planning, ensure better bound performance). brings
question many belief points included. However number points
consideration. likely collections belief points (e.g., frequently encountered)
likely produce good value function others. brings question
beliefs included.
number approaches proposed literature. example, exact value
function approaches use linear programs identify points value function needs
improved (Cheng, 1988; Littman, 1996; Zhang & Zhang, 2001), however typically
expensive. value function approximated learning value regular points,
using fixed-resolution (Lovejoy, 1991a), variable-resolution (Zhou & Hansen, 2001) grid.
less expensive solving LPs, scales poorly number states increases. Alternately, one use heuristics generate grid-points (Hauskrecht, 2000; Poon, 2001). tends
scalable, though significant experimentation required establish heuristics
useful.
section presents five heuristic strategies selecting belief points, fast naive
random sampling, increasingly sophisticated stochastic simulation techniques.
effective strategy propose one carefully selects points likely largest
impact reducing error bound (Theorem 3.1).
strategies consider focus selecting reachable beliefs, rather getting
uniform coverage entire belief simplex. Therefore useful begin discussion
looking reachability assessed.
348

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

exact POMDP value iteration solutions optimal initial belief, PBVI (and
related techniques) assume known initial belief b0 . shown Figure 2, use
initial belief build tree reachable beliefs. representation, path tree
corresponds sequence belief space, increasing depth corresponds increasing plan
horizon. selecting set belief points PBVI, including reachable beliefs would guarantee optimal performance (conditioned initial belief), expense computational
grow exponentially planning horitractability, since set reachable beliefs, ,
sufficiently small computational
zon. Therefore, best select subset B
tractability, sufficiently large good value function approximation.2

b0

...
ba z ba z

ba z

0 q

0 0 a0 z0

1 1

ba z

1 q

...

ba z

1 0

...
...

...

...

ba z ba z

...
ba z ba z
p 0

p 1

ba z

p q

...

0 1

...

...
...

0 0

...

...
ba z

0 0 ap zq

ba z

0 1 a0 z0

ba z

0 1 ap zq

...

...

...

...

Figure 2: set reachable beliefs
domains initial belief known (or unique), still possible use reachability analysis sampling initial beliefs (or using set known initial beliefs) seed
multiple reachability trees.
discuss five strategies selecting belief points, used within
PBVI framework perform expansion belief set.
4.1 Random Belief Selection (RA)
first strategy simplest. consists sampling belief points uniform distribution entire belief simplex. sample simplex, cannot simply sample
P
b(s) independently [0, 1] (this would violate constraint b(s) = 1). Instead, use
algorithm described Table 3 (see Devroye, 1986, details including proof uniform
coverage).
random point selection strategy, unlike strategies presented below, focus
reachable beliefs. reason, necessarily advocate approach. However
include obvious choice, far simplest implement, used
related work Hauskrecht (2000) Poon (2001). smaller domains (e.g., <20 states),
2. strategies discussed assume belief point set, B, approximately doubles size belief
expansion. ensures number rounds value iteration logarithmic (in final number belief
points needed). Alternately, strategy could used (with little modification) add fixed number new
belief points, may require many rounds value iteration. Since value iteration much expensive
belief computation, seems appropriate double size B expansion.

349

fiP INEAU , G ORDON & HRUN

Bnew =EXPANDRA (B, )
Bnew = B
Foreach b B
:= number states
= 0 :
btmp [i]=randuniform (0,1)
End
Sort btmp ascending order
= 1 : 1
bnew [i]=btmp [i + 1] btmp [i]
End
Bnew = Bnew bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11
12
13
14

Table 3: Algorithm belief expansion random action selection

performs reasonably well, since belief simplex relatively low-dimensional. large domains
(e.g., 100+ states), cannot provide good coverage belief simplex reasonable number
points, therefore exhibits poor performance. demonstrated experimental results
presented Section 6.
remaining belief selection strategies make use belief tree (Figure 2) focus
reachable beliefs, rather trying cover entire belief simplex.
4.2 Stochastic Simulation Random Action (SSRA)
generate points along belief tree, use technique called stochastic simulation. involves
running single-step forward trajectories belief points already B. Simulating single-step
forward trajectory given b B requires selecting action observation pair (a, z),
computing new belief (b, a, z) using Bayesian update rule (Eqn 7). case
Stochastic Simulation Random Action (SSRA), action selected forward simulation
picked (uniformly) random full action set. Table 4 summarizes belief expansion
procedure SSRA. First, state drawn belief distribution b. Second, action
drawn random full action set. Next, posterior state s0 drawn transition
model (s, a, s0 ). Finally, observation z drawn observation model O(s0 , a, z). Using
triple (b, a, z), calculate new belief bnew = (b, a, z) (according Equation 7),
add set belief points Bnew .
strategy better picking points random (as described above), restricts
Bnew belief tree (Fig. 2). However belief tree still large, especially
branching factor high, due large numbers actions/observations. selective
paths belief tree explored, one hope effectively restrict belief set
further.
350

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

Bnew =EXPANDSSRA (B, )
Bnew = B
Foreach b B
s=randmultinomial (b)
a=randuniform (A)
s0 =randmultinomial (T (s, a, ))
z=randmultinomial (O(s0 , a, ))
bnew = (b, a, z) (see Eqn 7)
Bnew = Bnew bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11

Table 4: Algorithm belief expansion random action selection

similar technique stochastic simulation discussed Poon (2001), however belief set initialized differently (not using b0 ), therefore stochastic simulations
restricted set reachable beliefs.
4.3 Stochastic Simulation Greedy Action (SSGA)
procedure generating points using Stochastic Simulation Greedy Action (SSGA)
based well-known -greedy exploration strategy used reinforcement learning (Sutton &
Barto, 1998). strategy similar SSRA procedure, except rather choosing
action randomly, SSEA choose greedy action (i.e., current best action given belief
b) probability 1 , chose random action probability (we use = 0.1).
action selected, perform single-step forward simulation SSRA yield new belief
point. Table 5 summarizes belief expansion procedure SSGA.
similar technique, featuring stochastic simulation using greedy actions, outlined
Hauskrecht (2000). However case, belief set included extreme points belief
simplex, stochastic simulation done extreme points, rather initial
belief.
4.4 Stochastic Simulation Exploratory Action (SSEA)
error bound Section 3 suggests PBVI performs best belief set uniformly dense
set reachable beliefs. belief point strategies proposed thus far ignore information.
next approach propose gradually expands B greedily choosing new reachable beliefs
improve worst-case density.
Unlike SSRA SSGA select single action simulate forward trajectory
given b B, Stochastic Sampling Exploratory Action (SSEA) one step forward
simulation action, thus producing new beliefs {ba0 , ba1 , ...}. However accept
new beliefs {ba0 , ba1 , ...}, rather calculates L1 distance ba closest
neighbor B. keep point ba farthest away point already B.
351

fiP INEAU , G ORDON & HRUN

Bnew =EXPANDSSGA (B, )
Bnew = B
Foreach b B
s=randmultinomial (b)
randuniform [0, 1] <
a=randuniform (A)
Else
P
a=argmax sS (s)b(s)
End
s0 =randmultinomial (T (s, a, ))
z=randmultinomial (O(s0 , a, ))
bnew = (b, a, z) (see Eqn 7)
Bnew = Bnew bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

Table 5: Algorithm belief expansion greedy action selection

use L1 norm calculate distance belief points consistent error bound
Theorem 3.1. Table 6 summarizes SSEA expansion procedure.
Bnew =EXPANDSSEA (B, )
Bnew = B
Foreach b B
Foreach
s=randmultinomial (b)
s0 =randmultinomial (T (s, a, ))
z=randmultinomial (O(s0 , a, ))
ba = (b, a, z) (see Eqn 7)
End
P
bnew = maxaA minb0 Bnew sS |ba (s) b0 (s)|
Bnew = Bnew bnew (see Eqn 7)
End
Return Bnew

1
2
3
4
5
6
7
8
9
10
11
12
13

Table 6: Algorithm belief expansion exploratory action selection

4.5 Greedy Error Reduction (GER)
SSEA strategy able improve worst-case density reachable beliefs,
directly minimize expected error. would directly minimize
352

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

error, measure bound error (Lemma 1). therefore propose final strategy
greedily adds candidate beliefs effectively reduce error bound.
empirical results, presented below, show strategy successful one discovered
thus far.
understand expand belief set GER strategy, useful re-consider
belief tree, reproduce Figure 3. node tree corresponds specific belief.
divide nodes three sets. Set 1 includes belief points already B,
case b0 ba0 z0 . Set 2 contains belief points immediate descendants points
B (i.e., nodes grey zone). candidates select new
points added B. call set envelope (denoted B). Set 3 contains reachable
beliefs.

b0

...
ba z ba z

0 q

1 q

...

...

1 1

...
...

0

0 a0 z0

...

...

ba z

1 0

...
ba z

...

ba z ba z
p 0

p 1

ba z

p q

...

0 1

ba z ba z

...
...

0 0

...

ba z

ba z

0 0 ap zq

...

...

Figure 3: set reachable beliefs
need decide belief b removed envelope B added set
active belief points B. Every point added B improve estimate value
function. new point reduce error bounds (as defined Section 3 points
already B; however, error bound new point might quite large. means
largest error bound points B monotonically decrease; however, particular
point B (such initial belief b0 ) error bound decreasing.
find point reduce error bound, look analysis
Lemma 1. Lemma 1 bounds amount additional error single point-based backup introduces. Write b0 new belief considering adding, write b belief
already B. Write value hyper-plane b, write 0 b0 . lemma
points out,
(b0 ) (0 ) (b0 b)
evaluating error, need minimize b B. Also, since know
0 done backups b0 , make conservative assumption choose
worst-case value 0 [Rmin /(1 ), Rmax /(1 )]|S| . Thus, evaluate:
(

(b0 ) min
bB

X
sS

max
(s))(b0 (s) b(s)) b0 (s) b(s)
( R1
Rmin
( 1 (s))(b0 (s) b(s)) b0 (s) < b(s)

353

(28)

fiP INEAU , G ORDON & HRUN

one could simply pick candidate b0 B currently largest error bound,3
would ignore reachability considerations. Rather, evaluate error b B,
weighing error fringe nodes reachability probability:
(b0 ),

X

(b) = max
aA

O(b, a, z) ( (b, a, z))


X

= max
aA

(29)

zZ


XX


zZ

(s, a, s0 )O(s0 , a, z)b(s) ( (b, a, z)),

sS s0

noting (b, a, z) B, ( (b, a, z)) evaluated according Equation 28.
Using Equation 29, find existing point b B largest error bound.
directly reduce error adding set one descendants. select next-step belief
(b, a, z) maximizes error bound reduction:
B

=

B (b, a, z),

b, := argmax

(30)
X

O(b, a, z) ( (b, a, z))

(31)

bB,aA zZ

z := argmax O(b, a, z) ( (b, a, z))

(32)

zZ

Table 7 summarizes GER approach belief point selection.
Bnew =EXPANDGER (B, )
Bnew = B
N =|B|
= 1 : N
P
b, := argmaxbB,aA zZ O(b, a, z) ( (b, a, z))
z := argmaxzZ O(b, a, z) ( (b, a, z))
bnew = (b, a, z)
Bnew = Bnew bnew
End
Return Bnew

1
2
3
4
5
6
7
8
9
10

Table 7: Algorithm belief expansion

complexity adding one new points GER O(SAZB) (where S=#states,
A=#actions, Z=#observations, B=#beliefs already selected). comparison, value backup (for
one point) O(S 2 AZB), point typically needs updated several times. point
empirical results below, belief selection (even GER) takes minimal time compared
value backup.
concludes presentation belief selection techniques PBVI framework.
summary, three factors consider picking belief point: (1) likely
3. tried this, however perform well empirically suggest Equation 29,
consider probability reaching belief.

354

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

occur? (2) far belief points already selected? (3) current approximate
value point? simplest heuristic (RA) accounts none these, whereas
others (SSRA, SSGA, SSEA) account one, GER incorporates three factors.
4.6 Belief Expansion Example
consider simple example, shown Figure 4, illustrate difference various
belief expansion techniques outlined above. 1D POMDP (Littman, 1996) four states, one
goal (indicated star). two actions, left right, expected
(deterministic) effect. goal state fully observable (observation=goal), three
states aliased (observation=none). reward +1 received goal state,
otherwise reward zero. assume discount factor = 0.75. initial distribution
uniform non-goal states, system resets distribution whenever goal reached.

Figure 4: 1D POMDP
belief set B always initialized contain initial belief b0 . Figure 5 shows part
belief tree, including original belief set (top node), envelope (leaf nodes).
consider belief expansion method might do.
b0=[ 1/ 3 1/ 3 0 1/ 3 ]
a=left

a=right

[ 2/ 3 0 1/ 3 0 ]

Pr(z=none)=2/3

b1=[ 1 0 0 0 ]

[ 0 1/ 3 1/ 3 1/ 3 ]

Pr(z=goal) = 1/3

Pr(z=none)=2/3

b2=[ 0 0 1 0 ]

b3=[ 0 0.5 0 0.5 ]

Pr(z=goal) = 1/3

b4=[ 0 0 1 0 ]

Figure 5: 1D POMDP belief tree
Random heuristic pick belief point (with equal probability) entire belief
simplex. directly expand branches belief tree, eventually put samples
nearby.
Stochastic Simulation Random Action 50% chance picking action.
Then, regardless action picked, theres 2/3 chance seeing observation none,
1/3 chance seeing observation goal. result, SSRA select: P r(bnew = b1) = 0.5 32 ,
P r(bnew = b2) = 0.5 13 , P r(bnew = b3) = 0.5 23 , P r(bnew = b4) = 0.5 31 .
355

fiP INEAU , G ORDON & HRUN

Stochastic Simulation Greedy Action first needs know policy b0 .
iterations point-based updates (Section 2.4) applied initial (single point) belief set reveal
(b0 ) = lef t.4 result, expansion belief greedily select action lef proba
bility 1 + |A|
= 0.95 (assuming = 0.1 |A| = 2). Action right selected belief

expansion probability |A|
= 0.05. Combining along observation probabilities,
tell SSGA expand follows: P r(bnew = b1) = 0.95 32 , P r(bnew = b2) = 0.95 13 ,
P r(bnew = b3) = 0.05 23 , P r(bnew = b4) = 0.05 13 .
Predicting choice Stochastic Simulation Exploratory Action slightly complicated. Four cases occur, depending outcomes random forward simulation b0 :
1. action left goes b1 (P r = 2/3) action right goes b3 (P r = 2/3), b1
selected ||b0 b1 ||1 = 4/3 whereas ||b0 b3 ||1 = 2/3. case occur
P r = 4/9.
2. action left goes b1 (P r = 2/3) action right goes b4 (P r = 1/3), b4
selected ||b0 b4 ||1 = 2. case occur P r = 2/9.
3. action left goes b2 (P r = 1/3) action right goes b3 (P r = 2/3), b2
selected ||b0 b2 ||1 = 2. case occur P r = 2/9.
4. action left goes b2 (P r = 1/3) action right goes b4 (P r = 1/3), either
selected (since equidistant b0 ). case b2 b4 P r = 1/18
selected.
told, P r(bnew = b1) = 4/9, P r(bnew = b2) = 5/18, P r(bnew = b3) = 0, P r(bnew = b4) =
5/18.
looking belief expansion using Greedy Error Reduction, need compute
error ( (b0 , a, z)), a, z. consider Equation 28: since B one point, b0 , necessarily b = b0 . estimate , apply multiple steps value backup b0 obtain
= [0.94 0.94 0.92 1.74]. Using b such, estimate error candidate belief: (b1 ) = 2.93, (b2 ) = 4.28, (b3 ) = 1.20, (b4 ) = 4.28. Note B
one point, dominating factor distance b0 . Next, factor observation
probabilities, Eqns 31-32, allows us determine = lef z = none,
therefore select bnew = b1 .
summary, note SSGA, SSEA GER favor selecting b1 , whereas SSRA picks
option equal probability (considering b2 b4 actually same). general,
problem size, reasonable expand entire belief tree. techniques
discussed quickly, except RA pick exact nodes belief
tree, select equally good nearby beliefs. example provided simply illustrate
different choices made strategy.

5. Review Point-Based Approaches POMDP Solving
previous section describes new class point-based algorithms POMDP solving. idea
using point-based updates POMDPs explored previously literature,
4. may obvious reader, follows directly repeated application equations 2023.

356

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

section summarize main results. approaches discussed below, procedure
updating value function given point remains unchanged (as outlined Section 2.4).
Rather, approaches mainly differentiated belief points selected,
updates ordered.
5.1 Exact Point-Based Algorithms
earlier exact POMDP techniques use point-based backups optimize value function limited regions belief simplex (Sondik, 1971; Cheng, 1988). techniques
typically require solving multiple linear programs find candidate belief points value
function sub-optimal, expensive operation. Furthermore, guarantee exact solution found, relevant beliefs must generated systematically, meaning reachable
beliefs must considered. result, methods typically cannot scale beyond handful
states/actions/observations.
work Zhang Zhang (2001), point-based updates interleaved standard dynamic programming updates accelerate planning. case points generated
systematically, rather backups applied set witness points LP points.
witness points identified result standard dynamic programming updates, whereas
LP points identified solving linear programs identify beliefs value
yet improved. procedures significantly expensive belief selection heuristics presented paper results limited domains dozen
states/actions/observations. Nonetheless approach guaranteed converge optimal solution.
5.2 Grid-Based Approximations
exists many approaches approximate value function using finite set belief points
along values. points often distributed according grid pattern belief
space, thus name grid-based approximation. interpolation-extrapolation rule specifies
value non-grid points function value neighboring grid-points. approaches
ignore convexity POMDP value function.
Performing value backups grid-points relatively straightforward: dynamic programming
updates specified Equation 11 adapted grid-points simple polynomial-time
algorithm. Given set grid points G, value bG G defined by:
"
G

V (b ) = max


#
X

X

G

b (s)R(s, a) +

sS

P r(z | a, b)V ( (b, a, z)) .

(33)

zZ

(b, a, z) part grid, V ( (b, a, z)) defined value backups. Otherwise,
V ( (b, a, z)) approximated using interpolation rule as:
V ( (b, a, z) =

|G|
X

(i)V (bG
),

(34)

i=1

P|G|

(i) 0 i=1 (i) = 1. produces convex combination grid-points.
two interesting questions respect grid-based approximations (1) calculate
interpolation function; (2) select grid points.
357

fiP INEAU , G ORDON & HRUN

general, find interpolation leads best value function approximation point
b requires solving following linear program:
Minimize

|G|
X

(i)V (bG
)

(35)

i=1

Subject

b=

|G|
X

(i)bG


(36)

i=1
|G|
X

(i) = 1

(37)

i=1

0 (i) 1, 1 |G|.

(38)

Different approaches proposed select grid points. Lovejoy (1991a) constructs
fixed-resolution regular grid entire belief space. benefit value interpolations
calculated quickly considering neighboring grid-points. disadvantage number
grid points grows exponentially dimensionality belief (i.e., number
states). simpler approach would select random points belief space (Hauskrecht,
1997). requires slower interpolation estimating value new points.
methods less ideal beliefs encountered uniformly distributed.
particular, many problems characterized dense beliefs edges simplex (i.e.,
probability mass focused states, states zero probability), low
belief density middle simplex. distribution grid-points better reflects
actual distribution belief points therefore preferable.
Alternately, Hauskrecht (1997) proposes using corner points belief simplex (e.g.,
[1 0 0 . . . ], [0 1 0 . . . ], . . . , [0 0 0 . . . 1]), generating additional successor belief points
one-step stochastic simulations (Eqn 7) corner points. proposes approximate
interpolation algorithm uses values |S|1 critical points plus one non-critical point
grid. alternative approach Brafman (1997), builds grid starting
critical points belief simplex, uses heuristic estimate usefulness gradually
adding intermediate points (e.g., bk = 0.5bi + 0.5bj , pair points). Hauskrechts
Brafmans methodsgenerally referred non-regular grid approximationsrequire fewer
points Lovejoys regular grid approach. However interpolation rule used calculate
value non-grid points typically expensive compute, since involves searching
grid points, rather neighboring sub-simplex.
Zhou Hansen (2001) propose grid-based approximation combines advantages
regular non-regular grids. idea sub-sample regular fixed-resolution grid
proposed Lovejoy. gives variable resolution grid since parts beliefs
densely sampled others restricting grid points lie fixed-resolution grid
approach guarantee fast value interpolation non-grid points. Nonetheless, algorithm
often requires large number grid points achieve good performance.
Finally, Bonet (2002) proposes first grid-based algorithm POMDPs -optimality
(for > 0). approach requires thorough coverage belief space every point
within grid-point. value update grid point fast implement, since
interpolation rule depends nearest neighbor one-step successor belief
grid point (which pre-computed). main limitation fact -coverage belief
358

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

space attained using exponentially many grid points. Furthermore, method
requires good coverage entire belief space, opposed algorithms Section 4,
focus coverage reachable beliefs.
5.3 Approximate Point-Based Algorithms
similar PBVI-class algorithms approaches update value
gradient grid point (Lovejoy, 1991a; Hauskrecht, 2000; Poon, 2001). methods able
preserve piecewise linearity convexity value function, define value function
entire belief simplex. methods use random beliefs, and/or require inclusion large number fixed beliefs corners probability simplex. contrast,
PBVI-class algorithms propose (with exception PBVI+RA) select reachable beliefs,
particular belief points improve error bounds quickly possible. idea
using reachability analysis (also known stochastic simulation) generate new points explored earlier approaches (Hauskrecht, 2000; Poon, 2001). However analysis
indicated stochastic simulation superior random point placements. re-visit
question (and conclude otherwise) empirical evaluation presented below.
recently, technique closely related PBVI called Perseus proposed (Vlassis
& Spaan, 2004; Spaan & Vlassis, 2005). Perseus uses point-based backups similar ones
used PBVI, two approaches differ two ways. First, Perseus uses randomly generated
trajectories belief space select set belief points. contrast beliefpoint selection heuristics outlined PBVI. Second, whereas PBVI systematically updates
value belief points every epoch value iteration, Perseus selects subset points
update every epoch. method used select points following: points randomly
sampled one time value updated. continues value points
improved. insight resides observing updating -vector one point often
improves value estimate nearby points (which removed sampling set).
approach conceptually simple empirically effective.
HSVI algorithm (Smith & Simmons, 2004) another point-based algorithm, differs
PBVI picks belief points, orders value updates. maintains lower
upper bound value function approximation, uses select belief points.
updating upper bound requires solving linear programs generally expensive
step. ordering value update follows: whenever belief point expanded
belief tree, HSVI updates value direct ancestors (parents, grand-parents, etc.,
way back initial belief head node). contrast PBVI performs batch
belief point expansions, followed batch value updates points. respects,
HSVI PBVI share many similarities: offer anytime performance, theoretical guarantees,
scalability; finally HSVI takes reachability account. evaluate empirical
differences HSVI PBVI next section.
Finally, RTBSS algorithm (Paquet, 2005) offers online version point-based algorithms.
idea construct belief reachability tree similar Figure 2, using current belief
top node, terminating tree fixed depth d. value node
computed recursively finite planning horizon d. algorithm eliminate subtrees
calculating bound value, comparing value computed subtrees.
RTBSS fact combined offline algorithms PBVI, offline algorithm
359

fiP INEAU , G ORDON & HRUN

used pre-compute lower bound exact value function; used increase subtree
pruning, thereby increasing depth online tree construction thus quality
solution. online algorithm yield fast results large POMDP domains. However
overall solution quality achieve error guarantees offline approaches.

6. Experimental Evaluation
section looks variety simulated POMDP domains evaluate empirical performance
PBVI. first three domainsTiger-grid, Hallway, Hallway2are extracted established POMDP literature (Cassandra, 1999). fourthTagwas introduced
earlier work new challenge POMDP algorithms.
first goal experiments establish scalability PBVI framework;
accomplished showing PBVI-type algorithms successfully solve problems excess
800 states. demonstrate PBVI algorithms compare favorably alternative approximate
value iteration methods. Finally, following example Section 4.6, study larger scale
impact belief selection strategy, confirms superior performance GER
strategy.
6.1 Maze Problems
exists set benchmark problems commonly used evaluate POMDP planning algorithms (Cassandra, 1999). section presents results demonstrating performance PBVIclass algorithms problems. benchmark problems relatively small
(at 92 states, 5 actions, 17 observations) compared robotics planning domains,
useful analysis point view comparison previous work.
initial performance analysis focuses three well-known problems POMDP literature: Tiger-grid (also known Maze33), Hallway, Hallway2. three maze navigation
problems various sizes. problems fully described Littman, Cassandra, Kaelbling
(1995a); parameterization available Cassandra (1999).
Figure 6a presents results Tiger-grid domain. Replicating earlier experiments Brafman (1997), test runs terminate 500 steps (theres automatic reset every time goal
reached) results averaged 151 runs.
Figures 6b 6c present results Hallway Hallway2 domains, respectively.
case, test runs terminated goal reached 251 steps (whichever occurs first),
results averaged 251 runs. consistent earlier experiments Littman,
Cassandra, Kaelbling (1995b).
three figures compare performance three different algorithms:
1. PBVI Greedy Error Reduction (GER) belief point selection (Section 4.5).
2. QMDP (Littman et al., 1995b),
3. Incremental Pruning (Cassandra, Littman, & Zhang, 1997),
QMDP heuristic (Littman et al., 1995b) takes account partial observability current step, assumes full observability subsequent steps:
QM DP (b) = argmax
aA

360

X
sS

b(s)QM DP (s, a).

(39)

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

resulting policy ability resolve uncertainty, cannot benefit long-term
information gathering, compare actions different information potential. QMDP seen
providing good performance baseline. three problems considered, finds policy
extremely quickly, policy clearly sub-optimal.
end spectrum, Incremental Pruning algorithm (Zhang & Liu, 1996; Cassandra et al., 1997) direct extension enumeration algorithm described above. principal insight pruning dominated -vectors (Eqn 19) interleaved directly
cross-sum operator (Eqn 16). resulting value function same, algorithm
efficient discards unnecessary vectors earlier on. Incremental Pruning algorithm
theoretically find optimal policy, three problems considered would take far
long. fact, iterations exact backups completed reasonable time. three
problems, resulting short-horizon policy worse corresponding PBVI policy.
shown Figure 6, PBVI+GER provides much better time/performance trade-off. finds
policies better obtained QMDP, matter seconds, thereby
demonstrating suffer paralyzing complexity Incremental Pruning.
take closer look results may surprised see performance
PBVI actually decreases points (e.g., dip Fig. 6c), unexpected.
important remember theoretical properties PBVI guarantee bound
estimate value function, shown here, necessarily imply policy
needs improve monotonically. Nonetheless, value function converges, policy
(albeit slower rate).
6.2 Tag Problem
previous section establishes good performance PBVI well-known simulation problems, quite small fully demonstrate scalability algorithm.
provide better understanding PBVIs effectiveness large problems, section presents
results obtained applying PBVI Tag problem, robot version popular game
lasertag. problem, agent must navigate environment goal searching for,
tagging, moving target (Rosencrantz, Gordon, & Thrun, 2003). Real-world versions
problem take many forms, Section 7 present similar problem domain
interactive service robot must find elderly patient roaming corridors nursing home.
synthetic scenario considered order magnitude larger (870 states)
POMDP benchmarks literature (Cassandra, 1999). formulated POMDP problem, goal robot optimize policy allowing quickly find person, assuming
person moves (stochastically) according fixed policy. spatial configuration
environment used throughout experiment illustrated Figure 7.
state space described cross-product two position features, Robot =
{s0 , . . . , s29 } Person = {s0 , . . . , s29 , sf ound }. start independently-selected random
positions, scenario finishes Person = sf ound . robot select five actions:
{North, South, East, West, Tag}. reward 1 imposed motion action; Tag action
results +10 reward robot person cell, 10 otherwise. Throughout scenario, Robots position fully observable, Move action predictable
deterministic effect, e. g.:
P r(Robot = s10 | Robot = s0 , N orth) = 1,
361

fiP INEAU , G ORDON & HRUN

2.5

0.7
PBVI+GER
QMDP
IncPrune

PBVI+GER
QMDP
IncPrune

0.6

2

1.5

REWARD

REWARD

0.5

1

0.4
0.3
0.2

0.5
0.1
0 2
10

1

0

10

1

2

10
10
TIME (secs)

0 2
10

3

10

10

1

0

10

1

10
10
TIME (secs)

(a) Tiger-grid

2

10

3

10

(b) Hallway

0.45
0.4

PBVI+GER
QMDP
IncPrune

0.35

REWARD

0.3
0.25
0.2
0.15
0.1
0.05
0 2
10

1

0

10

1

10
TIME (secs)

2

10

10

(c) Hallway2

Figure 6: PBVI performance well-known POMDP problems. figure shows sum
discounted reward function computation time different problem domain.

26

27

28

23

24

25

20

21

22

10

11

12

13

14

15

16

17

18

19

0

1

2

3

4

5

6

7

8

9

Figure 7: Spatial configuration domain

362

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

adjacent cell direction. position person, hand,
completely unobservable unless agents cell. Meanwhile step,
person (with omniscient knowledge) moves away robot P r = 0.8 stays place
P r = 0.2, e. g.:
P r(P erson = s16 | P erson = s15 &Robot = s0 ) = 0.4
P r(P erson = s20 | P erson = s15 &Robot = s0 ) = 0.4
P r(P erson = s15 | P erson = s15 &Robot = s0 ) = 0.2.
Figure 8 shows performance PBVI Greedy Error Reduction Tag domain. Results averaged 1000 runs, using different (randomly chosen) start positions run.
QMDP approximation tested provide baseline comparison. results show gradual improvement PBVIs performance samples added (each shown data point represents
new expansion belief set value backups). confirms computation time directly related number belief points. PBVI requires fewer 100 belief points overcome
QMDP, performance keeps improving points added. Performance appears
converging approximately 250 belief points. results show PBVI-class algorithm
effectively tackle problem 870 states.
6
PBVI+GER
QMDP
8

REWARD

10
12
14
16
18
20 0
10

1

10

2

3

10
10
TIME (secs)

4

10

5

10

Figure 8: PBVI performance Tag problem. show sum discounted reward function
computation time.

problem far beyond reach Incremental Pruning algorithm. single iteration
optimal value iteration problem size could produce 1020 -vectors pruning.
Therefore, applied.
section describes one version Tag problem, used simulation purposes
work others (Braziunas & Boutilier, 2004; Poupart & Boutilier, 2004; Smith &
Simmons, 2004; Vlassis & Spaan, 2004). fact, problem re-formulated variety
ways accommodate different environments, person motion models, observation models.
Section 7 discusses variations problem using realistic robot person models,
presents results validated onboard independently developed robot simulator.
363

fiP INEAU , G ORDON & HRUN

6.3 Empirical Comparison PBVI-Class Algorithms
establish good performance PBVI+GER number problems, consider
empirical results different PBVI-class algorithms. allows us compare effects
various belief expansion heuristics. repeat experiments Tiger-grid, Hallway,
Hallway2 Tag domains, outlined above, case compare performance five
different PBVI-class algorithms:
1. PBVI+RA: PBVI belief points selected randomly belief simplex (Section 4.1).
2. PBVI+SSRA: PBVI belief points selected using stochastic simulation random action (Section 4.2).
3. PBVI+SSGA: PBVI belief points selected using stochastic simulation greedy action
(Section 4.3).
4. PBVI+SSEA: PBVI belief points selected using stochastic simulation exploratory
action (Section 4.4).
5. PBVI+GER: PBVI belief points selected using greedy error reduction (Section 4.5).
PBVI-class algorithms converge optimal value function given sufficiently large
set belief points. rate converge depends ability generally pick
useful points, leave points containing less information. Since computation time
directly proportional number belief points, algorithm best performance
generally one find good solution fewest belief points.
Figure 9 shows comparison performance five PBVI-class algorithms
enumerated four problem domains. pictures, present performance
results function computation time.5
seen results, smallest domainTiger-gridPBVI+GER similar performance random approach PBVI+RA. Hallway domain, PBVI+GER reaches nearoptimal performance earlier algorithms. Hallway2, unclear five
algorithms best, though GER seems converge earlier.
larger Tag domain, situation interesting. PBVI+GER combination
clearly superior others. reason believe PBVI+SSEA could match performance, would require order twice many points so. Nonetheless, PBVI+SSEA
performs better either PBVI+SSRA PBVI+SSGA. random heuristic (PBVI+RA),
reward improve regardless many belief points added (4000+), therefore include results. results presented Figure 9 suggest choice
belief points crucial dealing large problems. general, believe GER (and
SSEA lesser degree) superior heuristics solving domains large numbers
action/observation pairs, ability selectively chooses branches
reachability tree explore.
side note, surprised SSGAs poor performance (in comparison SSRA)
Tiger-grid Tag domains. could due poorly tuned greedy bias ,
5. Nearly identical graphs produced showing performance results function number belief points.
confirms complexity analysis showing computation time directly related number belief
points.

364

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

2.5

0.7
RA
SSRA
SSGA
SSEA
GER

0.5

1.5

REWARD

REWARD

2

0.6

1

RA
SSRA
SSGA
SSEA
GER

0.4
0.3
0.2

0.5
0.1
0 2
10

1

0

10

1

10
10
TIME (secs)

2

10

0 2
10

3

10

1

10

(a) Tiger-grid

0

1

10
TIME (secs)

2

10

10

(b) Hallway
6

0.45

REWARD

0.3

8

SSRA
SSGA
SSEA
GER

10
REWARD

0.4
0.35

RA
SSRA
SSGA
SSEA
GER

0.25
0.2
0.15

12
14
16

0.1
18

0.05
0 2
10

1

10

0

10
TIME (secs)

1

10

20 0
10

2

10

(c) Hallway2

1

10

2

3

10
10
TIME (secs)

4

10

5

10

(d) Tag

Figure 9: Belief expansion results showing execution performance function computation
time.

investigate length. Future investigations using problems larger number actions may
shed better light issue.
terms computational requirement, GER expensive compute, followed
SSEA. However cases, time perform belief expansion step generally negligible
(< 1%) compared cost value update steps. Therefore seems best use
effective (though expensive) heuristic.
PBVI framework accommodate wide variety strategies, past described
paper. example, one could extract belief points directly sampled experimental traces.
subject future investigations.
6.4 Comparative Analysis
results outlined show PBVI-type algorithms able handle wide spectrum
large-scale POMDP domains, sufficient compare performance PBVI
365

fiP INEAU , G ORDON & HRUN

QMDP Incremental Pruningthe two ends spectrumas done Section 6.1. fact
significant activity recent years development fast approximate POMDP
algorithms, worthwhile spend time comparing PBVI framework
alternative approaches. made easy fact many validated using
set problems described above.
Table 8 summarizes performance large number recent POMDP approximation algorithms, including PBVI, four target domains: Tiger-grid, Hallway, Hallway2, Tag.
algorithms listed selected based availability comparable published results available
code, cases algorithm could re-implemented easily.
compare empirical performance, terms execution performance versus planning,
set simulation domains. However often case, results show
single algorithm best solving problems. therefore compile summary
attributes characteristics algorithm, attempt tell algorithm may best
types problems. Table 8 includes (whenever possible) goal completion rates, sum
rewards, policy computation time, number required belief points, policy size (number
-vectors, number nodes finite state controllers). number belief points policy
size often identical, however latter smaller single -vector best multiple
belief points.
results marked [*] computed us 3GHz Pentium 4; results likely
computed different platforms, therefore time comparisons may approximate best.
Nonetheless number samples size final policy useful indicators
computation time. results reported PBVI correspond earliest data point Figures 6 8 PBVI+GER achieves top performance.
Algorithms listed order performance, starting algorithm(s) achieving highest reward. results assume standard (not lookahead) controller (see Hauskrecht, 2000,
definition).
Overall, results indicate algorithms achieve sub-par performance terms
expected reward. case QMDP, fundamental limitations algorithm.
Incremental Pruning exact value-directed compression theoretically reach optimal
performance, would require longer computation time so. grid method (see Tiger-grid
results), BPI (see Tiger-grid, Hallway Tag results) PBUA (see Tag results) suffer
similar problem, offer much graceful performance degradation. worth noting none
approaches assumes known initial belief, effect solving harder problems.
results BBSLS sufficiently extensive comment length, appears able
find reasonable policies small controllers (see Tag results).
remaining algorithmsHSVI, Perseus, PBVI+GERall offer comparable
performance relatively large POMDP domains. HSVI seems offer good control performance full range tasks, requires bigger controllers, therefore probably slower,
especially domains high stochasticity (e.g., Tiger-grid, Hallway, Hallway2). trade-offs
Perseus PBVI+GER less clear: planning time, controller size performance
quality quite comparable, fact two approaches similar. Similarities
differences two approaches explored Section 7.

366

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

Method
Tiger-Grid (Maze33)
HSVI (Smith & Simmons, 2004)
Perseus (Vlassis & Spaan, 2004)
PBUA (Poon, 2001)
PBVI+GER[*]
BPI (Poupart & Boutilier, 2004)
Grid (Brafman, 1997)
QMDP (Littman et al., 1995b)[*]
IncPrune (Cassandra et al., 1997)[*]
Exact VDC (Poupart & Boutilier, 2003)[*]
Hallway
PBUA (Poon, 2001)
HSVI (Smith & Simmons, 2004)
PBVI+GER[*]
Perseus (Vlassis & Spaan, 2004)
BPI (Poupart & Boutilier, 2004)
QMDP (Littman et al., 1995b)[*]
Exact VDC (Poupart & Boutilier, 2003)[*]
IncPrune (Cassandra et al., 1997)[*]
Hallway2
PBVI+GER[*]
Perseus (Vlassis & Spaan, 2004)
HSVI (Smith & Simmons, 2004)
PBUA (Poon, 2001)
BPI (Poupart & Boutilier, 2004)
Grid (Brafman, 1997)
QMDP (Littman et al., 1995b)[*]
Exact VDC (Poupart & Boutilier, 2003)[*]
IncPrune (Cassandra et al., 1997)[*]
Tag
HSVI (Smith & Simmons, 2004)
PBVI+GER[*]
Perseus (Vlassis & Spaan, 2004)
BBSLS (Braziunas & Boutilier, 2004)
BPI (Poupart & Boutilier, 2004)
QMDP (Littman et al., 1995b)[*]
PBUA (Poon, 2001)[*]
IncPrune (Cassandra et al., 1997)[*]

Goal%

Reward Conf.Int.

Time(s)

|B|

||

n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.
n.a.

2.35
2.34
2.30
2.27 0.13
1.81
0.94
0.276
0.0
0.0

10341
104
12116
397
163420
n.v.
0.02
24hrs+
24hrs+

n.v.
10000
660
512
n.a.
174
n.a.
n.a.
n.a.

4860
134
n.v.
508
1500
n.a.
5
n.v.
n.v.

100
100
100
n.v.
n.v.
51
39
39

0.53
0.52
0.51 0.03
0.51
0.51
0.265
0.161
0.161

450
10836
19
35
249730
0.03
24hrs+
24hrs+

300
n.v.
64
10000
n.a.
n.a.
n.a.
n.a.

n.v.
1341
64
55
1500
5
n.v.
n.v.

100
n.v.
100
100
n.v.
98
22
48
48

0.37 0.04
0.35
0.35
0.35
0.28
n.v.
0.109
0.137
0.137

6
10
10010
27898
274280
n.v.
1.44
24hrs+
24hrs+

32
10000
n.v.
1840
n.a.
337
n.a.
n.a.
n.a.

31
56
1571
n.v.
1500
n.a.
5
n.v.
n.v.

100
100
n.v.
n.v.
n.v.
19
0
0

-6.37
-6.75 0.39
-6.85
-8.31
-9.18
-16.62
-19.9
-19.9

10113
8946
3076
100054
59772
1.33
24hrs+
24hrs+

n.v.
256
10000
n.a.
n.a.
n.a.
4096
n.a.

1657
203
205
30
940
5
n.v.
n.v.

n.a.=not applicable

n.v.=not available

[*]=results computed us

Table 8: Results PBVI standard POMDP domains

367

fiP INEAU , G ORDON & HRUN

6.5 Error Estimates
results presented thus far suggest PBVI framework performs best using
Greedy Error Reduction (GER) technique selecting belief points. scheme, decide belief points included, estimate error bound set candidate points
pick one largest error estimate. error bound estimated described
Equation 28. consider question estimate evolves points
added. natural intuition first points, error estimates large,
density belief set increases, error estimates become much smaller.
Figure 10 reconsiders four target domains: Tiger-grid, Hallway, Hallway2 Tag.
case, present reward performance function number belief points (top
row graphs), error estimate point selected according order points
picked (bottom row graphs). addition, bottom graphs show (in dashed line) trivial
Rmin
bound error ||Vt Vt || Rmax1
, valid t-step value function arbitrary
policy. expected, bound typically tighter trivial bound. Tag, occurs
number belief points exceeds number states, surprising, given
bound depends distance reachable beliefs, states reachable beliefs
domain. Overall, seems reasonably good correspondence improvement
performance decrease error estimates. conclude figure even
though PBVI error quite loose, fact informative guiding exploration belief
simplex.
note significant variance error estimates one belief point next,
illustrated non-monotonic behavior curves bottom graphs Figure 10.
behavior attributed possibilities. First, fact error estimate
given belief approximate. value function used calculate error estimate
approximate. addition, fact new belief points always selected
envelope reachable beliefs, set reachable beliefs. suggests GER could
improved maintaining deeper envelope candidate belief points. Currently envelope
contains points 1-step forward simulations points already selected. may
useful consider points 23 steps ahead. predict would reduce jaggedness seen
Figure 10, importantly, reduce number points necessary good performance.
course, tradeoff time spent selecting points time spent planning would
re-evaluated light.

7. Robotic Applications
overall motivation behind work described paper desire provide high-quality
robust planning real-world autonomous systems, particular robots. practical side, search robust robot controller large part guided Nursebot
project (Pineau, Montermerlo, Pollack, Roy, & Thrun, 2003). overall goal project
develop personalized robotic technology play active role providing improved care
services non-institutionalized elderly people. Pearl, shown Figure 11, main robotic
platform used project.
many services nursing-assistant robot could provide (Engelberger, 1999; Lacey
& Dawson-Howe, 1998), much work date focused providing timely cognitive reminders (e.g., medications take, appointments attend, etc.) elderly subjects (Pollack, 2002).
368

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

Tigergrid

Hallway

2.5

Reward

2

Hallway2

0.8

0.4

0.6

0.3

0.4

0.2

0.2

0.1

Tag
5

10

1.5
1

15

0.5
0
0
10

1

10

2

10

3

10

0
0
10

# belief points
60

1

10

2

10

0
0
10

3

10

# belief points

1

10

2

10

20

20

Error

2

10

3

10

# belief points

400

15
40

15

300

10

20

200

10
5

100

10
0
0
10

1

10

500

50

30

20
3
0
10 10

# belief points

1

10

2

10

# belief points

0
3
0
10 10

1

10

2

10

3

10

# belief points

5
0
10

1

10

2

10

# belief points

3

10

0
0
10

1

10

2

10

3

10

# belief points

Figure 10: Sum discounted reward (top graphs) estimate bound error (bottom
graphs) function number selected belief points.

Figure 11: Pearl Nursebot, interacting elderly people nursing facility
important component task finding patient whenever time issue reminder.
task shares many similarities Tag problem presented Section 6.2. case,
however, robot-generated map real physical environment used basis spatial
configuration domain. map shown Figure 12. white areas correspond free
space, black lines indicate walls (or obstacles) dark gray areas visible
accessible robot. One easily imagine patients room physiotherapy unit lying
either end corridor, common area shown upper-middle section.
overall goal robot traverse domain order find missing patient
deliver message. robot must systematically explore environment, reasoning
spatial coverage human motion patterns, order find person.
369

fiP INEAU , G ORDON & HRUN

Figure 12: Map environment
7.1 POMDP Modeling
problem domain represented jointly two state features: RobotPosition, PersonPosition.
feature expressed discretization environment. experiments
assume discretization 2 meters, means 26 discrete cells feature, total
676 states.
assumed person robot move freely throughout space. robots
motion deterministically controlled choice action (North, South, East, West). robot
fifth action (DeliverMessage), concludes scenario used appropriately (i.e.,
robot person location).
persons motion stochastic falls one two modes. Part time, person
moves according Brownian motion (e.g., moves cardinal direction P r = 0.1, otherwise stays put). times, person moves directly away robot. Tag domain
Section 6.2 assumes person always moves always moves away robot. realistic person cannot see robot. current experiment instead assumes person
moves according Brownian motion robot far away, moves away robot
closer (e.g., < 4m). person policy designed way encourage robot
find robust policy.
terms state observability, two components: robot sense
position, sense persons position. first case, assumption
robot knows position times. may seem generous (or
optimistic) assumption, substantial experience domains size maps quality
demonstrated robust localization abilities (Thrun et al., 2000). especially true
planning operates relatively coarse resolution (2 meters) compared localization precision
(10 cm). exact position information assumed planning domain, execution
phase (during actually measure performance) update belief using full localization
information, includes positional uncertainty whenever appropriate.
Regarding detection person, assumption robot knowledge
persons position unless s/he within range 2 meters. plausible given robots
sensors. However, even short-range, small probability (P r = 0.01) robot
miss person therefore return false negative.
general, one could make sensible assumptions persons likely position (e.g., based
knowledge daily activities), however currently information therefore assume uniform distribution initial positions. persons subsequent movements
370

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

expressed motion model described (i.e., mix Brownian motion purposeful avoidance).
reward function straightforward: R = 1 motion action, R = 10
robot decides DeliverMessage cell person, R = 100
robot decides DeliverMessage persons absence. task terminates robot
successfully delivers message (i.e., = DeliverM essage srobot = sperson ). assume
discount factor 0.95.
assume known initial belief, b0 , consisting uniform distribution states.
used selecting belief points planning, subsequently executing testing
final policy.
initial map (Fig. 12) domain collected mobile robot, slightly cleaned
hand remove artifacts (e.g., people walking by). assumed model parameters
described here, applied PBVI planning problem such. Value updates belief point
expansions applied alternation (in simulation) policy able find person
99% trials (trials terminated person found 100 execution steps).
final policy implemented tested onboard publicly available CARMEN robot simulator (Montemerlo, Roy, & Thrun, 2003).
7.2 Comparative Evaluation PBVI Perseus
subtask described here, 626 states, beyond capabilities exact POMDP solvers.
Furthermore, demonstrated below, MDP-type approximations equipped handle
uncertainty type exhibited task. main purpose analysis evaluate
effectiveness point-based approach described paper address problem.
results Tag domain (Section 6.2) hint fact PBVI algorithms may able
handle task, realistic map modified motion model provide new challenges.
begin investigation directly comparing performance PBVI (with GER belief points selection) Perseus algorithm complex robot domain. Perseus
described Section 5; results presented produced using code provided authors (Perseus, 2004). Results algorithms assume fixed POMDP model generated
robot simulator. model stored solved offline algorithm.
PBVI Perseus parameters set. PBVI requires: number new belief
points add expansion (Badd ) planning horizon expansion (H). Perseus
requires: number belief points generate random walk (B) maximum planning
time (T ). Results presented assume following parameter settings: Badd = 30, H = 25,
B=10,000, =1500. algorithms fairly robust changes parameters.6
Figure 13 summarizes results experiment. suggest number observations.
shown Figure 13(a), algorithms find best solution similar time,
PBVI+GER better anytime performance Perseus (e.g., much better policy found
given 100 sec).
shown Figure 13(b), algorithms require similar number -vectors.
shown Figure 13(c), PBVI+GER requires many fewer beliefs.
6. 25% change parameter value yielded sensibly similar results terms reward number vectors,
though course time, memory, number beliefs varied.

371

fiP INEAU , G ORDON & HRUN

5
10

120
PBVI+GER
GER
Perseus
QMDP

PBVI+GER
Perseus
100

15
# alpha vectors

REWARD

20
25
30
35

80
60
40

40
20
45
50 1
10

0

1

10

2

3

10
10
TIME (secs)

10

0 0
10

4

10

1

10

5

10

PBVI+GER
Perseus
4

10

# beliefs

3

10

2

10

1

10

0

10 0
10

1

10

2

10
TIME (secs)

3

10

4

10

(b)
Memory requirement = (#alphas + #beliefs)*#states

(a)

2

10
TIME (secs)

3

10

4

10

(c)

8

10

PBVI+GER
Perseus
7

10

6

10

5

10

4

10

0

10

1

10

2

10
TIME (secs)

3

10

4

10

(d)

Figure 13: Comparison PBVI Perseus robot simulation domain
requires fewer beliefs, PBVI+GER much lower memory requirements;
quantified Figure 13(d).
new results suggest PBVI Perseus similar performance objective
find near-optimal solution, time memory constrained. cases one willing
trade accuracy time, PBVI may provide superior anytime performance. cases
memory limited, PBVIs conservative approach respect belief point selection
advantageous. properties suggest PBVI may scale better large domains.
7.3 Experimental Results Robot Simulator
results presented assume POMDP model used planning testing (i.e., compute reward Figure 13(a)). useful carry large number
experiments. model however cannot entirely capture dynamics realistic robot system,
therefore concern policy learned point-based methods perform
well realistic robot. verify robustness approach, final PBVI control policy
implemented tested onboard publicly available CARMEN robot simulator (Montemerlo
et al., 2003).
372

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

resulting policy illustrated Figure 14. figure shows five snapshots obtained
single run. particular scenario, person starts far end left corridor.
persons location shown figures since observable robot.
figure instead shows belief person positions, represented distribution point samples
(grey dots Fig. 14). point represents plausible hypothesis persons position.
figure shows robot starting far right end corridor (Fig. 14a). robot moves toward
left rooms entrance (Fig. 14b). proceeds check entire room (Fig. 14c).
relatively certain person nowhere found, exits room (Fig. 14d),
moves left branch corridor, finally finds person end
corridor (Fig. 14e).
policy optimized start position (for person robot). scenario
shown Figure 14 one longer execution traces since robot ends searching entire
environment finding person. interesting compare choice action
snapshots (b) (d). robot position practically identical. Yet (b) robot chooses
go room, whereas (d) robot chooses move toward left. direct
result planning beliefs, rather states. belief distribution person positions
clearly different two cases, therefore policy specifies different course
action.
Figure 15 looks policy obtained solving problem using QMDP heuristic. Four snapshots offered different stages specific scenario, assuming person
started far left side robot far right side (Fig. 15a). proceeding
room entrance (Fig. 15b), robot continues corridor almost reaches end
(Fig. 15c). turns around comes back toward room entrance, stations
(Fig. 15d) scenario forcibly terminated. result, robot cannot find person
s/he left edge corridor room. Whats more, runningaway behavior adopted subject, even person starts elsewhere corridor,
robot approaches person gradually retreat left similarly escape robot.
Even though QMDP explicitly plan beliefs, generate different policy actions
cases state identical belief different. seen comparing Figure 15 (b) (d). these, robot identically located, however belief person
positions different. (b), probability mass left robot, therefore travels direction. (d), probability mass distributed evenly three branches
(left corridor, room, right corridor). robot equally pulled directions therefore stops
there. scenario illustrates strength QMDP. Namely, many cases
necessary explicitly reduce uncertainty. However, shows sophisticated
approaches needed handle cases.
results show PBVI perform outside bounds simple maze domains,
able handle realistic problem domains. particular, throughout evaluation, robot
simulator way constrained behave described POMDP model (Sec. 7.1).
means robots actions often stochastic effects, robots position always
fully observable, belief tracking performed asynchronously (i.e., always
straightforward ordering actions observations). Despite misalignment model
assumed planning, execution environment, control policy optimized PBVI could
successfully used complete task.

373

fiP INEAU , G ORDON & HRUN

(a) t=1

(b) t=7

(c) t=12

(d) t=17

(e) t=29
Figure 14: Example PBVI policy successfully finding person

374

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

(a) t=1

(b) t=7

(c) t=17

(d) t=27
Figure 15: Example QMDP policy failing find person

375

fiP INEAU , G ORDON & HRUN

8. Discussion
paper describes class anytime point-based POMDP algorithms called PBVI, combines point-based value updates strategic selection belief points, solve large POMDPs.
extensions PBVI framework, whereby value updates applied groups belief
points according spatial distribution, described (Pineau, Gordon, & Thrun, 2004).
main contributions pertaining PBVI framework summarized.
Scalability. PBVI framework important step towards truly scalable POMDP solutions.
achieved bounding policy size selection small set belief points.
Anytime planning. PBVI-class algorithms alternates steps value updating steps
belief point selection. new points added, solution improves, expense increased
computational time. trade-off controlled adjusting number points. algorithm terminated either satisfactory solution found, planning time
elapsed.
Bounded error. provide theoretical bound error approximation introduced
PBVI framework. result holds range belief point selection methods, lead
directly development new PBVI-type algorithm: PBVI+GER, estimates
error bound used directly select belief points. Furthermore find bounds
useful assessing stop adding belief points.
Exploration. proposed set new point selection heuristics, explore tree
reachable beliefs select useful belief points. successful technique described, Greedy
Error Reduction (GER), uses estimate error bound candidate belief points select
useful points.
Improved empirical performance. PBVI demonstrated ability reduce planning time
number well-known POMDP problems, including Tiger-grid, Hallway, Hallway2.
operating set discrete points, PBVI algorithms perform polynomial-time value updates,
thereby overcoming curse history paralyzes exact algorithms. GER technique used
select points allows us solve large problems fewer belief points alternative approaches.
New problem domain. PBVI applied new POMDP planning domain (Tag),
generated approximate solution outperformed baseline algorithms QMDP Incremental
Pruning. new domain since adopted test case algorithms (Vlassis &
Spaan, 2004; Smith & Simmons, 2004; Braziunas & Boutilier, 2004; Poupart & Boutilier, 2004).
fosters increased ease comparison new techniques. comparative analysis
provided Section 7.2 highlighting similarities differences PBVI Perseus.
Demonstrated performance. PBVI applied context robotic search-and-rescue
type scenario, mobile robot required search environment find non-stationary
individual. PBVIs performance evaluated using realistic, independently-developed, robot
simulator.
significant portion paper dedicated thorough comparative analysis point-based
methods. includes evaluating range point-based selection methods, well evaluating
mechanisms ordering value updates. comparison point-based selection techniques suggest GER method presented Section 4.5 superior naive techniques. terms
ordering value updates, randomized strategy used Perseus algorithm appears
effective accelerate planning. natural next step would combine GER belief selection
heuristic Perseuss random value updates. performed experiments along lines,
376

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

achieve significant speed-up current performance PBVI Perseus (e.g.,
reported Figure 13(a)). likely belief points chosen carefully (as GER),
points needs updated systematically therefore additional benefit
using randomized value updates.
Looking towards future, important remember demonstrated
ability solve problems large POMDP standards, many real-world domains far exceed
complex domains considered paper. particular, unusual problem
expressed number multi-valued state features, case number states grows
exponentially number features. concern belief point
-vector dimensionality |S| (where |S| number states) dimensions updated
simultaneously. important issue address improve scalability point-based value
approaches general.
various existing attempts overcoming curse dimensionality POMDPs.
thesee. g. belief compression techniques Roy Gordon (2003)cannot
incorporated within PBVI framework without compromising theoretical properties (as discussed Section 3). Others, particular exact compression algorithm Poupart Boutilier
(2003), combined PBVI. However, preliminary experiments direction
yielded little performance improvement. reason believe approximate value compression would yield better results, expense forgoing PBVIs theoretical properties. challenge therefore devise function-approximation techniques reduce
dimensionality effectively, maintaining convexity properties solution.
secondary (but less important) issue concerning scalability PBVI pertains
number belief points necessary obtain good solution. problems addressed thus far
usually solved O(|S|) belief points, need true. worse case, number
belief points necessary may exponential plan length. PBVI framework accommodate wide variety strategies generating belief points, Greedy Error Reduction
technique seems particularly effective. However unlikely definitive answer belief
point selection. general terms, relates closely well-known issue exploration
versus exploitation, arises across wide array problem-solving techniques.
promising opportunities future research aside, PBVI framework already
pushed envelope POMDP problems solved existing computational resources.
field POMDPs matures, finding ways computing policies efficiently likely continue
major bottleneck. hope point based algorithms PBVI play leading
role search efficient algorithms.

Acknowledgments
authors wish thank Craig Boutilier, Michael Littman, Andrew Moore Matthew Mason
many thoughtful comments discussions regarding work. thank Darius Braziunas,
Pascal Poupart, Trey Smith Nikos Vlassis, conversations regarding algorithms
results. contributions Michael Montemerlo Nicholas Roy conducting empirical
robot evaluations gratefully acknowledged. Finally, thank three anonymous reviewers
one dedicated editor (Sridhar Mahadevan) whose feedback significantly improved paper.
work funded DARPA MARS program NSFs ITR program (Project: Robotic
Assistants Elderly, PI: J. Dunbar-Jacob).
377

fiP INEAU , G ORDON & HRUN

References
Astrom, K. J. (1965). Optimal control markov decision processes incomplete state estimation. Journal Mathematical Analysis Applications, 10, 174205.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. (1996). Neuro-Dynamic Programming. Athena Scientific.
Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90(1-2), 281300.
Bonet, B. (2002). epsilon-optimal grid-based algorithm partially obserable Markov decision
processes. Machine Learning: Proceedings 2002 International Conference (ICML),
pp. 5158.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions
computational leverage. Journal Artificial Intelligence Research, 11, 194.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes. Proceedings Fourteenth Conference Uncertainty Artificial Intelligence (UAI), pp. 3342.
Brafman, R. I. (1997). heuristic variable grid solution method POMDPs. Proceedings
Fourteenth National Conference Artificial Intelligence (AAAI), pp. 727733.
Braziunas, D., & Boutilier, C. (2004). Stochastic local search POMDP controllers. Proceedings Nineteenth National Conference Artificial Intelligence (AAAI), pp. 690696.
Burgard, W., Cremers, A. B., Fox, D., Hahnel, D., Lakemeyer, G., Schulz, D., Steiner, W., & Thrun,
S. (1999). Experiences interactive museum tour-guide robot. Artificial Intelligence,
114, 355.
Cassandra, A. (1999).
Tonys
research/ai/pomdp/code/index.html.

POMDP

page.

http://www.cs.brown.edu/

Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast, exact
method partially observable Markov decision processes. Proceedings Thirteenth
Conference Uncertainty Artificial Intelligence (UAI), pp. 5461.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32(3), 333377.
Cheng, H.-T. (1988). Algorithms Partially Observable Markov Decision Processes. Ph.D. thesis,
University British Columbia.
Dean, T., & Kanazawa, K. (1988). Probabilistic temporal reasoning. Proceedings Seventh
National Conference Artificial Intelligence (AAAI), pp. 524528.
Devroye, L. (1986). Non-Uniform Random Variate Generation. Springer-Verlag, New York.
Engelberger, G. (1999). Handbook Industrial Robotics. John Wiley Sons.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach application theorem
proving problem solving. Artificial Intelligence, 2, 189208.
Hauskrecht, M. (1997). Incremental methods computing bounds partially observable Markov
decision processes. Proceedings Fourteenth National Conference Artificial Intelligence (AAAI), pp. 734739.
378

fiA NYTIME P OINT-BASED PPROXIMATIONS L ARGE POMDP

Hauskrecht, M. (2000). Value-function approximations partially observable Markov decision
processes. Journal Artificial Intelligence Research, 13, 3394.
Jazwinski, A. M. (1970). Stochastic Processes Filtering Theory. Academic, New York.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101, 99134.
Kalman, R. E. (1960). new approach linear filtering prediction problems. Transactions
ASME, Journal Basic Engineering, 82, 3545.
Lacey, G., & Dawson-Howe, K. M. (1998). application robotics mobility aid
elderly blind. Robotics Autonomous Systems, 23, 245252.
Littman, M. L. (1996). Algorithms Sequential Decision Making. Ph.D. thesis, Brown University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995a). Learning policies partially obsevable environments: Scaling up. Tech. rep. CS-95-11, Brown University, Department
Computer Science.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995b). Learning policies partially obsevable environments: Scaling up. Proceedings Twelfth International Conference
Machine Learning, pp. 362370.
Lovejoy, W. S. (1991a). Computationally feasible bounds partially observed Markov decision
processes. Operations Research, 39(1), 162175.
Lovejoy, W. S. (1991b). survey algorithmic methods partially observable Markov decision
processes. Annals Operations Research, 28, 4766.
McAllester, D., & Roseblitt, D. (1991). Systematic nonlinear planning. Proceedings Ninth
National Conference Artificial Intelligence (AAAI), pp. 634639.
Monahan, G. E. (1982). survey partially observable Markov decision processes: Theory, models, algorithms. Management Science, 28(1), 116.
Montemerlo, M., Roy, N., & Thrun, S. (2003). Perspectives standardization mobile robot
programming: Carnegie Mellon navigation (CARMEN) toolkit. Proceedings
IEEE/RSJ International Conference Intelligent Robots Systems (IROS), Vol. 3, pp. pp
24362441.
Paquet, S. (2005). Distributed Decision-Making Task Coordination Dynamic, Uncertain
Real-Time Multiagent Environments. Ph.D. thesis, Universite Laval.
Penberthy, J. S., & Weld, D. (1992). UCPOP: sound, complete, partial order planning ADL.
Proceedings Third International Conference Knowledge Representation Reasoning, pp. 103114.
Perseus (2004) http://staff.science.uva.nl/mtjspaan/software/approx.
Pineau, J., Gordon, G., & Thrun, S. (2004). Applying metric-trees belief-point POMDPs.
Neural Information Processing Systems (NIPS), Vol. 16.
Pineau, J., Montermerlo, M., Pollack, M., Roy, N., & Thrun, S. (2003). Towards robotic assistants
nursing homes: challenges results. Robotics Autonomous Systems, 42(3-4), 271281.
Pollack, M. (2002). Planning technology intelligent cognifitve orthotics. Proceedings
6th International Conference AI Planning & Scheduling (AIPS).
379

fiP INEAU , G ORDON & HRUN

Poon, K.-M. (2001). fast heuristic algorithm decision-theoretic planning. Masters thesis,
Hong-Kong University Science Technology.
Poupart, P., & Boutilier, C. (2000). Value-directed belief state approximation POMDPs.
Proceedings Sixteenth Conference Uncertainty Artificial Intelligence (UAI), pp.
409416.
Poupart, P., & Boutilier, C. (2003). Value-directed compression POMDPs. Advances Neural
Information Processing Systems (NIPS), Vol. 15.
Poupart, P., & Boutilier, C. (2004). Bounded finite state controllers. Advances Neural Information Processing Systems (NIPS), Vol. 16.
Rabiner, L. R. (1989). tutorial hidden Markov models selected applications speech
recognition. Proceedings IEEE, 77(2), 257285.
Rosencrantz, M., Gordon, G., & Thrun, S. (2003). Locating moving entities dynamic indoor
environments teams mobile robots. Second International Joint Conference
Autonomous Agents MultiAgent Systems (AAMAS), pp. 233240.
Roy, N., & Gordon, G. (2003). Exponential family PCA belief compression POMDPs.
Advances Neural Information Processing Systems (NIPS), Vol. 15, pp. 10431049.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. Proceedings
Twentieth Conference Uncertainty Artificial Intelligence (UAI).
Sondik, E. J. (1971). Optimal Control Partially Observable Markov Processes. Ph.D. thesis,
Stanford University.
Spaan, M., & Vlassis, N. (2005). Perseus: Randomized point-based value iteration POMDPs.
Journal Artificial Intelligence Research (JAIR), 195220.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press.
Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2000). Robust Monte Carlo localization mobile
robots. Artificial Intelligence, 128(1-2), 99141.
Vlassis, N., & Spaan, M. T. J. (2004). fast point-based algorithm POMDPs. Proceedings
Belgian-Dutch Conference Machine Learning.
White, C. C. (1991). survey solution techniques partially observed Markov decision
process. Annals Operations Research, 32, 215230.
Zhang, N. L., & Liu, W. (1996). Planning stochastic domains: Problem characteristics approximation. Tech. rep. HKUST-CS96-31, Dept. Computer Science, Hong Kong University Science Technology.
Zhang, N. L., & Zhang, W. (2001). Speeding convergence value iteration partially
observable Markov decision processes. Journal Artificial Intelligence Research, 14, 29
51.
Zhou, R., & Hansen, E. A. (2001). improved grid-based approximation algorithm POMDPs.
Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI),
pp. 707716.

380


