journal artificial intelligence

submitted published

complex question answering unsupervised learning
approaches experiments
yllias chali

chali cs uleth ca

university lethbridge
lethbridge ab canada k

shafiq r joty

rjoty cs ubc ca

university british columbia
vancouver bc canada v z

sadid hasan

hasan cs uleth ca

university lethbridge
lethbridge ab canada k

abstract
complex questions require inferencing synthesizing information multiple
documents seen kind topic oriented informative multi document summarization goal produce single text compressed version set
documents minimum loss relevant information experiment
one empirical method two unsupervised statistical machine learning techniques
k means expectation maximization em computing relative importance
sentences compare approaches experiments
empirical outperforms two techniques em performs better
k means however performance approaches depends entirely feature
set used weighting features order measure importance
relevance user query extract different kinds features e lexical lexical semantic cosine similarity basic element tree kernel syntactic shallow semantic
document sentences use local search technique learn weights
features best knowledge study used tree kernel functions
encode syntactic semantic information complex tasks computing
relatedness query sentences document sentences order generate
query focused summaries answers complex questions methods
generating summaries e empirical k means em effects syntactic
shallow semantic features bag words bow features

introduction
vast increase amount online text available demand access different types information led renewed interest broad range information
retrieval ir related areas go beyond simple document retrieval areas
include question answering topic detection tracking summarization multimedia retrieval chemical biological informatics text structuring text mining genomics etc
automated question answering qa ability machine answer questions simple
complex posed ordinary human languageis perhaps exciting technological development past six seven years strzalkowski harabagiu
c

ai access foundation rights reserved

fichali joty hasan

expectations already tremendous reaching beyond discipline subfield natural
language processing nlp
tool finding documents web search engines proven adequate
although limitation expressiveness user terms query formulation certain limitations exist search engine query complex
question answering tasks require multi document summarization aggregated
search faceted search represents information need cannot answered
single document example look comparison average number
years marriage first birth women u asia europe answer
likely contained multiple documents multi document summarization useful
type query currently tool market designed meet
kind information need
qa attempts deal wide range question types including fact list
definition hypothetical semantically constrained cross lingual questions
questions call simple questions easier answer example
question president bangladesh asks persons name type
question e factoid requires small snippets text answer question
countries pope john paul ii visited sample list question asking
list small snippets text
made substantial headway factoid list questions researchers
turned attention complex information needs cannot answered
simply extracting named entities persons organizations locations dates etc documents unlike informationally simple factoid questions complex questions often seek multiple different types information simultaneously presuppose one single
answer meet information needs example factoid question
accurate hiv tests safely assumed submitter question looking number range numbers however complex questions
causes aids wider focus question suggests submitter
may single well defined information need therefore may amenable
receiving additional supporting information relevant yet undefined informational goal harabagiu lacatusu hickl questions require inferencing
synthesizing information multiple documents
well known qa systems korean navers knowledge search
pioneers community qa tool allows users ask question get
answers users navers knowledge roughly times entries
wikipedia used millions korean web users given day people
say koreans addicted internet naver january knowledge search database included million user generated information
another popular answer service yahoo answers community driven knowledge market website launched yahoo allows users submit questions
answered answer questions users people vote best answer site
gives members chance earn points way encourage participation
naver model december yahoo answers million users
http kin naver com



ficomplex question answering unsupervised approaches

million answers google qa system paid editors launched
april fully closed december
however computational linguistics point view information synthesis
seen kind topic oriented informative multi document summarization goal
produce single text compressed version set documents minimum loss
relevant information unlike indicative summaries help determine whether
document relevant particular topic informative summaries must attempt
answers
focus extractive summarization subset
sentences original documents chosen contrasts abstractive summarization information text rephrased although summaries produced
humans typically extractive state art summarization systems
extraction achieve better automated abstraction
experimented one empirical two well known unsupervised statistical machine
learning techniques k means em evaluated performance generating topicoriented summaries however performance approaches depends entirely
feature set used weighting features order measure importance
relevance user query extract different kinds features e lexical lexical
semantic cosine similarity basic element tree kernel syntactic shallow semantic
document sentences used gradient descent local search technique
learn weights features
traditionally information extraction techniques bow augmented language modeling task requires use complex semantics approaches bow often inadequate perform fine level textual
analysis improvements bow given use dependency trees syntactic parse trees hirao suzuki isozaki maeda punyakanok roth yih
zhang lee b adequate dealing complex questions
whose answers expressed long articulated sentences even paragraphs shallow
semantic representations bearing compact information could prevent sparseness
deep structural approaches weakness bow moschitti quarteroni
basili manandhar pinpointing answer question relies deep understanding semantics attempting application syntactic semantic
information complex qa seems natural best knowledge study used
tree kernel functions encode syntactic semantic information complex tasks
computing relatedness query sentences document sentences
order generate query focused summaries answers complex questions
methods generating summaries e empirical k means em effects
syntactic shallow semantic features bow features
past three years complex questions focus much attention
automatic question answering multi document summarization mds communities typically current complex qa evaluations including aquaint
relationship qa pilot text retrieval conference trec relationship qa task
trec definition others require systems return unstructured lists http answers google com



fichali joty hasan

didate answers response complex question however recently mds evaluations including document understanding conference duc tasked
systems returning paragraph length answers complex questions responsive
relevant coherent
experiments duc data including syntactic semantic features improves performance comparison among approaches shown
comparing duc participants systems achieve top scores
statistically significant difference system duc
best system
organized follows section focuses related work section
gives brief description intended final model section describes features
extracted section discusses learning issues presents learning approaches
section discusses remove redundant sentences adding final
summary section describes experimental study conclude discuss future
directions section

related work
researchers world working query summarization trying different
directions see methods provide best
number sentence retrieval systems ir information retrieval
techniques systems typically dont use lot linguistic information still
deserve special attention murdock croft propose translation model specifically
monolingual data significantly improves sentence retrieval query
likelihood translation train parallel corpus used corpus question answer pairs losada presents comparison multiple bernoulli
multinomial context sentence retrieval task shows multivariate bernoulli model really outperform popular multinomial retrieving
relevant sentences losada fernandez propose novel sentence retrieval method
extracting highly frequent terms top retrieved documents reinforce idea top retrieved data valuable source enhance retrieval systems
specially true short queries usually query sentence matching terms argue method improves significantly precision top ranks
handling poorly specified information needs
lexrank method addressed erkan radev successful
generic multi document summarization topic sensitive lexrank proposed otterbacher erkan radev lexrank set sentences document cluster
represented graph nodes sentences links nodes induced similarity relation sentences system ranks sentences
according random walk model defined terms inter sentence similarities
similarities sentences topic description question
concepts coherence cohesion enable us capture theme text coherence represents overall structure multi sentence text terms macro level
relations clauses sentences halliday hasan cohesion defined
halliday hasan property holding text together one single grammat

ficomplex question answering unsupervised approaches

ical unit relations e ellipsis conjunction substitution reference lexical
cohesion elements text lexical cohesion defined cohesion
arises semantic relations collocation repetition synonym hypernym hyponym holonym meronym etc words text morris hirst
lexical cohesion among words represented lexical chains sequences
semantically related words summarization methods lexical chain first extract nouns compound nouns named entities candidate words li sun kit
webster wordnet systems semantic similarity
nouns compound nouns lexical chains built two steps
building single document strong chains disambiguating senses words
building multi chain merging strongest chains single documents
one chain
systems rank sentences formula involves lexical chain b keywords query c named entities example li et al uses following
formula
score p chain p query p namedentity
p chain sum scores chains whose words come
candidate sentence p query sum co occurrences key words topic
sentence p namedentity number name entities existing topic
sentence three coefficients set empirically top ranked
sentences selected form summary
harabagiu et al introduce paradigm processing complex questions
relies combination question decompositions b factoid qa techniques
c multi document summarization mds techniques question decomposition
procedure operates markov chain following random walk mixture
model bipartite graph relations established concepts related topic
complex question subquestions derived topic relevant passages manifest
relations decomposed questions submitted state art qa system
order retrieve set passages later merged comprehensive answer mds system question decompositions method
significantly enhance relevance comprehensiveness summary length answers
complex questions
approaches probabilistic pingali k varma
toutanova brockett gamon jagarlamudi suzuki vanderwende pingali
et al rank sentences mixture model component
model statistical model
score qiscore qf ocus q



wordnet http wordnet princeton edu widely used semantic lexicon english language
groups english words e nouns verbs adjectives adverbs sets synonyms called synsets
provides short general definitions e gloss definition records semantic relations
synonym sets



fichali joty hasan

score score sentence query independent score qiscore
query dependent score qfocus calculated probabilistic toutanova
et al learns log linear sentence ranking model maximizing three metrics
sentence goodness rouge oracle b pyramid derived c model frequency
scoring function learned fitting weights set feature functions sentences
document set trained optimize sentence pair wise ranking criterion
scoring function adapted apply summaries rather sentences take
account redundancy among sentences
pingali et al reduce document sentences dropping words
contain important information toutanova et al vanderwende suzuki
brockett zajic lin dorr schwartz heuristically decompose
document sentences smaller units apply small set heuristics parse
tree create alternatives original sentence possibly multiple
simplified versions available selection
approaches multi document summarization try cluster sentences
together guo stylios use verb arguments e subjects times locations
actions clustering sentence method establishes indices information
verb arguments subject first index time second location third
action fourth sentences closest subjects index put
cluster sorted according temporal sequence earliest
latest sentences spaces locations index value cluster
marked clusters ranked sizes top clusters
chosen applying cluster reduction module system generates compressed
extract summaries
approaches recognizing textual entailment sentence alignment
question answering use syntactic semantic information order measure
similarity two textual units indeed motivated us include syntactic
semantic features get structural similarity document sentence query
sentence discussed section maccartney grenager de marneffe cer manning
use typed dependency graphs dependency trees represent text
hypothesis try good partial alignment typed dependency
graphs representing hypothesis contains n nodes text graph contains
nodes search space n use incremental beam search combined
node ordering heuristic approximate global search space possible
alignments locally decomposable scoring function chosen score
alignment sum local node edge alignment scores scoring measure
designed favor alignments align semantically similar subgraphs irrespective
polarity reason nodes receive high alignment scores words represent
semantically similar synonyms antonyms receive highest score unrelated
words receive lowest alignment scores incorporate local edge scores
shape paths nodes text graph correspond adjacent
nodes hypothesis graph final step make decision whether
hypothesis entailed text conditioned typed dependency graphs
well best alignment make decision use supervised


ficomplex question answering unsupervised approaches

statistical logistic regression classifier feature space features gaussian
prior parameter regularization
hirao et al represent sentences dependency tree path dtp incorporate syntactic information apply string subsequence kernel ssk measure
similarity dtps two sentences introduce extended string
subsequence kernel esk incorporate semantics dtps kouylekov magnini
use tree edit distance dependency trees text
hypothesis recognize textual entailment according text entails
hypothesis h exists sequence transformations e deletion insertion
substitution applied obtain h overall cost certain
threshold punyakanok et al represent question sentence containing
answer dependency trees add semantic information e named entity
synonyms related words dependency trees apply approximate
tree matching order decide similar given pair trees use
edit distance matching criteria approximate tree matching methods
improvement bow scoring methods


accomplish task answering complex questions extract important features sentences document collection measure relevance
query sentences document collection analyzed levels
document sentences represented vector feature values feature set
includes lexical lexical semantic statistical similarity syntactic semantic features
graph similarity measures chali joty b reimplemented many
features successfully applied many related fields nlp
use simple local search technique fine tune feature weights use
statistical clustering em k means select relevant sentences
summary generation experimental systems perform better
include tree kernel syntactic semantic features though summaries
syntactic semantic feature achieve good graph cosine
similarity lexical semantic features important selecting relevant sentences
local search technique outperforms two em performs
better k means learning later sections describe subparts
systems details

feature extraction
section describe features used score sentences
provide detailed examples get feature values first describe
syntactic semantic features introducing work follow
detailed description features commonly used question answering
summarization communities
query document sentences used examples taken duc collection



fichali joty hasan

syntactic shallow semantic features
task query summarization requires use complex syntactic
semantics approaches bow often inadequate perform fine level
textual analysis importance syntactic semantic features context
described zhang lee moschitti et al bloehdorn moschitti
moschitti basili bloehdorn moschitti b
effective way integrate syntactic semantic structures machine learning use tree kernel functions collins duffy moschitti quarteroni
successfully applied question classification zhang lee
moschitti basili syntactic semantic information used effectively measure similarity two textual units maccartney et al best
knowledge study used tree kernel functions encode syntactic semantic
information complex tasks computing relatedness query
sentences document sentences another good way encode shallow syntactic
information use basic elements hovy lin zhou fukumoto
uses dependency relations experiments including syntactic semantic
features improves performance sentence selection complex question answering
task chali joty
encoding syntactic structures
basic element overlap measure shallow syntactic information dependency relations proved effective finding similarity two textual
units hirao et al incorporate information basic elements
defined follows hovy et al
head major syntactic constituent noun verb adjective adverbial phrases
expressed single item
relation head single dependent expressed triple
head modifier relation
triples encode syntactic information one decide whether two units
match easily longer units hovy et al extracted bes
sentences query package distributed isi
get bes sentence computed likelihood ratio lr
following zhou lin hovy sorting bes according lr scores produced
ranked list goal generate summary answer users questions
ranked list bes way contains important bes top may may
relevant users questions filter bes checking whether contain
word query word queryrelatedwords defined section
example consider following sentence get score
query describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
website http www isi edu cyl



ficomplex question answering unsupervised approaches

sentence frankfurt body said annual report released today
decided two themes currency history european civilization
abstract concrete paintings
score
decided themes obj considered contain word
query words query relevant words report annual mod taken
contains query word report way filter bes related
query score sentence sum scores divided number bes
sentence limiting number top bes contribute calculation
sentence scores remove bes little importance sentences
fewer important bes set threshold topmost bes ranked
list contribute normalized sentence score computation
set threshold took bes counted calculating scores
sentences
tree kernels order calculate syntactic similarity query
sentence first parse sentence well query syntactic tree
moschitti parser charniak calculate similarity
two trees tree kernel reimplemented tree kernel model
proposed moschitti et al
build trees next task measure similarity trees
every tree represented dimensional vector v v v vm
th element vi number occurrences th tree fragment tree
tree fragments tree sub trees include least one production
restriction production rules broken incomplete parts moschitti
et al figure shows example tree portion subtrees

figure example tree b sub trees np covering press
implicitly enumerate possible tree fragments fragments
axis dimensional space note could done implicitly since
number extremely large collins duffy define tree
kernel whose computational complexity depend
tree kernel two trees actually inner product v v


fichali joty hasan

k v v



define indicator function ii n sub tree seen rooted node n
otherwise follows

vi

x

x

ii n vi

n n

ii n



n n

n n set nodes respectively derive
k v v

x

vi vi



x



x x

n n n n

x



x

ii n ii n



c n n



n n n n

define c n n ii n ii n next note c n n
computed polynomial time due following recursive definition
p

productions n n different c n n
productions n n n n pre terminals
c n n
else productions n n pre terminals
nc n

c n n



c ch n j ch n j



j

nc n number children n tree productions n
n nc n nc n th child node n ch n
cases query composed two sentences compute similarity
document sentence query sentences qi take
average scores syntactic feature value
syntactic similarity value

pn

k qi

n

n number sentences query q sentence consideration tk similarity value tree kernel sentence query
sentence q syntactic structure example following sentence
query q get score


ficomplex question answering unsupervised approaches

figure example semantic trees
query q describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
sentence europes currency euro rival u dollar international
currency long term der spiegel magazine reported sunday
scores
average score
semantic features
though introducing syntactic information gives improvement bow use
syntactic parses adequate dealing complex questions whose answers expressed long articulated sentences even paragraphs shallow semantic
representations bearing compact information could prevent sparseness deep
structural approaches weakness bow maccartney et al moschitti
et al
initiatives propbank pb kingsbury palmer made design
accurate automatic semantic role labeling srl systems assert hacioglu pradhan ward martin jurafsky possible hence attempting application srl
qa seems natural pinpointing answer question relies deep understanding
semantics example consider pb annotation
arg target use arg french franc arg currency
annotation used design shallow semantic representation
matched semantically similar sentences e g
arg vatican target use arg italian lira arg currency
order calculate semantic similarity sentences first represent
annotated sentence query tree structures figure called semantic tree
st proposed moschitti et al semantic tree arguments replaced
important wordoften referred semantic head look noun
first verb adjective adverb semantic head argument
none present take first word argument semantic head


fichali joty hasan

figure two sts composing stn
however sentences rarely contain single predicate rather typically propositions contain one subordinate clauses instance let us consider slight modification
second sentence vatican located wholly within italy uses italian lira
currency main predicate uses subordinate predicate located
srl system outputs following two annotations
arg vatican located wholly within italy target uses arg italian
lira arg currency
arg vatican target located argm loc wholly argm loc within
italy uses italian lira currency
giving sts figure see figure argument node
corresponds entire subordinate clause label leaf st e g leaf
arg st node actually root subordinate clause figure b
taken separately sts express whole meaning sentence hence
accurate define single structure encoding dependency two
predicates figure c refer kind nested sts stns
note tree kernel tk function defined section computes number
common subtrees two trees subtrees subject constraint
nodes taken none children original tree though
definition subtrees makes tk function appropriate syntactic trees well
suited semantic trees st instance although two sts figure share
subtrees rooted st node kernel defined computes match
critical aspect steps tk function productions
two evaluated nodes identical allow match descendants
means common substructures cannot composed node
children effective st representation would require moschitti et al solve
designing shallow semantic tree kernel sstk allows portions
st match
shallow semantic tree kernel sstk reimplemented sstk according
model given moschitti et al sstk two ideas first changes


ficomplex question answering unsupervised approaches

st shown figure adding slot nodes accommodate argument labels
specific order fixed number slots possibly filled null arguments
encode possible predicate arguments leaf nodes filled wildcard character
may alternatively accommodate additional information slot nodes used
way adopted tk function generate fragments containing one
children example shown frames b c figure previously
pointed arguments directly attached root node kernel function
would generate structure children structure children e
empty moschitti et al

figure semantic tree fragments
second original tree kernel would generate many matches slots filled
null label set step tk calculation
n n pre terminal node child label null c n n
subtract one unit c n n step
nc n

c n n



j

c ch n j ch n j



changes generate c substituted place original c
eq gives sstk
example following sentence query q get semantic score
query q describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
sentence frankfurt body said annual report released today
decided two themes currency history european civilization
abstract concrete paintings
scores
average score


fichali joty hasan

lexical features
discuss lexical features commonly used qa
summarization communities reimplemented
n gram overlap
n gram overlap measures overlapping word sequences candidate document
sentence query sentence view measure overlap scores query pool
sentence pool created order create query sentence pool took
query document sentence created set related sentences replacing
content words first sense synonyms wordnet example given stemmed
document sentence john write poem sentence pool contains john compose
poem john write verse form along given sentence
measured recall n gram scores sentence p following formula
n gramscore p maxi maxj n gram si qj
p
gram countmatch gramn
n gram q p n
gramn count gramn




n stands length n gram n countmatch gramn
number n grams co occurring query candidate sentence qj j th
sentence query pool si th sentence sentence pool sentence p
gram overlap measure
gram overlap score measures number words common sentence hand
query related words computed follows
gram overlap score

p

countmatch w
w count w

w

p



set content words candidate sentence countmatch
number matches sentence content words query related words count gramn
number w
note order measure gram score took query related words instead
exact query words motivation behind sentence word
exactly query words synonyms hypernyms hyponym gloss
words get counted
example
query describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
sentence frankfurt body said annual study released today
decided two themes currency history european civilization
abstract concrete paintings
hence forth content words nouns verbs adverbs adjectives



ficomplex question answering unsupervised approaches

gram score normalization
note sentence gram overlap score even though
exact word common query words got score sentence word
study synonym query word report
n gram overlap measures
calculate n gram overlap scores example considering
following query sentence document sentence duc collection
matching grams euro january january hence employing
formula given get following gram score normalization gram score
found accordingly
query sentence describe steps taken worldwide reaction prior introduction
euro january include predictions expectations reported
press
document sentence despite skepticism actual realization single european currency scheduled january preparations design
euro note already begun
gram
gram
lcs wlcs
sequence w w w wn subsequence another sequence x x x xm
exists strict increasing sequence indices x
j n xij wj cormen leiserson rivest given two sequences
longest common subsequence lcs common subsequence
maximum length lin
longer lcs two sentences similar two sentences
used lcs f measure estimate similarity document sentence
length query sentence q length n follows
lcs q

lcs q
plcs q
n
flcs q plcs q rlcs q
rlcs q





lcs q length longest common subsequence q
constant determines importance precision recall computing
lcs measure document sentence query sentence viewed sequence words
normalize feature values corresponding sentence respect entire context
particular document



fichali joty hasan

intuition longer lcs two similar
recall rlcs q ratio length longest common subsequence
q document sentence length measures completeness whereas precision
plcs q ratio length longest common subsequence q
query sentence length measure exactness obtain equal importance
precision recall set value equation called lcs
f measure notice flcs q flcs nothing
common q
one advantage lcs require consecutive matches insequence matches reflect sentence level word order n grams advantage
automatically includes longest sequence common n grams therefore predefined n gram length necessary moreover property value less
equal minimum unigram e gram f measure q unigram recall
reflects proportion words present q unigram precision
proportion words q unigram recall precision count
co occurring words regardless orders lcs counts sequence co occurrences
awarding credit sequence unigram matches lcs measure captures
sentence level structure natural way consider following example
john shot thief
john shot thief
thief shot john
reference sentence sentences consideration
would gram score since one bigram e thief
common however different meanings case lcs
score score therefore better
according lcs
however lcs suffers one disadvantage counts main sequence
words therefore alternative lcses shorter sequences reflected
final score example given following candidate sentence
thief john shot
reference lcs counts thief john shot
therefore lcs score gram would prefer
order measure lcs score sentence took similar previous section wordnet e creation sentence pool query pool calculated
lcs score following formula

lcs score maxi maxj flcs si qj



qj j th sentence query pool si th sentence
sentence pool


ficomplex question answering unsupervised approaches

basic lcs differentiate lcses different spatial
relations within embedding sequences lin example given reference
sequence two candidate sequences follows
b c e f g
b c h k
h b k c
lcs score however better choice
consecutive matches improve basic lcs method store length
consecutive matches encountered far regular two dimensional dynamic program table
computing lcs call weighted lcs wlcs use k indicate length
current consecutive matches ending words xi yj given two sentences x
wlcs score x computed similar dynamic programming
procedure stated lin use wlcs advantage measuring
similarity taking words higher dimension string kernels indeed
reduces time complexity computed wlcs f measure
way query pool sentence pool
w lcs score maxi maxj fwlcs si qj



example
query sentence describe steps taken worldwide reaction prior introduction
euro january include predictions expectations reported
press
document sentence despite skepticism actual realization single european currency scheduled january preparations design
euro note already begun
matching strings euro january longest common
subsequence considering sentence related sentences wlcs set
weight normalization get following lcs wlcs scores
sentence applying formula
lcs score
wlcs score
skip bigram measure
skip bigram pair words sentence order allowing arbitrary gaps skipbigram measures overlap skip bigrams candidate sentence query
sentence lin rely query pool sentence pool
wordnet considering following sentences


fichali joty hasan

john shot thief
john shoot thief
thief shoot john
thief john shot
get sentence c skip bigrams example following
skip bigrams john shot john john thief shot shot thief
thief three skip bi gram matches john john thief thief
one skip bi gram match thief two skip bi gram matches
john shot thief
skip bi gram score document sentence length query
sentence q length n computed follows
skip q
c
skip q
pskip q
c n
fskip q pskip q rskip q
rskip q





skip q number skip bi gram matches q
constant determines importance precision recall set value
associate equal importance precision recall c combination
function call equation skip bigram f measure computed skip
bigram f measure formula

skip bigram maxi maxj fskip si qj



example given following query sentence get skip bigrams
january january euro january
applying equations get skip bi gram score normalization
query describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
sentence despite skepticism actual realization single european currency
scheduled january preparations design euro note
already begun
skip bi gram score
c n r

n
r nr



ficomplex question answering unsupervised approaches

note skip bi gram counts order matching word pairs lcs counts
one longest common subsequence put constraint maximum skip distance
dskip two order words form skip bi gram avoids spurious matches
example set dskip equivalent bi gram
overlap measure lin set dskip word pairs words
apart form skip bi grams experiment set dskip order ponder
words apart get skip bi grams
modifying equations allow maximum skip distance limit
straightforward following lin count skip bi gram matches skip q
within maximum skip distance replace denominators equations
actual numbers within distance skip bi grams reference sentence
candidate sentence respectively
head head related words overlap
number head words common two sentences indicate much
relevant order extract heads sentence query
sentence query parsed minipar dependency tree extract
heads call exact head words example head word sentence john
eats rice eat
take synonyms hyponyms hypernyms query head words
sentence head words form set words call head related words
measured exact head score head related score follows
p

w headset countmatch w



w headrelset countmatch w



exactheadscore
headrelatedscore

p

p

p

w headset count w

w headrelset count w

headset set head words sentence countmatch number
matches headset query sentence headrelset set
synonyms hyponyms hypernyms head words sentence countmatch
number matches head related words query sentence
example list head words query sentence measures
query describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
heads query include reaction step take describe report euro introduction press
prediction expectation
sentence frankfurt body said annual report released today
decided two themes currency history european civilization
abstract concrete paintings
http www cs ualberta ca lindek minipar htm
hypernym hyponym levels restricted respectively



fichali joty hasan

heads sentence history release currency body report painting say civilization
theme decide
exact head score






head related score
lexical semantic features
form set words call queryrelatedwords taking content words
query first sense synonyms nouns hypernyms hyponyms nouns
gloss definitions wordnet
synonym overlap
synonym overlap measure overlap list synonyms content
words extracted candidate sentence query related words computed
follows
synonym overlap score

p

w synset countmatch w
w synset count w

p



synset synonym set content words sentence countmatch
number matches synset query related words
hypernym hyponym overlap
hypernym hyponym overlap measure overlap list hypernyms level
hyponyms level nouns extracted sentence consideration
query related words computed follows
hypernym hyponym overlap score

p

h hypset countmatch h
h hypset count h

p



hypset hyponym hyponym set nouns sentence countmatch
number matches hypset query related words
gloss overlap
gloss overlap measure overlap list content words extracted
gloss definition nouns sentence consideration query related
words computed follows
gloss overlap score

p

g glossset countmatch g

p

g glossset count g



glossset set content words e nouns verbs adjectives taken
gloss definition nouns sentence countmatch number matches
glossset query related words


ficomplex question answering unsupervised approaches

example
example given query following sentence gets synonym overlap score
hypernym hyponym overlap score gloss overlap score
query describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
sentence frankfurt body said annual report released today
decided two themes currency history european civilization
abstract concrete paintings
synonym overlap score
hypernym hyponym overlap score
gloss overlap score
statistical similarity measures
statistical similarity measures co occurrence similar words corpus
two words termed similar belong context used thesaurus
provided dr dekang lin purpose used two statistical similarity
measures
dependency similarity measure
method uses dependency relations among words order measure similarity lin b extracts dependency triples uses statistical
measure similarity given corpus one retrieve similar words
given word similar words grouped clusters
note word one cluster cluster represents
sense word similar words sense selecting right cluster
word goals create bag similar words query
words ii get bag similar words dependency query words
measure overlap score sentence words
creating bag similar words
query word extract clusters thesaurus order
determine right cluster query word measure overlap score
query related words e exact words synonyms hypernyms hyponyms gloss
clusters hypothesis cluster words common query
related words right cluster assumption first synonym correct
sense choose cluster word highest overlap score

overlap scorei

p

w queryrelatedw ords countmatch w



cluster argmaxi overlap scorei



w queryrelatedw ords count w

p

http www cs ualberta ca lindek downloads htm



fichali joty hasan

queryrelatedwords set exact words synonyms hyponyms hypernyms
gloss words words query e query words countmatch number
matches query related words ith cluster similar words
measuring overlap score
get clusters query words measured overlap
cluster words sentence words call dependency similarity measure

dependencym easure

w senw ords countmatch w

p

p

w senw ords count w



senwords set words sentence countmatch number
matches sentence words cluster similar words
proximity similarity measure
similarity computed linear proximity relationship words
lin uses information theoretic definition similarity measure
similarity similar words grouped clusters took similar
measure feature previous section except used different thesaurus
example
considering following query sentence get following measures
query describe steps taken worldwide reaction prior introduction euro
january include predictions expectations reported press
sentence frankfurt body said annual report released today
decided two themes currency history european civilization
abstract concrete paintings
dependency similarity score
proximity similarity score
graph similarity measure
erkan radev used concept graph centrality rank set sentences
producing generic multi document summaries similarity graph produced
sentences document collection graph node represents sentence
edges nodes measure cosine similarity respective pair sentences
degree given node indication important sentence figure
shows example similarity graph sentences
similarity graph constructed sentences ranked according
eigenvector centrality lexrank performed well context generic summarization apply lexrank query focused context topic sensitive version lexrank
proposed otterbacher et al followed similar order calculate
feature score sentence determined mixture model relevance
sentence query similarity sentence high scoring sentences


ficomplex question answering unsupervised approaches

figure lexrank similarity
relevance question
first stem sentences collection compute word idfs inverse
document frequency following formula
n
idfw log
sfw






n total number sentences cluster sfw number
sentences word w appears
stem questions remove stop words relevance sentence
question q computed
rel q

x

wq

log tfw log tfw q idfw



tfw tfw q number times w appears q respectively
mixture model
previous section measured relevance sentence question
sentence similar high scoring sentences cluster high
score instance sentence gets high score question relevance
model likely contain answer question related sentence may
similar question likely contain answer otterbacher et al

capture idea following mixture model

p q



x
sim v
rel q
p

p
zc rel z q
zc sim z v
vc



p v q



p q score sentence given question q determined sum
relevance question similarity sentences collection
c set sentences collection value parameter call


fichali joty hasan

bias trade two terms equation set empirically higher
values prefer relevance question similarity sentences
denominators terms normalization although computationally
expensive equation calculates sum entire collection since required
model sense global impact voting sentences measure
cosine similarity weighted word idfs similarity two sentences cluster

sim x qp

p

wx

tfw x tfw idfw


xi x tfxi x idfxi

qp


yi tfyi idfyi



equation written matrix notation follows
p da b p



square matrix given index elements th column
proportional rel q b square matrix entry b j
proportional sim j matrices normalized row sums add
note normalization rows resulting square matrix q
da b add matrix called stochastic defines markov
chain view sentence state markov chain q j specifies
transition probability state state j corresponding markov chain
vector p looking eq stationary distribution markov chain
intuitive interpretation stationary distribution understood concept
random walk graph representation markov chain probability
transition made current node nodes similar query
probability transition made nodes lexically similar current
node every transition weighted according similarity distributions element
vector p gives asymptotic probability ending corresponding state
long run regardless starting state stationary distribution markov chain
computed simple iterative called power method erkan radev
starts uniform distribution iteration eigenvector updated
multiplying transpose stochastic matrix since markov chain
irreducible aperiodic guaranteed terminate

ranking sentences
use several methods order rank sentences generate summaries applying
features described section section describe systems detail
learning feature weights local search strategy
order fine tune weights features used local search technique initially set feature weights w wn equal values e see
train weights duc data set current weights
score sentences generate summaries accordingly evaluate summaries


ficomplex question answering unsupervised approaches

input stepsize l weight initial value v
output vector w
learned weights
initialize weight values wi v
n
rg rg prev
true
scoresentences w

generatesummaries
rg evaluaterouge
rg rg
prev wi
wi l
rg rg
else
break
end
end
end
return w

tuning weights local search technique
automatic evaluation tool rouge lin described section rouge
value works feedback learning loop learning system tries maximize
rouge score every step changing weights individually specific step size e
means learn weight wi change value wi keeping weight
values wj j stagnant weight wi achieves local maximum
e hill climbing rouge value
learned feature weights compute final scores sentences
formula
scorei x w




x feature vector th sentence w
weight vector scorei
score th sentence
statistical machine learning approaches
experimented two unsupervised statistical learning techniques features
extracted previous section sentence selection
k means learning
expectation maximization em learning
k means learning
k means hard clustering defines clusters center mass
members start set initial cluster centers chosen randomly go


fichali joty hasan

several iterations assigning object cluster whose center closest
objects assigned recompute center cluster centroid
members distance function use squared euclidean distance
mean
instead true euclidean distance
since square root monotonically growing function squared euclidean distance
true euclidean distance computation overload smaller
square root dropped
learned means clusters k means next
task rank sentences according probability model used bayesian
model order bayes law says

x qk p qk
p x
x
p x
x qk p qk
p x
pk
x qk p qk
k p x

x
p qk x




qk cluster x feature vector representing sentence parameter
set class set weights clusters equiprobable e p qk
x qk gaussian probability distribution gaussian
k calculated p x
probability density function pdf dimensional random variable x given

x
p
x

e


x
x
x

x


dp

det



mean vector covariance matrix parameters
k means calculate
gaussian distribution get means
covariance matrix unbiased covariance estimation procedure

j

n
x
xi j x
x j
x
n



em learning
em gaussian mixture well known method cluster analysis
useful outcome model produces likelihood value clustering model
likelihood values used select best model number different
providing number parameters e number
clusters


ficomplex question answering unsupervised approaches

x represented feature vector length
input sample n data points x
l
input number clusters k
output array k means scores
data array dnk k k
data array c k nk
randomly choose k data points k initial means k k k
repeat
n
j k
xi j k x
xi j x
xi j
ij kx
end
ik il l k
assign x c k
end
end
p
k
c
x c

xj

j


c
c
end
change occurs
calculating covariances cluster
k
c
c
j
c ij c
c ij
c
end

end
calculating scores sentences
n
j k




x j j x j
yij e







j
det

end
j k
p
wj k
zij yij wj k
j yij wj
end
k k
max
push zim
end
return
computing k means similarity measure



fichali joty hasan

significant em converges local maximum
likelihood function hence quality depends initialization
along method improving initialization discussed later
section
em soft version k means described k means
start set random cluster centers c ck iteration soft assignment
data points every cluster calculating membership probabilities em
iterative two step procedure expectation step maximization step
expectation step compute expected values hidden variables hi j cluster
membership probabilities given current parameters compute likely
object belongs clusters maximization step computes likely
parameters model given cluster membership probabilities
data points considered generated mixture model k gaussians
form

p x

k
x

p c p x c



k
x


p c p x





total likelihood model k components given observed data points
x x x n

l x




n x
k


j

x j
p c j p x

n
x

k
x



log

j

n x
k


j

xi
j j
wj p x

xi
j j taking log likelihood
wj p x




p probability density function e eq j j mean
covariance matrix component j respectively component contributes proportion
p
wj total population k
j wj
log likelihood used instead likelihood turns product sum
describe em estimating gaussian mixture
singularities covariance matrix must non singular invertible
em may converge position covariance matrix becomes singular
close singular means invertible anymore covariance

matrix becomes singular close singular em may wrong clusters
restrict covariance matrices become singular testing cases iteration
follows
q

e update

else update


ficomplex question answering unsupervised approaches

discussion starting values em
convergence rate success clustering em degraded
poor choice starting values means covariances weights components experimented one summary document number duc
order test impact initial values em cluster
means initialized
p heuristic spreads randomly around ean dat
standard deviation cov dat initial covariance set cov dat
initial values weights wj k k number clusters
dimensional data points parameters j th component follows

j rand
j dat
wj

q

dat dat

k

highly variable nature tests reflected inconsistent values total log likelihood repeated experiments indicated
random starting values initial estimates means frequently gave poor
two possible solutions order get good
random starting values specified run em several times choose initial configuration get maximum
log likelihood among configurations choosing best one among several runs
computer intensive process improve outcome em gaussian
mixture necessary better method estimating initial means
components
best starting position em regard estimates means
would one estimated mean per cluster closer true mean
cluster
achieve aim explored widely used k means cluster
means finding method means found k means clustering
utilized initial means em calculate initial covariance matrices
unbiased covariance estimation procedure equation
ranking sentences
sentences clustered em identify sentences
xi qr denotes clusare question relevant checking probabilities p qr x
x x considered
ter question relevant sentence x p qr x
question relevant cluster mean values greater one
considered question relevant cluster
next task rank question relevant sentences order include
summary done easily multiplying feature vector x weight
vector w
learned applying local search technique equation


fichali joty hasan

input sample n data points x represented feature vector length
l
input number clusters k
output array em scores
k k k k equal priors set
start k initial gaussian n
p qk k
repeat

x j
estimation step compute probability p qk x


data point xj j n belong class qk
j n
k k

x j
p qk x





xj qk
p qk p x
xj
p x



end
end
maximization step
k k
j n
update means

k







x j
k k
p qk p x




x j

k p qk p x
k k

pk





update variances

k




xj
j x j p qk x
pn

xj
j p qk x

pn


xj
xj x
xj
x
j p qk x
k
pn

xj
j p qk x

pn




k

update priors

p qk

n
x

x j
p qk x
n j

end
end
total likelihood increase falls desired threshold
return
computing em similarity measure



ficomplex question answering unsupervised approaches

redundancy checking generating summary
sentences scored easiest way create summaries output
topmost n sentences required summary length reached case
ignoring factors redundancy coherence
know text summarization clearly entails selecting salient information putting together coherent summary answer summary consists
multiple separately extracted sentences different documents obviously
selected text snippets individually important however many competing sentences included summary issue information overlap parts
output comes mechanism addressing redundancy needed therefore
summarization systems employ two levels analysis first content level every
sentence scored according features concepts covers second textual level
added final output sentences deemed important
compared similar candidates included final answer summary goldstein kantrowitz mittal carbonell
observed authors called maximum marginal relevance mmr following hovy et al modeled overlap intermediate summary
added candidate summary sentence
call overlap ratio r r inclusively setting r
means candidate summary sentence added intermediate summary
sentence overlap ratio less equal

experimental evaluation
section describes experiments conducted duc dataset
provided nist questions experiments address include
different features affect behavior summarizer system
one k means em local search performs better
particular
used main task duc evaluation task
given complex question topic description collection relevant documents
task synthesize fluent well organized word summary documents
answers question topic
documents duc came aquaint corpus comprising newswire
articles associated press york times xinhua news
agency nist assessors developed topics interest choose set
documents relevant document cluster topic topic document
cluster given different nist assessors including developer topic
assessor created word summary document cluster satisfies information
http www nlpir nist gov projects duc
national institute standards technology



fichali joty hasan

need expressed topic statement multiple reference summaries used
evaluation summary content
purpose experiments study impact different features accomplish generated summaries topics duc seven
systems defined
lex system generates summaries lexical features section
n gram n lcs wlcs skip bi gram head head synonym overlap
lexsem system considers lexical semantic features section synonym hypernym hyponym gloss dependency proximity similarity
syn system generates summary syntactic feature section
cos system generates summary graph method section
sys system considers features except syntactic semantic features
features except section
sys system considers features except semantic feature features
except section
system generates summaries taking features section account
automatic evaluation
rouge carried automatic evaluation summaries rouge lin
toolkit widely adopted duc automatic summarization evaluation rouge stands recall oriented understudy gisting evaluation
collection measures determines quality summary comparing reference summaries created humans measures count number overlapping units
n gram word sequences word pairs system generated summary
evaluated ideal summaries created humans available rouge measures
rouge n n rouge l rouge w rouge rouge n n gram
recall candidate summary set reference summaries rouge l measures
longest common subsequence lcs takes account sentence level structure
similarity naturally identifies longest co occurring insequence n grams automatically
rouge w measures weighted longest common subsequence wlcs providing improvement basic lcs method computation credit sentences
consecutive matches words rouge overlap skip bigrams candidate summary set reference summaries skip bigram pair words
sentence order allowing arbitrary gaps rouge measures
applied automatic evaluation summarization systems achieved promising
lin
systems report widely accepted important metrics rouge
rouge su present rouge scores since never shown
correlate human judgement rouge measures calculated running


ficomplex question answering unsupervised approaches

rouge stemming removal stopwords rouge run time parameters
set duc evaluation setup
rouge pl u r n w l
confidence interval important evaluation metrics systems
report significance meaningful comparison use rouge tool
purpose rouge uses randomized method named bootstrap resampling compute
confidence interval used sampling points bootstrap resampling
report evaluation scores one baseline system base column
tables order level improvement systems achieve baseline
system generates summaries returning leading sentences words
ht ext field recent document
presenting highlight top two f scores bottom one f score
indicate significance glance
discussion
k means learning table shows rouge scores different combinations
features k means learning noticeable k means performs best
graph cosine similarity feature note including syntactic feature
improve score including syntactic semantic features increases score
significant amount summaries lexical features give us good
rouge evaluation
scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys









base




table rouge measures k means learning

table shows rouge scores different combinations features k means
learning rouge graph cosine similarity feature performs well
get significant improvement rouge score include syntactic feature
features semantic features affect score much lexical semantic features
perform well

scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys









table rouge measures k means learning



base




fichali joty hasan

table shows rouge su scores best features without syntactic
semantic including syntactic semantic features features degrades scores
summaries lexical features achieve good scores
scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys









base




table rouge su measures k means learning
table shows confidence interval f measures k means learning
important rouge evaluation metrics systems comparison confidence
interval baseline system seen systems performed significantly
better baseline system cases
systems
baseline
lex
lexsem
syn
cos
sys
sys


rouge









rouge









rouge su









table confidence intervals k means system

em learning table table different rouge measures feature
combinations context em learning easily noticed
measures get significant amount improvement rouge scores include
syntactic semantic features along features get improvement
sys f score include syntactic feature improvement include
syntactic semantic features cosine similarity measure perform well
k means experiments summaries considering lexical features achieve
good
table shows confidence interval f measures em learning important rouge evaluation metrics systems comparison confidence
interval baseline system see systems performed significantly
better baseline system cases
local search technique rouge scores feature combinations
given table table summaries generated including features perform


ficomplex question answering unsupervised approaches

scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys









base









base









base




table rouge measures em learning

scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys




table rouge measures em learning

scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys




table rouge su measures em learning

systems
baseline
lex
lexsem
syn
cos
sys
sys


rouge









rouge









rouge su









table confidence intervals em system



fichali joty hasan

best scores measures get improvement sys f score
include syntactic feature improvement sys f score include
syntactic semantic features case lexical features lex perform well
better features
scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys









base




table rouge measures local search technique

scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys









base




table rouge measures local search technique

scores
recall
precision
f score

lex




lexsem




syn




cos




sys




sys









base




table rouge su measures local search technique
table shows confidence interval f measures local search technique
important rouge evaluation metrics systems comparison confidence interval baseline system systems performed significantly
better baseline system cases
comparison
reported see three systems clearly outperform baseline system table shows f scores reported rouge measures
table reports confidence intervals baseline system best system
duc three techniques taking features consideration
see method local search technique outperforms two
em performs better k means analyze deeply
cases rouge su local search confidence intervals overlap
best duc system


ficomplex question answering unsupervised approaches

systems
baseline
lex
lexsem
syn
cos
sys
sys


rouge









rouge









rouge su









table confidence intervals local search system


baseline
best system
k means
em
local search

rouge






rouge






rouge su






table rouge f scores different systems


baseline
best system
k means
em
local search

rouge






rouge






rouge su






table confidence intervals different systems



fichali joty hasan

manual evaluation
sample summaries drawn different systems generated summaries
conduct extensive manual evaluation order analyze effectiveness
approaches manual evaluation comprised pyramid evaluation contents
user evaluation get assessment linguistic quality overall responsiveness
pyramid evaluation
duc main task topics selected optional community
pyramid evaluation volunteers different sites created pyramids annotated
peer summaries duc main task given guidelines sites among
created pyramids used pyramids annotate peer summaries
compute modified pyramid scores used ducview jar annotation tool
purpose table table modified pyramid scores systems
three baseline systems score reported peer summaries
baseline system generated returning leading sentences words
ht ext field recent document see
systems perform better baseline system inclusion syntactic semantic
features yields better scores three notice lexical
semantic features best terms modified pyramid scores
user evaluation
university graduate students judged summaries linguistic quality overall
responsiveness given score integer poor good
guided consideration following factors grammaticality non redundancy
referential clarity focus structure coherence assigned
content responsiveness score automatic summaries content score
integer poor good amount information
summary helps satisfy information need expressed topic narrative
measures used duc table table present average linguistic
quality overall responsive scores systems three
baseline systems scores given meaningful comparison closer look
systems perform worse baseline system terms
linguistic quality achieve good scores case overall responsiveness
obvious tables exclusion syntactic semantic features often causes
lower scores hand lexical lexical semantic features good overall
responsiveness scores three
systems cumulatively systems randomly chose
summaries systems
http www cs columbia edu becky duc pyramid guidelines html
equals sum weights summary content units scus peer summary matches
normalized weight ideally informative summary consisting number contributors
peer
http www cs columbia edu ani duc tool html



ficomplex question answering unsupervised approaches

systems
baseline
lex
lexsem
syn
cos
sys
sys


modified pyramid scores









table modified pyramid scores k means system

systems
baseline
lex
lexsem
syn
cos
sys
sys


modified pyramid scores









table modified pyramid scores em system

systems
baseline
lex
lexsem
syn
cos
sys
sys


modified pyramid scores









table modified pyramid scores local search system



fichali joty hasan

systems
baseline
lex
lexsem
syn
cos
sys
sys


linguistic quality









overall responsiveness









table linguistic quality responsive scores k means system

systems
baseline
lex
lexsem
syn
cos
sys
sys


linguistic quality









overall responsiveness









table linguistic quality responsive scores em system

systems
baseline
lex
lexsem
syn
cos
sys
sys


linguistic quality









overall responsiveness









table linguistic quality responsive scores local search system



ficomplex question answering unsupervised approaches

conclusion future work
presented works answering complex questions extracted eighteen important features sentences document collection later used
simple local search technique fine tune feature weights weight wi
achieves local maximum rouge value way learn
weights rank sentences multiplying feature vector weight vector
experimented two unsupervised learning techniques em k means
features extracted assume two clusters sentences queryrelevant query irrelevant learned means clusters k means
used bayesian model order rank sentences learned means
k means used initial means em applied em cluster sentences two classes query relevant
query irrelevant take query relevant sentences rank learned
weights e local search methods generating summaries filter
redundant sentences redundancy checking module generate summaries
taking top n sentences
experimented effects different kinds features evaluated
systems automatically rouge report significance
confidence intervals conducted two types manual evaluation pyramid
user evaluation analyze performance systems experimental
mostly following approaches achieve promising b
empirical local search technique outperforms two learning
techniques em performs better k means c systems achieve
better include tree kernel syntactic semantic features
cases rouge su local search confidence intervals overlap
best duc system
experimenting supervised learning techniques e svm maxent crf etc analyzing perform prior produced huge amount labeled data automatically similarity measures rouge
toutanova et al
future plan decompose complex questions several simple questions
measuring similarity document sentence query sentence
certainly serve create limited trees subsequences might increase
precision thus expect decomposing complex questions sets
subquestions entail systems improve average quality answers returned
achieve better coverage question whole

acknowledgments
thank anonymous reviewers useful comments earliest version
special thanks go colleagues proofreading grateful
graduate students took part user evaluation process
reported supported natural sciences engineering council
nserc grant university lethbridge


fichali joty hasan

appendix stop word list

reuters
may
nov
tue

accordingly

alone

another
anyway
appropriate
ask
awfully
becomes

better

cant
certainly
comes
containing
currently
didnt
dont

else
etc
everyone
except
followed
forth
get
goes
h
hasnt

ap
jun
dec
wed

across
aint
along
amid

anyways

asking
b
becoming
believe

c
cannot
changes
concerning
contains

different
done
edu
elsewhere
etc
everything
f
following
four
gets
going



jan
jul
tech
thu
able
actually

already
among
anybody
anywhere
arent
associated



beyond
cmon
cant
clearly
consequently
corresponding
definitely


eg
enough
even
everywhere
far
follows

getting
gone
hadnt
havent



feb
aug
news
fri


allow

amongst
anyhow
apart
around

became

beside

cs
cause
co
consider
could
described

downwards
e g
entirely
ever
ex



given
got
happens


mar
sep
index
sat

afterwards
allows
although

anyone
appear

available

beforehand
besides
brief
came
causes
com
considering
couldnt
despite
doesnt

eight
especially
every
exactly
fifth
former
furthermore
gives
gotten
hardly


apr
oct
mon

according

almost


anything
appreciate
aside
away
become
behind
best


certain
come
contain
course


e

et
everybody
example
five
formerly
g
go
greetings

hes

ficomplex question answering unsupervised approaches

hello
hereafter
hi

im
immediate
indicated
inward

keep
l
less
likely

mean


nearly
nevertheless
non
nothing

old
onto

overall
perhaps
probably
r
regarding

help
hereby

howbeit
ive

indicates


keeps
lately
lest
little
mainly
meanwhile
mostly

necessary

none
novel





placed
provides
rather
regardless

hence
herein

however
ie
inasmuch
inner
isnt

kept
later
let
look
many
merely
mr
n
need
next
noone

often



p
please
q
rd
regards


hereupon


e
inc
insofar

j
know
latter
lets
looking
may
might
ms
namely
needs
nine

nowhere
oh
one
others

particular
plus
que

relatively





hither
id

indeed
instead
itd

knows
latterly

looks
maybe

much
nd
neither

normally

ok
ones
otherwise
outside
particularly
possible
quite
really
respectively

heres

hopefully
ill
ignored
indicate

itll
k
known
least
liked
ltd

moreover
must
near
never
nobody

obviously
okay

ought

per
presumably
qv
reasonably
right

fichali joty hasan


says
seemed
sensible
shall

sometime
specified
sup
tell
thanx

theres
thereupon
theyve

thus
towards
twice
unless
us
usually
via


werent
whenever
wherein
whither
whose
within
wouldnt
youd


said
second
seeming
sent


sometimes
specify
sure
tends


thereafter

think
though

tried
two
unlikely
use
uucp
viz
wasnt
weve


whereupon


without
x
youll



secondly
seems
serious

somebody
somewhat
specifying

th
thats

thereby

third
three
together
tries
u

used
v
vs
way
welcome
whats
wheres
wherever
whos

wont

youre
z

saw
see
seen
seriously
shouldnt
somehow
somewhere
still
ts

thats

therefore
theyd



truly
un
unto
useful
value
w

well
whatever
whereafter
whether
whoever
willing
wonder
yes
youve
zero



say
seeing
self
seven
since
someone
soon
sub
take
thank

thence
therein
theyll
thorough
throughout
took
try


uses

want
wed
went

whereas

whole
wish
would
yet


saying
seem
selves
several
six
something
sorry

taken
thanks


theres
theyre
thoroughly
thru
toward
trying
unfortunately
upon


wants
well

whence
whereby



would



ficomplex question answering unsupervised approaches

references
bloehdorn moschitti combined syntactic semantic kernels text
classification th european conference ir ecir pp
rome italy
bloehdorn moschitti b structure semantics expressive text kernels
cikm pp
chali joty r improving performance random walk model
answering complex questions proceedings th annual meeting
acl hlt short section pp oh usa
chali joty r b selecting sentences answering complex questions
proceedings emnlp pp hawaii usa
charniak e maximum entropy inspired parser technical report cs
brown university computer science department
collins duffy n convolution kernels natural language proceedings
neural information processing systems pp vancouver canada
cormen r leiserson c e rivest r l introduction
mit press
erkan g radev r lexrank graph lexical centrality salience
text summarization journal artificial intelligence
goldstein j kantrowitz mittal v carbonell j summarizing text documents sentence selection evaluation metrics proceedings nd international acm conference development information retrieval
sigir pp berkeley ca
guo stylios g multi document summarization system proceedings document understanding conference nist
hacioglu k pradhan ward w martin j h jurafsky shallow
semantic parsing support vector machines technical report tr cslr university colorado
halliday hasan r cohesion english longman london
harabagiu lacatusu f hickl answering complex questions random
walk proceedings th annual international acm sigir conference
development information retrieval pp acm
hirao suzuki j isozaki h maeda e dependency sentence
alignment multiple document summarization proceedings coling pp
geneva switzerland coling


fichali joty hasan

hovy e lin c zhou l fukumoto j automated summarization evaluation basic elements proceedings fifth conference language
resources evaluation genoa italy
kingsbury p palmer treebank propbank proceedings
international conference language resources evaluation las palmas spain
kouylekov magnini b recognizing textual entailment tree edit distance
proceedings pascal challenges workshop recognising textual
entailment challenge
li j sun l kit c webster j query focused multi document summarizer lexical chains proceedings document understanding
conference rochester nist
lin c rouge package automatic evaluation summaries proceedings workshop text summarization branches post conference workshop
association computational linguistics pp barcelona spain
lin information theoretic definition similarity proceedings
international conference machine learning pp madison wisconsin
lin b automatic retrieval clustering similar words proceedings
international conference computational linguistics association
computational linguistics pp montreal canada
losada language modeling sentence retrieval comparison
multiple bernoulli multinomial information retrieval theory workshop glasgow uk
losada fernandez r highly frequent terms sentence retrieval
proc th string processing information retrieval symposium spire pp
santiago de chile
maccartney b grenager de marneffe cer manning c learning recognize features valid textual entailments proceedings human
language technology conference north american chapter acl p
york usa
morris j hirst g lexical cohesion computed thesaural relations
indicator structure text computational linguistics
moschitti efficient convolution kernels dependency constituent syntactic
trees proceedings th european conference machine learning berlin
germany
moschitti basili r tree kernel question answer classification question answering systems proceedings th international
conference language resources evaluation genoa italy


ficomplex question answering unsupervised approaches

moschitti quarteroni kernels linguistic structures answer extraction proceedings th conference association computational
linguistics acl short section columbus oh usa
moschitti quarteroni basili r manandhar exploiting syntactic
shallow semantic kernels question answer classificaion proceedings
th annual meeting association computational linguistics pp
prague czech republic acl
murdock v croft w b translation model sentence retrieval hlt
proceedings conference human language technology empirical methods
natural language processing pp morristown nj usa acl
otterbacher j erkan g radev r random walks questionfocused sentence retrieval proceedings human language technology conference
conference empirical methods natural language processing pp
vancouver canada
pingali p k r varma v iiit hyderabad duc proceedings
document understanding conference rochester nist
punyakanok v roth yih w mapping dependencies trees application
question answering proceedings ai math florida usa
strzalkowski harabagiu advances open domain question answering
springer
toutanova k brockett c gamon jagarlamudi j suzuki h vanderwende
l pythy summarization system microsoft duc
proceedings document understanding conference rochester nist
vanderwende l suzuki h brockett c microsoft duc
task focused summarization sentence simplification lexical expansion
proceedings document understanding conference rochester nist
zajic lin j dorr b j schwartz r sentence compression component multi document summarization system proceedings document
understanding conference rochester nist
zhang lee w question classification support vector machines
proceedings special interest group information retrieval pp toronto
canada acm
zhang lee w b language modeling passage question
answering proceedings twelfth text retreival conference pp
gaithersburg maryland
zhou l lin c hovy e multi dccument summarizer
query interpretation proceedings document understanding conference vancouver b c canada




