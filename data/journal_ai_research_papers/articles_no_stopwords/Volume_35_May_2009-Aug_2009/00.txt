Journal Artificial Intelligence Research 35 (2009) 1-47

Submitted 01/09; published 05/09

Complex Question Answering: Unsupervised Learning
Approaches Experiments
Yllias Chali

chali@cs.uleth.ca

University Lethbridge
Lethbridge, AB, Canada, T1K 3M4

Shafiq R. Joty

rjoty@cs.ubc.ca

University British Columbia
Vancouver, BC, Canada, V6T 1Z4

Sadid A. Hasan

hasan@cs.uleth.ca

University Lethbridge
Lethbridge, AB, Canada, T1K 3M4

Abstract
Complex questions require inferencing synthesizing information multiple
documents seen kind topic-oriented, informative multi-document summarization goal produce single text compressed version set
documents minimum loss relevant information. paper, experiment
one empirical method two unsupervised statistical machine learning techniques:
K-means Expectation Maximization (EM), computing relative importance
sentences. compare results approaches. experiments show
empirical approach outperforms two techniques EM performs better
K-means. However, performance approaches depends entirely feature
set used weighting features. order measure importance
relevance user query extract different kinds features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic shallow-semantic)
document sentences. use local search technique learn weights
features. best knowledge, study used tree kernel functions
encode syntactic/semantic information complex tasks computing
relatedness query sentences document sentences order generate
query-focused summaries (or answers complex questions). methods
generating summaries (i.e. empirical, K-means EM) show effects syntactic
shallow-semantic features bag-of-words (BOW) features.

1. Introduction
vast increase amount online text available demand access different types information led renewed interest broad range Information
Retrieval (IR) related areas go beyond simple document retrieval. areas
include question answering, topic detection tracking, summarization, multimedia retrieval, chemical biological informatics, text structuring, text mining, genomics, etc.
Automated Question Answering (QA)the ability machine answer questions, simple
complex, posed ordinary human languageis perhaps exciting technological development past six seven years (Strzalkowski & Harabagiu, 2008).
c
2009
AI Access Foundation. rights reserved.

fiChali, Joty, & Hasan

expectations already tremendous, reaching beyond discipline (a subfield Natural
Language Processing (NLP)) itself.
tool finding documents web, search engines proven adequate.
Although limitation expressiveness user terms query formulation, certain limitations exist search engine query. Complex
question answering tasks require multi-document summarization aggregated
search, faceted search, represents information need cannot answered
single document. example, look comparison average number
years marriage first birth women U.S., Asia, Europe, answer
likely contained multiple documents. Multi-document summarization useful
type query currently tool market designed meet
kind information need.
QA research attempts deal wide range question types including: fact, list,
definition, how, why, hypothetical, semantically-constrained, cross-lingual questions.
questions, call simple questions, easier answer. example,
question: president Bangladesh? asks persons name. type
question (i.e. factoid) requires small snippets text answer. Again, question:
countries Pope John Paul II visited? sample list question, asking
list small snippets text.
made substantial headway factoid list questions, researchers
turned attention complex information needs cannot answered
simply extracting named entities (persons, organizations, locations, dates, etc.) documents. Unlike informationally simple factoid questions, complex questions often seek multiple different types information simultaneously presuppose one single
answer meet information needs. example, factoid question like:
accurate HIV tests? safely assumed submitter question looking number range numbers. However, complex questions like:
causes AIDS? wider focus question suggests submitter
may single well-defined information need therefore may amenable
receiving additional supporting information relevant (as yet) undefined informational goal (Harabagiu, Lacatusu, & Hickl, 2006). questions require inferencing
synthesizing information multiple documents.
well known QA systems Korean Navers Knowledge search1 ,
pioneers community QA. tool allows users ask question get
answers users. Navers Knowledge roughly 10 times entries
Wikipedia. used millions Korean web users given day. people
say Koreans addicted internet Naver. January 2008 Knowledge Search database included 80 million pages user-generated information.
Another popular answer service Yahoo! Answers community-driven knowledge market website launched Yahoo!. allows users submit questions
answered answer questions users. People vote best answer. site
gives members chance earn points way encourage participation based
Naver model. December 2006, Yahoo! Answers 60 million users 65
1. http://kin.naver.com/

2

fiComplex Question Answering: Unsupervised Approaches

million answers. Google QA system2 based paid editors launched
April 2002 fully closed December 2006.
However, computational linguistics point view information synthesis
seen kind topic-oriented informative multi-document summarization. goal
produce single text compressed version set documents minimum loss
relevant information. Unlike indicative summaries (which help determine whether
document relevant particular topic), informative summaries must attempt find
answers.
paper, focus extractive approach summarization subset
sentences original documents chosen. contrasts abstractive summarization information text rephrased. Although summaries produced
humans typically extractive, state art summarization systems
based extraction achieve better results automated abstraction. Here,
experimented one empirical two well-known unsupervised statistical machine
learning techniques: K-means EM evaluated performance generating topicoriented summaries. However, performance approaches depends entirely
feature set used weighting features. order measure importance
relevance user query extract different kinds features (i.e. lexical, lexical
semantic, cosine similarity, basic element, tree kernel based syntactic shallow-semantic)
document sentences. used gradient descent local search technique
learn weights features.
Traditionally, information extraction techniques based BOW approach augmented language modeling. task requires use complex semantics, approaches based BOW often inadequate perform fine-level textual
analysis. improvements BOW given use dependency trees syntactic parse trees (Hirao, , Suzuki, Isozaki, & Maeda, 2004; Punyakanok, Roth, & Yih, 2004;
Zhang & Lee, 2003b), adequate dealing complex questions
whose answers expressed long articulated sentences even paragraphs. Shallow
semantic representations, bearing compact information, could prevent sparseness
deep structural approaches weakness BOW models (Moschitti, Quarteroni,
Basili, & Manandhar, 2007). pinpointing answer question relies deep understanding semantics both, attempting application syntactic semantic
information complex QA seems natural. best knowledge, study used
tree kernel functions encode syntactic/semantic information complex tasks
computing relatedness query sentences document sentences
order generate query-focused summaries (or answers complex questions).
methods generating summaries (i.e. empirical, K-means EM) show effects
syntactic shallow-semantic features BOW features.
past three years, complex questions focus much attention
automatic question-answering Multi Document Summarization (MDS) communities. Typically, current complex QA evaluations including 2004 AQUAINT
Relationship QA Pilot, 2005 Text Retrieval Conference (TREC) Relationship QA Task,
TREC definition (and others) require systems return unstructured lists can2. http://answers.google.com/

3

fiChali, Joty, & Hasan

didate answers response complex question. However recently, MDS evaluations (including 2005, 2006 2007 Document Understanding Conference (DUC)) tasked
systems returning paragraph-length answers complex questions responsive,
relevant, coherent.
experiments based DUC 2007 data show including syntactic semantic features improves performance. Comparison among approaches shown.
Comparing DUC 2007 participants, systems achieve top scores
statistically significant difference results system results DUC
2007 best system.
paper organized follows: Section 2 focuses related work, Section 3
gives brief description intended final model, Section 4 describes features
extracted, Section 5 discusses learning issues presents learning approaches,
Section 6 discusses remove redundant sentences adding final
summary, Section 7 describes experimental study. conclude discuss future
directions Section 8.

2. Related Work
Researchers world working query-based summarization trying different
directions see methods provide best results.
number sentence retrieval systems based IR (Information Retrieval)
techniques. systems typically dont use lot linguistic information, still
deserve special attention. Murdock Croft (2005) propose translation model specifically
monolingual data, show significantly improves sentence retrieval query
likelihood. Translation models train parallel corpus used corpus question/answer pairs. Losada (2005) presents comparison multiple-Bernoulli models
multinomial models context sentence retrieval task shows multivariate Bernoulli model really outperform popular multinomial models retrieving
relevant sentences. Losada Fernandez (2007) propose novel sentence retrieval method
based extracting highly frequent terms top retrieved documents. results reinforce idea top retrieved data valuable source enhance retrieval systems.
specially true short queries usually query-sentence matching terms. argue method improves significantly precision top ranks
handling poorly specified information needs.
LexRank method addressed Erkan Radev (2004) successful
generic multi-document summarization. topic-sensitive LexRank proposed Otterbacher, Erkan, Radev (2005). LexRank, set sentences document cluster
represented graph nodes sentences, links nodes induced similarity relation sentences. system ranks sentences
according random walk model defined terms inter-sentence similarities
similarities sentences topic description question.
Concepts coherence cohesion enable us capture theme text. Coherence represents overall structure multi-sentence text terms macro-level
relations clauses sentences (Halliday & Hasan, 1976). Cohesion, defined
Halliday Hasan (1976), property holding text together one single grammat4

fiComplex Question Answering: Unsupervised Approaches

ical unit based relations (i.e. ellipsis, conjunction, substitution, reference, lexical
cohesion) various elements text. Lexical cohesion defined cohesion
arises semantic relations (collocation, repetition, synonym, hypernym, hyponym, holonym, meronym, etc.) words text (Morris & Hirst, 1991).
Lexical cohesion among words represented lexical chains sequences
semantically related words. summarization methods based lexical chain first extract nouns, compound nouns named entities candidate words (Li, Sun, Kit, &
Webster, 2007). using WordNet3 systems find semantic similarity
nouns compound nouns. lexical chains built two steps:
1. Building single document strong chains disambiguating senses words.
2. Building multi-chain merging strongest chains single documents
one chain.
systems rank sentences using formula involves a) lexical chain, b) keywords query c) named entities. example, Li et al. (2007) uses following
formula:
Score = P (chain) + P (query) + P (namedEntity)
P (chain) sum scores chains whose words come
candidate sentence, P (query) sum co-occurrences key words topic
sentence, P (namedEntity) number name entities existing topic
sentence. three coefficients , set empirically. top ranked
sentences selected form summary.
Harabagiu et al. (2006) introduce new paradigm processing complex questions
relies combination (a) question decompositions; (b) factoid QA techniques;
(c) Multi-Document Summarization (MDS) techniques. question decomposition
procedure operates Markov chain. is, following random walk mixture
model bipartite graph relations established concepts related topic
complex question subquestions derived topic-relevant passages manifest
relations. Decomposed questions submitted state-of-the-art QA system
order retrieve set passages later merged comprehensive answer MDS system. show question decompositions using method
significantly enhance relevance comprehensiveness summary-length answers
complex questions.
approaches based probabilistic models (Pingali, K., & Varma,
2007; Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, & Vanderwende, 2007). Pingali
et al. (2007) rank sentences based mixture model component
model statistical model:
Score(s) = QIScore(s) + (1 ) QF ocus(s, Q)

(1)

3. WordNet (http://wordnet.princeton.edu/) widely used semantic lexicon English language.
groups English words (i.e. nouns, verbs, adjectives adverbs) sets synonyms called synsets,
provides short, general definitions (i.e. gloss definition), records various semantic relations
synonym sets.

5

fiChali, Joty, & Hasan

Score(s) score sentence s. Query-independent score (QIScore)
query-dependent score (QFocus) calculated based probabilistic models. Toutanova
et al. (2007) learns log-linear sentence ranking model maximizing three metrics
sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, (c) Model Frequency.
scoring function learned fitting weights set feature functions sentences
document set trained optimize sentence pair-wise ranking criterion.
scoring function adapted apply summaries rather sentences take
account redundancy among sentences.
Pingali et al. (2007) reduce document-sentences dropping words
contain important information. Toutanova et al. (2007), Vanderwende, Suzuki,
Brockett (2006), Zajic, Lin, Dorr, Schwartz (2006) heuristically decompose
document-sentences smaller units. apply small set heuristics parse
tree create alternatives original sentence (possibly multiple)
simplified versions available selection.
approaches multi-document summarization try cluster sentences
together. Guo Stylios (2003) use verb arguments (i.e. subjects, times, locations
actions) clustering. sentence method establishes indices information
based verb arguments (subject first index, time second, location third
action fourth). sentences closest subjects index put
cluster sorted according temporal sequence earliest
latest. Sentences spaces/locations index value cluster
marked out. clusters ranked based sizes top 10 clusters
chosen. Then, applying cluster reduction module system generates compressed
extract summaries.
approaches Recognizing Textual Entailment, Sentence Alignment,
Question Answering use syntactic and/or semantic information order measure
similarity two textual units. indeed motivated us include syntactic
semantic features get structural similarity document sentence query
sentence (discussed Section 4.1). MacCartney, Grenager, de Marneffe, Cer, Manning
(2006) use typed dependency graphs (same dependency trees) represent text
hypothesis. try find good partial alignment typed dependency
graphs representing hypothesis (contains n nodes) text (graph contains
nodes) search space O((m + 1)n). use incremental beam search combined
node ordering heuristic approximate global search space possible
alignments. locally decomposable scoring function chosen score
alignment sum local node edge alignment scores. scoring measure
designed favor alignments align semantically similar subgraphs, irrespective
polarity. reason, nodes receive high alignment scores words represent
semantically similar. Synonyms antonyms receive highest score unrelated
words receive lowest. Alignment scores incorporate local edge scores based
shape paths nodes text graph correspond adjacent
nodes hypothesis graph. final step make decision whether
hypothesis entailed text conditioned typed dependency graphs
well best alignment them. make decision use supervised
6

fiComplex Question Answering: Unsupervised Approaches

statistical logistic regression classifier (with feature space 28 features) Gaussian
prior parameter regularization.
Hirao et al. (2004) represent sentences using Dependency Tree Path (DTP) incorporate syntactic information. apply String Subsequence Kernel (SSK) measure
similarity DTPs two sentences. introduce Extended String
Subsequence Kernel (ESK) incorporate semantics DTPs. Kouylekov Magnini
(2005) use tree edit distance algorithms dependency trees text
hypothesis recognize textual entailment. According approach, text entails
hypothesis H exists sequence transformations (i.e. deletion, insertion
substitution) applied obtain H overall cost certain
threshold. Punyakanok et al. (2004) represent question sentence containing
answer dependency trees. add semantic information (i.e. named entity,
synonyms related words) dependency trees. apply approximate
tree matching order decide similar given pair trees are. use
edit distance matching criteria approximate tree matching. methods
show improvement BOW scoring methods.

3. Approach
accomplish task answering complex questions extract various important features sentences document collection measure relevance
query. sentences document collection analyzed various levels
document sentences represented vector feature-values. feature set
includes lexical, lexical semantic, statistical similarity, syntactic semantic features,
graph-based similarity measures (Chali & Joty, 2008b). reimplemented many
features successfully applied many related fields NLP.
use simple local search technique fine-tune feature weights. use
statistical clustering algorithms: EM K-means select relevant sentences
summary generation. Experimental results show systems perform better
include tree kernel based syntactic semantic features though summaries based
syntactic semantic feature achieve good results. Graph-based cosine
similarity lexical semantic features important selecting relevant sentences.
find local search technique outperforms two EM performs
better K-means based learning. later sections describe subparts
systems details.

4. Feature Extraction
section, describe features used score sentences.
provide detailed examples4 show get feature values. first describe
syntactic semantic features introducing work. follow
detailed description features commonly used question answering
summarization communities.
4. query document sentences used examples taken DUC 2007 collection.

7

fiChali, Joty, & Hasan

4.1 Syntactic Shallow Semantic Features
task query-based summarization requires use complex syntactic
semantics, approaches BOW often inadequate perform fine-level
textual analysis. importance syntactic semantic features context
described Zhang Lee (2003a), Moschitti et al. (2007), Bloehdorn Moschitti
(2007a), Moschitti Basili (2006) Bloehdorn Moschitti (2007b).
effective way integrate syntactic semantic structures machine learning algorithms use tree kernel functions (Collins & Duffy, 2001; Moschitti & Quarteroni,
2008) successfully applied question classification (Zhang & Lee, 2003a;
Moschitti & Basili, 2006). Syntactic semantic information used effectively measure similarity two textual units MacCartney et al. (2006). best
knowledge, study used tree kernel functions encode syntactic/semantic
information complex tasks computing relatedness query
sentences document sentences. Another good way encode shallow syntactic
information use Basic Elements (BE) (Hovy, Lin, Zhou, & Fukumoto, 2006)
uses dependency relations. experiments show including syntactic semantic
features improves performance sentence selection complex question answering
task (Chali & Joty, 2008a).
4.1.1 Encoding Syntactic Structures
Basic Element (BE) Overlap Measure Shallow syntactic information based dependency relations proved effective finding similarity two textual
units (Hirao et al., 2004). incorporate information using Basic Elements
defined follows (Hovy et al., 2006):
head major syntactic constituent (noun, verb, adjective adverbial phrases),
expressed single item.
relation head-BE single dependent, expressed triple:
(head|modifier|relation).
triples encode syntactic information one decide whether two units
match not- easily longer units (Hovy et al., 2006). extracted BEs
sentences (or query) using package distributed ISI5 .
get BEs sentence, computed Likelihood Ratio (LR)
following Zhou, Lin, Hovy (2005). Sorting BEs according LR scores produced
BE-ranked list. goal generate summary answer users questions.
ranked list BEs way contains important BEs top may may
relevant users questions. filter BEs checking whether contain
word query word QueryRelatedWords (defined Section 4.3).
example, consider following sentence get score 0.77314.
Query: Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
5. website:http://www.isi.edu/ cyl/BE

8

fiComplex Question Answering: Unsupervised Approaches

Sentence: Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
Score: 0.77314
Here, decided|themes|obj considered contain word
query words query relevant words report|annual|mod taken
contains query word report. way, filter BEs related
query. score sentence sum scores divided number BEs
sentence. limiting number top BEs contribute calculation
sentence scores remove BEs little importance sentences
fewer important BEs. set threshold 100 topmost 100 BEs ranked
list contribute normalized sentence score computation. paper,
set threshold took BEs counted calculating scores
sentences.
Tree Kernels Approach order calculate syntactic similarity query
sentence first parse sentence well query syntactic tree
(Moschitti, 2006) using parser Charniak (1999). calculate similarity
two trees using tree kernel. reimplemented tree kernel model
proposed Moschitti et al. (2007).
build trees, next task measure similarity trees.
this, every tree represented dimensional vector v(T ) = (v1 (T ), v2 (T ), vm (T )),
i-th element vi (T ) number occurrences i-th tree fragment tree
. tree fragments tree sub-trees include least one production
restriction production rules broken incomplete parts (Moschitti
et al., 2007). Figure 1 shows example tree portion subtrees.

Figure 1: (a) example tree (b) sub-trees NP covering press.
Implicitly enumerate possible tree fragments 1, 2, , m. fragments
axis m-dimensional space. Note could done implicitly since
number extremely large. this, Collins Duffy (2001) define tree
kernel algorithm whose computational complexity depend m.
tree kernel two trees T1 T2 actually inner product v(T1 ) v(T2 ):
9

fiChali, Joty, & Hasan

K(T1 , T2 ) = v(T1 ).v(T2 )

(2)

define indicator function Ii (n) 1 sub-tree seen rooted node n
0 otherwise. follows:

vi (T1 ) =

X

X

Ii (n1 ), vi (T2 ) =

n1 N1

Ii (n2 )

(3)

n2 N2

N1 N2 set nodes T1 T2 respectively. So, derive:
K(T1 , T2 ) = v(T1 ).v(T2 ) =

X

vi (T1 )vi (T2 )



X

=

X X

n1 N1 n2 N2

X

=

X

Ii (n1 )Ii (n2 )



C(n1 , n2 )

(4)

n1 N1 n2 N2

define C(n1 , n2 ) = Ii (n1 )Ii (n2 ). Next, note C(n1 , n2 )
computed polynomial time due following recursive definition:
P

1. productions n1 n2 different C(n1 , n2 ) = 0
2. productions n1 n2 same, n1 n2 pre-terminals,
C(n1 , n2 ) = 1
3. Else productions n1 n2 pre-terminals,
nc(n1 )

C(n1 , n2 ) =



(1 + C(ch(n1 , j), ch(n2 , j)))

(5)

j=1

nc(n1 ) number children n1 tree; productions n1
n2 nc(n1 ) = nc(n2 ). i-th child-node n1 ch(n1 , i).
cases query composed two sentences compute similarity
document sentence (s) query-sentences (qi ) take
average scores syntactic feature value.
Syntactic similarity value =

Pn

i=1 K(qi , s)

n

n number sentences query q sentence consideration. TK similarity value (tree kernel) sentence query
sentence q based syntactic structure. example, following sentence
query q get score:
10

fiComplex Question Answering: Unsupervised Approaches

Figure 2: Example semantic trees
Query (q): Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence (s): Europes new currency, euro, rival U.S. dollar international
currency long term, Der Spiegel magazine reported Sunday.
Scores: 90, 41
Average Score: 65.5
4.1.2 Semantic Features
Though introducing syntactic information gives improvement BOW, use
syntactic parses, adequate dealing complex questions whose answers expressed long articulated sentences even paragraphs. Shallow semantic
representations, bearing compact information, could prevent sparseness deep
structural approaches weakness BOW models (MacCartney et al., 2006; Moschitti
et al., 2007).
Initiatives PropBank (PB) (Kingsbury & Palmer, 2002) made design
accurate automatic Semantic Role Labeling (SRL) systems ASSERT (Hacioglu, Pradhan, Ward, Martin, & Jurafsky, 2003) possible. Hence, attempting application SRL
QA seems natural pinpointing answer question relies deep understanding
semantics both. example, consider PB annotation:
[ARG0 all][TARGET use][ARG1 french franc][ARG2 currency]
annotation used design shallow semantic representation
matched semantically similar sentences, e.g.
[ARG0 Vatican][TARGET use][ARG1 Italian lira][ARG2 currency]
order calculate semantic similarity sentences first represent
annotated sentence (or query) using tree structures Figure 2 called Semantic Tree
(ST) proposed Moschitti et al. (2007). semantic tree arguments replaced
important wordoften referred semantic head. look noun
first, verb, adjective, adverb find semantic head argument.
none present take first word argument semantic head.
11

fiChali, Joty, & Hasan

Figure 3: Two STs composing STN
However, sentences rarely contain single predicate, rather typically propositions contain one subordinate clauses. instance, let us consider slight modification
second sentence: Vatican, located wholly within Italy uses Italian lira
currency. Here, main predicate uses subordinate predicate located.
SRL system outputs following two annotations:
(1) [ARG0 Vatican located wholly within Italy][TARGET uses][ARG1 Italian
lira][ARG2 currency]
(2) [ARG0 Vatican][TARGET located] [ARGM-LOC wholly][ARGM-LOC within
Italy] uses Italian lira currency
giving STs Figure 3. see Figure 3(A), argument node
corresponds entire subordinate clause label leaf ST (e.g. leaf
ARG0). ST node actually root subordinate clause Figure 3(B).
taken separately, STs express whole meaning sentence. Hence,
accurate define single structure encoding dependency two
predicates Figure 3(C). refer kind nested STs STNs.
Note tree kernel (TK) function defined Section 4.1.1 computes number
common subtrees two trees. subtrees subject constraint
nodes taken none children original tree. Though
definition subtrees makes TK function appropriate syntactic trees, well
suited semantic trees (ST). instance, although two STs Figure 2 share
subtrees rooted ST node, kernel defined computes match.
critical aspect steps (1), (2), (3) TK function productions
two evaluated nodes identical allow match descendants.
means common substructures cannot composed node
children effective ST representation would require. Moschitti et al. (2007) solve
problem designing Shallow Semantic Tree Kernel (SSTK) allows portions
ST match.
Shallow Semantic Tree Kernel (SSTK) reimplemented SSTK according
model given Moschitti et al. (2007). SSTK based two ideas: first, changes
12

fiComplex Question Answering: Unsupervised Approaches

ST, shown Figure 4 adding SLOT nodes. accommodate argument labels
specific order fixed number slots, possibly filled null arguments
encode possible predicate arguments. Leaf nodes filled wildcard character *
may alternatively accommodate additional information. slot nodes used
way adopted TK function generate fragments containing one
children example shown frames (b) (c) Figure 4. previously
pointed out, arguments directly attached root node kernel function
would generate structure children (or structure children, i.e.
empty) (Moschitti et al., 2007).

Figure 4: Semantic tree fragments
Second, original tree kernel would generate many matches slots filled
null label set new step 0 TK calculation:
(0) n1 (or n2 ) pre-terminal node child label null, C(n1 , n2 ) = 0;
subtract one unit C(n1 , n2 ), step 3:
nc(n1 )

(3) C(n1 , n2 ) =



j=1

(1 + C(ch(n1 , j), ch(n2 , j))) 1

(6)

changes generate new C which, substituted (in place original C )
Eq. 4, gives new SSTK.
example, following sentence query q get semantic score:
Query (q): Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence (s): Frankfurt-based body said annual report released today
decided two themes new currency history European civilization
abstract concrete paintings.
Scores: 6, 12
Average Score: 9
13

fiChali, Joty, & Hasan

4.2 Lexical Features
Here, discuss lexical features commonly used QA
summarization communities. reimplemented research.
4.2.1 N-gram Overlap
N-gram overlap measures overlapping word sequences candidate document
sentence query sentence. view measure overlap scores, query pool
sentence pool created. order create query (or sentence) pool, took
query (or document) sentence created set related sentences replacing
content words6 first-sense synonyms using WordNet. example, given stemmed
document-sentence: John write poem, sentence pool contains: John compose
poem, John write verse form along given sentence.
measured recall based n-gram scores sentence P using following formula:
N gramScore(P ) = maxi (maxj N gram(si , qj ))
P
gram Countmatch (gramn )
N gram(S, Q) = P n
gramn Count (gramn )

(7)
(8)

n stands length n-gram (n = 1, 2, 3, 4), Countmatch (gramn )
number n-grams co-occurring query candidate sentence, qj j-th
sentence query pool, si i-th sentence sentence pool sentence P .
1-gram Overlap Measure
1-gram overlap score measures number words common sentence hand
query related words. computed follows:
1gram Overlap Score =

P

Countmatch (w1 )
w1 Count (w1 )

w1

P

(9)

set content words candidate sentence Countmatch
number matches sentence content words query related words. Count (gramn )
number w1 .
Note order measure 1-gram score took query related words instead
exact query words. motivation behind sentence word(s)
exactly query words synonyms, hypernyms, hyponym gloss
words, get counted.
Example:
Query Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence Frankfurt-based body said annual study released today
decided two themes new currency: history European civilization
abstract concrete paintings.
6. hence forth content words nouns, verbs, adverbs adjectives.

14

fiComplex Question Answering: Unsupervised Approaches

1-gram Score 0.06666 (After normalization7 ).
Note sentence 1-gram overlap score 0.06666 even though
exact word common query words. got score sentence word
study synonym query word report.
N-gram Overlap Measures
above, calculate n-gram overlap scores. example, considering
following query sentence document sentence (From DUC 2007 collection), 4
matching 2-grams: (1 1999,of Euro, January January 1). Hence, employing
formula given above, get following 2-gram score normalization. 3-gram score
found accordingly.
Query Sentence: Describe steps taken worldwide reaction prior introduction
Euro January 1, 1999. Include predictions expectations reported
press.
Document Sentence: Despite skepticism actual realization single European currency scheduled January 1, 1999, preparations design
Euro note already begun.
2-gram: 0.14815
3-gram: 0.0800
4.2.2 LCS WLCS
sequence W = [w1 , w2 , ..., wn ] subsequence another sequence X = [x1 , x2 , ..., xm ] ,
exists strict increasing sequence [i1 , i2 , ..., ] indices X
j = 1, 2, ..., n xij = wj (Cormen, Leiserson, & Rivest, 1989). Given two sequences
S1 S2 , longest common subsequence (LCS) S1 S2 common subsequence
maximum length (Lin, 2004).
longer LCS two sentences is, similar two sentences are.
used LCS-based F-measure estimate similarity document sentence
length query sentence Q length n follows:
LCS(S, Q)

LCS(S, Q)
Plcs (S, Q) =
n
Flcs (S, Q) = (1 ) Plcs (S, Q) + Rlcs (S, Q)
Rlcs (S, Q) =

(10)
(11)
(12)

LCS(S, Q) length longest common subsequence Q,
constant determines importance precision recall. computing
LCS measure document sentence query sentence viewed sequence words.
7. normalize feature values corresponding sentence respect entire context
particular document.

15

fiChali, Joty, & Hasan

intuition longer LCS two similar are.
recall (Rlcs (S, Q)) ratio length longest common subsequence
Q document sentence length measures completeness. Whereas precision
(Plcs (S, Q)) ratio length longest common subsequence Q
query sentence length measure exactness. obtain equal importance
precision recall set value 0.5. Equation 12 called LCS-based
F-measure. Notice Flcs 1 when, S=Q; Flcs 0 nothing
common Q.
One advantage using LCS require consecutive matches insequence matches reflect sentence level word order n-grams. advantage
automatically includes longest in-sequence common n-grams. Therefore, predefined n-gram length necessary. Moreover, property value less
equal minimum unigram (i.e. 1-gram) F-measure Q. Unigram recall
reflects proportion words present Q; unigram precision
proportion words Q S. Unigram recall precision count
co-occurring words regardless orders; LCS counts in-sequence co-occurrences.
awarding credit in-sequence unigram matches, LCS measure captures
sentence level structure natural way. Consider following example:
S1 John shot thief
S2 John shot thief
S3 thief shot John
Using S1 reference sentence, S2 S3 sentences consideration S2
S3 would 2-gram score since one bigram (i.e. thief)
common S1. However, S2 S3 different meanings. case LCS S2
score 3/4=0.75 S3 score 2/4=0.5 = 0.5. Therefore, S2 better
S3 according LCS.
However, LCS suffers one disadvantage counts main in-sequence
words; therefore, alternative LCSes shorter sequences reflected
final score. example, given following candidate sentence:
S4 thief John shot
Using S1 reference, LCS counts either thief John shot both;
therefore, S4 LCS score S3 2-gram would prefer S4 S3.
order measure LCS score sentence took similar approach previous section using WordNet (i.e. creation sentence pool query pool). calculated
LCS score using following formula:

LCS score = maxi (maxj Flcs (si , qj ))

(13)

qj j-th sentence query pool, si i-th sentence
sentence pool.
16

fiComplex Question Answering: Unsupervised Approaches

basic LCS problem differentiate LCSes different spatial
relations within embedding sequences (Lin, 2004). example, given reference
sequence two candidate sequences Y1 Y2 follows:
S: B C E F G
Y1 : B C H K
Y2 : H B K C
Y1 Y2 LCS score. However, Y1 better choice Y2
Y1 consecutive matches. improve basic LCS method store length
consecutive matches encountered far regular two dimensional dynamic program table
computing LCS. call weighted LCS (WLCS) use k indicate length
current consecutive matches ending words xi yj . Given two sentences X Y,
WLCS score X computed using similar dynamic programming
procedure stated Lin (2004). use WLCS advantage measuring
similarity taking words higher dimension string kernels indeed
reduces time complexity. before, computed WLCS-based F-measure
way using query pool sentence pool.
W LCS score = maxi (maxj Fwlcs (si , qj ))

(14)

Example:
Query Sentence: Describe steps taken worldwide reaction prior introduction
Euro January 1, 1999. Include predictions expectations reported
press.
Document Sentence: Despite skepticism actual realization single European currency scheduled January 1, 1999, preparations design
Euro note already begun.
find 6 matching strings: (of 1 Euro 1999 January) longest common
subsequence considering sentence related sentences. WLCS set
weight 1.2. normalization, get following LCS WLCS scores
sentence applying formula.
LCS Score: 0.27586
WLCS Score: 0.15961
4.2.3 Skip-Bigram Measure
skip-bigram pair words sentence order allowing arbitrary gaps. Skipbigram measures overlap skip-bigrams candidate sentence query
sentence (Lin, 2004). rely query pool sentence pool using
WordNet. Considering following sentences:
17

fiChali, Joty, & Hasan

S1 John shot thief
S2 John shoot thief
S3 thief shoot John
S4 thief John shot
get sentence C(4,2)=6 skip-bigrams8 . example, S1 following
skip-bigrams: (John shot, John the, John thief, shot the, shot thief
thief) S2 three skip bi-gram matches S1 (John the, John thief, thief),
S3 one skip bi-gram match S1 (the thief), S4 two skip bi-gram matches
S1 (John shot, thief).
skip bi-gram score document sentence length query
sentence Q length n computed follows:
SKIP2 (S, Q)
C(m, 2)
SKIP2 (S, Q)
Pskip2 (S, Q) =
C(n, 2)
Fskip2 (S, Q) = (1 ) Pskip2 (S, Q) + Rskip2 (S, Q)
Rskip2 (S, Q) =

(15)
(16)
(17)

SKIP2 (S, Q) number skip bi-gram matches Q,
constant determines importance precision recall. set value
0.5 associate equal importance precision recall. C combination
function. call equation 17 skip bigram-based F-measure. computed skip
bigram-based F-measure using formula:

SKIP BIGRAM = maxi (maxj Fskip2 (si , qj ))

(18)

example, given following query sentence, get 8 skip-bigrams: (on 1,
January 1, January 1999, Euro, 1 1999, 1999, January on).
Applying equations above, get skip bi-gram score 0.05218 normalization.
Query Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence Despite skepticism actual realization single European currency
scheduled January 1, 1999, preparations design Euro note
already begun.
Skip bi-gram Score: 0.05218
8. C(n, r) =

n!
r!(nr)!

18

fiComplex Question Answering: Unsupervised Approaches

Note skip bi-gram counts in-order matching word pairs LCS counts
one longest common subsequence. put constraint maximum skip distance,
dskip , two in-order words form skip bi-gram avoids spurious matches
from. example, set dskip 0 equivalent bi-gram
overlap measure (Lin, 2004). set dskip 4 word pairs 4 words
apart form skip bi-grams. experiment set dskip = 4 order ponder
4 words apart get skip bi-grams.
Modifying equations: 15, 16, 17 allow maximum skip distance limit
straightforward: following Lin (2004) count skip bi-gram matches, SKIP2 (S, Q),
within maximum skip distance replace denominators equations
actual numbers within distance skip bi-grams reference sentence
candidate sentence respectively.
4.2.4 Head Head Related-words Overlap
number head words common two sentences indicate much
relevant other. order extract heads sentence (or query),
sentence (or query) parsed Minipar9 dependency tree extract
heads call exact head words. example, head word sentence: John
eats rice eat.
take synonyms, hyponyms, hypernyms10 query-head words
sentence-head words form set words call head-related words.
measured exact head score head-related score follows:
P

w1 HeadSet Countmatch (w1 )

(19)

w1 HeadRelSet Countmatch (w1 )

(20)

ExactHeadScore =
HeadRelatedScore =

P

P

P

w1 HeadSet Count (w1 )

w1 HeadRelSet Count (w1 )

HeadSet set head words sentence Countmatch number
matches HeadSet query sentence. HeadRelSet set
synonyms, hyponyms, hypernyms head words sentence Countmatch
number matches head-related words query sentence.
example, list head words query sentence measures:
Query: Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Heads Query: include, reaction, step, take, describe, report, Euro, introduction, press,
prediction, 1999, expectation
Sentence: Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
9. http://www.cs.ualberta.ca/ lindek/minipar.htm
10. hypernym hyponym levels restricted 2 3 respectively.

19

fiChali, Joty, & Hasan

Heads Sentence: history, release, currency, body, report,painting, say, civilization,
theme, decide.
Exact Head Score:

1
11

= 0.09

Head Related Score: 0
4.3 Lexical Semantic Features
form set words call QueryRelatedWords taking content words
query, first-sense synonyms, nouns hypernyms/hyponyms, nouns
gloss definitions using WordNet.
4.3.1 Synonym Overlap
synonym overlap measure overlap list synonyms content
words extracted candidate sentence query related words. computed
follows:
Synonym Overlap Score =

P

w1 SynSet Countmatch (w1 )
w1 SynSet Count (w1 )

P

(21)

SynSet synonym set content words sentence Countmatch
number matches SynSet query related words.
4.3.2 Hypernym/Hyponym Overlap
hypernym/hyponym overlap measure overlap list hypernyms (level
2) hyponyms (level 3) nouns extracted sentence consideration
query related words. computed follows:
Hypernym/hyponym overlap score =

P

h1 HypSet Countmatch (h1 )
h1 HypSet Count (h1 )

P

(22)

HypSet hyponym/hyponym set nouns sentence Countmatch
number matches HypSet query related words.
4.3.3 Gloss Overlap
gloss overlap measure overlap list content words extracted
gloss definition nouns sentence consideration query related
words. computed follows:
Gloss Overlap Score =

P

g1 GlossSet Countmatch (g1 )

P

g1 GlossSet Count (g1 )

(23)

GlossSet set content words (i.e. nouns, verbs adjectives) taken
gloss definition nouns sentence Countmatch number matches
GlossSet query related words.
20

fiComplex Question Answering: Unsupervised Approaches

Example:
example, given query following sentence gets synonym overlap score
0.33333, hypernym/hyponym overlap score 0.1860465 gloss overlap score 0.1359223.
Query Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
Synonym Overlap Score: 0.33333
Hypernym/Hyponym Overlap Score: 0.1860465
Gloss Overlap Score: 0.1359223
4.4 Statistical Similarity Measures
Statistical similarity measures based co-occurrence similar words corpus.
Two words termed similar belong context. used thesaurus
provided Dr. Dekang Lin11 purpose. used two statistical similarity
measures:
Dependency-based similarity measure
method uses dependency relations among words order measure similarity (Lin, 1998b). extracts dependency triples uses statistical approach
measure similarity. Using given corpus one retrieve similar words
given word. similar words grouped clusters.
Note word one cluster. cluster represents
sense word similar words sense. So, selecting right cluster
word problem. goals are: i) create bag similar words query
words ii) get bag similar words (dependency based) query words
measure overlap score sentence words.
Creating Bag Similar Words:
query-word extract clusters thesaurus. order
determine right cluster query word measure overlap score
query related words (i.e. exact words, synonyms, hypernyms/hyponyms gloss)
clusters. hypothesis cluster words common query
related words right cluster assumption first synonym correct
sense. choose cluster word highest overlap score.

Overlap scorei =

P

w1 QueryRelatedW ords Countmatch (w1 )

(24)

Cluster = argmaxi (Overlap Scorei )

(25)

w1 QueryRelatedW ords Count (w1 )

P

11. http://www.cs.ualberta.ca/ lindek/downloads.htm

21

fiChali, Joty, & Hasan

QueryRelatedWords set exact words, synonyms, hyponyms/hypernyms,
gloss words words query (i.e query words) Countmatch number
matches query related words ith cluster similar words.
Measuring Overlap Score:
get clusters query words measured overlap
cluster words sentence words call dependency based similarity measure:

DependencyM easure =

w1 SenW ords Countmatch (w1 )

P

P

w1 SenW ords Count (w1 )

(26)

SenWords set words sentence Countmatch number
matches sentence words cluster similar words.
Proximity-based similarity measure
similarity computed based linear proximity relationship words
(Lin, 1998a). uses information theoretic definition similarity measure
similarity. similar words grouped clusters. took similar approach
measure feature previous section except used different thesaurus.
Example:
Considering following query sentence get following measures:
Query: Describe steps taken worldwide reaction prior introduction Euro
January 1, 1999. Include predictions expectations reported press.
Sentence: Frankfurt-based body said annual report released today
decided two themes new currency: history European civilization
abstract concrete paintings.
Dependency-based Similarity Score: 0.0143678
Proximity-based Similarity Score: 0.04054054
4.5 Graph-based Similarity Measure
Erkan Radev (2004) used concept graph-based centrality rank set sentences
producing generic multi-document summaries. similarity graph produced
sentences document collection. graph node represents sentence.
edges nodes measure cosine similarity respective pair sentences.
degree given node indication important sentence is. Figure 5
shows example similarity graph 4 sentences.
similarity graph constructed, sentences ranked according
eigenvector centrality. LexRank performed well context generic summarization. apply LexRank query-focused context topic-sensitive version LexRank
proposed Otterbacher et al. (2005). followed similar approach order calculate
feature. score sentence determined mixture model relevance
sentence query similarity sentence high-scoring sentences.
22

fiComplex Question Answering: Unsupervised Approaches

Figure 5: LexRank similarity
Relevance Question
first stem sentences collection compute word IDFs (Inverse
Document Frequency) using following formula:
N +1
idfw = log
0.5 + sfw




(27)

N total number sentences cluster, sfw number
sentences word w appears in.
stem questions remove stop words. relevance sentence
question q computed by:
rel(s|q) =

X

wq

log (tfw,s + 1) log (tfw,q + 1) idfw

(28)

tfw,s tfw,q number times w appears q, respectively.
Mixture Model
previous section measured relevance sentence question
sentence similar high scoring sentences cluster high
score. instance, sentence gets high score based question relevance
model likely contain answer question related sentence, may
similar question itself, likely contain answer (Otterbacher et al.,
2005).
capture idea following mixture model:

p(s|q) =

(

X
sim(s, v)
rel(s|q)
P
+ (1 d)
P
zC rel(z|q)
zC sim(z, v)
vC

)

p(v|q)

(29)

p(s|q), score sentence given question q, determined sum
relevance question similarity sentences collection.
C set sentences collection. value parameter call
23

fiChali, Joty, & Hasan

bias trade-off two terms equation set empirically. higher
values prefer relevance question similarity sentences.
denominators terms normalization. Although computationally
expensive, equation 29 calculates sum entire collection since required
model sense global impact voting sentences. measure
cosine similarity weighted word IDFs similarity two sentences cluster:

sim(x, y) = qP

P

wx,y

tfw,x tfw,y (idfw )2

2
xi x (tfxi ,x idfxi )

qP

2
yi (tfyi ,y idfyi )

(30)

Equation 29 written matrix notation follows:
p = [dA + (1 d)B]T p

(31)

square matrix given index i, elements i-th column
proportional rel(i|q). B square matrix entry B(i,j)
proportional sim(i,j). matrices normalized row sums add 1.
Note result normalization rows resulting square matrix Q =
[dA + (1 d)B] add 1. matrix called stochastic defines Markov
chain. view sentence state Markov chain Q(i,j) specifies
transition probability state state j corresponding Markov chain.
vector p looking Eq. 31 stationary distribution Markov chain.
intuitive interpretation stationary distribution understood concept
random walk graph representation Markov chain. probability
transition made current node nodes similar query.
probability (1-d) transition made nodes lexically similar current
node. Every transition weighted according similarity distributions. element
vector p gives asymptotic probability ending corresponding state
long run regardless starting state. stationary distribution Markov chain
computed simple iterative algorithm called power method (Erkan & Radev,
2004). starts uniform distribution. iteration eigenvector updated
multiplying transpose stochastic matrix. Since Markov chain
irreducible aperiodic algorithm guaranteed terminate.

5. Ranking Sentences
use several methods order rank sentences generate summaries applying
features described Section 4. section describe systems detail.
5.1 Learning Feature-weights: Local Search Strategy
order fine-tune weights features, used local search technique. Initially set feature-weights, w1 , , wn , equal values (i.e. 0.5) (see Algorithm 1).
train weights using DUC 2006 data set. Based current weights
score sentences generate summaries accordingly. evaluate summaries using
24

fiComplex Question Answering: Unsupervised Approaches

Input: Stepsize l, Weight Initial Value v
Output: vector w
~ learned weights
Initialize weight values wi v.
1 n
rg1 = rg2 = prev = 0
(true)
scoreSentences(w)
~
generateSummaries()
rg2 = evaluateROUGE()
rg1 rg2
prev = wi
wi + = l
rg1 = rg2
else
break
end
end
end
return w
~
Algorithm 1: Tuning weights using Local Search technique
automatic evaluation tool ROUGE (Lin, 2004) (described Section 7) ROUGE
value works feedback learning loop. learning system tries maximize
ROUGE score every step changing weights individually specific step size (i.e.
0.01). means, learn weight wi change value wi keeping weight
values (wj j6=i ) stagnant. weight wi algorithm achieves local maximum
(i.e. hill climbing) ROUGE value.
learned feature-weights compute final scores sentences
using formula:
scorei = x~i .w
~

(32)

x~i feature vector i-th sentence, w
~ weight vector, scorei
score i-th sentence.
5.2 Statistical Machine Learning Approaches
experimented two unsupervised statistical learning techniques features
extracted previous section sentence selection problem:
1. K-means learning
2. Expectation Maximization (EM) learning
5.2.1 K-means Learning
K-means hard clustering algorithm defines clusters center mass
members. start set initial cluster centers chosen randomly go
25

fiChali, Joty, & Hasan

several iterations assigning object cluster whose center closest.
objects assigned recompute center cluster centroid
) members. distance function use squared Euclidean distance
mean (
instead true Euclidean distance.
Since square root monotonically growing function squared Euclidean distance
result true Euclidean distance computation overload smaller
square root dropped.
learned means clusters using K-means algorithm next
task rank sentences according probability model. used Bayesian
model order so. Bayes law says:

x|qk , )P (qk |)
p(x
x|)
p(x
x|qk , )P (qk |)
p(x
PK
x|qk , )p(qk |)
k=1 p(x

x, ) =
P (qk |x
=

(33)

qk cluster, x feature vector representing sentence, parameter
set class models. set weights clusters equiprobable (i.e. P (qk |) =
x|qk , ) using gaussian probability distribution. gaussian
1/K). calculated p(x
probability density function (pdf) d-dimensional random variable x given by:

x) =
p(,
) (x

e

1
x
)T 1 (x
x
)
(x
2

dp
)
2 det(

(34)

, mean vector, , covariance matrix, parameters
) K-means algorithm calculate
gaussian distribution. get means (
covariance matrix using unbiased covariance estimation procedure:

j =

N
1 X
xi j )(x
x j )T
(x
N 1 i=1

(35)

5.2.2 EM Learning
EM algorithm gaussian mixture models well known method cluster analysis.
useful outcome model produces likelihood value clustering model
likelihood values used select best model number different
models providing number parameters (i.e. number
clusters).
26

fiComplex Question Answering: Unsupervised Approaches

x) represented feature vector length
Input: sample n data-points (x
L
Input: Number Clusters K
Output: array K-means-based Scores
Data: Array dnK , K , K
Data: Array C K , nK
Randomly choose K data-points K initial means: k , k = 1, , K.
repeat
1 n
j 1 K
xi j k2 = (x
xi j )T (x
xi j )
ij = kx
end
ik < il , l 6= k
assign x C k .
end
end
P
1 K
C
x C

xj

j

=
C i|
|C
end
change occurs ;
/* calculating covariances cluster
1 K
C i|
= |C
j 1
C ij ) (C
C ij )T
+ = (C
end
= (1/(m 1))
end
/* calculating scores sentences
1 n
j 1 K
1
1


2 (x j ) j (x j )
yij = e

2

*/

*/

j )
det(

end
j 1 K
P
// where, wj = 1/K
zij = (yij wj )/ K
j=1 yij wj ;
end
k ) k
= max(
Push zim
end
return
Algorithm 2: Computing K-means based similarity measure

27

fiChali, Joty, & Hasan

significant problem EM algorithm converges local maximum
likelihood function hence quality result depends initialization.
problem along method improving initialization discussed later
section.
EM soft version K-means algorithm described above. K-means
start set random cluster centers c1 ck . iteration soft assignment
data-points every cluster calculating membership probabilities. EM
iterative two step procedure: 1. Expectation-step 2. Maximization-step.
expectation step compute expected values hidden variables hi,j cluster
membership probabilities. Given current parameters compute likely
object belongs clusters. maximization step computes likely
parameters model given cluster membership probabilities.
data-points considered generated mixture model k-gaussians
form:

P (x) =

k
X

P (C = i)P (x|C = i) =

i=1

k
X

, )
P (C = i)P (x|

(36)

i=1

total likelihood model k components, given observed data points
X = x 1 , , x n , is:

L(|X)

=


n X
k


i=1 j=1

x |j ) =
P (C = j)P (x

n
X

k
X

i=1

log

j=1

n X
k


i=1 j=1

xi |
j , j )
wj P (x

xi |
j , j ) ( taking log likelihood )
wj P (x

(37)
(38)

P probability density function (i.e. eq 34). j j mean
covariance matrix component j, respectively. component contributes proportion,
P
wj , total population that: K
j=1 wj = 1.
Log likelihood used instead likelihood turns product sum.
describe EM algorithm estimating gaussian mixture.
Singularities covariance matrix must non-singular invertible.
EM algorithm may converge position covariance matrix becomes singular
| = 0) close singular, means invertible anymore. covariance
(|
matrix becomes singular close singular EM may result wrong clusters.
restrict covariance matrices become singular testing cases iteration
algorithm follows:
q

| > 1e9 ) update
( |
else update
28

fiComplex Question Answering: Unsupervised Approaches

Discussion: Starting values EM algorithm
convergence rate success clustering using EM algorithm degraded
poor choice starting values means, covariances, weights components. experimented one summary (for document number D0703A DUC
2007) order test impact initial values EM algorithm. cluster
means initialized
p heuristic spreads randomly around ean(DAT A)
standard deviation Cov(DAT A) 10. initial covariance set Cov(DAT A)
initial values weights wj = 1/K K number clusters.
is, d-dimensional data-points parameters j th component follows:

~j = rand(1, , d)
j = (DAT A)
wj

q

(DAT A) 10 + ~(DAT A)

= 1/K

highly variable nature results tests reflected inconsistent values total log likelihood results repeated experiments indicated
using random starting values initial estimates means frequently gave poor
results. two possible solutions problem. order get good results
using random starting values (as specified algorithm) run EM algorithm several times choose initial configuration get maximum
log likelihood among configurations. Choosing best one among several runs
computer intensive process. So, improve outcome EM algorithm gaussian
mixture models, necessary find better method estimating initial means
components.
best starting position EM algorithm, regard estimates means,
would one estimated mean per cluster closer true mean
cluster.
achieve aim explored widely used K-means algorithm cluster
(means) finding method. is, means found K-means clustering
utilized initial means EM calculate initial covariance matrices
using unbiased covariance estimation procedure (Equation 35).
Ranking Sentences
sentences clustered EM algorithm, identify sentences
xi , ) qr denotes clusare question-relevant checking probabilities, P (qr |x
x , ) > 0.5 x considered
ter question-relevant. sentence x , P (qr |x
question-relevant. cluster mean values greater one
considered question-relevant cluster.
next task rank question-relevant sentences order include
summary. done easily multiplying feature vector x~i weight
vector w
~ learned applying local search technique (Equation 32).
29

fiChali, Joty, & Hasan

Input: Sample n data-points ( x ) represented feature vector length
L
Input: Number Clusters K
Output: array EM-based Scores
k , k ) k = 1, , K, equal priors set
Start K initial Gaussian models: N (
P (qk ) = 1/K.
repeat
(i)
x j , (i) )
/* Estimation step: compute probability P (qk |x
(i)

data point xj , j = 1, , n, belong class qk
j 1 n
k 1 K
(i)
x j , (i) ) =
P (qk |x

(i)

(i)

xj |qk , (i) )
P (qk |(i) )p(x
xj |(i) )
p(x
(i)

=
end
end
/* Maximization step:
k 1 K
j 1 n
// update means:
i+1
k

=

(i)

(i)

x j |
k , k )
P (qk |(i) )p(x

(i)
(i)
(i)
x j |
(i)
k=1 P (qk | )p(x
k , k )

PK

*/

=

// update variances:
(i+1)
k

*/

(i)
xj , (i) )
j=1 x j P (qk |x
Pn
(i)
xj , (i) )
j=1 P (qk |x

Pn

(i)
xj
xj , (i) )(x
xj (i+1)
)(x
j=1 P (qk |x
k
PN
(i)
xj , (i) )
j=1 P (qk |x

Pn

(i+1)
)

k

// update priors:

P (qk (i + 1)|(i+1) ) =

n
1X
(i)
x j , (i) )
P (qk |x
n j=1

end
end
total likelihood increase falls desired threshold ;
return
Algorithm 3: Computing EM-based similarity measure

30

fiComplex Question Answering: Unsupervised Approaches

6. Redundancy Checking Generating Summary
sentences scored easiest way create summaries output
topmost N sentences required summary length reached. case,
ignoring factors: redundancy coherence.
know text summarization clearly entails selecting salient information putting together coherent summary. answer summary consists
multiple separately extracted sentences different documents. Obviously,
selected text snippets individually important. However, many competing sentences included summary issue information overlap parts
output comes mechanism addressing redundancy needed. Therefore,
summarization systems employ two levels analysis: first content level every
sentence scored according features concepts covers, second textual level,
when, added final output, sentences deemed important
compared similar candidates included final answer summary. Goldstein, Kantrowitz, Mittal, Carbonell (1999)
observed authors called Maximum-Marginal-Relevance (MMR). Following Hovy et al. (2006) modeled overlap intermediate summary
to-be-added candidate summary sentence.
call overlap ratio R, R 0 1 inclusively. Setting R = 0.7
means candidate summary sentence, s, added intermediate summary,
S, sentence overlap ratio less equal 0.7.

7. Experimental Evaluation
section describes results experiments conducted using DUC12 2007 dataset
provided NIST 13 . questions experiments address include:
different features affect behavior summarizer system?
one algorithms (K-means, EM Local Search) performs better
particular problem?
used main task DUC 2007 evaluation. task was:
Given complex question (topic description) collection relevant documents,
task synthesize fluent, well-organized 250-word summary documents
answers question(s) topic.
documents DUC 2007 came AQUAINT corpus comprising newswire
articles Associated Press New York Times (1998-2000) Xinhua News
Agency (1996-2000). NIST assessors developed topics interest choose set
25 documents relevant (document cluster) topic. topic document
cluster given 4 different NIST assessors including developer topic.
assessor created 250-word summary document cluster satisfies information
12. http://www-nlpir.nist.gov/projects/duc/
13. National Institute Standards Technology

31

fiChali, Joty, & Hasan

need expressed topic statement. multiple reference summaries used
evaluation summary content.
purpose experiments study impact different features. accomplish generated summaries 45 topics DUC 2007 seven
systems defined below:
LEX system generates summaries based lexical features (Section 4.2):
n-gram (n=1,2,3,4), LCS, WLCS, skip bi-gram, head, head synonym overlap.
LEXSEM system considers lexical semantic features (Section 4.3): synonym, hypernym/hyponym, gloss, dependency-based proximity-based similarity.
SYN system generates summary based syntactic feature (Section 4.1.1).
COS system generates summary based graph-based method (Section 4.5).
SYS1 system considers features except syntactic semantic features
(All features except section 4.1).
SYS2 system considers features except semantic feature (All features
except section 4.1.2)
system generates summaries taking features (Section 4) account.
7.1 Automatic Evaluation
ROUGE carried automatic evaluation summaries using ROUGE (Lin,
2004) toolkit, widely adopted DUC automatic summarization evaluation. ROUGE stands Recall-Oriented Understudy Gisting Evaluation.
collection measures determines quality summary comparing reference summaries created humans. measures count number overlapping units
n-gram, word-sequences, word-pairs system-generated summary
evaluated ideal summaries created humans. available ROUGE measures
are: ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W ROUGE-S. ROUGE-N n-gram
recall candidate summary set reference summaries. ROUGE-L measures
longest common subsequence (LCS) takes account sentence level structure
similarity naturally identifies longest co-occurring insequence n-grams automatically.
ROUGE-W measures weighted longest common subsequence (WLCS) providing improvement basic LCS method computation credit sentences
consecutive matches words. ROUGE-S overlap skip-bigrams candidate summary set reference summaries skip-bigram pair words
sentence order allowing arbitrary gaps. ROUGE measures
applied automatic evaluation summarization systems achieved promising
results (Lin, 2004).
systems, report widely accepted important metrics: ROUGE-2
ROUGE-SU. present ROUGE-1 scores since never shown
correlate human judgement. ROUGE measures calculated running
32

fiComplex Question Answering: Unsupervised Approaches

ROUGE-1.5.5 stemming removal stopwords. ROUGE run-time parameters
set DUC 2007 evaluation setup. are:
ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a
show 95% confidence interval important evaluation metrics systems
report significance meaningful comparison. use ROUGE tool
purpose. ROUGE uses randomized method named bootstrap resampling compute
confidence interval. used 1000 sampling points bootstrap resampling.
report evaluation scores one baseline system (The BASE column)
tables order show level improvement systems achieve. baseline
system generates summaries returning leading sentences (up 250 words)
hT EXT field recent document(s).
presenting results highlight top two F-scores bottom one F-score
indicate significance glance.
7.1.1 Results Discussion
K-means Learning Table 1 shows ROUGE-1 scores different combinations
features K-means learning. noticeable K-means performs best
graph-based cosine similarity feature. Note including syntactic feature
improve score. Also, including syntactic semantic features increases score
significant amount. Summaries based lexical features give us good
ROUGE-1 evaluation.
Scores
Recall
Precision
F-score

LEX
0.366
0.397
0.381

LEXSEM
0.360
0.393
0.376

SYN
0.346
0.378
0.361

COS
0.378
0.408
0.393

SYS1
0.376
0.403
0.389

SYS2
0.365
0.415
0.388


0.366
0.415
0.389

BASE
0.312
0.369
0.334

Table 1: ROUGE-1 measures K-means learning

Table 2 shows ROUGE-2 scores different combinations features K-means
learning. ROUGE-1 graph-based cosine similarity feature performs well here.
get significant improvement ROUGE-2 score include syntactic feature
features. Semantic features affect score much. Lexical Semantic features
perform well here.

Scores
Recall
Precision
F-score

LEX
0.074
0.080
0.077

LEXSEM
0.076
0.084
0.080

SYN
0.063
0.069
0.065

COS
0.085
0.092
0.088

SYS1
0.074
0.080
0.077

SYS2
0.077
0.107
0.090


0.076
0.109
0.090

Table 2: ROUGE-2 measures K-means learning

33

BASE
0.060
0.072
0.064

fiChali, Joty, & Hasan

Table 3 shows: ROUGE-SU scores best features without syntactic
semantic. Including syntactic/semantic features features degrades scores.
Summaries based lexical features achieve good scores.
Scores
Recall
Precision
F-score

LEX
0.131
0.154
0.141

LEXSEM
0.127
0.152
0.138

SYN
0.116
0.139
0.126

COS
0.139
0.162
0.149

SYS1
0.135
0.176
0.153

SYS2
0.134
0.174
0.152


0.134
0.174
0.152

BASE
0.105
0.124
0.112

Table 3: ROUGE-SU measures K-means learning
Table 4 shows 95% confidence interval (for F-measures K-means learning)
important ROUGE evaluation metrics systems comparison confidence
interval baseline system. seen systems performed significantly
better baseline system cases.
Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


ROUGE-1
0.326680 - 0.342330
0.362976 - 0.400498
0.357154 - 0.395594
0.345512 - 0.377525
0.372804 - 0.413440
0.367817 - 0.408390
0.358237 - 0.400000
0.350756 - 0.404275

ROUGE-2
0.060870 - 0.068840
0.064983 - 0.090981
0.069909 - 0.091376
0.056041 - 0.076337
0.075127 - 0.104377
0.063284 - 0.095170
0.065219 - 0.093733
0.066281 - 0.095393

ROUGE-SU
0.108470 - 0.116720
0.128390 - 0.157784
0.126157 - 0.151831
0.116191 - 0.136799
0.134971 - 0.164885
0.132061 - 0.162509
0.123703 - 0.153165
0.124157 - 0.159447

Table 4: 95% confidence intervals K-means system

EM learning Table 5 Table 7 show different ROUGE measures feature
combinations context EM learning. easily noticed
measures get significant amount improvement ROUGE scores include
syntactic semantic features along features. get 3-15% improvement
SYS1 F-score include syntactic feature 2-24% improvement include
syntactic semantic features. cosine similarity measure perform well
K-means experiments. Summaries considering lexical features achieve
good results.
Table 8 shows 95% confidence interval (for F-measures EM learning) important ROUGE evaluation metrics systems comparison confidence
interval baseline system. see systems performed significantly
better baseline system cases.
Local Search Technique ROUGE scores based feature combinations
given Table 9 Table 11. Summaries generated including features perform
34

fiComplex Question Answering: Unsupervised Approaches

Scores
Recall
Precision
F-score

LEX
0.383
0.415
0.398

LEXSEM
0.357
0.390
0.373

SYN
0.346
0.378
0.361

COS
0.375
0.406
0.390

SYS1
0.379
0.411
0.395

SYS2
0.399
0.411
0.405


0.398
0.399
0.399

BASE
0.312
0.369
0.334


0.090
0.138
0.109

BASE
0.060
0.072
0.064


0.143
0.185
0.161

BASE
0.105
0.124
0.112

Table 5: ROUGE-1 measures EM learning

Scores
Recall
Precision
F-score

LEX
0.088
0.095
0.092

LEXSEM
0.079
0.087
0.083

SYN
0.063
0.069
0.065

COS
0.087
0.094
0.090

SYS1
0.084
0.091
0.088

SYS2
0.089
0.116
0.100

Table 6: ROUGE-2 measures EM learning

Scores
Recall
Precision
F-score

LEX
0.145
0.171
0.157

LEXSEM
0.128
0.153
0.139

SYN
0.116
0.139
0.126

COS
0.138
0.162
0.149

SYS1
0.143
0.167
0.154

SYS2
0.145
0.186
0.163

Table 7: ROUGE-SU measures EM learning

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


ROUGE-1
0.326680 - 0.342330
0.382874 - 0.416109
0.352610 - 0.395048
0.345512 - 0.377525
0.366364 - 0.410020
0.378068 - 0.413658
0.360319 - 0.414068
0.378177 - 0.412705

ROUGE-2
0.060870 - 0.068840
0.075084 - 0.110454
0.070816 - 0.095856
0.056041 - 0.076337
0.076088 - 0.104243
0.077480 - 0.099739
0.073661 - 0.112157
0.077515 - 0.115231

ROUGE-SU
0.108470 - 0.116720
0.144367 - 0.172449
0.125276 - 0.154562
0.115713 - 0.136599
0.133251 - 0.164110
0.141550 - 0.168759
0.130022 - 0.171378
0.141345 - 0.164849

Table 8: 95% confidence intervals EM system

35

fiChali, Joty, & Hasan

best scores measures. get 7-15% improvement SYS1 F-score
include syntactic feature 8-19% improvement SYS1 F-score include
syntactic semantic features. case lexical features (LEX) perform well
better features (ALL).
Scores
Recall
Precision
F-score

LEX
0.379
0.411
0.394

LEXSEM
0.358
0.390
0.373

SYN
0.346
0.378
0.361

COS
0.375
0.406
0.390

SYS1
0.382
0.414
0.397

SYS2
0.388
0.434
0.410


0.390
0.438
0.413

BASE
0.312
0.369
0.334

Table 9: ROUGE-1 measures local search technique

Scores
Recall
Precision
F-score

LEX
0.085
0.092
0.088

LEXSEM
0.079
0.087
0.083

SYN
0.063
0.069
0.065

COS
0.087
0.094
0.090

SYS1
0.086
0.093
0.090

SYS2
0.095
0.114
0.104


0.099
0.116
0.107

BASE
0.060
0.072
0.064

Table 10: ROUGE-2 measures local search technique

Scores
Recall
Precision
F-score

LEX
0.143
0.168
0.155

LEXSEM
0.128
0.153
0.139

SYN
0.116
0.139
0.126

COS
0.138
0.162
0.149

SYS1
0.145
0.170
0.157

SYS2
0.148
0.195
0.169


0.150
0.196
0.170

BASE
0.105
0.124
0.112

Table 11: ROUGE-SU measures local search technique
Table 12 shows 95% confidence interval (for F-measures local search technique)
important ROUGE evaluation metrics systems comparison confidence interval baseline system. find systems performed significantly
better baseline system cases.
7.1.2 Comparison
results reported see three algorithms systems clearly outperform baseline system. Table 13 shows F-scores reported ROUGE measures
Table 14 reports 95% confidence intervals baseline system, best system
DUC 2007, three techniques taking features (ALL) consideration.
see method based local search technique outperforms two
EM algorithm performs better K-means algorithm. analyze deeply, find
cases ROUGE-SU local search confidence intervals overlap
best DUC 2007 system.
36

fiComplex Question Answering: Unsupervised Approaches

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


ROUGE-1
0.326680 - 0.342330
0.380464 - 0.409085
0.353458 - 0.394853
0.345512 - 0.377525
0.366364 - 0.410020
0.381544 - 0.414534
0.370310 - 0.415768
0.384897 - 0.416301

ROUGE-2
0.060870 - 0.068840
0.078002 - 0.100107
0.070845 - 0.096261
0.056041 - 0.076337
0.076088 - 0.104243
0.079550 - 0.101246
0.078760 - 0.114175
0.084181 - 0.114753

ROUGE-SU
0.108470 - 0.116720
0.143851 - 0.166648
0.125342 - 0.154729
0.115713 - 0.136599
0.133251 - 0.164110
0.144551 - 0.170047
0.141043 - 0.174575
0.146302 - 0.171736

Table 12: 95% confidence intervals local search system

Algorithms
Baseline
Best System
K-means
EM
Local Search

ROUGE-1
0.334
0.438
0.389
0.399
0.413

ROUGE-2
0.064
0.122
0.089
0.109
0.107

ROUGE-SU
0.112
0.174
0.152
0.161
0.170

Table 13: ROUGE F-scores different systems

Algorithms
Baseline
Best System
K-means
EM
Local Search

ROUGE-1
0.326680 - 0.342330
0.431680 - 0.445970
0.350756 - 0.404275
0.378177 - 0.412705
0.384897 - 0.416301

ROUGE-2
0.060870 - 0.068840
0.118000 - 0.127680
0.066281 - 0.095393
0.077515 - 0.115231
0.084181 - 0.114753

ROUGE-SU
0.108470 - 0.116720
0.169970 - 0.179390
0.124157 - 0.159447
0.141345 - 0.164849
0.146302 - 0.171736

Table 14: 95% confidence intervals different systems

37

fiChali, Joty, & Hasan

7.2 Manual Evaluation
sample 105 summaries14 drawn different systems generated summaries
conduct extensive manual evaluation order analyze effectiveness
approaches. manual evaluation comprised Pyramid-based evaluation contents
user evaluation get assessment linguistic quality overall responsiveness.
7.2.1 Pyramid Evaluation
DUC 2007 main task, 23 topics selected optional community-based
pyramid evaluation. Volunteers 16 different sites created pyramids annotated
peer summaries DUC main task using given guidelines15 . 8 sites among
created pyramids. used pyramids annotate peer summaries
compute modified pyramid scores16 . used DUCView.jar17 annotation tool
purpose. Table 15 Table 17 show modified pyramid scores systems
three algorithms. baseline systems score reported. peer summaries
baseline system generated returning leading sentences (up 250 words)
hT EXT field recent document(s). results see
systems perform better baseline system inclusion syntactic semantic
features yields better scores. three algorithms notice lexical
semantic features best terms modified pyramid scores.
7.2.2 User Evaluation
10 university graduate students judged summaries linguistic quality overall
responsiveness. given score integer 1 (very poor) 5 (very good)
guided consideration following factors: 1. Grammaticality, 2. Non-redundancy,
3. Referential clarity, 4. Focus 5. Structure Coherence. assigned
content responsiveness score automatic summaries. content score
integer 1 (very poor) 5 (very good) based amount information
summary helps satisfy information need expressed topic narrative.
measures used DUC 2007. Table 18 Table 20 present average linguistic
quality overall responsive scores systems three algorithms.
baseline systems scores given meaningful comparison. closer look
results, find systems perform worse baseline system terms
linguistic quality achieve good scores case overall responsiveness.
obvious tables exclusion syntactic semantic features often causes
lower scores. hand, lexical lexical semantic features show good overall
responsiveness scores three algorithms.
14. 7 systems 3 algorithms, cumulatively 21 systems. Randomly chose
5 summaries 21 systems.
15. http://www1.cs.columbia.edu/ becky/DUC2006/2006-pyramid-guidelines.html
16. equals sum weights Summary Content Units (SCUs) peer summary matches,
normalized weight ideally informative summary consisting number contributors
peer.
17. http://www1.cs.columbia.edu/ ani/DUC2005/Tool.html

38

fiComplex Question Answering: Unsupervised Approaches

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Modified Pyramid Scores
0.13874
0.44984
0.51758
0.45762
0.50368
0.42872
0.41666
0.49900

Table 15: Modified pyramid scores K-means system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Modified Pyramid Scores
0.13874
0.51894
0.53226
0.45058
0.48484
0.47758
0.44734
0.49756

Table 16: Modified pyramid scores EM system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Modified Pyramid Scores
0.13874
0.49760
0.53912
0.43512
0.49510
0.46976
0.46404
0.47944

Table 17: Modified pyramid scores local search system

39

fiChali, Joty, & Hasan

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Linguistic Quality
4.24
3.08
4.08
3.24
4.00
2.72
3.12
3.56

Overall Responsiveness
1.80
3.20
3.80
3.60
3.60
2.20
2.80
3.80

Table 18: Linguistic quality responsive scores K-means system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Linguistic Quality
4.24
4.08
3.56
4.20
3.80
3.68
4.20
3.36

Overall Responsiveness
1.80
4.40
3.40
3.80
4.00
3.80
3.60
3.40

Table 19: Linguistic quality responsive scores EM system

Systems
Baseline
LEX
LEXSEM
SYN
COS
SYS1
SYS2


Linguistic Quality
4.24
3.24
3.12
2.64
3.40
3.40
3.12
3.20

Overall Responsiveness
1.80
2.40
4.20
2.00
3.40
3.60
3.80
3.20

Table 20: Linguistic quality responsive scores local search system

40

fiComplex Question Answering: Unsupervised Approaches

8. Conclusion Future Work
paper presented works answering complex questions. extracted eighteen important features sentences document collection. Later used
simple local search technique fine-tune feature weights. weight, wi ,
algorithm achieves local maximum ROUGE value. way, learn
weights rank sentences multiplying feature-vector weight-vector.
experimented two unsupervised learning techniques: 1) EM 2) K-means
features extracted. assume two clusters sentences: 1. queryrelevant 2. query-irrelevant. learned means clusters using K-means
algorithm used Bayesian model order rank sentences. learned means
K-means algorithm used initial means EM algorithm. applied EM algorithm cluster sentences two classes : 1) query-relevant 2)
query-irrelevant. take query-relevant sentences rank using learned
weights (i.e. local search). methods generating summaries filter
redundant sentences using redundancy checking module generate summaries
taking top N sentences.
experimented effects different kinds features. evaluated
systems automatically using ROUGE report significance results
95% confidence intervals. conducted two types manual evaluation: 1) Pyramid
2) User Evaluation analyze performance systems. experimental
results mostly show following: (a) approaches achieve promising results, (b)
empirical approach based local search technique outperforms two learning
techniques EM performs better K-means algorithm, (c) systems achieve
better results include tree kernel based syntactic semantic features,
(d) cases ROUGE-SU local search confidence intervals overlap
best DUC 2007 system.
experimenting supervised learning techniques (i.e. SVM, MAXENT, CRF etc) analyzing perform problem. Prior that, produced huge amount labeled data automatically using similarity measures ROUGE
(Toutanova et al., 2007).
future plan decompose complex questions several simple questions
measuring similarity document sentence query sentence.
certainly serve create limited trees subsequences might increase
precision. Thus, expect decomposing complex questions sets
subquestions entail systems improve average quality answers returned
achieve better coverage question whole.

Acknowledgments
thank anonymous reviewers useful comments earliest version
paper. Special thanks go colleagues proofreading paper. grateful
graduate students took part user evaluation process. research
reported supported Natural Sciences Engineering Research Council
(NSERC) research grant University Lethbridge.
41

fiChali, Joty, & Hasan

Appendix A. Stop Word List

reuters
may
nov
tue

accordingly

alone

another
anyway
appropriate
ask
awfully
becomes

better

cant
certainly
comes
containing
currently
didnt
dont

else
etc
everyone
except
followed
forth
get
goes
h
hasnt

ap
jun
dec
wed

across
aint
along
amid

anyways

asking
b
becoming
believe

c
cannot
changes
concerning
contains

different
done
edu
elsewhere
etc.
everything
f
following
four
gets
going



jan
jul
tech
thu
able
actually

already
among
anybody
anywhere
arent
associated



beyond
cmon
cant
clearly
consequently
corresponding
definitely


eg
enough
even
everywhere
far
follows

getting
gone
hadnt
havent

42

feb
aug
news
fri


allow

amongst
anyhow
apart
around

became

beside

cs
cause
co
consider
could
described

downwards
e.g.
entirely
ever
ex



given
got
happens


mar
sep
index
sat

afterwards
allows
although

anyone
appear

available

beforehand
besides
brief
came
causes
com
considering
couldnt
despite
doesnt

eight
especially
every
exactly
fifth
former
furthermore
gives
gotten
hardly


apr
oct
mon

according

almost
always

anything
appreciate
aside
away
become
behind
best


certain
come
contain
course


e
either
et
everybody
example
five
formerly
g
go
greetings

hes

fiComplex Question Answering: Unsupervised Approaches

hello
hereafter
hi

im
immediate
indicated
inward

keep
l
less
likely

mean


nearly
nevertheless
non
nothing

old
onto

overall
perhaps
probably
r
regarding

help
hereby

howbeit
ive

indicates


keeps
lately
lest
little
mainly
meanwhile
mostly

necessary
new
none
novel





placed
provides
rather
regardless

hence
herein

however
ie
inasmuch
inner
isnt

kept
later
let
look
many
merely
mr.
n
need
next
noone

often



p
please
q
rd
regards


hereupon


i.e.
inc
insofar

j
know
latter
lets
looking
may
might
ms.
namely
needs
nine

nowhere
oh
one
others

particular
plus
que

relatively

43



hither
id

indeed
instead
itd

knows
latterly

looks
maybe

much
nd
neither

normally

ok
ones
otherwise
outside
particularly
possible
quite
really
respectively

heres

hopefully
ill
ignored
indicate

itll
k
known
least
liked
ltd

moreover
must
near
never
nobody

obviously
okay

ought

per
presumably
qv
reasonably
right

fiChali, Joty, & Hasan


says
seemed
sensible
shall

sometime
specified
sup
tell
thanx

theres
thereupon
theyve

thus
towards
twice
unless
us
usually
via


werent
whenever
wherein
whither
whose
within
wouldnt
youd


said
second
seeming
sent


sometimes
specify
sure
tends


thereafter

think
though

tried
two
unlikely
use
uucp
viz
wasnt
weve


whereupon


without
x
youll



secondly
seems
serious

somebody
somewhat
specifying

th
thats

thereby

third
three
together
tries
u

used
v
vs
way
welcome
whats
wheres
wherever
whos

wont

youre
z

saw
see
seen
seriously
shouldnt
somehow
somewhere
still
ts

thats

therefore
theyd



truly
un
unto
useful
value
w

well
whatever
whereafter
whether
whoever
willing
wonder
yes
youve
zero

44

say
seeing
self
seven
since
someone
soon
sub
take
thank

thence
therein
theyll
thorough
throughout
took
try


uses
various
want
wed
went

whereas

whole
wish
would
yet


saying
seem
selves
several
six
something
sorry

taken
thanks


theres
theyre
thoroughly
thru
toward
trying
unfortunately
upon
using

wants
well

whence
whereby



would



fiComplex Question Answering: Unsupervised Approaches

References
Bloehdorn, S., & Moschitti, A. (2007a). Combined syntactic semantic kernels text
classification. 29th European Conference IR Research, ECIR 2007, pp. 307318
Rome, Italy.
Bloehdorn, S., & Moschitti, A. (2007b). Structure semantics expressive text kernels.
CIKM-2007, pp. 861864.
Chali, Y., & Joty, S. R. (2008a). Improving performance random walk model
answering complex questions.. Proceedings 46th Annual Meeting
ACL-HLT. Short Paper Section, pp. 912 OH, USA.
Chali, Y., & Joty, S. R. (2008b). Selecting sentences answering complex questions.
Proceedings EMNLP, pp. 304313 Hawaii, USA.
Charniak, E. (1999). Maximum-Entropy-Inspired Parser. Technical Report CS-99-12
Brown University, Computer Science Department.
Collins, M., & Duffy, N. (2001). Convolution Kernels Natural Language. Proceedings
Neural Information Processing Systems, pp. 625632 Vancouver, Canada.
Cormen, T. R., Leiserson, C. E., & Rivest, R. L. (1989). Introduction Algorithms.
MIT Press.
Erkan, G., & Radev, D. R. (2004). LexRank: Graph-based Lexical Centrality Salience
Text Summarization. Journal Artificial Intelligence Research, 22, 457479.
Goldstein, J., Kantrowitz, M., Mittal, V., & Carbonell, J. (1999). Summarizing Text Documents: Sentence Selection Evaluation Metrics. Proceedings 22nd International ACM Conference Research Development Information Retrieval,
SIGIR, pp. 121128 Berkeley, CA.
Guo, Y., & Stylios, G. (2003). New Multi-document Summarization System. Proceedings Document Understanding Conference. NIST.
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & Jurafsky, D. (2003). Shallow
Semantic Parsing Using Support Vector Machines. Technical Report TR-CSLR2003-03 University Colorado.
Halliday, M., & Hasan, R. (1976). Cohesion English. Longman, London.
Harabagiu, S., Lacatusu, F., & Hickl, A. (2006). Answering complex questions random
walk models. Proceedings 29th annual international ACM SIGIR conference
Research development information retrieval, pp. 220 227. ACM.
Hirao, T., , Suzuki, J., Isozaki, H., & Maeda, E. (2004). Dependency-based sentence
alignment multiple document summarization. Proceedings Coling 2004, pp.
446452 Geneva, Switzerland. COLING.
45

fiChali, Joty, & Hasan

Hovy, E., Lin, C. Y., Zhou, L., & Fukumoto, J. (2006). Automated Summarization Evaluation Basic Elements. Proceedings Fifth Conference Language
Resources Evaluation Genoa, Italy.
Kingsbury, P., & Palmer, M. (2002). Treebank PropBank. Proceedings
international conference Language Resources Evaluation Las Palmas, Spain.
Kouylekov, M., & Magnini, B. (2005). Recognizing textual entailment tree edit distance
algorithms. Proceedings PASCAL Challenges Workshop: Recognising Textual
Entailment Challenge.
Li, J., Sun, L., Kit, C., & Webster, J. (2007). Query-Focused Multi-Document Summarizer Based Lexical Chains. Proceedings Document Understanding
Conference Rochester. NIST.
Lin, C. Y. (2004). ROUGE: Package Automatic Evaluation Summaries. Proceedings Workshop Text Summarization Branches Out, Post-Conference Workshop
Association Computational Linguistics, pp. 7481 Barcelona, Spain.
Lin, D. (1998a). Information-Theoretic Definition Similarity. Proceedings
International Conference Machine Learning, pp. 296304 Madison, Wisconsin.
Lin, D. (1998b). Automatic Retrieval Clustering Similar Words. Proceedings
International Conference Computational Linguistics Association
Computational Linguistics, pp. 768774 Montreal, Canada.
Losada, D. (2005). Language modeling sentence retrieval: comparison
multiple-bernoulli models multinomial models. Information Retrieval Theory Workshop Glasgow, UK.
Losada, D., & Fernandez, R. T. (2007). Highly frequent terms sentence retrieval.
Proc. 14th String Processing Information Retrieval Symposium, SPIRE07, pp.
217228 Santiago de Chile.
MacCartney, B., Grenager, T., de Marneffe, M., Cer, D., & Manning, C. D. (2006). Learning recognize features valid textual entailments. Proceedings Human
Language Technology Conference North American Chapter ACL, p. 4148
New York, USA.
Morris, J., & Hirst, G. (1991). Lexical cohesion computed thesaural relations
indicator structure text. Computational Linguistics, 17 (1), 2148.
Moschitti, A. (2006). Efficient convolution kernels dependency constituent syntactic
trees. Proceedings 17th European Conference Machine Learning Berlin,
Germany.
Moschitti, A., & Basili, R. (2006). Tree Kernel approach Question Answer Classification Question Answering Systems. Proceedings 5th international
conference Language Resources Evaluation Genoa, Italy.
46

fiComplex Question Answering: Unsupervised Approaches

Moschitti, A., & Quarteroni, S. (2008). Kernels linguistic structures answer extraction. Proceedings 46th Conference Association Computational
Linguistics (ACL08). Short Paper Section Columbus, OH, USA.
Moschitti, A., Quarteroni, S., Basili, R., & Manandhar, S. (2007). Exploiting Syntactic
Shallow Semantic Kernels Question/Answer Classificaion. Proceedings
45th Annual Meeting Association Computational Linguistics, pp. 776783
Prague, Czech Republic. ACL.
Murdock, V., & Croft, W. B. (2005). translation model sentence retrieval. HLT 05:
Proceedings conference Human Language Technology Empirical Methods
Natural Language Processing, pp. 684691 Morristown, NJ, USA. ACL.
Otterbacher, J., Erkan, G., & Radev, D. R. (2005). Using Random Walks Questionfocused Sentence Retrieval. Proceedings Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 915922
Vancouver, Canada.
Pingali, P., K., R., & Varma, V. (2007). IIIT Hyderabad DUC 2007. Proceedings
Document Understanding Conference Rochester. NIST.
Punyakanok, V., Roth, D., & Yih, W. (2004). Mapping dependencies trees: application
question answering. Proceedings AI & Math Florida, USA.
Strzalkowski, T., & Harabagiu, S. (2008). Advances Open Domain Question Answering.
Springer.
Toutanova, K., Brockett, C., Gamon, M., Jagarlamudi, J., Suzuki, H., & Vanderwende,
L. (2007). pythy summarization system: Microsoft research duc 2007.
proceedings Document Understanding Conference Rochester. NIST.
Vanderwende, L., Suzuki, H., & Brockett, C. (2006). Microsoft Research DUC2006:
Task-Focused Summarization Sentence Simplification Lexical Expansion.
Proceedings Document Understanding Conference Rochester. NIST.
Zajic, D. M., Lin, J., Dorr, B. J., & Schwartz, R. (2006). Sentence Compression Component Multi-Document Summarization System. Proceedings Document
Understanding Conference Rochester. NIST.
Zhang, A., & Lee, W. (2003a). Question Classification using Support Vector Machines.
Proceedings Special Interest Group Information Retrieval, pp. 2632 Toronto,
Canada. ACM.
Zhang, D., & Lee, W. S. (2003b). Language Modeling Approach Passage Question
Answering. Proceedings Twelfth Text REtreival Conference, pp. 489495
Gaithersburg, Maryland.
Zhou, L., Lin, C. Y., & Hovy, E. (2005). BE-based Multi-dccument Summarizer
Query Interpretation. Proceedings Document Understanding Conference Vancouver, B.C., Canada.

47


