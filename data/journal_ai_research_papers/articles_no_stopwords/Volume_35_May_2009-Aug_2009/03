Journal Artificial Intelligence Research 35 (2009) 557-591

Submitted 11/08; published 07/09

Optimal Value Information Graphical Models
Andreas Krause

KRAUSEA @ CALTECH . EDU

California Institute Technology,
1200 E California Blvd.,
Pasadena, CA 91125 USA

Carlos Guestrin

GUESTRIN @ CS . CMU . EDU

Carnegie Mellon University,
5000 Forbes Ave.,
Pittsburgh, PA 15213 USA

Abstract
Many real-world decision making tasks require us choose among several expensive observations. sensor network, example, important select subset sensors
expected provide strongest reduction uncertainty. medical decision making tasks, one
needs select tests administer deciding effective treatment.
general practice use heuristic-guided procedures selecting observations. paper,
present first efficient optimal algorithms selecting observations class probabilistic
graphical models. example, algorithms allow optimally label hidden variables Hidden
Markov Models (HMMs). provide results selecting optimal subset observations,
obtaining optimal conditional observation plan.
Furthermore prove surprising result: graphical models tasks, one designs
efficient algorithm chain graphs, HMMs, procedure generalized polytree graphical models. prove optimizing value information NPPP -hard even
polytrees. follows results computing decision theoretic value information objective functions, commonly used practice, #P-complete problem even
Naive Bayes models (a simple special case polytrees).
addition, consider several extensions, using algorithms scheduling observation selection multiple sensors. demonstrate effectiveness approach several
real-world datasets, including prototype sensor network deployment energy conservation
buildings.

1. Introduction
probabilistic reasoning, one choose among several possible expensive observations,
often central issue decide variables observe order effectively increase
expected utility (Howard, 1966; Howard & Matheson, 1984; Mookerjee & Mannino, 1997;
Lindley, 1956). medical expert system, example, multiple tests available, test
different cost (Turney, 1995; Heckerman, Horvitz, & Middleton, 1993). systems,
thus important decide tests perform order become certain
patients condition, minimum cost. Occasionally, cost testing even exceed value
information possible outcome, suggesting discontinue testing.
following running example motivates research empirically evaluated Section 6.
Consider temperature monitoring task, wireless temperature sensors distributed across

c
2009
AI Access Foundation. rights reserved.

fiK RAUSE & G UESTRIN

building. task become certain temperature distribution, whilst minimizing
energy expenditure, critically constrained resource (Deshpande, Guestrin, Madden, Hellerstein, &
Hong, 2004). fine-grained building monitoring required obtain significant energy savings
(Singhvi, Krause, Guestrin, Garrett, & Matthews, 2005).
Many researchers suggested use myopic (greedy) approaches select observations (Scheffer, Decomain, & Wrobel, 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen,
1997; Bayer-Zubek, 2004; Kapoor, Horvitz, & Basu, 2007). Unfortunately, general, heuristic provide performance guarantees. paper, present efficient algorithms,
guarantee optimal nonmyopic value information chain graphical models. example,
algorithms used optimal active labeling hidden states Hidden Markov Models (HMMs, Baum & Petrie, 1966). address two settings: subset selection, optimal
subset observations obtained open-loop fashion, conditional plans, sequential,
closed-loop plan observation strategy depends actual value observed variables (c.f., Figure 1). knowledge, first optimal efficient algorithms
observation selection diagnostic planning based value information class graphical models. settings, address filtering smoothing versions: Filtering
important online decision making, decisions utilize observations made
past. Smoothing arises example structured classification tasks, temporal
dimension data, hence observations taken account. call approach
VO IDP algorithms use Dynamic Programming optimize Value Information. evaluate VO IDP algorithms empirically three real-world datasets, show
well-suited interactive classification sequential data.
inference problems graphical models, computing marginal distributions
finding probable explanation, solved efficiently chain-structured graphs,
solved efficiently polytrees. prove problem selecting best k
observations maximizing decision theoretic value information NPPP -complete even
discrete polytree graphical models, giving complexity theoretic classification core artificial
intelligence problem. NPPP -complete problems believed significantly harder NPcomplete even #P-complete problems commonly arising context graphical models.
furthermore prove evaluating decision-theoretic value information objective functions
#P-complete even case Naive Bayes models, simple special case polytree graphical
models frequently used practice (c.f., Domingos & Pazzani, 1997).
Unfortunately, hardness results show that, problem scheduling single sensor optimally solved using algorithms, problem scheduling multiple, correlated
sensors wildly intractable. Nevertheless, show VO IDP algorithms single sensor
scheduling used approximately optimize multi-sensor schedule. demonstrate
effectiveness approach real sensor network testbed building management.
summary, provide following contributions:
present first optimal algorithms nonmyopically computing optimizing value
information chain graphical models.
show optimizing decision theoretic value information NPPP -hard discrete
polytree graphical models. computing decision theoretic value information #Phard even Naive Bayes models.

558

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS



Tmorn =high?

Tnoon =high?

yes

Teve =high?

Figure 1: Example conditional plan.
present several extensions algorithms, e.g., tree graphical models
leaves, multiple correlated chains (for multi-sensor scheduling).
extensively evaluate algorithms several real-world problems, including sensor
scheduling real sensor testbed active labeling bioinformatics Natural Language Processing.

2. Problem Statement
assume state world described collection random variables
XV = (X1 , . . . , Xn ), V index set. example, V could denote set locations, Xi
models temperature reading sensor placed location V. subset
= {i1 , . . . , ik } V, use notation XA refer random vector XA = (Xi1 , . . . , Xik ).
algorithms extend continuous distributions, generally assume variables XV discrete. take Bayesian approach, assume prior probability distribution
P (XV ) outcomes variables. Suppose select subset variables, XA (for
V), observe XA = xA . example, set locations place sensors,
set medical tests decide perform. observing realization variables
XA = xA , compute posterior distribution variables P (XV | XA = xA ). Based
posterior probability obtain reward R(P (XV | XA = xA )). example, reward function could depend uncertainty (e.g., measured entropy) distribution
P (XV | XA = xA ). describe several examples detail below.
general, selecting observation, know ahead time observations
make. Instead, distribution possible observations. Hence,
interested expected reward, take expectation possible observations.
optimizing selection variables, consider different settings: subset selection, goal pick subset V variables, maximizing
X
= argmax
P (XA = xA )R(P (XV | XA = xA )),
(1)


xA

impose constraints set allowed pick (e.g., number
variables selected, etc.). subset selection setting, commit selection
variables get see realization.
Instead, sequentially select one variable other, letting choice depend
observations made past. setting, would find conditional plan

559

fiK RAUSE & G UESTRIN

maximizes
= argmax


X

P (xV )R(P (XV | X(xV ) = x(xV ) )).

(2)

xV

Hereby, conditional plan select different set variables possible state
world xV . use notation (xV ) V refer subset variables selected
conditional plan state XV = xV . Figure 1 presents example conditional plan
temperature monitoring example. define notion conditional planning formally
Section 4.2.
general setup selecting observations goes back decision analysis literature
notion value information Howard (1966) statistical literature notion
Bayesian Experimental Design Lindley (1956). paper, refer Problems (1)
(2) problems optimizing value information.
paper, show complexity solving value information problems depend properties probability distribution P . give first algorithms optimally
solving value information interesting challenging class distributions including Hidden Markov Models. present hardness results showing optimizing value information
wildly intractable (NPPP -complete) even probability distributions efficient inference possible (even Naive Bayes models discrete polytrees).
2.1 Optimization Criteria
paper, consider class local reward1 functions Ri , defined
marginal probability distributions variables Xi . class computational advantage
local rewards evaluated using probabilistic inference techniques. total reward
sum local rewards.
Let subset V. P (Xj | XA = xA ) denotes marginal distribution variable Xj conditioned observations XA = xA . example, temperature monitoring
application, Xj models temperature location j V. conditional marginal distribution
P (Xj = xj | XA = xA ) models conditional distribution temperature location j
observing temperature locations V.
classification purposes, appropriate consider max-marginals
P max (Xj = xj | XA = xA ) = max P (XV = xV , Xj = xj | XA = xA ),
xV

is, Xj set value xj , probability probable assignment XV = xV
random variables (including Xj simplicity notation) conditioned observations
XA = xA .
local reward Rj functional probability distribution P P max Xj .
is, Rj takes entire distribution variable Xj maps reward value. Typically,
reward functions chosen certain peaked distributions obtain higher reward.
simplify notation, write
Rj (Xj | xA ) , Rj (P (Xj | XA = xA ))
1. Local reward functions widely used additively independent utility models, (c.f., Keeney & Raiffa, 1976).

560

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

denote reward variable Xj upon observing XA = xA ,
X
Rj (Xj | XA ) ,
P (XA = xA )Rj (Xj | xA )
xA

refer expected local rewards, expectation taken assignments xA
observations A. Important local reward functions include:
Residual entropy. set
Rj (Xj | xA ) = H(Xj | xA ) =

X

P (xj | xA ) log2 P (xj | xA ),

xj

objective optimization problem becomes minimize sum residual entropies. Optimizing reward function attempts reduce uncertainty predicting marginals Xi .
choose reward function running example measure uncertainty temperature distribution.
P
Joint entropy. Instead minimizing sum residual entropies H(Xi ), attempt minimize joint entropy entire distribution,
X
H(XV ) =
P (xV ) log2 P (xV ).
xV

Note, joint entropy depends full probability distribution P (XV ), rather
marginals P (Xi ), hence local. Nevertheless, exploit chain rule joint
entropy H(XB ) set random variables B = {1, . . . , m} (c.f., Cover & Thomas, 1991),
H(XB ) = H(X1 ) + H(X2 | X1 ) + H(X3 | X1 , X2 ) + + H(Xm | X1 , . . . , Xm1 ).
Hence, choose local reward functions Rj (Xj | XA ) = H(Xj | X1 , . . . , Xj1 , XA ),
optimize non-local reward function (the joint entropy) using local reward functions.
Decision-theoretic value information. concept local reward functions includes
concept decision theoretic value information. notion value information widely
used (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), formalized, e.g.,
context influence diagrams (Howard & Matheson, 1984) Partially Observable Markov Decision Processes (POMDPs, Smallwood & Sondik, 1973). variable Xj , let Aj finite set
actions. Also, let Uj : Aj dom Xj R utility function mapping action Aj
outcome x dom Xj real number. maximum expected utility principle states actions
selected maximize expected utility,
X
EUj (a | XA = xA ) =
P (xj | xA )Uj (a, xj ).
xj

certain Xj , economically choose action. idea
captured notion value information, choose local reward function
Rj (Xj | xA ) = max EUj (a | xA ).


561

fiK RAUSE & G UESTRIN

Margin structured prediction. consider margin confidence:
Rj (Xj | xA ) = P max (xj | xA ) P max (xj | xA ),

xj = argmax P max (xj | xA ) xj = argmax P max (xj | xA ),
xj 6=xj

xj

describes margin likely outcome closest runner up. reward
function useful structured classification purposes, shown Section 6.
Weighted mean-squared error. variables continuous, might want minimize
mean squared error prediction. choosing
Rj (Xj | xA ) = wj Var(Xj | xA ),

Z
Var(Xj | xA ) =


P (xj | xA ) xj

Z

x0j P (x0j

|

xA )dx0j

2
dxj

conditional variance Xj given XA = xA , wj weight indicating importance
variable Xj .
Monitoring critical regions (Hotspot sampling). Suppose want use sensors detecting fire. generally, want detect, j, whether Xj Cj , Cj dom Xj
critical region variable Xj . local reward function
Rj (Xj | xA ) = P (Xj Cj | xA )
favors observations maximize probability detecting critical regions.
Function optimization (Correlated bandits). Consider setting collection
random variables XV taking numerical
P values interval [m, m], and, selecting
variables, get reward xi . setting arises want optimize unknown
(random) function, evaluating function expensive. setting, encouraged
evaluate function likely obtain high values. maximize expected
total reward choose local reward function
Z
Rj (Xj | xA ) = xj P (xj | xA )dxj ,
i.e., expectation variable Xj given observations xA . setting optimizing random
function considered version classical k-armed bandit problem correlated
arms. details relationship bandit problems given Section 8.
examples demonstrate generality notion local reward. Note examples apply continuous distributions well discrete distributions.

562

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

2.2 Cost Selecting Observations
want capture constraint observations expensive. mean
observation Xj associated positive penalty Cj effectively decreases reward.
example, might interested trading accuracy sensing energy expenditure. Alternatively, possible define budget B selecting observations, one associated
integer cost j . Here, want select observations whose sum cost within budget,
costs decrease reward. running example, sensors could powered
solar power, regain certain amount energy per day, allows certain amount
sensing. formulation optimization
penalties budgets.
P problems allows P
simplify notation write C(A) = jA Cj (A) = jA j extend C sets.
Instead fixed penalties costs per observation, depend state
world. example, medical domain, applying particular diagnostic test bear different
risks health patient, depending patients illness. algorithms develop
adapted accommodate dependencies straight-forward manner.
present details conditional planning algorithm Section 4.2.

3. Decomposing Rewards
section, present key observation allows us develop efficient algorithms
nonmyopically optimizing value information class chain graphical models.
algorithms presented Section 4.
set random variables XV = {X1 , . . . , Xn } forms chain graphical model (a chain),
Xi conditionally independent XV\{i1,i,i+1} given Xi1 Xi+1 . Without loss generality
assume joint distribution specified prior P (X1 ) variable X1
conditional probability distributions P (Xi+1 | Xi ). time series model temperature
measured one sensor example formulated chain graphical model. Note
transition probabilities P (Xi+1 | Xi ) allowed depend index (i.e., chain models
allowed nonstationary). Chain graphical models extensively used machine
learning signal processing.
Consider example Hidden Markov Model unrolled n time steps, i.e., V partitioned hidden variables {X1 , . . . , Xn } emission variables {Y1 , . . . , Yn }. HMMs,
Yi always observed variables Xi form chain. many applications,
discussed Section 6, observe hidden variables Xi well, e.g., asking
expert, addition observing emission variables. cases, problem selecting
expert labels belongs class chain graphical models addressed paper, since
variables Xi form chain conditional observed values emission variables Yi . idea
generalized class Dynamic Bayesian Networks separators time
slices size one, separators selected observation. formulation
includes certain conditional random fields (Lafferty, McCallum, & Pereira, 2001) form
chains, conditional emission variables (the features).
Chain graphical models originating time series additional, specific properties:
system online decision making, observations past present time steps
taken account, observations made future. generally referred
filtering problem. setting, notation P (Xi | XA ) refer distribution
Xi conditional observations XA prior including time i. structured classification
563

fiK RAUSE & G UESTRIN

Figure 2: Illustration decomposing rewards idea. reward chain 1:7 observing
variables X1 , X4 X7 decomposes sum chain 1:4 plus reward chain
4:7 plus immediate reward observing XP
4 minus cost observing X4 . Hereby
brevity use notation Rew(a : b) = bj=a Rj (Xj | X1 , X4 , X7 ).

problems discussed Section 6, general observations made anywhere chain must
taken account. situation usually referred smoothing problem. provide
algorithms filtering smoothing.
describe key insight, allows efficient optimization chains. Consider
set observations V. j variable observed, i.e., j A, local reward
simply R(Xj | XA ) = R(Xj | Xj ). consider j
/ A, let Aj subset
containing closest ancestor (and smoothing problem closest descendant) Xj
XA . conditional independence property graphical model implies that, given XAj , Xj
independent rest observed variables, i.e., P (Xj | XA ) = P (Xj | XAj ). Thus,
follows R(Xj | XA ) = R(Xj | XAj ).
observations imply expected reward set observations decomposes
along chain. simplicity notation, add two independent dummy variables X0 Xn+1 ,
R0 = C0 = 0 = Rn+1 = Cn+1 = n+1 = 0. Let = {i0 , . . . P
, im+1 } il < il+1 ,
i0 = 0 im+1 = n + 1. Using notation, total reward R(A) = j Rj (Xj | XA )
smoothing case given by:


iv+1 1

X
X
Riv (Xiv | Xiv ) Civ +
Rj (Xj | Xiv , Xiv+1 ) .
v=0

j=iv +1

filtering settings, simply replace Rj (Xj | Xiv , Xiv+1 ) Rj (Xj | Xiv ). Figure 2 illustrates
decomposition.

4. Efficient Algorithms Optimizing Value Information
section, present algorithms efficiently nonmyopically optimizing value information chain graphical models.
4.1 Efficient Algorithms Optimal Subset Selection Chain Models
subset selection problem, want find informative subset variables observe
advance, i.e., observations made. running example, would,
deploying sensors, identify k time points expected provide informative
sensor readings according model.

564

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

First, define objective function L subsets V
L(A) =

n
X

Rj (Xj | XA ) C(A).

(3)

j=1

subset selection problem find optimal subset
=

argmax L(A)
AV,(A)B

maximizing sum expected local rewards minus penalties, subject constraint
total cost must exceed budget B.
solve optimization problem using dynamic programming algorithm, chain
broken sub-chains using insight Section 3. Consider sub-chain variable Xa
Xb . define Lsm
a:b (k) represent expected total reward sub-chain Xa , . . . , Xb ,
lt
smoothing setting Xa Xb observed, budget level k. Lfa:b
(k) represents
expected reward filtering setting Xa observed. formally:
lt
(k)
Lfa:b

=

b1
X

max

A{a+1...b1}
j=a+1
(A)k

Rj (Xj | XA , Xa ) C(A),

filtering version,
Lsm
a:b (k) =

b1
X

max

Rj (Xj | XA , Xa , Xb ) C(A),

A{a+1...b1}
j=a+1
(A)k

smoothing version. Note cases, L0:n+1 (B) = maxA:(A)B L(A), Equation (3), i.e., computing values La:b (k), compute maximum expected total reward
entire chain.
f lt
compute Lsm
a:b (k) La:b (k) using dynamic programming. base case simply:
lt
Lfa:b
(0) =

b1
X

Rj (Xj | Xa ),

j=a+1

filtering,
b1
X

Lsm
a:b (0) =

Rj (Xj | Xa , Xb ),

j=a+1

smoothing. recursion La:b (k) two cases: choose spend
budget, reaching base case, break chain two sub-chains, selecting optimal
observation Xj , < j < b. filtering smoothing


La:b (k) = max La:b (0),
max
{Rj (Xj | Xj ) Cj + La:j (0) + Lj:b (k j )} .
j:a<j<b,j k

565

fiK RAUSE & G UESTRIN

Input: Budget B, rewards Rj , costs j penalties Cj
Output: Optimal selection observation times
begin
0 < b n + 1 compute La:b (0);
k = 1 B
0 < b n + 1
sel(1) := La:b (0);
j = + 1 b 1 sel(j) := Rj (Xj | Xj ) Cj + La:j (0) + Lj:b (k j );
La:b (k) = maxj{a+1,...,b1,1} sel(j);
a:b (k) = argmaxj{a+1,...,b1,1} sel(j);
end
end
:= 0; b := n + 1; k := B; := ;
repeat
j := a:b (k);
j 0 := {j}; := j; k := k j ;
j = 1 ;
end
Algorithm 1: VO IDP algorithm optimal subset selection (for filtering smoothing).
first, may seem recursion consider optimal split budget
two sub-chains. However, since subset problem open-loop order observations
irrelevant, need consider split points first sub-chain receives zero budget.
pseudo code implementation dynamic programming approach, call VO IDP
subset selection given Algorithm 1. algorithm fills dynamic programming tables
two loops, inner loop ranging pairs (a, b), < b, outer loop increasing k. Within
inner loop, computing best reward sub-chain b, fills table sel,
sel(j) reward could obtained making observation j, sel(1)
reward observation made.
addition computing optimal rewards La:b (k) could achieved sub-chain : b
budget k, algorithm stores choices a:b (k) realize maximum score. Here,
a:b (k) index next variable selected sub-chain : b budget
k, 1 variable selected. order recover optimal subset budget k,
Algorithm 1 uses quantities a:b recover optimal subset tracing maximal values
occurring dynamic programming equations. Using induction proof, obtain:
Theorem 1 (Subset Selection). dynamic programming algorithm described computes
optimal subset budget B ( 61 n3 + O(n2 ))B evaluations expected local rewards.
Note consider different costs variable, would simply choose j =
1 variables compute La:b (N ). note variables Xi continuous,
algorithm still applicable integrations inferences necessary computing
expected rewards performed efficiently. case, example, Gaussian linear
model (i.e., variables Xi normally distributed) local reward functions residual
entropies residual variances variable.

566

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

4.2 Efficient Algorithms Optimal Conditional Planning Chain Models
conditional plan problem, want compute optimal sequential querying policy :
observe variable, pay penalty, depending values observed past, select next
query, proceeding long budget suffices. objective find plan highest
expected reward, where, possible sequence observations, budget B exceeded.
filtering, select observations future, whereas smoothing case, next
observation anywhere chain. running example, filtering algorithm would
appropriate: sensors would sequentially follow conditional plan, deciding
informative times sense based previous observations. Figure 1 shows example
conditional plan.
4.2.1 F ROM UBSET ELECTION C ONDITIONAL P LANNING
Note contrast subset selection setting considered Section 4.1, conditional planning, set variables depends state world XV = xV . Hence,
state, conditional plan could select different set variables, (xV ) V. example, consider Figure 1, set possible observations V = {morn, noon, eve},
XV = {Tmorn , Tnoon , Teve }. world state xV = (high, low, high), conditional
plan presented Figure 1 would select (xV ) = {morn, eve}, whereas,
xV = (low, low, high), would select (xV ) = {morn, noon}. Since conditional plan
function (random) state world, set-valued random variable. order optimize
Problem (2), define objective function2
J() =

X

P (xV )

xV

n
X



Rj (Xj | x(xV ) ) C((xV )) ,

j=1

i.e., expected sum local rewards given observations made plan (xV ) state XV = xV
minus penalties selected variables, expectation taken respect
distribution P (XV ). addition defining value policy J(), define cost ()
() = max ((xV )),
xV

maximum cost (A) (as defined Section 2.2) set = (xV ) could selected
policy , state world XV = xV .
Based notation, goal find policy
= argmax J() () B,


i.e., policy maximum value, guaranteed never cost exceeding budget
B. Hereby class sequential policies (i.e., those, observations chosen
sequentially, based observations previously made).
useful introduce following notation:
J(xA ; k) = max J( | XA = xA ) () k,


(4)

2. Recall that, filtering setting, R(Xj | x(xV ) ) , R(Xj | xA0 ), A0 = {t (xV ) s.t. j}, i.e.,
observations past taken account.

567

fiK RAUSE & G UESTRIN


J( | XA = xA ) =

X

n
X


P (xV | XA = xA )
Rj (Xj | x(xV ) ) C((xV )) .

xV

j=1

Hence, J(xA ; k) best possible reward achieved sequential policy cost
k, observing XA = xA . Using notation, goal find optimal plan
reward J(; B).
value function J satisfies following recursion. base case considers exhausted
budget:
X
J(xA ; 0) =
Rj (Xj | xA ) C(A).
jV

recursion, holds





X
J(xA ; k) = max J(xA ; 0), max
,
P (xj | xA )J(xA , xj ; k j )


j
/

(5)

xj

i.e., best one state XA = xA budget k either stop selecting variables,
chose best next variable act optimally thereupon.
Note easily allow cost j depend state xj variable Xj . case,
would simply replace j j (xj ), define J(XA , r) = whenever r < 0. Equivalently,
let penalty C(A) depend state replacing C(A) C(xA ).
Relationship finite-horizon Markov Decision Processes (MDPs). Note function
J(xA ; k) defined (4) analogous concept value function Markov Decision Processes (c.f., Bellman, 1957): finite-horizon MDPs, value function V (s; k) models maximum expected reward obtainable starting state performing k actions. value
function holds
X
V (s; k) = R(s, k) + max
P (s0 | s, a)V (s0 ; k 1),


s0

P (s0 | s, a) probability transiting state s0 performing action state s,
R(s, k) immediate reward obtained state k steps still left. recursion,
similar Eq. (5), exploited value iteration algorithm solving MDPs.
conditional planning problem unit observation cost (i.e., (A) = |A|) could modeled
finite-horizon MDP, states correspond observed evidence XA = xA , actions correspond
observing variables (or making observation) transition probabilities given
probability observing particular instantiation selected variable. immediate reward
R(s, k) = 0 k > 0, R(s, 0) expected reward (in value information problem)
observing assignment (i.e., R(P (XV | s)) C(s)). observations unit cost,
MDP, holds V (xA ; k) = J(xA ; k). Unfortunately, conditional planning problem, since
state MDP uniquely determined observed evidence XA = xA , state space
exponentially large. Hence, existing algorithms solving MDPs exactly (such value iteration)
cannot applied solve large value information problems. Section 4.2.2, develop
efficient dynamic programming algorithm conditional planning chain graphical models
avoids exponential increase complexity.
568

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

4.2.2 DYNAMIC P ROGRAMMING PTIMAL C ONDITIONAL P LANNING C HAINS
propose dynamic programming algorithm obtaining optimal conditional plan
similar subset algorithm presented Section 4.1. Again, utilize decomposition
rewards described Section 3. difference observation selection budget
allocation depend actual values observations. order compute value
function J(xA ; k) entire chain, compute value functions Ja:b (xA ; k) subchains Xa , . . . , Xb .
base case dynamic programming approach deals zero budget setting:
f lt
Ja:b
(xa ; 0)

=

b1
X

Rj (Xj | Xa = xa ),

j=a+1

filtering,
sm
(xa , xb ; 0) =
Ja:b

b1
X

Rj (Xj | Xa = xa , Xb = xb ),

j=a+1

smoothing. recursion defines Ja:b (xa ; k) (or Ja:b (xa , xb ; k) smoothing), expected
reward problem restricted sub-chain Xa , . . . , Xb conditioned values Xa = xa
(and Xb = xb smoothing), budget limited k. compute quantity,
iterate possible split points j, < j < b. observe notable difference
filtering smoothing case. smoothing, must consider possible
splits budget two resulting sub-chains, since observation time j might
require us make additional, earlier observation:
X

n
sm
sm
P (Xj = xj | Xa = xa , Xb = xb )
Ja:b (xa ,xb ; k) = max Ja:b (xa , xb ; 0), max
a<j<b

Rj (Xj | xj ) Cj (xj ) +

max
0lkj (xj )

xj



sm
Ja:j
(xa , xj ; l)

+

sm
(xj , xb ; k
Jj:b



.
l j (xj ))

Looking back time possible filtering case, hence recursion simplifies

X
n
f lt
f lt
Ja:b (xa ; k) = max Ja:b (xa ; 0),
max
P (Xj = xj | Xa = xa )
a<j<b:j (xj )k

Rj (Xj | xj ) Cj (xj ) +

xj

f lt
Ja:j
(xa ; 0)

+

f lt
Jj:b
(xj ; k


j (xj ))
.

J f lt J sm , optimal reward obtained J0:n+1 (; B) = J(; B) = J( ).
Algorithm 2 presents pseudo code implementation smoothing version filtering case
straight-forward modification. call Algorithm 2 VO IDP algorithm conditional planning. algorithm fill dynamic programming tables using three loops, inner loop
ranging assignments xa , xb , middle loop ranging pairs (a, b) < b,
outer loop covers increasing values k B. Within innermost loop, algorithm
computes table sel sel(j) optimal reward achievable selecting variable j next.
569

fiK RAUSE & G UESTRIN

value expectation possible observation variable Xj make. Note
every possible instantiation Xj = xj different allocation remaining budget k j (xj )
left right sub-chain (a : j j : b respectively) chosen. quantity (j, xj )
tracks optimal budget allocation.
Input: Budget B, rewards Rj , costs j penalties Cj
Output: Optimal conditional plan (a:b , a:b )
begin
sm (x , x ; 0);
0 < b n + 1, xa dom Xa , xb dom Xb compute Ja:b
b
k = 1 B
0 < b n+1, xa dom Xa , xb dom Xb
sm (0);
sel(1) := Ja:b
< j < b
sel(j) := 0;
xj dom Xj
0 l k j (xj )
sm (x , x ; l) + J sm (x , x ; k l (x ));
bd(l) := Ja:j
j
j j
j
b
j:b
end
sel(j) := sel(j) + P (xj | xa , xb ) [Rj (Xj | xj ) Cj (xj ) + maxl bd(j)];
(j, xj ) = argmaxl bd(j);
end
end
sm (k) = max
Ja:b
j{a+1,...,b1,1} sel(j);
a:b (xa , xb ; k) = argmaxj{a+1,...,b1,1} sel(j);
xj dom Xa:b (k) a:b (xa , xb , xj ; k) = (a:b (k), xj );
end
end
end
Algorithm 2: VO IDP algorithm computating optimal conditional plan (for smoothing
setting).
Input: Budget k, observations Xa = xa , Xb = xb , ,
begin
j := a:b (xa , xb ; k);
j 0
Observe Xj = xj ;
l := a:b (xa , xb , xj ; k);
Recurse k := l, Xa = xa Xj = xj instead Xb = xb ;
Recurse k := k l j , Xj = xj instead Xa = xa , Xb = xb ;
end
end
Algorithm 3: Observation selection using conditional planning.
plan compactly encoded quantities a:b a:b . Hereby, a:b (xa , xb ; k)
determines next variable query observing Xa = xa Xb = xb , remaining budget k. a:b (xa , xb , xj ; k) determines allocation budget new observation
Xj = xj made. Considering exponential number possible sequences observations,
570

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

remarkable optimal plan represented using polynomial space. Algorithm 3
indicates computed plan executed. procedure recursive, requiring parameters := 0, xa := 1, b := n + 1, xb := 1 k := B initial call. temperature
monitoring example, could first collect temperature timeseries training data,
learn chain model data. Offline, would compute conditional plan (for
filtering setting), encode quantities a:b a:b . would deploy computed
plan actual sensor node, together implementation Algorithm 3. computation optimal plan (Algorithm 2) fairly computationally expensive, execution plan
(Algorithm 3) efficient (selecting next timestep observation requires single lookup
a:b a:b tables) hence well-suited deployment small, embedded device.
summarize analysis following Theorem:
Theorem 2 (Conditional Planning). algorithm smoothing presented computes
optimal conditional plan d3 B 2 ( 61 n3 + O(n2 )) evaluations local rewards,
maximum domain size random variables X1 , . . . , Xn . filtering case, optimal plan computed using d3 B ( 61 n3 + O(n2 )) evaluations, or, budget used,
d3 ( 16 n4 + O(n3 )) evaluations.
faster computation filtering / no-budget case obtained observing
require third maximum computation, distributes budget sub-chains.
Also, note contrary algorithm computing optimal subsets Section 4.1, Algorithm 2 requires evaluations form R(Xj | XA = xA ), general computed
d2 times faster expectations R(Xj | XA ). consideration, subset selection
algorithm general factor B faster, even though conditional planning algorithm
nested loops.
4.3 Efficient Algorithms Trees Leaves
Sections 4.1 4.2 presented dynamic programming-based algorithms optimize value information chain graphical models. fact, key observations Section 3
local rewards decompose along chains holds chain graphical models, trees.
formally, tree graphical model joint probability distribution P (XV ) collection
random variables XV P (XV ) factors
P (XV ) =

1
i,j (Xi , Xj ),
Z
(i,j)E

i,j nonnegative potential function, mapping assignments xi xj nonnegative
real numbers, E V V set edges form undirected tree index set V, Z
normalization constant enforcing valid probability distribution.
dynamic programming algorithms presented previous sections extended
tree models straightforward manner. Instead identifying optimal subsets conditional
plans sub-chains, algorithms would select optimal subsets plans sub-trees
increasing size. Note however number sub-trees grow exponentially number
leaves tree: star n leaves example number subtrees exponential
n. fact, counting number subtrees arbitrary tree n vertices believed
intractable (#P-complete, Goldberg & Jerrum, 2000). However, trees contain
571

fiK RAUSE & G UESTRIN

small (constant) number leaves, number subtrees polynomial, optimal subset
conditional plans computed polynomial time.

5. Theoretical Limits
Many problems solved efficiently discrete chain graphical models efficiently solved discrete polytrees3 . Examples include probabilistic inference probable explanation (MPE).
Section 4.3 however seen complexity dynamic programming algorithms chains increases dramatically extended trees: complexity increases exponentially number leafs tree.
prove that, perhaps surprisingly, problem optimizing value information,
exponential increase complexity cannot avoided, reasonable complexity theoretic assumptions. making statement formal, briefly review complexity classes
used results.
5.1 Brief Review Relevant Computational Complexity Classes
briefly review complexity classes used following statements presenting complete
problem class. details see, e.g., references Papadimitriou (1995)
Littman, Goldsmith, Mundhenk (1998). class NP contains decision problems
polynomial-time verifiable proofs. well-known complete problem 3SAT
instances Boolean formulas conjunctive normal form containing three literals per
clause (3CNF form). complexity class #P contains counting problems. complete problem
class #P #3SAT counts number satisfying instances 3CNF formula.
PP decision version class #P: complete problem AJSAT , decides
whether given 3CNF formula satisfied majority, i.e., half
possible assignments. B Turing machine based complexity classes, AB
complexity class derived allowing Turing machines deciding instances oracle calls
Turing machines B. intuitively think problems class AB
solved Turing Machine class A, special command solves problem B.
PP similar #P PPP = P#P , i.e., allow deterministic polynomial time Turing
machine access counting oracle, cannot solve complex problems give
access majority oracle. Combining ideas, class NPPP class problems
solved nondeterministic polynomial time Turing machines access majority
(or counting) oracle. complete problem NPPP EM AJSAT which, given 3CNF
variables X1 , . . . , X2n , decides whether exists assignment X1 , . . . , Xn
satisfied majority assignments Xn+1 , . . . , X2n . NPPP introduced
found natural class modeling AI planning problems seminal work Littman et al.
(1998). example, MAP assignment problem NPPP -complete general graphical
models, shown Park Darwiche (2004).
complexity classes satisfy following set inclusions (where inclusions assumed,
known strict):
P NP PP PPP = P#P NPPP .
3. Polytrees Bayesian Networks form trees edge directions dropped.

572

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

5.2 Complexity Computing Optimizing Value Information
order solve optimization problems, likely evaluate objective
function, i.e., expected local rewards. first result states that, even specialize decision theoretic value information objective functions defined Section 2.1, problem
intractable even Naive Bayes models, special case discrete polytrees. Naive Bayes models
often used classification tasks (c.f., Domingos & Pazzani, 1997), class variable
predicted noisy observations (features), assumed conditionally independent given
class variable. sense, Naive Bayes models next simplest (from perspective
inference) class Bayesian networks chains. Note Naive Bayes models correspond
stars referred Section 4.3, number subtrees exponential number
variables.
Theorem 3 (Hardness computation Naive Bayes models). computation decision
theoretic value information functions #P-complete even Naive Bayes models.
hard approximate factor unless P = NP.
immediate corollary subset selection problem PP-hard Naive Bayes
models:
Corollary 4 (Hardness subset selection Naive Bayes models). problem determining,
given Naive Bayes model, constants c B, cost function set decision-theoretic value
information objective functions Ri , whether subset variables V
L(A) c (A) B PP-hard.
fact, show subset selection arbitrary discrete polytrees (that general
Naive Bayes models, inference still tractable) even NPPP -complete, complexity
class containing problems believed significantly harder NP #P complete
problems. result provides complexity theoretic classification value information, core
AI problem.
Theorem 5 (Hardness subset selection computation polytrees). problem determining, given discrete polytree, constants c B, cost function set decision-theoretic
value information objective functions Ri , whether subset variables V
L(A) c (A) B NPPP -complete.
running example, implies generalized problem optimally selecting k sensors
network correlated sensors likely computationally intractable without resorting
heuristics. corollary extends hardness subset selection hardness conditional plans.
Corollary 6 (Hardness conditional planning computation polytrees). Computing conditional plans PP-hard Naive Bayes models NPPP -hard discrete polytrees.
proofs results section stated Appendix. rely reductions complete
problems NP, #P NPPP involving boolean formulae problems computing / optimizing value information. reductions inspired works Littman et al. (1998) Park
Darwiche (2004), require development novel techniques, new reductions
Boolean formulae Naive Bayes polytree graphical models associated appropriate reward
functions, ensuring observation selections lead feasible assignments Boolean formulae.
573

fiK RAUSE & G UESTRIN

Percent improvement

10

1

Optimal conditional plan

8

Mean margin optimal subset
Mean margin greedy heuristic

1
0.98

0.9

Mean F1 score

0.96

6

0.94

0.8

4
2

0.7
Optimal subset
Greedy heuristic

0
1

4

8
12
16
20
Number observations

24

(a) Sensor scheduling

0.6

0.9

Mean accuracy
greedy heuristic
1

2
3
4
5
Number observations

(b) CpG island detection

Mean margin

0.92

Mean accuracy
optimal subset

6

0.88
0.86
0

10
20
30
40
Number observations

50

(c) Part Speech Tagging

Figure 3: Experimental results. (a) Temperature data: Improvement uniform spacing
heuristic. (b) CpG island data set: Effect increasing number observations
margin classification accuracy. (c) Part-of-Speech tagging data set: Effect increasing number observations margin F1 score.

6. Experiments
section, evaluate algorithms several real world data sets. special focus
comparison optimal methods greedy heuristic heuristic methods selecting observations, algorithms used interactive structured classification.
6.1 Temperature Time Series
first data set consists temperature time series collected sensor network deployed
Intel Research Berkeley (Deshpande et al., 2004) described running example. Data
continuously collected 19 days, linear interpolation used case missing samples.
temperature measured every 60 minutes, discretized 10 bins 2 degrees
Kelvin. avoid overfitting, used pseudo counts = 0.5 learning model. Using
parameter sharing, learned four sets transition probabilities: 12 - 7am, 7 - 12 pm,
12 pm - 7 pm 7 pm - 12 am. Combining data three adjacent sensors, got 53 sample
time series.
goal task select k 24 time points day, sensor
readings informative. experiment designed compare performance
optimal algorithms, greedy heuristic, uniform spacing heuristic, distributed k
observations uniformly day. Figure 3(a) shows relative improvement optimal algorithms greedy heuristic uniform spacing heuristic. performance measured
decrease expected entropy, zero observations baseline. seen k less
half possible observations, optimal algorithms decreased expected uncertainty several percent heuristics. improvement gained optimal plan
subset selection algorithms appears become drastic large number observations
(over half possible observations) allowed. Furthermore, large number observations,
optimal subset subset selected greedy heuristic almost identical.

574

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

6.2 CpG-Island Detection
studied bioinformatics problem finding CpG islands DNA sequences. CpG islands
regions genome high concentration cytosine-guanine sequence. areas
believed mainly located around promoters genes, frequently expressed
cell. experiment, considered gene loci HS381K22, AF047825 AL133174,
GenBank annotation listed three, two one CpG islands each. ran algorithm
50 base window beginning end island, using transition emission
probabilities Durbin, Eddy, Krogh, Mitchison (1999) Hidden Markov Model,
used sum margins reward function.
goal experiment locate beginning ending CpG islands
precisely asking experts, whether certain bases belong CpG region not. Figure 3(b) shows mean classification accuracy mean margin scores increasing number
observations. results indicate that, although expected margin scores similar
optimal algorithm greedy heuristic, mean classification performance optimal algorithm still better performance greedy heuristic. example, making 6
observations, mean classification error obtained optimal algorithm 25% lower
error obtained greedy heuristic.
6.3 Part-of-Speech Tagging
third experiment, investigated structured classification task part-of-speech (POS)
tagging (CoNLL, 2003). Problem instances sequences words (sentences), word
part entity (e.g., European Union), entity belongs one five categories:
Location, Miscellaneous, Organization, Person Other. Imagine application, automatic
information extraction guided expert: algorithms compute optimal conditional plan
asking expert, trying optimize classification performance requiring little expert
interaction possible.
used conditional random field structured classification task, node corresponds word, joint distribution described node potentials edge potentials.
sum margins used reward function. Measure classification performance F1
score, geometric mean precision recall. goal experiment analyze
addition expert labels increases classification performance, indirect, decomposing reward function used algorithms corresponds real world classification performance.
Figure 3(c) shows increase mean expected margin F1 score increasing number observations, summarized ten 50 word sequences. seen classification
performance effectively enhanced optimally incorporating expert labels. Requesting
three 50 labels increased mean F1 score five percent. following
example illustrates effect: one scenario words entity, sportsman P. Simmons,
classified incorrectly P. Simmons Miscellaneous. first request
optimal conditional plan label Simmons. Upon labeling word correctly, word P.
automatically labeled correctly also, resulting F1 score 100 percent.

575

fiK RAUSE & G UESTRIN

7. Applying Chain Algorithms General Graphical Models
Section 4 seen algorithms used schedule single sensor, assuming time
series sensor readings (e.g., temperature) form Markov chain. natural assumption
sensor networks (Deshpande et al., 2004). deploying sensor networks however, multiple
sensors need scheduled. time series sensors independent, could use
algorithms schedule sensors independently other. However, practice,
measurements correlated across different sensors fact, dependence essential
allow generalization measurements locations sensor placed. following, describe approach using single-sensor scheduling algorithm coordinate
multiple sensors.
formally, interested monitoring spatiotemporal phenomenon set locations = {1, . . . , m}, time steps = {1, . . . , }. locationtime pair s, t,
associate random variable Xs,t describes state phenomenon location
time. random vector XS,T fully describes relevant state world vector XS,t
describes state particular time step t. before, make Markov assumption, assuming
conditional independence XS,t XS,t0 given XS,t1 t0 < 1.
Similarly single-chain case, consider reward functions Rs,t associated
variable Xs,t . goal select, timestep, set sensors activate,
order maximize sum expected rewards. Letting A1:t = A1 , expected total
reward given
X
Rs,t (Xs,t | XA1:t )
s,t

filtering setting (i.e., observations past taken account evaluating
rewards),
X
Rs,t (Xs,t | XA1:T )
s,t

smoothing setting (where observations taken account). generalization
conditional planning done described Section 2.
Note case single sensor (` = 1), problem optimal sensor scheduling
solved using Algorithm 1. Unfortunately, optimization problem wildly intractable even
case two sensors, ` = 2:
Corollary 7 (Hardness sensor selection two chains). Given model two dependent
chains, constants c B, cost function set decision theoretic value information
functions Rs,t , NPPP -complete determine whether subset A1:T variables
L(A1:T ) c (A1:T ) B.
following, develop approximate algorithm uses optimal single-chain algorithms performs well practice.
7.1 Approximate Sensor Scheduling Lower Bound Maximization
reason sudden increase complexity case multiple chains decomposition rewards along sub-chains (as described Section 3) extend case multiple

576

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

S(1)1

S(1)2

S(1)3

S(1)4

S(1)5

S(2)1

S(2)2

S(2)3

S(2)4

S(2)5

Figure 4: Scheduling multiple correlated sensors dynamic processes.
sensors, since influence flow across chains. Figure 4 visualizes problem there, distri(1)
(1)
(2)
bution sensor (2) depends three observations S1 S4 sensor (1) S2
sensor (2).
address complexity issue using (approximate) extension decomposition approach used single chains. focus decision-theoretic value information objective (as described Section 2.1), local reward functions, residual entropy,
used well.
Considering recent observations. first approximation, allow sensor take
account recent observations. Intuitively, appears reasonable approximation,
especially potential scheduling times reasonably far apart. Formally, evaluating
local rewards time t, replace set observations time t, A1:t subset
A01:t A1:t


A01:t = (s, t) A1:t : t0 (s, t0 ) A1:t ,
i.e, sensor s, last observation (with largest time index t) kept.
approximate Rs,t (Xs,t | A1:t ) Rs,t (Xs,t | A01:t ). Figure 4 example, A1:5 =
{(s1 , 1), (s2 , 2), (s1 , 4)}, total expected utility time t5 would computed using observations A01:5 = {(s2 , 2), (s1 , 4)}, i.e., using time t4 sensor one, time t2 sensor two,
(1)
ignoring influence originating observation S1 flowing chains indicated
dashed arrow. following proposition proves approximation lower bound
true value information:
Proposition 8 (Monotonicity value information). decision-theoretic value information Rs,t (A) set sensors monotonic A,
Rs,t (A0 ) Rs,t (A)
A0 A.
Proposition 8 proves conditioning recent observations decrease
objective function, hence maximizing approximate objective implies maximizing lower bound
true objective.
coordinate ascent approach. propose following heuristic maximizing lower
bound expected utility. Instead jointly optimizing schedules (timesteps selected
sensor), algorithm repeatedly iterate sensors. sensors s,
optimize selected observations As1:T , holding schedules sensors fixed.
577

fiK RAUSE & G UESTRIN

procedure resembles coordinate ascent approach, coordinate ranges possible
schedules fixed sensor s.
optimizing sensor s, algorithm finds schedule As1:T


[
X
As1:T = argmax
Rs,t Xs,t | XA01:t
XA0s0 (As1:T ) B,
(6)
A1:T

s0 6=s

s,t

1:t

i.e., maximizes, schedules A1:T , sum expected rewards time steps
0
sensors, given schedules As1:T non-selected sensors s0 .
Solving single-chain optimization problem. order solve maximization problem
(6) individual sensors, use dynamic programming approach introduced
lt
Section 4. recursive case Lfa:b
(k) k > 0 exactly same. However, base case
computed
b1 X


X
[
f lt
La:b (0) =
Rs,j Xs,j | Xa
XA0s0 ,
s0 6=s

j=a+1

1:j

i.e., takes account recent observation non-selected sensors s0 .
lt
(0). First all,
Several remarks need made computation base case Lfa:b
naive implementation, computation expected utility


[
Rs,j Xs,j | Xa
XA0s0
s0 6=s

1:j

requires time exponential number chains. case since, order compute
reward Rs,t , chain, possible observations XA0s
= xA0s
could made need
1:t
1:t
taken account. computation requires computing expectation joint distribution
P (XA01:t ), exponential size. increase complexity avoided using sampling
approximation: Hoeffdings inequality used derive polynomial bounds sample complexity approximating value information arbitrarily small additive error , similarly
done approach Krause Guestrin (2005a)4 . practice, small number samples
appears provide reasonable performance. Secondly, inference becomes intractable
increasing number sensors. Approximate inference algorithms algorithm proposed
Boyen Koller (1998) provide viable way around problem.
Analysis. Since sensors maximize global objective L(A1:T ), coordinated ascent
approach guaranteed monotonically increase global objective every iteration (ignoring
possible errors due sampling approximate inference). Hence must converge (to local
optimum) finite number steps. procedure formalized Algorithm 4.
Although cannot general provide performance guarantees procedure, building algorithm provides optimal schedule sensor isolation,
benefit observations provided remaining sensors. Also, note sensors
independent, Algorithm 4 obtain optimal solution. Even sensors correlated,
obtained solution least good solution obtained scheduling sensors independently other. Algorithm 4 always converge, always compute lower bound
4. absolute error evaluating reward Rs,t accumulate total error |T ||S|
variables hence error optimal schedule.

578

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

Input: Budget B
Output: Selection A1 , . . . , A` observation times sensor
begin
Select Ai , 1 ` random;
repeat
= 1 `
Use Algorithm 1 select observations Ai sensor i, conditioning current
sensor scheduling Aj , j 6= i, remaining sensors;
end
Compute improvement total expected utility;
small enough ;
end
Algorithm 4: Multi-Sensor scheduling.
expected total utility. Considering intractability general problem even two chains
(c.f., , Corollary 7), properties reassuring. experiments, coordinated sensor
scheduling performed well, discussed Section 7.2.
7.2 Proof Concept Study Real Deployment
work Singhvi et al. (2005), presented approach optimizing light control
buildings, purpose satisfying building occupants preferences lighting conditions,
simultaneously minimizing energy consumption. approach, wireless sensor network
deployed monitors building environmental conditions (such sunlight intensity
etc.). sensors feed measurements building controller actuates lighting system
(lamps, blinds, etc.) accordingly. every timestep , building controller choose
action affects lighting conditions locations building. Utility functions
Ut (a, xS,t ) specified map chosen actions current lighting levels utility
value. utility chosen capture users preferences light levels, well
energy consumption lighting system. Details utility functions described detail
Singhvi et al..
evaluated multi-sensor scheduling approach real building controller testbed,
described detail Singhvi et al.. experiments, used Algorithm 4 schedule three
sensors, allowing sensor choose subset ten time steps (in one-hour intervals
daytime). varied number timesteps sensor activated, computed
total energy consumption total user utility (as defined Singhvi et al.). Figure 5(a) shows
mean user utility energy savings achieved, number observations varying
observations continuous sensing (10 observations discretization)5 . results imply
using predictive model active sensing strategy, even small number observations
achieves results approximately good results achieved continuous sensing.
Figure 5(b) presents mean total utility achieved using observations, one observation ten
observations per sensor day. seen even single observation per sensor increases
total utility close level achieved continuous sensing. Figure 5(c) shows mean energy
5. Note Figure 5(a), energy cost utility plotted different units directly compared.

579

fiK RAUSE & G UESTRIN

6

12
Energy cost

10

1 Observ./
sensor

15

10 Observ./
sensor
Energy cost

8

Total utility

User utility energy cost

14

4
observ.

2

8

observ.

10

5

0

1 Observ./
sensor

Measured user utility
6

0

1

2 3
Number observations

10

2

(a) Sensing scheduling evaluation

10

12
14
Hour day

16

18

0

(b) Total utility

10

10 Observ./
sensor

12
14
Hour day

16

18

(c) Energy cost

Figure 5: Active sensing results.
consumption required experiment. Here, single sensor observation strategy comes
even closer power savings achieved continuous sensing.
Since sensor network battery lifetime general inversely proportional amount
power expended sensing communication, conclude sensor scheduling strategy
promises lead drastic increases sensor network lifetime, deployment permanence reduced maintenance cost. testbed, network lifetime could increased factor 3
without significant reduction user utility increase energy cost.

8. Related Work
section, review related work number different areas.
8.1 Optimal Experimental Design
Optimal experimental design general methodology selecting informative experiments infer
aspects state world (such parameters particular nonlinear function,
etc.). large literature different approaches experimental design (c.f., Chaloner &
Verdinelli, 1995; Krause, Singh, & Guestrin, 2007).
Bayesian experimental design, prior distribution possible states world assumed, experiments chosen, e.g., reduce uncertainty posterior distribution.
general form, Bayesian experimental design pioneered Lindley (1956). users encode
preferences utility function U (P (), ? ), first argument, P (), distribution
states world (i.e., parameters) second argument, ? , true state
world. Observations xA collected, change expected utility prior P ()
posterior P ( | XA = xA ) used design criterion. sense, value observation problems considered paper considered instances Bayesian experimental design
problems. Typically, Bayesian Experimental Design employed continuous distributions, often
multivariate normal distribution. choosing different utility functions, different notions
optimality defined, including A- D- optimality developed (Chaloner & Verdinelli,
1995). posterior covariance matrix |A , whose
maximum

eigenvalue max ,
Bayesian A-, D-, E- optimality minimizes tr |A , det |A , max |A , respectively. terminology Section 2.1, D-optimality corresponds choosing total entropy,
A-optimality corresponds (weighted) mean-squared error criteria.

580

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

Even multivariate normal distributions, optimal Bayesian Experimental design NP-hard
(Ko, Lee, & Queyranne, 1995). applications experimental design, number experiments selected often large compared number design choices. cases, one
find fractional design (i.e., non-integral solution defining proportions experiments
performed), round fractional solutions. fractional formulation, A-, D-,
E-optimality criteria solved exactly using semi-definite program (Boyd & Vandenberghe,
2004). however known bounds integrality gap, i.e., loss incurred
rounding process.
algorithms presented Section 4.1 used optimally solve non-fractional Bayesian
Experimental Design problems chain graphical models, even continuous distributions,
long inference distributions tractable (such normal distributions). paper hence
provides new class combinatorial algorithms interesting class Bayesian experimental
design problems.
8.2 Value Information Graphical Models
Decision-theoretic value information frequently used principled information gathering (c.f., Howard, 1966; Lindley, 1956; Heckerman et al., 1993), popularized decision
analysis context influence diagrams (Howard & Matheson, 1984). sense, value
information problems special cases Bayesian experimental design problems, prior
distribution particular structure, typically given graphical model considered
paper.
Several researchers (Scheffer et al., 2001; van der Gaag & Wessels, 1993; Dittmer & Jensen,
1997; Kapoor et al., 2007) suggested myopic, i.e., greedy approaches selectively gathering
evidence graphical models, considered paper, which, unlike algorithms presented
paper. algorithms applicable much general graphical models,
theoretical guarantees. Heckerman et al. (1993) propose method compute
maximum expected utility specific sets observations. work considers general
graphical models paper (Naive Bayes models certain extensions), provide
large sample guarantees evaluation given sequence observations, use heuristic
without guarantees select sequences. Bilgic Getoor (2007) present branch bound
approach towards exactly optimizing value information complex probabilistic models.
contrast algorithms described paper however, approach running time
worst-case exponential. Munie Shoham (2008) present algorithms hardness results
optimizing special class value information objective functions motivated optimal
educational testing problems. algorithms apply different class graphical models
chains, apply specific objective functions, rather general local reward functions
considered paper. Radovilsky, Shattah, Shimony (2006) extended previous version
paper (Krause & Guestrin, 2005a) obtain approximation algorithms guarantees
case noisy observations (i.e., selecting subset emission variables observe, rather
selecting among hidden variables considered paper).
8.3 Bandit Problems Exploration / Exploitation
important class sequential value information problems class Bandit problems.
classical k-armed bandit problem, formalized Robbins (1952), slot machine given
581

fiK RAUSE & G UESTRIN

k arms. draw arm results reward success probability pi fixed
arm, different (and independent) across arm. selecting arms pull, important
problem trade exploration (i.e., estimation success probabilities arms)
exploitation (i.e., repeatedly pulling best arm known far). celebrated result Gittins
Jones (1979) shows fixed number draws, optimal strategy computed
polynomial time, using dynamic programming based algorithm. similar sense
optimal sequential strategy computed polynomial time, Gittins algorithm however
different structure dynamic programming algorithms presented paper.
Note using function optimization objective function described Section 2.1,
approach used solve particular instance bandit problems, arms
required independent, but, contrary classical notion bandit problems,
chosen repeatedly.
8.4 Probabilistic Planning
Optimized information gathering extensively studied planning community.
Bayer-Zubek (2004) example proposed heuristic method based Markov Decision Process framework. However, approach makes approximations without theoretical guarantees.
problem optimizing decision theoretic value information naturally formalized
(finite-horizon) Partially Observable Markov Decision Process (POMDP, Smallwood & Sondik,
1973). Hence, principle, algorithms planning POMDPs, anytime algorithm
Pineau, Gordon, Thrun (2006), employed optimizing value information. Unfortunately, state space grows exponentially number variables considered
selection problem. addition, complexity planning POMDPs grows exponentially
cardinality state space, hence doubly-exponentially number variables selection. steep increase complexity makes application black-box POMDP solvers infeasible.
Recently, Ji, Parr, Carin (2007) demonstrated use POMDP planning multi-sensor
scheduling problem. presenting promising empirical results, approach however uses
approximate POMDP planning techniques without theoretical guarantees.
robotics literature, Stachniss, Grisetti, Burgard (2005), Sim Roy (2005)
Kollar Roy (2008) presented approaches information gathering context Simultaneous Localization Mapping (SLAM). None approaches however provide guarantees
quality obtained solutions. Singh, Krause, Guestrin, Kaiser, Batalin (2007)
present approximation algorithm theoretical guarantees problem planning informative path environmental monitoring using Gaussian Process models. contrast
algorithms presented paper, dealing complex probabilistic models
complex cost functions arising path planning, approach requires submodular objective
functions (a property hold value information show Proposition 9).
8.5 Sensor Selection Scheduling
context wireless sensor networks, sensor nodes limited battery hence
enable small number measurements, optimizing value information selected
sensors plays key role. problem deciding selectively turn sensors order
conserve power first discussed Slijepcevic Potkonjak (2001) Zhao, Shin, Reich
(2002). Typically, assumed sensors associated fixed sensing region, spatial
582

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

domain needs covered regions associated selected sensors. Abrams, Goel,
Plotkin (2004) present efficient approximation algorithm theoretical guarantees
problem. Deshpande, Khuller, Malekian, Toossi (2008) present approach problem
based semidefinite programming (SDP), handling general constraints providing tighter
approximations. approaches described apply problem optimizing sensor schedules complex utility functions as, e.g., increase prediction accuracy
objectives considered paper. address shortcomings, Koushanfary, Taft,
Potkonjak (2006) developed approach sensor scheduling guarantees specified prediction accuracy based regression model. However, approach relies solution
Mixed Integer Program, intractable general. Zhao et al. (2002) proposed heuristics
selectively querying nodes sensor network order reduce entropy prediction. Unlike algorithms presented paper, approaches performance
guarantees.
8.6 Relationship Machine Learning
Decision Trees (Quinlan, 1986) popularized value information criterion creating
conditional plans. Unfortunately, guarantees performance greedy method.
subset selection problem instance feature selection central issue machine
learning, vast amount literature (see Molina, Belanche, & Nebot, 2002 survey).
However, aware work providing similarly strong performance guarantees
algorithms considered paper.
problem choosing observations strong connection field active learning
(c.f., Cohn, Gharamani, & Jordan, 1996; Tong & Koller, 2001) learning system designs
experiments based observations. sample complexity bounds derived
active learning problems (c.f., Dasgupta, 2005; Balcan, Beygelzimer, & Langford, 2006),
aware active learning algorithms perform provably optimal (even restricted
classes problem instances).
8.7 Previous Work Authors
previous version paper appeared work Krause Guestrin (2005b).
contents Section 7 appeared part work Singhvi et al. (2005). present version
much extended, new algorithmic hardness results detailed discussions.
light negative results presented Section 5, cannot expect able optimize value information complex models chains. However, instead attempting
solve optimal solution, one might wonder whether possible obtain good approximations. authors showed (Krause & Guestrin, 2005a; Krause et al., 2007; Krause, Leskovec,
Guestrin, VanBriesen, & Faloutsos, 2008) large number practical objective functions satisfy intuitive diminishing returns property: Adding new observation helps
observations far, less already made many observations. intuition formalized using combinatorial concept called submodularity. fundamental result Nemhauser
et al. proves optimizing submodular utility function, myopic greedy algorithm
fact provides near-optimal solution, within constant factor (11/e) 63% optimal.
Unfortunately, decision theoretic value information satisfy submodularity.

583

fiK RAUSE & G UESTRIN

Proposition 9 (Non-submodularity value information). Decision-theoretic value information submodular, even Naive Bayes models.
Intuitively, value information non-submodular, need make several observations
order convince need change action.

9. Conclusions
described novel efficient algorithms optimal subset selection conditional plan computation chain graphical models (and trees leaves), including HMMs. empirical
evaluation indicates algorithms improve upon commonly used heuristics decreasing expected uncertainty. algorithms effectively enhance performance interactive
structured classification tasks.
Unfortunately, optimization problems become wildly intractable even slight generalization chains. presented surprising theoretical limits, indicate even class
decision theoretic value information functions (as widely used, e.g., influence diagrams
POMDPs) cannot efficiently computed even Naive Bayes models. identified optimization value information new class problems intractable (NPPP -complete)
polytrees.
hardness results, along recent results polytree graphical models, NPcompleteness maximum posteriori assignment (Park & Darwiche, 2004) NP-hardness
inference conditional linear Gaussian models (Lerner & Parr, 2001), suggest possibility
developing generalized complexity characterization problems hard polytree graphical
models.
light theoretical limits computing optimal solutions, natural question ask
whether approximation algorithms non-trivial performance guarantees found. Recent
results Krause Guestrin (2005a), Radovilsky et al. (2006) Krause et al. (2007) show
case interesting classes value information problems.

Acknowledgments
would thank Ben Taskar providing part-of-speech tagging model, Reuters
making news archive available. would thank Brigham Anderson Andrew Moore helpful comments discussions. work partially supported NSF
Grants No. CNS-0509383, CNS-0625518, ARO MURI W911NF0710287 gift Intel.
Carlos Guestrin partly supported Alfred P. Sloan Fellowship, IBM Faculty Fellowship ONR Young Investigator Award N00014-08-1-0752 (2008-2011). Andreas Krause
partially supported Microsoft Research Graduate Fellowship.

Appendix
Proof Theorem 3. Membership #P arbitrary discrete polytrees straightforward since
inference models P. Let instance #3SAT , count
number assignments X1 , . . . , Xn satisfying . Let C = {C1 , . . . , Cm } set
clauses. create Bayesian network 2n + 1 variables, X1 , . . . , Xn , U1 , . . . , Un Y,
Xi conditionally independent given Y. Let uniformly distributed values
584

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS


U1

U2

Un


X1

X2

Xn

Figure 6: Graphical model used proof Theorem 3.
{n, (n 1), . . . , 1, 1, . . . , 1, m}, Ui Bernoulli prior p = 0.5. Let
observed variables Xi CPTs defined following way:

1, Xi = u satisfies clause Cj ;
Xi | [Y = +j, Ui = u]
0, otherwise.

0, = j;
Xi | [Y = j, Ui = u]
u, otherwise.
model, presented Figure 6, holds X1 = X2 = = Xn = 1 iff U1 , . . . , Un
encode satisfying assignment , > 0. Hence, observe X1 = X2 = = Xn = 1,
know > 0 certainty. Furthermore, least one Xi = 0, know
P (Y > 0 | X = x) < 1. Let nodes zero reward, except Y, assigned
reward function following properties (we show model local
reward function using decision-theoretic value information):
(n+m)2n
, P (Y > 0 | XA = xA ) = 1;

R(Y | XA = xA ) =
0,
otherwise.
argument, expected reward
X
R(Y | X1 , . . . , Xn ) =
P (Y = y)P (U = u)P (x| u)R(Y | X = x)
u,y,x

=

X

P (Y > 0)P (u)

u sat

X
(n + m)2n
=
1

u sat

exactly number satisfying assignments . Note model defined yet
Naive Bayes model. However, easily turned one marginalizing U.
show realize reward function properties maximum expected utility sense. Let = {d1 , d2 } set two decisions. Define utility function
property:

(n+m)2n

,
= d1 > 0;


(n+m)22n+1
u(y, d) =
, = d1 < 0;

0, n
otherwise.
reward R(Y | XA ) given decision-theoretic value information:
X
X
R(Y | XA ) =
P (xA ) max
P (y | xA )u(y, d).
xA



585



fiK RAUSE & G UESTRIN

Figure 7: Graphical model used proof Theorem 5.
utility function u based following consideration. Upon observing particular instantiation variables X1 , . . . , Xn make decision variable Y. goal achieve
number times action d1 chosen exactly corresponds number satisfying assignments . accomplished following way. Xi 1, know Ui
encoded satisfying assignment, > 0 probability 1. case, action d1 chosen.
need make sure whenever least one Xi = 0 (which indicates either < 0
U satisfying assignment) decision d2 chosen. Now, least one Xi = 0, either
= j > 0 clause j satisfied, < 0. utilities designed unless
n
P (Y > 0 | XA = xA ) 1 n22m , action d2 gives higher expected reward 0. Hereby,
n2n
2m lower bound probability misclassification P (Y < 0 | XA = xA ).
Note construction immediately proves hardness approximation: Suppose
polynomial time algorithm computes approximation R within
factor > 1 (which depend problem instance) R = R(Y | X1 , . . . , Xn ). R > 0
implies R > 0, R = 0 implies R = 0. Hence, approximation R used
decide whether satisfiable not, implying P = NP.
Proof Corollary 4. Let 3CNF formula. convert Naive Bayes model variables X1 , . . . , Xn construction Theorem 3. function L(V)
V = {1, . . . , n} set variables Xi counts number satisfying assignments .
Note function L(A) V = {1, . . . , n} monotonic, i.e., L(A) L(V)
V, shown Proposition 8. Hence majority assignments satisfies
L(V) > 2n1 .
Proof Theorem 5. Membership follows fact inference polytrees P discrete polytrees: nondeterministic Turing machine #P oracle first guess selection
variables, compute value information using Theorem 3 (since computation
#P-complete arbitrary discrete polytrees), compare constant c.
show hardness, let instance EM AJSAT , find instantiation
X1 , . . . , Xn (X1 , . . . , X2n ) true majority assignments Xn+1 , . . . , X2n .
Let C = {C1 , . . . , Cm } set 3CNF clauses. Create Bayesian network shown Figure 7,
nodes Ui , uniform Bernoulli prior. Add bivariate variables Yi = (seli , pari ),
0 2n, seli takes values {0, . . . , m} pari parity bit. CPTs Yi

586

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

defined as: sel0 uniformly varies {1, . . . , m}, par0 = 0, Y1 , . . . , Y2n :

0, j = 0, ui satisfies Cj ;
seli | [seli1 = j, Ui = ui ]
j, otherwise;
pari | [pari1 = bi1 , Ui ] bi1 Ui ,
denotes parity (XOR) operator. add variables ZiT ZiF 1 n
let

Uniform({0, 1}), ui = 1;

Zi | [Ui = ui ]
0,
otherwise;
Uniform denotes uniform distribution. Similarly, let

Uniform({0, 1}), ui = 0;
ZiF | [Ui = ui ]
0,
otherwise.
Intuitively, ZiT = 1 guarantees us Ui = 1, whereas ZiT = 0 leaves us uncertain Ui .
case ZiF symmetric.
use subset selection algorithm choose Zi encode solution EM AJSAT .
ZiT chosen, indicate Xi set true, similarly ZiF indicates false assignment
Xi . parity function going used ensure exactly one {ZiT , ZiF } observed
i.
first assign penalties nodes except ZiT , ZiF 1 n, Uj
n + 1 j 2n, assigned zero penalty. Let nodes zero reward, except
Y2n , assigned following reward:
n
4 , P (sel2n = 0 | XA = xA ) = 1
[P (par2n = 1 | XA = xA ) = 1 P (par2n = 0 | XA = xA ) = 1];
R(Y2n | XA = xA ) =

0,
otherwise.
Note sel2n = 0 probability 1 iff U1 , . . . , U2n encode satisfying assignment . Furthermore, get positive reward certain sel2n = 0, i.e., chosen observation
set must contain proof satisfied, certain par2n . parity certainty
occur certain assignment U1 , . . . , U2n . possible infer
value Ui certainty observing one Ui , ZiT ZiF . Since, = 1, . . . , n, cost
observing Ui , receive reward must observe least one ZiT ZiF . Assume
compute optimal subset budget 2n, receive positive reward
observing exactly one ZiT ZiF .
interpret selection ZiT ZiF assignment first n variables EM AJSAT .
Let R = R(Y2n | O). claim EM AJSAT R > 0.5. First let
EM AJSAT , assignment x1 , . . . , xn first n variables. add Un+1 , . . . , U2n
add ZiT iff xi = 1 ZiF iff xi = 0. selection guarantees R > 0.5.
assume R > 0.5. call assignment U1 , . . . , U2n consistent 1 n,
ZiT O, Ui = 1 ZiF Ui = 0. consistent assignment, chance
observations Zi prove consistency 2n . Hence R > 0.5 implies majority
provably consistent assignments satisfy hence EM AJSAT . proves subset
selection NPPP complete.
Note realize local reward function R sense maximum expected utility
similarly described Proof Theorem 3.
587

fiK RAUSE & G UESTRIN

Proof Corollary 6. constructions proof Theorem 4 Theorem 5 prove
computing conditional plans PP-hard NPPP -hard respectively, since, instances,
plan positive reward must observe variables corresponding valid instantiations (i.e.,
X1 , . . . , Xn Corollary 4, Un+1 , . . . , U2n one Z1 , . . . , Zn satisfy
parity condition Theorem 5). cases, order selection irrelevant, and, hence,
conditional plan effectively performs subset selection.
Proof Corollary 7. proof follows observation polytree construction
proof Theorem 5 arranged two dependent chains. transformation, revert
arc ZiT Ui applying Bayes rule. make sure number
nodes sensor timeslice, triple variables Yi , calling copies Yi0 Yi00 .
conditional probability tables given equality constraints, Yi0 = Yi Yi00 = Yi0 .
transformation, variables associated timesteps 3i 2 (for 1) given sets
00 , Z }. timesteps 3i 1 associated sets {U , }, timesteps 3i associated
{Yi1



{ZiF , Yi0 }.
Proof Proposition 8. bound follows fact maximization convex,
application Jensens inequality. Using induction argument, simply need show
L(A) L().
!
X
X
L(A) =
P (XA = xA )
max EU (a, t, x | XA1:t = xA1:t )
xA



tV

!


X
tV

=

X
tV

max


X

P (XA = xA )EU (a, t, x | XA1:t = xA1:t )

xA

max EU (a, t, x) = L()



EU (a, t, x | XA1:t = xA1:t ) =

X

P (xt | XA1:t = xA1:t )Ut (a, xt )

xt

expected utility action time observing XA1:t = xA1:t .
Proof Proposition 9. Consider following binary classification problem assymetric cost.
one Bernoulli random variable (the class label) P (Y = 1) = 0.5
P (Y = 1) = 0.5. two noisy observations X1 , X2 , conditionally independent given Y. Let P (Xi = Y) = 3/4 (i.e., observations agree class label
probability 3/4, disagree probability 1/4. three actions, a1 (classifying 1),
a1 (classifying -1) a0 (not assigning label). define utility functon U
gain utility 1 assign label correctly (U (a1 , 1) = U (a1 , 1) = 1), 3 misassign
label (U (a1 , 1) = U (a1 , 1) = 3), 0 choose a0 , i.e., assign label. Now,
2
2
6
> 0.
verify L() = L({X1 }) = L({X2 }) = 0, L({X1 , X2 }) = 43 3 14 = 16
Hence, adding X2 X1 increases utility adding X2 empty set, contradicting
submodularity.

588

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

References
Abrams, Z., Goel, A., & Plotkin, S. (2004). Set k-cover algorithms energy efficient monitoring
wireless sensor networks.. IPSN.
Balcan, N., Beygelzimer, A., & Langford, J. (2006). Agnostic active learning. ICML.
Baum, L. E., & Petrie, T. (1966). Statistical inference probabilistic functions finite state
Markov chains. Ann. Math. Stat, 37, 15541563.
Bayer-Zubek, V. (2004). Learning diagnostic policies examples systematic search. UAI.
Bellman, R. (1957). Markovian decision process. Journal Mathematics Mechanics, 6.
Bilgic, M., & Getoor, L. (2007). Voila: Efficient feature-value acquisition classification.
Twenty-Second Conference Artificial Intelligence (AAAI).
Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge UP.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes. Uncertainty Artificial Intelligence (UAI).
Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental design: review. Statistical Science,
10(3), 273304.
Cohn, D. A., Gharamani, Z., & Jordan, M. I. (1996). Active learning statistical models. J AI
Research, 4, 129145.
CoNLL (2003).
Conference computational natural language learning shared task.
http://cnts.uia.ac.be/conll2003/ner/.
Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley Interscience.
Dasgupta, S. (2005). Coarse sample complexity bounds active learning. NIPS.
Deshpande, A., Guestrin, C., Madden, S., Hellerstein, J., & Hong, W. (2004). Model-driven data
acquisition sensor networks. VLDB.
Deshpande, A., Khuller, S., Malekian, A., & Toossi, M. (2008). Energy efficient monitoring
sensor networks. LATIN.
Dittmer, S., & Jensen, F. (1997). Myopic value information influence diagrams. UAI, pp.
142149, San Francisco.
Domingos, P., & Pazzani, M. (1997). optimality simple Bayesian classifier
zero-one loss. Machine Learning, 29, 103137.
Durbin, R., Eddy, S. R., Krogh, A., & Mitchison, G. (1999). Biological Sequence Analysis : Probabilistic Models Proteins Nucleic Acids. Cambridge University Press.
Gittins, J. C., & Jones, D. M. (1979). dynamic allocation index discounted multiarmed
bandit problem. Biometrika, 66(3), 561565.
Goldberg, L. A., & Jerrum, M. (2000). Counting unlabelled subtrees tree #p-complete. LMS
J Comput. Math., 3, 117124.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). approximate nonmyopic computation
value information. IEEE Trans. Pattern Analysis Machine Intelligence, 15, 292298.

589

fiK RAUSE & G UESTRIN

Howard, R. A. (1966). Information value theory. IEEE Transactions Systems Science
Cybernetics (SSC-2).
Howard, R. A., & Matheson, J. (1984). Readings Principles Applications Decision
Analysis II, chap. Influence Diagrams, pp. 719762. Strategic Decision Group, Menlo Park.
Reprinted 2005 Decision Analysis 2(3) 127-143.
Ji, S., Parr, R., & Carin, L. (2007). Non-myopic multi-aspect sensing partially observable
Markov decision processes. IEEE Transactions Signal Processing, 55(6), 27202730.
Kapoor, A., Horvitz, E., & Basu, S. (2007). Selective supervision: Guiding supervised learning
decision-theoretic active learning. International Joint Conference Artificial Intelligence
(IJCAI).
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences Value
Trade-offs. Wiley.
Ko, C., Lee, J., & Queyranne, M. (1995). exact algorithm maximum entropy sampling.
Operations Research, 43(4), 684691.
Kollar, T., & Roy, N. (2008). Efficient optimization information-theoretic exploration slam.
AAAI.
Koushanfary, F., Taft, N., & Potkonjak, M. (2006). Sleeping coordination comprehensive sensing
using isotonic regression domatic partitions. Infocom.
Krause, A., & Guestrin, C. (2005a). Near-optimal nonmyopic value information graphical
models. Proc. Uncertainty Artificial Intelligence (UAI).
Krause, A., & Guestrin, C. (2005b). Optimal nonmyopic value information graphical models
- efficient algorithms theoretical limits. Proc. IJCAI.
Krause, A., Leskovec, J., Guestrin, C., VanBriesen, J., & Faloutsos, C. (2008). Efficient sensor
placement optimization securing large water distribution networks. Journal Water Resources Planning Management, 136(6).
Krause, A., Singh, A., & Guestrin, C. (2007). Near-optimal sensor placements Gaussian processes: Theory, efficient algorithms empirical studies. JMLR.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic models
segmenting labeling sequence data. ICML.
Lerner, U., & Parr, R. (2001). Inference hybrid networks: Theoretical limits practical algorithms. UAI.
Lindley, D. V. (1956). measure information provided experiment. Annals
Mathematical Statistics, 27, 9861005.
Littman, M., Goldsmith, J., & Mundhenk, M. (1998). computational complexity probabilistic
planning. Journal Artificial Intelligence Research, 9, 136.
Molina, L., Belanche, L., & Nebot, A. (2002). Feature selection algorithms: survey experimental evaluation. ICDM.
Mookerjee, V. S., & Mannino, M. V. (1997). Sequential decision models expert system optimization. IEEE Trans. Knowl. Data Eng., 9(5), 675687.

590

fiO PTIMAL VALUE NFORMATION G RAPHICAL ODELS

Munie, M., & Shoham, Y. (2008). Optimal testing structured knowledge. Twenty-Third Conference Artificial Intelligence (AAAI).
Papadimitriou, C. H. (1995). Computational Complexity. Addison-Wesley.
Park, J. D., & Darwiche, A. (2004). Complexity results approximation strategies map
explanations. Journal Aritificial Intelligence Research, 21, 101133.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations large pomdps.
JAIR, 27, 335380.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81106.
Radovilsky, Y., Shattah, G., & Shimony, S. E. (2006). Efficient deterministic approximation algorithms non-myopic value information graphical models. IEEE International
Conference Systems, Man Cybernetics (SMC), Vol. 3, pp. 25592564.
Robbins, H. (1952). aspects sequential design experiments. Bulletin American
Mathematical Society, 58, 527535.
Scheffer, T., Decomain, C., & Wrobel, S. (2001). Active learning partially hidden Markov models
information extraction. ECML/PKDD Workshop Instance Selection.
Sim, R., & Roy, N. (2005). Global a-optimal robot exploration slam. IEEE International
Conference Robotics Automation (ICRA).
Singh, A., Krause, A., Guestrin, C., Kaiser, W. J., & Batalin, M. A. (2007). Efficient planning
informative paths multiple robots. International Joint Conference Artificial Intelligence (IJCAI), pp. 22042211, Hyderabad, India.
Singhvi, V., Krause, A., Guestrin, C., Garrett, J., & Matthews, H. (2005). Intelligent light control
using sensor networks. Proc. 3rd ACM Conference Embedded Networked Sensor
Systems (SenSys).
Slijepcevic, S., & Potkonjak, M. (2001). Power efficient organization wireless sensor networks.
ICC.
Smallwood, R., & Sondik, E. (1973). optimal control partially observable Markov decision
processes finite horizon. Operations Research, 21, 10711088.
Stachniss, C., Grisetti, G., & Burgard, W. (2005). Information gain-based exploration using raoblackwellized particle filters. Robotics Science Systems (RSS).
Tong, S., & Koller, D. (2001). Active learning parameter estimation Bayesian networks.
NIPS.
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation hybrid genetic decision
tree induction algorithm. Journal Artificial Intelligence Research, 2, 369409.
van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering diagnostic belief networks.
AISB Quart., 86, 2334.
Zhao, F., Shin, J., & Reich, J. (2002). Information-driven dynamic sensor collaboration tracking
applications. IEEE Signal Processing, 19(2), 6172.

591


