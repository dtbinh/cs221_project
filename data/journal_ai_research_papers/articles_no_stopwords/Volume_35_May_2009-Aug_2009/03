journal artificial intelligence

submitted published

optimal value information graphical
andreas krause

krausea caltech edu

california institute technology
e california blvd
pasadena ca usa

carlos guestrin

guestrin cs cmu edu

carnegie mellon university
forbes ave
pittsburgh pa usa

abstract
many real world decision making tasks require us choose among several expensive observations sensor network example important select subset sensors
expected provide strongest reduction uncertainty medical decision making tasks one
needs select tests administer deciding effective treatment
general practice use heuristic guided procedures selecting observations
present first efficient optimal selecting observations class probabilistic
graphical example allow optimally label hidden variables hidden
markov hmms provide selecting optimal subset observations
obtaining optimal conditional observation plan
furthermore prove surprising graphical tasks one designs
efficient chain graphs hmms procedure generalized polytree graphical prove optimizing value information nppp hard even
polytrees follows computing decision theoretic value information objective functions commonly used practice p complete even
naive bayes simple special case polytrees
addition consider several extensions scheduling observation selection multiple sensors demonstrate effectiveness several
real world datasets including prototype sensor network deployment energy conservation
buildings

introduction
probabilistic reasoning one choose among several possible expensive observations
often central issue decide variables observe order effectively increase
expected utility howard howard matheson mookerjee mannino
lindley medical expert system example multiple tests available test
different cost turney heckerman horvitz middleton systems
thus important decide tests perform order become certain
patients condition minimum cost occasionally cost testing even exceed value
information possible outcome suggesting discontinue testing
following running example motivates empirically evaluated section
consider temperature monitoring task wireless temperature sensors distributed across

c

ai access foundation rights reserved

fik rause g uestrin

building task become certain temperature distribution whilst minimizing
energy expenditure critically constrained resource deshpande guestrin madden hellerstein
hong fine grained building monitoring required obtain significant energy savings
singhvi krause guestrin garrett matthews
many researchers suggested use myopic greedy approaches select observations scheffer decomain wrobel van der gaag wessels dittmer jensen
bayer zubek kapoor horvitz basu unfortunately general heuristic provide performance guarantees present efficient
guarantee optimal nonmyopic value information chain graphical example
used optimal active labeling hidden states hidden markov hmms baum petrie address two settings subset selection optimal
subset observations obtained open loop fashion conditional plans sequential
closed loop plan observation strategy depends actual value observed variables c f figure knowledge first optimal efficient
observation selection diagnostic value information class graphical settings address filtering smoothing versions filtering
important online decision making decisions utilize observations made
past smoothing arises example structured classification tasks temporal
dimension data hence observations taken account call
vo idp use dynamic programming optimize value information evaluate vo idp empirically three real world datasets
well suited interactive classification sequential data
inference graphical computing marginal distributions
finding probable explanation solved efficiently chain structured graphs
solved efficiently polytrees prove selecting best k
observations maximizing decision theoretic value information nppp complete even
discrete polytree graphical giving complexity theoretic classification core artificial
intelligence nppp complete believed significantly harder npcomplete even p complete commonly arising context graphical
furthermore prove evaluating decision theoretic value information objective functions
p complete even case naive bayes simple special case polytree graphical
frequently used practice c f domingos pazzani
unfortunately hardness scheduling single sensor optimally solved scheduling multiple correlated
sensors wildly intractable nevertheless vo idp single sensor
scheduling used approximately optimize multi sensor schedule demonstrate
effectiveness real sensor network testbed building management
summary provide following contributions
present first optimal nonmyopically computing optimizing value
information chain graphical
optimizing decision theoretic value information nppp hard discrete
polytree graphical computing decision theoretic value information phard even naive bayes



fio ptimal value nformation g raphical odels



tmorn high

tnoon high

yes

teve high

figure example conditional plan
present several extensions e g tree graphical
leaves multiple correlated chains multi sensor scheduling
extensively evaluate several real world including sensor
scheduling real sensor testbed active labeling bioinformatics natural language processing

statement
assume state world described collection random variables
xv x xn v index set example v could denote set locations xi
temperature reading sensor placed location v subset
ik v use notation xa refer random vector xa xi xik
extend continuous distributions generally assume variables xv discrete take bayesian assume prior probability distribution
p xv outcomes variables suppose select subset variables xa
v observe xa xa example set locations place sensors
set medical tests decide perform observing realization variables
xa xa compute posterior distribution variables p xv xa xa
posterior probability obtain reward r p xv xa xa example reward function could depend uncertainty e g measured entropy distribution
p xv xa xa describe several examples detail
general selecting observation know ahead time observations
make instead distribution possible observations hence
interested expected reward take expectation possible observations
optimizing selection variables consider different settings subset selection goal pick subset v variables maximizing
x
argmax
p xa xa r p xv xa xa



xa

impose constraints set allowed pick e g number
variables selected etc subset selection setting commit selection
variables get see realization
instead sequentially select one variable letting choice depend
observations made past setting would conditional plan



fik rause g uestrin

maximizes
argmax


x

p xv r p xv x xv x xv



xv

hereby conditional plan select different set variables possible state
world xv use notation xv v refer subset variables selected
conditional plan state xv xv figure presents example conditional plan
temperature monitoring example define notion conditional formally
section
general setup selecting observations goes back decision analysis literature
notion value information howard statistical literature notion
bayesian experimental design lindley refer
optimizing value information
complexity solving value information depend properties probability distribution p give first optimally
solving value information interesting challenging class distributions including hidden markov present hardness showing optimizing value information
wildly intractable nppp complete even probability distributions efficient inference possible even naive bayes discrete polytrees
optimization criteria
consider class local reward functions ri defined
marginal probability distributions variables xi class computational advantage
local rewards evaluated probabilistic inference techniques total reward
sum local rewards
let subset v p xj xa xa denotes marginal distribution variable xj conditioned observations xa xa example temperature monitoring
application xj temperature location j v conditional marginal distribution
p xj xj xa xa conditional distribution temperature location j
observing temperature locations v
classification purposes appropriate consider max marginals
p max xj xj xa xa max p xv xv xj xj xa xa
xv

xj set value xj probability probable assignment xv xv
random variables including xj simplicity notation conditioned observations
xa xa
local reward rj functional probability distribution p p max xj
rj takes entire distribution variable xj maps reward value typically
reward functions chosen certain peaked distributions obtain higher reward
simplify notation write
rj xj xa rj p xj xa xa
local reward functions widely used additively independent utility c f keeney raiffa



fio ptimal value nformation g raphical odels

denote reward variable xj upon observing xa xa
x
rj xj xa
p xa xa rj xj xa
xa

refer expected local rewards expectation taken assignments xa
observations important local reward functions include
residual entropy set
rj xj xa h xj xa

x

p xj xa log p xj xa

xj

objective optimization becomes minimize sum residual entropies optimizing reward function attempts reduce uncertainty predicting marginals xi
choose reward function running example measure uncertainty temperature distribution
p
joint entropy instead minimizing sum residual entropies h xi attempt minimize joint entropy entire distribution
x
h xv
p xv log p xv
xv

note joint entropy depends full probability distribution p xv rather
marginals p xi hence local nevertheless exploit chain rule joint
entropy h xb set random variables b c f cover thomas
h xb h x h x x h x x x h xm x xm
hence choose local reward functions rj xj xa h xj x xj xa
optimize non local reward function joint entropy local reward functions
decision theoretic value information concept local reward functions includes
concept decision theoretic value information notion value information widely
used c f howard lindley heckerman et al formalized e g
context influence diagrams howard matheson partially observable markov decision processes pomdps smallwood sondik variable xj let aj finite set
actions let uj aj dom xj r utility function mapping action aj
outcome x dom xj real number maximum expected utility principle states actions
selected maximize expected utility
x
euj xa xa
p xj xa uj xj
xj

certain xj economically choose action idea
captured notion value information choose local reward function
rj xj xa max euj xa




fik rause g uestrin

margin structured prediction consider margin confidence
rj xj xa p max xj xa p max xj xa

xj argmax p max xj xa xj argmax p max xj xa
xj xj

xj

describes margin likely outcome closest runner reward
function useful structured classification purposes shown section
weighted mean squared error variables continuous might want minimize
mean squared error prediction choosing
rj xj xa wj var xj xa

z
var xj xa


p xj xa xj

z

x j p x j



xa dx j


dxj

conditional variance xj given xa xa wj weight indicating importance
variable xj
monitoring critical regions hotspot sampling suppose want use sensors detecting fire generally want detect j whether xj cj cj dom xj
critical region variable xj local reward function
rj xj xa p xj cj xa
favors observations maximize probability detecting critical regions
function optimization correlated bandits consider setting collection
random variables xv taking numerical
p values interval selecting
variables get reward xi setting arises want optimize unknown
random function evaluating function expensive setting encouraged
evaluate function likely obtain high values maximize expected
total reward choose local reward function
z
rj xj xa xj p xj xa dxj
e expectation variable xj given observations xa setting optimizing random
function considered version classical k armed bandit correlated
arms details relationship bandit given section
examples demonstrate generality notion local reward note examples apply continuous distributions well discrete distributions



fio ptimal value nformation g raphical odels

cost selecting observations
want capture constraint observations expensive mean
observation xj associated positive penalty cj effectively decreases reward
example might interested trading accuracy sensing energy expenditure alternatively possible define budget b selecting observations one associated
integer cost j want select observations whose sum cost within budget
costs decrease reward running example sensors could powered
solar power regain certain amount energy per day allows certain amount
sensing formulation optimization
penalties budgets
p allows p
simplify notation write c ja cj ja j extend c sets
instead fixed penalties costs per observation depend state
world example medical domain applying particular diagnostic test bear different
risks health patient depending patients illness develop
adapted accommodate dependencies straight forward manner
present details conditional section

decomposing rewards
section present key observation allows us develop efficient
nonmyopically optimizing value information class chain graphical
presented section
set random variables xv x xn forms chain graphical model chain
xi conditionally independent xv given xi xi without loss generality
assume joint distribution specified prior p x variable x
conditional probability distributions p xi xi time series model temperature
measured one sensor example formulated chain graphical model note
transition probabilities p xi xi allowed depend index e chain
allowed nonstationary chain graphical extensively used machine
learning signal processing
consider example hidden markov model unrolled n time steps e v partitioned hidden variables x xn emission variables yn hmms
yi observed variables xi form chain many applications
discussed section observe hidden variables xi well e g asking
expert addition observing emission variables cases selecting
expert labels belongs class chain graphical addressed since
variables xi form chain conditional observed values emission variables yi idea
generalized class dynamic bayesian networks separators time
slices size one separators selected observation formulation
includes certain conditional random fields lafferty mccallum pereira form
chains conditional emission variables features
chain graphical originating time series additional specific properties
system online decision making observations past present time steps
taken account observations made future generally referred
filtering setting notation p xi xa refer distribution
xi conditional observations xa prior including time structured classification


fik rause g uestrin

figure illustration decomposing rewards idea reward chain observing
variables x x x decomposes sum chain plus reward chain
plus immediate reward observing xp
minus cost observing x hereby
brevity use notation rew b bj rj xj x x x

discussed section general observations made anywhere chain must
taken account situation usually referred smoothing provide
filtering smoothing
describe key insight allows efficient optimization chains consider
set observations v j variable observed e j local reward
simply r xj xa r xj xj consider j
let aj subset
containing closest ancestor smoothing closest descendant xj
xa conditional independence property graphical model implies given xaj xj
independent rest observed variables e p xj xa p xj xaj thus
follows r xj xa r xj xaj
observations imply expected reward set observations decomposes
along chain simplicity notation add two independent dummy variables x xn
r c rn cn n let p
im il il
im n notation total reward r j rj xj xa
smoothing case given


iv

x
x
riv xiv xiv civ
rj xj xiv xiv
v

j iv

filtering settings simply replace rj xj xiv xiv rj xj xiv figure illustrates
decomposition

efficient optimizing value information
section present efficiently nonmyopically optimizing value information chain graphical
efficient optimal subset selection chain
subset selection want informative subset variables observe
advance e observations made running example would
deploying sensors identify k time points expected provide informative
sensor readings according model



fio ptimal value nformation g raphical odels

first define objective function l subsets v
l

n
x

rj xj xa c



j

subset selection optimal subset


argmax l
av b

maximizing sum expected local rewards minus penalties subject constraint
total cost must exceed budget b
solve optimization dynamic programming chain
broken sub chains insight section consider sub chain variable xa
xb define lsm
b k represent expected total reward sub chain xa xb
lt
smoothing setting xa xb observed budget level k lfa b
k represents
expected reward filtering setting xa observed formally
lt
k
lfa b



b
x

max

b
j
k

rj xj xa xa c

filtering version
lsm
b k

b
x

max

rj xj xa xa xb c

b
j
k

smoothing version note cases l n b maxa b l equation e computing values la b k compute maximum expected total reward
entire chain
f lt
compute lsm
b k la b k dynamic programming base case simply
lt
lfa b


b
x

rj xj xa

j

filtering
b
x

lsm
b

rj xj xa xb

j

smoothing recursion la b k two cases choose spend
budget reaching base case break chain two sub chains selecting optimal
observation xj j b filtering smoothing


la b k max la b
max
rj xj xj cj la j lj b k j
j j b j k



fik rause g uestrin

input budget b rewards rj costs j penalties cj
output optimal selection observation times
begin
b n compute la b
k b
b n
sel la b
j b sel j rj xj xj cj la j lj b k j
la b k maxj b sel j
b k argmaxj b sel j
end
end
b n k b
repeat
j b k
j j j k k j
j
end
vo idp optimal subset selection filtering smoothing
first may seem recursion consider optimal split budget
two sub chains however since subset open loop order observations
irrelevant need consider split points first sub chain receives zero budget
pseudo code implementation dynamic programming call vo idp
subset selection given fills dynamic programming tables
two loops inner loop ranging pairs b b outer loop increasing k within
inner loop computing best reward sub chain b fills table sel
sel j reward could obtained making observation j sel
reward observation made
addition computing optimal rewards la b k could achieved sub chain b
budget k stores choices b k realize maximum score
b k index next variable selected sub chain b budget
k variable selected order recover optimal subset budget k
uses quantities b recover optimal subset tracing maximal values
occurring dynamic programming equations induction proof obtain
theorem subset selection dynamic programming described computes
optimal subset budget b n n b evaluations expected local rewards
note consider different costs variable would simply choose j
variables compute la b n note variables xi continuous
still applicable integrations inferences necessary computing
expected rewards performed efficiently case example gaussian linear
model e variables xi normally distributed local reward functions residual
entropies residual variances variable



fio ptimal value nformation g raphical odels

efficient optimal conditional chain
conditional plan want compute optimal sequential querying policy
observe variable pay penalty depending values observed past select next
query proceeding long budget suffices objective plan highest
expected reward possible sequence observations budget b exceeded
filtering select observations future whereas smoothing case next
observation anywhere chain running example filtering would
appropriate sensors would sequentially follow conditional plan deciding
informative times sense previous observations figure shows example
conditional plan
f rom ubset election c onditional p lanning
note contrast subset selection setting considered section conditional set variables depends state world xv xv hence
state conditional plan could select different set variables xv v example consider figure set possible observations v morn noon eve
xv tmorn tnoon teve world state xv high low high conditional
plan presented figure would select xv morn eve whereas
xv low low high would select xv morn noon since conditional plan
function random state world set valued random variable order optimize
define objective function
j

x

p xv

xv

n
x



rj xj x xv c xv

j

e expected sum local rewards given observations made plan xv state xv xv
minus penalties selected variables expectation taken respect
distribution p xv addition defining value policy j define cost
max xv
xv

maximum cost defined section set xv could selected
policy state world xv xv
notation goal policy
argmax j b


e policy maximum value guaranteed never cost exceeding budget
b hereby class sequential policies e observations chosen
sequentially observations previously made
useful introduce following notation
j xa k max j xa xa k




recall filtering setting r xj x xv r xj xa xv j e
observations past taken account



fik rause g uestrin


j xa xa

x

n
x


p xv xa xa
rj xj x xv c xv

xv

j

hence j xa k best possible reward achieved sequential policy cost
k observing xa xa notation goal optimal plan
reward j b
value function j satisfies following recursion base case considers exhausted
budget
x
j xa
rj xj xa c
jv

recursion holds





x
j xa k max j xa max

p xj xa j xa xj k j


j




xj

e best one state xa xa budget k stop selecting variables
chose best next variable act optimally thereupon
note easily allow cost j depend state xj variable xj case
would simply replace j j xj define j xa r whenever r equivalently
let penalty c depend state replacing c c xa
relationship finite horizon markov decision processes mdps note function
j xa k defined analogous concept value function markov decision processes c f bellman finite horizon mdps value function v k maximum expected reward obtainable starting state performing k actions value
function holds
x
v k r k max
p v k




p probability transiting state performing action state
r k immediate reward obtained state k steps still left recursion
similar eq exploited value iteration solving mdps
conditional unit observation cost e could modeled
finite horizon mdp states correspond observed evidence xa xa actions correspond
observing variables making observation transition probabilities given
probability observing particular instantiation selected variable immediate reward
r k k r expected reward value information
observing assignment e r p xv c observations unit cost
mdp holds v xa k j xa k unfortunately conditional since
state mdp uniquely determined observed evidence xa xa state space
exponentially large hence existing solving mdps exactly value iteration
cannot applied solve large value information section develop
efficient dynamic programming conditional chain graphical
avoids exponential increase complexity


fio ptimal value nformation g raphical odels

dynamic p rogramming ptimal c onditional p lanning c hains
propose dynamic programming obtaining optimal conditional plan
similar subset presented section utilize decomposition
rewards described section difference observation selection budget
allocation depend actual values observations order compute value
function j xa k entire chain compute value functions ja b xa k subchains xa xb
base case dynamic programming deals zero budget setting
f lt
ja b
xa



b
x

rj xj xa xa

j

filtering
sm
xa xb
ja b

b
x

rj xj xa xa xb xb

j

smoothing recursion defines ja b xa k ja b xa xb k smoothing expected
reward restricted sub chain xa xb conditioned values xa xa
xb xb smoothing budget limited k compute quantity
iterate possible split points j j b observe notable difference
filtering smoothing case smoothing must consider possible
splits budget two resulting sub chains since observation time j might
require us make additional earlier observation
x

n
sm
sm
p xj xj xa xa xb xb
ja b xa xb k max ja b xa xb max
j b

rj xj xj cj xj

max
lkj xj

xj



sm
ja j
xa xj l



sm
xj xb k
jj b




l j xj

looking back time possible filtering case hence recursion simplifies

x
n
f lt
f lt
ja b xa k max ja b xa
max
p xj xj xa xa
j b j xj k

rj xj xj cj xj

xj

f lt
ja j
xa



f lt
jj b
xj k


j xj


j f lt j sm optimal reward obtained j n b j b j
presents pseudo code implementation smoothing version filtering case
straight forward modification call vo idp conditional fill dynamic programming tables three loops inner loop
ranging assignments xa xb middle loop ranging pairs b b
outer loop covers increasing values k b within innermost loop
computes table sel sel j optimal reward achievable selecting variable j next


fik rause g uestrin

value expectation possible observation variable xj make note
every possible instantiation xj xj different allocation remaining budget k j xj
left right sub chain j j b respectively chosen quantity j xj
tracks optimal budget allocation
input budget b rewards rj costs j penalties cj
output optimal conditional plan b b
begin
sm x x
b n xa dom xa xb dom xb compute ja b
b
k b
b n xa dom xa xb dom xb
sm
sel ja b
j b
sel j
xj dom xj
l k j xj
sm x x l j sm x x k l x
bd l ja j
j
j j
j
b
j b
end
sel j sel j p xj xa xb rj xj xj cj xj maxl bd j
j xj argmaxl bd j
end
end
sm k max
ja b
j b sel j
b xa xb k argmaxj b sel j
xj dom xa b k b xa xb xj k b k xj
end
end
end
vo idp computating optimal conditional plan smoothing
setting
input budget k observations xa xa xb xb
begin
j b xa xb k
j
observe xj xj
l b xa xb xj k
recurse k l xa xa xj xj instead xb xb
recurse k k l j xj xj instead xa xa xb xb
end
end
observation selection conditional
plan compactly encoded quantities b b hereby b xa xb k
determines next variable query observing xa xa xb xb remaining budget k b xa xb xj k determines allocation budget observation
xj xj made considering exponential number possible sequences observations


fio ptimal value nformation g raphical odels

remarkable optimal plan represented polynomial space
indicates computed plan executed procedure recursive requiring parameters xa b n xb k b initial call temperature
monitoring example could first collect temperature timeseries training data
learn chain model data offline would compute conditional plan
filtering setting encode quantities b b would deploy computed
plan actual sensor node together implementation computation optimal plan fairly computationally expensive execution plan
efficient selecting next timestep observation requires single lookup
b b tables hence well suited deployment small embedded device
summarize analysis following theorem
theorem conditional smoothing presented computes
optimal conditional plan b n n evaluations local rewards
maximum domain size random variables x xn filtering case optimal plan computed b n n evaluations budget used
n n evaluations
faster computation filtering budget case obtained observing
require third maximum computation distributes budget sub chains
note contrary computing optimal subsets section requires evaluations form r xj xa xa general computed
times faster expectations r xj xa consideration subset selection
general factor b faster even though conditional
nested loops
efficient trees leaves
sections presented dynamic programming optimize value information chain graphical fact key observations section
local rewards decompose along chains holds chain graphical trees
formally tree graphical model joint probability distribution p xv collection
random variables xv p xv factors
p xv


j xi xj
z
j e

j nonnegative potential function mapping assignments xi xj nonnegative
real numbers e v v set edges form undirected tree index set v z
normalization constant enforcing valid probability distribution
dynamic programming presented previous sections extended
tree straightforward manner instead identifying optimal subsets conditional
plans sub chains would select optimal subsets plans sub trees
increasing size note however number sub trees grow exponentially number
leaves tree star n leaves example number subtrees exponential
n fact counting number subtrees arbitrary tree n vertices believed
intractable p complete goldberg jerrum however trees contain


fik rause g uestrin

small constant number leaves number subtrees polynomial optimal subset
conditional plans computed polynomial time

theoretical limits
many solved efficiently discrete chain graphical efficiently solved discrete polytrees examples include probabilistic inference probable explanation mpe
section however seen complexity dynamic programming chains increases dramatically extended trees complexity increases exponentially number leafs tree
prove perhaps surprisingly optimizing value information
exponential increase complexity cannot avoided reasonable complexity theoretic assumptions making statement formal briefly review complexity classes
used
brief review relevant computational complexity classes
briefly review complexity classes used following statements presenting complete
class details see e g references papadimitriou
littman goldsmith mundhenk class np contains decision
polynomial time verifiable proofs well known complete sat
instances boolean formulas conjunctive normal form containing three literals per
clause cnf form complexity class p contains counting complete
class p sat counts number satisfying instances cnf formula
pp decision version class p complete ajsat decides
whether given cnf formula satisfied majority e half
possible assignments b turing machine complexity classes ab
complexity class derived allowing turing machines deciding instances oracle calls
turing machines b intuitively think class ab
solved turing machine class special command solves b
pp similar p ppp p p e allow deterministic polynomial time turing
machine access counting oracle cannot solve complex give
access majority oracle combining ideas class nppp class
solved nondeterministic polynomial time turing machines access majority
counting oracle complete nppp em ajsat given cnf
variables x x n decides whether exists assignment x xn
satisfied majority assignments xn x n nppp introduced
found natural class modeling ai seminal work littman et al
example map assignment nppp complete general graphical
shown park darwiche
complexity classes satisfy following set inclusions inclusions assumed
known strict
p np pp ppp p p nppp
polytrees bayesian networks form trees edge directions dropped



fio ptimal value nformation g raphical odels

complexity computing optimizing value information
order solve optimization likely evaluate objective
function e expected local rewards first states even specialize decision theoretic value information objective functions defined section
intractable even naive bayes special case discrete polytrees naive bayes
often used classification tasks c f domingos pazzani class variable
predicted noisy observations features assumed conditionally independent given
class variable sense naive bayes next simplest perspective
inference class bayesian networks chains note naive bayes correspond
stars referred section number subtrees exponential number
variables
theorem hardness computation naive bayes computation decision
theoretic value information functions p complete even naive bayes
hard approximate factor unless p np
immediate corollary subset selection pp hard naive bayes

corollary hardness subset selection naive bayes determining
given naive bayes model constants c b cost function set decision theoretic value
information objective functions ri whether subset variables v
l c b pp hard
fact subset selection arbitrary discrete polytrees general
naive bayes inference still tractable even nppp complete complexity
class containing believed significantly harder np p complete
provides complexity theoretic classification value information core
ai
theorem hardness subset selection computation polytrees determining given discrete polytree constants c b cost function set decision theoretic
value information objective functions ri whether subset variables v
l c b nppp complete
running example implies generalized optimally selecting k sensors
network correlated sensors likely computationally intractable without resorting
heuristics corollary extends hardness subset selection hardness conditional plans
corollary hardness conditional computation polytrees computing conditional plans pp hard naive bayes nppp hard discrete polytrees
proofs section stated appendix rely reductions complete
np p nppp involving boolean formulae computing optimizing value information reductions inspired works littman et al park
darwiche require development novel techniques reductions
boolean formulae naive bayes polytree graphical associated appropriate reward
functions ensuring observation selections lead feasible assignments boolean formulae


fik rause g uestrin

percent improvement





optimal conditional plan



mean margin optimal subset
mean margin greedy heuristic






mean f score













optimal subset
greedy heuristic










number observations



sensor scheduling





mean accuracy
greedy heuristic






number observations

b cpg island detection

mean margin



mean accuracy
optimal subset











number observations



c part speech tagging

figure experimental temperature data improvement uniform spacing
heuristic b cpg island data set effect increasing number observations
margin classification accuracy c part speech tagging data set effect increasing number observations margin f score

experiments
section evaluate several real world data sets special focus
comparison optimal methods greedy heuristic heuristic methods selecting observations used interactive structured classification
temperature time series
first data set consists temperature time series collected sensor network deployed
intel berkeley deshpande et al described running example data
continuously collected days linear interpolation used case missing samples
temperature measured every minutes discretized bins degrees
kelvin avoid overfitting used pseudo counts learning model
parameter sharing learned four sets transition probabilities pm
pm pm pm combining data three adjacent sensors got sample
time series
goal task select k time points day sensor
readings informative experiment designed compare performance
optimal greedy heuristic uniform spacing heuristic distributed k
observations uniformly day figure shows relative improvement optimal greedy heuristic uniform spacing heuristic performance measured
decrease expected entropy zero observations baseline seen k less
half possible observations optimal decreased expected uncertainty several percent heuristics improvement gained optimal plan
subset selection appears become drastic large number observations
half possible observations allowed furthermore large number observations
optimal subset subset selected greedy heuristic almost identical



fio ptimal value nformation g raphical odels

cpg island detection
studied bioinformatics finding cpg islands dna sequences cpg islands
regions genome high concentration cytosine guanine sequence areas
believed mainly located around promoters genes frequently expressed
cell experiment considered gene loci hs k af al
genbank annotation listed three two one cpg islands ran
base window beginning end island transition emission
probabilities durbin eddy krogh mitchison hidden markov model
used sum margins reward function
goal experiment locate beginning ending cpg islands
precisely asking experts whether certain bases belong cpg region figure b shows mean classification accuracy mean margin scores increasing number
observations indicate although expected margin scores similar
optimal greedy heuristic mean classification performance optimal still better performance greedy heuristic example making
observations mean classification error obtained optimal lower
error obtained greedy heuristic
part speech tagging
third experiment investigated structured classification task part speech pos
tagging conll instances sequences words sentences word
part entity e g european union entity belongs one five categories
location miscellaneous organization person imagine application automatic
information extraction guided expert compute optimal conditional plan
asking expert trying optimize classification performance requiring little expert
interaction possible
used conditional random field structured classification task node corresponds word joint distribution described node potentials edge potentials
sum margins used reward function measure classification performance f
score geometric mean precision recall goal experiment analyze
addition expert labels increases classification performance indirect decomposing reward function used corresponds real world classification performance
figure c shows increase mean expected margin f score increasing number observations summarized ten word sequences seen classification
performance effectively enhanced optimally incorporating expert labels requesting
three labels increased mean f score five percent following
example illustrates effect one scenario words entity sportsman p simmons
classified incorrectly p simmons miscellaneous first request
optimal conditional plan label simmons upon labeling word correctly word p
automatically labeled correctly resulting f score percent



fik rause g uestrin

applying chain general graphical
section seen used schedule single sensor assuming time
series sensor readings e g temperature form markov chain natural assumption
sensor networks deshpande et al deploying sensor networks however multiple
sensors need scheduled time series sensors independent could use
schedule sensors independently however practice
measurements correlated across different sensors fact dependence essential
allow generalization measurements locations sensor placed following describe single sensor scheduling coordinate
multiple sensors
formally interested monitoring spatiotemporal phenomenon set locations time steps locationtime pair
associate random variable xs describes state phenomenon location
time random vector xs fully describes relevant state world vector xs
describes state particular time step make markov assumption assuming
conditional independence xs xs given xs
similarly single chain case consider reward functions rs associated
variable xs goal select timestep set sensors activate
order maximize sum expected rewards letting expected total
reward given
x
rs xs xa


filtering setting e observations past taken account evaluating
rewards
x
rs xs xa


smoothing setting observations taken account generalization
conditional done described section
note case single sensor optimal sensor scheduling
solved unfortunately optimization wildly intractable even
case two sensors
corollary hardness sensor selection two chains given model two dependent
chains constants c b cost function set decision theoretic value information
functions rs nppp complete determine whether subset variables
l c b
following develop approximate uses optimal single chain performs well practice
approximate sensor scheduling lower bound maximization
reason sudden increase complexity case multiple chains decomposition rewards along sub chains described section extend case multiple



fio ptimal value nformation g raphical odels





















figure scheduling multiple correlated sensors dynamic processes
sensors since influence flow across chains figure visualizes distri


bution sensor depends three observations sensor
sensor
address complexity issue approximate extension decomposition used single chains focus decision theoretic value information objective described section local reward functions residual entropy
used well
considering recent observations first approximation allow sensor take
account recent observations intuitively appears reasonable approximation
especially potential scheduling times reasonably far apart formally evaluating
local rewards time replace set observations time subset




e sensor last observation largest time index kept
approximate rs xs rs xs figure example
total expected utility time would computed observations e time sensor one time sensor two

ignoring influence originating observation flowing chains indicated
dashed arrow following proposition proves approximation lower bound
true value information
proposition monotonicity value information decision theoretic value information rs set sensors monotonic
rs rs

proposition proves conditioning recent observations decrease
objective function hence maximizing approximate objective implies maximizing lower bound
true objective
coordinate ascent propose following heuristic maximizing lower
bound expected utility instead jointly optimizing schedules timesteps selected
sensor repeatedly iterate sensors sensors
optimize selected observations holding schedules sensors fixed


fik rause g uestrin

procedure resembles coordinate ascent coordinate ranges possible
schedules fixed sensor
optimizing sensor finds schedule



x
argmax
rs xs xa
xa b









e maximizes schedules sum expected rewards time steps

sensors given schedules non selected sensors
solving single chain optimization order solve maximization
individual sensors use dynamic programming introduced
lt
section recursive case lfa b
k k exactly however base case
computed
b x


x

f lt
la b
rs j xs j xa
xa


j

j

e takes account recent observation non selected sensors
lt
first
several remarks need made computation base case lfa b
naive implementation computation expected utility



rs j xs j xa
xa


j

requires time exponential number chains case since order compute
reward rs chain possible observations xa
xa
could made need


taken account computation requires computing expectation joint distribution
p xa exponential size increase complexity avoided sampling
approximation hoeffdings inequality used derive polynomial bounds sample complexity approximating value information arbitrarily small additive error similarly
done krause guestrin practice small number samples
appears provide reasonable performance secondly inference becomes intractable
increasing number sensors approximate inference proposed
boyen koller provide viable way around
analysis since sensors maximize global objective l coordinated ascent
guaranteed monotonically increase global objective every iteration ignoring
possible errors due sampling approximate inference hence must converge local
optimum finite number steps procedure formalized
although cannot general provide performance guarantees procedure building provides optimal schedule sensor isolation
benefit observations provided remaining sensors note sensors
independent obtain optimal solution even sensors correlated
obtained solution least good solution obtained scheduling sensors independently converge compute lower bound
absolute error evaluating reward rs accumulate total error
variables hence error optimal schedule



fio ptimal value nformation g raphical odels

input budget b
output selection observation times sensor
begin
select ai random
repeat

use select observations ai sensor conditioning current
sensor scheduling aj j remaining sensors
end
compute improvement total expected utility
small enough
end
multi sensor scheduling
expected total utility considering intractability general even two chains
c f corollary properties reassuring experiments coordinated sensor
scheduling performed well discussed section
proof concept study real deployment
work singhvi et al presented optimizing light control
buildings purpose satisfying building occupants preferences lighting conditions
simultaneously minimizing energy consumption wireless sensor network
deployed monitors building environmental conditions sunlight intensity
etc sensors feed measurements building controller actuates lighting system
lamps blinds etc accordingly every timestep building controller choose
action affects lighting conditions locations building utility functions
ut xs specified map chosen actions current lighting levels utility
value utility chosen capture users preferences light levels well
energy consumption lighting system details utility functions described detail
singhvi et al
evaluated multi sensor scheduling real building controller testbed
described detail singhvi et al experiments used schedule three
sensors allowing sensor choose subset ten time steps one hour intervals
daytime varied number timesteps sensor activated computed
total energy consumption total user utility defined singhvi et al figure shows
mean user utility energy savings achieved number observations varying
observations continuous sensing observations discretization imply
predictive model active sensing strategy even small number observations
achieves approximately good achieved continuous sensing
figure b presents mean total utility achieved observations one observation ten
observations per sensor day seen even single observation per sensor increases
total utility close level achieved continuous sensing figure c shows mean energy
note figure energy cost utility plotted different units directly compared



fik rause g uestrin




energy cost



observ
sensor



observ
sensor
energy cost



total utility

user utility energy cost




observ





observ







observ
sensor

measured user utility







number observations





sensing scheduling evaluation





hour day







b total utility



observ
sensor



hour day





c energy cost

figure active sensing
consumption required experiment single sensor observation strategy comes
even closer power savings achieved continuous sensing
since sensor network battery lifetime general inversely proportional amount
power expended sensing communication conclude sensor scheduling strategy
promises lead drastic increases sensor network lifetime deployment permanence reduced maintenance cost testbed network lifetime could increased factor
without significant reduction user utility increase energy cost

related work
section review related work number different areas
optimal experimental design
optimal experimental design general methodology selecting informative experiments infer
aspects state world parameters particular nonlinear function
etc large literature different approaches experimental design c f chaloner
verdinelli krause singh guestrin
bayesian experimental design prior distribution possible states world assumed experiments chosen e g reduce uncertainty posterior distribution
general form bayesian experimental design pioneered lindley users encode
preferences utility function u p first argument p distribution
states world e parameters second argument true state
world observations xa collected change expected utility prior p
posterior p xa xa used design criterion sense value observation considered considered instances bayesian experimental design
typically bayesian experimental design employed continuous distributions often
multivariate normal distribution choosing different utility functions different notions
optimality defined including optimality developed chaloner verdinelli
posterior covariance matrix whose
maximum

eigenvalue max
bayesian e optimality minimizes tr det max respectively terminology section optimality corresponds choosing total entropy
optimality corresponds weighted mean squared error criteria



fio ptimal value nformation g raphical odels

even multivariate normal distributions optimal bayesian experimental design np hard
ko lee queyranne applications experimental design number experiments selected often large compared number design choices cases one
fractional design e non integral solution defining proportions experiments
performed round fractional solutions fractional formulation
e optimality criteria solved exactly semi definite program boyd vandenberghe
however known bounds integrality gap e loss incurred
rounding process
presented section used optimally solve non fractional bayesian
experimental design chain graphical even continuous distributions
long inference distributions tractable normal distributions hence
provides class combinatorial interesting class bayesian experimental
design
value information graphical
decision theoretic value information frequently used principled information gathering c f howard lindley heckerman et al popularized decision
analysis context influence diagrams howard matheson sense value
information special cases bayesian experimental design prior
distribution particular structure typically given graphical model considered

several researchers scheffer et al van der gaag wessels dittmer jensen
kapoor et al suggested myopic e greedy approaches selectively gathering
evidence graphical considered unlike presented
applicable much general graphical
theoretical guarantees heckerman et al propose method compute
maximum expected utility specific sets observations work considers general
graphical naive bayes certain extensions provide
large sample guarantees evaluation given sequence observations use heuristic
without guarantees select sequences bilgic getoor present branch bound
towards exactly optimizing value information complex probabilistic
contrast described however running time
worst case exponential munie shoham present hardness
optimizing special class value information objective functions motivated optimal
educational testing apply different class graphical
chains apply specific objective functions rather general local reward functions
considered radovilsky shattah shimony extended previous version
krause guestrin obtain approximation guarantees
case noisy observations e selecting subset emission variables observe rather
selecting among hidden variables considered
bandit exploration exploitation
important class sequential value information class bandit
classical k armed bandit formalized robbins slot machine given


fik rause g uestrin

k arms draw arm reward success probability pi fixed
arm different independent across arm selecting arms pull important
trade exploration e estimation success probabilities arms
exploitation e repeatedly pulling best arm known far celebrated gittins
jones shows fixed number draws optimal strategy computed
polynomial time dynamic programming similar sense
optimal sequential strategy computed polynomial time gittins however
different structure dynamic programming presented
note function optimization objective function described section
used solve particular instance bandit arms
required independent contrary classical notion bandit
chosen repeatedly
probabilistic
optimized information gathering extensively studied community
bayer zubek example proposed heuristic method markov decision process framework however makes approximations without theoretical guarantees
optimizing decision theoretic value information naturally formalized
finite horizon partially observable markov decision process pomdp smallwood sondik
hence principle pomdps anytime
pineau gordon thrun employed optimizing value information unfortunately state space grows exponentially number variables considered
selection addition complexity pomdps grows exponentially
cardinality state space hence doubly exponentially number variables selection steep increase complexity makes application black box pomdp solvers infeasible
recently ji parr carin demonstrated use pomdp multi sensor
scheduling presenting promising empirical however uses
approximate pomdp techniques without theoretical guarantees
robotics literature stachniss grisetti burgard sim roy
kollar roy presented approaches information gathering context simultaneous localization mapping slam none approaches however provide guarantees
quality obtained solutions singh krause guestrin kaiser batalin
present approximation theoretical guarantees informative path environmental monitoring gaussian process contrast
presented dealing complex probabilistic
complex cost functions arising path requires submodular objective
functions property hold value information proposition
sensor selection scheduling
context wireless sensor networks sensor nodes limited battery hence
enable small number measurements optimizing value information selected
sensors plays key role deciding selectively turn sensors order
conserve power first discussed slijepcevic potkonjak zhao shin reich
typically assumed sensors associated fixed sensing region spatial


fio ptimal value nformation g raphical odels

domain needs covered regions associated selected sensors abrams goel
plotkin present efficient approximation theoretical guarantees
deshpande khuller malekian toossi present
semidefinite programming sdp handling general constraints providing tighter
approximations approaches described apply optimizing sensor schedules complex utility functions e g increase prediction accuracy
objectives considered address shortcomings koushanfary taft
potkonjak developed sensor scheduling guarantees specified prediction accuracy regression model however relies solution
mixed integer program intractable general zhao et al proposed heuristics
selectively querying nodes sensor network order reduce entropy prediction unlike presented approaches performance
guarantees
relationship machine learning
decision trees quinlan popularized value information criterion creating
conditional plans unfortunately guarantees performance greedy method
subset selection instance feature selection central issue machine
learning vast amount literature see molina belanche nebot survey
however aware work providing similarly strong performance guarantees
considered
choosing observations strong connection field active learning
c f cohn gharamani jordan tong koller learning system designs
experiments observations sample complexity bounds derived
active learning c f dasgupta balcan beygelzimer langford
aware active learning perform provably optimal even restricted
classes instances
previous work authors
previous version appeared work krause guestrin b
contents section appeared part work singhvi et al present version
much extended algorithmic hardness detailed discussions
light negative presented section cannot expect able optimize value information complex chains however instead attempting
solve optimal solution one might wonder whether possible obtain good approximations authors showed krause guestrin krause et al krause leskovec
guestrin vanbriesen faloutsos large number practical objective functions satisfy intuitive diminishing returns property adding observation helps
observations far less already made many observations intuition formalized combinatorial concept called submodularity fundamental nemhauser
et al proves optimizing submodular utility function myopic greedy
fact provides near optimal solution within constant factor e optimal
unfortunately decision theoretic value information satisfy submodularity



fik rause g uestrin

proposition non submodularity value information decision theoretic value information submodular even naive bayes
intuitively value information non submodular need make several observations
order convince need change action

conclusions
described novel efficient optimal subset selection conditional plan computation chain graphical trees leaves including hmms empirical
evaluation indicates improve upon commonly used heuristics decreasing expected uncertainty effectively enhance performance interactive
structured classification tasks
unfortunately optimization become wildly intractable even slight generalization chains presented surprising theoretical limits indicate even class
decision theoretic value information functions widely used e g influence diagrams
pomdps cannot efficiently computed even naive bayes identified optimization value information class intractable nppp complete
polytrees
hardness along recent polytree graphical npcompleteness maximum posteriori assignment park darwiche np hardness
inference conditional linear gaussian lerner parr suggest possibility
developing generalized complexity characterization hard polytree graphical

light theoretical limits computing optimal solutions natural question ask
whether approximation non trivial performance guarantees found recent
krause guestrin radovilsky et al krause et al
case interesting classes value information

acknowledgments
would thank ben taskar providing part speech tagging model reuters
making news archive available would thank brigham anderson andrew moore helpful comments discussions work partially supported nsf
grants cns cns aro muri w nf gift intel
carlos guestrin partly supported alfred p sloan fellowship ibm faculty fellowship onr young investigator award n andreas krause
partially supported microsoft graduate fellowship

appendix
proof theorem membership p arbitrary discrete polytrees straightforward since
inference p let instance sat count
number assignments x xn satisfying let c c cm set
clauses create bayesian network n variables x xn u un
xi conditionally independent given let uniformly distributed values


fio ptimal value nformation g raphical odels


u

u

un


x

x

xn

figure graphical model used proof theorem
n n ui bernoulli prior p let
observed variables xi cpts defined following way

xi u satisfies clause cj
xi j ui u
otherwise

j
xi j ui u
u otherwise
model presented figure holds x x xn iff u un
encode satisfying assignment hence observe x x xn
know certainty furthermore least one xi know
p x x let nodes zero reward except assigned
reward function following properties model local
reward function decision theoretic value information
n n
p xa xa

r xa xa

otherwise
argument expected reward
x
r x xn
p p u u p x u r x x
u x



x

p p u

u sat

x
n n



u sat

exactly number satisfying assignments note model defined yet
naive bayes model however easily turned one marginalizing u
realize reward function properties maximum expected utility sense let set two decisions define utility function
property

n n





n n
u


n
otherwise
reward r xa given decision theoretic value information
x
x
r xa
p xa max
p xa u
xa







fik rause g uestrin

figure graphical model used proof theorem
utility function u following consideration upon observing particular instantiation variables x xn make decision variable goal achieve
number times action chosen exactly corresponds number satisfying assignments accomplished following way xi know ui
encoded satisfying assignment probability case action chosen
need make sure whenever least one xi indicates
u satisfying assignment decision chosen least one xi
j clause j satisfied utilities designed unless
n
p xa xa n action gives higher expected reward hereby
n n
lower bound probability misclassification p xa xa
note construction immediately proves hardness approximation suppose
polynomial time computes approximation r within
factor depend instance r r x xn r
implies r r implies r hence approximation r used
decide whether satisfiable implying p np
proof corollary let cnf formula convert naive bayes model variables x xn construction theorem function l v
v n set variables xi counts number satisfying assignments
note function l v n monotonic e l l v
v shown proposition hence majority assignments satisfies
l v n
proof theorem membership follows fact inference polytrees p discrete polytrees nondeterministic turing machine p oracle first guess selection
variables compute value information theorem since computation
p complete arbitrary discrete polytrees compare constant c
hardness let instance em ajsat instantiation
x xn x x n true majority assignments xn x n
let c c cm set cnf clauses create bayesian network shown figure
nodes ui uniform bernoulli prior add bivariate variables yi seli pari
n seli takes values pari parity bit cpts yi



fio ptimal value nformation g raphical odels

defined sel uniformly varies par n

j ui satisfies cj
seli seli j ui ui
j otherwise
pari pari bi ui bi ui
denotes parity xor operator add variables zit zif n
let

uniform ui

zi ui ui

otherwise
uniform denotes uniform distribution similarly let

uniform ui
zif ui ui

otherwise
intuitively zit guarantees us ui whereas zit leaves us uncertain ui
case zif symmetric
use subset selection choose zi encode solution em ajsat
zit chosen indicate xi set true similarly zif indicates false assignment
xi parity function going used ensure exactly one zit zif observed

first assign penalties nodes except zit zif n uj
n j n assigned zero penalty let nodes zero reward except
n assigned following reward
n
p sel n xa xa
p par n xa xa p par n xa xa
r n xa xa


otherwise
note sel n probability iff u u n encode satisfying assignment furthermore get positive reward certain sel n e chosen observation
set must contain proof satisfied certain par n parity certainty
occur certain assignment u u n possible infer
value ui certainty observing one ui zit zif since n cost
observing ui receive reward must observe least one zit zif assume
compute optimal subset budget n receive positive reward
observing exactly one zit zif
interpret selection zit zif assignment first n variables em ajsat
let r r n claim em ajsat r first let
em ajsat assignment x xn first n variables add un u n
add zit iff xi zif iff xi selection guarantees r
assume r call assignment u u n consistent n
zit ui zif ui consistent assignment chance
observations zi prove consistency n hence r implies majority
provably consistent assignments satisfy hence em ajsat proves subset
selection nppp complete
note realize local reward function r sense maximum expected utility
similarly described proof theorem


fik rause g uestrin

proof corollary constructions proof theorem theorem prove
computing conditional plans pp hard nppp hard respectively since instances
plan positive reward must observe variables corresponding valid instantiations e
x xn corollary un u n one z zn satisfy
parity condition theorem cases order selection irrelevant hence
conditional plan effectively performs subset selection
proof corollary proof follows observation polytree construction
proof theorem arranged two dependent chains transformation revert
arc zit ui applying bayes rule make sure number
nodes sensor timeslice triple variables yi calling copies yi yi
conditional probability tables given equality constraints yi yi yi yi
transformation variables associated timesteps given sets
z timesteps associated sets u timesteps associated
yi



zif yi
proof proposition bound follows fact maximization convex
application jensens inequality induction argument simply need
l l

x
x
l
p xa xa
max eu x xa xa
xa



tv




x
tv



x
tv

max


x

p xa xa eu x xa xa

xa

max eu x l



eu x xa xa

x

p xt xa xa ut xt

xt

expected utility action time observing xa xa
proof proposition consider following binary classification assymetric cost
one bernoulli random variable class label p
p two noisy observations x x conditionally independent given let p xi e observations agree class label
probability disagree probability three actions classifying
classifying assigning label define utility functon u
gain utility assign label correctly u u misassign
label u u choose e assign label




verify l l x l x l x x
hence adding x x increases utility adding x empty set contradicting
submodularity



fio ptimal value nformation g raphical odels

references
abrams z goel plotkin set k cover energy efficient monitoring
wireless sensor networks ipsn
balcan n beygelzimer langford j agnostic active learning icml
baum l e petrie statistical inference probabilistic functions finite state
markov chains ann math stat
bayer zubek v learning diagnostic policies examples systematic search uai
bellman r markovian decision process journal mathematics mechanics
bilgic getoor l voila efficient feature value acquisition classification
twenty second conference artificial intelligence aaai
boyd vandenberghe l convex optimization cambridge
boyen x koller tractable inference complex stochastic processes uncertainty artificial intelligence uai
chaloner k verdinelli bayesian experimental design review statistical science

cohn gharamani z jordan active learning statistical j ai

conll
conference computational natural language learning shared task
http cnts uia ac conll ner
cover thomas j elements information theory wiley interscience
dasgupta coarse sample complexity bounds active learning nips
deshpande guestrin c madden hellerstein j hong w model driven data
acquisition sensor networks vldb
deshpande khuller malekian toossi energy efficient monitoring
sensor networks latin
dittmer jensen f myopic value information influence diagrams uai pp
san francisco
domingos p pazzani optimality simple bayesian classifier
zero one loss machine learning
durbin r eddy r krogh mitchison g biological sequence analysis probabilistic proteins nucleic acids cambridge university press
gittins j c jones dynamic allocation index discounted multiarmed
bandit biometrika
goldberg l jerrum counting unlabelled subtrees tree p complete lms
j comput math
heckerman horvitz e middleton b approximate nonmyopic computation
value information ieee trans pattern analysis machine intelligence



fik rause g uestrin

howard r information value theory ieee transactions systems science
cybernetics ssc
howard r matheson j readings principles applications decision
analysis ii chap influence diagrams pp strategic decision group menlo park
reprinted decision analysis
ji parr r carin l non myopic multi aspect sensing partially observable
markov decision processes ieee transactions signal processing
kapoor horvitz e basu selective supervision guiding supervised learning
decision theoretic active learning international joint conference artificial intelligence
ijcai
keeney r l raiffa h decisions multiple objectives preferences value
trade offs wiley
ko c lee j queyranne exact maximum entropy sampling
operations
kollar roy n efficient optimization information theoretic exploration slam
aaai
koushanfary f taft n potkonjak sleeping coordination comprehensive sensing
isotonic regression domatic partitions infocom
krause guestrin c near optimal nonmyopic value information graphical
proc uncertainty artificial intelligence uai
krause guestrin c b optimal nonmyopic value information graphical
efficient theoretical limits proc ijcai
krause leskovec j guestrin c vanbriesen j faloutsos c efficient sensor
placement optimization securing large water distribution networks journal water resources management
krause singh guestrin c near optimal sensor placements gaussian processes theory efficient empirical studies jmlr
lafferty j mccallum pereira f conditional random fields probabilistic
segmenting labeling sequence data icml
lerner u parr r inference hybrid networks theoretical limits practical uai
lindley v measure information provided experiment annals
mathematical statistics
littman goldsmith j mundhenk computational complexity probabilistic
journal artificial intelligence
molina l belanche l nebot feature selection survey experimental evaluation icdm
mookerjee v mannino v sequential decision expert system optimization ieee trans knowl data eng



fio ptimal value nformation g raphical odels

munie shoham optimal testing structured knowledge twenty third conference artificial intelligence aaai
papadimitriou c h computational complexity addison wesley
park j darwiche complexity approximation strategies map
explanations journal aritificial intelligence
pineau j gordon g thrun anytime point approximations large pomdps
jair
quinlan j r induction decision trees machine learning
radovilsky shattah g shimony e efficient deterministic approximation non myopic value information graphical ieee international
conference systems man cybernetics smc vol pp
robbins h aspects sequential design experiments bulletin american
mathematical society
scheffer decomain c wrobel active learning partially hidden markov
information extraction ecml pkdd workshop instance selection
sim r roy n global optimal robot exploration slam ieee international
conference robotics automation icra
singh krause guestrin c kaiser w j batalin efficient
informative paths multiple robots international joint conference artificial intelligence ijcai pp hyderabad india
singhvi v krause guestrin c garrett j matthews h intelligent light control
sensor networks proc rd acm conference embedded networked sensor
systems sensys
slijepcevic potkonjak power efficient organization wireless sensor networks
icc
smallwood r sondik e optimal control partially observable markov decision
processes finite horizon operations
stachniss c grisetti g burgard w information gain exploration raoblackwellized particle filters robotics science systems rss
tong koller active learning parameter estimation bayesian networks
nips
turney p cost sensitive classification empirical evaluation hybrid genetic decision
tree induction journal artificial intelligence
van der gaag l wessels selective evidence gathering diagnostic belief networks
aisb quart
zhao f shin j reich j information driven dynamic sensor collaboration tracking
applications ieee signal processing




