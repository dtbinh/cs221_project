Journal Artificial Intelligence Research 35 (2009) 449-484

Submitted 1/09; published 7/09

Efficient Markov Network Structure Discovery
Using Independence Tests
Facundo Bromberg

fbromberg@frm.utn.edu.ar

Departamento de Sistemas de Informacion,
Universidad Tecnologica Nacional,
Mendoza, Argentina

Dimitris Margaritis
Vasant Honavar

dmarg@cs.iastate.edu
honavar@cs.iastate.edu

Dept. Computer Science,
Iowa State University,
Ames, IA 50011

Abstract
present two algorithms learning structure Markov network data:
GSMN GSIMN. algorithms use statistical independence tests infer structure successively constraining set structures consistent results
tests. recently, algorithms structure learning based maximum likelihood estimation, proved NP-hard Markov networks due
difficulty estimating parameters network, needed computation
data likelihood. independence-based approach require computation
likelihood, thus GSMN GSIMN compute structure efficiently
(as shown experiments). GSMN adaptation Grow-Shrink algorithm
Margaritis Thrun learning structure Bayesian networks. GSIMN extends GSMN additionally exploiting Pearls well-known properties conditional
independence relation infer novel independences known ones, thus avoiding performance statistical tests estimate them. accomplish efficiently GSIMN uses
Triangle theorem, introduced work, simplified version set
Markov axioms. Experimental comparisons artificial real-world data sets show
GSIMN yield significant savings respect GSMN , generating Markov
network comparable cases improved quality. compare GSIMN
forward-chaining implementation, called GSIMN-FCH, produces possible conditional independences resulting repeatedly applying Pearls theorems known
conditional independence tests. results comparison show GSIMN,
sole use Triangle theorem, nearly optimal terms set independences
tests infers.

1. Introduction
Graphical models (Bayesian Markov networks) important subclass statistical models possess advantages include clear semantics sound widely
accepted theoretical foundation (probability theory). Graphical models used
represent efficiently joint probability distribution domain. used
numerous application domains, ranging discovering gene expression pathways
bioinformatics (Friedman, Linial, Nachman, & Peer, 2000) computer vision (e.g. Geman

c
2009
AI Access Foundation. rights reserved.

fiBromberg, Margaritis, & Honavar

Figure 1: Example Markov network. nodes represent variables domain V =
{0, 1, 2, 3, 4, 5, 6, 7}.

& Geman, 1984, Besag, York, & Mollie, 1991, Isard, 2003, Anguelov, Taskar, Chatalbashev,
Koller, Gupta, Heitz, & Ng, 2005). One problem naturally arises construction
models data (Heckerman, Geiger, & Chickering, 1995, Buntine, 1994). solution
problem, besides theoretically interesting itself, holds potential
advancing state-of-the-art application domains models used.
paper focus task learning Markov networks (MNs) data
domains variables either discrete continuous distributed according
multidimensional Gaussian distribution. MNs graphical models consist two
parts: undirected graph (the model structure), set parameters. example
Markov network shown Figure 1. Learning models data consists two interdependent tasks: learning structure network, and, given learned structure,
learning parameters. work focus problem learning structure
MN domain data.
present two algorithms MN structure learning data: GSMN (Grow-Shrink
Markov Network learning algorithm) GSIMN (Grow-Shrink Inference-based Markov
Network learning algorithm). GSMN algorithm adaptation Markov networks
GS algorithm Margaritis Thrun (2000), originally developed learning
structure Bayesian networks. GSMN works first learning local neighborhood
variable domain (also called Markov blanket variable),
using information subsequent steps improve efficiency. Although interesting
useful itself, use GSMN point reference performance regard
time complexity accuracy achieved GSIMN, main result work.
GSIMN algorithm extends GSMN using Pearls theorems properties
conditional independence relation (Pearl, 1988) infer additional independences
set independences resulting statistical tests previous inferences, thus avoiding
execution tests data. allows savings execution time and, data
distributed, communication bandwidth.
rest paper organized follows: next section present previous
research related problem. Section 3 introduces notation, definitions presents
intuition behind two algorithms. Section 4 contains main algorithms, GSMN
GSIMN, well concepts practical details related operation. evaluate
GSMN GSIMN present results Section 5, followed summary
450

fiEfficient Markov Network Structure Discovery Using Independence Tests

work possible directions future research Section 6. Appendices B contain
proofs correctness GSMN GSIMN.

2. Related Work
Markov networks used physics computer vision communities (Geman &
Geman, 1984, Besag et al., 1991, Anguelov et al., 2005) historically
called Markov random fields. Recently interest use spatial
data mining, applications geography, transportation, agriculture, climatology,
ecology others (Shekhar, Zhang, Huang, & Vatsavai, 2004).
One broad popular class algorithms learning structure graphical models
score-based approach, exemplified Markov networks Della Pietra, Della Pietra,
Lafferty (1997), McCallum (2003). Score-based approaches conduct search
space legal structures attempt discover model structure maximum score.
Due intractable size search space i.e., space legal graphs,
super-exponential size, score-based algorithms must usually resort heuristic search.
step structure search, probabilistic inference step necessary evaluate
score (e.g., maximum likelihood, minimum description length, Lam & Bacchus, 1994,
pseudo-likelihood, Besag, 1974). Bayesian networks inference step tractable
therefore several practical score-based algorithms structure learning developed
(Lam & Bacchus, 1994, Heckerman, 1995, Acid & de Campos, 2003). Markov networks
however, probabilistic inference requires calculation normalizing constant (also
known partition function), problem known NP-hard (Jerrum & Sinclair, 1993,
Barahona, 1982). number approaches considered restricted class graphical
models (e.g. Chow & Liu, 1968, Rebane & Pearl, 1989, Srebro & Karger, 2001). However,
Srebro Karger (2001) prove finding maximum likelihood network NP-hard
Markov networks tree-width greater 1.
work area structure learning undirected graphical models concentrated learning decomposable (also called chordal) MNs (Srebro & Karger,
2001). example learning non-decomposable MNs presented work Hofmann Tresp (1998), approach learning structure continuous domains
non-linear relationships among domain attributes. algorithm removes edges
greedily based leave-one-out cross validation log-likelihood score. non-score based
approach work Abbeel, Koller, Ng (2006), introduces new class efficient algorithms structure parameter learning factor graphs, class graphical
models subsumes Markov Bayesian networks. approach based new
parameterization Gibbs distribution potential functions forced
probability distributions, supported generalization Hammersley-Clifford
theorem factor graphs. promising theoretically sound approach may
lead future practical efficient algorithms undirected structure learning.
work present algorithms belong independence-based constraintbased approach (Spirtes, Glymour, & Scheines, 2000). Independence-based algorithms exploit fact graphical model implies set independences exist distribution domain, therefore data set provided input algorithm (under
assumptions, see next section); work conducting set conditional independence
451

fiBromberg, Margaritis, & Honavar

tests data, successively restricting number possible structures consistent
results tests singleton (if possible), inferring structure
possible one. desirable characteristic independence-based approaches fact
require use probabilistic inference discovery structure.
Also, algorithms amenable proofs correctness (under assumptions).
Bayesian networks, independence-based approach mainly exemplified
SGS (Spirtes et al., 2000), PC (Spirtes et al., 2000), algorithms learn
Markov blanket step learning Bayesian network structure Grow-Shrink
(GS) algorithm (Margaritis & Thrun, 2000), IAMB variants (Tsamardinos, Aliferis,
& Statnikov, 2003a), HITON-PC HITON-MB (Aliferis, Tsamardinos, & Statnikov,
2003), MMPC MMMB (Tsamardinos, Aliferis, & Statnikov, 2003b), max-min hill
climbing (MMHC) (Tsamardinos, Brown, & Aliferis, 2006), widely used
field. Algorithms restricted classes trees (Chow & Liu, 1968) polytrees
(Rebane & Pearl, 1989) exist.
learning Markov networks previous work mainly focused learning Gaussian
graphical models, assumption continuous multivariate Gaussian distribution
made; results linear dependences among variables Gaussian noise (Whittaker, 1990, Edwards, 2000). recent approaches included works Dobra,
Hans, Jones, Nevins, Yao, West (2004), (Castelo & Roverato, 2006), Pena (2008),
Schafer Strimmer (2005), focus applications Gaussian graphical models
Bioinformatics. make assumption continuous Gaussian variables
paper, algorithms present applicable domains use
appropriate conditional independence test (such partial correlation). GSMN
GSIMN algorithms presented apply case arbitrary faithful distribution assumed probabilistic conditional independence test distribution
available. algorithms first introduced Bromberg, Margaritis, Honavar
(2006); contributions present paper include extending results conducting
extensive evaluation experimental theoretical properties. specifically,
contributions include extensive systematic experimental evaluation proposed algorithms (a) data sets sampled artificially generated networks varying
complexity strength dependences, well (b) data sets sampled networks
representing real-world domains, (c) formal proofs correctness guarantee
proposed algorithms compute correct Markov network structure domain,
stated assumptions.

3. Notation Preliminaries
denote random variables capitals (e.g., X, Y, Z) sets variables bold
capitals (e.g., X, Y, Z). particular, denote V = {0, . . . , n 1} set n
variables domain. name variables indices V; instance,
refer third variable V simply 3. denote data set size
(number data points) |D| N . use notation (XY | Z) denote
proposition X independent conditioned Z, disjoint sets variables X,
Y, Z. (X 6Y | Z) denotes conditional dependence. use (XY | Z) shorthand
({X}{Y } | Z) improve readability.
452

fiEfficient Markov Network Structure Discovery Using Independence Tests

Markov network undirected graphical model represents joint probability
distribution V. node graph represents one random variables
domain, absences edges encode conditional independences among them.
assume underlying probability distribution graph-isomorph (Pearl, 1988) faithful
(Spirtes et al., 2000), means faithful undirected graph. graph G
said faithful distribution graph connectivity represents exactly
dependencies independences existent distribution. detail, means
disjoint sets X, Y, Z V, X independent given Z set
vertices Z separates set vertices X set vertices graph G (this
sometimes called global Markov property, Lauritzen, 1996). words, means
that, removing vertices Z G (including edges incident them),
exists (undirected) path remaining graph variable X
variable Y. example, Figure 1, set variables {0, 5} separates set {4, 6}
set {2}. generally, shown (Pearl, 1988; Theorem 2, page 94 definition
graph isomorphism, page 93) necessary sufficient condition distribution
graph-isomorph set independence relations satisfy following axioms
disjoint sets variables X, Y, Z, W individual variable :

(Symmetry)
(Decomposition)
(Intersection)
(Strong Union)
(Transitivity)

(XY | Z)
(XY W | Z)
(XY | Z W)
(XW | Z Y)
(XY | Z)
(XY | Z)




(YX | Z)
(XY | Z) (XW | Z)

=
=
=

(XY W | Z)
(XY | Z W)
(X | Z) (Y | Z)

(1)

operation algorithms assume existence oracle
answer statistical independence queries. standard assumptions needed
formally proving correctness independence-based structure learning algorithms
(Spirtes et al., 2000).
3.1 Independence-Based Approach Structure Learning
GSMN GSIMN independence-based algorithms learning structure
Markov network domain. approach works evaluating number statistical
independence statements, reducing set structures consistent results
tests singleton (if possible), inferring structure possible one.
mentioned above, theory assume existence independence-query oracle
provide information conditional independences among domain variables.
viewed instance statistical query oracle (Kearns & Vazirani, 1994).
practice oracle exist; however, implemented approximately
statistical test evaluated data set D. example, discrete data
Pearsons conditional independence chi-square (2 ) test (Agresti, 2002), mutual
information test etc. continuous Gaussian data statistical test used
measure conditional independence partial correlation (Spirtes et al., 2000). determine
conditional independence two variables X given set Z data,
453

fiBromberg, Margaritis, & Honavar

statistical test returns p-value. p-value test equals probability obtaining
value test statistic least extreme one actually observed
given null hypothesis true, corresponds conditional independence
case. Assuming p-value test p(X, | Z), statistical test concludes
dependence p(X, | Z) less equal threshold i.e.,
(X 6Y | Z) p(X, | Z) .
quantity 1 sometimes referred tests confidence threshold. use
standard value = 0.05 experiments, corresponds confidence
threshold 95%.
faithful domain, shown (Pearl & Paz, 1985) edge exists
two variables X 6= V Markov network domain
dependent conditioned remaining variables domain, i.e.,
(X, ) edge iff (X 6Y | V {X, }).
Thus, learn structure, theoretically suffices perform n(n 1)/2 tests i.e.,
one test (X, | V {X, }) pair variables X, V, X 6= . Unfortunately,
non-trivial domains usually involves test conditions large number
variables. Large conditioning sets produce sparse contingency tables (count histograms)
result unreliable tests. number possible configurations
variables grows exponentially size conditioning setfor example,
2n cells test involving n binary variables, fill table one data point
per cell would need data set least exponential size i.e., N 2n . Exacerbating
problem, one data point per cell typically necessary reliable test:
recommended Cochran (1954), 20% cells contingency table
less 5 data points test deemed unreliable. Therefore GSMN
GSIMN algorithms (presented below) attempt minimize conditioning set size;
choosing order examining variables irrelevant variables
examined last.

4. Algorithms Related Concepts
section present main algorithms, GSMN GSIMN, supporting concepts required description. purpose aiding understanding
reader, discussing first describe abstract GSMN algorithm next
section. helps showing intuition behind algorithms laying foundation
them.
4.1 Abstract GSMN Algorithm
sake clarity exposition, discussing first algorithm GSMN ,
describe intuition behind describing general structure using abstract GSMN
algorithm deliberately leaves number details unspecified; filled-in
concrete GSMN algorithm, presented next section. Note choices

454

fiEfficient Markov Network Structure Discovery Using Independence Tests

Algorithm 1 GSMN algorithm outline: G = GSMN (V, D).
1: Initialize G empty graph.
2: variables X domain V
3:
/* Learn Markov Blanket BX X using GS algorithm. */
4:
BX GS (X, V, D)
5:
Add undirected edge G X variable BX .
6: return G

Algorithm 2 GS algorithm. Returns Markov Blanket BX variable X V: BX =
GS (X, V, D).
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

BX
/* Grow phase. */
variable V {X}
(X 6Y | BX ) (estimated using data D)
BX BX {Y }
goto 3 /* Restart grow loop. */
/* Shrink phase. */
variable BX
(XY | BX {Y }) (estimated using data D)
BX BX {Y }
goto 8 /* Restart shrink loop. */
return BX

details source optimizations reduce algorithms computational cost.
make explicit discuss concrete GSMN GSIMN algorithms.
abstract GSMN algorithm shown Algorithm 1. Given input data set
set variables V, GSMN computes set nodes (variables) BX adjacent
variable X V; completely determine structure domain MN.
algorithm consists main loop learns Markov blanket BX node
(variable) X domain using GS algorithm. constructs Markov network
structure connecting X variable BX .
GS algorithm first proposed Margaritis Thrun (2000) shown
Algorithm 2. consists two phases, grow phase shrink phase. grow phase
X proceeds attempting add variable current set hypothesized
neighbors X, contained BX , initially empty. BX grows variable
iteration grow loop X found dependent X
given current set hypothesized neighbors BX . Due (unspecified) ordering
variables examined (this explicitly specified concrete GSMN algorithm,
presented next section), end grow phase variables BX
might true neighbors X underlying MNthese called false positives.
justifies shrink phase algorithm, removes false positive BX
testing independence X conditioned BX {Y }. found independent
X shrink phase, cannot true neighbor (i.e., cannot edge X ),
GSMN removes BX . Assuming faithfulness correctness independence
query results, end shrink phase BX contains exactly neighbors X
underlying Markov network.
455

fiBromberg, Margaritis, & Honavar

next section present concrete implementation GSMN, called GSMN .
augments GSMN specifying concrete ordering variables X examined
main loop GSMN (lines 25 Algorithm 1), well concrete order
variables examined grow shrink phases GS algorithm (lines 36
811 Algorithm 2, respectively).
4.2 Concrete GSMN Algorithm
section discuss first algorithm, GSMN (Grow-Shrink Markov Network
learning algorithm), learning structure Markov network domain. Note
reason introducing GSMN addition main contribution, GSIMN
algorithm (presented later Section 4.5), comparison reasons. particular, GSIMN
GSMN identical structure, following order examination variables,
difference use inference GSIMN (see details subsequent
sections). Introducing GSMN therefore makes possible measure precisely (through
experimental results Section 5) benefits use inference performance.
GSMN algorithm shown Algorithm 3. structure similar abstract
GSMN algorithm. One notable difference order variables examined
specified; done initialization phase so-called examination order
grow order X variable X V determined. X priority
queues initially permutation V (X permutation V {X})
position variable queue denotes priority e.g., = [2, 0, 1] means
variable 2 highest priority (will examined first), followed 0 finally 1.
Similarly, position variable X determines order examined
grow phase X.
initialization phase algorithm computes strength unconditional
dependence pair variable X , given unconditional p-value
p(X, | ) independence test pair variables X 6= , denoted
pXY algorithm. (In practice logarithm p-values computed, allows
greater precision domains dependencies may strong weak.)
particular, algorithm gives higher priority (examines earlier) variables
lower average log p-value (line 5), indicating stronger dependence. average defined
as:
X
1
avg log(pXY ) =
log(pXY ).
|V| 1

6=X

grow order X variable X, algorithm gives higher priority variables
whose p-value (or equivalently log p-value) variable X small (line 8).
ordering due intuition behind folk-theorem (as Koller & Sahami, 1996,
puts it) states probabilistic influence association attributes tends
attenuate distance graphical model. suggests pair variables X
high unconditional p-value less likely directly linked. Note ordering
heuristic guaranteed hold general. example, may hold
underlying domain Bayesian network e.g., two spouses may independent
unconditionally dependent conditional common child. Note however
example apply faithful domains i.e., graph-isomorph Markov network.
456

fiEfficient Markov Network Structure Discovery Using Independence Tests

Algorithm 3 GSMN , concrete implementation GSMN: G = GSMN (V, D).
1:
2:
3:
4:
5:

Initialize G empty graph.
/* Initialization. */
X, V, X 6=
pXY p(X, | )


Initialize i, {0, . . . , n 1}, < avg log(pi j ) < avg log(pi j ) .
j

j

6: X V
7:
BX


8:
Initialize X j, j {0, . . . , n 1}, j < j pXX < pXX .
j

j

9:
Remove X X .
10: /* Main loop. */
11: empty
12:
X dequeue()
13:
/* Propagation phase. */
14:
{Y : examined X }
15:
F {Y : examined X
/ }
16:
T, move end X .
17:
F, move end X .
18:
/* Grow phase. */
19:

20:
X empty
21:
dequeue(X )
22:
pXY
23:
IGSMN (X, Y, S, F, T)
24:
{Y }
25:
/* Change grow order . */
26:
Move X beginning .
27:
W = S|S|2 S0
28:
Move W beginning .
29:
/* Change examination order. */
30:
W = S|S|1 S0
31:
W
32:
Move W beginning .
33:
break line 34
34:
/* Shrink phase. */
35:
= S|S|1 S0
36:
IGSMN (X, Y, {Y } , F, T)
37:
{Y }
38:
BX
39:
Add undirected edge G X variable BX .
40: return G

note correctness algorithms present depend holding i.e.,
prove Appendices B, GSMN GSIMN guaranteed return
correct structure assumptions stated Section 3 above. note
computational cost calculation pXY low due empty conditioning set.
remaining GSMN algorithm contains main loop (lines 1039)
variable V examined according examination order , determined
457

fiBromberg, Margaritis, & Honavar

Algorithm 4 IGSMN (X, Y, S, F, T): Calculate independence test (X, | S) propagation, possible, otherwise run statistical test data.
1:
2:
3:
4:
5:
6:
7:
8:
9:

/* Attempt infer dependence propagation. */

return false
/* Attempt infer independence propagation. */
F
return true
/* Else statistical test data. */
1(p(X,Y |Z)>) /* = true iff p-value statistical test (X, | S) > . */
return

initialization phase. main loop includes three phases: propagation phase (lines
1317), grow phase (lines 1833), shrink phase (lines 3437). propagation
phase optimization variables already computed
(i.e., variables already examined) collected two sets F T. Set F (T) contains
variables X
/ (X ). sets passed independence
procedure IGSMN , shown Algorithm 4, purpose avoiding execution
tests X algorithm. justified fact that, undirected
graphs, Markov blanket X X Markov blanket .
Variables already found contain X blanket (set F) cannot members
BX exists set variables rendered conditionally
independent X previous step, independence therefore inferred easily.
Note experiments section paper (Section 5) evaluate GSMN
without propagation phase, order measure effect propagation
optimization performance. Turning propagation accomplished simply setting
sets F (as computed lines 14 15, respectively) empty set.
Another difference GSMN abstract GSMN algorithm use condition pXY (line 22). additional optimization avoids independence test
case X found (unconditionally) independent initialization
phase, since case would imply X independent given conditioning
set axiom Strong Union.
crucial difference GSMN abstract GSMN algorithm GSMN
changes examination order grow order every variable X . (Since
X
/ X , excludes grow order X itself.) changes ordering proceed
follows: end grow phase variable X, new examination order (set
lines 3033) dictates next variable W examined X last
added growing phase yet examined (i.e., W still ).
grow order variables found dependent X changed; done
maximize number optimizations GSIMN algorithm (our main contribution
paper) shares algorithm structure GSMN . changes grow order
therefore explained detail Section 4.5 GSIMN presented.
final difference GSMN abstract GSMN algorithm restart
actions grow shrink phases GSMN whenever current Markov blanket
modified (lines 6 11 Algorithm 2), present GSMN . restarting

458

fiEfficient Markov Network Structure Discovery Using Independence Tests

Figure 2: Illustration operation GSMN using independence graph. figure
shows growing phase variable 5. Variables examined according
grow order 5 = [3, 4, 1, 6, 2, 7, 0].

loops necessary GS algorithm due original usage learning
structure Bayesian networks. task, possible true member
blanket X found initially independent grow loop conditioning
set found dependent later conditioned superset S.
could happen unshielded spouse X i.e., one common
children X existed direct link X underlying Bayesian
network. However, behavior impossible domain distribution faithful
Markov network (one assumptions): independence X given
must hold superset axiom Strong Union (see Eqs. (1)).
restart grow shrink loops therefore omitted GSMN order save
unnecessary tests. Note that, even though possible behavior impossible
faithful domains, possible unfaithful ones, experimentally evaluated
algorithms real-world domains assumption Markov faithfulness may
necessarily hold (Section 5).
proof correctness GSMN presented Appendix A.
4.3 Independence Graphs
demonstrate operation GSMN graphically concept independence
graph, introduce. define independence graph undirected
graph conditional independences dependencies single variables
represented one annotated edges them. solid (dotted) edge
variables X annotated Z represents fact X found
dependent (independent) given Z. conditioning set Z enclosed parentheses
edge represents independence dependence inferred Eqs. (1) (as
opposed computed statistical tests). Shown graphically:
459

fiBromberg, Margaritis, & Honavar

X
X
X
X

Z


(X 6Y | Z)



(XY | Z)



(X 6Y | Z) (inferred)



(XY | Z) (inferred)

Z
(Z)
(Z)

instance, Figure 2, dotted edge 5 1 annotated 3, 4 represents
fact (51 | {3, 4}). absence edge two variables indicates
absence information independence dependence variables
conditioning set.
Example 1. Figure 2 illustrates operation GSMN using independence graph
domain whose underlying Markov network shown Figure 1. figure shows
independence graph end grow phase variable 5, first examination
order . (We discuss example initialization phase GSMN ; instead,
assume examination () grow () orders shown figure.) According
vertex separation underlying network (Figure 1), variables 3, 4, 6, 7 found
dependent 5 growing phase i.e.,
I(5, 3 | ),
I(5, 4 | {3}),
I(5, 6 | {3, 4}),
I(5, 7 | {3, 4, 6})
therefore connected 5 independence graph solid edges annotated sets
, {3}, {3, 4} {3, 4, 6} respectively. Variables 1, 2, 0 found independent i.e.,
I(5, 1 | {3, 4}),
I(5, 2 | {3, 4, 6}),
I(5, 0 | {3, 4, 6, 7})
thus connected 5 dotted edges annotated {3, 4}, {3, 4, 6} {3, 4, 6, 7}
respectively.
4.4 Triangle Theorem
section present prove theorem used subsequent GSIMN
algorithm. seen, main idea behind GSIMN algorithm attempt decrease number tests done exploiting properties conditional independence
relation faithful domains i.e., Eqs. (1). properties seen inference rules
used derive new independences ones know true. careful
study axioms suggests two simple inference rules, stated Triangle
theorem below, sufficient inferring useful independence information
inferred systematic application inference rules. confirmed
experiments Section 5.
460

fiEfficient Markov Network Structure Discovery Using Independence Tests

Figure 3: Independence graph depicting Triangle theorem. Edges graph
labeled sets represent conditional independences dependencies. solid
(dotted) edge X labeled Z means X dependent
(independent) given Z. set label enclosed parentheses means edge
inferred theorem.

Theorem 1 (Triangle theorem). Given Eqs. (1), every variable X, , W sets Z1
Z2 {X, Y, W } Z1 = {X, Y, W } Z2 = ,
(X 6W | Z1 ) (W 6Y | Z2 )

=

(X 6Y | Z1 Z2 )

(XW | Z1 ) (W 6Y | Z1 Z2 )

=

(XY | Z1 ).

call first relation D-triangle rule second I-triangle rule.
Proof. using Strong Union Transitivity Eqs. (1) shown contrapositive form.
(Proof D-triangle rule):
Strong Union (X 6W | Z1 ) get (X 6W | Z1 Z2 ).
Strong Union (W 6Y | Z1 ) get (W 6Y | Z1 Z2 ).
Transitivity, (X 6W | Z1 Z2 ), (W 6Y | Z1 Z2 ), get (X 6Y | Z1 Z2 ).
(Proof I-triangle rule):
Strong Union (W 6Y | Z1 Z2 ) get (W 6Y | Z1 ).
Transitivity, (XW | Z1 ) (W 6Y | Z1 ) get (XY | Z1 ).

represent Triangle theorem graphically using independence graph construct Section 4.2. Figure 3 depicts two rules Triangle theorem using two
independence graphs.
Triangle theorem used infer additional conditional independences
tests conducted operation GSMN . example shown Figure 4, illustrates application Triangle theorem example presented
Figure 2. independence information inferred Triangle theorem shown
curved edges (note conditioning set edge enclosed parentheses).

461

fiBromberg, Margaritis, & Honavar

Figure 4: Illustration use Triangle theorem example Figure 2. set
variables enclosed parentheses correspond tests inferred Triangle
theorem using two adjacent edges antecedents. example, result
(17 | {3, 4}), inferred I-triangle rule, independence (51 | {3, 4})
dependence (5 67 | {3, 4, 6}).

example, independence edge (4, 7) inferred D-triangle rule adjacent edges (5, 4) (5, 7), annotated {3} {3, 4, 6} respectively. annotation
inferred edge {3}, intersection annotations {3} {3, 4, 6}.
example application I-triangle rule edge (1, 7), inferred edges
(5, 1) (5, 7) annotations {3, 4} {3, 4, 6} respectively. annotation
inferred edge {3, 4}, intersection annotations {3, 4, 6} {3, 4}.
4.5 GSIMN Algorithm
previous section saw possibility using two rules Triangle
theorem infer result novel tests grow phase. GSIMN algorithm
(Grow-Shrink Inference-based Markov Network learning algorithm), introduced section, uses Triangle theorem similar fashion extend GSMN inferring value
number tests GSMN executes, making evaluation unnecessary. GSIMN
GSMN work exactly way (and thus GSIMN algorithm shares exactly
algorithmic description i.e., follow Algorithm 3), differences
concentrated independence procedure use: instead using independence
procedure IGSMN GSMN , GSIMN uses procedure IGSIMN , shown Algorithm 5. Procedure IGSIMN , addition attempting propagate blanket information obtained
examination previous variables (as IGSMN does), attempts infer
value independence test provided input either Strong Union
axiom (listed Eqs. (1)) Triangle theorem. attempt successful, IGSIMN
returns value inferred (true false), otherwise defaults statistical test
data set (as IGSMN does). purpose assisting inference process, GSIMN
462

fiEfficient Markov Network Structure Discovery Using Independence Tests

Algorithm 5 IGSIMN (X, Y, S, F, T): Calculate independence test result inference (including propagation), possible. Record test result knowledge base.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:

/* Attempt infer dependence propagation. */

return false
/* Attempt infer independence propagation. */
F
return true
/* Attempt infer dependence Strong Union. */
(A, false) KXY
return false
/* Attempt infer dependence D-triangle rule. */
W
(A, false) KXW (B, false) KW B
Add (A B, false) KXY KY X .
return false
/* Attempt infer independence Strong Union. */
(A, true) KXY
return true
/* Attempt infer independence I-triangle rule. */
W
(A, true) KXW s.t. (B, false) KW s.t. B
Add (A, true) KXY KY X .
return true
/* Else statistical test data. */
1(p(X,Y |Z)>) /* = true iff p-value statistical test (X, | S) > . */
Add (S, t) KXY KY X .
return

IGSIMN maintain knowledge base KXY pair variables X , containing
outcomes tests evaluated far X (either data inferred).
knowledge bases empty beginning GSIMN algorithm (the initialization step shown algorithm since GSMN use it), maintained
within test procedure IGSIMN .
explain IGSIMN (Algorithm 5) detail. IGSIMN attempts infer independence value input triplet (X, | S) applying single step backward
chaining using Strong Union Triangle rules i.e., searches knowledge base
K = {KXY : X, V} antecedents instances rules input triplet
(X, | S) consequent. Strong Union rule used direct shown
Eqs. (1) contrapositive form. direct form used infer independences, therefore refer I-SU rule on. contrapositive form,
I-SU rule becomes (X 6Y | W) = (X 6Y | S), referred D-SU rule
since used infer dependencies. According D-Triangle D-SU rules,
dependence (X 6Y | S) inferred knowledge base K contains
1. test (X 6Y | A) S,
2. tests (X 6W | A) (W 6Y | B) variable W , B S,
463

fiBromberg, Margaritis, & Honavar

Figure 5: Illustration operation GSIMN. figure shows grow phase two
consecutively examined variables 5 7. figure shows variable
examined second 3 7, according change examination order
lines 3033 Algorithm 3. set variables enclosed parentheses
correspond tests inferred Triangle theorem using two adjacent edges
antecedents. results (7 63 | ), (7 64 | {3}), (7 66 | {3, 4}), (7 65 |
{3, 4, 6}) (b), shown highlighted, executed inferred tests
done (a).

respectively. According I-Triangle I-SU rules, independence (XY | S)
inferred knowledge base contains
3. test (XY | A) S,
4. tests (XW | A) (W 6Y | B) variable W , B A,
respectively.
changes grow orders variables occur inside grow phase
currently examined variable X (lines 2528 GSIMN i.e., Algorithm 3 IGSMN replaced IGSIMN .). particular, if, variable , algorithm reaches line 24,
i.e., pXY IGSIMN (X, Y, S) = false, X variables found
dependent X (i.e., variables currently S) promoted beginning
grow order . illustrated Figure 5 variable 7, depicts grow
phase two consecutively examined variables 5 7. figure, curved edges
show tests inferred IGSIMN grow phase variable 5. grow
order 7 changes 7 = [2, 6, 3, 0, 4, 1, 5] 7 = [3, 4, 6, 5, 2, 0, 1] grow phase
variable 5 complete variables 5, 6, 4 3 promoted (in order)
beginning queue. rationale observation increases
number tests inferred GSIMN next step: change examination
grow orders described chosen inferred tests learning
blanket variable 7 match exactly required algorithm future step.
464

fiEfficient Markov Network Structure Discovery Using Independence Tests

particular, note example set inferred dependencies variable
found dependent 5 7 exactly required initial part grow
phase variable 7, shown highlighted Figure 5(b) (the first four dependencies).
independence tests inferred (not conducted), resulting computational savings.
general, last dependent variable grow phase X maximum number
dependences independences inferred provides rationale change
grow order selection algorithm examined next.
shown assumptions GSMN , structure returned
GSIMN correct one i.e., set BX computed GSIMN algorithm equals
exactly neighbors X. proof correctness GSIMN based correctness
GSMN presented Appendix B.
4.6 GSIMN Technical Implementation Details
section discuss number practical issues subtly influence accuracy
efficiency implementation GSIMN. One order application I-SU, D-SU,
I-Triangle D-Triangle rules within function IGSIMN . Given independence-query
oracle, order application matterassuming one rules
inferring value independence, guaranteed produce
value due soundness axioms Eqs. (1) (Pearl, 1988). practice however,
oracle implemented statistical tests conducted data incorrect,
previously mentioned. particular importance observation false independences
likely occur false dependencies. One example case
domain dependencies weakin case pair variables connected (dependent)
underlying true network structure may incorrectly deemed independent paths
long enough. hand, false dependencies much rare
confidence threshold 1 = 0.95 statistical test tells us probability
false dependence chance alone 5%. Assuming i.i.d. data test, chance
multiple false dependencies even lower, decreasing exponentially fast. practical
observation i.e., dependencies typically reliable independences, provide
rationale way IGSIMN algorithm works. particular, IGSIMN prioritizes
application rules whose antecedents contain dependencies first i.e., D-Triangle
D-SU rules, followed I-Triangle I-SU rules. effect, uses statistical results
typically known greater confidence ones usually less reliable.
second practical issue concerns efficient inference. GSIMN algorithm uses onestep inference procedure (shown Algorithm 5) utilizes knowledge base K = {KXY }
containing known independences dependences pair variables X .
implement inference efficiently utilize data structure K purpose
storing retrieving independence facts constant time. consists two 2D arrays,
one dependencies another independencies. array n n size, n
number variables domain. cell array corresponds pair
variables (X, ), stores known independences (dependences) X
form list conditioning sets. conditioning set Z list, knowledge
base KXY represents known independence (XY | Z) (dependence (X 6Y | Z)).
important note length list 2, two

465

fiBromberg, Margaritis, & Honavar

tests done variable X execution GSIMN (done
growing shrinking phases). Thus, always takes constant time retrieve/store
independence (dependence), therefore inferences using knowledge base
constant time well. note uses Strong Union axion IGSIMN
algorithm constant time well, accomplished testing (at
two) sets stored KXY subset superset inclusion.

5. Experimental Results
evaluated GSMN GSIMN algorithms artificial real-world data sets.
experimental results presented show simple application
Pearls inference rules GSIMN algorithm results significant reduction number
tests performed compared GSMN without adversely affecting quality
output network. particular report following quantities:
Weighted number tests. weighted number tests computed
summation weight test executed, weight test (X, | Z)
defined 2+|Z|. quantity reflects time complexity algorithm (GSMN
GSIMN) used assess benefit GSIMN using inference instead
executing statistical tests data. standard method comparison
independence-based algorithms justified observation running
time statistical test triplet (X, | Z) proportional size N data
set number variables involved i.e., O(N (|Z|+2)) (and exponential
number variables involved nave implementation might assume).
one construct non-zero entries contingency table used
test examining data point data set exactly once, time proportional
number variables involved test i.e., proportional |{X, }Z| = 2+|Z|.
Execution time. order assess impact inference running time
(in addition impact statistical tests), report execution time
algorithm.
Quality resulting network. measure quality two ways.
Normalized Hamming distance. Hamming distance output
network structure underlying model another measure
quality output network, actual network used generate
data known. Hamming distance defined number reversed
edges two network structures, i.e., number times actual
edge true network missing returned network edge absent
true network exists algorithms output network. value zero
means output network correct structure. able compare
domains
different dimensionalities (number variables n) normalize

n
2 , total number node pairs corresponding domain.
Accuracy. real-world data sets underlying network unknown,
Hamming distance calculation possible. case impossible know
true value independence. therefore approximate statistical
test entire data set, use limited, randomly chosen subset (1/3
data set) learn network. measure accuracy compare result
466

fiEfficient Markov Network Structure Discovery Using Independence Tests

(true false) number conditional independence tests network
output (using vertex separation), tests performed full data
set.
experiments involving data sets used 2 statistical test estimation
conditional independences. mentioned above, rules thumb exist deem certain
tests potentially unreliable depending counts contingency table involved;
example, one rule Cochran (1954) deems test unreliable 20%
cells contingency table less 5 data points test. Due requirement
answer must obtained independence algorithm conducting test, used
outcomes tests well experiments. effect possibly unreliable
tests quality resulting network measured accuracy measures, listed
above.
next section present results domains underlying probabilistic
model known. followed real-world data experiments model structure
available.
5.1 Known-Model Experiments
first set experiments underlying model, called true model true network,
known Markov network. purpose set experiments conduct controlled
evaluation quality output network systematic study algorithms
behavior varying conditions domain size (number variables) amount
dependencies (average node degree network).
true network contains n variables generated randomly follows:
network initialized n nodes edges. user-specified parameter
network structure average node degree equals average number neighbors
per node. Given , every node set neighbors determined randomly
uniformly selecting first n2 pairs random permutation possible pairs.
factor 1/2 necessary edge contributes degree two nodes.
conducted two types experiments using known network structure: Exact learning
experiments sample-based experiments.
5.1.1 Exact Learning Experiments
set known-model experiments, assume result statistical queries
asked GSMN GSIMN algorithms available, assumes existence
oracle answer independence queries. underlying model known,
oracle implemented vertex separation. benefits querying
true network independence two: First, ensures faithfulness correctness
independence query results, allows evaluation algorithms
assumptions correctness. Second, tests performed much faster actual
statistical tests data. allowed us evaluate algorithms large networkswe
able conduct experiments domains containing 100 variables.
first report weighted number tests executed GSMN without
propagation GSIMN. results summarized Figure 6, shows ratio
weighted number tests GSIMN two versions GSMN . One
467

fiWC(GSIMN) / WC(GSMN* propagation)

Ratio weighted cost GSIMN vs. GSMN* without propagation
1
0.9
0.8
0.7
0.6
=1
=2
=4
=8

0.5
0.4
0.3
0.2
0.1
0
0

10

20

30
40
50
60
70
80
Domain size (number variables)

90

100

WC(GSIMN) / WC(GSMN* without propagation)

Bromberg, Margaritis, & Honavar

Ratio weighted cost GSIMN vs. GSMN* propagation
1
0.9
0.8
0.7
0.6
0.5
0.4
=1
=2
=4
=8

0.3
0.2
0.1
0
0

10

20

30
40
50
60
70
80
Domain size (number variables)

90

100

Figure 6: Ratio weighted number tests GSIMN GSMN without propagation (left plot) propagation (right plot) network sizes (number
nodes) n = 100 average degree = 1, 2, 4, 8.
Algorithm 6 IFCH (X, Y, S, F, T). Forward-chaining implementation independence test
IGSIMN (X, Y, S, F, T).
1:
2:
3:
4:
5:
6:
7:

/* Query knowledge base. */
(S, t) KXY
return
result test (X, | S) /* = true iff test (X, | S) returns independence. */
Add (S, t) KXY KY X .
Run forward-chaining inference algorithm K, update K.
return

hundred true networks generated randomly pair (n, ), figure shows
mean value. see limiting reduction (as n grows large) weighted
number tests depends primarily average degree parameter . reduction
GSIMN large n dense networks ( = 8) approximately 40% compared GSMN
propagation 75% compared GSMN without propagation optimization,
demonstrating benefit GSIMN vs. GSMN terms number tests executed.
One reasonable question performance GSIMN extent inference
procedure complete i.e., tests GSIMN needs operation,
number tests infers (by applying single step backward chaining
Strong Union axiom Triangle theorem, rather executing statistical test
data) compare number tests inferred (for example using complete
automated theorem prover Eqs. (1))? measure this, compared number tests
done GSIMN number done alternative algorithm, call GSIMNFCH (GSIMN Forward Chaining). GSIMN-FCH differs GSIMN function
IFCH , shown Algorithm 6, replaces function IGSIMN GSIMN. IFCH exhaustively
produces independence statements inferred properties Eqs. (1)
using forward-chaining procedure. process iteratively builds knowledge base K
containing truth value conditional independence predicates. Whenever outcome
test required, K queried (line 2 IFCH Algorithm 6). value test
468

fiEfficient Markov Network Structure Discovery Using Independence Tests

Ratio Number tests GSIMN-FCH GSIMN

=1
=2
=4
=8

1.4
1.2

Ratio

1
0.8
0.6
0.4
0.2
0

2

3

4

5

6

7

8

9

10 11 12

Number variables (n)

Figure 7: Ratio number tests GSIMN-FCH GSIMN network sizes (number
variables) n = 2 n = 13 average degrees = 1, 2, 4, 8.

found K, returned (line 3). not, GSIMN-FCH performs test uses result
standard forward-chaining automatic theorem prover subroutine (line 6) produce
independence statements inferred test result K, adding new
facts K.
comparison number tests executed GSIMN vs. GSIMN-FCH presented
Figure 7, shows ratio number tests GSIMN GSIMN-FCH.
figure shows mean value four runs, corresponding network generated
randomly pair (n, ), = 1, 2, 4 8 n 12. Unfortunately, two
days execution GSIMN-FCH unable complete execution domains containing
13 variables more. therefore present results domain sizes 12 only.
figure shows n 9, every ratio exactly 1 i.e., tests inferable
produced use Triangle theorem GSIMN. smaller domains, ratio
0.95 exception single case, (n = 5, = 1).
5.1.2 Sample-based Experiments
set experiments evaluate GSMN (with without propagation) GSIMN
data sampled true model. allows realistic assessment
performance algorithms. data sampled true (known) Markov
network using Gibbs sampling.
exact learning experiments previous section structure true
network required, generated randomly fashion described above. sample data
known structure however, one needs specify network parameters.
random network, parameters determine strength dependencies among connected
variables graph. Following Agresti (2002), used log-odds ratio measure
strength probabilistic influence two binary variables X , defined

Pr(X = 0, = 0) Pr(X = 1, = 1)
XY = log
.
Pr(X = 0, = 1) Pr(X = 1, = 0)

469

fiBromberg, Margaritis, & Honavar

Hamming distance sampled data
n = 50, = 1, = 1.5

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
4

6

8

10

12

14

16

18

0.4
0.2
0

20

0

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
2

4

6

8

10

12

14

16

18

0.2
0
10

12

14

16

18

2

0.2
0
10

12

14

16

18

Data set size (thousands data points)

8

10

12

14

16

18

0.2
0
6

8

10

12

14

16

18

20

0.4
0.2
0
4

6

8

10

12

14

16

18

Data set size (thousands data points)

16

18

20

0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Hamming distance sampled data
n = 50, = 8, = 2.0

0.6

2

14

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0

12

0.6

20

1
0.8

10

Hamming distance sampled data
n = 50, = 4, = 2.0

0.4

4

8

Data set size (thousands data points)

0.6

2

6

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

6

4

1

Hamming distance sampled data
n = 50, = 8, = 1.5

0.6

6

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

4

1

Hamming distance sampled data
n = 50, = 8, = 1.0

2

0

Hamming distance sampled data
n = 50, = 2, = 2.0

0

20

1

0

0

Data set size (thousands data points)

0.2

Data set size (thousands data points)

0.8

0.2

20

0.4

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

18

0.4

Hamming distance sampled data
n = 50, = 4, = 1.5

0.6

6

16

0.6

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

14

0.6

Hamming distance sampled data
n = 50, = 4, = 1.0

2

12

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

1

0

10

1

Data set size (thousands data points)

0.8

8

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Hamming distance sampled data
n = 50, = 2, = 1.5
Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 50, = 2, = 1.0

0

6

1

Data set size (thousands data points)

1
0.8

4

Normalized Hamming distance

2

0.6

Normalized Hamming distance

0

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Normalized Hamming distance

0.8

Hamming distance sampled data
n = 50, = 1, = 2.0

1

Normalized Hamming distance

Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 50, = 1, = 1.0
1

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Data set size (thousands data points)

Figure 8: Normalized Hamming distances true network network output
GSMN (with without propagation) GSIMN domain size n = 50
average degrees = 1, 2, 4, 8.

network parameters generated randomly log-odds ratio every
pair variables connected edge graph specified value. set
experiments, used values = 1, = 1.5 = 2 every pair variables
network.
Figures 8 9 show plots normalized Hamming distance true
network output GSMN (with without propagation) GSIMN
domain sizes n = 50 n = 75 variables, respectively. plots show
Hamming distance GSIMN comparable ones GSMN algorithms

470

fiEfficient Markov Network Structure Discovery Using Independence Tests

Hamming distance sampled data
n = 75, = 1, = 1.5

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
4

6

8

10

12

14

16

18

0.4
0.2
0

20

0

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0.6
0.4
0.2
0
2

4

6

8

10

12

14

16

18

0.2
0
10

12

14

16

18

2

0.2
0
10

12

14

16

18

Data set size (thousands data points)

8

10

12

14

16

18

0.2
0
6

8

10

12

14

16

18

20

0.4
0.2
0
4

6

8

10

12

14

16

18

Data set size (thousands data points)

16

18

20

0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Hamming distance sampled data
n = 75, = 8, = 2.0

0.6

2

14

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

0

12

0.6

20

1
0.8

10

Hamming distance sampled data
n = 75, = 4, = 2.0

0.4

4

8

Data set size (thousands data points)

0.6

2

6

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

6

4

1

Hamming distance sampled data
n = 75, = 8, = 1.5

0.6

6

2

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

4

1

Hamming distance sampled data
n = 75, = 8, = 1.0

2

0

Hamming distance sampled data
n = 75, = 2, = 2.0

0

20

1

0

0

Data set size (thousands data points)

0.2

Data set size (thousands data points)

0.8

0.2

20

0.4

0

Normalized Hamming distance

Normalized Hamming distance

0.4

8

18

0.4

Hamming distance sampled data
n = 75, = 4, = 1.5

0.6

6

16

0.6

Data set size (thousands data points)

GSMN* without propagation
GSMN* propagation
GSIMN

4

14

0.6

Hamming distance sampled data
n = 75, = 4, = 1.0

2

12

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

20

1

0

10

1

Data set size (thousands data points)

0.8

8

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Hamming distance sampled data
n = 75, = 2, = 1.5
Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 75, = 2, = 1.0

0

6

1

Data set size (thousands data points)

1
0.8

4

Normalized Hamming distance

2

0.6

Normalized Hamming distance

0

GSMN* without propagation
GSMN* propagation
GSIMN

0.8

Normalized Hamming distance

0.8

Hamming distance sampled data
n = 75, = 1, = 2.0

1

Normalized Hamming distance

Normalized Hamming distance

Normalized Hamming distance

Hamming distance sampled data
n = 75, = 1, = 1.0
1

20

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.6
0.4
0.2
0
0

2

4

6

8

10

12

14

16

18

20

Data set size (thousands data points)

Figure 9: Normalized Hamming distance results Figure 8 domain size n = 75.

domain sizes n = 50 n = 75, average degrees = 1, 2, 4, 8 log-odds ratios = 1,
= 1.5 = 2. reinforces claim inference done GSIMN small
impact quality output networks.
Figure 10 shows weighted number tests GSIMN vs. GSMN (with without
propagation) sampled data set 20,000 points domains n = 50, n = 75,
average degree parameters = 1, 2, 4, 8 log-odds ratios = 1, 1.5 2. GSIMN
shows reduced weighted number tests respect GSMN without propagation
cases compared GSMN propagation cases (with exceptions
( = 4, = 2) ( = 8, = 1.5)). sparse networks weak dependences i.e.,
= 1, reduction larger 50% domain sizes, reduction much larger

471

fiBromberg, Margaritis, & Honavar

Weighted cost sampled data
= 1, = 1.0, 20,000 data points

Weighted cost sampled data
= 1, = 1.5, 20,000 data points

200000
150000
100000
50000
0

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

50

75

Weighted cost sampled data
= 2, = 1.0, 20,000 data points

100000
50000
0

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000

75

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000

75

50

Weighted cost sampled data
= 4, = 1.5, 20,000 data points

Weighted cost sampled data
= 4, = 2.0, 20,000 data points

200000
150000
100000
50000
0

250000

300000
GSMN* without propagation
GSMN* propagation
GSIMN

Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

75

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

50

Number variables

75

50

Number variables

Weighted cost sampled data
= 8, = 1.0, 20,000 data points

Weighted cost sampled data
= 8, = 2.0, 20,000 data points

300000
Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

250000

300000
GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

75

75
Number variables

Weighted cost sampled data
= 8, = 1.5, 20,000 data points

300000

75
Number variables

300000
Weighted number tests

Weighted number tests

250000

Number variables

Weighted cost sampled data
= 4, = 1.0, 20,000 data points

Weighted number tests

Weighted cost sampled data
= 2, = 2.0, 20,000 data points

0
50

300000

75

300000

Number variables

Number variables

50000

Number variables

0

50

100000

50

Weighted number tests

150000

250000

150000

Weighted cost sampled data
= 2, = 1.5, 20,000 data points
Weighted number tests

Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

50

200000

75

300000

200000

250000

GSMN* without propagation
GSMN* propagation
GSIMN

Number variables

300000

50

250000

0
50

Number variables

250000

Weighted number tests

GSMN* without propagation
GSMN* propagation
GSIMN

300000

Weighted number tests

250000

Weighted cost sampled data
= 1, = 2.0, 20,000 data points

300000
Weighted number tests

Weighted number tests

300000

250000

GSMN* without propagation
GSMN* propagation
GSIMN

200000
150000
100000
50000
0

50

75
Number variables

50

75
Number variables

Figure 10: Weighted number tests executed GSMN (with without propagation)
GSIMN |D| = 20, 000, domains sizes n = 50 75, average degree
parameters = 1, 2, 4, 8, log-odds ratios = 1, 1, 5, 2.

one observed exact learning experiments. actual execution times
various data set sizes network densities shown Figure 11 largest domain
n = 75, = 1, verifying reduction cost GSIMN various data set sizes.
Note reduction proportional number data points; reasonable
test executed must go entire data set construct contingency table.
confirms claim cost inference GSIMN small (constant time per
test, see discussion Section 4.6) compared execution time tests themselves,
indicates increasing cost benefits use GSIMN even large data sets.

472

fiEfficient Markov Network Structure Discovery Using Independence Tests

Execution times sampled data sets
n = 75 variables, = 1, = 1

Execution times sampled data sets
n = 75 variables, = 2, = 1

300

300
GSMN* without propagation
GSMN* propagation
GSIMN

GSMN* without propagation
GSMN* propagation
GSIMN

250
Execution time (sec)

Execution time (sec)

250
200
150
100
50

200
150
100
50

0

0
0

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

0

Execution times sampled data sets
n = 75 variables, = 4, = 1

Execution times sampled data sets
n = 75 variables, = 8, = 1

300

300
GSMN* without propagation
GSMN* propagation
GSIMN

GSMN* without propagation
GSMN* propagation
GSIMN

250
Execution time (sec)

250
Execution time (sec)

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

200
150
100
50

200
150
100
50

0

0
0

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

0

2000 4000 6000 8000 10000 12000 14000 16000 18000 20000

Figure 11: Execution times sampled data experiments = 1, = 1, 2 (top row)
= 4, 8 (bottom row) domain n = 75 variables.

5.1.3 Real-World Network Sampled Data Experiments
conducted sampled data experiments well-known real-world networks.
known repository Markov networks drawn real-world domains, instead
utilized well-known Bayesian networks widely used Bayesian network research
available number repositories.1 generate Markov networks
Bayesian network structures used process moralization (Lauritzen, 1996)
consists two steps: (a) connect pair nodes Bayesian network
common child undirected edge (b) remove directions edges.
results Markov network local Markov property valid i.e., node
conditionally independent nodes domain given direct neighbors.
procedure conditional independences may lost. This, however, affect
accuracy results compare independencies output network
moralized Markov network (as opposed Bayesian network).
conducted experiments using 5 real-world domains: Hailfinder, Insurance, Alarm,
Mildew, Water. domain sampled varying number data points
corresponding Bayesian network using logic sampling (Henrion, 1988), used input
GSMN (with without propagation) GSIMN algorithms. compared
network output algorithms original moralized network using
normalized Hamming distance metric previously described. results shown
1. used http://compbio.cs.huji.ac.il/Repository/. Accessed December 5, 2008.

473

fiBromberg, Margaritis, & Honavar

Hamming distance hailfinder data set

Hamming distance insurance data set

Hamming distance alarm data set

GSMN* without propagation
GSMN* propagation
GSIMN

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

2

4

6

8

10 12 14 16

1
GSMN* without propagation
GSMN* propagation
GSIMN

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

18 20 22

0

Data set size (thousands data points)

2

4

6

8

10 12 14 16

Normalized Hamming distance

Normalized Hamming distance

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

18 20 22

0

2

4

6

8

10 12 14 16

18 20 22

Data set size (thousands data points)

Hamming distance Water data set

GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.7

GSMN* without propagation
GSMN* propagation
GSIMN

0.9
0.8

Data set size (thousands data points)

Hamming distance Mildew data set
1
0.9

Normalized Hamming distance

1
Normalized Hamming distance

Normalized Hamming distance

1

0.6
0.5
0.4
0.3
0.2
0.1
0

1
0.9

GSMN* without propagation
GSMN* propagation
GSIMN

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0

2

4

6

8

10 12 14 16

18 20 22

0

Data set size (thousands data points)

2

4

6

8

10 12 14 16

18 20 22

Data set size (thousands data points)

Figure 12: Normalized Hamming distance network output GSMN (with
without propagation) GSIMN true Markov networks network using
varying data set sizes sampled Markov networks various real-world
domains modeled Bayesian networks.

Fig. 12 indicate distances produced three algorithms similar.
cases (e.g., Water Hailfinder) network resulting use GSIMN
actually better (of smaller Hamming distance) ones output GSMN
algorithms.
measured weighted cost three algorithms domains,
shown Fig. 13. plots show significant decrease weighted number tests
GSIMN respect GSMN algorithms: cost GSIMN 66% cost
GSMN without propagation average, savings 34%, cost GSIMN 28%
cost GSMN without propagation average, savings 72%.
5.2 Real-World Data Experiments
artificial data set studies previous section advantage allowing
controlled systematic study performance algorithms, experiments
real-world data necessary realistic assessment performance. Real data
challenging may come non-random topologies (e.g., possibly
irregular lattice many cases spatial data) underlying probability distribution
may faithful.
conducted experiments number data sets obtained UCI machine
learning data set repository (Newman, Hettich, Blake, & Merz, 1998). Continuous variables
data sets discretized using method widely recommended introductory statistics texts (Scott, 1992); dictates optimal number equally-spaced discretization
bins continuous variable k = 1 + log2 N , N number points
474

fiEfficient Markov Network Structure Discovery Using Independence Tests

GSMN* without propagation
GSMN* propagation
GSIMN

70000
60000
50000
40000
30000
20000

GSMN* without propagation
GSMN* propagation
GSIMN

8000

Weighted cost tests

Weighted cost tests

80000

Weighted cost tests insurance data set
9000
7000
5000
4000
3000
2000

14000
12000
10000
8000
6000
4000
2000

1000

0

0
0

5

10

15

20

0
0

5

Data set size (thousands data points)

10

15

0

5

10

15

GSMN* without propagation
GSMN* propagation
GSIMN

25000

8000
6000
4000
2000

20000
15000
10000
5000

0

0
0

5

10

15

20

0

Data set size (thousands data points)

5

10

15

20

Data set size (thousands data points)

Figure 13: Weighted cost tests conducted GSMN (with without propagation)
GSIMN algorithms various real-world domains modeled Bayesian
networks.
Weighted cost accuracy real-world data sets
1
acc(GSIMN) - acc(GSMN* without propagation)
acc(GSIMN) - acc(GSMN* propagation)
wc(GSIMN) / wc(GSMN* without propagation)
wc(GSIMN) / wc(GSMN* propagation)

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-0.1
-0.2

19 1

2 14 6

7 12 8

3 13 15 4
Data set index

5 10 11 18 17 9 16

Figure 14: Ratio weighted number tests GSIMN versus GSMN difference
accuracy GSIMN GSMN real data sets. Ratios smaller
1 positive bars indicate advantage GSIMN GSMN .
numbers x-axis indices data sets shown Table 1.

data set. data set algorithm, report weighted number conditional independence tests conducted discover network accuracy, defined
below.

475

20

Data set size (thousands data points)

Weighted cost tests Water data set

GSMN* without propagation
GSMN* propagation
GSIMN

Weighted cost tests

Weighted cost tests

20

Data set size (thousands data points)

Weighted cost tests Mildew data set
10000

GSMN* without propagation
GSMN* propagation
GSIMN

16000

6000

10000

Weighted cost tests alarm data set

Weighted cost tests

Weighted cost tests hailfinder data set

fiBromberg, Margaritis, & Honavar

Table 1: Weighted number tests accuracy several real-world data sets.
evaluation measure, best performance GSMN (with without
propagation) GSIMN indicated bold. number variables
domain denoted n number data points data set N .

#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

Data set
Name
echocardiogram
ecoli
lenses
hayes-roth
hepatitis
cmc
balance-scale
baloons
flag
tic-tac-toe
bridges
car
monks-1
haberman
nursery
crx
imports-85
dermatology
adult

n

N

14
9
5
6
20
10
5
5
29
10
12
7
7
5
9
16
25
35
10

61
336
24
132
80
1473
625
20
194
958
70
1728
556
306
12960
653
193
358
32561

Weighted number tests
GSMN
GSMN
GSIMN
(w/o prop.) (w/ prop.)
1311
1050
604
425
309
187
60
40
20
102
72
30
1412
980
392
434
292
154
82
47
29
60
40
20
5335
2787
994
435
291
119
520
455
141
194
140
67
135
93
42
98
76
42
411
270
123
1719
999
305
4519
3064
1102
9902
6687
2635
870
652
418

GSMN
(w/o prop.)
0.244
0.353
0.966
0.852
0.873
0.746
0.498
0.932
0.300
0.657
0.814
0.622
0.936
0.308
0.444
0.279
0.329
0.348
0.526

Accuracy
GSMN
(w/ prop.)
0.244
0.394
0.966
0.852
0.912
0.767
0.797
0.932
0.674
0.657
0.635
0.677
0.936
0.308
0.793
0.556
0.460
0.541
0.537

GSIMN
0.244
0.411
0.966
0.852
0.968
0.794
0.698
0.932
0.929
0.704
0.916
0.761
0.936
0.308
0.755
0.892
0.847
0.808
0.551

real-world data structure underlying Bayesian network (if any)
unknown, impossible measure Hamming distance resulting network
structure. Instead, measured estimated accuracy network produced GSMN
GSIMN comparing result (true false) number conditional independence
tests network learned (using vertex separation) result tests
performed data set (using 2 test). approach similar estimating accuracy
classification task unseen instances inputs triplets (X, Y, Z)
class attribute value corresponding conditional independence test.
used 1/3 real-world data set (randomly sampled) input GSMN GSIMN
entire data set 2 test. corresponds hypothetical scenario
much smaller data set available researcher, approximates true value
test outcome entire data set. Since number possible tests
exponential, estimated independence accuracy sampling 10,000 triplets (X, Y, Z)
randomly, evenly distributed among possible conditioning set sizes {0, . . . , n 2}
(i.e., 10000/(n 1) tests m). triplets constructed follows:
First, two variables X drawn randomly V. Second, conditioning set
determined picking first variables random permutation V {X, }.
Denoting set 10,000 triplets, triplet, Idata (t) result test
performed entire data set Inetwork (t) result test performed

476

fiEfficient Markov Network Structure Discovery Using Independence Tests

network output either GSMN GSIMN, estimated accuracy defined as:

ofifi
1 fifin
| Inetwork (t) = Idata (t) fifi.
accuracy
\ =
|T |

data sets, Table 1 shows detailed results accuracy weighted
number tests GSMN GSIMN algorithms. results plotted
Figure 14, horizontal axis indicating data set index appearing first column
Table 1. Figure 14 plots two quantities graph real-world data sets:
ratio weighted number tests GSIMN versus two GSMN algorithms
difference accuracies. data set, improvement GSIMN
GSMN corresponds number smaller 1 ratios positive histogram bar
accuracy differences. observe GSIMN reduced weighted number
tests every data set, maximum savings 82% GSMN without propagation
(for crx data set) 60% GSMN propagation (for crx data set
well). Moreover, 11 19 data sets GSIMN resulted improved accuracy, 6 tie
2 somewhat reduced accuracy compared GSMN propagation (for
nursery balance-scale data sets).

6. Conclusions Future Research
paper presented two algorithms, GSMN GSIMN, learning efficiently
structure Markov network domain data using independence-based
approach (as opposed NP-hard algorithms based maximum likelihood estimation)
evaluated performance measurement weighted number tests
require learn structure network quality networks learned
artificial real-world data sets. GSIMN showed decrease vast majority
artificial real-world domains output network quality comparable
GSMN , cases showing improvement. addition, GSIMN shown
nearly optimal number tests executed compared GSIMN-FCH, uses
exhaustive search produce independence information inferred Pearls
axioms. directions future research include investigation way topology
underlying Markov network affects number tests required quality
resulting network, especially commonly occurring topologies grids. Another
research topic impact number tests examination grow orderings
variables.

Acknowledgments
thank Adrian Silvescu insightful comments accuracy measures general advice
theory undirected graphical models.

Appendix A. Correctness GSMN
variable X V examined main loop GSMN algorithm (lines
1039), set BX variable X V constructed growing shrinking set S,
477

fiBromberg, Margaritis, & Honavar

starting empty set. X connected member BX produce
structure Markov network. prove procedure returns actual Markov
network structure domain.
proof correctness make following assumptions.
axioms Eqs. (1) hold.
probability distribution domain strictly positive (required Intersection
axiom hold).
Tests conducted querying oracle, returns true value underlying model.
algorithm examines every variable X inclusion (and thus BX )
grow phase (lines 18 33) and, added grow phase,
considers removal shrinking phase (lines 34 37). Note
one test executed X growing phase X; call grow
test X (line 23). Similarly, one tests executed X
shrinking phase; test (if executed) called shrink test X (line
36).
general idea behind proof show that, learning blanket X,
variable end shrinking phase dependence (X 6Y |
V {X, }) X holds (which, according Theorem 2 end
Appendix, implies edge X ). immediately prove one
direction.
Lemma 1.
/ end shrink phase, (XY | V {X, }).
Proof. Let us assume
/ end shrink phase. Then, either
added set grow phase (i.e., line 24 never reached), removed
shrink phase (i.e., line 37 reached). former true
(pXY > ) line 22 (indicating X unconditionally independent) found
independent X line 23. latter true found independent X
line 36. cases V {X, } (XY | A), Strong Union
(XY | V {X, }).
opposite direction proved Lemma 6 below. However, proof involved,
requiring auxiliary lemmas, observations, definitions. two main auxiliary
Lemmas 4 5. use lemma presented next (Lemma 2) inductively extend
conditioning set dependencies found grow shrink tests X ,
remaining variables V{X, }. Lemma shows that, certain independence
holds, conditioning set dependence increased one variable.
Lemma 2. Let X, V, Z V {X, }, Z Z. W V,
(X 6Y | Z) (XW | Z {Y }) = (X 6Y | Z {W }).

478

fiEfficient Markov Network Structure Discovery Using Independence Tests

Proof. prove contradiction, make use axioms Intersection (I), Strong
Union (SU), Decomposition (D). Let us assume (X 6Y | Z) (XW | Z {Y })
(XY | Z {W }).
(XY | Z {W }) (XW | Z {Y })
SU

=

(XY | Z {W }) (XW | Z {Y })



(X{Y, W } | Z)



=

(XY | Z) (XW | Z)

=

(XY | Z).

=

contradicts assumption (X 6Y | Z).
introduce notation definitions prove auxiliary lemmas.
denote SG value end grow phase (line 34) i.e., set
variables found dependent X grow phase, SS value end
shrink phase (line 39). denote G set variables found independent
X grow phase U = [U0 , . . . , Uk ] sequence variables shrunk
BX , i.e., found independent X shrink phase. sequence U assumed
ordered follows: < j variable Ui found independent X Uj
shrinking phase. prefix first variables [U0 , . . . , Ui1 ] U denoted
Ui . test performed algorithm, define k(t) integer
Uk(t) prefix U containing variables found independent X
loop t. Furthermore, abbreviate Uk(t) Ut .
definition U fact grow phase conditioning set
increases dependent variables only, immediately make following observation:
Observation 1. variable Ui U, denotes shrink test performed
X Ui Ut = Ui1 .
relate conditioning set shrink test Ut follows:
Lemma 3. SS = (X, | Z) shrink test , Z = SG Ut {Y }.
Proof. According line 36 algorithm, Z = {Y }. beginning shrink
phase (line 34) = SG , variables found independent afterward conducted
removed line 37. Thus, time performed, = SG Ut
conditioning set becomes SG Ut {Y }.
Corollary 1. (XUi | SG Ui ).
Proof. proof follows immediately Lemma 3, Observation 1, fact
Ui = Ui1 {Ui }.
following two lemmas use Lemma 2 inductively extend conditioning set
dependence X variable SS . first lemma starts shrink
test X (a dependence), extends conditioning set SS {Y } (or
equivalently SG {Y } Ut according Lemma 3) SG {Y }.
479

fiBromberg, Margaritis, & Honavar

Lemma 4. SS shrink test , (X 6Y | SG {Y }).
Proof. proof proceeds proving
(X 6Y | SG {Y } Ui )
induction decreasing values i, {0, 1, . . . , k(t)}, starting = k(t).
lemma follows = 0 noticing U0 = .
Base case (i = k(t)): Lemma 3, = (X, | SG {Y } Ut ), equals
(X, | SG {Y } Uk(t) ) definition Ut . Since SS , must case
found dependent, i.e., (X 6Y | SG {Y } Uk(t) ).
Inductive step: Let us assume statement true = m, 0 < k(t)1:
(X 6Y | SG {Y } Um ).

(2)

need prove true = 1:
(X 6Y | SG {Y } Um1 ).
Corollary 1,
(XUm | SG Um )
Strong Union,
(XUm | (SG Um ) {Y })

(XUm | (SG Um {Y }) {Y }).

(3)

Eqs. (2), (3) Lemma 2 get desired relation:
(X 6Y | (SG {Y } Um ) {Um }) = (X 6Y | SG {Y } Um1 ).

Observation 2. definition SG , every test = (X, | Z) performed
grow phase, Z SG .
following lemma completes extension conditioning set dependence
X SS universe variables V {X, }, starting SG {Y }
(where Lemma 4 left off) extending SG G {Y }.
Lemma 5. SS , (X 6Y | SG G {Y }).
Proof. proof proceeds proving
(X 6Y | SG Gi {Y })
induction increasing values 0 |G|, Gi denotes first elements
arbitrary ordering set G.
480

fiEfficient Markov Network Structure Discovery Using Independence Tests

Base Case (i = 0): Follows directly Lemma 4 = 0, since G0 = .
Inductive Step: Let us assume statement true = m, 0 < |G|:
(X 6Y | SG Gm {Y }).

(4)

need prove true = + 1:
(X 6Y | SG Gm+1 {Y }).

(5)

Observation 2 grow test Gm results independence:
(XGm | Z), Z SG .
Strong Union axiom become:
(XGm | Z {Y }), Z SG

(6)

(XGm | (Z {Y }) {Y }), Z SG .

(7)

equivalently
Since Z SG SG Gm , Z {Y } SG Gm , Eq. (4)
Lemma 2 get desired relation:
(X 6Y | (SG Gm {Y }) Gm ) = (X 6Y | SG Gm+1 {Y }).

Finally, prove X dependent every variable SS given universe
V {X, }.
Lemma 6. SS , (X 6Y | V {X, }).
Proof. Lemma 5,
(X 6Y | SG G {Y })
suffices prove SG G {Y } = V {X, }. loop 69 GSMN ,
queue X populated elements V {X}, then, line 21, removed
X . grow phase partitions X variables dependent X (set SG )
independent X (set G).
Corollary 2. SS (X 6Y | V {X, }).
Proof. Follows directly Lemmas 1 6.
Corollary immediately show graph returned
connecting X member BX = SS exactly Markov network domain
using following theorem, first published Pearl Paz (1985).
Theorem 2. (Pearl & Paz, 1985) Every dependence model satisfying symmetry, decomposition, intersection (Eqs. (1)) unique Markov network G = (V, E) produced
deleting complete graph every edge (X, ) (XY | V {X, }) holds
, i.e.,
(X, )
/ E (XY | V {X, }) .
481

fiBromberg, Margaritis, & Honavar

Appendix B. Correctness GSIMN
GSIMN algorithm differs GSMN use test subroutine IGSIMN
instead IGSMN (Algorithms 5 4, respectively), turn differs number
additional inferences conducted obtain independencies (lines 8 22).
inferences direct applications Strong Union axiom (which holds assumption)
Triangle theorem (which proven hold Theorem 1). Using correctness
GSMN (proven Appendix A) therefore conclude GSIMN algorithm
correct.

References
Abbeel, P., Koller, D., & Ng, A. Y. (2006). Learning factor graphs polynomial time
sample complexity. Journal Machine Learning Research, 7, 17431788.
Acid, S., & de Campos, L. M. (2003). Searching Bayesian network structures
space restricted acyclic partially directed graphs. Journal Artificial Intelligence
Research, 18, 445490.
Agresti, A. (2002). Categorical Data Analysis (2nd edition). Wiley.
Aliferis, C. F., Tsamardinos, I., & Statnikov, A. (2003). HITON, novel Markov blanket
algorithm optimal variable selection. Proceedings American Medical
Informatics Association (AMIA) Fall Symposium.
Anguelov, D., Taskar, B., Chatalbashev, V., Koller, D., Gupta, D., Heitz, G., & Ng, A.
(2005). Discriminative learning Markov random fields segmentation 3D range
data. Proceedings Conference Computer Vision Pattern Recognition
(CVPR).
Barahona, F. (1982). computational complexity Ising spin glass models. Journal
Physics A: Mathematical General, 15 (10), 32413253.
Besag, J. (1974). Spacial interaction statistical analysis lattice systems. Journal
Royal Statistical Society, Series B, 36, 192236.
Besag, J., York, J., & Mollie, A. (1991). Bayesian image restoration two applications
spatial statistics.. Annals Institute Statistical Mathematics, 43, 159.
Bromberg, F., Margaritis, D., & Honavar, V. (2006). Efficient Markov network structure discovery independence tests. Proceedings SIAM International Conference
Data Mining.
Buntine, W. L. (1994). Operations learning graphical models. Journal Artificial
Intelligence Research, 2, 159225.
Castelo, R., & Roverato, A. (2006). robust procedure Gaussian graphical model search
microarray data p larger n. Journal Machine Learning Research,
7, 26212650.
Chow, C., & Liu, C. (1968). Approximating discrete probability distributions dependence trees. IEEE Transactions Information Theory, 14 (3), 462 467.

482

fiEfficient Markov Network Structure Discovery Using Independence Tests

Cochran, W. G. (1954). methods strengthening common 2 tests. Biometrics,
10, 417451.
Della Pietra, S., Della Pietra, V., & Lafferty, J. (1997). Inducing features random fields.
IEEE Transactions Pattern Analysis Machine Intelligence, 19 (4), 390393.
Dobra, A., Hans, C., Jones, B., Nevins, J. R., Yao, G., & West, M. (2004). Sparse graphical
models exploring gene expression data. Journal Multivariate Analysis, 90, 196
212.
Edwards, D. (2000). Introduction Graphical Modelling (2nd edition). Springer, New
York.
Friedman, N., Linial, M., Nachman, I., & Peer, D. (2000). Using Bayesian networks
analyze expression data. Computational Biology, 7, 601620.
Geman, S., & Geman, D. (1984). Stochastic relaxation, gibbs distributions, bayesian
relation images.. IEEE Transactions Pattern Analysis Machine Intelligence,
6, 721741.
Heckerman, D. (1995). tutorial learning bayesian networks. Tech. rep. MSR-TR-95-06,
Microsoft Research.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:
combination knowledge statistical data. Machine Learning, 20, 197243.
Henrion, M. (1988). Propagation uncertainty probabilistic logic sampling Bayes
networks. Lemmer, J. F., & Kanal, L. N. (Eds.), Uncertainty Artificial Intelligence 2. Elsevier Science Publishers B.V. (North Holland).
Hofmann, R., & Tresp, V. (1998). Nonlinear Markov networks continuous variables.
Neural Information Processing Systems, Vol. 10, pp. 521529.
Isard, M. (2003). Pampas: Real-valued graphical models computer vision. IEEE
Conference Computer Vision Pattern Recognition, Vol. 1, pp. 613620.
Jerrum, M., & Sinclair, A. (1993). Polynomial-time approximation algorithms Ising
model. SIAM Journal Computing, 22, 10871116.
Kearns, M. J., & Vazirani, U. V. (1994). Introduction Computational Learning Theory.
MIT Press, Cambridge, MA.
Koller, D., & Sahami, M. (1996). Toward optimal feature selection. International Conference Machine Learning, pp. 284292.
Lam, W., & Bacchus, F. (1994). Learning Bayesian belief networks: approach based
MDL principle. Computational Intelligence, 10, 269293.
Lauritzen, S. L. (1996). Graphical Models. Oxford University Press.
Margaritis, D., & Thrun, S. (2000). Bayesian network induction via local neighborhoods.
Solla, S., Leen, T., & Muller, K.-R. (Eds.), Advances Neural Information Processing
Systems 12, pp. 505511. MIT Press.
McCallum, A. (2003). Efficiently inducing features conditional random fields. Proceedings Uncertainty Artificial Intelligence (UAI).

483

fiBromberg, Margaritis, & Honavar

Newman, D. J., Hettich, S., Blake, C. L., & Merz, C. J. (1998). UCI repository machine
learning databases. Tech. rep., University California, Irvine, Dept. Information
Computer Sciences.
Pena, J. M. (2008). Learning Gaussian graphical models gene networks false discovery rate control. Proceedings 6th European Conference Evolutionary
Computation, Machine Learning Data Mining Bioinformatics, pp. 165176.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann Publishers, Inc.
Pearl, J., & Paz, A. (1985). Graphoids: graph-based logic reasoning releveance
relations. Tech. rep. 850038 (R-53-L), Cognitive Systems Laboratory, University
California.
Rebane, G., & Pearl, J. (1989). recovery causal poly-trees statistical data.
Kanal, L. N., Levitt, T. S., & Lemmer, J. F. (Eds.), Uncertainty Artificial
Intelligence 3, pp. 175182, Amsterdam. North-Holland.
Schafer, J., & Strimmer, K. (2005). empirical bayes approach inferring large-scale
gene association networks. Bioinformatics, 21, 754764.
Scott, D. W. (1992). Multivariate Density Estimation. Wiley series probability
mathematical statistics. John Wiley & Sons.
Shekhar, S., Zhang, P., Huang, Y., & Vatsavai, R. R. (2004) Kargupta, H., Joshi, A.,
Sivakumar, K., & Yesha, Y. (Eds.), Trends Spatial Data Mining, chap. 19, pp.
357379. AAAI Press / MIT Press.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction, Search (2nd
edition). Adaptive Computation Machine Learning Series. MIT Press.
Srebro, N., & Karger, D. (2001). Learning Markov networks: Maximum bounded tree-width
graphs. ACM-SIAM Symposium Discrete Algorithms.
Tsamardinos, I., Aliferis, C. F., & Statnikov, A. (2003a). Algorithms large scale Markov
blanket discovery. Proceedings 16th International FLAIRS Conference, pp.
376381.
Tsamardinos, I., Aliferis, C. F., & Statnikov, A. (2003b). Time sample efficient discovery Markov blankets direct causal relations. Proceedings 9th ACM
SIGKDD International Conference Knowledge Discovery Data Mining, pp.
673678.
Tsamardinos, I., Brown, L. E., & Aliferis, C. F. (2006). max-min hill-climbing Bayesian
network structure learning algorithm. Machine Learning, 65, 3178.
Whittaker, J. (1990). Graphical Models Applied Multivariate Statistics. John Wiley &
Sons, New York.

484


