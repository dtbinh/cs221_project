journal artificial intelligence

submitted published

automatic induction bellman error features
probabilistic
jia hong wu
robert givan

jw alumni purdue edu
givan purdue edu

electrical computer engineering
purdue university w lafayette usa

abstract
domain specific features important representing structure throughout machine
learning decision theoretic state features provided domainindependent approximate value iteration learn weighted combinations
features often perform well heuristic estimates state value e g distance
goal successful applications real world domains often require features crafted human experts propose automatic processes learning useful domain specific feature sets
little human intervention methods select add features describe state space regions high inconsistency bellman equation statewise bellman error approximate
value iteration method applied real valued feature hypothesis space
corresponding learning method selecting features training sets state value pairs
evaluate method hypothesis spaces defined relational propositional feature
languages nine probabilistic domains approximate value iteration
relational feature space performs state art domain independent stochastic
relational method provides first domain independent plays tetris
successfully without human engineered features

introduction
substantial gap performance domain independent planners domainspecific planners domain specific human input able produce effective planners
competition domains well many game applications backgammon chess
tetris deterministic work tlplan bacchus kabanza shown
simple depth first search domain specific human input form temporal logic formulas
describing acceptable paths yields effective planner wide variety competition domains
stochastic feature value function representations used humanselected features great success applications backgammon sutton barto
tesauro tetris bertsekas tsitsiklis usage features provided human experts often critical success systems value function approximations
consider automating transition domain independent
domain specific performance replacing human input automatically learned domain properties thus study style planner learns encountering instances improve
performance subsequently encountered instances domain
focus stochastic machine learned value functions represented linear
combinations state space features goal augment state space representation
c

ai access foundation rights reserved

fiw u g ivan

machine discovered features facilitate accurate representation
value function resulting learned features used representing value function
instances domain allowing amortization learning costs across
solution multiple instances note property contrast competition
planners especially deterministic retain useful information instances thus solving regarded automatically
constructing domain specific planners domain independent techniques
learn features correlate well statewise bellman error value functions encountered provided feature language corresponding learner select
features space evaluate relational propositional feature
spaces recent approaches acquiring features stochastic substantial differences discuss detail section patrascu poupart
schuurmans boutilier guestrin gretton thiebaux sanner boutilier
keller mannor precup parr painter wakefield li littman previous work
evaluated selection relational features correlation statewise bellman error
recent theoretical parr et al uncontrolled markov processes exactly capturing statewise bellman error features repeatedly lead convergence
uncontrolled optimal value value function selected linear fixed point methods weight
training unfortunately machine learning approaches selecting features
transferred approximations statewise bellman error features case
work parr et al weaker imply convergence none theory transferred controlled case interest analysis much
difficult effective greedy policy consideration value function training
changing
consider controlled case known theoretical properties similar parr
et al shown lacking theory purpose demonstrate capability
statewise bellman error features empirically rich representations require machine
learning techniques lack approximation guarantees next give overview introducing markov decision processes value functions bellman error feature hypothesis
languages feature learning methods
use markov decision processes mdps model stochastic mdp
formal model single agent facing sequence action choices pre defined action space
transitioning within pre defined state space assume underlying stationary
stochastic transition model available action state transitions occur according
agents action choices agent receives reward action choice according state
visited possibly action chosen objective accumulating much reward
possible possibly favoring reward received sooner discounting averaging time
requiring reward received finite horizon
mdp solutions represented state value functions assigning real numbers states informally mdp solution techniques desire value function respects action transitions
good states large immediate rewards actions available lead
good states well known property formalized bellman equations recursively
characterize optimal value function see section degree given value function
fails respect action transitions way formalized next section referred
bellman error value function computed state


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

intuitively statewise bellman error high magnitude regions state space
appear undervalued overvalued relative action choices available state high
bellman error locally inconsistent value function example state inconsistently labeled
low value action available leads high value states
use machine learning fit features regions local inconsistency current value
function fit perfect features guarantee represent bellman update
current value function repeated bellman updates called value iteration known
converge optimal value function add learned features representation
train improved value function adding features available feature set
method learning features approximate value function
regarded boosting style learning linear combination features
viewed weighted combination ensemble simple hypotheses feature learned
viewed simple hypothesis selected match training distribution focused regions
previous ensemble getting wrong reflected high statewise bellman error throughout
region growth ensemble sequentially adding simple hypotheses selected correct
error ensemble far refer boosting style learning
important note method scores candidate features correlation statewise
bellman error current value function minimizing statewise bellman error
value function found candidate feature pre feature addition scoring much
less expensive scoring involves retraining weights feature especially
repeated many times different candidates relative current value function
use pre feature addition scoring select features controlled setting enables much
aggressive search features previously evaluated post feature addition
discussed work patrascu et al
considered selecting features feature description language
learning method exists effectively select features match state value training data
consider two different feature languages empirical evaluation human constructed
features typically compactly described relational language english wherein
feature value determined relations objects domain likewise consider
relational feature language domain predicates basic domain description
domain description may written example standard language ppddl
younes littman weissman asmuth take logical formulas one free variable
represent features count number true instantiations formula state
evaluated example number holes feature used many tetris experiments
bertsekas tsitsiklis driessens ramon gartner interpreted counting
number empty squares board filled squares
numeric features provide mapping states natural numbers
addition relational feature language consider propositional feature representation learning structure although propositional representation less expressive
relational one exist effective shelf learning packages utilize propositional representations indeed reformulate feature learning task
related classification use standard classification tool decision tree learner c
quinlan create binary valued features reformulation classification considers
sign magnitude statewise bellman error attempting learn features
characterize positive sign regions state space likewise negative sign regions


fiw u g ivan

standard supervised classification thus formulated c applied generate
decision tree feature use feature value function representation
propositional easier implement may attractive relational one
obvious advantage relational representation computing exact
statewise bellman error state significantly expensive estimating sign
experiments however relational produces superior
propositional learner relational demonstrates ability generalize features
sizes domain asset unavailable propositional representations
present experiments nine domains experiment starts single constant feature mapping states number forcing constant value function makes
distinctions states learn domain specific features weights automatically
generated sampled state trajectories adjusting weights feature added
evaluate performance policies select actions greedily relative learned value
functions evaluate learners stochastic computer game tetris seven domains two international probabilistic competitions younes et al
bonet givan method provides first domain independent playing
tetris successfully without human engineered features relational learner demonstrates
superior success ratio probabilistic competition domains compared
propositional probabilistic planners replan yoon fern givan
foalp sanner boutilier additionally propositional learner
outperforms work patrascu et al sysadmin domain evaluated

background
present relevant background use markov decision processes
markov decision processes
define terminology markov decision processes thorough discussion
markov decision processes see books bertsekas tsitsiklis sutton barto
markov decision process mdp tuple r finite state
space containing initial state selects non empty finite available action set
state reward function r assigns real reward state action state triple
action enabled state e transition probability function maps
state action pairs probability distributions p
given discount factor policy mapping state action
value function v gives expected discounted reward obtained state selecting action
state encountered discounting future rewards factor per time step

least one optimal policy v abbreviated v less v
every state policy following q function evaluates action respect
future value function v
x
q v
r v


recursive bellman equations use q describe v v follows first v
q v v maxaa q v q select ac

fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

tion greedily relative value function policy greedy v selects state action
arg maxaa q v
value iteration iterates operation
u v max

aa

x

r v



computing bellman update u v v producing sequence value functions converging
sup norm v regardless initial v used
define statewise bellman error b v value function v state
u v v inducing features correlation statewise
bellman error sign statewise bellman error sup norm distance
value function v optimal value function v bounded bellman error magnitude defined maxss b v e g see williams baird use term
statewise bellman error emphasize distinction widely used sup norm bellman
error
note computing u v thus statewise bellman error involve summation
entire state space whereas fundamental motivations require avoiding summations
many mdp interest transition matrix sparse way set states
reachable one step non zero probability small current state
statewise bellman error computed effectively appropriate representation
generally sparse manner sum effectively approximately evaluated
sampling next states according distribution represented
modeling goal oriented
stochastic goal oriented objective solving
guide agent toward designated state region e goal region model
structuring reward transition functions r action goal state leads
positive reward zero reward absorbing state reward zero everywhere else
retain discounting represent preference shorter paths goal alternatively
modeled stochastic shortest path mdps without discounting bertsekas
techniques easily generalized formalisms allow varying action costs well
model variation work
formally define goal oriented mdp mdp meeting following constraints use variables states actions require
contain zero reward absorbing state e r
transition function must assign one zero triples call
region states one goal region reward function constrained
r zero unless constructing goal oriented mdps
representations may introduce dummy actions carry transitions involving described

compactly represented mdps
work consider propositional relational state representations


fiw u g ivan

relational mdps spaces relationally represented e
finite set objects state predicates p action names n used define spaces
follows state fact application p n argument state predicate p object
arguments oi n state set state facts representing exactly true facts
state action instance os
n application n argument action name n objects
oi n action space ss set action instances
mdps compactly represented state action spaces use compact representations
transition reward functions one compact representation ppddl
language informally discussed next subsection formally presented work younes
et al
propositional action space explicitly specified state space compactly specified providing finite sequence basic state properties called state attributes
boolean integer real values propositional state vector values state
attributes
given relational mdp equivalent propositional mdp easily constructed grounding explicit action space constructed forming action name applications
set state attributes computed forming state predicate applications thus removing use
set objects representation
representing ppddl mdps
discuss represent goal oriented stochastic defined standardized
languages ppddl younes et al goal oriented mdps limit
focus goal regions described conjunctive sets state facts
reference follow used work fern yoon givan regarding
converting compactly represented mdps manner facilitates generalization instances first discuss several difficult representational issues
finally pull discussion together formal definition mdp analyze represent
given ppddl instance consider quantified disjunctive goals
handling goals would interesting useful extension work
p lanning omains



p roblems

domain distribution instances sharing state predicates pw
action names n action definitions actions take objects parameters defined
giving discrete finite probability distributions action outcomes specified
add delete lists state facts action parameters
given domain definition instance domain specifies finite object set
initial state si goal condition g initial state given set state facts goal
condition given conjunction state facts constructed predicates pw
state predicate associated arity indicating number objects relates state predicate
applied number objects domain form ground state fact true false
state states different possible ways select true state facts likewise action name
associated arity natural number indicating number objects action act upon action name
applied number objects form grounded action



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

ppddl r epresentation
ppddl standard language international probabilistic competitions
ppddl domain syntax syntax defined completely
define instance one specify domain definition definition
respective syntax conditional effects quantified preconditions allowed domain
definition
competitions customary specify domains providing generators accept size parameters input output ppddl instances
generators thus specify size parameterized domains important note however generators provided recent competitions specify
domains according definition used particular generators vary
action set state predicates instances generated relationship
different instances generated generators much looser required
definition domains somewhat arbitrary collections

logical language allows generalization
share state action language limit empirical evaluation section domains
provided generators specify domains defined
e without varying action definitions instances easily code
generator refer domains generators domains fixed action
definitions
g eneralization b etween p roblems varying ize
object set varies size without bound across instances domain
infinitely many possible states within different instances single domain mdp
analyze finite state space model domain infinite set mdps
seeking good policy form good value function one mdp
instance
value function infinite set mdps mapping disjoint union state
spaces mdps real numbers value function used greedily policy
mdps set however explicit representation value function would
infinite size use knowledge representation techniques compactly represent
value functions infinite set instance mdps given domain
compact representation derives generalization across domains fundamentally finding good generalizations mdps within single domain
representation value functions domains given sections
section discuss represent single finite mdp single
instance however note objective work good value functions
infinite collections mdps represent domains throughout
assume domain provided along means sampling example
domain sampling parameterized difficulty generally size
consider two candidate representations features one relational representation
capable generalizing sizes propositional representation restrict training
testing instances size



fiw u g ivan

easy example selected although ppddl provide
distributions benchmark domains often provided generators defining
distributions generators available use otherwise code
distributions instances
g eneralizing b etween p roblems



varying g oals

facilitate generalization instances different goals following work
martin geffner fern et al translate ppddl instance description
mdp state specifies true state goal action
transitions mdp never change goal presence goal within state
description allows value functions defined conditioning state depend
goal well goal region mdp simply mdp states specified
current state information matches specified goal information
formally translating ppddl instances compact mdps enrich given set
world state predicates pw adding copy predicate indicating desired state
predicate name goal description copy predicate p prepending word goal
name set goal description copies predicates pw denoted pg take
pw pg state predicates mdp corresponding instance intuitively
presence goal p b state indicates goal condition requires fact p b
part world state use goal predicates constructing compact mdp
ppddl description constructing initial state goal conditions true
goal predicates
use domain blocksworld example illustrate reformulation
domain used example fern et al goal condition blocksworld
described conjunction ground top facts world state predicate
top pw discussed implies predicate goal top pg
intuitively one ground instance predicate goal top b b means state
goal region block b directly top block b
tates



available actions

ppddl allows definition domains states meet preconditions
action applied however mdp formalism requires least one available action every
state translating ppddl instance mdp define action transitions
action taken dead state transitions deterministically absorbing state
consider states undesirable plan trajectories give added transitions
reward negative one unless source state goal state
r esulting mdp
pull together elements formally describe mdp r
given ppddl instance discussed section set defined
specifying predicates objects available ppddl description specifies sets n
action names objects well set pw world predicates construct enriched
set p pw pg state predicates define state space sets applications
predicates objects set state set ppddl action instances built


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

n satisfies preconditions except set empty set
ppddl action instances built n latter case say state dead
reward function r defined discussed previously section e r
goal condition g true r non goal dead state zero otherwise
define according semantics ppddl augmented semantics
section one satisfies g dead zero otherwise
transiting one state another never changes goal condition description states given
predicates pg mdp initial state ppddl initial state si augmented
goal condition g goal predicates pg propositional representation
desired easily constructed directly relational representation grounding
linear approximation value functions
many previous authors done patrascu et al sanner boutilier bertsekas
tsitsiklis tesauro tsitsiklis roy address large compactly represented implicitly representing value functions terms state space features
f r features f must select real value state describe two approaches
representing selecting features section
recall section goal learn value function family related mdp
assume state space features defined across union state spaces
family
represent
value functions linear combination l features extracted e
p
v li wi f goal features mapping states real
values weights wi v closely approximates v note single set features
weight vector defines value function mdps features defined
methods proposed select weights wi linear approximations see e g
sutton widrow hoff review use trajectory approximate
value iteration avi training methods easily substituted avi constructs



finite sequence value functions
one value function
pl v v v returns last

represented v wi determine weights wi v draw set
training states sn following policy greedy v different example
sampled provided distribution current level difficulty see
section discussion control difficulty number trajectories drawn
maximum length trajectory parameters avi method training state
compute bellman update u v mdp model instance
compute wi training states
wi wi

x
sj u v sj v sj
ni



j

learning rate ni number states sn
non zero weight updates weight update formula descend gradient l distance
v u v training states features first rescaled normalize
note according definitions section dead states technically goal states
negative rewards



fiw u g ivan

effective learning rate correct feature values rare occurrence training set pseudocode avi method drawing training sets following policy available online
appendix available jair website page
use greedy policy draw training examples order focus improvement
relevant states state distributions generated biased current
policy particular another option worth considering especially feature learning stuck would
long random walk distribution discussed work fern yoon givan
leave detailed exploration issue future work substantial discussion
issues arise selecting training distribution please see book sutton barto
worth noting policy training shown converge optimal value function
closely related reinforcement learning setting sarsa singh jaakkola
littman szepesvari
general avi often gives excellent practical greedy gradient descent
method environment convex due maximization operation bellman error
function guarantee quality weight vector found even case
convergence convergence guaranteed experiments divergent weight
training fact required handling note feature discovery methods
used weight selection approximate linear programming
properties avi undesirable application
implemented small modifications basic weight update rule order use avi
effectively setting described section online appendix available jair
website

feature discovering value function construction
state features provided domain independent avi learn
weighted combinations features often perform well heuristic estimates state value
e g distance goal describe methods select add features describe
state space regions high inconsistency bellman equation statewise bellman error
approximate value iteration methods applied real valued feature hypothesis
space corresponding learning method selecting features match real valued function
training set states use learner select features match statewise
bellman error function
noted use boosting style learning finding value functions iterating
selecting weights generating features focusing bellman error
current value function value function representation viewed weighted ensemble
single feature hypotheses start value function trivial feature constant
feature returning value one initial weight zero iteratively retrain
weights select features matching regions states current weighted ensemble
high statewise bellman error
take learning small learn features first
relatively lower difficulty increase difficulty time discussed lower
difficulty typically smaller state spaces shorter paths positive
deriving gradient descent weight update formula feature scaled ri



q

n

ni

giving ri

fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

r
initial feature vector
r
initial weight vector w
initial difficulty

difficulty
target level
time

yes

r
final r
w



increase
difficulty
r keep


learn feature
correlating bellman
error states
training
r set add
keep current
difficulty

r
w

r

select w approximately
minimizing
error
r bellman
r
v w

done

reweighted value
r r
function v w

yes

performance
current difficulty
meets threshold



generate feature
training set

figure control flow feature learning boxes double borders represent assumed subroutines method assume distribution parameterized
difficulty size

feedback e g goal states learning initially difficult typically lead
inability positive feedback random walk behavior learning first lower
difficulty found effective martin geffner yoon fern givan
experimentally section good value functions high difficulty
indeed learned fashion lower increasing difficulties
relies two assumed subroutines instantiated different ways
providing different subroutines first method weight selection assumed
method takes input domain fixed set features selects weight vector
value function domain provided features intend method
heuristically approximately minimize l bellman error choice weight vector
practice may easier adjust weights approximate l bellman error second feature
hypothesis space corresponding learner assumed provided system designer
control flow shown figure iteration fixed
distribution selects weights current feature set method attempting minimize
l bellman error define value function v selects training set states feature
learning learns feature correlating well statewise bellman error v adding
feature feature set user provided performance threshold function detects
increase difficulty formalization control flow given figure form
pseudo code


fiw u g ivan

feature discovering value function construction



inputs
initial feature vector initial weight vector
w
sequence distributions dmax increasing difficulty
performance threshold function
v tests performance value function v distribution



outputs
feature vector weight vector
w






w
w



max time




select
w approximately minimizing bellman error v
w dd



dd
w










else
generate sequence training states dd









learn feature f correlating bellman error feature b
w
states






f
w
w


return
w

notes
b statewise bellman error function defined section
code approximate value iteration avi shown online appendix available jair website
page example implementation line



code draw greedy
w n
shown online appendix page example impletraining

mentation line ntraining number states feature training set duplicated states removed
specified section



beam search code learning relational features beam search learn score b
w
example implementation line beam search learn shown figure section score
defined section

figure pseudo code learning set features
experiments reported section evaluate following choices assumed
subroutines experiments use avi select weights feature sets evaluate two
choices feature hypothesis space corresponding learner one relational one propositional described section
separate training sets drawn weight selection feature learning former
depend weight selection method described avi section latter
described section
difficulty increased sampled performance greedy policy current
difficulty exceeds user specified performance thresholds domain experiments


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

performance parameters measured success ratio percentage trials goal average successful plan length average number steps goal among successful trials
non goal oriented domains tetris sysadmin use different performance measures average total reward tetris bellman error sysadmin facilitate comparison patrascu
et al
assume user provided schedule difficulty increases
difficulty parameterized one parameter e g size may measured number
objects type domain independent automation increase difficulty
topic future give difficulty increase schedules performance thresholds
experiments section presenting experiments section
training set generation
training set selection feature set states training set constructed
repeatedly sampling example instance distribution current level
difficulty applying current greedy policy greedy v instance create
trajectory states encountered every state removing duplicates encountered added
training set size feature selection training set maximum length training
trajectory specified user parameters
retaining duplicate states training set another option considered preliminary empirical favored option certainly worth exploration
note goal finding near optimal value function necessarily make reference
state distribution widely used notion near optimal theory mdps
sup norm distance v moreover state distribution represented duplicates
training sets typically distribution badly flawed policy heeding distribution
prevent correcting bellman error critical states visited policy visited
rarely states may instance rarely visited good exits visited state region
misunderstood current value function point primary justification
removing duplicates empirical performance demonstrated section
similar reasoning would suggest removing duplicate states training sets avi weight
training described section many large avi training sets generated
experiments duplicate removal must carefully handled control runtime historical reasons
experiments shown include duplicate removal avi
possible occurs current greedy policy cannot reach enough states complete desired training set consecutive trajectories drawn without visiting state
desired training set size reached process modified follows point
method attempts complete training set drawing trajectories random walk
sampled example current distribution process leads
consecutive trajectories without state method terminates training set generation
uses current training set even though smaller target size
applicability method
feature discovering value function construction described require complete access
underlying mdp model avi updates training set generation
following computations model


fiw u g ivan

given state ability compute action set
given state action value function v ability compute q value
q v
given state action ability draw state next state distribution
defined
given state ability compute features selected feature language
computations state required selected feature learner examples
section introduce relational feature language learner require knowledge set domain predicates arities state conjunctive
set predicate facts see section
b section describe propositional feature language learner
require knowledge set propositional state attributes state truth
assignment attributes
first three items enable computation bellman update last item enables
computation estimated value function given weights features defining well
selection features feature learner requirements amount substantial access
model method must considered model technique
consequence requirements cannot directly applied
standard reinforcement learning setting model access via acting world
without ability reset selected states setting bellman error computations particular
states cannot necessarily carried would possible construct noisy bellman error
training set model free setting would appropriate future work explore use
training set feature learning
ppddl domains studied provide information needed perform
computations method applies domains natural represent ppddl
analyzed method computations implemented instance
tetris experiments section underlying model represented providing hand coded
routines computations within domain
analysis
mdp value iteration guaranteed converge optimal value function conducted
tabular value function representation presence discounting bertsekas although
weight selection avi designed mimic value iteration avoiding tabular representation
general guarantee weight updates track value iteration thus converge
optimal value function particular may weighted combination features
represents optimal value function likewise none represents bellman update u v
value function v produced avi weight training process learning system introduces
features existing feature ensemble response training set used
select feature pairs states statewise bellman error learned feature exactly
captures statewise bellman error concept exactly capturing training set generalizing


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

successfully feature space contain bellman update value function used
generate training data
aim features approximate bellman error feature take
function mapping states statewise bellman error theoretical properties bellman error
features uncontrolled markov processes e without max operator bellman equation recently discussed work parr et al addition
features close approximations thereof proven reduce weighted l norm distance best weight setting true uncontrolled value v linear fixed point
methods used train weights feature addition prior work wu givan
parallel empirically exploring effects selecting bellman
error features complex controlled case leading reported
clear simply add bellman error feature directly set corresponding weight one resulting value function would desired bellman update u v
current value function v adding features iteration would thus give us way
conduct value iteration exactly without enumerating states added feature would
describe bellman error value function defined terms previously added features posing
serious computational cost issue evaluating added features particular bellman
error feature value function v estimated particular state high confidence
evaluating value function v state polynomial sized sample next states
action chernoff bounds
however value function v upon previously added bellman error feature
evaluation v requires sampling possible action compute
manner amount sampling needed high confidence grows exponentially number
successive added features type levels sampling collapse one expectation
intervening choices actions often case decision theoretic sampling
feature selection method attempt tractably approximate exact value iteration method
learning concise efficiently computable descriptions bellman error feature
iteration
method thus viewed heuristic approximation exact value iteration exact
value iteration instance method obtained explicit state value table
feature representation generating training sets feature learning containing states
obtain exact value iteration would omit avi training instead set weight one
feature language learner shown approximate explicit features tightly
enough resulting approximate bellman update contraction l norm
easy prove tightening approximations v weights set one however
practical experiments use feature representations learners
approximation bound relative explicit features known

two candidate hypothesis spaces features
section describe two hypothesis spaces features relational feature space
propositional feature space along respective feature learning methods
two feature spaces assume learner provided training set states paired
statewise bellman error values


fiw u g ivan

note two feature space learner pairs lead two instances general method
others easily defined defining feature spaces corresponding learners
empirically evaluate two instances presented
relational features
relational mdp defined terms set state predicates state predicates basic
elements define feature representation language define generalpurpose means enriching basic set state predicates resulting enriched predicates
used predicate symbols standard first order predicate logic consider
formula logic one free variable feature follows
state relational mdp first order interpretation first order formula one free
variable function states natural numbers maps state number
objects state satisfy formula take first order formulas real valued
features normalizing real number zero onethis normalization done
dividing feature value maximum value feature take typically
total number objects domain smaller domains objects
quantifiers typed similar feature representation used work fawcett
feature representation used relational experiments learner describe
next subsection considers existentially quantified conjunctions literals one free
variable features space formulas thus effective feature space relational
experiments
example take blocksworld table object example x
predicate domain asserts block x top object
may block table possible feature domain described
x first order formula x one free variable formula
means object immediately block object x
essentially excludes table object block held arm
object set described feature n blocks un normalized value
feature n states block held arm n states
block held arm
e nriched p redicate et
interesting examples possible enriched predicate set define enrich
set state predicates p add binary predicate p transitive closure form
predicate p predicates min p max p identifying minimal maximal elements
predicate goal domains recall representation section
includes predicate p goal version predicate called goal p represent desired
state predicate p goal add means ends analysis predicate correct p
represent p facts present current state goal
objects x correct p x true p x goal p x
true p x true objects x connected path binary relation p relation
max p x true object x maximal element respect p e exists object
generalizations allow multiple free variables straightforward unclear utility time



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

p x true relation min p x true object x minimal element respect
p e exists object p x true
formally define feature grammar online appendix available jair website
page
example cont feature correct x means x stacked top
object current state goal state feature x
means current state x directly object e sequence
relations traversing path x inclusively feature max x
means x table object block towers placed table since
table object object feature min x means
object top x e x clear
learning relational features
select first order formulas candidate features beam search beam width w
present pseudo code beam search figure search starts basic features derived
automatically domain description repeatedly derives candidate features
best scoring w features found far adding features candidates keeping
best scoring w features times candidates added fixed depth times
best scoring feature found overall selected added value function representation
candidate features scored beam search correlation bellman error feature
formalized
specifically score candidate feature f correlation coefficient bellman
error feature b v estimated training set correlation coefficient functions


defined corr coef e e e
instead known

distribution compute value use states training set compute sampled
version following equations approximate true expectation e true standard
deviation random variable x
x
x
es x



x

corr coef sampled



x
x e x



es es es



scoring function feature selection regularized version correlation coefficient
feature target function
score f corr coef sampled f depth f
depth feature depth beam search first occurs
parameter learner representing degree regularization bias towards low depth features


fiw u g ivan

beam search learn
inputs

feature scoring function fscore features

outputs

feature f

system parameters

w beam width
maxd max number beam search iterations
degree regularization defined section










set basic features defined section
f
repeat
set beam b highest scoring w candidates f
candidate feature set f b
candidate f b
candidate f b f f
f f combine f f






maxd highest score far
return maximum scoring feature f f

notes
feature scoring function fscore f used rank candidates lines discussion sample
scoring function used relational experiments given section
candidate scores cached calls fscore candidate scored twice
value largest score feature depth

figure pseudo code beam search

value score f b v score well feature f correlates bellman error feature note features non negative still well correlated
bellman error negative presence constant feature representation allows non negative feature shifted automatically needed
remains specify features hypothesis space considered initial
basic features beam search specify means constructing complex features
simpler ones use extending beam search first take state predicate set p
domain enrich p described section enrichment p take basic
features existentially quantified applications possibly negated state predicates variables
zero one free variable grammar basic features defined follows
domain distinguishes objects naming constants allow constants arguments
predicates well



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

definition basic feature existentially quantified hliterali expression
one free variable see figure online appendix available jair website
page
feature free variables treated technically one free variable feature
variable used binary feature value zero total number
objects instantiating free variable different ways truth value
assume throughout every existential quantifier automatically renamed away every
variable system take basic features human provided features
may available add features experiments order clearly
evaluate methods ability discover domain structure
stage beam search add candidate features retaining w best scoring
features previous stage candidate features created follows feature
beam combined conjunctively basic feature method combination two features described figure figure shows non deterministic pseudo code
combining two input features way making non deterministic choices
candidate feature pseudo code refers feature formulas f f describing
two features places formulas others written free variable exposed
f x f substitution variable notated replacing notation
f z
combination conjoining feature formulas shown line figure however
additional complexity resulting combining two free variables possibly equating
bound variables two features two free variables equated substitution one existentially quantified combination done line two pairs
variables chosen one contributing feature may equated resulting
quantifier front described line every combination feature candidate
beam search construction lead logically redundant features cases
syntactically redundant well avoid syntactically redundant features end beam
search selecting highest scoring feature already feature set logical redundancy syntactic redundancy difficult detect avoid redundancy
automatically ordering beam search reduce generation symmetric expressions however testing logical equivalence features
language np hard chandra merlin deploy complete equivalence test

example assume two basic features z p x z w q w set
possible candidates generated combining two features
line figure runs zero times
x z p x z w q w xf x f
z p x z w q w f x yf
z p x z w q x w f x f x
line runs one time
u z p u z q u equating x w item
u x p x u q u equating x z item


fiw u g ivan

combine
inputs

features f x f

outputs

set features

return set features


perform one
f x f x
b f f
c f f x



f f



perform following variable equating step zero one two times
let v variable occurring f
let e expression form v v occurs
b let w variable occurring f
let e expression form w w occurs
c let u variable used
replace e u replace e u
e u

notes
choice b c choice number iterations step choices e e
steps b non deterministic choices
feature produced run non deterministic included set
features returned combine
assumed f f variables common renaming necessary operation

figure non deterministic combining two feature formulas

u p x u w q u w equating z item
u p x u q u equating z w item
u p x u q x u equating z w item
first three computed cases b c respectively remaining
five derive first three equating bound variables f f
features generated depth k language easily require enumerating k tuples
domain objects since cost evaluation grows exponentially k bound


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

maximum number quantifiers scope point feature formula q refuse
consider feature violating bound
values w q parameters controlling relational learner evaluate
set parameters discussed experimental setup description
section
provide brief discussion motivations feature combination method first
note additive combination features represent disjunctions features hence
consider conjunction feature combination chosen conjoin features
multiple ways varying handling combining free bound variables believe
choice uniquely effective provide example realization proposed featurediscovery architecture
choice feature representation combination method must trade cost
evaluation choices potential gain quality selected features
chosen limit individual features conjunction effectively limited features
horn clauses predicates negations univariate heads
propositional features
discuss second candidate hypothesis space features propositional representation use decision trees represent propositional features detailed discussion
classification decision trees found book mitchell decision tree
binary tree internal nodes labeled binary tests states edges labeled yes
representing binary tests leaves labeled classes case zero
one path tree root leaf label l identifies labeling set
stateseach state consistent state test path viewed labeled l tree
way decision tree real number labels leaves viewed labeling states
real numbers thus feature
learn decision trees training sets labeled states well known c
quinlan induces tree greedily matching training data root
use c induce featuresthe key construct suitable
training sets c induced features useful reducing bellman error
include possible state tests decision trees induce every grounded predicate
application state predicates well every previously selected decision tree feature
binary test leaf labels zero one
learning propositional features
construct binary features use sign bellman error feature magnitude sign statewise bellman error state serves indication whether
state undervalued overvalued current approximation least respect exactly
representing bellman update current value function identify collection
undervalued states feature assigning appropriate positive weight feature
representing disjunction overlapping features additive combination done third feature
representing conjunction inclusion exclusion negative weight conjunction
grounded predicate application predicate applied appropriate number objects instance



fiw u g ivan

increase value similarly identifying overvalued states feature assigning
negative weight decrease value note domains interest generally
large state space enumeration need classification learning generalize notions
overvalued undervalued across state space training sets sample states
enable method ignore states approximately converged discard states
statewise bellman error near zero training set specifically among states negative statewise bellman error discard state error closer zero median
within set among states positive statewise bellman error sophisticated methods discarding training data near intended boundary considered
future often introduce additional parameters method seek
initial simple evaluation overall discarding define
set remaining training pairs states positive statewise bellman error
likewise negative statewise bellman error
use positive examples negative examples supervised
classification case c used hypothesis space classification space
decision trees built tests selected primitive attributes defining state space
goal case use previously learned features decision trees attributes
concept resulting supervised learning treated feature linear approximation architecture initial weight zero
intent ideally develop approximately optimal value function value function
expected bellman error many states every state however low state wise
error states contribute high sup norm bellman error discarding training
states low statewise bellman error reflects tolerance low error threshold
representing degree approximation sought note technical motivation selecting
features upon bellman error focuses reducing sup norm bellman error given
motivation interested finding exact boundary positive negative
bellman error identifying states large magnitude bellman error
large magnitude error addressed feature addition
observe limited need separately learn feature matching due
following representability argument consider binary feature f complement f
exactly one f f true state given presence constant feature feature
set adding f f feature set yields set representable value functions assigning
weight w f effect assigning weight w f adding w weight
constant feature
discussion
discuss generalization capability learning time heuristic elements feature
learning method
g eneralization across varying omain izes
propositional feature space described varies size number objects relational
domain varied features learned one domain size generally meaningful
even necessarily defined domain sizes relational contrast able
generalize naturally different domains sizes experiments report ability


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

propositional technique learn within domain size directly attempt use
learning small gain performance large major
limitation producing good large domains
l earning ime
primary motivation giving generalization domain sizes order employ propositional resulting learner use highly efficient shelf classification
learning times reported section propositional learner learns
features orders magnitude faster relational learner
h euristic e lements



ethod

mentioned earlier heuristically approximates repeated addition bellman
error features linear value function approximation order carry value iteration
mentioned earlier value iteration guaranteed converge optimal value function
however due scale target heuristic approximations required discuss
motivations heuristic approximation employ briefly
first compute exact bellman error features instead use machine learning fit
training set sample states bellman error values selection training set
done heuristically trajectories drawn current greedy policy use policy
selection training data loosely motivated policy convergence reinforcement
learning singh et al serves focus training relevant states see section
second relational instance feature framework beam search method use
select highest scoring relational feature best fit bellman error ad hoc greedy
severely resource bounded fit obtained bellman error purely heuristic provide
heuristic method machine learning example intend future
provide better relational learners resulting better performance heuristic
elements current method discussed appendix work
viewed providing reduction stochastic structured machine learning numeric
functions see section
third propositional instance feature framework learner c selects hypotheses greedily reduction c classification relies explicit tolerance approximation form threshold used filter training data near zero bellman error
motivation approximation tolerance focus learner high bellman error states
allow method ignore almost converged states see section
fourth fundamental work use linear approximation value function
gradient descent weight selection case avi approximation methods key
handling large state spaces create need feature discovery avi method
includes empirically motivated heuristic methods controlling step size sign changes
weights see section online appendix available jair website
fifth rely human input select sequence difficulties encountered
feature discovery well performance thresholds difficulty increases
believe aspect automated future see section


fiw u g ivan

related work
automatic learning relational features approximate value function representation surprisingly frequently studied quite recently remains poorly understood
review recent work related one dimensions contribution
feature selection bellman error magnitude
feature selection bellman error recently studied uncontrolled policyevaluation context work keller et al parr et al attribute value
explicit state spaces rather relational feature representations feature selection
bellman error compared feature selection methods uncontrolled context
theoretically empirically work parr li taylor painter wakefield littman

extend work controlled decision making setting study incorporation
relational learning selection appropriate knowledge representation value functions
generalize different sizes within domain
main contribution work parr et al formally showing uncontrolled
case policy evaluation possibly approximate bellman error features provably tightens approximation error bounds e adding exact bellman error feature provably reduces
weighted l norm distance optimal value function achieved optimizing weights linear combination features extended weaker form
approximated bellman error features uncontrolled case limitation uncontrolled case substantial difference setting work limited experiments shown
use explicit state space representations technique learns completely set features
policy evaluation conducted policy iteration contrast method accumulates
features value iteration point limiting focus single policy constructing
feature set policy evaluation procedure amenable formal analysis
retaining learned features throughout value iteration policy implicitly considered value iteration greedy policy potentially changing throughout however
relational feature learning runtime cost feature learning currently high make
constructing feature sets repeatedly practically feasible
parr et al builds prior work keller et al studied uncontrolled setting work provides theoretical general framework provides
specific bellman error attribute value representations state represented real vector order select features provides apparent leverage
state real vector structured logical interpretation typical
benchmarks
feature discovery via goal regression
previous methods gretton thiebaux sanner boutilier useful features
first identifying goal regions high reward regions identifying additional regions regressing action definitions previously identified regions principle exploited
given state feature indicates value state able achieve feature
one step indicate value state regressing feature definition action


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

definitions yields definition states achieve feature one step repeated regression identify many regions states possibility transitioning
action sequence high reward region
exponentially many action sequences relative plan length
exponentially many regions discovered way well exponential increase size
representation region exponentials terms number regression steps
taken control exponential growth number features considered regression
implemented pruning optimizations control eliminate overlap regions
detected inexpensively well dropping unlikely paths however without scoring
technique fit bellman error used select features regression still
generates large number useless features currently effective regression
first order mdp planner described work sanner boutilier effective
disallowing overlapping features allow optimizations weight computation yet clearly
human designed feature sets fact overlapping features
inductive technique avoids issues considering compactly represented features
selecting match sampled statewise bellman error training data provide extensive
empirical comparison first order approximate linear programming technique foalp
work sanner boutilier empirical empirical evaluation
yields stronger across wide range probabilistic benchmarks goalregression implemented foalp although aspects approaches
goal regression candidate generation vary comparison well
regression approaches feature discovery related method fitting bellman
error exploit fact states reach valuable states must valuable e seek local consistency fact regression goal viewed special
case iteratively fitting features bellman error current value function depending
exact formulation k bellman error k step go value function
non zero otherwise nontrivially structured region states reach goal first
k steps significant differences bellman error regression
feature selection arise states reach goal different probabilities different
horizons fits magnitude bellman error smoothly consider
degree state reaches goal horizon immediately generalizes setting useful heuristic value function provided automatic feature
learning whereas goal regression appears require goal regions begin regression
spite issues believe approaches appropriate valuable
considered important sources automatically derived features future work
effective regression requires compact declarative action model available
inductive technique present require even pddl action model deductive component computation bellman error individual states representation
statewise bellman error computed sufficient technique empirical performance planner tetris model represented
giving program given state input returns explicit next state distribution
state foalp inapplicable representations due dependence logical deductive rea example second international probabilistic competition regression foalp planner
required human assistance domain providing needed domain information even though standard
pddl model provided competition sufficient planner



fiw u g ivan

soning believe inductive deductive approaches incorporating logical representation
important complementary
goal regression special case general generating candidate features transforming currently useful features others considered include
abstraction specialization decomposition fawcett human defined concept transformations dates back least landmark ai program davis lenat
work uses one means generating candidate features beam search logical formulas
increasing depth means candidate generation advantage strongly favoring concise inexpensive features may miss complex accurate useful features
directly generalizes means generating candidate features
centrally distinguishes previous work leveraging feature transformations
use statewise bellman error score candidate features foalp sanner boutilier
uses scoring function includes non pruned candidate features linear program
used approximately optimal value function zenith system fawcett uses
scoring function provided unspecified critic
previous scoring functions mdp feature selection
method work patrascu et al selects features estimating minimizing
l error value function retraining weights candidate feature
included l error used work instead bellman error difficulty retraining
weights minimize bellman error method focuses fitting bellman error
current approximation without retraining feature avoids expensive
retraining computation search able search much larger feature space effectively
work patrascu et al contains discussion relational representation l
scoring method could certainly used features represented predicate logic work date
tried potentially expensive
related work
include discussion additional distantly related directions appendix divided following subsections
relevant feature selection methods fahlman lebiere utgoff precup
rivest precup mahadevan maggioni petrik
structural model model free solution methods markov decision processes including
relational reinforcement learning rrl systems dzeroski deraedt driessens
driessens dzeroski driessens et al
b policy learning via boosting kersting driessens
c fitted value iteration gordon
exact value iteration methods first order mdps boutilier reiter price
holldobler skvortsova kersting van otterlo de raedt


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

inductive logic programming muggleton quinlan karalic bratko

approximate policy iteration relational domains fern et al discussion
relational decision list policy learners khardon martin geffner yoon et al

automatic extraction domain knowledge veloso carbonell perez borrajo fink
blythe kambhampati katukam qu estlin mooney fox long
gerevini schubert

experimental setting
present experiments nine stochastic domains including reward oriented
goal oriented domains use pentium xeon ghz machines gb memory section give general overview experiments giving detailed discussion
individual domains section first briefly discuss selection evaluation domains
section second section set evaluation relational feature learner
comparison variants replace key aspects random choice determine
importance additional details including many experimental parameter settings found
online appendix available jair website section
domains considered
evaluation domains necessary specify discount factor modeling
domain mdp discounting discount factor effectively specifies tradeoff
goals reducing expected plan length increasing success rate parameter
method domain studied feature learning method applied
choice simplicity choose throughout experiments note
discount factor used ys dmin domain formalization compare
previous work patrascu et al
etris
section evaluate performance relational propositional learners
stochastic computer game etris reward oriented domain goal player
maximize accumulated reward compare performance set handcrafted features performance randomly selected features
p lanning c ompetition omains
section evaluate performance relational learner seven goal oriented domains two international probabilistic competitions ippcs younes et al
bonet givan comparison purposes evaluate performance propositional learner two seven domains b locksworld variant b oxworld described
two domains illustrate difficulty learning useful propositional features complex domains compare relational planner
two recent competition stochastic planners replan yoon et al foalp sanner


fiw u g ivan

boutilier performed well competitions finally
compare obtained randomly selecting relational features tuning weights
complete description ppddl source domains used please see
work younes et al bonet givan
every goal oriented domain generator first second ippc considered inclusion experiments inclusion require domain fixed
action definitions defined section addition ground conjunctive goal regions four domains properties directly adapted three domains
properties
b oxworld modify generator goal region ground
conjunctive expression call resulting domain c onjunctive b oxworld
f ileworld construct obvious lifted version create generator restricted three folders domain action definitions vary number
folders call resulting domain l ifted f ileworld
owers h anoi create generator
resulting selection provides seven ippc domains empirical study provide
detailed discussions adapted domains section online appendix available jair
website well discuss reasons exclusion domains
ys dmin
conclude experiments comparing propositional learner previous method patrascu et al ys dmin domain used evaluation empirical
comparison ys dmin domain shown section
randomized variants method
major contribution introduction evaluation feature learning framework
controlled setting scoring bellman error scoring empirical work instantiates framework relational feature learning design greedy
beam search compare performance instance framework variants
replace key aspects randomized choice illustrating relative importance features two random choice experiments adapt method one following two
ways
labeling training states random scores instead bellman error scores target
value feature training set random number called
random scoring
narrowing beam search randomly rather greedily eliminate scoring beam search instead random selection narrow beam end
beam search scoring used select best resulting candidate called
random beam narrowing


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

original labels training data bellman error narrows beam greedily rather randomly called greedy beam search scoring plots
comparisons consider relational feature representation beam
search method used experiments two variants introduced presented
sections original method selects features perform much better
randomly selected features greediness beam search often
important achieving good performance

experimental
present experimental etris competition domains ys dmin
section starting introduction structure presentation
read
task evaluating feature learning system subtle complex particularly
factor relational case generalization sizes learning small
must evaluated resulting data extensive highly structured requiring
training reader understand interpret introduce reader structure

experiments propositional learning randomly selected propositional features size never varies within one run learner propositional representation section cannot generalize sizes run separate experiment
size considered experiment two independent trials trial starts single trivial
feature repeatedly adds features termination condition met feature addition avi used select weights combining features form value function
performance value function measured sampling performance greedy policy
compute average two trials performance function number
features used since single line plot performance function number
features several different fixed size learners compared one figure one
line done example figures performance measure used varies
appropriately domain presented
study ability relational representation section generalize sizes
study properly understood backdrop flowchart figure
described flowchart one trial learner learn sequence features encounter
sequence increasing difficulties one iteration learner add
feature increase difficulty depending current performance case
weights retrained avi performance measurement resulting greedy policy
taken different trials may increase size different points cannot meaningfully
average measurements two trials instead present two independent trials separately
two tables figures first trial present data
second time line plot showing performance function number features
size changes annotated along line plots figures note success
ratio generally increases along line features added falls size
increased etris however measure rows erased rather success ratio rows


fiw u g ivan

erased generally increases addition feature addition rows
available grid
interpret tables showing trials relational learner useful focus first
two rows labeled features difficulty rows taken together
progress learner adding features increasing size column table
represents indicated size indicated number learned features
one column next change one rowsif performance
policy shown column high enough difficulty increases
otherwise number features increases adding subtlety interpreting tables note several adjacent columns increase number features
sometimes splice two columns save space thus several features
added consecutively one size slowly increasing performance may
first last columns size consequent jump number
features columns likewise sometimes splice columns several consecutive columns increase difficulty found splicings save space
increase readability practice reading tables
performance numbers shown column success ratio average plan length number
rows erased etris refer performance weight tuned policy resulting
feature set difficulty column performance value
function without tuning weights target size thus quality measures
policy found feature learning current size point
target size illustrate progress learning small target size
via generalization
study deciding stop adding features instead
propositional relational experiments trials stopped experimenter judgment additional expensive value giving evaluating however
stop trials still improving unless unacceptable resource consumption
occurred
trial accumulated real time trial measured shown point
trial use real time rather cpu time reflect non cpu costs paging due
high memory usage
tetris
present experimental etris
overview



etris

game etris played rectangular board area usually size initially
empty program selects one seven shapes uniformly random player rotates
drops selected piece entry side board piles onto remaining fragments
pieces placed previously implementation whenever full row squares
occupied fragments pieces row removed board fragments top
removed row moved one row reward received row removed
process selecting locations rotations randomly drawn pieces continues board
full piece cannot placed anywhere board etris stochastic since


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

next piece place randomly drawn stochastic element
game etris used experimental domain previous mdp reinforcement learning
bertsekas tsitsiklis driessens et al set human selected features
described book bertsekas tsitsiklis yields good performance
used weighted linearly approximated value functions cannot fairly compare performance
domain probabilistic planners requiring ppddl input found natural
ppddl definition etris
performance metric etris number rows erased averaged trial
games reward scaling parameter rscale defined section online appendix page
selected
etris r elational f eature l earning r esults
represent etris grid rows columns objects use three primitive predicates
fill c r meaning square column c row r occupied r r meaning row
r directly row r beside c c meaning column c directly left
column c quantifiers used relational etris hypothesis space typed types
row column
state predicates representing piece drop however efficiency
reasons planner computes state value function grid next piece
limitation value function expressiveness allows significantly cheaper bellman backup computation one step lookahead greedy policy execution provides implicit reasoning
piece dropped piece grid next states
conduct relational etris experiments column n row board n initially
set rows threshold increasing difficulty adding one row score
least n rows erased target size experiments rows
relational etris experiments given figures discussed
etris p ropositional f eature l earning r esults
propositional learner describe etris state binary attributes represent
pieces currently dropped along one additional binary attribute
grid square representing whether square occupied adjacency relationships
grid squares represented procedurally coded action dynamics note
number state attributes depends size etris grid learned features
apply grid size separate selected
sizes
evaluate propositional feature learning column etris grids four different sizes
rows rows rows rows four trials shown together figure
average accumulated time required reach point figure shown figure
discussed
e valuating mportance b ellman error coring g reedy
b eam search etris
figure compares original alternatives vary training set
scoring greediness beam search discussed section two alternatives use


fiw u g ivan

trial
features
difficulty
score
accumulated time hr
target size score



























































































trial
features
difficulty
score
accumulated time hr
target size score

























































































































figure etris performance averaged games score shown average rows
erased difficulty shown number rows etris board
number columns difficulty increases average score greater
n n number rows etris board target
size rows columns omitted discussed section

average rows erased

tetris relational trial










































number features

figure plot average number lines erased etris games run
avi training learning relational features trial vertical lines indicate
difficulty increases number rows labeled along plot



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

average rows erased

tetris propositional










number features








figure plot average number lines erased etris games iteration
avi training learning propositional features averaged two trials

accumulated time hr

tetris propositional











number features








figure plot accumulated time required reach point figure averaged two
trials

schedule used original greedy beam search scoring etris
starting size however performance two alternatives never
good enough increase size
e valuating h uman designed f eatures etris
addition evaluating relational propositional feature learning evaluate
human selected features described book bertsekas tsitsiklis perform
selected sizes size start weights zero use avi


fiw u g ivan

average rows erased

impact greedy beam search scoring












number relational features
greedy beam search scoring original
random scoring variant
random beam narrowing variant

figure plot average number lines erased etris games relational features
learned original two alternatives discussed section
random scoring random beam narrowing averages two
independent trials trials two variants terminated fail make
progress several feature additions comparison purposes trial one original
greedy beam search scoring method shown reaching threshold difficulty
increase eleven feature additions trial two even better

average rows erased trial
average rows erased trial









figure average number lines erased etris games best weighted
combination human features found two trials avi four
sizes

process described section train weights features performance appears


k
human designed features
converge change learning rate k
require larger step size converge rapidly human designed features normalized
value experiments run two independent trials size
report performance best performing weight vector found trial figure
p erformance c omparison b etween ifferent pproaches



etris

several general trends emerge etris first addition learned
features almost increasing performance resulting tuned policy current
size target size best performance point reached suggests fact


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

relational prop prop prop prop
average feature learning
time min











figure table average feature learning time relational propositional approaches
selecting useful features clear evidence ability relational representation
usefully generalize sizes substantial performance developed target
size without ever training directly size
best performance learned propositional features much lower
learned relational features sizes shown even though larger feature training set
size many learned features used propositional suggests
rich relational representation indeed able better capture dynamics etris
propositional representation
performance random features etris significantly worse
learned features demonstrating performance improvements feature learning
due useful feature selection bellman error simply due increasing number
features
learned relational feature performance etris far worse obtained
human selected features avi size however etris
relational feature performance close human designed features human designed
features engineered perform well etris hence concepts useful
performing well smaller sizes may exist features
ime l earn e ach f eature
figure average time required learn relational feature propositional feature
etris
time required learn relational feature significantly longer required learn
propositional feature even though propositional larger feature training set size
used
c omparison



p revious etris specific l earners

evaluating domain independent techniques etris must first put aside strong performance already shown many times literature domain dependent techniques domain
must face published domain independent comparison points
order define state art target surpass latter provide baseline
two different approaches random feature selection targeted feature selection dramatically improves random selection former include
discussion domain specific elements key previous published etris
many previous domain specific efforts learning play etris bertsekas
tsitsiklis szita lorincz lagoudakis parr littman farias van roy
kakade typically provide human crafted domain dependent features deploy domain independent machine learning techniques combine features often tuning


fiw u g ivan

weights linear combination example domain specific feature counting number
covered holes board frequently used feature plausibly derived human
reasoning rules game realizing holes difficult fill later
action lead low scores prior work selection feature hand
automated feature selection process scoring correlation bellman error
frequently used domain specific features include column height difference height adjacent columns apparently selected relevant human reasoning rules
game
key question address whether useful features derived automatically decision making situation etris approached domain independent
system without human intervention method provided domain state representation primitive horizontal vertical positional predicates single constant feature
knowledge published evaluation etris rely
domain specific human inputs discussed expected performance
etris much weaker achieved domain specific systems cited
probabilistic competition domains
throughout evaluations learners domains use lower plan length cutoff
steps evaluating success ratio iterative learning features speed learning
use longer cutoff steps final evaluation policies comparison
planners evaluations target size reward scaling parameter rscale
defined section online appendix page selected throughout
domains
domains multi dimensional sizes remains open
change size different dimensions automatically increase difficulty learning
c onjunctive b oxworld z enotravel hand design sequence increasing sizes
discussed section evaluate feature learners total seven probabilistic competition domains following paragraphs provide full discussion
b locksworld c onjunctive b oxworld abbreviated five domains provide full discussion five domains appendix b
relational feature learner finds useful value function features four domains
b locksworld c onjunctive b oxworld ireworld l ifted f ileworld
three domains z enotravel e xploding b locksworld owers h anoi
relational feature learner makes progress representing useful fixed size value function
training sizes fails features generalize well larger sizes
b locksworld
probabilistic non reward version b locksworld first ippc actions pickup
putdown small probability placing handled block table object instead
selected destination
relational learner start blocks increase n blocks n
blocks whenever success ratio exceeds average successful plan length less
n target size blocks shown figures


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

trial
features
difficulty
success ratio
plan length
accumulated time hr
target size sr
target size slen



































trial
features
difficulty
success ratio
plan length
accumulated time hr
target size sr
target size slen






































figure b locksworld performance averaged relational learner
add one feature per column success ratio exceeds average successful plan
length less n n blocks increase difficulty
next column plan lengths shown successful trials difficulties
measured number blocks target size blocks columns
omitted discussed section

propositional learner sizes blocks shown
figure
relational learner consistently finds value functions perfect near perfect success
ratio blocks performance compares favorably recent rrl driessens
et al deterministic b locksworld goals severely restricted
instance single atoms success ratio performance around three ten blocks
single goal still lower achieved b locksworld
average plan length far optimal observed large plateaus induced value
function state regions states given value greedy policy wanders
merits study understand feature induction break
plateaus separately studied ability local search break plateaus wu
kalyanam givan
performance target size clearly demonstrates successful generalization sizes
relational representation
propositional demonstrate limitations propositional learner regarding lack
generalization sizes good value functions induced small
sizes blocks slightly larger sizes blocks render method ineffective
block initial random greedy policy cannot improved never finds


fiw u g ivan

success ratio

blocksworld relational trial
blocks



blocks

blocks

blocks
blocks


blocks




successful plan length










blocks



blocks



blocks

blocks

blocks

blocks

blocks
blocks










number features

figure b locksworld success ratio average successful plan length averaged
first trial figure relational learner

goal addition demonstrate learning additional features good policy
found degrade performance possibly avi performs worse higher dimensional
weight space
c onjunctive b oxworld
probabilistic non reward version b oxworld first ippc similar
familiar logistics domain used deterministic competitions except explicit connectivity graph cities defined logistics airports aircraft play important role
since possible move trucks one airport locations adjacent another airport locations adjacent b oxworld possible move boxes
without aircraft since cities may connected truck routes stochastic
element introduced domain truck moved one city another
small chance ending unintended city described section use
c onjunctive b oxworld modified version b oxworld experiments
start box relational learner increase n boxes n boxes
whenever success ratio exceeds average successful plan length better n
feature learning difficulties use cities use two target sizes boxes


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

blocksworld propositional

success ratio







successful plan length












accumulated time hr


















































number features

blocks

blocks

blocks

blocks

figure b locksworld performance success ratio average successful plan length averaged accumulated run time propositional learner averaged two trials



fiw u g ivan

trial
features



difficulty



success ratio


plan length

accumulated time hr


target size sr
target size slen

target size sr

target size slen


















































































trial
features



difficulty



success ratio


plan length

accumulated time hr
target size sr



target size slen
target size sr

target size slen




























































































figure c onjunctive b oxworld performance averaged add one
feature per column success ratio greater average successful plan
length less n n boxes increase difficulty next
column difficulty shown number boxes throughout learning
process number cities plan lengths shown successful trials two
target sizes used target size boxes cities target
size boxes cities columns omitted discussed
section

cities boxes cities relational learning shown figures
propositional learner cities boxes shown figures
interpreting c onjunctive b oxworld important focus average
successful plan length metric c onjunctive b oxworld random walk able
solve nearly often long plans learned features enable
direct solutions reflected average plan length metric
two relational features required significantly improved performance
tested unlike domains evaluate c onjunctive b oxworld domain
note oddly ippc competition domain used action preconditions prohibiting moving box away
destination preconditions bias random walk automatically towards goal consistency
competition retain odd preconditions although preconditions necessary good
performance



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

success ratio

conjuctive boxworld cities relational trial
box



boxes

box











successful plan length

box




boxes
boxes

box
boxes
boxes



boxes
box








number features

figure c onjunctive b oxworld success ratio average successful plan length averaged
first trial relational learner

learned features straightforwardly describable english first feature counts many
boxes correctly target city second feature counts many boxes trucks
note lack features rewarding trucks right place resulting
longer plan lengths due wandering value function plateaus features easily written knowledge representation e g count trucks located cities destinations
package truck require quantification cities packages severe
limitation quantification currently method efficiency reasons prevents consideration
features point worth noting regression feature discovery studied work gretton thiebaux sanner boutilier expected
identify features regarding trucks regressing goal action unloading
package destination combining bellman error method regression
methods promising future direction
nevertheless relational learner discovers two concise useful features dramatically
reduce plan length relative initial policy random walk significant success
automated domain independent induction features


fiw u g ivan

conjunctive boxworld propositional

success ratio







successful plan length


















































accumulated time hr












number features
box

box

box

figure c onjunctive b oxworld performance averaged accumulated run time propositional learner averaged two trials throughout learning process number cities



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

one trial relational feature learner c onjunctive b oxworld takes several days
even though fixed number cities training five cities techniques required improving efficiency feature learning provide
training larger numbers cities demonstrate current representation
learning methods adequately manage small city graphs even larger larger numbers
boxes deliver resulting value functions successfully generalize city
domain well known weakness avi apparent avi often works practice
theoretical guarantee quality weight vector found avi training alternatively approximate linear programming step could replace avi training provide
expensive perhaps robust weight selection c onjunctive b oxworld
avi training goes astray selecting weights box domain size trial
selected weights overemphasize first feature neglecting second feature revealed
data shown plan length performance degrades significantly one column
data avi repeated next size boxes good performance restored
similar one column degradation plan length occurs trial box box sizes
propositional experiments c onjunctive b oxworld note generally
adding learned propositional features degrades success rate performance relative initial
random walk policy introducing ineffective loops greedy policy resulting greedy
policies goal fewer steps random walk generally pay unacceptable drop
success ratio one exception policy found one box two
propositional features significantly reduces plan length preserving success ratio still
much weaker relational feature language
get severe size increases box suffering severe
degradation success rate modest gains successful plan length please note
accumulated runtime experiments large especially box avi
training expensive policies goal computing greedy policy
state long trajectory requires considering action number available actions
quite large domain reasons propositional technique evaluate sizes
larger three boxes
ummary r esults



dditional omains

figures present summary five additional probabilistic domains detailed full discussion domains please see appendix b
summary see feature learning successfully finds features perform well across increasing sizes two five domains ireworld l ifted f ileworld three domains z enotravel owers h anoi e xploding
b locksworld feature learning able make varying degrees progress fixed small sizes progress sometimes quite limited generalize well size increases
e valuating r elative mportance b ellman error coring
g reedy b eam search g oal oriented omains
figure compares original alternatives vary training set
scoring greediness beam search discussed section trial variant
generate greedy policy domain feature selection within relational representation


fiw u g ivan

tireworld trial

success ratio




nodes

nodes

nodes

nodes

nodes

nodes

nodes





nodes




nodes





successful plan length











nodes



nodes



nodes

nodes



nodes
nodes



nodes















number features

zenotravel trial
success ratio


cities person aircraft




cities people aircraft



cities people aircraft




successful plan length
























cities people aircraft

cities people aircraft




cities person aircraft























number features

figure summary ireworld z enotravel full discussion detailed
please see appendix b



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

exploding blocksworld trial

success ratio




blocks

blocks
blocks



blocks
blocks



blocks




successful plan length


























blocks



blocks


blocks

blocks



blocks

blocks

























number features

tower hanoi trial

success ratio


discs




discs



discs



discs

discs


successful plan length





























discs




discs

discs



















number features

figure summary e xploding b locksworld owers
discussion detailed please see appendix b





h anoi full

fiw u g ivan

success ratio

lifted fileworld trial
file



file

file

files

files

files

files

files



















successful plan length


files
files

files



files
files

files
files
files

files

files

files


file

file
file

files

files

files
files

file


















number features

figure summary l ifted f ileworld full discussion detailed
please see appendix b

alternating avi training difficulty increase feature generation original
trial domain select best performing policy running
target difficulty reached improvement least three feature additions
latter case generating least nine features evaluate greedy policy acquired
manner measuring average target size performance domain average
two trials shown figure
domain alternative random scoring perform comparably original greedy
beam search scoring exception three domain size combinations learners
perform poorly z enotravel block e xploding b locksworld disc owers
h anoi alternative random beam narrowing sometimes adequate replace original
domains greedy beam search critical performance
c omparison



r eplan



foalp

compare performance learned policies replan foalp
ppddl evaluation domains used use generators provided
competitions generate tested size except owers h anoi


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

domain
size
greedy beam scoring orig sr
greedy beam scoring orig slen
random scoring var sr
random scoring var slen
random beam narrowing var sr
random beam narrowing var slen
random walk sr
random walk slen

bw










box box tire zeno ex bw ex bw toh toh file
















































figure target size performance averaged relational features
learned original two alternatives discussed section
random walk averaged best two independent trials
target size

l ifted f ileworld one fixed size evaluate
performance planner times report fig success ratio
planner size averaged attempts policies learned
two independent trials shown indicated rfavi rfavi planner
minute time limit attempt average time required finish successful attempt
largest size domain reported figure
two trials learner domain evaluate policy performed best trial first target size policy set features
corresponding weight vector learned avi trial performance measured
success rate ties broken plan length remaining ties broken taking later
policy trial tied case consider policy policy
learned trial
planners performance incomparable replan winning domains losing others generally dominates foalp
rfavi performs best planners larger b locksworld c onjunctive b oxworld ireworld rfavi essentially tied replan performance
l ifted f ileworld rfavi loses replan remaining three domains e xploding
b locksworld z enotravel owers h anoi reasons difficulties last
three domains discussed sections presenting domains note
foalp learned policy z enotravel e xploding b locksworld owers
h anoi l ifted f ileworld
rfavi relies random walk explore plateaus states differentiated selected
features reliance frequently long plan lengths times failure
recently reported elsewhere early ongoing work remedying
search place random walk wu et al
rfavi learning different non learning online replanning used
replan determinized dropping probability parameters


fiw u g ivan

rfavi
rfavi
replan
foalp

blocks bw





blocks bw





blocks bw





blocks bw





rfavi
rfavi
replan
foalp

bx ci box





bx ci box





bx ci box





bx ci box





rfavi
rfavi
replan
foalp

nodes tire





nodes tire





nodes tire





ci pr zeno



n

rfavi
rfavi
replan
foalp

blocks ex bw



n

blocks ex bw discs toh






n
n

discs toh



n

bx ci box





files lifted file



n

figure comparison planner rfavi replan foalp success ratio
total attempts attempts owers h anoi l ifted f ileworld
size reported followed average successful plan length
parentheses two rows rfavi map two learning trials shown

bw bx tire zeno ex bw toh files
rfavi







rfavi







replan






foalp



n
n
n
n
figure average runtime successful attempts shown figure
largest size domain



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

important topic future try combine benefits obtained different
planners across domains
dominance rfavi foalp implies rfavi state
art among first order techniques work lifted form use lifted
generalization although foalp uses first order structure feature representation learned
features aimed satisfying goal predicates individually whole believe
goal decomposition technique sometimes work well small scale well
large
comparisons noted foalp read ppddl domain descriptions directly requires human written domain axioms learning unlike completely
automatic technique requiring numeric parameters characterizing domain
requirement human written domain axioms one reasons foalp compete
competition domains learned policy domains
tested
c onjunctive b oxworld note replan uses outcomes determinization discriminate likely unlikely outcomes truck movement
actions plans frequently selected rely unlikely outcomes perhaps choosing
move truck undesired location relying unlikely outcome accidentally moving
desired location plans usually fail resulting repeated replanning luckily selects high likelihood outcome plan execution happens get desired low likelihood
outcome behavior effect similar behavior learned value function exhibits discussed section learner failed feature rewarding appropriate
truck moves planners long plan lengths due many unhelpful truck moves however learned policy conducts random walk trucks much efficiently thus
successfully online replanning replan especially larger sizes
believe even dramatic improvements available improved knowledge representation features
sysadmin
full description ys dmin domain provided work guestrin koller parr
summarize description ys dmin domain machines connected
different topologies machine might fail step failure probability depends
number failed machines connected agent works toward minimizing number
failed machines rebooting machines one machine rebooted time step
n machines fixed topology dynamic state space sufficiently described n
propositional variables representing status certain machine
test domain purpose direct comparison performance propositional techniques published work patrascu et al test exactly
topologies evaluated measure performance measure reported sup norm bellman
error
evaluate method exact mdps used evaluation
work patrascu et al testing domain two different kinds topologies tested
hand convert nested universal quantifiers conditional effects original boxworld domain definition
equivalent form without universal quantifiers conditional effects allow replan read domain



fiw u g ivan





cycle topology

legs topology

figure illustration two topologies ys dmin domain nodes node
represents machine label indicates server machine specified work
patrascu et al

legs cycle legs topology three three node legs linear sequence three
connected nodes connected single central node one end cycle topology arranges
ten nodes one large cycle nodes topology two topologies
illustrated figure target learning domain keep many machines
operational possible number operating machines directly determines reward
step since nodes basic features statuses
nodes total states reward scaling parameter rscale defined section
online appendix available jair website page selected
work patrascu et al uses l sup norm bellman error performance
measurement ys dmin technique described seeks reduce mean bellman
error directly l bellman error report l bellman error averaged two
trials figure included figure shown work patrascu et al
select best shown algorithmic approaches legs
cycle topologies shown correspond setting cycle
topology x n setting legs topology terminology
topologies reduces l bellman error effectively per
feature well effectively overall experiments previously reported work
patrascu et al topologies bellman error eventually diverges avi cannot
handle complexity error function dimensionality increases still
achieve low bellman error remembering restoring best performing weighted feature set
weakened performance detected
note superior performance reducing bellman error could due entirely
part use avi weight training instead approximate linear programming alp
method used patrascu et al however systematic superiority known avi alp
suggest superior performance feature learning


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

sysadmin legs topology








bellman error












number features
legs learned

legs patrascu

sysadmin cycle topology





bellman error












number features
cycle learned
cycle patrascu

figure l bellman error ys dmin domain nodes two topologies values
work patrascu et al taken figure
work patrascu et al

demonstration generalization across sizes
asset relational feature representation presented learned relational
features applicable size domain section discussed


fiw u g ivan

target sizes
tetris blocks bw box city bx nodes tire files lifted file
intermediate sizes tetris blocks bw box city bx nodes tire files lifted file
generalize target size
learn intermediate size
random walk





















figure performance intermediate sized generalization
performance value functions learned target sizes evaluated
intermediate sized demonstrate generalization sizes comparison intermediate sized performance value functions learned directly intermediate size well performance random
walk generalization intermediate size learning averages two
trials etris average accumulated rows erased shown goal oriented
domains success ratio successful plan length parentheses shown
domain

modeling domain infinite set mdps one instance
domain infinite set mdps feature vector plus weight vector defines single
value function well defined every instance mdp discuss question whether framework single feature weight vector combination generalizes
good performance across sizes e value function v defined combination
whether greedy v performs similarly well different sizes
throughout section demonstrated direct application learned feature weight
vectors target sizes without retraining weights shown
target size lines tables domain etris b locksworld c onjunctive b oxworld ireworld l ifted f ileworld target size lines demonstrate direct
successful generalization target sizes even current sizes significantly smaller
domains notion size ys dmin insufficient progress significantly increase size learning small e xplod ing b locksworld z enotravel owers h anoi
subsection consider generalization larger target sizes selected intermediate sizes five domains specifically take weight vectors feature vectors
resulting end trials e weight vector retrained target sizes apply
directly selected intermediate sizes without weight retraining trials terminate learning reaches target sizes take weights features
best performing policy terminating sizes generalization shown
figure comparison table shows performance intermediate sized
value function learned directly size well performance
random walk size
note one trials etris terminates reaching target size due non improving performance
two trials l ifted f ileworld terminate target size performance already reaches optimality
learning reaches target size still although value functions learned smaller sizes
target size value functions evaluated generalization learned significantly larger sizes
intermediate evaluation size



fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

domain shown random walk much weaker generalization
showing presence generalization learned value functions across sizes
four goal oriented domains applying value functions learned target sizes equals
performance achieved value functions learned directly intermediate sizes better
performance c onjunctive b oxworld etris however generalization
match learning intermediate size note domains solution
strategy invariant respect size e g destroying incorrect towers form correct
ones b locksworld domains best plan strategy may change dramatically
size example etris larger number rows board allows strategies temporary
stack uncompleted rows smaller number rows favors strategies complete rows quickly
possible thus one necessarily expect generalization domain sizes every
domainthis conclusion expected hold whether considering generalization
value functions policies
included discussion policy generalization related work section appendix focusing previous work approximate policy iteration however note
policies generalize different sizes less well defined
value functions generalize previous api work defined
policies select actions states domain size work define value functions
assign numeric values states domain size none work guarantees finding good
optimal policy value function far know admit good compact value
functions admit good compact policies admit neither

discussion future
presented general framework automatically learning state value functions featurediscovery gradient weight training framework greedily select features
provided hypothesis space parameter method best correlate bellman
error features use avi weights associate features
proposed two different candidate hypothesis spaces features one two
spaces relational one features first order formulas one free variable beamsearch process used greedily select hypothesis hypothesis space considered propositional feature representation features decision trees hypothesis
space use standard classification c quinlan build feature best
correlates sign statewise bellman error instead sign magnitude
performance feature learning planners evaluated reward oriented
goal oriented domains demonstrated relational planner represents
state art feature discovering probabilistic techniques propositional planner
perform well relational planner cannot generalize instances
suggesting knowledge representation indeed critical success feature discovering
planners
although present propositional feature learning relation featurelearning knowledge representation difference difference
approaches historically propositional originally conceived reduction
classification learning attempt capture magnitude bellman error


fiw u g ivan

feature selection rather focuses sign error contrast relational
counts objects order match magnitude bellman error
difference cannot attribute performance differences
approaches knowledge representation choice differences performance could due
choice match sign propositional feature selection possible future experiment
identify sources performance variation would use propositional representation involving
regression trees dzeroski todorovski urbancic capture magnitude error
representation might possibly perform somewhat better decision tree representation
shown course would still enable generalization sizes relational
feature learner exhibits
bellman error reduction course one source guidance might followed
feature discovery experiments ippc domains many
domains successful plan length achieved much longer optimal discussed
section possible remedy deploying search previous work wu et al
learn features targeting dynamics inside plateaus use features decisionmaking plateaus encountered

acknowledgments
material upon work supported part national science foundation grant


appendix related work
feature selection approaches
f eature election

via

c onstructive f unction pproximation

automatic feature extraction sequential decision making studied work utgoff
precup via constructive function approximation utgoff precup work
viewed forerunner general framework limited propositional representations
binary valued features features single literal extensions old features conjunction work rivest precup variant cascade correlation fahlman
lebiere constructive neural network combined td learning learn
value functions reinforcement learning cascade correlation incrementally adds hidden units
multi layered neural networks hidden unit essentially feature built upon set
numerically valued basic features work provides framework generalizing prior efforts reduction supervised learning explicit reliance bellman error signal
feature hypothesis space corresponding learner deployed particular
demonstrate framework binary propositional features c learner rich
numeric valued relational features greedy beam search learner work provides first
evaluation automatic feature extraction benchmark domains several
competitions
work utgoff precup implicitly relies bellman error
explicit construction bellman error training set discussion selecting features correlate


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

bellman error instance work focuses first refining current feature weight
updates converging poorly high variance weight updates whereas work focuses first
finding feature correlates statewise bellman error regardless whether feature refines
current feature addition work selects features online weights current
features adjusted stationary target value function bellman
error considered selection next feature contrast work separates weight
training feature selection completely differences perhaps part due
reinforcement learning setting used utgoff precup opposed setting
work
selection hidden unit feature cascade correlation fahlman lebiere
covariance feature values errors output units output units
estimating value function training data providing bellman update value function
output unit error bellman error thus hidden units learned work rivest
precup approximations bellman error features learned features although made explicit work making goal capturing bellman error explicit
provide general reduction facilitates use learning method capture
resulting feature learning training sets particular able naturally demonstrate generalization across domain sizes several large domains relational feature learner contrast
single test domain work rivest precup small fixed size nonetheless
work important precursor
f eature c onstruction

via

pectral nalysis

feature learning frameworks value functions upon spectral analysis state space connectivity presented work mahadevan maggioni petrik
frameworks features eigenvectors connectivity matrices constructed random walk mahadevan maggioni eigenvectors probabilistic transition matrices petrik
features capture aspects long term behaviours opposed short term behaviours
captured bellman error features bellman error reduction requires iteration capture longterm behaviors
reward functions considered feature construction work mahadevan maggioni work petrik reward functions incorporated
learning krylov basis features variant bellman error features parr et al complement eigenvector features however even petriks framework reward incorporated
features used policy evaluation rather controlled environment consider
essential work use machine learning factored representations handle
large statespaces generalize different sizes spectral
analysis frameworks limited respect least current state development petrik presented explicit statespaces factorization
scaling large discrete domains proposed work mahadevan maggioni
features learned dimension factorization independent
dimensions believe assumption independence dimensions inappropriate
many domains including benchmark domains considered work mahadevan maggioni factorization suffers drawbacks propositional
solution recomputed different sizes domain


fiw u g ivan

lacks flexibility generalize different sizes provided relational

structural model model free solution methods markov decision
processes
r elational r einforcement l earning
work dzeroski et al relational reinforcement learning rrl system learns
logical regression trees represent q functions target mdps work related since
use relational representations automatically construct functions capture state value
addition q function trees policy tree learner introduced work dzeroski
et al finds policy trees q function trees learn explicit policy
description instead use greedy policies evaluation
logical expressions rrl regression trees used decision points computing
value function policy rather numerically valued features linear combination
method generalization across sizes achieved learning policy trees learned value
functions apply training sizes date empirical
failed demonstrate ability represent value function usefully familiar
benchmark domains good performance shown simplified goals placing
particular block onto particular block b technique fails capture structure richer
constructing particular arrangements blocksworld towers rrl
entered international competitions difficulties representing complex
relational value functions persist extensions original rrl work driessens dzeroski
driessens et al limited applicability shown benchmark
domains used work
p olicy l earning via b oosting
work kersting driessens boosting introduced incrementally
learn features represent stochastic policies policy iteration variant featurelearning framework clearly differs work policy representations learned instead
value function representations regression tree learner tilde blockeel de raedt
feature learner demonstrated advantages previous rrl work task accomplishing b block applicability simple continuous domain corridor
world demonstrated line rrl work limited applicability benchmark
domains shown one probable source limited applicability model free
reinforcement learning setting system model dynamics explicitly
f itted value teration
gordon presented method value iteration called fitted value iteration suitable
large state spaces require direct feature selection instead method relies
provided kernel function measuring similarity states selection kernel function
viewed kind feature selection kernel identifies state aspects significant
measuring similarity knowledge techniques class applied
large relational evaluated note selection


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

single relational kernel domains would measure state similarity domain independent
manner thus believe kernel could adapt individual domains way
work thus would expect inferior performance however
remains investigated selection domain specific kernels stochastic domains
automatically yet explored
e xact value teration f irst order mdp
previous work used lifted techniques exactly solve first order mdps reformulating exact
solution techniques explicit mdps value iteration boutilier et al holldobler skvortsova independently used two different first order languages situation
calculus fluent calculus respectively define first order mdps works bellman update procedure value iteration reformulated respective calculus resulting
two first order dynamic programming methods symbolic dynamic programming sdp firstorder value iteration fovi simple boxworld example human assisted computation
demonstrated sdp work method serves basis foalp sanner boutilier
replaces exact techniques heuristic approximation order scale techniques
benchmark domains application fovi domains demonstrated
colored blocksworld benchmark limited blocks holldobler karabaev
skvortsova
work kersting et al constraint logic programming used define relational
value iteration method mdp components states actions rewards first abstracted
form markov decision program lifted version mdp relational bellman operation
rebel used define updates q values state values empirical study rebel
limited step backups single predicate goals blocksworld
logistics domains
exact techniques suffer difficulty representing full complexity state value
function arbitrary goals even mildly complex domains previous works serve illustrate central motivation features compactly approximate structure
complex value function thus motivate automatic extraction features studied
work
comparison inductive logic programming
selecting numeric function relational states match bellman error training
set first order regression available systems described
inductive logic programming ilp literature quinlan karalic bratko
important note ilp work studied learning classifiers relational
data muggleton concerned learning numeric functions relational
data states latter called first order regression within ilp literature received less study relational classification choose design
proof concept relational learner experiments rather use one previous
systems separate work needed compare utility relational learner previous
regression systems purpose demonstrate utility bellman error training data
finding decision theoretic value function features simple learner suffices create
state art domain independent via automatic feature selection


fiw u g ivan

ilp classification systems often proceed general specific specific
general seeking concept match training data regression however
easy ordering numeric functions searched design instead method searches
basic logical expression language simple expressions complex expressions seeking
good matches training data order control branching factor still allowing
complex expressions considered heuristically build long expressions
short expressions score best words use beam search space expressions
several heuristic aspects method first define heuristic set basic expressions search begins second define heuristic method combining
expressions build complex expressions two heuristic elements designed
logical formula without disjunction one free variable built repeated combination basic expressions finally assumption high scoring expressions built
high scoring parts heuristic often true critical heuristic assumption
makes likely learner often miss complex features match training data well
known method guarantees tractably finding features
approximate policy iteration relational domains
planners use greedy policies derived learned value functions alternatively one directly learn representations policies policy tree learning work dzeroski et al
discussed previously appendix one example recent work uses relational decision list language learn policies small example generalize well
perform large khardon martin geffner yoon et al due
inductive nature line work however selected policies occasionally contain severe
flaws mechanism provided policy improvement policy improvement quite
challenging due astronomically large highly structured state spaces relational policy
language
work fern et al approximate version policy iteration addressing
issues presented starting base policy approximate policy iteration iteratively generates
training data improved policy policy rollout uses learning
work yoon et al capture improved policy compact decision list language
similar work learner work fern et al aims take flawed
solution structure improve quality conventional mdp techniques case finding
improved policy policy rollout machine learning unlike work work fern
et al improved policies learned form logical decision lists work
viewed complementary previous work exploring structured representation value
functions work explored structured representation policies approaches
likely relevant important long term effort solve structured stochastic decisionmaking
note feature representation considered generally mdp literature used represent value functions rather policies compact representation policies
done via value functions greedy execution directly example decision lists previous api work discussed uses direct representation policies never
uses compact representation value functions instead sampling value functions used
policy evaluation step policy iteration


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

one imagine different novel api compact feature
representation used value functions greedy execution policy representation
feature discovery similar explore value iteration could designed assist policy evaluation phase policy iteration leave development
evaluation idea future work expect two approaches api well
current value iteration advantages disadvantages vary domain
ways yet well understood domains natural compact direct policy
representations run see tarantula whereas others naturally compactly represented
via value functions prefer restaurants good review ratings area must
eventually develop means combine compact representations effectively
automatic extraction domain knowledge
substantial literature learning plan methods direct representation
value function reactive policy especially deterministic literature
techniques related acquire domain specific knowledge via experience domain much literature targets control knowledge particular search
planners estlin mooney kambhampati et al veloso et al distant
focus particular technology used limitation
deterministic domains unclear generalize work value function construction
probabilistic domains
however broader learning plan literature contains work producing declarative
learned domain knowledge could well exploited feature discovery value function representation work fox long pre processing module called tim
able infer useful domain specific specific structures typing objects
state invariants descriptions domain definition initial states invariants
targeted work improving efficiency graphplan planner suggest
future work could exploit invariants discovering features value function representation similarly work gerevini schubert discoplan infers state constraints
domain definition initial state order improve performance sat planners constraints could incorporated feature search method
date

appendix b discussions five probabilistic competition
domains
section presented relational propositional feature learners
b locksworld c onjunctive b oxworld present relational
feature learner following five probabilistic competition domains ireworld
z enotravel e xploding b locksworld owers h anoi l ifted f ileworld
b tireworld
use ireworld domain second ippc agent needs drive vehicle
graph start node goal node moving one node adjacent node
vehicle certain chance suffering flat tire still arriving adjacent node


fiw u g ivan

trial
features









difficulty









success ratio

plan length









accumulated time hr

target size sr
target size slen


















trial
features









difficulty









success ratio
plan length









accumulated time hr
target size sr

target size slen


































figure ireworld performance averaged relational learner add
one feature per column success ratio exceeds average successful plan
length less n n nodes increase difficulty next
column plan lengths shown successful trials difficulties measured
number nodes target size nodes columns omitted
discussed section

flat tire replaced spare tire spare tire present node
containing vehicle vehicle carrying spare tire vehicle pick spare
tire already carrying one one present node containing vehicle
default setting second ippc generator domain defines distribution
includes policy achieving goal probability one
create tradeoff goal achievement probability expected number steps
goal strongly planner favors goal achievement versus short trajectories goal
determined choice discount factor made section
start node relational learner increase n nodes n
nodes whenever success ratio exceeds average successful plan length better
n steps target size nodes shown figures
ireworld relational learner able features generalize well large
learner achieves success ratio node unknown
whether policy exceed success ratio distribution however neither
comparison planner foalp replan finds higher success rate policy
note improvements success rate domain necessarily associated
increases plan length success rate improvements may due path deviations
acquire spare tires


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

trial
features
difficulty
success ratio
plan length
accumulated time hr
target size sr
target size slen

























































































trial
features
difficulty
success ratio
plan length
accumulated time hr
target size sr
target size slen

























































































figure z enotravel performance averaged relational learner
difficulty shown table lists numbers cities travelers aircraft
target size cities travelers aircraft add one feature
per column success ratio exceeds increase difficulty
next column plan lengths shown successful trials

b zenotravel
use z enotravel domain second ippc goal domain fly travelers original location destination planes finite range discrete fuel levels
need fuelled fuel level reaches zero cont inue flying available activity
boarding debarking flying zooming refueling divided two stages activity
x modelled two actions start x finish x finish x activity high probability
nothing start action taken corresponding finish action must taken
repeatedly succeeds conflicting action started structure allows
failure rates finish actions simulate action costs used explicitly
representation competition plane moved locations flying
zooming zooming uses fuel flying higher success probability
start difficulty cities traveler aircraft relational
feature learner whenever success ratio exceeds increase number n travelers
aircraft number cities less n increase number cities one
otherwise target size cities travelers aircraft z enotravel
relational learner shown figures


fiw u g ivan

relational learner unable features enable avi achieve threshold success
rate cities travelers aircraft although relational features learned trials
stopped improvement performance achieved several iterations feature
addition broader search w q able better features
extend solvable size several cities success rate shown
use search parameters reported wu givan runtime
increases dramatically weeks believe speed effectiveness relational learning
needs improved excel domain likely major factor improved knowledge
representation features key concepts z enotravel easily represented
trial two figure shows striking event adding single feature useful value
function value function greedy policy cannot goal
success ratio degrades dramatically immediately note small size
ten percent trivial initial state satisfies goal addition
sixth feature trial two policy solve reflects
unreliability avi weight selection technique aspect feature discovery
avi free assign zero weight feature additional study
control avi replacement avi linear programming methods indicated
phenomenon however rare event extensive experiments
b exploding blocksworld
use e xploding b locksworld second ippc evaluate relational planner
domain differs normal blocksworld largely due blocks certain probability detonated put destroying objects beneath
detonating block blocks already detonated detonated goal
state domain described tower fragments fragments generally required
table destroyed objects cannot picked blocks cannot put destroyed objects destroyed object still part goal necessary relationships
established destroyed
start block relational learner increase n blocks n
blocks whenever success ratio exceeds target sizes blocks
e xploding b locksworld relational learner shown figures
e xploding b locksworld good enough planner increase
difficulty beyond block limited generalization block
little generalization block
performance domain quite weak believe due presence many
dead end states reachable high probability states table
one blocks needed goal destroyed object question achieved
required properties planner meaningful relevant features planner discovers
undesirable destroy table instance however resulting partial understanding domain cannot augmented random walk domains
b locksworld c onjunctive b oxworld enable steady improvement value leading goal random walk domain invariably lands agent dead end short
successful plan length low probability reaching goal shown high unsuccessful plan length caused wandering dead end region suggest need techniques


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

trial
features




difficulty




success ratio

plan length




accumulated time hr

target size sr
target size slen




target size sr



target size slen














trial
features





difficulty





success ratio

plan length





accumulated time hr
target size sr






target size slen
target size sr



target size slen








































































































figure e xploding b locksworld performance averaged relational
learner difficulties measured number blocks add one feature per
column success ratio exceeds increase difficulty next
column plan lengths shown successful trials target size
blocks target size blocks

aimed handling dead end regions handle domain demonstrate technique relies random walk form search learned features need
completely describe desired policy
b towers hanoi
use domain owers h anoi first ippc probabilistic version agent move one two discs simultaneously small probability
going dead end state move probability depends whether largest disc
moved type disc move one two time used note
one size
important note success rate generally unachievable domain due
unavoidable dead end states


fiw u g ivan

trial
features
difficulty
success ratio
plan length
accumulated time hr
target size sr
target size slen
target size sr
target size slen





























































































trial
features
difficulty
success ratio
plan length
accumulated time hr
target size sr
target size slen
target size sr
target size slen







































































































figure owers h anoi performance averaged relational learner
add one feature per column success ratio exceeds n n discs
increase difficulty next column plan lengths shown successful trials
difficulties measured number discs target size
discs size discs columns omitted discussed section

start disc relational learner increase difficulty
n discs n discs whenever success ratio exceeds n target sizes
discs owers h anoi relational learner shown figures
learner clearly able adapt three four disc achieving around
success rate four disc trials optimal solution four disc
success rate policy uses single disc moves large disc moved
uses double disc moves policies use single disc moves double disc moves
achieve success rates respectively four disc learned solution
occasionally moves disc way get closer goal reducing success
unfortunately trials increasing number features needed adapt
larger size trials even total features enough adapt
five disc thus know extend even five discs moreover
indicate poor generalization sizes
believe difficult learner humans represent good value function
across sizes humans deal domain formulating good recursive policy
establishing direct idea value state finding recursive policy automatically
interesting open question outside scope


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

b lifted fileworld
described section use domain l ifted f ileworld straightforwardly
lifted form f ileworld first ippc restricted three folders reach goal filing
files action needs taken file randomly determine folder file
go actions taking folder putting file folder returning folder
cabinet goal reached files correctly filed targeted folders
note f ileworld l ifted f ileworld benign domains
reachable dead ends non optimal actions directly reversible
random walk solves domain success rate one even thirty files technical challenge
posed minimize unnecessary steps minimize plan length optimal policy
solves n file n n steps depending random file types
generated
rather preset plan length threshold increasing difficulty function n
adopt policy increasing difficulty whenever method fails improve plan length adding
features specifically success ratio exceeds one feature added without improving
plan length remove feature increase difficulty instead
start file relational learner increase n files n files
whenever performance improve upon feature addition target size
files l ifted f ileworld relational learner shown figures
planner acquires optimal policy file target size
learning four features two trials domain reveal
weakness avi weight selection method although four features enough define optimal policy difficulty increases avi often fails weight assignment producing
policy happens feature addition triggered trial
domain extra features prevent avi finding good weights
subsequent iterations optimal policy recovered larger feature set nonetheless another indication improved performance may available via work alternative
weight selection approaches orthogonal topic feature selection

references
bacchus f kabanza f temporal logics express search control knowledge
artificial intelligence
bertsekas p dynamic programming optimal control athena scientific
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific
blockeel h de raedt l top induction first order logical decision trees
artificial intelligence
bonet b givan r non deterministic track international
competition website http www ldc usb bonet ipc
possible specify plan length threshold function triggering increase difficulty domain
done domains domain quite sensitive choice function end
must chosen trigger difficulty increase feature addition fruitless current difficulty
directly implemented automatic method triggering difficulty increase



fiw u g ivan

trial
features


difficulty
success ratio


plan length
accumulated time hr
target size sr

target size slen


































































































































trial
features

difficulty

success ratio


plan length
accumulated time hr

target size sr
target size slen





















































































































































figure l ifted f ileworld performance averaged relational
learner add one feature per column success ratio exceeds adding one
extra feature improve plan length increase difficulty
next column removing extra feature plan lengths shown successful trials
difficulties measured number files target size
files columns omitted discussed section

boutilier c reiter r price b symbolic dynamic programming first order mdps
proceedings seventeenth international joint conference artificial intelligence
pp
chandra merlin p optimal implementation conjunctive queries relational data
bases proceedings ninth annual acm symposium theory computing pp

davis r lenat knowledge systems artificial intelligence mcgraw hill
york
driessens k dzeroski integrating guidance relational reinforcement learning
machine learning
driessens k ramon j gartner graph kernels gaussian processes relational
reinforcement learning machine learning
dzeroski deraedt l driessens k relational reinforcement learning machine
learning
dzeroski todorovski l urbancic handling real numbers ilp step towards
better behavioural clones proceedings eighth european conference machine
learning pp


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

estlin mooney r j learning improve efficiency quality
proceedings fifteenth international joint conference artificial intelligence pp

fahlman lebiere c cascade correlation learning architecture advances
neural information processing systems pp
farias v f van roy b tetris study randomized constraint sampling probabilistic randomized methods design uncertainty springer verlag
fawcett knowledge feature discovery evaluation functions computational
intelligence
fern yoon givan r learning domain specific control knowledge random
walks proceedings fourteenth international conference automated
scheduling pp
fern yoon givan r approximate policy iteration policy language bias
solving relational markov decision processes journal artificial intelligence

fox long automatic inference state invariants tim journal artificial
intelligence
gerevini schubert l inferring state constraints domain independent
proceedings fifteenth national conference artificial intelligence pp
gordon g stable function approximation dynamic programming proceedings
twelfth international conference machine learning pp
gretton c thiebaux exploiting first order regression inductive policy selection
proceedings twentieth conference uncertainty artificial intelligence pp

guestrin c koller parr r max norm projections factored mdps proceedings seventeenth international joint conference artificial intelligence pp
holldobler karabaev e skvortsova flucap heuristic search planner
first order mdps journal artificial intelligence
holldobler skvortsova logic dynamic programming
proceedings workshop learning markov processesadvances
challenges nineteenth national conference artificial intelligence pp
kakade natural policy gradient advances neural information processing
systems pp
kambhampati katukam qu failure driven dynamic search control partial
order planners explanation artificial intelligence
karalic bratko first order regression machine learning
keller p mannor precup automatic basis function construction approximate dynamic programming reinforcement learning proceedings twenty third
international conference machine learning pp


fiw u g ivan

kersting k van otterlo de raedt l bellman goes relational proceedings
twenty first international conference machine learning pp
kersting k driessens k non parametric policy gradients unified treatment
propositional relational domains proceedings twenty fifth international conference machine learning pp
khardon r learning action strategies domains artificial intelligence
lagoudakis g parr r littman l least squares methods reinforcement
learning control setn proceedings second hellenic conference ai pp

mahadevan maggioni proto value functions laplacian framework learning representation control markov decision processes journal machine learning

martin geffner h learning generalized policies examples
concept languages applied intelligence
mitchell machine learning mcgraw hill
muggleton inductive logic programming generation computing
parr r li l taylor g painter wakefield c littman analysis linear
linear value function approximation feature selection reinforcement learning
proceedings twenty fifth international conference machine learning pp

parr r painter wakefield c li l littman analyzing feature generation
value function approximation proceedings twenty fourth international conference
machine learning pp
patrascu r poupart p schuurmans boutilier c guestrin c greedy linear valueapproximation factored markov decision processes proceedings eighteenth
national conference artificial intelligence pp
petrik analysis laplacian methods value function approximation mdps
proceedings twentith international joint conference artificial intelligence pp

quinlan j r c programs machine learning morgan kaufmann
quinlan j r learning first order definitions functions journal artificial intelligence

rivest f precup combining td learning cascade correlation networks
proceedings twentieth international conference machine learning pp
sanner boutilier c practical linear value approximation techniques first order
mdps proceedings twenty second conference uncertainty artificial intelligence pp
sanner boutilier c practical solution techniques first order mdps artificial
intelligence


fiautomatic nduction b ellman e rror f eatures p robabilistic p lanning

singh jaakkola littman szepesvari c convergence single step
policy reinforcement learning machine learning
sutton r learning predict methods temporal differences machine learning

sutton r barto g reinforcement learning introduction mit press
szita lorincz learning tetris noisy cross entropy method neural
computation
tesauro g temporal difference learning td gammon communications acm

tsitsiklis j roy b v analysis temporal difference learning function approximation ieee transactions automatic control
utgoff p e precup relative value function approximation tech rep university
massachusetts department computer science
utgoff p e precup constuctive function approximation motoda liu eds
feature extraction construction selection data mining perspective pp
kluwer
veloso carbonell j perez borrajo fink e blythe j integrating
learning prodigy architecture journal experimental theoretical ai

widrow b hoff jr e adaptive switching circuits ire wescon convention
record
williams r j baird l c tight performance bounds greedy policies
imperfect value functions tech rep northeastern university
wu j givan r discovering relational domain features probabilistic
proceedings seventeenth international conference automated
scheduling pp
wu j kalyanam r givan r stochastic enforced hill climbing proceedings
eighteenth international conference automated scheduling pp
wu j givan r feature discovering approximate value iteration methods proceedings symposium abstraction reformulation approximation pp
yoon fern givan r inductive policy selection first order mdps proceedings eighteenth conference uncertainty artificial intelligence pp
yoon fern givan r replan baseline probabilistic proceedings seventeenth international conference automated scheduling pp
younes h littman weissman asmuth j first probabilistic track
international competition journal artificial intelligence




