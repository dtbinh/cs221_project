Journal Artificial Intelligence Research 38 (2010) 687-755

Submitted 01/10; published 08/10

Automatic Induction Bellman-Error Features
Probabilistic Planning
Jia-Hong Wu
Robert Givan

JW @ ALUMNI . PURDUE . EDU
GIVAN @ PURDUE . EDU

Electrical Computer Engineering
Purdue University, W. Lafayette, 47907 USA

Abstract
Domain-specific features important representing problem structure throughout machine
learning decision-theoretic planning. planning, state features provided, domainindependent algorithms approximate value iteration learn weighted combinations
features often perform well heuristic estimates state value (e.g., distance
goal). Successful applications real-world domains often require features crafted human experts. Here, propose automatic processes learning useful domain-specific feature sets
little human intervention. methods select add features describe state-space regions high inconsistency Bellman equation (statewise Bellman error) approximate
value iteration. method applied using real-valued-feature hypothesis space
corresponding learning method selecting features training sets state-value pairs.
evaluate method hypothesis spaces defined relational propositional feature
languages, using nine probabilistic planning domains. show approximate value iteration
using relational feature space performs state-of-the-art domain-independent stochastic
relational planning. method provides first domain-independent approach plays Tetris
successfully (without human-engineered features).

1. Introduction
substantial gap performance domain-independent planners domainspecific planners. Domain-specific human input able produce effective planners
competition planning domains well many game applications backgammon, chess,
Tetris. deterministic planning, work TLPLAN (Bacchus & Kabanza, 2000) shown
simple depth-first search domain-specific human input, form temporal logic formulas
describing acceptable paths, yields effective planner wide variety competition domains.
stochastic planning, feature-based value-function representations used humanselected features great success applications backgammon (Sutton & Barto, 1998;
Tesauro, 1995) Tetris (Bertsekas & Tsitsiklis, 1996). usage features provided human experts often critical success systems using value-function approximations.
Here, consider problem automating transition domain-independent planning
domain-specific performance, replacing human input automatically learned domain properties. thus study style planner learns encountering problem instances improve
performance subsequently encountered problem instances domain.
focus stochastic planning using machine-learned value functions represented linear
combinations state-space features. goal augment state-space representation
c
2010
AI Access Foundation. rights reserved.

fiW U & G IVAN

planning new machine-discovered features facilitate accurate representation
value function. resulting learned features used representing value function
problem instances domain, allowing amortization learning costs across
solution multiple problem instances. Note property contrast competition
planners, especially deterministic planning, retain useful information problem instances. Thus, approach solving planning problems regarded automatically
constructing domain-specific planners, using domain-independent techniques.
learn features correlate well statewise Bellman error value functions encountered planning, using provided feature language corresponding learner select
features space. evaluate approach using relational propositional feature
spaces. recent approaches acquiring features stochastic planning substantial differences approach discuss detail Section 5 (Patrascu, Poupart,
Schuurmans, Boutilier, & Guestrin, 2002; Gretton & Thiebaux, 2004; Sanner & Boutilier, 2009;
Keller, Mannor, & Precup, 2006; Parr, Painter-Wakefield, Li, & Littman, 2007). previous work
evaluated selection relational features correlation statewise Bellman error.
Recent theoretical results (Parr et al., 2007) uncontrolled Markov processes show exactly capturing statewise Bellman error new features, repeatedly, lead convergence
uncontrolled optimal value value function selected linear-fixed-point methods weight
training. Unfortunately machine-learning approaches selecting features, results
transferred approximations statewise Bellman-error features: case, results
work Parr et al. (2007) weaker imply convergence. Also, none theory transferred controlled case interest here, analysis much
difficult effective (greedy) policy consideration value-function training
changing.
consider controlled case, known theoretical properties similar Parr
et al. (2007) shown. Lacking theory, purpose demonstrate capability
statewise Bellman error features empirically, rich representations require machine
learning techniques lack approximation guarantees. Next, give overview approach, introducing Markov decision processes, value functions, Bellman error, feature hypothesis
languages feature learning methods.
use Markov decision processes (MDPs) model stochastic planning problems. MDP
formal model single agent facing sequence action choices pre-defined action space,
transitioning within pre-defined state space. assume underlying stationary
stochastic transition model available action state transitions occur according
agents action choices. agent receives reward action choice according state
visited (and possibly action chosen), objective accumulating much reward
possible (possibly favoring reward received sooner, using discounting, averaging time,
requiring reward received finite horizon).
MDP solutions represented state-value functions assigning real numbers states. Informally, MDP solution techniques, desire value function respects action transitions
good states either large immediate rewards actions available lead
good states; well-known property formalized Bellman equations recursively
characterize optimal value function (see Section 2). degree given value function
fails respect action transitions way, formalized next section, referred
Bellman error value function, computed state.
688

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Intuitively, statewise Bellman error high magnitude regions state space
appear undervalued (or overvalued) relative action choices available. state high
Bellman error locally inconsistent value function; example, state inconsistently labeled
low value action available leads high-value states. approach
use machine learning fit new features regions local inconsistency current value
function. fit perfect, new features guarantee represent Bellman update
current value function. Repeated Bellman updates, called value iteration, known
converge optimal value function. add learned features representation
train improved value function, adding new features available feature set.
method learning new features using approximate value function
regarded boosting-style learning approach. linear combination features
viewed weighted combination ensemble simple hypotheses. new feature learned
viewed simple hypothesis selected match training distribution focused regions
previous ensemble getting wrong (as reflected high statewise Bellman error throughout
region). Growth ensemble sequentially adding simple hypotheses selected correct
error ensemble far refer boosting style learning.
important note method scores candidate features correlation statewise
Bellman error current value function, minimizing statewise Bellman error
value function found using new candidate feature. pre-feature-addition scoring much
less expensive scoring involves retraining weights new feature, especially
repeated many times different candidates, relative current value function.
use pre-feature-addition scoring select features controlled setting enables much
aggressive search new features previously evaluated post-feature-addition approach
discussed work Patrascu et al. (2002).
approach considered selecting features feature-description language
learning method exists effectively select features match state-value training data.
consider two different feature languages empirical evaluation. Human-constructed
features typically compactly described using relational language (such English) wherein
feature value determined relations objects domain. Likewise, consider
relational feature language, based domain predicates basic domain description. (The
domain description may written, example, standard planning language PPDDL
Younes, Littman, Weissman, & Asmuth, 2005.) Here, take logical formulas one free variable
represent features count number true instantiations formula state
evaluated. example, number holes feature used many Tetris experiments
(Bertsekas & Tsitsiklis, 1996; Driessens, Ramon, & Gartner, 2006) interpreted counting
number empty squares board filled squares them.
numeric features provide mapping states natural numbers.
addition relational feature language, consider using propositional feature representation learning structure. Although propositional representation less expressive
relational one, exist effective off-the-shelf learning packages utilize propositional representations. Indeed, show reformulate feature learning task
related classification problem, use standard classification tool, decision-tree learner C4.5
(Quinlan, 1993), create binary-valued features. reformulation classification considers
sign, magnitude, statewise Bellman error, attempting learn features
characterize positive-sign regions state space (or likewise negative-sign regions).
689

fiW U & G IVAN

standard supervised classification problem thus formulated C4.5 applied generate
decision-tree feature, use new feature value-function representation.
propositional approach easier implement may attractive relational one
obvious advantage using relational representation, computing exact
statewise Bellman error state significantly expensive estimating sign.
experiments, however, find relational approach produces superior results
propositional learner. relational approach demonstrates ability generalize features
problem sizes domain, asset unavailable propositional representations.
present experiments nine domains. experiment starts single, constant feature, mapping states number, forcing constant value function makes
distinctions states. learn domain-specific features weights automatically
generated sampled state trajectories, adjusting weights new feature added.
evaluate performance policies select actions greedily relative learned value
functions. evaluate learners using stochastic computer-game Tetris seven planning domains two international probabilistic planning competitions (Younes et al., 2005;
Bonet & Givan, 2006). method provides first domain-independent approach playing
Tetris successfully (without human-engineered features). relational learner demonstrates
superior success ratio probabilistic planning-competition domains compared
propositional approach probabilistic planners FF-Replan (Yoon, Fern, & Givan, 2007)
FOALP (Sanner & Boutilier, 2006, 2009). Additionally, show propositional learner
outperforms work Patrascu et al. (2002) SysAdmin domain evaluated there.

2. Background
present relevant background use Markov Decision Processes planning.
2.1 Markov Decision Processes
define terminology Markov decision processes. thorough discussion
Markov decision processes, see books Bertsekas Tsitsiklis (1996) Sutton Barto
(1998). Markov decision process (MDP) tuple (S, A, R, T, s0 ). Here, finite state
space containing initial state s0 , selects non-empty finite available action set A(s)
state S. reward function R assigns real reward state-action-state triple (s, a, )
action enabled state s, i.e., A(s). transition probability function maps
state-action pairs (s, a) probability distributions S, P(S), A(s).
Given discount factor 0 < 1 policy mapping state action A(s),
value function V (s) gives expected discounted reward obtained state selecting action
(s) state encountered discounting future rewards factor per time step.

least one optimal policy V (s), abbreviated V (s), less V (s)
every state s, policy . following Q function evaluates action respect
future-value function V ,
X
Q(s, a, V ) =
(s, a, )[R(s, a, ) + V (s )].


Recursive Bellman equations use Q() describe V V follows. First, V (s) =
Q(s, (s), V ). Then, V (s) = maxaA(s) Q(s, a, V ). using Q(), select ac690

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

tion greedily relative value function. policy Greedy(V ) selects, state s, action
arg maxaA(s) Q(s, a, V ).
Value iteration iterates operation
U(V )(s) = max

aA(s)

X

(s, a, )[R(s, a, ) + V (s )],



computing Bellman update U(V ) V , producing sequence value functions converging
sup-norm V , regardless initial V used.
define statewise Bellman error B(V, s) value function V state
U(V )(s) V (s). inducing new features based correlation statewise
Bellman error, based sign statewise Bellman error. sup-norm distance
value function V optimal value function V bounded using Bellman error magnitude, defined maxsS |B(V, s)| (e.g., see Williams & Baird, 1993). use term
statewise Bellman error emphasize distinction widely used sup-norm Bellman
error.
note computing U(V ), thus statewise Bellman error, involve summation
entire state space, whereas fundamental motivations require avoiding summations.
many MDP problems interest, transition matrix sparse way set states
reachable one step non-zero probability small, current state. problems,
statewise Bellman error computed effectively using appropriate representation .
generally, sparse manner, sum effectively approximately evaluated
sampling next states according distribution represented .
2.2 Modeling Goal-oriented Problems
Stochastic planning problems goal-oriented, objective solving problem
guide agent toward designated state region (i.e., goal region). model problems
structuring reward transition functions R action goal state leads
positive reward zero-reward absorbing state, reward zero everywhere else.
retain discounting represent preference shorter paths goal. Alternatively,
problems modeled stochastic shortest path MDPs without discounting (Bertsekas, 1995).
techniques easily generalized formalisms allow varying action costs well,
model variation work.
formally, define goal-oriented MDP MDP meeting following constraints. Here, use variables states actions A(s). require
contain zero-reward absorbing state , i.e., R(, a, s) = 0 (, a, ) = 1
a. transition function must assign either one zero triples (s, a, ), call
region states (s, a, ) one goal region. reward function constrained
R(s, a, ) zero unless = . constructing goal-oriented MDPs problem
representations, may introduce dummy actions carry transitions involving described
here.
2.3 Compactly Represented MDPs
work, consider propositional relational state representations.
691

fiW U & G IVAN

relational MDPs, spaces A(s) relationally represented, i.e.,
finite set objects O, state predicates P , action names N used define spaces
follows. state fact application p(o1 , . . . , ) n-argument state predicate p object
arguments oi , n1 . state set state facts, representing exactly true facts
state. action instance a(o1 , . . . , oS
n ) application n-argument action name n objects
oi , n. action space = sS A(s) set action instances.
MDPs compactly represented state action spaces use compact representations
transition reward functions. One compact representation PPDDL planning
language, informally discussed next subsection formally presented work Younes
et al. (2005).
propositional problems, action space explicitly specified state space compactly specified providing finite sequence basic state properties called state attributes,
Boolean, integer, real values. propositional state vector values state
attributes.
Given relational MDP, equivalent propositional MDP easily constructed grounding, explicit action space constructed forming action-name applications
set state attributes computed forming state-predicate applications, thus removing use
set objects representation.
2.4 Representing PPDDL Planning Problems using MDPs
discuss represent goal-oriented stochastic planning problems defined standardized
planning languages PPDDL (Younes et al., 2005) goal-oriented MDPs. limit
focus problems goal regions described (conjunctive) sets state facts.
reference follow approach used work Fern, Yoon, Givan (2006) regarding
converting planning problems compactly represented MDPs manner facilitates generalization problem instances. first discuss several difficult representational issues
finally pull discussion together formal definition MDP analyze represent
given PPDDL problem instance. consider quantified and/or disjunctive goals,
handling goals would interesting useful extension work.
2.4.1 P LANNING OMAINS



P ROBLEMS

planning domain distribution problem instances sharing state predicates PW ,
action names N , action definitions. Actions take objects parameters, defined
giving discrete finite probability distributions action outcomes, specified
using add delete lists state facts action parameters.
Given domain definition, problem instance domain specifies finite object set O,
initial state si goal condition G. initial state given set state facts goal
condition given conjunction state facts, constructed predicates PW .
1. state predicate associated arity indicating number objects relates. state predicate
applied number objects domain form ground state fact either true false
state; states different possible ways select true state facts. Likewise, action name
associated arity natural number indicating number objects action act upon. action name
applied number objects form grounded action.

692

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

2.4.2 PPDDL R EPRESENTATION
PPDDL standard planning language international probabilistic planning competitions.
PPDDL, planning domain syntax planning problem syntax defined. completely
define planning instance, one specify domain definition problem definition using
respective syntax. Conditional effects quantified preconditions allowed domain
definition.
planning competitions, customary specify planning domains providing problem generators accept size parameters input output PPDDL problem instances.
generators thus specify size-parameterized planning domains. important note, however, problem generators provided recent planning competitions specify planning
domains according definition used here. particular, problem generators vary
action set state predicates instances generated. relationship
different problem instances generated generators much looser required
definition, domains somewhat arbitrary collections planning
problems.
logical language allows generalization problems problems
share state action language, limit empirical evaluation Section 7 domains
provided problem generators specify planning domains defined here,
i.e., without varying action definitions instances (or easily code
generator). refer domains generators planning domains fixed action
definitions.
2.4.3 G ENERALIZATION B ETWEEN P ROBLEMS VARYING IZE
object set varies size, without bound, across problem instances domain,
infinitely many possible states within different instances single domain. MDP
analyze finite state space, model planning domain infinite set MDPs
seeking good policy (in form good value function), one MDP
problem instance2 .
value function infinite set MDPs mapping disjoint union state
spaces MDPs real numbers. value function used greedily policy
MDPs set. However, explicit representation value function would
infinite size. Here, use knowledge representation techniques compactly represent
value functions infinite set problem instance MDPs given planning domain.
compact representation derives generalization across domains, approach fundamentally finding good generalizations MDPs within single planning domain.
representation value functions planning domains given Sections 2.5 4.
section, discuss represent single finite MDP single planning problem
instance. However, note objective work find good value functions
infinite collections MDPs represent planning domains. Throughout paper,
assume planning domain provided along means sampling example problems
domain, sampling parameterized difficulty (generally, problem size)
2. paper consider two candidate representations features; one these, relational representation,
capable generalizing problem sizes. propositional representation, restrict training
testing problem instances size.

693

fiW U & G IVAN

easy example problems selected. Although, PPDDL provide problem
distributions, benchmark planning domains often provided problem generators defining
distributions: generators available, use them, otherwise code
distributions problem instances.
2.4.4 G ENERALIZING B ETWEEN P ROBLEMS



VARYING G OALS

facilitate generalization problem instances different goals, following work
Martin Geffner (2004) Fern et al. (2006), translate PPDDL instance description
MDP state specifies true state goal is. Action
transitions MDP never change goal, presence goal within state
description allows value functions (that defined conditioning state) depend
goal well. goal region MDP simply MDP states specified
current state information matches specified goal information.
Formally, translating PPDDL problem instances compact MDPs, enrich given set
world-state predicates PW adding copy predicate indicating desired state
predicate. name goal-description copy predicate p prepending word goal-
name. set goal-description copies predicates PW denoted PG , take
PW PG state predicates MDP corresponding planning instance. Intuitively,
presence goal-p(a,b) state indicates goal condition requires fact p(a, b)
part world state. use goal predicates constructing compact MDP
PPDDL description constructing initial state, goal conditions true
goal predicates.
use domain Blocksworld example illustrate reformulation (the
domain used example Fern et al., 2006). goal condition Blocksworld
problem described conjunction ground on-top-of facts. world-state predicate
on-top-of PW . discussed above, implies predicate goal-on-top-of PG .
Intuitively, one ground instance predicate, goal-on-top-of(b1,b2), means state
goal region, block b1 directly top block b2.
2.4.5 TATES



AVAILABLE ACTIONS

PPDDL allows definition domains states meet preconditions
action applied. However, MDP formalism requires least one available action every
state. translating PPDDL problem instance MDP define action transitions
action taken dead state transitions deterministically absorbing state.
consider states undesirable plan trajectories, give added transitions
reward negative one unless source state goal state.
2.4.6 R ESULTING MDP
pull together elements formally describe MDP = (S, A, R, T, s0 )
given PPDDL planning problem instance. discussed Section 2.3, set defined
specifying predicates objects available. PPDDL description specifies sets N
action names objects, well set PW world predicates. construct enriched
set P = PW PG state predicates define state space sets applications
predicates objects O. set A(s) state set PPDDL action instances built
694

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

N satisfies preconditions, except set empty, A(s) set
PPDDL action instances built N O. latter case, say state dead.
reward function R defined discussed previously Section 2.2; i.e., R(s, a, ) = 1
goal condition G true s, R(s, a, ) = 1 non-goal dead state, zero otherwise.
define (s, a, ) according semantics PPDDL augmented semantics
Section 2.2T (s, a, ) one satisfies G, dead, = , zero otherwise.3
Transiting one state another never changes goal condition description states given
predicates PG . MDP initial state s0 PPDDL problem initial state si augmented
goal condition G using goal predicates PG . propositional representation
desired, easily constructed directly relational representation grounding.
2.5 Linear Approximation Value Functions
many previous authors done (Patrascu et al., 2002; Sanner & Boutilier, 2009; Bertsekas &
Tsitsiklis, 1996; Tesauro, 1995; Tsitsiklis & Roy, 1997), address large compactly represented and/or implicitly representing value functions terms state-space features
f : R. features f must select real value state. describe two approaches
representing selecting features Section 4.
Recall Section 1 goal learn value function family related MDP
problems. assume state-space features defined across union state spaces
family.
represent
value functions using linear combination l features extracted s, i.e.,
P
V (s) = li=0 wi (s), f0 (s) = 1. goal find features (each mapping states real
values) weights wi V closely approximates V . Note single set features
weight vector defines value function MDPs features defined.
Various methods proposed select weights wi linear approximations (see, e.g.,
Sutton, 1988 Widrow & Hoff, 1960). Here, review use trajectory-based approximate
value iteration (AVI) approach. training methods easily substituted. AVI constructs
1
2

finite sequence value functions
one. value function
Pl V , V , . . . , V , returns last

represented V (s) = i=0 wi (s). determine weights wi+1 V , draw set
training states s1 , s2 , . . . , sn following policy Greedy(V ) different example problems
sampled provided problem distribution current level problem difficulty. (See
Section 3 discussion control problem difficulty.) number trajectories drawn
maximum length trajectory parameters AVI method. training state s,
compute Bellman update U(V )(s) MDP model problem instance.
compute wi+1 training states using
wi+1 = wi +

1 X
(sj )(U(V )(sj ) V (sj )),
ni

(1)

j

learning rate ni number states s1 , s2 , . . . , sn (s)
non-zero. Weight updates using weight-update formula descend gradient L2 distance
V U(V ) training states, features first rescaled normalize
3. Note according definitions Section 2.2, dead states technically goal states,
negative rewards.

695

fiW U & G IVAN

effective learning rate correct feature values rare occurrence training set.4 Pseudocode AVI method drawing training sets following policy available Online
Appendix 1 (available JAIR website), page 2.
Here, use greedy policy draw training examples order focus improvement
relevant states. state distributions generated biased current
policy; particular, another option worth considering, especially feature learning stuck, would
long random walk distribution discussed work Fern, Yoon, Givan (2004).
leave detailed exploration issue future work. substantial discussion
issues arise selecting training distribution, please see book Sutton Barto (1998).
worth noting on-policy training shown converge optimal value function
closely related reinforcement learning setting using SARSA algorithm (Singh, Jaakkola,
Littman, & Szepesvari, 2000).
general, AVI often gives excellent practical results, greedy gradient-descent
method environment convex due maximization operation Bellman error
function. such, guarantee quality weight vector found, even case
convergence. Convergence guaranteed, and, experiments, divergent weight
training fact problem required handling. note feature-discovery methods
used weight-selection algorithms approximate linear programming,
properties AVI undesirable application.
implemented small modifications basic weight update rule order use AVI
effectively setting; described Section 5 Online Appendix 1 (available JAIR
website).

3. Feature-Discovering Value-function Construction
planning, state features provided, domain-independent algorithms AVI learn
weighted combinations features often perform well heuristic estimates state value
(e.g., distance goal). describe methods select add features describe
state-space regions high inconsistency Bellman equation (statewise Bellman error)
approximate value iteration. methods applied using real-valued-feature hypothesis
space corresponding learning method selecting features match real-valued function
training set states. Here, use learner select features match statewise
Bellman error function.
noted above, use boosting style learning approach finding value functions, iterating
selecting weights generating new features focusing Bellman error
current value function. value function representation viewed weighted ensemble
single-feature hypotheses. start value function trivial feature, constant
feature always returning value one, initial weight zero. iteratively retrain
weights select new features matching regions states current weighted ensemble
high statewise Bellman error.
take learning small problems approach learn features first problems
relatively lower difficulty, increase problem difficulty time, discussed below. Lower
difficulty problems typically smaller state spaces and/or shorter paths positive
4. deriving gradient-descent weight-update formula, feature scaled ri =

696

q

n
,
ni

giving = ri .

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

r
Initial feature vector
r
Initial weight vector w
Initial problem difficulty

Difficulty
target level
time?

Yes

r
Final r
w



Increase problem
difficulty
r D. Keep
.

Learn new feature
correlating Bellman
error states
training
r set, add
. Keep current
problem difficulty D.

r
w

r

Select w approximately
minimizing
error
r Bellman
r
V = w

Done

Reweighted value
r r
function V = w

Yes

Performance
current difficulty
meets threshold?



Generate feature
training set

Figure 1: Control flow feature learning. Boxes double borders represent assumed subroutines method. assume problem distribution parameterized
problem difficulty (such problem size).

feedback (e.g. goal states). Learning initially difficult problems typically lead
inability find positive feedback random-walk behavior; result learning first lower
difficulty problems found effective (Martin & Geffner, 2004; Yoon, Fern, & Givan,
2002). show experimentally Section 7 good value functions high difficulty problems
indeed learned fashion problems lower, increasing difficulties.
approach relies two assumed subroutines, instantiated different ways
providing different algorithms subroutines. First, method weight selection assumed;
method takes input problem domain fixed set features, selects weight vector
value function problem domain using provided features. intend method
heuristically approximately minimize L Bellman error choice weight vector,
practice may easier adjust weights approximate L2 Bellman error. Second, feature
hypothesis space corresponding learner assumed provided system designer.
control flow approach shown Figure 1. iteration fixed problem
distribution selects weights current feature set (using method attempting minimize
L Bellman error) define new value function V , selects training set states feature
learning, learns new feature correlating well statewise Bellman error V , adding
feature feature set. user-provided performance-threshold function detects
increase problem difficulty. formalization control flow given Figure 2, form
pseudo-code.
697

fiW U & G IVAN

Feature-discovering Value-function Construction



Inputs:
Initial feature vector 0 , initial weight vector
w 0,
Sequence problem distributions D1 , D2 , , Dmax increasing difficulty,
Performance threshold function .
// (D, V ) tests performance value function V distribution D.



Outputs:
Feature vector , weight vector
w





0,
w
w 0, 1

1.

(d > max time)




Select
w approximately minimizing Bellman error V =
w Dd



(Dd ,
w )

2.
3.
4.
5.

+ 1

6.

else
Generate sequence training states using Dd

7.
8.
9.
10.




Learn new feature f correlating Bellman error feature B(
w , )
states






( ; f ),
w (
w ; 0)


return ,
w

Notes:
1. B(, ) statewise-Bellman error function, defined Section 2.1.
2. code approximate value iteration AVI, shown Online Appendix 1 (available JAIR website)
page 2, example implementation line 3.



3. code draw(Greedy(
w ), N
), shown Online Appendix 1 page 2, example impletraining

mentation line 7. Ntraining number states feature training set. Duplicated states removed
specified Section 3.1.



4. beam-search code learning relational features beam-search-learn(score(, T, B(
w , )))
example implementation line 8, beam-search-learn shown figure 3 Section 3, score
defined Section 4.2.

Figure 2: Pseudo-code learning set features.
experiments reported Section 7, evaluate following choices assumed
subroutines. experiments use AVI select weights feature sets. evaluate two
choices feature hypothesis space corresponding learner, one relational one propositional, described Section 4.
Separate training sets drawn weight selection feature learning; former
depend weight selection method, described AVI Section 2.5, latter
described section.
Problem difficulty increased sampled performance greedy policy current
difficulty exceeds user-specified performance thresholds. planning-domain experiments,
698

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

performance parameters measured success ratio (percentage trials find goal) average successful plan length (the average number steps goal among successful trials).
non-goal-oriented domains Tetris SysAdmin use different performance measures: average total reward Tetris Bellman error SysAdmin (to facilitate comparison Patrascu
et al., 2002).
assume user-provided schedule problem difficulty increases problems
difficulty parameterized one parameter (e.g., size may measured number
objects type); domain-independent automation increase difficulty
topic future research. give difficulty-increase schedules performance thresholds
experiments section presenting experiments, Section 7.
3.1 Training Set Generation
training set selection new feature set states. training set constructed
repeatedly sampling example problem instance problem distribution current level
difficulty, applying current greedy policy Greedy(V ) problem instance create
trajectory states encountered. Every state (removing duplicates) encountered added
training set. size feature-selection training set maximum length training
trajectory specified user parameters algorithm.
Retaining duplicate states training set another option considered. preliminary empirical results favored option, certainly worth exploration.
note goal finding near-optimal value function necessarily make reference
state distribution: widely used notion near-optimal theory MDPs
sup-norm distance V . Moreover, state distribution represented duplicates
training sets typically distribution badly flawed policy; heeding distribution
prevent correcting Bellman error critical states visited policy, visited
rarely. (These states may be, instance, rarely visited good exits visited state region
misunderstood current value function.) point, primary justification
removing duplicates empirical performance demonstrated Section 7.
Similar reasoning would suggest removing duplicate states training sets AVI weight
training, described Section 2.5. many large AVI training sets generated
experiments, duplicate removal must carefully handled control runtime; historical reasons,
experiments shown include duplicate removal AVI.
possible problem occurs current greedy policy cannot reach enough states complete desired training set. 200 consecutive trajectories drawn without visiting new state
desired training set size reached, process modified follows. point,
method attempts complete training set drawing trajectories using random walk (again
using sampled example problems current problem distribution). process leads
200 consecutive trajectories without new state, method terminates training-set generation
uses current training set even though smaller target size.
3.2 Applicability Method
Feature-discovering value-function construction described require complete access
underlying MDP model. AVI updates training set generation based
following computations model:
699

fiW U & G IVAN

1. Given state ability compute action set A(s).
2. Given state s, action A(s), value function V , ability compute Q-value
Q(s, a, V ).
3. Given state action A(s), ability draw state next state distribution
defined (s, a, ).
4. Given state s, ability compute features selected feature language
computations state required selected feature learner. examples,
(a) Section 4, introduce relational feature language learner require knowledge set domain predicates (and arities) state conjunctive
set predicate facts (see Section 2.3),
(b) and, Section 4, describe propositional feature language learner
require knowledge set propositional state attributes state truth
assignment attributes.
first three items enable computation Bellman update last item enables
computation estimated value function given weights features defining well
selection new features feature learner. requirements amount substantial access
problem model; result method must considered model-based technique.
consequence requirements algorithm cannot directly applied
standard reinforcement learning setting model access via acting world
without ability reset selected states; setting Bellman error computations particular
states cannot necessarily carried out. would possible construct noisy Bellman error
training set model-free setting would appropriate future work explore use
training set feature learning.
PPDDL planning domains studied provide information needed perform
computations, method applies domains natural represent PPDDL.
analyzed method computations implemented. instance,
Tetris experiments Section 7.2, underlying model represented providing hand-coded
routines computations within domain.
3.3 Analysis
MDP value iteration guaranteed converge optimal value function conducted
tabular value-function representation presence discounting (Bertsekas, 1995). Although
weight selection AVI designed mimic value iteration, avoiding tabular representation,
general guarantee weight updates track value iteration thus converge
optimal value function. particular, may weighted combination features
represents optimal value function, likewise none represents Bellman update U(V )
value function V produced AVI weight training process. learning system introduces
new features existing feature ensemble response problem: training set used
select new feature pairs states statewise Bellman error. learned feature exactly
captures statewise Bellman-error concept (by exactly capturing training set generalizing
700

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

successfully) new feature space contain Bellman update value function used
generate training data.
aim find features approximate Bellman error feature, take
function mapping states statewise Bellman error. Theoretical properties Bellman error
features uncontrolled Markov processes (i.e., without max operator Bellman equation) recently discussed work Parr et al. (2007), addition
features (or close approximations thereof) proven reduce weighted L2 -norm distance best weight setting true (uncontrolled) value V , linear fixed-point
methods used train weights feature addition. Prior work (in Wu & Givan,
2005), parallel it, empirically exploring effects selecting Bellman
error features complex controlled case, leading results reported here.
clear simply add Bellman error feature directly, set corresponding weight one, resulting value function would desired Bellman update U(V )
current value function V . Adding features iteration would thus give us way
conduct value iteration exactly, without enumerating states. added feature would
describe Bellman error value function defined terms previously added features, posing
serious computational cost issue evaluating added features. particular, Bellman
error feature value function V estimated particular state high confidence
evaluating value function V state polynomial-sized sample next states
action (based Chernoff bounds).
However, value function V based upon previously added Bellman-error feature,
evaluation V requires sampling (again, possible action) compute.
manner, amount sampling needed high confidence grows exponentially number
successive added features type. levels sampling collapse one expectation
intervening choices actions, often case decision-theoretic sampling.
feature selection method attempt tractably approximate exact value iteration method
learning concise efficiently computable descriptions Bellman-error feature
iteration.
method thus viewed heuristic approximation exact value iteration. Exact
value iteration instance method obtained using explicit state-value table
feature representation generating training sets feature learning containing states
obtain exact value iteration would omit AVI training instead set weight one.
feature language learner shown approximate explicit features tightly
enough (so resulting approximate Bellman update contraction L norm),
easy prove tightening approximations V result weights set one. However,
practical results experiments, use feature representations learners
approximation bound relative explicit features known.

4. Two Candidate Hypothesis Spaces Features
section describe two hypothesis spaces features, relational feature space
propositional feature space, along respective feature learning methods.
two feature spaces, assume learner provided training set states paired
statewise Bellman error values.
701

fiW U & G IVAN

Note two feature-space-learner pairs lead two instances general method
others easily defined defining new feature spaces corresponding learners.
paper empirically evaluate two instances presented here.
4.1 Relational Features
relational MDP defined terms set state predicates. state predicates basic
elements define feature-representation language. Below, define generalpurpose means enriching basic set state predicates. resulting enriched predicates
used predicate symbols standard first-order predicate logic. consider
formula logic one free variable feature, follows5 .
state relational MDP first-order interpretation. first-order formula one free
variable function states natural numbers maps state number
objects state satisfy formula. take first-order formulas real-valued
features normalizing real number zero onethis normalization done
dividing feature value maximum value feature take, typically
total number objects domain, smaller domains objects (and
quantifiers) typed. similar feature representation used work Fawcett (1996).
feature representation used relational experiments, learner describe
next subsection considers existentially quantified conjunctions literals (with one free
variable) features. space formulas thus effective feature space relational
experiments.
Example 4.1: Take Blocksworld table object example, on(x, y)
predicate domain asserts block x top object y,
may block table. possible feature domain described
on(x, y), first-order formula x one free variable. formula
means object immediately block object x,
essentially excludes table object block held arm (if any)
object set described feature. n blocks problems, un-normalized value
feature n states block held arm, n 1 states
block held arm.
4.1.1 E NRICHED P REDICATE ET
interesting examples possible enriched predicate set define. enrich
set state predicates P , add binary predicate p transitive closure form
predicate p+ predicates min-p max-p identifying minimal maximal elements
predicate. goal-based domains, recall problem representation (from Section 2.4)
includes, predicate p, goal version predicate called goal-p represent desired
state predicate p goal. Here, add means-ends analysis predicate correct-p
represent p facts present current state goal.
So, objects x y, correct-p(x,y) true p(x, y) goal-p(x,y)
true. p+(x, y) true objects x connected path binary relation p. relation
max-p(x) true object x maximal element respect p, i.e., exists object
5. Generalizations allow multiple free variables straightforward unclear utility time.

702

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

p(x, y) true. relation min-p(x) true object x minimal element respect
p, i.e., exists object p(y, x) true.
formally define feature grammar Online Appendix 1 (available JAIR website)
page 3.
Example 4.1 (cont.): feature correct-on(x, y) means x stacked top
object current state goal state. feature on+(x, y)
means current state, x directly object y, i.e., sequence
relations traversing path x y, inclusively. feature max-on+(x)
means x table object block-towers placed table, since
table object object. feature min-on+(x) means
object top x, i.e., x clear.
4.2 Learning Relational Features
select first-order formulas candidate features using beam search beam width W .
present pseudo-code beam search Figure 3. search starts basic features derived
automatically domain description repeatedly derives new candidate features
best scoring W features found far, adding new features candidates keeping
best scoring W features times. new candidates added fixed depth times,
best scoring feature found overall selected added value-function representation.
Candidate features scored beam search correlation Bellman error feature
formalized below.
Specifically, score candidate feature f correlation coefficient Bellman
error feature B(V, ) estimated training set. correlation coefficient functions

(s)}
defined corr-coef(, ) = E{(s) (s)}E{(s)}E{
. Instead using known

distribution compute value, use states training set compute sampled
version using following equations approximate true expectation E true standard
deviation random variable X:
1 X
X(s ),
Es {X(s)} =
|s |


X,s =

corr-coef-sampled(, , ) =



1 X
(X(s ) E{X(s)})2 ,
|s |


Es {(s) (s)} Es {(s)}Es { (s)}
.
,s ,s

scoring function feature selection regularized version correlation coefficient
feature target function
score(f, , ) = |corr-coef-sampled(f, , )|(1 depth(f )),
depth feature depth beam search first occurs,
parameter learner representing degree regularization (bias towards low-depth features).
703

fiW U & G IVAN

beam-search-learn
Inputs:

Feature scoring function fscore : features [0, 1]

Outputs:

New feature f

System parameters:

W : Beam width
maxd : Max number beam-search iterations
: Degree regularization, defined Section 4.2

1.
2.
3.
4.
5.
6.
7.
8.

set basic features, defined Section 4.2.
1, F I.
repeat
Set beam B highest scoring W candidates F .
Candidate feature set F B.
candidate f1 B
candidate f2 (B I), f2 6= f1
F = F combine(f1 , f2 ).

9.
10.
11.

+ 1.
(d > maxd ) (highest score far (1 d)).
return maximum scoring feature f F .

Notes:
1. Feature scoring function fscore(f ) used rank candidates lines 4 11. discussion sample
scoring function, used relational experiments, given Section 4.2.
2. Candidate scores cached calls fscore, candidate scored twice.
3. value (1 d) largest score feature depth have.

Figure 3: Pseudo-code beam search.

value score(f, , B(V, )) score well feature f correlates Bellman error feature. Note features non-negative, still well correlated
Bellman error (which negative), presence constant feature representation allows non-negative feature shifted automatically needed.
remains specify features hypothesis space considered initial,
basic, features beam search, specify means constructing complex features
simpler ones use extending beam search. first take state predicate set P
domain enrich P described Section 4.1. enrichment P , take basic
features existentially quantified applications (possibly negated) state predicates variables
zero one free variable6 . grammar basic features defined follows.
6. domain distinguishes objects naming constants, allow constants arguments
predicates well.

704

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Definition: basic feature existentially quantified hliterali expression
one free variable (see Figure 3 Online Appendix 1, available JAIR website,
page 3).
feature free variables treated technically one-free-variable feature
variable used; results binary feature value either zero total number
objects, instantiating free variable different ways always results truth value.
assume throughout every existential quantifier automatically renamed away every
variable system. take basic features human-provided features
may available, add features experiments paper order clearly
evaluate methods ability discover domain structure own.
stage beam search add new candidate features (retaining W best scoring
features previous stage). new candidate features created follows. feature
beam combined conjunctively other, basic feature. method combination two features described Figure 4. figure shows non-deterministic pseudo-code
combining two input features, way making non-deterministic choices results
new candidate feature. pseudo-code refers feature formulas f1 f2 describing
two features. places, formulas others written free variable exposed,
f1 (x) f2 (y). substitution variable notated replacing notation,
f1 (z).
combination conjoining feature formulas, shown line 2 Figure 4; however,
additional complexity resulting combining two free variables possibly equating
bound variables two features. two free variables either equated (by substitution) one existentially quantified combination done, line 1. two pairs
variables, chosen one contributing feature, may equated, resulting
quantifier front, described line 3. Every combination feature candidate.
beam-search construction lead logically redundant features cases
syntactically redundant well. avoid syntactically redundant features end beam
search selecting highest scoring feature already feature set. Logical redundancy syntactic redundancy difficult detect. avoid redundancy
automatically using ordering beam search reduce generation symmetric expressions . However, testing logical equivalence features
language NP-hard (Chandra & Merlin, 1977), deploy complete equivalence test
here.
Example 4.2: Assume two basic features z p(x, z) w q(y, w). set
possible candidates generated combining two features are:
line 3 Figure 4 runs zero times,
1. (x z p(x, z)) (w q(y, w)), xf1 (x) f2 (y)
2. (z p(x, z)) (y w q(y, w)), f1 (x) yf2 (y),
3. (z p(x, z)) (w q(x, w)), f1 (x) f2 (x)
line 3 runs one time,
4. u ((z p(u, z)) (q(y, u))), equating x w item 1 above,
5. u (x p(x, u)) (q(y, u)), equating x z item 1 above,
705

fiW U & G IVAN

combine
Inputs:

Features f1 (x), f2 (y)

Outputs:

Set features {o1 }

return set features o1 result from:
1.

Perform one
a. f1 = (x)f1 (x)
b. f2 = (y)f2 (y)
c. f2 = f2 (x)

2.

o1 = f1 f2

3.

Perform following variable equating step zero, one, two times:
a. Let v variable occurring f1 o1 .
Let e1 expression form (v)1 (v) occurs o1
b. Let w variable occurring f2 o1 .
Let e2 expression form (w)2 (w) occurs o1
c. Let u new variable, used o1
d. o2 = replace e1 1 (u) replace e2 2 (u) o1
e. o1 = (u)o2

Notes:
1. choice 1a, 1b, 1c, choice number iterations step 3, choices e1 e2
steps 3a 3b non-deterministic choices.
2. feature produced run non-deterministic algorithm included set
features returned combine.
3. assumed f1 f2 variables common, renaming necessary operation.

Figure 4: non-deterministic algorithm combining two feature formulas.

6. u (p(x, u) (w q(u, w))), equating z item 2 above,
7. u (p(x, u) (y q(y, u))), equating z w item 2 above,
8. u (p(x, u) ( q(x, u))), equating z w item 3 above.
first three computed using cases 1a, 1b, 1c, respectively. remaining
five derive first three equating bound variables f1 f2 .
Features generated depth k language easily require enumerating k-tuples
domain objects. Since cost evaluation grows exponentially k, bound
706

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

maximum number quantifiers scope point feature formula q, refuse
consider feature violating bound.
values W , , d, q parameters controlling relational learner evaluate
paper. set parameters discussed experimental setup description
Section 6.
provide brief discussion motivations feature combination method. First,
note additive combination features represent disjunctions features7 ; hence,
consider conjunction feature combination. Here, chosen conjoin features
multiple ways, varying handling/combining free bound variables. believe
choice uniquely effective, provide example realization proposed featurediscovery architecture.
choice feature representation combination method must trade cost
evaluation choices potential gain quality selected features. Here,
chosen limit individual features conjunction; effectively, limited features
Horn clauses predicates negations, univariate heads.
4.3 Propositional Features
discuss second candidate hypothesis space features, using propositional representation. use decision trees represent propositional features. detailed discussion
classification using decision trees found book Mitchell (1997). decision tree
binary tree internal nodes labeled binary tests states, edges labeled yes
representing results binary tests, leaves labeled classes (in case, either zero
one). path tree root leaf label l identifies labeling set
stateseach state consistent state-test results path viewed labeled l tree.
way, decision tree real number labels leaves viewed labeling states
real numbers, thus feature.
learn decision trees training sets labeled states using well known C4.5 algorithm
(Quinlan, 1993). algorithm induces tree greedily matching training data root
down. use C4.5 induce new featuresthe key algorithm construct suitable
training sets C4.5 induced features useful reducing Bellman error.
include possible state tests decision trees induce every grounded predicate
application8 state predicates, well every previously selected decision-tree feature
(each binary test leaf labels zero one).
4.4 Learning Propositional Features
construct binary features, use sign Bellman error feature, magnitude. sign statewise Bellman error state serves indication whether
state undervalued overvalued current approximation, least respect exactly
representing Bellman update current value function. identify collection
undervalued states new feature, assigning appropriate positive weight feature
7. Representing disjunction overlapping features using additive combination done third feature
representing conjunction, using inclusion/exclusion negative weight conjunction.
8. grounded predicate application predicate applied appropriate number objects problem instance.

707

fiW U & G IVAN

increase value. Similarly, identifying overvalued states new feature assigning
negative weight decrease value. note domains interest generally
large state-space enumeration, need classification learning generalize notions
overvalued undervalued across state space training sets sample states.
enable method ignore states approximately converged, discard states
statewise Bellman error near zero either training set. Specifically, among states negative statewise Bellman error, discard state error closer zero median
within set; among states positive statewise Bellman error. sophisticated methods discarding training data near intended boundary considered
future research; often introduce additional parameters method. Here, seek
initial simple evaluation overall approach. discarding, define +
set remaining training pairs states positive statewise Bellman error,
likewise negative statewise Bellman error.
use + positive examples negative examples supervised
classification algorithm; case, C4.5 used. hypothesis space classification space
decision trees built tests selected primitive attributes defining state space
goal; case, use previously learned features decision trees attributes.
concept resulting supervised learning treated new feature linear approximation architecture, initial weight zero.
intent, ideally, develop approximately optimal value function. value function
expected Bellman error many states, every state; however, low state-wise
error states contribute high sup-norm Bellman error. discarding training
states low statewise Bellman error reflects tolerance low error threshold
representing degree approximation sought. Note technical motivation selecting
features based upon Bellman error focuses reducing sup-norm Bellman error; given
motivation, interested finding exact boundary positive negative
Bellman error identifying states large magnitude Bellman error (so
large-magnitude error addressed feature addition).
observe limited need separately learn feature matching due
following representability argument. Consider binary feature F complement F ,
exactly one F F true state. Given presence constant feature feature
set, adding F F feature set yields set representable value functions (assigning
weight w F effect assigning weight w F adding w weight
constant feature).
4.5 Discussion
discuss generalization capability, learning time, heuristic elements feature
learning method.
4.5.1 G ENERALIZATION ACROSS VARYING OMAIN IZES
propositional feature space described varies size number objects relational
domain varied. result, features learned one domain size generally meaningful (or
even necessarily defined) domain sizes. relational approach is, contrast, able
generalize naturally different domains sizes. experiments report ability
708

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

propositional technique learn within domain size directly, attempt use
approach learning small problems gain performance large problems. major
limitation producing good results large domains.
4.5.2 L EARNING IME
primary motivation giving generalization domain sizes order employ propositional approach resulting learner use highly efficient, off-the-shelf classification
algorithms. learning times reported Section 7 show propositional learner learns new
features orders magnitude faster relational learner.
4.5.3 H EURISTIC E LEMENTS



ETHOD

mentioned earlier, algorithm heuristically approximates repeated addition Bellman
error features linear value-function approximation order carry value iteration.
mentioned earlier, value iteration guaranteed converge optimal value function.
However, due scale problems target, heuristic approximations required. discuss
motivations heuristic approximation employ briefly here.
First, compute exact Bellman error features. Instead, use machine learning fit
training set sample states Bellman error values. selection training set
done heuristically, using trajectories drawn current greedy policy. use on-policy
selection training data loosely motivated on-policy convergence results reinforcement
learning (Singh et al., 2000), serves focus training relevant states. (See Section 3.1.)
Second, relational instance feature framework, beam-search method use
select highest scoring relational feature (with best fit Bellman error) ad-hoc, greedy,
severely resource bounded. fit obtained Bellman error purely heuristic. provide
heuristic method machine learning problem example, intend future
research provide better relational learners resulting better planning performance. Heuristic
elements current method discussed Appendix A.3. work
viewed providing reduction stochastic planning structured machine learning numeric
functions. (See Section 3.)
Third, propositional instance feature framework,, learner C4.5 selects hypotheses greedily. Also, reduction C4.5 classification relies explicit tolerance approximation form threshold used filter training data near-zero Bellman error.
motivation approximation tolerance focus learner high Bellman error states
allow method ignore almost converged states. (See Section 4.4.)
Fourth, fundamental work use linear approximation value function
gradient-descent-based weight selection (in case AVI). approximation methods key
approach handling large state spaces create need feature discovery. AVI method
includes empirically motivated heuristic methods controlling step size sign changes
weights. (See Section 5 Online Appendix 1, available JAIR website.)
Fifth, rely human input select sequence problem difficulties encountered
feature discovery well performance thresholds problem difficulty increases.
believe aspect algorithm automated future research. (See Section 3.)
709

fiW U & G IVAN

5. Related Work
Automatic learning relational features approximate value-function representation surprisingly frequently studied quite recently, remains poorly understood. Here,
review recent work related one dimensions contribution.
5.1 Feature Selection Based Bellman Error Magnitude
Feature selection based Bellman error recently studied uncontrolled (policyevaluation) context work Keller et al. (2006) Parr et al. (2007), attribute-value
explicit state spaces rather relational feature representations. Feature selection based
Bellman error compared feature selection methods uncontrolled context
theoretically empirically work Parr, Li, Taylor, Painter-Wakefield, Littman
(2008).
Here, extend work controlled decision-making setting study incorporation
relational learning selection appropriate knowledge representation value functions
generalize problems different sizes within domain.
main contribution work Parr et al. (2007) formally showing, uncontrolled
case policy evaluation, using (possibly approximate) Bellman-error features provably tightens approximation error bounds, i.e., adding exact Bellman error-feature provably reduces
(weighted L2 -norm) distance optimal value function achieved optimizing weights linear combination features. result extended weaker form
approximated Bellman-error features, uncontrolled case. limitation uncontrolled case substantial difference setting work. limited experiments shown
use explicit state-space representations, technique learns completely new set features
policy evaluation conducted policy iteration. contrast, method accumulates
features value iteration, point limiting focus single policy. Constructing
new feature set policy evaluation procedure amenable formal analysis
retaining learned features throughout value iteration policy implicitly considered value iteration (the greedy policy) potentially changing throughout. However,
using relational feature learning, runtime cost feature learning currently high make
constructing new feature sets repeatedly practically feasible.
Parr et al. (2007) builds prior work Keller et al. (2006) studied uncontrolled setting. work provides theoretical results general framework, provides
specific approach using Bellman error attribute value representations (where state represented real vector) order select new features. approach provides apparent leverage
problems state real vector, structured logical interpretation, typical
planning benchmarks.
5.2 Feature Discovery via Goal Regression
previous methods (Gretton & Thiebaux, 2004; Sanner & Boutilier, 2009) find useful features
first identifying goal regions (or high reward regions), identifying additional regions regressing action definitions previously identified regions. principle exploited
given state feature indicates value state, able achieve feature
one step indicate value state. Regressing feature definition action
710

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

definitions yields definition states achieve feature one step. Repeated regression identify many regions states possibility transitioning
action sequence high-reward region.
exponentially many action sequences relative plan length,
exponentially many regions discovered way, well exponential increase size
representation region. exponentials terms number regression steps
taken. control exponential growth number features considered, regression
implemented pruning optimizations control eliminate overlap regions
detected inexpensively well dropping unlikely paths. However, without scoring
technique (such fit Bellman-error used paper) select features, regression still
generates large number useless new features. currently effective regression-based
first-order MDP planner, described work Sanner Boutilier (2009), effective
disallowing overlapping features allow optimizations weight computation. Yet clearly
human-designed feature sets fact overlapping features.
inductive technique avoids issues considering compactly represented features,
selecting match sampled statewise Bellman error training data. provide extensive
empirical comparison First-Order Approximate Linear Programming technique (FOALP)
work Sanner Boutilier (2009) empirical results. empirical evaluation
yields stronger results across wide range probabilistic planning benchmarks goalregression approach implemented FOALP (although aspects approaches
goal-regression candidate generation vary comparison well).
Regression-based approaches feature discovery related method fitting Bellman
error exploit fact states reach valuable states must valuable, i.e. seek local consistency. fact, regression goal viewed special
case iteratively fitting features Bellman error current value function. Depending
exact problem formulation, k, Bellman error k-step-to-go value function
non-zero (or otherwise nontrivially structured) region states reach goal first
k + 1 steps. Significant differences Bellman error approach regression-based
feature selection arise states reach goal different probabilities different
horizons. approach fits magnitude Bellman error, smoothly consider
degree state reaches goal horizon. approach immediately generalizes setting useful heuristic value function provided automatic feature
learning, whereas goal-regression approach appears require goal regions begin regression.
spite issues, believe approaches appropriate valuable
considered important sources automatically derived features future work.
Effective regression requires compact declarative action model, always available9 .
inductive technique present require even PDDL action model, deductive component computation Bellman error individual states. representation
statewise Bellman error computed sufficient technique. empirical results show performance planner Tetris, model represented
giving program that, given state input, returns explicit next state distribution
state. FOALP inapplicable representations due dependence logical deductive rea9. example, Second International Probabilistic Planning Competition, regression-based FOALP planner
required human assistance domain providing needed domain information even though standard
PDDL model provided competition sufficient planner.

711

fiW U & G IVAN

soning. believe inductive deductive approaches incorporating logical representation
important complementary.
goal regression approach special case general approach generating candidate features transforming currently useful features. Others considered include
abstraction, specialization, decomposition (Fawcett, 1996). Research human-defined concept transformations dates back least landmark AI program (Davis & Lenat, 1982).
work uses one means generating candidate features: beam search logical formulas
increasing depth. means candidate generation advantage strongly favoring concise inexpensive features, may miss complex accurate/useful features.
approach directly generalizes means generating candidate features.
centrally distinguishes approach previous work leveraging feature transformations
use statewise Bellman error score candidate features. FOALP (Sanner & Boutilier, 2006,
2009) uses scoring function, includes non-pruned candidate features linear program
used find approximately optimal value function; Zenith system (Fawcett, 1996) uses
scoring function provided unspecified critic.
5.3 Previous Scoring Functions MDP Feature Selection
method, work Patrascu et al. (2002), selects features estimating minimizing
L1 error value function results retraining weights candidate feature
included. L1 error used work instead Bellman error difficulty retraining
weights minimize Bellman error. method focuses fitting Bellman error
current approximation (without retraining new feature), avoids expensive
retraining computation search able search much larger feature space effectively.
work Patrascu et al. (2002) contains discussion relational representation, L1
scoring method could certainly used features represented predicate logic; work date
tried (potentially expensive) approach.
5.4 Related Work
include discussion additional, distantly related research directions Appendix A, divided following subsections:
1. relevant feature selection methods (Fahlman & Lebiere, 1990; Utgoff & Precup, 1997,
1998; Rivest & Precup, 2003; Mahadevan & Maggioni, 2007; Petrik, 2007);
2. Structural model-based model-free solution methods Markov decision processes, including
(a) Relational reinforcement learning (RRL) systems (Dzeroski, DeRaedt, & Driessens,
2001; Driessens & Dzeroski, 2004; Driessens et al., 2006),
(b) Policy learning via boosting (Kersting & Driessens, 2008),
(c) Fitted value iteration (Gordon, 1995),
(d) Exact value iteration methods first-order MDPs (Boutilier, Reiter, & Price, 2001;
Holldobler & Skvortsova, 2004; Kersting, Van Otterlo, & De Raedt, 2004);
712

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

3. Inductive logic programming algorithms (Muggleton, 1991; Quinlan, 1996; Karalic & Bratko,
1997);
4. Approximate policy iteration relational domains (Fern et al., 2006), discussion
relational decision-list-policy learners (Khardon, 1999; Martin & Geffner, 2004; Yoon et al.,
2002);
5. Automatic extraction domain knowledge (Veloso, Carbonell, Perez, Borrajo, Fink, &
Blythe, 1995; Kambhampati, Katukam, & Qu, 1996; Estlin & Mooney, 1997; Fox & Long,
1998; Gerevini & Schubert, 1998).

6. Experimental Setting
present experiments nine stochastic planning domains, including reward-oriented
goal-oriented domains. use Pentium 4 Xeon 2.8GHz machines 3GB memory. section, give general overview experiments giving detailed results discussion
individual domains Section 7. Here, first, briefly discuss selection evaluation domains
Section 6.1. Second, Section 6.2 set evaluation relational feature learner
comparison variants replace key aspects algorithm random choice determine
importance. Additional details, including many experimental parameter settings, found
Online Appendix 1 (available JAIR website) Section 3.
6.1 Domains Considered
evaluation domains below, necessary specify discount factor modeling
domain MDP discounting. discount factor effectively specifies tradeoff
goals reducing expected plan length increasing success rate. parameter
method, domain studied, feature-learning method applied
choice . Here, simplicity, choose 0.95 throughout experiments. note
discount factor used YS DMIN domain formalization compare
previous work Patrascu et al. (2002).
6.1.1 ETRIS
Section 7.2 evaluate performance relational propositional learners using
stochastic computer-game ETRIS, reward-oriented domain goal player
maximize accumulated reward. compare results performance set handcrafted features, performance randomly selected features.
6.1.2 P LANNING C OMPETITION OMAINS
Section 7.3, evaluate performance relational learner seven goal-oriented planning domains two international probabilistic planning competitions (IPPCs) (Younes et al.,
2005; Bonet & Givan, 2006). comparison purposes, evaluate performance propositional learner two seven domains (B LOCKSWORLD variant B OXWORLD described
below). Results two domains illustrate difficulty learning useful propositional features complex planning domains. compare results relational planner
two recent competition stochastic planners FF-Replan (Yoon et al., 2007) FOALP (Sanner &
713

fiW U & G IVAN

Boutilier, 2006, 2009) performed well planning competitions. Finally,
compare results obtained randomly selecting relational features tuning weights
them. complete description of, PPDDL source for, domains used, please see
work Younes et al. (2005) Bonet Givan (2006).
Every goal-oriented domain problem generator first second IPPC considered inclusion experiments. inclusion, require planning domain fixed
action definitions, defined Section 2.4, addition ground conjunctive goal regions. Four domains properties directly, adapted three domains
properties:
1. B OXWORLD, modify problem generator goal region always ground
conjunctive expression. call resulting domain C ONJUNCTIVE -B OXWORLD.
2. F ILEWORLD, construct obvious lifted version, create problem generator restricted three folders domain action definitions vary number
folders. call resulting domain L IFTED -F ILEWORLD 3.
3. OWERS H ANOI, create problem generator.
resulting selection provides seven IPPC planning domains empirical study. provide
detailed discussions adapted domains Section 2 Online Appendix 1 (available JAIR
website), well discuss reasons exclusion domains.
6.1.3 YS DMIN
conclude experiments comparing propositional learner previous method Patrascu et al. (2002), using YS DMIN domain used evaluation there. empirical
comparison YS DMIN domain shown Section 7.4.
6.2 Randomized Variants Method
major contribution introduction evaluation feature learning framework
controlled setting based scoring Bellman-error (BE Scoring). empirical work instantiates framework relational feature-learning algorithm design based greedy
beam-search. Here, compare performance instance framework variants
replace key aspects randomized choice, illustrating relative importance features. two random-choice experiments, adapt method one following two
ways:
1. Labeling training states random scores instead Bellman Error scores. target
value feature training set random number -1 1. algorithm called
Random Scoring.
2. Narrowing beam search randomly rather greedily. eliminate scoring beam search, instead using random selection narrow beam; end
beam search scoring used select best resulting candidate. algorithm called
Random Beam Narrowing.
714

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

original algorithm, labels training data Bellman error narrows beam greedily rather randomly, called Greedy Beam Search/BE Scoring plots.
comparisons, consider relational feature representation, beam
search method used. Experiments two variants introduced here, presented
Sections 7.2.4 7.3.4, show original method selects features perform much better
randomly selected features, greediness beam search often (but always)
important achieving good performance.

7. Experimental Results
present experimental results ETRIS, planning competition domains, YS DMIN
section, starting introduction structure result presentation.
7.1 Read Results
task evaluating feature-learning planning system subtle complex. particularly
factor relational case generalization problem sizes learning small
problems must evaluated. resulting data extensive highly structured, requiring
training reader understand interpret. introduce reader structure
results.
experiments propositional learning (or randomly selected propositional features), problem size never varies within one run learner, propositional representation Section 4.3 cannot generalize sizes. run separate experiment
size considered. experiment two independent trials; trial starts single trivial
feature repeatedly adds features termination condition met. feature addition, AVI used select weights combining features form value function,
performance value function measured (by sampling performance greedy policy).
compute average (of two trials) performance function number
features used. Since results single line plot performance function number
features, several different fixed-problem-size learners compared one figure, one
line each, done example Figures 7 14. performance measure used varies
appropriately domain presented below.
study ability relational representation Section 4.1 generalize sizes.
study properly understood backdrop flowchart Figure 1.
described flowchart, one trial learner learn sequence features encounter
sequence increasing problem difficulties. One iteration learner either add new
feature increase problem difficulty (depending current performance). either case,
weights retrained AVI performance measurement resulting greedy policy
taken. different trials may increase size different points, cannot meaningfully
average measurements two trials. Instead, present two independent trials separately
two tables, Figures 5 12. first trial, present data
second time line plot showing performance function number features, problem
size changes annotated along line, plots Figures 6 13. Note success
ratio generally increases along line features added, falls problem size
increased. (In ETRIS, however, measure rows erased rather success ratio, rows
715

fiW U & G IVAN

erased generally increases either addition new feature addition new rows
available grid.)
interpret tables showing trials relational learner, useful focus first
two rows, labeled # features Problem difficulty. rows, taken together, show
progress learner adding features increasing problem size. column table
represents result indicated problem size using indicated number learned features.
one column next, change one rowsif performance
policy shown column high enough, problem difficulty increases,
otherwise number features increases. adding subtlety interpreting tables, note several adjacent columns increase number features,
sometimes splice two columns save space. Thus, several features
added consecutively one problem size, slowly increasing performance, may show
first last columns problem size, consequent jump number
features columns. likewise sometimes splice columns several consecutive columns increase problem difficulty. found splicings save space
increase readability practice reading tables.
Performance numbers shown column (success ratio average plan length, number
rows erased, ETRIS) refer performance weight-tuned policy resulting
feature set problem difficulty. show column performance value
function (without re-tuning weights) target problem size. Thus, show quality measures
policy found feature learning current problem size point
target problem size, illustrate progress learning small problems target size
via generalization.
study problem deciding stop adding features. Instead,
propositional relational experiments, trials stopped experimenter judgment additional results expensive value giving evaluating algorithm. However,
stop trials still improving unless unacceptable resource consumption
occurred.
Also, trial, accumulated real time trial measured shown point
trial. use real time rather CPU time reflect non-CPU costs paging due
high memory usage.
7.2 Tetris
present experimental results ETRIS.
7.2.1 OVERVIEW



ETRIS

game ETRIS played rectangular board area, usually size 10 20, initially
empty. program selects one seven shapes uniformly random player rotates
drops selected piece entry side board, piles onto remaining fragments
pieces placed previously. implementation, whenever full row squares
occupied fragments pieces, row removed board fragments top
removed row moved one row; reward received row removed.
process selecting locations rotations randomly drawn pieces continues board
full new piece cannot placed anywhere board. ETRIS stochastic since
716

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

next piece place always randomly drawn, stochastic element
game. ETRIS used experimental domain previous MDP reinforcement learning
research (Bertsekas & Tsitsiklis, 1996; Driessens et al., 2006). set human-selected features
described book Bertsekas Tsitsiklis (1996) yields good performance
used weighted linearly approximated value functions. cannot fairly compare performance
domain probabilistic planners requiring PPDDL input found natural
PPDDL definition ETRIS.
performance metric ETRIS number rows erased averaged 10,000 trial
games. reward-scaling parameter rscale (defined Section 5 Online Appendix 1 page 8)
selected 1.
7.2.2 ETRIS R ELATIONAL F EATURE L EARNING R ESULTS
represent ETRIS grid using rows columns objects. use three primitive predicates:
fill(c, r), meaning square column c, row r occupied; below(r1 , r2 ), meaning row
r1 directly row r2 ; beside(c1 , c2 ), meaning column c1 directly left
column c2 . quantifiers used relational ETRIS hypothesis space typed using types
row column.
state predicates representing piece drop; however, efficiency
reasons planner computes state value function grid, next piece.
limitation value-function expressiveness allows significantly cheaper Bellman-backup computation. one-step lookahead greedy policy execution provides implicit reasoning
piece dropped, piece grid next states.
conduct relational ETRIS experiments 10-column, n-row board, n initially
set 5 rows. threshold increasing problem difficulty adding one row score
least 15 + 20(n 5) rows erased. target problem size experiments 20 rows.
results relational ETRIS experiments given Figures 5 6 discussed below.
7.2.3 ETRIS P ROPOSITIONAL F EATURE L EARNING R ESULTS
propositional learner, describe ETRIS state 7 binary attributes represent
7 pieces currently dropped, along one additional binary attribute
grid square representing whether square occupied. adjacency relationships
grid squares represented procedurally coded action dynamics. Note
number state attributes depends size ETRIS grid, learned features
apply problems grid size. result, show separate results selected problem
sizes.
evaluate propositional feature learning 10-column ETRIS grids four different sizes: 5
rows, 7 rows, 9 rows, 20 rows. Results four trials shown together Figure 7
average accumulated time required reach point Figure 7 shown Figure 8.
results discussed below.
7.2.4 E VALUATING MPORTANCE B ELLMAN - ERROR CORING G REEDY
B EAM - SEARCH ETRIS
Figure 9 compares original algorithm alternatives vary either training set
scoring greediness beam search, discussed Section 6.2. two alternatives, use
717

fiW U & G IVAN

Trial #1
# features
Problem difficulty
Score
Accumulated time (Hr.)
Target size score

0
5
0.2
0.0
0.3

1
5
0.5
2
1.3

2
5
1.0
4.2
1.4

3
5
3.0
5.2
1.8

11
5
18
20
178

11
6
31
21
238

12
6
32
24
261

17
6
35
39
176

17
7
55
42
198

18
7
56
46
211

18
8
80
50
217

18
9
102
57
221

18
10
121
65
220

18
15
234
111
268

18
20
316
178
317

Trial #2
# features
Problem difficulty
Score
Accumulated time (Hr.)
Target size score

0
5
0.2
0.0
0.3

1
5
0.6
2.4
1.7

8
5
16
15
104

8
6
28
15
113

12
6
36
27
108

12
7
53
29
116

14
7
56
39
130

14
11
133
66
192

15
11
136
76
196

15
12
151
87
199

16
12
156
97
206

16
13
167
103
211

17
13
168
110
211

26
13
175
211
219

26
14
192
220
225

27
14
210
236
218

27
17
238
276
231

28
17
251
295
231

29
17
240
318
233

33
17
241
408
231

Figure 5: ETRIS performance (averaged 10,000 games). Score shown average rows
erased, problem difficulty shown number rows ETRIS board.
number columns always 10. Difficulty increases average score greater
15+20*(n-5), n number rows ETRIS board. Target problem
size 20 rows. columns omitted discussed Section 7.1.

Average Rows Erased

Tetris, Relational, Trial 1
350
300
250
200
150
100
50
0

1020
1015

106

106

105

105
0

1010
107

2

4

6

8

10

12

14

16

18

Number Features

Figure 6: Plot average number lines erased 10,000 ETRIS games run
AVI training learning relational features (trial 1). Vertical lines indicate
difficulty increases (in number rows), labeled along plot.

718

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Average Rows Erased

Tetris, Propositional
14
12
10
8
6
4
2
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50

Number Features
105

107

109

1020

Figure 7: Plot average number lines erased 10,000 ETRIS games iteration
AVI training learning propositional features, averaged two trials.

Accumulated Time (Hr.)

Tetris, Propositional
160
140
120
100
80
60
40
20
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50

Number Features
105

107

109

1020

Figure 8: Plot accumulated time required reach point Figure 7, averaged two
trials.

schedule used original Greedy Beam Search/BE Scoring algorithm ETRIS
starting 10 5 problem size. However, performance two alternatives never
good enough increase problem size.
7.2.5 E VALUATING H UMAN - DESIGNED F EATURES ETRIS
addition evaluating relational propositional feature learning approach, evaluate
human-selected features described book Bertsekas Tsitsiklis (1996) perform
selected problem sizes. problem size, start weights zero use AVI
719

fiW U & G IVAN

Average Rows Erased

Impact Greedy Beam Search Scoring
18
16
14
12
10
8
6
4
2
0
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50

Number Relational Features
Greedy Beam Search/BE Scoring (original algorithm)
Random Scoring (variant 1)
Random Beam Narrowing (variant 2)

Figure 9: Plot average number lines erased 10,000 ETRIS games relational features
learned original algorithm two alternatives discussed Section 6.2.
Random Scoring Random Beam Narrowing, results averages two
independent trials. Trials two variants terminated fail make
progress several feature additions. comparison purposes, trial one original
Greedy Beam Search/BE Scoring method shown, reaching threshold difficulty
increase eleven feature additions (trial two even better).

Average rows erased, Trial 1
Average rows erased, Trial 2

10 5 10 7 10 9 10 20
19
86
267 17,954
19
86
266 18,125

Figure 10: average number lines erased 10,000 ETRIS games best weighted
combination human features found two trials AVI four problem
sizes.

process described Section 2.5 train weights 21 features performance appears
30
3
1+k/100
human-designed features
converge. change learning rate 1+k/100
require larger step-size converge rapidly. human-designed features normalized
value 0 1 experiments. run two independent trials problem size
report performance best-performing weight vector found trial, Figure 10.
7.2.6 P ERFORMANCE C OMPARISON B ETWEEN IFFERENT PPROACHES



ETRIS

Several general trends emerge results ETRIS. First all, addition new learned
features almost always increasing performance resulting tuned policy (on current
size target size), best performance point reached. suggests fact
720

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Relational Prop. 10 5 Prop.10 7 Prop.10 9 Prop.10 20
Average feature learning
time (Min.)

167

44

52

60

44

Figure 11: Table average feature learning time relational propositional approaches.
selecting useful features. find clear evidence ability relational representation
usefully generalize problem sizes: substantial performance developed target
problem size without ever training directly size.
find best performance learned propositional features much lower
learned relational features problem sizes shown here, even though larger feature training set
size many learned features used propositional approach. suggests
rich relational representation indeed able better capture dynamics ETRIS
propositional representation.
find performance using random features ETRIS significantly worse
using learned features, demonstrating performance improvements feature learning
due useful feature selection (using Bellman error), simply due increasing number
features.
learned relational feature performance 10 20 ETRIS far worse obtained
using human-selected features AVI size. However, 10 5 ETRIS
relational feature performance close human-designed features. human-designed
features engineered perform well 10 20 ETRIS hence concepts useful
performing well smaller problem sizes may exist features.
7.2.7 IME L EARN E ACH F EATURE
Figure 11 show average time required learn relational feature propositional feature
ETRIS.
time required learn relational feature significantly longer required learn
propositional feature, even though propositional approach larger feature training set size
used.
7.2.8 C OMPARISON



P REVIOUS ETRIS - SPECIFIC L EARNERS

evaluating domain-independent techniques ETRIS, must first put aside strong performance already shown many times literature domain-dependent techniques domain.
Then, must face problem published domain-independent comparison points
order define state-of-the-art target surpass. latter problem, provide baseline
two different approaches random feature selection, show targeted feature selection dramatically improves random selection. former problem, include
discussion domain-specific elements key previous published results ETRIS.
many previous domain-specific efforts learning play ETRIS (Bertsekas
& Tsitsiklis, 1996; Szita & Lorincz, 2006; Lagoudakis, Parr, & Littman, 2002; Farias & Van Roy,
2004; Kakade, 2001). Typically, provide human-crafted domain-dependent features, deploy domain-independent machine learning techniques combine features (often tuning
721

fiW U & G IVAN

weights linear combination). example, domain-specific feature counting number
covered holes board frequently used. feature plausibly derived human
reasoning rules game, realizing holes difficult fill later
action lead low scores. prior work, selection feature hand,
automated feature-selection process (such scoring correlation Bellman error).
frequently used domain-specific features include column height difference height adjacent columns, apparently selected relevant human reasoning rules
game.
key research question address, then, whether useful features derived automatically, decision-making situation ETRIS approached domain-independent
system without human intervention. method provided domain-state representation using primitive horizontal vertical positional predicates, single constant feature.
knowledge, research published evaluation ETRIS rely
domain-specific human inputs discussed. expected, performance
ETRIS much weaker achieved domain-specific systems cited.
7.3 Probabilistic Planning Competition Domains
Throughout evaluations learners planning domains, use lower plan-length cutoff
1000 steps evaluating success ratio iterative learning features, speed learning.
use longer cutoff 2000 steps final evaluation policies comparison
planners evaluations target problem size. reward-scaling parameter rscale
(defined Section 5 Online Appendix 1 page 8) selected 1 throughout planning
domains.
domains multi-dimensional problem sizes, remains open research problem
change problem size different dimensions automatically increase difficulty learning.
Here, C ONJUNCTIVE -B OXWORLD Z ENOTRAVEL, hand-design sequence increasing problem sizes.
discussed Section 6.1.2, evaluate feature learners total seven probabilistic planning competition domains. following paragraphs, provide full discussion
B LOCKSWORLD C ONJUNCTIVE -B OXWORLD, abbreviated results five domains. provide full discussion five domains Appendix B.
relational feature learner finds useful value-function features four domains
(B LOCKSWORLD, C ONJUNCTIVE -B OXWORLD, IREWORLD, L IFTED -F ILEWORLD 3).
three domains (Z ENOTRAVEL, E XPLODING B LOCKSWORLD, OWERS H ANOI),
relational feature learner makes progress representing useful fixed-size value function
training sizes, fails find features generalize well problems larger sizes.
7.3.1 B LOCKSWORLD
probabilistic, non-reward version B LOCKSWORLD first IPPC, actions pickup
putdown small probability placing handled block table object instead
selected destination.
relational learner, start 3 blocks problems. increase n blocks n + 1
blocks whenever success ratio exceeds 0.9 average successful plan length less
30(n 2). target problem size 20 blocks. Results shown Figures 12 13.
722

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Trial #1
# features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
1
2
2
3
3
3
3
3
3
3
4
4
5
10
15
1.00 1
1 0.95 1
1
1 0.97
89 45 20 133 19
33 173 395
0.5 1.0 1.5 2.2 3.3 3.9 10
36
0
0
0
0 0.98 0.96 0.98 0.97




761 724 754 745

Trial #2
# features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
3
1
80
0.5
0


1
2
2
3
3
3
3
3
3
4
4
5
10
15
1
1 0.94 1
1
1 0.96
48 19 125 17
34 167 386
1.0 1.4 2.0 3.3 3.8 9.4 33
0
0
0 0.97 0.98 0.98 0.98



768 750 770 741

Figure 12: B LOCKSWORLD performance (averaged 600 problems) relational learner.
add one feature per column success ratio exceeds 0.9 average successful plan
length less 30(n 2), n blocks, increase problem difficulty
next column. Plan lengths shown successful trials only. Problem difficulties
measured number blocks, target problem size 20 blocks. columns
omitted discussed Section 7.1.

propositional learner, results problem sizes 3, 4, 5, 10 blocks shown
Figure 14.
relational learner consistently finds value functions perfect near-perfect success
ratio 15 blocks. performance compares favorably recent RRL (Driessens
et al., 2006) results deterministic B LOCKSWORLD, goals severely restricted to,
instance, single atoms, success ratio performance around 0.9 three ten blocks
(for single goal) still lower achieved here. results B LOCKSWORLD show
average plan length far optimal. observed large plateaus induced value
function: state regions states given value greedy policy wanders.
problem merits study understand feature-induction break
plateaus. Separately, studied ability local search break plateaus (Wu,
Kalyanam, & Givan, 2008).
performance target size clearly demonstrates successful generalization sizes
relational representation.
propositional results demonstrate limitations propositional learner regarding lack
generalization sizes. good value functions induced small
problem sizes (3 4 blocks), slightly larger sizes 5 10 blocks render method ineffective.
10 block problems, initial random greedy policy cannot improved never finds
723

fiW U & G IVAN

Success Ratio

Blocksworld, Relational, Trial 1
3 blocks

1

3 blocks

3 blocks

4, 5, 10 blocks
15 blocks

0.95
4 blocks
0.9
0.85
0.80

Successful Plan Length

0

1

2

400

3
15 blocks

300
200
3 blocks

100

3 blocks

4 blocks

10 blocks

3 blocks

5 blocks
4 blocks

0
0

1

2

3

Number Features

Figure 13: B LOCKSWORLD success ratio average successful plan length (averaged 600
problems) first trial Figure 12 using relational learner.

goal. addition, results demonstrate learning additional features good policy
found degrade performance, possibly AVI performs worse higher dimensional
weight space results.
7.3.2 C ONJUNCTIVE -B OXWORLD
probabilistic, non-reward version B OXWORLD first IPPC similar
familiar Logistics domain used deterministic planning competitions, except explicit connectivity graph cities defined. Logistics, airports aircraft play important role
since possible move trucks one airport (and locations adjacent it) another airport (and locations adjacent it). B OXWORLD, possible move boxes
without using aircraft since cities may connected truck routes. stochastic
element introduced domain truck moved one city another,
small chance ending unintended city. described Section 6.1, use
C ONJUNCTIVE -B OXWORLD, modified version B OXWORLD, experiments.
start 1-box problems relational learner increase n boxes n + 1 boxes
whenever success ratio exceeds 0.9 average successful plan length better 30n.
feature-learning problem difficulties use 5 cities. use two target problem sizes: 15 boxes
724

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Blocksworld, Propositional

Success Ratio

1
0.8
0.6
0.4
0.2

Successful Plan Length

450
400
350
300
250
200
150
100
50
0

Accumulated Time (Hr.)

0

90
80
70
60
50
40
30
20
10
0

0

2

4

6

8

10

0

2

4

6

8

10

0

2

4

6

8

10

Number Features

3 blocks

4 blocks

5 blocks

10 blocks

Figure 14: B LOCKSWORLD performance success ratio average successful plan length (averaged 600 problems), accumulated run-time propositional learner, averaged two trials.

725

fiW U & G IVAN

Trial #1
# features
0
1
2
Problem difficulty
1
1
1
Success ratio
0.97 1
1
Plan length
226 84 23
Accumulated time (Hr.) 7.2 10 13
0.98 1
1
Target size #1 SR
Target size #1 Slen.
1056 359 93
Target size #2 SR
0.16 0.90 0.97
Target size #2 Slen.
1583 996 238

2
2
1
37
14
1
91
0.97
230

2
3
1
44
16
1
90
0.96
233

2
5
1
54
21
1
92
0.98
244

2
10
1
77
42
1
90
0.96
240

2
11
1
80
49
1
92
0.98
238

2
12
1
313
57
1
355
0.90
1024

2
13
1
87
65
1
90
0.98
240

2
15
1
92
84
1
91
0.96
239

Trial #2
# features
0
1
2
Problem difficulty
1
1
1
Success ratio
0.97 1
1
Plan length
235 85 24
Accumulated time (Hr.) 7.3 11 14
Target size #1 SR
0.96 1
1
1019 365 90
Target size #1 Slen.
Target size #2 SR
0.19 0.9 0.97
Target size #2 Slen.
1574 982 226

2
2
1
34
16
1
91
0.97
230

2
3
1
43
18
1
91
0.98
233

2
5
1
54
23
1
92
0.98
233

2
9
1
72
39
1
89
0.97
242

2
10
1
299
45
1
359
0.92
1006

2
11
1
80
51
1
89
0.98
231

2
12
1.00
310
60
1
363
0.91
1026

2
13
1
84
68
1
90
0.97
240

2
15
1
91
86
1
90
0.96
233

Figure 15: C ONJUNCTIVE -B OXWORLD performance (averaged 600 problems). add one
feature per column success ratio greater 0.9 average successful plan
length less 30n, n boxes, increase problem difficulty next
column. Problem difficulty shown number boxes. Throughout learning
process number cities 5. Plan lengths shown successful trials only. Two
target problem sizes used. Target problem size #1 15 boxes 5 cities. Target
problem size #2 10 boxes 10 cities. columns omitted discussed
Section 7.1.

5 cities, 10 boxes 10 cities. Relational learning results shown Figures 15 16,
results propositional learner 5 cities 1, 2, 3 boxes shown Figures 17.
interpreting C ONJUNCTIVE -B OXWORLD results, important focus average
successful plan-length metric. C ONJUNCTIVE -B OXWORLD problems, random walk able
solve problem nearly always, often long plans10 . learned features enable
direct solutions reflected average plan-length metric.
two relational features required significantly improved performance problems
tested. Unlike domains evaluate, C ONJUNCTIVE -B OXWORLD domain
10. note that, oddly, IPPC competition domain used action preconditions prohibiting moving box away
destination. preconditions bias random walk automatically towards goal. consistency
competition results, retain odd preconditions, although preconditions necessary good
performance algorithm.

726

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Success Ratio

Conjuctive-Boxworld, 5 Cities, Relational, Trial 1
1 box

1

1, 2, 3, 5, 10, 15 boxes

1 box

0.95
0.90
0

1

2

250

Successful Plan Length

1 box
200
150
100

15 boxes
10 boxes

1 box
5 boxes
3 boxes

50

2 boxes
1 box

0
0

1

2

Number Features

Figure 16: C ONJUNCTIVE -B OXWORLD success ratio average successful plan length (averaged
600 problems) first trial using relational learner.

learned features straightforwardly describable English. first feature counts many
boxes correctly target city. second feature counts many boxes trucks.
note lack features rewarding trucks right place (resulting
longer plan lengths due wandering value-function plateaus). features easily written knowledge representation (e.g. count trucks located cities destinations
package truck), require quantification cities packages. severe
limitation quantification currently method efficiency reasons prevents consideration
features point. worth noting regression-based feature discovery, studied work Gretton Thiebaux (2004) Sanner Boutilier (2009), expected
identify features regarding trucks regressing goal action unloading
package destination. Combining Bellman-error-based method regression-based
methods promising future direction.
Nevertheless, relational learner discovers two concise useful features dramatically
reduce plan length relative initial policy random walk. significant success
automated domain-independent induction problem features.
727

fiW U & G IVAN

Conjunctive-Boxworld, Propositional

Success Ratio

1
0.8
0.6
0.4
0.2

Successful Plan Length

0
0

2

4

6

8

10

0

2

4

6

8

10

0

2

4

6

8

10

500
450
400
350
300
250
200
150
100
50
0

Accumulated Time (Hr.)

450
400
350
300
250
200
150
100
50
0

Number Features
1 box

2 box

3 box

Figure 17: C ONJUNCTIVE -B OXWORLD performance (averaged 600 problems) accumulated run-time propositional learner, averaged two trials. Throughout learning process number cities 5.

728

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

One trial relational feature learner C ONJUNCTIVE -B OXWORLD takes several days,
even though fixed number cities training problems five cities. New techniques required improving efficiency feature learning provide results
training larger numbers cities. results demonstrate current representation
learning methods adequately manage small city graphs even larger larger numbers
boxes deliver, resulting value functions successfully generalize 10-city problems.
domain, well known weakness AVI apparent. AVI often works practice,
theoretical guarantee quality weight vector found AVI training. (Alternatively, approximate linear programming step could replace AVI training provide
expensive perhaps robust weight selection.) C ONJUNCTIVE -B OXWORLD results,
AVI training goes astray selecting weights 12 box domain size Trial 1. result,
selected weights overemphasize first feature, neglecting second feature. revealed
data shown plan-length performance degrades significantly one column
data. AVI repeated next problem size (13 boxes), good performance restored.
similar one-column degradation plan length occurs trial 2 10-box 12-box sizes.
propositional experiments C ONJUNCTIVE -B OXWORLD, note that, generally,
adding learned propositional features degrades success-rate performance relative initial
random walk policy introducing ineffective loops greedy policy. resulting greedy
policies find goal fewer steps random walk, generally pay unacceptable drop
success ratio so. one exception policy found one-box problems using two
propositional features, significantly reduces plan length preserving success ratio. Still,
result much weaker relational feature language.
problems get severe problem size increases, 3-box problems suffering severe
degradation success rate modest gains successful plan length. please note
accumulated runtime experiments large, especially 3-box problems. AVI
training expensive policies find goal. Computing greedy policy
state long trajectory requires considering action, number available actions
quite large domain. reasons, propositional technique evaluate sizes
larger three boxes.
7.3.3 UMMARY R ESULTS



DDITIONAL OMAINS

Figures 18 20, present summary results five additional probabilistic planning domains. detailed results full discussion domains, please see Appendix B.
summary results, see feature learning approach successfully finds features perform well across increasing problem sizes two five domains, IREWORLD L IFTED F ILEWORLD 3. three domains (Z ENOTRAVEL, OWERS H ANOI, E XPLODING
B LOCKSWORLD), feature learning able make varying degrees progress fixed small problem sizes, progress (sometimes quite limited) generalize well size increases.
7.3.4 E VALUATING R ELATIVE MPORTANCE B ELLMAN - ERROR CORING
G REEDY B EAM - SEARCH G OAL - ORIENTED OMAINS
Figure 21 compares original algorithm alternatives vary either training set
scoring greediness beam search, discussed Section 6.2. trial variant,
generate greedy policy domain using feature selection within relational representation
729

fiW U & G IVAN

Tireworld, Trial 1

Success Ratio

1
0.9

4 nodes

4, 5 nodes

6 nodes

20, 30 nodes

6 nodes

9 nodes

9, 10 nodes

3

4

4 nodes

0.8
0.7
0.6
4 nodes

0.5
0.4
0

Successful Plan Length

0

1

2

5

6

20, 30 nodes

5

4 nodes

4

4 nodes

4 nodes

3

9, 10 nodes
6, 9 nodes

2

4, 5, 6 nodes

1
0
0

1

2

3

4

5

Number Features

Zenotravel, Trial 1
Success Ratio

1
3 cities, 1 person, 1 aircraft

0.8
0.6

3 cities, 2 people, 2 aircraft

0.4

3 cities, 2 people, 2 aircraft

0
0.2

Successful Plan Length

0

1

2

3

4

5

6

7

8

9

500
400

3 cities, 2 people, 2 aircraft

3 cities, 2 people, 2 aircraft

300
200

3 cities, 1 person, 1 aircraft

100
0
0

1

2

3

4

5

6

7

8

9

Number Features

Figure 18: Summary results IREWORLD Z ENOTRAVEL. full discussion detailed
results, please see Appendix B.

730

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Exploding Blocksworld, Trial 1

Success Ratio

1
0.8

3 blocks

3 blocks
3 blocks

0.6

3 blocks
4 blocks

0.4

4 blocks

0.2
0

Successful Plan Length

0

1

2

3

4

5

6

7

8

9

10

6
5

4 blocks

4

4 blocks

3
3 blocks

3 blocks

2

3 blocks

3 blocks

1
0
0

1

2

3

4

5

6

7

8

9

10

Number Features

Tower Hanoi, Trial 1

Success Ratio

1
2 discs

0.8
0.6

4 discs

0.4

3 discs

0.2

5 discs

4 discs
0

Successful Plan Length

0

1

2

3

4

5

6

7

20
9

8

38
10

50
40
30

3 discs

20
10

4 discs

2 discs
0
0

1

2

3

4

5

6

7

8

Number Features

Figure 19: Summary results E XPLODING B LOCKSWORLD OWERS
discussion detailed results, please see Appendix B.

731



H ANOI. full

fiW U & G IVAN

Success Ratio

Lifted-Fileworld3, Trial 1
1 file

1

1 file

1 file

1 2 files

14 16 files

16 files

2 14 files

16 20 files

0.95
0.90
0

1

2

3

4

5

6

7

Successful Plan Length

60
16 files
14 files

20 files

40

14 files
15 files

13 files
12 files
10 files

11 files

16 files

16 files

20
1 file

1 file
1 file

4 files

2 files

3 files
2 files

1 file

0
0

1

2

3

4

5

6

7

Number Features

Figure 20: Summary results L IFTED -F ILEWORLD 3. full discussion detailed results,
please see Appendix B.

(alternating AVI training, difficulty increase, feature generation original algorithm).
trial, domain, select best performing policy, running algorithm
target problem difficulty reached improvement least three feature additions;
latter case generating least nine features. evaluate greedy policy acquired
manner, measuring average target-problem-size performance domain, average
results two trials. results shown Figure 21.
domain alternative Random Scoring perform comparably original Greedy
Beam Search/BE Scoring, exception three domain/size combinations learners
perform poorly (Z ENOTRAVEL, 10-block E XPLODING B LOCKSWORLD, 5-disc OWERS
H ANOI ). alternative Random Beam Narrowing sometimes adequate replace original
approach, domains, greedy beam search critical performance.
7.3.5 C OMPARISON



FF-R EPLAN



FOALP

compare performance learned policies FF-Replan FOALP
PPDDL evaluation domains used above. use problem generators provided planning
competitions generate 30 problems tested problem size except OWERS H ANOI
732

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Domain
Size
Greedy Beam/BE Scoring (orig.) SR
Greedy Beam/BE Scoring (orig.) SLen.
Random Scoring (var. 1) SR
Random Scoring (var. 1) SLen.
Random Beam Narrowing (var. 2) SR
Random Beam Narrowing (var. 2) SLen.
Random walk SR
Random walk SLen.

BW
20
0.98
748
0

0.01
258
0


Box Box Tire Zeno EX-BW EX-BW TOH TOH File
(15,5) (10,10) 30 (10,2,2)
5
10
4
5 30
1
0.98 0.92 0.11
0.34
0.03 0.51 0.00 1
90
235
5
1137
6
23
4
14 65
0.99 0.21 0.67 0.05
0.27
0.01 0.24 0.03 1
946 1582
6
910
6
12
13 26 215
1
0.99 0.91 0.13
0.35
0.02 0.25 0.01 1
90
242
6
1127
8
19
38 84 250
0.97 0.18 0.18 0.06
0.13
0
0.09 0.00 1
1038 1579
6
865
4

14 14 251

Figure 21: Target-problem-size performance (averaged 600 problems) relational features
learned original algorithm two alternatives discussed Section 6.2,
random walk, averaged best results two independent trials
target problem size.

L IFTED -F ILEWORLD 3, one fixed problem problem size. evaluate
performance planner 30 times problem, report Fig. 22 success ratio
planner problem size (averaged attempts). policies, learned
two independent trials shown above, indicated RFAVI #1 RFAVI #2. planner
30-minute time limit attempt. average time required finish successful attempt
largest problem size domain reported Figure 23.
two trials learner domain, evaluate policy performed best trial (first) target problem size. (Here, policy set features
corresponding weight vector learned AVI trial.) Performance measured
success rate, ties broken plan length. remaining ties broken taking later
policy trial tied. case, consider policy policy
learned trial.
results show planners performance incomparable FF-Replan (winning domains, losing others) generally dominates FOALP.
RFAVI performs best planners larger B LOCKSWORLD, C ONJUNCTIVE B OXWORLD, IREWORLD problems. RFAVI essentially tied FF-Replan performance
L IFTED -F ILEWORLD 3. RFAVI loses FF-Replan remaining three domains, E XPLODING
B LOCKSWORLD, Z ENOTRAVEL, OWERS H ANOI. Reasons difficulties last
three domains discussed sections presenting results domains. note
FOALP learned policy Z ENOTRAVEL, E XPLODING B LOCKSWORLD, OWERS
H ANOI , L IFTED -F ILEWORLD 3.
RFAVI relies random walk explore plateaus states differentiated selected
features. reliance frequently results long plan lengths times results failure.
recently reported elsewhere early results ongoing work remedying problem
using search place random walk (Wu et al., 2008).
RFAVI learning approach different non-learning online replanning used
FF-Replan, problem determinized, dropping probability parameters.
733

fiW U & G IVAN

RFAVI #1
RFAVI #2
FF-Replan
FOALP

15 blocks BW
1 (483)
1.00 (463)
0.93 (52)
1 (56)

20 blocks BW
1 (584)
1.00 (578)
0.91 (71)
0.73 (73)

25 blocks BW
0.85 (1098)
0.85 (1099)
0.7 (96)
0.2 (96)

30 blocks BW
0.75 (1243)
0.77 (1227)
0.23 (118)
0.07 (119)

RFAVI #1
RFAVI #2
FF-Replan
FOALP

(10BX,5CI)Box
1 (76)
1 (75)
1 (70)
1 (35)

(10BX,10CI)Box
0.97 (225)
0.97 (223)
0.98 (256)
0.70 (257)

(10BX,15CI)Box
0.93 (459)
0.93 (454)
0.93 (507)
0.28 (395)

(15BX,5CI)Box
1 (90)
1 (90)
1 (88)
0.99 (56)

RFAVI #1
RFAVI #2
FF-Replan
FOALP

20 nodes Tire
0.87 (5)
0.85 (4)
0.76 (2)
0.92 (4)

30 nodes Tire
0.85 (7)
0.84 (7)
0.73 (3)
0.90 (5)

40 nodes Tire
0.98 (6)
0.97 (6)
0.83 (3)
0.91 (5)

(10CI,2PR,2AT)Zeno
0.06 (1240)
0.07 (1252)
1 (99)
N/A

RFAVI #1
RFAVI #2
FF-Replan
FOALP

5 blocks EX-BW
0.25 (8)
0.25 (8)
0.91 (7)
N/A

10 blocks EX-BW 4 discs TOH
0.02 (30)
0.43 (4)
0.01 (35)
0.47 (4)
0.45 (20)
0.57 (3)
N/A
N/A

5 discs TOH
0 ()
0 ()
0.37 (7)
N/A

(20BX,20CI)Box
0.82 (959)
0.82 (989)
0.35 (1069)
0.0 (711)

30 files Lifted-File
1 (65)
1 (65)
1 (66)
N/A

Figure 22: Comparison planner (RFAVI) FF-Replan FOALP. Success ratio
total 900 attempts (30 attempts OWERS H ANOI L IFTED -F ILEWORLD 3)
problem size reported, followed average successful plan length
parentheses. two rows RFAVI map two learning trials shown paper.

30 BW (20,20) BX 40 Tire (10,2,2) Zeno 10 EX-BW 5 TOH 30 Files
RFAVI #1
106s
83s
1s
51s
2s

1s
RFAVI #2
105s
86s
0s
51s
3s

1s
FF-Replan 872s
739s
0s
1s
8s
3s
10s
FOALP
16s
173s
24s
N/A
N/A
N/A
N/A
Figure 23: Average runtime successful attempts, results shown Figure 22,
largest problem size domain.

734

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

important topic future research try combine benefits obtained different
planners across domains.
dominance RFAVI FOALP results implies RFAVI state
art among first-order techniques work problem lifted form use lifted
generalization. Although FOALP uses first-order structure feature representation, learned
features aimed satisfying goal predicates individually, whole. believe
goal-decomposition technique sometimes work well small problems scale well
large problems.
comparisons, noted FOALP read PPDDL domain descriptions directly, requires human-written domain axioms learning, unlike completely
automatic technique (requiring numeric parameters characterizing domain).
requirement human-written domain axioms one reasons FOALP compete
competition domains learned policy domains
tested here.
C ONJUNCTIVE -B OXWORLD11 , note FF-Replan uses outcomes problem determinization discriminate likely unlikely outcomes truck-movement
actions. result, plans frequently selected rely unlikely outcomes (perhaps choosing
move truck undesired location, relying unlikely outcome accidentally moving
desired location). plans usually fail, resulting repeated replanning luckily selects high-likelihood outcome plan execution happens get desired low-likelihood
outcome. behavior effect similar behavior learned value function exhibits because, discussed Section 7.3.2, learner failed find feature rewarding appropriate
truck moves. planners result long plan lengths due many unhelpful truck moves. However, learned policy conducts random walk trucks much efficiently (and thus
successfully) online replanning FF-Replan, especially larger problem sizes.
believe even dramatic improvements available improved knowledge representation features.
7.4 SysAdmin
full description YS DMIN domain provided work Guestrin, Koller, Parr
(2001). Here, summarize description. YS DMIN domain, machines connected
different topologies. machine might fail step, failure probability depends
number failed machines connected it. agent works toward minimizing number
failed machines rebooting machines, one machine rebooted time step. problem
n machines fixed topology, dynamic state space sufficiently described n
propositional variables, representing on/off status certain machine.
test domain purpose direct comparison performance propositional techniques published results work Patrascu et al. (2002). test exactly
topologies evaluated measure performance measure reported there, sup-norm Bellman
error.
evaluate method exact problems (same MDPs) used evaluation
work Patrascu et al. (2002) testing domain. Two different kinds topologies tested:
11. hand-convert nested universal quantifiers conditional effects original BOXWORLD domain definition
equivalent form without universal quantifiers conditional effects allow FF-Replan read domain.

735

fiW U & G IVAN





Cycle Topology

3-legs Topology

Figure 24: Illustration two topologies YS DMIN domain (10 nodes). node
represents machine. label indicates server machine, specified work
Patrascu et al. (2002).

3-legs cycle. 3-legs topology three three-node legs (each linear sequence three
connected nodes) connected single central node one end. cycle topology arranges
ten nodes one large cycle. 10 nodes topology. two topologies
illustrated Figure 24. target learning domain keep many machines
operational possible, number operating machines directly determines reward
step. Since 10 nodes basic features on/off statuses
nodes, total 1024 states. reward-scaling parameter rscale (defined Section 5
Online Appendix 1, available JAIR website, page 8) selected 10.
work Patrascu et al. (2002) uses L (sup norm) Bellman error performance
measurement YS DMIN. technique, described above, seeks reduce mean Bellman
error directly L Bellman error. report L Bellman error, averaged two
trials, Figure 25. included Figure 25 results shown work Patrascu et al.
(2002). select best result shown (from various algorithmic approaches) 3-legs
cycle topologies shown paper. correspond d-o-s setting cycle
topology d-x-n setting 3-legs topology, terminology paper.
topologies show algorithm reduces L Bellman error effectively per
feature well effectively overall experiments previously reported work
Patrascu et al. (2002). topologies show Bellman error eventually diverges AVI cannot
handle complexity error function dimensionality increases. algorithm still
achieve low Bellman error remembering restoring best-performing weighted feature set
weakened performance detected.
note superior performance reducing Bellman error could due entirely
part use AVI weight training instead approximate linear programming (ALP),
method used Patrascu et al. However, systematic superiority known AVI ALP,
results suggest superior performance feature learning itself.
736

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

SysAdmin, 3-Legs Topology
33

10

12

9

Bellman Error

8
7
6
5
4
3
2
1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

Number Features
3-legs, Learned

3-legs, Patrascu

SysAdmin, Cycle Topology
25 42

10
9

Bellman Error

8
7
6
5
4
3
2
1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

Number Features
Cycle, Learned
Cycle, Patrascu

Figure 25: L Bellman error YS DMIN domain (10 nodes) two topologies. Values
results work Patrascu et al. (2002) taken Figure 2 3
work Patrascu et al. (2002).

7.5 Demonstration Generalization Across Problem Sizes
asset relational feature representation presented paper learned relational
features applicable problem size domain. section 2.4, discussed
737

fiW U & G IVAN

Target problem sizes
10 20 Tetris 15 blocks BW (15 box, 5 city) BX 30 nodes Tire 30 files Lifted-File
Intermediate problem sizes 10 10 Tetris 10 blocks BW (10 box, 5 city) BX 15 nodes Tire 10 files Lifted-File
Generalize target size
Learn intermediate size
Random walk

55
119
0.1

1 (171)
1 (170)
0 ()

1 (76)
1 (188)
0.97 (893)

0.88 (4)
0.89 (4)
0.29 (6)

1 (25)
1 (25)
1 (88)

Figure 26: Performance intermediate-sized problems generalization. show
performance value functions learned target problem sizes evaluated
intermediate-sized problems, demonstrate generalization sizes. comparison, intermediate-sized problems, show performance value functions learned directly intermediate size well performance random
walk. Generalization results intermediate size learning results averages two
trials. ETRIS, average accumulated rows erased shown. goal-oriented
domains, success ratio successful plan length (in parentheses) shown
domain.

modeling planning domain infinite set MDPs, one problem instance
domain. infinite set MDPs, feature vector plus weight vector defines single
value function well defined every problem instance MDP. discuss question whether framework find single feature/weight vector combination generalizes
good performance across problem sizes, i.e., value function V defined combination,
whether Greedy(V ) performs similarly well different problem sizes.
Throughout Section 7, demonstrated direct application learned feature/weight
vectors target problem sizes, (without retraining weights)these results shown
target-size lines result tables domain. ETRIS, B LOCKSWORLD, C ONJUNCTIVE B OXWORLD, IREWORLD, L IFTED -F ILEWORLD 3, target-size lines demonstrate direct
successful generalization target sizes even current problem sizes significantly smaller.
(In domains, either notion problem size (S YS DMIN), insufficient planning progress significantly increase problem size learning small problems (E XPLOD ING B LOCKSWORLD , Z ENOTRAVEL , OWERS H ANOI ).)
subsection, consider generalization (larger) target sizes selected intermediate sizes five domains. Specifically, take weight vectors feature vectors
resulting end trials (i.e. weight vector retrained target sizes), apply
directly selected intermediate problem sizes without weight retraining. trials terminate learning reaches target problem sizes12 , take weights features result
best performing policy terminating problem sizes. generalization results shown
Figure 26; comparison, table shows performance intermediate-sized
problems value function learned directly size, well performance
random walk size.
12. Note one trials ETRIS terminates reaching target size due non-improving performance,
two trials L IFTED -F ILEWORLD 3 terminate target-size performance already reaches optimality
learning reaches target size. Still, although value functions learned smaller sizes
target size, value functions evaluated generalization learned significantly larger sizes
intermediate evaluation size.

738

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

domain shown random walk result much weaker generalization result,
showing presence generalization learned value functions across problem sizes.
four goal-oriented planning domains, applying value functions learned target sizes equals
performance achieved value functions learned directly intermediate sizes (with better
performance C ONJUNCTIVE -B OXWORLD). ETRIS, however, generalization result
match result learning intermediate size. note domains, solution
strategy invariant respect problem size (e.g. destroying incorrect towers form correct
ones B LOCKSWORLD). domains best plan/strategy may change dramatically
size. example, ETRIS, larger number rows board allows strategies temporary
stack uncompleted rows, smaller number rows favors strategies complete rows quickly
possible. Thus one necessarily expect generalization domain sizes every
domainthis conclusion expected hold whether considering generalization
value functions policies.
included discussion policy-based generalization related work section (Appendix A.4), focusing previous work approximate policy iteration. However, note
policies generalize problems different sizes less well defined
value functions generalize problems. previous API work, defined
policies select actions states domain size; work define value functions
assign numeric values states domain size. None work guarantees finding good
optimal policy value function; far know, problems admit good compact value
functions, admit good compact policies, admit both, neither.

8. Discussion Future Research
presented general framework automatically learning state-value functions featurediscovery gradient-based weight training. framework, greedily select features
provided hypothesis space (which parameter method) best correlate Bellman
error features, use AVI find weights associate features.
proposed two different candidate hypothesis spaces features. One two
spaces relational one features first-order formulas one free-variable, beamsearch process used greedily select hypothesis. hypothesis space considered propositional feature representation features decision trees. hypothesis
space, use standard classification algorithm C4.5 (Quinlan, 1993) build feature best
correlates sign statewise Bellman error, instead using sign magnitude.
performance feature-learning planners evaluated using reward-oriented
goal-oriented planning domains. demonstrated relational planner represents
state-of-the-art feature-discovering probabilistic planning techniques. propositional planner
perform well relational planner, cannot generalize problem instances,
suggesting knowledge representation indeed critical success feature-discovering
planners.
Although present results propositional feature-learning approach relation featurelearning approach, knowledge representation difference difference
approaches. Historically, propositional approach originally conceived reduction
classification learning, attempt capture magnitude Bellman error
739

fiW U & G IVAN

feature selection, rather focuses sign error. contrast, relational approach
counts objects order match magnitude Bellman error.
difference, cannot attribute performance differences
approaches knowledge representation choice. differences performance could due
choice match sign propositional feature selection. possible future experiment
identify sources performance variation would use propositional representation involving
regression trees (Dzeroski, Todorovski, & Urbancic, 1995) capture magnitude error.
representation might possibly perform somewhat better decision-tree representation
shown here, course would still enable generalization sizes relational
feature learner exhibits.
Bellman-error reduction course one source guidance might followed
feature discovery. experiments IPPC planning domains, find many
domains successful plan length achieved much longer optimal, discussed
Section 7.3.5. possible remedy deploying search previous work (Wu et al.,
2008) learn features targeting dynamics inside plateaus, use features decisionmaking plateaus encountered.

Acknowledgments
material based upon work supported part National Science Foundation Grant
No. 0905372.

Appendix A. Related Work
A.1 Feature Selection Approaches
A.1.1 F EATURE ELECTION

VIA

C ONSTRUCTIVE F UNCTION PPROXIMATION

Automatic feature extraction sequential decision-making studied work Utgoff
Precup (1997), via constructive function approximation (Utgoff & Precup, 1998). work
viewed forerunner general framework, limited propositional representations,
binary-valued features, new features single-literal extensions old features conjunction. work Rivest Precup (2003) variant Cascade-Correlation (Fahlman
& Lebiere, 1990), constructive neural network algorithm, combined TD-learning learn
value functions reinforcement learning. Cascade-Correlation incrementally adds hidden units
multi-layered neural networks, hidden unit essentially feature built upon set
numerically-valued basic features. work provides framework generalizing prior efforts reduction supervised learning, explicit reliance Bellman error signal,
feature hypothesis space corresponding learner deployed. particular,
demonstrate framework binary propositional features using C4.5 learner rich
numeric-valued relational features using greedy beam-search learner. work provides first
evaluation automatic feature extraction benchmark planning domains several planning
competitions.
work Utgoff Precup (1997) implicitly relies Bellman error,
explicit construction Bellman error training set discussion selecting features correlate
740

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Bellman error. instance, work focuses first refining current feature weight
updates converging poorly (high variance weight updates), whereas work focuses first
finding feature correlates statewise Bellman error, regardless whether feature refines
current feature. addition, work selects features online weights current
features adjusted, stationary target value function Bellman
error considered selection next new feature. contrast, work separates weight
training new feature selection completely. (These differences perhaps part due
reinforcement learning setting used Utgoff & Precup, 1997, opposed planning setting
work.)
selection hidden unit feature Cascade-Correlation (Fahlman & Lebiere, 1990) based
covariance feature values errors output units. output units
estimating value function, training data providing Bellman update value function,
output unit error Bellman error. Thus, hidden units learned work Rivest
Precup (2003) approximations Bellman-error features learned features are, although made explicit work. making goal capturing Bellman error explicit
here, provide general reduction facilitates use learning method capture
resulting feature-learning training sets. particular, able naturally demonstrate generalization across domain sizes several large domains, using relational feature learner. contrast,
single test domain work Rivest Precup (2003) small fixed size. Nonetheless,
work important precursor approach.
A.1.2 F EATURE C ONSTRUCTION

VIA

PECTRAL NALYSIS

Feature-learning frameworks value functions based upon spectral analysis state-space connectivity presented work Mahadevan Maggioni (2007) Petrik (2007).
frameworks, features eigenvectors connectivity matrices constructed random walk (Mahadevan & Maggioni, 2007) eigenvectors probabilistic transition matrices (Petrik, 2007).
features capture aspects long-term problem behaviours, opposed short-term behaviours
captured Bellman-error features. Bellman-error reduction requires iteration capture longterm behaviors.
Reward functions considered feature construction work Mahadevan Maggioni (2007); work Petrik (2007), reward functions incorporated
learning Krylov basis features, variant Bellman error features (Parr et al., 2008), complement eigenvector features. However, even Petriks framework, reward incorporated
features used policy evaluation rather controlled environment consider.
Essential work use machine learning factored representations handle
large statespaces generalize problems different sizes. spectral
analysis frameworks limited respect (at least current state development). approach Petrik (2007) presented explicit statespaces, factorization approach
scaling large discrete domains proposed work Mahadevan Maggioni (2007).
approach, features learned dimension factorization, independent
dimensions. believe assumption independence dimensions inappropriate
many domains, including benchmark planning domains considered work. Mahadevan Maggioni factorization approach suffers drawbacks propositional
approach: solution recomputed problems different sizes domain
741

fiW U & G IVAN

lacks flexibility generalize problems different sizes provided relational
approach.
A.2 Structural Model-based Model-free Solution Methods Markov Decision
Processes
A.2.1 R ELATIONAL R EINFORCEMENT L EARNING
work Dzeroski et al. (2001), relational reinforcement learning (RRL) system learns
logical regression trees represent Q-functions target MDPs. work related since
use relational representations automatically construct functions capture state value.
addition Q-function trees, policy tree learner introduced work Dzeroski
et al. (2001) finds policy trees based Q-function trees. learn explicit policy
description instead use greedy policies evaluation.
logical expressions RRL regression trees used decision points computing
value function (or policy) rather numerically valued features linear combination,
method. Generalization across problem sizes achieved learning policy trees; learned value
functions apply training problem sizes. date, empirical results approach
failed demonstrate ability represent value function usefully familiar planning
benchmark domains. good performance shown simplified goals placing
particular block onto particular block B, technique fails capture structure richer
problems constructing particular arrangements Blocksworld towers. RRL
entered international planning competitions. difficulties representing complex
relational value functions persist extensions original RRL work (Driessens & Dzeroski,
2004; Driessens et al., 2006), limited applicability shown benchmark planning
domains used work.
A.2.2 P OLICY L EARNING VIA B OOSTING
work Kersting Driessens (2008), boosting approach introduced incrementally
learn features represent stochastic policies. policy-iteration variant featurelearning framework, clearly differs work policy representations learned instead
value function representations. Using regression tree learner TILDE (Blockeel & De Raedt,
1998), feature learner demonstrated advantages previous RRL work task accomplishing on(A,B) 10-block problem. Applicability simple continuous domain (the corridor
world) demonstrated. line RRL work, limited applicability benchmark
planning domains shown here. One probable source limited applicability model-free
reinforcement-learning setting system model problem dynamics explicitly.
A.2.3 F ITTED VALUE TERATION
Gordon (1995) presented method value iteration called fitted value iteration suitable
large state spaces require direct feature selection. Instead, method relies
provided kernel function measuring similarity states. Selection kernel function
viewed kind feature selection, kernel identifies state aspects significant
measuring similarity. knowledge, techniques class applied
large relational planning problems evaluated paper. note selection
742

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

single relational kernel domains would measure state similarity domain-independent
manner thus believe kernel could adapt individual domains way
work does. Thus would expect inferior performance approach; however,
remains investigated. Selection domain-specific kernels stochastic planning domains,
automatically, yet explored.
A.2.4 E XACT VALUE TERATION F IRST- ORDER MDP
Previous work used lifted techniques exactly solve first-order MDPs reformulating exact
solution techniques explicit MDPs, value iteration. Boutilier et al. (2001) Holldobler Skvortsova (2004) independently used two different first-order languages (situation
calculus fluent calculus, respectively) define first-order MDPs. works, Bellman update procedure value iteration reformulated using respective calculus, resulting
two first-order dynamic-programming methods: symbolic dynamic programming (SDP), firstorder value iteration (FOVI). simple boxworld example human-assisted computation
demonstrated SDP work, method serves basis FOALP (Sanner & Boutilier,
2009), replaces exact techniques heuristic approximation order scale techniques
benchmark planning domains. Application FOVI planning domains demonstrated
colored blocksworld benchmark, limited 10 blocks (Holldobler, Karabaev, &
Skvortsova, 2006).
work Kersting et al. (2004), constraint logic programming used define relational
value iteration method. MDP components, states, actions, rewards, first abstracted
form Markov decision program, lifted version MDP. relational Bellman operation
(ReBel) used define updates Q-values state values. Empirical study ReBel
approach limited 10-step backups single-predicate goals blocksworld
logistics domains.
Exact techniques suffer difficulty representing full complexity state-value
function arbitrary goals even mildly complex domains. previous works serve illustrate central motivation using problem features compactly approximate structure
complex value function, thus motivate automatic extraction features studied
work.
A.3 Comparison Inductive Logic Programming Algorithms
problem selecting numeric function relational states match Bellman-error training
set first-order regression problem available systems described
Inductive logic programming (ILP) literature (Quinlan, 1996; Karalic & Bratko, 1997).
important note ILP work studied learning classifiers relational
data (Muggleton, 1991), concerned learning numeric functions relational
data (such states). latter problem called first-order regression within ILP literature, received less study relational classification. Here, choose design
proof-of-concept relational learner experiments rather use one previous
systems. Separate work needed compare utility relational learner previous
regression systems; purpose demonstrate utility Bellman-error training data
finding decision-theoretic value-function features. simple learner suffices create
state-of-the-art domain-independent planning via automatic feature selection.
743

fiW U & G IVAN

ILP classification systems often proceed either general specific, specific
general, seeking concept match training data. regression, however,
easy ordering numeric functions searched. design instead method searches
basic logical expression language simple expressions complex expressions, seeking
good matches training data. order control branching factor, still allowing
complex expressions considered, heuristically build long expressions
short expressions score best. words, use beam search space expressions.
several heuristic aspects method. First, define heuristic set basic expressions search begins. Second, define heuristic method combining
expressions build complex expressions. two heuristic elements designed
logical formula without disjunction, one free variable, built repeated combination basic expressions. Finally, assumption high-scoring expressions built
high-scoring parts heuristic (and often true). critical heuristic assumption
makes likely learner often miss complex features match training data well.
known method guarantees tractably finding features.
A.4 Approximate Policy Iteration Relational Domains
planners use greedy policies derived learned value functions. Alternatively, one directly learn representations policies. policy-tree learning work Dzeroski et al.
(2001), discussed previously Appendix A.2.1, one example. Recent work uses relational decision-list language learn policies small example problems generalize well
perform large problems (Khardon, 1999; Martin & Geffner, 2004; Yoon et al., 2002). Due
inductive nature line work, however, selected policies occasionally contain severe
flaws, mechanism provided policy improvement. policy improvement quite
challenging due astronomically large highly structured state spaces relational policy
language.
work Fern et al. (2006), approximate version policy iteration addressing
issues presented. Starting base policy, approximate policy iteration iteratively generates
training data improved policy (using policy rollout) uses learning algorithm
work Yoon et al. (2002) capture improved policy compact decision-list language
again. Similar work, learner work Fern et al. (2006) aims take flawed
solution structure improve quality using conventional MDP techniques (in case, finding
improved policy policy rollout) machine learning. Unlike work, work Fern
et al. (2006) improved policies learned form logical decision lists. work
viewed complementary previous work exploring structured representation value
functions work explored structured representation policies. approaches
likely relevant important long-term effort solve structured stochastic decisionmaking problems.
note feature-based representation, considered generally MDP literature, used represent value functions rather policies. Compact representation policies
done via value functions (with greedy execution) directly, example, using decision lists. previous API work discussed uses direct representation policies, never
uses compact representation value functions. Instead, sampling value functions used
policy evaluation step policy iteration.
744

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

One imagine different novel approach API compact feature-based
representation used value functions, greedy execution policy representation.
approach, feature discovery similar explore value iteration could designed assist policy evaluation phase policy iteration. leave development
evaluation idea future work. expect two approaches API, well
current approach value iteration, advantages disadvantages vary domain
ways yet well understood. domains natural compact direct policy
representations (run see tarantula), whereas others naturally compactly represented
via value functions (prefer restaurants good review ratings). Research area must
eventually develop means combine compact representations effectively.
A.5 Automatic Extraction Domain Knowledge
substantial literature learning plan using methods direct representation
value function reactive policy, especially deterministic planning literature.
techniques related acquire domain specific knowledge via planning experience domain. Much literature targets control knowledge particular search-based
planners (Estlin & Mooney, 1997; Kambhampati et al., 1996; Veloso et al., 1995), distant
approach focus particular planning technology used limitation
deterministic domains. unclear generalize work value-function construction
probabilistic domains.
However, broader learning-to-plan literature contains work producing declarative
learned domain knowledge could well exploited feature discovery value function representation. work Fox Long (1998), pre-processing module called TIM
able infer useful domain-specific problem-specific structures, typing objects
state invariants, descriptions domain definition initial states. invariants
targeted work improving planning efficiency Graphplan based planner, suggest
future work could exploit invariants discovering features value function representation. Similarly, work Gerevini Schubert (1998), DISCOPLAN infers state constraints
domain definition initial state order improve performance SAT-based planners; again, constraints could incorporated feature search method
date.

Appendix B. Results Discussions Five Probabilistic Planning Competition
Domains
Section 7.3, presented results relational propositional feature learners
B LOCKSWORLD C ONJUNCTIVE -B OXWORLD. present results relational
feature learner following five probabilistic planning competition domains: IREWORLD,
Z ENOTRAVEL, E XPLODING B LOCKSWORLD, OWERS H ANOI, L IFTED -F ILEWORLD 3.
B.1 Tireworld
use IREWORLD domain second IPPC. agent needs drive vehicle
graph start node goal node. moving one node adjacent node,
vehicle certain chance suffering flat tire (while still arriving adjacent node).
745

fiW U & G IVAN

Trial #1
# features
0
1
2
3
3
3
4
4
5
Problem difficulty
4
4
4
4
5
6
6
9
9
Success ratio
0.52 0.81 0.84 0.86 0.86 0.84 0.88 0.85 0.86
Plan length
4
3
4
2
2
2
3
3
4
Accumulated time (Hr.) 0.3 3.1 12 17 18 18 19 21 22
0.17 0.53 0.81 0.83 0.83 0.82 0.90 0.91 0.91
Target size SR
Target size Slen.
5
4
9
5
4
4
6
6
6

5
10
0.86
4
23
0.91
6

Trial #2
# features
0
1
2
3
3
3
4
4
4
Problem difficulty
4
4
4
4
5
6
6
10 20
0.52 0.81 0.85 0.86 0.93 0.81 0.89 0.85 0.86
Success ratio
Plan length
4
3
3
2
3
2
3
4
4
Accumulated time (Hr.) 0.5 3.7 6.9 10 11 11 12 14 18
Target size SR
0.19 0.49 0.80 0.82 0.91 0.62 0.92 0.91 0.90
Target size Slen.
7
3
9
4
5
2
5
5
6

4
30
0.88
5
24
0.88
6

5
20
0.91
5
29
0.92
5

5
30
0.91
5
36
0.92
6

Figure 27: IREWORLD performance (averaged 600 problems) relational learner. add
one feature per column success ratio exceeds 0.85 average successful plan
length less 4n, n nodes, increase problem difficulty next
column. Plan lengths shown successful trials only. Problem difficulties measured
number nodes, target problem size 30 nodes. columns omitted
discussed Section 7.1.

flat tire replaced spare tire, spare tire present node
containing vehicle, vehicle carrying spare tire. vehicle pick spare
tire already carrying one one present node containing vehicle.
default setting second-IPPC problem generator domain defines problem distribution
includes problems policy achieving goal probability one.
problems create tradeoff goal-achievement probability expected number steps
goal. strongly planner favors goal achievement versus short trajectories goal
determined choice discount factor made Section 6.1.
start 4-node problems relational learner increase n nodes n + 1
nodes whenever success ratio exceeds 0.85 average successful plan length better
4n steps. target problem size 30 nodes. results shown Figures 18 27.
IREWORLD, relational learner able find features generalize well large
problems. learner achieves success ratio 0.9 30 node problems. unknown
whether policy exceed success ratio problem distribution; however, neither
comparison planner, FOALP FF-Replan, finds higher success-rate policy.
note improvements success rate domain necessarily associated
increases plan length success-rate improvements may due path deviations
acquire spare tires.
746

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Trial #1
# features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
3,1,1
0.79
253
0.75
0.06
916

1
3,1,1
0.8
255
1.7
0.08
1024

1
3,2,2
0.59
413
3.4
0.09
1064

2
3,2,2
0.52
440
7.1
0.09
1114

3
3,2,2
0.54
437
11
0.12
1050

4
3,2,2
0.55
450
15
0.11
1125

5
3,2,2
0.54
411
19
0.10
1111

6
3,2,2
0.52
440
25
0.08
1115

7
3,2,2
0.56
426
30
0.11
1061

8
3,2,2
0.53
428
36
0.08
1174

9
3,2,2
0.55
451
41
0.12
1195

Trial #2
# features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size SR
Target size Slen.

0
3,1,1
0.77
262
1.3
0.05
814

1
3,1,1
0.79
254
2.3
0.10
1008

2
3,1,1
0.80
233
3.3
0.10
1007

2
3,2,2
0.55
391
5.3
0.09
1067

3
3,2,2
0.55
425
8.9
0.09
1088

4
3,2,2
0.50
415
13
0.08
1014

5
3,2,2
0.53
422
17
0.10
1078

6
3,2,2
0.12
0
22
0.02
0

7
3,2,2
0.12
0
29
0.02
0

8
3,2,2
0.12
0
36
0.02
0

9
3,2,2
0.10
0
43
0.01
0

Figure 28: Z ENOTRAVEL performance (averaged 600 problems) relational learner.
problem difficulty shown table lists numbers cities, travelers, aircraft,
target problem size 10 cities, 2 travelers, 2 aircraft. add one feature
per column success ratio exceeds 0.8, increase problem difficulty
next column. Plan lengths shown successful trials only.

B.2 Zenotravel
use Z ENOTRAVEL domain second IPPC. goal domain fly travelers original location destination. Planes (finite-range, discrete) fuel levels,
need re-fuelled fuel level reaches zero cont inue flying. available activity
(boarding, debarking, flying, zooming, refueling) divided two stages, activity
X modelled two actions start-X finish-X. finish-X activity (high) probability
nothing. start action taken, corresponding finish action must taken
(repeatedly) succeeds conflicting action started. structure allows
failure rates finish actions simulate action costs (which used explicitly
problem representation competition). plane moved locations flying
zooming. Zooming uses fuel flying, higher success probability.
start problem difficulty 3 cities, 1 traveler, 1 aircraft using relational
feature learner. Whenever success ratio exceeds 0.8, increase number n travelers
aircraft 1 number cities less 5n 2, increase number cities one
otherwise. target problem size 10 cities, 2 travelers, 2 aircraft. Z ENOTRAVEL results
relational learner shown Figures 18 28.
747

fiW U & G IVAN

relational learner unable find features enable AVI achieve threshold success
rate (0.8) 3 cities, 2 travelers, 2 aircraft, although 9 relational features learned. trials
stopped improvement performance achieved several iterations feature
addition. Using broader search (W = 160, q = 3, = 3) able find better features
extend solvable size several cities success rate 0.9 (not shown results
paper use search parameters, reported Wu & Givan, 2007), runtime
increases dramatically, weeks. believe speed effectiveness relational learning
needs improved excel domain, likely major factor improved knowledge
representation features key concepts Z ENOTRAVEL easily represented.
Trial two Figure 28 shows striking event adding single new feature useful value
function results value function greedy policy cannot find goal all,
success ratio degrades dramatically immediately. Note small problem size,
ten percent problems trivial, initial state satisfies goal. addition
sixth feature trial two, problems policy solve. reflects
unreliability AVI weight-selection technique aspect feature discovery:
all, AVI free assign zero weight new feature, not. Additional study
control AVI and/or replacement AVI linear programming methods indicated
phenomenon; however, rare event extensive experiments.
B.3 Exploding Blocksworld
use E XPLODING B LOCKSWORLD second IPPC evaluate relational planner.
domain differs normal Blocksworld largely due blocks certain probability detonated put down, destroying objects beneath (but
detonating block). Blocks already detonated detonated again. goal
state domain described tower fragments, fragments generally required
table. Destroyed objects cannot picked up, blocks cannot put destroyed objects (but destroyed object still part goal necessary relationships
established destroyed).
start 3-block problems using relational learner increase n blocks n + 1
blocks whenever success ratio exceeds 0.7. target problem sizes 5 10 blocks.
E XPLODING B LOCKSWORLD results relational learner shown Figures 19 29.
results E XPLODING B LOCKSWORLD good enough planner increase
difficulty beyond 4-block problems, results show limited generalization 5-block
problems, little generalization 10-block problems.
performance domain quite weak. believe due presence many
dead-end states reachable high probability. states either table
one blocks needed goal destroyed, object question achieved
required properties. planner find meaningful relevant features: planner discovers
undesirable destroy table, instance. However, resulting partial understanding domain cannot augmented random walk (as domains
B LOCKSWORLD C ONJUNCTIVE -B OXWORLD) enable steady improvement value, leading goal; random walk domain invariably lands agent dead end. short
successful plan length, low probability reaching goal, (not shown here) high unsuccessful plan length (caused wandering dead end region) suggest need new techniques
748

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Trial #1
# features
0
1
2
3
Problem difficulty
3
3
3
3
Success ratio
0.56 0.58 0.56 0.63
Plan length
1
2
1
2
Accumulated time (Hr.) 0.6 1.4 2.2 3.1
0.12 0.12 0.14 0.22
Target size #1 SR
Target size #1 Slen.
3
3
3
5
Target size #2 SR
0
0
0 0.00
Target size #2 Slen.


10

4
3
0.56
1
4.2
0.20
4
0.00
4

Trial #2
# features
0
1
2
3
4
Problem difficulty
3
3
3
3
3
Success ratio
0.56 0.56 0.55 0.63 0.55
Plan length
1
2
1
2
1
Accumulated time (Hr.) 0.6 1.3 2.1 2.9 3.7
Target size #1 SR
0.14 0.15 0.12 0.18 0.17
4
3
4
6
4
Target size #1 Slen.
Target size #2 SR
0
0
0 0.01 0.00
Target size #2 Slen.


19 18

5
6
7
7
8
9
3
3
3
4
4
4
0.68 0.62 0.71 0.4 0.45 0.43
1
2
2
4
5
4
5.9 8.7 11 12 20 28
0.31 0.16 0.34 0.33 0.31 0.31
6
9
6
6
5
5
0.03 0 0.02 0.03 0.02 0.02
24 19 26 23 22

5
3
0.75
2
4.6
0.33
6
0.02
26

5
4
0.45
4
5.3
0.31
6
0.01
27

6
4
0.45
5
14
0.32
6
0.01
15

7
4
0.43
5
22
0.31
6
0.02
21

8
4
0.42
4
31
0.28
5
0.01
15

10
4
0.44
5
38
0.29
5
0.01
15

9
4
0.36
4
39
0.30
5
0.01
18

Figure 29: E XPLODING B LOCKSWORLD performance (averaged 600 problems) relational
learner. Problem difficulties measured number blocks. add one feature per
column success ratio exceeds 0.7, increase problem difficulty next
column. Plan lengths shown successful trials only. Target problem size #1 5
blocks, target problem size #2 10 blocks.

aimed handling dead-end regions handle domain. results demonstrate technique relies random walk (or form search) learned features need
completely describe desired policy.
B.4 Towers Hanoi
use domain OWERS H ANOI first IPPC. probabilistic version wellknown problem, agent move one two discs simultaneously, small probability
going dead-end state move, probability depends whether largest disc
moved type disc move (one two time) used. note
one planning problem problem size here.
important note 100% success rate generally unachievable domain due
unavoidable dead-end states.
749

fiW U & G IVAN

Trial #1
# features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size #1 SR
Target size #1 Slen.
Target size #2 SR
Target size #2 Slen.

0
1
1
2
3
3
4
5
6
7
8
8 20 38
2
2
3
3
3
4
4
4
4
4
4
5
5
5
0.70 0.75 0.11 0.44 0.73 0
0
0
0
0 0.51 0
0
0
4
2
43
26
4





4



0.0 0.0 0.1 0.2 0.3 0.4 0.5 1.1 1.2 2.1 2.2 2.3 18 53
0.07 0.15 0.01 0.08 0.03 0
0
0
0
0 0.52 0.53 0 0.43
13
9
90
95
37





4
4

4
0.00 0
0
0 0.00 0
0
0
0
0
0
0
0
0
11



107









Trial #2
# features
Problem difficulty
Success ratio
Plan length
Accumulated time (Hr.)
Target size #1 SR
Target size #1 Slen.
Target size #2 SR
Target size #2 Slen.

0
0
1
2
3
3
4
5
6
7
8
2
3
3
3
3
4
4
4
4
4
4
0.71 0.23 0.14 0.42 0.75 0
0
0
0
0 0.53
4
12
37
25
4





4
0.0 0.0 0.2 0.3 0.3 0.4 0.5 1.1 1.9 2.3 2.6
0.1 0.09 0.0 0.09 0.03 0
0
0
0
0 0.49
14
11 105 95
41





4
0.00 0.1
0
0 0.00 0
0
0
0
0
0
16
29


107






8
5
0

2.7
0

0


20
5
0

6
0

0


38
5
0

16
0

0


Figure 30: OWERS H ANOI performance (averaged 600 problems) relational learner.
add one feature per column success ratio exceeds 0.7n1 n discs,
increase problem difficulty next column. Plan lengths shown successful trials
only. Problem difficulties measured number discs, target problem size #1
4 discs size #2 5 discs. columns omitted discussed Section 7.1.

start 2-disc problem relational learner increase problem difficulty
n discs n + 1 discs whenever success ratio exceeds 0.7n1 . target problem sizes
4 5 discs. OWERS H ANOI results relational learner shown Figures 19 30.
learner clearly able adapt three- four-disc problems, achieving around 50%
success rate four disc problem trials. optimal solution four disc problem
success rate 75%. policy uses single disc moves large disc moved
uses double disc moves. Policies use single disc moves double disc moves
achieve success rates 64% 58%, respectively, four disc problem. learned solution
occasionally moves disc way get closer goal, reducing success.
Unfortunately, trials show increasing number new features needed adapt
larger problem size, trials even 38 total features enough adapt
five-disc problem. Thus, know approach extend even five discs. Moreover,
results indicate poor generalization problem sizes.
believe difficult learner (and humans) represent good value function
across problem sizes. Humans deal domain formulating good recursive policy,
establishing direct idea value state. Finding recursive policy automatically
interesting open research question outside scope paper.
750

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

B.5 Lifted-Fileworld3
described Section 6.1, use domain L IFTED -F ILEWORLD 3, straightforwardly
lifted form F ILEWORLD first IPPC, restricted three folders. reach goal filing
files, action needs taken file randomly determine folder file
go into. actions taking folder, putting file folder, returning folder
cabinet. goal reached files correctly filed targeted folders.
note F ILEWORLD L IFTED -F ILEWORLD 3 benign domains.
reachable dead ends non-optimal actions, directly reversible.
Random walk solves domain success rate one even thirty files. technical challenge
posed minimize unnecessary steps minimize plan length. optimal policy
solves n-file problem 2n + 1 2n + 5 steps, depending random file types
generated.
Rather preset plan-length threshold increasing difficulty (as function n),
adopt policy increasing difficulty whenever method fails improve plan length adding
features. Specifically, success ratio exceeds 0.9 one feature added without improving
plan length, remove feature increase problem difficulty instead.13
start 1 file problems relational learner increase n files n + 1 files
whenever performance improve upon feature addition. target problem size 30
files. L IFTED -F ILEWORLD 3 results relational learner shown Figures 20 31.
results show planner acquires optimal policy 30-file target size problem
learning four features, two trials. results domain reveal
weakness AVI weight-selection method. Although four features enough define optimal policy, problem difficulty increases, AVI often fails find weight assignment producing
policy. happens, feature addition triggered, trial 1.
domain, results show extra features prevent AVI finding good weights
subsequent iterations, optimal policy recovered larger feature set. Nonetheless, another indication improved performance may available via work alternative
weight-selection approaches, orthogonal topic feature selection.

References
Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge
planning. Artificial Intelligence, 116, 123191.
Bertsekas, D. P. (1995). Dynamic Programming Optimal Control. Athena Scientific.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Blockeel, H., & De Raedt, L. (1998). Top-down induction first-order logical decision trees.
Artificial Intelligence, 101, 285297.
Bonet, B., & Givan, R. (2006). Non-deterministic planning track 2006 international planning
competition. Website. http://www.ldc.usb.ve/ bonet/ipc5/.
13. possible specify plan-length threshold function triggering increase difficulty domain,
done domains. find domain quite sensitive choice function, end
must chosen trigger difficulty increase feature addition fruitless current difficulty.
So, directly implemented automatic method triggering difficulty increase.

751

fiW U & G IVAN

Trial #1
# features
0 1 2
1 1 1
Problem difficulty
Success ratio
1 1 1
14 8 4
Plan length
Accumulated time (Hr.) 0.0 0.0 0.0
Target size SR
1 1 1
Target size Slen.
251 134 87

3
1
1
3
0.0
0


3
2
1
7
0.1
0


4
2
1
6
0.1
0


4
3
1
9
0.1
0


4
4
1
11
0.2
1.00
87

4
12
1
29
5.9
1
93

4
13
1
31
7.3
1
65

4
14
1
49
8.9
1
90

5
14
1
37
10
1
91

5
15
1
35
13
1
65

5
16
1
55
15
1
91

6
16
1
37
17
1
65

7
16
1
37
19
1
65

7
18
1
41
37
1
65

7
19
1
43
49
1
111

7
20
1
45
62
1
65

Trial #2
# features
0 1 2
Problem difficulty
1 1 1
Success ratio
1 1 1
14 8 4
Plan length
Accumulated time (Hr.) 0.0 0.0 0.0
1 1 1
Target size SR
Target size Slen.
251 135 88

3
1
1
3
0.0
0


3
2
1
7
0.1
0


4
2
1
6
0.1
0


4
3
1
9
0.1
0


4
4 4 4 4
4
5 8 9 10
1
1 1 1 1
12 14 21 23 25
0.2 0.6 2.5 3.1 3.9
0.96 1 1 1 1
85 88 82 82 91

4
14
1
33
9.0
1
96

4
15
1
35
11
1
87

4
16
1
62
13
1
91

4
17
1
65
19
1
93

4
18
1
41
27
1
97

4
19
1
43
30
1
65

4
20
1
49
34
1
65

4
23
1
91
50
1
107

4
24
1
53
66
1
82

4
25
1
55
74
1
65

4
8
1
21
2.4
1.00
82

4
10
1
25
3.8
1
91

4
11
1
30
4.8
1
88

Figure 31: L IFTED -F ILEWORLD 3 performance (averaged 600 problems) relational
learner. add one feature per column success ratio exceeds 0.9 adding one
extra feature improve plan length, increase problem difficulty
next column (after removing extra feature). Plan lengths shown successful trials
only. Problem difficulties measured number files, target problem size
30 files. columns omitted discussed Section 7.1.

Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order MDPs.
Proceedings Seventeenth International Joint Conference Artificial Intelligence,
pp. 690700.
Chandra, A., & Merlin, P. (1977). Optimal implementation conjunctive queries relational data
bases. Proceedings Ninth Annual ACM Symposium Theory Computing, pp.
7790.
Davis, R., & Lenat, D. (1982). Knowledge-Based Systems Artificial Intelligence. McGraw-Hill,
New York.
Driessens, K., & Dzeroski, S. (2004). Integrating guidance relational reinforcement learning.
Machine Learning, 57, 271304.
Driessens, K., Ramon, J., & Gartner, T. (2006). Graph kernels gaussian processes relational
reinforcement learning. Machine Learning, 64, 91119.
Dzeroski, S., DeRaedt, L., & Driessens, K. (2001). Relational reinforcement learning. Machine
Learning, 43, 752.
Dzeroski, S., Todorovski, L., & Urbancic, T. (1995). Handling real numbers ILP: step towards
better behavioural clones. Proceedings Eighth European Conference Machine
Learning, pp. 283286.
752

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Estlin, T. A., & Mooney, R. J. (1997). Learning improve efficiency quality planning.
Proceedings Fifteenth International Joint Conference Artificial Intelligence, pp.
12271232.
Fahlman, S., & Lebiere, C. (1990). cascade-correlation learning architecture. Advances
Neural Information Processing Systems 2, pp. 524 532.
Farias, V. F., & Van Roy, B. (2004). Tetris: study randomized constraint sampling. Probabilistic Randomized Methods Design Uncertainty. Springer-Verlag.
Fawcett, T. (1996). Knowledge-based feature discovery evaluation functions. Computational
Intelligence, 12(1), 4264.
Fern, A., Yoon, S., & Givan, R. (2004). Learning domain-specific control knowledge random
walks. Proceedings Fourteenth International Conference Automated Planning
Scheduling, pp. 191199.
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration policy language bias:
Solving relational Markov decision processes. Journal Artificial Intelligence Research, 25,
75118.
Fox, M., & Long, D. (1998). automatic inference state invariants TIM. Journal Artificial
Intelligence Research, 9, 367421.
Gerevini, A., & Schubert, L. (1998). Inferring state constraints domain-independent planning.
Proceedings Fifteenth National Conference Artificial Intelligence, pp. 905912.
Gordon, G. (1995). Stable function approximation dynamic programming. Proceedings
Twelfth International Conference Machine Learning, pp. 261268.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression inductive policy selection.
Proceedings Twentieth Conference Uncertainty Artificial Intelligence, pp. 217
225.
Guestrin, C., Koller, D., & Parr, R. (2001). Max-norm projections factored MDPs. Proceedings Seventeenth International Joint Conference Artificial Intelligence, pp. 673680.
Holldobler, S., Karabaev, E., & Skvortsova, O. (2006). FluCaP: heuristic search planner
first-order MDPs. Journal Artificial Intelligence Research, 27, 419439.
Holldobler, S., & Skvortsova, O. (2004). logic-based approach dynamic programming.
Proceedings Workshop Learning Planning Markov ProcessesAdvances
Challenges Nineteenth National Conference Artificial Intelligence, pp. 3136.
Kakade, S. (2001). natural policy gradient. Advances Neural Information Processing
Systems 14, pp. 15311538.
Kambhampati, S., Katukam, S., & Qu, Y. (1996). Failure driven dynamic search control partial
order planners: explanation based approach. Artificial Intelligence, 88(1-2), 253315.
Karalic, A., & Bratko, I. (1997). First order regression. Machine Learning, 26, 147176.
Keller, P., Mannor, S., & Precup, D. (2006). Automatic basis function construction approximate dynamic programming reinforcement learning. Proceedings Twenty-Third
International Conference Machine Learning, pp. 449456.
753

fiW U & G IVAN

Kersting, K., Van Otterlo, M., & De Raedt, L. (2004). Bellman goes relational. Proceedings
Twenty-First International Conference Machine Learning, pp. 465472.
Kersting, K., & Driessens, K. (2008). Non-parametric policy gradients: unified treatment
propositional relational domains. Proceedings Twenty-Fifth International Conference Machine learning, pp. 456463.
Khardon, R. (1999). Learning action strategies planning domains. Artificial Intelligence, 113(12), 125148.
Lagoudakis, M. G., Parr, R., & Littman, M. L. (2002). Least-squares methods reinforcement
learning control. SETN 02: Proceedings Second Hellenic Conference AI, pp.
249260.
Mahadevan, S., & Maggioni, M. (2007). Proto-value functions: Laplacian framework learning representation control Markov decision processes. Journal Machine Learning
Research, 8, 21692231.
Martin, M., & Geffner, H. (2004). Learning generalized policies planning examples using
concept languages. Applied Intelligence, 20, 919.
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
Muggleton, S. (1991). Inductive logic programming. New Generation Computing, 8(4), 295318.
Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., & Littman, M. (2008). analysis linear
models, linear value-function approximation, feature selection reinforcement learning.
Proceedings Twenty-Fifth International Conference Machine Learning, pp. 752
759.
Parr, R., Painter-Wakefield, C., Li, L., & Littman, M. (2007). Analyzing feature generation
value-function approximation. Proceedings Twenty-Fourth International Conference
Machine Learning, pp. 737744.
Patrascu, R., Poupart, P., Schuurmans, D., Boutilier, C., & Guestrin, C. (2002). Greedy linear valueapproximation factored Markov decision processes. Proceedings Eighteenth
National Conference Artificial Intelligence, pp. 285291.
Petrik, M. (2007). analysis Laplacian methods value function approximation MDPs.
Proceedings twentith International Joint Conference Artificial Intelligence, pp.
25742579.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann.
Quinlan, J. R. (1996). Learning first-order definitions functions. Journal Artificial Intelligence
Research, 5, 139161.
Rivest, F., & Precup, D. (2003). Combining TD-learning cascade-correlation networks.
Proceedings Twentieth International Conference Machine Learning, pp. 632639.
Sanner, S., & Boutilier, C. (2006). Practical linear value-approximation techniques first-order
MDPs. Proceedings Twenty-Second Conference Uncertainty Artificial Intelligence, pp. 409417.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques first-order MDPs. Artificial
Intelligence, 173(5-6), 748788.
754

fiAUTOMATIC NDUCTION B ELLMAN -E RROR F EATURES P ROBABILISTIC P LANNING

Singh, S., Jaakkola, T., Littman, M., & Szepesvari, C. (2000). Convergence results single-step
on-policy reinforcement-learning algorithms. Machine Learning, 38(3), 287308.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine Learning,
3, 944.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press.
Szita, I., & Lorincz, A. (2006). Learning tetris using noisy cross-entropy method. Neural
Computation, 18, 29362941.
Tesauro, G. (1995). Temporal difference learning TD-Gammon. Communications ACM,
38(3), 5868.
Tsitsiklis, J., & Roy, B. V. (1997). analysis temporal-difference learning function approximation. IEEE Transactions Automatic Control, 42(5), 674690.
Utgoff, P. E., & Precup, D. (1997). Relative value function approximation. Tech. rep., University
Massachusetts, Department Computer Science.
Utgoff, P. E., & Precup, D. (1998). Constuctive function approximation. Motoda, & Liu (Eds.),
Feature Extraction, Construction, Selection: Data-Mining Perspective, pp. 219235.
Kluwer.
Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating planning
learning: PRODIGY architecture. Journal Experimental Theoretical AI, 7(1),
81120.
Widrow, B., & Hoff, Jr, M. E. (1960). Adaptive switching circuits. IRE WESCON Convention
Record, 96104.
Williams, R. J., & Baird, L. C. (1993). Tight performance bounds greedy policies based
imperfect value functions. Tech. rep., Northeastern University.
Wu, J., & Givan, R. (2007). Discovering relational domain features probabilistic planning.
Proceedings Seventeenth International Conference Automated Planning
Scheduling, pp. 344351.
Wu, J., Kalyanam, R., & Givan, R. (2008). Stochastic enforced hill-climbing. Proceedings
Eighteenth International Conference Automated Planning Scheduling, pp. 396403.
Wu, J., & Givan, R. (2005). Feature-discovering approximate value iteration methods. Proceedings Symposium Abstraction, Reformulation, Approximation, pp. 321331.
Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection first-order MDPs. Proceedings Eighteenth Conference Uncertainty Artificial Intelligence, pp. 568576.
Yoon, S., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning. Proceedings Seventeenth International Conference Automated Planning Scheduling, pp. 352358.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). first probabilistic track
international planning competition. Journal Artificial Intelligence Research, 24, 851887.

755


