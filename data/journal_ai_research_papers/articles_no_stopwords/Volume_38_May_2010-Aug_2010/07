journal artificial intelligence

submitted published

minimum relative entropy principle
learning acting
pedro ortega
daniel braun

peortega dcc uchile cl
dab cam ac uk

department engineering
university cambridge
cambridge cb pz uk

abstract
proposes method construct adaptive agent universal
respect given class experts expert designed specifically particular
environment adaptive control formalized minimizing
relative entropy adaptive agent expert suitable
unknown environment agent passive observer optimal solution
well known bayesian predictor however agent active past actions need
treated causal interventions stream rather normal probability
conditions shown solution variational given
stochastic controller called bayesian control rule implements adaptive
behavior mixture experts furthermore shown mild assumptions
bayesian control rule converges control law suitable expert

introduction
behavior environment control signal fully known
designer choose agent produces desired dynamics instances include hitting target cannon known weather conditions solving maze
map controlling robotic arm manufacturing plant however
environment unknown designer faces adaptive control
example shooting cannon lacking appropriate measurement equipment finding
way unknown maze designing autonomous robot martian exploration
adaptive control turns far difficult non adaptive counterpart
good policy carefully trade explorative versus exploitative actions
e actions identification environments dynamics versus actions control
desired way even environments dynamics known belong particular class optimal agents available constructing corresponding optimal
adaptive agent general computationally intractable even simple toy duff
thus finding tractable approximations major focus
recently proposed reformulate statement classes
control minimization relative entropy criterion example
large class optimal control solved efficiently statement
reformulated minimization deviation dynamics controlled system
uncontrolled system todorov kappen gomez opper
work similar introduced adaptive control class agents
c

ai access foundation rights reserved

fiortega braun

given agent tailored different environment adaptive controllers
derived minimum relative entropy principle particular one construct
adaptive agent universal respect class minimizing average relative
entropy environment specific agent
however extension straightforward syntactical difference
actions observations taken account formulating variational
specifically actions treated interventions obeying rules
causality pearl spirtes glymour scheines dawid distinction
made variational unique solution given stochastic control rule
called bayesian control rule control rule particularly interesting
translates adaptive control line inference applied
forward time furthermore work shows mild assumptions adaptive
agent converges environment specific agent
organized follows section introduces notation sets adaptive
control section formulates adaptive control minimum relative entropy
initial nave need causal considerations motivated
bayesian control rule derived revised relative entropy criterion
section conditions convergence examined proof given section
illustrates usage bayesian control rule multi armed bandit
undiscounted markov decision processes section discusses properties bayesian
control rule relates previous work literature section concludes

preliminaries
following agent environment formalized causal
sequences agent environment coupled exchange symbols following standard
interaction protocol discrete time observation control signals treatment
dynamics fully probabilistic particular actions observations
random variables contrast typical decision theoretic agent formulation
treating observations random variables russell norvig proofs
provided appendix
notation set denoted calligraphic letter words set alphabet
element symbol used mean thing respectively strings finite
concatenations symbols sequences infinite
concatenations denotes set


strings length n n set finite strings furthermore ai defined set one way
infinite sequences alphabet tuples written parentheses
strings notation ai ai shorthand string starting first index symbols underlined glue together ao
aoi ai oi function log x meant taken w r base unless
indicated otherwise
interactions possible symbols drawn two finite sets let denote
set inputs observations let denote set outputs actions set z ao
interaction set string aot ao interaction string optionally ending


fia minimum relative entropy principle learning acting

ot ak ok similarly one sided infinite sequence
interaction sequence set interaction strings length denoted z
sets finite interaction strings sequences denoted z z respectively
interaction string length denoted
system agents environments formalized systems system
probability distribution pr interaction sequences z pr uniquely determined
conditional probabilities
pr ao

pr ot ao



aot z conditional probabilities represent generative law
propensity case issuing symbol evidential probability plausibility
case observing symbol two interpretations applies particular case
becomes apparent system coupled another system

agent
p



environment
q

figure model interactions agent p environment q define probability distribution interaction sequences

interaction system let p q two systems interaction system p q
coupling two systems giving rise generative distribution g describes
probabilities actually govern stream two systems coupled g
specified equations
g ao p ao
g ot ao q ot ao
valid aot z g true probability distribution interaction
sequences arises coupling two systems streams specifically
system p p ao probability producing action given history
ao p ot ao predicted probability observation ot given history


fiortega braun

ao hence p sequence input stream sequence
output stream contrast roles actions observations reversed
case system q thus sequence output stream sequence
input stream previous model interaction fairly general many
interaction protocols translated scheme convention given
interaction system p q p agent constructed designer q
environment controlled agent figure illustrates setup
control environment q said known iff agent p property
aot z
p ot ao q ot ao
intuitively means agent knows statistics environments future
behavior past particular knows effects given controls
environment known designer agent build custom made policy
p resulting generative distribution g produces interaction sequences
desirable done multiple ways instance controls chosen
resulting policy maximizes given utility criterion resulting
trajectory interaction system stays close enough prescribed trajectory formally
q known conditional probabilities p ao aot z
chosen resulting generative distribution g interaction sequences given

g ao p ao

g ot ao q ot ao p ot ao
desirable p said tailored q
adaptive control environment q unknown task designing appropriate agent p constitutes adaptive control specifically
work deals case designer already class agents tailored
class possible environments formally assumed q going drawn
probability p set q qm mm possible systems interaction starts countable set furthermore one set p pm mm
systems pm tailored qm interaction system
pm qm generative distribution gm produces desirable interaction sequences
designer construct system p behavior close possible
custom made system pm realization qm q

adaptive systems
main goal adaptive control outlined
previous section reformulated universal compression
informally motivated follows suppose agent p implemented machine
interfaced environment q whenever agent interacts environment
agents state changes necessary consequence interaction change
state take place many possible ways updating internal memory consulting


fia minimum relative entropy principle learning acting

random number generator changing physical location orientation forth
naturally design agent facilitates interactions complicates others
instance agent designed explore natural environment might
incur low memory footprint recording natural images
memory inefficient recording artificially created images one abstracts away
inner workings machine decides encode state transitions binary
strings minimal amount resources bits required implement
state changes derived directly associated probability distribution p
context adaptive control agent constructed minimizes
expected amount changes necessary implement state transitions equivalently
maximally compresses experience thereby compression taken
stand alone principle design adaptive agents
universal compression nave construction adaptive agents
coding theory compressing sequence observations unknown
source known adaptive coding solved constructing universal compressors e codes adapt fly source within predefined class
mackay codes obtained minimizing average deviation predictor true source constructing codewords predictor
subsection procedure used derive adaptive agent ortega braun
formally deviation predictor p true distribution pm measured
relative entropy first would construct agent b
minimize total expected relative entropy pm constructed follows define
history dependent relative entropies action observation ot
x
pm ao

ao
dm
pm ao log
pr ao



ot
ao
dm

x
ot

pm ot ao log

pm ot ao

pr ot ao

pm ot ao qm ot ao qm known pr
argument variational one removes dependency past
averaging possible histories
x



ao
pm ao dm
dm
ao

ot
dm



x

ot
ao
pm ao dm

ao

finally total expected relative entropy pr pm obtained summing
time steps averaging choices true environment
lim sup


x

p




x






dm
dm



relative entropy known kl divergence measures average amount extra
bits necessary encode symbols due usage wrong predictor



fiortega braun

one define variational respect pr agent b one
looking system pr minimizes total expected relative entropy e
b arg min pr
pr

solution equation system b defined set equations
x
b ao
pm ao wm ao


b ot ao

x


pm ot ao wm ao





valid aot z mixture weights
p pm ao

p pm ao
p pm ao

wm ao p

p pm ao
wm ao p



reference see work haussler opper opper clear
b bayesian mixture agents pm one defines conditional
probabilities
p ao pm ao

p ot ao pm ao

aot z equation rewritten
b ao
b ot ao

x


x


p ao p ao p ao
p ot ao p ao p ot ao



p ao wm ao p ao wm ao posterior
probabilities elements given past interactions hence conditional
probabilities minimize total expected divergence predictive
distributions p ao p ot ao one obtains standard probability theory
particular bayes rule interesting provides teleological interpretation
bayes rule
behavior b described follows given time b maintains
mixture systems pm weighting given mixture coefficients
wm whenever action observation ot produced agent
environment respectively weights wm updated according bayes rule
addition b issues action suggested system pm drawn randomly according
weights wt
however important b arises due fact
system passively observing symbols actively generating
subjective interpretation probability theory conditionals play role observations


fia minimum relative entropy principle learning acting

made agent generated external source interpretation suits
symbols issued environment however symbols generated system require fundamentally different belief update
intuitively difference explained follows observations provide information
allows agent inferring properties environment contrast actions
carry information environment thus incorporated differently
belief agent following section illustrate simple
statistical example
causality
causality study functional dependencies events stands contrast
statistics abstract level said study equivalence dependencies
e co occurrence correlation amongst events causal statements differ fundamentally
statistical statements examples highlight differences many
smokers get lung cancer opposed smokers lung cancer assign
f x opposed compare f x programming languages f
opposed f newtonian physics study causality recently enjoyed
considerable attention researchers fields statistics machine learning
especially last decade significant progress made towards formal
understanding causation shafer pearl spirtes et al dawid
subsection aim provide essential tools required understand causal
interventions depth exposition causality reader referred
specialized literature
illustrate need causal considerations case generated symbols consider
following thought experiment suppose statistician asked design model
simple time series x x x decides use bayesian method assume
collects first observation x x computes posterior probability density function
pdf parameters model given data bayes rule
p x x r

p x x p

p x x p

p x x likelihood x given p prior pdf
use model predict next observation drawing sample x predictive
pdf
z
p x x x x p x x x x p x x
p x x x x likelihood x given x note x
drawn p x x x x understands nature x different
x x informative change belief state bayesian model
x non informative thus reflection belief state hence would
never use x condition bayesian model mathematically seems imply

p x x x x p x x


fiortega braun

x generated p x x x simple independence assumption correct following elaboration example
statistician told source waiting simulated data point x
order produce next observation x x depend x hands x
obtains observation x bayes rule posterior pdf parameters

p x x x x x x p x x p
r

p x x x x x x p x x p
p x x x x x x likelihood data x given old
data x parameters simulated data x notice looks almost
posterior pdf p x x x x x x given
r

p x x x x x x p x x x x p x x p
p x x x x x x p x x x x p x x p

exception latter case bayesian update contains likelihoods
simulated data p x x x x suggests equation variant
posterior pdf p x x x x x x simulated data x treated
different way data x x
define pdf p pdfs p p x p x x x identical
p p x p x x x respectively differ p x x
p x x x x
dirac delta function p identical p assumes
value x fixed x given x p simulated data x non informative
log p x x x
one computes posterior pdf p x x x x x x one obtains
equation
r

p x x x x x x p x x x x p x x p
p x x x x x x p x x x x p x x p
p x x x x x x p x x p
r

p x x x x x x p x x p

thus order explain equation posterior pdf given observed data x x
generated data x one intervene p order account fact x
non informative given x words statistician defining value
x changed natural regime brings series x x x
mathematically expressed redefining pdf
two essential ingredients needed carry interventions first one needs
know functional dependencies amongst random variables probabilistic model
provided causal model e unique factorization joint probability
note conceptually broken two steps first samples x p x x x
second imposes value x x setting p x x x x



fia minimum relative entropy principle learning acting

distribution random variables encoding causal dependencies general
case defines partial order random variables previous thought experiment causal model joint pdf p x x x given set conditional
pdfs
p p x p x x p x x x
second one defines intervention sets x value x denoted x x
operation causal model replacing conditional probability x dirac
delta function x x kronecker delta xx continuous discrete variable x
respectively thought experiment easily seen
p x x x x x x p x x x x x x
thereby
p x x x x x x p x x x x x x
causal contain additional information available joint probability
distribution alone appropriate model given situation depends story
told note intervention lead different respective causal
differ thus causal model
p x p x x p x x x p x x x
intervention x x would differ p e
p x x x x x x p x x x x x x
even though causal represent joint probability distribution
following use shorthand notation x x x random variable
obvious context
causal construction adaptive agents
following discussion previous section adaptive agent p going constructed minimizing expected relative entropy expected pm time
treating actions interventions definition conditional probabilities
equation total expected relative entropy characterize p interventions going defined assuming environment chosen first symbol depends
functionally environment previously generated symbols causal model
given
p p p p p
importantly interventions index set intervened probability distributions derived
base probability distribution hence set fixed intervention sequences form
indexes probability distributions observation sequences
one defines set criteria indexed intervention sequences


fiortega braun

clear solution define history dependent intervened relative
entropies action observation ot

ao
cm

x


ot
ao
cm

x
ot

p ao log

p ao
pr ao

p ot ao log

p ot ao

pr ot ao

pr given arbitrary agent note past actions treated interventions
particular p ao represents knowledge state past actions already
issued next action known yet averaging previous relative
entropies pasts yields


cm

x

ao
ot

cm


ao
p ao cm

x

ao

ot
ao
p ao cm

ao c ot ao
knowledge state time represented cm



averages taken treating past actions interventions finally define total exat c ot time averaged
pected relative entropy pr pm sum cm

possible draws environment

c lim sup


x

p




x





cm
cm




variational consists choosing agent p system pr minimizing
c c pr e
p arg min c pr

pr

following theorem shows variational unique solution
central theme
theorem solution equation system p defined set equations
x

p ao p ao



p ot ao p ot ao

p ao vm ao

x


p ot ao vm ao



valid aot z mixture weights
qt

p ao

qt


p ao
p

vm ao vm ao p

p





fia minimum relative entropy principle learning acting

bayesian control rule given set operation modes p mm
interaction sequences z prior distribution p
parameters probability action given
x
p aot p aot

p aot


posterior probability operation modes given recursion
p ot ao p ao



p ot ao p ao

p aot p

table summary bayesian control rule
theorem says optimal solution variational precisely
predictive distribution actions observations treating actions interventions
observations conditionals e solution one would obtain applying
standard probability causal calculus provides teleological interpretation
agent p akin nave agent b constructed section behavior p differs
important aspect b given time p maintains mixture systems
pm weighting systems given mixture coefficients vm contrast
b p updates weights vm whenever observation ot produced
environment update follows bayes rule treats past actions interventions
dropping evidence provide addition p issues action suggested
system drawn randomly according weights vm
summary
adaptive control formalized designing agent unknown environment chosen class possible environments environment specific agents
known bayesian control rule allows constructing adaptive agent combining
agents resulting adaptive agent universal respect environment
class context constituent agents called operation modes adaptive
agent represented causal interaction sequences e conditional
probabilities p ao p ot ao aot z
index parameter characterizing operation mode probability distribution
input stream output stream called hypothesis policy operation mode
table collects essential equations bayesian control rule particular
rule stated recursive belief update

convergence
aim section develop set sufficient conditions convergence
provide proof convergence simplify exposition analysis limited


fiortega braun

case controllers finite number input output

policy diagrams
following use policy diagrams useful informal tool analyze effect
policies environments figure illustrates example

state space


ao



policy

figure policy diagram one imagine environment collection states
connected transitions labeled symbols zoom highlights state
taking action collecting observation leads state
sets states transitions represented enclosed areas similar venn
diagram choosing particular policy environment amounts partially
controlling transitions taken state space thereby choosing probability
distribution state transitions e g markov chain given environmental
dynamics probability mass concentrates certain areas state space
choosing policy thought choosing subset environments
dynamics following policy represented subset state space
enclosed directed curve illustrated

policy diagrams especially useful analyze effect policies different hypotheses environments dynamics agent endowed set operation
modes seen hypotheses environments underlying dynamics
given observation p ot ao associated policies given action p ao sake simplifying interpretation
policy diagrams assume existence state space mapping
histories states note however assumptions made obtain
section
divergence processes
central question section investigate whether bayesian control rule converges correct control law whether p aot p ao
true operation mode e operation mode p ot ao
q ot ao obvious discussion rest section
general true
easily seen equation showing convergence amounts
posterior distribution p ao concentrates probability mass subset operation


fia minimum relative entropy principle learning acting

modes essentially output stream
x
x
p ao p ao
p ao p ao p ao
mm

mm

hence understanding asymptotic behavior posterior probabilities
p aot
crucial particular need understand conditions quantities
converge zero posterior rewritten
q
p aot p
p p ao

p
p aot p
qt




p aot p
p
p ao

summands one index dropped denominator one
obtains bound
p aot


p p ao

p
p ao


valid inequality seen convenient
analyze behavior stochastic process


dt km


x


ln

p ao
p ao

divergence process reference indeed dt km


p
p p ao

lim
edt km



p
p ao p

lim



thus clearly p aot figure illustrates simultaneous realizations
divergence processes controller intuitively speaking processes provide lower
bounds accumulators surprise value measured information units
divergence process random walk whose value time depends whole
history time makes divergence processes cumbersome characterize
fact statistical properties depend particular policy applied
hence given divergence process different growth rates depending policy
figure indeed behavior divergence process might depend critically
distribution actions used example happen divergence process
stays stable one policy diverges another context bayesian
control rule aggravated time step policy
applied determined stochastically specifically true operation mode
dt km random variable depends realization aot drawn






p ao p ao


fiortega braun

dt








figure realization divergence processes associated controller
operation modes divergence processes diverge whereas
stay dotted bound hence posterior probabilities
vanish

dt













figure application different policies lead different statistical properties
divergence process



fia minimum relative entropy principle learning acting

mt drawn p p ao p mt ao
deal heterogeneous nature divergence processes one introduce
temporal decomposition demultiplexes original process many sub processes
belonging unique policies let nt set time steps time
let nt let define sub divergence dt km random variable
x p ao

gm
ln
p ao


drawn

pm ao ao







p ao
p ao


nt ao given conditions kept constant
definition plays role policy used sample actions time
steps clearly realization divergence process dt km decomposed
sum sub divergences e
x
gm tm
dt km



tm mm forms partition nt figure shows example decomposition
dt








figure decomposition divergence process sub divergences
averages sub divergences play important role analysis define
average realizations gm
x
pm ao ao gm
gm
ao

notice nt
x
p ao
p ao p ao ln

gm
p ao
ao


gibbs inequality particular

gm

clearly holds well nt


gm

gm




fiortega braun

boundedness
general divergence process complex virtually classes distributions
interest control go well beyond assumptions stationarity
increased complexity jeopardize analytic tractability divergence process
predictions asymptotic behavior made anymore specifically
growth rates divergence processes vary much realization realization posterior distribution operation modes vary qualitatively
realizations hence one needs impose stability requirement akin ergodicity limit
class possible divergence processes class analytically tractable
purpose following property introduced
divergence process dt km said bounded variation iff
c nt




figm gm c
probability

dt











figure divergence process bounded variation realizations curves
sub divergence stay within band around mean curve
figure illustrates property boundedness key property going
used construct section first important posterior
probability true input output model bounded
theorem let set operation modes controller
divergence process dt km bounded variation
n

p aot


probability
core

one wants identify operation modes whose posterior probabilities vanish
enough characterize modes whose hypothesis match
true hypothesis figure illustrates three hypotheses along
associated policies shown h h share prediction made region differ


fia minimum relative entropy principle learning acting

region b hypothesis h differs everywhere others assume h true long
apply policy p hypothesis h make wrong predictions thus divergence
process diverge expected however evidence h accumulated
one applies policy p long enough time controller eventually
enter region b hence accumulate counter evidence h
h

h

b


h

b


p

p
p

figure hypothesis h true agrees h region policy p cannot
disambiguate three hypotheses

long enough mean p executed short period
controller risks visiting disambiguating region unfortunately neither right
policy right length period run known beforehand hence agent
needs clever time allocating strategy test policies finite time intervals
motivates following definition
core operation mode denoted subset containing
operation modes behaving policy formally operation mode

e core iff c n

gm c
probability gm sub divergence dt km pr
nt
words agent apply policy time step probability
least strategy expected sub divergence gm dt km grows
unboundedly core note demanding strictly positive
probability execution time step guarantees agent run
possible finite time intervals following theorem shows posterior probabilities
operation modes core vanish almost surely
theorem let set operation modes agent
divergence process dt km bounded variation
p aot
almost surely
consistency
even operation mode core e given essentially indistinguishable control still happen different
policies figure shows example hypotheses h h share region


fiortega braun

differ region b addition operation modes policies p p respectively confined region note operation modes core
however policies different means unclear whether multiplexing
policies time ever disambiguate two hypotheses undesirable could
impede convergence right control law
h

h
b

b
p

p





figure example inconsistent policies operation modes core
different policies

thus clear one needs impose restrictions mapping hypotheses policies respect figure one make following observations
operation modes policies select subsets region therefore
dynamics preferred dynamics b
knowing dynamics preferred dynamics b allows us
drop region b analysis choosing policy
since hypotheses agree region choose policy order
consistent selection criterion
motivates following definition operation mode said consistent
iff implies
ao




fip aot p aot

words core ms policy converge policy
following theorem shows consistency sufficient condition convergence
right control law
theorem let set operation modes agent
divergence process dt km bounded variation consistent

p ao p ao
almost surely


fia minimum relative entropy principle learning acting

summary
section proof convergence bayesian control rule true operation
mode provided finite set operation modes convergence
hold two necessary conditions assumed boundedness consistency first one
boundedness imposes stability divergence processes partial influence
policies contained within set operation modes condition regarded
ergodicity assumption second one consistency requires hypothesis makes
predictions another hypothesis within relevant subset dynamics
hypotheses share policy relevance formalized core
operation mode concepts proof strategies strengthen intuition potential
pitfalls arise context controller design particular could
asymptotic analysis recast study concurrent divergence processes
determine evolution posterior probabilities operation modes thus abstracting
away details classes distributions extension
infinite sets operation modes left future work example one could think
partitioning continuous space operation modes essentially different regions
representative operation modes subsume neighborhoods grunwald

examples
section illustrate usage bayesian control rule two examples
common reinforcement learning literature multi armed bandits markov
decision processes
bandit
consider multi armed bandit robbins stated follows
suppose n armed bandit e slot machine n levers pulled lever
provides reward drawn bernoulli distribution bias hi specific lever
reward r obtained probability hi reward r probability
hi objective game maximize time averaged reward iterative
pulls continuum range stationary strategies one parameterized n
probabilities si n
indicating probabilities pulling lever difficulty arising
bandit balance reward maximization knowledge already
acquired attempting actions improve knowledge dilemma known
exploration versus exploitation tradeoff sutton barto
ideal task bayesian control rule possible bandit
known optimal agent indeed bandit represented n dimensional bias vector
mn n given bandit optimal policy consists
pulling lever highest bias operation mode given
hi p ot mi

si p





maxj mj
else

fiortega braun









b
























figure space bandit configurations partitioned n regions according
optimal lever panel b armed armed bandit cases
respectively

apply bayesian control rule necessary fix prior distribution
bandit configurations assuming uniform distribution bayesian control rule
z

p p aot
p aot


update rule given
q
r
n

mj j mj fj
p p

p aot r
qt



b rj fj
p dm
p
j



rj fj counts number times reward obtained
pulling lever j number times reward obtained respectively observe
summation discrete operation modes replaced integral
continuous space configurations last expression see posterior
distribution lever biases given product n beta distributions thus
sampling action amounts first sample operation mode obtaining bias
mi beta distribution parameters ri choosing
action corresponding highest bias arg maxi mi pseudo code seen

simulation bayesian control rule described compared two
agents greedy strategy decay line gittins indices line
test bed consisted bandits n levers whose biases drawn uniformly
beginning run every agent play runs time steps
performance curves individual runs averaged greedy strategy
selects random action small probability given otherwise plays
lever highest expected reward parameters determined empirically
values several test runs adjusted way
maximize average performance last trials simulations gittins
method indices computed horizon geometric discounting
e close one approximate time averaged reward
shown figure


fia minimum relative entropy principle learning acting

bcr bandit
n
initialize ri zero
end

sample
interaction
set arg maxi mi issue
obtain environment

avg reward

update belief

ra ra
else
fa fa
end
end




bayesian control rule
greedy
gittins indices



























best lever








figure comparison n armed bandit bayesian control rule solid
line greedy agent dashed line gittins indices dotted line
runs averaged top panel shows evolution average
reward bottom panel shows evolution percentage times
best lever pulled



fiortega braun

seen greedy strategy quickly reaches acceptable level performance
seems stall significantly suboptimal level pulling optimal lever
time contrast gittins strategy bayesian control rule essentially asymptotic performance differ initial transient phase
gittins strategy significantly outperforms bayesian control rule least three
observations worth making first gittins indices pre computed
line time complexity scales quadratically horizon computations
horizon steps took several hours machines contrast bayesian
control rule could applied without pre computation second even though gittins
method actively issues optimal information gathering actions bayesian control
rule passively samples actions posterior distribution operation modes
end methods rely convergence underlying bayesian estimator
implies methods information bottleneck since bayesian estimator requires amount information converge thus active information gathering
actions affect utility transient phase permanent state efficient bandit found literature auer cesabianchi
fischer
markov decision processes
markov decision process mdp defined tuple x r x state space
action space ta x x pr x x probability action
taken state x x lead state x x r x r r immediate
reward obtained state x x action interaction proceeds time steps
time action issued state xt x leading reward
rt r xt state xt starts next time step stationary closedloop control policy x assigns action state mdps
exists optimal stationary deterministic policy thus one needs consider
policies undiscounted mdps average rewardpper time step fixed policy
initial state x defined x limt e r shown bertsekas
x x x x x assumption markov chain
policy ergodic assume mdps ergodic stationary policies
order keep intervention model particularly simple follow q notation
watkins optimal policy characterized terms optimal
average reward optimal relative q values q x state action pair x
solutions following system non linear equations singh

brute force adaptive agent would roughly look follows first agent
starts prior distribution mdps e g product dirichlet distributions transition
probabilities cycle agent samples full transition matrix distribution
solves dynamic programming computed optimal policy uses issue
next action discards policy subsequently updates distribution mdps
next observed state however main text follow different avoids solving
mdp every time step



fia minimum relative entropy principle learning acting

state x x action
q x r x

x

x x


h

q x



pr x x max





h

r x ex max
x


q x










optimal policy defined x arg maxa q x state x x
setup allows straightforward solution bayesian control rule
learnable mdp characterized q values average reward
known solution accordingly operation mode given q
r obtain likelihood model inference realize equation
rewritten predicts instantaneous reward r x sum mean
instantaneous reward plus noise term given q values average reward
mdp labeled
r x q x max
q x max
q x e max
q x x




z

z


noise

mean instantaneous reward x x

assuming reasonably approximated normal distribution n p
precision p write likelihood model immediate reward r
q values average reward e
r

n p
p


p r x x

exp r x x



order determine intervention model operation mode simply exploit
properties q values gives

arg maxa q x
p x

else
apply bayesian control rule posterior distribution p xt needs
computed fortunately due simplicity likelihood model one easily devise
conjugate prior distribution apply standard inference methods see appendix actions determined sampling operation modes posterior executing
action suggested corresponding intervention resulting
similar bayesian q learning dearden friedman russell dearden friedman andre differs way actions selected pseudo code listed

simulation tested mdp agent grid world example give intuition
achieved performance contrasted achieved r learning
used r learning variant presented work singh
together uncertainty exploration strategy mahadevan corresponding
update equations

q x q x r max
q x





r max
q x




q x







fiortega braun

bcr mdp gibbs sampler
initialize entries zero
set initial state x x

gibbs sweep
sample
q b visited states
sample q b
end
interaction
set arg maxa q x issue
obtain r x environment
update hyperparameters
x x p r
x x x x
x x p
x x x x p
set x x
end

goal

membranes

b bayesian control rule

c r learning c

r learning c

initial steps

x maze

e r learning c

f average reward



c
c



low
probability

c


last steps

high
probability

bayesian control rule











x time steps

figure grid world domain panel illustrates setup columns
b e illustrate behavioral statistics upper lower
row calculated first last time steps randomly
chosen runs probability state color encoded arrows
represent frequent actions taken agents panel f presents
curves obtained averaging ten runs



fia minimum relative entropy principle learning acting

average reward
bcr
r learning c
r learning c
r learning c






table average reward attained different end run
mean standard deviation calculated runs

learning rates exploration strategy chooses fixed probability
c
pexp action maximizes q x f x
c constant f x
represents number times action tried state x thus higher values
c enforce increased exploration
study mahadevan grid world described especially useful
test bed analysis rl purposes particular interest
easy design experiments containing suboptimal limit cycles figure panel
illustrates grid world controller learn policy leads
initial location goal state step agent move adjacent
space left right agent reaches goal state next position
randomly set square grid uniform probability start another trial
one way membranes allow agent move one direction
experiments membranes form inverted cups
agent enter side leave bottom playing role
local maximum transitions stochastic agent moves correct square

probability p
free adjacent spaces uniform distribution

probability p rewards assigned follows default reward r
agent traverses membrane obtains reward r reaching goal state
assigns r parameters chosen simulation following
mdp agent chosen hyperparameters precision p
r learning chosen learning rates exploration
constant set c c c total runs carried
presented figure table r learning
learns optimal policy given sufficient exploration panels e bottom row whereas
bayesian control rule learns policy successfully figure f learning curve
r learning c c initially steeper bayesian controller however
latter attains higher average reward around time step onwards attribute
shallow initial transient phase distribution operation modes
flat reflected initially random exploratory behavior

discussion
key idea work extend minimum relative entropy principle e
variational principle underlying bayesian estimation adaptive control


fiortega braun

coding point view work extends idea maximal compression
observation stream whole experience agent containing agents actions
observations minimizes amount bits write saving encoding
stream minimizes amount bits required produce decode
action mackay ch
extension non trivial important caveat coding sequences unlike observations actions carry information could used
inference adaptive coding actions issued decoder
inference ones actions logically inconsistent leads paradoxes
nozick seemingly innocuous issue turned intricate
investigated intensely recent past researchers focusing issue
causality pearl spirtes et al dawid work contributes body
providing evidence actions cannot treated probability
calculus alone
causal dependencies carefully taken account minimizing relative
entropy leads rule adaptive control called bayesian control rule
rule allows combining class task specific agents agent universal
respect class resulting control law simple stochastic control rule
completely general parameter free analysis shows control
rule converges true control law mild assumptions
critical issues
causality virtually every adaptive control method literature successfully treats
actions conditionals observation streams never worries causality
thus bother interventions decision theoretic setup decision
maker chooses policy maximizing
p expected utility u outcomes
e arg max e u pr u choosing formally
equivalent choosing kronecker delta function probability distribution
policies case conditional probabilities pr pr coincide
since
pr pr pr pr pr
sense choice policy causally precedes interactions
discussed section however uncertainty policy e pr
causal belief updates crucial essentially arises
uncertainty policy resolved interactions hence treating
actions interventions seamlessly extends status random variables
prior probabilities likelihood policies come predictor
bayesian control rule essentially bayesian predictor thereby entails almost modeling paradigm designer define class hypotheses
environments construct appropriate likelihood choose suitable
prior probability distribution capture uncertainty similarly sufficient domain knowledge analogous procedure applied construct suitable
operation modes however many situations difficult even


fia minimum relative entropy principle learning acting

intractable example one design class operation modes
pre computing optimal policies given class environments formally let
class hypotheses modeling environments let class policies given
utility criterion u define set operation modes constructing operation mode arg max e u
however computing optimal policy many cases intractable
cases remedied characterizing operation modes optimality
equations solved probabilistic inference example mdp
agent section recently applied similar adaptive control
linear quadratic regulators braun ortega
bayesian methods bayesian control rule treats adaptive control
bayesian inference hence typically associated
bayesian methods carry agents constructed bayesian control
rule analytical computational nature example
many probabilistic posterior distribution
closed form solution exact probabilistic inference general computationally
intensive even though large literature efficient approximate inference particular classes bishop many
suitable line probabilistic inference realistic environment classes
bayesian control rule versus bayes optimal control directly maximizing subjective expected utility given environment class minimizing
expected relative entropy given class operation modes two methods
different assumptions optimality principles bayesian
control rule bayes optimal controller indeed easy design experiments
bayesian control rule converges exponentially slower converge
bayes optimal controller maximum utility consider following
simple example environment k state mdp k consecutive actions
reach state reward interception b action leads back
initial state consider second environment first actions
b interchanged bayes optimal controller figures true environment k
actions k consecutive bs consider bayesian control rule
optimal action environment environment b uniform prior
operation modes stays uniform posterior long reward
observed hence bayesian control rule chooses time step b
equal probability policy takes k actions accidentally choose
row bs length k bayesian control rule optimal
bayes optimal controller converges time k bayesian control
rule needs exponentially longer one way remedy might allow
bayesian control rule sample actions operation mode several
time steps row rather randomizing controllers every cycle however
one considers non stationary environments strategy
break consider example increasing mdp k bayes optimal
controller converges steps bayesian control rule converge
realizations boundedness assumption violated


fiortega braun

relation existing approaches
ideas underlying work unique bayesian control rule
following selection previously published work recent bayesian reinforcement
learning literature related ideas found
compression principles literature important amount work
relating compression intelligence mackay hutter b particular
even proposed compression ratio objective quantitative measure
intelligence mahoney compression used basis theory
curiosity creativity beauty schmidhuber
mixture experts passive sequence prediction mixing experts studied
extensively literature cesa bianchi lugosi study onlinepredictors hutter bayes optimal predictors mixed bayes mixtures
used universal prediction hutter control case idea
mixtures expert controllers previously evoked
mosaic architecture haruno wolpert kawato universal learning
bayes mixtures experts reactive environments studied work
poland hutter hutter
stochastic action selection idea actions random variables
entails expressed work hutter b
study section regarded thorough investigation open
stochastic action selection approaches found thesis wyatt examines exploration strategies po mdps learning automata
narendra thathachar probability matching duda hart stork
amongst others particular thesis discusses theoretical properties
extension probability matching context multi armed bandit
proposed choose lever according likely optimal
shown strategy converges thus providing simple method guiding
exploration
relative entropy criterion usage minimum relative entropy criterion
derive control laws underlies kl control methods developed work todorov
kappen et al shown large class
optimal control solved efficiently statement
reformulated minimization deviation dynamics controlled
system uncontrolled system related idea conceptualize
inference toussaint harmeling storkey
equivalence maximization expected future return likelihood
maximization applicable mdps pomdps
duality become active field current see example work
rasmussen deisenroth fast model rl techniques
used control continuous state action spaces


fia minimum relative entropy principle learning acting

conclusions
work introduces bayesian control rule bayesian rule adaptive control
key feature rule special treatment actions causal calculus
decomposition adaptive agent mixture operation modes e environmentspecific agents rule derived minimizing expected relative entropy
true operation mode carefully distinguishing actions observations furthermore bayesian control rule turns exactly predictive distribution
next action given past interactions one would obtain probability
causal calculus furthermore shown agents constructed bayesian
control rule converge true operation mode mild assumptions boundedness
related ergodicity consistency demanding two indistinguishable hypotheses share policy
presented bayesian control rule way solve adaptive control
minimum relative entropy principle thus bayesian control rule
regarded principled adaptive control novel optimality
criterion heuristic approximation traditional bayes optimal control since
takes similar form bayes rule adaptive control could translated
line inference actions sampled stochastically posterior
distribution important note however statement formulated
usual bayes optimal adaptive control
future relationship two statements deserves investigation

acknowledgments
thank marcus hutter david wingate zoubin ghahramani jose aliste jose donoso
humberto maturana anonymous reviewers comments earlier versions
manuscript inspiring discussions thank ministerio de planificacion de chile
mideplan bohringer ingelheim fonds bif funding

appendix proofs
proof theorem
proof proof follows line argument solution equation
crucial difference
treated interventions consider without loss
p actions
equation note relative entropy
generality summand p cm
written difference two logarithms one term depends pr varied
therefore one pull term write constant c yields

c

x


p

x

ao

p ao

x




p ao ln pr ao

fiortega braun

substituting p ao p ao p ao p bayes rule rearrangement terms leads
xx
x
c
p ao p ao
p ao ln pr ao
ao

c

x

p ao



x


ao

p ao ln pr ao

p
inner sum form x p x ln q x e cross entropy q x p x
minimized q x p x x let p denote optimum distribution
pr choosing optimum one obtains p ao p ao note
solution variational p
independent p
weighting p ao since


argument applies summand p cm
p cm equation
variational mutually independent hence
p ao p ao

p ot ao p ot ao

aot z p ao introduce variable via marginalization
apply chain rule
x
p ao
p ao p ao


term p aot developed

p ao p


p ao p
qt
p p ao p ao
p
qt



p
p ao p ao
qt
p p ao

p
qt


p
p ao

p ao p

first equality obtained applying bayes rule second chain
rule probabilities get last equality one applies interventions causal
factorization thus p ao p ao p ao
equations characterizing p ot ao obtained similarly
proof theorem
proof pointed particular realization divergence process
dt km decomposed
x
dt km
gm tm


gm tm sub divergences dt km tm form partition nt
however since dt km bounded variation one
c nt nt inequality




figm tm gm tm c


fia minimum relative entropy principle learning acting

holds probability however due
gm tm
thus

gm tm c

previous inequalities hold simultaneously divergence process
bounded well inequality
dt km c



holds probability choose

max ln pp


since ln pp

added right hand side

definition dt km taking exponential rearranging terms one obtains


p










p ao e

p






p ao

c identifying posterior probabilities
dividing sides normalizing constant yields inequality
p aot e p aot


inequality holds simultaneously probability
particular minm e
p aot p aot
since valid maxm p aot
p aot




one gets





probability
arbitrary related equation




proof theorem
proof divergence process dt km decomposed sum sub divergences
see equation
x
gm tm

dt km




furthermore every
one c
n nt




figm gm c


fiortega braun

probability applying bound summands yields lower
bound
x
x

gm tm
gm tm c






holds probability
due inequality one




gm tm hence
x

gm tm c gm tm c


c maxm c members set tm determined stochastically
specifically ith member included tm probability p aoi
theorem since
one gm tm

probability arbitrarily chosen implies
lim dt km lim gm tm c





probability arbitrary related
upper bound posterior probabilities yields final
p dt km
e

p

lim p aot lim


proof theorem
proof use abbreviations pm p ao wm p ao
decompose p ao
x
x
pm wm
pm wm

p ao





first sum right hand side lower bounded zero upper bounded
x
x
pm wm
wm






pm due theorem wm almost surely given
let time wm choosing
maxm previous inequality holds simultaneously
probability hence
x
x
pm wm
wm







bound second sum one proceeds follows every member
one pm pm hence following similar construction
one choose inequalities




fipm pm


fia minimum relative entropy principle learning acting

hold simultaneously precision applying second sum equation
yields bounds
x
x
x


pm wm
pm wm
pm wm




pm







multiplicative constants placed front sum note


x



wm

x




wm

use inequalities allows simplifying lower upper bounds respectively
x
pm
wm pm pm


pm

x






wm pm pm

combining inequalities yields final





p

ao



p






holds probability arbitrary related
arbitrary precision







gibbs sampling implementation mdp agent
inserting likelihood given equation equation bayesian control rule
one obtains following expression posterior
p ot


p x x p r x x p






p x x p r x x p dm
p r x x p
r





p r x x p dm
r



replaced sum integration finite dimensional real space
containing average reward q values observed states
simplified term p x x constant
likelihood model p r x x equation encodes set independent normal distributions immediate reward means x x indexed triples
x x x x words given x x rewards drawn
normal distribution unknown mean x x known variance sufficient statistics given n x x number times transition x x
action r x x mean rewards obtained transition
conjugate prior distribution well known given normal distribution
hyperparameters
r
n







exp x x
p x x n



fiortega braun

posterior distribution given
p x x ot n x x x x
posterior hyperparameters computed
p n x x r x x
p n x x
x x p n x x

x x



introducing shorthand v x maxa q x write posterior distribution
p ot n




x
x x x x q x v x

x x
x

x x
x x

posterior distribution q values difficult obtain
q x enters posterior distribution linearly non linearly however
fix q x within max operations amounts treating v x
constant within single gibbs step conditional distribution approximated



p q x ot n q x x



q x

x

x x x x v x
x
x
x
x x
x
x

expect approximation hold resulting update rule constitutes contraction operation forms basis stochastic approximation mahadevan gibbs sampler draws values normal distributions cycle adaptive controller one carry several gibbs sweeps
obtain sample improve mixing markov chain however experimental
shown single gibbs sweep per state transition performs reasonably well
parameter vector drawn bayesian control rule proceeds taking
optimal action given equation note entries transitions
occurred need represented explicitly similarly q values visited
states need represented explicitly


fia minimum relative entropy principle learning acting

references
auer p cesabianchi n fischer p finite time analysis multiarmed
bandit machine learning
bertsekas dynamic programming deterministic stochastic
prentice hall upper saddle river nj
bishop c pattern recognition machine learning springer
braun ortega p minimum relative entropy principle adaptive
control linear quadratic regulators th conference informatics control
automation robotics vol pp
cesa bianchi n lugosi g prediction learning games cambridge university press
dawid p beware dag journal machine learning
appear
dearden r friedman n andre model bayesian exploration
proceedings fifteenth conference uncertainty artificial intelligence pp

dearden r friedman n russell bayesian q learning aaai
iaai proceedings fifteenth national tenth conference artificial intelligence innovative applications artificial intelligence pp american association artificial intelligence
duda r hart p e stork g pattern classification second edition
wiley sons inc
duff optimal learning computational procedures bayes adaptive markov
decision processes ph thesis director andrew barto
grunwald p minimum description length principle mit press
haruno wolpert kawato mosaic model sensorimotor learning
control neural computation
haussler opper mutual information metric entropy cumulative
relative entropy risk annals statistics
hutter self optimizing pareto optimal policies general environments
bayes mixtures colt
hutter optimality universal bayesian prediction general loss alphabet
journal machine learning
hutter online prediction bayes versus experts tech rep presented
eu pascal workshop learning theoretic bayesian inductive principles
ltbip
hutter b universal artificial intelligence sequential decisions algorithmic probability springer berlin


fiortega braun

kappen b gomez v opper optimal control graphical model inference
jmlr appear
mackay j c information theory inference learning cambridge university press
mahadevan average reward reinforcement learning foundations
empirical machine learning
mahoney v text compression test artificial intelligence aaai iaai
pp
narendra k thathachar l learning automata survey ieee
transactions systems man cybernetics smc
nozick r newcombs two principles choice rescher n ed
essays honor carl g hempel pp reidel
opper bayesian online learning online learning neural
networks
ortega p braun bayesian rule adaptive control causal
interventions third conference artificial general intelligence pp
pearl j causality reasoning inference cambridge university press
cambridge uk
poland j hutter defensive universal learning experts alt
rasmussen c e deisenroth p recent advances reinforcement learning
vol lecture notes computer science lnai chap probabilistic inference
fast learning control pp springer verlag
robbins h aspects sequential design experiments bulletin american
mathematical socierty
russell norvig p artificial intelligence modern rd edition
prentice hall
schmidhuber j simple algorithmic theory subjective beauty novelty surprise
interestingness attention curiosity creativity art science music jokes journal
sice
shafer g art causal conjecture mit press
singh p reinforcement learning average payoff markovian decision
processes national conference artificial intelligence pp
spirtes p glymour c scheines r causation prediction search nd
edition springer verlag york
sutton r barto g reinforcement learning introduction mit press
cambridge
todorov e linearly solvable markov decision advances neural
information processing systems vol pp


fia minimum relative entropy principle learning acting

todorov e efficient computation optimal actions proceedings national
academy sciences u
toussaint harmeling storkey probabilistic inference solving
po mdps tech rep edi inf rr university edinburgh
watkins c learning delayed rewards ph thesis university cambridge
cambridge england
wyatt j exploration inference learning reinforcement ph thesis
department artificial intelligence university edinburgh




