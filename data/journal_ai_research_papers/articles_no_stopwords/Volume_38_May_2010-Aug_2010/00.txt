Journal Artificial Intelligence Research 38 (2010) 1-48

Submitted 11/09; published 05/10

Using Local Alignments Relation Recognition
Sophia Katrenko
Pieter Adriaans
Maarten van Someren

S.Katrenko@uva.nl
P.W.Adriaans@uva.nl
M.W.vanSomeren@uva.nl

Informatics Institute, University Amsterdam
Science Park 107, 1098XG Amsterdam, Netherlands

Abstract
paper discusses problem marrying structural similarity semantic relatedness Information Extraction text. Aiming accurate recognition relations,
introduce local alignment kernels explore various possibilities using
task. give definition local alignment (LA) kernel based Smith-Waterman
score sequence similarity measure proceed range possibilities computing similarity elements sequences. show distributional similarity
measures obtained unlabeled data incorporated learning task semantic knowledge. experiments suggest LA kernel yields promising results
various biomedical corpora outperforming two baselines large margin. Additional
series experiments conducted data sets seven general relation types,
performance LA kernel comparable current state-of-the-art results.

1. Introduction
Despite fact much work done automatic relation extraction (or recognition) past decades, remains popular research topic. main reason
keen interest relation recognition lies utility. concepts semantic relations
identified, used variety applications question answering
(QA), ontology construction, hypothesis generation others.
ontology construction, relation studied is-a relation (or hypernymy), organizes concepts taxonomy (Snow, Jurafsky, & Ng, 2006). information retrieval, semantic relations used two ways, refine queries actual
retrieval, manipulate output returned search engine (e.g. identifying
whether fragment text contains given relation not). widely used relations
query expansion hypernymy (or broader terms thesaurus) synonymy.
Semantic relations useful different stages question answering.
taken account identifying type question considered actual answer extraction time (van der Plas, 2008). Yet another application
relations constructing new scientific hypothesis given evidence found text.
type knowledge discovery text often based co-occurrence analysis and, many
cases, corroborated via experiments laboratories (Swanson & Smalheiser, 1999).
Another reason extraction semantic relations interest lies diversity
relations. Different relations need different extraction methods. Many existing information
extraction systems originally designed work generic data (Grishman & Sundheim, 1996), became evident domain knowledge often necessary successful

c
2010
AI Access Foundation. rights reserved.

fiKatrenko, Adriaans, & van Someren

extraction. instance, relation extraction biomedical domain would require
accurate recognition named entities gene names (Clegg, 2008), area
food needs information relevant named entities toxic substances.
generic relations syntactic information often sufficient. Consider,
instance, following sentences (with arguments relations written italics):
(1)

Mary looked back whispered: know every tree forest, every scent.
(Part-Whole relation)

(2)

person infected particular flu virus strain develops antibodies
virus. (Cause-Effect relation)

(3)

apples basket. (Content-Container relation)

sentences exemplify binary relations, namely Part-Whole (tree part
forest), Cause-Effect (virus causes flu) Content-Container (apples contained
basket). easily notice syntactic context (1) (3) same, namely,
arguments cases connected preposition in. However,
context highly ambiguous even though allows us reduce number
potential semantic relations, still sufficient able discriminate
Part - Whole Content - Container relation. words, world knowledge
trees, forests, apples baskets necessary classify relations correctly.
situation changes even drastically consider example (2). Here, explicit
indication causation. Nevertheless, knowing flu virus is,
able infer Cause - Effect relation holds.
examples (1), (2) (3) highlight several difficulties characterize semantic
relation extraction. Generic relations often occur nominal complexes flu
virus (2) lack sentential context boosts approaches paraphrasing (Nakov,
2008). However, even noun compounds one combine world knowledge
compounds context arrive correct interpretation.
Computational approaches relation recognition problem often rely two-step
procedure. First, relation arguments identified. Depending relation hand,
step often involves named entity recognition arguments relations.
second step check whether relation holds. relation arguments provided (e.g.,
basket apples (3)), relation extraction reduced second step. Previous
work relation extraction suggests case accuracy relation recognition
much higher case discovered automatically (Bunescu
et al., 2005). Furthermore, existing solutions relation extraction (including work
presented paper) focus relation examples occur within single sentence
consider discourse (McDonald, 2005). Recognizing relations wider scope
interesting enterprise, would require take account anaphora resolution
types linguistic analysis.
Approaches relation extraction based hand-written patterns timeconsuming many cases need expert formulate test patterns. Although
patterns often precise, usually produce poor recall (Thomas et al., 2000).
general, hand-written patterns two types. first type sequential based
2

fiUsing Local Alignments Relation Recognition

frequently occurring sequences words sentence. Hand-written sequential patterns
initially used extraction Hypernymy (Hearst, 1992), several attempts
extend relations. second type patterns (Khoo, Chan, & Niu, 2000) take
syntactic structure sentence account. dependency structure sentence
usually represented tree patterns become subtrees. patterns
sometimes referred graphical patterns. identify examples Cause-Effect
relation, Khoo et al. (2000) applied type patterns texts medical domain.
study showed graphical patterns sensitive errors made parsers,
cover examples test data extract many spurious instances.
alternative using hand-written patterns supervised Machine Learning. Then,
relations labeled used train classifier recognize relations new
texts. One approach learn generalized extraction patterns patterns expressed
characters, words syntactic categories words. approaches involve clustering
based co-occurrence (Davidov & Rappoport, 2008). recent years kernel-based methods
become popular handle high-dimensional problems (Zelenko et al.,
2003; Bunescu & Mooney, 2006; Airola et al., 2008). methods transform text fragments, complete sentences segments around named entitites verbs, vectors,
apply Support Vector Machines classify new fragments.
Machine Learning methods use prior knowledge given system
addition labeled examples (Scholkopf, 1997, p. 17). use prior knowledge often
motivated by, example, poor quality data data sparseness. Prior knowledge
used many ways, changing representation existing training examples adding
examples unlabelled data. NLP tasks, prior knowledge exists form
manually (or automatically) constructed ontologies large collections unannotated data.
enrich textual data thereby improve recognition relations (Sekimizu,
Park, & Tsujii, 1998; Tribble & Fahlman, 2007). Recently, Zhang et al. (2008) showed
semantic correlation words learned unlabelled text collections, transferred
among documents used improve document classification. general,
use large collections text allows us derive almost information needed, done
varying accuracy. contrast, existing resources created humans provide
precise information, less likely cover possible areas interest.
paper, work Bunescu Mooney (2006), use syntactic
structure sentences, particular, dependency paths. stems observation
linguistic units organized complex structures understanding words
word senses relate often requires contextual information. Relation extraction
viewed supervised classification problem. training set consists examples
given relation goal construct model applied new, unseen
data set, recognize instances given relation new data set. recognition
relations use kernel-based classifier applied dependency paths. However,
instead vector-based kernel directly use similarity dependency paths
show information existing ontologies large text corpora employed.
paper organized follows. start reviewing existing kernel methods
work sequences (Section 2). Section 3, give definition local alignment kernel
based Smith-Waterman measure. proceed discussing used
context natural language processing (NLP) tasks, particularly extracting
3

fiKatrenko, Adriaans, & van Someren

relations text (Section 3.2). method described, report two types
data sets (biomedical generic) used experiments (Section 4) elaborate
experiments (Sections 5 6). Section 7 discusses findings detail.
Section 8 concludes paper discussing possible future directions.

2. Kernel Methods
past years witnessed boost interest kernel methods, theoretical analysis
practical applications various fields (Burges, 1998; Shawe-Taylor & Christianini,
2000). idea method works different structures representations,
starting simplest representation using limited number attributes complex
structures trees, seems indeed attractive.
define kernel function, recall standard setting supervised classification. training set n objects (instances) (x1 , y1 ), . . . , (xn , yn ) x1 , . . . , xn X
input examples input space X corresponding labels y1 , . . . , yn {0,1},
goal infer function h : X {0, 1} approximates target function t.
However, h still err data reflected loss function, l(h(xi ), yi ).
Several loss functions proposed literature far, best known
zero-one loss. loss function outputs 1 time method errs
data point (h(xi ) 6= yi ), 0 otherwise.
key idea kernel methods lies implicit mapping objects highdimensional space (by using mapping function ) considering inner product
(similarity) k(xi , xj ) =< (xi ), (xj ) >, rather representing explicitly. Functions used kernel methods
symmetric positive semi-definite,
Pbe
n Pn
whereby positive semi-definiteness defined i=1 j=1 ci cj k(xi , xj ) 0 n > 0,
objects x1 , . . . , xn X , choice real numbers c1 , . . . , cn R. function
positive semi-definite, algorithm may find global optimal solution.
requirements w.r.t. symmetry positive semi-definiteness met, kernel called
valid.
Using idea kernel mapping, Cortes Vapnik (1995) introduced support vector
machines (SVM) method seeks linear separation two classes
input points function f (x) f (x) = wT (x) + b, wT Rp , b R
h(x) = sign(f (x)). Here, wT stands slope linear function b
offset. Often, exist several functions separate data well,
equally good. hyperplane separates mapped examples largest possible
margin would best option (Vapnik, 1982).
SVMs solve following optimization problem:
n

X
1
k w k2 +C
l(h(xi ), yi )
f (x)=wT x+b 2
argmin

(4)

i=1

Equation 4, first part equation corresponds margin maximization
(by minimizing 12 k w k2 ), second takes account error training
set minimized (where C penalty term). hyperplane found
may correspond non-linear boundary original input space. exist number
4

fiUsing Local Alignments Relation Recognition

standard kernels linear kernel, Gaussian kernel others. Information
data problem motivate choice particular kernel.
shown Haussler (1999) complex kernel (referred convolution kernel )
defined using simpler kernels.
forms machine learning representations using prior knowledge defined
along methods exploiting it. Inductive logic programming offers one possible
solution use explicitly, form additional Horn clauses (Camacho, 1994).
Bayesian learning paradigm information hypothesis without seeing data encoded Bayesian prior (Mitchell, 1997) higher level distribution hierarchical
Bayesian setting. less obvious though represent use prior knowledge
learning frameworks. case SVMs, three possible ways incorporating
prior knowledge (Lauer & Bloch, 2008). named sampling methods (prior knowledge used generate new data), kernel methods (prior knowledge incorporated
kernel function by, instance, creating new kernel), optimization methods
(prior knowledge used reformulate optimization problem by, example, adding
additional constraints). choice kernel based general statistical properties
domain, attractive possibility incorporate explicit domain knowledge
kernel. improve kernel smoothing space: instances
similar higher probability belonging class kernel without
prior knowledge.
follows, review number kernels strings proposed
research community past years. natural domain look
biomedical field many problems formulated string classification
(protein classification amino acid sequences, name few). Sequence representation
is, however, applicable biomedical area, considered
many natural language processing tasks. introducing kernels used
biomedicine, move NLP domain present recent work relation extraction
employing kernel methods.
2.1 Spectrum Kernel
Leslie, Eskin, Noble (2002) proposed discriminative approach protein classification.
sequence x X , authors define m-spectrum set contiguous
subsequences x whose length equal m. possible m-long subsequences q
indexed frequency occurrence (q (x)). Consequently, feature map
sequence x alphabet equals (x) = (q (x))qAm . spectrum kernel two
sequences x defined inner product corresponding feature maps:
kS (x, y) =< (x), (y) >.
Now, even assuming contiguous subsequences small m, feature space consider
large. authors propose detect subsequences length using suffix
tree method guarantees fast computation kernel matrix. spectrum kernel
tested task protein homology detection, best results achieved
setting relatively small number (3). novelty Leslie et al.s (2002) method
lies generality low computational complexity.

5

fiKatrenko, Adriaans, & van Someren

2.2 Mismatch Kernels
mismatch kernel introduced later Leslie et al. (2004) essentially extension latter. obvious limitation spectrum kernel considered
subsequences contiguous match exactly. mismatch kernel contiguity preserved match criterion changed. words, instead looking
possible subsequences length given subsequence, one searching
possible subsequences length allowing r mismatches. comparison
result larger subset subsequences, kernels defined way still calculated rather fast. kernel formulated similarly spectrum kernel
major difference computing feature map
P sequences. precisely, feature
map sequence x defined m,r (x) = qS m,r (q) m,r (q) = ( (q))Am .
(q) binary indicates whether sequence belongs set m-length sequences
differ q r elements (1) (0). clear r set
0, mismatch kernel reduced spectrum kernel. complexity mismatch
kernel computation linear respect sum sequence lengths.
authors show mismatch kernel yields state-of-the-art performance protein classification task outputs subsequences informative
biological point view.
2.3 Kernel Methods NLP
One merits kernel methods possibility designing kernels different structures, strings trees. NLP field (and relation extraction, particular)
work roughly falls two categories. first, kernels defined plain
text using sequences words. second uses linguistic structures dependency
paths trees output shallow parsing. short review take
chronological perspective rather start methods based sequences
proceed approaches make use syntactic information.
year spectrum kernel designed, Lodhi et al. (2002) introduced string subsequence kernels provide flexible means work text data.
particular, subsequences necessarily contiguous weighted according
length (using decay factor ). length subsequences fixed advance.
authors claim even without use linguistic information kernels
able capture semantic information. reflected better performance
text classification task compared bag-of-words approach. Lodhi et al.s (2002)
kernel works sequences characters, kernel proposed Cancedda et al. (2003)
applied word sequences. String kernels extended syllable kernels
proved well text categorization (Saunders, Tschach, & Shawe-Taylor, 2002).
kernels defined recursively, computation efficient.
instance, time complexity Lodhi et al.s (2002) kernel O(n|s||t|), n
length subsequence, documents.
2.3.1 Subsequence Kernels
recognition binary relations, natural way consider words located
around relation arguments. approach taken Bunescu Mooney
6

fiUsing Local Alignments Relation Recognition

(2005b) whose choice sequences motivated textual patterns found corpora.
instance, observed relations expressed subject-verb-object constructions others part noun prepositional phrases. result, three types
sequences considered: fore-between (words two named entities),
(words two entities) between-after (words two
entities). length sequences restricted. handle data sparseness, authors
generalize existing sequences using PoS tags, entity types WordNet synsets.
generalized subsequence kernel recursively defined number weighted sparse subsequences two sequences share. absence syntactic information, assumption
made long subsequences likely represent positive examples
penalized. subsequence kernel computed three types sequences
resulting relation kernel defined sum three subkernels. Experimental results
biomedical corpus encouraging, showing relation kernel performs better
manually written patterns approach based longest common subsequences.
method proposed Giuliano et al. (2006) largely inspired work Bunescu
Mooney (2005b). However, instead looking subsequences three types sequences, authors treat bag-of-words define called global kernel
follows. First, sequence type (pattern) P represented vector whose elements
counts many times token used P . local kernel defined similarly
using words surrounding named entities (left right context). final shallow
linguistic kernel defined combination global local kernels. Experiments biomedical corpora suggest kernel outperforms subsequence kernel
Bunescu Mooney.
2.3.2 Distributional Kernels
Recently, Seaghdha Copestake (2008) introduced distributional kernels co- occurrence probability distributions. co-occurrence statistics use form
either syntactic relations n-grams. show possible derive kernels
distances Jensen-Shannon divergence (JSD) Euclidean distance (L2 ) (Lee, 1999).
JSD smoothed version Kullback-Leibler divergence, information-theoretic measure dissimilarity two probability distributions. main motivation behind
approach lies fact distributional similarity measures proved useful
NLP tasks. extract co-occurrence information, authors use two corpora, British
National Corpus (BNC) Web 1T 5-Gram Corpus (which contains 5-grams
observed frequency counts collected Web). Distributional kernels
proved successful number tasks compound interpretation, relation
extraction verb classification. them, JSD kernel clearly outperforms
Gaussian linear kernels. Moreover, estimating distributional similarity BNC
corpus yields performance similar results obtained Web 1T 5-Gram Corpus.
interesting finding BNC corpus used estimate similarity
syntactic relations whereas latter corpus contains n-grams only. importantly,
method Seaghdha Copestake provides empirical support claim using
distributional similarity beneficial relation extraction.

7

fiKatrenko, Adriaans, & van Someren

2.3.3 Kernels Syntactic Structures
Kernels defined unpreprocessed text data seem attractive applied
directly text language. However, general are, lose precision compared methods use syntactic analysis. Re-ranking parsing
trees (Collins & Duffy, 2001) one first applications kernel methods NLP
problems. accomplish goal, authors rely subtrees pair trees
common. Later on, Moschitti (2006) explored convolution kernels dependency
constituency structures semantic role labeling question classification. work
introduces novel kernel called partial tree kernel (PT). essentially built
two kernels proposed before, subtree kernel (ST) contains descendant nodes
target root (including leaves) subset tree kernel (SST) flexible
allows internal subtrees necessarily encompass leaves. partial tree
generalization subset tree whereby partial structures grammar allowed (i.e.,
parts production rules [VP [V]] form valid PT). Moschitti demonstrated
PTs obtain better performance dependency structures SSTs, latter
yield better results constituent trees.
2.3.4 Kernel Shallow Parsing Output
Zelenko et al. (2003) use shallow parsing designed kernels extract relations text.
contrast full parsing, shallow parsing produces partial interpretations sentences.
node tree enriched information roles (that correspond
arguments relation). similarity two trees determined similarity
nodes. Depending similarity computed, Zelenko et al. define two types
kernels, contiguous subtree kernels sparse kernels. types tested two types
relations, person-affiliation organization-location exhibiting good performance.
particular, sparse kernels outperform contiguous subtree kernels leading conclusion
partial matching important dealing typically sparse natural language
data. However, computation sparse kernel takes O(mn3 ) time (where n
number children two relation examples, i.e. shallow trees, consideration,
n), algorithm contiguous subtree kernel runs time O(mn).
2.3.5 Shortest Path Kernel
Bunescu Mooneys (2005a) shortest path kernel represents yet another approach
relation extraction kernel-based relies information found dependency trees.
main assumption entire dependency structure relevant, one
focus path connecting two relation arguments instead. similar
paths are, likely two relation examples belong category. spirit
previous work, Bunescu Mooney seek generalizations existing paths
adding information sources part speech (PoS) categories named entity types.
shortest path relation arguments extracted kernel two
sequences (paths) x = {x1 , . . . , xn } x0 = {x01 , . . . , x0m } computed follows:

8

fiUsing Local Alignments Relation Recognition

0

kB (x, x ) =



0Q
n

0
i=1 f (xi , xi )

6= n
m=n

(5)

Equation 5, f (xi , x0i ) number features shared xi x0i . Bunescu
Mooney (2005a) use several features word (e.g., protesters), part speech tag (e.g.,
N N S), generalized part speech tag (e.g., N oun), entity type (e.g., P ERSON )
applicable. addition, direction feature ( ) employed. reproduce
example paper.
Example 1 Given two dependency paths exemplify relation Located
actions Brcko arrival Beijing, paths expanded
additional features mentioned above. easy see comparing path (6)
path (7) gives us score 18 (3111213 = 18).





Brcko



actions
NNP

P RP
[] N N []
[]
N oun

P ERSON
N oun
LOCAT ION













(6)


Beijing

NNP

[]

N oun
LOCAT ION

(7)





arrival

[] N N
[]
P RP
N oun
P ERSON








time complexity shortest path kernel O(n), n stands length
dependency path.
Dependency paths considered recent work relation recognition (Erkan,
Ozgur, & Radev, 2007). Here, Erkan et al. (2007) use dependency paths input
compare means cosine similarity edit distance. authors motivate
choice need compare dependency paths different length. Further, various machine learning methods used classification, including SVM transuctive SVM
(TSVM), extension SVM (Joachims, 1999). particular, TSVM makes use
labeled unlabeled data first classifying unlabeled examples searching
maximum margin separates positive negative instances sets.
authors conclude edit distance performs better cosine similarity measure,
TSVM slightly outperforms SVM.
Airola et al. (2008) propose graph kernel makes use entire dependency
structure. work, sentence represented two subgraphs, one
built dependency analysis, corresponds linear structure
sentence. Further, kernel defined paths two vertices graph.
method Airola et al. (2008) achieves state-of-the-art performance biomedical
data sets, discussed, together shortest path kernel work

9

fiKatrenko, Adriaans, & van Someren

Erkan et al. (2007), Section 5 relation extraction biomedical domain
paper.
Finally, kernels defined graphs syntactic structures,
graphs semantic network. illustrated Seaghdha (2009), uses graph
kernels graph built hyponymy relations WordNet. Even though
syntactic information utilized, kernels proved perform well extraction
various generic relations.
kernels reviewed section deal sequences trees albeit different ways. empirical findings suggest kernels allow partial matching usually
perform better compared methods similarity defined exact match.
alleviate problem exact matching, researchers suggested generalizing
elements existing structures (Bunescu & Mooney, 2005a) others opted flexible
comparison. view, types methods complement (Saunders
et al., 2002). flexible partial matching methods are, may suffer low precision penalization mismatch low. holds approaches use
generalization strategies may easily overgeneralize. possible solution would
combine both, provided mismatches penalized well generalizations
semantically plausible rather based part speech categories. idea
explored present paper evaluated relation recognition task.
nutshell, goals paper following: (i) study possibilities
using local alignment kernel relation extraction text, (ii) exploration
use prior knowledge alignment kernel (iii) extensive evaluation
automatic recognition two types relations, biomedical generic.

3. Local Alignment Kernel
One note short overview kernels designed NLP many
researchers use partial structures propose variants subsequence kernels (Bunescu
& Mooney, 2005b), partial tree kernel (Moschitti, 2006), kernel shallow parsing
output (Zelenko et al., 2003) relation extraction. paper focus dependency
paths input formulate following requirements kernel function:
allow partial matching similarity measured paths
different length
possible incorporate prior knowledge
Recall prior knowledge mean information comes either larger corpora existing resources ontologies. instance, knowing development
synonymous evolution contexts help recognize two different words
close semantically. information especially useful meaning relevant
detecting relations may differ form.
following subsection define local alignment kernel satisfies
requirements show incorporate prior knowledge.

10

fiUsing Local Alignments Relation Recognition

3.1 Smith-Waterman Measure Local Alignments
work motivated recent advances biomedical field. shown
possible design valid kernels based similarity measure strings (Saigo,
Vert, & Akutsu, 2006). example, Saigo, Vert, Ueda, Akutsu (2004) consider
Smith-Waterman (SW) similarity measure (Smith & Waterman, 1981) (see below) measure similarity two sequences amino acids.
String distance measures divided measures based terms, edit-distance
Hidden Markov models (HMM) (Cohen, Ravikumar, & Fienberg, 2003). Term-based
distances measures based TF-IDF score, consider pair word sequences
two sets words ignoring order. contrast, string edit distances (or string similarity
measures) treat entire sequences compare using transformation operations,
convert sequence x sequence x0 . Examples Levenshtein distance,
Needleman-Wunsch (Needleman & Wunsch, 1970) Smith-Waterman (Smith
& Waterman, 1981) measures. Levenshtein distance used natural
language processing field component variety tasks, including semantic role
labeling (Sang et al., 2005), construction paraphrase corpora (Dolan, Quirk, & Brockett,
2004), evaluation machine translation output (Leusch, Ueffing, & Ney, 2003), others.
Smith-Waterman measure mostly used biological domain, are, however,
applications modified Smith-Waterman measure text data well (Monge &
Elkan, 1996; Cohen et al., 2003). HMM-based measures present probabilistic extensions
edit distances (Smith, Yeganova, & Wilbur, 2003).
hypothesis string similarity measures best basis kernel
relation extraction. case, order words appear likely relevant
sparse data usually prevents estimation probabilities (as work Smith et al.,
2003). general, two sequences aligned several possible ways. possible
search either alignment spans entire sequences (global alignment),
alignment based similar subsequences (local alignment). case
sequences amino acids relation extraction, local patterns likely
important factor determines similarity. Therefore need similarity measure
emphasizes local alignments.
Formally, define pairwise alignment L elements two sequences
x = x1 x2 . . . xn x0 = x01 x02 . . . x0m , pairing = {l (i, j)}, l = 1, . . . , L, 1 n,
1 j m, 1 l n, 1 l m. Example 2 (ii), third element first sequence
aligned first element second one, denoted 1 (3, 1).
Example 2 Given sequences x=abacde x0 =ace, two possible alignments (with gaps
indicated -) follows.
(i) global alignment



b
-


-

c
c


-

e
e

Alignment:

= {1 (1, 1), 2 (4, 2), 3 (6, 3)}




c
c


-

e
e

Alignment:

= {1 (3, 1), 2 (4, 2), 3 (6, 3)}

(ii) local alignment

-

b
-

11

fiKatrenko, Adriaans, & van Someren

example, number gaps inserted x0 align x number
elements match cases. Yet, biological
linguistic context may prefer alignment (ii), closely matching substrings,
local alignments, better indicator similarity shared items far apart.
is, therefore, better use measure puts less weight gaps
start end strings (as Example 2 (ii)). done using local
alignment mechanism searches similar subsequences two sequences.
Local alignments employed sequences dissimilar different length,
global alignments considered sequences roughly length.
measures mentioned above, Smith-Waterman measure local alignment
measure, Needleman-Wunsch measure compares two sequences based global
alignments.
Definition 1 (Global alignment) Given two sequences x = x1 . . . xn x0 = x01 . . . x0m ,
global alignment pair sequences y0 length,
obtained inserting zero gaps first element either x x0 ,
element x x0 .
Definition 2 (Local alignment) Given two sequences x = x1 . . . xn x0 = x01 . . . x0m ,
local alignment pair subsequences x x0 , whose similarity
maximal.
clarify mean local global alignments, give definition
Smith-Waterman Needleman-Wunsch measures. Given two sequences x = x1 x2 . . . xn
x0 = x01 x02 . . . x0m length n respectively, Smith-Waterman measure defined
similarity score best local alignment:

sw(x, x0 ) =

max

A(x,x0 )

s(x, x0 , )

(8)

equation above, s(x, x0 , ) score local alignment sequence x x0
denotes set possible alignments. best local alignment efficiently
found using dynamic programming. this, one fills matrix SW partial
alignments follows:

0



SW(i 1, j 1) + d(xi , x0j )
SW (i, j) = max
1in,
SW(i 1, j) G


1jm
SW(i, j 1) G

(9)

Equation 9, d(xi , x0j ) denotes substitution score two elements xi x0j
G stands gap penalty. Using equation possible find partial alignments,
stored matrix cell (i, j) reflects score alignment x1 . . . xi

12

fiUsing Local Alignments Relation Recognition


c
e

0
0
0
0


0
2
1
0

b
0
1
1
0


0
2
1
0

c
0
1
4
3


0
0
3
3

e
0
0
2
5


c
e

(a) Smith-Waterman measure

0
0
0
0


0
2
1
0

b
0
1
1
0


0
0
0
0

c
0
-1
2
1


0
-1
1
1

e
0
-1
0
3

(b) Needleman-Wunsch measure

Table 1: Matrices computing Smith-Waterman Needleman-Wunsch scores sequences x=abacde x0 =ace, gap G = 1, substitution score d(xi , x0j ) = 2
xi = x0j , d(xi , x0j ) = 1 xi 6= x0j .

x01 . . . x0j . cell largest value matrix contains Smith-Waterman
score.
Needleman-Wunsch measure, searches global alignments, defined similarly, except fact cells matrix contain negative scores:

NW(i 1, j 1) + d(xi , x0j )
NW (i, j) = max
NW(i 1, j) G
1in,

NW(i, j 1) G
1jm

(10)

Smith-Waterman measure seen modification Needleman-Wunsch
method. disallowing negative scores matrix, regions high dissimilarity
avoided and, result, local alignments preferred. Moreover, NeedlemanWunsch score equals largest value last column last row, Smith-Waterman
similarity score corresponds largest value matrix.
Let us reconsider Example 2 show global local alignments alignments
two sequences x=abacde x0 =ace obtained. arrive actual alignments, one
set gap parameter G substitution scores. Assume use following
settings: gap G = 1, substitution score d(xi , x0j ) = 2 xi = x0j , d(xi , x0j ) = 1
xi 6= x0j . values chosen illustrative purpose only, realistic
case, e.g., alignment protein sequences, choice substitution scores usually
motivated biological evidence. gapping, Smith Waterman (1981) suggested
use gap value least equal difference match (d(xi , x0j ),
xi = x0j ) mismatch (d(xi , x0j ), xi 6= x0j ). Then, Smith-Waterman NeedlemanWunsch similarity scores x x0 calculated according Equation 9
Equation 10 given Table 1.
First, first row first column matrix initialized 0. Then,
matrix filled computing maximum score cell defined Equation 9
Equation 10. score best local alignment equal largest element
13

fiKatrenko, Adriaans, & van Someren

matrix (5), Needleman-Wunsch score 3. Note possible trace back
steps taken arrive final alignment (the cells boldface). left-right
step corresponds insertion, top-down step deletion (these lead gaps),
diagonal step implies alignment two sequences elements.
Since prefer use local alignments dependency paths, natural choice would
use Smith-Waterman measure kernel function. However, Saigo et al. (2004)
observed Smith-Waterman measure may result valid kernel
may positive semi-definite. give definition LA kernel, states
two sequences similar many local alignments high scores,
Equation 11.
kL (x, x0 ) =

X

0

es(x,x ,)

(11)

A(x,x0 )

Here, s(x, x0 , ) local alignment score ( 0) scaling parameter.
define LA kernel kL (as Equation 11) two sequences x x0 , needed
take account transformation operations used local alignments. First, one
define kernel elements corresponds individual alignments, ka . Second,
since type alignment allows gaps, another kernel gapping, kg . Last
least, recall local alignments parts sequences may aligned,
elements x x0 may left out. elements influence alignment
score kernel used cases, k0 , set constant, k0 (x, x0 ) = 1. Finally,
LA kernel composition several kernels (k0 , ka , kg ), spirit
convolution kernels (Haussler, 1999).
According Saigo et al. (2004), similarity aligned sequences elements (ka kernel)
defined follows:

0
|x| =
6 1 |x0 | =
6 1
0
ka (x, x ) =
(12)
0)
d(x,x
e
otherwise
either x, x0 one element, kernel would result 0. Otherwise,
calculated using substitution score d(x, x0 ) x x0 . score reflects
similar two sequences elements and, depending domain, computed using
prior knowledge given domain.
gapping kernel defined similarly alignment kernel Equation 12, whereby
scaling parameter preserved, gap penalties used instead similarity
function two elements:
0

kg (x, x0 ) = e(g(|x|)+g(|x |))

(13)

Here, g stands gap function. Naturally, gap length 0 function returns
zero. gaps length n, reasonable define gap terms gap opening
gap extension e, g(n) = + e (n 1). case possible decide whether longer
gaps penalized shorter ones, much. instance,
14

fiUsing Local Alignments Relation Recognition

three consecutive gaps alignment, first gap counted gap opening,
two gap extension. consecutive gaps (i.e., gaps length n > 1) gap
equal importance, gap opening equal gap extension. If, however,
length gaps matter, one would prefer penalize gap opening more,
give little weight gap extension.
kernels combined follows:
k(r) (x, x0 ) = k0 (ka kg )r1 ka k0

(14)

Equation 14, k(r) (x, x0 ) stands alignment r elements x x0 possibly
r 1 gaps. Similarity aligned elements calculated ka , gapping kg . Since
could r 1 gaps, corresponds following part equation:
(ka kg )r1 . Further, rth aligned element, one ka added. Given
discussion above, k0 added initial final part. follows Equation 14,
elements x x0 aligned, k(r) equals k0 , 1. elements x
x0 aligned gaps, value k(r) (ka )r .
Finally, LA kernel equal sum taken possible local alignments
sequences x x0 :

0

kL (x, x ) =


X

k(i) (x, x0 )

(15)

i=0

results biological domain suggest kernels based Smith-Waterman
distance relevant comparison amino acids string kernels (Saigo et al.,
2006). clear whether holds applied natural language processing tasks.
view, could depend parameters used, substitution
matrix penalty gaps.
3.1.1 Computational complexity
LA kernel, many kernels discussed Section 2, efficiently calculated using dynamic programming. two sequences x x0 , length n respectively,
complexity proportional n m. Additional costs may come substitution matrix, which, unlike biomedical domain, become large. However,
look-up substitution scores done efficient manner well, leads
fast kernel computation. instance, calculating kernel matrix largest data
set used paper, AImed (3,763 instances), takes 805 seconds 2.93 GHz Intel(R)
Core(TM)2 machine.
3.2 Designing Local Alignment Kernel Relation Extraction
Smith-Waterman measure based transformations, particular deletions elements different strings. However, elements different may still
similar degree. similarities used part similarity measure.
example, two elements words different synonyms,
count less different completely unrelated. call
15

fiKatrenko, Adriaans, & van Someren

similarities substitution scores (Equation 12) define two different ways:
basis distributional similarity basis semantic relatedness ontology.
Example 1 would able infer Brcko similar Beijing, even
though two words match exactly. Furthermore, phrases arrival
Beijing arrival January, would kernel say Brcko
similar Beijing January. use information prior knowledge
makes possible measure similarity two words, one test set
training set, even match exactly. review two types
measures based statistical distributions relatedness WordNet.
3.2.1 Distributional Similarity Measures
number distributional similarity measures proposed years, including
Cosine, Dice Jaccard coefficients. Distributional similarity measures extensively studied (Lee, 1999; Weeds, Weir, & McCarthy, 2004). main hypothesis
behind distributional measures words occurring context
similar meaning (Firth, 1957). Context defined either using proximity text,
employing grammatical relations. paper, use first option context
sequence words text length set advance.
Measure

Formula

Cosine

d(xi , x0j ) = P

Dice

d(xi , x0j ) =

L2

d(xi , x0j ) =

P (c|xi )P (c|x0j )
P
0 2
2
c P (c|xi )
c P (c|xj )

P

c

2F (xi )F (x0j )
F (xi )F (x0j )

qP

c (P (c|xi )

P (c|x0j ))2

Table 2: list functions used estimate distributional similarity measures.
chosen following measures: Dice, Cosine L2 (Euclidean) whose definitions given Table 2. definition Cosine L2, possible use either
frequency counts probability estimates derived unsmoothed relative frequencies.
Here, adopt definitions given Lee (1999), based probability estimates P . Recall x x0 two sequences would wish compare,
corresponding elements xi x0j . Further, c stands context. definition
Dice coefficient, F (xi ) = {c : P (c|xi ) > 0}. mainly interested symmetric measures
(d(xi , x0j ) = d(x0j , xi )) symmetric positive semi-definite matrix required kernel methods. Euclidean measure defined Table 2 necessarily vary 0
1. reason, given list pairs words (xi , x0j ) xi fixed j = 1, . . . ,
corresponding L2 score, maximum value maxj d(xi , x0j ) detected used
normalize scores list. Furthermore, unlike Dice Cosine, return 1
case two words equal, Euclidean score equals 0. next step, substract
obtained normalized value 1 ascertain scores within interval [0, 1]
16

fiUsing Local Alignments Relation Recognition

largest value (1) assigned identical words. view, procedure
make comparison selected distributional similarity measures respect
influence LA kernel transparent.
Distributional similarity measures suitable information available.
case data annotated means taxonomy (e.g., WordNet),
possible consider measures defined taxonomy. Availability hand-crafted
resources, WordNet, comprise various relations concepts, enables
making distinctions different concepts subtle way.
3.2.2 WordNet Relatedness Measures
generic relations, commonly used resource WordNet (Fellbaum, 1998),
lexical database English. WordNet, words grouped together synsets
synset consists list synonymous words collocations (e.g., fountain pen),
pointers describe relations synset synsets (Fellbaum, 1998).
WordNet employed different purposes studying semantic constraints
certain relation types (Girju, Badulescu, & Moldovan, 2006; Katrenko & Adriaans, 2008),
enriching training set (Giuliano et al., 2007; Nulty, 2007).
compare two concepts given synsets c1 c2 use five different measures
proposed past years. rely notions length
shortest path two concepts c1 c2 , len(c1 , c2 ), depth node
WordNet hierarchy (which equal length path root given
synset ci ), dep(ci ), least common subsumer (or lowest super-ordinate) c1
c2 , lcs(c1 , c2 ), turn synset. measures exclusively based
notions belong conceptual similarity proposed Palmer Wu (1995) (simwup
Equation 16) formula scaled semantic similarity introduced Leacock
Chodorow (1998) (simlch Equation 17). 1 major difference lies
fact simlch consider least common subsumer c1 c2 uses
maximum depth WordNet hierarchy instead. Conceptual similarity ignores
focuses subhierarchy includes synsets.

simwup (c1 , c2 ) =

2 dep(lcs(c1 , c2 ))
len(c1 , lcs(c1 , c2 )) + len(c2 , lcs(c1 , c2 )) + 2 dep(lcs(c1 , c2 ))

simlch (c1 , c2 ) = log

len(c1 , c2 )
2 maxcW ordN et dep(c)

(16)

(17)

Aiming combining information several sources, Resnik (1995) introduced yet another measure grounded information content (simres Equation 18). Intuitively,
two synsets c1 c2 located deeper hierarchy path one synset
another short, similar. path two synsets long
least common subsumer placed relatively close root, indicates synsets
1. equations similarity measures defined WordNet, subscripts refer similarity measure
(e.g., lch, wup simlch simwup , respectively)

17

fiKatrenko, Adriaans, & van Someren

c1 c2 much common. quantify intuition, necessary derive
probability estimate lcs(c1 , c2 ) done employing existing corpora.
precisely, p(lcs(c1 , c2 )) stands probability encountering instance concept
lcs(c1 , c2 ).
simres (c1 , c2 ) = log p(lcs(c1 , c2 ))

(18)

One biggest shortcomings Resniks method fact least
common subsumer appears Equation 18. One easily imagine full-blown hierarchy
relatedness concepts subsumed lcs(ci , cj ) heavily vary.
words, using lcs only, one able make subtle distinctions two
pairs concepts share least common subsumer. overcome this, Jiang
Conrath (1997) proposed solution takes account information synsets
compared (simjcn Equation 19). comparing Equation 19 Equation 18,
notice equation incorporates probability encountering
lcs(c1 , c2 ), probability estimates c1 c2 .
simjcn (c1 , c2 ) = 2 log p(lcs(c1 , c2 )) (log p(c1 ) + log p(c2 ))

(19)

Lin (1998) defined similarity two concepts using much commonality
differences involved. Similarly two previous approaches, uses
information theoretic notions derives similarity measure simlin given Equation 20.

simlin (c1 , c2 ) =

2 log p(lcs(c1 , c2 ))
log p(c1 ) + log p(c2 )

(20)

past, semantic relatedness measures evaluated different NLP tasks (Budanitsky & Hirst, 2006; Ponzetto & Strube, 2007) concluded measure
performs best problems. evaluation, use semantic relatedness
validation generic relations study depth contribute final results.
3.2.3 Substitution Matrix Relation Extraction
now, discussed two possible ways calculating substitution score d(, ),
using either distributional similarity measures, measures defined WordNet. However,
dependency paths generated parsers may contain words (or lemmata),
syntactic functions subjects, objects, modifiers, others. take
account, revise definition d(, ). assume sequences x = x1 x2 . . . xn
x0 = x01 x02 . . . x0m contain words (xi W W refers set words) syntactic
functions accompanied direction (xi
/ W ). elements W unique words (or
lemmata) found dependency paths, instance, paths
actions Brcko arrival Beijing Example (1) Section 2.3.5,
W = {his, actions, in, Brcko, arrival, Beijing}. dependency paths use present
work include information syntactic functions, instance awareness
joy. case, W = {awareness, come, joy} W = {
18

prep f rom

prep f rom nsubj



, }.



nsubj

come

fiUsing Local Alignments Relation Recognition

Then,

d(xi , x0j )




1
0
d0 (xi , x0j ) =


0



0

xi , x0j W
xi , x0j
/ W & xi = x0j
0
xi , xj
/ W & xi 6= x0j
xi W & x0j
/W
xi
/ W & x0j W

(21)

Equation (21) states whenever element xi sequence x compared
element x0j sequence x0 , substitution score equal either (i) similarity
score case elements words (lemmata), (ii) 1, elements
syntactic function, (iii) 0, case.
follows discussion similarity measures above, two ways define
d(xi , x0j ), using either distributional similarity xi x0j (Section 3.2.1),
WordNet similarity, provided annotated WordNet synsets (Section 3.2.2).

4. Experimental Set-up
section, describe data sets used experiments provide
information data collections used estimating distributional similarity.
4.1 Data
evaluate performance LA kernel, consider two types text data, domainspecific data, comes biomedical domain generic domain-independent
data represents variety well-known widely used relations PartWhole Cause-Effect.
work, extract dependency path two nodes corresponding
arguments binary relation. assume analysis results tree since
acyclic graph, exists unique path pair nodes.
consider, however, structures might derived full syntactic analysis
in, example, subtrees (Moschitti, 2006).
4.1.1 Biomedical Relations
Corpora use three corpora come biomedical field contain annotations either interacting proteins - BC-PPI2 (1,000 sentences), AImed (Bunescu & Mooney,
2005b) interactions among proteins genes LLL (77 sentences training set
87 test set, Nedellec, 2005). BC-PPI corpus created sampling sentences BioCreAtive challenge, AImed corpus sampled Medline
collection. LLL corpus composed querying Medline term Bacillus subtilis. difference among three corpora lies directionality interactions.
Table 3 shows, relations AImed corpus strictly symmetric, LLL asymmetric BC-PPI contains types. differences number training instances
AImed corpus explained fact correspond dependency
2. Available http://www2.informatik.hu-berlin.de/~hakenber/.

19

fiKatrenko, Adriaans, & van Someren

paths named entities. parsing fails produces several disconnected graphs per
sentence, dependency path extracted.
Parser
LinkParser
LinkParser
Stanford
Stanford
Enju

Data set
LLL (train)
LLL (test)
BC-PPI
AImed
AImed

#examples
618
476
664
3763
5272

#pos
153
83
250
922
918

direction
asymmetric
asymmetric
mixed
symmetric
symmetric

a. Even though actual annotations test data given, number interactions
test data set provided LLL organizers.

Table 3: Statistics biomedical data sets LLL, BC-PPI, AImedd. table, #pos
stands number positive examples per data set #examples indicates
number examples total.

goal relation extraction three cases output correct interactions
biomedical entities (genes proteins) found input data.
biomedical entities already provided, need named entity recognition.
discrepancy training test sets used LLL challenge.
Unlike training set, sentence example least one interaction,
test set contains sentences interaction. organizers LLL challenge distinguish sentences without coreferences. Sentences coreferences
usually appositions, shown one examples below. first sentence (4.1.1)
example sentence without coreferences (with interaction ykuD SigK),
whereas second one sentence coreference (with interaction spoIVA
sigmaE). precisely, spoIVA refers phrase one genes
known interact sigmaE. therefore infer spoIVA interacts sigmaE. Sentences without coreferences form subset, refer LLL-nocoref,
sentences coreferences part separate subset LLL-coref.
(22) ykuD transcribed SigK RNA polymerase T4 sporulation.
(23) Finally, show proper localization SpoIVA required expression one
genes which, spoIVA, control mother cell
transcription factor sigmaE.
assumed relations sentences coreferences harder recognize. show LA kernel performs subsets, report experimental results full set test data (LLL-all), subsets (LLL-coref LLL-nocoref).
Syntactic analysis analyzed BC-PPI corpus Stanford parser. LLL
corpus already preprocessed LinkParser output checked
experts. enable comparison previous work, used AImed corpus parsed

20

fiUsing Local Alignments Relation Recognition

Stanford parser 3 Enju parser 4 (which exactly correspond input
experiments Erkan et al., 2007 Stre et al., 2008). Unlike Stanford parser,
Enju based Head-driven Phrase Structure Grammar (HPSG). output
Enju parser presented two ways, either predicate argument structure
phrase structure tree. Predicate argument structures describe relations words
sentence, phrase structure presents sentence structure form clauses
phrases. addition, Enju trained GENIA corpus includes model
parsing biomedical texts.
(24) Cbf3 contains three proteins, Cbf3a, Cbf3b Cbf3c.

contains

dobj

nsubj

proteins

Cbf3

num

conj

conj conj

three

Cbf3a
nsubj

Cbf3b
dobj

Cbf3 contains proteins
nsubj
dobj
Cbf3 contains proteins
nsubj
dobj
Cbf3 contains proteins

Cbf3b

conj

Cbf3a
Cbf3b
conj
Cbf3c
conj

Figure 1: Stanford parser output representation Example (24).
Figure 1 shows dependency tree obtained Stanford parser sentence
(24). sentence mentions three interactions among proteins, precisely,
Cbf3 Cbf3a, Cbf3 Cbf3b, Cbf3 Cbf3c. three dependency
paths contain words (lemmata) syntactic functions (such subj subject) plus
direction traversing tree. Figure 2 presents output sentence provided
Enju parser. upper part refers phrase structure tree lower part
shows paths extracted predicate argument structure. two parsers clearly
differ output. First, Stanford parser conveniently generates paths
three interaction pairs Enju analyzer not. Second, output
Stanford parser excludes prepositions conjunctions attached syntactic
functions whereas Enju analyzer lists parsing results. differences
3. Available http://nlp.stanford.edu/software/lex-parser.shtml.
4. Available http://www-tsujii.is.s.u-tokyo.ac.jp/enju/.

21

fiKatrenko, Adriaans, & van Someren

lead different input sequences later fed LA kernel. Consequently,
variations input may translate differences final performance.

Cbf3
Cbf3
Cbf3

ARG1/verb



ARG1/verb



ARG1/verb



contain
contain
contain

ARG2/verb



ARG2/verb



ARG2/verb



protein
protein
protein

ARG1/app



ARG1/app



ARG1/app



,
,
,

ARG2/app



ARG2/app



ARG2/app



Cbf3a
Cbf3a
Cbf3a

ARG1/coord



ARG1/coord



,

ARG2/coord





Cbf3b

ARG2/coord



Cbf3c

Figure 2: Enjus output representation Example (24).
addition, work employing AImed, dependency paths
Figure 1 Figure 2 preprocessed following way. actual named entities
arguments relation replaced label, e.g. PROTEIN. Consequently,
nsubj

dobj

conj

first path Figure 1 becomes PROTEIN contains proteins PROTEIN.
able compare results AImed performance reported work
Erkan et al. (2007) Stre et al. (2008), use exactly dependency paths
argument labels. However, study whether using labels instead actual named entities
impact final results LLL data set, carry two experiments.
first one, dependency paths contain named entities, whereas second contain
labels. second experiment referred adding word LABEL name (as
LLL-all-LABEL Table 7).
4.1.2 Generic Relations
second type relations consider generic relations. arguments
sometimes annotated using external resources WordNet, makes possible
use semantic relatedness measures defined them. example approach

22

fiUsing Local Alignments Relation Recognition

data used SemEval-2007 challenge, Task 04: Classification Semantic Relations
Nominals (Girju et al., 2009).
goal Task 4 classify seven semantic relations (Cause - Effect, Instrument - Agency, Product - Producer, Origin - Entity, Theme - Tool, Part Whole Content - Container), whose examples collected Web using
predefined queries. words, given set examples relation, expected output would binary classification whether example belongs given
relation not. arguments relation annotated synsets WordNet
hierarchy, Figure 3. Given sentence pair (spiritual awareness, joy)
corresponding synsets joy%1:12:00 awareness%1:09:00, would mean classifier decide whether pair example Cause-Effect relation.
particular sentence retrieved quering Web phrase joy comes *.
synsets manually selected WordNet hierarchy. seven semantic
relations used challenge, gives seven binary classification problems.

Genuine <e1>joy</e1> comes <e2>spiritual awareness</e2> life absolute clarity direction, living purpose.
WordNet(e1) = joy%1:12:00, WordNet(e2) = awareness%1:09:00,
Query: joy comes *, Cause-Effect(e2, e1) = true

Figure 3: annotated example Cause - Effect SemEval-2007, Task 4
training data set.

relation type
Origin - Entity
Product - Producer
Theme - Tool
Instrument - Agency
Part - Whole
Content - Container
Cause - Effect

#examples (train)
140
140
140
140
140
140
140

#pos (train)
54
85
58
71
65
65
73

#examples (test)
81
93
71
78
72
74
80

direction
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric

Table 4: Distribution SemEval-2007, Task 4 examples (training test data),
#pos stands number positive examples per data set #examples
indicates number examples total.

Syntactic analysis generate dependency paths, seven data sets used SemEval 2007, Task 4, analyzed Stanford parser. dependency path sentence
Figure 3 given (25).
23

fiKatrenko, Adriaans, & van Someren

(25) awareness#n#1

prep f rom



nsubj

come joy#n#1

Here, words annotated WordNet PoS tag attached, followed sense.
instance, awareness noun current context first sense used,
corresponds awareness#n#1.
4.2 Substitution Matrix
build substitution matrix LA kernel, use either distributional similarity
WordNet semantic relatedness measures. data set dependency paths,
contains unique elements (words syntactic functions), size matrix t.
k elements words, number substitution scores computed
distributional similarity (or semantic relatedness) measures equals k(k + 1)/2. due
fact measures use symmetric. substitution matrix built
corpus used experiments, results three substitution matrices
biomedical domain (for BC-PPI, LLL, AImed) seven substitution matrices
generic relations. follows, discuss settings used calculating
substitution matrix detail.
Distributional similarity estimated either using contextual information (O
Seaghdha & Copestake, 2008), exploring grammatical relations words (Lee,
1999). work opt contextual information. motivated presence
words belonging different parts speech dependency paths. instance,
even though, according dependency grammar theory (Melcuk, 1988), adjectives
govern words, may still occur dependency paths. words, even
parsing fail, may produce unreliable syntactic structures. able compare
words part speech, decided estimate distributional similarity based
contextual information, rather grammatical relations.
computing distributional similarity, may happen given word xi
occur corpus. handle cases, always set d(xi , xi ) = 1 (the largest possible
similarity score), d(xi , x0j ) = 0 xi 6= x0j (the lowest possible similarity score).
4.2.1 Biomedical domain
estimate distributional similarity biomedical domain, use TREC 2006
Genomics collection (Hersch, Cohen, Roberts, & Rakapalli, 2006) contains 162,259
documents 49 journals. documents preprocessed removing HTMLtags, citations text reference sections stemmed Porter stemmer (van
Rijsbergen, Robertson, & Porter, 1980). Furthermore, query-likelihood approach
Dirichlet smoothing (Chen & Goodman, 1996) used retrieve document passages given
query. passages ranked according likelihood generating query. Dirichlet smoothing used avoid zero probabilities poor probability estimates (which may
happen words occur documents). k unique words occurring set
dependency paths sequences fed queries collect corpus estimating similarity.
Immediate context surrounding pair words used calculate distributional
similarity words. set context window 2 (2 tokens right 2

24

fiUsing Local Alignments Relation Recognition

tokens left word focus) perform kind preprocessing
PoS tagging.
4.2.2 Generic relations
generic relations, use WordNet relatedness measures described Section 3.2.2.
already shown WordNet relatedness measures work synsets,
assumes words manually annotated information WordNet.
Since done relations arguments (see example Figure 3),
words sentences (and, correspondingly, dependency paths), build
substitution matrix follows. two words annotated WordNet, substitution score equals value returned relatedness measure used. word
pair, equals 1 whenever words identical, 0 otherwise. 5 example,
consider words dependency path (25) Wu-Palmer (wup) relatedness
measure, substitution scores obtain follows:

d(awareness#n#1, awareness#n#1) = 1
d(awareness#n#1, come) = 0
d(awareness#n#1, joy#n#1) = 0.35
d(prep from, come) = 0
d(prep from, joy#n#1) = 0
d(come, nsubj) = 0
d(nsubj, nsubj) = 1
d(joy#n#1, joy#n#1) = 1

d(awareness#n#1, prep from) = 0
d(awareness#n#1, nsubj) = 0
d(prep from, prep from) = 1
d(prep from, nsubj) = 0
d(come, come) = 1
d(come, joy#n#1) = 0
d(nsubj, joy#n#1) = 0

Figure 4: substitution scores dependency path (25) using wup measure.
Syntactic relations (prep from, subj) accompanied direction
dependency tree traversal (either ).

dependency path (25), 5 unique elements (t), 2 annotated
WordNet synsets (k). Consequently, 5*6/2 = 15 substitution scores total,
3 computed using WordNet relatedness.
compute WordNet relatedness, use WordNet::Similarity package WordNet 3.0 (Pedersen, Patwardhan, & Michelizzi, 2004).
4.3 Baselines Kernel Settings
section, discuss two baselines kernel settings.
4.3.1 Baselines
test well local alignment kernels perform compared kernels proposed past,
implemented shortest path kernel described work Bunescu Mooney
5. applies cases relation arguments could annotated WordNet
information.

25

fiKatrenko, Adriaans, & van Someren

(2005a) (Section 2.3.5) one baselines (Baseline I). method seems
natural choice operates data structures (dependency paths).
Similarly Bunescu Mooneys (2005a) work, experiments use lemma, part
speech tag direction, consider entity type negative polarity items.
choice LA kernel paper motivated ability
compare sequences flexible way, possibility explore additional
information (not present training set) via substitution matrix. baseline,
Baseline II, used test whether choice similarity measures affects results.
case, substitution scores d(, ) calculated using distributional similarity
WordNet relatedness, generated randomly within interval [0, 1].
4.3.2 Kernel settings
kernels compute used together support vector machine tool LibSVM
(Chang & Lin, 2001) detect hyperplanes separating positive examples negative
ones. plugging kernel matrices 10-fold cross-validation LibSVM,
normalized Equation 26.

0

0

k(x, y)

k(x , ) = p

k(x, x)k(y, y)

(26)

handle imbalanced data sets (most notably AImed BC-PPI), examples
weighted using inverse-class probability (i.e. training examples class weighted
1/prob(A) prob(A) fraction training examples class A). significance
tests done using two-tailed paired t-test confidence level 95% ( = 0.05).
addition, experiments tuned penalty parameter C (Equation 4)
range (26 , 24 , . . . , 212 ).
use LA kernel, one set following parameters: gap opening cost,
gap extension cost, scaling parameter . cross-validation experiments,
gap opening cost set 1.2, extension cost 0.2 scaling parameter
1. choice scaling value motivated experiments amino acids
biological domain (Saigo et al., 2004). initial experiments, present
study parameter values varied.

5. Experiment I: Domain-Specific Relations
goal evaluation study behavior LA kernel domain-specific
relations biomedical domain. section, report experiments conducted
three biomedical corpora using LA kernel based distributional similarity measures, two baselines results published previously (e.g., using graph kernel Airola
et al., 2008 tree kernel Stre et al., 2008). best knowledge, string
kernels applied dependency paths yet. However, gap-weighted string
kernel (described Section 2) allows gapping thus compared LA
kernel. test Lodhi et al.s (2002) kernel performs dependency paths, use

26

fiUsing Local Alignments Relation Recognition

three corpora. tuned parameters string kernel set length
subsequences 4 decay factor 0.5. 6
5.1 LLL BC-PPI Data Sets
subsection presents results two biomedical data sets, BC-PPI LLL. Whenever
possible, discuss performance previously reported literature.
10-fold cross-validation results BC-PPI corpus presented Table 5
LLL training data set Table 6. LA kernel based distributional similarity
measures (LA-Dice, LA-Cosine LA-L2) performs significantly better two baselines. Recall Baseline corresponds shortest path approach (Section 2.3.5)
Baseline II LA kernel randomly generated substitution scores. contrast
Baseline I, able handle sequences different lengths including gaps. According
Equation 5, comparison two sequences different lengths results 0-score.
Nevertheless, still yields high recall, precision much lower. explained
fact shortest path uses PoS tags. Even though two sequences
length different, comparison may still result non-zero score, provided
part speech tags match. Furthermore, Baseline II suggests accurate estimation substitution scores important achieving good performance. Baseline II may
yield better results Baseline I, randomly generated substitution scores degrade
performance.
Method
LA-Dice
LA-Cosine
LA-L2
Baseline
Baseline II
Gap-weighted string kernel (Lodhi et al., 2002)

Precision
75.56
76.40
77.56
32.04
66.36
72.00

Recall
79.72
80.66
79.31
75.63
54.48
75.31

F-score
77.56
78.13
78.42
45.00
59.80
73.62

Table 5: 10-fold cross-validation BC-PPI data set.
first glance, choice distributional similarity measures affect
overall performance yielded LA kernel. BC-PPI data, method based
L2 measure outperforms methods based Dice (p.07) Cosine,
differences latter case significant. statistically significant differences
observed method based Dice Cosine.
contrast BC-PPI data set, kernels use Dice Cosine measures
LLL data set significantly outperform one based L2 (at p1.22107
p1.33106 , respectively).
data sets, LA method using distributional similarity measures significantly
outperforms baselines. Interestingly, gap-weighted string kernel Lodhi et al.
(2002) yields good performance seems better choice subsequence
6. Lodhi et al. (2002) mentioned paper F1 numbers (with respect SSK) seem
peak subsequence length 4 7.

27

fiKatrenko, Adriaans, & van Someren

kernel based shallow linguistic information (Giuliano et al., 2006). Recent work
LLL (Fundel, Kueffner, & Zimmer, 2007) employs dependency information but, contrast
method, serves representation extraction rules defined. Airola
et al. (2008) apply graph kernel-based approach extract interactions use, among
others, LLL AImed data sets. seen Table 6, method yields results
comparable gap-weighted string kernel dependency paths.
best knowledge, performance achieved LA kernel LLL training set
highest (in terms F-score) among results reported
literature.
Method
LA-Dice
LA-Cosine
LA-L2
Baseline
Baseline II
Graph kernel (Airola et al., 2008)
Gap-weighted string kernel (Lodhi et al., 2002)
Shallow linguistic kernel (Giuliano et al., 2006)
Rule-based method (Fundel et al., 2007)

Precision
88.76
88.63
86.80
39.02
65.82
72.5
83.66
62.10
68

Recall
81.62
82.09
75.04
100.00
41.32
87.2
71.11
61.30
83

F-score
85.03
85.23
80.49
56.13
50.76
76.8
76.88
61.70
75

Table 6: 10-fold cross-validation LLL-all training data set.
apply method LLL test data (Table 7). 7 Even though performance test set poorer, LA-Dice outperforms baselines. addition,
gap-weighted string kernel (Lodhi et al., 2002) seems perform much worse test
set. LA kernel, precision high, recall decreases (and drastically
data subset includes co-references). might due fact
sentences incomplete parses generated and, consequently, dependency paths
entities found. 91 567 possible interaction pairs generated
test data, dependency path extracted. contrast, approach reported
Giuliano et al. (2006) make use syntactic information, data subset
without coreferences achieves higher recall.
hand, lower recall caused using actual names proteins
genes arguments. work reported before, relation arguments
named entities often replaced types (e.g., PROTEIN) used
input learning algorithm. conducted additional experiments using named entity
types dependency paths, led great improvement terms recall
F-score (Table 7, LLL-coref-LABEL, LLL-nocoref-LABEL, LLL-coref-LABEL). method
clearly outperforms shallow linguistic kernel achieves better results
best-performing system LLL competition (Sbest ), which, according Nedellec (2005),
applied Markov logic syntactic paths.
7. Airola et al. (2008) report performance LLL data set and, reason, information
graph all-paths kernel included Table 7.

28

fiUsing Local Alignments Relation Recognition

Data set
LLL-coref
LLL-nocoref
LLL-all
LLL-all
LLL-all
LLL-coref-LABEL
LLL-nocoref-LABEL
LLL-all-LABEL
LLL-coref
LLL-nocoref
LLL-all
LLL-all
LLL-all

Method
LA-Dice
LA-Dice
LA-Dice
Baseline
Baseline II
LA-Dice
LA-Dice
LA-Dice
Shallow linguistic kernel (Giuliano
Shallow linguistic kernel (Giuliano
Shallow linguistic kernel (Giuliano
Gap-weighted string kernel (Lodhi
Sbest (Nedellec, 2005)

et
et
et
et

al.,
al.,
al.,
al.,

2006)
2006)
2006)
2002)

Precision
52.3
70.7
72.7
48.6
12.9
60.0
69.0
74.5
29.0
54.8
56.0
56.0
60.9

Recall
37.9
53.7
48.1
43.3
45.7
51.7
53.7
53.0
31.0
62.9
61.4
16.8
46.2

F-score
44.0
61.0
57.9
45.8
20.1
55.5
60.4
61.9
30.0
58.6
58.6
25.9
52.6

Table 7: Results LLL test data set.

5.2 AImed Data Set
Yet another data set consider AImed. data set often used
experiments relation extraction biomedical domain, enables comparison
methods. noted, however, particular case, corpus
collection documents (abstracts). may lead two ways performing 10-fold
cross-validation. One possibility lies randomly splitting data 10 parts,
cross-validation level documents. experiments report
done using first setting directly compared methods described
work Stre et al. (2008), Erkan et al. (2007) Giuliano et al. (2006). addition,
use dependency paths LA kernel ones employed Stre et al.
Erkan et al.. results Airola et al. (2008) Bunescu (2007) obtained
cross-validating level documents.
conducted experiments setting distributional measure Dice, referred
LA-Dice Table 8. upper part table used dependency paths generated
Stanford parser lower part obtained Enju. discussed
Section 2, Erkan et al. (2007) use similarity measures compare dependency paths,
consider additional sources whose information incorporated
learning procedure. They, however, experiment supervised (SVM) semi-supervised
learning (TSVM), number training instances varied. Table 8 shows best
performance achieved Erkan et al.s (2007) method. Among models based
SVM, one Cosine distance, SVM-Cos, yields best results. TSVM
setting, one Edit measure performs best. observe LA-Dice slightly
outperforms has, particular, high precision.
work, Stre et al. (2008) explore several parsers combinations features.
features include paths Enju, word dependencies generated
data-driven KSDEP parser, word features. KSDEP parser based probabilistic

29

fiKatrenko, Adriaans, & van Someren

shift-reduce algorithm (Sagae & Tsujii, 2007). general, method Stre et al.
uses SVM, case focuses tree kernels (discussed Section 2.3.3). make
fair comparison, conducted experiments paths obtained deep syntactic analysis
(Enju parser) compared scores Stre et al.s (2008) results. contrast
previous experiments, achieve higher recall lower precision. Overall, LA
kernel yields better performance one reported Stre et al. However,
different sets features combined (parses Enju KSDEP plus word features Enju+KSDEP+W Table 8), overall performance improved.
Method
LA-Dice
Baseline (Bunescu, 2007)
Baseline II
SVM-Cos (Erkan et al., 2007)
TSVM-Edit (Erkan et al., 2007)
Gap-weighted string kernel (Lodhi et al., 2002)
LA-Dice
Tree kernel (Stre et al., 2008)
Tree kernel (Stre et al., 2008)
Graph kernel (Airola et al., 2008)
Shallow linguistic kernel (Giuliano et al., 2006)

Parser
Stanford
Collins
Stanford
Stanford
Stanford
Stanford
Enju
Enju
Enju+KSDEP+W
Charniak-Lease
none

Precision
69.09
69.08
48.89
61.99
59.59
67.25
71.16
76.0
78.1
52.9
60.9

Recall
54.63
35.00
25.06
54.99
60.68
54.67
46.71
39.7
62.7
61.8
57.2

F-score
61.02
46.46
33.07
58.09
59.96
60.31
56.40
52.0
69.5
56.4
59.0

Table 8: 10-fold cross-validation AImed data set.
Bunescu (2007) reports evaluation results AImed corpus form
precision-recall curve. consider highest precision obtained experiments (69.09 71.16, depending input), roughly corresponds recall
35% plot (referred Baseline Table 8). sum, shortest path approach
never approaches performance LA kernel biomedical data sets
studied here. baseline, Baseline II, achieves lowest scores
methods presented here.
Table 8 illustrates various methods trained AImed corpus,
many different parsers used. noted graph kernel
trained tested syntactic representation generated Charniak-Lease
parser, shortest path kernel explored dependency paths obtained
Collins parser. Charniak-Lease parser statistical parser trained biomedical
data (Lease & Charniak, 2005), whose phrase structures transformed dependencies. Likewise, Collins parser statistical parser (Collins, 1999). leads
question whether choice syntactic parser significant impact extraction
results. compare impact syntactic parsers relation extraction AImed,
Miyao et al. (2008) conducted complex study eight parsers (including Stanford analyzer) five parse representations 8 . consider two cases. first one,
parsers trained biomedical data. Regardless parser used
experiments, accuracy extraction task similar. second experiment,
8. either various dependency tree formats (e. g., Stanford dependency format), phrase
structures, predicate-arguments structures.

30

fiUsing Local Alignments Relation Recognition

parsers re-trained domain-specific data. case, shown
relation extraction results improved. actual gain, however, vary
one parser another.
AImed data, LA kernel Dice measure gives state-of-the-art results.
outperformed approaches use information dependency
paths.
5.3 LA Kernel Parameters
Saigo et al. (2004) already shown scaling parameter (Equation 11)
significant impact accuracy. carried additional experiments varying
gap values value . Results visualized Figure 5. opening extension
gap values separated slash symbol values X-axis form a/b
read opening gap set extension gap equal b.
kernel matrices normalized examples weighted. According previous
experiments, results yielded Dice measure significantly differ
ones achieved Cosine measure selected Dice measure conduct
experiments. performance BC-PPI data set shown Figure 5.

F-score
76
74
72
70
68
66
64
62
60
58

76
74
72
70
68
66
64
62
60
58

12/2
12/12
gaps

24/2
24/12

0.1

0.3

0.5

0.8

1

5

scaling

Figure 5: Varying gaps scaling () parameter BC-PPI data set (10-fold
cross-validation): F-score.

31

fiKatrenko, Adriaans, & van Someren

80
79
78
77
76
75
74
73
72
71
70

Precision
90
85
80
75
70
65

5

60

1

12/2

0.8
12/12
gaps

scaling

0.5

24/2

0.3
24/12

0.1

Figure 6: Varying gaps scaling () parameter BC-PPI data set (10-fold
cross-validation): Precision.

75
70

Recall

65

90

60

80

55

70

50

60

45

50

5

40

1

12/2

0.8
12/12
gaps

0.5

24/2

scaling

0.3
24/12

0.1

Figure 7: Varying gaps scaling () parameter BC-PPI data set (10-fold
cross-validation): Recall.
32

fiUsing Local Alignments Relation Recognition

results Figure 5 indicate decreasing leads decrease overall performance. Moreover, varying gap values causes subtle changes F-score,
changes drastic changes due lower .
Changes F-score likely explained variances precision
recall. investigate matter, look measures depend parameter
changes. set low value, one expect nearly diminish impact
substitution matrix, i.e. similarity among elements. reason hypothesize
larger values scaling parameter result higher recall. Indeed, Figure 7
supports hypothesis recall plot resembles one F-score. Varying
parameter values much lower impact precision (Figure 6) nonetheless precision
decrease parameter becomes larger.
Overall, seems influence final results most, although gap values make
contribution well. According results obtained, setting extension gap e
large value (or equal opening gap o) undesirable. Since scaling parameter
applied substitution matrix gap values well, setting
0.5 decreases effects gap penalization similarity elements. Consequently,
best performance achieved setting 1. suggests final performance
LA kernel influenced combination parameters choice crucial
obtaining good performance.

6. Experiment II: Generic Relations
Another series experiments carried seven generic relations SemEval
- 2007 challenge, Task 4. choice data sets case motivated two
factors. First, semantic relations used differ relations biomedical
domain. Second, since arguments relations annotated WordNet, becomes
possible explore information WordNet use prior knowledge LA
kernel.
Many participants challenge considered WordNet either explicitly (Tribble &
Fahlman, 2007; Kim & Baldwin, 2007), part complex system (Giuliano et al.,
2007). Since always obvious use WordNet yields best performance, many researchers made additional decisions use supersenses (Hendrickx et al., 2007), selection predefined number high-level concepts (Nulty, 2007),
cutting WordNet hierarchy certain level (Bedmar et al., 2007). systems
one Nakov (2007) based solely information collected Web.
Even though became evident best performing systems used WordNet, variance results remarkable clear whether difference performance
explained machine learning methods used, combination features,
factors.
SemEval-2007 Task-4 data set includes relation examples nominal
compounds (like coffee maker), greatly reduces availability information
two arguments dependency paths. relation arguments case linked
one grammatical relation (e.g., coffee maker linked grammatical relation
nn, corresponds noun compound). assume, therefore, information coming
WordNet especially helpful dependency paths short.

33

fiKatrenko, Adriaans, & van Someren

experiments used 5 relatedness measures defined earlier Section 3.2 plus one additional
measure called random. random measure indicates relatedness values
two relation arguments generated randomly (within [0, 1]) thus
suitable baseline (Baseline II). Similarly experiments biomedical domain,
another baseline shortest path kernel (Baseline I). Note Task 4 overview
paper, Girju et al. (2007) reported three baselines, which, case, (i) guessing
true false examples, depending class majority class test
set (Baseline III), (ii) always guessing true (Baseline IV), (iii) guessing true false
probability corresponds class distribution test set (Baseline V).
first question interest implications choice semantic relatedness
measure performance LA kernel. answer question, perform
10-fold cross-validation training set (Figure 9, Figure 10 Figure 11). Among
5 measures jcn resnik fail perform better random score.
cases, Resnik score outperformed measures. behaviour LeacockChodorow score (lch) jcn varies one semantic relation another. instance, use
jcn seems boost precision Cause-Effect, Part-Whole, Product - Producer,
Theme - Tool. remaining three relations clearly best-performing
measure.
check whether differences relatedness measures, carried
significance tests comparing measures relations. findings summarized
Table 9. Here, symbol two relatedness measures stands measure
equivalence, or, words, indicates significant difference. Similarly
experiments biomedical field, significance tests conducted using
two-tailed paired t-test confidence level 95%. addition, two measures
b, > b means performs significantly better b. instance, ranking
Cause - Effect Table 9 read follows. two best performing measures
wup lch, significantly outperform lin, followed random res, which,
turn, yield significantly better results jcn. seen table wup
lch clearly best performing measures seven relations (each best
measure six seven relations).
Relation type
Cause - Effect
Instrument - Agency
Product - Producer
Origin - Entity
Theme - Tool
Part - Whole
Content - Container

Ranking
wup lch > lin
wup lch > lin
wup lch > lin
wup lch > lin
lch > lin wup
wup lin lch
wup > lch > lin

>
>

>
>
>


res random > jcn
res > jcn random
jcn res > random
res jcn > random
res > jcn > random
res > jcn random
res > jcn random

Table 9: Ranking relatedness measures respect accuracy training sets ( stands measure equivalence, > b indicates measure
significantly outperforms b).

34

fiUsing Local Alignments Relation Recognition

relation, applied best performing measure training set
particular relation test data. results reported Table 10. average, LA
kernel employing WordNet relatedness measures significantly outperforms two baselines.
Moreover, compared best results SemEval-2007 competition (Beamer
et al., 2007), method approaches performance yielded best system (bestSV ).
system used various lexical, syntactic, semantic feature sets,
expanded training set adding examples many different sources. already
mentioned Section 2 recent work Seaghdha (2009) explores WordNet
structure graph kernels classify semantic relations. overall performance
achieved method (Table 10) comparable one LA kernel,
unclear whether semantic relations one approaches performs
better.
Relation type
Cause - Effect
Instrument - Agency
Product - Producer
Origin - Entity
Theme - Tool
Part - Whole
Content - Container
Average
Baseline
Baseline II
Baseline III
Baseline IV
Baseline V
bestSV
Gap-weighted string kernel (Lodhi et al., 2002)
WordNet kernels (O Seaghdha, 2009)

Accuracy
61.25
75.64
75.27
74.07
73.24
80.56
71.62
73.09
58.23
55.83
57.0
48.5
48.5
76.3
61.19
74.1

Precision
62.50
73.17
76.71
75.86
67.86
70.00
74.29
71.48
52.50
61.61
81.3
48.5
48.5
79.7
66.2
-

Recall
60.98
78.95
90.32
61.11
65.52
80.77
68.42
72.30
54.30
55.50
42.9
100.0
57.1
69.8
47.52
-

F-score
61.73
75.95
82.96
67.69
66.67
75.00
71.23
71.60
49.19
53.93
30.8
64.8
48.5
72.4
43.02
71.0

measure
lch
wup
lch
wup
lch
wup
wup

Table 10: Results SemEval-2007, Task 4 test data set (selecting best performing
measure training set relation).

addition, report results SemEval Task 4 test set per relatedness measure
(Table 11), averages seven relations. Similarly findings
training set, wup lch best performing measures test data well.
One would expect optimal use prior knowledge allow us reduce
number training instances without significant changes performance. study
(and whether) amount training data influences results test set, split
training set several subsets, creating model subset applying
SemEval-2007, Task 4 test data. split corresponds split used challenge
organizers. Figure 8 9 suggests, relations recognized well even relatively
small data sample used. exception Theme-Tool relation increasing
9. model trained 35 Origin-Entity examples classifies none test examples positive,
reason point Figure 8 relation given 35 training examples.

35

fiKatrenko, Adriaans, & van Someren

training data clearly helps. finding line results Giuliano et al. (2007)
whose system combination kernels data. results indicate
relations one (Theme-Tool) extracted well, even quarter
training set used.
Relatedness measure
wup
lch
lin
res
jcn
random

Accuracy
72.91
72.96
65.27
62.94
55.55
56.57

Precision
71.20
72.31
62.01
62.51
52.25
53.10

Recall
72.56
70.93
67.07
59.66
69.28
52.94

F-score
71.62
71.02
63.65
60.46
57.07
52.83

Table 11: Results SemEval-2007, Task 4 test data set, averages 7 relations
per WordNet relatedness measure.

Learning curve
100
90
80
70

F-score

60
50
Cause-Effect
Instrument-Agency
Product-Producer
Origin-Entity
Theme-Tool
Part-Whole
Content-Container

40
30
20
10
0

35

70
105
training examples

140

Figure 8: Learning curve SemEval-2007, Task 4 test data set.
recent work SemEval Task 4 data set includes investigation distributional kernels (O Seaghdha & Copestake, 2008), pattern clusters (Davidov & Rappoport,
2008), relational similarity (Nakov & Hearst, 2008), WordNet kernels. Unlike WordNet
kernels, first three approaches use WordNet. Seaghdha Copestake (2008)
report accuracy 70.7 F-score 67.5 best results yielded distributional kernels best performance Davidov Rappoports (2008) method
accuracy 70.1, F-score 70.6. WordNet kernels, similarly findings
LA kernel, yield better accuracy methods using WordNet (74.1),

36

fiUsing Local Alignments Relation Recognition

Cause-Effect
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Instrument-Agency
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Product-Producer
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Figure 9: 10-fold cross-validation training set (Cause - Effect, Instrument Agency Product - Producer relations).

37

fiKatrenko, Adriaans, & van Someren

Origin-Entity
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Theme-Tool
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Part-Whole
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Figure 10: 10-fold cross-validation training set (Origin - Entity, Theme - Tool
Part - Whole relations).

38

fiUsing Local Alignments Relation Recognition

Content-Container
100

precision
recall
F-score

90
80
70
60
50
40
30
20
10
0

wup

lin

lch
res
similarity measure

jcn

random

Figure 11: 10-fold cross-validation training set (Content - Container relation).

F-score comparable performance reported Seaghdha Copestake (2008)
Davidov Rappoport (2008).

7. Discussion
section revisit goals stated end Section 2 discuss
findings detail.
7.1 LA Kernel Relation Extraction
introduced LA kernel, proven effective biomedical problems,
NLP domain showed well suited relation extraction. particular, experiments two different domains either outperform existing methods yield
performance par existing state-of-the-art kernels.
One motivations using LA kernel relation extraction task
exploit prior knowledge. Here, explore two possibilities, distributional similarity
information provided WordNet.
7.1.1 Distributional Similarity Measures
setting, consider three distributional measures already studied
before. instance, Lee (1999) uses detect similar nouns based verb-object
co-occurrence pairs. results suggest Jaccard coefficient (which related
Dice measure) one best performing measures followed others including
Cosine. Euclidean distance fell group largest error rates. Given previous
work Lee (1999), one would expect Euclidean distance achieve worse results
39

fiKatrenko, Adriaans, & van Someren

two measures. Indeed, LLL corpus, LA kernel employing L2 shows
significant decrease performance. measures, method using Dice
significantly outperforms one based L2 measure LLL corpus
significant improvement BC-PPI data set. Based experiments
conducted, conclude LA kernel using Dice Cosine measures performs
similarly LLL data set BC-PPI corpus. Given results various biomedical corpora (and different settings experimented with), obtained experimental
support choosing Dice Cosine measure Euclidean distance.
7.1.2 WordNet Similarity Measures
generic relations, semantic relatedness plays significant role. difference
F-score models use semantic relatedness kernel relatedness
values generated randomly (Baseline II) amounts nearly 20%. measures exhibit
different performance seven generic relations considered.
observe, instance, wup, lch, lin almost always yield best results, matter
relation considered. found Resnik score Jiang Conraths measure
yield lower results measures. Even though F-scores per relation vary
quite substantially (by placing Cause-Effect, Theme-Tool, Origin-Entity among
difficult relations extract), two measures, wup lch, top-performing
measures seven relations. two measures explore WordNet taxonomy using
length paths two concepts, depth WordNet hierarchy and,
consequently, belong path-based measures. three measures, res, lin
jcn information content based measures, relatedness two concepts
defined amount information share. experiments LA
kernel generic relation recognition suggest that, particular case, path-based
measures preferred information content based measures.
stress, however, evaluation semantic relatedness measures context relation recognition, one means draw conclusion
top measures NLP tasks stay same. example, Budanitsky
Hirst (2006) use semantic relatedness measures detect malapropism show
Jiang Conraths measure (jcn) yields best results, followed Lins measure (lin),
one Leacock Chodorow (lch), Resniks measure (res).
results quite similar findings consider res measure, jcn
top accuracy ranking list seven semantic relations
studied.
7.2 Factors Parameters Influence LA Kernel Performance
experiments two domains shown LA kernel either outperforms existing
methods corpora, yields performance par existing state-of-the-art
kernels.
7.2.1 Baselines
advantage LA kernel Bunescu shortest path method (Baseline I)
capable handling paths different lengths. allowing gaps penalizing them,
40

fiUsing Local Alignments Relation Recognition

final kernel matrix becomes less sparse. shortest path approach attempts
generalize dependency paths, usually overgeneralizes leads high
recall scores (Table 5 Table 6) poor overall performance. One explanation
overgeneralization may method accounts well structural similarity (provided
sequences length) fails provide finer distinctions among dependency
paths. Consider, example, two sequences trip makes tram coffee makes
guy, whereby first path represents negative instance Product-Producer
relation second path corresponds positive one. Even though match
exactly, elements match nouns singular. Consequently, comparison
according shortest path method result relatively high similarity score.
contrast, LA kernel consider similarity elements pairs trip-coffee
tram-guy obtain low scores.
addition, Baseline II, based randomly generated substitution scores,
performs poor data sets (or comparable Baseline I). leads us conclusion
accurate estimation similarities another reason LA kernel performs well
relation extraction.
7.2.2 Comparison Methods
already pointed out, obvious shortcoming Baseline inability
handle dependency paths different length. reason, applied
gap-weighted string kernel (Lodhi et al., 2002) data sets. case, dependency
paths compared flexible way gapping allowed, additional
information used. kernel outperforms Baseline increasing precision relation
extraction preserving relatively high recall. data set fails yield
good results LLL test data, believe due differences LLL
training test data. data sets, LA kernel achieves better performance
gap-weighted string kernel. margin, however, different different data sets.
biomedical domain, differences two methods clearly seen
BC-PPI LLL data sets, results AImed corpus comparable.
However, methods tested AImed get higher scores unless use
features dependency paths. holds types cross-validation used
corpus. generic relations, difference LA kernel gapweighted string kernel much larger. particular, case gap-weighted kernel,
precision high, recall much lower. explained fact generic
relations benefit knowledge found WordNet recall achieved LA kernel
is, therefore, high. gap-weighted kernel access information found
dependency paths and, reason, fails find relations.
LA kernel achieves best performance LLL training set, outperforming
graph kernel (Airola et al., 2008), shallow linguistic kernel (Giuliano et al., 2006)
rule-based system Fundel et al. (2007). three used different input
methods, varying plain text dependency structures. reason, direct
comparison unfortunately possible, conclude methods employing
dependency information always among best performing approaches.

41

fiKatrenko, Adriaans, & van Someren

Two approaches whose performance reported AImed data set include tree kernel (Stre et al., 2008) TSVM (Erkan et al., 2007).
explore syntactic information different ways. Stre et al. consider subtrees,
method Erkan et al. similarities approach relies
dependency path comparison. comparison, use information already
available dependency paths (SVM setting), dependency paths (TSVM setting). According Lauer Bloch (2008), TSVMs fall category using prior
knowledge sampling methods, explores prior knowledge generating new
examples. contrast, employ information large unlabeled text sources order
enable finer comparison dependency paths always work supervised learning
setting. Using evaluation procedure work Stre et al. Erkan et al.
show LA kernel outperforms methods, differences data set
much smaller data sets used.
7.2.3 LA Parameters
demonstrated choice LA parameters crucial achieving good performance. experiments, scaling parameter contributes overall performance
most, parameters gap values taken account well.
approaches infinity, LA kernel approximates Smith-Waterman distance,
increasing necessarily positive impact final performance.
finding line results reported Saigo et al. (2004) homology detection
task. best performance yielded setting scaling parameter 1 bit higher,
penalizing gap extension less gap opening.

8. Conclusions Future Work
presented novel approach relation extraction based local alignments sequences. Using LA kernel provides us opportunity explore various
sources information study role relation recognition. Possible future directions include, therefore, examination distributional similarity measures, studying
impact extraction generic relations, looking sources information could helpful relation recognition. may interesting consider
relational similarity (Turney, 2006), looks correspondence relation
instances. case, one able infer doctor corresponds scalpel
similar way fisherman net (where (scalpel, doctor) (net, fisherman)
examples Instrument - Agency).
Despite sparseness problem might occur WordNet-based measures
used, measures advantage distributional measures treating elements compared concepts rather words. NLP community, steps
already taken solve problem clustering words large corpora aiming
word sense discovery (Pennacchiotti & Pantel, 2006). Recently, Mohammad (2008)
thesis investigated compatibility distributional measures ontological ones.
using corpus statistics thesaurus, author introduced distributional profiles
senses defined distance measures them. Even though new approach calculat-

42

fiUsing Local Alignments Relation Recognition

ing similarity tested generic corpora, would certain interest apply
domain-specific data.
Overall, local alignment kernels provide flexible means work data sequences.
First, allow partial match sequences particularly important
dealing text. Second, possible incorporate prior knowledge learning
process preserving kernel validity. general, LA kernels applied
NLP problems long input data form sequences.

Acknowledgments
authors wish thank Simon Carter Gerben de Vries comments
proofreading, three anonymous reviewers highly valuable feedback.
acknowledge input Adaptive Information Management (AIM) group
University Amsterdam. preliminary version work dicussed
22nd International Conference Computational Linguistics (CoLing 2008)
Seventh International Tbilisi Symposium Language, Logic Computation (2007).
work carried context Virtual Laboratory e-Science project
(www.vl-e.nl). project supported BSIK grant Dutch Ministry
Education, Culture Science (OC&W) part ICT innovation program
Ministry Economic Affairs (EZ).

References
Airola, A., Pyysalo, S., Bjorne, J., Pahikkala, T., Ginter, F., & Salakoski, T. (2008). Allpaths graph kernel protein-protein interaction extraction evaluation crosscorpus learning. BMC Bioinformatics, 9 (Suppl II).
Beamer, B., Bhat, S., Chee, B., Fister, A., Rozovskaya, A., & Girju, R. (2007). UIUC:
Knowledge-rich Approach Identifying Semantic Relations Nominals.
Proceedings Workshop Semantic Evaluations (SemEval), Prague, Czech
Republic.
Bedmar, I. S., Samy, D., & Martinez, J. L. (2007). UC3M: Classification semantic relations
nominals using sequential minimal optimization. SemEval-2007.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Bunescu, R. C. (2007). Learning Information Extraction. Ph.D. thesis, Department
Computer Sciences, University Texas Austin.
Bunescu, R. C., Ge, R., Kate, R. J., Marcotte, E. M., Mooney, R. J., Ramani, A. K., &
Wong, Y. W. (2005). Comparative experiments learning information extractors
proteins interactions. Artificial Intelligence Medicine, 33, 139155.
Bunescu, R. C., & Mooney, R. J. (2005a). shortest path dependency kernel relation
extraction. Joint Conference Human Language Technology / Empirical Methods
Natural Language Processing (HLT/EMNLP), Vancouver, BC.

43

fiKatrenko, Adriaans, & van Someren

Bunescu, R. C., & Mooney, R. J. (2005b). Subsequence kernels relation extraction.
Proceedings 19th Conference Neural Information Processing Systems,
Vancouver, BC.
Bunescu, R. C., & Mooney, R. J. (2006). Text Mining Natural Language Processing,
chap. Extracting Relations Text. Word Sequences Dependency Paths.
Springer.
Burges, C. J. C. (1998). tutorial support vector machines pattern recognition.
Data Mining Knowledge Discovery, 2 (2), 121167.
Camacho, R. (1994). use background knowledge inductive logic programming.
Report.
Cancedda, N., Gaussier, E., Goutte, C., & Renders, J.-M. (2003). Word-sequence kernels.
Journal Machine Learning Research, 3, 10591082.
Chang, C.-C., & Lin, C.-J. (2001). LIBSVM: library support vector machines. Software
available http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Chen, S. F., & Goodman, J. (1996). empirical study smoothing techniques language
modeling. ACL96.
Clegg, A. B. (2008). Computational-Linguistic Approaches Biological Text Mining. Ph.D.
thesis, University London.
Cohen, W. W., Ravikumar, P., & Fienberg, S. (2003). comparison string distance
metrics name-matching tasks. IIWeb 2003, pp. 7378.
Collins, M. (1999). Head-Driven Statistical Models Natural Language Parsing. Ph.D.
thesis, University Pennsylvania.
Collins, M., & Duffy, N. (2001). Convolution kernels natural language. Advances
Neural Information Processing Systems 14, pp. 625632. MIT Press.
Cortes, C., & Vapnik, V. (1995). Support vector networks. Machine Learning, 20 (3),
273297.
Davidov, D., & Rappoport, A. (2008). Classification semantic relationships
nominals using pattern clusters. Proceedings ACL-08:HLT, pp. 227235.
Dolan, W. B., Quirk, C., & Brockett, C. (2004). Unsupervised construction large paraphrase corpora: Exploiting massively parallel news sources. COLING 2004, Geneva,
Switzerland.
Erkan, G., Ozgur, A., & Radev, D. R. (2007). Semi-supervised classification extracting
protein interaction sentences using dependency parsing. 2007 Joint Conference
Empirical Methods Natural Language Processing Computational Natural
Language Learning, pp. 228237.
Fellbaum, C. (1998). WordNet: Electronic Lexical Database. MIT Press.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies Linguistic Analysis.
Philological Society, Oxford. Reprinted Palmer, F. (ed.), 1968.
Fundel, K., Kueffner, R., & Zimmer, R. (2007). RelEx - relation extraction using dependency
parse trees. Bioinformatics, 23 (3).
44

fiUsing Local Alignments Relation Recognition

Girju, R., Badulescu, A., & Moldovan, D. (2006). Automatic discovery part-whole relations. Computational Linguistics, 32 (1), 83135.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). SemEval2007 Task 04: Classification semantic relations nominals. ACL 2007.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2009). Classification semantic relations nominals. Language Resources Evaluation,
43 (2), 105121.
Giuliano, C., Lavelli, A., Pighin, D., & Romano, L. (2007). FBK-IRST: Kernel methods
semantic relation extraction. SemEval-2007.
Giuliano, C., Lavelli, A., & Romano, L. (2006). Exploiting shallow linguistic information
relation extraction biomedical literature. EACL 2006.
Grishman, R., & Sundheim, B. (1996). Message Understanding Conference - 6: brief
history. Proceedings 16th International Conference Computational Linguistics.
Haussler, D. (1999). Convolution kernels discrete structures. Tech. rep. UCS-CRL-99-10,
UC Santa Cruz.
Hearst, M. (1992). Automatic acquisition hyponyms large text data. Proceedings
COLING-92, pp. 539545.
Hendrickx, I., Morante, R., Sporleder, C., & van den Bosch, A. (2007). ILK: Machine
learning semantic relations shallow features almost data. SemEval2007.
Hersch, W., Cohen, A. M., Roberts, P., & Rakapalli, H. K. (2006). TREC 2006 genomics
track overview. Proceedings 15th Text Retrieval Conference.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings International Conference Research
Computational Linguistics (ROCLING X), pp. 1933.
Joachims, T. (1999). Transductive inference text classification using Support Vector
Machines. Proceedings ICML.
Katrenko, S., & Adriaans, P. (2008). Semantic types generic relation arguments:
Detection evaluation. Proceedings 46th Annual Meeting Association Computational Linguistics: Human Language Technologies (ACL/HLT),
Columbus, USA.
Khoo, C. S. G., Chan, S., & Niu, Y. (2000). Extracting causal knowledge medical database using graphical patterns. Proceedings 38th Annual Meeting
Association Computational Linguistics, pp. 336343, Morristown, NJ, USA.
Association Computational Linguistics.
Kim, S. N., & Baldwin, T. (2007). MELB-KB: Nominal classifications noun compound
interpretation. SemEval-2007.
Lauer, F., & Bloch, G. (2008). Incorporating Prior Knowledge Support Vector Machines
Classification: Review. Neurocomputing, 71, 15781594.
45

fiKatrenko, Adriaans, & van Someren

Leacock, C., & Chodorow, M. (1998). Combining local context WordNet similarity
word sense identification. MIT Press, Cambridge, MA.
Lease, M., & Charniak, E. (2005). Parsing biomedical literature. Proceedings IJCNLP.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th annual meeting Association Computational Linguistics Computational Linguistics,
pp. 2532.
Leslie, C., Eskin, E., Cohen, A., Weston, J., & Noble, W. S. (2004). Mismatch string kernels
discriminative protein classification. Bioinformatics, 20 (4), 467476.
Leslie, C., Eskin, E., & Noble, W. S. (2002). spectrum kernel: string kernel SVM
protein classification. Pacific Symposium Biocomputing 7, pp. 566575.
Leusch, G., Ueffing, N., & Ney, H. (2003). novel string-to-string distance measure
applications machine translation evaluation. Machine Translation Summit IX,
pp. 240247, New Orleans, LO.
Lin, D. (1998). information-theoretic definition similarity. Proceedings 15th
International Conference Machine Learning, pp. 296304.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Christianini, N., & Watkins, C. (2002). Text
classification using string kernels. Journal Machine Learning Research, 2, 419444.
McDonald, R. (2005). Extracting relations unstructured text. Tech. rep. MS-CIS-0506, UPenn.
Melcuk, I. (1988). Dpendency syntax: theory practice. SUNY Press.
Mitchell, T. (1997). Machine Learning. McGraw Hill.
Miyao, Y., Stre, R., Sagae, K., Matsuzaki, T., & Tsuji, J. (2008). Task-oriented evaluation
syntactic parsers representations. Proceedings ACL-08:HLT, pp. 46
54.
Mohammad, S. (2008). Measuring Semantic Distance using distributional profiles concepts. Ph.D. thesis, Graduate Department Computer Science University
Toronto.
Monge, A. E., & Elkan, C. (1996). field matching problem: Algorithms applications.
KDD 1996, pp. 267270.
Moschitti, A. (2006). Efficient convolution kernels dependency constituent syntactic
trees. ECML 2006, pp. 318329.
Nakov, P. (2007). UCB: System description SemEval task #4. SemEval-2007.
Nakov, P. (2008). Paraphrasing verbs noun compound interpretation. Proceedings
Workshop Multiword Expressions (MWE08), conjunction Language
Resources Evaluation conference, Marrakech, Morocco, 2008.
Nakov, P., & Hearst, M. A. (2008). Solving relational similarity problems using web
corpus. Proceedings ACL-08:HLT.
Nedellec, C. (2005). Learning Language Logic - Genic Interaction Extraction Challenge.
Proceedings Learning Language Logic workshop.
46

fiUsing Local Alignments Relation Recognition

Needleman, S. B., & Wunsch, C. D. (1970). general method applicable search
similarities amino acid sequence two proteins. Journal Molecular Biology,
48 (3), 443453.
Nulty, P. (2007). UCD-PN: Classification semantic relations nominals using
WordNet web counts. SemEval-2007.
Seaghdha, D. (2009). Semantic classification WordNet kernels. Proceedings
North American Chapter Association Computational Linguistics - Human
Language Technologies Conference (NAACL-HLT), Boulder, CO.
Seaghdha, D., & Copestake, A. (2008). Semantic classification distributional kernels.
Proceedings CoLing 2008, Manchester, UK.
Palmer, M., & Wu, Z. (1995). Verb semantics English-Chinese translation. Tech. rep.,
Technical Report No. MS-CIS-95-39, Department Computer & Information Science,
University Pennsylvania.
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity - Measuring
Relatedness Concepts. Proceedings Nineteenth National Conference
Artificial Intelligence (AAAI-04), pp. 10241025, San Jose, CA.
Pennacchiotti, M., & Pantel, P. (2006). Ontologizing semantic relations. ACL-44: Proceedings 21st International Conference Computational Linguistics
44th annual meeting Association Computational Linguistics, pp. 793800,
Morristown, NJ, USA. Association Computational Linguistics.
Ponzetto, S. P., & Strube, M. (2007). Knowledge Derived Wikipedia Computing
Semantic Relatedness. Journal Artificial Intelligence Research, 30, 181212.
Resnik, P. (1995). Using information content evaluate semantic similarity. Proceedings
14th International Joint Conference Artificial Intelligence, pp. 448453.
Stre, R., Sagae, K., & Tsuji, J. (2008). Syntactic features protein-protein interaction
extraction. 2nd International Symposium Languages Biology Medicine,
pp. 6.16.14.
Sagae, K., & Tsujii, J. (2007). Dependency parsing domain adaptation LR models
parser ensembles. Proceedings EMNLP-CoNLL.
Saigo, H., Vert, J.-P., & Akutsu, T. (2006). Optimizing amino acid substitution matrices
local alignment kernel. BMC Bioinformatics, 7:246.
Saigo, H., Vert, J.-P., Ueda, N., & Akutsu, T. (2004). Protein homology detection using
string alignment kernels. Bioinformatics, 20 (11), 16821689.
Sang, E. F. T. K., Canisius, S., van den Bosch, A., & Bogers, T. (2005). Applying spelling
error correction techniques improving semantic role labeling. Proceedings
Ninth Conference Natural Language Learning, CoNLL-2005, Ann Arbor, MI.
Saunders, C., Tschach, H., & Shawe-Taylor, J. (2002). Syllables string kernel
extensions. ICML 2002.
Scholkopf, B. (1997). Support vector learning. Ph.D. thesis, Berlin Technical University.

47

fiKatrenko, Adriaans, & van Someren

Sekimizu, T., Park, H. S., & Tsujii, J. (1998). Identifying interaction genes
gene products based frequently seen verbs Medline abstracts. Genome
Informatics, 9, 6271.
Shawe-Taylor, J., & Christianini, N. (2000). Support Vector Machines KernelBased Learning Methods. Cambridge University Press.
Smith, L. H., Yeganova, L., & Wilbur, W. J. (2003). Hidden Markov models optimized
sequence alignment. Computational Biology Chemistry, 27 (1), 77 84.
Smith, T. F., & Waterman, M. S. (1981). Identification common molecular subsequences.
Journal Molecular Biology, 147, 195197.
Snow, R., Jurafsky, D., & Ng, A. Y. (2006). Learning named entity hyponyms question
answering. Proceedings COLING/ACL.
Swanson, D. R., & Smalheiser, N. R. (1999). Implicit text linkages Medline records:
Using Arrowsmith aid scientific discovery. Library Trends, 48 (1).
Thomas, J., Milward, D., Ouzounis, C., & Pulman, S. (2000). Automatic extraction
protein interactions scientific abstracts. Proceedings Pacific Symposium
Biocomputing.
Tribble, A., & Fahlman, S. E. (2007). CMU-AT: Semantic distance background knowledge identifying semantic relations. SemEval-2007.
Turney, P. D. (2006). Similarity semantic relations. Computational Linguistics, 32 (3),
379416.
van der Plas, L. (2008). Automatic Lexico-Semantic Acquisition Question Answering.
Ph.D. thesis, University Groningen.
van Rijsbergen, C. J., Robertson, S. E., & Porter, M. F. (1980). New models probabilistic
information retrieval. Tech. rep. 5587, British Library Research Development
Report.
Vapnik, V. (1982). Estimation Dependences Based Empirical Data. New York:
SPringer Verlag.
Weeds, J., Weir, D., & McCarthy, D. (2004). Characterising measures lexical distributional similarity. Proceedings CoLing 2004.
Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods relation extraction.
Journal Machine Learning Research, 3, 10831106.
Zhang, Y., Schneider, J., & Dubrawski, A. (2008). Learning semantic correlation:
alternative way gain unlabeled text. Proceedings 22nd Conference
Neural Information Processing Systems, Vancouver, BC.

48


