Journal Artificial Intelligence Research 38 (2010) 475-511

Submitted 04/10; published 08/10

Minimum Relative Entropy Principle
Learning Acting
Pedro A. Ortega
Daniel A. Braun

peortega@dcc.uchile.cl
dab54@cam.ac.uk

Department Engineering
University Cambridge
Cambridge CB2 1PZ, UK

Abstract
paper proposes method construct adaptive agent universal
respect given class experts, expert designed specifically particular
environment. adaptive control problem formalized problem minimizing
relative entropy adaptive agent expert suitable
unknown environment. agent passive observer, optimal solution
well-known Bayesian predictor. However, agent active, past actions need
treated causal interventions I/O stream rather normal probability
conditions. shown solution new variational problem given
stochastic controller called Bayesian control rule, implements adaptive
behavior mixture experts. Furthermore, shown mild assumptions,
Bayesian control rule converges control law suitable expert.

1. Introduction
behavior environment control signal fully known,
designer choose agent produces desired dynamics. Instances problem include hitting target cannon known weather conditions, solving maze
map controlling robotic arm manufacturing plant. However,
environment unknown, designer faces problem adaptive control.
example, shooting cannon lacking appropriate measurement equipment, finding
way unknown maze designing autonomous robot Martian exploration.
Adaptive control turns far difficult non-adaptive counterpart.
good policy carefully trade explorative versus exploitative actions,
i.e. actions identification environments dynamics versus actions control
desired way. Even environments dynamics known belong particular class optimal agents available, constructing corresponding optimal
adaptive agent general computationally intractable even simple toy problems (Duff,
2002). Thus, finding tractable approximations major focus research.
Recently, proposed reformulate problem statement classes
control problems based minimization relative entropy criterion. example,
large class optimal control problems solved efficiently problem statement
reformulated minimization deviation dynamics controlled system
uncontrolled system (Todorov, 2006, 2009; Kappen, Gomez, & Opper, 2010).
work, similar approach introduced adaptive control. class agents
c
2010
AI Access Foundation. rights reserved.

fiOrtega & Braun

given, agent tailored different environment, adaptive controllers
derived minimum relative entropy principle. particular, one construct
adaptive agent universal respect class minimizing average relative
entropy environment-specific agent.
However, extension straightforward. syntactical difference
actions observations taken account formulating variational
problem. specifically, actions treated interventions obeying rules
causality (Pearl, 2000; Spirtes, Glymour, & Scheines, 2000; Dawid, 2010). distinction
made, variational problem unique solution given stochastic control rule
called Bayesian control rule. control rule particularly interesting
translates adaptive control problem on-line inference problem applied
forward time. Furthermore, work shows mild assumptions, adaptive
agent converges environment-specific agent.
paper organized follows. Section 2 introduces notation sets adaptive
control problem. Section 3 formulates adaptive control minimum relative entropy
problem. initial, nave approach, need causal considerations motivated.
Then, Bayesian control rule derived revised relative entropy criterion.
Section 4, conditions convergence examined proof given. Section 5
illustrates usage Bayesian control rule multi-armed bandit problem
undiscounted Markov decision processes. Section 6 discusses properties Bayesian
control rule relates previous work literature. Section 7 concludes.

2. Preliminaries
following agent environment formalized causal models I/O
sequences. Agent environment coupled exchange symbols following standard
interaction protocol discrete time, observation control signals. treatment
dynamics fully probabilistic, particular, actions observations
random variables, contrast typical decision-theoretic agent formulation
treating observations random variables (Russell & Norvig, 2010). proofs
provided appendix.
Notation. set denoted calligraphic letter A. words set & alphabet
element & symbol used mean thing respectively. Strings finite
concatenations symbols sequences infinite
concatenations. denotes set


strings length n based A, := n0 set finite strings. Furthermore, := {a1 a2 . . . |ai = 1, 2, . . .} defined set one-way
infinite sequences based alphabet A. Tuples written parentheses (a1 , a2 , a3 )
strings a1 a2 a3 . notation ai := a1 a2 . . . ai shorthand string starting first index. Also, symbols underlined glue together ao
aoi := a1 o1 a2 o2 . . . ai oi . function log(x) meant taken w.r.t. base 2, unless
indicated otherwise.
Interactions. possible I/O symbols drawn two finite sets. Let denote
set inputs (observations) let denote set outputs (actions). set Z := AO
interaction set. string aot ao<t interaction string (optionally ending
476

fiA Minimum Relative Entropy Principle Learning Acting

ot ) ak ok O. Similarly, one-sided infinite sequence a1 o1 a2 o2 . . .
interaction sequence. set interaction strings length denoted Z .
sets (finite) interaction strings sequences denoted Z Z respectively.
interaction string length 0 denoted .
I/O System. Agents environments formalized I/O systems. I/O system
probability distribution Pr interaction sequences Z . Pr uniquely determined
conditional probabilities
Pr(at |ao<t ),

Pr(ot |ao<t )

(1)

aot Z . conditional probabilities either represent generative law
(propensity) case issuing symbol evidential probability (plausibility)
case observing symbol. two interpretations applies particular case
becomes apparent I/O system coupled another I/O system.

Agent
P

a1 o1 a2 o2 a3 o3 a4 o4 a5 o5

Environment
Q

Figure 1: model interactions. agent P environment Q define probability distribution interaction sequences.

Interaction System. Let P, Q two I/O systems. interaction system (P, Q)
coupling two systems giving rise generative distribution G describes
probabilities actually govern I/O stream two systems coupled. G
specified equations
G(at |ao<t ) := P(at |ao<t )
G(ot |ao<t ) := Q(ot |ao<t )
valid aot Z . Here, G models true probability distribution interaction
sequences arises coupling two systems I/O streams. specifically,
system P, P(at |ao<t ) probability producing action given history
ao<t P(ot |ao<t ) predicted probability observation ot given history
477

fiOrtega & Braun

ao<t . Hence, P, sequence o1 o2 . . . input stream sequence a1 a2 . . .
output stream. contrast, roles actions observations reversed
case system Q. Thus, sequence o1 o2 . . . output stream sequence
a1 a2 . . . input stream. previous model interaction fairly general, many
interaction protocols translated scheme. convention, given
interaction system (P, Q), P agent constructed designer, Q
environment controlled agent. Figure 1 illustrates setup.
Control Problem. environment Q said known iff agent P property
aot Z ,
P(ot |ao<t ) = Q(ot |ao<t ).
Intuitively, means agent knows statistics environments future
behavior past, particular, knows effects given controls.
environment known, designer agent build custom-made policy
P resulting generative distribution G produces interaction sequences
desirable. done multiple ways. instance, controls chosen
resulting policy maximizes given utility criterion; resulting
trajectory interaction system stays close enough prescribed trajectory. Formally,
Q known, conditional probabilities P(at |ao<t ) aot Z
chosen resulting generative distribution G interaction sequences given

G(at |ao<t ) = P(at |ao<t )

G(ot |ao<t ) = Q(ot |ao<t ) = P(ot |ao<t )
desirable, P said tailored Q.
Adaptive Control Problem. environment Q unknown, task designing appropriate agent P constitutes adaptive control problem. Specifically,
work deals case designer already class agents tailored
class possible environments. Formally, assumed Q going drawn
probability P (m) set Q := {Qm }mM possible systems interaction starts, countable set. Furthermore, one set P := {Pm }mM
systems M, Pm tailored Qm interaction system
(Pm , Qm ) generative distribution Gm produces desirable interaction sequences.
designer construct system P behavior close possible
custom-made system Pm realization Qm Q?

3. Adaptive Systems
main goal paper show problem adaptive control outlined
previous section reformulated universal compression problem.
informally motivated follows. Suppose agent P implemented machine
interfaced environment Q. Whenever agent interacts environment,
agents state changes necessary consequence interaction. change
state take place many possible ways: updating internal memory; consulting
478

fiA Minimum Relative Entropy Principle Learning Acting

random number generator; changing physical location orientation; forth.
Naturally, design agent facilitates interactions complicates others.
instance, agent designed explore natural environment, might
incur low memory footprint recording natural images,
memory-inefficient recording artificially created images. one abstracts away
inner workings machine decides encode state transitions binary
strings, minimal amount resources bits required implement
state changes derived directly associated probability distribution P.
context adaptive control, agent constructed minimizes
expected amount changes necessary implement state transitions, equivalently,
maximally compresses experience. Thereby, compression taken
stand-alone principle design adaptive agents.
3.1 Universal Compression Nave Construction Adaptive Agents
coding theory, problem compressing sequence observations unknown
source known adaptive coding problem. solved constructing universal compressors, i.e. codes adapt on-the-fly source within predefined class
(MacKay, 2003). codes obtained minimizing average deviation predictor true source, constructing codewords using predictor.
subsection, procedure used derive adaptive agent (Ortega & Braun, 2010).
Formally, deviation predictor P true distribution Pm measured
relative entropy 1 . first approach would construct agent B
minimize total expected relative entropy Pm . constructed follows. Define
history-dependent relative entropies action observation ot
X
Pm (at |ao<t )

(ao<t ) :=
Dm
Pm (at |ao<t ) log
Pr(at |ao<t )



ot
(ao<t ) :=
Dm

X
ot

Pm (ot |ao<t ) log

Pm (ot |ao<t )
,
Pr(ot |ao<t )

Pm (ot |ao<t ) = Qm (ot |ao<t ) Qm known Pr
argument variational problem. Then, one removes dependency past
averaging possible histories:
X


:=
(ao<t )
Pm (ao<t )Dm
Dm
ao<t

ot
Dm

:=

X

ot
(ao<t ).
Pm (ao<t )Dm

ao<t

Finally, total expected relative entropy Pr Pm obtained summing
time steps averaging choices true environment:
:= lim sup


X

P (m)




X
=1




.
+ Dm
Dm

(2)

1. relative entropy known KL-divergence measures average amount extra
bits necessary encode symbols due usage (wrong) predictor.

479

fiOrtega & Braun

Using (2), one define variational problem respect Pr. agent B one
looking system Pr minimizes total expected relative entropy (2), i.e.
B := arg min D(Pr).
Pr

solution Equation 3 system B defined set equations
X
B(at |ao<t ) =
Pm (at |ao<t )wm (ao<t )


B(ot |ao<t ) =

X


Pm (ot |ao<t )wm (ao<t )

(3)

(4)

valid aot Z , mixture weights
P (m)Pm (ao<t )

P (m )Pm (ao<t )
P (m)Pm (ao<t )
.
wm (ao<t ) := P

P (m )Pm (ao<t )
wm (ao<t ) := P

(5)

reference, see work Haussler Opper (1997) Opper (1998). clear
B Bayesian mixture agents Pm . one defines conditional
probabilities
P (at |m, ao<t ) := Pm (at |ao<t )
(6)
P (ot |m, ao<t ) := Pm (at |ao<t )

aot Z , Equation 4 rewritten
B(at |ao<t ) =
B(ot |ao<t ) =

X


X


P (at |m, ao<t )P (m|ao<t ) = P (at |ao<t )
P (ot |m, ao<t )P (m|ao<t ) = P (ot |ao<t )

(7)

P (m|ao<t ) = wm (ao<t ) P (m|ao<t ) = wm (ao<t ) posterior
probabilities elements given past interactions. Hence, conditional
probabilities (4) minimize total expected divergence predictive
distributions P (at |ao<t ) P (ot |ao<t ) one obtains standard probability theory,
particular, Bayes rule. interesting, provides teleological interpretation
Bayes rule.
behavior B described follows. given time t, B maintains
mixture systems Pm . weighting given mixture coefficients
wm . Whenever new action new observation ot produced (by agent
environment respectively), weights wm updated according Bayes rule.
addition, B issues action suggested system Pm drawn randomly according
weights wt .
However, important problem B arises due fact
system passively observing symbols, actively generating them.
subjective interpretation probability theory, conditionals play role observations
480

fiA Minimum Relative Entropy Principle Learning Acting

made agent generated external source. interpretation suits
symbols o1 , o2 , o3 , . . . issued environment. However, symbols generated system require fundamentally different belief update.
Intuitively, difference explained follows. Observations provide information
allows agent inferring properties environment. contrast, actions
carry information environment, thus incorporated differently
belief agent. following section illustrate problem simple
statistical example.
3.2 Causality
Causality study functional dependencies events. stands contrast
statistics, which, abstract level, said study equivalence dependencies
(i.e. co-occurrence correlation) amongst events. Causal statements differ fundamentally
statistical statements. Examples highlight differences many,
smokers get lung cancer? opposed smokers lung cancer?; assign
f (x) opposed compare = f (x) programming languages; F/m
opposed F = Newtonian physics. study causality recently enjoyed
considerable attention researchers fields statistics machine learning.
Especially last decade, significant progress made towards formal
understanding causation (Shafer, 1996; Pearl, 2000; Spirtes et al., 2000; Dawid, 2010).
subsection, aim provide essential tools required understand causal
interventions. in-depth exposition causality, reader referred
specialized literature.
illustrate need causal considerations case generated symbols, consider
following thought experiment. Suppose statistician asked design model
simple time series X1 , X2 , X3 , . . . decides use Bayesian method. Assume
collects first observation X1 = x1 . computes posterior probability density function
(pdf) parameters model given data using Bayes rule:
p(|X1 = x1 ) = R

p(X1 = x1 |)p()
,
p(X1 = x1 | )p( )

p(X1 = x1 |) likelihood x1 given p() prior pdf .
use model predict next observation drawing sample x2 predictive
pdf
Z
p(X2 = x2 |X1 = x1 ) = p(X2 = x2 |X1 = x1 , ) p(|X1 = x1 ) d,
p(X2 = x2 |X1 = x1 , ) likelihood x2 given x1 . Note x2
drawn p(X2 = x2 |X1 = x1 , ). understands nature x2 different
x1 : x1 informative change belief state Bayesian model,
x2 non-informative thus reflection models belief state. Hence, would
never use x2 condition Bayesian model. Mathematically, seems imply

p(|X1 = x1 , X2 = x2 ) = p(|X1 = x1 )
481

fiOrtega & Braun

x2 generated p(X2 |X1 = x1 ) itself. simple independence assumption correct following elaboration example show.
statistician told source waiting simulated data point x2
order produce next observation X3 = x3 depend x2 . hands x2
obtains new observation x3 . Using Bayes rule, posterior pdf parameters

p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X1 = x1 |) p()
R
(8)
p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X1 = x1 | ) p( )
p(X3 = x3 |X1 = x1 , X2 = x2 , ) likelihood new data x3 given old
data x1 , parameters simulated data x2 . Notice looks almost
posterior pdf p(|X1 = x1 , X2 = x2 , X3 = x3 ) given
R

p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X2 = x2 |X1 = x1 , ) p(X1 = x1 |) p()
p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X2 = x2 |X1 = x1 , ) p(X1 = x1 | ) p( )

exception latter case, Bayesian update contains likelihoods
simulated data p(X2 = x2 |X1 = x1 , ). suggests Equation 8 variant
posterior pdf p(|X1 = x1 , X2 = x2 , X3 = x3 ) simulated data x2 treated
different way data x1 x3 .
Define pdf p pdfs p (), p (X1 |), p (X3 |X1 , X2 , ) identical
p(), p(X1 |) p(X3 |X2 , X1 , ) respectively, differ p (X2 |X1 , ):
p (X2 |X1 , ) = (X2 x2 ).
Dirac delta function. is, p identical p assumes
value X2 fixed x2 given X1 . p , simulated data x2 non-informative:
log2 p (X2 = x2 |X1 , ) = 0.
one computes posterior pdf p (|X1 = x1 , X2 = x2 , X3 = x3 ), one obtains result
Equation 8:
R

p (X3 = x3 |X1 = x1 , X2 = x2 , ) p (X2 = x2 |X1 = x1 , ) p (X1 = x1 |) p ()
p (X3 = x3 |X1 = x1 , X2 = x2 , )p (X2 = x2 |X1 = x1 , ) p (X1 = x1 | ) p ( )
p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X1 = x1 |) p()
=R
.
p(X3 = x3 |X1 = x1 , X2 = x2 , ) p(X1 = x1 | ) p( )

Thus, order explain Equation 8 posterior pdf given observed data x1 x3
generated data x2 , one intervene p order account fact x2
non-informative given x1 . words, statistician, defining value
X2 herself2 , changed (natural) regime brings series X1 , X2 , X3 , . . .,
mathematically expressed redefining pdf.
Two essential ingredients needed carry interventions. First, one needs
know functional dependencies amongst random variables probabilistic model.
provided causal model, i.e. unique factorization joint probability
2. Note conceptually broken two steps: first, samples x2 p(X2 |X1 = x1 );
second, imposes value X2 = x2 setting p (X2 |X1 , ) = (X2 x2 ).

482

fiA Minimum Relative Entropy Principle Learning Acting

distribution random variables encoding causal dependencies. general
case, defines partial order random variables. previous thought experiment, causal model joint pdf p(, X1 , X2 , X3 ) given set conditional
pdfs
p(), p(X1 |), p(X2 |X1 , ), p(X3 |X1 , X2 , ).
Second, one defines intervention sets X value x, denoted X x,
operation causal model replacing conditional probability X Dirac
delta function (X x) Kronecker delta xX continuous discrete variable X
respectively. thought experiment, easily seen
p (, X1 = x1 , X2 = x2 , X3 = x3 ) = p(, X1 = x1 , X2 x2 , X3 = x3 )
thereby,
p (|X1 = x1 , X2 = x2 , X3 = x3 ) = p(|X1 = x1 , X2 x2 , X3 = x3 ).
Causal models contain additional information available joint probability
distribution alone. appropriate model given situation depends story
told. Note intervention lead different results respective causal
models differ. Thus, causal model
p(X3 ), p(X2 |X3 ), p(X1 |X2 , X3 ), p(|X1 , X2 , X3 )
intervention X2 x2 would differ p , i.e.
p (, X1 = x1 , X2 = x2 , X3 = x3 ) 6= p(, X1 = x1 , X2 x2 , X3 = x3 ),
even though causal models represent joint probability distribution.
following, paper use shorthand notation x := X x random variable
obvious context.
3.3 Causal Construction Adaptive Agents
Following discussion previous section, adaptive agent P going constructed minimizing expected relative entropy expected Pm , time
treating actions interventions. Based definition conditional probabilities
Equation 6, total expected relative entropy characterize P using interventions going defined. Assuming environment chosen first, symbol depends
functionally environment previously generated symbols, causal model
given
P (m), P (a1 |m), P (o1 |m, a1 ), P (a2 |m, a1 , o1 ), P (o2 |m, a1 , o1 , a2 ), . . .
Importantly, interventions index set intervened probability distributions derived
base probability distribution. Hence, set fixed intervention sequences form
a1 , a2 , . . . indexes probability distributions observation sequences o1 , o2 , . . ..
this, one defines set criteria indexed intervention sequences,
483

fiOrtega & Braun

clear solution. Define history-dependent intervened relative
entropies action observation ot

(ao<t ) :=
Cm

X


ot
(ao<t ) :=
Cm

X
ot

P (at |m, ao<t ) log2

P (at |m, ao<t )
Pr(at |ao<t )

P (ot |m, ao<t ) log2

P (ot |m, ao<t )
,
Pr(ot |ao<t )

Pr given arbitrary agent. Note past actions treated interventions.
particular, P (at |m, ao<t ) represents knowledge state past actions already
issued next action known yet. Then, averaging previous relative
entropies pasts yields

=
Cm

X

ao<t
ot
=
Cm


(ao<t )
P (ao<t |m)Cm

X

ao<t

ot
(ao<t ).
P (ao<t |m)Cm

(ao ) C ot (ao ),
again, knowledge state time represented Cm
<t
<t

averages taken treating past actions interventions. Finally, define total exat + C ot ) time, averaged
pected relative entropy Pr Pm sum (Cm

possible draws environment:

C := lim sup


X

P (m)




X
=1




+ Cm
Cm
.

(9)

variational problem consists choosing agent P system Pr minimizing
C = C(Pr), i.e.
P := arg min C(Pr).
(10)
Pr

following theorem shows variational problem unique solution,
central theme paper.
Theorem 1. solution Equation 10 system P defined set equations
X

P(at |ao<t ) = P (at |ao<t ) =



P(ot |ao<t ) = P (ot |ao<t ) =

P (at |m, ao<t )vm (ao<t )

X


P (ot |m, ao<t )vm (ao<t )

(11)

valid aot Z , mixture weights
Qt1

=1 P (o |m, ao< )
.
Qt1


=1 P (o |m , ao< )
P (m )

vm (ao<t ) = vm (ao<t ) := P

P (m)

484

(12)

fiA Minimum Relative Entropy Principle Learning Acting

Bayesian Control Rule: Given set operation modes {P (|m, )}mM
interaction sequences Z prior distribution P (m)
parameters M, probability action at+1 given
X
P (at+1 |m, aot )P (m|aot ),
(13)
P (at+1 |aot ) =


posterior probability operation modes given recursion
P (ot |m, ao<t )P (m|ao<t )
.


P (ot |m , ao<t )P (m |ao<t )

P (m|aot ) = P

Table 1: Summary Bayesian control rule.
theorem says optimal solution variational problem (10) precisely
predictive distribution actions observations treating actions interventions
observations conditionals, i.e. solution one would obtain applying
standard probability causal calculus. provides teleological interpretation
agent P akin nave agent B constructed Section 3.1. behavior P differs
important aspect B. given time t, P maintains mixture systems
Pm . weighting systems given mixture coefficients vm . contrast
B, P updates weights vm whenever new observation ot produced
environment. update follows Bayes rule treats past actions interventions
dropping evidence provide. addition, P issues action suggested
system drawn randomly according weights vm .
3.4 Summary
Adaptive control formalized problem designing agent unknown environment chosen class possible environments. environment-specific agents
known, Bayesian control rule allows constructing adaptive agent combining
agents. resulting adaptive agent universal respect environment
class. context, constituent agents called operation modes adaptive
agent. represented causal models interaction sequences, i.e. conditional
probabilities P (at |m, ao<t ) P (ot |m, ao<t ) aot Z ,
index parameter characterizing operation mode. probability distribution
input stream (output stream) called hypothesis (policy) operation mode.
Table 1 collects essential equations Bayesian control rule. particular,
rule stated using recursive belief update.

4. Convergence
aim section develop set sufficient conditions convergence
provide proof convergence. simplify exposition, analysis limited
485

fiOrtega & Braun

case controllers finite number input-output models.

4.1 Policy Diagrams
following use policy diagrams useful informal tool analyze effect
policies environments. Figure 2 illustrates example.

state space


ao



policy

Figure 2: policy diagram. One imagine environment collection states
connected transitions labeled I/O symbols. zoom highlights state
taking action collecting observation leads state .
Sets states transitions represented enclosed areas similar Venn
diagram. Choosing particular policy environment amounts partially
controlling transitions taken state space, thereby choosing probability
distribution state transitions (e.g. Markov chain given environmental
dynamics). probability mass concentrates certain areas state space,
choosing policy thought choosing subset environments
dynamics. following, policy represented subset state space
(enclosed directed curve) illustrated above.

Policy diagrams especially useful analyze effect policies different hypotheses environments dynamics. agent endowed set operation
modes seen hypotheses environments underlying dynamics,
given observation models P (ot |m, ao<t ), associated policies, given action models P (at |m, ao<t ), M. sake simplifying interpretation
policy diagrams, assume existence state space : (A O) mapping
I/O histories states. Note however assumptions made obtain
results section.
4.2 Divergence Processes
central question section investigate whether Bayesian control rule converges correct control law not. is, whether P (at |aot ) P (at |m , ao<t )
true operation mode, i.e. operation mode P (ot |m , ao<t ) =
Q(ot |ao<t ). obvious discussion rest section,
general true.
easily seen Equation 13, showing convergence amounts show
posterior distribution P (m|ao<t ) concentrates probability mass subset operation
486

fiA Minimum Relative Entropy Principle Learning Acting

modes essentially output stream ,
X
X
P (at |m, ao<t )P (m|ao<t )
P (at |m , ao<t )P (m|ao<t ) P (at |m , ao<t ).
mM

mM

Hence, understanding asymptotic behavior posterior probabilities
P (m|aot )
crucial here. particular, need understand conditions quantities
converge zero. posterior rewritten
Q
P (aot |m)P (m)
P (m) =1 P (o |m, ao< )
.
=P
P (m|aot ) = P
Qt




P (aot |m )P (m )
P (m )
=1 P (o |m , ao< )

summands one index dropped denominator, one
obtains bound
P (m|aot )


P (m) P (o |m, ao< )
,
P (m )
P (o |m , ao< )
=1

valid M. inequality, seen convenient
analyze behavior stochastic process


dt (m km) :=


X
=1

ln

P (o |m , ao< )
P (o |m, ao< )

divergence process reference . Indeed, dt (m km)
,

P (m)
P (m) P (o |m, ao< )

= lim
edt (m km) = 0,



P (m )
P (o |m , ao< ) P (m )

lim

=1

thus clearly P (m|aot ) 0. Figure 3 illustrates simultaneous realizations
divergence processes controller. Intuitively speaking, processes provide lower
bounds accumulators surprise value measured information units.
divergence process random walk whose value time depends whole
history time t1. makes divergence processes cumbersome characterize
fact statistical properties depend particular policy applied;
hence, given divergence process different growth rates depending policy
(Figure 4). Indeed, behavior divergence process might depend critically
distribution actions used. example, happen divergence process
stays stable one policy, diverges another. context Bayesian
control rule problem aggravated, time step, policy
applied determined stochastically. specifically, true operation mode,
dt (m km) random variable depends realization aot drawn




=1

P (a |m , ao )P (o |m , ao ),
487

fiOrtega & Braun

dt
1
2
3
4


0

Figure 3: Realization divergence processes 1 4 associated controller
operation modes m1 m4 . divergence processes 1 2 diverge, whereas 3
4 stay dotted bound. Hence, posterior probabilities m1
m2 vanish.

dt
1

2
3

2

1
0

3


Figure 4: application different policies lead different statistical properties
divergence process.

488

fiA Minimum Relative Entropy Principle Learning Acting

m1 , m2 , . . . , mt drawn P (m1 ), P (m2 |ao1 ), . . . , P (mt |ao<t ).
deal heterogeneous nature divergence processes, one introduce
temporal decomposition demultiplexes original process many sub-processes
belonging unique policies. Let Nt := {1, 2, . . . , t} set time steps time t.
Let Nt , let m, M. Define sub-divergence dt (m km) random variable
X P (o |m , ao )
<
gm (m; ) :=
ln
P (o |m, ao< )


drawn

Pm ({ao } |{ao } ) :=







P (a |m , ao< )
P (o |m , ao< ) ,


:= Nt \ {ao } given conditions kept constant.
definition, plays role policy used sample actions time
steps . Clearly, realization divergence process dt (m km) decomposed
sum sub-divergences, i.e.
X
gm (m; Tm ),
dt (m km) =
(14)


{Tm }mM forms partition Nt . Figure 5 shows example decomposition.
dt
1
2
3



0

Figure 5: Decomposition divergence process (1) sub-divergences (2 & 3).
averages sub-divergences play important role analysis. Define
average realizations gm (m; )
X
Pm ({ao } |{ao } )gm (m; ).
Gm (m; ) :=
(ao )

Notice Nt ,
X
P (o |m , ao< )
P (a |m , ao< )P (o |m , ao< ) ln
0,
Gm (m; { }) =
P (o |m, ao< )
ao


Gibbs inequality. particular,

Gm (m ; { }) = 0.

Clearly, holds well Nt :


Gm (m; ) 0,

Gm (m ; ) = 0.
489

(15)

fiOrtega & Braun

4.3 Boundedness
general, divergence process complex: virtually classes distributions
interest control go well beyond assumptions i.i.d. stationarity.
increased complexity jeopardize analytic tractability divergence process,
predictions asymptotic behavior made anymore. specifically,
growth rates divergence processes vary much realization realization, posterior distribution operation modes vary qualitatively
realizations. Hence, one needs impose stability requirement akin ergodicity limit
class possible divergence-processes class analytically tractable.
purpose following property introduced.
divergence process dt (m km) said bounded variation iff > 0,
C 0, M, Nt




figm (m; ) Gm (m; )fi C
probability 1 .

dt

1

2

3



0

Figure 6: divergence process bounded variation, realizations (curves 2 &
3) sub-divergence stay within band around mean (curve 1).
Figure 6 illustrates property. Boundedness key property going
used construct results section. first important result posterior
probability true input-output model bounded below.
Theorem 2. Let set operation modes controller
divergence process dt (m km) bounded variation. Then, > 0, > 0,
N,

P (m |aot )
|M|

probability 1 .
4.4 Core

one wants identify operation modes whose posterior probabilities vanish,
enough characterize modes whose hypothesis match
true hypothesis. Figure 7 illustrates problem. Here, three hypotheses along
associated policies shown. H1 H2 share prediction made region differ
490

fiA Minimum Relative Entropy Principle Learning Acting

region B. Hypothesis H3 differs everywhere others. Assume H1 true. long
apply policy P2 , hypothesis H3 make wrong predictions thus divergence
process diverge expected. However, evidence H2 accumulated.
one applies policy P1 long enough time controller eventually
enter region B hence accumulate counter-evidence H2 .
H1

H2

B


H3

B


P3

P1
P2

Figure 7: hypothesis H1 true agrees H2 region A, policy P2 cannot
disambiguate three hypotheses.

long enough mean? P1 executed short period,
controller risks visiting disambiguating region. unfortunately, neither right
policy right length period run known beforehand. Hence, agent
needs clever time-allocating strategy test policies finite time intervals.
motivates following definition.
core operation mode , denoted [m ], subset containing
operation modes behaving policy. formally, operation mode

/ [m ] (i.e. core) iff C 0, > 0, > 0 t0 N,
t0 ,
Gm (m; ) C
probability 1 , Gm (m; ) sub-divergence dt (m km), Pr{
} Nt .
words, agent apply policy time step probability
least , strategy expected sub-divergence Gm (m; ) dt (m km) grows
unboundedly, core . Note demanding strictly positive
probability execution time step guarantees agent run
possible finite time-intervals. following theorem shows, posterior probabilities
operation modes core vanish almost surely.
Theorem 3. Let set operation modes agent
divergence process dt (m km) bounded variation.
/ [m ], P (m|aot ) 0
almost surely.
4.5 Consistency
Even operation mode core , i.e. given essentially indistinguishable control, still happen different
policies. Figure 8 shows example this. hypotheses H1 H2 share region
491

fiOrtega & Braun

differ region B. addition, operation modes policies P1 P2 respectively confined region A. Note operation modes core other.
However, policies different. means unclear whether multiplexing
policies time ever disambiguate two hypotheses. undesirable, could
impede convergence right control law.
H2

H1
B

B
P2

P1





Figure 8: example inconsistent policies. operation modes core
other, different policies.

Thus, clear one needs impose restrictions mapping hypotheses policies. respect Figure 8, one make following observations:
1. operation modes policies select subsets region A. Therefore,
dynamics preferred dynamics B.
2. Knowing dynamics preferred dynamics B allows us
drop region B analysis choosing policy.
3. Since hypotheses agree region A, choose policy order
consistent selection criterion.
motivates following definition. operation mode said consistent
iff [m ] implies < 0, t0 , t0
ao<t ,




fiP (at |m, aot ) P (at |m , aot )fi < .

words, core , ms policy converge policy.
following theorem shows consistency sufficient condition convergence
right control law.
Theorem 4. Let set operation modes agent that:
divergence process dt (m km) bounded variation; m, M, consistent
. Then,
P (at |ao<t ) P (at |m , ao<t )
almost surely .
492

fiA Minimum Relative Entropy Principle Learning Acting

4.6 Summary
section, proof convergence Bayesian control rule true operation
mode provided finite set operation modes. convergence result
hold, two necessary conditions assumed: boundedness consistency. first one,
boundedness, imposes stability divergence processes partial influence
policies contained within set operation modes. condition regarded
ergodicity assumption. second one, consistency, requires hypothesis makes
predictions another hypothesis within relevant subset dynamics,
hypotheses share policy. relevance formalized core
operation mode. concepts proof strategies strengthen intuition potential
pitfalls arise context controller design. particular could show
asymptotic analysis recast study concurrent divergence processes
determine evolution posterior probabilities operation modes, thus abstracting
away details classes I/O distributions. extension results
infinite sets operation modes left future work. example, one could think
partitioning continuous space operation modes essentially different regions
representative operation modes subsume neighborhoods (Grunwald, 2007).

5. Examples
section illustrate usage Bayesian control rule two examples
common reinforcement learning literature: multi-armed bandits Markov
decision processes.
5.1 Bandit Problems
Consider multi-armed bandit problem (Robbins, 1952). problem stated follows.
Suppose N -armed bandit, i.e. slot-machine N levers. pulled, lever
provides reward drawn Bernoulli distribution bias hi specific lever.
is, reward r = 1 obtained probability hi reward r = 0 probability
1hi . objective game maximize time-averaged reward iterative
pulls. continuum range stationary strategies, one parameterized N
probabilities {si }N
i=1 indicating probabilities pulling lever. difficulty arising
bandit problem balance reward maximization based knowledge already
acquired attempting new actions improve knowledge. dilemma known
exploration versus exploitation tradeoff (Sutton & Barto, 1998).
ideal task Bayesian control rule, possible bandit
known optimal agent. Indeed, bandit represented N -dimensional bias vector
= [m1 , . . . , mN ] = [0; 1]N . Given bandit, optimal policy consists
pulling lever highest bias. is, operation mode given by:
hi = P (ot = 1|m, = i) = mi

si = P (at = i|m) =

493

(

1 = maxj {mj },
0 else.

fiOrtega & Braun

m2

0

a)

1

b)
m1 m2

m2

1

1

m2 m1 , 3

m3
0

m1

0
0

m1

1

1

Figure 9: space bandit configurations partitioned N regions according
optimal lever. Panel b show 2-armed 3-armed bandit cases
respectively.

apply Bayesian control rule, necessary fix prior distribution
bandit configurations. Assuming uniform distribution, Bayesian control rule
Z
(16)
P (at+1 = i|m)P (m|aot )
P (at+1 = i|aot ) =


update rule given
Q
r
N

mj j (1 mj )fj
P (m) =1 P (o |m, )
=
P (m|aot ) = R
Qt



B(rj + 1, fj + 1)
=1 P (o |m , ) dm
P (m )
j=1

(17)

rj fj counts number times reward obtained
pulling lever j number times reward obtained respectively. Observe
summation discrete operation modes replaced integral
continuous space configurations. last expression see posterior
distribution lever biases given product N Beta distributions. Thus,
sampling action amounts first sample operation mode obtaining bias
mi Beta distribution parameters ri + 1 + 1, choosing
action corresponding highest bias = arg maxi mi . pseudo-code seen
Algorithm 1.
Simulation: Bayesian control rule described compared two
agents: -greedy strategy decay (on-line) Gittins indices (off-line).
test bed consisted bandits N = 10 levers whose biases drawn uniformly
beginning run. Every agent play 1000 runs 1000 time steps each.
Then, performance curves individual runs averaged. -greedy strategy
selects random action small probability given otherwise plays
lever highest expected reward. parameters determined empirically
values = 0.1, = 0.99 several test runs. adjusted way
maximize average performance last trials simulations. Gittins
method, indices computed horizon 1300 using geometric discounting
= 0.999, i.e. close one approximate time-averaged reward. results
shown Figure 10.
494

fiA Minimum Relative Entropy Principle Learning Acting

Algorithm 1 BCR bandit.
= 1, . . . , N
Initialize ri zero.
end
= 1, 2, 3, . . .
Sample using (17).
{ Interaction }
Set arg maxi mi issue a.
Obtain environment.

Avg. Reward

{Update belief}
= 1
ra = ra + 1
else
fa = fa + 1
end
end

0.85
0.80

Bayesian control rule
-greedy
Gittins indices

0.75
0.70
0

200

400

600

800

1000

0

200

400

600

800

1000

% Best Lever

100
80
60
40
20
0

Figure 10: Comparison N -armed bandit problem Bayesian control rule (solid
line), -greedy agent (dashed line) using Gittins indices (dotted line).
1,000 runs averaged. top panel shows evolution average
reward. bottom panel shows evolution percentage times
best lever pulled.

495

fiOrtega & Braun

seen -greedy strategy quickly reaches acceptable level performance,
seems stall significantly suboptimal level, pulling optimal lever 60%
time. contrast, Gittins strategy Bayesian control rule show essentially asymptotic performance, differ initial transient phase
Gittins strategy significantly outperforms Bayesian control rule. least three
observations worth making here. First, Gittins indices pre-computed
off-line. time complexity scales quadratically horizon, computations
horizon 1300 steps took several hours machines. contrast, Bayesian
control rule could applied without pre-computation. Second, even though Gittins
method actively issues optimal information gathering actions Bayesian control
rule passively samples actions posterior distribution operation modes,
end methods rely convergence underlying Bayesian estimator.
implies methods information bottleneck, since Bayesian estimator requires amount information converge. Thus, active information gathering
actions affect utility transient phase, permanent state. efficient algorithms bandit problems found literature (Auer, CesaBianchi, &
Fischer, 2002).
5.2 Markov Decision Processes
Markov Decision Process (MDP ) defined tuple (X , A, T, r): X state space;
action space; Ta (x; x ) = Pr(x |a, x) probability action
taken state x X lead state x X ; r(x, a) R := R immediate
reward obtained state x X action A. interaction proceeds time steps
= 1, 2, . . . time t, action issued state xt1 X , leading reward
rt = r(xt1 , ) new state xt starts next time step + 1. stationary closedloop control policy : X assigns action state. MDPs always
exists optimal stationary deterministic policy thus one needs consider
policies. undiscounted MDPs average rewardPper time step fixed policy
initial state x defined (x) = limt E [ 1t =0 r ]. shown (Bertsekas,
1987) (x) = (x ) x, x X assumption Markov chain
policy ergodic. Here, assume MDPs ergodic stationary policies.
order keep intervention model particularly simple3 , follow Q-notation
Watkins (1989). optimal policy characterized terms optimal
average reward optimal relative Q-values Q(x, a) state-action pair (x, a)
solutions following system non-linear equations (Singh, 1994):

3. brute-force adaptive agent problem would roughly look follows. First, agent
starts prior distribution MDPs, e.g. product Dirichlet distributions transition
probabilities. Then, cycle, agent samples full transition matrix distribution
solves using dynamic programming. computed optimal policy, uses issue
next action, discards policy. Subsequently, updates distribution MDPs using
next observed state. However, main text follow different approach avoids solving
MDP every time step.

496

fiA Minimum Relative Entropy Principle Learning Acting

state x X action A,
Q(x, a) + = r(x, a) +

X

x X


h

Q(x
,

)
Pr(x |x, a) max





h

= r(x, a) + Ex max
x,

.
Q(x
,

)



(18)



optimal policy defined (x) := arg maxa Q(x, a) state x X .
setup allows straightforward solution Bayesian control rule,
learnable MDP (characterized Q-values average reward)
known solution . Accordingly, operation mode given = [Q, ] =
R|A||O|+1 . obtain likelihood model inference m, realize Equation 18
rewritten predicts instantaneous reward r(x, a) sum mean
instantaneous reward plus noise term given Q-values average reward
MDP labeled
r(x, a) = Q(x, a) + max
Q(x , ) + max
Q(x , ) E[max
Q(x , )|x, a]



|
{z
}
{z
}
|
noise

mean instantaneous reward (x,a,x )

Assuming reasonably approximated normal distribution N(0, 1/p)
precision p, write likelihood model immediate reward r using
Q-values average reward, i.e.
r

n p
p

2
P (r|m, x, a, x ) =
(19)
exp (r (x, a, x )) .
2
2

order determine intervention model operation mode, simply exploit
properties Q-values, gives
(
1 = arg maxa Q(x, )
P (a|m, x) =
(20)
0 else.
apply Bayesian control rule, posterior distribution P (m|at , xt ) needs
computed. Fortunately, due simplicity likelihood model, one easily devise
conjugate prior distribution apply standard inference methods (see Appendix A.5). Actions determined sampling operation modes posterior executing
action suggested corresponding intervention models. resulting algorithm
similar Bayesian Q-learning (Dearden, Friedman, & Russell, 1998; Dearden, Friedman, & Andre, 1999), differs way actions selected. pseudo-code listed
Algorithm 2.
Simulation: tested MDP-agent grid-world example. give intuition
achieved performance, results contrasted achieved R-learning.
used R-learning variant presented work Singh (1994, Algorithm 3)
together uncertainty exploration strategy (Mahadevan, 1996). corresponding
update equations

Q(x, a) (1 )Q(x, a) + r + max
Q(x , )



(21)

(1 ) + r + max
Q(x
,

)

Q(x,
a)
,



497

fiOrtega & Braun

Algorithm 2 BCR-MDP Gibbs sampler.
Initialize entries zero.
Set initial state x x0 .
= 1, 2, 3, . . .
{Gibbs sweep}
Sample using (30).
Q(y, b) visited states
Sample Q(y, b) using (31).
end
{ Interaction }
Set arg maxa Q(x, ) issue a.
Obtain = (r, x ) environment.
{Update hyperparameters}
)(x,a,x )+p r
(x, a, x ) (x,a,x
(x,a,x )+p
(x, a, x ) (x, a, x ) + p
Set x x .
end

goal

membranes

b) Bayesian control rule

c) R-learning, C=5

d) R-learning, C=30

initial 5,000 steps

a) 7x7 Maze

e) R-learning, C=200

f) Average Reward
0.4
0.3

C=30
C=5

0.2

low
probability

C=200
0.1

last 5,000 steps

high
probability

Bayesian control rule
0.0
0

125

250

375

500

x1000 time steps

Figure 11: Results 77 grid-world domain. Panel (a) illustrates setup. Columns
(b)-(e) illustrate behavioral statistics algorithms. upper lower
row calculated first last 5,000 time steps randomly
chosen runs. probability state color-encoded, arrows
represent frequent actions taken agents. Panel (f) presents
curves obtained averaging ten runs.

498

fiA Minimum Relative Entropy Principle Learning Acting

Average Reward
BCR
R-learning, C = 200
R-learning, C = 30
R-learning, C = 5

0.3582 0.0038
0.2314 0.0024
0.3056 0.0063
0.2049 0.0012

Table 2: Average reward attained different algorithms end run.
mean standard deviation calculated based 10 runs.

, > 0 learning rates. exploration strategy chooses fixed probability
C
pexp > 0 action maximizes Q(x, a) + F (x,a)
, C constant, F (x, a)
represents number times action tried state x. Thus, higher values
C enforce increased exploration.
study (Mahadevan, 1996), grid-world described especially useful
test bed analysis RL algorithms. purposes, particular interest
easy design experiments containing suboptimal limit-cycles. Figure 11, panel
(a), illustrates 7 7 grid-world. controller learn policy leads
initial location goal state. step, agent move adjacent
space (up, down, left right). agent reaches goal state next position
randomly set square grid (with uniform probability) start another trial.
one-way membranes allow agent move one direction
other. experiments, membranes form inverted cups
agent enter side leave bottom, playing role
local maximum. Transitions stochastic: agent moves correct square
9
probability p = 10
free adjacent spaces (uniform distribution)
1
probability 1 p = 10 . Rewards assigned follows. default reward r = 0.
agent traverses membrane obtains reward r = 1. Reaching goal state
assigns r = 2.5. parameters chosen simulation following.
MDP-agent, chosen hyperparameters 0 = 1 0 = 1 precision p = 1.
R-learning, chosen learning rates = 0.5 = 0.001, exploration
constant set C = 5, C = 30 C = 200. total 10 runs carried
algorithm. results presented Figure 11 Table 2. R-learning
learns optimal policy given sufficient exploration (panels & e, bottom row), whereas
Bayesian control rule learns policy successfully. Figure 11f, learning curve
R-learning C = 5 C = 30 initially steeper Bayesian controller. However,
latter attains higher average reward around time step 125,000 onwards. attribute
shallow initial transient phase distribution operation modes
flat, reflected initially random exploratory behavior.

6. Discussion
key idea work extend minimum relative entropy principle, i.e.
variational principle underlying Bayesian estimation, problem adaptive control.
499

fiOrtega & Braun

coding point view, work extends idea maximal compression
observation stream whole experience agent containing agents actions
observations. minimizes amount bits write saving/encoding
I/O stream, minimizes amount bits required produce/decode
action (MacKay, 2003, Ch. 6).
extension non-trivial, important caveat coding I/O sequences: unlike observations, actions carry information could used
inference adaptive coding actions issued decoder itself. problem
inference ones actions logically inconsistent leads paradoxes
(Nozick, 1969). seemingly innocuous issue turned intricate
investigated intensely recent past researchers focusing issue
causality (Pearl, 2000; Spirtes et al., 2000; Dawid, 2010). work contributes body
research providing evidence actions cannot treated using probability
calculus alone.
causal dependencies carefully taken account, minimizing relative
entropy leads rule adaptive control called Bayesian control rule.
rule allows combining class task-specific agents agent universal
respect class. resulting control law simple stochastic control rule
completely general parameter-free. analysis paper shows, control
rule converges true control law mild assumptions.
6.1 Critical Issues
Causality. Virtually every adaptive control method literature successfully treats
actions conditionals observation streams never worries causality.
Thus, bother interventions? decision-theoretic setup, decision
maker chooses policy maximizing
P expected utility U outcomes
, i.e. := arg max E[U |] = Pr(|)U (). Choosing formally
equivalent choosing Kronecker delta function probability distribution
policies. case, conditional probabilities Pr(|) Pr(|) coincide,
since
Pr(, ) = Pr()Pr(|) = Pr(|) = Pr(, ).
sense, choice policy causally precedes interactions.
discussed Section 3 however, uncertainty policy (i.e. Pr() 6=
), causal belief updates crucial. Essentially, problem arises
uncertainty policy resolved interactions. Hence, treating
actions interventions seamlessly extends status random variables.
prior probabilities/likelihood models/policies come from? predictor
Bayesian control rule essentially Bayesian predictor thereby entails (almost) modeling paradigm. designer define class hypotheses
environments, construct appropriate likelihood models, choose suitable
prior probability distribution capture models uncertainty. Similarly, sufficient domain knowledge, analogous procedure applied construct suitable
operation modes. However, many situations difficult even
500

fiA Minimum Relative Entropy Principle Learning Acting

intractable problem itself. example, one design class operation modes
pre-computing optimal policies given class environments. Formally, let
class hypotheses modeling environments let class policies. Given
utility criterion U , define set operation modes := {m } constructing operation mode := (, ), , := arg max E[U |, ].
However, computing optimal policy many cases intractable.
cases, remedied characterizing operation modes optimality
equations solved probabilistic inference example MDP
agent Section 5.2. Recently, applied similar approach adaptive control
problems linear quadratic regulators (Braun & Ortega, 2010).
Problems Bayesian methods. Bayesian control rule treats adaptive control
problem Bayesian inference problem. Hence, problems typically associated
Bayesian methods carry agents constructed Bayesian control
rule. problems analytical computational nature. example,
many probabilistic models posterior distribution
closed-form solution. Also, exact probabilistic inference general computationally
intensive. Even though large literature efficient/approximate inference algorithms particular problem classes (Bishop, 2006), many
suitable on-line probabilistic inference realistic environment classes.
Bayesian control rule versus Bayes-optimal control. Directly maximizing (subjective) expected utility given environment class minimizing
expected relative entropy given class operation modes. two methods
based different assumptions optimality principles. such, Bayesian
control rule Bayes-optimal controller. Indeed, easy design experiments
Bayesian control rule converges exponentially slower (or converge
all) Bayes-optimal controller maximum utility. Consider following
simple example: Environment 1 k-state MDP k consecutive actions
reach state reward +1. interception B-action leads back
initial state. Consider second environment first actions
B interchanged. Bayes-optimal controller figures true environment k
actions (either k consecutive Bs). Consider Bayesian control rule:
optimal action Environment 1 A, Environment 2 B. uniform ( 21 , 21 ) prior
operation modes stays uniform posterior long reward
observed. Hence Bayesian control rule chooses time-step B
equal probability. policy takes 2k actions accidentally choose
row (or Bs) length k. Bayesian control rule optimal
too. Bayes-optimal controller converges time k, Bayesian control
rule needs exponentially longer. One way remedy problem might allow
Bayesian control rule sample actions operation mode several
time steps row rather randomizing controllers every cycle. However,
one considers non-stationary environments strategy
break down. Consider, example, increasing MDP k = 10 , Bayes-optimal
controller converges 100 steps, Bayesian control rule converge
realizations, boundedness assumption violated.
501

fiOrtega & Braun

6.2 Relation Existing Approaches
ideas underlying work unique Bayesian control rule.
following selection previously published work recent Bayesian reinforcement
learning literature related ideas found.
Compression principles. literature, important amount work
relating compression intelligence (MacKay, 2003; Hutter, 2004b). particular,
even proposed compression ratio objective quantitative measure
intelligence (Mahoney, 1999). Compression used basis theory
curiosity, creativity beauty (Schmidhuber, 2009).
Mixture experts. Passive sequence prediction mixing experts studied
extensively literature (Cesa-Bianchi & Lugosi, 2006). study onlinepredictors (Hutter, 2004a), Bayes-optimal predictors mixed. Bayes-mixtures
used universal prediction (Hutter, 2003). control case, idea
using mixtures expert-controllers previously evoked models
MOSAIC-architecture (Haruno, Wolpert, & Kawato, 2001). Universal learning
Bayes mixtures experts reactive environments studied work
Poland Hutter (2005) Hutter (2002).
Stochastic action selection. idea using actions random variables,
problems entails, expressed work Hutter (2004b, Problem
5.1). study Section 3 regarded thorough investigation open
problem. stochastic action selection approaches found thesis Wyatt (1997) examines exploration strategies (PO)MDPs, learning automata
(Narendra & Thathachar, 1974) probability matching (Duda, Hart, & Stork,
2001) amongst others. particular, thesis discusses theoretical properties
extension probability matching context multi-armed bandit problems.
There, proposed choose lever according likely optimal
shown strategy converges, thus providing simple method guiding
exploration.
Relative entropy criterion. usage minimum relative entropy criterion
derive control laws underlies KL-control methods developed work Todorov
(2006, 2009) Kappen et al. (2010). There, shown large class
optimal control problems solved efficiently problem statement
reformulated minimization deviation dynamics controlled
system uncontrolled system. related idea conceptualize planning
inference problem (Toussaint, Harmeling, & Storkey, 2006). approach based
equivalence maximization expected future return likelihood
maximization applicable MDPs POMDPs. Algorithms based
duality become active field current research. See example work
Rasmussen Deisenroth (2008), fast model-based RL techniques
used control continuous state action spaces.
502

fiA Minimum Relative Entropy Principle Learning Acting

7. Conclusions
work introduces Bayesian control rule, Bayesian rule adaptive control.
key feature rule special treatment actions based causal calculus
decomposition adaptive agent mixture operation modes, i.e. environmentspecific agents. rule derived minimizing expected relative entropy
true operation mode carefully distinguishing actions observations. Furthermore, Bayesian control rule turns exactly predictive distribution
next action given past interactions one would obtain using probability
causal calculus. Furthermore, shown agents constructed Bayesian
control rule converge true operation mode mild assumptions: boundedness,
related ergodicity; consistency, demanding two indistinguishable hypotheses share policy.
presented Bayesian control rule way solve adaptive control problems
based minimum relative entropy principle. Thus, Bayesian control rule either
regarded new principled approach adaptive control novel optimality
criterion heuristic approximation traditional Bayes-optimal control. Since
takes similar form Bayes rule, adaptive control problem could translated
on-line inference problem actions sampled stochastically posterior
distribution. important note, however, problem statement formulated
usual Bayes-optimal approach adaptive control same.
future relationship two problem statements deserves investigation.

Acknowledgments
thank Marcus Hutter, David Wingate, Zoubin Ghahramani, Jose Aliste, Jose Donoso,
Humberto Maturana anonymous reviewers comments earlier versions
manuscript and/or inspiring discussions. thank Ministerio de Planificacion de Chile
(MIDEPLAN) Bohringer-Ingelheim-Fonds (BIF) funding.

Appendix A. Proofs
A.1 Proof Theorem 1
Proof. proof follows line argument solution Equation 3
crucial difference
treated interventions. Consider without loss
P actions
Equation 9. Note relative entropy
generality summand P (m)Cm
written difference two logarithms, one term depends Pr varied.
Therefore, one pull term write constant c. yields

c

X


P (m)

X

ao<t

P (ao<t |m)

X


503

P (at |m, ao<t ) ln Pr(at |ao<t ).

fiOrtega & Braun

Substituting P (ao<t |m) P (m|ao<t )P (ao<t )/P (m) using Bayes rule rearrangement terms leads
XX
X
=c
P (m|ao<t )P (ao<t )
P (at |m, ao<t ) ln Pr(at |ao<t )
ao<t

=c

X

P (ao<t )



X


ao<t

P (at |ao<t ) ln Pr(at |ao<t ).

P
inner sum form x p(x) ln q(x), i.e. cross-entropy q(x) p(x),
minimized q(x) = p(x) x. Let P denote optimum distribution
Pr. choosing optimum one obtains P(at |ao<t ) = P (at |ao<t ) . Note
solution variational problem P
independent P
weighting P (ao<t ). Since


argument applies summand P (m)Cm
P (m)Cm Equation 9,
variational problems mutually independent. Hence,
P(at |ao<t ) = P (at |ao<t )

P(ot |ao<t ) = P (ot |ao<t )

aot Z . P (at |ao<t ), introduce variable via marginalization
apply chain rule:
X
P (at |ao<t ) =
P (at+1 |m, ao<t )P (m|ao<t ).


term P (m|aot ) developed

P (ao<t |m)P (m)


P (ao<t |m )P (m )
Qt1
P (m) =1 P (a |m, ao< )P (o |m, ao< )
=P
Qt1



P (m )
=1 P (a |m , ao< )P (o |m , ao< )
Qt1
P (m) =1 P (o |m, ao< )
.
=P
Qt1


P (m )
=1 P (o |m , ao< )

P (m|ao<t ) = P

first equality obtained applying Bayes rule second using chain
rule probabilities. get last equality, one applies interventions causal
factorization. Thus, P (a |m, ao< ) = 1 P (o |m, ao< ) = P (o |m, ao< ).
equations characterizing P (ot |ao<t ) obtained similarly.
A.2 Proof Theorem 2
Proof. pointed (14), particular realization divergence process
dt (m km) decomposed
X
dt (m km) =
gm (m ; Tm ),


gm (m ; Tm ) sub-divergences dt (m km) Tm form partition Nt .
However, since dt (m km) bounded variation M, one > 0,
C(m) 0, M, Nt Nt , inequality




figm (m ; Tm ) Gm (m ; Tm )fi C(m)
504

fiA Minimum Relative Entropy Principle Learning Acting

holds probability 1 . However, due (15),
Gm (m ; Tm ) 0
M. Thus,

gm (m ; Tm ) C(m).

previous inequalities hold simultaneously divergence process
bounded well. is, inequality
dt (m km) C(m)

(22)

holds probability (1 )M := |M|. Choose
(m)
(m) := max{0, ln PP(m
) }.
(m)
Since 0 ln PP(m
Using
) (m), added right hand side (22).

definition dt (m km), taking exponential rearranging terms one obtains


P (m )






=1

(m)

P (o |m , ao< ) e

P (m)




=1

P (o |m, ao< )

(m) := C(m) + (m) 0. Identifying posterior probabilities
dividing sides normalizing constant yields inequality
P (m |aot ) e(m) P (m|aot ).
2

inequality holds simultaneously probability (1 )M
particular := minm {e(m) }, is,
P (m |aot ) P (m|aot ).
since valid M, maxm {P (m|aot )}
P (m |aot )

1
M,

one gets


,


probability
1 arbitrary > 0 related equation :=

M2
1
1 .
A.3 Proof Theorem 3
Proof. divergence process dt (m km) decomposed sum sub-divergences
(see Equation 14)
X
gm (m; Tm ).
(23)
dt (m km) =




Furthermore, every
M, one > 0, C 0,
N Nt




figm (m; ) Gm (m; )fi C(m)
505

fiOrtega & Braun

probability 1 . Applying bound summands (23) yields lower
bound
X
X

gm (m; Tm )
Gm (m; Tm ) C(m)




(1 )M ,

holds probability
:= |M|. Due Inequality 15, one




6= , Gm (m; Tm ) 0. Hence,
X

Gm (m; Tm ) C(m) Gm (m; Tm ) C


C := maxm {C(m)}. members set Tm determined stochastically;
specifically, ith member included Tm probability P (m |aoi ) /M
> 0 Theorem 2. since
/ [m ], one Gm (m; Tm )

probability 1 arbitrarily chosen > 0. implies
lim dt (m km) lim Gm (m; Tm ) C





probability 1 , > 0 arbitrary related = 1 (1 )M +1 .
Using result upper bound posterior probabilities yields final result
P (m) dt (m km)
e
= 0.
P (m )

0 lim P (m|aot ) lim


A.4 Proof Theorem 4
Proof. use abbreviations pm (t) := P (at |m, ao<t ) wm (t) := P (m|ao<t ).
Decompose P (at |ao<t )
X
X
pm (t)wm (t) +
pm (t)wm (t).
(24)
P (at |ao<t ) =
m[m
/ ]

m[m ]

first sum right-hand side lower-bounded zero upper-bounded
X
X
pm (t)wm (t)
wm (t)
m[m
/ ]

m[m
/ ]

pm (t) 1. Due Theorem 3, wm (t) 0 almost surely. Given > 0
> 0, let t0 (m) time t0 (m), wm (t) < . Choosing
t0 := maxm {t0 (m)}, previous inequality holds t0 simultaneously
probability (1 )M . Hence,
X
X
pm (t)wm (t)
wm (t) < .
(25)
m[m
/ ]

m[m
/ ]

bound second sum (24) one proceeds follows. every member [m ],
one pm (t) pm (t) . Hence, following similar construction above,
one choose t0 t0 [m ], inequalities




fipm (t) pm (t)fi <
506

fiA Minimum Relative Entropy Principle Learning Acting

hold simultaneously precision > 0. Applying second sum Equation 24
yields bounds
X
X
X


pm (t) + wm (t).
pm (t)wm (t)
pm (t) wm (t)
m[m ]

m[m ]

pm (t)





m[m ]

multiplicative constants placed front sum. Note
1

X

m[m ]

wm (t) = 1

X

m[m
/ ]

wm (t) > 1 .

Use inequalities allows simplifying lower upper bounds respectively:
X
pm (t)
wm (t) > pm (t)(1 ) pm (t) 2 ,
m[m ]

pm (t) +

X


m[m ]

(26)

wm (t) pm (t) + < pm (t) + 2 .

Combining inequalities (25) (26) (24) yields final result:





P
(a
|ao
(t)
)

p

< (2 + ) = ,


<t

holds probability 1 arbitrary > 0 related = 1
arbitrary precision .





1

A.5 Gibbs Sampling Implementation MDP Agent
Inserting likelihood given Equation 19 Equation 13 Bayesian control rule,
one obtains following expression posterior
P (m|at , ot ) =
=

P (x |m, x, a)P (r|m, x, a, x )P (m|a<t , o<t )






P (x |m , x, a)P (r|m , x, a, x )P (m |a<t , o<t ) dm
P (r|m, x, a, x )P (m|a<t , o<t )
R
,




P (r|m , x, a, x )P (m |a<t , o<t ) dm
R

(27)

replaced sum integration , finite-dimensional real space
containing average reward Q-values observed states,
simplified term P (x |m, x, a) constant M.
likelihood model P (r|m , x, a, x ) Equation 27 encodes set independent normal distributions immediate reward means (x, a, x ) indexed triples
(x, a, x ) X X . words, given (x, a, x ), rewards drawn
normal distribution unknown mean (x, a, x ) known variance 2 . sufficient statistics given n(x, a, x ), number times transition x x
action a, r(x, a, x ), mean rewards obtained transition.
conjugate prior distribution well known given normal distribution
hyperparameters 0 0 :
r
n
2
0


0
.
(28)
exp 2 (x, a, x ) 0
P (m (x, a, x )) = N(0 , 1/0 ) =
2
507

fiOrtega & Braun

posterior distribution given
P (m (x, a, x )|at , ot ) = N((x, a, x ), 1/(x, a, x ))
posterior hyperparameters computed
0 0 + p n(x, a, x ) r(x, a, x )
0 + p n(x, a, x )
(x, a, x ) = 0 + p n(x, a, x ).

(x, a, x ) =

(29)

introducing shorthand V (x) := maxa Q(x, a), write posterior distribution
P (|at , ot ) = N(, 1/S)
(30)

=

1 X
(x, a, x )((x, a, x ) Q(x, a) + V (x )),

x,a,x
X
S=
(x, a, x ).
x,a,x

posterior distribution Q-values difficult obtain,
Q(x, a) enters posterior distribution linearly non-linearly . However,
fix Q(x, a) within max operations, amounts treating V (x)
constant within single Gibbs step, conditional distribution approximated



P (Q(x, a)|at , ot ) N Q(x, a), 1/S(x, a)
(31)


Q(x, a) =

X
1
(x, a, x )((x, a, x ) + V (x )),
S(x, a)
x
X
(x, a, x ).
S(x, a) =
x

expect approximation hold resulting update rule constitutes contraction operation forms basis stochastic approximation algorithms (Mahadevan, 1996). result, Gibbs sampler draws values normal distributions. cycle adaptive controller, one carry several Gibbs sweeps
obtain sample improve mixing Markov chain. However, experimental
results shown single Gibbs sweep per state transition performs reasonably well.
new parameter vector drawn, Bayesian control rule proceeds taking
optimal action given Equation 20. Note entries transitions
occurred need represented explicitly; similarly, Q-values visited
states need represented explicitly.
508

fiA Minimum Relative Entropy Principle Learning Acting

References
Auer, P., CesaBianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmed
bandit problem. Machine Learning, 47, 235256.
Bertsekas, D. (1987). Dynamic Programming: Deterministic Stochastic Models.
Prentice-Hall, Upper Saddle River, NJ.
Bishop, C. M. (2006). Pattern Recognition Machine Learning. Springer.
Braun, D. A., & Ortega, P. A. (2010). minimum relative entropy principle adaptive
control linear quadratic regulators. 7th conference informatics control,
automation robotics, Vol. 3, pp. 103108.
Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, Learning Games. Cambridge University Press.
Dawid, A. P. (2010). Beware DAG!. Journal Machine Learning Research, (to
appear).
Dearden, R., Friedman, N., & Andre, D. (1999). Model based bayesian exploration.
Proceedings Fifteenth Conference Uncertainty Artificial Intelligence, pp.
150159.
Dearden, R., Friedman, N., & Russell, S. (1998). Bayesian q-learning. AAAI
98/IAAI 98: Proceedings fifteenth national/tenth conference Artificial intelligence/Innovative applications artificial intelligence, pp. 761768. American Association Artificial Intelligence.
Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern Classification (Second edition).
Wiley & Sons, Inc.
Duff, M. O. (2002). Optimal learning: computational procedures bayes-adaptive markov
decision processes. Ph.D. thesis. Director-Andrew Barto.
Grunwald, P. (2007). Minimum Description Length Principle. MIT Press.
Haruno, M., Wolpert, D., & Kawato, M. (2001). Mosaic model sensorimotor learning
control. Neural Computation, 13, 22012220.
Haussler, D., & Opper, M. (1997). Mutual information, metric entropy cumulative
relative entropy risk. Annals Statistics, 25, 24512492.
Hutter, M. (2002). Self-optimizing pareto-optimal policies general environments
based bayes-mixtures. COLT.
Hutter, M. (2003). Optimality universal Bayesian prediction general loss alphabet.
Journal Machine Learning Research, 4, 971997.
Hutter, M. (2004a). Online prediction bayes versus experts. Tech. rep.. Presented
EU PASCAL Workshop Learning Theoretic Bayesian Inductive Principles
(LTBIP-2004).
Hutter, M. (2004b). Universal Artificial Intelligence: Sequential Decisions based Algorithmic Probability. Springer, Berlin.
509

fiOrtega & Braun

Kappen, B., Gomez, V., & Opper, M. (2010). Optimal control graphical model inference
problem. JMLR (to appear).
MacKay, D. J. C. (2003). Information Theory, Inference, Learning Algorithms. Cambridge University Press.
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms,
empirical results. Machine Learning, 22 (1-3), 159195.
Mahoney, M. V. (1999). Text compression test artificial intelligence. AAAI/IAAI,
pp. 486502.
Narendra, K., & Thathachar, M. A. L. (1974). Learning automata - survey. IEEE
Transactions Systems, Man, Cybernetics, SMC-4 (4), 323334.
Nozick, R. (1969). Newcombs problem two principles choice. Rescher, N. (Ed.),
Essays Honor Carl G. Hempel, pp. 114146. Reidel.
Opper, M. (1998). bayesian approach online learning. Online Learning Neural
Networks, 363378.
Ortega, P. A., & Braun, D. A. (2010). bayesian rule adaptive control based causal
interventions. third conference artificial general intelligence, pp. 121126.
Pearl, J. (2000). Causality: Models, Reasoning, Inference. Cambridge University Press,
Cambridge, UK.
Poland, J., & Hutter, M. (2005). Defensive universal learning experts. ALT.
Rasmussen, C. E., & Deisenroth, M. P. (2008). Recent Advances Reinforcement Learning,
Vol. 5323 Lecture Notes Computer Science, LNAI, chap. Probabilistic Inference
Fast Learning Control, pp. 229242. Springer-Verlag.
Robbins, H. (1952). aspects sequential design experiments. Bulletin American
Mathematical Socierty, 58, 527535.
Russell, S., & Norvig, P. (2010). Artificial Intelligence: Modern Approach (3rd edition).
Prentice-Hall.
Schmidhuber, J. (2009). Simple algorithmic theory subjective beauty, novelty, surprise,
interestingness, attention, curiosity, creativity, art, science, music, jokes. Journal
SICE, 48 (1), 2132.
Shafer, G. (1996). art causal conjecture. MIT Press.
Singh, S. P. (1994). Reinforcement learning algorithms average-payoff markovian decision
processes. National Conference Artificial Intelligence, pp. 700705.
Spirtes, P., Glymour, C., & Scheines, R. (2000). Causation, Prediction Search (2nd
edition). Springer-Verlag, New York.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Todorov, E. (2006). Linearly solvable markov decision problems. Advances Neural
Information Processing Systems, Vol. 19, pp. 13691376.
510

fiA Minimum Relative Entropy Principle Learning Acting

Todorov, E. (2009). Efficient computation optimal actions. Proceedings National
Academy Sciences U.S.A., 106, 1147811483.
Toussaint, M., Harmeling, S., & Storkey, A. (2006). Probabilistic inference solving
(po)mdps. Tech. rep. EDI-INF-RR-0934, University Edinburgh.
Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, University Cambridge,
Cambridge, England.
Wyatt, J. (1997). Exploration Inference Learning Reinforcement. Ph.D. thesis,
Department Artificial Intelligence, University Edinburgh.

511


