journal artificial intelligence

submitted published

local alignments relation recognition
sophia katrenko
pieter adriaans
maarten van someren

katrenko uva nl
p w adriaans uva nl
w vansomeren uva nl

informatics institute university amsterdam
science park xg amsterdam netherlands

abstract
discusses marrying structural similarity semantic relatedness information extraction text aiming accurate recognition relations
introduce local alignment kernels explore possibilities
task give definition local alignment la kernel smith waterman
score sequence similarity measure proceed range possibilities computing similarity elements sequences distributional similarity
measures obtained unlabeled data incorporated learning task semantic knowledge experiments suggest la kernel yields promising
biomedical corpora outperforming two baselines large margin additional
series experiments conducted data sets seven general relation types
performance la kernel comparable current state art

introduction
despite fact much work done automatic relation extraction recognition past decades remains popular topic main reason
keen interest relation recognition lies utility concepts semantic relations
identified used variety applications question answering
qa ontology construction hypothesis generation others
ontology construction relation studied relation hypernymy organizes concepts taxonomy snow jurafsky ng information retrieval semantic relations used two ways refine queries actual
retrieval manipulate output returned search engine e g identifying
whether fragment text contains given relation widely used relations
query expansion hypernymy broader terms thesaurus synonymy
semantic relations useful different stages question answering
taken account identifying type question considered actual answer extraction time van der plas yet another application
relations constructing scientific hypothesis given evidence found text
type knowledge discovery text often co occurrence analysis many
cases corroborated via experiments laboratories swanson smalheiser
another reason extraction semantic relations interest lies diversity
relations different relations need different extraction methods many existing information
extraction systems originally designed work generic data grishman sundheim became evident domain knowledge often necessary successful

c

ai access foundation rights reserved

fikatrenko adriaans van someren

extraction instance relation extraction biomedical domain would require
accurate recognition named entities gene names clegg area
food needs information relevant named entities toxic substances
generic relations syntactic information often sufficient consider
instance following sentences arguments relations written italics


mary looked back whispered know every tree forest every scent
part whole relation



person infected particular flu virus strain develops antibodies
virus cause effect relation



apples basket content container relation

sentences exemplify binary relations namely part whole tree part
forest cause effect virus causes flu content container apples contained
basket easily notice syntactic context namely
arguments cases connected preposition however
context highly ambiguous even though allows us reduce number
potential semantic relations still sufficient able discriminate
part whole content container relation words world knowledge
trees forests apples baskets necessary classify relations correctly
situation changes even drastically consider example explicit
indication causation nevertheless knowing flu virus
able infer cause effect relation holds
examples highlight several difficulties characterize semantic
relation extraction generic relations often occur nominal complexes flu
virus lack sentential context boosts approaches paraphrasing nakov
however even noun compounds one combine world knowledge
compounds context arrive correct interpretation
computational approaches relation recognition often rely two step
procedure first relation arguments identified depending relation hand
step often involves named entity recognition arguments relations
second step check whether relation holds relation arguments provided e g
basket apples relation extraction reduced second step previous
work relation extraction suggests case accuracy relation recognition
much higher case discovered automatically bunescu
et al furthermore existing solutions relation extraction including work
presented focus relation examples occur within single sentence
consider discourse mcdonald recognizing relations wider scope
interesting enterprise would require take account anaphora resolution
types linguistic analysis
approaches relation extraction hand written patterns timeconsuming many cases need expert formulate test patterns although
patterns often precise usually produce poor recall thomas et al
general hand written patterns two types first type sequential


fiusing local alignments relation recognition

frequently occurring sequences words sentence hand written sequential patterns
initially used extraction hypernymy hearst several attempts
extend relations second type patterns khoo chan niu take
syntactic structure sentence account dependency structure sentence
usually represented tree patterns become subtrees patterns
sometimes referred graphical patterns identify examples cause effect
relation khoo et al applied type patterns texts medical domain
study showed graphical patterns sensitive errors made parsers
cover examples test data extract many spurious instances
alternative hand written patterns supervised machine learning
relations labeled used train classifier recognize relations
texts one learn generalized extraction patterns patterns expressed
characters words syntactic categories words approaches involve clustering
co occurrence davidov rappoport recent years kernel methods
become popular handle high dimensional zelenko et al
bunescu mooney airola et al methods transform text fragments complete sentences segments around named entitites verbs vectors
apply support vector machines classify fragments
machine learning methods use prior knowledge given system
addition labeled examples scholkopf p use prior knowledge often
motivated example poor quality data data sparseness prior knowledge
used many ways changing representation existing training examples adding
examples unlabelled data nlp tasks prior knowledge exists form
manually automatically constructed ontologies large collections unannotated data
enrich textual data thereby improve recognition relations sekimizu
park tsujii tribble fahlman recently zhang et al showed
semantic correlation words learned unlabelled text collections transferred
among documents used improve document classification general
use large collections text allows us derive almost information needed done
varying accuracy contrast existing resources created humans provide
precise information less likely cover possible areas interest
work bunescu mooney use syntactic
structure sentences particular dependency paths stems observation
linguistic units organized complex structures understanding words
word senses relate often requires contextual information relation extraction
viewed supervised classification training set consists examples
given relation goal construct model applied unseen
data set recognize instances given relation data set recognition
relations use kernel classifier applied dependency paths however
instead vector kernel directly use similarity dependency paths
information existing ontologies large text corpora employed
organized follows start reviewing existing kernel methods
work sequences section section give definition local alignment kernel
smith waterman measure proceed discussing used
context natural language processing nlp tasks particularly extracting


fikatrenko adriaans van someren

relations text section method described report two types
data sets biomedical generic used experiments section elaborate
experiments sections section discusses findings detail
section concludes discussing possible future directions

kernel methods
past years witnessed boost interest kernel methods theoretical analysis
practical applications fields burges shawe taylor christianini
idea method works different structures representations
starting simplest representation limited number attributes complex
structures trees seems indeed attractive
define kernel function recall standard setting supervised classification training set n objects instances x xn yn x xn x
input examples input space x corresponding labels yn
goal infer function h x approximates target function
however h still err data reflected loss function l h xi yi
several loss functions proposed literature far best known
zero one loss loss function outputs time method errs
data point h xi yi otherwise
key idea kernel methods lies implicit mapping objects highdimensional space mapping function considering inner product
similarity k xi xj xi xj rather representing explicitly functions used kernel methods
symmetric positive semi definite
pbe
n pn
whereby positive semi definiteness defined j ci cj k xi xj n
objects x xn x choice real numbers c cn r function
positive semi definite may global optimal solution
requirements w r symmetry positive semi definiteness met kernel called
valid
idea kernel mapping cortes vapnik introduced support vector
machines svm method seeks linear separation two classes
input points function f x f x wt x b wt rp b r
h x sign f x wt stands slope linear function b
offset often exist several functions separate data well
equally good hyperplane separates mapped examples largest possible
margin would best option vapnik
svms solve following optimization
n

x

k w k c
l h xi yi
f x wt x b
argmin





equation first part equation corresponds margin maximization
minimizing k w k second takes account error training
set minimized c penalty term hyperplane found
may correspond non linear boundary original input space exist number


fiusing local alignments relation recognition

standard kernels linear kernel gaussian kernel others information
data motivate choice particular kernel
shown haussler complex kernel referred convolution kernel
defined simpler kernels
forms machine learning representations prior knowledge defined
along methods exploiting inductive logic programming offers one possible
solution use explicitly form additional horn clauses camacho
bayesian learning paradigm information hypothesis without seeing data encoded bayesian prior mitchell higher level distribution hierarchical
bayesian setting less obvious though represent use prior knowledge
learning frameworks case svms three possible ways incorporating
prior knowledge lauer bloch named sampling methods prior knowledge used generate data kernel methods prior knowledge incorporated
kernel function instance creating kernel optimization methods
prior knowledge used reformulate optimization example adding
additional constraints choice kernel general statistical properties
domain attractive possibility incorporate explicit domain knowledge
kernel improve kernel smoothing space instances
similar higher probability belonging class kernel without
prior knowledge
follows review number kernels strings proposed
community past years natural domain look
biomedical field many formulated string classification
protein classification amino acid sequences name sequence representation
however applicable biomedical area considered
many natural language processing tasks introducing kernels used
biomedicine move nlp domain present recent work relation extraction
employing kernel methods
spectrum kernel
leslie eskin noble proposed discriminative protein classification
sequence x x authors define spectrum set contiguous
subsequences x whose length equal possible long subsequences q
indexed frequency occurrence q x consequently feature map
sequence x alphabet equals x q x qam spectrum kernel two
sequences x defined inner product corresponding feature maps
ks x x
even assuming contiguous subsequences small feature space consider
large authors propose detect subsequences length suffix
tree method guarantees fast computation kernel matrix spectrum kernel
tested task protein homology detection best achieved
setting relatively small number novelty leslie et al method
lies generality low computational complexity



fikatrenko adriaans van someren

mismatch kernels
mismatch kernel introduced later leslie et al essentially extension latter obvious limitation spectrum kernel considered
subsequences contiguous match exactly mismatch kernel contiguity preserved match criterion changed words instead looking
possible subsequences length given subsequence one searching
possible subsequences length allowing r mismatches comparison
larger subset subsequences kernels defined way still calculated rather fast kernel formulated similarly spectrum kernel
major difference computing feature map
p sequences precisely feature
map sequence x defined r x qs r q r q q
q binary indicates whether sequence belongs set length sequences
differ q r elements clear r set
mismatch kernel reduced spectrum kernel complexity mismatch
kernel computation linear respect sum sequence lengths
authors mismatch kernel yields state art performance protein classification task outputs subsequences informative
biological point view
kernel methods nlp
one merits kernel methods possibility designing kernels different structures strings trees nlp field relation extraction particular
work roughly falls two categories first kernels defined plain
text sequences words second uses linguistic structures dependency
paths trees output shallow parsing short review take
chronological perspective rather start methods sequences
proceed approaches make use syntactic information
year spectrum kernel designed lodhi et al introduced string subsequence kernels provide flexible means work text data
particular subsequences necessarily contiguous weighted according
length decay factor length subsequences fixed advance
authors claim even without use linguistic information kernels
able capture semantic information reflected better performance
text classification task compared bag words lodhi et al
kernel works sequences characters kernel proposed cancedda et al
applied word sequences string kernels extended syllable kernels
proved well text categorization saunders tschach shawe taylor
kernels defined recursively computation efficient
instance time complexity lodhi et al kernel n n
length subsequence documents
subsequence kernels
recognition binary relations natural way consider words located
around relation arguments taken bunescu mooney


fiusing local alignments relation recognition

b whose choice sequences motivated textual patterns found corpora
instance observed relations expressed subject verb object constructions others part noun prepositional phrases three types
sequences considered fore words two named entities
words two entities words two
entities length sequences restricted handle data sparseness authors
generalize existing sequences pos tags entity types wordnet synsets
generalized subsequence kernel recursively defined number weighted sparse subsequences two sequences share absence syntactic information assumption
made long subsequences likely represent positive examples
penalized subsequence kernel computed three types sequences
resulting relation kernel defined sum three subkernels experimental
biomedical corpus encouraging showing relation kernel performs better
manually written patterns longest common subsequences
method proposed giuliano et al largely inspired work bunescu
mooney b however instead looking subsequences three types sequences authors treat bag words define called global kernel
follows first sequence type pattern p represented vector whose elements
counts many times token used p local kernel defined similarly
words surrounding named entities left right context final shallow
linguistic kernel defined combination global local kernels experiments biomedical corpora suggest kernel outperforms subsequence kernel
bunescu mooney
distributional kernels
recently seaghdha copestake introduced distributional kernels co occurrence probability distributions co occurrence statistics use form
syntactic relations n grams possible derive kernels
distances jensen shannon divergence jsd euclidean distance l lee
jsd smoothed version kullback leibler divergence information theoretic measure dissimilarity two probability distributions main motivation behind
lies fact distributional similarity measures proved useful
nlp tasks extract co occurrence information authors use two corpora british
national corpus bnc web gram corpus contains grams
observed frequency counts collected web distributional kernels
proved successful number tasks compound interpretation relation
extraction verb classification jsd kernel clearly outperforms
gaussian linear kernels moreover estimating distributional similarity bnc
corpus yields performance similar obtained web gram corpus
interesting finding bnc corpus used estimate similarity
syntactic relations whereas latter corpus contains n grams importantly
method seaghdha copestake provides empirical support claim
distributional similarity beneficial relation extraction



fikatrenko adriaans van someren

kernels syntactic structures
kernels defined unpreprocessed text data seem attractive applied
directly text language however general lose precision compared methods use syntactic analysis ranking parsing
trees collins duffy one first applications kernel methods nlp
accomplish goal authors rely subtrees pair trees
common later moschitti explored convolution kernels dependency
constituency structures semantic role labeling question classification work
introduces novel kernel called partial tree kernel pt essentially built
two kernels proposed subtree kernel st contains descendant nodes
target root including leaves subset tree kernel sst flexible
allows internal subtrees necessarily encompass leaves partial tree
generalization subset tree whereby partial structures grammar allowed e
parts production rules vp v form valid pt moschitti demonstrated
pts obtain better performance dependency structures ssts latter
yield better constituent trees
kernel shallow parsing output
zelenko et al use shallow parsing designed kernels extract relations text
contrast full parsing shallow parsing produces partial interpretations sentences
node tree enriched information roles correspond
arguments relation similarity two trees determined similarity
nodes depending similarity computed zelenko et al define two types
kernels contiguous subtree kernels sparse kernels types tested two types
relations person affiliation organization location exhibiting good performance
particular sparse kernels outperform contiguous subtree kernels leading conclusion
partial matching important dealing typically sparse natural language
data however computation sparse kernel takes mn time n
number children two relation examples e shallow trees consideration
n contiguous subtree kernel runs time mn
shortest path kernel
bunescu mooneys shortest path kernel represents yet another
relation extraction kernel relies information found dependency trees
main assumption entire dependency structure relevant one
focus path connecting two relation arguments instead similar
paths likely two relation examples belong category spirit
previous work bunescu mooney seek generalizations existing paths
adding information sources part speech pos categories named entity types
shortest path relation arguments extracted kernel two
sequences paths x x xn x x x computed follows



fiusing local alignments relation recognition



kb x x



q
n


f xi xi

n
n



equation f xi x number features shared xi x bunescu
mooney use several features word e g protesters part speech tag e g
n n generalized part speech tag e g n oun entity type e g p erson
applicable addition direction feature employed reproduce
example
example given two dependency paths exemplify relation located
actions brcko arrival beijing paths expanded
additional features mentioned easy see comparing path
path gives us score





brcko



actions
nnp

p rp
n n

n oun

p erson
n oun
locat ion
















beijing

nnp



n oun
locat ion







arrival

n n

p rp
n oun
p erson








time complexity shortest path kernel n n stands length
dependency path
dependency paths considered recent work relation recognition erkan
ozgur radev erkan et al use dependency paths input
compare means cosine similarity edit distance authors motivate
choice need compare dependency paths different length machine learning methods used classification including svm transuctive svm
tsvm extension svm joachims particular tsvm makes use
labeled unlabeled data first classifying unlabeled examples searching
maximum margin separates positive negative instances sets
authors conclude edit distance performs better cosine similarity measure
tsvm slightly outperforms svm
airola et al propose graph kernel makes use entire dependency
structure work sentence represented two subgraphs one
built dependency analysis corresponds linear structure
sentence kernel defined paths two vertices graph
method airola et al achieves state art performance biomedical
data sets discussed together shortest path kernel work



fikatrenko adriaans van someren

erkan et al section relation extraction biomedical domain

finally kernels defined graphs syntactic structures
graphs semantic network illustrated seaghdha uses graph
kernels graph built hyponymy relations wordnet even though
syntactic information utilized kernels proved perform well extraction
generic relations
kernels reviewed section deal sequences trees albeit different ways empirical findings suggest kernels allow partial matching usually
perform better compared methods similarity defined exact match
alleviate exact matching researchers suggested generalizing
elements existing structures bunescu mooney others opted flexible
comparison view types methods complement saunders
et al flexible partial matching methods may suffer low precision penalization mismatch low holds approaches use
generalization strategies may easily overgeneralize possible solution would
combine provided mismatches penalized well generalizations
semantically plausible rather part speech categories idea
explored present evaluated relation recognition task
nutshell goals following study possibilities
local alignment kernel relation extraction text ii exploration
use prior knowledge alignment kernel iii extensive evaluation
automatic recognition two types relations biomedical generic

local alignment kernel
one note short overview kernels designed nlp many
researchers use partial structures propose variants subsequence kernels bunescu
mooney b partial tree kernel moschitti kernel shallow parsing
output zelenko et al relation extraction focus dependency
paths input formulate following requirements kernel function
allow partial matching similarity measured paths
different length
possible incorporate prior knowledge
recall prior knowledge mean information comes larger corpora existing resources ontologies instance knowing development
synonymous evolution contexts help recognize two different words
close semantically information especially useful meaning relevant
detecting relations may differ form
following subsection define local alignment kernel satisfies
requirements incorporate prior knowledge



fiusing local alignments relation recognition

smith waterman measure local alignments
work motivated recent advances biomedical field shown
possible design valid kernels similarity measure strings saigo
vert akutsu example saigo vert ueda akutsu consider
smith waterman sw similarity measure smith waterman see measure similarity two sequences amino acids
string distance measures divided measures terms edit distance
hidden markov hmm cohen ravikumar fienberg term
distances measures tf idf score consider pair word sequences
two sets words ignoring order contrast string edit distances string similarity
measures treat entire sequences compare transformation operations
convert sequence x sequence x examples levenshtein distance
needleman wunsch needleman wunsch smith waterman smith
waterman measures levenshtein distance used natural
language processing field component variety tasks including semantic role
labeling sang et al construction paraphrase corpora dolan quirk brockett
evaluation machine translation output leusch ueffing ney others
smith waterman measure mostly used biological domain however
applications modified smith waterman measure text data well monge
elkan cohen et al hmm measures present probabilistic extensions
edit distances smith yeganova wilbur
hypothesis string similarity measures best basis kernel
relation extraction case order words appear likely relevant
sparse data usually prevents estimation probabilities work smith et al
general two sequences aligned several possible ways possible
search alignment spans entire sequences global alignment
alignment similar subsequences local alignment case
sequences amino acids relation extraction local patterns likely
important factor determines similarity therefore need similarity measure
emphasizes local alignments
formally define pairwise alignment l elements two sequences
x x x xn x x x x pairing l j l l n
j l n l example ii third element first sequence
aligned first element second one denoted
example given sequences x abacde x ace two possible alignments gaps
indicated follows
global alignment



b





c
c




e
e

alignment






c
c




e
e

alignment



ii local alignment



b




fikatrenko adriaans van someren

example number gaps inserted x align x number
elements match cases yet biological
linguistic context may prefer alignment ii closely matching substrings
local alignments better indicator similarity shared items far apart
therefore better use measure puts less weight gaps
start end strings example ii done local
alignment mechanism searches similar subsequences two sequences
local alignments employed sequences dissimilar different length
global alignments considered sequences roughly length
measures mentioned smith waterman measure local alignment
measure needleman wunsch measure compares two sequences global
alignments
definition global alignment given two sequences x x xn x x x
global alignment pair sequences length
obtained inserting zero gaps first element x x
element x x
definition local alignment given two sequences x x xn x x x
local alignment pair subsequences x x whose similarity
maximal
clarify mean local global alignments give definition
smith waterman needleman wunsch measures given two sequences x x x xn
x x x x length n respectively smith waterman measure defined
similarity score best local alignment

sw x x

max

x x

x x



equation x x score local alignment sequence x x
denotes set possible alignments best local alignment efficiently
found dynamic programming one fills matrix sw partial
alignments follows





sw j xi x j
sw j max

sw j g


jm
sw j g



equation xi x j denotes substitution score two elements xi x j
g stands gap penalty equation possible partial alignments
stored matrix cell j reflects score alignment x xi



fiusing local alignments relation recognition


c
e












b











c











e






c
e

smith waterman measure












b











c











e





b needleman wunsch measure

table matrices computing smith waterman needleman wunsch scores sequences x abacde x ace gap g substitution score xi x j
xi x j xi x j xi x j

x x j cell largest value matrix contains smith waterman
score
needleman wunsch measure searches global alignments defined similarly except fact cells matrix contain negative scores

nw j xi x j
nw j max
nw j g


nw j g
jm



smith waterman measure seen modification needleman wunsch
method disallowing negative scores matrix regions high dissimilarity
avoided local alignments preferred moreover needlemanwunsch score equals largest value last column last row smith waterman
similarity score corresponds largest value matrix
let us reconsider example global local alignments alignments
two sequences x abacde x ace obtained arrive actual alignments one
set gap parameter g substitution scores assume use following
settings gap g substitution score xi x j xi x j xi x j
xi x j values chosen illustrative purpose realistic
case e g alignment protein sequences choice substitution scores usually
motivated biological evidence gapping smith waterman suggested
use gap value least equal difference match xi x j
xi x j mismatch xi x j xi x j smith waterman needlemanwunsch similarity scores x x calculated according equation
equation given table
first first row first column matrix initialized
matrix filled computing maximum score cell defined equation
equation score best local alignment equal largest element


fikatrenko adriaans van someren

matrix needleman wunsch score note possible trace back
steps taken arrive final alignment cells boldface left right
step corresponds insertion top step deletion lead gaps
diagonal step implies alignment two sequences elements
since prefer use local alignments dependency paths natural choice would
use smith waterman measure kernel function however saigo et al
observed smith waterman measure may valid kernel
may positive semi definite give definition la kernel states
two sequences similar many local alignments high scores
equation
kl x x

x



es x x



x x

x x local alignment score scaling parameter
define la kernel kl equation two sequences x x needed
take account transformation operations used local alignments first one
define kernel elements corresponds individual alignments ka second
since type alignment allows gaps another kernel gapping kg last
least recall local alignments parts sequences may aligned
elements x x may left elements influence alignment
score kernel used cases k set constant k x x finally
la kernel composition several kernels k ka kg spirit
convolution kernels haussler
according saigo et al similarity aligned sequences elements ka kernel
defined follows


x
x


ka x x


x x
e
otherwise
x x one element kernel would otherwise
calculated substitution score x x x x score reflects
similar two sequences elements depending domain computed
prior knowledge given domain
gapping kernel defined similarly alignment kernel equation whereby
scaling parameter preserved gap penalties used instead similarity
function two elements


kg x x e g x g x



g stands gap function naturally gap length function returns
zero gaps length n reasonable define gap terms gap opening
gap extension e g n e n case possible decide whether longer
gaps penalized shorter ones much instance


fiusing local alignments relation recognition

three consecutive gaps alignment first gap counted gap opening
two gap extension consecutive gaps e gaps length n gap
equal importance gap opening equal gap extension however
length gaps matter one would prefer penalize gap opening
give little weight gap extension
kernels combined follows
k r x x k ka kg r ka k



equation k r x x stands alignment r elements x x possibly
r gaps similarity aligned elements calculated ka gapping kg since
could r gaps corresponds following part equation
ka kg r rth aligned element one ka added given
discussion k added initial final part follows equation
elements x x aligned k r equals k elements x
x aligned gaps value k r ka r
finally la kernel equal sum taken possible local alignments
sequences x x



kl x x


x

k x x





biological domain suggest kernels smith waterman
distance relevant comparison amino acids string kernels saigo et al
clear whether holds applied natural language processing tasks
view could depend parameters used substitution
matrix penalty gaps
computational complexity
la kernel many kernels discussed section efficiently calculated dynamic programming two sequences x x length n respectively
complexity proportional n additional costs may come substitution matrix unlike biomedical domain become large however
look substitution scores done efficient manner well leads
fast kernel computation instance calculating kernel matrix largest data
set used aimed instances takes seconds ghz intel r
core tm machine
designing local alignment kernel relation extraction
smith waterman measure transformations particular deletions elements different strings however elements different may still
similar degree similarities used part similarity measure
example two elements words different synonyms
count less different completely unrelated call


fikatrenko adriaans van someren

similarities substitution scores equation define two different ways
basis distributional similarity basis semantic relatedness ontology
example would able infer brcko similar beijing even
though two words match exactly furthermore phrases arrival
beijing arrival january would kernel say brcko
similar beijing january use information prior knowledge
makes possible measure similarity two words one test set
training set even match exactly review two types
measures statistical distributions relatedness wordnet
distributional similarity measures
number distributional similarity measures proposed years including
cosine dice jaccard coefficients distributional similarity measures extensively studied lee weeds weir mccarthy main hypothesis
behind distributional measures words occurring context
similar meaning firth context defined proximity text
employing grammatical relations use first option context
sequence words text length set advance
measure

formula

cosine

xi x j p

dice

xi x j

l

xi x j

p c xi p c x j
p


c p c xi
c p c xj

p

c

f xi f x j
f xi f x j

qp

c p c xi

p c x j

table list functions used estimate distributional similarity measures
chosen following measures dice cosine l euclidean whose definitions given table definition cosine l possible use
frequency counts probability estimates derived unsmoothed relative frequencies
adopt definitions given lee probability estimates p recall x x two sequences would wish compare
corresponding elements xi x j c stands context definition
dice coefficient f xi c p c xi mainly interested symmetric measures
xi x j x j xi symmetric positive semi definite matrix required kernel methods euclidean measure defined table necessarily vary
reason given list pairs words xi x j xi fixed j
corresponding l score maximum value maxj xi x j detected used
normalize scores list furthermore unlike dice cosine return
case two words equal euclidean score equals next step substract
obtained normalized value ascertain scores within interval


fiusing local alignments relation recognition

largest value assigned identical words view procedure
make comparison selected distributional similarity measures respect
influence la kernel transparent
distributional similarity measures suitable information available
case data annotated means taxonomy e g wordnet
possible consider measures defined taxonomy availability hand crafted
resources wordnet comprise relations concepts enables
making distinctions different concepts subtle way
wordnet relatedness measures
generic relations commonly used resource wordnet fellbaum
lexical database english wordnet words grouped together synsets
synset consists list synonymous words collocations e g fountain pen
pointers describe relations synset synsets fellbaum
wordnet employed different purposes studying semantic constraints
certain relation types girju badulescu moldovan katrenko adriaans
enriching training set giuliano et al nulty
compare two concepts given synsets c c use five different measures
proposed past years rely notions length
shortest path two concepts c c len c c depth node
wordnet hierarchy equal length path root given
synset ci dep ci least common subsumer lowest super ordinate c
c lcs c c turn synset measures exclusively
notions belong conceptual similarity proposed palmer wu simwup
equation formula scaled semantic similarity introduced leacock
chodorow simlch equation major difference lies
fact simlch consider least common subsumer c c uses
maximum depth wordnet hierarchy instead conceptual similarity ignores
focuses subhierarchy includes synsets

simwup c c

dep lcs c c
len c lcs c c len c lcs c c dep lcs c c

simlch c c log

len c c
maxcw ordn et dep c





aiming combining information several sources resnik introduced yet another measure grounded information content simres equation intuitively
two synsets c c located deeper hierarchy path one synset
another short similar path two synsets long
least common subsumer placed relatively close root indicates synsets
equations similarity measures defined wordnet subscripts refer similarity measure
e g lch wup simlch simwup respectively



fikatrenko adriaans van someren

c c much common quantify intuition necessary derive
probability estimate lcs c c done employing existing corpora
precisely p lcs c c stands probability encountering instance concept
lcs c c
simres c c log p lcs c c



one biggest shortcomings resniks method fact least
common subsumer appears equation one easily imagine full blown hierarchy
relatedness concepts subsumed lcs ci cj heavily vary
words lcs one able make subtle distinctions two
pairs concepts share least common subsumer overcome jiang
conrath proposed solution takes account information synsets
compared simjcn equation comparing equation equation
notice equation incorporates probability encountering
lcs c c probability estimates c c
simjcn c c log p lcs c c log p c log p c



lin defined similarity two concepts much commonality
differences involved similarly two previous approaches uses
information theoretic notions derives similarity measure simlin given equation

simlin c c

log p lcs c c
log p c log p c



past semantic relatedness measures evaluated different nlp tasks budanitsky hirst ponzetto strube concluded measure
performs best evaluation use semantic relatedness
validation generic relations study depth contribute final
substitution matrix relation extraction
discussed two possible ways calculating substitution score
distributional similarity measures measures defined wordnet however
dependency paths generated parsers may contain words lemmata
syntactic functions subjects objects modifiers others take
account revise definition assume sequences x x x xn
x x x x contain words xi w w refers set words syntactic
functions accompanied direction xi
w elements w unique words
lemmata found dependency paths instance paths
actions brcko arrival beijing example section
w actions brcko arrival beijing dependency paths use present
work include information syntactic functions instance awareness
joy case w awareness come joy w


prep f rom

prep f rom nsubj







nsubj

come

fiusing local alignments relation recognition



xi x j






xi x j








xi x j w
xi x j
w xi x j

xi xj
w xi x j
xi w x j
w
xi
w x j w



equation states whenever element xi sequence x compared
element x j sequence x substitution score equal similarity
score case elements words lemmata ii elements
syntactic function iii case
follows discussion similarity measures two ways define
xi x j distributional similarity xi x j section
wordnet similarity provided annotated wordnet synsets section

experimental set
section describe data sets used experiments provide
information data collections used estimating distributional similarity
data
evaluate performance la kernel consider two types text data domainspecific data comes biomedical domain generic domain independent
data represents variety well known widely used relations partwhole cause effect
work extract dependency path two nodes corresponding
arguments binary relation assume analysis tree since
acyclic graph exists unique path pair nodes
consider however structures might derived full syntactic analysis
example subtrees moschitti
biomedical relations
corpora use three corpora come biomedical field contain annotations interacting proteins bc ppi sentences aimed bunescu mooney
b interactions among proteins genes lll sentences training set
test set nedellec bc ppi corpus created sampling sentences biocreative challenge aimed corpus sampled medline
collection lll corpus composed querying medline term bacillus subtilis difference among three corpora lies directionality interactions
table shows relations aimed corpus strictly symmetric lll asymmetric bc ppi contains types differences number training instances
aimed corpus explained fact correspond dependency
available http www informatik hu berlin de hakenber



fikatrenko adriaans van someren

paths named entities parsing fails produces several disconnected graphs per
sentence dependency path extracted
parser
linkparser
linkparser
stanford
stanford
enju

data set
lll train
lll test
bc ppi
aimed
aimed

examples






pos






direction
asymmetric
asymmetric
mixed
symmetric
symmetric

even though actual annotations test data given number interactions
test data set provided lll organizers

table statistics biomedical data sets lll bc ppi aimedd table pos
stands number positive examples per data set examples indicates
number examples total

goal relation extraction three cases output correct interactions
biomedical entities genes proteins found input data
biomedical entities already provided need named entity recognition
discrepancy training test sets used lll challenge
unlike training set sentence example least one interaction
test set contains sentences interaction organizers lll challenge distinguish sentences without coreferences sentences coreferences
usually appositions shown one examples first sentence
example sentence without coreferences interaction ykud sigk
whereas second one sentence coreference interaction spoiva
sigmae precisely spoiva refers phrase one genes
known interact sigmae therefore infer spoiva interacts sigmae sentences without coreferences form subset refer lll nocoref
sentences coreferences part separate subset lll coref
ykud transcribed sigk rna polymerase sporulation
finally proper localization spoiva required expression one
genes spoiva control mother cell
transcription factor sigmae
assumed relations sentences coreferences harder recognize la kernel performs subsets report experimental full set test data lll subsets lll coref lll nocoref
syntactic analysis analyzed bc ppi corpus stanford parser lll
corpus already preprocessed linkparser output checked
experts enable comparison previous work used aimed corpus parsed



fiusing local alignments relation recognition

stanford parser enju parser exactly correspond input
experiments erkan et al stre et al unlike stanford parser
enju head driven phrase structure grammar hpsg output
enju parser presented two ways predicate argument structure
phrase structure tree predicate argument structures describe relations words
sentence phrase structure presents sentence structure form clauses
phrases addition enju trained genia corpus includes model
parsing biomedical texts
cbf contains three proteins cbf cbf b cbf c

contains

dobj

nsubj

proteins

cbf

num

conj

conj conj

three

cbf
nsubj

cbf b
dobj

cbf contains proteins
nsubj
dobj
cbf contains proteins
nsubj
dobj
cbf contains proteins

cbf b

conj

cbf
cbf b
conj
cbf c
conj

figure stanford parser output representation example
figure shows dependency tree obtained stanford parser sentence
sentence mentions three interactions among proteins precisely
cbf cbf cbf cbf b cbf cbf c three dependency
paths contain words lemmata syntactic functions subj subject plus
direction traversing tree figure presents output sentence provided
enju parser upper part refers phrase structure tree lower part
shows paths extracted predicate argument structure two parsers clearly
differ output first stanford parser conveniently generates paths
three interaction pairs enju analyzer second output
stanford parser excludes prepositions conjunctions attached syntactic
functions whereas enju analyzer lists parsing differences
available http nlp stanford edu software lex parser shtml
available http www tsujii u tokyo ac jp enju



fikatrenko adriaans van someren

lead different input sequences later fed la kernel consequently
variations input may translate differences final performance

cbf
cbf
cbf

arg verb



arg verb



arg verb



contain
contain
contain

arg verb



arg verb



arg verb



protein
protein
protein

arg app



arg app



arg app







arg app



arg app



arg app



cbf
cbf
cbf

arg coord



arg coord





arg coord





cbf b

arg coord



cbf c

figure enjus output representation example
addition work employing aimed dependency paths
figure figure preprocessed following way actual named entities
arguments relation replaced label e g protein consequently
nsubj

dobj

conj

first path figure becomes protein contains proteins protein
able compare aimed performance reported work
erkan et al stre et al use exactly dependency paths
argument labels however study whether labels instead actual named entities
impact final lll data set carry two experiments
first one dependency paths contain named entities whereas second contain
labels second experiment referred adding word label name
lll label table
generic relations
second type relations consider generic relations arguments
sometimes annotated external resources wordnet makes possible
use semantic relatedness measures defined example



fiusing local alignments relation recognition

data used semeval challenge task classification semantic relations
nominals girju et al
goal task classify seven semantic relations cause effect instrument agency product producer origin entity theme tool part whole content container whose examples collected web
predefined queries words given set examples relation expected output would binary classification whether example belongs given
relation arguments relation annotated synsets wordnet
hierarchy figure given sentence pair spiritual awareness joy
corresponding synsets joy awareness would mean classifier decide whether pair example cause effect relation
particular sentence retrieved quering web phrase joy comes
synsets manually selected wordnet hierarchy seven semantic
relations used challenge gives seven binary classification

genuine e joy e comes e spiritual awareness e life absolute clarity direction living purpose
wordnet e joy wordnet e awareness
query joy comes cause effect e e true

figure annotated example cause effect semeval task
training data set

relation type
origin entity
product producer
theme tool
instrument agency
part whole
content container
cause effect

examples train








pos train








examples test








direction
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric
asymmetric

table distribution semeval task examples training test data
pos stands number positive examples per data set examples
indicates number examples total

syntactic analysis generate dependency paths seven data sets used semeval task analyzed stanford parser dependency path sentence
figure given


fikatrenko adriaans van someren

awareness n

prep f rom



nsubj

come joy n

words annotated wordnet pos tag attached followed sense
instance awareness noun current context first sense used
corresponds awareness n
substitution matrix
build substitution matrix la kernel use distributional similarity
wordnet semantic relatedness measures data set dependency paths
contains unique elements words syntactic functions size matrix
k elements words number substitution scores computed
distributional similarity semantic relatedness measures equals k k due
fact measures use symmetric substitution matrix built
corpus used experiments three substitution matrices
biomedical domain bc ppi lll aimed seven substitution matrices
generic relations follows discuss settings used calculating
substitution matrix detail
distributional similarity estimated contextual information
seaghdha copestake exploring grammatical relations words lee
work opt contextual information motivated presence
words belonging different parts speech dependency paths instance
even though according dependency grammar theory melcuk adjectives
govern words may still occur dependency paths words even
parsing fail may produce unreliable syntactic structures able compare
words part speech decided estimate distributional similarity
contextual information rather grammatical relations
computing distributional similarity may happen given word xi
occur corpus handle cases set xi xi largest possible
similarity score xi x j xi x j lowest possible similarity score
biomedical domain
estimate distributional similarity biomedical domain use trec
genomics collection hersch cohen roberts rakapalli contains
documents journals documents preprocessed removing htmltags citations text reference sections stemmed porter stemmer van
rijsbergen robertson porter furthermore query likelihood
dirichlet smoothing chen goodman used retrieve document passages given
query passages ranked according likelihood generating query dirichlet smoothing used avoid zero probabilities poor probability estimates may
happen words occur documents k unique words occurring set
dependency paths sequences fed queries collect corpus estimating similarity
immediate context surrounding pair words used calculate distributional
similarity words set context window tokens right



fiusing local alignments relation recognition

tokens left word focus perform kind preprocessing
pos tagging
generic relations
generic relations use wordnet relatedness measures described section
already shown wordnet relatedness measures work synsets
assumes words manually annotated information wordnet
since done relations arguments see example figure
words sentences correspondingly dependency paths build
substitution matrix follows two words annotated wordnet substitution score equals value returned relatedness measure used word
pair equals whenever words identical otherwise example
consider words dependency path wu palmer wup relatedness
measure substitution scores obtain follows

awareness n awareness n
awareness n come
awareness n joy n
prep come
prep joy n
come nsubj
nsubj nsubj
joy n joy n

awareness n prep
awareness n nsubj
prep prep
prep nsubj
come come
come joy n
nsubj joy n

figure substitution scores dependency path wup measure
syntactic relations prep subj accompanied direction
dependency tree traversal

dependency path unique elements annotated
wordnet synsets k consequently substitution scores total
computed wordnet relatedness
compute wordnet relatedness use wordnet similarity package wordnet pedersen patwardhan michelizzi
baselines kernel settings
section discuss two baselines kernel settings
baselines
test well local alignment kernels perform compared kernels proposed past
implemented shortest path kernel described work bunescu mooney
applies cases relation arguments could annotated wordnet
information



fikatrenko adriaans van someren

section one baselines baseline method seems
natural choice operates data structures dependency paths
similarly bunescu mooneys work experiments use lemma part
speech tag direction consider entity type negative polarity items
choice la kernel motivated ability
compare sequences flexible way possibility explore additional
information present training set via substitution matrix baseline
baseline ii used test whether choice similarity measures affects
case substitution scores calculated distributional similarity
wordnet relatedness generated randomly within interval
kernel settings
kernels compute used together support vector machine tool libsvm
chang lin detect hyperplanes separating positive examples negative
ones plugging kernel matrices fold cross validation libsvm
normalized equation





k x

k x p

k x x k



handle imbalanced data sets notably aimed bc ppi examples
weighted inverse class probability e training examples class weighted
prob prob fraction training examples class significance
tests done two tailed paired test confidence level
addition experiments tuned penalty parameter c equation
range
use la kernel one set following parameters gap opening cost
gap extension cost scaling parameter cross validation experiments
gap opening cost set extension cost scaling parameter
choice scaling value motivated experiments amino acids
biological domain saigo et al initial experiments present
study parameter values varied

experiment domain specific relations
goal evaluation study behavior la kernel domain specific
relations biomedical domain section report experiments conducted
three biomedical corpora la kernel distributional similarity measures two baselines published previously e g graph kernel airola
et al tree kernel stre et al best knowledge string
kernels applied dependency paths yet however gap weighted string
kernel described section allows gapping thus compared la
kernel test lodhi et al kernel performs dependency paths use



fiusing local alignments relation recognition

three corpora tuned parameters string kernel set length
subsequences decay factor
lll bc ppi data sets
subsection presents two biomedical data sets bc ppi lll whenever
possible discuss performance previously reported literature
fold cross validation bc ppi corpus presented table
lll training data set table la kernel distributional similarity
measures la dice la cosine la l performs significantly better two baselines recall baseline corresponds shortest path section
baseline ii la kernel randomly generated substitution scores contrast
baseline able handle sequences different lengths including gaps according
equation comparison two sequences different lengths score
nevertheless still yields high recall precision much lower explained
fact shortest path uses pos tags even though two sequences
length different comparison may still non zero score provided
part speech tags match furthermore baseline ii suggests accurate estimation substitution scores important achieving good performance baseline ii may
yield better baseline randomly generated substitution scores degrade
performance
method
la dice
la cosine
la l
baseline
baseline ii
gap weighted string kernel lodhi et al

precision







recall







f score







table fold cross validation bc ppi data set
first glance choice distributional similarity measures affect
overall performance yielded la kernel bc ppi data method
l measure outperforms methods dice p cosine
differences latter case significant statistically significant differences
observed method dice cosine
contrast bc ppi data set kernels use dice cosine measures
lll data set significantly outperform one l p
p respectively
data sets la method distributional similarity measures significantly
outperforms baselines interestingly gap weighted string kernel lodhi et al
yields good performance seems better choice subsequence
lodhi et al mentioned f numbers respect ssk seem
peak subsequence length



fikatrenko adriaans van someren

kernel shallow linguistic information giuliano et al recent work
lll fundel kueffner zimmer employs dependency information contrast
method serves representation extraction rules defined airola
et al apply graph kernel extract interactions use among
others lll aimed data sets seen table method yields
comparable gap weighted string kernel dependency paths
best knowledge performance achieved la kernel lll training set
highest terms f score among reported
literature
method
la dice
la cosine
la l
baseline
baseline ii
graph kernel airola et al
gap weighted string kernel lodhi et al
shallow linguistic kernel giuliano et al
rule method fundel et al

precision










recall










f score










table fold cross validation lll training data set
apply method lll test data table even though performance test set poorer la dice outperforms baselines addition
gap weighted string kernel lodhi et al seems perform much worse test
set la kernel precision high recall decreases drastically
data subset includes co references might due fact
sentences incomplete parses generated consequently dependency paths
entities found possible interaction pairs generated
test data dependency path extracted contrast reported
giuliano et al make use syntactic information data subset
without coreferences achieves higher recall
hand lower recall caused actual names proteins
genes arguments work reported relation arguments
named entities often replaced types e g protein used
input learning conducted additional experiments named entity
types dependency paths led great improvement terms recall
f score table lll coref label lll nocoref label lll coref label method
clearly outperforms shallow linguistic kernel achieves better
best performing system lll competition sbest according nedellec
applied markov logic syntactic paths
airola et al report performance lll data set reason information
graph paths kernel included table



fiusing local alignments relation recognition

data set
lll coref
lll nocoref
lll
lll
lll
lll coref label
lll nocoref label
lll label
lll coref
lll nocoref
lll
lll
lll

method
la dice
la dice
la dice
baseline
baseline ii
la dice
la dice
la dice
shallow linguistic kernel giuliano
shallow linguistic kernel giuliano
shallow linguistic kernel giuliano
gap weighted string kernel lodhi
sbest nedellec

et
et
et
et

al
al
al
al






precision














recall














f score














table lll test data set

aimed data set
yet another data set consider aimed data set often used
experiments relation extraction biomedical domain enables comparison
methods noted however particular case corpus
collection documents abstracts may lead two ways performing fold
cross validation one possibility lies randomly splitting data parts
cross validation level documents experiments report
done first setting directly compared methods described
work stre et al erkan et al giuliano et al addition
use dependency paths la kernel ones employed stre et al
erkan et al airola et al bunescu obtained
cross validating level documents
conducted experiments setting distributional measure dice referred
la dice table upper part table used dependency paths generated
stanford parser lower part obtained enju discussed
section erkan et al use similarity measures compare dependency paths
consider additional sources whose information incorporated
learning procedure however experiment supervised svm semi supervised
learning tsvm number training instances varied table shows best
performance achieved erkan et al method among
svm one cosine distance svm cos yields best tsvm
setting one edit measure performs best observe la dice slightly
outperforms particular high precision
work stre et al explore several parsers combinations features
features include paths enju word dependencies generated
data driven ksdep parser word features ksdep parser probabilistic



fikatrenko adriaans van someren

shift reduce sagae tsujii general method stre et al
uses svm case focuses tree kernels discussed section make
fair comparison conducted experiments paths obtained deep syntactic analysis
enju parser compared scores stre et al contrast
previous experiments achieve higher recall lower precision overall la
kernel yields better performance one reported stre et al however
different sets features combined parses enju ksdep plus word features enju ksdep w table overall performance improved
method
la dice
baseline bunescu
baseline ii
svm cos erkan et al
tsvm edit erkan et al
gap weighted string kernel lodhi et al
la dice
tree kernel stre et al
tree kernel stre et al
graph kernel airola et al
shallow linguistic kernel giuliano et al

parser
stanford
collins
stanford
stanford
stanford
stanford
enju
enju
enju ksdep w
charniak lease
none

precision












recall












f score












table fold cross validation aimed data set
bunescu reports evaluation aimed corpus form
precision recall curve consider highest precision obtained experiments depending input roughly corresponds recall
plot referred baseline table sum shortest path
never approaches performance la kernel biomedical data sets
studied baseline baseline ii achieves lowest scores
methods presented
table illustrates methods trained aimed corpus
many different parsers used noted graph kernel
trained tested syntactic representation generated charniak lease
parser shortest path kernel explored dependency paths obtained
collins parser charniak lease parser statistical parser trained biomedical
data lease charniak whose phrase structures transformed dependencies likewise collins parser statistical parser collins leads
question whether choice syntactic parser significant impact extraction
compare impact syntactic parsers relation extraction aimed
miyao et al conducted complex study eight parsers including stanford analyzer five parse representations consider two cases first one
parsers trained biomedical data regardless parser used
experiments accuracy extraction task similar second experiment
dependency tree formats e g stanford dependency format phrase
structures predicate arguments structures



fiusing local alignments relation recognition

parsers trained domain specific data case shown
relation extraction improved actual gain however vary
one parser another
aimed data la kernel dice measure gives state art
outperformed approaches use information dependency
paths
la kernel parameters
saigo et al already shown scaling parameter equation
significant impact accuracy carried additional experiments varying
gap values value visualized figure opening extension
gap values separated slash symbol values x axis form b
read opening gap set extension gap equal b
kernel matrices normalized examples weighted according previous
experiments yielded dice measure significantly differ
ones achieved cosine measure selected dice measure conduct
experiments performance bc ppi data set shown figure

f score
























gaps
















scaling

figure varying gaps scaling parameter bc ppi data set fold
cross validation f score



fikatrenko adriaans van someren













precision

















gaps

scaling










figure varying gaps scaling parameter bc ppi data set fold
cross validation precision




recall































gaps





scaling






figure varying gaps scaling parameter bc ppi data set fold
cross validation recall


fiusing local alignments relation recognition

figure indicate decreasing leads decrease overall performance moreover varying gap values causes subtle changes f score
changes drastic changes due lower
changes f score likely explained variances precision
recall investigate matter look measures depend parameter
changes set low value one expect nearly diminish impact
substitution matrix e similarity among elements reason hypothesize
larger values scaling parameter higher recall indeed figure
supports hypothesis recall plot resembles one f score varying
parameter values much lower impact precision figure nonetheless precision
decrease parameter becomes larger
overall seems influence final although gap values make
contribution well according obtained setting extension gap e
large value equal opening gap undesirable since scaling parameter
applied substitution matrix gap values well setting
decreases effects gap penalization similarity elements consequently
best performance achieved setting suggests final performance
la kernel influenced combination parameters choice crucial
obtaining good performance

experiment ii generic relations
another series experiments carried seven generic relations semeval
challenge task choice data sets case motivated two
factors first semantic relations used differ relations biomedical
domain second since arguments relations annotated wordnet becomes
possible explore information wordnet use prior knowledge la
kernel
many participants challenge considered wordnet explicitly tribble
fahlman kim baldwin part complex system giuliano et al
since obvious use wordnet yields best performance many researchers made additional decisions use supersenses hendrickx et al selection predefined number high level concepts nulty
cutting wordnet hierarchy certain level bedmar et al systems
one nakov solely information collected web
even though became evident best performing systems used wordnet variance remarkable clear whether difference performance
explained machine learning methods used combination features
factors
semeval task data set includes relation examples nominal
compounds coffee maker greatly reduces availability information
two arguments dependency paths relation arguments case linked
one grammatical relation e g coffee maker linked grammatical relation
nn corresponds noun compound assume therefore information coming
wordnet especially helpful dependency paths short



fikatrenko adriaans van someren

experiments used relatedness measures defined earlier section plus one additional
measure called random random measure indicates relatedness values
two relation arguments generated randomly within thus
suitable baseline baseline ii similarly experiments biomedical domain
another baseline shortest path kernel baseline note task overview
girju et al reported three baselines case guessing
true false examples depending class majority class test
set baseline iii ii guessing true baseline iv iii guessing true false
probability corresponds class distribution test set baseline v
first question interest implications choice semantic relatedness
measure performance la kernel answer question perform
fold cross validation training set figure figure figure among
measures jcn resnik fail perform better random score
cases resnik score outperformed measures behaviour leacockchodorow score lch jcn varies one semantic relation another instance use
jcn seems boost precision cause effect part whole product producer
theme tool remaining three relations clearly best performing
measure
check whether differences relatedness measures carried
significance tests comparing measures relations findings summarized
table symbol two relatedness measures stands measure
equivalence words indicates significant difference similarly
experiments biomedical field significance tests conducted
two tailed paired test confidence level addition two measures
b b means performs significantly better b instance ranking
cause effect table read follows two best performing measures
wup lch significantly outperform lin followed random res
turn yield significantly better jcn seen table wup
lch clearly best performing measures seven relations best
measure six seven relations
relation type
cause effect
instrument agency
product producer
origin entity
theme tool
part whole
content container

ranking
wup lch lin
wup lch lin
wup lch lin
wup lch lin
lch lin wup
wup lin lch
wup lch lin









res random jcn
res jcn random
jcn res random
res jcn random
res jcn random
res jcn random
res jcn random

table ranking relatedness measures respect accuracy training sets stands measure equivalence b indicates measure
significantly outperforms b



fiusing local alignments relation recognition

relation applied best performing measure training set
particular relation test data reported table average la
kernel employing wordnet relatedness measures significantly outperforms two baselines
moreover compared best semeval competition beamer
et al method approaches performance yielded best system bestsv
system used lexical syntactic semantic feature sets
expanded training set adding examples many different sources already
mentioned section recent work seaghdha explores wordnet
structure graph kernels classify semantic relations overall performance
achieved method table comparable one la kernel
unclear whether semantic relations one approaches performs
better
relation type
cause effect
instrument agency
product producer
origin entity
theme tool
part whole
content container
average
baseline
baseline ii
baseline iii
baseline iv
baseline v
bestsv
gap weighted string kernel lodhi et al
wordnet kernels seaghdha

accuracy

















precision

















recall

















f score

















measure
lch
wup
lch
wup
lch
wup
wup

table semeval task test data set selecting best performing
measure training set relation

addition report semeval task test set per relatedness measure
table averages seven relations similarly findings
training set wup lch best performing measures test data well
one would expect optimal use prior knowledge allow us reduce
number training instances without significant changes performance study
whether amount training data influences test set split
training set several subsets creating model subset applying
semeval task test data split corresponds split used challenge
organizers figure suggests relations recognized well even relatively
small data sample used exception theme tool relation increasing
model trained origin entity examples classifies none test examples positive
reason point figure relation given training examples



fikatrenko adriaans van someren

training data clearly helps finding line giuliano et al
whose system combination kernels data indicate
relations one theme tool extracted well even quarter
training set used
relatedness measure
wup
lch
lin
res
jcn
random

accuracy







precision







recall







f score







table semeval task test data set averages relations
per wordnet relatedness measure

learning curve





f score



cause effect
instrument agency
product producer
origin entity
theme tool
part whole
content container











training examples



figure learning curve semeval task test data set
recent work semeval task data set includes investigation distributional kernels seaghdha copestake pattern clusters davidov rappoport
relational similarity nakov hearst wordnet kernels unlike wordnet
kernels first three approaches use wordnet seaghdha copestake
report accuracy f score best yielded distributional kernels best performance davidov rappoports method
accuracy f score wordnet kernels similarly findings
la kernel yield better accuracy methods wordnet



fiusing local alignments relation recognition

cause effect


precision
recall
f score












wup

lin

lch
res
similarity measure

jcn

random

instrument agency


precision
recall
f score












wup

lin

lch
res
similarity measure

jcn

random

product producer


precision
recall
f score












wup

lin

lch
res
similarity measure

jcn

random

figure fold cross validation training set cause effect instrument agency product producer relations



fikatrenko adriaans van someren

origin entity


precision
recall
f score












wup

lin

lch
res
similarity measure

jcn

random

theme tool


precision
recall
f score












wup

lin

lch
res
similarity measure

jcn

random

part whole


precision
recall
f score












wup

lin

lch
res
similarity measure

jcn

random

figure fold cross validation training set origin entity theme tool
part whole relations



fiusing local alignments relation recognition

content container


precision
recall
f score












wup

lin

lch
res
similarity measure

jcn

random

figure fold cross validation training set content container relation

f score comparable performance reported seaghdha copestake
davidov rappoport

discussion
section revisit goals stated end section discuss
findings detail
la kernel relation extraction
introduced la kernel proven effective biomedical
nlp domain showed well suited relation extraction particular experiments two different domains outperform existing methods yield
performance par existing state art kernels
one motivations la kernel relation extraction task
exploit prior knowledge explore two possibilities distributional similarity
information provided wordnet
distributional similarity measures
setting consider three distributional measures already studied
instance lee uses detect similar nouns verb object
co occurrence pairs suggest jaccard coefficient related
dice measure one best performing measures followed others including
cosine euclidean distance fell group largest error rates given previous
work lee one would expect euclidean distance achieve worse


fikatrenko adriaans van someren

two measures indeed lll corpus la kernel employing l shows
significant decrease performance measures method dice
significantly outperforms one l measure lll corpus
significant improvement bc ppi data set experiments
conducted conclude la kernel dice cosine measures performs
similarly lll data set bc ppi corpus given biomedical corpora different settings experimented obtained experimental
support choosing dice cosine measure euclidean distance
wordnet similarity measures
generic relations semantic relatedness plays significant role difference
f score use semantic relatedness kernel relatedness
values generated randomly baseline ii amounts nearly measures exhibit
different performance seven generic relations considered
observe instance wup lch lin almost yield best matter
relation considered found resnik score jiang conraths measure
yield lower measures even though f scores per relation vary
quite substantially placing cause effect theme tool origin entity among
difficult relations extract two measures wup lch top performing
measures seven relations two measures explore wordnet taxonomy
length paths two concepts depth wordnet hierarchy
consequently belong path measures three measures res lin
jcn information content measures relatedness two concepts
defined amount information share experiments la
kernel generic relation recognition suggest particular case path
measures preferred information content measures
stress however evaluation semantic relatedness measures context relation recognition one means draw conclusion
top measures nlp tasks stay example budanitsky
hirst use semantic relatedness measures detect malapropism
jiang conraths measure jcn yields best followed lins measure lin
one leacock chodorow lch resniks measure res
quite similar findings consider res measure jcn
top accuracy ranking list seven semantic relations
studied
factors parameters influence la kernel performance
experiments two domains shown la kernel outperforms existing
methods corpora yields performance par existing state art
kernels
baselines
advantage la kernel bunescu shortest path method baseline
capable handling paths different lengths allowing gaps penalizing


fiusing local alignments relation recognition

final kernel matrix becomes less sparse shortest path attempts
generalize dependency paths usually overgeneralizes leads high
recall scores table table poor overall performance one explanation
overgeneralization may method accounts well structural similarity provided
sequences length fails provide finer distinctions among dependency
paths consider example two sequences trip makes tram coffee makes
guy whereby first path represents negative instance product producer
relation second path corresponds positive one even though match
exactly elements match nouns singular consequently comparison
according shortest path method relatively high similarity score
contrast la kernel consider similarity elements pairs trip coffee
tram guy obtain low scores
addition baseline ii randomly generated substitution scores
performs poor data sets comparable baseline leads us conclusion
accurate estimation similarities another reason la kernel performs well
relation extraction
comparison methods
already pointed obvious shortcoming baseline inability
handle dependency paths different length reason applied
gap weighted string kernel lodhi et al data sets case dependency
paths compared flexible way gapping allowed additional
information used kernel outperforms baseline increasing precision relation
extraction preserving relatively high recall data set fails yield
good lll test data believe due differences lll
training test data data sets la kernel achieves better performance
gap weighted string kernel margin however different different data sets
biomedical domain differences two methods clearly seen
bc ppi lll data sets aimed corpus comparable
however methods tested aimed get higher scores unless use
features dependency paths holds types cross validation used
corpus generic relations difference la kernel gapweighted string kernel much larger particular case gap weighted kernel
precision high recall much lower explained fact generic
relations benefit knowledge found wordnet recall achieved la kernel
therefore high gap weighted kernel access information found
dependency paths reason fails relations
la kernel achieves best performance lll training set outperforming
graph kernel airola et al shallow linguistic kernel giuliano et al
rule system fundel et al three used different input
methods varying plain text dependency structures reason direct
comparison unfortunately possible conclude methods employing
dependency information among best performing approaches



fikatrenko adriaans van someren

two approaches whose performance reported aimed data set include tree kernel stre et al tsvm erkan et al
explore syntactic information different ways stre et al consider subtrees
method erkan et al similarities relies
dependency path comparison comparison use information already
available dependency paths svm setting dependency paths tsvm setting according lauer bloch tsvms fall category prior
knowledge sampling methods explores prior knowledge generating
examples contrast employ information large unlabeled text sources order
enable finer comparison dependency paths work supervised learning
setting evaluation procedure work stre et al erkan et al
la kernel outperforms methods differences data set
much smaller data sets used
la parameters
demonstrated choice la parameters crucial achieving good performance experiments scaling parameter contributes overall performance
parameters gap values taken account well
approaches infinity la kernel approximates smith waterman distance
increasing necessarily positive impact final performance
finding line reported saigo et al homology detection
task best performance yielded setting scaling parameter bit higher
penalizing gap extension less gap opening

conclusions future work
presented novel relation extraction local alignments sequences la kernel provides us opportunity explore
sources information study role relation recognition possible future directions include therefore examination distributional similarity measures studying
impact extraction generic relations looking sources information could helpful relation recognition may interesting consider
relational similarity turney looks correspondence relation
instances case one able infer doctor corresponds scalpel
similar way fisherman net scalpel doctor net fisherman
examples instrument agency
despite sparseness might occur wordnet measures
used measures advantage distributional measures treating elements compared concepts rather words nlp community steps
already taken solve clustering words large corpora aiming
word sense discovery pennacchiotti pantel recently mohammad
thesis investigated compatibility distributional measures ontological ones
corpus statistics thesaurus author introduced distributional profiles
senses defined distance measures even though calculat



fiusing local alignments relation recognition

ing similarity tested generic corpora would certain interest apply
domain specific data
overall local alignment kernels provide flexible means work data sequences
first allow partial match sequences particularly important
dealing text second possible incorporate prior knowledge learning
process preserving kernel validity general la kernels applied
nlp long input data form sequences

acknowledgments
authors wish thank simon carter gerben de vries comments
proofreading three anonymous reviewers highly valuable feedback
acknowledge input adaptive information management aim group
university amsterdam preliminary version work dicussed
nd international conference computational linguistics coling
seventh international tbilisi symposium language logic computation
work carried context virtual laboratory e science project
www vl e nl project supported bsik grant dutch ministry
education culture science oc w part ict innovation program
ministry economic affairs ez

references
airola pyysalo bjorne j pahikkala ginter f salakoski allpaths graph kernel protein protein interaction extraction evaluation crosscorpus learning bmc bioinformatics suppl ii
beamer b bhat chee b fister rozovskaya girju r uiuc
knowledge rich identifying semantic relations nominals
proceedings workshop semantic evaluations semeval prague czech
republic
bedmar samy martinez j l uc classification semantic relations
nominals sequential minimal optimization semeval
budanitsky hirst g evaluating wordnet measures lexical semantic
relatedness computational linguistics
bunescu r c learning information extraction ph thesis department
computer sciences university texas austin
bunescu r c ge r kate r j marcotte e mooney r j ramani k
wong w comparative experiments learning information extractors
proteins interactions artificial intelligence medicine
bunescu r c mooney r j shortest path dependency kernel relation
extraction joint conference human language technology empirical methods
natural language processing hlt emnlp vancouver bc



fikatrenko adriaans van someren

bunescu r c mooney r j b subsequence kernels relation extraction
proceedings th conference neural information processing systems
vancouver bc
bunescu r c mooney r j text mining natural language processing
chap extracting relations text word sequences dependency paths
springer
burges c j c tutorial support vector machines pattern recognition
data mining knowledge discovery
camacho r use background knowledge inductive logic programming
report
cancedda n gaussier e goutte c renders j word sequence kernels
journal machine learning
chang c c lin c j libsvm library support vector machines software
available http www csie ntu edu tw cjlin libsvm
chen f goodman j empirical study smoothing techniques language
modeling acl
clegg b computational linguistic approaches biological text mining ph
thesis university london
cohen w w ravikumar p fienberg comparison string distance
metrics name matching tasks iiweb pp
collins head driven statistical natural language parsing ph
thesis university pennsylvania
collins duffy n convolution kernels natural language advances
neural information processing systems pp mit press
cortes c vapnik v support vector networks machine learning

davidov rappoport classification semantic relationships
nominals pattern clusters proceedings acl hlt pp
dolan w b quirk c brockett c unsupervised construction large paraphrase corpora exploiting massively parallel news sources coling geneva
switzerland
erkan g ozgur radev r semi supervised classification extracting
protein interaction sentences dependency parsing joint conference
empirical methods natural language processing computational natural
language learning pp
fellbaum c wordnet electronic lexical database mit press
firth j r synopsis linguistic theory studies linguistic analysis
philological society oxford reprinted palmer f ed
fundel k kueffner r zimmer r relex relation extraction dependency
parse trees bioinformatics


fiusing local alignments relation recognition

girju r badulescu moldovan automatic discovery part whole relations computational linguistics
girju r nakov p nastase v szpakowicz turney p yuret semeval task classification semantic relations nominals acl
girju r nakov p nastase v szpakowicz turney p yuret classification semantic relations nominals language resources evaluation

giuliano c lavelli pighin romano l fbk irst kernel methods
semantic relation extraction semeval
giuliano c lavelli romano l exploiting shallow linguistic information
relation extraction biomedical literature eacl
grishman r sundheim b message understanding conference brief
history proceedings th international conference computational linguistics
haussler convolution kernels discrete structures tech rep ucs crl
uc santa cruz
hearst automatic acquisition hyponyms large text data proceedings
coling pp
hendrickx morante r sporleder c van den bosch ilk machine
learning semantic relations shallow features almost data semeval
hersch w cohen roberts p rakapalli h k trec genomics
track overview proceedings th text retrieval conference
jiang j j conrath w semantic similarity corpus statistics
lexical taxonomy proceedings international conference
computational linguistics rocling x pp
joachims transductive inference text classification support vector
machines proceedings icml
katrenko adriaans p semantic types generic relation arguments
detection evaluation proceedings th annual meeting association computational linguistics human language technologies acl hlt
columbus usa
khoo c g chan niu extracting causal knowledge medical database graphical patterns proceedings th annual meeting
association computational linguistics pp morristown nj usa
association computational linguistics
kim n baldwin melb kb nominal classifications noun compound
interpretation semeval
lauer f bloch g incorporating prior knowledge support vector machines
classification review neurocomputing


fikatrenko adriaans van someren

leacock c chodorow combining local context wordnet similarity
word sense identification mit press cambridge
lease charniak e parsing biomedical literature proceedings ijcnlp
lee l measures distributional similarity proceedings th annual meeting association computational linguistics computational linguistics
pp
leslie c eskin e cohen weston j noble w mismatch string kernels
discriminative protein classification bioinformatics
leslie c eskin e noble w spectrum kernel string kernel svm
protein classification pacific symposium biocomputing pp
leusch g ueffing n ney h novel string string distance measure
applications machine translation evaluation machine translation summit ix
pp orleans lo
lin information theoretic definition similarity proceedings th
international conference machine learning pp
lodhi h saunders c shawe taylor j christianini n watkins c text
classification string kernels journal machine learning
mcdonald r extracting relations unstructured text tech rep ms cis upenn
melcuk dpendency syntax theory practice suny press
mitchell machine learning mcgraw hill
miyao stre r sagae k matsuzaki tsuji j task oriented evaluation
syntactic parsers representations proceedings acl hlt pp

mohammad measuring semantic distance distributional profiles concepts ph thesis graduate department computer science university
toronto
monge e elkan c field matching applications
kdd pp
moschitti efficient convolution kernels dependency constituent syntactic
trees ecml pp
nakov p ucb system description semeval task semeval
nakov p paraphrasing verbs noun compound interpretation proceedings
workshop multiword expressions mwe conjunction language
resources evaluation conference marrakech morocco
nakov p hearst solving relational similarity web
corpus proceedings acl hlt
nedellec c learning language logic genic interaction extraction challenge
proceedings learning language logic workshop


fiusing local alignments relation recognition

needleman b wunsch c general method applicable search
similarities amino acid sequence two proteins journal molecular biology

nulty p ucd pn classification semantic relations nominals
wordnet web counts semeval
seaghdha semantic classification wordnet kernels proceedings
north american chapter association computational linguistics human
language technologies conference naacl hlt boulder co
seaghdha copestake semantic classification distributional kernels
proceedings coling manchester uk
palmer wu z verb semantics english chinese translation tech rep
technical report ms cis department computer information science
university pennsylvania
pedersen patwardhan michelizzi j wordnet similarity measuring
relatedness concepts proceedings nineteenth national conference
artificial intelligence aaai pp san jose ca
pennacchiotti pantel p ontologizing semantic relations acl proceedings st international conference computational linguistics
th annual meeting association computational linguistics pp
morristown nj usa association computational linguistics
ponzetto p strube knowledge derived wikipedia computing
semantic relatedness journal artificial intelligence
resnik p information content evaluate semantic similarity proceedings
th international joint conference artificial intelligence pp
stre r sagae k tsuji j syntactic features protein protein interaction
extraction nd international symposium languages biology medicine
pp
sagae k tsujii j dependency parsing domain adaptation lr
parser ensembles proceedings emnlp conll
saigo h vert j p akutsu optimizing amino acid substitution matrices
local alignment kernel bmc bioinformatics
saigo h vert j p ueda n akutsu protein homology detection
string alignment kernels bioinformatics
sang e f k canisius van den bosch bogers applying spelling
error correction techniques improving semantic role labeling proceedings
ninth conference natural language learning conll ann arbor mi
saunders c tschach h shawe taylor j syllables string kernel
extensions icml
scholkopf b support vector learning ph thesis berlin technical university



fikatrenko adriaans van someren

sekimizu park h tsujii j identifying interaction genes
gene products frequently seen verbs medline abstracts genome
informatics
shawe taylor j christianini n support vector machines kernelbased learning methods cambridge university press
smith l h yeganova l wilbur w j hidden markov optimized
sequence alignment computational biology chemistry
smith f waterman identification common molecular subsequences
journal molecular biology
snow r jurafsky ng learning named entity hyponyms question
answering proceedings coling acl
swanson r smalheiser n r implicit text linkages medline records
arrowsmith aid scientific discovery library trends
thomas j milward ouzounis c pulman automatic extraction
protein interactions scientific abstracts proceedings pacific symposium
biocomputing
tribble fahlman e cmu semantic distance background knowledge identifying semantic relations semeval
turney p similarity semantic relations computational linguistics

van der plas l automatic lexico semantic acquisition question answering
ph thesis university groningen
van rijsbergen c j robertson e porter f probabilistic
information retrieval tech rep british library development
report
vapnik v estimation dependences empirical data york
springer verlag
weeds j weir mccarthy characterising measures lexical distributional similarity proceedings coling
zelenko aone c richardella kernel methods relation extraction
journal machine learning
zhang schneider j dubrawski learning semantic correlation
alternative way gain unlabeled text proceedings nd conference
neural information processing systems vancouver bc




