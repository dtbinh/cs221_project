Journal Artificial Intelligence Research 38 (2010) 569-631

Submitted 12/09; published 08/10

Cause Identification Aviation Safety Incident Reports
via Weakly Supervised Semantic Lexicon Construction
Muhammad Arshad Ul Abedin
Vincent Ng
Latifur Khan

arshad@student.utdallas.edu
vince@hlt.utdallas.edu
lkhan@utdallas.edu

Department Computer Science
Erik Jonsson School Engineering & Computer Science
University Texas Dallas
800 W. Campbell Road; MS EC31
Richardson, TX 75080 U.S.A.

Abstract
Aviation Safety Reporting System collects voluntarily submitted reports aviation safety incidents facilitate research work aiming reduce incidents. effectively reduce incidents, vital accurately identify incidents occurred.
precisely, given set possible causes, shaping factors, task cause identification involves identifying shaping factors responsible
incidents described report. investigate two approaches cause identification.
approaches exploit information provided semantic lexicon, automatically constructed via Thelen Riloffs Basilisk framework augmented linguistic
algorithmic modifications. first approach labels report using simple heuristic, looks words phrases acquired semantic lexicon learning
process report. second approach recasts cause identification text classification problem, employing supervised transductive text classification algorithms
learn models incident reports labeled shaping factors using models
label unseen reports. experiments show heuristic-based approach
learning-based approach (when given sufficient training data) outperform baseline
system significantly.

1. Introduction
Safety paramount importance comes aviation industry. 2007 alone,
4659 incidents1 , including 26 fatal accidents 750 casualties2 . improve
aviation safety situation, Aviation Safety Reporting System (ASRS) established
1976 make safety incident data available researchers. ASRS collects voluntarily submitted reports aviation safety incidents written flight crews, attendants, controllers
related parties. reports contain number fixed fields free text narrative describing incident. However, data grown quite large years
getting increasingly difficult, impossible, analyze reports human
means. become necessary reports analyzed automated means.
1. http://asrs.arc.nasa.gov/
2. http://www.flightsafety.gov/
c
2010
AI Access Foundation. rights reserved.

fiAbedin, Ng & Khan

take full advantage data reduce safety incidents, necessary extract
reports happened why. known, possible
identify correlations incidents causes, take fruitful measures
toward eliminating causes. However, fixed fields reports devoted various
aspects happened incidents, fixed field indicates
incidents causes. Instead, reporter discusses report narrative thinks
caused incident, along incident description. Thus cause incident
extracted analyzing free text narrative. example, report shown next
illustrate task:
Report#424362. descending lit encountered Instrument Meteorological Conditions; rime ice; rain; moderate chop. turned
heading Auto-Pilot direct lit attitude indicator remained
bank. XCHKING; noticed Radio Magnetic IndicatorS 55 degree
headings. switched #2 corrected course. Auto-Pilot flight
director kicked off. continued problems altitude select
Auto-Pilot attempted re-engage it. radar vectors
approach descent 2300 feet noticed altitude 2000 feet
Mean Sea Level. stopped descent climbed 2300 feet Mean Sea
Level. Air Traffic Control noted altitude deviation time noticed.
thankful backup time flight director problems
cockpit. occurred end 13 hour crew day; bad weather; instrument problems; lack crew rest. First Officer (Pilot Flying)
right seat; 4 hours rest due inability go sleep
night before. tired trip lit-ORL-lit. eaten
7 hours.3
Posse et al. (2005) identify 14 important cause types, shaping factors,
influence occurrence aviation safety incident described ASRS report.
shaping factors contextual factors influenced reporters behavior
incident thus contributed occurrence incident. factors
attributed humans (e.g., pilot flight attendant psychological Pressure,
overly heavy Taskload, unprofessional Attitude impacts performance),
related surrounding environment (e.g., Physical Environment snow,
Communication Environment auditory interference). detailed description
14 shaping factors found Section 2.1.
report, find incident influenced three shaping factors,
namely Physical Environment (which concerns bad weather, mentioned above), Resource
Deficiency (which concerns problems equipment), Duty Cycle (which refers
physical exhaustion due long hours duty without adequate rest replenishment).
three shaping factors indicated different words phrases report.
instance, bad weather condition expressed using phrases rime ice, rain
moderate chop, details equipment problem appear sentence fragments
3. improve readability, report preprocessed original form using steps described
Section 2.2.

570

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

attitude indicator remained bank, 55 degree headings flight director problems.
issue long hours duty illustrated sentence fragments 13 hour
crew day tired trip. goal cause identification task aviation
safety domain, then, identify 14 shaping factors contributed incident
described report using lexical cues appearing report narrative.
However, mentioned earlier, sheer volume data makes prohibitive
analyze reports manually identify associated shaping factors. Thus,
focus research automated cause identification ASRS reports, involves
automatically analyzing report narrative identifying responsible shaping factors.
brings problem domain Natural Language Processing (NLP).
Since set texts (i.e., report narratives) set possible labels
texts (i.e., shaping factors), task naturally cast text classification
task. However, unlike topic-based text classification, cause-based text classification
addressed extensively NLP community. Previous work causal analysis quite
different nature cause-based text classification task. specifically, previous
cause analysis works involve text classification, focusing instead determining
existence causal relation two sentences events. instance,
work causal analysis question answering, question may involve
cause(s) event (e.g., Kaplan & Berry-Rogghe, 1991; Garcia, 1997; Khoo, Chan, & Niu,
2000; Girju, 2003). Here, focus finding causal relationship two sentence
components. another example, causal analysis equipment malfunction reports
attempted Grishman Ksiezyk (1990), whose work restricted analysis
reports related one specific piece equipment studied. analyze cause-effect
relations events leading malfunction described reports.
Cause identification aviation safety reports rather challenging problem,
result number factors specific ASRS dataset. First, unlike many NLP problems
underlying corpus composed set well-edited texts newspaper
reports, reviews, legal medical documents4 , ASRS reports mostly written
informal manner, since edited except removing author-identity
information, reports tend contain spelling grammatical mistakes. Second,
employ large amount domain-specific acronyms, abbreviations terminology. Third,
incident described report may caused one shaping factor.
Thus reports multiple shaping factor labels, making task challenging
binary classification, even multi-class problems instance one
label. all, scarcity labeled data task, coupled highly imbalanced
class distributions, makes difficult acquire accurate classifier via supervised learning.
Previous work cause identification ASRS reports done primarily
researchers NASA (see Posse et al., 2005) and, knowledge, involved manual
analysis reports. Specifically, NASA brought together experts aviation safety,
human factors, linguistics English language participate series brainstorming
sessions, generated collection seed keywords, simple expressions template
expressions related shaping factor. labeled reports shaping
factors looking related expressions report narrative. However,
4. Recently, work started processing blogs, may grammatical either, blogs
typically full domain-specific terminology.

571

fiAbedin, Ng & Khan

major weakness associated approach: involves large amount human effort
identifying relevant keywords expressions, yet resulting list keywords
expressions means exhaustive. Moreover, evaluated approach
20 manually labeled reports. small-scale evaluation means satisfactory
judged current standard NLP research. One contributions research
annotation 1333 ASRS reports shaping factors, serve standard
evaluation dataset different cause identification methods compared.
paper, investigate two alternative approaches cause identification,
exploit information provided automatically constructed semantic lexicon.
specifically, view large amount human involvement NASAs work,
aim replace manual selection seed words bootstrapping approach
automatically constructs semantic lexicon. Specifically, motivated Thelen Riloffs
(2002) Basilisk framework, learn semantic lexicon, consists set words
phrases semantically related shaping factors, follows. Starting small
set seed words phrases, augment seeds iteration automatically
finding fixed number words phrases related seeds corpus adding
seed list. importantly, however, propose four modifications
Basilisk framework potentially improve quality generated lexicon.
first linguistic modification: addition using parse-based features (e.g., subjectverb verb-object features) Basilisk, employ features computed
robustly (e.g., N-grams). remaining three algorithmic modifications
Basilisk framework, involving (1) use probabilistic semantic similarity measure, (2)
use common word pool, (3) enforcement minimum support maximum
generality constraints words extraction patterns, favors addition
frequently-occurring content-bearing words disfavors overly-general extraction patterns.
mentioned above, investigate two approaches cause identification exploit
automatically learned semantic lexicon. first approach heuristic approach,
which, motivated Posse et al. (2005), labels report shaping factor contains
least word phrase relevant shaping factor. Unlike Posse et al.s
work, relevant words phrases employed heuristic procedure
manually identified, automatically acquire words phrases via semisupervised semantic lexicon learning procedure described above. second approach
machine-learning approach somewhat orthogonal NASAs approach: instead
human identify seed words phrases relevant shaping factor,
humans annotate small subset available incident reports shaping
factors, apply machine learning algorithm train classifier automatically
label unseen report, using combinations N-gram features words phrases
automatically acquired aforementioned semantic lexicon learning procedure.
see, acquire cause identifier using Support Vector Machines (SVMs),
shown effective topic-based text classification. Since small
number labeled reports, attempt combine labeled unlabeled reports using
transductive version SVMs.
Since approaches rely simple linguistic knowledge sources involve N-grams
words phrases automatically acquired semantic lexicon learning procedure, one may argue use simple features sufficient cause
572

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

identification. important point means arguing
features sufficient cause identification. However, use simple features
relevant task motivated work performed NASA researchers,
who, mentioned above, manually identified seed words phrases shaping
factor (Posse et al., 2005). semantic lexicon learning procedure precisely aims learn
words phrases. error analysis reveals simple linguistic features
sufficient learning cause identification (and sophisticated knowledge
sources needed improve performance), one first attempts tackle cause
identification task, believe use simple features good starting point
establishes baseline future studies domain-specific problem
compared.
evaluate aforementioned two approaches manually annotated ASRS reports. experiments show number interesting results. First, best performance
achieved using heuristic approach, label report basis presence
automatically acquired lexicon words phrases report, achieving F-measure
50.21%. importantly, method significantly surpasses performance
baseline system, labels report basis presence small set manually
identified seed words phrases. results suggest employing automatically
acquired semantic lexicon relevant useful cause-based text classification
ASRS reports. Second, words phrases learned semantic lexicon, used
features training SVMs classification approach, improve performance
SVM classifier trained solely N-gram based features amount
training data small. However, increase amount training data (by crossvalidation), using lexicon words phrases features addition unigrams
bigrams helps improve classifier performance statistically significantly. particular,
observed F-measure 53.66% SVM classifiers using combination
unigrams, bigrams lexicon words phrases features. results confirm
words phrases learned semantic lexicon relevant valuable
features identifying responsible shaping factors. Nevertheless, magnitude
improvement indicates still much room improvement, may
achieved using deeper semantic features.
summary, believe work automated cause identification makes five
primary contributions:
show instead manually analyzing incident reports identify
relevant shaping factors, possible reduce amount human effort required
task manually analyzing small subset reports identifying
shaping factors rest reports using automated methods.
propose several modifications Thelen Riloffs (2002) semi-supervised lexicon learning framework, show Modified Basilisk framework allows us
acquire semantic lexicon yields significantly better performance cause
identification original Basilisk framework. Equally importantly, none
modifications geared towards cause identification task, hence
applicable generally semantic lexicon learning task. fact, addi573

fiAbedin, Ng & Khan

tional experiments suggest Modified Basilisk yields better accuracy Original
Basilisk bootstrapping general semantic categories.
show semantic lexicon learning useful cause identification ASRS
reports. particular, words phrases learned semantic lexicon
profitably used improve heuristic-based approach learning-based
approach (when given sufficient training data) cause identification. addition,
believe similar cause identification task causes described
text, may useful learn semantic lexicon containing key words
phrases related different types possible causes use key words
phrases features machine learning.
attempt deduce weaknesses approaches help direct future
research, performed analysis errors made best-performing
system, namely heuristic approach using semantic lexicon learned
modified Basilisk method randomly chosen subset test reports.
manually annotated subset reports relevant shaping factors.
set annotated reports, made publicly available, serve
standard evaluation set task future research comparing
approaches cause identification.
rest paper organized follows. Section 2, discuss dataset,
shaping factors, reports preprocessed annotated. Section 3 defines
baseline, simply looks small set manually extracted seed words
phrases report narratives. Section 4, describe semantic lexicon learning
procedure, based Basilisk lexicon learning procedure (Thelen & Riloff,
2002) augmented modifications. Section 5, discuss heuristic-based
learning-based approaches cause identification. evaluate two approaches
Section 6 discuss related work Section 7. Finally, Section 8, summarize
conclusions discuss future work.

2. Dataset
dataset used research aviation safety incident reports publicly available
website Aviation Safety Reporting System5 . used 140,599 reports collected period January 1998 December 2007. report contains
free text narrative written reporter several fixed fields incident
time place incident, environment information, details aircrafts
involved, reporting persons credentials, details anomaly, detector, resolution
consequence incident itself, description situation. words,
fixed fields report contain various information happened,
physical circumstances, cover incident took place. discussed
Posse et al. (2005) Ferryman, Posse, Rosenthal, Srivastava, Statler (2006),
narrative report contains information shaping factors incident.
5. Available http://asrs.arc.nasa.gov/search/database.html

574

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

reason, decided analyze free-text narrative report using NLP techniques identify shaping factor(s) incident may be, constructed
corpus task combining narratives 140,599 reports.
2.1 Shaping Factors
incidents described ASRS reports happen variety reasons. Posse et al.
(2005) focus 14 shaping factors, simply shapers. Following short description
shaping factors, taken verbatim work Posse et al..
1. Attitude: indication unprofessional antagonistic attitude controller
flight crew member.
2. Communication Environment: Interferences communications cockpit
noise, auditory interference, radio frequency congestion, language barrier.
3. Duty Cycle: strong indication unusual working period e.g., long day, flying
late night, exceeding duty time regulations, short inadequate rest
periods.
4. Familiarity: indication lack factual knowledge, new unfamiliar company, airport, aircraft.
5. Illusion: Illusions include bright lights cause something blend in, black hole,
white out, sloping terrain.
6. Physical Environment: Unusual physical conditions could impair flying
make things difficult, unusually hot cold temperatures inside cockpit,
cluttered workspace, visual interference, bad weather, turbulence.
7. Physical Factors: Pilot ailment could impair flying make things difficult, tired, fatigued, drugged, incapacitated, influenced alcohol,
suffering vertigo, illness, dizziness, hypoxia, nausea, loss sight, loss hearing.
8. Preoccupation: preoccupation, distraction, division attention creates
deficit performance, preoccupied, busy (doing something else),
distracted.
9. Pressure: Psychological pressure, feeling intimidated, pressured, pressed
time, low fuel.
10. Proficiency: general deficit capabilities, inexperience, lack training,
qualified, current, lack proficiency.
11. Resource Deficiency: Absence, insufficient number, poor quality resource,
overworked unavailable controller, insufficient out-of-date chart, equipment malfunction, inoperative, deferred, missing equipment.
575

fiAbedin, Ng & Khan

12. Taskload: Indicators heavy workload many tasks once, shorthanded crew.
13. Unexpected: Something sudden surprising expected.
14. Other: Anything else could shaper, shift change, passenger discomfort, disorientation.
2.2 Preprocessing
semantic lexicon learning approach cause identification, need identify
(1) part-of-speech (POS) word text, (2) phrases chunks
sentences, (3) grammatical roles words governing words. Ideally,
achieve high accuracies three tagging tasks, would manually annotate section
ASRS corpus appropriate annotations (e.g., POS tags, chunks) train
appropriate taggers tag rest corpus. However, laborintensive task, beyond scope paper. Therefore, used publicly
available tools trained standard corpora three tasks. inevitable
produce accurate automatic annotations corpus, see,
caused problem task.
corpus, first identify sentence boundaries using tool MXTERMINATOR6 . Second, run POS tagger CRFTagger (Phan, 2006b), uses Penn
Treebank tag set (Marcus, Santorini, & Marcinkiewicz, 1993), sentences detected
MXTERMINATOR. Third, run chunker CRFChunker (Phan, 2006a) tagged
text identify different types phrases. Also, Minipar parser (Lin, 1998) run
sentences identify grammatical roles words. However, report text
preprocessed applying tools reasons described following paragraphs.
reports ASRS data set usually informally written, using various domain
specific abbreviations acronyms. general, observed van Delden Gomez
(2004), Posse et al. (2005) Ferryman et al. (2006), narratives tend written
short, abbreviated manner, tend contain poor grammar. Also, text
converted upper-case. Following example narrative report:
TAXIING RAMP LAF NIGHT. MADE WRONG TURN
CROSSED RWY 10/28; ACTIVE TIME.
SIGN INDICATE RWY XING. CLRED DIRECTIONS XING. ACFT FIELD
TIME. MENTION ATIS SIGNS
CONSTRUCTION RAMP AREA. CTLR DIDNT QUESTION
US; BROUGHT SIT CROSSED
ACTIVE RWY. COMMUTER OPS 3 DAYS HVY FLYING;
REDUCED REST; RWY SIGNS BUSY LAST MIN COMMUTER PAPER WORK CHANGES; CONTRIBUTED RWY
INCURSION. 12 HR DAY 6 HR FLT TIME.
6. ftp://ftp.cis.upenn.edu/pub/adwait/jmx/, trained Wall Street Journal corpus

576

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

reports need preprocessing NLP techniques applied them,
since off-the-shelf tools (e.g., POS tagger) trained mixed-case texts.
example, running CRFTagger (which trained WSJ corpus correct cases)
first two sentences yield following:
1. TAXIING/NNP FROM/NNP THE/DT RAMP/NNP AT/IN LAF/NNP AT/IN
NIGHT/NN ./.
2. MADE/NNP A/DT WRONG/NNP TURN/NNP AND/CC CROSSED/VBD
RWY/NNP 10/28/CD ;/: THE/DT ACTIVE/NNP AT/IN THE/DT TIME/NN ./.
seen, tagger mislabels words TAXIING, FROM, MADE, WRONG
ACTIVE proper nouns (NNP), instead tagging verb, preposition, verb,
adjective adjective respectively. occurs good feature detecting proper
nouns sentence case first character. Since words begin capital
letter, tagger mistakes significant portion words NNP. Another reason
tagger performs poorly corpus lot abbreviations appear text.
example, XING HVY short crossing heavy. since
likely known POS tagger trained standard well-edited corpus, would
identified unknown words, likely tagged nouns instead verb
adjective respectively. Similar problems observed parsers chunkers.
reason, decided preprocess text expanding abbreviations
restoring cases words.
expand acronyms abbreviations, rely official list acronyms
abbreviations used ASRS reports7 . small number cases, abbreviation
acronym may one expansion. example, ARR may mean either arrival
arrive. cases arbitrarily chose one possibilities8 . Then, restore case,
set English word lists, place names person names9 applied text
identify known words. word report text appeared word lists,
converted lower case. unknown words left uppercase. result
process aforementioned narrative follows:
TAXIING ramp LAF night. made wrong turn crossed
runway 10/28; active time. sign indicate
runway crossing. cleared directions crossing.
aircraft field time. mention Automatic Terminal
Information Service signs construction ramp area.
controller DIDNt question us; brought situation
crossed active runway. commuter operations 3 days heavy flying;
7. See http://akama.arc.nasa.gov/ASRSDBOnline/pdf/ASRS_Decode.pdf.
8. better option would disambiguate alternative expansions based context (e.g.,
method followed Banko & Brill, 2001). However, number ambiguities acronyms
abbreviations list small (10, exact), either POS variations
word. Thus effect ambiguities performance NLP tools expected
minimal.
9. http://wordlist.sourceforge.net/

577

fiAbedin, Ng & Khan

reduced rest; runway signs busy last minute commuter paper work
changes; contributed runway incursion. 12 hour day 6 hour flight time.
ran POS tagger, CRFTagger, processed text observe
errors. example, tagged version aforementioned two sentences are:
1. TAXIING/VBG from/IN the/DT ramp/NN at/IN LAF/NNP at/IN night/NN ./.
2. made/VBN a/DT wrong/JJ turn/NN and/CC crossed/VBD runway/NN 10/28/CD
;/: the/DT active/JJ at/IN the/DT time/NN ./.
sentences correctly tagged. However, case restoration method
arguably simplistic. Hence, determine need perform fine-grained case
restoration, sought measure much would gain accurately restoring
case words sentences present heuristic method. check this,
randomly picked 100 sentences corpus. first ran POS tagger
sentences case-restored aforementioned heuristic case restoration
method. Then, manually corrected capitalization sentences re-ran
POS tagger case-restored sentences. tags thus generated compared,
found 99.7% agreement, means likely gain much terms
POS tagging accuracy correctly case restored text heuristically case restored
text. five differences 2049 words, three NNPs mislabeled NNs,
essentially effect outcomes research. Therefore, marginal utility
applying sophisticated case restoration methods seem enough justify
additional effort necessary, limit preprocessing step expansion abbreviations acronyms followed heuristic case restoration procedure described above.
complete flow preprocessing shown Figure 1.
2.3 Human Annotation Procedure
Recall need reports labeled shaping factors training cause identification classifiers testing performance two approaches cause identification.
Additionally, order learn semantic lexicon via bootstrapping, need small set
seed words phrases related shaping factor starting point. result,
performing language normalization, performed two types annotations: (1) labeling
set reports shaping factors, (2) identifying set seed words phrases
reports. annotation procedure described detail following sections.
2.3.1 Annotating Reports Shaping Factors
NASA previously developed heuristic approach tackle cause identification
task (Posse et al., 2005), approach evaluated 20 manually annotated reports,
far satisfactory far establishing strong baseline method concerned.
Thus decided annotate set reports evaluating automatic cause
identification methods.
complete set 140,599 reports, chose random set 1333 reports
annotation. subset divided two parts. first part, consisting 233 reports,
578

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Figure 1: Flow chart text preprocessing
annotated two persons (one undergraduate student one graduate student).
report, asked answer following question:
shaping factor(s) responsible incident described report?
annotators trained similar way labeled 20 reports used
evaluation NASA researchers (see Posse et al., 2005). Specifically, background
reading, annotators referred works Posse et al. Ferryman et al. (2006),
describe shaping factors, give examples words
phrases indicate influence shaping factors described incidents.
definitions shapers repeated Section 2.1. Following Posse et al.s method,
annotators explicitly instructed adhere definitions much possible
annotating reports shaping factors. annotations completed,
inter-annotator agreement computed using Krippendorffs (2004) statistics
described Artstein Poesio (2008), using Measuring Agreement Set-valued
Items (MASI) scoring metric (Passonneau, 2004). observed inter-annotator agreement,
, case found 0.72, indicates reliable agreement. 233
reports, completely agreed annotations 80 reports, completely disagreed
100 reports partially agreed 53 reports. annotators asked discuss
discrepancies. discussion, found discrepancies could
579

fiAbedin, Ng & Khan

primarily attributed vagueness descriptions shaping factors Posse et
al.s paper, interpreted differently two annotators.
annotators agreed descriptions shapers interpreted,
resolved differences annotation. discussion, remaining 1100
reports annotated one annotators. annotator asked
annotate subset reports (100 reports) cross-verification purpose10 ,
inter-annotator agreement, , case observed 0.66. 1333 reports
annotated first annotator divided three sets: training set (233 reports)
training cause identification classifiers, held-out development set (100 reports)
parameter tuning, test set (1000 reports) evaluating performance
approaches cause identification. distribution shaping factors training,
development test sets shown second, third fourth columns Table 1.
2.3.2 Extracting Seed Words Phrases
separate process, first author went first 233 reports annotators
worked on, selected words phrases relevant shaping factors.
judgment whether word phrase relevant shaping factor based careful
reading description shaping factors works Posse et al. (2005)
Ferryman et al. (2006), well example seed words selected NASA experts
shown two papers. specific task case was:
report, word phrase indicative
shaping factors? is, identify assign appropriate
shaping factor.
Note seed words phrases chosen without regard shaping factor
annotation document; picked possibility relevant
respective shaping factors. number seed words phrases shaping
factor shown last column Table 1. see, 177 seed words phrases
manually selected 233 training reports. completeness, show
seed words phrases extracted reports Appendix A. facilitate
research topic, annotated data used research made available
http://www.utdallas.edu/~maa056000/asrs.html.
Since gold standard compare list annotated
words phrases, difficult directly compute precision. However, get rough
idea precision, asked one annotators examine list identify
words phrases list believes correct. disagreement
one word. yields precision 99.44%, provides suggestive evidence
annotation fairly reliable. manually identified words phrases
used baseline cause identification system (see Section 3) served seeds
semantic lexicon learning procedure (see Section 4).
10. fairly standard procedure NLP research cross-annotate subset data
complexity cost individual annotation high. See works Zaidan, Eisner, Piatko (2007)
Kersey, Di Eugenio, Jordan, Katz (2009), instance.

580

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Table 1: Distribution shaping factors training, test development sets
Shaping factor
Reports Reports
Reports
Seed
training set
test set development words
test set
Attitude
17
30
5
8
Communication Environment
11
90
18
5
Duty Cycle
9
26
3
10
Familiarity
12
50
8
9
Illusion
1
2
0
1

36
217
36
8
Physical Environment
43
265
40
45
Physical Factors
10
35
3
8
Preoccupation
25
110
10
9
Pressure
5
30
3
10
Proficiency
43
247
23
12
Resource Deficiency
112
507
33
47
Taskload
6
29
7
2
Unexpected
3
10
1
3
Total
233
1000
100
177

3. Baseline System Cause Identification
discussed introduction, goal research label incident reports
shaping factors caused incidents. evaluate performance cause
identification methods, need baseline uses amount training data
methods described paper performs reasonably well test set.
Given cause identification relatively new under-investigated task, standard
baseline adopted task. fact, knowledge, related works
cause identification aviation safety domain conducted researchers
NASA (see Posse et al., 2005; Ferryman et al., 2006). result, construct baseline
system motivated Posse et al.s work. Specifically, baseline takes input set
seed words phrases manually collected shaping factors (see Section 2.3.2),
labels report Occurrence Heuristic: seed word phrase found
report, baseline annotates report shaping factor associated
seed. example, 11 hour duty day seed phrase associated shaping
factor Duty Cycle. Then, Occurrence Heuristic label report contains
phrase 11 hour duty daywith Duty Cycle. approach simple attractive
(1) need training, (2) evaluated easily, searching
seed words narrative report labeled, (3) report potentially
labeled one shaping factors. seed words phrases indeed
relevant respective shaping factors, identify reports related
shaping factors high degree precision.
581

fiAbedin, Ng & Khan

4. Semantic Lexicon Learning
described Section 3, baseline uses seed words phrases manually extracted
233 reports combination Occurrence Heuristic label reports
shaping factors. However, reports used evaluation may contain exactly
words phrases, may contain different variations, synonyms, words
phrases semantically similar seed words phrases. Thus baseline
may able label reports correctly looking words phrases
seed words list.
address potential problem, propose use semantic lexicon learning algorithms learn words phrases semantically similar seed words phrases
reports corpus containing narratives 140,599 reports. Using weakly supervised bootstrapping algorithm may allow us learn large number useful words
phrases corpus would required huge amounts human effort
done manually. Below, first describe general bootstrapping approach Section 4.1.
Then, Section 4.2, describe Basilisk framework learning semantic lexicon
unannotated corpus (Thelen & Riloff, 2002). Finally, Section 4.3, discuss
modifications Basilisk framework.
4.1 Weakly Supervised Lexicon Learning
mentioned earlier, employ weakly supervised bootstrapping approach building
semantic lexicon. use manually extracted seed words phrases
shaping factor (described Section 2.3.2) create initial semantic lexicon.
select words phrases unannotated reports semantically similar
words already appearing semantic lexicon. reports corpus need
labeled shaping factors. semantic similarity two words measured
using features extracted corpus word. process repeated iteratively:
iteration, certain number words added semantic lexicon,
words augmented lexicon used seeds following iteration.
process shown Figure 2.

Figure 2: Flow chart lexicon learning procedure

582

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

4.2 Basilisk Framework
Basilisk (Bootstrapping Approach SemantIc Lexicon Induction using Semantic Knowledge)
instantiation aforementioned generic semantic lexicon learning framework (Thelen & Riloff, 2002). Basilisk framework works first identifying patterns
extracting noun phrases corpus appear one three syntactic roles:
subject, direct object, prepositional phrase object. example, discussed Thelen Riloff, sentence John arrested collaborated Smith
murdered Brown, extraction patterns <subject> arrested, extracts
John, murdered <object> extracts Brown collaborated <pp object>
extracts Smith. Then, semantic category Sk , pattern pool constructed
patterns tend extract words Sk . measure tendency pattern Pj
extract words Sk , R log F metric used, defined as:
R log F (Pj ) =

Fj
log (Fj )
Nj

(1)

Here, Fj number (distinct) words Sk pattern Pj extracts, Nj
total number (distinct) words corpus Pj extracts. metric high
high precision patterns (i.e., patterns extract primarily words Sk ) high recall
patterns (i.e., patterns extract large number words Sk ). iteration i,
top (20 + i) patterns (in terms R log F scores) put pattern pool Sk .
Depleted patterns (i.e., patterns extracted words already semantic
lexicon) considered step. Then, head nouns phrases extracted
resulting patterns pattern pool put word pool Sk .
Next, subset words word pool selected added seed words
list. words word pool chosen relevant Sk .
specifically, word Wi word pool Sk , first AvgLog score calculated,
defined follows:

AvgLog (Wi , Sk ) =

W
Pi
X

log2 (Fj + 1)

j=1

W Pi

(2)

Here, W Pi number patterns extract word Wi , pattern Pj
extracts Wi , Fj number words extracted Pj belong Sk . Then,
semantic category Sk , five words chosen highest AvgLog score
category Sk .
multi-category learning, Thelen Riloff (2002) experimented different scoring metrics reported achieved best performance calculating diff
score word. given word word pool semantic category, diff
score takes consideration score word gets categories, returns
score based words score semantic category relative categories.
precisely, diff score defined follows:
dif f (Wi , Sk ) = AvgLog (Wi , Sk ) max (AvgLog (Wi , Sl ))
l6=k

583

(3)

fiAbedin, Ng & Khan

Here, Sk semantic category Wi evaluated. Thus diff score
high strong evidence Wi belongs semantic category Sk little evidence
belongs semantic categories. semantic category, diff score
calculated word categorys word pool, top five words
highest diff score added lexicon category. Two additional checks
made stage: (1) word word pool added category
earlier iteration, word discarded, (2) word found
one word pool added category highest score11 .
completed semantic categories, iteration ends, next iteration
begins augmented lexicon.
4.3 Modifications Basilisk Framework
see later subsection, analysis framework reveals
cases words selected Basilisk may relevant ones. reason,
propose three algorithmic modifications Basilisk framework: (1) using new semantic
similarity measure, (2) merging word pools one single pool assigning words
semantic categories, (3) imposing minimum support maximum generality criteria
patterns words added pattern pools word pools. addition, propose
one linguistic modification, employ type feature computed
robust manner words phrases corpus, namely, N-gram features.
rest subsection discusses modifications.
4.3.1 Modification 1: New Semantic Similarity Measure
seen Section 4.2, Basilisk framework uses AvgLog scoring function measure
semantic similarity words. diff score multi-category learning uses
AvgLog function compute evidence word belonging semantic category
relative categories. However, closer examination AvgLog function shows
may able properly predict semantic similarity circumstances.
understand reason, let us first make following observations: pattern Pj occurs
1000 times, extracts words category Sk 5 times, unlikely Pj strongly
related Sk . Similarly, word Wi occurs 1000 times, extracted pattern Pj 5
times, Pj small influence classification Wi . However, AvgLog score
able take factors consideration, precisely considers
absolute number semantic category members extracted patterns extract
word frequency extraction. see case, let us consider
word Wi extracted three patterns P1 , P2 P3 , frequencies shown
Table 2. P1 , P2 P3 extract five distinct seed words, AvgLog score
word W would 2.32, irrespective fact patterns actually extract word
seed words list tiny fraction occurrence corpus. P1 extracts
seed word 5% occurrence, P2 1% time, P3 , pattern extracts W
often, extracts lexicon word 0.5% times appears text. Clearly,
11. approach effectively assumes word belong one category. reasonable
assumption specific task since shaping factors distinct meanings.

584

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

patterns would suggest Wi related semantic category, yet gets
good score.
Table 2: Illustration problem AvgLog: unrelated words may high
similarity score. Wi word appears corpus extracted
patterns P1 , P2 P3

Patterns extract Wi
Number times Wi extracted pattern Pj
Number times pattern Pj occurs text
Number times word category Sk extracted pattern Pj
Number category words extracted pattern Pj
log2 (Fj + 1)
AvgLog (Wi )

P1
10
100
5
5
2.32

P2
P3
20
70
500 1000
5
5
5
5
2.32 2.32
2.32

Keeping mind, propose probabilistic metric, SemProb, computes
probability word Wi belongs semantic category Sk given extracted
patterns P1 , P2 , . . . , Pn . specifically, SemProb calculated follows:
SemP rob (Wi , Sk ) = P rob (Sk |Wi )
X
=
P rob (Sk |Pj ) P rob (Pj |Wi )

(4)

Pj

words, SemProb assumes semantic category Sk word Wi
conditionally independent given Pj , pattern extracts Wi . probabilities
equation estimated using maximum likelihood estimation corpus. Specifically,
compute P rob (Pj |Wi ), divide number times Pj extracts Wi corpus
total number times Wi appears corpus. compute P rob (Sk |Pj ), divide
number times Pj extracts word semantic category Sk total number
times Pj appears corpus. given word Wi given semantic category
Sk , sum products two quantities patterns extract Wi
gives probability category Sk given word Wi . method suffer
problem faced AvgLog since depends probability word extracted
patterns patterns probability extracting words category.
example Table 2, SemProb metric word Wi 0.0105, illustrating
low probability Wi belonging semantic category Sk is. details given
Table 3.
4.3.2 Modification 2: Common Word Pool
Since compute Eqn (4) every word word pool categories
assign word semantic category probability highest, change
framework one common word pool semantic categories.
585

fiAbedin, Ng & Khan

Table 3: Illustration effectiveness SemProb: unrelated words get low similarity
score.

Patterns extract Wi
Number times Wi extracted pattern Pj
Number times pattern Pj occurs text
Number times word category Sk extracted pattern Pj
P rob (Wi extracted Pj )
P rob (Pj extracts word Sk )
P rob (Wi extracted Pj ) P rob (Pj extracts word Sk )
SemP rob (Wi , Sk ) = P rob (Wi belongs semantic category Sk )

P1
10
100
5
0.1
0.05
0.005

P2
P3
20
70
500
1000
5
5
0.2
0.7
0.01
0.005
0.002 0.0035
0.0105

still separate pattern pools different semantic categories, words related
patterns pattern pools put common word pool, allocated
probable semantic category there. separate word pools
semantic category, add fixed number words category
iterations. constraint may undesirably cause word added category
likely. However, since one word pool modification,
constraint add fixed number words category,
assign word likely category. Thus number words added
different categories may vary iteration.
4.3.3 Modification 3: Minimum Support Maximum Generality
scenarios SemProb metric produce undesirable results.
example, consider infrequent word Wi occurs entire corpus exactly once.
Assume pattern Pj , extracts Wi , extracts words semantic category Sk
70% probability. So, according SemProb, probability Wi belongs Sk becomes
70%. However, sufficient evidence Wi belongs Sk . cases
uncommon, imposed minimum word frequency constraint words
put word pool, words appear less certain number times
considered. pattern appears infrequently corpus lead
problem. Consider infrequent pattern, Pj , appears exactly twice corpus
extracts two words. one words happen seed word,
word 50% probability belong category seed word Pj
R log F value 0.5. However, since Pj infrequent, convey good evidence
membership semantic category, allow Pj put words
word pool. Therefore, disallow low frequency patterns included
pattern pool adding constraint patterns put pattern pool must
minimum pattern frequency. Besides two constraints imposed frequency
occurrence words patterns, employ two additional constraints. first
586

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

maximum pattern generality constraint: motivated Rychly Kilgarriff (2007),
remove consideration patterns general (i.e., patterns extract
many words), imposing upper limit number distinct words pattern
added pattern pool extract. second maximum word frequency
constraint: since content-bearing words likely lower frequency (see Davidov
& Rappoport, 2006), impose upper limit maximum number times word
appears corpus. four thresholds associated four frequency-based
constraints tuned automatically using held-out development set.
4.3.4 Modification 4: N-gram Patterns
addition parse-tree-based subject-verb verb-object patterns already employed
Basilisk, employ N-gram-based extraction patterns, goal robustly capturing context words appear. construct N-gram extraction
patterns follows. noun adjective, X, corpus, create two N-gram
patterns extracting X: (a) preceding N words + hXi, (b) hXi + succeeding
N words. example, sentence ... solid line thunderstorms detected ...,
bigram patterns thunderstorms would be: line hXi hXi detected.
complete sentence approaching ATL area solid line thunderstorms
detected vicinity airport, words extracting bigram patterns
would be:
ATL: approaching hXi, hXi area
area: ATL hXi, hXi solid
solid: area hXi, hXi line
line: solid hXi, hXi thunderstorms
thunderstorms: line hXi, hXi detected
vicinity: hXi, hXi
airport: hXi
addition constructing N-gram patterns extracting words, construct
N-gram patterns extracting phrases. so, first remove articles (a, an, the)
possessive pronouns adjectives (e.g., my, his) beginning phrases
corpus. noun phrase adjective phrase, X, appears corpus,
create two N-gram patterns extracting X: (a) preceding N words + hXi, (b)
hXi + succeeding N words. example, sentence last 5 legs
approaching end 8 hour duty day 7 hour hard time flying day, would
extract following phrases following bigram patterns:
5 legs: last hXi, hXi approaching
end: approaching hXi, hXi
587

fiAbedin, Ng & Khan

8 hour duty day: end hXi, hXi 7
7 hour hard time flying day: day hXi
Thus use three types patterns experiments: bigram patterns extracting
words, bigram patterns extracting phrases, parse-tree-based subject-verb verbobject patterns. patterns generated reports corpus generated
combining narratives 140,599 unlabeled reports described Section 2.2.
see, three types patterns beneficial use far performance
concerned. Section 6, show automatically select best subset
patterns use based development set.

5. Semantic Lexicon-Based Approaches Cause Identification
ASRS Reports
investigate heuristic-based approach learning-based approach cause identification, exploit information provided automatically acquired semantic
lexicon. section describes details two approaches.
5.1 Heuristic-Based Approach
heuristic-based approach operates essentially way baseline cause
identification system described Section 3, Occurrence Heuristic used label
report shaping factors. difference words phrases used
Occurrence Heuristic baseline manually identified, whereas
heuristic-based approach acquired Modified Basilisk procedure.
5.2 Learning-Based Approach
learning-based approach cause identification problem recast classification task. Note multi-class multi-labeled classification task: 14
classes report labeled one class. number approaches
proposed tackle multi-class multi-labeled classification tasks. rest
section, describe three existing approaches multi-class multi-labeled text classification explore experiments (Section 5.2.1), provide overview
theory Support Vector Machines (SVMs), underlying learning algorithm use
train classifiers employed three approaches (Section 5.2.2).
5.2.1 Three Approaches Multi-Class Multi-Labeled Text Classification
One-Versus-All. approach, train one binary classifier shaping factor
Sk determine whether report labeled Sk . specifically, follow
One-Versus-All classification scheme: given Sk , reports training set
contains Sk set labels (assigned annotator) positive instances
binary classifier rest reports training set negative instances.
training, apply classifiers report test set independently
reports, label report Sk corresponding classifier classifies
588

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

report positive. Thus convert cause identification multi-class multi-labeled
document classification task.
learning algorithm used principle train classifiers OneVersus-All scheme, use Support Vector Machines12 training testing classifiers,
primarily due successes various text classification tasks. classifier trained
two types features: (1) unigrams bigrams report narratives, (2)
words phrases semantic lexicon. feature values TF*IDF values.
shaping factor-labeled data set 1333 reports substantially larger
set 20 reports annotated NASA researchers (see Section 1), arguably fairly
small machine learning perspective. Hence, conceivable performance
SVM classifiers would limited small size training data. result,
investigate whether improve One-Versus-All approach using transductive
SVM, version inductive SVM described attempts improve
classifier performance combining labeled unlabeled data (see Section 5.2.2
overview transductive learning). cause identification task, unlabeled
reports test set serve unlabeled data transductive learning procedure.
MetaLabeler. second approach, employ MetaLabeler (Tang, Rajan, & Narayanan,
2009) classifying multi-class multi-labeled text data. Here, model first learned
predicts number labels instance may have. addition, set binary classifier models, one possible label, learned predict likelihood label
instance. instance classified, first model predicts K, number
possible labels instance, output second set classifiers, K
labels chosen highest likelihood instance.
implementation approach, first model learned using SVMmulticlass ,
implementation multi-class SVM described Crammer Singer (2002)13 .
second set classifiers set described Section 5.2.2. case,
given instance x, decision functions f (x) = w x b classifiers
evaluated, positive decision values sorted. top K labels corresponding
highest values decision functions assigned instance.
multiclass classifier set binary classifiers trained using types
features One-Versus-All approach, namely unigrams bigrams reports,
words phrases semantic lexicon. feature values
One-Versus-All approach, namely TF*IDF values.
Ensembles Pruned Sets. Pruned Sets approach (Read, Pfahringer, & Holmes,
2008), multi-class multi-label text classification problem transformed multiclass single-label text classification problem selecting subset label combinations
frequently occurring dataset assigning unique pseudo-label chosen
label combination.
first step algorithm choose label sets training. step,
label sets chosen meet minimum frequency requirement training
set. Using minimum frequency constraint prunes away infrequently occurring label sets
frequency less p, leaving label combinations frequent thus
12. implemented SVMlight software package Joachims (1999)
13. Available http://svmlight.joachims.org/svm_multiclass.html

589

fiAbedin, Ng & Khan

important. training instances labeled pruned label sets
removed training set. minimum cardinality parameter, b, used
reintroduce pruned instances back training set order minimize
information loss pruning process. First label sets rejected instances
broken smaller subsets least size b. new subsets
frequency higher p reintroduced, pruned training instances whose label
sets supersets newly accepted label sets reinstated training set.
role parameter b case ensure many instances small
label sets put back, cause average number labels reduce,
resulting smaller number labels per instance classification time.
next step learn classifiers selected label sets. First, accepted label
set assigned unique pseudo-label, thus transforming multi-label classification problem single-label classification problem. ensemble classifiers learned
predict pseudo-labels given instance (using multi-class SVM implementation MetaLabeler), classifier ensemble trained different
random sample training data. Since (1) label sets training classifiers
represent subset label combinations present original training data
(2) test data may contain label combinations present training
data, ensemble classifiers allows system generate label combinations
observed training time. example, let label combinations {l1 , l3 } {l2 , l3 }
present training data. Then, one classifier ensemble labels test instance
{l1 , l3 } another classifier ensemble labels instance {l2 , l3 },
instance may labeled {l1 , l2 , l3 } (depending actual voting policy
effect classification time) even combination present training data.
classifiers ensemble built using two types features One-VersusAll approach, namely unigrams bigrams reports words phrases
semantic lexicon learned modified Basilisk framework.
Finally, classifying instance, classifiers assigns one pseudo-label
instance. pseudo-labels mapped back original label combination
vote actual label counted normalized dividing number
classifiers, , order bring prediction possible label range
0.0 1.0. threshold used label prediction value
greater equal assigned instance. scheme used make possible
assign label combinations unseen training time test instances.
5.2.2 Overview Support Vector Machines
SVMs shown effective text classification (Joachims, 1999).
describe two versions SVMs: (1) inductive SVMs, learn classifier solely
labeled data, (2) transductive SVMs, learn classifier labeled
unlabeled data.
Inductive SVMs. Given training set consisting data points belonging two classes,
inductive SVM aims find separating hyperplane maximizes distance
separating hyperplane nearest data points. nearest data points act
support vectors plane.
590

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

formally, let data set data points
= {(xi , ci ) |xi Rn , ci {1, 1} , 1 m}

(5)

point xi represented n-dimensional vector associated class label
ci . inductive SVM classifier attempts find hyperplane w x b = 0
maximum distance nearest data points opposite labels. hyperplane would
middle two hyperplanes containing support vectors class.
2
. Therefore,
two hyperplanes wxb = 1 wxb = 1, distance |w|
desired separating hyperplane found solving following quadratic programming
optimization problem:
Minimize
subject

1
|w|2
2
ci (w xi b) 1, 1

(6)

However, practice many classes linearly separable. handle cases, set
slack variables used represent misclassification point xi . problem
becomes:
X
1
|w|2 + C

Minimize
2


subject

ci (w xi b) 1 , > 0, 1

(7)

additional variables representing training errors C constant representing trade-off training error margin. details found Cortes
Vapnik (1995). experiments, use radial basis function (RBF)
kernel,



2
every dot product replaced function k (x, x ) = exp |x, x | , > 0.
addition, C chosen cross-validation training set.
Transductive SVMs. transductive setting, addition set labeled data
points, exploit set unlabeled data points, = {xi |xi Rn , 1 k},
taken test set. described Joachims (1999), goal minimize
expected number classification errors test set. expected error rate
defined Vapnik (1998) follows:
Z
1X
(8)
(hL (xi ) , ci ) dP (x1 , c1 ) . . . dP (xk , ck )
R (L) =
k


L = , hL hypothesis learned L, (a, b) zero = b
one otherwise. labeling ci test data hyperplane maximizes
separations training testing positive negative instances found solving
following quadratic programming optimization problem, modified version
Eqn (7):
X
X
1
j
|w|2 + C
+ C
Minimize
2


subject

j

ci (w xi b) 1 , > 0, 1

cj w xj b 1 j , j > 0, 1 j k
591

(9)

fiAbedin, Ng & Khan

Similar inductive SVM Section 5.2.2, use RBF kernel experiments
involving transductive SVM.

6. Evaluation
goal evaluation study effectiveness two approaches cause identification, namely semantic lexicon learning approach classification approach.
testing performance approaches randomly chosen set reports
manually annotated shaping factors caused incidents described (Section 2.3.1). start describing experimental setup (Section 6.1),
followed baseline results (Section 6.2) performance two approaches
(Sections 6.3 6.4). describe experiment increase amount
training data available classification approach investigate impacts
performance (Section 6.5). that, perform analysis errors bestperforming approach (Section 6.6) conduct additional experiments attempt
gain better insight cause identification task help direct future research
(Section 6.7). Finally, present summary major conclusions draw
experiments (Section 6.8).
6.1 Experimental Setup
described Section 2.3, 140,599 reports entire corpus, manually
annotated 1333 incident reports shaping factors. used first 233
(1) manually extract initial seed words phrases semantic lexicon
learning procedure, (2) train classifiers identifying shaping factors associated
report. remaining reports, used 1000 reports test data 100 reports
development data (for parameter tuning).
6.1.1 Evaluation Metrics
mentioned Section 2.1, 14 shaping factors, report may labeled
one shaping factors. evaluate performance cause
identification approaches based well automatic annotations match human
annotations reports test set. evaluation, use precision, recall
F-measure, computed described Sebastiani (2002). Specifically,
shaping factor Si , = 1, 2, . . . 14, let ni number reports test set
human annotator labeled Si , i.e., number true Si -labeled reports test
set. Further, let pi number reports automatic labeling scheme Ci
labeled Si , let tpi number reports Ci labeled correctly Si .
Then, shaping factor Si , following performance metrics:
Precisioni fraction reports really caused shaping factor Si among
reports labeled Si labeling scheme.
P recisioni =

592

tpi
pi

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Recalli percentage reports really caused shaping factor Si labeled
labeling scheme shaping factor Si .
Recalli =

tpi
ni

Thus obtain measure labeling schemes performance shaping
factors. obtain overall performance labeling scheme, sum counts
(i.e., ni , pi tpi ) shaping factors compute micro-averaged precision,
recall F-measure aggregated counts described Sebastiani repeated
follows:
P
tpi
P recision = Pi
pi
Pi
tpi
Recall = Pi
ni
2 P recision Recall
F -measure =
P recision + Recall
Thus labeling scheme one set overall scores reflecting performance
classes.
6.1.2 Statistical Significance Tests
determine whether labeling scheme better another, apply two statistical
significance tests McNemars test (Everitt, 1977; Dietterich, 1998) stratified approximate randomization test (Noreen, 1989) test whether difference performances really statistically significant. McNemars test compares two labeling schemes
basis errors (i.e., whether labeling schemes making mistakes), stratified approximate randomization test compares labeling schemes
F-measure. tests extensively used machine learning NLP literature. particular, stratified approximate randomization standard significance test
employed organizers Message Understanding Conferences determine
difference F-measure scores achieved two information extraction systems significant (see Chinchor, 1992; Chinchor, Hirschman, & Lewis, 1993). Since ultimately
concerned difference F-measure scores two labeling schemes cause
identification, discussion statistical significance rest section focused solely stratified approximate randomization test. tests, determine
significance level p < 0.05.
6.2 Baseline System
Recall use baseline heuristic method described Section 3,
Occurrence Heuristic used label report using seed words phrases manually
extracted 233 training reports. Results, shown Experiment 1 section
Table 4, reported terms precision (P), recall (R), F-measure (F). last
two columns show whether particular automatic labeling scheme significantly better
593

fiAbedin, Ng & Khan

baseline respect McNemars test (MN) stratified approximate randomization test (AR) [Statistical significance insignificance denoted X
X, respectively]. evaluated 1000 reports test set, baseline achieves
precision 56.48%, recall 40.47% F-measure 47.15%.
Table 4: Report labeling performance different methods.
Approach Feature Set
P
R
F MN
AR
Experiment 1: Baseline
Heuristic Seed words
56.48 40.47 47.15 N/A N/A
Experiment 2: Semantic lexicon approach
Lexicon modified Basilisk
53.15 47.57 50.21
X
X
Heuristic
Lexicon original Basilisk
49.23 42.78 45.78
X
X
Experiment 3: Supervised One-Versus-All classification approach
Unigrams
37.54 64.50 47.46
X
X
Unigrams bigrams
42.19 47.39 44.64
X
X
SVM
Lexicon words
48.72 37.08 42.11
X
X
Unigrams lexicon words
37.05 65.96 47.45
X
X
Unigrams, bigrams, lexicon words 51.19 36.59 42.68
X
X
Experiment 4: Transductive One-Versus-All classification approach
Unigrams
11.84 67.78 20.16
X
X
Unigrams bigrams
50.00 33.86 40.38
X
X
SVM
Lexicon modified Basilisk
42.83 30.64 35.73
X
X
Unigrams lexicon words
51.30 38.29 43.85
X
X
Unigrams, bigrams, lexicon words 55.90 32.77 41.32
X
X
Experiment 5: MetaLabeler approach
Unigrams
58.80 16.63 25.92
X
X
Unigrams bigrams
66.02 20.51 31.30
X
X
SVM
Lexicon words
63.23 17.11 26.93
X
X
Unigrams lexicon words
70.29 20.39 31.61
X
X
Unigrams, bigrams, lexicon words 68.79 24.21 35.82
X
X
Experiment 6: Ensembles pruned sets approach
Unigrams
22.44 63.05 33.09
X
X
Unigrams bigrams
22.22 67.42 33.42
X
X
SVM
Lexicon modified Basilisk
20.72 73.67 32.35
X
X
Unigrams lexicon words
23.72 85.25 37.12
X
X
Unigrams, bigrams, lexicon words 16.93 71.42 27.37
X
X
Experiment 7: Additional training data 5-fold cross-validation
Unigrams
42.21 63.65 50.76
X
X
Unigrams bigrams
43.58 58.31 49.88
X
X
SVM
Lexicon words
56.06 40.41 46.97
X
X
Unigrams lexicon words
54.75 52.43 53.56
X
X
Unigrams, bigrams, lexicon words 54.81 52.55 53.66
X
X

594

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

6.3 Experiments Semantic Lexicon Approach
Recall semantic lexicon learning approach, label report test set using
Occurrence Heuristic combination semantic lexicon learned modified
Basilisk framework described Section 4.3. showing results approach,
first describe tune parameters modified Basilisk framework.
6.3.1 Parameters
modified Basilisk framework five parameters tune. first four thresholds resulting four frequency-based constraints involving minimum support
maximum generality (see Modification 3 Section 4.3.3). specifically, four
threshold parameters (1) minimum frequency word (M inW ), (2) maximum frequency word (M axW ), (3) minimum frequency pattern (M inP ),
(4) maximum number words extracted pattern (M axP ). addition, recall
Section 4.3.4 three types patterns (namely, subject-verb/verb-object patterns, bigram patterns extracting words, bigram patterns extracting phrases).
fifth parameter pattern parameter, determines subset
three types patterns use. goal tune five parameters jointly
development set. words, want find parameter combination yields
best F-measure Occurrence Heuristic used label reports development set. However, maintain computational tractability, need limit number
values parameter take. Specifically, limit five different combinations four threshold parameters (see Table 5), combination,
find subset three types patterns yields best F-measure development set. Hence total number experiments need run 35 (= 7 (the number
(non-empty) subsets three types patterns) 5 (the number combinations
first four parameters)). experiment indicates combination 3 Table 5,
together bigram patterns extracting phrases, yields best F-measure
development set, therefore chosen best parameter combination involving
five parameters.
new words phrases acquired first two iterations modified Basilisk
using parameter combination shown Appendix B. see new
words acquired first two iterations eight 14 categories. reasons
(1) unlike original Basilisk framework, modified Basilisk employs common
word pool, thus longer requiring five words must added category
bootstrapping iteration; (2) application minimum support words led
filtering infrequently-extracted words. two reasons together ensure
modified Basilisk framework focuses learning high-precision words category.
6.3.2 Results
semantic lexicon learned using best parameter combination (based performance development set) used label reports test set. see
row 1 Experiment 2 Table 4, Modified Basilisk approach achieves precision
53.15%, recall 47.57% F-measure 50.21%. comparison baseline,
method lower precision higher recall. increased recall shows
595

fiAbedin, Ng & Khan

Table 5: Combinations four threshold parameters modified Basilisk framework.
Combination
Combination
Combination
Combination
Combination
Combination

1
2
3
4
5

inW
25
25
10
10
10

axW
2500
2500
2500
2500
5000

inP
250
100
250
250
250

axP
100
100
100
250
100

reports covered expanded lexicon. However, learned lexicon contains
general words resulted drop precision. Overall, higher Fmeasure, statistically significantly better baseline according
significance tests. vindicates premise learning words phrases
relevant shaping factors help us identify shaping factors reports.
6.3.3 Results Using Original Basilisk
better understand whether proposed linguistic algorithmic modifications
Basilisk framework (see Section 4.3) indeed beneficial cause identification
task, repeated experiment described above, except replaced lexicon
generated using modified Basilisk framework one generated using original
Basilisk framework. specifically, implemented original Basilisk framework
described Thelen Riloff (2002), one minor difference: case
bigram patterns extracting phrases, word pools described Section 4.2 populated
entire phrases instead head words. done seed words list
extracted Section 2.3.2 contains words phrases hence would learn
entire phrases.
parameter tune original Basilisk framework pattern parameter,
which, mentioned above, determines subset three types patterns use.
Therefore, construct seven lexicons (corresponding seven non-empty subsets
three types patterns) using original Basilisk framework, determine
lexicon yields best performance development set. experiment indicates
best development result achieved bigram patterns extracting
phrases used. Applying corresponding semantic lexicon combination
Occurrence Heuristic classify reports test set, observe precision 49.23%,
recall 42.78% F-measure 45.78% (see row 2 Experiment 2 section
Table 4). lower precision higher recall indicates lexicon learned
words general (i.e., words appear many reports little
discriminative power). new words phrases acquired first two iterations
original Basilisk shown Appendix C. seen, original Basilisk framework
adds lot words, many relevant shaping factors
added, semantically similar seed words shaping factor.
596

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Hence, although recall improves small amount, precision drops significantly, leading
precipitation F-measure. results suggest proposed modifications
original Basilisk framework indeed beneficial far cause identification task
concerned.
6.4 Experiments Classification Approach
Recall classification approach cause identification, train SVM classifier
shaping factor Sk determine whether report labeled Sk . desired,
approach allows report test set potentially receive multiple labels, since
resulting 14 SVM classifiers applied independently report. investigate
effect different feature sets performance cause identification, employ five
feature sets experiments: (1) unigrams only; (2) unigrams bigrams; (3) lexicon
words only; (4) unigrams lexicon words; (5) unigrams, bigrams lexicon words.
unigrams bigrams generated reports training set first
removing stop-words ignoring case information, semantic lexicon
one constructed modified Basilisk framework. showing results
supervised transductive experiments, first describe parameters associated
classification approach.
6.4.1 Parameters
SVM classifier, two parameters tune. first parameter
percentage features use. Feature selection shown improve performance
text classification tasks (Yang & Pedersen, 1997). result, employ information
gain (IG), one effective methods feature selection according Yang
Pedersens experimental results. Since assume words semantic lexicon
relevant cause identification, apply feature selection lexicon words.
Rather, apply feature selection unigrams bigrams. specifically,
unigrams used features (as first five feature sets mentioned
beginning subsection), select N % unigrams highest IG,
value N tuned using development set. unigrams bigrams used
features (as second fifth feature sets), combine unigrams bigrams
one feature set select N % unigrams bigrams highest IG,
value N tuned using development set. experiments, tested 10
values N : 10, 20, . . ., 100.
second parameter associated SVM classifiers classification threshold.
default, SVM sets classification threshold 0, meaning every data point
classification value 0 classified positive, rest classified
negative. However, since SVM classifier trained optimize classification accuracy,
best classification threshold may 0 cause identification task,
goal optimize F-measure. result, parameterize classification threshold,
allowing take one 21 values: 2.0, 1.8, . . . , 1.8, 2.0.
usual, tune two parameters described jointly rather independently.
words, possible value combination percentages features
597

fiAbedin, Ng & Khan

classification threshold, compute F-measure classifiers development set
classes choose value pair yields maximum F-measure.
get better idea two parameters impact performance, show
Figure 3 F-measure changes development set vary values
two parameters, experiment underlying SVM classifiers employ
unigrams features. see, best F-measure achieved employing
top 50% unigrams classification threshold 0.8. Using default parameter values
(no feature selection classification threshold 0) yields F-measure approximately
18%. Overall, results provide suggestive evidence parameters
large impact performance.
F-measure Vs. classification threshold
different percentages unigram features
100
Top 10% Unigrams
Top 20% Unigrams
Top 30% Unigrams
Top 40% Unigrams
Top 50% Unigrams
Top 60% Unigrams
Top 70% Unigrams
Top 80% Unigrams
Top 90% Unigrams
Top 100% Unigrams

90
80

F-measure (%)

70
60
50
40
30
20
10
0
-2

-1.5

-1

-0.5
0
0.5
Classification threshold

1

1.5

2

Figure 3: Variation F-measure different percentages unigram features classification thresholds used SVM classification.

6.4.2 Supervised One-Versus-All Classifiers: Results Discussions
Results supervised One-Versus-All classification approach using five feature sets
described shown Experiment 3 section Table 4.14 see,
feature sets 1 (unigrams only) 4 (unigrams lexicon words) used, achieve
best results F-measure scores 47.46% 47.45%, respectively. However, even
best results statistically indistinguishable baseline result (according
approximate randomization test), significantly worse result produced
14. Recall supervised approach, SVM classifiers trained 233 reports
training set.

598

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

modified Basilisk approach (row 1 Experiment 2) [see Appendix D, contains
statistical significance test results obtained applying stratified approximate randomization test pair experiments Table 4].
fact, indicate Occurrence Heuristic made effective use
learned semantic lexicon SVM classifiers: SVM classifiers trained
lexicon words features (row 3 Experiment 3) produced significantly worse
F-measure score (42.11%) Occurrence Heuristic (50.21%), due large
drops recall precision. Overall, results suggest supervised approach performs worse heuristic-based semantic lexicon approach task.
hypothesize limited amount training data available SVM learner contributed poor performance supervised approach. test hypothesis
Section 6.5
Two additional observations worth mentioning. First, comparing rows 1 4
Experiment 3, see lexicon words useful cause identification
presence unigrams. Second, comparing rows 1 2 rows 4 5 Experiment
3, see using bigrams hurts performance. likely reason attributed
feature selection method: since choose top N % features, bigram features
significantly outnumber unigram features, thus potentially diminishing effect
latter. One solution problem employ separate parameters selecting
unigrams bigrams, decided choice, would lead explosion
size parameter space.
6.4.3 Transductive One-Versus-All Classifiers: Results Discussions
investigate whether useful exploit unlabeled data, employ transductive SVM
combine labeled unlabeled data. Essentially, repeated experiments
supervised One-Versus-All classification approach, except trained transductive
SVM classifier using (labeled) reports training set (unlabeled)
reports test set described Section 5.2.2. two parameters percentage
features used classification threshold tuned jointly maximize F-measure
development set, described supervised approach, except transductive
SVMs used parameter tuning step trained using training set labeled data
development set unlabeled data.
Results transductive SVM classifiers shown Experiment 4 section
Table 4. Overall, transductive results significantly worse corresponding
results Experiment 3. However, conclusions draw transductive
results slightly different drawn supervised results. First, using
bigrams significantly improves performance lexicon words absent (comparing
rows 1 2 Experiment 3) hurts performance lexicon words present
(comparing rows 4 5). Second, adding lexicon words unigram-only feature
set (comparing rows 1 4) significantly improves performance, suggesting potential
usefulness lexicon features. Nevertheless, Experiments 3 4 indicate (1)
using lexicon words features far adequate, (2) best performance
achieved lexicon words added unigrams features.
599

fiAbedin, Ng & Khan

6.4.4 Results Additional Supervised Approaches
Next, present results two additional supervised approaches, namely MetaLabeler ensembles pruned sets (Section 5.2.1). feature sets used
approaches used One-Versus-All method. OneVersus-All method, approaches use SVM underlying learning algorithm
classifier training.
MetaLabeler. parameter needs tuned MetaLabeler approach
percentage features use (N ), selected based classification performance (F-measure) development set.
Results MetaLabeler approach shown Experiment 5 section Table 4. interesting points results. First, MetaLabeler
method results much better precision methods. Second, method
shows consistent performance improvement bigram features added, seen
comparing first second, fourth fifth rows MetaLabeler results.
Third, inclusion lexicon word features found improve performance,
seen comparing first fourth, second fifth, rows MetaLabeler
results. two observations show MetaLabeler approach properly take
advantage increasingly richer feature sets used experiments, best
performance occurring types features used (fifth row). Unfortunately,
approach suffers poor recall, fact prevents even matching, let alone
surpassing, F-measure scores methods. Since method discards less
probable labels assigns labels documents, precision much improved
recall suffers.
Ensembles Pruned Set. Among parameters ensembles pruned sets
approach, number classifiers ensemble, , size sample
training data classifier ensemble trained, chosen
ones used Read et al. (2008), namely 10 63% respectively. rest
parameters pruned set approach, namely minimum cardinality (b), minimum
support (p), percentage features use (N ), threshold label assignment (t)
selected jointly based classification performance (F-measure) development
set. values specific value b chosen 2, 3 5. possible
values p tested experiment 3, 5 10. threshold parameter chosen
values 0.1, 0.2, . . . , 1.0, percentage features, N chosen
values 10%, 20%, . . . , 100%. Thus 900 parameter combinations feature set,
parameter combinations, combination performance
development test set best (in terms F-measure) chosen running system
test set.
Results pruned set approach shown Experiment 6 section Table 4.
Here, see best performance combination unigram lexicon word features,
better performance using unigrams lexicon words individually. However,
performance degraded inclusion bigrams combination. Precision
much lower methods, indicates selection label
sets training set 233 reports may adequate.
600

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

6.5 Experiments Using Additional Training Data
results experiments somewhat surprising: best-performing supervised classification approach One-Versus-All approach performs significantly worse
modified Basilisk approach. hypothesize poor performance attributed scarcity (labeled) training data. test hypothesis, conducted
set experiments increased amount training data One-Versus-All
supervised classification approach applying cross-validation. specifically, take
test set 1000 reports split five disjoint subsets equal size, T1 , T2 , . . . , T5 .
Then, construct training set merging Tj , 6= j,
original training set 233 reports. that, train SVM classifier merged
training set test set Ti . done five folds, compute
F-measure entire test set. words, results report set
experiments F-measure scores averaged five folds. experimented
five set features used supervised experiments Section 6.4. two
parameters, percentage features used classification threshold, tuned
exactly way supervised experiments.
Results set experiments shown Experiment 7 section Table 4.
comparison results Experiment 3, F-measure increases uniformly significantly.
provides empirical evidence performance supervised classifiers limited
amount data trained. feature sets 4 (unigrams
lexicon words) 5 (unigrams, bigrams lexicon words), achieve best results
F-measure scores 53.56% 53.66% respectively difference
statistically insignificant. two results turn significantly better
modified Basilisk (row 1 Experiment 2), according approximate randomization
test. addition, except feature set 3 (lexicon words only), results obtained
experiment significantly better baseline, according approximate
randomization test. Overall, results suggest difficulty cause identification
task: comparing rows 4 Experiments 3 5, see F-measure increases
6% number training reports increased 233 1033.
points deserve mentioning. previous learning-based experiments, using lexicon words features yields worst result set experiments,
combining unigrams lexicon words still yields one best results. Nevertheless,
comparison Experiment 3, using bigrams still improve performance,
hurt performance (from statistical significance point view). Perhaps
importantly, comparing rows 1 4 Experiment 7, see augmenting unigrams
lexicon words yields significantly better performance. indicates lexicon
words indeed useful features cause identification, usefulness may
revealed small labeled training set used, seen Experiment 3. Learning algorithms attempt learn features important relevant given classification
task based training examples see, training examples,
better able learn relevance features. results show
poignant illustration phenomenon: SVM learner able use lexicon word
features effectively given large number training instances. seen
clearly SVM learning curves Section 6.7.3. indicates lexicon
601

fiAbedin, Ng & Khan

words useful features sufficiently large training data. However,
lexicon words may still used effectively ways linguistic features even
training set small, see results Experiment 2, uses
lexicon words combination Occurrence Heuristic achieve performances
statistically significantly better baseline.
6.6 Error Analysis Lessons Learned
order gain clearer insight cause identification problem help direct
future research, manually analyzed errors made best-performing system (i.e.,
heuristic based approach using semantic lexicon learned modified Basilisk
framework) randomly chosen 100-report subset test set. specifically,
looked false negatives (cases annotator labeled report shaping
factor system not) false positives (cases system labeled
report shaping factor annotator not). false negative, tried
determine system failed correctly label report, false positive,
tried determine system labeled report erroneously. Table 6 shows
number false positives false negatives along reasons errors
discovered analysis. following sections discuss errors reasons
detail. Note since shaping factor may indicated one keyword
single report, one reason false negative (positive) error.
Thus sum frequencies different types false negative (positive) errors greater
total number false negatives (positives).
Table 6: Error analysis details: different reasons false positive false negative
errors.
False negatives
Sentence fragments bigger phrases
Implicit causes cannot identified keywords
Phrases learned
False positives
Keyword general
Keyword indicates concept appears report
contribute incident
Wrongly learned keyword
Keyword used negative context
Keyword used hypothetical context

58
24
23
14
83
50
32

Percentage
41.38%
39.66%
24.14%

6
3
1

7.23%
3.61%
1.20%

60.24%
38.55%

False negatives. false negative error, read report narrative identify
word, phrase sentence fragment may indicate shaping factor
system missed. analysis, identified three reasons false negatives
follows:
602

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

1. Required sentence fragments larger phrase. identified 24 sentence
fragments bigger phrases (i.e., consist two phrases).
example, sentence fragment never DCA consists 4
phrases: never been, to, DCA before. Together, convey meaning
reporter unfamiliar DCA, possible identify single
word phrase conveys meaning. Since framework learns
phrases, possible learn sentence fragments.
2. Cause identifiable specific words phrases. 21 instances, specific
word, phrase sentence fragment could identified could pinpoint shaping
factors responsible incident. example, number reports, including
report#566757, describe incidents miscommunication
pilot air traffic controller, miscommunication must understood
following conversation. human reading report easily understand
pilot claiming controller said one thing controller claiming
said something different, detect kind scenario, machine would need
generate complete model discourse identifies specific topic
conversation, participants, claims participant makes topic,
fact claims contradictory, fact contradiction arises
miscommunication them. preprocessed narrative report
shown Appendix E.
3. Missing phrases. 14 cases necessary phrase missing semantic
lexicon learned modified Basilisk framework. 14 phrases, six
phrases infrequent considered modified Basilisk framework due
minimum frequency criterion. example, phrase temperature flux
appears entire corpus hence considered system.
Two phrases verb phrases, could learned focused
learning noun phrases adjective phrases. four phrases
semantically similar seed word shaping factors. example,
phrase garbled transmission semantically similar seed word
shaping factor Communication Environment, disturbance, static, radio
discipline, congestion noise. Finally, two phrases
learned system, learned time put
word pool, words higher scores selected instead.
False positives. case false positives, looked report narrative
keyword found content determine indication shaping
factor incident described report incorrect. different reasons
identified follows.
1. general keywords. observed large number false positives due
keywords general (i.e., keywords extracted learned
shaping factor may appear phrases related shaping
factor). example, keyword failure correct indicator Resource Deficiency
appears text complete electrical failure, alternator failure, etc.,
appears text failure follow Air Traffic Control instructions,
603

fiAbedin, Ng & Khan

indicate Resource Deficiency shaping factor. identified 50 cases
caused keywords general.
2. Concept present contributing incident. Another frequently faced
problem sometimes concepts identified keywords present
report, act shaper incident described report.
example, report#324831, reporter mentions flying solo,
indication Taskload, incident due Physical Environments, namely
snow foggy weather. fact flying solo merely mentioned
part description overall situation. preprocessed version report
given Appendix E. total, observed 32 cases.
3. Incorrectly learned words phrases. six cases semantic lexicon learner learned incorrect words phrases related
shaping factors assigned. example, framework incorrectly learned word shaping factor Resource Deficiency, thus
number reports mislabeled Resource Deficiency.
4. Negative context. three cases keyword appeared
negative context, typically signaled contextual valence shifter
hardly (Polanyi & Zaenen, 2006). example, keyword aircraft damage, indicator Resource Deficiency, appears report#569901 apparent
aircraft damage, results false positive.
5. Hypothetical context. one case keyword appeared
hypothetical context reporter conjectures possible scenario.
keyword single pilot, indicator Taskload, appeared report#534432
could happen pilot especially single pilot, resulting false positive.
Lessons learned. error analysis provides valuable insight nature
problem well hints one proceed order improve performance
system. analyzing frequent errors, present following lessons
learned analysis. First all, useful learn high-precision keywords
phrases general ones largest part false positive errors attributed
general keywords. However, high-precision keywords phrases
likely low frequencies, hence one would adapt learning methods
learn useful words phrases infrequent ones. Second, one must take account
fact relevant portions text may larger phrases, even going
clause sentences. cannot identified learning words phrases, N-grams
reasonable size. Thus, robust methods needed learn useful sentence
fragments useful sentence structures. Finally, cases one cannot hope
identify using methods look keywords, phrases, sentence fragments even sentence
structures, i.e., cases cause incident understood
discourse, cases concept present description yet plays part
incident. Much deeper analysis simple bag-of-anything models needed
avoiding two types errors, represent almost one third
errors analyzed subset. former needs method distinguish relevant
604

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

sentences irrelevant ones. example, Patwardhan Riloff (2007) discuss relevant
sentence classifier trained small set seed patterns set documents
marked relevant irrelevant useful context. latter problem
requires discourse analysis method that, discussed earlier, model conversations
identify relations correctly. shows though possible identify shaping
factors reports using words phrases certain extent, much deeper natural
language techniques needed accurately identify full range causes.
6.7 Additional Analyses
section present outcomes number additional analyses performed
cause identification task approaches task. Section 6.7.1 study
relative difficulties classifying different shaping factors. Sections 6.7.2 6.7.3
show learning curves semantic lexicon based approach learning based
approach respectively, i.e., performances two approaches vary
provided different amounts training data. Finally Section 6.7.4 discuss
outcomes experiment conducted determine modifications Basilisk
framework useful learning general semantic categories.
6.7.1 Per-Class Results
get insight classes difficult classify, perform analysis
per-class performance two labeling schemes: best heuristic-based method (i.e.,
Occurrence Heuristic using lexicon learned modified Basilisk framework) [see
first part Table 7] best learning-based method (i.e., 5-fold SVM classifiers
using unigrams, bigrams lexicon words features) [see second part Table 7].
conjunction Table 1, two classes stand prominently difficult classify
Illusion Taskload. classes little representation training,
test development sets, small number seed words, result poor
performance approaches. easily identifiable classes Physical
Environment, Physical Factors, Resource Deficiency Preoccupation,
labeling schemes F-measures better 40%. general classes better
representation training, testing development sets, reasonable
number words phrases semantic lexicon. believe difference
characteristics classes valuable insight helpful future work.
6.7.2 Lexicon Learning Curve
mentioned Section 2.3.2, used total 177 seed words phrases.
first glance, number seeds may seem large far bootstrapping experiments
concerned. However, considering fact 177 seeds distributed 14
shaping factors, average 12.6 words phrases per shaping factor.
Nevertheless, would still interesting examine cause identification performance
affected reduce number seeds shaping factor used Modified
Basilisk bootstrapping process. result, ran set experiments measure
cause identification performance uses semantic lexicon learned Modified Basilisk
given different number seed words, parameters specific Modified
605

fiAbedin, Ng & Khan

Table 7: Per-class performance results. upper table shows per-class performance
Occurrence Heuristic using lexicon learned modified Basilisk framework.
lower table shows per-class performance 5-fold SVM classifiers using unigrams,
bigrams lexicon words features.

Shaping Factor
Attitude
Communication Environment
Duty Cycle
Familiarity
Illusion

Physical Environment
Physical Factors
Preoccupation
Pressure
Proficiency
Resource Deficiency
Taskload
Unexpected
Overall

TP
3
9
3
31
0
25
195
22
78
14
40
360
0
4
784

FN
27
81
23
19
2
192
70
13
32
16
207
147
29
6
864

TN
957
888
973
872
996
766
638
958
822
902
723
225
965
976
11661

FP
13
22
1
78
2
17
97
7
68
68
30
268
6
14
691

Precision
18.75%
29.03%
75.00%
28.44%
0.00%
59.52%
66.78%
75.86%
53.42%
17.07%
57.14%
57.32%
0.00%
22.22%
53.15%

Recall
10.00%
10.00%
11.54%
62.00%
0.00%
11.52%
73.58%
62.86%
70.91%
46.67%
16.19%
71.01%
0.00%
40.00%
47.57%

F-measure
13.04%
14.88%
20.00%
38.99%
0.00%
19.31%
70.02%
68.75%
60.94%
25.00%
25.24%
63.44%
0.00%
28.57%
50.21%

Shaping Factor
Attitude
Communication Environment
Duty Cycle
Familiarity
Illusion

Physical Environment
Physical Factors
Preoccupation
Pressure
Proficiency
Resource Deficiency
Taskload
Unexpected
Overall

TP
2
20
10
18
0
52
182
20
55
6
102
399
0
0
866

FN
28
70
16
32
2
165
83
15
55
24
145
108
29
10
782

TN
964
871
962
924
998
685
623
955
848
961
639
247
971
990
11638

FP
6
39
12
26
0
98
112
10
42
9
114
246
0
0
714

Precision
25.00%
33.90%
45.45%
40.91%
0.00%
34.67%
61.90%
66.67%
56.70%
40.00%
47.22%
61.86%
0.00%
0.00%
54.81%

Recall
6.67%
22.22%
38.46%
36.00%
0.00%
23.96%
68.68%
57.14%
50.00%
20.00%
41.30%
78.70%
0.00%
0.00%
52.55%

F-measure
10.53%
26.85%
41.67%
38.30%
0.00%
28.34%
65.12%
61.54%
53.14%
26.67%
44.06%
69.27%
0.00%
0.00%
53.66%

606

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Basilisk set described Section 6.3.1. specifically, chose top 3, 4, 5, 6,
7, 10, 15 20 seed words phrases shaping factor (in terms frequency
entire corpus), ran modified Basilisk framework ten iterations using
aforementioned parameters.
Note, however, shaping factors number manually selected
seed words phrases. example, Illusion, Taskload Unexpected 1, 2 3
seed words phrases respectively, whereas Resource Deficiency Physical Environment
47 45 respectively (see last column Table 1). Hence, experiments
number seeds used shaping factor exceeds number manually
selected seeds shaper, manually selected seeds used. example, since
Unexpected three manually selected seeds, used experiments
least three seeds used shaping factor.
Occurrence Heuristic used lexicons thus generated evaluate
performance test set. resulting learning curve, terms F-measure
test set 1000 reports, shown Figure 4. addition, since baseline
compare performance based seed words, baseline learning curve
corresponding reduced seed words set shown. expected, increasing
number seed words monotonically improves F-measure. However, improvement
baseline particularly small fewer seven seed words used,
highest improvement observed seven seed words phrases. on, adding
seeds improves overall performance, improvement baseline slowly
diminishes.
Lexicon Learning Curve
50
48
46

F-measure (%)

44
42
40
38
36
Baseline
Performance learned lexicon

34
32
30
0

2

4

6

8
10
12
Number Seeds

14

16

18

20

Figure 4: Variation F-measure different number seeds words per category.

607

fiAbedin, Ng & Khan

SVM Learning Curve
60
F-measure Test Set
58
56

F-measure (%)

54
52
50
48
46
44
42
40
0

100

200

300 400 500 600 700
Number Training Instances

800

900 1000

Figure 5: Variation F-measure different number training reports.

6.7.3 SVM Learning Curve
discussed Section 6.5, hypothesize failure SVM classifiers perform better baseline due scarcity training instances available
learner. One may argue SVM shown work well small datasets.
So, natural question is: much smaller training set see
statistically significant drop cause identification performance? answer question,
plot learning curve One-Versus-All classification approach, using features
combination unigrams, bigrams, lexicon word features five-fold cross validation
setting, setting yields best performance Table 4. Specifically,
generated random subsets training sets sizes 50, 100, . . . , 1000 instances. Parameters, namely percentage features classification threshold, chosen
way original experiment described Section 6.5, F-measure
evaluated entire test set. curve shown Figure 5, data point
computed averaging results five independent runs. see,
general trend performance improvement increase number training
instances. addition, trained 50% training set, see cause
identification system started perform statistically significantly worse system
trained available instances according stratified approximate
randomization test.
608

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

6.7.4 General Usefulness Modifications Basilisk
order test whether modifications made Basilisk framework useful
lexicon learning general, added two general categories shaping factors
bootstrapping experiments, namely People Equipment. two categories
added because, firstly, words phrases added categories would easy verify
(i.e., whether words phrases representing people equipment), secondly,
similar original context Basilisk framework originally evaluated (i.e., learning words categories BUILDING, EVENT, HUMAN, LOCATION,
TIME, WEAPON terrorism reports). two additional categories added
seed lexicon described Section 2.3.2, bootstrapped running Original Basilisk Modified Basilisk separately ten iterations, parameters specific
Basilisk frameworks set way described Section 6.3.1. seed
words two categories selected manner done Thelen
Riloff (2002), i.e., phrases corpus sorted frequency five
frequent phrases belonging categories manually identified. seed
words used two categories:
People: Captain, controller, First Officer, RPTR, passenger
Equipment: aircraft, airplane, Collision Avoidance System II, engine, Auto-Pilot
order verify words phrases learned two frameworks correctly
belong assigned category, first author computer science graduate student
affiliated research went generated lexicons. Appendices F G
show lexicons generated Original Basilisk Modified Basilisk respectively.
facilitate analysis, divide words phrases generated lexicon three
categories: (1) determined correct human judges; (2)
determined correct one judge; (3) determined incorrect
judges.
lexicon generated Original Basilisk, find category People, 29
50 words phrases determined correct judges, 6 determined
exactly one judges correct; category Equipment, 6 50 words
phrases correct according judges, 22 correct according exactly one
judges. hand, lexicon generated Modified Basilisk, find
category People, 44 50 words phrases determined correct
judges, 3 determined exactly one judges correct; category
Equipment, 34 50 words phrases correct according judges, 9
correct according exactly one judges. comparison clearly shows
modifications made Basilisk framework specific particular
task; rather, modifications improved lexicon building performance general.
6.8 Summary Conclusions
end section providing summary major conclusions draw
experiments.
609

fiAbedin, Ng & Khan

heuristic approach cause identification, labels report using Occurrence Heuristic combination words phrases automatically acquired
using Modified Basilisk framework, surpasses performance baseline system, applies Occurrence Heuristic combination seed words
phrases manually identified training documents. difference F-measure
two systems statistically significant according McNemars test
stratified approximate randomization test. suggests words
phrases semantic lexicon learned via Modified Basilisk relevant effective
cause identification.
Adding learned lexicon words N-gram-based feature set training SVM
classifiers beneficial cause identification training set sufficiently
large, exhibited statistically significant increase F-measure. provides
suggestive evidence words phrases semantic lexicon learned via
Modified Basilisk relevant useful features cause identification.
used combination Occurrence Heuristic, semantic lexicon learned
Modified Basilisk framework offers significantly better performance
cause identification task one obtained using original Basilisk framework. Additional experiments reveal Modified Basilisk useful
cause identification, offers performance superior Original Basilisk
bootstrapping general semantic categories People Equipment.
Among three multi-class multi-labeled text classification approaches experimented with, One-Versus-All works significantly better MetaLabeler Pruned
Sets cause identification. Transductive learning, used combination
One-Versus-All approach, significantly hurts performance, suggesting unlabeled data cannot profitably exploited given fairly small amount labeled
data.
best system achieves F-measure around 53.7%, indicates cause
identification difficult task, lot room improvement.
provide directions future research, performed analysis errors made
best-performing system. particular, found performance currently
limited part several factors. First, number cases
relevant text indicating responsible shaping factor may larger phrases.
Second, indicators shaping factor may mentioned report without influencing incident described report. Finally, cases
shaping factors cannot identified simply looking words, phrases even
sentence fragments much deeper analysis required cases.
Increasing number seed words phrases employed Modified Basilisk improves cause identification performance, marginal performance improvement
added seed diminishes successive additions. words, results
seem suggest using seed words unlikely improve much
current performance; rather would promising start small number
high frequency seeds improve upon bootstrapping process.
610

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

learning curve plotted One-Versus-All classification approach shows
cause identification performance increases number training instances.
particular, trained 50% training set, see resulting cause
identification system performs statistically significantly worse one trained
available instances.
Overall, approaches rely automatically learned lexicon words phrases
adequate cause identification, relevant task. mentioned previously, use motivated labor-intensive procedure NASA
researchers employed manually identifying seed words phrases shaping factor (Posse et al., 2005). work represents one first attempts tackle cause
identification task, believe use simple features good starting point
establishes baseline future studies problem compared.
main take home message research though possible solve
problem set solve, namely automated cause identification, learning
relevant keywords sentence fragments suitable bag-of-words models,
remains significant portion data remains unlabeled mislabeled
methods. match performance level achieved topical text classification
tasks, much deeper linguistic analysis relevant sentence detection discourse analysis
methods identifying disagreements, disputes hostile attitudes needed.
lesson cornerstone research area.

7. Related Work
section, describe works related research. particular,
discussion focuses causal analysis well approaches semantic lexicon construction text classification, organized follows. First, discuss causal analysis
appeared different fields. Second, discuss different semantic lexicon
learning algorithms. Third, discuss works deal extraction pattern learning.
Fourth, describe different algorithms unsupervised word clustering thesaurus
building. Finally, include discussion related work multi-class multi-labeled text
classification.
Causal analysis. Major research causality performed mainly fields
philosophy psychology. field philosophy, seminal works causality
conducted Hume (1739, 1748), provides one influential definitions
cause object followed another, objects, similar first,
followed objects similar second. Or, words, where, first object
been, second never existed. basis later, much stronger
definitions causation (e.g., Lewis, 1973; Ganeri, Noordhof, & Ramachandran, 1996).
Notable investigations causation field psychology include Cheng (1997),
defines causation terms probabilistic contrast model; Griffiths Tenenbaum
(2005), discuss learning cause effect relationships using causal graphical
models; Halpern Pearl (2005), provide explanations causality means
structural equations governing random variables representing events. Although
611

fiAbedin, Ng & Khan

works provide important background definitions contributing understanding
causality, order identify causes naturally written text must turn NLP.
field NLP, little work cause identification similar problem.
Research causality focuses mainly identifying causal relations two sentence
components. instance, Girju (2003) describes method automatically discovering
lexico-semantic patterns refer causation. particular, focuses explicit
intra-sentential pattern hN P1 verb N P2 i, verb simple causative. shows
patterns used improve performance system answering
cause-effect type questions. Khoo et al. (2000) use graphical pattern matching identify
causal relations medical article abstracts. use hand-crafted patterns
matched parse trees sentences. subtrees parse tree match
patterns extracted causes effects. Similarly, Garcia (1997) uses hand-crafted
extraction patterns identify causal relations sentences French language.
limitation approaches focus identifying causal relations
sentence, whereas reports multi-sentence discourses.
Grishman Ksiezyk (1990) use domain modeling, discourse analysis causal inference find cause-effect relations events leading equipment malfunctions
short equipment failure reports. specifically, first apply syntactic analysis
produce parse trees sentences reports using augmented context-free
grammar. apply semantic analysis map (1) verbs syntactic relations
domain-specific predicates relations (2) noun phrases references components
domain model. Finally, apply discourse analysis predicates construct
time-graph showing temporal causal relationships elementary facts.
temporal relations derived text structures words (e.g., when, then,
etc.) order appearance text, causal relations determined querying simulation model equipment built using domain knowledge. Specifically,
possible causal link posed query model test relation holds.
Overall, method relies heavily domain model equipment studied,
research focuses one specific piece equipment.
NASAs research identifying causes incidents report narratives
performed Posse et al. (2005), describe specific experiment
brought together experts manually analyze report narratives identify words,
phrases expressions related shaping factors, mentioned earlier. Later
work Ferryman et al. (2006) take manually extracted expressions ground truth
compare anomalies described reports shaping factors derived
applying expressions reports. Specifically, attempt learn
expressions automatically; rather, focus finding possible correlations
shaping factors anomalies.
Algorithms semantic lexicon learning. number semantic lexicon learning
algorithms follow iterative bootstrapping approach, starting small number
semantically labeled seed words. Roark Charniak (1998) propose method constructing semantic lexicons based co-occurrence statistics nouns conjunctions, lists
appositives. start small seed nouns list, iteratively add similar words
list. word similarity measured ratio many times word occur
612

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

together seed word number times word appear corpus.
construction, rank words log-likelihood statistic (Dunning, 1993). However,
due general brevity reports, co-occurrences lists rather
corpus, useful us use context-based similarities Thelen Riloff
(2002). describe Basilisk framework learning semantic lexicon using extraction patterns features. apply weakly supervised bootstrapping approach
start small manually constructed seed lexicon iteratively add semantically
similar words it. This, described detail Section 4.2, basis
lexicon learning approach.
number improvements Basilisk framework, generally bootstrapping approaches, proposed. Basilisk framework, number iterations
parameter chosen arbitrarily. Rather making arbitrary choice,
Yangarber (2003) proposes method detecting termination unsupervised semantic
pattern learning processes. method requires documents must labeled
relevant irrelevant. Since information available corpus, useful us. Curran, Murphy, Scholz (2007) suggest improvement traditional
bootstrapping methods discarding words contexts appear related
one category, order minimize semantic drift enforce mutual exclusion.
hand, handle cases comparing conditional probabilities
different categories words belong. Zhang, Zhou, Huang, Wu (2008)
present bootstrapping graph mutual reinforcement-based bootstrapping (GMR)
(Hassan, Hassan, & Emam, 2006), modification Basilisk method. Similar us,
explore using N-grams capture context, use different set pattern
word scoring formulas. learning multiple categories simultaneously, introduce
scoring system based entropy pattern. report better results Basilisk
MUC-4 dataset (see Sundheim, 1992).
Among non-bootstrapping approaches, Ando (2004) presents new method constructing semantic lexicons unannotated corpus using set semantic classes set
seed words phrases semantic class. uses spectral analysis improve
feature vectors projecting useful portions vectors subspace removing harmful portions vectors. resultant feature vectors used
centroid-based classifier using cosine similarity measure label words. Avancini,
Lavelli, Sebastiani, Zanoli (2006) take classification approach semantic lexicon
construction. cast problem term (meaning words phrases) categorization task (dual document categorization task), similar bag-of-word
model, represent terms bag-of-documents. use variation adaptive
boosting algorithm, AdaBoost.M H KR , trained small seed lexicon
used classify noun terms corpus zero, one semantic categories.
Algorithms learning extraction patterns. approach semantic lexicon construction uses extraction patterns features, present methods aim
improve extraction pattern collection process. Riloff (1996) describes AutoSlog-TS
system learns extraction patterns untagged text. However, needs pre-classified
corpus text classified relevant irrelevant; mentioned earlier,
access information. Phillips Riloff (2007) present method boot613

fiAbedin, Ng & Khan

strapping algorithm learn role-identifying nouns, used learn important
extraction patterns, role-identifying expressions. However, focus mainly
identifying roles words events.
Patwardhan Riloff (2007) provide another extraction pattern learning approach
using relevant regions. require documents pre-classified relevant
irrelevant documents. Using small set seed patterns, classify sentences
documents relevant irrelevant sentences. semantically appropriate extraction patterns learned using semantic affinity metric separated primary
secondary patterns. approach directly usable us due unavailability
documents pre-classified relevant irrelevant categories.
Recently, Internet increasingly used natural language research. Patwardhan Riloff (2006) use AutoSlog-TS system (Riloff, 1996) learn domain specific
extraction patterns processing documents retrieved querying web selected
domain-specific words. Using web interesting promising enhancement and,
mentioned Section 8, intend extend work using Google corpus (Brants &
Franz, 2006).
Algorithms thesaurus building unsupervised word clustering. Another
area research closely related semantic lexicon learning thesaurus building.
Building thesaurus requires discovering groups semantically similar words, though
stops short assigning semantic class labels words. Thus shares problem
measuring semantic similarity grouping similar words semantic lexicon building
task. discuss several approaches thesaurus building task.
Clustering used extensively thesaurus building, mostly unsupervised nature ability handle large volumes data. seminal work
direction Pereira, Tishby, Lee (1993), present unsupervised method
soft clustering words using distributions words different contexts. approach generates overlapping word clusters, grouping words based contexts
appear in. Baker McCallum (1998) use Pereira et al.s distributional clustering technique perform feature space reduction supervised classification nave Bayes
using clusters features. Lin Pantel (2001) present approach generating
collection sets semantically similar words, concepts, using clustering method,
UNICON, dependency relations features. Pantel Lin (2002) present another
clustering approach, clustering committee, using contextual features point wise
mutual information feature values, compare better Lin Pantels
results. Rohwer Freitag (2004) present clustering-based automatic thesaurus building
process unannotated corpus. propose information theoretic co-clustering
algorithm groups together highly frequent words clusters similar part-of-speech
category. pursue additional process, lexicon optimization, grow lexicon
assigning less frequent words likely clusters.
Among non-cluster-based methods, Davidov Rappoport (2006) present unsupervised method discovering groups words similar meanings. achieve
(1) identifying high frequency words content words, (2) identifying symmetric
lexical relationship patterns, (3) applying graph clique-set algorithm generate word
categories co-occurrence information content words symmetric patterns.
614

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Concentrating performance issues plague attempts build thesaurus
large corpus, Rychly Kilgarriff (2007) present two methods improving performance
general context-based thesaurus building algorithms. first method compare
word pairs context common. second method use
heuristic removing contexts general (i.e., contexts
certain number distinct words). research, adopted second method
(see Section 4.3). applied partitioned sequential approach construction
process. Though thesaurus building usually require annotated corpus set
seed words phrases, directly applicable task growing semantic
lexicon learn words specific semantic categories.
method control words learned classes discovered
word groups belong to. may possible adapt method semantic lexicon
growing classifying word groups semantic classes using seed words
phrases. However, method extended extract noun adjective phrases.
Algorithms multi-class multi-labeled text classification. mentioned previously, cause identification, cast text classification problem, multi-class
multi-labeled text classification problem, since 14 shaping factors total
document may labeled one shaping factors. several
popular approaches solving multi-class multi-labeled text classification problem.
first, one approaches followed research, independently train binary
classifier class, apply classifier test instance isolation.
case, underlying learner Support Vector Machines (Joachims, 1998). Godbole
Sarawagi (2004) suggest number improvements scheme, namely, including class
labels suggested preliminary set classifiers features, removing negative examples
close classification hyperplane, selectively removing classes
one-versus-others classifications scheme. Another notable method, followed Tsoumakas
Vlahavas (2007) Read et al. (2008), treat unique set labels
new label, thus converting problem multi-class single-labeled one. works
differ construction new labels. former, called RAndom
k-LabELsets, RAKEL, builds ensemble classifiers randomly sampling label sets
size k; whereas latter adopts method filtering observed label sets minimum
support. Tang et al. (2009), hand, take different approach: train one
classifier predicts number labels test instance would have, choose
many labels instance based output another classifier ranks labels
likelihood instance. works use SVM underlying learner.
addition, approaches make assumption classes correlated high
degree. However, analysis dataset present evidence strong
correlation. 140 documents multiple labels test set, 68 unique
label sets, seven frequency least five. Thus increasing number
labels would aggravate already imbalanced class distribution.
Among approaches, mention two systems use probabilistic generative models. McCallum Nigam (1999) propose system starts small set keywords
unlabeled documents, learns nave Bayes classifier bootstrapping process
keyword-induced labels using hierarchical shrinkage expectation maximization
615

fiAbedin, Ng & Khan

held-out data set. Ueda Saito (2002) present another generative model called Parametric Mixture Models, treats multi-labeled text parametric mixture words
relevant label. work closely related Latent Dirichlet Allocation (Blei,
Ng, & Jordan, 2003). generative models usually assume document related
particular topic would high frequency words related topic. research,
documents mostly devoted description event occurred, cause
event mentioned briefly. makes generative models less suitable
task hand generative models would likely generate models related events
causes. comprehensive review different approaches multi-class
multi-label text classification found work Tsoumakas Katakis (2007).

8. Conclusions
investigated two approaches cause identification task, goal
understand aviation safety incident happened via identification causes,
shaping factors, responsible incident. approaches exploit information
provided semantic lexicon, automatically constructed via Thelen Riloffs
(2002) Basilisk framework augmented three algorithmic modifications (namely,
use probabilistic similarity measure, use common word pool, enforcement minimum support maximum generality constraints words extraction
patterns) one linguistic modification (the use N-gram-based extraction patterns).
heuristic-based approach labels report employing Occurrence Heuristic,
simply looks words phrases acquired semantic lexicon learning process report. learning-based approach labels report employing inductive
transductive support vector machines learn models reports labeled shaping factors. experimental results indicate heuristic-based approach
supervised learning approach (when given sufficient training data) significantly outperform baseline, which, motivated NASAs work, labels report simply using
Occurrence Heuristic combination set manually-identified seed words
phrases. importantly, results heuristic-based approach indicate modifications original Basilisk framework beneficial far cause identification
concerned, results learning-based approach indicates usefulness lexicon
words used combination unigrams features training SVM
classifier. Overall, set prove possible automate cause
identification task manually analyzing small number reports using information thus generated train machine learning methods identify shaping factors
rest reports. experiments able prove feasibility approach,
usefulness learning semantic lexicon using words features.
Nevertheless, best system achieves F-measure around 53.7%, indicates
cause identification difficult task, lot room improvement.
particular, analysis errors made best system 100 randomly chosen test
documents provides valuable insights task well directions future research.
experience current research, intend extend work
following directions. First foremost, plan extend approach handle text
fragments bigger phrases. Second, order improve quality labeling,
616

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

propose work improving lexicon learning performance using different
semantic similarity measures. instance, would study performance
semantic similarities weighting functions suggested Curran Moens (2002)
context. Third, plan use thoroughly normalized text better parsing
tagging, well relevant region information (Ko, Park, & Seo, 2004; Patwardhan
& Riloff, 2007). Fourth, propose augment semantic lexicon, specifically using
Google N-grams corpus (Brants & Franz, 2006) extract frequent N-gram patterns
words. Fifth, propose explore recent lexicon construction methods
unsupervised word clustering (Pantel & Ravichandran, 2004), spectral analysis, mutual
exclusion bootstrapping, co-clustering exploiting symmetric patterns. Finally, order
handle shaping factors difficult identify words occurring
reports, propose employ much deeper analysis text semantic level.
taken step making annotated incident reports publicly available,
hope stimulate research under-investigated problem NLP
community.

Acknowledgments
authors would thank anonymous reviewers provided us comments
invaluable improving quality paper. research supported
part NASA grant NNX08AC35A. opinions, findings, conclusions recommendations expressed paper authors necessarily reflect
views official policies, either expressed implied, NASA.

617

fiAbedin, Ng & Khan

Appendix A: Seed Words
seed words manually extracted 233 reports training set (see
Section 2.3.2 details).
Shaping Factor
Attitude
Communication
Environment
Duty Cycle
Familiarity
Illusion

Physical
Environment

Physical Factors
Preoccupation
Pressure
Proficiency

Resource
Deficiency

Taskload
Unexpected

Seed words
get HOMEITIS, attitude, inattentiveness, get THEREITIS, complacency, overconfidence, sarcastic, inattention
disturbance, static, radio discipline, congestion, noise
11 hour duty day, inadequate rest, last 4 legs, heavy flying, reduced
rest, all-night flight, 12 hour day, red eye, ten leg day, night
familiarization, familiar, new, first departure, unfamiliar, unfamiliarity, familiar, low time, first landing
bright lights
noise abatement policy, disoriented, confused, medical emergency,
economic considerations, disorientation, drunk passenger, confusion
cold, clouds, dark, setting sun, sun glare, obscured, visibility, hazy
stratus, birds, fog bank, solid overcast, snow, weather, rime, gust, low
weather, surface winds, jet blast, lightning, sea gulls, high ceilings,
hot, tailwind, chop, dark, sea gull, winds, scattered, high tailwinds, extremely dark, bright, icing, turbulence, RPTED wind,
terrain, bird strike, crosswind, thunderstorm, glare, reduced visibility, high flying birds, fog, severe winter weather, cloud, ice
tired, HYPOXIA, tiredness, tired, fatigued, disorientation, fatigue, rest
distracted, preoccupied, mental lapse, busy, DISTRS, distraction,
attention, inattention, absorbed
hurry, running late, pressure, low fuel, fuel considerations, behind
schedule, late, peer pressure, pressure, rushing
mistakes, mistaken, new hire, inexperience, forgotten, less 100
hours, newly rated, training, recent pilot, inadvertently, bad turn,
MISINTERPED
loose connection, erratic, blown, overheated, bang, collapse, idea,
unavailable, placarded, crack, Service, damage, smoke, inoperative, failure, leak, deferred items, communication failure, loss,
unreliable, FDRS problem, bump, shaking, master caution, inadequately lighted, unreadable, disconnected, malfunction, shudder, absence, hazard, inaccurate, UNFLAGGED, fire, broken, fluctuations,
compressor stall, deferral, unusable, wrong, intermittently, warning,
discrepancies, faulty, deferred, intermittent, missing
single pilot, solo
unexpected, suddenly, UNFORECAST

618

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Appendix B: Sample Lexicon Words Learned Modified Basilisk
semantic lexicon words learned modified Basilisk framework
first two iterations.
Shaping Factor
Attitude
Communication
Environment
Duty Cycle
Familiarity
Illusion

Physical
Environment
Physical Factors
Preoccupation

Pressure
Proficiency
Resource
Deficiency
Taskload
Unexpected

New words

aligned, fairly new, familiar
initial confusion, minimum fuel emergency, misunderstanding,
weather emergency
TRWS, conflict message, cumulonimbus, large cells, numerous thunderstorms, occasionally severe, thunderstorm cells, weather buildups,
weather cell, weather en route
first factor
adequate attention, much attention, close attention, close enough
attention, crew attention, enough attention, much attention, proper
attention, strict attention, close attention

different, amiss, awry, obviously wrong, resulting loss, seriously
wrong, slight loss, temporary loss, terribly wrong, wrong

619

fiAbedin, Ng & Khan

Appendix C: Sample Lexicon Words Learned Original Basilisk
semantic lexicon words learned original Basilisk framework first
two iterations.
Shaping Factor
Attitude

Communication
Environment

Duty Cycle

Familiarity

Illusion



Physical
Environment

Physical Factors

Preoccupation

New words
Air Traffic Control security, aileron yoke displacement, anomalous VFR Omni-Directional Radio Range information, assured TFR
avoidance, betrayal, concern urgency, forgetting air carrier X,
magnified problem, operational complacency, unseen unknown
turbulence
9001 noise, BNA runway 31 approach plate, LIGHTSPEED 20K
noise, Non- noise, OVERSPD bell, active noise, clearance delivery
transmission, left engine stall, static Emergency Locator Transmitter, stuck trim elevator movement
10 plus 16 layover, 2 different time frames, 3 back-to-back continuous duty trips, 4 hour break, 69 minutes, 8 hour 15 minute flight time
day, Pacific Daylight Time departure, TPA flight, XC15 departure,
scheduled 2- leg continuous duty
partial unfamiliarity, perceived familiarity, Command familiarity, Command unfamiliarity, blue panel indication light, dispatch
work desks, generally unfamiliar, inexperience unfamiliarity, new
everyday, past experience familiarity
1 1/2 Nautical Mile SSW, 1/2-1/4 point, 10 end, 50 feet side, Elmendorf required use, 1/2 mile downwind, airspace E, foxtrot
intersection, lateral boundaries, mile right
misinformation, Flight Management System/heading anomalies, confusion/conflict, disoriented confused, intense panic, micro sleep,
miss numerous times, mistake inconvenience, note closure problem,
start terror
MHT class Celsius, STRATO-cumulus, Thur morning, clouds underneath, compacted snow ice, fair weather cumulus, next morning
weather, puffy cumulus clouds, thin scattered clouds, well developed
cumulus clouds
HYPOXIA/carbon monoxide, Minimum Equipment List 24-32-02,
basically tired, cardiac distress, indicating system problem, internal
bleeding, interrupted fuel flow, oncoming seizure, stress overload,
upper respiratory problems
Captain First Officer attention, Terminal Radar Approach Control Facility distraction, close enough attention, consequently attention, good enough attention, lip service, mind attention, much
mind, real attention, real close attention
Continued next page

620

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Shaping Factor
Pressure

Proficiency

Resource
Deficiency

Taskload

Unexpected

New words
Minimum Equipment List pressure, consistent answer, elevator pressure, intense pressure, part # Coordinated Universal Time, repercussion, right engine pressure, significant pressure, slow gear, wheel
pressure
& P school, CL65 ground school, basic flight training, hard lesson,
initial annual proficiency training, occurrence strive, rating
training, several military flying clubs, situation event, time limited
simulator sessions
Air Traffic Control loss, altitude deviation/loss, apparently inoperative, either inoperative, even reexamining, intermittent inoperative, known traffic conflict loss, observed loss, recently Los,
thankfully accurate
16500 # turboprop, A320 type aircraft, AVIAT husky A1 two place
tail DRAGGER aircraft, Cessna 402 type aircraft, Cessna model
421 type aircraft, L1011-250, LGA-MHT flight, McDonnell Douglas
MD11, solo cross country FLTS, solo cross country privileges
significant, 1 jolt, approximately 5-10 sec, choppy aircraft, consistently moderate, contributing workload factor, industry issue,
severe, rapid immediate, real cushion

621

fiAbedin, Ng & Khan

Appendix D: Additional Stratified Approximate Randomization Tests

MLW

OLW

SVM-U

SVM-UB

SVM-L

SVM-UL

SVM-UBL

SVMT-U

SVMT-UB

SVMT-L

SVMT-UL

SVMT-UBL

SVM5-U

SVM5-UB

SVM5-L

SVM5-UL

SVM5-UBL

Methoda
SW
MLW
OLW
SVM-U
SVM-UB
SVM-L
SVM-UL
SVM-UBL
SVMT-U
SVMT-UB
SVMT-L
SVMT-UL
SVMT-UBL
SVM5-U
SVM5-UB
SVM5-L
SVM5-UL
SVM5-UBL

SW

ascertain statistical significance difference F-measure scores
different report labeling methods, performed stratified approximate randomization
test 9,999 shuffles pairs results Experiments 1 5 Table 4.
table shows method column statistically significantly better
method row level p < 0.05. before, statistical significance
insignificance denoted X X, respectively.

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
-

a. Legend: SW = Occurrence Heuristic using seed words, MLW = Occurrence Heuristic using modified
Basilisk lexicon, OLW = Occurrence Heuristic using original Basilisk lexicon, SVM-U = SVM using
unigrams, SVM-UB = SVM using unigrams bigrams, SVM-L = SVM using lexicon words, SVM-UL
= SVM using unigrams lexicon words, SVM-UBL = SVM using unigrams, bigrams lexicon
words, SVMT-U = transductive SVM using unigrams, SVMT-UB = transductive SVM using unigrams
bigrams, SVMT-L = transductive SVM using lexicon words, SVMT-UL = transductive SVM using
unigrams lexicons, SVMT-UBL = transductive SVM using unigrams, bigrams lexicon words,
SVM5-U = 5-fold SVM using unigrams, SVM5-UB = 5-fold SVM using unigrams bigrams, SVM5-L
= 5-fold SVM using lexicon words, SVM5-UL = 5-fold SVM using unigrams lexicon words, SVM5UBL = 5-fold SVM using unigrams, bigrams lexicon words.

622

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Appendix E: Sample Preprocessed Reports
Report ACN#324831
RETURNING waukegan regional airport practice area located 5-20 mile
W airport; flying solo student pilot; 3000 feet Mean Sea Level Visual
Flight Rules. cloud area 5 mile W airport obscured view ahead reduced
altitude proceed Visual Flight Rules returned 3000 feet passing thin
cloud line. area n; containing fix references airport location; shrouded
clouds fog ground level. true lake michigan shoreline
E. ground substantially snow covered. although airspace airport
undoubtedly clear; practice area; orientation field lost me.
climbed 4500 feet increase overview; without benefit. returning 3000 feet;
flew believed n airfield landfall airport. must
S; however; proceeding flew ord class B airspace. coinciding
lost; contacted waukegan tower; realizing flown Federal Aviation
Regulation had. directed contact ord approach frequency given;
beginning ord approach vectored back waukegan airport; frequency changed
tower control blessedly cleared land. time lost 1 hour 15 minutes
1 1/2 hours.
Report ACN#566757
following event occurred REPOSITIONING; taxi; W side
side isp airport. initially contacted longitude island tower asking permission REPOS
W side OPS Base Operations Office tower (the side). controller
replied start taxi via taxiway W hold short runway 6. read back
instructions stating start taxi via taxiway W holding short runway 6. taxiing;
aircraft taxiway W holding short runway 6; performing run-up.
controller asked able get around aircraft. replied able.
controller said use caution taxiing around aircraft cross runway 6.
taxiing across runway 6; noticed aircraft short final runway 6. clear
runway aircraft touched down. controller came frequency
said instructed hold short runway 6. replied cleared across
runway 6. controller said call tower park. replied roger; call
park. called talked controller minutes later said
instructed hold short runway 6. told cleared across
runway. feel pilots controllers need listen decipher
said acting it.

623

fiAbedin, Ng & Khan

Appendix F: Lexicon Learned Original Basilisk Categories People
Equipment
following table shows words phrases learned original Basilisk framework
categories people equipment (see Section 6.7.4).
Category
People

Equipment

New words
Agreed judges correct: ABQ tower procedure specialist, ACN 126721 reporter, AFSFO, AVP tower specialist, Air Route
Traffic Control Center specialist, Air Traffic Control facility reps,
BDR tower specialist, BHM control, BUF field operations officer,
CAE tower specialist, Chicago quality control, DFW maintenance
manager, Flight Service Station dispatcher, SFOLM Captain,
SII program manager, Stearman pilot, TLH supervisor, bur local
controller, casino manager, cos Air Traffic Control chief, flight test
engineers, local balloon repairman, outbound Captain First Officer, repair facility pilot, shift boss, spokesperson, station management individual, technician desk, tower supervisor/manager
Identified one judge correct: Flight Standards District
Office ORL, maintenance supervisor, approach controller verbatim, freighter aircraft approach, tower, passenger
fatigue
Agreed judges incorrect: ACN 670635, ACN 682482,
AT6 aircraft, B737-300/500 SRM, EMB service manual, Non-air
carrier aircraft, RPTR ACN 518698, RPTR ACN 601074, RPTR
ACN 658075, RPTR ACN 664336, RPTR ACN 676343, RPTR ACN
88920, cabin company, aircraft center, reliable research resources
Agreed judges correct: Collision Avoidance System II 10 Distance Measuring Equipment screen, Collision Avoidance
System II B737-200, Collision Avoidance System II EHSI, Collision
Avoidance System II IVSI display, Collision Avoidance System II
Missed Approach Point page, Collision Avoidance System II RR,
Continued next page

624

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Category

New words
Identified one judge correct: Collision Avoidance System II VSI overlay, Resolution Advisory stopped aircraft, Collision Avoidance System II stop climb alert, Collision Avoidance
System II traffic ; climb advisory, Collision Avoidance System II traffic ; traffic aural warning, Collision Avoidance System
II 3 mile circle, Collision Avoidance System II 5 mile scale, Collision Avoidance System II 6 mile scale, Collision Avoidance System II POPUP traffic, Collision Avoidance System II Resolution
Advisory alerts, Collision Avoidance System II Resolution Advisory
climb priority, Collision Avoidance System II Resolution Advisory
climb warning, Collision Avoidance System II Resolution Advisory
signals, Collision Avoidance System II Resolution Advisory zone,
Collision Avoidance System II Resolution Advisory/Traffic Advisory
alert, Collision Avoidance System II Resolution Advisory/Traffic Advisory alerts/advisories, Collision Avoidance System II Resolution
Advisory/altitude deviation, Collision Avoidance System II Traffic
Advisory Resolution Advisory alerts, Collision Avoidance System II WINDSHEAR warning, Collision Avoidance System II advisory alert, Collision Avoidance System II advisory alert warning,
Collision Avoidance System II warning aircraft
Agreed judges incorrect: Collision Avoidance System II 10 Oclock 2 1/2 3 mile, Collision Avoidance System II Resolution Advisory climb command, Collision Avoidance
System II Resolution Advisory area, Collision Avoidance System
II Resolution Advisory climb descent, Collision Avoidance System II Resolution Advisory data tag, Collision Avoidance System
II Resolution Advisory descent, Collision Avoidance System II Resolution Advisory green band target, Collision Avoidance System II
Resolution Advisory increase climb, Collision Avoidance System II
Resolution Advisory maneuvering, Collision Avoidance System II
Resolution Advisory messages, Collision Avoidance System II Resolution Advisory recovery procedure, Collision Avoidance System
II Resolution Advisory requirement, Collision Avoidance System II
Resolution Advisory requiring climb, Collision Avoidance System
II Resolution Advisory resolution, Collision Avoidance System II
Traffic Advisory notification, Collision Avoidance System II Traffic
Advisory/Resolution Advisory aircraft, Collision Avoidance System
II Traffic Advisory/Resolution Advisory event, Collision Avoidance
System II action requirements, Collision Avoidance System II advice,
Collision Avoidance System II advisories instructions, Collision
Avoidance System II caution, Collision Avoidance System II quit

625

fiAbedin, Ng & Khan

Appendix G: Lexicon Learned Modified Basilisk Categories People
Equipment
following table shows words phrases learned modified Basilisk framework
categories people equipment (see Section 6.7.4).
Category
People

Equipment

New words
Agreed judges correct: First Officer, First Officer, ; First Officer, CP, Captain, Captain RPTR, Captain trainee,
Co-Captain, Co-pilot, First Officer, First Officer # 2, Initial Operating Experience Captain, PAXS, Pilot Flying First Officer,
Potomac controller, RPTING Captain, RPTING First Officer, RPTING pilot, RPTR Captain, RPTR pilot, S/O, ZOA supervisor, air
carrier pilot, aircraft X pilot, aircraft commander, passenger, analyst, First Officer, baron pilot, biplane pilot, controller,
facility person, first observer, flight attendant # 3, flight attendants
passenger, flying Captain, forward observer, passenger, passenger crew, passenger flight attendants, right seat pilot, second observer, sic, specialist, student Captain, supervisor/Controller,
tower Controller, tower operator, training pilot
Identified one judge correct: RPTR, gate passenger,
First Officer
Agreed judges incorrect: departure departure,
neither Captain, CLRLY
Agreed judges correct: # 1 Auto-Pilot, #
2 Auto-Pilot, 3 AUTOPLTS, AUTOFLT, AUTOTHROTTLE,
AUTOTHROTTLE Auto-Pilot, AUTOTHROTTLES, AUTOTHROTTLES Auto-Pilot, AUTOTHRUST, Auto-Pilot
# 1, Auto-Pilot # 2, Auto-Pilot B, Auto-Pilot AUTOTHRUST, Auto-Pilot PMs, Auto-Pilot throttles, AutoPilot/AUTOTHROTTLES, Cessna 180, Collision Avoidance System
II system, ENGS # 2 # 3, aircraft ABCD, aircraft Auto-Pilot,
aircraft engine, allowed aircraft, automatic pilot, automatic throttle,
automatic throttles, autopilot, center Auto-Pilot, craft, emergency
engine, left Auto-Pilot, left hand engine, parked plane, right AutoPilot
Identified one judge correct:
problem engine, maintenance aircraft, later aircraft, aircraft aircraft, Collision
Avoidance System II alert, # 1 Constant Speed Drive, Auto-Pilot
AUTOTHROTTLES, WDB 2, perf
Agreed judges incorrect: aircraft beginning, aircraft
parallel, normal aircraft, person property, persons property,
aircraft, time aircraft

626

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

References
Ando, R. K. (2004). Semantic lexicon construction: Learning unlabeled data via
spectral analysis. Proceedings 8th Conference Computational Natural
Language Learning, pp. 916.
Artstein, R., & Poesio, M. (2008). Inter-coder agreement computational linguistics.
Computional Linguistics, 34 (4), 555596.
Avancini, H., Lavelli, A., Sebastiani, F., & Zanoli, R. (2006). Automatic expansion
domain-specific lexicons term categorization. ACM Transactions Speech
Language Processing (TSLP), 3 (1), 130.
Baker, L. D., & McCallum, A. K. (1998). Distributional clustering words text classification. Proceedings 21st Annual International ACM SIGIR Conference
Research Development Information Retrieval, pp. 96103.
Banko, M., & Brill, E. (2001). Mitigating paucity-of-data problem: Exploring effect training corpus size classifier performance natural language processing.
Proceedings 1st International Conference Human Language Technology
Research.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,
Philadelphia, USA.
Cheng, P. W. (1997). covariation causation: causal power theory. Psychological
Review, 104 (2), 367405.
Chinchor, N. (1992). statistical significance MUC-4 results. Proceedings
4th Message Understanding Conference, pp. 3050.
Chinchor, N., Hirschman, L., & Lewis, D. D. (1993). Evaluating message understanding systems: analysis Third Message Understanding Conference (MUC-3).
Computational Linguistics, 19, 409449.
Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20 (3), 273
197.
Crammer, K., & Singer, Y. (2002). algorithmic implementation multiclass kernelbased vector machines. Journal Machine Learning Research, 2, 265292.
Curran, J. R., & Moens, M. (2002). Improvements automatic thesaurus extraction.
Proceedings ACL 2002 Workshop Unsupervised Lexical Acquisition, pp.
5966.
Curran, J. R., Murphy, T., & Scholz, B. (2007). Minimising semantic drift mutual
exclusion bootstrapping. Proceedings 10th Conference Pacific Association Computational Linguistics, pp. 172180.
627

fiAbedin, Ng & Khan

Davidov, D., & Rappoport, A. (2006). Efficient unsupervised discovery word categories
using symmetric patterns high frequency words. Proceedings 21st International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 297304.
Dietterich, T. G. (1998). Approximate statistical tests comparing supervised classification learning algorithms. Neural Computation, 10 (7), 18951923.
Dunning, T. (1993). Accurate methods statistics surprise coincidence. Computational Linguistics, 19 (1), 6174.
Everitt, B. S. (1977). Analysis Contingency Tables. Chapman Hall.
Ferryman, T. A., Posse, C., Rosenthal, L. J., Srivastava, A. N., & Statler, I. C. (2006).
happened, why: Toward understanding human error based automated
analyses incident reports Volume II. Tech. rep. NASA/TP2006-213490, National
Aeronautics Space Administration.
Ganeri, J., Noordhof, P., & Ramachandran, M. (1996). Counterfactuals preemptive
causation. Analysis, 56 (4), 219225.
Garcia, D. (1997). COATIS, NLP system locate expressions actions connected
causality links. Proceedings 10th European Workshop Knowledge
Acquisition, Modeling Mangement, pp. 347352.
Girju, R. (2003). Automatic detection causal relations question answering. Proceedings ACL 2003 Workshop Multilingual Summarization Question
Answering, pp. 7683.
Godbole, S., & Sarawagi, S. (2004). Discriminative methods multi-labeled classification.
Proceedings 8th Pacific-Asia Conference Knowledge Discovery Data
Mining, pp. 2230.
Griffiths, T. L., & Tenenbaum, J. B. (2005). Structure strength causal induction.
Cognitive Psychology, 51, 334384.
Grishman, R., & Ksiezyk, T. (1990). Causal temporal text analysis: role
domain model. Proceedings 13th International Conference Computational
Linguistics, pp. 126131.
Halpern, J. Y., & Pearl, J. (2005). Causes explanations: structural-model approach.
Part I: Causes. British Journal Philosophy Science, 56, 843887.
Hassan, H., Hassan, A., & Emam, O. (2006). Unsupervised information extraction approach
using graph mutual reinforcement. Proceedings 2006 Conference Empirical
Methods Natural Language Processing, pp. 501508.
Hume, D. (1999 (Original work published 1748)). Enquiry Concerning Human Understanding. Oxford University Press, USA.
Hume, D. (2000 (Original work published 1739)). Treatise Human Nature. Oxford
University Press, USA.
Joachims, T. (1999). Advances Kernel Methods - Support Vector Learning, chap. Making
large-Scale SVM Learning Practical. MIT-Press.
628

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Joachims, T. (1998). Text categorization suport vector machines: Learning many
relevant features. Proceedings 10th European Conference Machine Learning, pp. 137142.
Joachims, T. (1999). Transductive inference text classification using support vector
machines. Proceedings 16th International Conference Machine Learning,
pp. 200209.
Kaplan, R., & Berry-Rogghe, G. (1991). Knowledge-based acquisition causal relationships
text. Knowledge Acquisition, 3 (3), 317337.
Kersey, C., Di Eugenio, B., Jordan, P., & Katz, S. (2009). KSC-PaL: peer learning agent
encourages students take initiative. Proceedings 4th Workshop
Innovative Use NLP Building Educational Applications, pp. 5563.
Khoo, C. S. G., Chan, S., & Niu, Y. (2000). Extracting causal knowledge medical
database using graphical patterns. Proceedings 38th Annual Meeting
Association Computational Linguistics, pp. 336343.
Ko, Y., Park, J., & Seo, J. (2004). Improving text categorization using importance
sentences. Information Processing Management, 40 (1), 6579.
Krippendorff, K. (2004). Content analysis: introduction methodology. Sage Publications, Inc.
Lewis, D. (1973). Causation. Journal Philosophy, 70 (17), 556567.
Lin, D. (1998). Dependency-based evaluation MINIPAR. Proceedings LREC
Workshop Evaluation Parsing Systems, pp. 317329.
Lin, D., & Pantel, P. (2001). Induction semantic classes natural language text.
Proceedings 7th ACM SIGKDD International Conference Knowledge Discovery Data Mining, pp. 317322.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated
corpus English: Penn Treebank. Computational Linguistics, 19 (2), 313330.
Special Issue Using Large Corpora.
McCallum, A., & Nigam, K. (1999). Text classification bootstrapping keywords,
EM shrinkage. Proceedings ACL Workshop Unsupervised Learning
Natural Language Processing, pp. 5258.
Noreen, E. W. (1989). Computer-Intensive Methods Testing Hypotheses : Introduction. Wiley.
Pantel, P., & Lin, D. (2002). Discovering word senses text. Proceedings 8th
ACM SIGKDD International Conference Knowledge Discovery Data Mining,
pp. 613619.
Pantel, P., & Ravichandran, D. (2004). Automatically labeling semantic classes. Proceedings Human Language Technology Conference North American Chapter
Association Computational Linguistics, pp. 321328.
Passonneau, R. (2004). Computing reliability coreference annotation. Proceedings
Fourth International Conference Language Resources Evaluation, Vol. 4,
pp. 15031506.
629

fiAbedin, Ng & Khan

Patwardhan, S., & Riloff, E. (2006). Learning domain-specific information extraction patterns web. Proceedings COLING/ACL Workshop Information
Extraction Beyond Document, pp. 6673.
Patwardhan, S., & Riloff, E. (2007). Effective information extraction semantic affinity
patterns relevant regions. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language
Learning, pp. 717727.
Pereira, F. C. N., Tishby, N., & Lee, L. (1993). Distributional clustering English words.
Proceedings 31st Annual Meeting Association Computational Linguistics, pp. 183190.
Phan, X.-H. (2006a). CRFChunker: CRF English Phrase Chunker. http://crfchunker.
sourceforge.net/.
Phan, X.-H. (2006b). CRFTagger: CRF English POS Tagger.
sourceforge.net/.

http://crftagger.

Phillips, W., & Riloff, E. (2007). Exploiting role-identifying nouns expressions information extraction. Proceedings International Conference Recent Advances
Natural Language Processing.
Polanyi, L., & Zaenen, A. (2006). Contextual valence shifters. Computing Attitude
Affect Text: Theory Applications, pp. 110. Springer Verlag.
Posse, C., Matzke, B., Anderson, C., Brothers, A., Matzke, M., & Ferryman, T. (2005).
Extracting information narratives: application aviation safety reports.
Proceedings 2005 IEEE Aerospace Conference, pp. 36783690.
Read, J., Pfahringer, B., & Holmes, G. (2008). Multi-label classification using ensembles
pruned sets. Proceedings 8th IEEE International Conference Data
Mining, pp. 9951000.
Riloff, E. (1996). Automatically generating extraction patterns untagged text.
Proceedings 13th National Conference Artificial Intelligence, pp. 10441049.
Roark, B., & Charniak, E. (1998). Noun-phrase co-occurrence statistics semi-automatic
semantic lexicon construction. Proceedings 17th International Conference
Computational Linguistics, pp. 11101116.
Rohwer, R., & Freitag, D. (2004). Towards full automation lexicon construction.
Proceedings Computational Lexical Semantics Workshop HLT-NAACL 2004,
pp. 916.
Rychly, P., & Kilgarriff, A. (2007). efficient algorithm building distributional
thesaurus (and Sketch Engine developments). Proceedings ACL 2007
Demo Poster Sessions, pp. 4144.
Sebastiani, F. (2002). Machine learning automated text categorization. ACM Computing
Surveys, 34 (1), 147.
Sundheim, B. M. (1992). Overview fourth message understanding evaluation
conference. Proceedings Fourth Message Understanding Conference, pp. 3
21.
630

fiCause Identification via Weakly Supervised Semantic Lexicon Construction

Tang, L., Rajan, S., & Narayanan, V. K. (2009). Large scale multi-label classification via
metalabeler. Proceedings International World Wide Web Conference, pp. 211
220.
Thelen, M., & Riloff, E. (2002). bootstrapping method learning semantic lexicons
using extraction pattern contexts. Proceedings 2002 Conference Empirical
Methods Natural Language Processing, pp. 214221.
Tsoumakas, G., & Katakis, I. (2007). Multi-label classification: overview. International
Journal Data Warehousing Mining, 3 (3), 113.
Tsoumakas, G., & Vlahavas, I. P. (2007). Random k -labelsets: ensemble method
multilabel classification. Proceedings 18th European Conference Machine
Learning, Vol. 4701 Lecture Notes Computer Science, pp. 406417.
Ueda, N., & Saito, K. (2002). Parametric mixture models multi-labeled text. Advances
Neural Information Processing Systems 15, pp. 721728.
van Delden, S., & Gomez, F. (2004). Retrieving NASA problem reports: case study
natural language information retrieval. Data Knowledge Engineering, 48 (2),
231246.
Vapnik, V. N. (1998). Statistical Learning Theory. Wiley.
Yang, Y., & Pedersen, J. O. (1997). comparative study feature selection text categorization. Proceedings 14th International Conference Machine Learning,
pp. 412420.
Yangarber, R. (2003). Counter-training discovery semantic patterns. Proceedings
41st Annual Meeting Association Computational Linguistics, pp.
343350.
Zaidan, O. F., Eisner, J., & Piatko, C. (2007). Using annotator rationales improve machine learning text categorization. Proceedings Human Language Technology Conference North American Chapter Association Computational
Linguistics, pp. 260267.
Zhang, Q., Zhou, Y., Huang, X., & Wu, L. (2008). Graph mutual reinforcement based
bootstrapping. Information Retrieval Technology, 4993/2008, 203212.

631


