journal artificial intelligence

submitted published

coactive learning
pannaga shivaswamy

pshivaswamy linkedin com

linkedin corporation
stierlin ct
mountain view ca usa

thorsten joachims

tj cs cornell edu

department computer science
cornell university
ithaca ny usa

abstract
propose coactive learning model interaction learning system
human user common goal providing maximum utility
user interactions coactive learning model take following form
step system e g search engine receives context e g query predicts object
e g ranking user responds correcting system necessary providing slightly
improved necessarily optimal object feedback argue preference
feedback inferred large quantity observable user behavior e g clicks
web search unlike optimal feedback required expert model cardinal
valuations required bandit learning despite relaxed requirements feedback
possible adapt many existing online learning
coactive

framework particular provide achieve average regret
terms cardinal utility even though learning never observes cardinal utility
values directly provide log average regret
case strongly convex loss functions extensive empirical study demonstrates
applicability model movie recommendation task well
ranking web search

introduction
wide range systems use today interaction human system takes
following form user issues command e g query receives possibly
structured response e g ranking user interacts e g
clicks thereby providing implicit feedback users utility function three
examples systems typical interaction patterns
web search response query search engine presents ranking b c
observes user clicks documents b
movie recommendation online service recommends movie user however
user rents movie b browsing collection
machine translation online machine translator used translate wiki page
language b system observes corrections user makes translated text
c

ai access foundation rights reserved

fishivaswamy joachims

examples user provides feedback
system however feedback incremental improvement necessarily
optimal example clicks web search infer
user would preferred ranking b c one presented however
unlikely best possible ranking similarly recommendation example
movie b preferred movie may even better movies
user browsing machine translation example corrected text need best possible translation language language b
three examples typically receives slightly improved user
feedback necessarily optimal prediction cardinal utilities conjecture
many applications fall schema ranging news filtering personal
robotics
propose coactive learning model system user interactions
formalize coactive learning general model interaction learning system
user define suitable notion regret validate key modeling assumption
namely whether observable user behavior provide valid feedback model
user study web search model viewed cooperative learning process
system user parties aim optimize utility lack means
achieve goal specifically boundedly rational user computationally
limited maximizing utility space alternatives system limited
well knows users utility function
proposed online learning framework differs significantly existing online learning
terms observed feedback see related works section comparison
strength proposed framework possible derive wide range coactive
learning adapting existing online convex optimization
provide template coactive learning several instances
template case prove worst case analysis
carries conventional online learning framework coactive
learning despite differences two particular cases linear
utility convex cost functions regret bounds matching
lower bound regret bound improved second order
strongly convex functions learning perform structured output
prediction see bakir hofmann scholkopf smola taskar vishwanathan thus
applied wide variety study several interesting extensions
framework batch updates expected feedback exponentiated learning
finally provide extensive empirical evaluations movie
recommendation web search task showing highly efficient
effective practical settings
rest organized follows discuss related work section
section formally introduce coactive learning model motivate model
real world user study present linear version along
several extensions section section detail general schema deriving
coactive learning regret bounds particular derive exponentiated gradient section propose coactive learning
minimizing general convex losses strongly convex losses sections


ficoactive learning

empirical evaluation proposed framework done section
conclude section include proofs appendix

related works
coactive learning model bridges gap two forms feedback
well studied online learning one side multi armed bandit model e g
auer cesa bianchi freund schapire b auer cesa bianchi fischer
chooses action observes utility action
side utilities possible actions revealed case learning
expert advice e g cesa bianchi lugosi online convex optimization zinkevich
hazan agarwal kale online convex optimization bandit setting
flaxman kalai mcmahan continuous relaxations expert
bandit respectively model information two arms revealed
iteration one presented one receive feedback user
sits expert bandit setting closely related coactive learning
dueling bandits setting yue broder kleinberg joachims yue joachims
key difference arms chosen dueling
bandits setting whereas one arms chosen user coactive learning
setting model allows contextual information contextual bandits langford
zhang however arms structured objects rankings
summary framework compares existing frameworks shown
table types feedback explored literature example
multi class classification makes prediction
context feedback received whether prediction correct wrong opposed
actual label crammer gentile kakade shalev shwartz tewari
seen observing partial feedback opposed actual cardinal feedback
bandit
pointed coactive learning conventional online learning
operate different types environments coactive learning present
object observe another object feedback online convex learning
pick vector step observe gradient vector feedback despite
contrast online learning coactive learning two presented
closely related work zinkevich hazan et al
possible adapt regret bounds
corresponding regrets bounds coactive learning heart
analysis well known idea polyak tsypkin descent
necessarily need know gradients vector positive inner product
gradient expectation suffices
feedback coactive learning takes form preference different
ordinal regression ranking ordinal regression e g crammer singer assumes
training examples x rank coactive learning model absolute ranks
never revealed closely related learning pairs examples herbrich graepel obermayer freund iyer schapire singer chu ghahramani
since circumvents need absolute ranks relative orderings required vari

fishivaswamy joachims

framework
bandits
experts
dueling bandits
coactive learning


pull arm
pull arm
pull two arms
pull arm

feedback
observe cardinal reward arm pulled
observe cardinal rewards arms
observe feedback one better
observe another arm better

table comparison different online learning frameworks

ants pairwise ranking applied natural language processing
haddow arun koehn zhang lei barzilay jaakkola globerson
image annotation weston bengio usunier however existing approaches require
iid assumption typically perform batch learning finally large body
work ranking see liu approaches different coactive learning
require training data x optimal ranking query x however
draw upon structured prediction approaches ranking design

coactive learning first proposed shivaswamy joachims
serves journal extension adding complete discussion batch updates
expected feedback exponentiated gradient log
strongly convex loss functions substantially extended empirical evaluation
since coactive learning applied intrinsically diverse retrieval raman
shivaswamy joachims learning ranking function click feedback raman
joachims shivaswamy schnabel optimizing social welfare raman joachims
personal robotics jain wojcik joachims saxena pattern discovery boley
mampaey kang tokmakov wrobel robotic monitoring somers hollinger
extended allow approximate inference goetschalckx fern tadepalli

coactive learning model
introduce coactive learning model interaction rounds learning
system e g search engine human search user human learning
goal obtaining good round learning
observes context xt x e g search query presents structured object
yt e g ranked list urls utility yt user context xt x
described utility function u xt yt unknown learning
feedback human user returns improved object yt e g reordered list urls
e
u xt yt u xt yt



object yt exists fact allow violations formally
model user feedback section
process user generates feedback yt understood
approximately utility maximizing action user modeled boundedly rational
improvements strict margin clear section



ficoactive learning

agent particular user selects feedback object yt approximately maximizing
utility user defined subset yt possible
yt argmaxyy u xt



approximately boundedly rational user may employ tools e g query
reformulations browsing construct subset perform search importantly
however feedback yt typically optimal label defined
yt argmaxyy u xt



way coactive learning covers settings user cannot manually optimize
argmax full e g produce best possible ranking web search
difficulty expressing bandit style cardinal rating u xt yt yt consistent manner
puts preference feedback yt stark contrast supervised learning approaches
require xt yt even importantly model implies reliable preference feedback derived observable user behavior e clicks
demonstrate section web search conjecture similar feedback strategies
exist many application settings e g jain et al boley et al somers
hollinger goetschalckx et al especially users assumed act
approximately boundedly rational according u
despite weak preference feedback aim nevertheless present
objects utility close optimal yt whenever presents
object yt context xt say suffers regret u xt yt u xt yt time
step formally consider average regret suffered steps
follows
regt


x
u xt yt u xt yt






goal learning minimize regt thereby providing human
predictions yt high utility note however cardinal value u never observed
learning u revealed ordinally preferences
quantifying preference feedback quality
provide theoretical guarantees regret learning coactive
setting need quantify quality user feedback note quantification
tool theoretical analysis prerequisite parameter quantify
feedback quality much improvement provides utility space simplest case
say user feedback strictly informative following inequality satisfied
u xt yt u xt yt u xt yt u xt yt



inequality unknown parameter feedback utility
yt higher yt fraction maximum possible utility range u xt yt
u xt yt term right hand side inequality ensures user feedback


fishivaswamy joachims

yt better yt better margin u xt yt u xt yt violations
feedback model allowed introducing slack variables
u xt yt u xt yt u xt yt u xt yt



note restricted positive negative well refer
feedback model informative feedback note possible express
feedback quality appropriate value regret bounds
contain quantifying extent informative modeling assumption violated
finally consider even weaker feedback model positive utility
gain achieved expectation user actions
et u xt yt u xt yt u xt yt u xt yt



refer feedback expected informative feedback equation
expectation users choice yt given yt context xt e
distribution pxt yt yt dependent xt
rest use linear model utility function
u x w x



w rn parameter vector unknown learning system users
x rn known joint feature map known system
k x k r



x x note x structured objects
user study preferences clicks
validate reliable preferences specified equation indeed inferred
implicit user behavior particular focus preference feedback clicks
web search draw upon data user study joachims granka pan hembrooke
radlinski gay study subjects undergraduate students n
asked answer identical questions informational navigational google
search engine queries lists clicks recorded subject queries
grouped query chains question average query chain contained
queries clicks lists
use following strategy infer ranking users clicks prepend
ranking first query chain user clicked throughout
whole query chain assess whether u x indeed larger u x assumed
learning model measure utility terms standard
measure retrieval quality
p r x
information retrieval use dcg x log r x
relevance score th document ranking see manning raghavan schutze
strictly speaking value slack variable depends choice definition utility
however brevity explicitly dependence notation
make slightly different assumption section
done manually automated high accuracy jones klinkner



ficumulative distribution function

coactive learning













normal condition
swapped condition
reversed condition
conditions











dcg x ybar dcg x









figure cumulative distribution utility differences presented ranking
click feedback ranking terms dcg three experimental conditions
overall
get ground truth relevance assessments r x five independent human assessors
students asked manually rank set encountered query
chain assessors given true answers navigational queries
linearly normalize resulting ranks relative relevance score r x
document
evaluate whether feedback ranking indeed better ranking
originally presented e dcg x dcg x figure plots
cumulative distribution functions cdfs dcg x dcg x three
experimental conditions well average conditions cdfs shifted far
right showing preference feedback strategy highly accurate
informative focusing first average conditions utility difference strictly
positive queries strictly negative imbalance
significant binomial sign test p among remaining cases
dcg difference zero due e click top click
note learning easily detect cases may explicitly eliminate
feedback overall shows implicit feedback indeed produce accurate
preferences
remains shown whether reliability feedback affected
quality current prediction e u xt yt user study users actually
received retrieval quality degraded purpose particular
one third subjects received googles top reverse order condition reversed another third received rankings top two positions swapped condition
swapped figure shows users provide accurate preferences across
substantial range retrieval quality intuitively worse retrieval system may make
harder good makes easier baseline yt improve upon
intuition formally captured definition informative feedback optimal value
vs trade however likely depend many application specific factors
user motivation corpus properties query difficulty following therefore
present require knowledge theoretical bounds hold
value experiments explore large range


fishivaswamy joachims

preference perceptron
initialize w

observe xt
present yt argmaxyy wt xt
obtain feedback yt
update wt wt xt yt xt yt
end

preference perceptron coactive learning
section start presenting analyzing basic coactive learning model call preference perceptron preference
perceptron maintains weight vector wt initialized time step
observes context xt presents object maximizes wt xt
observes user feedback yt weight vector wt updated
direction xt yt xt yt
although update preference perceptron appears similar standard perceptron multi class classification key differences first standard
perceptron requires true label feedback whereas much weaker feedback
suffices second standard analysis perceptron bounds
number mistakes made margin radius examples
contrast analysis bounds different regret captures graded notion utility
standard perceptron mistake bound novikoff contains r kwk
bound following theorem contains rkwk r defined
theorem average regret preference perceptron upper bounded
w follows


x
rkw k

regt







proof first consider kwt k
wt wt wt wt wt xt yt xt yt
xt yt xt yt xt yt xt yt
wt wt r r
line one simply used update rule line two used
fact wt xt yt xt yt choice yt
k x k r update rule
wt w wt w xt yt xt yt w



x

u xt yt u xt yt







ficoactive learning

use fact wt w kw kkwt k cauchy schwarz inequality
implies

x

u xt yt u xt yt r kw k


informative modeling user feedback



x

u xt yt u xt yt




x


r kw k



claimed follows
first term regret bound denotes quality feedback terms violation
informative feedback particular user feedback strictly
informative

slack variables vanish regt
trivial design even better regret strict informative
assumption cardinality context set x finite one interesting aspects
bound theorem subsequent minimize
regret even context xt different every step thus x could infinite
regret bound still holds
note bound theorem holds w slacks
corresponding w
though user feedback modeled via informative feedback
require knowledge plays role analysis
far characterized user behavior terms deterministic feedback actions
however bound expected regret suffices weaker model expected informative feedback equation applicable
corollary expected informative feedback model expected regret
user behavior distribution preference perceptron upper bounded
follows


e regt

x rkw k









corollary proved following argument theorem taking
expectations user feedback
e wt wt e wt wt e wt xt yt xt yt
et xt yt xt yt xt yt xt yt
e wt wt r
e denotes expectation user feedback yt given yt context
xt follows e wt wt r


fishivaswamy joachims

batch preference perceptron
initialize w
l


observe xt
present yt argmaxyy wl xt
obtain feedback yt
k
p
update wl wl tj xj yj xj yj
l l
st
end
end

applying jensens inequality concave function get
q

e wt w kw ke kwt k kw k e wt wt
corollary follows definition expected informative feedback
lower bound
upper bound theorem cannot improved general respect
scaling following lemma given coactive learning
construct sequence examples
even feedback suffers

average regret
lemma coactive learning
linear utility exist xt objects
w regt steps
proof consider x x rt kxk define
joint feature map x yx consider contexts e et ej
j th component equal one others equal zero let
yt

sequence outputs



contexts
e





e

construct
w








yt construction kw k let user feedback
tth step yt choices user feedback
informative
pt

since yt yt yet regret w et yt w et yt

batch update
preference perceptron stated makes update every iteration
applications due high volumes feedback might possible
update frequently scenarios natural consider variant
makes update every k iterations simply uses wt obtained


ficoactive learning

generic template coactive learning
initialize w

observe xt
present yt argmaxyy wt xt
obtain feedback yt
perform update wt gradient w xt yt xt yt obtain wt
end
previous update next update type updates shown called
mini batch updates used distributed online optimization dekel giladbachrach shamir xiao steps batch update shown
easy following regret bound batch updates
lemma average regret batch preference perceptron upper
bounded w follows


x
rkw k k

regt






lemma implies mini batches slow learning factor equal
batch size see section empirically convergence substantially faster

deriving coactive learning
preference perceptron regret minimizes defined eqn
one point design space different regret definitions learning
coactive learning section outline general strategy deriving coactive
learning existing online optimization furthermore
demonstrate general notions regret meaningful feasible coactive
learning derive coactive learning general convex strongly convex
losses
coactive learning derive section follow general template
outlined initializing w iteration context xt observed
presents yt maximizing current utility estimate represented wt
feedback yt observed simply takes gradient w xt yt
xt yt uses update standard online convex optimization
obtain wt wt
case upper bound regret proposed derived
following strategy first start notion regret suited
coactive learning upper bound regret first reducing form
standard online convex opimization regret bound applied
gives regret bound original coactive turn case use
template derive specific however still provide self contained
proof appendix clearly pointing used regret bound
corresponding online convex optimization


fishivaswamy joachims

exponentiated preference perceptron
intialize w n


observe xt
present yt argmaxyy wt xt
obtain feedback yt

wt
wti exp xt yt xt yt zt zt weights add
one
end

exponentiated preference perceptron
illustrate generic strategy deriving coactive learning first derive
exponentiated gradient coactive learning used alternative
preference perceptron exponentiated gradient inherits ability
learn quickly sparse weight vectors
unlike additive updates preference perceptron exponentiated gradient
summarized performs multiplicative updates exponentiated
closely related exponentiated classification kivinen
warmuth start initializes weights uniformly subsequent update
step rate associated rate depends upper bound norm
features e k k time horizon multiplicative
update weights normalized sum one steps repeat
since updates multiplicative weights initially positive wt guaranteed
remain positive orthant note algoithm assumed
know standard techniques see cesa biachi lugosi b
convert dependence however extensions
focus
provide regret bound regret bounds
depended norm features bound exponentiated
depends norm features
theorem w rn kw k w expected informative feedback average expected regret exponentiated preference perceptron upper bounded

x
log


regt







e regt


x log









k x k


ficoactive learning

proof start regret coactive learning defined
regt




x
u xt yt u xt yt



x








u xt yt u xt yt



x


x




w xt yt







w xt yt


x








equation used definition informative feedback defined
eqn viewing exponentiated online gradient descent
easy derive following regret bound techniques initially introduced kivinen
warmuth



x
x



wt xt yt xt yt
u xt yt u xt yt log n






since could specific bound literature self contained proof provided
appendix proof regt first upper bounded terms difference
kl w wt kl w wt telescoping argument used get
observing wt xt yt xt yt get

x





u xt yt u xt yt log n





combining obtain average regret bound proof expected
regret bound analogous preference perceptron
theorem theorem bounds regret
terms noisein feedback first term additional terms converge zero
rate key difference theorem regret bound
exponentiated scales logarithmically number features
norm w advantageous optimal w sparse
convex preference perceptron
generalizing definition regret eqn allow every time step
unknown convex loss function ct r r determines loss
ct u xt yt u xt yt time difference utility yt
optimal yt functions ct assumed non increasing non increasing assumption
ct intuition loss higher u xt yt farther
u xt yt sub derivatives ct assumed bounded formally c
g r c denotes sub derivative ct
vector w determines utility yt context xt assumed closed


fishivaswamy joachims

convex preference perceptron
initialize w

set
observe xt
present yt argmaxyy wt xt
obtain feedback yt
update wt wt xt yt xt yt
project wt arg minub ku wt k
end
bounded convex set b whose diameter denoted b case convex losses
consider following notion regret


x
x

cregt
ct u xt yt u xt yt
ct








bound ct minimum possible convex loss since u xt yt u xt yt
never greater zero definition yt hence regret compares loss
best loss could achieved convex loss note
case ct definition regret reduces earlier definition regret
linear case eqn
minimizes average convex loss two differences
first rate associated update time
second every update resulting vector wt projected back set b
closely related online convex optimization propsed
zinkevich however online convex optimization assumes
gradient loss ct observed iteration doesnt observe
gradient directly observes improved object yt presenting object
yt
earlier regret bounds expressed terms slack variables however
following section bounds expressed terms clipped version
slack variables defined max
theorem convex preference perceptron informative feedback nonincreasing convex losses ct bounded sub derivative
w b



g x g
b
b r

cregt











similarly expected informative feedback



g x g
b
b r

e cregt












ficoactive learning

proof theorem provided appendix b idea proof
first divide time steps two types depending
nature feedback
pt
allows us upper bound cregt terms wt w xt yt xt yt
term upper bounded following argument zinkevich even
coactive learning framework
definition cregt eqn theorem upper bounds
average convex loss via minimum achievable loss quality feedback
previous theorem strict
informative feedback average loss approaches best achievable loss albeit larger constant factors
case linear utility bounds theorem theorem sufficient
average slack variables zero achieve zero regret however
case convex losses upper bound regret approaches zero average
clipped slack variables zero
second order preference perceptron
particular class convex functions turns give much stronger
regret bounds general convex losses improvement special class losses
parallels improvements online convex optimization general convex losses zinkevich
strongly convex losses hazan et al
definition convex function f r strongly convex points x
following condition satisfied fixed
f x f f x x


x




f x denotes sub derivative x
shows second order preference perceptron strongly convex losses
previous second order preference perceptron maintains
weight vector wt step presenting yt context xt still
previous however addition weight vector maintains
additional matrix constructed outer product vector xt yt
xt yt update step projection steps involve shown
closely related online convex optimization
proposed hazan et al however pointed case
observes user preference feedback step unlike online convex
optimization observe gradients still possible prove regret bound
strongly convex case following
theorem second order preference learning expected strongly
convex non increasing functions ct bounded sub derivatives




x g x g b
gn
r
cregt



log













r
x g x g b
gn
e cregt



log













fishivaswamy joachims

second order preference perceptron
intialize w

g

observe xt
present yt argmaxyy wt xt
obtain feedback yt
xt yt xt yt xt yt xt yt
update wt wt
xt yt xt yt
project wt arg minwb wt w wt w
end
initialization parameter shown
prove theorem appendix c proof theorem divide
time steps two types starting possible upper bound cregt
form resulting terms upper bounded similar arguments
online strongly convex optimization hazan et al
user feedback strictly informative w b first two
terms regret bound logt scaling however
linear dependence dimensionality joint feature map regret bound
second order preference perceptron
even though appears need invert matrix second order preference perceptron avoided since updates rank one
woodbury matrix inversion lemma


xt yt xt yt xt yt xt yt






xt yt xt yt xt yt xt yt

xt yt xt yt
xt yt xt yt



thus practice second order preference perceptron update
bt iteration nevertheless projection step obtain wt involves solving
quadratically constrained quadratic program b ball fixed radius still
takes n time hence second order preference perceptron computationally
demanding convex preference perceptron experiments
second order preference perceptron might still quite useful low noise data

experiments
empirically evaluate coactive learning two real world datasets
two datasets differ nature prediction feedback first dataset
operate structured objects rankings whereas second dataset atomic
items movies presented received feedback


ficoactive learning

datasets user feedback
first provide detailed description two datasets used experiments along provide details strategies used dataset
generating user feedback
structured feedback web search
first dataset publicly available dataset yahoo chapelle chang
learning rank web search dataset consists query url feature vectors denoted
xqi query q url relevance rating riq ranges zero irrelevant
four perfectly relevant pose ranking structured prediction defined
joint feature map follows
w q


x
w xqyi

log





equation denotes ranking yi index url
placed position ranking thus measure considers top five urls
query q computes score graded relevance note utility
function defined via feature map analogous dcg see manning et al
replacing relevance label linear prediction features
query qt time step coactive learning present ranking ytq
maximizes wt qt note merely amounts sorting documents
scores wt xqi done efficiently utility regret eqn
p
definition utility w q given tt w qt yqt qt yqt yqt
denotes optimal ranking respect w consider best least
squares fit relevance labels features entire dataset obtain
yqt eqn yqt argmaxyy w qt experiments query
ordering randomly permuted twenty times report average standard error

used following two user generating simulated user feedback
experiments first feedback model idealized version feedback whereas second
feedback directly relevance labels available dataset
strict informative feedback model user assumed provide
strictly informative feedback given value e slacks zero given predicted ranking yt user would go list found five urls
placed top list resulting yt satisfied strictly informative
feedback condition w r optimal w model assumes user
access w hence idealized feedback
noisy feedback depth k feedback model given ranking query
user would go list inspecting top k urls urls list
shorter specified k value five urls highest relevance labels riq
placed top five locations user feedback note produces noisy
feedback since linear model perfectly fit relevance labels dataset


fishivaswamy joachims

item feedback movie recommendation
contrast structured prediction previous dataset considered
second dataset atomic predictions namely movie recommendation iteration
movie presented user feedback consists single movie well
used movielens dataset grouplense org consists million ratings
movies rated users movie ratings range one five
randomly divided users two equally sized sets first set used obtain
feature vector xj movie j svd embedding method collaborative
filtering see bell koren eqn dimensionality feature vectors
regularization parameters chosen optimize cross validation accuracy
first dataset terms squared error second set users considered
recommending movies movie features xj experiment setup
simulates task recommending movies user movie features old
users
tx
user second set found best least squares approximation wi
j
users utility functions available ratings enabled us impute utility
values movies explicitly
rated user furthermore allowed us
p
x x average difference
measure regret user tt wi


utility recommended movie xt best available movie xt denote
best available movie time xt obtained eqn experiment
user gave particular movie feedback recommended movie
feedback movie removed set candidates subsequent recommendations
experiments report average regret values averaged users
test set
simulate user behavior considered following two feedback
dataset
strict informative feedback previous dataset model user
assumed provide strictly informative feedback given value e slacks
zero given predicted movie yt user assumed watch movie
already highest rating remaining corpus movies user
picks another movie corpus lowest utility still satisfies strict informative assumption model assumes user access w
hence idealized feedback
noisy feedback feedback model given movie user assumed
access actual rating movies available assumed
round imputed rating nearest legal rating value used two sub strategies
user provides feedback better feedback user provides
smallest rating actual rating rounded rating strictly better
rating best feedback user provides
highest rating actual rating rounded rating remaining corpus could
multiple movies satisfying criteria ties broken uniformly
random among movies note feedback model noisy feedback
due rounding movie ratings discrete values


ficoactive learning

preference perceptron
first set experiments analyze empirical performance scaling behavior
basic preference perceptron variants
strong versus weak feedback
goal first experiment explore regret changed feedback quality get feedback different quality levels used strict informative
feedback values
















avg util regret

avg dcg regret























































figure regret strict informative feedback values websearch left movie recommendation right
figure shows experiment three different values overall regret
typically substantially reduced tens hundreds iterations expected
regret lower compared regret lower values note however
difference two curves much smaller factor ten note
differences less prominent case web search strictly
informative feedback strictly informative feedback
user feedback model could providing much stronger feedback required
particular value expected theoretical bounds since user feedback
linear model noise utility regret approaches zero cases note
standard error plots giving indication statistical significance
left plots figure standard errors high lower iterations become lower
iterations plots rest error bars small
may difficult visually identify
rest strict informative feedback consider
unless explicitly mention otherwise
noisy feedback
previous experiment user feedback actual utility values computed
optimal w next study regret changes noisy feedback user behavior


fishivaswamy joachims

follow linear utility model web search dataset use noisy feedback
depths movie dataset use noisy feedback
better best variant




depth
depth



better
best



avg util regret

avg util regret






















































figure regret noisy feedback web search left movie recommendation
right

experiment shown figure first observation make
case web search regret values converge zero similarly
case movie recommendation regret values higher previous
experiment line theory shows regret converging
average slack variables user provide strict informative feedback
interestingly case web search average regret slightly higher
user goes greater depth providing feedback due fact relevance
labels dataset noisy user maximizes noisy utility larger set
urls selection true utility maximizers becomes less reliable degrades
user feedback quality
rest web search consider noisy feedback depth
case movie recommendation consider better version noisy feedback
unless explicitly mention otherwise
batch updates
section consider batch preference perceptron


regret bound section scales factor k strict informative feedback
update made every k iterations verify whether
empirical performance scales suggested bound web search movies
considered strict informative feedback noisy feedback types
feedback use batch perceptron values k report
resulting average regret
experiments shown figure figure expected
value k becomes smaller regret converges faster however observe


ficoactive learning





k
k
k
k



k
k
k
k



avg util regret

avg util regret






















































figure regret versus time batch updates strict informative feedback
web search left movie recommendation right

































k
k
k
k



avg util regret

avg util regret



k
k
k
k
























figure regret versus time batch updates noisy feedback web search
left movie recommendation right


empirical scaling k substantially better k factor suggested lemma
feasibility coactive learning systems
might impractical update every iteration
expected user feedback
user feedback deterministic experiments far sub section consider probabilistic feedback study behavior preference perceptron
recall provided upper bound expected regret expected user feedback
corollary
provide informative feedback expectation consider following strategy
given object yt context xt user would first generate deterministic feedback yt


fishivaswamy joachims

following strict informative feedback model web search
movie recommendation addition consider five randomly generated objects
feedback put uniform probability mass randomly generated objects
remaining mass deterministic feedback user feedback still
informative expectation


expct feedback
det feedback




expct feedback
det feedback



avg util regret

avg util regret





















































figure expected feedback versus deterministic feedback web search left movie
recommendation right

experiment shown figure reference plot
regret curve deterministic informative feedback seen
much difference deterministic expected feedback higher numbers
iterations seen regret converges zero even informative
feedback expectation suggested corollary
comparison ranking svm
compare several baselines starting conventional
ranking svm joachims repeatedly trained iteration previous
qt
svm model used present ranking user ysvm
user returns ranking
qt
ysvm strict informative feedback one experiment noisy
qt
qt
feedback pairs examples qt ysvm
qt ysvm
used training
pairs ranking svm note training ranking svm iteration would
prohibitive expensive since involves solving quadratic program cross validating
regularization parameter c thus retrained svm whenever examples
added training set first training first iteration
one pair examples starting random yq c value fixed
pairs examples reliable cross validation became possible
pairs training set c value obtained via five fold cross note case web search user model provide strictly informative larger




ficoactive learning

validation c value determined svm trained training
examples available time svm model used present rankings
next retraining




svm
pref perceptron



svm
pref perceptron



avg util regret

avg util regret



















































figure preference perceptron versus ranking svm strict informative feedback web search left movie recommendation right



svm
pref perceptron









avg util regret

avg util regret





































svm
pref perceptron

















figure preference perceptron versus ranking svm noisy feedback web search
left movie recommendation right

experiment shown figure figure case
strict informative feedback preference perceptron performed much better
svm movie recommendation comparably web search case
noisy feedback preference perceptron performs significantly better svm
range datasets took around minutes run
preference perceptron experiment took hours run svm experiment


fishivaswamy joachims

web dataset permutation dataset similary movie recommendation
task took around seconds run preference perceptron user took
around seconds run svm user preference
perceptron perform par better svms tasks fraction
computational cost
comparison dueling bandit
second baseline compare preference perceptron dueling
bandit yue joachims step dueling bandit
makes comparison vector w perturbed version w random
direction u w w u produced two weight vectors
assessed user techniques interleaving radlinski kurup joachims
providing preference w w preference feedback determines
update dueling bandits makes w w preferred retained
next round w preferred small step length taken direction
perturbation u


dueling bandit
pref perceptron













avg util regret

avg util regret













































dueling bandit
pref perceptron



















figure preference perceptron versus dueling bandit web search left plot
strict informative feedback right plot shows noisy feedback
first experiment web search step first obtained two ranked lists
w w features used obtain ranked lists identical
used preference perceptron two rankings interleaved interleaved
ranking presented user first experiment user provided strict informative feedback interleaved ranking second experiment user
provided noisy feedback depending feedback inferred two rankings preferred team game method proposed radlinski et al
w preferred tie step taken w preferred
step length taken direction u regret dueling bandit
measured considering utility interleaved ranking unlike preference
perceptron dueling bandit two parameters need


ficoactive learning

tuned considered values parameters x grid simply chose
best parameter values dueling bandits hindsight
experiment shown figure despite advantage setting
parameters best possible values seen dueling bandit performs
significantly worse compared preference perceptron orders magnitude
example performance dueling bandit around iterations matched
preference perceptron less iterations types feedback
surprising since dueling bandit basically relies random vectors
determine direction step needs taken coactive learning model
user feedback provides better random direction guide


dueling bandit
pref perceptron









avg util regret

avg util regret



































dueling bandit
pref perceptron















figure preference perceptron versus dueling bandit movie recommendation
left plot utility values whereas right plot shows
rounded values

similarly conducted comparison dueling bandit
movie recommendation dataset however unlike web search experiment dueling
bandit model somewhat unnatural dataset experimental setup since interleaving two rankings natural whereas interleaving two items therefore consider
different setup two movies obtained w w dueling bandit
user feedback merely indicate two movies higher
rating noisy case user feedback actual rating rounded rating noise free case user feedback utility values case
utility dueling bandit considered average utility two movies selected
comparison
performance dueling bandit experiment shown figure preference perceptron regret curves strict informative
feedback better noisy feedback shows reference
seen dueling bandit performs substantially worse compared
preference perceptron


fishivaswamy joachims

exponentiated versus additive updates
experiment compare exponentiated additive
preference perceptron exponentiated components
must non negative obtained non negative follows





min w

max w im



equation denotes ith component moreover modified
joint feature map exponentiated follows
e



x

x
im
x im



modifications non negative components moreover easy verify e x w x makes regret
exponentiated directly comparable regret additive
exponentiated fixed rate parameter inversely depends
time horizon large small situation consider update


wt
wti exp xt yt xt yt zt
since small approximate exponential term equation
first order approximation
exp xt yt xt yt xt yt xt yt
thus exponentiated updates resemble updates additive
normalization factor despite normalization factor empirically observed behavior two nearly identical though exact thus empirically
evaluated exponentiated variable rate parameter time
note empirical without formal theoretical guarantees variable
rate
experiment shown figure figure strict informative
feedback noisy feedback respectively seen exponentiated tends performs slightly better additive small number
iterations time horizon becomes large two seem comparable
performance cases
minimizing convex losses
section empirically evaluate convex preference perceptron
second order preference perceptron
put superscript e distinguish joint feature map w used experiments
exponentiated



ficoactive learning





exponentiated
pref perceptron



exponentiated
pref perceptron



avg util regret

avg util regret






















































figure exponentiated versus additive strict informative feedback websearch left movie recommendation right





exponentiated
pref perceptron



exponentiated
pref perceptron



avg util regret

avg util regret






















































figure exponentiated versus additive noisy feedback web search left
movie recommendation right

convex perceptron versus second order
regret bounds section one get lower regret strongly convex
functions second order first order convex perceptron applies
general convex functions section evaluate relative performance
first order second order empirically purpose considered
quadratic loss c largest utility value possible x
w convex ball radius kw k verified loss function strongly
convex b set datasets


fishivaswamy joachims

second order
convex perceptron






























second order
convex perceptron



util regret

quad regret




































figure cumulative regret convex perceptron second order convex perceptron web search





second order
convex perceptron





util regret

quad regret



second order
convex perceptron













































figure cumulative regret convex perceptron second order convex perceptron movie recommendation

first set experiments considered strict informative feedback ran
second order well convex preference perceptron
value second order perceptron simply set one recorded regt
cregt values methods note regt corresponds utility
regret defined
experiment shown figure figure demonstrate
qualitative difference two plot cumulative regret e
regt cregt figures cumulative regret second order
linear log scale shows convergence regret indeed


ficoactive learning

logarithmic compared much slower convergence convex preference perceptron
interestingly even cumulative regret raw utility values empirically shows
similar behavior purely empirical since theoretically log average
regret holds strongly convex losses linear loss strongly convex




weak
strong
weak
strong
weak
strong



util regret

quad regret





weak
strong
weak
strong
weak
strong




































































figure sensitivity second order preference perceptron parameter
value





weak
strong
weak
strong
weak
strong

quad regret










util regret





weak
strong
weak
strong
weak
strong






































































figure sensitivity second order preference perceptron parameter
value movie recommendation

previous experiment fixed value second order one
study sensitivity second order value parameter
figures regret values given number iterations swept
range values dotted lines performance convex preference
perceptron comparison case web search wide range parameter


fishivaswamy joachims

values performance good parameter takes extreme
value side performance deteriorates range suitable
values much broader web search dataset movie recommendation
interesting note performed empirically best among
values tried




second order
convex perceptron





second order
convex perceptron

util regret

quad regret























































figure strong convex versus weak convex noisy feedback web seach



second order
convex perceptron











util regret

quad regret





second order
convex perceptron














































figure strong convex versus weak convex noisy feedback movie recommendation

tested convex noisy feedback regret bounds contain slack terms right hand side thus user feedback informative
regret bounds second order first order
dominated slack variables empirical performance two noisy feedback shown figures case web search
second order first order nearly identi

ficoactive learning

cal however case movie recommendation still advantage
second order
summary second order performs substantially superior noise
circumstances presence noise feedback two
drastically different behaviors

conclusions
proposed coactive learning model online learning preferences
especially suited implicit user feedback unlike supervised learning approaches
coactive learning require optimal labels merely noisy feedback
improves prediction model cardinal utilities observed
sits experts bandits settings argue coactive learning
applicable wide range systems aim optimize observable
user actions
provide several provably optimize regret coactive learning
framework empirically validate effectiveness proposed framework
web search ranking movie recommendation datasets simulations noisy
noise free feedback recurring theme wide variety conventional
online learning converted coactive learning despite
differences learning model nature feedback notion regret
conjecture many online learning could similarly converted
practically useful coactive learning
coactive learning model proposed ability use weak
feedback observable user behavior offer wide range opportunities learning
approaches application ranging natural language processing information retrieval robotics several opportunities developing
coactive learning model example convex loss
minimization assume gradient convex losses bounded however
practical situations convex loss minimized known apriori
interesting direction study whether utilize
gradient loss perform better theoretically empirically another question
whether better exist special cases linear utility model lower
bound argument dimensionally joint feature maps grow
given horizon dimensionality joint feature map fixed
interesting question better regret proposed


acknowledgments
work funded part nsf awards iis iis iis
work done pannaga shivaswamy postdoctoral associate cornell
university thank peter frazier bobby kleinberg karthik raman tobias schnabel


fishivaswamy joachims

yisong yue helpful discussions thank anonymous reviewers thoughtful
comments earlier version

appendix proof theorem
proof look kl divergence w wt evolves
kl w wt kl w wt



n
x

n
x


wi log wt
wti

wi xt yt xt yt log zt



w xt yt xt yt log zt

p

second line pulled log zt sum since n
w consider


last term equation denoting xt yt xt yt brevity
definition

n
x
log zt log
wti exp
log


n
x

wti

















log wt
wt



second line used fact exp x x x x rate ensures
last line used fact log x x combing
get
w wt

kl w wt kl w wt



adding inequalities get



x
x
kl w wt kl w wt x
w wt xt yt xt yt











kl w w



rearranging inequality substituting value
get



x
x



u xt yt u xt yt
wt xt yt xt yt log n






log n




ficoactive learning

used fact kl w w log n since w initialized uniformly moreover holders inequality obtained
wt xt yt kwt k k xt yt k
inequality along informative feedback gives claimed

appendix b proof theorem
proof first divide set time steps two different sets nature
feedback
u xt yt u xt yt
j u xt yt u xt yt
brevity denote xt xt b b rest proof start
considering following term single time step
ct u xt yt u xt yt ct


wt yt yt

ct u xt yt u xt yt ct





wt yt yt
w yt yt

ct
ct







w wt yt yt w yt yt


ct









wt yt yt w yt yt g


wt yt yt g
j
w

inequalities second line follows fact ct
non increasing third line follows informative feedback eqn fourth
line follows since function ct convex obtain first term next inequality
case since c g wt yt yt choice yt
second terms case obtained fact c w yt yt
upper bounded g step clipped version slack variables
needed proof finally w yt yt positive negative depending
feedback leads two different cases depending whether j
since context xt clear suppress notation brevity
convex function f f f x x f f denotes sub derivative f



fishivaswamy joachims

summing inequalities get

x

ct w yt yt








gx


g


wt yt yt



x


x

ct



x

g






ti

wt w yt yt



gx
w yt yt


g



x




gx
w yt yt




tj

p
obtained last line simply adding subtracting g tj w yt yt
right side previous inequality point mostly follow proof
techniques online convex optimization zinkevich
bound first term right hand side purpose consider
following
kwt w k kwt yt yt w k
kwt w k k yt yt k wt w yt yt



rearranging terms equation get

kwt w k



kwt w k


wt w yt yt



kwt w k k yt yt k



kwt w k r


last line used fact kwt w k kwt w k since wt
projection wt convex set b contains vector w
bound first term following telescoping argument


x





kwt w k
kwt w k r






x
x




kw w k

kwt w k r








x





b

b r







b r

obtained second line simply rearranging terms expression

line used boundedness property set b well

pton third
fact
final line follows cancelling terms fact



ficoactive learning

consider third term right hand side

w yt yt


w yt yt




first inequality follows informative feedback whereas second inequal

ity follows fact w yt yp
definition yt finally bound

g
follows trivial fact ti
obtain bound expected regret consider convex loss step conditioned user behavior far


wt yt yt
et ct






et w yt yt
wt yt yt
ct
et ct







w yt yt
wt yt yt
et ct
et ct







get wt yt yt w yt yt


get wt yt yt
tj
ct w yt yt



second line follows definition expected informative feedback
third line follows jensens inequality obtain last line following argument
similar proof theorem bound follows expected version


appendix c proof theorem
proof first divide time steps two different sets nature feedback

u xt yt u xt yt
j u xt yt u xt yt


fishivaswamy joachims

start considering single time step
ct w yt yt ct


wt yt yt


ct w yt yt ct





w yt yt
wt yt yt
ct
ct










w yt yt
w wt yt yt
w wt yt yt

ct

























wt yt yt
yt

ti
w
w wt yt yt
g



















w
w
w











j
g


w

inequalities second line follows fact ct
non increasing third line follows fact function ct non increasing
following inequality follows definition
u xt yt u xt yt u xt yt u xt yt
fourth line follows strong convexity last line follows line
reasoning proof theorem
consider last term cases
























w wt yt yt







w wt yt yt

w wt yt yt









w wt yt yt
w yt yt wt yt yt












w wt yt yt






w yt yt









w wt yt yt







equations second third lines follow simple algebraic expansion
expression first line fourth line follows definition informative
feedback fact wt yt yt last line follows fact
w yt yt definition yt


ficoactive learning

summing terms substituting bound get


x

ct w yt yt




x

ct




x w yt yt pt
w wt yt yt


g

g




ti







x wt w yt yt w wt yt yt
g





p
p

tt
g tt
gx

w yt yt




tj

pt g pt
gx






wt w yt yt
wt w yt yt








x

wt yt yt











p
yt
obtained third inequality adding subtracting g tj w



obtain last line used fact since

p finally

used argument similar proof theorem bound g
w



tj

obtained factor two sum slacks term point use arguments
similar online convex optimization strongly convex losses hazan et al


next consider wt w wt w express interms wt

wt w wt w


wt
yt yt w wt yt yt w

wt w wt w yt yt
yt yt wt w yt yt

wt w yt yt yt yt wt w wt w wt w

yt yt
yt yt w wt yt yt

rearranging terms equation get


wt w yt yt yt yt wt w

wt w wt w wt w wt w yt yt
yt yt
wt w yt yt



fishivaswamy joachims

identify term left hand side inequality occurs
expression would bound therefore





x



x

wt w yt yt wt w yt yt




wt w wt w wt w wt w yt yt
byt yt



w w w w


x

yt yt
yt yt



b

n
log




r





p

used fact tt yt yt
yt yt n log r
n dimens ionality x r upper bound norm joint
feature maps e k x k r proof fact found hazan et al

references
auer p cesa bianchi n fischer p finite time analysis multiarmed
bandit machine learning
auer p cesa bianchi n freund schapire r b non stochastic multiarmed bandit siam journal computing
bakir g h hofmann scholkopf b smola taskar b vishwanathan eds
predicting structured data mit press
bell r koren scalable collaborative filtering jointly derived neighborhood interpolation weights icdm
boley mampaey kang b tokmakov p wrobel one click mining
interactive local pattern discovery implicit preference performance learning proceedings acm sigkdd workshop interactive data exploration
analytics pp
cesa bianchi n lugosi g prediction learning games cambridge university press
cesa bianchi n lugosi g b prediction learning games cambridge
university press cambridge uk
chapelle chang yahoo learning rank challenge overview jmlr proceedings track
chu w ghahramani z preference learning gaussian processes icml
crammer k singer pranking ranking nips


ficoactive learning

crammer k gentile c multiclass classification bandit feedback
adaptive regularization proceedings th international conference machine learning icml
dekel gilad bachrach r shamir xiao l optimal distributed online
prediction mini batches jmlr
flaxman kalai mcmahan h b online convex optimization
bandit setting gradient descent without gradient soda
freund iyer r schapire r e singer efficient boosting
combining preferences journal machine learning
goetschalckx r fern tadepalli p coactive learning locally optimal
solving conference american association artificial intelligence
aaai pp
haddow b arun koehn p samplerank training phrase machine
translation proceedings sixth workshop statistical machine translation
pp edinburgh scotland association computational linguistics
hazan e agarwal kale logarithmic regret online convex
optimization machine learning
herbrich r graepel obermayer k large margin rank boundaries
ordinal regression advances large margin classifiers mit press
jain wojcik b joachims saxena learning trajectory preferences
manipulators via iterative improvement neural information processing systems
nips pp
joachims optimizing search engines clickthrough data acm sigkdd
conference knowledge discovery data mining kdd pp
joachims granka l pan b hembrooke h radlinski f gay g evaluating accuracy implicit feedback clicks query reformulations web
search acm transactions information systems tois
jones r klinkner k beyond session timeout automatic hierarchical
segmentation search topics query logs cikm
kakade shalev shwartz tewari efficient bandit
online multiclass prediction proceedings th international conference
machine learning icml
kivinen j warmuth exponentiated gradient versus gradient gradient descent linear predictors journal information computation
langford j zhang epoch greedy multi armed bandits
side information nips
liu learning rank information retrieval foundations trends
information retrieval
manning c raghavan p schutze h introduction information retrieval
cambridge university press


fishivaswamy joachims

novikoff convergence proofs perceptrons proceedings symposium
mathematical theory automata vol xii pp
polyak b tsypkin pseudogradient adaptation training
automatic remote control
radlinski f kurup joachims clickthrough data reflect retrieval quality conference information knowledge management cikm
raman k joachims learning socially optimal information systems
egoistic users european conference machine learning ecml pp
raman k joachims shivaswamy p schnabel stable coactive learning
via perturbation international conference machine learning icml pp

raman k shivaswamy p joachims online learning diversify
implicit feedback kdd
shivaswamy p joachims online structured prediction via coactive learning
icml
somers hollinger g coactive learning human expert robotic
monitoring rss workshop robotic monitoring
weston j bengio usunier n wsabie scaling large vocabulary
image annotation proceedings international joint conference artificial
intelligence ijcai
yue broder j kleinberg r joachims k armed dueling bandits
colt
yue joachims interactively optimizing information retrieval systems
dueling bandits icml
zhang lei barzilay r jaakkola globerson steps excellence simple inference refined scoring dependency trees proceedings
nd annual meeting association computational linguistics
long papers pp baltimore maryland association computational
linguistics
zinkevich online convex programming generalized infinitesimal gradient
ascent icml




