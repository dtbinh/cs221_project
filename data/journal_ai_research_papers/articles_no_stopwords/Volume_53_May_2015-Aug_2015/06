Journal Artificial Intelligence Research 53 (2015) 699-720

Submitted 03/15; published 08/15

Tree-Width Computational Complexity
MAP Approximations Bayesian Networks
Johan Kwisthout

j.kwisthout@donders.ru.nl

Radboud University Nijmegen
Donders Institute Brain, Cognition Behaviour
Montessorilaan 3, 6525 HR Nijmegen, Netherlands

Abstract
problem finding probable explanation designated set variables given partial evidence (the MAP problem) notoriously intractable problem
Bayesian networks, compute exactly approximate. known,
theoretical considerations practical experience, low tree-width typically
essential prerequisite efficient exact computations Bayesian networks.
paper investigate whether holds approximating MAP. define four
notions approximating MAP (by value, structure, rank, expectation) argue
intractable general. prove efficient value-approximations,
structure-approximations, rank-approximations MAP instances high tree-width
violate Exponential Time Hypothesis. contrast, show MAP sometimes efficiently expectation-approximated, even instances high tree-width,
probable explanation high probability. introduce complexity class
FERT, analogous class FPT, capture notion fixed-parameter expectationapproximability. suggest road-map future research yields fixed-parameter
tractable results expectation-approximate MAP, even graphs high tree-width.

1. Introduction
One important computational problems Bayesian networks MAP problem, i.e., problem finding joint value assignment designated set variables
(the MAP variables) maximum posterior probability, given partial observation
remaining variables. MAP problem notably intractable; NPPP -hard,
strictly harder (given usual assumptions computational complexity theory)
PP-hard inference problem (Park & Darwiche, 2004). sense, seen combining
optimization problem inference problem, potentially contribute
problems complexity (Park & Darwiche, 2004, p. 113). Even variables
network binary network (very restricted) polytree topology, MAP
remains NP-hard (De Campos, 2011). optimization inference
part problem computed tractably (for example, tree-width
network small, cardinality variables low, probable joint value
assignment high probability) MAP computed tractably (Kwisthout, 2011).
known that, arbitrary probability distributions assumption
Exponential Time Hypothesis, low tree-width moralized graph Bayesian network
necessary condition Inference problem Bayesian networks tractable

2015 AI Access Foundation. rights reserved.

fiKwisthout

(Kwisthout, Bodlaender, & van der Gaag, 2010); result easily extended MAP,
show Section 4.
MAP intractable approximate (Abdelbar & Hedetniemi, 1998; Kwisthout,
2011, 2013; Park & Darwiche, 2004). obviously case particular instance
MAP problem approximated efficiently computed exactly
efficiently, yet unclear whether approximate MAP computations rendered
tractable different conditions exact MAP computations. Crucial
question mean statement algorithm approximates MAP problem.
Typically, computer science, approximation algorithms guarantee output
algorithm value within bound value optimal solution.
example, canonical approximation algorithm Vertex Cover problem selects
edge random, puts endpoints vertex cover, removes nodes
instance. algorithm guaranteed get solution twice
number nodes vertex cover optimal vertex set. However, typical Bayesian
approximation algorithms guarantee; contrast, may converge
optimal value given enough time (such Metropolis-Hastings algorithm), may
find optimal solution high probability success (such repeated local search
strategies).
paper assess four different notions approximation relevant MAP
problem; particular value-approximation, structure-approximation, rank-approximation,
expectation-approximation MAP. introducing notation providing
preliminaries (Section 2), show approximations intractable
assumption P 6= NP, respectively NP 6 BPP (Section 3). Building result
Kwisthout et al. (2010) show Section 4 bounded tree-width indeed
necessary condition efficient value-approximation, structure-approximation, rankapproximation MAP. Section 5 argue need case expectationapproximation. introduce parameterized complexity classes FERT (Fixed Error
Randomized Tractable) FPERT (Fixed Parameter Error Randomized Tractable)
natural extensions class FPT. introduce MAP variant additional
constraints show Constrained-MAP intractable (PP-hard) general;
however, Constrained-MAP FERT parameterized probability
probable explanation, even tree-width high. conclude paper Section
6.

2. Preliminaries
section, introduce notational conventions provide preliminaries
Bayesian networks, graph theory, complexity theory; particular definitions
MAP problem, tree-width, parameterized complexity theory, Exponential Time
Hypothesis. thorough discussion concepts, reader referred
textbooks Darwiche (2009), Arora Barak (2009), Downey
Fellows (1999).
700

fiTree-Width MAP Approximations

2.1 Bayesian Networks
Bayesian network B = (GB , Pr) graphical structure succinctly represents joint
probability distribution set stochastic variables. B includes directed acyclic graph
GB = (V, A), V models (in one-to-one mapping) stochastic variables
models conditional (in)dependences them, set parameter probabilities
Pr form conditional probability tables (CPTs), capturing strengths
relationships
Q variables. network models joint probability distribution
Pr(V) = ni=1 Pr(Vi | (Vi )) variables; here, (Vi ) denotes parents Vi GB .
notational convention use upper case letters denote individual nodes
network, upper case bold letters denote sets nodes, lower case letters denote value
assignments nodes, lower case bold letters denote joint value assignments sets
nodes. use node variable interchangeably.
One key computational problems Bayesian networks problem find
probable explanation set observations, i.e., joint value assignment designated set variables (the explanation set) maximum posterior probability given
observed variables (the joint value assignment evidence set) network.
network bi-partitioned explanation variables evidence variables problem
known Probable Explanation (MPE). general problem,
network includes variables neither observed explained (referred
intermediate variables) known (Partial Marginal) MAP. problem
typically defined formally follows:
MAP
Instance: Bayesian network B = (GB , Pr), V partitioned set
evidence nodes E joint value assignment e, set intermediate nodes I,
explanation set H.
Output: joint value assignment h H joint value assignments h0
H, Pr(h | e) Pr(h0 | e).
remainder, use following definitions. arbitrary MAP instance
{B, H, E, I, e}, let cansol B refer set candidate solutions {B, H, E, I, e},
optsol B cansol B denoting optimal solution (or, case draw, one optimal
solutions) MAP instance. cansol B ordered according probability
candidate solutions (breaking ties candidate solutions probability
arbitrarily), optsol 1...m
refers set first elements cansol B , viz.
B
probable solutions MAP instance. particular notion approximation,
refer (unspecified) approximate solution approxsol B cansol B .
2.2 Tree-Width
important structural property Bayesian network B tree-width,
defined minimum width tree-decomposition (or equivalently, minimal size
largest clique triangulation) moralization GM
B network. Treewidth plays important role complexity analysis Bayesian networks, many
otherwise intractable computational problems rendered tractable, provided
tree-width network small. moralization (or moralized graph) GM
B
701

fiKwisthout

undirected graph obtained GB adding arcs connect pairs
parents variable, dropping directions. triangulation GM
B

chordal graph GT embeds GB subgraph. chordal graph graph
include loops three variables without pair adjacent.
tree-decomposition (Robertson & Seymour, 1986) triangulation GT tree
TG node Xi TG bag nodes constitute clique GT ;
every i, j, k, Xj lies path Xi Xk TG , Xi Xk Xj .
context Bayesian networks, tree-decomposition often referred junction
tree clique tree B. width tree-decomposition TG graph GT defined
size largest bag TG minus 1, i.e., maxi (|Xi | 1). tree-width tw
Bayesian network B minimum width possible tree-decompositions
triangulations GM
B .
2.3 Complexity Theory
assume reader familiar basic notions complexity theory,
intractability proofs, computational complexity classes P NP, polynomialtime reductions. section shortly review additional concepts use
throughout paper, namely complexity classes PP BPP, Exponential Time
Hypothesis basic principles parameterized complexity theory.
complexity classes PP BPP defined classes decision problems
decidable probabilistic Turing machine (i.e., Turing machine makes stochastic
state transitions) polynomial time particular (two-sided) probability error.
difference two classes bound error probability. Yes-instances
problems PP accepted probability 1/2 + , may depend exponentially
input size (i.e., = 1/cn constant c > 1). Yes-instances problems BPP
accepted probability polynomially bounded away 1/2 (i.e., = 1/nc ).
PP-complete problems, problem determining whether majority truth
assignments Boolean formula satisfies , considered intractable; indeed,
shown NP PP. contrast, problems BPP considered tractable.
Informally, decision problem BPP exists efficient randomized (Monte
Carlo) algorithm decides high probability correctness. Given error
polynomially bounded away 1/2, probability answering correctly boosted
arbitrarily close 1 still requiring polynomial time. obviously
BPP PP, reverse unlikely; particular, conjectured BPP = P (Clementi,
Rolim, & Trevisan, 1998).
Exponential Time Hypothesis (ETH), introduced Impagliazzo Paturi (2001),
states exists constant c > 1 deciding 3Sat instance n
variables takes least (cn ) time. Note ETH stronger assumption
assumption P 6= NP. sub-exponential
polynomial-time algorithm

3
3Sat, algorithm running O(2 n ), would contradict ETH would
imply P = NP. assume ETH proofs show necessity low
tree-width efficient approximation MAP.
Sometimes problems intractable (i.e., NP-hard) general, become tractable
parameters problem assumed small. problem called
702

fiTree-Width MAP Approximations

fixed-parameter tractable parameter (or set {1 , . . . , } parameters)
solved time, exponential (or even worse) polynomial input size
|x|, i.e., time O(f () |x|c ) constant c > 1 arbitrary computable function
f . practice, means problem instances solved efficiently, even
problem NP-hard general, known small. contrast, problem
NP-hard even small, problem denoted para-NP-hard .
parameterized complexity class FPT consists fixed parameter tractable problems .
traditionally defined mapping problem instances natural numbers
(e.g., Flum & Grohe, 2006, p. 4), one easily enhance theory rational parameters
(Kwisthout, 2011). context paper, particular consider rational
parameters range [0, 1], liberally mix integer rational parameters.

3. Approximating MAP
widely known, practical experiences theoretical results, small
tree-width often necessary constraint render exact Bayesian inferences tractable.
However, often assumed intractable computations efficiently approximated using inexact algorithms; assumption appears warranted observation
many cases approximation algorithms seem reasonable job in, e.g., estimating posterior distributions, even networks high tree-width exact computations
infeasible (Cheng & Druzdzel, 2000; Sontag, Meltzer, Globerson, Weiss, & Jaakkola,
2008). Whether observation firm theoretical basis, i.e., whether approximation
algorithms cannot principle perform well even situations tree-width
grow large, date known.
Crucial answering question make precise efficiently approximated actually pertains to. on-line Merriam-Webster dictionary lists one entries
approximate similar exactly (something). computer science,
similarity typically defined terms value: approximate solution value
close value optimal solution. However, notions approximation
relevant. One think approximating value optimal solution,
appearance: approximate solution A0 closely resembles optimal solution. Also, one
define approximate solution one ranks close optimal solution: approximate solution A00 ranks within top-m solutions. Note notions refer
completely different solutions. One situations second-best solution
resemble optimal solution all, whereas solutions look almost
low value compared optimal solution (Van Rooij & Wareham, 2012;
Kwisthout, 2013). Similarly, second-best solution may either value almost
good optimal solution, much worse.
many practical applications, particular Bayesian inferences, definitions
approximation (fully) capture actual notion interested in. example, trying approximate MAP explanation using sort randomized
computation, guarantee quality solution found, however, may
bound likeliness good solution. current state-of-the-art approximate
algorithms MAP (AnnealedMAP, Yuan, Lu, & Druzdzel, 2004; P-Loc, Park & Darwiche, 2001; BP-LS, Park & Darwiche, 2004) employ strategy. added notion
703

fiKwisthout

approximation here, induced use randomized computations, allowance
bounded amount error.1
remainder section elaborate notions approximation
applied MAP problem. give formal definitions approximate
problems show intractable general. MAP-approximation
value structure interpret known results literature. MAPapproximation rank give formal proof intractability; MAP-approximation
using randomized algorithms give argument complexity theory.
3.1 Value-Approximation
Value-approximating MAP problem finding explanation approxsol B cansol B
value, close value optimal solution. closeness defined
additive relative manner; additive meaning absolute difference
probability optimal approximate solution smaller value ;
relative ratio probability optimal approximate solution
smaller value . problems intractable general. Abdelbar
Hedetniemi (1998) proved NP-hardness relative value-approximation constant
1. result holds networks binary variables, three incoming
arcs per variable, evidence. addition, Kwisthout (2011) showed NP-hard
general find explanation approxsol B Pr(approxsol B , e) > constant
> 0, thus Pr(optsol B , e) Pr(approxsol B , e) > Pr(optsol B , e) .
latter result holds even networks binary variables, two incoming
arcs per variable, single evidence variable, intermediate variables (i.e.,
approximate MPE problem).
Definition 3.1 (additive value-approximation MAP) Let optsol B optimal
solution MAP problem. explanation approxsol B cansol B defined -additive
value-approximate optsol B Pr(optsol B , e) Pr(approxsol B , e) .
Result 3.2 (Kwisthout, 2011) NP-hard -additive value-approximate MAP
> Pr(optsol B , e) constant > 0.
Definition 3.3 (relative value-approximation MAP) Let optsol B optimal
solution MAP problem. explanation approxsol B cansol B defined -relative
Pr(optsol B | e)
value-approximate optsol B Pr(approxsol
| e) .
B

Result 3.4 (Abdelbar & Hedetniemi, 1998) NP-hard -relative value-approximate
Pr(optsol B | e)
MAP Pr(approxsol
| e) > 1.
B

1. Observe algorithms always converge optimal solution, may take exponential time
(e.g., MCMC-type approaches). However, turn algorithm expectationapproximation algorithm adding clock halts computations time, polynomially input
size, returning current best solution may may optimal (Gill, 1977).

704

fiTree-Width MAP Approximations

3.2 Structure-Approximation
Structure-approximating MAP problem finding explanation approxsol B
cansol B structurally resembles optimal solution. captured using solution
distance function, metric associated optimization problem relating candidate
solutions optimal solution (Hamilton, Muller, van Rooij, & Wareham, 2007).
MAP, typical structure distance function dH (approxsol B , optsol B ) Hamming distance explanation approxsol B probable explanation optsol B .
shown Kwisthout (2013) algorithm calculate value even single
variable probable explanation polynomial time, unless P = NP; is,
NP-hard find explanation dH (approxsol B , optsol B ) |optsol B | 1, even
variables network bi-partitioned explanation evidence variables,
variable three possible values.
Definition 3.5 (structure-approximation MAP) Let optsol B optimal solution MAP problem let dH Hamming distance. explanation approxsol B
cansol B defined d-structure-approximate optsol B dH (approxsol B , optsol B ) d.
Result 3.6 (Kwisthout, 2013) NP-hard d-structure-approximate MAP
|optsol B | 1.
3.3 Rank-Approximation
Apart allowing explanation resembles, probability close to,
probable explanation, define approximate solution approxsol B explanation one best explanations, constant m, is, approxsol B
optsol 1...m
m. Note explanation may resemble probable
B
explanation needs relatively high probability, ranked within
probable explanations. denote approximation rank-approximation.
Definition 3.7 (rank-approximation MAP) Let optsol 1...m
cansol B set
B
probable solutions MAP problem let optsol B optimal solution.
explanation approxsol B cansol B defined m-rank-approximate optsol B approxsol B
optsol 1...m
.
B
prove NP-hard m-rank-approximate MAP constant m.
reduction variant LexSat, based reduction Kwisthout, Bodlaender,
van der Gaag (2011). LexSat defined follows:
LexSAT
Instance: Boolean formula n variables X1 , . . . , Xn .
Output: lexicographically largest truth assignment x X = {X1 , . . . , Xn }
satisfies ; output satisfiable.
Here, lexicographical order truth assignments maps truth assignment x = x1 , . . . , xn
string {0, 1}n , {0}n (all variables set false) lexicographically smallest,
{1}n (all variables set true) lexicographically largest truth assignment. LexSat
NP-hard; particular, LexSat proven complete class FPNP (Krentel,
705

fiKwisthout

V









X0

X1





X2

X3

X4

X
Figure 1: Example construction Bex LexSat0 instance ex
1988). proofs use following variant always returns truth assignment
(rather , case unsatisfiable):
LexSAT0
Instance: Boolean formula n variables X1 , . . . , Xn .
Output: lexicographically largest satisfying truth assignment x = (X0 )
satisfies .
Note satisfiable, X0 never set false lexicographically largest
satisfying truth assignment , yet X0 necessarily set false satisfiable;
hence, unsatisfying truth assignments always ordered satisfying truth assignments lexicographical ordering. Note LexSat trivially reduces LexSat0 using
simple transformation. claim following.
Theorem 3.8 algorithm find approximation approxsol B optsol 1...m
,
B
constant m, polynomial time, unless P = NP.
proof describe polynomial-time one-Turing reduction2 LexSat0 mrank-approximated-MAP arbitrary constant m. reduction largely follows
reduction presented Kwisthout et al. (2011) additions. take
following LexSat0 -instance running example proof: ex = X1 (X2 X3 );
correspondingly, ex = (X0 ) (X1 (X2 X3 )) example. set = 3
example construct. construct Bayesian network B follows (Figure 1).
variable Xi , introduce binary root variable Xi B possible
values true false. set prior probability distribution variables
i+1 1
Pr(Xi = true) = 1/2 22n+2
. addition, include uniformly distributed variable
Xn+1 B values x1n+1 , . . . , xm
n+1 . variables X0 , . . . , Xn together form set
X. Note prior probability joint value assignment x X higher prior
probability different joint value assignment x0 X, corresponding
2. function problem f polynomial-time one-Turing reducible function problem g exist
polynomial-time computable functions T1 T2 every x,f (x) = Tl (x, g(T2 (x))) (Toda,
1994). One-Turing reductions seen equivalent many-one reductions, applied
function problems.

706

fiTree-Width MAP Approximations

truth assignment x LexSat0 instance lexicographically larger truth assignment
x0 . running example, Pr(X0 = true) = 15/32, Pr(X1 = true) =
13/32, Pr(X2 = true) = 9/32, Pr(X3 = true) = 1/32, Pr(X4 = x1 ) = Pr(X4 =
4
x24 ) = Pr(X4 = x34 ) = 1/3. Observe Pr(X0 ) . . . Pr(Xi1 ) Pr(Xi ) >
Pr(X0 ) Pr(Xi1 ) Pr(Xi ) every i, i.e., ordering property stated
attained.
logical operator , introduce additional binary variable B
possible values true false, parents sub-formulas (or single subformula, case negation operator) bound operator. conditional
probability distribution variable matches truth table operator, i.e., Pr(T =
true | (T )) = 1 operator evaluates true particular truth
value sub-formulas bound . top-level operator denoted V . readily
seen Pr(V = true | x) = 1 truth assignment variables
matches x satisfies , Pr(V = true | x) = 0 otherwise. Observe
m-valued variable Xn+1 independent every variable B . note
network, including prior conditional probabilities, described using number
bits polynomial size . MAP instance constructed , set
V evidence set V = true observation set X {Xn+1 } explanation
set.
Proof. Let instance LexSat0 , let B network constructed
described above. joint value assignment x X Pr(X = x | V =
true) = Pr(X = x) normalization constant > 0 x corresponds satisfying
truth assignment , Pr(X = x | V = true) = 0 x corresponds non-satisfying
truth assignment . Given prior probability distribution variables X,
satisfying joint assignments x X ordered posterior probability
Pr(x | V = true) > 0, non-satisfying joint value assignments probability
Pr(x | V = true) = 0 thus ordered satisfying assignments. joint value
assignment highest posterior probability thus lexicographically largest
satisfying truth assignment .
take m-th valued variable Xn+1 account, every x,
joint value assignments X {Xn+1 } probability since Pr(x, Xn+1 | V =
true) = Pr(x | V = true) Pr(Xn+1 ). then, joint value assignments xm
X {Xn+1 } correspond lexicographically largest satisfying truth assignment x
posterior probability Pr(xm | V = true). Thus, algorithm
returns one m-th ranked joint value assignments explanation set X {Xn+1 }
evidence V = true transformed polynomial time algorithm
solves LexSat0 . conclude algorithm m-rank-approximate MAP,
constant m, polynomial time, unless P = NP.

Note that, technically speaking, result even stronger: LexSat0 FPNP complete reduction described actually one-Turing reduction LexSat0
m-rank-approximation-MAP, latter problem FPNP -hard. strengthen
result observing variables (minus V ) mimic operators deterministically depend parents thus added explanation set without
substantially changing proof above. implies m-rank-approximation-MPE
FPNP -hard. Lastly, strengthen result replacing m-th valued variable
707

fiKwisthout

Xn+1 dlog2 unconnected binary variables Xn+1 Xn+dlog2 uniform probability. Still, algorithm returning one m-th ranked joint value assignments
X{Xn+1 , . . . , Xn+dlog2 } polynomial time effectively solve LexSat0 polynomial
time.
Result 3.9 NP-hard m-rank-approximate MAP constant m.
3.4 Expectation-Approximation
last notion MAP approximation discuss returns polynomial time
explanation approxsol B cansol B likely probable explanation,
allows small margin error; i.e., small probability answer
optimal solution, guarantees given quality solution.
approximations closely related randomized algorithms run polynomial
time whose output small probability error, viz., Monte Carlo algorithms.
notion approximationwhich refer expectation-approximation (Kwisthout &
van Rooij, 2013)is particularly relevant typical Bayesian approximation methods,
Monte Carlo sampling repeated local search algorithms.
Definition 3.10 (expectation-approximation MAP) Let optsol B optimal solution MAP problem let E expectation function (Papoulis, 1984).
explanation approxsol B cansol B defined -expectation-approximate optsol B
E(Pr(optsol B ) 6= Pr(approxsol B )) < .
order practical relevance, want error small, i.e., casted
decision problem, want probability answering correctly bounded away
1/2. case, amplify probability answering correctly arbitrarily
close 1 polynomial time, repeated evocation algorithm. Otherwise, e.g.,
error depends exponentially size input, need exponential number
repetitions achieve result. Problems enjoy polynomial-time Monte Carlo
algorithms complexity class BPP; problems may need exponential time
reduce probability error arbitrarily close 0 complexity class PP.
MAP NP-hard, efficient randomized algorithm solving MAP polynomial time
bounded probability error, would imply NP BPP. considered
highly unlikely, almost every problem enjoys efficient randomized algorithm
proven P, i.e., decidable deterministic polynomial time.3 various
grounds believed BPP = P (Clementi et al., 1998), thus efficient randomized algorithm MAP would (under assumption) establish P = NP. Therefore,
algorithm expectation-approximate MAP polynomial time bounded margin
error unless NP BPP. result holds MPE, already NP-hard,
even binary variables in-degree 2 (Kwisthout, 2011).4
3. dramatic example problem PRIMES: given natural number, decide whether
prime. efficient randomized algorithms PRIMES around quite time
(establishing PRIMES BPP), fairly recently proven PRIMES P (Agrawal,
Kayal, & Saxena, 2004).
4. fact, holds value-approximation, structure-approximation, rank-approximation MAP
well, three problems NP-hard (see Abdelbar & Hedetniemi, 1998, p. 35).

708

fiTree-Width MAP Approximations

Result 3.11 cannot exist randomized algorithm -expectation-approximates
MAP polynomial time < 1/2 1/nc constant c unless NP BPP.
3.5 Discussion
previous subsections showed approximation notions established
fact intractable, various assumptions. results hold MAP general,
many cases strengthened hold MPE (i.e., network two-partitioned
evidence explanation variables); either case, cardinality c in-degree
nodes (and consequently, size CPTs) bounded. results hold
empty (or singleton) evidence sets. results summarized Table 1.
Approximation
value, additive
value, ratio
structure
rank
expectation

constraints
c = 2, = 2,
|E| = 1, =
c = 2, = 3,
E=
c = 3, = 3,
I=
c = 2, = 2,
|E| = 1, =
c = 2, = 2,
|E| = 1, =

assumption
P 6= NP

reference
(Kwisthout, 2011, p. 1462)

P 6= NP

(Abdelbar & Hedetniemi, 1998, p. 24)

P 6= NP

(Kwisthout, 2013, p. 345)

P 6= NP

Section 3.3

NP 6 BPP

Section 3.4

Table 1: Summary intractability results MAP approximations

4. Necessity Low Tree-Width Efficient Approximation MAP
previous section shown four notions approximating MAP,
efficient general approximation algorithm constructed unless either P = NP NP
BPP. However, MAP fixed-parameter tractable number problem parameters;
example, {tw, c, q}MAP FPT parameters tree-width (tw), cardinality
variables (c = maxi |(Vi V)|), probability probable solution (q =
Pr(optsol B , e)). Surely, compute {1 , . . . , }MAP exactly FPT time,
approximate {1 , . . . , }MAP FPT time. question remains, however, whether
approximate MAP fixed-parameter tractable different set parameters
exact MAP.
Tree-width shown necessary parameter efficient exact computation Inference problem (and, trivial adjustment illustrated Section 4.3,
MAP), assumption ETH holds (Kwisthout et al., 2010).
section, show low tree-width necessary parameter efficient
approximate computation value-approximations, structure-approximations, rankapproximations. argue (in Section 5) necessary parameter
efficient expectation-approximation. next sub-section review so-called treewidth-preserving reductions (tw-reductions), special kind polynomial many-one reductions preserve tree-width instances (Kwisthout et al., 2010). Sub-section
709

fiKwisthout

4.2 sketch notion used tw-reduce Constraint Satisfaction
Inference. Together known result Constraint Satisfaction instances
high tree-width cannot sub-exponential algorithms, unless ETH fails (Marx,
2007), established Kwisthout et al. cannot (general-purpose) algorithm decides Inference instances high tree-width sub-exponential time,
unless ETH fails. Here, Inference problem problem deciding whether
Bayesian network B designated sets H E rational number q, case
Pr(H = h | E = e) > q. precisely, following theorem proved:
Theorem 4.1 exists computable function f Inference decided
algorithm running time
f (GM
B )

o(

kBk

tw(GM
B ) )
log tw(GM )
B

arbitrary Inference instances (B, H, h, E, e, q) moralized graph GM
B treeM
width tw(GB ), ETH fails.
reader referred Kwisthout et al. (2010) full proof.5 remainder
section, show proof augmented establish similar results
MAP, value-approximate MAP, structure-approximate MAP, rank-approximate MAP
(Sub-sections 4.3 4.4).
4.1 Tree-Width-Preserving Reductions
Tree-width-preserving reductions defined Kwisthout et al. (2010) means reduce
Constraint Satisfaction Inference ensuring tree-width preserved
instances reduction, modulo linear factor.
Definition 4.2 (Kwisthout et al., 2010) Let B computational problems
tree-width defined instances B. say polynomialtime tree-width-preserving reducible, tw-reducible, B exists polynomial-time
computable function g linear function l x g(x) B
tw(g(x)) = l(tw(x)). pair (g, l) called tw-reduction.
use notion show Constraint Satisfaction tw-reduces MAP,
value-approximate MAP, structure-approximate MAP, rank-approximate MAP.
4.2 Proof Sketch
tw-reduction (binary) Constraint Satisfaction Inference, presented
Kwisthout et al. (2010), constructs Bayesian network BI instance = (V, D, C)
Constraint Satisfaction, V denotes set variables I, denotes set
values variables, C denotes set binary constraints defined V V.
5. results Kwisthout et al. (2010) rule existence special-case algorithms,
assume (and utilize) particular property instance, particular orientation arcs
particular planarity properties graph structure, failing assumption violated.
results current paper, built result, inherit constraint.

710

fiTree-Width MAP Approximations

R1

R4
X1

X2

X4
X3
R2

R3

Figure 2: Example construction BI example CSP instance
constructed network BI includes uniformly distributed variables Xi , corresponding
variables V, binary variables Rj , corresponding constraints C.
parents variables Rj variables Xi bound constraints;
conditional probability distributions match imposed constraints variables
(i.e., Pr(Rj = true | x ((Rj ))) = 1 joint value assignment x
variables bound Rj matches constraints imposed Rj . Figure 2,
taken Kwisthout et al., shows result construction far example
Constraint Satisfaction instance four variables X1 X4 , C contains four
constraints bind respectively (X1 , X2 ), (X1 , X4 ), (X2 , X3 ), (X3 , X4 ).
tree-width thus obtained network equals max(2, tw(GI )), GI
primal graph I; note tree-width BI increases tree-width GI
1. order enforce constraints simultaneously enforced, constraint nodes
Rj need connected extra nodes mimicking operators. crucial aspect
tw-reduction topography connection nodes Rj : care must taken
blow tree-width arbitrarily connecting nodes, e.g., log-deep binary
tree. original proof uses minimal tree-decomposition moralization BI
describes procedure select nodes need connected tree-width
resulting graph tree-width GI plus 3. conditional probability
distribution nodes Ak defined follows.
V

1 x = V (Ak ) (V = true)
Pr(Ak = true | x) =
0 otherwise
node Ak without parents, Pr(Ak = true) = 1. graph results
applying procedure example given Figure 3 (also taken Kwisthout
et al., 2010). Now, Pr(A1 = true | x) = 1 x corresponds satisfying value assignment
V 0 otherwise; correspondingly, Pr(A1 = true) > 0 Constraint
Satisfaction instance satisfiable.
4.3 MAP Result
tw-reduction described previous sub-section easily modified twreduction Constraint Satisfaction MAP. adding binary node
711

fiKwisthout

R1

R4
X1

X2

X4
X3

A1

R2

R3

A2

A3

A4

A5

A6

Figure 3: Resulting graph BI adding nodes Ak appropriate arcs
VI thus obtained graph, A1 parent conditional probability
Pr(VI = true | A1 = true) = 1 Pr(VI = true | A1 = false) = 1/2 ,
number, smaller 1/|D||V| . Consequently, Pr(VI = true) > 1/2
satisfiable, Pr(VI = true) < 1/2 satisfiable; hence, MAP query
explanation set H = {VI } return VI = true satisfiable. added
single node BI , A1 parent, thus increasing tree-width BI
1. Hence, Constraint Satisfaction tw-reduces MAP.
4.4 Approximation Intractability Results
similar way modify reduction Sub-section 4.2 show valueapproximations, structure-approximations, rank-approximations tw-reduced
Constraint Satisfaction, sketched below.
4.4.1 Value-Approximation
add binary node VI , A1 parent, conditional probability
Pr(VI = true | A1 = true) = 1 Pr(VI = true | A1 = false) = 0. observe
variable set true. enforces Pr(A1 = true | VI = true)
non-zero probability (i.e., solvable) since otherwise conflicting evidence
thus constructed network. Thus, value-approximation algorithm explanation
set H = {A1 } evidence e = {VI = true} return solution approxsol B
cansol B Pr(approxsol B , e) > constant > 0, (that is, approximates additively
B)
) effectively solves Constraint
> Pr(optsol B ) relatively > Pr(optsol

Satisfaction: exists solution non-zero probability, construction dictates
must solvable. Given added single node BI , A1 parent,
increases tree-width BI 1. Hence, Constraint Satisfaction twreduces value-approximate MAP.
712

fiTree-Width MAP Approximations

4.4.2 Structure-Approximation
Observe tw-reduction MAP Sub-section 4.3 that, since H consists
singleton binary variable, trivially algorithm find explanation
approxsol B cansol B dH (approxsol B , optsol B ) |optsol B | 1 = 0 since would
solve MAP query. extend result hold explanation sets size
k constant k, i.e., structure-approximation algorithm guarantee return
correct value one k variables H polynomial time instances high
tree-width, unless ETH fails.
Instead adding single binary node VI tw-reduction MAP, add k
binary nodes VI1 . . . VIk , A1 parent Pr(VIj = true | A1 =
true) = 1 Pr(VIj = true | A1 = false) = 1/2 1 j k

described Sub-section 4.3. MAP query explanation set H = 1jk VIj
return 1jk VIk = true satisfiable; satisfiable, MAP
query return 1jk VIk = false probable explanation. Hence, structureapproximation algorithm correctly return value one variables H,
effectively solves Constraint Satisfaction. added k nodes BI , A1
parent outgoing arcs, tree-width BI increases 1. Hence,
Constraint Satisfaction tw-reduces structure-approximate MAP.
4.4.3 Rank-Approximation
modify proof Sub-section 4.3 follows. addition adding binary node
VI specified section, add dlog2 unconnected binary variables MI =
dlog
{MI1 . . . MI 2 } uniform probability H; m-rank-approximate MAP query
explanation set H = {VI } MI return VI = true (and MI set arbitrary value)
satisfiable. addition MI increase tree-width, hence,
Constraint Satisfaction tw-reduces m-rank-approximate MAP.
4.5 Discussion
efficient exact computation, value-approximation, structure-approximation, rankapproximation MAP showed bounded tree-width necessary condition, assumption ETH, general-purpose algorithm accepts arbitrary
instances. rule possibility may exist special-purpose algorithms compute approximate MAP explanations specific networks
special structure distribution (as already concluded Kwisthout et al. (2010)
Inference problem Bayesian networks). However, previous sub-section
shows, approximation problems intractable even extreme lower bounds
approximation quality, nature reductions follows
effectively approximate MAP explanations value, structure, rank, decide
problem exactly well. leaves little room efficient approximation algorithms
MAP instances high tree-width approximate value, structure, rank.
713

fiKwisthout

5. Expectation-Approximation Classes FERT FPERT
previous section showed cannot value-approximate, structure-approximate,
rank-approximate MAP instances high tree-width, unless ETH fails.
expectation-approximation? appears strategy employed
previous subsection cannot used show similar result expectation-approximation. fact, reasons believe efficient expectation-approximation MAP
indeed depends different set parameters notions approximation
discussed above, bounded tree-width necessary particular notion
approximation. notion parameterized approximability well captured
traditional fixed parameter tractable class FPT; therefore, introduce parameterized complexity classes FERT (Fixed Error Randomized Tractable) FPERT
(Fixed Parameter Error Randomized Tractable) characterize notion efficient parameterized expectation-approximation. Intuitively, contrast class FPT,
parameterizes running time, classes parameterize error probability (FERT),
respectively running time error probability (FPERT).
best knowledge, previous work proposes parameterize probability acceptance probabilistic Turing machine. Montoya Muller
(2013) define class BPFPT assumes bounded error independent
parameterization , amount randomness (operationalized number
coins used) bounded. Arvind Raman (2002) propose randomized approximation
algorithms counting problems, running time approximation ratio
parameterized, error probability constant. authors, however, assume
bounded (rather parameterized) error.
next section set formal machinery results. introduce natural parameterizations MajSAT, respectively E-MajSAT, FERT,
respectively FPERT. show restricted variant MAP indeed FERT, parameterized probability probable explanation, Frugal
Explanations problem (Kwisthout, 2015) FPERT number parameterizations.
elaborate relation classes classes BPP, PP, FPT,
finally, propose road map future research.
5.1 Parameterizing Error Bound Randomized Algorithms
formally define complexity class FERT follows:
Definition 5.1 Let decision problem let parameterization .
FERT exists probabilistic Turing machine
halts time, polynomial size input x, following acceptance criteria.
accepts Yes-instances probability 1/2 + min(f (), 1/|x|c ) constant c
arbitrary function f : R h0, 1/2]; No-instances accepted probability 1/2.
Observe definition demand halts time, polynomial input
size (and independent parameterization ), yet probability acceptance
Yes-instances may depend function . Intuitively, class FERT characterizes
problems efficiently computed randomized algorithm (i.e., polynomial
time, error arbitrarily close 0) bounded. canonical parameterized problem
714

fiTree-Width MAP Approximations

FERT {r}MajSAT, r denotes fraction satisfying truth assignments (or, equivalently, probability random truth assignment accepts).
follows corollary following result Littman, Majercik, Pitassi (2001):
Lemma 5.2 (adapted Littman et al., 2001) Let v number accepting
truth assignments Boolean formula , let v estimate v found via random
sampling using w samples variables . Let > 0 target approximation
2
error. probability |v v| > less 2e2 w .
Note solving MajSAT-instance, target approximation error directly
depends probability r random truth assignment accepts, acceptable error
(i.e., error still gives correct answer MajSAT instance) random
sampling algorithm = |r 1/2|. So, probability acceptance random truth
assignment polynomially bounded away 1/2, guarantee arbitrarily small
error using polynomially many samples using straightforward randomized algorithm.
Corollary 5.3 {r}MajSAT FERT.
allow parameterization running time probability acceptance, get complexity class FPERT defined follows:
Definition 5.4 Let decision problem let {1 , 2 } parameterization
. {1 , 2 } FPERT exists probabilistic Turing
machine halts time O(f1 (1 ) |x|c1 ), accepts Yes-instances
probability 1/2 + min(f2 (2 ), 1/|x|c2 ), accepts No-instances probability 1/2.
Here, f1 : R R f2 : R h0, 1/2] arbitrary computable functions c1 c2
constants.
define canonical problem FPERT, based observation {p}-SAT
FPT (Flum & Grohe, 2006, parameter p denotes number variables
formula) Corollary 5.3:
E-MajSAT
Instance: Let Boolean formula n variables xi , = 1, . . . , n, n 1,
furthermore partition variables sets XE XM .
Question: truth assignment XE majority truth
assignments XM satisfy ?
Parameter: 1 = p; 2 = r; p number variables set XE ; define r
follows. Let rxE denote ratio accepting truth assignments XM given
particular truth assignment xE XE . define r = minxE (|1/2 rxE |).
Informally, r describes minimum absolute distance 1/2 fraction accepting
truth assignments truth assignment xE XE . Observe try (bruteforce) truth assignments XE and, truth assignment, expectation-approximate
whether truth assignment majority truth assignments XM satisfy
. algorithm runs time O(2p nc ) constant c, probability least
1/2 + f (r) answering correctly (using polynomial number samples).
Corollary 5.5 {p, r}E-MajSAT FPERT.
715

fiKwisthout

5.2 Parameterized Expectation-Approximation MAP
Proving problem FPT normally done constructively, i.e., giving
deterministic algorithm decides time O(f () |x|c ) constant c > 1. Similarly,
proving FERT done giving randomized algorithm6 decides
polynomial time error 1/2 min(f (), 1/|x|c ). succeed giving
algorithm MAP general, however, prove restricted variant MAP
FERT parameterized probability probable explanation,
despite restricted variant remains PP-complete general bounded treewidth necessary parameter approximate problem value, structure, rank:
ConstrainedMAP
Instance: MAP. addition, demand E = , H consists singleton
node H outgoing edges, (H) = {true, false}.
Question: Pr(H = true) > 1/2?
Parameter: q = Pr(H = true).
PP-completeness Constrained-MAP follows trivial modification PPcompleteness proof Inference described Kwisthout (2009, Lemma 2.7 Lemma
2.9). Furthermore, given reductions Constraint Satisfaction MAP,
value-approximate MAP, structure-approximate MAP, rank-approximate MAP respect
restrictions imposed Constrained-MAP, necessity bounded treewidth follows.
show {q}Constrained-MAP FERT, parameter q = Pr(H =
true), give following approximation algorithm. Observe H binary sink
node (i.e., outgoing edges) B evidence. simple forward sampling
strategy (Henrion, 1986) approximate distribution H sampling values
variables network according probability distribution CPTs.
thus estimate Pr(H) taking samples; decide upon approxsol B using estimation.
Note degree error given particular number samples depends directly
probability q. precise, using Chernoff bound compute number

samples N needed degree error lower 1/(q 1/2)2 ln 1/ . gives
us fixed-parameter randomized tractable algorithm parameter {q}.
Corollary 5.6 {q}Constrained-MAP FERT.
Another parameterized problem shown fixed-error, fixed parameter
randomized tractable Frugal Explanations heuristic approach MAP, introduced Kwisthout (2015). heuristic (that either marginalizes samples intermediate variables, based subjective partition intermediate variables (into
set I+ set ) according expected contribution deciding upon MAP
explanation) expectation-approximated tractably tree-width network low, cardinality variables small, set I+ small, probability
distribution samples suffice decide upon MFE explanation
high probability. first three parameters ensure bounded running time, whereas
6. refer algorithm fixed-error randomized tractable algorithm.

716

fiTree-Width MAP Approximations

para-NPPP
FPERT

para-PP

FERT

para-NP

BPP

FPT
P

Figure 4: Inclusion properties complexity classes P, BPP, FERT, FPERT, FPT,
para-NP, para-PP, para-NPPP

final parameter ensures bounded probability error. Hence, {tw, c, |I+ |} {b}MFE
FPERT, tw denotes tree-width network, c denotes cardinality
variables, |I+ | denotes number variables marginalize (not sample) over, b
denotes parameter describing bias towards particular explanation. addition
shown {|H|, c, |I+ |} {b}MFE FPERT, |H| denotes size
explanation set.
Corollary 5.7 {tw, c, |I+ |} {b}MFE FPERT {|H|, c, |I+ |} {b}MFE FPERT.
5.3 Relation FERT, FPERT, Complexity Classes
complexity class FERT introduced randomized analog FPT. Rather
parameterizing running time (as arbitrary function polynomially
input size), parameterize probability acceptance Yes-instances. BPP, FERT,
para-PP form natural analogs P, FPT, para-NP, respectively. class FPERT
parameterizes running time probability acceptance, using two parameter
sets 1 2 . thus BPP FERT PP, FERT FPERT,
FPT FPERT. Obviously, FPERT para-NPPP , every slice {1 , 2 }1 NPPP (see
Flum & Grohe, 2006, discussion slices parameterized problems). inclusion
relations depicted Figure 4.
conjectured BPP = P (Clementi et al., 1998); however, clear whether
conjecture transposed parameterized world; is, whether
conjectured FERT = FPT. known {q, |I|}MAP {q, tw}MAP fixed
parameter tractable (Kwisthout, 2011); Constrained-MAP special case MAP,
results hold Constrained-MAP. However, intractability proof
neither |I| tw bounded. best knowledge parameterized
complexity result known (in either direction) {q}Constrained-MAP.
717

fiKwisthout

5.4 Efficient MAP Approximation: Road-map Future Research
established particular, constrained version MAP efficiently approximated expectation-approximations probability MAP explanation
high (where tree-width instance may unbounded). next step would
investigate parameterized approximability current state-of-the-art approximation
algorithms MAP show parameterization regimes algorithms
shown FERT FPERT.
different perspective, interesting explore parameterization
error randomized algorithms (for example) establish analog Whierarchy parameterizations. allows us derive fine-grained negative
parameterization results, similar way proving W[1]-hardness leads finegrained negative results proving para-NP-hardness.

6. Conclusion
paper analyzed whether low tree-width prerequisite approximating MAP
Bayesian networks. formalized four distinct notions approximating MAP (by value,
structure, rank, expectation) argued approximate MAP intractable general
using either notions. case value-approximation, structure-approximation,
rank-approximation showed MAP cannot approximated using notions
(non-trivial) instances high tree-width, ETH holds. However, showed
constrained version MAP, despite PP-hard general, tractably
expectation-approximated probable explanation high probability.
proposed complexity classes FERT FPERT capture parameterization
error (respectively error running time), rather running time. results
contributed fuller understanding make state-of-the-art
approximation algorithms MAP feasible practice.

7. Acknowledgements
previous version paper (Kwisthout, 2014) published Proceedings
Seventh European Workshop Probabilistic Graphical Models (PGM 2014). author
wishes thank workshop participants (both sets of) anonymous reviewers stimulating discussion worthwhile suggestions. thanks particular Hans Bodlaender
Todd Wareham valuable comments earlier version manuscript.

References
Abdelbar, A. M., & Hedetniemi, S. M. (1998). Approximating MAPs belief networks
NP-hard theorems. Artificial Intelligence, 102, 2138.
Agrawal, M., Kayal, N., & Saxena, N. (2004). PRIMES P. Annals Mathematics,
160 (2), 781793.
Arora, S., & Barak, B. (2009). Computational Complexity: Modern Approach. Cambridge
University Press.
718

fiTree-Width MAP Approximations

Arvind, V., & Raman, V. (2002). Approximation algorithms parameterized counting problems. Bose, P., & Morin, P. (Eds.), Algorithms Computation, Vol. 2518
Lecture Notes Computer Science, pp. 453464. Springer Berlin Heidelberg.
Cheng, J., & Druzdzel, M. (2000). AIS-BN: adaptive importance sampling algorithm
evidential reasoning large Bayesian networks. Journal Artificial Intelligence
Research, 13 (1), 155188.
Clementi, A., Rolim, J., & Trevisan, L. (1998). Recent advances towards proving P=BPP.
Allender, E. (Ed.), Bulletin EATCS, Vol. 64. EATCS.
Darwiche, A. (2009). Modeling Reasoning Bayesian Networks. Cambridge University Press.
De Campos, C. P. (2011). New complexity results MAP Bayesian networks.
Walsh, T. (Ed.), Proceedings Twenty-Second International Joint Conference
Artificial Intelligence, pp. 21002106.
Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Springer Verlag, Berlin.
Flum, G., & Grohe, M. (2006). Parameterized Complexity Theory. Springer, Berlin.
Gill, J. T. (1977). Computational complexity Probabilistic Turing Machines. SIAM
Journal Computing, 6 (4), 675695.
Hamilton, M., Muller, M., van Rooij, I., & Wareham, H. (2007). Approximating solution
structure. Demaine, E., Gutin, G., Marx, D., & Stege, U. (Eds.), Structure Theory
FPT Algorithmics Graphs, Digraphs Hypergraphs, No. 07281 Dagstuhl
Seminar Proceedings.
Henrion, M. (1986). Propagating uncertainty Bayesian networks probabilistic logic
sampling. Kanal, L., & Lemmer, J. (Eds.), Proceedings Second Annual
Conference Uncertainty Artificial Intelligence, pp. 149164. New York: Elsevier
Science.
Impagliazzo, R., & Paturi, R. (2001). complexity k-SAT. Journal Computer
System Sciences, 62 (2), 367 375.
Krentel, M. W. (1988). complexity optimization problems. Journal Computer
System Sciences, 36, 490509.
Kwisthout, J. (2009). Computational Complexity Probabilistic Networks. Ph.D.
thesis, Faculty Science, Utrecht University, Netherlands.
Kwisthout, J. (2011). probable explanations Bayesian networks: Complexity
tractability. International Journal Approximate Reasoning, 52 (9), 1452 1469.
Kwisthout, J. (2013). Structure approximation probable explanations Bayesian
networks. van der Gaag, L. (Ed.), Proceedings Twelfth European Conference
Symbolic Quantitative Approaches Reasoning Uncertainty, Vol. 7958
LNAI, pp. 340351. Springer-Verlag.
Kwisthout, J. (2014). Treewidth computational complexity MAP approximations.
van der Gaag, L., & Feelders, A. (Eds.), Proceedings Seventh European
Workshop Probabilistic Graphical Models, Vol. 8754 Lecture Notes Computer
Science, pp. 271285. Springer International Publishing.
719

fiKwisthout

Kwisthout, J. (2015). frugal explanations Bayesian networks. Artificial Intelligence,
218, 56 73.
Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2010). necessity bounded
treewidth efficient inference Bayesian networks. Coelho, H., Studer, R.,
& Wooldridge, M. (Eds.), Proceedings 19th European Conference Artificial
Intelligence, pp. 237242. IOS Press.
Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2011). complexity finding
kth probable explanations probabilistic networks. Cerna, I., Gyimothy, T.,
Hromkovic, J., Jefferey, K., Kralovic, R., Vukolic, M., & Wolf, S. (Eds.), Proceedings
37th International Conference Current Trends Theory Practice
Computer Science, Vol. LNCS 6543, pp. 356367. Springer.
Kwisthout, J., & van Rooij, I. (2013). Bridging gap theory practice
approximate Bayesian inference. Cognitive Systems Research, 24, 28.
Littman, M. L., Majercik, S. M., & Pitassi, T. (2001). Stochastic boolean satisfiability.
Journal Automated Reasoning, 27 (3), 251296.
Marx, D. (2007). beat treewidth?. Proceedings 48th Annual IEEE
Symposium Foundations Computer Science, pp. 169179.
Montoya, J.-A., & Muller, M. (2013). Parameterized random complexity.. Theory Computing Systems, 52 (2), 221270.
Papoulis, A. (1984). Probability, Random Variables, Stochastic Processes (2nd edition).
New York: McGraw-Hill.
Park, J. D., & Darwiche, A. (2001). Approximating MAP using local search. Proceedings
17th Conference Uncertainty Artificial Intelligence, pp. 403410. Morgan
Kaufmann Publishers, San Francisco, California, 2001.
Park, J. D., & Darwiche, A. (2004). Complexity results approximation settings
MAP explanations. Journal Artificial Intelligence Research, 21, 101133.
Robertson, N., & Seymour, P. (1986). Graph minors II: Algorithmic aspects tree-width.
Journal Algorithms, 7, 309322.
Sontag, D., Meltzer, T., Globerson, A., Weiss, Y., & Jaakkola, T. (2008). Tightening LP
relaxations MAP using message-passing. Proceedings 24th Conference
Uncertainty Artificial Intelligence, pp. 503510. AUAI Press.
Toda, S. (1994). Simple characterizations P(#P) complete problems. Journal
Computer System Sciences, 49, 117.
Van Rooij, I., & Wareham, H. (2012). Intractability approximation optimization
theories cognition. Journal Mathematical Psychology, 56 (4), 232 247.
Yuan, C., Lu, T., & Druzdzel, M. J. (2004). Annealed MAP. Chickering, D., & Halpern,
J. (Eds.), Proceedings Twentieth Conference Uncertainty Artificial Intelligence, pp. 628635. AUA.

720


