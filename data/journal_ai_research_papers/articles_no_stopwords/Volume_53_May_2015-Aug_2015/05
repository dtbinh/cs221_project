Journal Artificial Intelligence Research 53 (2015) 745-778

Submitted 02/15; published 08/15

AutoFolio:
Automatically Configured Algorithm Selector
Marius Lindauer

lindauer@cs.uni-freiburg.de

University Freiburg

Holger H. Hoos

hoos@cs.ubc.ca

University British Columbia

Frank Hutter

fh@cs.uni-freiburg.de

University Freiburg

Torsten Schaub

torsten@cs.uni-potsdam.de

University Potsdam
INRIA Rennes

Abstract
Algorithm selection (AS) techniques involve choosing set algorithms
one expected solve given problem instance efficiently substantially
improved state art solving many prominent AI problems, SAT, CSP,
ASP, MAXSAT QBF. Although several procedures introduced,
surprisingly, none dominates others across scenarios. Furthermore,
procedures parameters whose optimal values vary across scenarios. holds
specifically machine learning techniques form core current procedures, hyperparameters. Therefore, successfully apply new problems,
algorithms benchmark sets, two questions need answered: (i) select
approach (ii) set parameters effectively. address problems
simultaneously using automated algorithm configuration. Specifically, demonstrate
automatically configure claspfolio 2, implements large variety
different approaches respective parameters single, highly-parameterized
algorithm framework. approach, dubbed AutoFolio, allows researchers practitioners across broad range applications exploit combined power many different
methods. demonstrate AutoFolio significantly improve performance
claspfolio 2 8 13 scenarios Algorithm Selection Library, leads
new state-of-the-art algorithm selectors 7 scenarios, matches state-ofthe-art performance (statistically) scenarios. Compared best single
algorithm scenario, AutoFolio achieves average speedup factors 1.3
15.4.

1. Introduction
last decade, tremendous progress Boolean constraint solving technology
achieved several areas within AI, SAT (Biere, 2013), ASP (Gebser, Kaufmann,
& Schaub, 2012), CSP (Tamura, Taga, Kitagawa, & Banbara, 2009), Max-SAT (Abrame
& Habet, 2014) QBF (Janota, Klieber, Marques-Silva, & Clarke, 2012).
areas, multiple algorithms complementary solving strategies exist, none dominates others kinds problem instances. fact exploited algorithm selection (AS) (Rice, 1976) methods, use characteristics individual probc
2015
AI Access Foundation. rights reserved.

fiLindauer, Hoos, Hutter, & Schaub

lem instances (so-called instance features) choose promising algorithm instance. Algorithm selectors empirically shown improve state art
solving heterogeneous instance sets and, result, many prizes competitions. instance, SATzilla (Xu, Hutter, Hoos, & Leyton-Brown, 2008) several
categories multiple SAT competitions, claspfolio 1 (Gebser, Kaminski, Kaufmann,
Schaub, Schneider, & Ziller, 2011b) NP-track 2011 ASP Competition, CPHydra (OMahony, Hebrard, Holland, Nugent, & OSullivan, 2008) 2008 CSP
competition, ISAC++ (Ansotegui, Malitsky, & Sellmann, 2014) partial Max-SAT
Crafted Industrial track 2014 Max-SAT Competition, AQME (Pulina &
Tacchella, 2009) first stage main track 2010 QBF Competition.
Although many new approaches proposed years (cf. Smith-Miles,
2008; Kotthoff, 2014), two flexible frameworks allow re-implementing
comparing existing approaches fair uniform way: LLAMA (Kotthoff, 2013)
claspfolio 2 (Hoos, Lindauer, & Schaub, 2014). these, claspfolio 2
comprehensive, encompassing strategies algorithm selection systems 3S (Kadioglu,
Malitsky, Sabharwal, Samulowitz, & Sellmann, 2011), aspeed (Hoos, Kaminski, Lindauer,
& Schaub, 2015), claspfolio 1 (Gebser et al., 2011b), ISAC (Kadioglu, Malitsky, Sellmann, & Tierney, 2010), ME-ASP (Maratea, Pulina, & Ricca, 2014), SNNAP (Collautti,
Malitsky, Mehta, & OSullivan, 2013) SATzilla (Xu et al., 2008; Xu, Hutter, Hoos,
& Leyton-Brown, 2011).
Figure 1 illustrates performance benefits existing selection strategies (as realized claspfolio 2) yield across wide range benchmarks Algorithm Selection Library (Bischl et al., 2015b, 2015a). observe approach strengths
weaknesses different scenarios. SATzilla11-like approach (the default
claspfolio 2) performs best overall, achieves better performance
approaches considered 8 13 scenarios, 3S, aspeed ISAC yielding
better performance remaining cases.
note selection approaches used fixed default parameter
configuration might therefore fall short full performance potential. example, imputation missing instance features used approaches considered Figure 1; use improve performance scenarios (e.g.,
ASP-POTASSCO), yields improvements others (e.g., SAT12-RAND,
SATzilla11-like approach plus mean imputation outperforms single best algorithm
factor 1.2).
Generally, well known performance many machine learning techniques
depends hyper-parameter settings (e.g., case SVM, kernel, kernel hyperparameter soft margin; cf. Bergstra, Bardenet, Bengio, & Kegl, 2011; Snoek, Larochelle,
& Adams, 2012; Thornton, Hutter, Hoos, & Leyton-Brown, 2013). However, hyperparameters machine learning models used Figure 1 fixed manually, based
limited experiments. Therefore, performance algorithm selection systems
considered could likely improved using carefully chosen hyper-parameter
settings.
Facing new algorithm selection problem, thus answer three salient questions: (i) selection approach use; (ii) set parameters selection
approach (and underlying machine learning model) effectively; (iii) make
746

filio
-1.
0-l
ISA
ike
C-l
ike

-AS
P-l
ike
SA
Tzi
lla
'09
-lik
SA
e
Tzi
lla
'11
-lik
Au
e
toF
oli


cla

sp
fo


asp

ee

3S

-lik

e

AutoFolio: Automatically Configured Algorithm Selector

ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

4.1
1.5
6.5
2.9
10.9
7.7
2.6
1.2
3.9
1.5
1.7
1.2
0.8

1.4
1.0
2.7
3.6
6.3
4.9
3.6
1.1
4.7
1.1
1.8
0.8
0.8

2.8
2.1
1.6
1.2
3.5
2.3
1.1
1.2
1.2
1.2
1.1
1.2
0.6

3.8
2.1
4.9
1.3
4.3
2.8
1.2
1.3
2.5
1.1
1.1
1.2
0.9

1.9
2.6
2.1
1.1
3.1
2.8
1.0
1.2
1.8
1.1
1.0
1.1
0.9

2.9
2.5
3.4
1.5
4.9
3.7
1.9
1.1
2.6
1.4
1.5
1.3
0.9

4.2
3.1
8.6
2.3
6.5
9.8
2.3
1.2
3.8
1.8
1.9
1.3
0.9

4.2
3.2
8.6
3.5
7.8
10.1
3.2
1.5
15.4
3.0
3.2
1.8
1.3

geo. mean

2.6

2.0

1.5

1.9

1.5

2.0

2.8

3.9

Figure 1: Factors selection approach re-implemented claspfolio 2 outperformed single best algorithm 13 ASlib scenarios w.r.t. penalized average runtime
(PAR10, counts timeout 10 times given runtime cutoff). results
10-fold cross-validation, ignoring test instances solved solver.
last row shows geometric mean 13 scenarios.

best use techniques augmenting pure AS, pre-solving schedules (Xu et al., 2008;
Kadioglu et al., 2011). Instead common, manual trial-and-error approach, propose automatically answer questions using automated algorithm configuration
methods (Hutter, Hoos, Leyton-Brown, & Stutzle, 2009) configure flexible frameworks. manual approach error-prone, potentially biased requires substantial human expert time knowledge, approach introduce fully automatic,
unbiased, leverages full power broad range methods. thus facilitates
easier effective use algorithm selection makes techniques accessible
broader community.
Specifically, present AutoFolio, general approach automatically determining
strong algorithm selection method particular dataset, using algorithm configuration
search flexible design space algorithm selection methods. provide
open-source implementation AutoFolio (www.ml4aad.org/autofolio/) based
algorithm configurator SMAC (Hutter, Hoos, & Leyton-Brown, 2011) algorithm
selection framework claspfolio 2 (Hoos et al., 2014). last column Figure 1 previews
results obtained AutoFolio clearly shows significant improvements
claspfolio 2 10 13 scenarios ASlib.
747

fiLindauer, Hoos, Hutter, & Schaub

Instance

Algorithm
Portfolio

Compute
Features

Select Algorithm

Solve Instance
Algorithm

Figure 2: General outline algorithm selection.

2. Background: Algorithm Configuration Selection
section, briefly introduce standard approaches algorithm selection algorithm configuration form basis AutoFolio approach.
2.1 Algorithm Selection
Figure 2 shows general outline algorithm selection (Rice, 1976). given problem
instance, first compute cheap instance features; numerical characteristics,
including simple ones (such number variables clauses SAT instance)
complex ones (such statistics gathered short probing runs actual SAT
solver given instance). Based features, appropriate algorithm
algorithm portfolio (Huberman, Lukose, & Hogg, 1997; Gomes & Selman, 2001) selected
solve given instance. overall workflow subject runtime cutoff.
One major challenge algorithm selection find good mapping instance
features algorithms. general offline algorithm selection approach consider,
done based training data. Specifically, given portfolio algorithms set
problem instances I, use training data performance matrix size #I #A
feature matrix containing fixed-size feature vector I. Based training
data, learn mapping instance features algorithms using machine learning
techniques, k-NN (Maratea et al., 2014), g-means (Kadioglu et al., 2010) random
forests (Xu et al., 2011).
2.1.1 Related Work Algorithm Selection Systems
Recent successful algorithm selection systems include SATzilla (Xu et al., 2008; Xu, Hutter, Hoos, & Leyton-Brown, 2012a), 3S (Kadioglu et al., 2011; Malitsky, Sabharwal, Samulowitz, & Sellmann, 2012, 2013b), ISAC (Kadioglu et al., 2010; Ansotegui et al., 2014),
CSHC (Malitsky, Sabharwal, Samulowitz, & Sellmann, 2013a) claspfolio 1 (Gebser
et al., 2011b). recent years, systems showed excellent performance competitions
SAT, MAXSAT ASP. briefly review following.
original version pioneering algorithm selection system SATzilla (Xu et al.,
2008) learned mapping instance features algorithms training ridge regression models. regression model predicts performance algorithm given
instance. Based predicted performances, SATzilla selects algorithm
best predicted performance. SATzillas latest version (Xu et al., 2011) uses classification
models that, pair algorithms, predict better-performing one, selects
algorithm run using simple voting predictions thus obtained. models
cost-sensitive, is, training instance pairwise classification models
748

fiAutoFolio: Automatically Configured Algorithm Selector

weighted performance loss incurred selecting worse two algorithms.
Furthermore, SATzilla introduced concept pre-solving schedules, is, short
instance-independent schedule algorithms running limited amount time. one
algorithm pre-solving schedule solves given instance, SATzilla immediately
terminate successfully, saving time required compute instance features. Furthermore,
pre-solving schedules increase robustness algorithm selectors relying
one selected algorithm pre-solvers solve given instance. One drawback
SATzilla use grid search possible pre-solving schedules three
pre-solvers; schedule considered, SATzilla performs algorithm subset selection
trains classification models, require substantial amounts time (in
experiments, 4 CPU days).
3S (Kadioglu et al., 2011; Malitsky et al., 2012, 2013b) uses k-nearest neighbour
approach select algorithm. given problem instance solved, determines
set similar training instances instance feature space selects algorithm
best performance instance set. performance k-NN approach
improved distance-based weighting (that is, weighting algorithm performance
instance instances distance new given instance) using clusteringbased adaptive neighbourhood size (to adjust size neighbourhood different
areas feature space). Furthermore, 3S uses mixed integer programming compute
pre-solving schedules efficiently SATzilla.
ISAC (Kadioglu et al., 2010) clusters instances instance feature space using
g-means algorithm stores cluster centre well best-performing algorithm
cluster. new problem instance, determines nearest cluster centre
(1-NN) selects algorithm associated it.
cost-sensitive hierarchical clustering system CSHC (Malitsky et al., 2013a)
partitions feature space clusters, instead ISACs unsupervised clustering
approach, creates partitioning supervised top-down fashion, much decision
regression tree algorithm. Starting instances (the entire feature space)
root tree, recursively splits instances associated node two child
nodes, choosing split along single feature value, performance
best-performing algorithm child node optimized. cost-sensitive supervised
approach based trees closely resembles cost-sensitive random forests SATzilla,
difference that, contrast SATzillas pairwise voting approach, builds
single model.
Last least, claspfolio 1 (Gebser et al., 2011b) predecessor claspfolio 2, use (and describe Section 2.1.2). contrast flexible framework claspfolio 2, claspfolio 1 inspired earlier version SATzilla
uses regression approach, different machine learning method (support
vector regression instead ridge regression).
systems algorithm selection combine extend techniques, example, combining regression clustering approaches (Collautti et al., 2013),
selecting algorithm portfolios (Yun & Epstein, 2012; Lindauer, Hoos, & Hutter, 2015a)
schedules (Amadini, Gabbrielli, & Mauro, 2014) instead single algorithm. additional information, refer interested reader two recent surveys algorithm
selection (Smith-Miles, 2008; Kotthoff, 2014).
749

fiLindauer, Hoos, Hutter, & Schaub

Feature
Generator

Training Instances

Algorithms

Instance Features
Groups

Algorithm
Performance
ASlib Scenario

Feature
Preprocessing

Performance
Preprocessing
Train
Selection Model(s)



Performance Estimation

Pre-Solving Schedule
aspeed

Selection

Scheduling
Offline Training

(Test) Instance

Compute Features

Select Algorithm

failed
Run Backup
Algorithm

Run Pre-Solving
Schedule
successful
Run Selected
Algorithm
Online Solving

Figure 3: General workflow claspfolio 2. Objects algorithms instances
shown rectangles, activities depicted rectangles rounded corners.
Activities related algorithm selection shown red activities related algorithm
schedules yellow.
2.1.2 Algorithm Selection Framework claspfolio 2
explain algorithm selection framework claspfolio 2 (Hoos et al., 2014; Lindauer, Hoos, & Schaub, 2015c) detail, since provides basis
concrete implementation general AutoFolio approach, used experiments.
claspfolio 2 framework implements idea algorithm selection flexible
general way. provides general view individual components algorithm
selectors, based implements many different selection approaches associated
techniques. Therefore, claspfolio 2 natural candidate serve basis
AutoFolio approach.
Figure 3 shows workflow claspfolio 2, divided ASlib Scenario
input claspfolio 2; Offline Training Selection Scheduling; Online Solving
new instance:
ASlib scenario. input, claspfolio 2 reads algorithm selection scenario, supporting format Algorithm Selection library, ASlib. consists
performance matrix, instance features, groups instance features1 optional information, cross-validation splits ground truth problem
1. note that, according definition ASlib, feature group enables list instance features
computed common block feature computation code, jointly incur cost
running code.

750

fiAutoFolio: Automatically Configured Algorithm Selector

instances (for example, whether SAT instance satisfiable unsatisfiable).
full specification ASlib format, refer interested reader aslib.net.
Offline training selection. Based given scenario (training) data, claspfolio 2
pre-processes instance features (for example, normalization feature imputation)
performance data (for example, log-transformation). Using machine learning
techniques, claspfolio 2 learns selection model maps instance features
algorithms.
Offline training scheduling. compute efficient pre-solving schedule, claspfolio 2 first estimates performance Selection module using internal
cross-validation training data (Arrow I). Based performance estimation,
claspfolio 2 computes timeout-minimal pre-solving schedule using Answer Set
Programming aspeed (Hoos et al., 2015), assigning algorithm (potentially
zero-length) time slice overall runtime budget. estimation Selection
module necessary compute runtime budget pre-solving schedule.
Selection module performs well, pre-solving schedule may empty,
pre-solving schedule cannot perform better perfect predictor (that is, predictor always selects best solver). contrast, prediction performs
poorly (for example, result non-informative instance features), pre-solving
schedule may allocated complete time budget, Selection module
ignored.
Online solving. Solving workflow follows: feature generator computes
instance features new problem instance solved; computation fails
(for example, time memory constraints) feature imputation
strategy selected, backup solver i.e., single best performing solver
offline training run instance; otherwise, previously trained selection
model uses instance features select algorithm expected perform well.
pre-solving schedule available, schedule runs either instance feature
computation selection algorithm, depending parameter setting
claspfolio 2 latter version shown Figure 3. former
advantage time compute instance features saved instance
solved pre-solving. latter advantage algorithm chosen
selector removed pre-solving schedule prevent running twice.
list techniques implemented modules given Section 3.2.
2.2 Algorithm Configuration
Figure 4 shows general outline algorithm configuration methods. Given parameterized algorithm possible parameter settings C, set training problem instances
I, performance metric : C R, objective algorithm configuration
problem find parameter configuration c C minimizes across instances
I. Prominent examples performance metric optimized runtime,
solution quality, misclassification cost target algorithm achieves. configuration
751

fiLindauer, Hoos, Hutter, & Schaub

Instances

Algorithm
Configuration Space C

Select c C

Assess A(c)
0

Returns
Best Found
Configuration c

Return Performance
Configuration Task

Figure 4: General outline algorithm configuration.
procedure (or short configurator ) iteratively evaluates performance parameter configurations c C (by running one instances I) uses
result decide next configurations evaluate. given budget configuration process exhausted, configurator returns best known parameter
configuration found then.
n parameters p1 , . . . , pn , respective domains D1 , . . . , Dn , parameter
configuration space C = D1 Dn cross-product domains,
parameter configuration c C assigns value parameter. several types
parameters, including real-valued, integer-valued categorical ones (which finite,
unordered domain; example, choice different machine learning algorithms).
Furthermore, configuration spaces structured; specifically, parameter pi
conditional another parameter pj , value pi relevant parent
parameter pj set specific value. example, case pj categorical
choice machine learning algorithms, pi sub-parameter one
algorithms; pi active pj chooses algorithm parameterizes further.
date, four general configuration procedures: ParamILS (Hutter et al.,
2009), GGA (Ansotegui, Sellmann, & Tierney, 2009), irace (Lopez-Ibanez, Dubois-Lacoste,
Stutzle, & Birattari, 2011), SMAC (Hutter et al., 2011). principle, could use
configurator general AutoFolio approach. practice,
found SMAC often yield better results ParamILS GGA (Hutter et al., 2011;
Hutter, Lindauer, Balint, Bayless, Hoos, & Leyton-Brown, 2015; Lindauer, Hoos, Hutter, &
Schaub, 2015b), thus use basis concrete implementation AutoFolio
discussed following. describe SMAC detail.
2.2.1 SMAC: Sequential Model-Based Algorithm Configuration
sequential model-based algorithm configuration method SMAC (Hutter et al., 2011;
Hutter, Hoos, & Leyton-Brown, 2015a) uses regression models approximate performance metric : C R (Hutter, Xu, Hoos, & Leyton-Brown, 2014). follows
general algorithm configuration workflow above, alternating evaluations
parameter configurations instances decision phases, configurator uses
data gathered far select configurations evaluate next instances.
SMACs decision phases involve constructing regression model : C R based
data observed far, using model (as well models uncertainty
predictions) select promising configurations try next. step automatically
752

fiAutoFolio: Automatically Configured Algorithm Selector

trades exploration (evaluating regions configuration space model
uncertain) exploitation (evaluating configurations predicted perform well).
order save time evaluating new configurations cnew C, SMAC first evaluates
single instance I; additional evaluations carried (using doubling
schedule) if, based evaluations date, cnew appears outperform SMACs best
known configuration c. evaluated number runs cnew c,
cnew still performs better, SMAC updates best known configuration c cnew .
2.2.2 Previous Applications Algorithm Configuration
Algorithm configuration demonstrated effective optimizing algorithms wide range problems, including SAT-based formal verification (Hutter, Babic,
Hoos, & Hu, 2007), timetabling (Chiarandini, Fawcett, & Hoos, 2008), multi-objective optimization (Lopez-Ibanez & Stutzle, 2010), mixed integer programming (Hutter, Hoos, &
Leyton-Brown, 2010), AI planning (Vallati, Fawcett, Gerevini, Hoos, & Saetti, 2013), generation heuristics (Mascia, Lopez-Ibanez, Dubois-Lacoste, & Stutzle, 2014), occupancy
scheduling (Lim, van den Briel, Thiebaux, Backhaus, & Bent, 2015) kidney exchange
matching (Dickerson & Sandholm, 2015). important special case algorithm configuration hyperparameter optimization machine learning (Bergstra et al., 2011; Snoek
et al., 2012; Eggensperger et al., 2013).
previous line work related application configuration algorithm
selection Auto-WEKA (Thornton et al., 2013). Auto-WEKA addresses combined
problem selecting machine learning algorithm WEKA framework (Hall, Frank,
Holmes, Pfahringer, Reutemann, & Witten, 2009) optimizing hyperparameters.
AutoFolio needs solve combined algorithm selection hyperparameter
optimization problem, particular needs problem formulations
considers: regression, classification clustering. important design choices
AutoFolio pre-solving parameters, well instance features use.
AutoFolio applies one meta-solving strategy (algorithm configuration) another one
(algorithm selection). previous application meta-solving strategy selfconfiguration ParamILS (Hutter et al., 2009). However, case, self-configuration
yielded modest improvement default configuration ParamILS, whereas
here, achieve substantial improvements default configuration claspfolio 2.
Algorithm configuration algorithm selection previously combined
different way, using configuration find good parameter settings highly parameterized algorithm, using selection choose per-instance
basis. Two systems implement approach date: ISAC (Kadioglu et al., 2010)
Hydra (Xu, Hoos, & Leyton-Brown, 2010). ISAC first clusters training problem instances
homogeneous subsets, uses configurator find good solver parameterization
cluster, uses selector choose parameterizations. Hydra
iteratively adds new solver parameterizations initially empty portfolio-based selector,
step tasking configurator find solver parameterization improves
current portfolio.
753

fiLindauer, Hoos, Hutter, & Schaub

Training Data

Test Data

10-fold cross-validation = 10 meta instances

Figure 5: Split instance sets algorithm selection scenarios; cross-validation performed
inside configuration process, test set withheld evaluating configured selector.

3. Configuration Algorithm Selectors
present AutoFolio approach using algorithm configurators automatically
customize flexible algorithm selection (AS) frameworks specific scenarios. apply
algorithm configuration context, need specify parameterized selector
configuration space, well performance metric judge performance.
3.1 Formal Problem Statement
judge performance algorithm selection (AS) system scenario,
crucial partition given set problem instances training test set, use
system training set train selector s, evaluate test
set instances. (If training set instead used evaluate performance, perfect
system could simply memorize best solver instance without learning anything
useful new problem instances). standard notion training-test split
machine learning.
scenario includes algorithms A, problem instances I, performance feature
data D, loss function l : R minimized (for example, algorithms
runtime solution cost), data split disjoint sets Dtrain Dtest . Let
S(Dtrain ) : denote selector learned system trained data
Dtrain . Then, performance S, P (S) average performance algorithms
selects instances test data set Dtest :
P (S) =

X
1

l(S(Dtrain ), i).
|Dtest |

(1)

iDtest

Likewise, evaluate performance system Sc parameterized
configuration c P (Sc ). However, perform algorithm configuration simply
minimizing P (Sc ) respect c C: would amount peeking test set
many times, even though would yield configuration c low P (Sc ), could
expected perform well instances contained Dtest . Instead, order
obtain unbiased evaluation configured selectors performance end, need
hold back test set instances touched configuration process.
order still able optimize parameters without access test set, standard
solution machine learning partition training set further, k cross-validation
folds. Overall, use instance set selection scenario illustrated Figure 5:
(i) split full set instances training test set (ii) training data
754

fiAutoFolio: Automatically Configured Algorithm Selector

Algorithm 1: AutoFolio: Automated configuration algorithm selector
Input : algorithm configurator AC, algorithm selector S, configuration space C
S, training data algorithm scenario (with performance feature
matrix), number folds k

5

randomly split D(1) , . . . , D(k)
start AC D(1) , . . . , D(k) meta instances, using average loss across
meta-instances performance metric m, using target algorithm
configuration space C
configuration budget remaining
AC selects configuration c C meta instance n {1 . . . k}
train Sc D\D(n) , assess loss D(n) return loss AC

6

return best configuration c found AC

1
2

3
4

partitioned k folds (in experiments, use k = 10), used
follows.
(1)
(k)
Let Dtrain , . . . , Dtrain random partition training set Dtrain . crossvalidation performance CV (Sc ) Sc training set then:


k
X
X
1
1


(j)
CV (Sc ) =
l(Sc (Dtrain \Dtrain ), i)
(2)
(j)
k
|Dtrain |
(j)
j=1
iDtrain

end, optimize performance CV (Sc ) determining configuration c C
selector good cross-validation performance
c arg min CV (Sc ),

(3)

cC

evaluate c training selector Sc entire training data evaluating
P (Sc ) Dtest , defined Equation 1.
(j)
Following Thornton et al. (2013), use k folds Dtrain one instance within
configuration process. order avoid confusion instances
base-level problem instances (e.g., SAT instances) solved inside instance,
refer instance meta-instance. note many configurators, FocusedILS (Hutter et al., 2009), irace (Lopez-Ibanez et al., 2011) SMAC (Hutter et al.,
2011), discard configurations perform poorly subset meta-instances
therefore evaluate k cross-validation folds every configuration.
saves time lets us evaluate configurations within configuration
budget. Based considerations, Algorithm 1 outlines process configure
algorithm selector AutoFolio.
Since instances scenario could split configuration testing sets
many different ways, one split necessarily yield representative performance
estimate. Therefore, yield confident results evaluation, perform additional outer cross-validation (as given ASlib scenario) instead single training-test
755

fiLindauer, Hoos, Hutter, & Schaub

split. is, consider multiple training-test splits, configure selector training set, assess final configurations respective test data sets, average results.
note, however, practical application AS, one would single training set (which would still split k cross-validation splits internally) single test
set.
3.2 Configuration Space Selectors
existing algorithm selectors implement one specific algorithm selection approach, using
one specific machine learning technique. note, however, selection approaches,
least implicitly, admit flexibility, particular could used range
machine learning techniques. example, SATzilla11 (Xu et al., 2011) uses voting
pairwise performance predictions obtained cost-sensitive random forest classifiers, but,
principle, could use cost-sensitive binary classifiers instead random forests.
Based observation, consider hierarchically structured configuration space
top-level parameter determines overall algorithm selection approach
example, regression approach, used SATzilla09 (Xu et al., 2008) k-NN
approach, used ME-ASP (Maratea et al., 2014). selection approaches,
choose different regression techniques, example, ridge regression, lasso
regression, support vector regression random forest regression. machine
learning techniques configured (hyper-)parameters.
Besides selection approach, techniques used preprocessing training data (for example, z-score feature normalization feature preprocessing step
log-transformation runtime data performance preprocessing step). Preprocessing
techniques configured independently selection approach, therefore
handled top-level parameters.
use third group parameters control pre-solving schedules (Kadioglu et al.,
2011; Xu et al., 2011), including parameters determine time budget pre-solving
number pre-solvers considered. Pre-solving techniques freely combined
selection approaches; always needed, added top-level binary
parameter completely activates deactivates use pre-solvers; presolving parameters conditional switch.
implemented choices claspfolio 2 system described Section 2.1.2.
Figure 6 illustrates complete configuration space thus obtained. current version,
use concrete implementation AutoFolio approach, covers six
different algorithm selection approaches:
(hierarchical) regression (inspired SATzilla09; Xu et al., 2008) learns regression
model algorithm; new instace, selects algorithm best
predicted performance;
multiclass classification (inspired LLAMA; Kotthoff, 2013) learns classification
model directly selects algorithm based features new instance;
pairwise classification (inspired SATzilla11; Xu et al., 2011) learns (cost-sensitive)
classification model pairs algorithms; new instance, evaluates models selects algorithm votes;
756

fiAutoFolio: Automatically Configured Algorithm Selector

transformation

pre-solving

yes

instance
weighting

contribution
filtering

normalization

approach

imputation

p : PCA

Performance Preprocessing

max_feature_time

Feature Preprocessing

4
multi-class
classification

pairwise
classification

c (SVM)
gamma(SVM)

(hierarchical)
regression

clustering

SNNAP

k : k-NN

max cluste r

k
best_n

random
forest

SVM

gradient
boosting

random
forest

SVM

gradient
boosting

ridge

lasso

SVR

random
forest

ridge

lasso

SVR

random
forest

3

2

3

3

2

3

1

1

3

2

1

1

3

2

k-means

Gaussian
mixture

spectral
clustering

Figure 6: Configuration space claspfolio 2, including 22 categorial parameters, 15
integer valued parameters 17 continous parameters. Parameters double boxes
top-level parameters; single boxes represent algorithm selection approaches based classes
machine learning techniques, dashed boxes machine learning techniques dotted boxes
indicate number low-level parameters. Parameter boxes used default configuration filled grey.
clustering (inspired ISAC; Kadioglu et al., 2010) determines subsets similar training
instances feature space best algorithm subsets; new
instance, determines nearest cluster center selects associated algorithm;
k-NN (inspired 3S; Kadioglu et al., 2011, ME-ASP; Maratea et al., 2014) determines set similar training instances feature space given new instance
selects algorithm best performance instance set;
SNNAP (inspired Collautti et al., 2013) predicts performance algorithm
regression models uses information k-NN approach predicted
performance space.
approaches, claspfolio 2 covers least three different machine
learning techniques (where appropriate). listed Figure 6; example, pairwise
classification based random forests, SVMs gradient boosting (with 3, 2 3
hyper-parameters, respectively). preprocessing strategies, supports:
Performance preprocessing:
transformation applies log (Xu et al., 2008) z-score normalization (Collautti
et al., 2013) performance data;
instance weighting weights instances impact performance
algorithm selector, is, instances get low weight available algorithms perform equally, high weight algorithms differ substantially
performance (Kotthoff, Gent, & Miguel, 2012);
contribution filtering removes algorithms less specified contribution performance oracle (also known virtual best solver) (Xu
et al., 2012a); form algorithm subset selection.

757

fiLindauer, Hoos, Hutter, & Schaub

Feature preprocessing:
normalization transforms instance features min-max, z-score, decimalpoint, log scheme application PCA;
p:PCA applies principal component analysis features selects top p
principal components, p parameter (if PCA activated);
imputation fills missing feature values median, average frequent
value feature imputation deactivated feature vector incomplete
given instance, single best solver statically selected;
max feature time limits amount time spent collect features ensures
much time spent feature computation; however, result
incomplete features missing values (which get imputed imputation
active).
chose default configuration claspfolio 2 (used initialize algorithm
configurator) SATzilla11-like configuration, since shown effective
SAT (Xu et al., 2012a) ASP (Hoos et al., 2014), since overall high performance
evident results Figure 1. configuration uses pairwise cost-sensitive random
forest classifiers, z-score feature normalization pre-solving schedule three
pre-solvers. Since assume prior knowledge algorithm selection scenarios,
default configuration uses default instance features defined scenario designers.
chose claspfolio 2 basis AutoFolio, designed
flexible known perform well.2 note principle, selectors,
SATzilla (Xu et al., 2008), ISAC (Kadioglu et al., 2010), SNNAP (Collautti et al.,
2013) LLAMA (Kotthoff, 2013), could generalized similar way.
addition using claspfolio 2 algorithm selection framework, current
version AutoFolio employs algorithm configurator SMAC (described Section
2.2.1). selection framework, configurator exchangeable: chose
SMAC, performed best across algorithm configuration problems studied
far, principle, configurators could used, as, GGA (Ansotegui et al.,
2009) irace (Lopez-Ibanez et al., 2011). Preliminary results (Lindauer et al., 2015b)
showed ParamILS optimize performance claspfolio 2,
inferior SMAC one scenario, performance advantage small.

4. Empirical Performance Analysis
section, empirically analyze performance AutoFolio approach.
experiments, AutoFolio employs claspfolio 2 using well-known machine learning package scikit-learn (Pedregosa et al., 2011) (version 0.14.1) algorithm configurator SMAC (version 2.08.00). ran AutoFolio thirteen algorithm selection
scenarios make Algorithm Selection Library 1.0 (Bischl et al., 2015b).3
2. Results performance claspfolio 2 compared state-of-the-art algorithm selectors
found aslib.net.
3. note experiments ASlib scenarios, claspfolio 2 algorithm selectors
need perform actual runs algorithms feature generators, ASlib scenarios already

758

fiAutoFolio: Automatically Configured Algorithm Selector

shown Table 1, scenarios comprise wide variety hard combinatorial
problems; includes performance data range solvers (between 2
31) set instances, instance features organized feature groups associated
costs. scenarios consider here, performance objective runtime minimization.
high level, scenarios comprise following data:
ASP-POTASSCO: runtimes different parameter configurations ASP solver
clasp broad range ASP instances collected Potassco group (Gebser,
Kaminski, Kaufmann, Ostrowski, Schaub, & Schneider, 2011a);
CSP-2010: runtimes single solver two different configurations (with
without lazy learning; Gent, Jefferson, Kotthoff, Miguel, Moore, Nightingale, & Petrie,
2010) collection CSP instances;
MAXSAT12-PMS: runtime data 2012 MaxSAT Evaluation;
PREMARSHALLING: runtimes -based IDA -based solvers premarshalling problem, real-world, time-sensitive pre-marshalling problem instances
operations research literature;
PROTEUS-2014: runtimes different CSP SAT solvers range CSP
instances, preprocessed various CSP-to-SAT translation techniques;
QBF-2011: runtime data QBF solvers AQME system (Pulina &
Tacchella, 2009) QBF instances 2010 QBF Solver Evaluation;
SAT11-HAND, SAT11-INDU SAT11-RAND: runtime data respective tracks 2011 SAT Competition;
SAT12-ALL, SAT12-HAND, SAT12-INDU SAT12-RAND: runtimes various SAT solvers broad range SAT instances used train algorithm
selection system SATzilla (Xu, Hutter, Shen, Hoos, & Leyton-Brown, 2012b)
respective tracks 2012 SAT Challenge.
refer Bischl et al. (2015b) details scenarios, including baseline
experiments showing algorithm selection applied effectively scenarios. point using common library allows us compare AutoFolio
fair uniform way algorithm selection methods. However, price pay
uniform comparison necessarily consider current state-of-the-art
algorithms solving respective problems, since ASlib data collected
several years ago. Furthermore, note current version ASlib consists
deterministic performance data. expect future versions consider scenarios stochastic performance data multiple runs per algorithm instance, using
different pseudo-random number seeds. AutoFolio applied stochastic scenarios straightforward manner, optimizing mean performance across runs
instance
contain necessary performance data feature vectors (in order allow fair comparison
algorithm selectors based data, without confounding factor due hardware platform
used run experiments).

759

fiLindauer, Hoos, Hutter, & Schaub

Scenario
ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

#I
1294
2024
876
527
4021
1368
296
300
600
1614
767
1167
1362

#U #A #f #fg
82
253
129
0
428
314
77
47
108
20
229
209
322

11
2
6
4
22
5
15
18
9
31
31
31
31

138
86
37
16
198
46
115
115
115
115
115
115
115

4
1
1
1
4
1
10
10
10
10
10
10
10

tc
600
5000
2100
3600
3600
3600
5000
5000
5000
1200
1200
1200
1200

Reference
(Hoos et al., 2014)
(Gent et al., 2010)
(Malitsky et al., 2013)
(Tierney & Malitsky, 2015)
(Hurley et al., 2014)
(Pulina & Tacchella, 2009)
(Xu et al., 2008)
(Xu et al., 2008)
(Xu et al., 2008)
(Xu et al., 2012b)
(Xu et al., 2012b)
(Xu et al., 2012b)
(Xu et al., 2012b)

Table 1: Overview algorithm selection scenarios Algorithm Selection Library,
showing number instances #I, number unsolvable instances #U (U I), number
algorithms #A, number features #f , number feature groups #fg , cutoff time tc
literature reference.

4.1 Algorithm Configuration Setup
Following standard practice (Hutter et al., 2009), performed multiple (in case, 12)
independent runs algorithm configurator SMAC scenario selected
configuration claspfolio 2 best performance training data. configurator run allocated total time budget 2 CPU days. single run claspfolio 2
limited 1 CPU hour, using runsolver tool (Roussel, 2011). performance
metric, used penalized average runtime penalty factor 10 (PAR10), counts
timeout 10 times given runtime cutoff (runtime cutoffs differ ASlib
scenarios). study optimization PAR10 influenced metrics,
number timeouts. time required evaluate single configuration claspfolio 2 varied 10 CPU seconds 1 CPU hour reference machine (see
below), mostly depending difficulty optimizing pre-solving schedules.
obtain robust estimate AutoFolios performance, used 10-fold outer crossvalidation given specific ASlib scenarios, is, configured claspfolio 2 ten
times scenario (with different training-test splits). Therefore, total, performed
12 10 = 120 configuration runs 2 CPU days three different configuration spaces
(see Section 4.2) thirteen ASlib benchmarks, requiring total 9 360
CPU days (25 CPU years). note although thorough evaluation AutoFolio
required substantial amounts computation, applying single benchmark set
given training-test split would require 12 independent configuration runs two
days could thus performed weekend modern desktop machine.
Furthermore, applying AutoFolio new algorithm selection benchmark set cheap
comparison collecting data new benchmark set. instance, collect
760

fiAutoFolio: Automatically Configured Algorithm Selector

AutoFoliovote
AutoFolio
AutoFolioext

categorical

integer

real

conditionals

configurations

18 28
28 38
47 247

7
15
15

3
15
15

14
44
44

1 106 6 108
3 1011 2 1014
2 1017 2 1077

Table 2: Overview configuration spaces number categorical, integer-valued
real-valued parameters, number conditionals, estimation number
configurations ignoring real-valued parameters. number categorical values
varies scenarios depending number algorithms, features feature
groups.
algorithm performance data ASlib scenarios required 25.7 CPU days
(ASP-POTASSCO) 596.7 CPU days (PROTEUS-2014), average 212.3
CPU days (9 times much configuration budget AutoFolio).
performed experiments bwUniCluster Karlsruhe, whose machines
equipped two Octa-Core Intel Xeon E5-2670 (2.6 GHz, 20 MB cache) CPUs
64 GB RAM each, running Hat Enterprise Linux 6.4. note, however, runtimes
selected algorithms feature computations part ASlib scenarios
depend hardware used.
4.2 Different Configuration Spaces
mentioned earlier, AutoFolio used optimize performance single approach algorithm selectors, SATzilla, multi-approach selectors, LLAMA
claspfolio 2, much larger configuration spaces (see Figure 6). Therefore, studied three different parameter spaces AutoFolio based claspfolio 2:
AutoFolio considers configuration space described Section 3.2 additionally
adds binary parameters enable disable feature groups4 defined
specific algorithm selection scenario. Algorithm subset selection done using
heuristic based marginal contribution algorithm oracle performance;
AutoFoliovote considers subset configuration space AutoFolio,
fixes algorithm selection approach pairwise classification voting scheme;
AutoFolioext considers configuration space AutoFolio, instead parameters feature group, added binary parameters instance feature
selectable algorithm. increases number parameters substantially example, adds 220 additional parameters PROTEUS-2014
scenario.

4. selected feature groups result empty feature set, claspfolio 2 statically select single
best algorithm training data.

761

fiLindauer, Hoos, Hutter, & Schaub

Scenario
ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

Default
PAR10 #TOs
124.8
384.7
264.0
2513.8
3274.2
1068.4
7093.2
7851.2
3684.0
2087.0
2081.2
1019.8
708.2

19
10
7
33
321
26
29
37
34
261
86
69
52

AutoFoliovote
PAR10
#TOs
119.8
329.7
135.7
1953.6
1274.0
866.6
5781.4
6616.5
1441.9
890.4
1079.5
682.9
391.6

18
8
3
24
110
19
23
31
12
102
43
44
29

AutoFolio
PAR10
#TOs
125.0
355.1
246.3
2005.1
1379.2
910.2
5552.8
5932.3
967.4
979.1
1212.3
774.6
440.8

19
9
7
25
117
21
22
27
7
115
49
52
33

AutoFolioext
PAR10
#TOs
152.7
358.1
268.2
1922.5
3102.7
946.9
8085.8
7671.3
1301.7
1077.0
1285.5
990.7
543.1

25
9
8
24
280
22
33
36
10
126
52
67
41

Table 3: Comparing different configuration spaces AutoFolio based test performance. best performance shown bold face; indicate performance significantly better default configuration claspfolio 2 significance levels
= 0.05 = 0.1, respectively, according one-sided permutation test 100 000
permutations. Performances values that, according permutation test, significantly worse (at = 0.05) best performance given scenario marked
.

fixed selection approach AutoFoliovote pairwise classification
voting scheme, since SATzilla11-like promising single approach experiments (see, e.g., Figure 1). hand, extended configuration space,
AutoFolioext , obtained adding algorithm subset selection feature selection
configuration task. Feature selection well known improve many machine learning
models, often small subset instance features necessary predict runtime
algorithms (Hutter, Hoos, & Leyton-Brown, 2013).
note configuration AutoFoliovote found AutoFolio,
configuration AutoFolio part AutoFolioext , is, AutoFoliovote
AutoFolio AutoFolioext . Table 2 gives overview configuration space sizes.
4.3 Analysis Configuration Process
Table 3, compare performance default configuration claspfolio 2
(namely, SATzilla11-like) configurations optimized AutoFoliovote ,
AutoFolio AutoFolioext . selection scenarios, AutoFoliovote improved
performance test data comparison default configuration claspfolio 2.
AutoFolio improved one scenario AutoFolioext three scenarios.
Performance improvements test data statistically significant = 0.1 = 0.05
ten seven scenarios AutoFoliovote , nine seven AutoFolio,
five four AutoFolioext , respectively, according one-sided permutation test
100 000 permutations.
762

fiAutoFolio: Automatically Configured Algorithm Selector

11 13 ASlib scenarios, configuration least one configuration
spaces considered led statistically significant improvements ( = 0.1); discuss
remaining two scenarios, ASP-POTASSCO CSP-2010. ASP-POTASSCO,
performance improved substantially training data (AutoFolio reduced PAR10
score 30%), transfer test data (with none differences
test performances statistically significant). note default configuration
claspfolio 2 manually optimized scenario (Hoos et al., 2014),
AutoFolio found similar configurations similar performance. CSP2010, AutoFolio variants improved default, insignificantly so.
note hard improve performance substantially benchmark,
contains two algorithms.
PREMARSHALLING, AutoFolio solved 8 additional problem instances reduced PAR10 25%; nevertheless, performance difference weakly
significant (at = 0.1). due strong constraints pre-solving schedule
default configuration claspfolio 2 (at 3 solvers 256 seconds).
extensive pre-solving schedules decreased number timeouts PREMARSHALLING, introduced overhead many instances
scenario, making harder AutoFolio achieve significant performance improvements. scatter plot Figure 7a shows AutoFolio produced fewer timeouts
default claspfolio 2, AutoFolio required higher runtime instances (points diagonal). Similarly, AutoFolio solved lot instances
PROTEUS-2014 QBF-2011, AutoFolio higher runtime
instances (see Figure 7c 7b). However, number timeouts improved much PROTEUS-2014 (from 321 117) performance improvement
statistically significant here. Finally, SAT12-ALL example clear-cut
case: AutoFolio improved performance claspfolio 2 instances
substantially reduced number timeouts (see Figure 7d).
Overall, AutoFoliovote performed best experiments, followed AutoFolio,
distance, AutoFolioext . respect statistical significance, AutoFoliovote AutoFolio performed quite similarly, former better three times
latter better once. Based results, suspect added flexibility
AutoFolio compared AutoFoliovote pays configuration budget
large enough evaluate enough configurations effectively search larger space.
case three SAT11 scenarios, AutoFolio reached best
performance: scenarios contain relatively problem instances, making
evaluation claspfolio 2 quite fast allowing SMAC evaluate 40 000 configurations within 2 days. contrast, evaluation configuration largest ASlib
scenario, PROTEUS-2014, cost hour, SMAC evaluated 600
configurations, enough explore design space AutoFolio; accordingly, performance AutoFolioext PROTEUS-2014 improved slightly
comparison default configuration, AutoFoliovote made progress faster
performed statistically significantly better AutoFolio. Therefore, believe
AutoFolio good choice evaluate many configurations,
scenario small large configuration budget available. hand,
763

fiLindauer, Hoos, Hutter, & Schaub

100x

10x

2x

100x

10x

2x

2x

2x

1000

1000
10x

10x

100

100
100x

Configured

Configured

100x

10

10

1

1

0.1

0.1

0.01
0.01

0.1

1

10
Default

100

0.01
0.01

1000

0.1

1

10
Default

100

1000

(a) PREMARSHALLING. Number timeouts (b) QBF-2011. Number timeouts reduced
reduced 33 (default) 25 (configured).
26 (default) 21 (configured).
100x

10x

2x

100x

1000

2x

10x

2x
2x

1000
10x

100

10x

10

100x

100

Configured

Configured

100x

10

1

1

0.1

0.1
0.01
0.01

0.1

1

10
Default

100

0.01
0.01

1000

0.1

1

10
Default

100

1000

(c) PROTEUS-2014. Number timeouts
(d) SAT12-ALL. Number timeouts reduced
reduced 321 (default) 117 (configured). 261 (default) 115 (configured).

Figure 7: Scatter plots comparing per-instance performance default claspfolio 2
(SATzilla11-like) AutoFolio. Left: PREMARSHALLING, AutoFolio improved penalized average runtime (PAR10) reducing number timeouts,
cost increased runtimes many instances. Right: SAT12-ALL, AutoFolio
improved performance instances reduced number timeouts.

AutoFoliovote used larger scenarios configuration budget
quite small.
Figure 8 shows progress configuration process terms training performance
function time SAT11-HAND PROTEUS-2014, scenarios
764

fiAutoFolio: Automatically Configured Algorithm Selector

8000
3000
Performance

PAR10

7000
6000
5000

Autofolio
Autofolio_ext
Autofolio_vote

4000
29

210

211

212

213 214
Time (s)

2500
2000

Autofolio
Autofolio_ext
Autofolio_vote

1500
1000
215

216

217

29

210

(a) SAT11-HAND

211

212

213 214
Time (s)

215

216

217

(b) PROTEUS-2014

Figure 8: training PAR10 performance best configuration time. line
shows median 10 folds outer cross-validation filled area indicates
performance 25 75-quantile.

fewest configuration evaluations performed fixed configuration budget.
scenarios, large configuration space AutoFolioext resulted period
stagnation performance improved. PROTEUS-2014, performance started
improve near end configuration budget. contrast, AutoFolio
AutoFoliovote performed quite similarly scenarios, AutoFoliovote
somewhat faster make progress (note logarithmic time axis). Surprisingly us,
different selection approaches chosen AutoFolio AutoFoliovote .
restricted configuration space, AutoFoliovote choose pairwise classification
voting scheme, AutoFolio used approaches outer folds
scenarios: regression (2 times two scenarios), clustering (1 3 times,
resp.) SNNAP (3 4 times, resp.).
Figure 8, estimate influence configuration budget
performance final algorithm selector. example, halve configuration time
budget 1 day, penalized average runtime training set increases
8%.
4.4 Choices Lead Good Performance?
analyze choices important AutoFolio, applied two complementary methods assessing parameter importance algorithm configuration spaces:
functional ANOVA (Hutter, Hoos, & Leyton-Brown, 2014, 2015b) global measure
parameter importance ablation analysis (Fawcett & Hoos, 2015b, 2015a) local
measure. high-level overview parameters AutoFolio, refer back
Section 3.2; full details, including default values ranges parameters, given
online appendix available www.ml4aad.org/autofolio.
765

fiLindauer, Hoos, Hutter, & Schaub

4.4.1 Functional ANOVA (fANOVA)
Functional ANOVA (fANOVA, see, e.g., Sobol, 1993) general method partitioning
variance function components corresponding subsets arguments. Hutter
et al. (2014) demonstrated technique applied efficiently quantify
importance algorithms parameters. approach re-use performance data
collected configuration process purpose (without requiring new algorithm
executions) therefore computationally efficient (in experiments, required
minutes). overall approach fit empirical performance model (Hutter, Xu, Hoos,
& Leyton-Brown, 2014) : C R measured performance data,
used predict performance arbitrary configurations instances, study
parameter importance model. fitting model, fANOVA marginalizes
across problem instances:
1 X
f(c) =

m(c, i).
(4)
|I|
iI

computes variance function f across entire configuration space C
partitions variance additive components due subset algorithms
parameters. particular interest unary subsets, often explain substantial part
variance tend easiest interpret. important note fANOVA
partitions variance f entire configuration space. provides
global measure parameter importance, takes account many poorly-performing
configurations.
use fANOVA context study, ASlib scenario, merged
performance data 12 independent SMAC runs removed data points reported timeout5 resulted empty feature set. latter,
case claspfolio 2 statically selects single best solver, causing parameters
become unimportant performance claspfolio 2.
brevity, report results scenario SAT12-ALL. Table 4 shows ten
important parameters AutoFolio AutoFolioext scenario.
configuration spaces, maximal time spent compute instance features (max-featuretime) turned important parameter. parameter important,
setting small result features (or even none, disabling
selection mechanism) setting large lead increased overhead feature
computation (see Figure 9).
second important parameter AutoFolio marginal contribution
filtering heuristic algorithm subset selection. Algorithm subset selection especially important scenarios based SAT solving, include many SAT
solvers performance solvers often highly correlated (Xu et al.,
2012a). AutoFolioext , contribution filtering heuristic less important,
configuration space includes binary parameters individual algorithm, allowing
configurator (here SMAC) directly perform subset selection. context, including mphaseSATm marchrw special importance. solver mphaseSATm
single best algorithm SAT12-ALL one highest marginal contributions
5. observed timeouts particular configuration larger data sets: clustering approach
spectral clustering.

766

fiAutoFolio: Automatically Configured Algorithm Selector

Parameter

Main Effect

max-feature-time
contr-filter
approach
feature-step:CG
pre-solving
impute
perf:transformation
time-pre-solving
feature:normalization
pre-solving:max-solver

Parameter

23.43% 2.05
6.82% 2.30
6.39% 0.63
0.76% 0.09
0.69% 0.09
0.29% 0.06
0.26% 0.05
0.22% 0.06
0.06% 0.01
0.05% 0.01

Main Effect

max-feature-time
approach
pre-solving
contr-filter
algorithms:mphaseSATm
imputation
F:algorithms:marchrw
time-pre-solving
pre-solving:sec mode
perf:transformation

(a) AutoFolio

11.07% 5.32
5.90% 4.40
1.29% 1.61
0.80% 0.92
0.72% 0.22
0.69% 0.27
0.30% 0.18
0.23% 0.41
0.11% 0.24
0.11% 0.04

(b) AutoFolioext

Table 4: Average main effects ( stdev) outer cross-validation splits ten
important claspfolio 2 parameters SAT12-ALL according fANOVA.

Marginal PAR10 Scores

5851

4475

30920

100 200 300 400 500 600
max-feat-time [sec]

Figure 9: Marginal performance predictions parameter max-feature-time data
one outer fold configuration space AutoFolio. blue line indicates mean
predicted marginal performance red area standard deviation.
oracle. Similarly, marchrw high marginal contribution algorithm
whose performance highly correlated another solver (see exploratory
data analysis Bischl et al., 2015b).
note analysis determines global parameter importance
respect entire parameter space. example, importance maximal feature
computation time mostly high, crucial change improve
performance claspfolio 2, configuration space contains settings
drastically worsen performance. gain complementary insights parameters changed improve performance, next performed ablation analysis.6
6. note fANOVA used yield local analysis parameter importance partitioning variance performance high-performance regions given configuration space (Hutter

767

fi2000

2200

1800

2000

1600

1800

1400

1600
PAR10

PAR10

Lindauer, Hoos, Hutter, & Schaub

1200
1000

1400
1200

800

1000

600

800

400
ute ime lter lize :sp ans opt :CG eaf aps sic res jois
impture-t ontr-finorma -stepsnce_trspeed--steps ples_pl s:ls_s eps:Ba_featups:lob

c
ure ature n_same-ste ure-st f-max re-ste
-fe
featperfor
fe f-mi atur feat ing:r eatu
max
vot f
ng:r fe



v

600

e e e p f r c
s:CG put tim -op aliz ran joi sap s:s lea ure filte asi
tep im ature- speed normance_teps:lobps:ls_re-stepmples_x_featcontr- teps:B

e
te
-s

ur
-fe
form re-s e-s atu n_s f-m eature
feat
max
per featu featur fge:rf-mioting:r
f
v
n
voti

(a) 2nd outer fold

(b) 7th outer fold

Figure 10: Ablation paths two outer-folds SAT12-ALL. (a), important
parameter impute feature-step:CG smaller effect. (b), feature-step:CG
important parameter impute effect performance.

4.4.2 Ablation Analysis
Ablation analysis provides answer question changes parameter values
one configuration another caused biggest improvement performance?.
iteratively changing parameter value largest impact performance
path two given configurations, e.g., default configuration algorithm
optimized configuration. Unlike fANOVA, ablation analysis attempt
summarize parameter importance across entire configuration space, focusses locally
paths configurations interest. results obtained ablation analysis
therefore complementary fANOVA. Unfortunately, ablation costly,
since requires new algorithm runs assess performance configurations path
two given configurations. AutoFolio experiments SAT12-ALL,
allocated time budget 6 days maximum wall-clock time permitted jobs
cluster ablation 10 outer cross-validation folds, within
budget, obtained results 6 those.
ablation results indicate feature-step:CG Boolean parameter enables
disables computation clause graph features single important parameter
change claspfolio 2s default. default, feature-step:CG activated,
turns clause graph features often expensive compute within
time allow feature computation. Therefore, indeed good decision
configuration procedure deactivate optional feature computation step. According
ablation results, done 5 6 outer cross-validation folds and, average,
5 folds, responsible 99% performance improvements achieved
et al., 2014); here, this, since used ablation analysis study parameter importance
locally.

768

fiAutoFolio: Automatically Configured Algorithm Selector

configuration (standard deviation 37%7 ). contrast, seen fANOVA results,
feature-step:CG quite unimportant globally, main effect 0.76%. second
important parameter change activation feature imputation (impute);
average, responsible 39% overall performance improvement (standard
deviation 56%) made 6 outer cross-validation folds analyzed.8 However,
impute effect performance feature-step:CG deactivated
impute changed ablation path. case 2 6 ablation
paths (e.g., see Figure 10a) hence, impute impact performance 4
paths (e.g., see Figure 10b). two parameters dependent effects, since imputation
particularly important clause graph features computed: features time
many large instances thus require imputation.
globally important parameter, according fANOVA, max-feature-time,
found rather unimportant change default value. parameter
changed default optimized configuration outer folds SAT12-ALL,
since default value already good average 2% overall performance improvement could attributed change. note along
Ablation path, max-feature-time never flipped value resulted worse performance default configuration, many poorly-performing values
exist explain globally high importance parameter.
4.5 Comparison Algorithm Selectors
Table 5, compare AutoFolio SATzilla159 (Xu et al., 2011), SNNAP (version 1.4; Collautti et al., 2013) ISAC (implementation SNNAP 1.4; Kadioglu
et al., 2010).10 note ISAC SNNAP pure algorithm selectors, whereas
SATzilla15 claspfolio 2 additionally use pre-solver schedules. Furthermore,
added nave approach, RandSel, simulating uninformed user selects uniformly random SNNAP, ISAC SATzilla15. Overall, AutoFolio performed best 7 13 scenarios statistically indistinguishable
best system scenarios, according one-sided permutation test 100 000
permutations significance level = 0.05. Therefore, AutoFolio system
achieves state-of-the-art performance scenarios.
SATzilla15 performed second best, yielded statistically significantly worse performance AutoFolio 5 13 scenarios. Even though statistically significant,
SATzilla15 performed slightly better AutoFolio 5 scenarios. reason
7. large standard deviation arises fact folds, parameter change actually
responsible 100% performance difference: folds, change alone would
sufficed achieve better performance optimized configuration.
8. sum relative performance subset parameter improvements limited 100%, since
computed relative difference default optimized configuration. 5
6 ablation paths, parameter changes lead better performance final optimized
configuration, parameter changed worsened performance again.
9. Alexandre Frechette, current main developer SATzilla, provided internal new implementation
SATzilla (version 0.9.1b-count-cheap-feat-12) longer limited SAT.
10. state-of-the-art selectors, 3S (Kadioglu et al., 2011) CSHC (Malitsky et al., 2013a),
publicly available training procedures, therefore unable train
scenarios.

769

fiLindauer, Hoos, Hutter, & Schaub

ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

Oracle

SB

SNNAP

ISAC

21.3
107.7
40.7
227.6
26.3
95.9
478.3
419.9
227.3
93.7
113.2
88.1
46.9

534.1
1087.4
2111.6
7002.9
10756.3
9172.3
17815.8
8985.6
14938.6
2967.9
3944.2
1360.6
568.5

203.8
1087.5
895
9042.1
4058.7
7386.2
9209.3
6632.6
4859
1427.5
2180.5
789
593.1

291.9
1027
786.4
5880.8
3328
3813.5
13946.2
8461.2
3140.4
2989.3
4110.8
1409.5
434.5

SATzilla15 RandSel AutoFolio
170
276
166.8
3179.1
2050.3
1245.2
6211.5
8048.8
877.5
876.9
1031.5
839.7
485.3

221.6
796.8
615.6
6034
3145.6
4148.3
9789
7714.2
2958.9
1764.5
2440.9
1012.7
504.3

125
355
246.3
2005.1
1379.2
910
5552.7
5932.3
967
979
1212
774.6
440

Table 5:
Performance comparison AutoFolio, SNNAP, ISAC,
SATzilla15, well single best solver (SB, selected based PAR10 training
set) baseline, oracle (also known virtual best solver) bound optimal performance algorithm selector. show PAR10 scores averaged 10 outer
cross-validation folds, instances solved solver removed test set
avoid artificially inflating PAR10-scores. RandSel column shows expected
performance picking uniformly random one SNNAP, ISAC SATzilla15.
best performance shown bold face. performance values statistically significantly better best-performing system given scenario, according
one-sided permutation test 100 000 permutations significance level = 0.05,
marked .
might SATzilla15 performs extensive search determine best combination pre-solving schedule (grid search), algorithm subset (iterated local search)
trained selection model.
note that, order compensate 24 CPU days spent find well-performing
configuration AutoFolio, compared simply using single best solver, average
across scenarios AutoFolio would consecutively solve instances 42 CPU
days (standard deviation 23), less two times configuration budget.
Although AutoFolio improved substantially single best solver (SB)
scenarios (up speedup factor 15.4 SAT11-RAND), still gap
Oracle performance (also known virtual best solver SAT community).
gap could closed least two ways: (i) using larger configuration budget
AutoFolio, (ii) developing better instance features, basis
algorithm selection methods.

5. Conclusions
presented AutoFolio best knowledge, first approach automatically configuring algorithm selectors. Using concrete realization approach based
highly parameterized algorithm selection framework claspfolio 2, showed
using state-of-the-art algorithm configurators, algorithm selectors customized
770

fiAutoFolio: Automatically Configured Algorithm Selector

robustly achieve peak performance across range algorithm selection scenarios.
resulting approach performs significantly (and sometimes substantially) better manually configured selectors applied out-of-the-box previously unseen algorithm
selection scenarios.
comprehensive experiments 13 algorithm selection scenarios different
domains (SAT, Max-SAT, CSP, ASP, QBF, container pre-marshalling) make
algorithm selection library ASlib, concrete realization AutoFolio outperformed
best single solver selection benchmark factors 1.3 15.4 (geometric
average: 3.9) terms PAR10 scores. Overall, AutoFolio established improved stateof-the-art performance 7 13 scenarios performed par previous
state-of-the-art approaches scenarios; overall, clearly yielded robust
performance across diverse set benchmarks.
studied effect different configuration spaces. Here, showed
medium-size configuration space AutoFolio lead state-of-the-art performance
configuration budget allows evaluation sufficiently many configurations.
contrast, selection scenario large (in terms number algorithms problem
instances), configuration budget limited, configuration constrained
space, used AutoFoliovote , typically leads better performance.
performance AutoFoliovote independently verified ICON Challenge
Algorithm Selection (Kotthoff, 2015), evaluated 8 different systems
small configuration budget 12 CPU hours respect three metrics: PAR10 score,
number instances solved misclassification penalty. throughout paper,
metric optimized AutoFolio PAR10 score, AutoFolio ranked first
respect metric. ranked first respect number instances solved
second respect misclassification penalty (leading overall second place).
future work, plan investigate potential gains larger configuration
spaces (including feature algorithm subset selection) used effectively.
end, would (i) study performance larger configuration budgets
allow configurator assess configurations; (ii) evaluate algorithm configurators, irace (Lopez-Ibanez et al., 2011) GGA (Ansotegui et al., 2009); (iii)
extend configuration space AutoFolio implementing algorithm selection
approaches (e.g., CSHC; Malitsky et al., 2013a); (iv) shrink larger configuration space
based analysis parameter importance fANOVA (Hutter et al., 2014)
Ablation (Fawcett & Hoos, 2015b), allowing configurator focus important parameters; (v) automatically select pre-configured algorithm selectors,
based features given algorithm selection scenario, improve performance
starting automatic configuration configurations thus selected (Feurer, Springenberg, & Hutter, 2015). Another promising avenue reducing computational cost
approach would pre-select algorithms, features, problem instances based
techniques proposed Hoos et al. (2013) based collaborative filtering approach
Misir Sebag (2013). Finally, plan investigate extent AutoFolio
configure algorithm selection systems selecting parallel portfolios (Lindauer et al.,
2015a) exploit increasing availability parallel computing resources.
Overall, believe automated configuration algorithm selection systems improves performance versatility systems across broad range application
771

fiLindauer, Hoos, Hutter, & Schaub

domains. AutoFolio approach facilitates future improvements, making easier realize assess performance potential inherent new design choices
various components algorithm selection system. open-source implementation
AutoFolio available www.ml4aad.org/autofolio/.

Acknowledgements
M. Lindauer supported DFG (German Research Foundation) Emmy
Noether grant HU 1900/2-1 project SCHA 550/8-3, H. Hoos NSERC Discovery
Grant, F. Hutter DFG Emmy Noether grant HU 1900/2-1 T. Schaub
DFG project SCHA 550/8-3, respectively. work performed
computational resource bwUniCluster funded Ministry Science, Research Arts
universities State Baden-Wurttemberg, Germany, within framework
program bwHPC.

References
Abrame, A., & Habet, D. (2014). extension learning Max-SAT. Endriss, U.,
& Leite, J. (Eds.), Proceedings 7th European Starting AI Researcher Symposium
(STAIRS14), Vol. 264 Frontiers Artificial Intelligence Applications, pp. 1
10. IOS Press.
Amadini, R., Gabbrielli, M., & Mauro, J. (2014). SUNNY: lazy portfolio approach
constraint solving. Theory Practice Logic Programming, 14 (4-5), 509524.
Ansotegui, C., Malitsky, Y., & Sellmann, M. (2014). Maxsat improved instance-specific
algorithm configuration. Brodley, C., & Stone, P. (Eds.), Proceedings Twentyeighth National Conference Artificial Intelligence (AAAI14), pp. 25942600. AAAI
Press.
Ansotegui, C., Sellmann, M., & Tierney, K. (2009). gender-based genetic algorithm
automatic configuration algorithms. Gent, I. (Ed.), Proceedings
Fifteenth International Conference Principles Practice Constraint Programming (CP09), Vol. 5732 Lecture Notes Computer Science, pp. 142157. SpringerVerlag.
Bergstra, J., Bardenet, R., Bengio, Y., & Kegl, B. (2011). Algorithms hyper-parameter
optimization. Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., & Weinberger,
K. (Eds.), Proceedings 25th International Conference Advances Neural
Information Processing Systems (NIPS11), pp. 25462554.
Biere, A. (2013). Lingeling, plingeling treengeling entering sat competition 2013.
Balint, A., Belov, A., Heule, M., & Jarvisalo, M. (Eds.), Proceedings SAT Competition 2013: Solver Benchmark Descriptions, Vol. B-2013-1 Department
Computer Science Series Publications B, pp. 5152. University Helsinki.
Bischl, B., Kerschke, P., Kotthoff, L., Lindauer, M., Malitsky, Y., Frechette, A., Hoos, H.,
Hutter, F., Leyton-Brown, K., Tierney, K., & Vanschoren, J. (2015a). www.aslib.net.
772

fiAutoFolio: Automatically Configured Algorithm Selector

Bischl, B., Kerschke, P., Kotthoff, L., Lindauer, M., Malitsky, Y., Frechette, A., Hoos,
H., Hutter, F., Leyton-Brown, K., Tierney, K., & Vanschoren, J. (2015b). Aslib:
benchmark library algorithm selection. Computing Research Repository (CoRR),
abs/1506.02465.
Chiarandini, M., Fawcett, C., & Hoos, H. (2008). modular multiphase heuristic solver
post enrolment course timetabling. Proceedings Seventh International
Conference Practice Theorysy Automated Timetabling (PATAT08, pp.
18.
Collautti, M., Malitsky, Y., Mehta, D., & OSullivan, B. (2013). SNNAP: Solver-based
nearest neighbor algorithm portfolios. Blockeel, H., Kersting, K., Nijssen,
S., & Zelezny, F. (Eds.), Machine Learning Knowledge Discovery Databases
(ECML/PKDD13), Vol. 8190 Lecture Notes Computer Science, pp. 435450.
Springer-Verlag.
Dickerson, J., & Sandholm, T. (2015). Futurematch: Combining human value judgments
machine learning match dynamic environments. Bonet, B., & Koenig,
S. (Eds.), Proceedings Twenty-nineth National Conference Artificial Intelligence (AAAI15), pp. 622628. AAAI Press.
Eggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek, J., Hoos, H., & Leyton-Brown,
K. (2013). Towards empirical foundation assessing Bayesian optimization
hyperparameters. NIPS Workshop Bayesian Optimization Theory Practice.
Fawcett, C., & Hoos, H. (2015a). www.cs.ubc.ca/labs/beta/Projects/Ablation/.
Fawcett, C., & Hoos, H. (2015b). Analysing differences algorithm configurations
ablation. Journal Heuristics, 128.
Feurer, M., Springenberg, T., & Hutter, F. (2015). Initializing Bayesian hyperparameter
optimization via meta-learning. Bonet, B., & Koenig, S. (Eds.), Proceedings
Twenty-nineth National Conference Artificial Intelligence (AAAI15), pp. 1128
1135. AAAI Press.
Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Schneider, M.
(2011a). Potassco: Potsdam answer set solving collection. AI Communications,
24 (2), 107124.
Gebser, M., Kaminski, R., Kaufmann, B., Schaub, T., Schneider, M., & Ziller, S. (2011b).
portfolio solver answer set programming: Preliminary report. Delgrande, J.,
& Faber, W. (Eds.), Proceedings Eleventh International Conference Logic
Programming Nonmonotonic Reasoning (LPNMR11), Vol. 6645 Lecture Notes
Computer Science, pp. 352357. Springer-Verlag.
Gebser, M., Kaufmann, B., & Schaub, T. (2012). Conflict-driven answer set solving:
theory practice. Artificial Intelligence, 187-188, 5289.
Gent, I., Jefferson, C., Kotthoff, L., Miguel, I., Moore, N., Nightingale, P., & Petrie, K.
(2010). Learning use lazy learning constraint solving. Coelho, H., Studer,
R., & Wooldridge, M. (Eds.), Proceedings Nineteenth European Conference
Artificial Intelligence (ECAI10), pp. 873878. IOS Press.
773

fiLindauer, Hoos, Hutter, & Schaub

Gomes, C., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126 (1-2),
4362.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009).
WEKA data mining software: update. SIGKDD Explorations, 11 (1), 1018.
Hoos, H., Kaminski, R., Lindauer, M., & Schaub, T. (2015). aspeed: Solver scheduling via
answer set programming. Theory Practice Logic Programming, 15, 117142.
Hoos, H., Kaufmann, B., Schaub, T., & Schneider, M. (2013). Robust benchmark set selection boolean constraint solvers. Pardalos, P., & Nicosia, G. (Eds.), Proceedings
Seventh International Conference Learning Intelligent Optimization
(LION13), Vol. 7997 Lecture Notes Computer Science, pp. 138152. SpringerVerlag.
Hoos, H., Lindauer, M., & Schaub, T. (2014). claspfolio 2: Advances algorithm selection
answer set programming. Theory Practice Logic Programming, 14, 569585.
Huberman, B., Lukose, R., & Hogg, T. (1997). economic approach hard computational
problems. Science, 275, 5154.
Hurley, B., Kotthoff, L., Malitsky, Y., & OSullivan, B. (2014). Proteus: hierarchical
portfolio solvers transformations. Simonis, H. (Ed.), Proceedings
Eleventh International Conference Integration AI Techniques Constraint Programming (CPAIOR14), Vol. 8451 Lecture Notes Computer Science,
pp. 301317. Springer-Verlag.
Hutter, F., Babic, D., Hoos, H., & Hu, A. (2007). Boosting verification automatic tuning
decision procedures. OConner, L. (Ed.), Formal Methods Computer Aided
Design (FMCAD07), pp. 2734. IEEE Computer Society Press.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2010). Automated configuration mixed integer
programming solvers. Lodi, A., Milano, M., & Toth, P. (Eds.), Proceedings
Seventh International Conference Integration AI Techniques Constraint Programming (CPAIOR10), Vol. 6140 Lecture Notes Computer Science,
pp. 186202. Springer-Verlag.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2011). Sequential model-based optimization
general algorithm configuration. Coello, C. (Ed.), Proceedings Fifth
International Conference Learning Intelligent Optimization (LION11), Vol.
6683 Lecture Notes Computer Science, pp. 507523. Springer-Verlag.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2014). efficient approach assessing
hyperparameter importance. Xing, E., & Jebara, T. (Eds.), Proceedings 31th
International Conference Machine Learning, (ICML14), Vol. 32, pp. 754762.
Omnipress.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2015a). www.ml4aad.org/smac.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2015b). www.ml4aad.org/fanova.
Hutter, F., Hoos, H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic
algorithm configuration framework. Journal Artificial Intelligence Research, 36,
267306.
774

fiAutoFolio: Automatically Configured Algorithm Selector

Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). Identifying key algorithm parameters
instance features using forward selection. Pardalos, P., & Nicosia, G. (Eds.),
Proceedings Seventh International Conference Learning Intelligent Optimization (LION13), Vol. 7997 Lecture Notes Computer Science, pp. 364381.
Springer-Verlag.
Hutter, F., Lindauer, M., Balint, A., Bayless, S., Hoos, H., & Leyton-Brown, K. (2015).
Configurable SAT Solver Challenge (CSSC). Artificial Intelligence. review.
Hutter, F., Xu, L., Hoos, H., & Leyton-Brown, K. (2014). Algorithm runtime prediction:
Methods evaluation. Artificial Intelligence, 206, 79111.
Janota, M., Klieber, W., Marques-Silva, J., & Clarke, E. (2012). Solving QBF counterexample guided refinement. Cimatti, A., & Sebastiani, R. (Eds.), Proceedings
Fifteenth International Conference Theory Applications Satisfiability Testing (SAT12), Vol. 7317 Lecture Notes Computer Science, pp. 114128.
Springer-Verlag.
Kadioglu, S., Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2011). Algorithm selection scheduling. Lee, J. (Ed.), Proceedings Seventeenth International Conference Principles Practice Constraint Programming (CP11),
Vol. 6876 Lecture Notes Computer Science, pp. 454469. Springer-Verlag.
Kadioglu, S., Malitsky, Y., Sellmann, M., & Tierney, K. (2010). ISAC - instance-specific
algorithm configuration. Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings Nineteenth European Conference Artificial Intelligence (ECAI10),
pp. 751756. IOS Press.
Kotthoff, L. (2013). LLAMA: leveraging learning automatically manage algorithms.
Computing Research Repository (CoRR), abs/1306.1031.
Kotthoff, L. (2014). Algorithm selection combinatorial search problems: survey. AI
Magazine, 4860.
Kotthoff, L. (2015). ICON Challenge Algorithm Selection..
icon-fet.eu/challengeas.

http://challenge.

Kotthoff, L., Gent, I., & Miguel, I. (2012). evaluation machine learning algorithm
selection search problems. AI Communications, 25 (3), 257270.
Lim, B., van den Briel, M., Thiebaux, S., Backhaus, S., & Bent, R. (2015). HVAC-Aware
Occupancy Scheduling. Bonet, B., & Koenig, S. (Eds.), Proceedings Twentynineth National Conference Artificial Intelligence (AAAI15), pp. 679686. AAAI
Press.
Lindauer, M., Hoos, H., & Hutter, F. (2015a). sequential algorithm selection parallel
portfolio selection. Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Proceedings Nineth International Conference Learning Intelligent Optimization
(LION15), Lecture Notes Computer Science, pp. 116. Springer-Verlag.
Lindauer, M., Hoos, H., Hutter, F., & Schaub, T. (2015b). Autofolio: Algorithm configuration algorithm selection. Proceedings Workshops Twenty-nineth
National Conference Artificial Intelligence (AAAI15).
775

fiLindauer, Hoos, Hutter, & Schaub

Lindauer, M., Hoos, H., & Schaub, T. (2015c). www.cs.uni-potsdam.de/claspfolio/.
Lopez-Ibanez, M., Dubois-Lacoste, J., Stutzle, T., & Birattari, M. (2011). irace package,
iterated race automatic algorithm configuration. Tech. rep., IRIDIA, Universite
Libre de Bruxelles, Belgium.
Lopez-Ibanez, M., & Stutzle, T. (2010). Automatic configuration multi-objective ACO
algorithms. Dorigo, M., M-Birattari, Caro, G. D., Doursat, R., Engelbrecht,
A. P., Floreano, D., Gambardella, L., Gro, R., Sahin, E., Sayama, H., & Stutzle,
T. (Eds.), Proceedings Seventh International Conference Swarm Intelligence
(ANTS10), Lecture Notes Computer Science, pp. 95106. Springer-Verlag.
Malitsky, Y., Mehta, D., & OSullivan, B. (2013). Evolving instance specific algorithm
configuration. Helmert, M., & Roger, G. (Eds.), Proceedings Sixth Annual
Symposium Combinatorial Search (SOCS14). AAAI Press.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2012). Parallel SAT solver
selection scheduling. Milano, M. (Ed.), Proceedings Eighteenth International Conference Principles Practice Constraint Programming (CP12),
Vol. 7514 Lecture Notes Computer Science, pp. 512526. Springer-Verlag.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013a). Algorithm portfolios
based cost-sensitive hierarchical clustering. Rossi, F. (Ed.), Proceedings
23rd International Joint Conference Artificial Intelligence (IJCAI13), pp. 608
614.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013b). Boosting sequential solver portfolios: Knowledge sharing accuracy prediction. Pardalos, P.,
& Nicosia, G. (Eds.), Proceedings Seventh International Conference Learning Intelligent Optimization (LION13), Vol. 7997 Lecture Notes Computer
Science, pp. 153167. Springer-Verlag.
Maratea, M., Pulina, L., & Ricca, F. (2014). multi-engine approach answer-set programming. Theory Practice Logic Programming, 14, 841868.
Mascia, F., Lopez-Ibanez, M., Dubois-Lacoste, J., & Stutzle, T. (2014). Grammar-based
generation stochastic local search heuristics automatic algorithm configuration tools. Computers & OR, 51, 190199.
Misir, M., & Sebag, M. (2013). Algorithm selection collaborative filtering problem.
Tech. rep., INRIA & LRI, Universite Paris Sud XI.
OMahony, E., Hebrard, E., Holland, A., Nugent, C., & OSullivan, B. (2008). Using casebased reasoning algorithm portfolio constraint solving. Bridge, D., Brown,
K., OSullivan, B., & Sorensen, H. (Eds.), Proceedings Nineteenth Irish Conference Artificial Intelligence Cognitive Science (AICS08).
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,
D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning
Python. Journal Machine Learning Research, 12, 28252830.
Pulina, L., & Tacchella, A. (2009). self-adaptive multi-engine solver quantified boolean
formulas. Constraints, 14 (1), 80116.
776

fiAutoFolio: Automatically Configured Algorithm Selector

Rice, J. (1976). algorithm selection problem. Advances Computers, 15, 65118.
Roussel, O. (2011). Controlling solver execution runsolver tool. Journal
Satisfiability, Boolean Modeling Computation, 7, 139144.
Smith-Miles, K. (2008). Cross-disciplinary perspectives meta-learning algorithm
selection. ACM Computing Surveys, 41 (1).
Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization machine learning algorithms. Bartlett, P., Pereira, F., Burges, C., Bottou, L., &
Weinberger, K. (Eds.), Proceedings 26th International Conference Advances
Neural Information Processing Systems (NIPS12), pp. 29602968.
Sobol, I. (1993). Sensitivity estimates nonlinear mathematical models. Mathematical
Modeling Computational Experiment, 1 (4), 407414.
Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling finite linear CSP
SAT. Constraints, 14 (2), 254272.
Thornton, C., Hutter, F., Hoos, H., & Leyton-Brown, K. (2013). Auto-WEKA: combined
selection hyperparameter optimization classification algorithms. I.Dhillon,
Koren, Y., Ghani, R., Senator, T., Bradley, P., Parekh, R., He, J., Grossman, R., &
Uthurusamy, R. (Eds.), 19th ACM SIGKDD International Conference Knowledge Discovery Data Mining (KDD13), pp. 847855. ACM Press.
Tierney, K., & Malitsky, Y. (2015). algorithm selection benchmark container premarshalling problem. Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Proceedings Nineth International Conference Learning Intelligent Optimization
(LION15), Lecture Notes Computer Science, pp. 1722. Springer-Verlag.
Vallati, M., Fawcett, C., Gerevini, A., Hoos, H., & Saetti, A. (2013). Automatic generation
efficient domain-optimized planners generic parametrized planners. Helmert,
M., & Roger, G. (Eds.), Proceedings Sixth Annual Symposium Combinatorial
Search (SOCS14). AAAI Press.
Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically configuring algorithms portfolio-based selection. Fox, M., & Poole, D. (Eds.), Proceedings
Twenty-fourth National Conference Artificial Intelligence (AAAI10), pp. 210216.
AAAI Press.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithm selection SAT. Journal Artificial Intelligence Research, 32, 565606.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2011). Hydra-MIP: Automated algorithm configuration selection mixed integer programming. RCRA workshop
Experimental Evaluation Algorithms Solving Problems Combinatorial
Explosion International Joint Conference Artificial Intelligence (IJCAI).
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2012a). Evaluating component solver
contributions portfolio-based algorithm selectors. Cimatti, A., & Sebastiani,
R. (Eds.), Proceedings Fifteenth International Conference Theory Applications Satisfiability Testing (SAT12), Vol. 7317 Lecture Notes Computer
Science, pp. 228241. Springer-Verlag.
777

fiLindauer, Hoos, Hutter, & Schaub

Xu, L., Hutter, F., Shen, J., Hoos, H., & Leyton-Brown, K. (2012b). SATzilla2012: improved
algorithm selection based cost-sensitive classification models. Balint, A., Belov,
A., Diepold, D., Gerber, S., Jarvisalo, M., & Sinz, C. (Eds.), Proceedings SAT
Challenge 2012: Solver Benchmark Descriptions, Vol. B-2012-2 Department
Computer Science Series Publications B, pp. 5758. University Helsinki.
Yun, X., & Epstein, S. (2012). Learning algorithm portfolios parallel execution.
Hamadi, Y., & Schoenauer, M. (Eds.), Proceedings Sixth International Conference Learning Intelligent Optimization (LION12), Vol. 7219 Lecture Notes
Computer Science, pp. 323338. Springer-Verlag.

778


