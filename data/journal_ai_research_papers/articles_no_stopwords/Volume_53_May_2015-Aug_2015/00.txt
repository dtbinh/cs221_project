Journal Artificial Intelligence Research 53 (2015) 1-40

Submitted 08/14; published 05/15

Coactive Learning
Pannaga Shivaswamy

pshivaswamy@linkedin.com

LinkedIn Corporation
2029 Stierlin Ct
Mountain View, CA 94043, USA

Thorsten Joachims

tj@cs.cornell.edu

Department Computer Science
Cornell University
Ithaca, NY 14853, USA

Abstract
propose Coactive Learning model interaction learning system
human user, common goal providing results maximum utility
user. Interactions Coactive Learning model take following form:
step, system (e.g. search engine) receives context (e.g. query) predicts object
(e.g. ranking); user responds correcting system necessary, providing slightly
improved necessarily optimal object feedback. argue preference
feedback inferred large quantity observable user behavior (e.g., clicks
web search), unlike optimal feedback required expert model cardinal
valuations required bandit learning. Despite relaxed requirements feedback,
show possible adapt many existing online learning algorithms
coactive

framework. particular, provide algorithms achieve O(1/ ) average regret
terms cardinal utility, even though learning algorithm never observes cardinal utility
values directly. provide algorithm O(log(T )/T ) average regret
case -strongly convex loss functions. extensive empirical study demonstrates
applicability model algorithms movie recommendation task, well
ranking web search.

1. Introduction
wide range systems use today, interaction human system takes
following form. user issues command (e.g. query) receives possibly
structured result response (e.g. ranking). user interacts results (e.g.
clicks), thereby providing implicit feedback users utility function. three
examples systems typical interaction patterns:
Web Search: response query, search engine presents ranking [A, B, C, D, ...]
observes user clicks documents B D.
Movie Recommendation: online service recommends movie user. However,
user rents movie B browsing collection.
Machine Translation: online machine translator used translate wiki page
language B. system observes corrections user makes translated text.
c
2015
AI Access Foundation. rights reserved.

fiShivaswamy & Joachims

examples, user provides feedback results
system. However, feedback incremental improvement, necessarily
optimal result. example, clicks web search results infer
user would preferred ranking [B, D, A, C, ...] one presented. However,
unlikely best possible ranking. Similarly recommendation example,
movie B preferred movie A, may even better movies
user find browsing. machine translation example, corrected text need best possible translation language language B.
three examples, algorithm typically receives slightly improved result user
feedback, necessarily optimal prediction cardinal utilities. conjecture
many applications fall schema, ranging news filtering personal
robotics.
paper, propose Coactive Learning model system-user interactions.
formalize Coactive Learning general model interaction learning system
user, define suitable notion regret, validate key modeling assumption
namely whether observable user behavior provide valid feedback model
user study web search. new model viewed cooperative learning process
system user, parties aim optimize utility lack means
achieve goal own. Specifically, (boundedly rational) user computationally
limited maximizing utility space alternatives, system limited
well knows users utility function.
proposed online learning framework differs significantly existing online learning
models terms observed feedback (see related works section comparison).
strength proposed framework possible derive wide range coactive
learning algorithms adapting existing online algorithms convex optimization.
provide template Coactive Learning algorithms show several instances
template paper, case, prove worst case analysis
algorithm carries conventional online learning framework coactive
learning despite differences two models.In particular, cases linear
utility models convex cost functions show O(1/ ) regret bounds matching
lower bound. show regret bound improved second order
algorithm strongly convex functions. learning algorithms perform structured output
prediction (see Bakir, Hofmann, Scholkopf, Smola, Taskar, & Vishwanathan, 2007) thus
applied wide variety problems. study several interesting extensions
framework using batch updates, expected feedback, exponentiated learning
algorithm. Finally, provide extensive empirical evaluations algorithms movie
recommendation web search task, showing algorithms highly efficient
effective practical settings.
rest paper organized follows. discuss related work Section 2.
Section 3 formally introduce coactive learning model motivate model
real-world user study. present linear version algorithm along
several extensions Section 4. Section 5, detail general schema deriving
coactive learning algorithms regret bounds. particular, derive exponentiated gradient algorithm Section 5.1, propose coactive learning algorithms
minimizing general convex losses -strongly convex losses Sections 5.2 5.3.
2

fiCoactive Learning

empirical evaluation proposed framework algorithms done Section 6
conclude Section 7. include proofs Appendix.

2. Related Works
Coactive Learning Model bridges gap two forms feedback
well studied online learning. one side multi-armed bandit model (e.g.,
Auer, Cesa-Bianchi, Freund, & Schapire, 2002b; Auer, Cesa-Bianchi, & Fischer, 2002a),
algorithm chooses action observes utility (only) action.
side, utilities possible actions revealed case learning
expert advice (e.g., Cesa-Bianchi & Lugosi, 2006a). Online convex optimization (Zinkevich,
2003; Hazan, Agarwal, & Kale, 2007) online convex optimization bandit setting
(Flaxman, Kalai, & McMahan, 2005) continuous relaxations expert
bandit problems respectively. model, information two arms revealed
iteration (the one presented one receive feedback user),
sits expert bandit setting. closely related Coactive Learning
dueling bandits setting (Yue, Broder, Kleinberg, & Joachims, 2009; Yue & Joachims,
2009). key difference arms chosen algorithm dueling
bandits setting, whereas one arms chosen user Coactive Learning
setting. model allows contextual information contextual bandits (Langford &
Zhang, 2007), however, arms problem structured objects rankings.
summary framework compares existing frameworks shown
Table 1. types feedback explored literature. example,
multi-class classification problems, algorithm makes prediction based
context, feedback received whether prediction correct wrong opposed
actual label (Crammer & Gentile, 2011; Kakade, Shalev-Shwartz, & Tewari, 2008).
seen observing partial feedback (as opposed actual cardinal feedback)
bandit problem.
pointed above, Coactive Learning algorithms conventional online learning
algorithms operate different types environments. Coactive Learning algorithms present
object observe another object feedback, online convex learning algorithms
pick vector step observe gradient vector feedback. Despite
contrast online learning Coactive Learning, two algorithms presented
paper closely related work Zinkevich (2003) Hazan et al.
(2007). show possible adapt regret bounds algorithms
corresponding regrets bounds Coactive Learning. heart algorithms
analysis well-known idea (Polyak & Tsypkin, 1973) descent algorithms
necessarily need know gradients, vector positive inner product
gradient expectation suffices.
feedback Coactive Learning takes form preference, different
ordinal regression ranking. Ordinal regression (e.g., Crammer & Singer, 2001) assumes
training examples (x, y), rank. Coactive Learning model, absolute ranks
never revealed. closely related learning pairs examples (Herbrich, Graepel, & Obermayer, 2000; Freund, Iyer, Schapire, & Singer, 2003; Chu & Ghahramani, 2005),
since circumvents need absolute ranks; relative orderings required. Vari3

fiShivaswamy & Joachims

Framework
Bandits
Experts
Dueling Bandits
Coactive Learning

Algorithm
pull arm
pull arm
pull two arms
pull arm

Feedback
observe cardinal reward arm pulled
observe cardinal rewards arms
observe feedback one better
observe another arm better

Table 1: comparison different online learning frameworks.

ants pairwise ranking algorithms applied Natural Language Processing
(Haddow, Arun, & Koehn, 2011; Zhang, Lei, Barzilay, Jaakkola, & Globerson, 2014)
image annotation (Weston, Bengio, & Usunier, 2011). However, existing approaches require
iid assumption typically perform batch learning. Finally, large body
work ranking (see Liu, 2009). approaches different Coactive Learning
require training data (x, y) optimal ranking query x. However,
draw upon structured prediction approaches ranking problems design
models.
Coactive learning first proposed Shivaswamy Joachims (2012); paper
serves journal extension paper, adding complete discussion batch updates
expected feedback, exponentiated gradient algorithm, O(log(T )/T ) algorithm
-strongly convex loss functions, substantially extended empirical evaluation.
Since then, coactive learning applied intrinsically diverse retrieval (Raman,
Shivaswamy, & Joachims, 2012), learning ranking function click feedback (Raman,
Joachims, Shivaswamy, & Schnabel, 2013), optimizing social welfare (Raman & Joachims,
2013), personal robotics (Jain, Wojcik, Joachims, & Saxena, 2013), pattern discovery (Boley,
Mampaey, Kang, Tokmakov, & Wrobel, 2013), robotic monitoring (Somers & Hollinger,
2014), extended allow approximate inference (Goetschalckx, Fern, & Tadepalli, 2014).

3. Coactive Learning Model
introduce coactive learning model interaction (in rounds) learning
system (e.g. search engine) human (search user) human learning
algorithm goal (of obtaining good results). round t, learning
algorithm observes context xt X (e.g. search query) presents structured object
yt (e.g. ranked list URLs). utility yt user context xt X
described utility function U (xt , yt ), unknown learning algorithm.
feedback human user returns improved object yt (e.g. reordered list URLs),
i.e.,1
U (xt , yt ) > U (xt , yt ),

(1)

object yt exists. fact, allow violations (1) formally
model user feedback Section 3.1.
process user generates feedback yt understood
approximately utility-maximizing action, user modeled boundedly rational
1. improvements strict, margin, clear Section 3.1.

4

fiCoactive Learning

agent. particular, user selects feedback object yt approximately maximizing
utility user-defined subset Yt possible Y.
yt = argmaxyY U (xt , y)

(2)

approximately boundedly rational user may employ various tools (e.g., query
reformulations, browsing) construct subset perform search. Importantly,
however, feedback yt typically optimal label defined as,
yt := argmaxyY U (xt , y).

(3)

way, Coactive Learning covers settings user cannot manually optimize
argmax full (e.g. produce best possible ranking web search),
difficulty expressing bandit-style cardinal rating U (xt , yt ) yt consistent manner.
puts preference feedback yt stark contrast supervised learning approaches
require (xt , yt ). even importantly, model implies reliable preference feedback (1) derived observable user behavior (i.e., clicks),
demonstrate Section 3.2 web search. conjecture similar feedback strategies
exist many application settings (e.g., Jain et al., 2013; Boley et al., 2013; Somers &
Hollinger, 2014; Goetschalckx et al., 2014), especially users assumed act
approximately boundedly rational according U .
Despite weak preference feedback, aim algorithm nevertheless present
objects utility close optimal yt . Whenever, algorithm presents
object yt context xt , say suffers regret U (xt , yt ) U (xt , yt ) time
step t. Formally, consider average regret suffered algorithm steps
follows:
REGT =


1X
(U (xt , yt ) U (xt , yt )) .


(4)

t=1

goal learning algorithm minimize REGT , thereby providing human
predictions yt high utility. Note, however, cardinal value U never observed
learning algorithm, U revealed ordinally preferences (1).
3.1 Quantifying Preference Feedback Quality
provide theoretical guarantees regret learning algorithm coactive
setting, need quantify quality user feedback. Note quantification
tool theoretical analysis, prerequisite parameter algorithm. quantify
feedback quality much improvement provides utility space. simplest case,
say user feedback strictly -informative following inequality satisfied:
U (xt , yt ) U (xt , yt ) (U (xt , yt ) U (xt , yt )).

(5)

inequality, (0, 1] unknown parameter. Feedback utility
yt higher yt fraction maximum possible utility range U (xt , yt )
U (xt , yt ). term right hand side inequality ensures user feedback
5

fiShivaswamy & Joachims

yt better yt , better margin (U (xt , yt )U (xt , yt )). Violations
feedback model allowed introducing slack variables :2
U (xt , yt ) U (xt , yt ) = (U (xt , yt ) U (xt , yt )) .

(6)

Note restricted positive, negative well. refer
feedback model -informative feedback. Note possible express
feedback quality using (6) appropriate value . regret bounds
contain , quantifying extent -informative modeling assumption violated.
Finally, consider even weaker feedback model positive utility
gain achieved expectation user actions:
Et [U (xt , yt ) U (xt , yt )] = (U (xt , yt ) U (xt , yt )) .

(7)

refer feedback expected -informative feedback. equation,
expectation users choice yt given yt context xt (i.e.,
distribution Pxt [yt |yt ] dependent xt ).
rest paper, use linear model utility function,
U (x, y) = w> (x, y),

(8)

w RN parameter vector unknown learning system users
: X RN known joint feature map known system that3
k(x, y)k`2 R,

(9)

x X Y. Note x structured objects.
3.2 User Study: Preferences Clicks
validate reliable preferences specified Equation (1) indeed inferred
implicit user behavior. particular, focus preference feedback clicks
web-search draw upon data user study (Joachims, Granka, Pan, Hembrooke,
Radlinski, & Gay, 2007). study, subjects (undergraduate students, n = 16)
asked answer 10 identical questions 5 informational, 5 navigational using Google
search engine. queries, result lists, clicks recorded. subject, queries
grouped query chains question4 . average, query chain contained 2.2
queries 1.8 clicks result lists.
use following strategy infer ranking users clicks: prepend
ranking first query chain results user clicked throughout
whole query chain. assess whether U (x, y) indeed larger U (x, y) assumed
learning model, measure utility terms standard
measure retrieval quality
P10 r(x,y[i])
Information Retrieval. use DCG@10(x, y) = i=1 log i+1 , r(x, y[i])
relevance score i-th document ranking (see Manning, Raghavan, & Schutze,
2. Strictly speaking, value slack variable depends choice definition utility.
However, brevity, explicitly show dependence notation.
3. make slightly different assumption Section 5.1.
4. done manually, automated high accuracy (Jones & Klinkner, 2008).

6

fiCumulative Distribution Function

Coactive Learning

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Normal Condition
Swapped Condition
Reversed Condition
Conditions
-5

-4

-3

-2

-1
0
1
DCG(x,ybar)-DCG(x,y)

2

3

4

5

Figure 1: Cumulative distribution utility differences presented ranking
click-feedback ranking terms DCG@10 three experimental conditions
overall.
2008). get ground-truth relevance assessments r(x, d), five independent human assessors
(students) asked manually rank set results encountered query
chain. assessors given true answers navigational queries.
linearly normalize resulting ranks relative relevance score r(x, d) [0..5]
document.
evaluate whether feedback ranking indeed better ranking
originally presented, i.e., DCG@10(x, y) > DCG@10(x, y). Figure 1 plots
Cumulative Distribution functions (CDFs) DCG@10(x, y) DCG@10(x, y) three
experimental conditions, well average conditions. CDFs shifted far
right 0, showing preference feedback strategy highly accurate
informative. Focusing first average conditions, utility difference strictly
positive 60% queries, strictly negative 10%. imbalance
significant (binomial sign test, p < 0.0001). Among remaining 30% cases
DCG@10 difference zero, 88% due = (i.e. click top 1 click).
Note learning algorithm easily detect cases may explicitly eliminate
feedback. Overall, shows implicit feedback indeed produce accurate
preferences.
remains shown whether reliability feedback affected
quality current prediction, i.e., U (xt , yt ). user study, users actually
received results retrieval quality degraded purpose. particular,
one third subjects received Googles top 10 results reverse order (condition reversed) another third received rankings top two positions swapped (condition
swapped). Figure 1 shows, find users provide accurate preferences across
substantial range retrieval quality. Intuitively, worse retrieval system may make
harder find good results, makes easier baseline yt improve upon.
intuition formally captured definition -informative feedback. optimal value
vs. trade-off, however, likely depend many application-specific factors,
user motivation, corpus properties, query difficulty. following, therefore
present algorithms require knowledge , theoretical bounds hold
value , experiments explore large range .
7

fiShivaswamy & Joachims

Algorithm 1 Preference Perceptron.
Initialize w1 0
= 1
Observe xt
Present yt argmaxyY wt> (xt , y)
Obtain feedback yt
Update: wt+1 wt + (xt , yt ) (xt , yt )
end

4. Preference Perceptron Coactive Learning
section, start presenting analyzing basic algorithm coactive learning model, call Preference Perceptron (Algorithm 1). Preference
Perceptron maintains weight vector wt initialized 0. time step t,
algorithm observes context xt presents object maximizes wt> (xt , y).
algorithm observes user feedback yt weight vector wt updated
direction (xt , yt ) (xt , yt ).
Although update preference perceptron appears similar standard perceptron multi-class classification problems, key differences. First, standard
perceptron algorithm requires true label feedback, whereas much weaker feedback
suffices algorithm. Second, standard analysis perceptron bounds
number mistakes made algorithm based margin radius examples.
contrast, analysis bounds different regret captures graded notion utility.
Further, standard perceptron mistake bound (Novikoff, 1962) contains R2 kwk2
bound following Theorem contains Rkwk R defined (9).
Theorem 1 average regret preference perceptron algorithm upper bounded,
(0, 1] w follows:


1 X
2Rkw k
.
REGT
+


t=1

(10)

Proof First, consider kwT +1 k2 , have,
wT>+1 wT +1 = wT> wT + 2wT> ((xT , yT ) (xT , yT ))
+ ((xT , yT ) (xT , yT ))> ((xT , yT ) (xT , yT )
wT> wT + 4R2 4R2 T.
line one, simply used update rule algorithm 1. line two, used
fact wT> ((xT , yT ) (xT , yT )) 0 choice yT Algorithm 1
k(x, y)k R. Further, update rule algorithm 1, have,
wT>+1 w = wT> w + ((xT , yT ) (xT , yT ))> w
=


X

(U (xt , yt ) U (xt , yt )) .

t=1

8

(11)

fiCoactive Learning

use fact wT>+1 w kw kkwT +1 k (Cauchy-Schwarz inequality),
implies

X

(U (xt , yt ) U (xt , yt )) 2R kw k.
t=1

-informative modeling user feedback (6),



X

(U (xt , yt ) U (xt , yt ))

t=1


X


2R kw k,

t=1

claimed result follows.
first term regret bound denotes quality feedback terms violation
-informative feedback. particular, user feedback strictly
-informative

, slack variables (10) vanish REGT = O(1/ ).
trivial design algorithms (with even better regret) strict -informative
assumption cardinality context set X finite. One interesting aspects
bound (Theorem 1) subsequent results minimize
regret even context xt different every step. Thus, |X | could infinite
regret bound still holds.
note bound Theorem 1 holds w (0, 1]. slacks
based corresponding w .
Though user feedback modeled via -informative feedback, algorithm
require knowledge ; plays role analysis.
far, characterized user behavior terms deterministic feedback actions.
However, bound expected regret suffices, weaker model Expected Informative Feedback Equation (7) applicable.
Corollary 2 expected -informative feedback model, expected regret (over
user behavior distribution) preference perceptron algorithm upper bounded
follows:


E[REGT ]

1 X 2Rkw k
.
+



t=1

(12)

corollary proved following argument Theorem 1, taking
expectations user feedback:
E[wT>+1 wT +1 ] = E[wT> wT ] + E[2wT> ((xT , yT ) (xT , yT ))]
+ ET [((xT , yT ) (xT , yT ))> ((xT , yT ) (xT , yT )]
E[wT> wT ] + 4R2 .
above, E denotes expectation user feedback yt given yt context
xt . follows E[wT>+1 wT +1 ] 4T R2 .
9

fiShivaswamy & Joachims

Algorithm 2 Batch Preference Perceptron.
Initialize w1 0
l1
s0
= 1
Observe xt
Present yt argmaxyY wl> (xt , y)
Obtain feedback yt
== + k
P
Update: wl+1 wl + tj=s (xj , yj ) (xj , yj )
l l+1
st
end
end

Applying Jensens inequality concave function , get:
q
>
E[wT w ] kw kE[kwT k] kw k E[wT> wT ].
corollary follows definition expected -informative feedback.
4.1 Lower Bound
show upper bound Theorem 1 cannot improved general respect
scaling . following lemma, given Coactive Learning algorithm,
construct sequence examples
where, even = 1 feedback, algorithm suffers

average regret (1/ ).
Lemma 3 coactive learning algorithm
linear utility, exist xt , objects
w REGT steps (1/ ).
Proof Consider problem = {1, +1}, X = {x RT : kxk = 1}. Define
joint feature map (x, y) = yx. Consider contexts e1 , . . . , eT ej
j th component equal one others equal zero. Let y1 , .
. . yT
T,
sequence outputs



contexts
e
,
.
.
.
,
e
.
Construct
w
=
[y
/
1

1

>
y2 / , . . . , yT / ] , construction kw k = 1. Let user feedback
tth step yt . choices, user feedback
always -informative = 1
1 PT

since yt = yt . Yet, regret algorithm t=1 (w> (et , yt ) w> (et , yt )) =
( 1T ).
4.2 Batch Update
Preference Perceptron stated Algorithm 1 makes update every iteration.
applications, due high volumes feedback, might possible
update frequently. scenarios, natural consider variant Algorithm 1
makes update every k iterations; algorithm simply uses wt obtained
10

fiCoactive Learning

Algorithm 3 Generic Template Coactive Learning Algorithms
Initialize w1
= 1
Observe xt
Present yt argmaxyY wt> (xt , y)
Obtain feedback yt
Perform update wt using gradient w> ((xt , yt )(xt , yt )) obtain wt+1 .
end
previous update next update. type updates shown Algorithm 2 called
mini-batch updates used distributed online optimization (Dekel, GiladBachrach, Shamir, & Xiao, 2012). steps batch update algorithm shown
Algorithm 2. easy show following regret bound batch updates:
Lemma 4 average regret batch preference perceptron algorithm upper
bounded, (0, 1] w follows:


1 X
2Rkw k k

REGT
+
.



t=1
lemma implies mini-batches slow learning factor equal
batch size, see Section 6.2.3 empirically convergence substantially faster.

5. Deriving Algorithms Coactive Learning
Preference Perceptron regret minimizes, defined Eqn. (4),
one point design space different regret definitions learning algorithms
coactive learning. section, outline general strategy deriving coactive
learning algorithms existing algorithms online optimization. Furthermore,
demonstrate general notions regret meaningful feasible coactive
learning, derive coactive learning algorithms general convex -strongly convex
losses.
coactive learning algorithms derive section follow general template
outlined Algorithm 3. initializing w1 , iteration context xt observed
algorithm presents yt maximizing current utility estimate represented wt .
feedback yt observed, algorithm simply takes gradient w> ((xt , yt )
(xt , yt )) uses update standard online convex optimization algorithm
obtain wt+1 wt .
case, upper bound regret proposed algorithm derived
using following strategy. First, start notion regret suited
coactive learning. upper bound regret first reducing form
results standard online convex opimization regret bound applied.
gives regret bound original coactive algorithm turn. case, use
template algorithm derive specific algorithm. However, still provide self-contained
proof (in appendix) clearly pointing used regret bound
corresponding online convex optimization algorithm.
11

fiShivaswamy & Joachims

Algorithm 4 Exponentiated Preference Perceptron
Intialize w1i N1
2S1T
= 1
Observe xt
Present yt argmaxyY wt> (xt , y)
Obtain feedback yt

wt+1
wti exp((i (xt , yt ) (xt , yt )))/Zt , Zt weights add
one.
end

5.1 Exponentiated Preference Perceptron
illustrate generic strategy deriving coactive learning algorithms, first derive
exponentiated gradient algorithm coactive learning used alternative
Preference Perceptron. exponentiated gradient algorithm inherits ability
learn quickly sparse weight vectors.
Unlike additive updates Preference Perceptron, exponentiated gradient
algorithm summarized Algorithm 4 performs multiplicative updates. exponentiated
algorithm closely related exponentiated algorithms classification (Kivinen &
Warmuth, 1997). start, initializes weights uniformly. subsequent update
step rate associated it. rate depends upper bound ` norm
features (i.e., k(, )k` S) time horizon . multiplicative
update, weights normalized sum one, steps algorithm repeat.
Since updates multiplicative weights initially positive, wt guaranteed
remain positive orthant algorithm. note Algoithm 4 assumed
know S. standard techniques (see Cesa-Biachi & Lugosi, 2006b)
convert algorithm dependence , however, extensions
focus paper.
provide regret bound Algorithm 4. regret bounds Algorithm 1
Algorithm 2 depended `2 norm features, bound exponentiated
algorithm depends ` norm features.
Theorem 5 w RN kw k`1 = 1, w 0, (expected) informative feedback average (expected) regret Exponentiated Preference Perceptron upper bounded as:

1 X
2 log(m)S


REGT
+
+ ,


2
t=1

E[REGT ]


1 X 2 log(m)S


+
+ ,


2
t=1

k(x, y)k` S.
12

fiCoactive Learning

Proof start regret coactive learning algorithm defined (4):
REGT =

=


1X
(U (xt , yt ) U (xt , yt ))

t=1

X

1


1
=


(U (xt , yt ) U (xt , yt )) +

t=1

X


1 X


t=1

w> (xt , yt )

t=1





w> (xt , yt )


1 X

+


(13)

t=1

equation, used definition -informative feedback defined
Eqn. (6). viewing Algorithm 4 exponentiated online gradient descent algorithm,
easy derive following regret bound using techniques initially introduced Kivinen
Warmuth (1997),



X
X


>
(wt ((xt , yt ) (xt , yt )))
(U (xt , yt ) U (xt , yt )) + 2 log(N )S +
.
2
t=1

t=1

Since could find specific bound literature, self-contained proof provided
Appendix A. proof, REGT first upper bounded terms difference
KL(w||wt+1 ) KL(w||wt ). telescoping argument used get result.
Observing wt> ((xt , yt ) (xt , yt )) 0, get,

X
t=1




(U (xt , yt ) U (xt , yt )) 2 log(N )S +
.
2

(14)

Combining (13) (14), obtain average regret bound. proof expected
regret bound analogous Preference Perceptron.
result Theorem 1, result (Theorem 5) bounds regret
terms noisein feedback (first term) additional terms converge zero
rate O(1/ ). key difference Theorem 1 regret bound
exponentiated algorithm scales logarithmically number features,
`1 -norm w, advantageous optimal w sparse.
5.2 Convex Preference Perceptron
Generalizing definition regret Eqn. (4), allow every time step
t, (unknown) convex loss function ct : R R determines loss
ct (U (xt , yt ) U (xt , yt )) time based difference utility yt
optimal yt . functions ct assumed non-increasing. Non-increasing assumption
ct based intuition loss higher U (xt , yt ) farther
U (xt , yt ). Further, sub-derivatives ct assumed bounded. Formally, c0t ()
[G, 0] R c0t () denotes sub-derivative ct () .
vector w determines utility yt context xt assumed closed
13

fiShivaswamy & Joachims

Algorithm 5 Convex Preference Perceptron.
Initialize w1 0
= 1
Set 1t
Observe xt
Present yt argmaxyY wt> (xt , y)
Obtain feedback yt
Update: wt+1 wt + ((xt , yt ) (xt , yt ))
Project: wt+1 arg minuB ku wt+1 k2
end
bounded convex set B whose diameter denoted |B|. case convex losses,
consider following notion regret:


1X
1X

CREGT :=
ct (U (xt , yt ) U (xt , yt ))
ct (0)


t=1

(15)

t=1

bound (16), ct (0) minimum possible convex loss since U (xt , yt ) U (xt , yt )
never greater zero definition yt . Hence regret compares loss
algorithm best loss could achieved convex loss. Note that,
case ct () = , definition regret reduces earlier definition regret
linear case (Eqn. (4)).
Algorithm 5 minimizes average convex loss. two differences
algorithm Algorithm 1. First, rate associated update time
t. Second, every update, resulting vector wt+1 projected back set B.
Algorithm 5 closely related online convex optimization algorithm propsed
Zinkevich (2003). However, online convex optimization algorithm assumes
gradient loss (ct ()) observed iteration. algorithm doesnt observe
gradient directly, observes improved object yt presenting object
yt .
earlier regret bounds expressed terms slack variables . However,
following section, bounds expressed terms clipped version
slack variables defined t+ := max(0, ).
Theorem 6 Convex Preference Perceptron -informative feedback, nonincreasing convex losses ct () bounded sub-derivative, have, (0, 1]
w B,



2G X + G
|B|
|B| 4R2
+
CREGT
+
+
.

2


t=1

(16)

Similarly, expected -informative feedback, have,



2G X + G
|B|
|B| 4R2
+
E[CREGT ]
+
+
.

2


t=1
14

(17)

fiCoactive Learning

proof Theorem provided Appendix B. idea proof
first divide time steps two types depending
nature feedback.
PT
allows us upper bound CREGT terms t=1 (wt w )> ((xt , yt ) (xt , yt )).
term upper bounded following argument Zinkevich (2003) even
Coactive Learning framework.
definition CREGT Eqn. (15), theorem upper bounds
average convex loss via minimum achievable loss quality feedback.
previous result (Theorem 1), strict
-informative feedback, average loss approaches best achievable loss O(1/ ), albeit larger constant factors.
case linear utility bounds Theorem 1 Theorem 5, sufficient
average slack variables zero achieve zero regret. However,
case convex losses, upper bound regret approaches zero average
clipped slack variables zero.
5.3 Second-Order Preference Perceptron
particular class convex functions, turns give much stronger
regret bounds general convex losses. improvement special class losses
parallels improvements online convex optimization general convex losses (Zinkevich,
2003) -strongly convex losses (Hazan et al., 2007).
Definition 7 convex function f : R -strongly convex points x
D, following condition satisfied fixed > 0:
f (x) f (y) + f (x)> (x y)


||y x||2 ,
2

(18)

f (x) denotes sub-derivative x.
Algorithm 6 shows Second-order Preference Perceptron -strongly convex losses.
previous algorithms, Second-order Preference Perceptron maintains
weight vector wt , step presenting yt based context xt still
previous algorithms. However, addition weight vector, maintains
additional matrix constructed outer product vector (xt , yt )
(xt , yt ). update step projection steps involve shown
Algorithm 6. Algorithm 6 closely related online convex optimization algorithm
proposed Hazan et al. (2007). However, pointed case Algorithm 5,
algorithm observes user preference feedback step unlike online convex
optimization algorithms observe gradients. still possible prove regret bound
-strongly convex case, following result.
Theorem 8 second order preference learning algorithm, (expected) -strongly
convex, non-increasing functions ct , bounded sub-derivatives, have,
2



X + 2 2G X + G|B|
GN
4R
CREGT
+
+
+
log
+1 ,
(19)
2T 2


2T

t=1
t=1
2



4R
X +2 2G X + G|B|
GN
E[CREGT ]
+
+
+
log
+1 ,
(20)
2T 2


2T

t=1

t=1

15

fiShivaswamy & Joachims

Algorithm 6 Second-order Preference Perceptron.
Intialize w1 0
A0
/G.
= 1
Observe xt
Present yt argmaxyY wt> (xt , y)
Obtain feedback yt
At1 + [(xt , yt ) (xt , yt )][(xt , yt ) (xt , yt )]>
Update: wt+1 wt + A1
[(xt , yt ) (xt , yt )]
Project: wt+1 = arg minwB (wt+1 w)> (wt+1 w)
end
where, > 0 initialization parameter, shown Algorithm 6.
prove Theorem Appendix C. proof Theorem 6, divide
time steps two types. Starting this, possible upper bound CREGT
form resulting terms upper bounded using similar arguments
online strongly convex optimization (Hazan et al., 2007).
user feedback strictly -informative w B, first two
terms regret bound (19) result O( logT ) scaling . However,
linear dependence dimensionality joint feature map regret bound
Second-order Preference Perceptron algorithm.
Even though appears need invert matrix Second-order Preference Perceptron, avoided since updates rank one.
Woodbury matrix inversion lemma, have:
> > 1
A1
= (At1 + [(xt , yt ) (xt , yt )][(xt , yt ) (xt , yt )] ) )

= A1
t1

> 1
A1
t1 [(xt , yt ) (xt , yt )][(xt , yt ) (xt , yt )] At1

1/() + [(xt , yt ) (xt , yt )]> A1
t1 [(xt , yt ) (xt , yt )]

.

Thus, practice, Second-order Preference Perceptron update
Bt iteration. Nevertheless, projection step obtain wt+1 involves solving
quadratically-constrained quadratic program B ball fixed radius, still
takes O(N 3 ) time. Hence, Second-order Preference Perceptron computationally
demanding Convex Preference Perceptron. show experiments,
Second-order Preference Perceptron might still quite useful low-noise data.

6. Experiments
empirically evaluate Coactive Learning algorithms two real-world datasets.
two datasets differ nature prediction feedback. first dataset,
algorithms operate structured objects (rankings) whereas second dataset, atomic
items (movies) presented received feedback.
16

fiCoactive Learning

6.1 Datasets User Feedback Models
First, provide detailed description two datasets used experiments. Along this, provide details strategies used dataset
generating user feedback.
6.1.1 Structured Feedback: Web-Search
first dataset publicly available dataset Yahoo! (Chapelle & Chang, 2011)
learning rank web-search. dataset consists query-url feature vectors (denoted
xqi query q URL i), relevance rating riq ranges zero (irrelevant)
four (perfectly relevant). pose ranking structured prediction problem, defined
joint feature map follows:
w> (q, y) =

5
X
w> xqyi
.
log(i + 1)

(21)

i=1

equation, denotes ranking yi index URL
placed position ranking. Thus, measure considers top five URLs
query q computes score based graded relevance. Note utility
function defined via feature-map analogous DCG@5 (see, Manning et al., 2008)
replacing relevance label linear prediction based features.
query qt time step t, Coactive Learning algorithms present ranking ytq
maximizes wt> (qt , y). Note merely amounts sorting documents
scores wt> xqi , done efficiently. utility regret Eqn. (4), based
P
definition utility w> (q, y) given T1 Tt=1 w> ((qt , yqt ) (qt , yqt )). yqt
denotes optimal ranking respect w , consider best least
squares fit relevance labels features using entire dataset. obtain
yqt Eqn. 3, is, yqt = argmaxyY w> (qt , y). experiments, query
ordering randomly permuted twenty times report average standard error
results.
used following two user models generating simulated user feedback
experiments. first feedback model idealized version feedback whereas second
feedback based directly relevance labels available dataset:
Strict -informative feedback: model, user assumed provide
strictly -informative feedback given value (i.e., slacks zero). Given predicted ranking yt , user would go list found five URLs that,
placed top list, resulting yt satisfied strictly -informative
feedback condition w.r.t. optimal w . model assumes user
access w hence idealized feedback.
Noisy feedback depth k: feedback model, given ranking query,
user would go list inspecting top k URLs (or URLs list
shorter) specified k value. Five URLs highest relevance labels (riq )
placed top five locations user feedback. Note produces noisy
feedback since linear model perfectly fit relevance labels dataset.
17

fiShivaswamy & Joachims

6.1.2 Item Feedback: Movie Recommendation
contrast structured prediction problem previous dataset, considered
second dataset atomic predictions, namely movie recommendation. iteration,
movie presented user, feedback consists single movie well.
used MovieLens dataset grouplense.org consists million ratings
3900 movies rated 6040 users. movie ratings range one five.
randomly divided users two equally sized sets. first set used obtain
feature vector xj movie j using SVD embedding method collaborative
filtering (see Bell & Koren, 2007, Eqn. (15)). dimensionality feature vectors
regularization parameters chosen optimize cross-validation accuracy
first dataset terms squared error. second set users, considered
problem recommending movies based movie features xj . experiment setup
simulates task recommending movies new user based movie features old
users.
Tx
user second set, found best least squares approximation wi
j
users utility functions available ratings. enabled us impute utility
values movies explicitly
rated user. Furthermore, allowed us
P
> (x x ), average difference
measure regret user T1 Tt=1 wi


utility recommended movie xt best available movie xt . denote
best available movie time xt obtained Eqn. 3. experiment,
user gave particular movie feedback, recommended movie
feedback movie removed set candidates subsequent recommendations.
experiments report (average) regret values averaged 3020 users
test set.
simulate user behavior, considered following two feedback models
dataset:
Strict -informative feedback: previous dataset, model, user
assumed provide strictly -informative feedback given value (i.e., slacks
zero). Given predicted movie yt , user assumed watch movie
already highest rating remaining corpus movies. not, user
picks another movie corpus lowest utility still satisfies strict informative assumption. model assumes user access w ,
hence idealized feedback.
Noisy feedback: feedback model, given movie y, user assumed
access either actual rating movies (when available) assumed
round imputed rating nearest legal rating value. used two sub-strategies
user provides feedback. better feedback, user provides
smallest rating (actual rating rounded rating) strictly better
rating y. best feedback, user provides
highest rating (actual rating rounded rating) remaining corpus. could
multiple movies satisfying criteria, ties broken uniformly
random among movies. Note feedback model results noisy feedback
due rounding movie ratings discrete values.
18

fiCoactive Learning

6.2 Preference Perceptron
first set experiments, analyze empirical performance scaling behavior
basic Preference Perceptron Algorithm variants.
6.2.1 Strong Versus Weak Feedback
goal first experiment explore regret algorithm changed feedback quality. get feedback different quality levels , used strict -informative
feedback various values.
1.8

6

= 0.1
= 0.5
= 1.0

1.6

= 0.1
= 0.5
= 1.0

5

avg. util regret

avg. dcg regret

1.4
1.2
1
0.8

4

3

2

0.6
0.4

1
0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10



1

2

10

10

3

10



Figure 2: Regret based strict -informative feedback various values websearch (left) movie recommendation (right).
Figure 2 shows results experiment three different values. Overall, regret
typically substantially reduced tens hundreds iterations. expected,
regret = 1.0 lower compared regret lower values. Note, however,
difference two curves much smaller factor ten. note
differences less prominent case web-search. strictly
-informative feedback strictly -informative feedback . So,
user feedback model, could providing much stronger feedback required
particular value. expected theoretical bounds, since user feedback
based linear model noise, utility regret approaches zero cases. Note
show standard error plots, giving indication statistical significance.
left plots Figure 2, standard errors high lower iterations become lower
iterations. plots rest paper, error bars small
may difficult visually identify.
rest paper, strict -informative feedback, consider = 0.5
unless explicitly mention otherwise.
6.2.2 Noisy Feedback
previous experiment, user feedback based actual utility values computed
optimal w . next study regret changes noisy feedback user behavior
19

fiShivaswamy & Joachims

follow linear utility model. web-search dataset, use noisy feedback
depths 10 25, movie dataset use noisy feedback
better best variant it.
1.6

6

depth=10
depth=25

1.4

better
best

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10



1

2

10

10

3

10



Figure 3: Regret based noisy feedback web-search (left) movie recommendation
(right).

results experiment shown Figure 3. first observation make
case web-search, regret values converge zero. Similarly,
case movie recommendation regret values higher previous
experiment. results line theory shows regret converging
average slack variables user provide strict informative feedback
. Interestingly, case web-search average regret slightly higher
user goes greater depth providing feedback. due fact relevance
labels dataset noisy user maximizes (noisy) utility larger set
URLs, selection (true) utility maximizers becomes less reliable, degrades
user feedback quality.
rest paper, web-search consider noisy feedback depth=10.
case movie recommendation, consider better version noisy feedback
unless explicitly mention otherwise.
6.2.3 Batch Updates
section, consider Batch Preference Perceptron
algorithm (Algorithm 2).

regret bound Section 4.2 scales factor k strict -informative feedback,
update made every k iterations algorithm. verify whether
empirical performance scales suggested bound. web-search movies,
considered strict -informative feedback noisy feedback. types
feedback, use Batch Perceptron Algorithm various values k report
resulting average regret.
results experiments shown Figure 4 Figure 5. expected,
value k becomes smaller, regret converges faster. However, observe
20

fiCoactive Learning

1.6

6

k=1
k = 10
k=20
k=50

1.4

k=1
k=10
k=20
k=50

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

1

2

10



10

3

10



Figure 4: Regret versus time based batch updates strict -informative feedback
web-search (left) movie recommendation (right).

1.6
1.4
1.2
1

4

3

0.8

2

0.6

1

0.4 0
10

1

10

2

3

10

10

k=1
k=10
k=20
k=50

5

avg. util regret

avg. util regret

6

k=1
k = 10
k=20
k=50

0 0
10

4

10



1

2

10

10

3

10



Figure 5: Regret versus time based batch updates noisy feedback web-search
(left) movie recommendation (right).


empirical scaling k substantially better k factor suggested Lemma 4.
results show feasibility using Coactive Learning algorithms systems
might impractical update every iteration.
6.2.4 Expected User Feedback
user feedback deterministic experiments far. sub-section, consider probabilistic feedback study behavior Preference Perceptron algorithm.
Recall provided upper bound expected regret expected user feedback
Corollary 2.
provide -informative feedback expectation, consider following strategy.
Given object yt context xt , user would first generate deterministic feedback yt
21

fiShivaswamy & Joachims

following strict -informative feedback model ( = 0.5 web-search = 1.0
movie recommendation).5 addition, consider five randomly generated objects
feedback. put uniform probability mass randomly generated objects
remaining mass deterministic feedback user feedback still
-informative = 0.5 expectation.
6

Expct. feedback
Det. feedback

1.6
1.4

Expct. Feedback
Det. Feedback

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1
0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10



1

2

10

10

3

10



Figure 6: Expected feedback versus deterministic feedback web-search (left) movie
recommendation (right).

results experiment shown Figure 6. reference, plot
regret curve deterministic -informative feedback = 0.5. seen
much difference deterministic expected feedback higher numbers
iterations. seen regret converges zero even -informative
feedback expectation suggested Corollary 2.
6.2.5 Comparison Ranking SVM
compare algorithms several baselines, starting conventional
Ranking SVM (Joachims, 2002) repeatedly trained. iteration, previous
qt
SVM model used present ranking user (ysvm
). user returns ranking
qt
(ysvm ) based strict -informative feedback one experiment based noisy
qt
qt
feedback other. pairs examples (qt , ysvm
) (qt , ysvm
) used training
pairs ranking SVM. Note training ranking SVM iteration would
prohibitive expensive, since involves solving quadratic program cross-validating
regularization parameter C. Thus, retrained SVM whenever 10% examples
added training set. first training first iteration
one pair examples (starting random yq1 ), C value fixed 100
50 pairs examples, reliable cross-validation became possible.
50 pairs training set, C value obtained via five-fold cross5. Note that, case web-search, user model provide strictly -informative larger
0.5.

22

fiCoactive Learning

validation. C value determined, SVM trained training
examples available time. SVM model used present rankings
next retraining.
1.6

6

SVM
Pref. Perceptron

1.4

SVM
Pref. Perceptron

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4
3
2

0.4
1
0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

1

2

10

10

3

10





Figure 7: Preference Perceptron versus Ranking SVM strict -informative feedback web-search (left) movie recommendation (right).

6

SVM
Pref. Perceptron

1.4

5

1.2

4

avg. util regret

avg. util regret

1.6

1

3

0.8

2

0.6

1

0.4 0
10

1

10

2

3

10

10

0 0
10

4

10

SVM
Pref. Perceptron

1

2

10

10

3

10





Figure 8: Preference Perceptron versus Ranking SVM noisy feedback web-search
(left) movie recommendation (right).

results experiment shown Figure 7 Figure 8. case
strict -informative feedback, Preference Perceptron performed much better
SVM movie recommendation, comparably web search. case
noisy feedback, Preference Perceptron performs significantly better SVM
range datasets. took around 20 minutes run
Preference Perceptron experiment, took 20 hours run SVM experiment
23

fiShivaswamy & Joachims

web-dataset permutation dataset. Similary, movie recommendation
task took around 125 seconds run preference perceptron user took
around 260 seconds run SVM user. results show preference
perceptron perform par better SVMs tasks fraction
computational cost.
6.2.6 Comparison Dueling Bandit
second baseline, compare Preference Perceptron algorithm dueling
bandit approach Yue Joachims (2009). step, dueling bandit algorithm
makes comparison vector w perturbed version w1 (in random
direction u w1 = w + u). results produced two weight vectors
assessed user techniques interleaving (Radlinski, Kurup, & Joachims,
2008), providing preference w w1 . preference feedback determines
update dueling bandits algorithm makes w. w preferred, retained
next round. w1 preferred, small step length taken direction
perturbation u.
1.8

Dueling Bandit
Pref. Perceptron

1.6

1.6

1.4

1.4

1.2

1.2

avg. util regret

avg. util regret

1.8

1
0.8

1
0.8

0.6

0.6

0.4

0.4

0.2

0.2

0 0
10

1

10

2

3

10

10

0 0
10

4

10



Dueling Bandit
Pref. Perceptron

1

10

2

3

10

10

4

10



Figure 9: Preference Perceptron versus Dueling Bandit web-search. left plot based
strict -informative feedback, right plot shows noisy feedback.
first experiment web-search, step, first obtained two ranked lists
based w w1 . features used obtain ranked lists identical
used Preference Perceptron. two rankings interleaved. interleaved
ranking presented user. first experiment, user provided strict informative feedback interleaved ranking. second experiment, user
provided noisy feedback. Depending feedback, inferred two rankings preferred using Team-Game method proposed Radlinski et al. (2008).
w preferred tie, step taken. w1 preferred,
step length taken direction u. regret dueling bandit algorithm
measured considering utility interleaved ranking. Unlike Preference
Perceptron algorithm, dueling bandit algorithm two parameters ( ) need
24

fiCoactive Learning

tuned. considered 25 values parameters (5x5 grid) simply chose
best parameter values dueling bandits algorithm hindsight.
results experiment shown Figure 9. Despite advantage setting
parameters best possible values, seen dueling bandit algorithm performs
significantly worse compared preference perceptron algorithm orders magnitude.
example, performance dueling bandit around 28,000 iterations matched
preference perceptron less 100 iterations types feedback.
surprising, since dueling bandit algorithm basically relies random vectors
determine direction step needs taken. Coactive Learning model,
user feedback provides (better random) direction guide algorithm.
6

Dueling Bandit
Pref. Perceptron

5

5

4

4

avg. util regret

avg. util regret

6

3

3

2

2

1

1

0 0
10

1

2

10

10

0 0
10

3

10



Dueling Bandit
Pref. Perceptron

1

2

10

10

3

10



Figure 10: Preference Perceptron versus Dueling Bandit movie recommendation.
left plot based utility values whereas right plot shows results
rounded values.

Similarly, conducted comparison dueling bandit algorithm
movie recommendation dataset. However, unlike web-search experiment, dueling
bandit model somewhat unnatural dataset experimental setup, since interleaving two rankings natural whereas interleaving two items not. therefore consider
different setup. Two movies obtained based w w1 dueling bandit
algorithm. User feedback merely indicate two movies higher
rating. noisy case, user feedback based actual rating rounded rating. noise-free case, user feedback based utility values. either case,
utility dueling bandit considered average utility two movies selected
comparison.
performance dueling bandit algorithm experiment shown Figure 10. Preference Perceptron algorithm, regret curves strict -informative
feedback ( = 0.5) better noisy feedback shows reference.
seen dueling bandit algorithm performs substantially worse compared
Preference Perceptron algorithm.
25

fiShivaswamy & Joachims

6.3 Exponentiated versus Additive Updates
experiment, compare exponentiated algorithm (Algorithm 4) additive
Preference Perceptron algorithm. exponentiated algorithm, components
must non-negative.6 obtained non-negative follows:
[we ]i


=

min(0, [w ]i )
1 m,
max(0, [w ]im ) + 1 2m.

(22)

equation, [we ]i denotes ith component . Moreover, modified
joint feature map exponentiated algorithm follows:
e



[ (x, y)]i =

+[(x, y)]i
1im
[(x, y)]im + 1 2m

(23)

modifications, non-negative components moreover, easy verify > e (x, y) = w> (x, y). makes regret
exponentiated algorithm directly comparable regret additive algorithm.
exponentiated algorithm fixed rate parameter inversely depends
time horizon . large, small. situation, consider update
Algorithm 4:

wt+1
wti exp((i (xt , yt ) (xt , yt )))/Zt .
Since, small, approximate exponential term equation
first order approximation:
exp((i (xt , yt ) (xt , yt ))) 1 + (i (xt , yt ) (xt , yt )).
Thus exponentiated updates resemble updates additive algorithm
normalization factor. Despite normalization factor, empirically observed behavior two algorithms nearly identical (though exact). thus empirically
evaluated exponentiated algorithm variable rate parameter = 2S1t time t.
Note empirical result without formal theoretical guarantees variable
rate.
Results experiment shown Figure 11 Figure 12 strict -informative
feedback noisy feedback respectively. seen exponentiated algorithm tends performs slightly better additive algorithm small number
iterations. time horizon becomes large, two algorithms seem comparable
performance cases.
6.4 Minimizing Convex Losses
section, empirically evaluate Convex Preference Perceptron (Algorithm 5)
Second-Order Preference Perceptron (Algorithm 6).
6. put superscript e distinguish joint feature map w used experiments
exponentiated algorithm.

26

fiCoactive Learning

1.6

6

Exponentiated
Pref. Perceptron

1.4

Exponentiated
Pref. Perceptron

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

1

2

10



10

3

10



Figure 11: Exponentiated versus Additive strict -informative feedback websearch (left) movie recommendation (right).

1.6

6

Exponentiated
Pref. Perceptron

1.4

Exponentiated
Pref. Perceptron

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10



1

2

10

10

3

10



Figure 12: Exponentiated versus Additive noisy feedback web-search (left)
movie recommendation (right).

6.4.1 Convex Perceptron Versus Second-Order Algorithm
regret bounds Section 5 show one get lower regret -strongly convex
functions using second-order algorithm, first-order Convex Perceptron applies
general convex functions. section, evaluate relative performance
first-order second-order algorithms empirically. purpose, considered
quadratic loss c() = ( )2 largest utility value possible (x, y)
w convex ball radius kw k. verified loss function -strongly
convex. B set 100 algorithms datasets.
27

fiShivaswamy & Joachims

Second Order
Convex Perceptron

0.05

500

0.04

400

0.03

300

0.02

200

0.01

100

0 0
10

1

2

10

Second Order
Convex Perceptron

600

Util regret

Quad regret

0.06

3

10

10

0 0
10

4

10

1

2

10

3

10



10

4

10



Figure 13: Cumulative regret convex perceptron second order convex perceptron web-search.

0.03

250

Second Order
Convex Perceptron

200

0.02

Util regret

Quad. regret

0.025

Second Order
Convex Perceptron

0.015

150

100

0.01
50

0.005
0 0
10

1

2

10

10

0 0
10

3

10



1

2

10

10

3

10



Figure 14: Cumulative regret convex perceptron second order convex perceptron movie recommendation.

first set experiments, considered strict -informative feedback. ran
second-order algorithm well Convex Preference Perceptron algorithm 5.
value second order perceptron simply set one. recorded REGT
CREGT values methods. Note REGT corresponds utility
regret defined 4.
Results experiment shown Figure 13 Figure 14. demonstrate
qualitative difference two algorithms, plot cumulative regret (i.e.
REGT CREGT ) figures. cumulative regret second-order
algorithm linear log-scale. shows convergence regret indeed
28

fiCoactive Learning

logarithmic, compared much slower convergence Convex Preference Perceptron.
Interestingly, even cumulative regret based raw utility values empirically shows
similar behavior. purely empirical result, since theoretically, O(log(T )/T ) average
regret holds strongly convex losses linear loss strongly convex.
0

10

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

1

Util. regret

Quad regret

10

4

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

10

3

10

2

10

3

10

2

6

4

10

2

10

0

10

10

2

10

10 6
10

4

10

4

2

10

0

10



10

2

10

4

10



Figure 15: Sensitivity second order preference perceptron algorithm parameter
value .

1.2

10

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

Quad. regret

1.4

10

10

2.4

10
Util. regret

1.3

10

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

2.5

1.5

10

2.3

10

1.6

10

2.2

10
1.7

10

2.1

4

10

2

0

10

10

2

10

4

10



10

4

10

2

0

10

10

2

10

4

10



Figure 16: Sensitivity second order preference perceptron algorithm parameter
value movie recommendation.

previous experiment, fixed value second-order algorithm one.
study sensitivity second-order algorithm value parameter.
Figures 15 16 show regret values given number iterations swept
range values. dotted lines show performance Convex Preference
Perceptron comparison. case web-search, wide range parameter
29

fiShivaswamy & Joachims

values performance algorithm good. parameter takes extreme
value either side, performance algorithm deteriorates. range suitable
values much broader web-search dataset movie recommendation.
interesting note algorithms performed empirically best = 1 among
values tried.
0.12

14000

Second Order
Convex Perceptron

12000

0.1

Second Order
Convex Perceptron

Util regret

Quad regret

10000
0.08
0.06
0.04

6000
4000

0.02
0 0
10

8000

2000
1

2

10

3

10

10

0 0
10

4

10

1

2

10

3

10

10

4

10





Figure 17: Strong convex versus weak convex noisy feedback web-seach.

1400

Second Order
Convex Perceptron

0.14

1200

0.12

1000

0.1

Util regret

Quad. regret

0.16

0.08

Second Order
Convex Perceptron

800
600

0.06
400

0.04

200

0.02
0 0
10

1

2

10

10

0 0
10

3

10



1

2

10

10

3

10



Figure 18: Strong convex versus weak convex noisy feedback movie recommendation.

tested convex algorithms noisy feedback. regret bounds contain slack terms right hand side. Thus, user feedback -informative
, regret bounds second-order algorithm first-order algorithm
dominated slack variables. empirical performance two algorithms noisy feedback shown Figures 17 18. case web-search,
results second-order algorithm first-order algorithm nearly identi30

fiCoactive Learning

cal. However, case movie recommendation, still advantage
second-order algorithm.
summary, second-order algorithm performs substantially superior no-noise
circumstances. presence noise feedback, two algorithms show
drastically different behaviors.

7. Conclusions
proposed Coactive Learning new model online learning preferences
especially suited implicit user feedback. Unlike supervised learning approaches,
Coactive Learning algorithms require optimal labels, merely (noisy) feedback
improves prediction. model, cardinal utilities observed,
sits experts bandits settings, argue Coactive Learning
applicable wide range systems aim optimize based observable
user actions.
provide several algorithms provably optimize regret Coactive Learning
framework, empirically validate effectiveness proposed framework
web-search ranking movie recommendation datasets simulations noisy
noise-free feedback. recurring theme paper wide variety conventional
online learning algorithms converted Coactive Learning algorithms, despite
differences learning model itself, nature feedback notion regret.
conjecture many online learning algorithms could similarly converted
practically useful Coactive Learning algorithms.
Coactive Learning model, algorithms proposed, ability use weak
feedback observable user behavior offer wide range opportunities new learning
approaches application problems ranging natural language processing information retrieval robotics. several opportunities developing
algorithms Coactive Learning model. example, algorithm convex loss
minimization assume gradient convex losses bounded. However,
practical situations, convex loss minimized known apriori.
interesting research direction study whether algorithms utilize
gradient loss perform better either theoretically empirically. Another question
whether better algorithms exist special cases linear utility model. lower
bound based argument dimensionally joint feature maps grow
given horizon . dimensionality joint feature map fixed,
interesting research question is: algorithms better regret proposed
algorithms?

Acknowledgments
work funded part NSF awards IIS-0905467, IIS-1247637, IIS-1142251.
work done Pannaga Shivaswamy postdoctoral associate Cornell
University. thank Peter Frazier, Bobby Kleinberg, Karthik Raman, Tobias Schnabel
31

fiShivaswamy & Joachims

Yisong Yue helpful discussions. thank anonymous reviewers thoughtful
comments earlier version paper.

Appendix A. Proof Theorem 5
Proof look KL divergence w wt evolves,
KL(w||wt ) KL(w||wt+1 ) =

=

N
X
i=1
N
X


wi log(wt+1
/wti )

wi ((i (xt , yt ) (xt , yt ))) log(Zt )

i=1

= w> ((xt , yt ) (xt , yt )) log(Zt ).
(24)
P

second line, pulled log(Zt ) sum since N
i=1 w = 1. Now, consider


last term equation. Denoting (xt , yt ) (xt , yt ) brevity,
have, definition,
!
N
X
log(Zt ) = log
wti exp(i )
log

i=1
N
X

wti (1



2



2

!

+ + )

i=1



log 1 + wt> + 2 2
wt> + 2 2 .

(25)

second line used fact exp(x) 1 + x + x2 x 1. rate ensures
(i ) 1. last line, used fact log(1 + x) x. Combing (24)
(25), get,
(w wt )>

KL(w||wt ) KL(w||wt+1 )
+ 2 .


Adding inequalities, get:


1
X
X
KL(w||wt ) KL(w||wt+1 ) X 2
(w wt )> ((xt , yt ) (xt , yt ))
+
.

t=1

t=1



t=1

KL(w||w0 )
+ 2 T.


Rearranging inequality, substituting value Algorithm 4,
get:



X
X


>
(U (xt , yt ) U (xt , yt ))
wt ((xt , yt ) (xt , yt )) + 2 log(N )S +
2
t=1
t=1



2 log(N )S +
.
2
32

fiCoactive Learning

above, used fact KL(w||w1 ) log(N ) since w1 initialized uniformly. Moreover, Holders inequality, obtained
wt> (xt , yt ) kwt k`1 k(xt , yt )k` S.
inequality along -informative feedback gives claimed result.

Appendix B. Proof Theorem 6
Proof First, divide set time steps two different sets based nature
feedback:
:= {t : U (xt , yt ) U (xt , yt )) 0; 1 },
J := {t : U (xt , yt ) U (xt , yt )) < 0; 1 }.
brevity denote (xt , a) (xt , b) by7 (a, b) rest proof. start
considering following term single time step t:
ct (U (xt , yt ) U (xt , yt )) ct (0)

>
wt (yt , yt )

ct (U (xt , yt ) U (xt , yt )) ct


>

>
wt (yt , yt )
w (yt , yt )

ct
=ct




>

>
(w wt ) (yt , yt ) 0 w (yt , yt )


ct






+
>
>
(wt (yt , yt ) + w (yt , yt ))G/


(wt> (yt , yt ) + t+ )G/
J.
w> (y ,y )

inequalities, second line follows fact 0 ct ()
non-increasing. third line follows -informative feedback (Eqn. (6)). fourth
line follows since function ct convex.8 obtain first term next inequality
(in either case) since c0t () [G, 0] wt> (yt , yt ) 0 choice yt
algorithm. second terms (in either case) obtained fact c0t (w> (yt , yt ))
upper bounded t+ G. step clipped version slack variables
needed proof. Finally, w> (yt , yt ) either positive negative depending
feedback leads two different cases depending whether J.
7. Since context xt always clear, suppress notation brevity.
8. convex function f , f (y) f (x) (y x)f 0 (y) f 0 (y) denotes sub-derivative f y.

33

fiShivaswamy & Joachims

Summing inequalities 1 , get:

X

ct (w> (yt , yt ))

t=1






GX


G


wt> (yt , yt ) +

t=1

X


X

ct (0)

t=1

X

G


t+

t=1

tI

(wt w )> (yt , yt ) +

t=1

GX >
w (yt , yt )


G



X
t=1

t+ +

GX >
w (yt , yt ).


(26)

tJ

P
obtained last line simply adding subtracting G tJ w> (yt , yt )/
right side previous inequality. point, mostly follow proof
techniques online convex optimization (Zinkevich, 2003).
bound first term right hand side (26). purpose, consider
following:
kwt+1 w k2 = kwt + (yt , yt ) w k2
= kwt w k2 + t2 k(yt , yt )k2 + 2t (wt w )> (yt , yt ).

(27)

Rearranging terms equation, get:
1
kwt w k2
2t
1

kwt w k2
2t

(wt w )> (yt , yt ) =

1

kwt+1 w k2 + k(yt , yt )k2
2t
2
1
kwt+1 w k2 + 2t R2
2t

where, last line, used fact kwt+1 w k2 kwt+1 w k2 since wt+1
projection wt+1 convex set B (which contains vector w ).
bound first term (26) using following telescoping argument.


X
1
1
2
2
2
kwt w k
kwt+1 w k + 2t R
2t
2t
t=1



X
X
1
1
1
2
kw1 w k +

kwt w k2 + 2R2


21
2t 2t1
t=2
t=1


X

1
1
1

|B| +

|B| + 2R2 (2 1)
21
2t 2t1
t=2


+1

|B| + 4R2 .
2
above, obtained second line simply rearranging terms expression
above.
line, used boundedness property set B, well

PTOn third
fact t=1
2 1. final line follows cancelling terms fact
= 1/ .
34

fiCoactive Learning

Now, consider third term right hand side (26):

w> (yt , yt )
+
+
w> (yt , yt ) + .




first inequality follows -informative feedback. Whereas second inequal

ity follows fact w> (yt , yP
) 0 definition yt . Finally, bound (16)
+
G
follows trivial fact 0 tI .
obtain bound expected regret, consider convex loss step conditioned user behavior far:


wt> (yt , yt )
Et ct



>

>
Et [w (yt , yt ) ]
wt (yt , yt )
ct
Et ct




>
>

w (yt , yt )
wt (yt , yt )
Et ct
Et ct



+
>
>

GEt [wt (yt , yt ) + w (yt , yt )]/


GEt [wt> (yt , yt ) + t+ ]/
tJ
ct (w> (yt , yt ))



second line follows definition expected -informative feedback
third line follows Jensens inequality. obtain last line following argument
similar proof Theorem 6. bound follows expected version
(27).

Appendix C. Proof Theorem 8
Proof First, divide time steps two different sets based nature feedback:

:= {t : U (xt , yt ) U (xt , yt )) 0; 1 },
J := {t : U (xt , yt ) U (xt , yt )) < 0; 1 }.
35

fiShivaswamy & Joachims

start considering single time step t, have:
ct (w> (yt , yt )) ct (0)
>

wt (yt , yt )
>

ct (w (yt , yt )) ct

>
>

+
w (yt , yt )
wt (yt , yt )
ct
ct






2

+ >
>
w (yt , yt ) t+
(w wt )> (yt , yt ) t+
(w wt ) (yt , yt )
0
ct









2







2
+
+
>
>



wt> (yt ,yt )
,yt )

tI
+ w (y
2 (w wt ) (yt ,yt )
G






(28)





+
+ 2
> (y ,y )
> (y ,y )



w
(w
w
)








+ 2

J.
G


w> (y ,y )

inequalities, second line follows fact 0 ct ()
non-increasing. third line follows fact function ct () non-increasing
following inequality follows definition t+ :
U (xt , yt ) U (xt , yt ) + (U (xt , yt ) U (xt , yt )) t+ .
fourth line follows strong convexity. last line follows line
reasoning proof Theorem 6.
consider last term cases:


2
=


2

=


2




2




2

2
(w wt )> (yt , yt ) t+





2
2
(w wt )> (yt , yt )
t+
t+ (w wt )> (yt , yt )

+

22
2


2
2
(w wt )> (yt , yt )
+ w> (yt , yt ) t+ wt> (yt , yt ) t+
+ 2




2
22

2


2
(w wt )> (yt , yt )
t+
t+
t+
>

+
w (yt , yt ) +




22


2
2
(w wt )> (yt , yt )
+
+ 2.
(29)

2


equations, second third lines follow simple algebraic expansion
expression first line. fourth line follows definition -informative
feedback fact wt> (yt , yt ) 0. last line follows fact
w> (yt , yt ) 0 definition yt .
36

fiCoactive Learning

Now, summing terms (28) substituting bound, get,


X

ct (w> (yt , yt ))

t=1


X

ct (0)

t=1

2 !
X w> (yt , yt ) PT + 2
(w wt )> (yt , yt )

t=1
G
+
G


22
t=1
tI
!


2

>
>
X (wt w ) (yt , yt ) (w wt ) (yt , yt )
G


2

t=1
P
P
2
Tt=1 t+
G Tt=1 t+
GX >
+
w (yt , yt ) +
+

22

tJ

2 PT + 2 2G PT +
GX

>
>
t=1
t=1

(wt w ) (yt , yt )
(wt w ) (yt , yt )
+
.
+

2
22


X

wt> (yt , yt ) t+
+



2



t=1

>
P
,yt )
above, obtained third inequality adding subtracting G tJ w (y
.

2
obtain last line, used fact 1/ 1/ since (0,

P 1]). Finally,
> (y , )
used argument similar proof theorem 6 bound G
w



tJ

obtained factor two sum slacks term. point, use arguments
similar online convex optimization strongly convex losses (Hazan et al.,
2007).

Next, consider (wt+1 w )> (wt+1 w ) express interms wt At1 :

(wt+1 w )> (wt+1 w )
1
>
=(wt A1
(yt , yt ) w ) (wt (yt , yt ) w )
>
=(wt w )> (wt w ) + (yt , yt )> A1
(yt , yt ) 2(wt w ) (yt , yt )

=(wt w )> (yt , yt )(yt , yt )> (wt w ) + (wt w )> At1 (wt w )
>
+ (yt , yt )> A1
(yt , yt ) + 2(w wt ) (yt , yt )

Rearranging terms equation, get:


(wt w )> (yt , yt )(yt , yt )> (wt w )
2
(wt w )> At1 (wt w ) (wt+1 w )> (wt+1 w ) + (yt , yt )> A1
(yt , yt ).
2(wt w )> (yt , yt )

37

fiShivaswamy & Joachims

identify term left hand side inequality occurs
expression would bound (29). therefore have,
2




X

t=1

X

(wt w )> (yt , yt ) ((wt w )> (yt , yt ))2




(wt w )> At1 (wt w ) (wt+1 w )> (wt+1 w ) + (yt , yt )> A1
( byt , yt )

t=1

(w1 w )> A0 (w1 w ) +


X

(yt , yt )> A1
(yt , yt )

t=1

|B| +

N
log




4R2



+1 .

P
2
above, used fact Tt=1 (yt , yt )> A1
(yt , yt ) N log(4R /+1),
N dimens ionality (x, y) R upper bound norm joint
feature maps (i.e. k(x, y)k`2 R. proof fact found Hazan et al. (2007).

References
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002a). Finite-time analysis multiarmed
bandit problem. Machine Learning, 47 (2-3), 235256.
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. (2002b). non-stochastic multiarmed bandit problem. SIAM Journal Computing, 32 (1), 4877.
Bakir, G. H., Hofmann, T., Scholkopf, B., Smola, A., Taskar, B., & Vishwanathan, S. (Eds.).
(2007). Predicting Structured Data. MIT Press.
Bell, R. M., & Koren, Y. (2007). Scalable collaborative filtering jointly derived neighborhood interpolation weights. ICDM.
Boley, M., Mampaey, M., Kang, B., Tokmakov, P., & Wrobel, S. (2013). One click mining:
Interactive local pattern discovery implicit preference performance learning. Proceedings ACM SIGKDD Workshop Interactive Data Exploration
Analytics, pp. 2735.
Cesa-Bianchi, N., & Lugosi, G. (2006a). Prediction, learning, games. Cambridge University Press.
Cesa-Bianchi, N., & Lugosi, G. (2006b). Prediction, Learning, Games. Cambridge
University Press, Cambridge, UK.
Chapelle, O., & Chang, Y. (2011). Yahoo! learning rank challenge overview. JMLR Proceedings Track, 14, 124.
Chu, W., & Ghahramani, Z. (2005). Preference learning gaussian processes. ICML.
Crammer, K., & Singer, Y. (2001). Pranking ranking. NIPS.
38

fiCoactive Learning

Crammer, K., & Gentile, C. (2011). Multiclass classification bandit feedback using
adaptive regularization. Proceedings 28th International Conference Machine Learning (ICML).
Dekel, O., Gilad-Bachrach, R., Shamir, O., & Xiao, L. (2012). Optimal distributed online
prediction using mini-batches. JMLR, 13, 165202.
Flaxman, A., Kalai, A. T., & McMahan, H. B. (2005). Online convex optimization
bandit setting: gradient descent without gradient. SODA.
Freund, Y., Iyer, R. D., Schapire, R. E., & Singer, Y. (2003). efficient boosting algorithm
combining preferences. Journal Machine Learning Research, 4, 933969.
Goetschalckx, R., Fern, A., & Tadepalli, P. (2014). Coactive learning locally optimal
problem solving.. Conference American Association Artificial Intelligence
(AAAI), pp. 18241830.
Haddow, B., Arun, A., & Koehn, P. (2011). Samplerank training phrase-based machine
translation. Proceedings Sixth Workshop Statistical Machine Translation,
pp. 261271, Edinburgh, Scotland. Association Computational Linguistics.
Hazan, E., Agarwal, A., & Kale, S. (2007). Logarithmic regret algorithms online convex
optimization. Machine Learning, 69 (2-3), 169192.
Herbrich, R., Graepel, T., & Obermayer, K. (2000). Large margin rank boundaries
ordinal regression. Advances Large Margin Classifiers. MIT Press.
Jain, A., Wojcik, B., Joachims, T., & Saxena, A. (2013). Learning trajectory preferences
manipulators via iterative improvement. Neural Information Processing Systems
(NIPS), pp. 575583.
Joachims, T. (2002). Optimizing search engines using clickthrough data. ACM SIGKDD
Conference Knowledge Discovery Data Mining (KDD), pp. 133142.
Joachims, T., Granka, L., Pan, B., Hembrooke, H., Radlinski, F., & Gay, G. (2007). Evaluating accuracy implicit feedback clicks query reformulations web
search. ACM Transactions Information Systems (TOIS), 25 (2).
Jones, R., & Klinkner, K. (2008). Beyond session timeout: automatic hierarchical
segmentation search topics query logs. CIKM.
Kakade, S. M., Shalev-Shwartz, S., & Tewari, A. (2008). Efficient bandit algorithms
online multiclass prediction. Proceedings 25th International Conference
Machine Learning (ICML).
Kivinen, J., & Warmuth, M. (1997). Exponentiated gradient versus gradient gradient descent linear predictors. Journal Information Computation, 132 (1), 164.
Langford, J., & Zhang, T. (2007). epoch-greedy algorithm multi-armed bandits
side information. NIPS.
Liu, T.-Y. (2009). Learning rank information retrieval. Foundations Trends
Information Retrieval, 3.
Manning, C., Raghavan, P., & Schutze, H. (2008). Introduction Information Retrieval.
Cambridge University Press.
39

fiShivaswamy & Joachims

Novikoff, A. (1962). convergence proofs perceptrons. Proceedings Symposium
Mathematical Theory Automata, Vol. XII, pp. 615622.
Polyak, B., & Tsypkin, Y. (1973). Pseudogradient adaptation training algorithms.
Automatic Remote Control, 12, 8394.
Radlinski, F., Kurup, M., & Joachims, T. (2008). clickthrough data reflect retrieval quality?. Conference Information Knowledge Management (CIKM).
Raman, K., & Joachims, T. (2013). Learning socially optimal information systems
egoistic users. European Conference Machine Learning (ECML), pp. 128144.
Raman, K., Joachims, T., Shivaswamy, P., & Schnabel, T. (2013). Stable coactive learning
via perturbation. International Conference Machine Learning (ICML), pp.
837845.
Raman, K., Shivaswamy, P., & Joachims, T. (2012). Online learning diversify
implicit feedback. KDD.
Shivaswamy, P., & Joachims, T. (2012). Online structured prediction via coactive learning.
ICML.
Somers, T., & Hollinger, G. (2014). Coactive learning human expert robotic
monitoring. RSS Workshop Robotic Monitoring.
Weston, J., Bengio, S., & Usunier, N. (2011). Wsabie: Scaling large vocabulary
image annotation. Proceedings International Joint Conference Artificial
Intelligence (IJCAI).
Yue, Y., Broder, J., Kleinberg, R., & Joachims, T. (2009). k-armed dueling bandits
problem. COLT.
Yue, Y., & Joachims, T. (2009). Interactively optimizing information retrieval systems
dueling bandits problem. ICML.
Zhang, Y., Lei, T., Barzilay, R., Jaakkola, T., & Globerson, A. (2014). Steps excellence: Simple inference refined scoring dependency trees. Proceedings
52nd Annual Meeting Association Computational Linguistics (Volume
1: Long Papers), pp. 197207, Baltimore, Maryland. Association Computational
Linguistics.
Zinkevich, M. (2003). Online convex programming generalized infinitesimal gradient
ascent. ICML.

40


