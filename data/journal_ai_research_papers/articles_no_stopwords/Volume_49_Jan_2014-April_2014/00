journal artificial intelligence

submitted published

multimodal distributional semantics
elia bruni

elia bruni unitn

center mind brain sciences
university trento italy

nam khanh tran

ntran l de

l center
hannover germany

marco baroni

marco baroni unitn

center mind brain sciences
university trento italy
department information engineering computer science
university trento italy

abstract
distributional semantic derive computational representations word meaning
patterns co occurrence words text success
story computational linguistics able provide reliable estimates semantic
relatedness many semantic tasks requiring however distributional
extract meaning information exclusively text extremely impoverished
basis compared rich perceptual sources ground human semantic knowledge
address lack perceptual grounding distributional exploiting computer
vision techniques automatically identify discrete visual words images
distributional representation word extended encompass co occurrence
visual words images associated propose flexible architecture
integrate text image distributional information set
empirical tests integrated model superior purely text
provides somewhat complementary semantic information respect latter

introduction
distributional hypothesis states words occur similar contexts semantically similar claim multiple theoretical roots psychology structuralist linguistics lexicography possibly even later writings wittgenstein firth
harris miller charles wittgenstein however distributional hypothesis huge impact computational linguistics last two decades mainly
empirical reasons suggests simple practical method harvest
word meaning representations large scale record contexts words
occur easy assemble large collections texts corpora use contextual profiles surrogates meaning nearly contemporary corpus approaches
semantics rely contextual evidence one way another systematic
extensive application distributional methods found call distributional
semantic dsms known literature vector space semantic space
ai access foundation rights reserved

fibruni tran baroni

meaning landauer dumais sahlgren schtze turney
pantel
dsms meaning word approximated vector keeps track
patterns co occurrence word text corpora degree semantic
similarity generally relatedness budanitsky hirst two words
precisely quantified terms geometric distance vectors representing
example car automobile might occur terms street gas
driver thus distributional vectors likely close cuing fact
words synonyms extended empirical evidence shown distributional
semantics good harvesting effective meaning representations large scale
confirming validity distributional hypothesis see references section

still successes distributional semantics suffers obvious limitation
represents meaning word entirely terms connections words long
tradition studies cognitive science philosophy stressed
meaning symbols e g words entirely accounted terms symbols e g
words without links outside world e g via perception deeply problematic issue often referred symbol grounding harnad
dsms come attack lack grounding glenberg robertson
although specific criticisms vented might entirely well founded
burgess little doubt limitation textual contexts makes
dsms dissimilar humans thanks senses access rich sources
perceptual knowledge learning meaning words much cognitive scientists argued meaning directly embodied sensory motor processing
see work de vega glenberg graesser different views embodiment
cognitive science indeed last decades large amount behavioural neuroscientific evidence amassed indicating knowledge words concepts
inextricably linked perceptual motor systems example perceiving actiondenoting verbs kick lick involves activation areas brain controlling
foot tongue movements respectively pulvermueller hansen olkkonen walter gegenfurtner asked subjects adjust color fruit images objects
appeared achromatic objects generally adjusted color shifted
away subjects gray point direction opposite typical color fruit
e g bananas shifted towards blue subjects overcorrected typical
yellow color typical color influences lexical access example subjects faster
naming pumpkin picture presented orange grayscale
representation slowest another color therriault yaxley zwaan
final example kaschak madden therriault yaxley aveyard blanchard zwaan
found subjects slower processing sentence describing action
sentence presented concurrently visual stimulus depicting motion opposite
harnard original discussing formal symbols postulated fodors language
thought fodor rather words natural language however latter
represented terms connections words case dsms grounding
arises follow recent literature issue referring symbol grounding
symbols natural language words



fimultimodal distributional semantics

direction described e g car approached harder process concurrently
perception motion away see review barsalou review
evidence conceptual linguistic competence strongly embodied
one might argue concerns dsms grounded embodied
exaggerated overlook fact patterns linguistic co occurrence
exploited dsms reflect semantic knowledge acquired perception
linguistic perceptual information strongly correlated louwerse
dogs often brown pink likely talk brown dogs
pink dogs consequently child learn useful facts meaning concept
denoted dog direct perception linguistic input explains among
things congenitally blind subjects excellent knowledge color terms
see e g connolly gleitman thompson schill one could hypothesize
meaning representations extracted text corpora indistinguishable
derived perception making grounding redundant however fairly
extensive literature showing case many studies andrews vigliocco
vinson baroni barbu murphy poesio baroni lenci riordan
jones underlined text derived dsms capture encyclopedic functional
discourse related properties word meanings tend miss concrete aspects
intuitively might harvest text information bananas tropical eatable
yellow authors write obvious statements
bananas yellow hand studies humans
asked describe concepts features produce equivalent sense contextual
features exploited dsms preponderantly perceptual nature bananas yellow
tigers stripes
discrepancy dsms humans per se proof dsms
face empirical difficulties computational semantic however interested
potential implications dsms humans acquire use language
case many dsm developers e g griffiths steyvers tenenbaum
landauer dumais lund burgess many others complete
lack grounding perception serious blow psychological plausibility
exposes criticism classic ungrounded symbolic received
even empirical level reasonable expect dsms enriched perceptual
information would outperform purely textual counterparts useful computational semantic must capture human semantic knowledge human semantic knowledge
strongly informed perception
accept grounding dsms perception desirable avenue
must ask practical source perceptual information embed dsms
several interesting recent experiments use features produced human subjects concept
description tasks called semantic norms surrogate true perceptual features
andrews et al johns jones silberer lapata steyvers
reasonable first step integration methods proposed studies
perfectly fair tendency might part triggered fact subjects asked
describe concepts might encouraged focus perceptual aspects experimenters
instructions example mcrae cree seidenberg mcnorgan asked subjects list first
physical properties internal external parts object looks



fibruni tran baroni

quite sophisticated subject produced features unsatisfactory practically
theoretically see however work reported kievit kylar jones
crowdsourcing project addressing kinds concerns practically subjectgenerated properties limits experiments words denote concepts described
semantic norms even large norms contain features hundred concepts
theoretically features produced subjects concept description tasks far removed
sort implicit perceptual features supposed stand example
since expressed words limited conveyed verbally
moreover subjects tend produce salient distinctive properties
state dogs head since thats hardly distinctive feature animal
article explore direct route integrate perceptual information
dsms exploit recent advances computer vision grauman leibe
availability documents combine text images automatically extract visual
features naturally co occurring words multimodal corpora imagebased features combined standard text features obtain perceptuallyenhanced distributional vectors rely natural extension distributional hypothesis encompasses similarity linguistic context
similarity visual context interestingly landauer dumais one classic papers
laid groundwork distributional semantics already touched grounding
issue proposed speculatively solution along lines one implementing
f one judiciously added numerous pictures scenes without rabbits
context columns corpus matrix filled handful appropriate cells
rabbit hare word rows dsm could easily learn words rabbit hare
go pictures containing rabbits ones without forth landauer
dumais p
although vision one source perceptual data reasonable starting point
convenience availability suitable data train
probably dominating modality determining word meaning one piece
evidence claim widely used subject generated semantic norms mcrae et al
contain distinct perceptual features total
visual nature
relatively low level noisy features extract images multimodal corpora contribute meaningful information distributional representation
word meaning report systematic comparison network semantic relations entertained set concrete nouns traditional text
novel image distributional spaces confirming image features indeed
semantically meaningful moreover expected provide somewhat complementary
information respect text features thus found practical effective way extract perceptual information must consider next combine textand image derived features build multimodal distributional semantic model
propose general parametrized architecture multimodal fusion given appropriate
sample data automatically determines optimal mixture text image features used target semantic task finally evaluate multimodal dsms
thank mike jones pointing interesting historical connection us



fimultimodal distributional semantics

two separate semantic tasks namely predicting degree semantic relatedness assigned
word pairs humans categorizing nominal concepts classes
tasks multimodal dsms consistently outperform purely textual confirming
supposition humans performance computational
meaning improves meaning grounded perception
article structured follows section provides relevant background
computational linguistics image analysis discusses related work lay
general architecture multimodal fusion distributional semantics section
necessary implementation details provided section section presents experiments tested section concludes summarizing current
well sketching come next

background related work
section first give brief introduction traditional distributional semantic
e solely textual information describe image analysis
techniques adopt extract manipulate visual information next discuss earlier
attempts construct multimodal distributional representation meaning finally
describe relevant strategies combine information coming text images
proposed inside computer vision community
distributional semantics
last decades number different distributional semantic dsms word
meaning proposed computational linguistics relying assumption
word meaning learned directly linguistic environment
semantic space one common types dsm approximate
meaning words vectors record distributional history corpus
turney pantel distributional semantic model encoded matrix whose
rows semantic vectors representing meanings set target words
component semantic vector function occurrence counts corresponding
target word certain context see lowe formal treatment definitions
context range simple ones documents occurrence another word
inside fixed window target word linguistically sophisticated ones
occurrence certain words connected target special syntactic relations
pad lapata sahlgren turney pantel raw targetcontext counts collected transformed association scores typically
discount weights components whose corresponding word context pairs high
probability chance co occurrence evert rank matrix containing
semantic vectors rows optionally decreased dimensionality reduction
might provide beneficial smoothing getting rid noise components allow
efficient storage computation landauer dumais sahlgren schtze
finally distributional semantic similarity pair target words estimated
similarity function takes semantic vectors input returns scalar
similarity score output


fibruni tran baroni

many different semantic space literature probably best
known latent semantic analysis lsa landauer dumais highdimensional semantic space words derived use co occurrence information
words passages occur another well known example
hyperspace analog language model hal lund burgess word
represented vector containing weighted co occurrence values word
words fixed window semantic space rely syntactic relations instead
windows grefenstette curran moens pad lapata general
overviews semantic space provided clark erk manning
schtze sahlgren turney pantel
recently probabilistic topic receiving increasing attention
alternative implementation dsms blei ng jordan griffiths et al
probabilistic topic rely co occurrence information large corpora derive meaning differently semantic space assumption
words corpus exhibit probabilistic structure connected topics words
represented points high dimensional space probability distribution
set topics conversely topic defined probability distribution
different words probabilistic topic tackle meaning representation
means statistical inference use word corpus infer hidden topic structure
distributional semantic whether geometric probabilistic kind
ultimately mainly used provide similarity score arbitrary pairs words
employ indeed shown effective
modeling wide range semantic tasks including judgments semantic relatedness
word categorization
several data sets assess well dsm captures human intuitions
semantic relatedness rubenstein goodenough set rubenstein goodenough wordsim finkelstein gabrilovich matias rivlin solan wolfman
ruppin usually constructed asking subjects rate set word
pairs according similarity scale average rating pair taken
estimate perceived relatedness words e g dollar buck cord smile
measure well distributional model approximates human semantic intuitions
usually correlation measure similarity scores generated model
human ratings computed highest correlation aware wordsim
set employ obtained model called temporal
semantic analysis captures patterns word usage time concepts
represented time series corpus temporally ordered documents radinsky
agichtein gabrilovich markovitch temporal knowledge could integrated
perceptual knowledge encode model direct comparison point
agirre alfonseca hall kravalova pasa soroa presented extensive evaluation distributional wordnet semantic wordsim achieving
maximum correlation across parameters
wordnet available http wordnet princeton edu large computational lexicon english
nouns verbs adjectives adverbs grouped sets synonyms synsets expressing
distinct concept



fimultimodal distributional semantics

humans good grouping together words concepts denote
classes semantic relatedness murphy therefore cognitive aware
representation meaning must proficiency categorization e g poesio
almuhareb baroni et al concept categorization moreover useful
applications automated ontology construction recognizing textual entailment
unlike similarity ratings categorization requires discrete decision group coordinates cohyponyms class performed applying standard clustering techniques
model generated vectors representing words categorized example
almuhareb poesio data set almuhareb poesio employ
includes concepts wordnet balanced terms frequency degree ambiguity distributional model rothenhusler schtze exploits syntactic
information reach state art performance almuhareb poesio data set maximum clustering purity across parameter window distributional
baroni lenci directly comparable text
achieves purity
semantic tasks dsms applied include semantic priming generation
salient properties concepts intuitions thematic fit verb arguments see
e g baroni lenci baroni et al mcdonald brew pad lapata
pad pad erk distributional semantic vectors used wide
range applications require representation word meaning particular
objective measure meaning relatedness including document classification clustering
retrieval question answering automatic thesaurus generation word sense disambiguation
query expansion textual advertising areas machine translation dumais
turney pantel
visual words
ideally build multimodal dsm would extract visual information
images way similar text thanks well known image
analysis technique namely bag visual words bovw indeed possible discretize
image content produce visual units somehow comparable words text known
visual words bosch zisserman munoz csurka dance fan willamowski
bray nister stewenius sivic zisserman yang jiang hauptmann
ngo therefore semantic vectors extracted corpus images
associated target textual words similar pipeline commonly
used construct text vectors collect co occurrence counts target words
discrete image contexts visual words approximate semantic relatedness
two words similarity function visual words representing
bovw technique extract visual word representations documents inspired
traditional bag words bow method information retrieval bow turn
dictionary method represent textual document bag e order
considered contains words dictionary bovw extends idea visual
documents namely images describing collection discrete regions capturing
appearance ignoring spatial structure visual equivalent ignoring word
order text bag visual word representation image convenient image

fibruni tran baroni






ffffff

fififi






figure representing images bovw salient image patches keypoints contain rich local information detected represented vectors low level
features called descriptors ii descriptors mapped visual words
basis distance centers clusters corresponding visual words


preliminary clustering step shown
figure iii images finally
represented bag visual words feature vector according distribution
visual words contain images depicting things rotations
occlusions small differences low level descriptors might still similar
distribution visual words hence object traced robustly
across images conditions change

analysis point view translates usually large set high dimensional local
descriptors single sparse vector representation across images importantly size
original set varies image image bag visual word representation
fixed dimensionality therefore machine learning default expect
fixed dimensionality vectors input e g supervised classification unsupervised
clustering used tackle typical image analysis tasks object recognition
image segmentation video tracking motion detection etc
specifically similarly terms text document image local interest
points keypoints defined salient image patches contain rich local information
image however keypoint types images come shelf word


fimultimodal distributional semantics

types text documents local interest points grouped types e visual
words within across images image represented number
occurrences type analogously bow following pipeline typically
followed every image data set keypoints automatically detected note
recent approaches dense pixelwise sampling keypoints preferred
detecting salient ones solution adopt
explained section represented vectors low level features called descriptors
keypoint vectors grouped across images number clusters
similarity descriptor space cluster treated discrete visual word
keypoints mapped onto visual words image represented bovw feature
vector recording many times visual word occurs way move
representing image varying number high dimensional keypoint descriptor vectors
representation terms single visual word count vector fixed dimensionality
across images advantages discussed visual word assignment
use represent image content exemplified figure two images
similar content described terms bag visual word vectors
kind image content visual word captures exactly depends number
factors including descriptors used identify represent keypoints clustering
number target visual words selected general local interest points
assigned visual word tend patches similar low level appearance
local patterns need correlated object level parts present images
grauman leibe
multimodal distributional semantics
availability large amounts mixed media web one hand
discrete representation images visual words escaped attention
computational linguists interested enriching distributional representations word
meaning visual features
feng lapata propose first multimodal distributional semantic model
generative probabilistic setting requires extraction textual visual features
mixed media corpus latent dimensions estimated
probabilistic process assumes document generated sampling textual
visual words words represented distribution set latent
multimodal dimensions topics griffiths et al derived surface textual
visual features feng lapata experiment collection documents downloaded
bbc news website corpus test semantic representations
free association norms nelson mcevoy schreiber subset
pairs wordsim obtaining gains performance visual information taken
account correlations human judgments respectively compared
textual modality standalone respectively even performance still
well state art wordsim see section
main drawbacks textual visual data must
extracted corpus thus limiting choice corpora used
generative probabilistic elegant allow much flexibility


fibruni tran baroni

two information channels combined implement feng
lapata method mixlda training esp game data set source labeled
images adopt model possible data set contains images
textual labels describing general recapture feng lapatas
idea common latent semantic space latent multimodal mixing step pipeline
see section
leong mihalcea exploit textual visual information obtain multimodal distributional semantic model feng lapata merge two sources
information learning joint semantic model leong mihalcea propose strategy
akin call scoring level fusion come separate text
image similarity estimates combine obtain multimodal score
particular use two combination methods summing scores computing
harmonic mean differently feng lapata leong mihalcea extract visual information corpus manually coded resource namely imagenet
database deng dong socher li fei fei large scale ontology images
handcoded annotated visual resource imagenet faces sort
manually developed lexical database wordnet faces respect
textual information applications severely limited imagenet coverage
example imagenet currently restricted nominal concepts interest
model computational simulation word meaning acquisition naturally occurring language visual data somewhat reduced humans learn meaning
mountain set carefully annotated images mountains little else crowding
occluding scene evaluation leong mihalcea experiment small subsets wordsim obtaining improvements although level report
highest reported correlation word pairs furthermore use
data set tune test
bruni tran baroni propose instead directly concatenate textand image vectors produce single multimodal vector represent words
call feature level fusion text distributional vector representing word taken state art distributional semantic model baroni
lenci concatenated vector representing word visual features extracted images esp game collection use
obtain promising performance wordsim test sets although appreciably lower
report obtain maximum correlation text
image features used together compare table
attempts use multimodal derived text images perform
specific semantic tasks reported bergsma goebel use textual
image cues model selectional preferences verbs nouns likely
arguments verbs experiment shows several cases visual information
useful text task example looking textual corpora words
carillon migas mamey much useful information obtained guess
three plausible argument verb eat hand
http image net org



fimultimodal distributional semantics

exploiting google image search functionality enough images words
found vision model edible things classify correctly
finally evaluate multimodal task discovering color concrete objects showing relation words denoting concrete things
typical color better captured visual information taken account bruni
boleda baroni tran moreover multimodality helps distinguishing literal nonliteral uses color terms
multimodal fusion
textual information used image analysis mostly done different
aims text used improve image related tasks typically
attempt model relation specific images specific words textual passages
e g barnard duygulu forsyth de freitas blei jordan berg berg shih
farhadi hejrati sadeghi young rashtchian hockenmaier forsyth griffin
wahab newell kulkarni premraj dhar li choi berg berg
contrast want use image derived features improve representation word
meaning ii interested capturing meaning word types basis
sets images connected word model specific word image relations
despite differences challenges addressed image analysis literature deals exploiting textual cues similar ones face particular
merging fusing textual visual cues common representational
space exactly face construct multimodal semantic space
traditionally image analysis community distinguishes two classes fusion
schemes namely early fusion late fusion former fuses modalities feature space
latter fuses modalities semantic similarity space analogously call
feature level scoring level fusion respectively example escalante hrnadez
sucar montes propose image retrieval system multimodal documents
early late fusion strategies combination image textual
channels considered early fusion settings include weighted linear combination
two channels global strategy different retrieval systems used contemporarily
entire joint data set late fusion strategies include per modality strategy
documents retrieved one channel hierarchical setting
first text image combination used independently query database
aggregated four weighted combinations vreeswijk huurnink
smeulders train visual concept classifier abstract subject categories
biology history late fusion image text information
combined output level first obtaining classification scores image
text separately joining similarly multimodal mixing
step pham maillot lim chevallet caicedo ben abdallah gonzlez
nasraoui propose early fusion two inputs mapped onto
latent space dimensionality reduction techniques e g singular value decomposition
multimodal representation obtained way directly used retrieve image
documents
http images google com



fibruni tran baroni

framework multimodal distributional semantics
section general flexible architecture multimodal semantics presented
architecture makes use distributional semantic textual visual
information build multimodal representation meaning merge two sources
uses parameter pipeline able capture previously proposed combination
strategies advantage explored within single system
input multimodal architecture
construct multimodal representation meaning semantic model single
modality implemented independently actual parameters chosen
creation point view black box requirements
model satisfy order guarantee good functioning framework
first place modality must provide separate representation leave room
fusion strategies afterwards modality must encode semantic
information pertaining word interest fixed size vectorial representation
moreover assume text image vectors normalized arranged
matrices words rows co occurring elements columns
follows assume harvested matrix text semantic vectors
one image semantic vectors set target words representing
respectively verbal visual information words section give
details specific implementation construct matrices
multimodal fusion
pipeline two main steps
latent multimodal mixing text vision matrices concatenated obtaining single matrix whose row vectors projected onto single common space
make interact
multimodal similarity estimation information text image
matrices combined two ways obtain similarity estimates pairs target
words feature level scoring level
figure describes infrastructure propose fusion first introduce mixing
phase promote interaction modalities call latent multimodal mixing
step part approaches would consider feature level fusion see
keep separated might benefit scoring level fusion well
mixing performed proceed integrate textual visual features
reviewed section literature fusion performed two main levels
feature level scoring level first case features first combined
considered single input operations second case task performed separately
different sets features separate combined
advantages limitations incorporated
multimodal infrastructure together constitute call multimodal similarity


fimultimodal distributional semantics






































figure multimodal fusion combining textual visual information semantic
model



estimation feature level requires one learning step e determining
parameters feature vector combination offers richer vector representation
combined information used purposes e g image text
features could used together train classifier benefits scoring level
include possibility different representations principle even vectorial
different similarity scores different modalities ease increasing decreasing
number different modalities used representation
latent multimodal mixing
preparatory step textual visual components projected
onto common representation lower dimensionality discover correlated latent factors
connections made source matrix taking account
information connections present matrix originating patterns covariance overlap importantly assume mixing done via dimensionality
reduction technique following characteristics parameter k determines


fibruni tran baroni

dimensionality reduced space fact k equals rank
original matrix reduced matrix identical considered good approximation
original one commonly used singular value decomposition reduction method
adopt mixing step satisfies constraints
toy example mixing might beneficial consider concepts pizza
coin could use features text semantic vectors e record cooccurrences target words concepts part vector dimensions
words likely occur similar contexts text obviously visually similar original text features pizza coin might highly correlated
however mixing multimodal space might associated high
weights reduced space component similar distributions
visual features cue roundness consequently two textual features originally
uncorrelated might drawn closer multimodal mixing corresponding concepts visually similar resulting mixed textual features sense
visually enriched vice versa mixed visual features interestingly psychologists
shown certain conditions words pizza coin strongly
associated perceptually similar prime e g pecher zeelenberg raaijmakers
note matrices obtained splitting reduced rank matrix back
original textual visual blocks number feature columns original
textual visual blocks values smoothed dimensionality
reduction explain details achieved specific implementation
next paragraph matrices used calculate similarity score word
pair merging information feature scoring levels
mixing svd implementation perform mixing across text imagebased features applying singular value decomposition svd matrix obtained concatenating two feature types row wise row concatenated
matrix describes target word textual visual space svd widely used technique
best approximation original data points space lower underlying
dimensionality whose basis vectors principal components latent dimensions
selected capture much variance original space possible manning
raghavan schtze ch performing svd concatenated textual
visual matrices project two types information space
described linear combinations principal components following description
pham et al svd matrix rank r factorization form
u v




u matrix eigenvectors derived




r r diagonal matrix singular values

square roots eigenvalues







v matrix eigenvectors derived

computed svdlibc http tedlab mit edu dr svdlibc



fimultimodal distributional semantics

context matrix given normalizing two feature matrices separately
concatenating selecting k largest values matrix keeping
corresponding columns matrices u v reduced matrix mk given
mk uk k vkt
k r dimensionality latent space mk keeps number
columns dimensions rank k k free parameter tune
development sets note k equals rank original matrix trivially
mk thus consider performing svd reduction special case
svd helps searching optimal parameters
note n columns vkt k n matrix mk
number columns first j columns contain textual features columns
j n contain visual features hold mk although latter
values features affected global svd smoothing thus
current implementation pipeline figure block splitting attained simply
dividing mk textual mixed matrix containing first j columns visual mixed
matrix containing remaining columns
multimodal similarity estimation
similarity function following distributional hypothesis dsms describe word
terms contexts occurs therefore measure similarity two words
dsms need function capable determining similarity two descriptions e
two semantic vectors literature many different similarity functions
used compare two semantic vectors including cosine similarity euclidean distance l
norm jaccards coefficient jensen shannon divergence lins similarity extensive
evaluation different similarity measures see work weeds
focus cosine similarity since shown effective measure
many semantic benchmarks bullinaria levy pad lapata
given system geometric principles cosine together euclidean
distance principled choice measure similarity example
measures listed developed probabilistic considerations
applicable vectors encode well formed probability distributions typically
case example multimodal mixing vectors might contain negative
values
cosine two semantic vectors b dot product divided product
lengths
pi n

ai bi
qp
n
n



bi

cos b qp

cosine ranges orthogonal vectors parallel vectors pointing
opposite directions cosine values respectively


fibruni tran baroni

feature level fusion feature level fusion fl use linear weighted fusion
method combine text image feature vectors words single representation use latter estimate similarity pairs linear weighted
combination function defined
f ft fv
vector concatenate operator
scoring level fusion scoring level fusion sl text image matrices
used estimate similarity pairs independently scores combined obtain
final estimate linear weighted scoring function
st sv
general form special cases given fixed normalized text image
matrices multimodal parametrized k dimensionality latent space
fl vs sl weight text component fl similarity estimation weight text
component sl
note k r r rank original combined matrix latent multimodal
mixing returns original combined matrix actual mixing picking sl
corresponds textual visual matrix respectively thus derive
special cases text k r sl images k r sl
used called text image section simple
bruni et al two matrices concatenated without mixing
parametrization k r fl called naivefl model summing
leong mihalcea corresponds k r sl naivesl picking
k r sl amounts performing latent multimodal mixing textual
features reverse mixed image features textmixed
imagemixed respectively reducing parametrized
means given development set specific task requires similarity
measurements discover data driven way best
task hand example certain task might discover better
text another mixed text features yet another text image
features
formally given set k kn r n dimensionalities latent space kn
equal original dimensionality arbitrary steps chosen values
sets r potential weights text block fl
l r l weights text block sl l
calculate number possible configurations explore totc n l unless n
l large e consider small intervals values tested
completely feasible perform full search best parameters certain task
without approximate optimization methods experiments n l
consequently totc


fimultimodal distributional semantics

implementation details
implementation multimodal framework visual feature extraction
procedure publicly available open source moreover visual feature extraction
procedure presented bruni bordignon liska uijlings sergienya
construction text semantic matrix
reviewed section text distributional model encoded matrix whose rows semantic vectors representing meaning set target words
important parameters model choice target contextual elements
source corpora used extract co occurrence information context delimiting
scope co occurrence function transform raw counts statistical association scores downplaying impact frequent elements
source corpora collect co occurrence counts concatenation two corpora
ukwac wackypedia size b running words tokens respectively
ukwac collection web linguistically controlled crawl uk
domain conducted mid wackypedia built mid dump
english wikipedia corpora automatically annotated lemma dictionary form part speech pos category information treetagger
freely publicly available widely used linguistic
target context elements since source corpora annotated lemma
part speech information take account extracting target context
words e g string sang treated instance verb lemma sing collect
semantic vectors set k target words lemmas namely top k frequent
nouns k frequent adjectives k frequent verbs combined corpora
k lemmas employed contextual elements consequently textbased semantic encoded k k matrix note combine
text matrices image ones preserve rows target words
image vector trimming matrix size k
context define context terms words co occur within window fixed
width tradition popular hal model lund burgess window
attractive simplicity fact require resourceintensive advanced linguistic annotation moreover reported
state art semantic tasks rapp sahlgren bruni
uijlings baroni sebe window methods use
outperform document context model sophisticated syntax lexicalpattern model men wordsim test sets introduced section
see post hoc analysis document model discussed end
section consider two variants window window chose
particular variants arbitrarily representatives narrow wide windows respectively
see https github com fuse https github com vsem respectively
http www ims uni stuttgart de projekte corplex treetagger
http wacky sslmit unibo



fibruni tran baroni

window records sentence internal co occurrence nearest content words left
right target word function words articles prepositions ignored
window considers larger window content words left right target
narrower window expected capture narrower kind semantic similarity
one exists terms closely taxonomically related example coordinate
concepts dog cat pairs superordinate subordinate concepts animal
dog rationale behind expectation terms share many narrow window
collocates similar semantically syntactically
hand broader window capture broader kind topical similarity one
would expect words tend occur paragraphs example war oil
rather distant concepts taxonomic sense might easily occur
discourse see work sahlgren discussion effects context
width distributional semantic
association score transform raw co occurrence counts nonnegative local mutual information lmi association scores lmi scores obtained multiplying raw
counts pointwise mutual information nonnegative case close approximation log likelihood ratio scores one widely used weighting
schemes computational linguistics evert nonnegative lmi target element
context element c defined


p c

p p c



lm c max count c log

worth observing extensive study parameters affect quality
semantic vectors bullinaria levy bullinaria levy found
model similar window co occurrence statistics ukwac narrow window
lemmatized content word collocates nonnegative pointwise mutual information instead
lmi performs near top variety semantic tasks thus independent
grounds claim state art text model
construction image semantic matrix
given image semantic vectors novelty respect text ones
next subsections dedicate space constructed including
full details source corpus utilize input pipeline section
particular image analysis technique choose extract visual collocates finally
arrange semantic vectors constitute visual block distributional
semantic matrix section
image source corpus
adopt source corpus esp game data set contains k images labeled
famous game purpose developed louis von ahn two
http www cs cmu edu biglou resources



fimultimodal distributional semantics


fffi fiff
fifi fffi
fifi

fffi
fififf







fififffi

fifi


fifi






figure
samples images tags esp game data set

people partnered online must independently rapidly agree appropriate word
label random selected images word entered partners certain number
game rounds word added tag image becomes taboo term
next rounds game involving image encourage players produce
terms describing image von ahn tags images data set form
vocabulary distinct word types images tags average standard
deviation word tag images average standard deviation
words format text tags lemmatized pos tagged annotate words parts speech could
run pos tagger since words context e tag appears alphabetically
within small list words labeling image within ordinary sentence
required pos tagger thus used heuristic method assigned words
esp game vocabulary frequent tag textual corpora
esp game corpus interesting data set point view since
one hand rather large know tags contains related images
hand product experts labelling representative images
noisy annotation process often poor quality uninteresting images e g logos randomly
downloaded web thus analogously characteristics textual corpus
must able exploit large scale statistical information robust
noise cleaner illustrative examples concept available
carefully constructed databases imagenet see section noisy tag annotations


fibruni tran baroni

available massive scale sites flickr facebook want
eventually exploit data important methods work noisy input
advantage esp game respect imagenet images associated
concrete noun categories adjectives verbs nouns related
events e g vacation party travel etc practical point view clean
data sets imagenet still relatively small making experimentation standard
benchmarks difficult concrete looking benchmarks experiment mid
imagenet covers half pairs wordsim test set less
almuhareb poesio words future want explore
extent higher quality data sources improve image require larger
databases benchmarks relying restricted vocabulary
image samples figure exemplify different kinds noise characterize
esp game data set top bottom left top right images
scene cluttered partially occluded top center image hardly good representative accompanying words building tower square similarly center
bottom image partially good illustration coin certainly good
example man finally bottom right image useless visual feature extraction
perspective
image semantic vector construction
collect co occurrence counts target words image contexts adopting
bovw pipeline already explained particularly convenient order
discretize visual information visual collocates adopting currently
considered standard implementation bovw future could explore
cutting edge ways build image semantic vectors local linear encoding
wang yang yu lv huang gong fisher encoding perronnin sanchez
mensink chatfield lempitsky vedaldi zisserman present systematic
evaluation several recent methods
current implementation composed following steps extraction
local descriptors vectors low level features encode geometric
information area around keypoint e pixel interest sift descriptors ii constructing vector representation image assigning
local descriptors clusters corresponding visual words recording distribution
across clusters vector presupposes preliminary step clustering
applied whole image collection sample determine visual word vocabulary iii including spatial information representation
spatial binning iv summing visual word occurrences across list images associated
word label obtain co occurrence counts associated word label
transforming counts association scores analogously done text
analysis process without spatial binning schematically illustrated figure
hypothetical example three images collection labeled
word monkey details follow
http www flickr com
http www facebook com



fimultimodal distributional semantics

dense sampling
pixels
interest

mapping sift
descriptors visual
word clusters

extracting
local
descriptors

sift x

monkey





monkey











labeled
images

monkey





instance
counts



monkey
monkey









monkey









monkey
total
counts

figure procedure build image semantic vector target word first
bag visual word representation image labeled target word
computed case three images labeled target word monkey
visual word occurrences across instance counts summed obtain
co occurrence counts associated target word

local descriptors construct local descriptors pixels interest use scaleinvariant feature transform sift lowe chose sift invariance
image scale orientation noise distortion partial invariance illumination changes
sift vector formed measuring local image gradients region around
location orientation feature multiple scales particular contents
sampling subregions explored around keypoint resulting
samples magnitude gradients orientations calculated would
already sift feature vector components however extract color
sift descriptors hsv hue saturation value space bosch zisserman munoz
use hsv encodes color information similar way humans


fibruni tran baroni

compute sift descriptors hsv component gives dimensions
per descriptor per channel color channels averaged obtain final
dimensional descriptors experimented different color scales
luv lab rgb obtaining significantly worse performance compared hsv
development set introduced therefore conduct experiments
van de sande gevers snoek present systematic evaluation color
features
instead searching interesting keypoints salient patch detection
use computationally intensive thorough dense keypoint sampling
patches fixed size localized regular grid covering whole image
repeated multiple scales sift descriptors computed regular grid every
five pixels four scales pixel radii zeroing low contrast descriptors
extraction use vl phow command included vlfeat toolbox vedaldi
fulkerson implementation shown close lowes original
much faster dense feature extraction nowak jurie triggs report
systematic evaluation different patch sampling strategies
importantly sift feature vectors extracted large corpus representative
images populate feature space subsequently quantized discrete number
visual words clustering step performed every sift vector local descriptor
original images translated visual word determining
cluster nearest quantized space
visual vocabulary map sift descriptors visual words first cluster local
descriptors extracted images training image corpus dimensional
space k means clustering encode descriptor index
cluster visual word belongs k means common way constructing
visual vocabularies grauman leibe given set x xn rd n training
descriptors k means aims partition n descriptors k sets k n minimize
p
cumulative approximation error ni xi qi k centroids k rd
data means assignments q qn k use approximated version
k means called lloyds implemented vlfeat toolbox
construct visual vocabulary extracted sift descriptors k
images esp game data set tune parameter k used men development
set see section varying k steps found
optimal k likely performance peaked even
visual words enhancements could attained adopting larger visual vocabularies
via efficient implementations bovw pipeline example chatfield et al

image representation given set descriptors x xn sampled image let
qi assignment descriptor xi corresponding visual word bag ofvisual words representation image nonnegative vector v rk vk
qi k q ranging number visual words vocabulary
case representation vector visual words obtained via hard quantization
e assignment local descriptor vector single nearest codeword


fimultimodal distributional semantics

spatial binning consolidated way introducing weak geometry bovw use
spatial histograms grauman darrell lazebnik schmid ponce
main idea divide image several spatial regions perform entire visual
word extraction counting pipeline region concatenate vectors
experiments spatial regions obtained dividing image total
regions therefore crossing values k spatial region increase
feature dimensions times total components vectors
co occurrence counts weighting bovw representations built
target textual word associated list images labeled
visual word occurrences across list images summed obtain co occurrence
counts associated target textual word total target words
constitute esp game tags image semantic vector associated
image semantic matrix text one raw counts
transformed nonnegative lmi difference lmi computed
target element textual word context element c visual word instead
note standard textual accumulating visual words
images contain word without taking account fact words might
denote concepts multiple appearances polysemous even hide homonyms
bank vector include visual words extracted river well building pictures
interesting direction would cluster images associated
word order distinguish visual senses word e g along lines
done textual reisinger mooney
multimodal fusion tuning
performed two separate parameter optimizations one specifically semantic relatedness task men development see section specifically
clustering task battig see section determined best model
performing exhaustive search across svd k powers fl sl
varying inclusive steps similarly total explored one highest performance development data
chosen note tuning performed separately window window

mixlda
reimplement feng lapatas discussed section comparable
setting treat esp game data set mixed media corpus
image together associated tags constitutes document image extract
image features procedure described use words
labeling image obtain text features features stored
term document matrix image treated document term
textual tag visual word extracted image obtain matrix size
k k k textual words word list resulting intersection
words used experimental data sets k visual words k documents images


fibruni tran baroni

latent dirichlet allocation mixlda model trained matrix tuned
men development set varying number topics kt optimal value
kt mixlda target word evaluation set represented
vector giving distribution latent topics

experiments
test semantic representation three different tasks evaluating distribution different kinds semantic relations among words neighbours modeling
word relatedness judgments clustering words superordinate concepts
together tasks give us clear idea general quality
relative contribution visual information meaning representation
differentiation semantic relations
acquire qualitative insight well text image capturing word meaning test bless baroni lenci evaluation semantic similarity benchmark recently introduced baroni lenci analyze specific
aspects lexico semantic knowledge rather focusing point estimate quality
model specific semantic task bless allows us assess overall pattern
semantic relations model tends capture run bless evaluation
combining textual visual channels together sanity check semantic
meaningfulness image vectors looking potential complementary information respect text motivate fusion note since
combining textual visual sources tuning parameters report
benchmark method
bless contains set pivot words denoting concrete concepts use pivots
since remaining sufficiently large set related words covered
pivots data set contains number related words
relata instantiating following common semantic relations pivots coord
relatum noun co hyponym coordinate pivot alligator lizard
hyper relatum noun hypernym superordinate pivot alligatorreptile mero relatum noun referring meronym part material
pivot alligator teeth attri relatum adjective expressing attribute
pivot alligator ferocious event relatum verb referring action
event involving concept alligator swim ran n ran j ran v finally control
cases pivot matched set random nouns alligator trombone adjectives
alligator electronic verbs alligator conclude respectively
pivot bless contains set relata category ranging hypernyms random nouns per pivot average way bless highlight
broader semantic properties model independently specific preferences
example model assigns high score alligator ferocious model
assigns high score alligator green correctly treated picked
lda computed gensim http radimrehurek com gensim



fimultimodal distributional semantics

relevant attribute alligators time comparison specific relata
selected allows granular qualitative analysis differences
following guidelines baroni lenci analyze semantic model
follows compute cosine model vectors representing
pivots relata picking relatum highest cosine
relations nearest hypernym nearest random noun etc transform
similarity scores collected way pivot onto standardized z scores
get rid pivot specific effects produce boxplot summarizing distribution
scores per relation across pivots example leftmost box first panel
figure reports distribution standardized cosines nearest coordinate relata
respective pivots besides analyzing distributions qualitatively discuss
significant differences cosines different relation types obtained via
tukeys honestly significance tests thus correcting multiple pairwise comparisons abdi
williams

fig report bless nearest relata distributions purely textual model window window distribution shows even stronger skew favour coordinate
neighbours purely visual model call image next sections patterns
produced text model left panel illustrate sensible word meaning profile
look coordinates similar terms alligator maximally similar
crocodile followed superordinates reptile parts teeth semantically related
adjectives attri ferocious verbs event swim less close pivots still
random item
right panel shows distribution relata image semantic vectors
overall pattern quite similar one observed text vectors
clear preference coordinates followed hypernyms parts attributes
events random relata away pivots semantically
meaningful categories coordinates significantly closer relata
hypernyms meronyms significantly closer attributes events
turn significantly closer random category although difference
hypernyms parts significant representation intriguingly
image vectors slight preference imageable parts teeth
abstract hypernyms reptile difference statistical import one
events attributes text model shows significant preference
events whereas two categories statistically indistinguishable image
model see shortly relative preference latter attributes probably
due tendency pick perceptual adjectives denoting color size
looking closely specific relata picked text image
striking differences pertain attributes text image
picked attribute pivot cases compare
overlap across non random relation types table reports attributes picked
text vs image random cases two mismatch


fibruni tran baroni

image semantic vectors





















text semantic vectors

coord

hyper

mero

attri

event

ran n

ran j

ran v

coord

hyper

mero

attri

event

ran n

ran j

ran v

figure distribution z normalized cosines words instantiating relations across
bless pivots text vectors window model

pivot
cabbage
carrot
cherry
deer
dishwasher
elephant
glider
gorilla
hat
hatchet

text
leafy
fresh
ripe
wild
electric
wild
heavy
wild
white
sharp

image
white
orange
red
brown
white
white
white
black
old
short

pivot
helicopter
onion
oven
plum
sofa
sparrow
stove
tanker
toaster
trout

text
heavy
fresh
electric
juicy
comfortable
wild
electric
heavy
electric
fresh

image
old
white

red
old
little
hot
grey

old

table attributes preferred text window vs image



fimultimodal distributional semantics

immediately clear table despite fact pivots nouns
denoting concrete concepts text model almost never picks adjectives denoting
salient perceptual properties particular visual properties white hat leafy
cabbage text model focuses instead encyclopedic properties fresh
ripe wild electric comfortable line earlier analyses ungrounded
semantics provided text andrews et al baroni et al baroni
lenci riordan jones differs greatly trend found
image model cases closest attribute latter model color
remaining cases size short little one instance hot surprisingly four
old
conclude analysis presented confirms one hand hypothesis
image distributional vectors contain sufficient information capture network
sensible word meaning relations intriguing differences relations picked text image pointing complementarity
word relatedness
standard distributional semantics literature budanitsky hirst sahlgren
assess performance task predicting degree semantic relatedness two words rated human judges test
ws men benchmarks
benchmarks method
ws wordsim see section widely used benchmark constructed
asking subjects rate set word pairs point meaning similarity scale
averaging ratings e g dollar buck gets high average rating professor cucumber
low one target words cover ws pairs thus correlations reported
directly comparable reported studies used ws however
text much higher ws coverage evaluated larger ws
set cover window window achieve correlations respectively
thus comparing multimodal purely textual
state art ws see reported section
second benchmark use men marco elia nam resource creators
developed us specifically purpose testing multimodal created
large data set comparable ws benchmarks commonly used
computational semantics community contains words appear image
labels esp game mirflickr collections thus ensuring full coverage
researchers train visual resources men consists word pairs
normalized semantic relatedness ratings provided amazon mechanical turk
workers via crowdflower interface example beach sand men score
bakery zebra received score
http www cs technion ac il gabr resources data wordsim
http press liacs nl mirflickr
http crowdflower com



fibruni tran baroni

compared ws men sufficiently large allow us separate development
test data avoiding issues overfitting use indeed men pairs development set
model tuning pairs evaluation test set importantly development
set used best configuration men test set ws
thus ws evaluation illustrates well parameters learned training data
specific data set generalize applied semantic task different
data set
evaluated follows pair data set compute cosine
model vectors representing words pair calculate spearman
correlation cosines pooled human ratings pairs idea
higher correlation better model simulate relatedness
scores
men construction earlier version men used first time
authors bruni et al since current article first major publication
focus specifically recently improved benchmark
extending ratings provide details constructed
word pairs constitute men randomly selected words occur
least times concatenated ukwac wackypedia text corpora least
times tags esp game mirflickr tagged image collections order
avoid picking pairs weakly related would happen sample
random word pairs list ranked possible pairs cosines according
text model window gather word pairs needed construction
men subsequently picked first word pairs another sampled
pairs placed cosine ranked list last block pairs
remaining items
acquire human semantic relatedness judgments decided ask comparative
judgments two pair exemplars time rather absolute scores single pairs
done creators ws constitute natural way evaluate
target pairs since human judgments comparative nature person evaluates
given target vacuum relation certain context
moreover binary choices preferred make construction right
wrong control items straightforward see footnote operationally word pair
randomly matched comparison pair coming set items
rated single turker less related comparison item
validity confirmed high annotation accuracy observe
control set high correlation men scores ratings collected
likert scale report
control items correct annotations created prior running job amazon mechanical turk
act hidden tests randomly shown turkers complete job way
calculate quality contributors performance reject annotations accuracy
drops certain percentage set required minimum precision equal obtained
almost average accuracy overall control items great help train quickly workers
perform required task create control items harvested two equally sized sets word
pairs ws one containing pairs high relatedness score one containing pairs
low relatedness score control item obtained juxtaposing high score pair
low score pair treating pair higher score one selected



fimultimodal distributional semantics

instructions annotators warned sometimes candidate pairs could
contain words related meaning cases asked pick pair
strongly related words e g wheels car dog race somewhat related pairs
first one preferred every car wheels every dog involved
race cases annotators could neither pair contains closely related
words cases instructed pick pair contained slightly
related words e g neither auction car cup asphalt closely related words
first pair picked fancy vintage cars sold auctions requested
participants native speakers accepted connecting english
speaking country cannot guarantee non natives take part study
subject filtering techniques control pairs see footnote ensures
data speakers good command english retained
transform binary preference data relatedness scores retrieved pairs
evaluated randomly picked comparison pairs thus received score
point scale given number times pair picked
related two score subsequently normalized dividing
number times pair picked related example fun night
chosen related comparison pair times thus normalized score
given note comparison recorded preference
assigned one two pairs avoid dependencies final scores assigned
different pairs times pair selected random comparison item
another pair counted ratings pair
raters saw men pairs matched different random items number
pairs varying rater rater possible compute annotator agreement
scores men however get sense human agreement first third author
rated pairs presented different random orders standard likert scale
spearman correlation two authors correlation average
ratings men scores one hand high correlation suggests
men contains meaningful semantic ratings taken
upper bound computational realistically achieve simulating
human men judgments
high score men pairs include pairs terms strictly taxonomically close cathedral church terms connected broader semantic
relations whole part flower petal item related event boat fishing
etc reason prefer refer men semantic relatedness rather
similarity score data set note ws capturing broader notion relatedness agirre et al men publicly available downloaded
http clic cimec unitn elia bruni men

table reports correlations men testing ws data sets
window window textual model automated tuning method selected k
annotators related control items manually checked examples control items
hotel word vs psychology depression telephone communication vs face locomotive



fibruni tran baroni

model
text
image
naivefl
naivesl
mixlda
textmixed
imagemixed
tunedfl
tunedsl

window
men ws
















window
men ws
















table spearman correlation men wordsim coefficients significant p tunedfl model selected automatically men
development data tunedsl automatically tuned fixing sl similarity estimation

textual information comes window k window optimal
feature level fl similarity estimation cases since input
matrices row normalized latter setting assigns equal weights textual
visual components called tunedfl table scoring level
sl strategy similar weights assigned two channels k values
tunedfl performed slightly worse tunedfl report
best sl tuned development men data well tunedsl
reported table naivefl naivesl mixlda textmixed imagemixed
parameters tuned manually order gain insights combination strategies
representing ideas earlier literature
first two rows table text image
mixing text shows comparable performances data sets image correlates
significantly better men ws correlations lower text
accordance found earlier studies next three rows
earlier multimodal approaches took consideration bruni et al
feng lapata leong mihalcea naivefl analogous
bruni et al method textual visual matrices concatenated without
mixing performs slightly better text men attains lower performance ws
naivesl equivalent leong mihalceas summing text
image sources combined scoring level obtains improvements men loosing
several correlation points ws compared text
implementation mixlda achieves poor men ws one
might attribute fact feng lapatas constrained
source textual visual model image data set poor source
textmixed imagemixed best k values found development data
set textual sources



fimultimodal distributional semantics

textmixed
tunedfl
tunedsl

window




window




table pearson correlation best multimodal combinations wordsim
subset covered feng lapata coefficients significant p
pearson used instead spearman full comparability feng
lapata assigned similarity pairs
missing vector feng lapata report correlation mixlda

textual data however outperforming original mixlda
large margin latter ws test set strongly disfavoured particular
feng lapata report correlation subset ws pairs covered
model tested system subset despite fact
missing one vectors pairs almost one third
forced assign cosines cases despite huge handicap
still attaining much higher correlations original mixlda feng lapata
pairs illustrated interesting fusion strategies table
analyzing effects fusion strategies first see uniform enhancement men ws textmixed imagemixed obtained first
performing latent multimodal mixing combined matrix textual features textmixed visual features imagemixed textmixed reaches best
performance overall ws source textual significantly better
text men according two tailed paired permutation test moore mccabe
looking automatically selected tunedfl model reaches best performance overall significantly outperforms text data sets
significantly better textmixed men window difference approaching significance window well p tunedsl competitive
significantly better text window sizes textmixed window
noticeably worse tunedfl ws window actually
slight advantage men window difference tunedfl tunedsl
never significant
worth remarking textmixed bit worse full fusion
still achieves high correlations human judgments extremely high
correlation tunedfl best model suggests
benefits multimodality already captured latent mixing textmixed attractive
model less parameters whole pipeline compact
tunedfl since discards visual features mixing
validating shown significant improvements visual features added distributional one could object improvements due
fact information larger number features higher dimensional


fibruni tran baroni

vectors feature level fusion complex model two similarity scores
independent variables predict human judgments scoring level fusion experiments provide evidence respond objection
first built purely textual number features multimodal
instead collecting co occurrence target terms k
frequent content lemmas corpus see section extended list
context items k frequent content lemmas larger
textual virtually identical k dimensional vectors reported
table correlation window model men instead thus
least large corpus window k features
pretty much exhausted useful textual information nature simply
quantity extra visual features add matters
answer objection scoring level complex model
two independent variables text image base similarities instead one casted
standard inferential statistical terms see e g baayen ch specifically fitted ordinary linear regression predict men ws ratings
text similarities vs text image similarities comparability
spearman correlation reported analyses replicated
transforming ratings similarities ranks variables highly significant
experiments importantly sequential f tests nested revealed
cases adding image similarities explains significantly variance
would expected chance given extra parameter p
qualitative analysis acquire qualitative insights multimodality contributing meaning representation first picked top related pairs
combined men ws norms would confident indeed highly
related pairs humans looked within subset pairs
pronounced difference cosines text tunedfl window
textual source first column table presents pairs considered
related humans relatedness better captured text second column
pairs relatedness better captured tunedfl
notice relations better captured tunedfl coordinates
synonyms pertaining concrete objects candy chocolate bicycle bike apple cherry
military soldier paws whiskers stream waterfall cheetah lion indeed
maximally visually similar objects case paws whiskers
surrounds purely text model hand captures relations
times day imageable well delimited concrete objects
dawn dusk sunrise sunset captures properties concepts expressed adjectives
dog canine skyscraper tall cat feline pregnancy pregnant rain misty least one
case spotting relation requires encyclopedic knowledge grape wine thus
hypothesize added value multimodally enhanced model derives
power vision finding relations concrete objects taxonomic level
detecting particularly tight forms relatedness synonymy
coordination


fimultimodal distributional semantics

text
dawn dusk
sunrise sunset
canine dog
grape wine
foliage plant
foliage petal
skyscraper tall
cat feline
pregnancy pregnant
misty rain

tunedfl
pet puppy
candy chocolate
paw pet
bicycle bike
apple cherry
copper metal
military soldier
paws whiskers
stream waterfall
cheetah lion

table top pairs whose relatedness better captured text window
vs tunedfl

observed one reviewer given taxonomic nature information captured
multimodal interesting compare future work features
directly extracted linguistic taxonomy wordnet observe passing
manually constructed resource unlike extracted textual corpora likely
reflect linguistic perceptual knowledge lexicographers built

going opposite direction another reviewer observed might get
mileage combining visual features textual less taxonomic nature
hypothesis partially confirmed fact obtain larger relative improvement mixing vision window window look back table see
section think narrower window mainly captures taxonomic
relations larger one broader topical themes explore conjecture
ran men ws experiments combining visual vectors document
textual model e semantic space whose dimensions record number occurrences
words documents space expected capture mostly topical information
estimates relatedness basis tendency words occur
documents sahlgren document model alone good
window obtained spearman correlation men
ws combining image led relative improvements comparable
inferior attained window best combined model correlations
men ws conclude looking textual
complementary respect visual information seems reasonable direction
develop multimodal systems cover broader range semantic phenomena simply
emphasizing topical side textual evidently suffice


fibruni tran baroni

concreteness factor modeling relatedness ratings pilot
study
previous experiments observed trend towards division labour text image latter apt capturing similarity
among concrete concepts properties one strongest limitations current
version framework fact every target word assumed equally perceptually salient consequently uniformly enriched visual information intuitively
might want distinguish instead concrete words chair cat require
integration perceptual information representation abstract words
consequence absurd represented purely symbolic linguistic basis
indeed recchia jones recently presented evidence lexical decision
naming tasks rich physical contexts favour activation concrete concepts whereas rich
linguistic contexts facilitate activation abstract concepts follow pilot
experiment presented section want pave way systematic introduction concreteness factor multimodal meaning representation operationally
separate abstract concrete word pairs semantic relatedness benchmark
men assessing contribution textual visual information approximating word
meaning two domains independently importantly use automated method
determine word concrete abstract eye future integration
automatically determined abstractness score fusion
particular use abstractness scores automatically assigned recently introduced turney neuman assaf cohen scores calculated
computing difference sum text semantic similarities target
word set concrete paradigm words sum semantic similarities
set abstract paradigm words words e paradigm words words
abstractness score computed represented co occurrence
matrix gathered large corpus university websites co occurrence counts
transformed positive pointwise mutual information scores church hanks
resulting matrix smoothed svd pairwise semantic similarity measured
cosines paradigm words turn selected supervised learning method
trained subject rated words mrc psycholinguistic database machine usable
dictionary coltheart examples highly abstract words automatically rated
list purvey sense improbable examples highly concrete
words e words low abstractness score donut bullet shoe

abstractness score assigned men testing words divided
data set two subsets one containing concrete word pairs men conc
pairs containing abstract pairs mixed pairs pairs formed one
concrete one abstract word men abst pairs word considered concrete
abstract score abstract otherwise example word pair arm bicycle
considered concrete scores respectively fun relax considered
abstract scores respectively design orange considered mixed
scores respectively experimented window purely


fimultimodal distributional semantics

model
window
image
tunedfl

men conc




men abst




men full




table spearman correlation men divided concrete abstract
subsets full data set repeated coefficients significant
p

textual model image usual visual model tunedfl trained men development
multimodal model
table correlation scores three two men subsets
well repeating correlations attain full set first worth
noticing higher correlations men conc men abst suggesting
approximating similarity judgments pairs concrete pairs general easier
task distributional semantics suspect humans well besides broad
effect observe clear interaction added value visual component
men abst men conc fact tunedfl gains performance
men conc compared window performance essentially
text model case men abst indicates visual information
mostly beneficial concrete domain maintains neutral timidly positive
impact abstract domain recall case men abst contains mixed
pairs
conclude section followed qualitative analysis main
relatedness pilot experiment focusing concreteness factor showed
divide men benchmark concrete abstract subsets visual
information enhances text model concrete domain impact
strong exploited automatic scoring function divide data set
concrete abstract subsets thus see reporting
validation turney et al importantly purposes
encouragement incorporate automated abstractness concreteness scoring way
model mixes textual visual information word word basis
concept categorization
verify conclusions reached ws men extend different semantic tasks
particular assess whether multimodal able capture organize
meaning humans use two existing concept categorization benchmarks
call battig almuhareb poesio ap respectively goal cluster set
nominal concepts broader categories already discussed section
particular use battig exclusively tuning way used men
development set previous section ap testing ap
reported word relatedness task tuning testing sets quite similar


fibruni tran baroni

men development men testing two subsets data set words
ws similar men task challenging since battig ap
two independent data sets built following different strategies populated
different kinds concepts namely concrete unambiguous concepts battig
vs mixture concrete abstract possibly ambiguous concepts ap adopted
present challenging training testing regime felt neither data set
sufficient size allow split development testing data details follow
benchmarks method
battig benchmark introduced baroni et al battig
montague norms van overschelde rawson dunlosky consists
highly prototypical concepts common concrete categories concepts
per class battig contains basic level concepts belonging categories bird eagle
owl kitchenware bowl spoon vegetable broccoli potato version
cover concepts different classes
ap introduced almuhareb poesio made nouns
different wordnet classes version cover ap contains concepts
clustered classes vehicle airplane car time aeon future social
unit brigade nation data set contains many difficult cases unusual ambiguous
instances class casuarina samba trees
sets following original proponents others cluster words
pairwise cosines semantic space defined model cluto toolkit
karypis use clutos built repeated bisections global optimization
method accepting clutos default values cluster quality often evaluated
percentage purity zhao karypis nir number items th true
gold standard class assigned r th cluster n total number items
k number clusters
purity

x
n
max nri
n

words number items belonging majority true class e represented
class cluster summed across clusters divided total number items
best scenario purity cluster quality deteriorates
since lack full ap coverage report directly comparable
studies used however text perfect coverage
evaluated full set achieve purities window window
state art levels comparable reported section
confidently claim improvements achieved multimodality
obtained comparing competitive purely textual

table reports percentage purities ap clustering task best automatically selected model tunedfl uses fl similarity estimation previous task
similar svd k window window parameters


fimultimodal distributional semantics

model
text
image
naivefl
naivesl
mixlda
textmixed
imagemixed
tunedfl
tunedsl

window
ap










window
ap










table percentage purities ap tunedfl model automatically
selected battig data tunedsl automatically tuned fixing sl similarity estimation

ones found relatedness suggesting particular parameter choice robust
could used box tasks well tunedsl best sl method
tuning battig set ks tunedfl window
window
analogously previous semantic task see image model alone
level text although ap purities significantly chance
p simulated distributions random cluster assignment thus
confirmation fact image vectors capture important aspects
meaning previous task mixlda achieves poor
looking text enhanced visual information see general
improvement performance almost multimodal combination strategies except
naivefl window naivesl window even textmixed benefits
visual smoothing cases outperformed tunedfl whose performance
similar tunedsl actually slightly better window interestingly tunedsl outperforms text window despite fact single combination
strongly unbalanced towards textual similarity indicating visual information beneficial even textual information accounts lions share
composed estimate
relatedness task adding equal amount textual features instead
image ones help window purity k textual features
even lowers performance window purity thus improvement brought
visual features must attributed quality quantity
according two tailed permutation test even largest difference tunedfl
text window significant might due brittleness purity
statistics leading high variance permutations possibly suboptimal tuning
recall respect tuning phase performed rather different data
set battig compared data set eventually evaluated ap


fibruni tran baroni

however overall trends encouraging line found
relatedness study

conclusion
provided extensive introduction distributional semantics named multimodal distributional semantics multimodal
distributional semantic model integrates traditional text representation meaning
information coming vision way tries answer critique distributional lack grounding since base representation meaning entirely
linguistic input neglecting statistical information inherent perceptual experience
humans instead exploit course truly multimodal representation meaning
account entire spectrum human senses hand line
still embryonic stage still shortage perceptual data
available techniques automatize processing article
focused analysis visual perceptual channel disposal
large data sets effective methods analyze
particular exploited esp game data set image documents
tagged words describing content harvest visual information adopted
bag visual words technique discretizes image content ways analogous
standard text distributional representations introduced multimodal framework
optimizes text image fusion data driven fashion development data
conducted number experiments assess quality obtained
first investigated general semantic properties purely image model
assess overall quality well look information complementary present
text found systematic differences two modalities preference
encyclopedic properties text model perceptual properties case
image model proceeded test selection obtained combination
text image representations via multimodal framework used two
benchmarks word relatedness one benchmark word categorization
cases obtained systematic improvement performance multimodal
compared standalone channels
still looking numerical cannot deny improvement performance attained including visual information dramatic indeed pessimistic
interpretation experiments could confirm hypothesis louwerse
others e g louwerse louwerse connell tillman datla hutchinson
louwerse perceptual information already encoded sufficient degree
linguistic data direct visual features dont bring much table however showed
statistical validation tests important namely
adding visual information improves text alone robust reliable think
realistic take home message experiments reported establishing
basic mentioned drawbacks overcome
work
first deliberately used general semantic benchmarks state art text
performance computational methods might getting close


fimultimodal distributional semantics

ceiling correlation best still percentage points go
men estimated upper bound raters agreement see section
improvements bound quite small concerning ap benchmark consider
difficult would even humans categorize casuarina samba among
trees indeed error analysis tunedfl clustering suggests factors
might lead better performance little vision example
model wrongly clusters branch social unit according ap trees merges
concepts melon peach fruit ap mandarin lime trees lack
contextual information hard dispute model choices similarly tunedfl
splits ap animal class cluster small domestic mammals cats dogs kittens
mice puppies rats cluster containing everything else mostly larger mammals
cows elephants clustering procedure information
classes searching e g animals general small animals
hard see performance could improved thanks better semantic features visual
kinds moreover data sets include abstract terms specifically
designed test grounded aspects meaning visual features might help
think made sense start investigation general benchmarks
semantics opposed ad hoc test sets viability multimodal
however future want focus experimental challenges strengths
visually enhanced might emerge clearly took first step direction
bruni et al focused specifically visual features help
processing literal metaphorical colours
another factor take account large scale image data sets
techniques extract features infancy might able
improve performance developing better image regarding data
sets explained section chose esp game obviously
sub optimal many respects discuss regarding features
mentioned beginning section recent advances image processing
fisher encoding might lead better ways extract information contained images
experiments compared automatically tuned multimodal model
settings showing overall stability superiority two important caveats
first experiments good already obtained visual information
smooth text features without visual features directly called
textmixed note already multimodal visual
information crucially used improve quality textual dimensions indeed
weve seen consistently outperforms non multimodally smoothed text features
textmixed good full tuned model simplicity makes
attractive
second although automated tuning led us prefer feature level scoring level
fusion development sets tunedsl clearly worse tunedfl one case
window ws suggesting least evaluation settings considered
difference two fusion strategies crucial however comparing
naive versions strategies tuned ones across clear
tuning important obtain consistently good performance confirming usefulness
general fusion architecture


fibruni tran baroni

conducted pilot experiment concreteness abstractness factor assess
impact meaning representation check good candidate
weighted fusion strategy plan investigate future fact current version
multimodal framework parametrization combination strategy works
global level e words could productive combine textual
visual information word word basis tune two modality contributions
meaning representation depending particular nature single word concrete
vs abstract constitute neat binary distinction words rather
thought ideal distinction offset less abrupt real world formulation
takes account degree according certain word considered concrete
abstract doubt words backdrop squalor sharp evoke
perceptual cues gathered experience time
unequivocal amount abstractness accompanying plan refine
concreteness scoring method order make focus specifically imageable
components concreteness expect relevant visual channel
developments focus techniques extract image semantic
example pilot study bruni et al exploit methods developed
computer vision improve object recognition capturing object location felzenszwalb
girshick mcallester deva ramanan de sande uijlings gevers smeulders
possible extract better image semantic vectors first
localizing objects denoted words extracting visual information
object location surround independently interestingly discovered
image semantic vectors extracted object surround effective
object location tested word relatedness task example
fact pictures containing deers wolves depict similar surrounds tells us
creatures live similar environments thus likely somewhat
related seen distributional hypothesis transposed images objects
semantically similar occur similar visual contexts nevertheless work
considered proof concept since experimented words future
studies test larger number words
obviously much room improvement many exciting routes
explore hope framework empirical presented study
convinced reader multimodal distributional semantics promising avenue
pursue development human meaning

acknowledgments
thank jasper uijlings valuable suggestions image analysis pipeline
lot code many ideas came giang binh tran owe gemma boleda many
ideas useful comments peter turney kindly shared abstractness score list
used section yair neuman generously helped preliminary analysis
impact abstractness multimodal mirella lapata kindly made
wordsim set used experiments feng lapata available us
thank jair associated editor reviewers helpful suggestions constructive


fimultimodal distributional semantics

criticism google partially funded project google award third
author bless study section first presented bruni et al

references
abdi h williams l newman keuls tukey test salkind n frey b
dougherty eds encyclopedia design pp sage thousand
oaks ca
agirre e alfonseca e hall k kravalova j pasa soroa study
similarity relatedness distributional wordnet approaches
proceedings hlt naacl pp boulder co
almuhareb poesio concept learning categorization web
proceedings cogsci pp stresa italy
andrews vigliocco g vinson integrating experiential distributional
data learn semantic representations psychological review
baayen h analyzing linguistic data practical introduction statistics
r cambridge university press cambridge uk
barnard k duygulu p forsyth de freitas n blei jordan matching words pictures journal machine learning
baroni barbu e murphy b poesio strudel distributional semantic
model properties types cognitive science
baroni lenci concepts properties word spaces italian journal
linguistics
baroni lenci distributional memory general framework corpusbased semantics computational linguistics
baroni lenci blessed distributional semantic evaluation
proceedings emnlp gems workshop pp edinburgh uk
barsalou l grounded cognition annual review psychology
berg berg shih j automatic attribute discovery characterization
noisy web data eccv pp crete greece
bergsma goebel r visual information predict lexical preference
proceedings ranlp pp hissar bulgaria
blei ng jordan latent dirichlet allocation journal
machine learning
bosch zisserman munoz x image classification random forests
ferns proceedings iccv pp rio de janeiro brazil
bosch zisserman munoz x scene classification hybrid generative discriminative ieee transactions pattern analysis machine
intelligence
bruni e boleda g baroni tran n k distributional semantics
technicolor proceedings acl pp jeju island korea


fibruni tran baroni

bruni e bordignon u liska uijlings j sergienya vsem open
library visual semantics representation proceedings acl sofia bulgaria
bruni e tran g b baroni distributional semantics text images
proceedings emnlp gems workshop pp edinburgh uk
bruni e uijlings j baroni sebe n distributional semantics eyes
image analysis improve computational representations word meaning
proceedings acm multimedia pp nara japan
budanitsky hirst g evaluating wordnet measures lexical semantic
relatedness computational linguistics
bullinaria j levy j extracting semantic representations word cooccurrence statistics computational study behavior methods

bullinaria j levy j extracting semantic representations word cooccurrence statistics stop lists stemming svd behavior methods

burgess c theory operational definitions computational memory
response glenberg robertson journal memory language

caicedo j ben abdallah j gonzlez f nasraoui multimodal representation indexing automated annotation retrieval image collections via nonnegative matrix factorization neurocomputing
chatfield k lempitsky v vedaldi zisserman devil
details evaluation recent feature encoding methods proceedings bmvc
dundee uk
church k hanks p word association norms mutual information lexicography computational linguistics
clark vector space lexical meaning lappin fox c eds
handbook contemporary semantics nd ed blackwell malden press
coltheart mrc psycholinguistic database quarterly journal experimental psychology
connolly gleitman l thompson schill effect congenital blindness
semantic representation everyday concepts proceedings national
academy sciences
csurka g dance c fan l willamowski j bray c visual categorization
bags keypoints workshop statistical learning computer vision
eccv pp prague czech republic
curran j moens improvements automatic thesaurus extraction
proceedings acl workshop unsupervised lexical acquisition pp
philadelphia pa


fimultimodal distributional semantics

de sande k v uijlings j gevers smeulders segmentation selective
search object recognition proceedings iccv pp barcelona
spain
de vega glenberg graesser eds symbols embodiment debates
meaning cognition oxford university press oxford uk
deng j dong w socher r li l j fei fei l imagenet large scale
hierarchical image database proceedings cvpr pp miami beach
fl
dumais data driven approaches information access cognitive science

erk k vector space word meaning phrase meaning survey
language linguistics compass
escalante h j hrnadez c sucar l e montes late fusion heterogeneous methods multimedia image retrieval proceedings icmr vancouver
canada
evert statistics word cooccurrences dissertation stuttgart university
farhadi hejrati sadeghi young p rashtchian c hockenmaier j
forsyth every picture tells story generating sentences images
proceedings eccv crete greece
felzenszwalb p girshick r mcallester deva ramanan object detection discriminatively trained part ieee transactions pattern
analysis machine intelligence
feng lapata visual information semantic representation proceedings hlt naacl pp los angeles ca
finkelstein l gabrilovich e matias rivlin e solan z wolfman g ruppin
e placing search context concept revisited acm transactions
information systems
firth j r papers linguistics oxford university press oxford
uk
fodor j language thought crowell press york
glenberg robertson symbol grounding meaning comparison
high dimensional embodied theories meaning journal memory language
grauman k darrell pyramid match kernel discriminative classification
sets image features proceedings iccv pp beijing china
grauman k leibe b visual object recognition morgan claypool san
francisco
grefenstette g explorations automatic thesaurus discovery kluwer boston



fibruni tran baroni

griffin l wahab h newell distributional learning appearance plos
one published online http www plosone org article info doi
journal pone
griffiths steyvers tenenbaum j topics semantic representation
psychological review
hansen olkkonen walter gegenfurtner k memory modulates color
appearance nature neuroscience
harnad symbol grounding physica nonlinear phenomena

harris z distributional structure word
johns b jones perceptual inference global lexical similarity topics
cognitive science
karypis g cluto clustering toolkit tech rep university minnesota
department computer science
kaschak madden c therriault yaxley r aveyard blanchard zwaan
r perception motion affects language processing cognition b b
kievit kylar b jones semantic pictionary project proceedings
cogsci pp austin tx
kulkarni g premraj v dhar li choi berg c berg l
baby talk understanding generating simple image descriptions proceedings
cvpr colorado springs msa
landauer dumais solution platos latent semantic analysis theory acquisition induction representation knowledge psychological
review
lazebnik schmid c ponce j beyond bags features spatial pyramid
matching recognizing natural scene categories proceedings cvpr pp
washington dc
leong c w mihalcea r going beyond text hybrid image text
measuring word relatedness proceedings ijcnlp pp
lloyd least squares quantization pcm ieee transactions information
theory
louwerse symbol interdependency symbolic embodied cognition topics
cognitive science
louwerse connell l taste words linguistic context perceptual
simulation predict modality words cognitive science
lowe object recognition local scale invariant features proceedings
iccv pp
lowe distinctive image features scale invariant keypoints international
journal computer vision


fimultimodal distributional semantics

lowe w towards theory semantic space proceedings cogsci pp
edinburgh uk
lund k burgess c producing high dimensional semantic spaces lexical
co occurrence behavior methods
manning c raghavan p schtze h introduction information retrieval
cambridge university press cambridge uk
manning c schtze h foundations statistical natural language processing
mit press cambridge
mcdonald brew c distributional model semantic context effects
lexical processing proceedings acl pp barcelona spain
mcrae k cree g seidenberg mcnorgan c semantic feature production
norms large set living nonliving things behavior methods

miller g charles w contextual correlates semantic similarity language
cognitive processes
moore mccabe g introduction practice statistics edition
freeman york
murphy g big book concepts mit press cambridge
nelson mcevoy c schreiber university south florida word association rhyme word fragment norms http www usf edu freeassociation
nister stewenius h scalable recognition vocabulary tree proceedings ieee computer society conference computer vision pattern
recognition cvpr pp
nowak e jurie f triggs b sampling strategies bag features image
classification proceedings eccv pp graz austria
pad lapata dependency construction semantic space
computational linguistics
pad u pad erk k flexible corpus modelling human plausibility
judgements proceedings emnlp pp prague czech republic
pecher zeelenberg r raaijmakers j pizza prime coin perceptual
priming lexical decision pronunciation journal memory language

perronnin f sanchez j mensink improving fisher kernel large scale
image classification proceedings eccv pp berlin heidelberg
pham maillot n lim j h chevallet j p latent semantic fusion
model image retrieval annotation proceedings cikm pp
lisboa portugal
poesio almuhareb identifying concept attributes classifier
proceedings acl workshop deep lexical semantics pp ann arbor
mi


fibruni tran baroni

pulvermueller f brain mechanisms linking language action nature reviews
neuroscience
radinsky k agichtein e gabrilovich e markovitch word time
computing word relatedness temporal semantic analysis proceedings
www pp hyderabad india
rapp r word sense discovery sense descriptor dissimilarity proceedings th mt summit pp orleans la
recchia g jones semantic richness abstract concepts frontiers
human neuroscience
reisinger j mooney r j multi prototype vector space word meaning proceedings naacl pp los angeles ca
riordan b jones redundancy perceptual linguistic experience
comparing feature distributional semantic representation topics
cognitive science
rothenhusler k schtze h unsupervised classification dependency
word spaces proceedings eacl gems workshop pp athens
greece
rubenstein h goodenough j contextual correlates synonymy communications acm
sahlgren introduction random indexing http www sics se mange
papers ri intro pdf
sahlgren word space model dissertation stockholm university
sahlgren distributional hypothesis italian journal linguistics

schtze h ambiguity resolution natural language learning csli stanford
ca
silberer c lapata grounded semantic representation proceedings emnlp conll pp jeju korea
sivic j zisserman video google text retrieval object matching videos proceedings iccv pp nice france
steyvers combining feature norms text data topic acta
psychologica
therriault yaxley r zwaan r role color diagnosticity object
recognition representation cognitive processing
tillman r datla v hutchinson louwerse head toe embodiment statistical linguistic frequencies proceedings cogsci pp
austin tx
turney p neuman assaf cohen literal metaphorical sense
identification concrete abstract context proceedings emnlp pp
edinburgh uk


fimultimodal distributional semantics

turney p pantel p frequency meaning vector space semantics journal artificial intelligence
van de sande k gevers snoek c evaluating color descriptors object
scene recognition ieee transactions pattern analysis machine intelligence

van overschelde j rawson k dunlosky j category norms updated
expanded version battig montague norms journal memory
language
vedaldi fulkerson b vlfeat open portable library computer
vision proceedings acm multimedia pp firenze italy
von ahn l games purpose computer
vreeswijk huurnink b smeulders w text image subject
classifiers dense works better proceedings acm multimedia pp
scottsdale az
wang j yang j yu k lv f huang gong locality constrained
linear coding image classification proceedings cvpr pp san
francisco ca
weeds j measures applications lexical distributional similarity ph
thesis department informatics university sussex
wittgenstein l philosophical investigations blackwell oxford uk translated
g e anscombe
yang j jiang g hauptmann ngo c w evaluating bag visualwords representations scene classification wang j z boujemaa n bimbo
li j eds multimedia information retrieval pp acm
zhao karypis g criterion functions document clustering experiments
analysis tech rep university minnesota department computer
science




