Journal Artificial Intelligence Research 49 (2014) 1-47

Submitted 7/13; published 1/14

Multimodal Distributional Semantics
Elia Bruni

elia.bruni@unitn.it

Center Mind/Brain Sciences,
University Trento, Italy

Nam Khanh Tran

ntran@l3s.de

L3S Research Center,
Hannover, Germany

Marco Baroni

marco.baroni@unitn.it

Center Mind/Brain Sciences,
University Trento, Italy
Department Information Engineering Computer Science,
University Trento, Italy

Abstract
Distributional semantic models derive computational representations word meaning
patterns co-occurrence words text. models success
story computational linguistics, able provide reliable estimates semantic
relatedness many semantic tasks requiring them. However, distributional models
extract meaning information exclusively text, extremely impoverished
basis compared rich perceptual sources ground human semantic knowledge.
address lack perceptual grounding distributional models exploiting computer
vision techniques automatically identify discrete visual words images,
distributional representation word extended encompass co-occurrence
visual words images associated with. propose flexible architecture
integrate text- image-based distributional information, show set
empirical tests integrated model superior purely text-based approach,
provides somewhat complementary semantic information respect latter.

1. Introduction
distributional hypothesis states words occur similar contexts semantically similar. claim multiple theoretical roots psychology, structuralist linguistics, lexicography possibly even later writings Wittgenstein (Firth, 1957;
Harris, 1954; Miller & Charles, 1991; Wittgenstein, 1953). However, distributional hypothesis huge impact computational linguistics last two decades mainly
empirical reasons, is, suggests simple practical method harvest
word meaning representations large scale: record contexts words
occur easy-to-assemble large collections texts (corpora) use contextual profiles surrogates meaning. Nearly contemporary corpus-based approaches
semantics rely contextual evidence one way another, systematic
extensive application distributional methods found call distributional
semantic models (DSMs), known literature vector space semantic space
2014 AI Access Foundation. rights reserved.

fiBruni, Tran & Baroni

models meaning (Landauer & Dumais, 1997; Sahlgren, 2006; Schtze, 1997; Turney &
Pantel, 2010).
DSMs, meaning word approximated vector keeps track
patterns co-occurrence word text corpora, degree semantic
similarity or, generally, relatedness (Budanitsky & Hirst, 2006) two words
precisely quantified terms geometric distance vectors representing
them. example, car automobile might occur terms street, gas
driver, thus distributional vectors likely close, cuing fact
words synonyms. Extended empirical evidence shown distributional
semantics good harvesting effective meaning representations large scale,
confirming validity distributional hypothesis (see references Section 2.1
below).
Still, successes, distributional semantics suffers obvious limitation
represents meaning word entirely terms connections words. long
tradition studies cognitive science philosophy stressed models
meaning symbols (e.g., words) entirely accounted terms symbols (e.g.,
words) without links outside world (e.g., via perception) deeply problematic, issue often referred symbol grounding problem (Harnad, 1990).
DSMs come attack lack grounding (Glenberg & Robertson,
2000).1 Although specific criticisms vented might entirely well-founded
(Burgess, 2000), little doubt limitation textual contexts makes
DSMs dissimilar humans, who, thanks senses, access rich sources
perceptual knowledge learning meaning words much cognitive scientists argued meaning directly embodied sensory-motor processing
(see work de Vega, Glenberg, & Graesser, 2008, different views embodiment
cognitive science). Indeed, last decades large amount behavioural neuroscientific evidence amassed indicating knowledge words concepts
inextricably linked perceptual motor systems. example, perceiving actiondenoting verbs kick lick involves activation areas brain controlling
foot tongue movements, respectively (Pulvermueller, 2005). Hansen, Olkkonen, Walter, Gegenfurtner (2006) asked subjects adjust color fruit images objects
appeared achromatic. objects generally adjusted color shifted
away subjects gray point direction opposite typical color fruit,
e.g., bananas shifted towards blue subjects overcorrected typical
yellow color. Typical color influences lexical access: example, subjects faster
naming pumpkin picture presented orange grayscale
representation, slowest another color (Therriault, Yaxley, & Zwaan, 2009).
final example, Kaschak, Madden, Therriault, Yaxley, Aveyard, Blanchard, Zwaan
(2005) found subjects slower processing sentence describing action
sentence presented concurrently visual stimulus depicting motion opposite
1. Harnard, original paper, discussing formal symbols, postulated Fodors language
thought (Fodor, 1975), rather words natural language. However, latter
represented terms connections words, case DSMs, grounding problem
arises, follow recent literature issue referring symbol grounding,
symbols natural language words.

2

fiMultimodal Distributional Semantics

direction described (e.g., car approached harder process concurrently
perception motion away you). See review Barsalou (2008) review
evidence conceptual linguistic competence strongly embodied.
One might argue concerns DSMs grounded embodied
exaggerated, overlook fact patterns linguistic co-occurrence
exploited DSMs reflect semantic knowledge acquired perception,
linguistic perceptual information strongly correlated (Louwerse, 2011).
dogs often brown pink, likely talk brown dogs
pink dogs. Consequently, child learn useful facts meaning concept
denoted dog direct perception linguistic input (this explains, among
things, congenitally blind subjects excellent knowledge color terms;
see, e.g., Connolly, Gleitman, & Thompson-Schill, 2007). One could hypothesize
meaning representations extracted text corpora indistinguishable
derived perception, making grounding redundant. However, fairly
extensive literature showing case. Many studies (Andrews, Vigliocco,
& Vinson, 2009; Baroni, Barbu, Murphy, & Poesio, 2010; Baroni & Lenci, 2008; Riordan
& Jones, 2011) underlined text-derived DSMs capture encyclopedic, functional
discourse-related properties word meanings, tend miss concrete aspects.
Intuitively, might harvest text information bananas tropical eatable,
yellow (because authors write obvious statements
bananas yellow). hand, studies show how, humans
asked describe concepts, features produce (equivalent sense contextual
features exploited DSMs) preponderantly perceptual nature: Bananas yellow,
tigers stripes, on.2
discrepancy DSMs humans not, per se, proof DSMs
face empirical difficulties computational semantic models. However, interested
potential implications DSMs models humans acquire use language
case many DSM developers (e.g., Griffiths, Steyvers, & Tenenbaum, 2007;
Landauer & Dumais, 1997; Lund & Burgess, 1996, many others) complete
lack grounding perception serious blow psychological plausibility,
exposes criticism classic ungrounded symbolic models received.
Even empirical level, reasonable expect DSMs enriched perceptual
information would outperform purely textual counterparts: Useful computational semantic models must capture human semantic knowledge, human semantic knowledge
strongly informed perception.
accept grounding DSMs perception desirable avenue research,
must ask find practical source perceptual information embed DSMs.
Several interesting recent experiments use features produced human subjects concept
description tasks (so-called semantic norms) surrogate true perceptual features
(Andrews et al., 2009; Johns & Jones, 2012; Silberer & Lapata, 2012; Steyvers, 2010).
reasonable first step, integration methods proposed studies
2. perfectly fair, tendency might part triggered fact that, subjects asked
describe concepts, might encouraged focus perceptual aspects experimenters
instructions. example McRae, Cree, Seidenberg, McNorgan (2005) asked subjects list first
physical properties, internal external parts, [the object] looks.

3

fiBruni, Tran & Baroni

quite sophisticated, using subject-produced features unsatisfactory practically
theoretically (see however work reported Kievit-Kylar & Jones, 2011,
crowdsourcing project addressing kinds concerns). Practically, using subjectgenerated properties limits experiments words denote concepts described
semantic norms, even large norms contain features hundred concepts.
Theoretically, features produced subjects concept description tasks far removed
sort implicit perceptual features supposed stand for. example,
since expressed words, limited conveyed verbally.
Moreover, subjects tend produce salient distinctive properties.
state dogs head, since thats hardly distinctive feature animal!
article, explore direct route integrate perceptual information
DSMs. exploit recent advances computer vision (Grauman & Leibe, 2011)
availability documents combine text images automatically extract visual
features naturally co-occurring words multimodal corpora. imagebased features combined standard text-based features obtain perceptuallyenhanced distributional vectors. this, rely natural extension distributional hypothesis, encompasses similarity linguistic context,
similarity visual context. Interestingly, Landauer Dumais, one classic papers
laid groundwork distributional semantics, already touched grounding
issue proposed, speculatively, solution along lines one implementing
here: [I]f one judiciously added numerous pictures scenes without rabbits
context columns [. . . ] corpus matrix, filled handful appropriate cells
rabbit hare word rows, [a DSM] could easily learn words rabbit hare
go pictures containing rabbits ones without, forth. (Landauer &
Dumais, 1997, p. 227).3
Although vision one source perceptual data, reasonable starting point,
convenience (availability suitable data train models)
probably dominating modality determining word meaning. one piece
evidence claim, widely used subject-generated semantic norms McRae et al.
(2005) contain 3,594 distinct perceptual features total, and, these, 3,099 (86%)
visual nature!
relatively low-level noisy features extract images multimodal corpora contribute meaningful information distributional representation
word meaning? report results systematic comparison network semantic relations entertained set concrete nouns traditional text-based
novel image-based distributional spaces confirming image-based features are, indeed,
semantically meaningful. Moreover, expected, provide somewhat complementary
information respect text-based features. thus found practical effective way extract perceptual information, must consider next combine textand image-derived features build multimodal distributional semantic model.
propose general parametrized architecture multimodal fusion that, given appropriate
sample data, automatically determines optimal mixture text- image-based features used target semantic task. Finally, evaluate multimodal DSMs
3. thank Mike Jones pointing interesting historical connection us.

4

fiMultimodal Distributional Semantics

two separate semantic tasks, namely predicting degree semantic relatedness assigned
word pairs humans, categorizing nominal concepts classes. show
tasks multimodal DSMs consistently outperform purely textual models, confirming
supposition that, humans, performance computational models
meaning improves meaning grounded perception.
article structured follows. Section 2 provides relevant background
computational linguistics image analysis, discusses related work. lay
general architecture multimodal fusion distributional semantics Section 3.
necessary implementation details provided Section 4. Section 5 presents experiments tested approach. Section 6 concludes summarizing current
results well sketching come next.

2. Background Related Work
section first give brief introduction traditional distributional semantic models
(i.e., based solely textual information). Then, describe image analysis
techniques adopt extract manipulate visual information. Next, discuss earlier
attempts construct multimodal distributional representation meaning. Finally,
describe relevant strategies combine information coming text images
proposed inside computer vision community.
2.1 Distributional Semantics
last decades, number different distributional semantic models (DSMs) word
meaning proposed computational linguistics, relying assumption
word meaning learned directly linguistic environment.
Semantic space models one common types DSM. approximate
meaning words vectors record distributional history corpus
(Turney & Pantel, 2010). distributional semantic model encoded matrix whose
rows semantic vectors representing meanings set target words.
component semantic vector function occurrence counts corresponding
target word certain context (see Lowe, 2001, formal treatment). Definitions
context range simple ones (such documents occurrence another word
inside fixed window target word) linguistically sophisticated ones (such
occurrence certain words connected target special syntactic relations)
(Pad & Lapata, 2007; Sahlgren, 2006; Turney & Pantel, 2010). raw targetcontext counts collected, transformed association scores typically
discount weights components whose corresponding word-context pairs high
probability chance co-occurrence (Evert, 2005). rank matrix containing
semantic vectors rows optionally decreased dimensionality reduction,
might provide beneficial smoothing getting rid noise components and/or allow
efficient storage computation (Landauer & Dumais, 1997; Sahlgren, 2005; Schtze,
1997). Finally, distributional semantic similarity pair target words estimated
similarity function takes semantic vectors input returns scalar
similarity score output.
5

fiBruni, Tran & Baroni

many different semantic space models literature. Probably best
known Latent Semantic Analysis (LSA, Landauer & Dumais, 1997), highdimensional semantic space words derived use co-occurrence information
words passages occur. Another well-known example
Hyperspace Analog Language model (HAL, Lund & Burgess, 1996), word
represented vector containing weighted co-occurrence values word
words fixed window. semantic space models rely syntactic relations instead
windows (Grefenstette, 1994; Curran & Moens, 2002; Pad & Lapata, 2007). General
overviews semantic space models provided Clark (2013), Erk (2012), Manning
Schtze (1999), Sahlgren (2006) Turney Pantel (2010).
recently, probabilistic topic models receiving increasing attention
alternative implementation DSMs (Blei, Ng, & Jordan, 2003; Griffiths et al., 2007).
Probabilistic topic models rely co-occurrence information large corpora derive meaning but, differently semantic space models, based assumption
words corpus exhibit probabilistic structure connected topics. Words
represented points high-dimensional space probability distribution
set topics. Conversely, topic defined probability distribution
different words. Probabilistic topic models tackle problem meaning representation
means statistical inference: use word corpus infer hidden topic structure.
Distributional semantic models, whether geometric probabilistic kind,
ultimately mainly used provide similarity score arbitrary pairs words,
employ them. Indeed, models shown effective
modeling wide range semantic tasks including judgments semantic relatedness
word categorization.
several data sets assess well DSM captures human intuitions
semantic relatedness, Rubenstein Goodenough set (Rubenstein & Goodenough, 1965) WordSim353 (Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman,
& Ruppin, 2002). Usually constructed asking subjects rate set word
pairs according similarity scale. Then, average rating pair taken
estimate perceived relatedness words (e.g., dollar-buck: 9.22, cord-smile:
0.31). measure well distributional model approximates human semantic intuitions,
usually correlation measure similarity scores generated model
human ratings computed. highest correlation aware WordSim353
set employ 0.80 obtained model called Temporal
Semantic Analysis, captures patterns word usage time concepts
represented time series corpus temporally-ordered documents (Radinsky,
Agichtein, Gabrilovich, & Markovitch, 2011). temporal knowledge could integrated
perceptual knowledge encode model. direct comparison point,
Agirre, Alfonseca, Hall, Kravalova, Pasa, Soroa (2009) presented extensive evaluation distributional WordNet-based semantic models WordSim, achieving
maximum correlation 0.66 across various parameters.4
4. WordNet, available http://wordnet.princeton.edu/, large computational lexicon English
nouns, verbs, adjectives adverbs grouped sets synonyms (synsets), expressing
distinct concept.

6

fiMultimodal Distributional Semantics

Humans good grouping together words (or concepts denote)
classes based semantic relatedness (Murphy, 2002), therefore cognitive-aware
representation meaning must show proficiency categorization (e.g., Poesio
& Almuhareb, 2005; Baroni et al., 2010). Concept categorization moreover useful
applications automated ontology construction recognizing textual entailment.
Unlike similarity ratings, categorization requires discrete decision group coordinates/cohyponyms class performed applying standard clustering techniques
model-generated vectors representing words categorized. example,
Almuhareb-Poesio data set (Almuhareb & Poesio, 2005), employ below,
includes 402 concepts WordNet, balanced terms frequency degree ambiguity. distributional model Rothenhusler Schtze (2009) exploits syntactic
information reach state-of-the-art performance Almuhareb-Poesio data set (maximum clustering purity across various parameter: 0.79). window-based distributional
approach Baroni Lenci (2010), directly comparable text-based models,
achieves 0.65 purity.
semantic tasks DSMs applied include semantic priming, generation
salient properties concepts intuitions thematic fit verb arguments (see,
e.g., Baroni & Lenci, 2010; Baroni et al., 2010; McDonald & Brew, 2004; Pad & Lapata,
2007; Pad, Pad, & Erk, 2007). Distributional semantic vectors used wide
range applications require representation word meaning, particular
objective measure meaning relatedness, including document classification, clustering
retrieval, question answering, automatic thesaurus generation, word sense disambiguation,
query expansion, textual advertising areas machine translation (Dumais, 2003;
Turney & Pantel, 2010).
2.2 Visual Words
Ideally, build multimodal DSM, would extract visual information
images way similar text. Thanks well-known image
analysis technique, namely bag-of-visual-words (BoVW), indeed possible discretize
image content produce visual units somehow comparable words text, known
visual words (Bosch, Zisserman, & Munoz, 2007; Csurka, Dance, Fan, Willamowski, &
Bray, 2004; Nister & Stewenius, 2006; Sivic & Zisserman, 2003; Yang, Jiang, Hauptmann,
& Ngo, 2007). Therefore, semantic vectors extracted corpus images
associated target (textual) words using similar pipeline commonly
used construct text-based vectors: Collect co-occurrence counts target words
discrete image-based contexts (visual words), approximate semantic relatedness
two words similarity function visual words representing them.
BoVW technique extract visual word representations documents inspired
traditional bag-of-words (BoW) method Information Retrieval. BoW turn
dictionary-based method represent (textual) document bag (i.e., order
considered), contains words dictionary. BoVW extends idea visual
documents (namely images), describing collection discrete regions, capturing
appearance ignoring spatial structure (the visual equivalent ignoring word
order text). bag-of-visual-word representation image convenient image7

fiBruni, Tran & Baroni






ffffff

fififi






Figure 1: Representing images BoVW: (i) Salient image patches keypoints contain rich local information detected represented vectors low-level
features called descriptors; (ii) Descriptors mapped visual words
basis distance centers clusters corresponding visual words


(the preliminary clustering step shown
figure); (iii) Images finally
represented bag-of-visual-words feature vector according distribution
visual words contain. Images depicting things rotations,
occlusions, small differences low-level descriptors might still similar
distribution visual words, hence object traced robustly
across images conditions change.

analysis point view translates usually large set high-dimensional local
descriptors single sparse vector representation across images. Importantly, size
original set varies image image, bag-of-visual-word representation
fixed dimensionality. Therefore, machine learning algorithms default expect
fixed-dimensionality vectors input (e.g., supervised classification unsupervised
clustering) used tackle typical image analysis tasks object recognition,
image segmentation, video tracking, motion detection, etc.
specifically, similarly terms text document, image local interest
points keypoints defined salient image patches contain rich local information
image. However keypoint types images come off-the-shelf word
8

fiMultimodal Distributional Semantics

types text documents. Local interest points grouped types (i.e., visual
words) within across images, image represented number
occurrences type it, analogously BoW. following pipeline typically
followed. every image data set, keypoints automatically detected (note
recent approaches dense, pixelwise sampling keypoints preferred
detecting salient ones only, solution adopt,
explained Section 4.2.2) represented vectors low-level features called descriptors.
Keypoint vectors grouped across images number clusters based
similarity descriptor space. cluster treated discrete visual word.
keypoints mapped onto visual words, image represented BoVW feature
vector recording many times visual word occurs it. way, move
representing image varying number high-dimensional keypoint descriptor vectors
representation terms single visual word count vector fixed dimensionality
across images, advantages discussed above. Visual word assignment
use represent image content exemplified Figure 1, two images
similar content described terms bag-of-visual-word vectors.
kind image content visual word captures exactly depends number
factors, including descriptors used identify represent keypoints, clustering
algorithm number target visual words selected. general, local interest points
assigned visual word tend patches similar low-level appearance;
local patterns need correlated object-level parts present images
(Grauman & Leibe, 2011).
2.3 Multimodal Distributional Semantics
availability large amounts mixed media Web, one hand,
discrete representation images visual words, other, escaped attention
computational linguists interested enriching distributional representations word
meaning visual features.
Feng Lapata (2010) propose first multimodal distributional semantic model.
generative probabilistic setting requires extraction textual visual features
mixed-media corpus, latent dimensions estimated
probabilistic process assumes document generated sampling textual
visual words. Words represented distribution set latent
multimodal dimensions topics (Griffiths et al., 2007) derived surface textual
visual features. Feng Lapata experiment collection documents downloaded
BBC News website corpus. test semantic representations
free association norms Nelson, McEvoy, Schreiber (1998) subset 253
pairs WordSim, obtaining gains performance visual information taken
account (correlations human judgments 0.12 0.32 respectively), compared
textual modality standalone (0.08 0.25 respectively), even performance still
well state-of-the-art WordSim (see Section 2.1 above).
main drawbacks approach textual visual data must
extracted corpus, thus limiting choice corpora used,
generative probabilistic approach, elegant, allow much flexibility
9

fiBruni, Tran & Baroni

two information channels combined. Below, re-implement Feng
Lapata method (MixLDA) training ESP-Game data set, source labeled
images adopt model. possible data set contains images
textual labels describing them. general, recapture Feng Lapatas
idea common latent semantic space latent multimodal mixing step pipeline
(see Section 3.2.1 below).
Leong Mihalcea (2011) exploit textual visual information obtain multimodal distributional semantic model. Feng Lapata merge two sources
information learning joint semantic model, Leong Mihalcea propose strategy
akin call Scoring Level fusion below: Come separate text-
image-based similarity estimates, combine obtain multimodal score.
particular, use two combination methods: summing scores computing
harmonic mean. Differently Feng Lapata, Leong Mihalcea extract visual information corpus manually coded resource, namely ImageNet
database (Deng, Dong, Socher, Li, & Fei-Fei, 2009), large-scale ontology images.5 Using
handcoded annotated visual resource ImageNet faces sort problems
using manually developed lexical database WordNet faces respect
textual information, is, applications severely limited ImageNet coverage (for
example, ImageNet currently restricted nominal concepts), interest
model computational simulation word meaning acquisition naturally occurring language visual data somewhat reduced (humans learn meaning
mountain set carefully annotated images mountains little else crowding
occluding scene). evaluation, Leong Mihalcea experiment small subsets WordSim, obtaining improvements, although level report
(the highest reported correlation 0.59 56 word pairs). Furthermore use
data set tune test models.
Bruni, Tran, Baroni (2011) propose instead directly concatenate textand image-based vectors produce single multimodal vector represent words,
call Feature Level fusion below. text-based distributional vector representing word, taken state-of-the-art distributional semantic model (Baroni &
Lenci, 2010), concatenated vector representing word visual features, extracted images ESP-Game collection use here.
obtain promising performance WordSim test sets, although appreciably lower
results report (we obtain maximum correlation 0.52 text-
image-based features used together; compare Table 2 below).
Attempts use multimodal models derived text images perform
specific semantic tasks reported. Bergsma Goebel (2011) use textual
image-based cues model selectional preferences verbs (which nouns likely
arguments verbs). experiment shows several cases visual information
useful text task. example, looking textual corpora words
carillon, migas mamey, much useful information obtained guess
three plausible argument verb eat. hand, show
5. http://image-net.org/

10

fiMultimodal Distributional Semantics

that, exploiting Google image search functionality,6 enough images words
found vision-based model edible things classify correctly.
Finally, evaluate multimodal models task discovering color concrete objects, showing relation words denoting concrete things
typical color better captured visual information taken account (Bruni,
Boleda, Baroni, & Tran, 2012). Moreover, show multimodality helps distinguishing literal nonliteral uses color terms.
2.4 Multimodal Fusion
textual information used image analysis, mostly done different
aims ours: Text used improve image-related tasks, typically
attempt model relation specific images specific words textual passages
(e.g., Barnard, Duygulu, Forsyth, de Freitas, Blei, & Jordan, 2003; Berg, Berg, & Shih,
2010; Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, & Forsyth, 2010; Griffin,
Wahab, & Newell, 2013; Kulkarni, Premraj, Dhar, Li, Choi, Berg, & Berg, 2011).
contrast, (i) want use image-derived features improve representation word
meaning (ii) interested capturing meaning word types basis
sets images connected word, model specific word-image relations.
Despite differences, challenges addressed image analysis literature deals exploiting textual cues similar ones face. particular,
problem merging, fusing, textual visual cues common representational
space exactly face construct multimodal semantic space.
Traditionally, image analysis community distinguishes two classes fusion
schemes, namely early fusion late fusion. former fuses modalities feature space,
latter fuses modalities semantic similarity space, analogously call
Feature Level Scoring Level fusion, respectively. example, Escalante, Hrnadez,
Sucar, Montes (2008) propose image retrieval system multimodal documents.
early late fusion strategies combination image textual
channels considered. Early fusion settings include weighted linear combination
two channels global strategy different retrieval systems used contemporarily
entire, joint data set. Late fusion strategies include per-modality strategy,
documents retrieved using one channel hierarchical setting
first text, image combination used independently query database
results aggregated four weighted combinations. Vreeswijk, Huurnink,
Smeulders (2011) train visual concept classifier abstract subject categories
biology history using late fusion approach image text information
combined output level, is, first obtaining classification scores image-
text-based models separately joining them. Similarly multimodal mixing
step, Pham, Maillot, Lim, Chevallet (2007) Caicedo, Ben-Abdallah, Gonzlez,
Nasraoui (2012) propose early fusion two inputs mapped onto
latent space using dimensionality reduction techniques (e.g., Singular Value Decomposition).
multimodal representation obtained way directly used retrieve image
documents.
6. http://images.google.com/

11

fiBruni, Tran & Baroni

3. Framework Multimodal Distributional Semantics
section, general flexible architecture multimodal semantics presented.
architecture makes use distributional semantic models based textual visual
information build multimodal representation meaning. merge two sources,
uses parameter-based pipeline able capture previously proposed combination
strategies, advantage explored within single system.
3.1 Input Multimodal Architecture
construct multimodal representation meaning, semantic model single
modality implemented. Independently actual parameters chosen
creation (that, point view, black box), requirements
model satisfy order guarantee good functioning framework.
first place, modality must provide separate representation, leave room
various fusion strategies afterwards. Then, modality must encode semantic
information pertaining word interest fixed-size vectorial representation.
Moreover, assume text- image-based vectors normalized arranged
matrices words rows co-occurring elements columns.
follows, assume harvested matrix text-based semantic vectors,
one image-based semantic vectors set target words, representing,
respectively, verbal visual information words. Section 4 give
details specific implementation construct matrices.
3.2 Multimodal Fusion
pipeline based two main steps:
(1) Latent Multimodal Mixing: text vision matrices concatenated, obtaining single matrix whose row vectors projected onto single, common space
make interact.
(2) Multimodal Similarity Estimation: Information text- image-based
matrices combined two ways obtain similarity estimates pairs target
words: Feature Level Scoring Level.
Figure 2 describes infrastructure propose fusion. First, introduce mixing
phase promote interaction modalities call Latent Multimodal Mixing.
step part approaches would consider Feature Level fusion (see
below), keep separated might benefit Scoring Level fusion well.
mixing performed, proceed integrate textual visual features.
reviewed Section 2.4 above, literature fusion performed two main levels,
Feature Level Scoring Level. first case features first combined
considered single input operations, second case task performed separately
different sets features separate results combined. approach
advantages limitations incorporated
multimodal infrastructure together constitute call Multimodal Similarity
12

fiMultimodal Distributional Semantics






































Figure 2: Multimodal fusion combining textual visual information semantic
model.



Estimation. Feature Level approach requires one learning step (i.e., determining
parameters feature vector combination) offers richer vector-based representation
combined information, used purposes (e.g., image text
features could used together train classifier). Benefits Scoring Level approach
include possibility different representations (in principle, even vectorial)
different similarity scores different modalities ease increasing (or decreasing)
number different modalities used representation.
3.2.1 Latent Multimodal Mixing
preparatory step textual visual components projected
onto common representation lower dimensionality discover correlated latent factors.
result new connections made source matrix taking account
information connections present matrix, originating patterns covariance overlap. Importantly, assume mixing done via dimensionality
reduction technique following characteristics: parameter k determines
13

fiBruni, Tran & Baroni

dimensionality reduced space fact k equals rank
original matrix reduced matrix identical considered good approximation
original one. commonly used Singular Value Decomposition reduction method
adopt mixing step satisfies constraints.
toy example mixing might beneficial, consider concepts pizza
coin, could use features text-based semantic vectors (i.e., record cooccurrences target words concepts part vector dimensions).
words likely occur similar contexts text, obviously visually similar. So, original text features pizza coin might highly correlated.
However, mixing multimodal space, might associated (have high
weights on) reduced space component, similar distributions
visual features cue roundness. Consequently, two textual features originally
uncorrelated might drawn closer multimodal mixing, corresponding concepts visually similar, resulting mixed textual features are, sense,
visually enriched, vice versa mixed visual features (interestingly, psychologists
shown that, certain conditions, words pizza coin, strongly
associated perceptually similar, prime other; e.g., Pecher, Zeelenberg, & Raaijmakers, 1998).
Note matrices obtained splitting reduced-rank matrix back
original textual visual blocks number feature columns original
textual visual blocks, values smoothed dimensionality
reduction (we explain details achieved specific implementation
next paragraph). matrices used calculate similarity score word
pair (re-)merging information feature scoring levels.
Mixing SVD implementation, perform mixing across text- imagebased features applying Singular Value Decomposition (SVD)7 matrix obtained concatenating two feature types row-wise (so row concatenated
matrix describes target word textual visual space). SVD widely used technique
find best approximation original data points space lower underlying
dimensionality whose basis vectors (principal components latent dimensions)
selected capture much variance original space possible (Manning,
Raghavan, & Schtze, 2008, Ch. 18). performing SVD concatenated textual
visual matrices, project two types information space,
described linear combinations principal components. Following description
Pham et al. (2007), SVD matrix rank r factorization form
= U V




U : matrix eigenvectors derived




: r r diagonal matrix singular values

: square roots eigenvalues







V : matrix eigenvectors derived

7. Computed SVDLIBC: http://tedlab.mit.edu/~dr/SVDLIBC/

14

fiMultimodal Distributional Semantics

context, matrix given normalizing two feature matrices separately
concatenating. selecting k largest values matrix keeping
corresponding columns matrices U V , reduced matrix Mk given
Mk = Uk k Vkt
k < r dimensionality latent space. Mk keeps number
columns/dimensions , rank k. k free parameter tune
development sets. Note k equals rank original matrix, trivially
Mk = . Thus consider performing SVD reduction special case
SVD, helps searching optimal parameters.
Note that, n columns, Vkt k n matrix, Mk
number columns . first j columns contain textual features, columns
j + 1 n contain visual features, hold Mk , although latter
values features affected global SVD smoothing. Thus,
current implementation pipeline Figure 2, block splitting attained simply
dividing Mk textual mixed matrix containing first j columns, visual mixed
matrix containing remaining columns.
3.2.2 Multimodal Similarity Estimation
Similarity Function Following distributional hypothesis, DSMs describe word
terms contexts occurs. Therefore, measure similarity two words
DSMs need function capable determining similarity two descriptions (i.e.,
two semantic vectors). literature, many different similarity functions
used compare two semantic vectors, including cosine similarity, Euclidean distance, L1
norm, Jaccards coefficient, Jensen-Shannon divergence, Lins similarity. extensive
evaluation different similarity measures, see work Weeds (2003).
focus cosine similarity since shown effective measure
many semantic benchmarks (Bullinaria & Levy, 2007; Pad & Lapata, 2007). Also,
given system based geometric principles, cosine, together Euclidean
distance, principled choice measure similarity. example,
measures listed above, developed probabilistic considerations,
applicable vectors encode well-formed probability distributions, typically
case (for example, multimodal mixing, vectors might contain negative
values).
cosine two semantic vectors b dot product divided product
lengths:
Pi=n

i=1 ai bi
qP
i=n 2
i=n 2


i=1
i=1 bi

cos(a, b) = qP

cosine ranges 0 (orthogonal vectors) |1| (parallel vectors pointing
opposite directions cosine values 1 -1, respectively).
15

fiBruni, Tran & Baroni

Feature Level Fusion Feature Level fusion (FL), use linear weighted fusion
method combine text- image-based feature vectors words single representation use latter estimate similarity pairs. linear weighted
combination function defined
F = Ft (1 ) Fv
vector-concatenate operator.
Scoring Level Fusion Scoring Level fusion (SL), text- image-based matrices
used estimate similarity pairs independently. scores combined obtain
final estimate using linear weighted scoring function:
= St + (1 ) Sv
General Form Special Cases Given fixed normalized text- image-based
matrices, multimodal approach parametrized k (dimensionality latent space),
FL vs. SL, (weight text component FL similarity estimation) (weight text
component SL).
Note k=r, r rank original combined matrix, Latent Multimodal
Mixing returns original combined matrix (no actual mixing). Picking SL =1
=0 corresponds using textual visual matrix only, respectively. thus derive
special cases models text (k=r, SL, =1) images (k=r, SL, =0)
used (called Text Image models Results section below). simple approach
Bruni et al. (2011), two matrices concatenated without mixing,
parametrization k=r, FL, =0.5 (called NaiveFL model, below). summing approach
Leong Mihalcea (2011) corresponds k=r, SL, =0.5 (NaiveSL, below). Picking
k<r, SL, =1 amounts performing latent multimodal mixing, using textual
features only; reverse mixed image features = 0 (Textmixed
Imagemixed , respectively). Reducing models parametrized
approach means that, given development set specific task requires similarity
measurements, discover data-driven way various models best
task hand (for example, certain task might discover better
using text only, another mixed text features, yet another text image
features, on).
Formally, given set k1 , ..., kn R n dimensionalities latent space (with kn
equal original dimensionality, arbitrary steps chosen values),
sets 1 , ..., R potential weights text block FL (with 1 = 0 = 1)
1 , ..., l R l weights text block SL (with 1 = 0 l = 1),
calculate number possible configurations explore totc = n(m + l). Unless n,
l large (i.e., consider small intervals values tested),
completely feasible perform full search best parameters certain task
without approximate optimization methods. experiments, n = 9, = l = 11,
consequently totc = 198.
16

fiMultimodal Distributional Semantics

4. Implementation Details
implementation multimodal framework visual feature extraction
procedure publicly available open source.8 Moreover visual feature extraction
procedure presented Bruni, Bordignon, Liska, Uijlings, Sergienya (2013).
4.1 Construction Text-Based Semantic Matrix
reviewed Section 2.1 above, text-based distributional model encoded matrix whose rows semantic vectors representing meaning set target words.
Important parameters model choice target contextual elements,
source corpora used extract co-occurrence information, context delimiting
scope co-occurrence, function transform raw counts statistical association scores downplaying impact frequent elements.
Source Corpora collect co-occurrence counts concatenation two corpora,
ukWaC Wackypedia (size: 1.9B 820M running words, tokens, respectively).
ukWaC collection Web pages based linguistically-controlled crawl .uk
domain conducted mid 2000s. Wackypedia built mid-2009 dump
English Wikipedia. corpora automatically annotated lemma (dictionary form) part-of-speech (POS) category information using TreeTagger,9
freely publicly available,10 widely used linguistic research.
Target Context Elements Since source corpora annotated lemma
part-of-speech information, take account extracting target context
words (e.g., string sang treated instance verb lemma sing). collect
semantic vectors set 30K target words (lemmas), namely top 20K frequent
nouns, 5K frequent adjectives 5K frequent verbs combined corpora.
30K lemmas employed contextual elements (consequently, textbased semantic models encoded 30K30K matrix). Note combine
text matrices image-based ones, preserve rows (target words)
image-based vector, trimming matrix size 20,52530K.
Context define context terms words co-occur within window fixed
width, tradition popular HAL model (Lund & Burgess, 1996). Window-based
models attractive simplicity fact require resourceintensive advanced linguistic annotation. moreover reported
state art various semantic tasks (Rapp, 2003; Sahlgren, 2008), Bruni,
Uijlings, Baroni, Sebe (2012) show window-based methods use
outperform document-as-context model sophisticated syntax- lexicalpattern-based model MEN WordSim test sets introduced Section 5.2
(see post-hoc analysis using document-based model discussed end
Section 5.2.2 below). consider two variants, Window2 Window20 (we chose
particular variants arbitrarily, representatives narrow wide windows, respectively).
8. See https://github.com/s2m/FUSE/ https://github.com/vsem/, respectively.
9. http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
10. http://wacky.sslmit.unibo.it/

17

fiBruni, Tran & Baroni

Window2 records sentence-internal co-occurrence nearest 2 content words left
right target word (function words articles prepositions ignored).
Window20 considers larger window 20 content words left right target.
narrower window expected capture narrower kind semantic similarity,
one exists terms closely taxonomically related, example coordinate
concepts (dog cat) pairs superordinate subordinate concepts (animal
dog). rationale behind expectation terms share many narrow-window
collocates similar, semantically syntactically.
hand, broader window capture broader kind topical similarity, one
would expect words tend occur paragraphs (for example, war oil,
rather distant concepts taxonomic sense, might easily occur
discourse). See work Sahlgren (2006) discussion effects context
width distributional semantic models.
Association Score transform raw co-occurrence counts nonnegative Local Mutual Information (LMI) association scores. LMI scores obtained multiplying raw
counts Pointwise Mutual Information, nonnegative case close approximation Log-Likelihood Ratio scores, one widely used weighting
schemes computational linguistics (Evert, 2005). nonnegative LMI target element
context element c defined as:


P(t, c)
,0
P(t)P(c)



LM I(t, c) = max Count(t, c) log

worth observing that, extensive study parameters affect quality
semantic vectors, Bullinaria Levy (2007) Bullinaria Levy (2012) found
model similar Window2 (co-occurrence statistics ukWaC, narrow window,
lemmatized content word collocates, nonnegative pointwise mutual information instead
LMI) performs near top variety semantic tasks. Thus, independent
grounds claim using state-of-the-art text-based model.
4.2 Construction Image-Based Semantic Matrix
Given image-based semantic vectors novelty respect text-based ones,
next subsections dedicate space constructed them, including
full details source corpus utilize input pipeline (Section 4.2.1),
particular image analysis technique choose extract visual collocates finally
arrange semantic vectors constitute visual block distributional
semantic matrix (Section 4.2.2).
4.2.1 Image Source Corpus
adopt source corpus ESP-Game data set11 contains 100K images, labeled
famous game purpose developed Louis von Ahn, two
11. http://www.cs.cmu.edu/~biglou/resources/

18

fiMultimodal Distributional Semantics


fffi fiff
fifi fffi
fifi

fffi
fififf







fififffi

fifi


fifi






Figure
3: Samples images tags ESP-Game data set

people partnered online must independently rapidly agree appropriate word
label random selected images. word entered partners certain number
game rounds, word added tag image, becomes taboo term
next rounds game involving image, encourage players produce
terms describing image (Von Ahn, 2006). tags images data set form
vocabulary 20,515 distinct word types. Images 14 tags average (4.56 standard
deviation), word tag 70 images average (737.71 standard deviation).
words format text-based models, tags lemmatized POS-tagged. annotate words parts speech, could
run POS-tagger, since words context (i.e., tag appears alphabetically
within small list words labeling image within ordinary sentence
required POS-tagger). Thus used heuristic method, assigned words
ESP-Game vocabulary frequent tag textual corpora.
ESP-Game corpus interesting data set point view since,
one hand, rather large know tags contains related images.
hand, product experts labelling representative images,
noisy annotation process often poor-quality uninteresting images (e.g., logos) randomly
downloaded Web. Thus, analogously characteristics textual corpus,
algorithms must able exploit large-scale statistical information, robust
noise. cleaner illustrative examples concept available
carefully constructed databases ImageNet (see Section 2.3), noisy tag annotations
19

fiBruni, Tran & Baroni

available massive scale sites Flickr12 Facebook,13 want
eventually exploit data important methods work noisy input.
advantage ESP-Game respect ImageNet images associated
concrete noun categories adjectives, verbs nouns related
events (e.g., vacation, party, travel, etc). practical point view, clean
data sets ImageNet still relatively small, making experimentation standard
benchmarks difficult. concrete, looking benchmarks experiment with, mid
2013, ImageNet covers half pairs WordSim353 test set, less
40% Almuhareb-Poesio words. future want explore
extent higher-quality data sources improve image-based models, require larger
databases, benchmarks relying restricted vocabulary.
image samples Figure 6 exemplify different kinds noise characterize
ESP-Game data set. top bottom left top right images
scene cluttered partially occluded. top center image hardly good representative accompanying words building, tower(s) square. Similarly, center
bottom image partially good illustration coin, certainly good
example man! Finally, bottom right image useless visual feature extraction
perspective.
4.2.2 Image-Based Semantic Vector Construction
collect co-occurrence counts target words image-based contexts adopting
BoVW pipeline that, already explained 2.2, particularly convenient order
discretize visual information visual collocates. adopting currently
considered standard implementation BoVW. future, could explore
cutting-edge ways build image-based semantic vectors, local linear encoding
(Wang, Yang, Yu, Lv, Huang, & Gong, 2010) Fisher encoding (Perronnin, Sanchez, &
Mensink, 2010). Chatfield, Lempitsky, Vedaldi, Zisserman (2011) present systematic
evaluation several recent methods.
current implementation composed following steps: (i) Extraction
local descriptors, is, vectors low-level features encode geometric
information area around keypoint, i.e., pixel interest (here, SIFT descriptors); (ii) Constructing vector representation image assigning
local descriptors clusters corresponding visual words, recording distribution
across clusters vector (this presupposes preliminary step clustering
algorithm applied whole image collection sample, determine visual word vocabulary) (iii) Including spatial information representation
spatial binning; (iv) Summing visual word occurrences across list images associated
word label obtain co-occurrence counts associated word label
transforming counts association scores, analogously done text
analysis. process (without spatial binning) schematically illustrated Figure 4,
hypothetical example three images collection labeled
word monkey. details follow.
12. http://www.flickr.com
13. http://www.facebook.com

20

fiMultimodal Distributional Semantics

Dense sampling
pixels
interest

Mapping SIFT
descriptors visual
word clusters

Extracting
local
descriptors

SIFT 4x4

monkey:

0

4

monkey

3

4

0

3

+

Labeled
images

monkey:

2

11

Instance
counts

+

monkey
monkey:

0

3

9

0

monkey:

2

18

12

7

monkey
Total
counts

Figure 4: procedure build image-based semantic vector target word. First,
bag-of-visual-word representation image labeled target word
computed (in case, three images labeled target word monkey).
Then, visual word occurrences across instance counts summed obtain
co-occurrence counts associated target word.

Local Descriptors construct local descriptors pixels interest use ScaleInvariant Feature Transform (SIFT) (Lowe, 1999, 2004). chose SIFT invariance
image scale, orientation, noise, distortion partial invariance illumination changes.
SIFT vector formed measuring local image gradients region around
location orientation feature multiple scales. particular, contents
4 4 sampling subregions explored around keypoint. resulting
16 samples, magnitude gradients 8 orientations calculated, would
already result SIFT feature vector 128 components. However, extract color
SIFT descriptors HSV (Hue, Saturation Value) space (Bosch, Zisserman, & Munoz,
2008). use HSV encodes color information similar way humans
21

fiBruni, Tran & Baroni

do. compute SIFT descriptors HSV component. gives 3128 dimensions
per descriptor, 128 per channel. Color channels averaged obtain final
128-dimensional descriptors. experimented different color scales,
LUV, LAB RGB, obtaining significantly worse performance compared HSV
development set introduced 5.2.1, therefore conduct experiments
them. Van de Sande, Gevers, Snoek (2010) present systematic evaluation color
features.
Instead searching interesting keypoints salient patch detection algorithm,
use computationally intensive thorough dense keypoint sampling
approach, patches fixed size localized regular grid covering whole image
repeated multiple scales. SIFT descriptors computed regular grid every
five pixels, four scales (10, 15, 20, 25 pixel radii) zeroing low contrast descriptors.
extraction use vl_phow command included VLFeat toolbox (Vedaldi
& Fulkerson, 2010). implementation shown close Lowes original
much faster dense feature extraction. Nowak, Jurie, Triggs (2006) report
systematic evaluation different patch sampling strategies.
Importantly, SIFT feature vectors extracted large corpus representative
images populate feature space, subsequently quantized discrete number
visual words clustering. step performed, every SIFT vector (local descriptor)
original new images translated visual word determining
cluster nearest quantized space.
Visual Vocabulary map SIFT descriptors visual words, first cluster local
descriptors extracted images training image corpus 3128-dimensional
space using k-means clustering algorithm, encode descriptor index
cluster (visual word) belongs. k-means common way constructing
visual vocabularies (Grauman & Leibe, 2011). Given set x1 , ..., xn RD n training
descriptors, k-means aims partition n descriptors k sets (k n) minimize
P
cumulative approximation error ni=1 ||xi qi ||2 , K centroids 1 , ..., K RD
data-to-means assignments q1 , ..., qN {1, ..., K}. use approximated version
k-means called Lloyds algorithm (1982) implemented VLFeat toolbox.
construct visual vocabulary extracted SIFT descriptors 100K
images ESP-Game data set. tune parameter k used MEN development
set (see Section 5.2.1). varying k 500 5000 steps 500, found
optimal k 5000. likely performance peaked even 5000
visual words enhancements could attained adopting larger visual vocabularies
via efficient implementations BoVW pipeline, example Chatfield et al.
(2011).
Image Representation Given set descriptors x1 , ..., xn sampled image, let
qi assignment descriptor xi corresponding visual word. bag-ofvisual-words representation image nonnegative vector v Rk vk = |{i :
qi = k}|, q ranging 1 number visual words vocabulary (in
case, 5000). representation vector visual words obtained via hard quantization
(i.e., assignment local descriptor vector single nearest codeword).
22

fiMultimodal Distributional Semantics

Spatial Binning consolidated way introducing weak geometry BoVW use
spatial histograms (Grauman & Darrell, 2005; Lazebnik, Schmid, & Ponce, 2006).
main idea divide image several (spatial) regions perform entire visual
word extraction counting pipeline region concatenate vectors.
experiments spatial regions obtained dividing image 4 4, total
16 regions. Therefore, crossing values k spatial region, increase
feature dimensions 16 times, total 80,000 components vectors.
Co-occurrence Counts Weighting BoVW representations built,
target (textual) word associated list images labeled it;
visual word occurrences across list images summed obtain co-occurrence
counts associated target (textual) word. total, 20,515 target words (those
constitute ESP-Game tags) image-based semantic vector associated.
image-based semantic matrix, text-based one, raw counts
transformed nonnegative LMI. difference LMI computed
target element textual word context element c visual word instead.
Note that, standard textual approach, accumulating visual words
images contain word without taking account fact words might
denote concepts multiple appearances, polysemous even hide homonyms
(our bank vector include visual words extracted river well building pictures).
interesting direction research would cluster images associated
word order distinguish visual senses word, e.g., along lines
done textual models Reisinger Mooney (2010).
4.3 Multimodal Fusion Tuning
performed two separate parameter optimizations, one specifically semantic relatedness task (using MEN development, see Section 5.2.1) specifically
clustering task (using Battig, see Section 5.3.1). determined best model
performing exhaustive search across SVD k (from 24 212 powers 2), FL SL
varying 0 1 (inclusive) steps 0.1 similarly . total, 198 models explored one highest performance development data
chosen. Note tuning performed separately Window2 Window20 models.

4.4 MixLDA
reimplement Feng Lapatas approach (discussed Section 2.3) comparable
setting ours, treat ESP-Game data set mixed-media corpus
image together associated tags constitutes document. image, extract
image-based features procedure described 4.2.2 use words
labeling image obtain text-based features. features stored
term-by-document matrix, image treated document term
either textual tag visual word extracted image. obtain matrix size
90K100K, 10K textual words (the word list resulting intersection
words used experimental data sets), 80K visual words 100K documents (images).
23

fiBruni, Tran & Baroni

Latent Dirichlet Allocation (MixLDA) model trained matrix tuned
MEN development set varying number topics Kt .14 optimal value find
Kt = 128. MixLDA, target word evaluation set represented
vector giving distribution 128 latent topics.

5. Experiments
test semantic representation three different tasks, is, evaluating distribution different kinds semantic relations among words neighbours (5.1), modeling
word relatedness judgments (5.2) clustering words superordinate concepts (5.3).
Together, tasks give us clear idea general quality models
relative contribution visual information meaning representation.
5.1 Differentiation Semantic Relations
acquire qualitative insight well text- image-based models capturing word meaning, test BLESS (Baroni-Lenci Evaluation Semantic Similarity), benchmark recently introduced Baroni Lenci (2011) analyze specific
aspects lexico-semantic knowledge. Rather focusing point estimate quality
model specific semantic task, BLESS allows us assess overall pattern
semantic relations model tends capture. run BLESS evaluation
combining textual visual channels together sanity check semantic
meaningfulness image-based vectors, looking potential complementary information respect text motivate fusion. Note since
combining textual visual sources, tuning parameters report.
5.1.1 Benchmark Method
BLESS contains set 200 pivot words denoting concrete concepts (we use 184 pivots,
since remaining 16 sufficiently large set related words covered
models). pivots, data set contains number related words,
relata, instantiating following 8 common semantic relations pivots: coord:
relatum noun co-hyponym (coordinate) pivot (alligator-lizard);
hyper: relatum noun hypernym (superordinate) pivot (alligatorreptile); mero: relatum noun referring meronym, is, part material
pivot (alligator-teeth); attri: relatum adjective expressing attribute
pivot (alligator-ferocious); event: relatum verb referring action
event involving concept (alligator-swim); ran.n, ran.j ran.v, finally, control
cases pivot matched set random nouns (alligator-trombone), adjectives
(alligator-electronic) verbs (alligator-conclude), respectively.
pivot, BLESS contains set relata category (ranging 7 hypernyms 33 random nouns per pivot average). way, BLESS highlight
broader semantic properties model independently specific preferences.
example, model assigns high score alligator-ferocious model
assigns high score alligator-green correctly treated models picked
14. LDA computed Gensim: http://radimrehurek.com/gensim/

24

fiMultimodal Distributional Semantics

relevant attribute alligators. time, comparison specific relata
selected models allows granular qualitative analysis differences.
Following guidelines Baroni Lenci (2011), analyze semantic model
follows. compute cosine model vectors representing 184
pivots relata, picking relatum highest cosine
8 relations (the nearest hypernym, nearest random noun, etc.). transform
8 similarity scores collected way pivot onto standardized z scores (to
get rid pivot-specific effects), produce boxplot summarizing distribution
scores per relation across 184 pivots (for example, leftmost box first panel
Figure 5 reports distribution 184 standardized cosines nearest coordinate relata
respective pivots). Besides analyzing distributions qualitatively, discuss
significant differences cosines different relation types obtained via
Tukeys Honestly Significance tests, thus correcting multiple pairwise comparisons (Abdi
& Williams, 2010).
5.1.2 Results
Fig. 5, report BLESS nearest relata distributions purely textual model Window20 (the Window2 distribution shows even stronger skew favour coordinate
neighbours) purely visual model call Image next sections. patterns
produced text-based model (left panel) illustrate sensible word meaning profile
look like: coordinates similar terms (an alligator maximally similar
crocodile), followed superordinates (reptile) parts (teeth). Semantically related
adjectives (attri: ferocious) verbs (event: swim) less close pivots, still
random item.
right panel shows distribution relata image-based semantic vectors.
overall pattern quite similar one observed text-based vectors:
clear preference coordinates, followed hypernyms parts, attributes
events, random relata away pivots semantically
meaningful categories. models, coordinates significantly closer relata
hypernyms meronyms, significantly closer attributes events,
turn significantly closer random category. Although difference
hypernyms parts significant either representation, intriguingly
image-based vectors show slight preference imageable parts (teeth)
abstract hypernyms (reptile). difference statistical import one
events attributes, text-based model shows significant preference
events, whereas two categories statistically indistinguishable image-based
model (as see shortly, relative preference latter attributes probably
due tendency pick perceptual adjectives denoting color size).
Looking closely specific relata picked text- image-based models,
striking differences pertain, again, attributes. text- image-based
models picked attribute pivot 20% cases (compare 40%
overlap across non-random relation types). Table 1 reports attributes picked
text- vs. image-based models 20 random cases two mismatch.
25

fiBruni, Tran & Baroni

Image-based semantic vectors

-2

-2

-1

-1

0

0

1

1

2

2

Text-based semantic vectors

COORD

HYPER

MERO

ATTRI

EVENT

RAN.N

RAN.J

RAN.V

COORD

HYPER

MERO

ATTRI

EVENT

RAN.N

RAN.J

RAN.V

Figure 5: Distribution z-normalized cosines words instantiating various relations across
BLESS pivots. Text-based vectors Window20 model.

pivot
cabbage
carrot
cherry
deer
dishwasher
elephant
glider
gorilla
hat
hatchet

text
leafy
fresh
ripe
wild
electric
wild
heavy
wild
white
sharp

image
white
orange
red
brown
white
white
white
black
old
short

pivot
helicopter
onion
oven
plum
sofa
sparrow
stove
tanker
toaster
trout

text
heavy
fresh
electric
juicy
comfortable
wild
electric
heavy
electric
fresh

image
old
white
new
red
old
little
hot
grey
new
old

Table 1: Attributes preferred text- (Window20) vs. image-based models.

26

fiMultimodal Distributional Semantics

immediately clear table that, despite fact pivots nouns
denoting concrete concepts, text-based model almost never picks adjectives denoting
salient perceptual properties (and particular visual properties: white hat leafy
cabbage). text-based model focuses instead encyclopedic properties fresh,
ripe, wild, electric comfortable. line earlier analyses ungrounded
semantics provided text-based models (Andrews et al., 2009; Baroni et al., 2010; Baroni
& Lenci, 2008; Riordan & Jones, 2011), differs greatly trend found
image-based model. 12/20 cases, closest attribute latter model color.
remaining cases, size (short, little), one instance hot and, surprisingly, four
old.
conclude, analysis presented confirms, one hand, hypothesis
image-based distributional vectors contain sufficient information capture network
sensible word meaning relations. other, intriguing differences relations picked text- image-based models, pointing complementarity.
5.2 Word Relatedness
standard distributional semantics literature (Budanitsky & Hirst, 2006; Sahlgren,
2006), assess performance models task predicting degree semantic relatedness two words rated human judges. test models
WS MEN benchmarks.
5.2.1 Benchmarks Method
WS, is, WordSim35315 (see Section 2.1) widely used benchmark constructed
asking 13 subjects rate set 353 word pairs 11-point meaning similarity scale
averaging ratings (e.g., dollar/buck gets high average rating, professor/cucumber
low one). target words cover 252 WS pairs (thus, correlations reported
directly comparable reported studies used WS). However,
text-based models much higher WS coverage (96%). evaluated larger WS
set cover, Window2 Window20 achieve 0.64 0.68 correlations, respectively.
thus comparing multimodal approach purely textual models
state art WS (see results reported Section 2.1 above).
second benchmark use, MEN (for Marco, Elia Nam, resource creators)
developed us, specifically purpose testing multimodal models. created
large data set that, comparable WS benchmarks commonly used
computational semantics community, contains words appear image
labels ESP-Game MIRFLICKR-1M16 collections, thus ensuring full coverage
researchers train visual models resources. MEN consists 3,000 word pairs
[0, 1]-normalized semantic relatedness ratings provided Amazon Mechanical Turk
workers (via CrowdFlower17 interface). example, beach/sand MEN score
0.96, bakery/zebra received 0 score.
15. http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/
16. http://press.liacs.nl/mirflickr/
17. http://crowdflower.com/

27

fiBruni, Tran & Baroni

Compared WS, MEN sufficiently large allow us separate development
test data, avoiding issues overfitting. use indeed 2,000 MEN pairs (development set)
model tuning 1,000 pairs evaluation (test set). Importantly, development
set used find best configuration MEN test set WS.
Thus, WS evaluation illustrates well parameters learned training data
specific data set generalize applied semantic task different
data set.
Models evaluated follows. pair data set, compute cosine
model vectors representing words pair, calculate Spearman
correlation cosines (pooled) human ratings pairs, idea
higher correlation better model simulate relatedness
scores.
MEN Construction earlier version MEN used first time
authors Bruni et al. (2012) since current article first major publication
focus specifically it, recently improved benchmark
extending ratings, provide details constructed.
word pairs constitute MEN randomly selected words occur
least 700 times concatenated ukWaC Wackypedia text corpora least 50
times tags ESP-Game MIRFLICKR-1M tagged image collections. order
avoid picking pairs weakly related, would happen sample
random word pairs list, ranked possible pairs cosines according
text-based model Window20. gather 3000 word pairs needed construction
MEN, subsequently picked first 1000 word pairs, another 1000 sampled
pairs placed 1001 3000 cosine-ranked list last block 1000 pairs
remaining items.
acquire human semantic relatedness judgments, decided ask comparative
judgments two pair exemplars time rather absolute scores single pairs,
done creators WS. constitute natural way evaluate
target pairs, since human judgments comparative nature. person evaluates
given target, vacuum, relation certain context.
Moreover, binary choices preferred make construction right
wrong control items straightforward (see Footnote 18). Operationally, word pair
randomly matched comparison pair coming set 3000 items
rated single Turker either less related comparison item.
validity approach confirmed high annotation accuracy observe
control set,18 high correlation MEN scores ratings collected
Likert scale report below.
18. control items correct annotations created prior running job Amazon Mechanical Turk,
act hidden tests randomly shown Turkers complete job. way,
calculate quality contributors performance reject annotations accuracy
drops certain percentage (we set required minimum precision equal 70%, obtained
almost 100% average accuracy overall). Control items great help train quickly new workers
perform required task. create control items harvested two equally-sized sets word
pairs WS, one containing pairs high relatedness score, one containing pairs
low relatedness score. control item obtained juxtaposing high score pair
low score pair treating pair higher score one selected

28

fiMultimodal Distributional Semantics

instructions, annotators warned sometimes candidate pairs could
contain words related meaning cases asked pick pair
strongly related words (e.g., wheels-car dog-race somewhat related pairs,
first one preferred every car wheels every dog involved
race). cases, annotators could find neither pair contains closely related
words, cases instructed pick pair contained slightly
related words (e.g., neither auction-car cup-asphalt closely related words,
first pair picked fancy vintage cars sold auctions). requested
participants native speakers accepted connecting English
speaking country. cannot guarantee non-natives take part study,
subject filtering techniques based control pairs (see Footnote 18) ensures
data speakers good command English retained.
transform binary preference data relatedness scores retrieved pairs,
evaluated 50 randomly picked comparison pairs, thus received score
50-point scale (given number times 50 pair picked
related two). score subsequently normalized 0 1 dividing
number times pair picked related 50. example, fun-night
chosen related comparison pair 20 times, thus normalized score
given 20 50 = 0.4. Note that, comparison, recorded preference
assigned one two pairs, avoid dependencies final scores assigned
different pairs (that is, times pair selected random comparison item
another pair counted ratings pair).
raters saw MEN pairs matched different random items, number
pairs varying rater rater, possible compute annotator agreement
scores MEN. However, get sense human agreement, first third author
rated 3,000 pairs (presented different random orders) standard 1-7 Likert scale.
Spearman correlation two authors 0.68, correlation average
ratings MEN scores 0.84. one hand, high correlation suggests
MEN contains meaningful semantic ratings. other, taken
upper bound computational models realistically achieve simulating
human MEN judgments.
high-score MEN pairs include pairs terms strictly taxonomically close (cathedral-church: 0.94) terms connected broader semantic
relations, whole-part (flower-petal: 0.92), item related event (boat-fishing: 0.9),
etc. reason, prefer refer MEN semantic relatedness rather
similarity score data set. Note WS capturing broader notion relatedness (Agirre et al., 2009). MEN publicly available downloaded from:
http://clic.cimec.unitn.it/~elia.bruni/MEN.
5.2.2 Results
Table 2 reports correlations MEN testing WS data sets using either
Window2 Window20 textual model. automated tuning method selected k = 29
annotators related. control items manually checked. Examples control items
hotel-word vs. psychology-depression, telephone-communication vs. face-locomotive.

29

fiBruni, Tran & Baroni

Model
Text
Image
NaiveFL
NaiveSL
MixLDA
Textmixed
Imagemixed
TunedFL
TunedSL

Window2
MEN WS
0.73
0.70
0.43
0.36
0.75
0.67
0.76
0.69
0.30
0.23
0.77 0.73
0.55
0.52
0.78 0.72
0.78 0.71

Window20
MEN WS
0.68
0.70
0.43
0.36
0.73
0.67
0.74
0.64
0.30
0.23
0.74 0.75
0.57
0.51
0.76 0.75
0.77 0.72

Table 2: Spearman correlation models MEN WordSim (all coefficients significant p < 0.001). TunedFL model selected automatically MEN
development data; TunedSL automatically tuned fixing SL similarity estimation.

(when textual information comes Window2) k = 210 (with Window20) optimal,
Feature Level (FL) similarity estimation = 0.5 cases (since input
matrices row-normalized, latter setting assigns equal weights textual
visual components). models called TunedFL table. Scoring Level
(SL) strategy (again similar weights assigned two channels, k values
TunedFL) performed slightly worse TunedFL, report results
best SL-based models tuned development MEN data well (TunedSL).
models reported table (NaiveFL, NaiveSL, MixLDA, Textmixed Imagemixed ),
parameters tuned manually order gain insights combination strategies
representing ideas earlier literature.19
first two rows table show results text- image-based models,
mixing. Text shows comparable performances data sets. Image correlates
significantly better MEN WS correlations lower Text,
accordance found earlier studies. next three rows find
results earlier multimodal approaches took consideration (Bruni et al., 2011;
Feng & Lapata, 2010; Leong & Mihalcea, 2011). NaiveFL approach (analogous
Bruni et al.s method), textual visual matrices concatenated without
mixing, performs slightly better Text MEN, attains lower performance WS.
NaiveSL (equivalent Leong Mihalceas summing approach), text
image sources combined scoring level, obtains improvements MEN, loosing
several correlation points WS compared Text.
implementation MixLDA achieves poor results MEN WS. One
might attribute fact Feng Lapatas approach constrained using
source textual visual model image data set poor source
19. Textmixed Imagemixed , best k values found development data.
set 210 textual sources.

30

fiMultimodal Distributional Semantics

Textmixed
TunedFL
TunedSL

Window2
0.47
0.46
0.46

Window20
0.49
0.49
0.47

Table 3: Pearson correlation best multimodal combinations WordSim
subset covered Feng Lapata (2010) (all coefficients significant p <
0.001; Pearson used instead Spearman full comparability Feng
Lapata). models assigned 0 similarity 71/253 pairs
missing vector. Feng Lapata (2010) report 0.32 correlation MixLDA.

textual data. approach however outperforming original MixLDA
large margin latter WS test set, strongly disfavoured. particular,
Feng Lapata (2010) report correlation 0.32 subset 253 WS pairs covered
model. tested system subset, despite fact
missing one vectors 71 pairs (almost one third), models
forced assign 0 cosines cases. Despite huge handicap, models
still attaining much higher correlations original MixLDA Feng Lapata
pairs, illustrated interesting fusion strategies Table 3.
Analyzing effects fusion strategies, first see uniform enhancement MEN WS Textmixed Imagemixed (the models obtained first
performing latent multimodal mixing combined matrix, using textual features Textmixed visual features Imagemixed ). Textmixed reaches best
performance overall WS source textual models, significantly better
Text MEN according two-tailed paired permutation test (Moore & McCabe,
2005). Looking automatically selected TunedFL model, reaches best performance overall. significantly outperforms Text models data sets,
significantly better Textmixed MEN Window20 (the difference approaching significance Window2 well: p = 0.06). TunedSL competitive.
significantly better Text window sizes Textmixed Window20.
noticeably worse TunedFL WS Window20 only, actually
slight advantage MEN Window20 (the difference TunedFL TunedSL
never significant).
worth remarking Textmixed bit worse full fusion models,
still achieves high correlations human judgments extremely high
correlation TunedFL best model ( = 0.98). suggests
benefits multimodality already captured latent mixing. Textmixed attractive
model less parameters whole pipeline compact
TunedFL, since discards visual features using mixing.
Validating Results shown significant improvements visual features added distributional models, one could object improvements due
fact using information: larger number features (higher-dimensional
31

fiBruni, Tran & Baroni

vectors) Feature Level fusion, complex model (two similarity scores
independent variables predict human judgments) Scoring Level fusion. experiments provide evidence respond objection.
First, built purely textual models number features multimodal
models is, instead collecting co-occurrence target terms 30K
frequent content lemmas corpus (see Section 4.1 above), extended list
context items 110K frequent content lemmas. results larger
textual models virtually identical 30K-dimensional vectors reported
Table 2 (correlation Window20 model MEN 0.69 instead 0.68). Thus,
least using large corpus window-based approach, 30K features
pretty much exhausted useful textual information, nature, simply
quantity extra visual features add matters.
answer objection Scoring Level approach using complex model,
two independent variables (text- image-base similarities) instead one, casted
problem standard inferential statistical terms (see, e.g, Baayen, 2008, ch. 6). Specifically, fitted ordinary linear regression models predict MEN WS ratings
text-based similarities vs. text- image-based similarities (for comparability
Spearman correlation results reported above, analyses replicated
transforming ratings similarities ranks). variables highly significant
experiments, and, importantly, sequential F-tests nested models revealed
cases adding image-based similarities explains significantly variance
would expected chance given extra parameter (p < 0.01).
Qualitative Analysis acquire qualitative insights multimodality contributing meaning representation, first picked top 200 related pairs
combined MEN WS norms, would confident indeed highly
related pairs humans, looked, within subset, pairs
pronounced difference cosines Text TunedFL, using Window20
textual source. is, first column Table 4 presents pairs considered
related humans relatedness better captured Text, second column
pairs relatedness better captured TunedFL.
Notice 7/10 relations better captured TunedFL coordinates
synonyms pertaining concrete objects (candy/chocolate, bicycle/bike, apple/cherry,
military/soldier, paws/whiskers, stream/waterfall cheetah/lion), indeed
maximally visually similar (either objects or, case paws/whiskers,
surrounds). purely text-based model, hand, captures relations
times day, that, imageable, well-delimited concrete objects
(dawn/dusk, sunrise/sunset). captures properties concepts expressed adjectives
(dog/canine, skyscraper/tall, cat/feline, pregnancy/pregnant, rain/misty), least one
case spotting relation requires encyclopedic knowledge (grape/wine). thus
hypothesize added value multimodally-enhanced model derives
power vision finding relations concrete objects taxonomic level,
results detecting particularly tight forms relatedness, synonymy
coordination.
32

fiMultimodal Distributional Semantics

Text
dawn/dusk
sunrise/sunset
canine/dog
grape/wine
foliage/plant
foliage/petal
skyscraper/tall
cat/feline
pregnancy/pregnant
misty/rain

TunedFL
pet/puppy
candy/chocolate
paw/pet
bicycle/bike
apple/cherry
copper/metal
military/soldier
paws/whiskers
stream/waterfall
cheetah/lion

Table 4: Top 10 pairs whose relatedness better captured Text (Window20)
vs. TunedFL.

observed one reviewer, given taxonomic nature information captured
multimodal approach, interesting compare future work features
directly extracted linguistic taxonomy, WordNet. observe passing
manually-constructed resource, unlike extracted textual corpora, likely
reflect linguistic perceptual knowledge lexicographers built
it.
Going opposite direction, another reviewer observed might get
mileage combining visual features textual models less taxonomic nature.
hypothesis partially confirmed fact obtain larger relative improvement mixing vision Window20 Window2 (look back Table 2, see
Section 4.1 think narrower window mainly captures taxonomic
relations, larger one broader topical themes). explore conjecture,
re-ran MEN WS experiments combining visual vectors document-based
textual model (i.e., semantic space whose dimensions record number occurrences
words documents). space expected capture mostly topical information,
estimates relatedness basis tendency words occur
documents (Sahlgren, 2006). document-based model alone good
window-based models (it obtained Spearman correlation 0.68 MEN 0.63
WS), combining image-based models led relative improvements comparable
inferior attained Window20 (the best combined-model correlations
0.73 MEN 0.70 WS). conclude that, looking textual models
complementary respect visual information seems reasonable direction
develop multimodal systems cover broader range semantic phenomena, simply
emphasizing topical side textual models evidently suffice.
33

fiBruni, Tran & Baroni

5.2.3 Concreteness Factor Modeling Relatedness Ratings: Pilot
Study
previous experiments, observed trend towards division labour text- image-based models, latter apt capturing similarity
among concrete concepts properties. One strongest limitations current
version framework fact every target word assumed equally perceptually salient consequently uniformly enriched visual information. Intuitively,
might want distinguish instead concrete words, chair cat, require
integration perceptual information representation, abstract words,
consequence absurd, represented purely symbolic/linguistic basis.
Indeed, Recchia Jones (2012) recently presented evidence that, lexical decision
naming tasks, rich physical contexts favour activation concrete concepts, whereas rich
linguistic contexts facilitate activation abstract concepts. follow-up pilot
experiment presented section want pave way systematic introduction concreteness factor multimodal meaning representation. Operationally,
separate abstract concrete word pairs semantic relatedness benchmark
MEN, assessing contribution textual visual information approximating word
meaning two domains independently. Importantly, use automated method
determine word concrete abstract, eye future integration
automatically-determined abstractness score fusion algorithm.
particular, use abstractness scores automatically assigned algorithm recently introduced Turney, Neuman, Assaf, Cohen (2011). Scores calculated
computing difference sum text-based semantic similarities target
word set concrete paradigm words sum semantic similarities
set abstract paradigm words. words (i.e., paradigm words words
abstractness score computed) represented co-occurrence based
matrix gathered large corpus university websites. Co-occurrence counts
transformed Positive Pointwise Mutual Information scores (Church & Hanks, 1990)
resulting matrix smoothed SVD. Pairwise semantic similarity measured
cosines. paradigm words turn selected supervised learning method
trained subject-rated words MRC Psycholinguistic Database Machine Usable
Dictionary (Coltheart, 1981). Examples highly abstract words automatically rated
list purvey: 1.00, sense: 0.96 improbable: 0.92, examples highly concrete
words (i.e., words low abstractness score) donut: 0.00, bullet: 0.07 shoe:
0.10.
abstractness score assigned MEN testing words, divided
data set two subsets, one containing concrete word pairs (MEN-conc, 837
pairs), containing abstract pairs mixed pairs, pairs formed one
concrete one abstract word (MEN-abst, 163 pairs). word considered concrete
abstract score 0.5, abstract otherwise. example, word pair arm-bicycle
considered concrete (with scores 0.33 0.35 respectively), fun-relax considered
abstract (with scores 0.6 0.59 respectively) design-orange considered mixed
(with scores 0.55 0.20 respectively). experimented Window20 purely
34

fiMultimodal Distributional Semantics

Model
Window20
Image
TunedFL

MEN-conc
0.70
0.47
0.78

MEN-abst
0.51
0.37
0.52

MEN-full
0.68
0.43
0.76

Table 5: Spearman correlation models MEN divided concrete abstract
subsets. Results full data set repeated. coefficients significant
p < 0.001.

textual model, Image usual visual model TunedFL trained MEN development
multimodal model.
Table 5 show correlation scores three models two MEN subsets
(as well repeating correlations attain full set). First all, worth
noticing models higher correlations MEN-conc MEN-abst, suggesting
approximating similarity judgments pairs concrete pairs general easier
task distributional semantics (and, suspect, humans well!). Besides broad
effect, observe clear interaction added value visual component
MEN-abst MEN-conc. fact, TunedFL gains 11% performance
MEN-conc compared Window20, performance essentially
text-only model case MEN-abst. indicates visual information
mostly beneficial concrete domain, maintains neutral (timidly positive)
impact abstract domain (recall that, case, MEN-abst contains mixed
pairs).
conclude, section followed qualitative analysis main
relatedness results pilot experiment focusing concreteness factor. showed
divide MEN benchmark concrete abstract subsets, visual
information enhances text-based model concrete domain, impact
strong. exploited automatic scoring function divide data set
concrete abstract subsets. thus see results reporting
validation Turney et al.s algorithm, and, importantly purposes,
encouragement incorporate automated abstractness/concreteness scoring way
model mixes textual visual information word-by-word basis.
5.3 Concept Categorization
verify conclusions reached WS MEN extend different semantic tasks and,
particular, assess whether multimodal approach able capture organize
meaning humans do, use two existing concept categorization benchmarks
call Battig Almuhareb-Poesio (AP), respectively, goal cluster set
(nominal) concepts broader categories, already discussed Section 2.1.
particular, use Battig exclusively tuning (in way used MEN
development set previous section) AP testing. results AP
reported. word relatedness task tuning testing sets quite similar
35

fiBruni, Tran & Baroni

(MEN development MEN testing two subsets data set words
WS similar MEN), task challenging since Battig AP
two independent data sets built following different strategies populated
different kinds concepts, namely concrete unambiguous concepts Battig,
vs. mixture concrete abstract, possibly ambiguous concepts AP. adopted
present challenging training testing regime felt neither data set
sufficient size allow split development testing data. details follow.
5.3.1 Benchmarks Method
Battig benchmark introduced Baroni et al. (2010) based Battig
Montague norms Van Overschelde, Rawson, Dunlosky (2004). consists
83 highly prototypical concepts 10 common concrete categories (up 10 concepts
per class). Battig contains basic-level concepts belonging categories bird (eagle,
owl. . . ), kitchenware (bowl, spoon. . . ) vegetable (broccoli, potato. . . ). version
cover 77 concepts 10 different classes.
AP introduced Almuhareb Poesio (2005) made 402 nouns
21 different WordNet classes. version cover, AP contains 231 concepts
clustered 21 classes vehicle (airplane, car. . . ), time (aeon, future. . . ) social
unit (brigade, nation). data set contains many difficult cases unusual ambiguous
instances class, casuarina samba trees.
sets, following original proponents others, cluster words based
pairwise cosines semantic space defined model using CLUTO toolkit
(Karypis, 2003). use CLUTOs built-in repeated bisections global optimization
method, accepting CLUTOs default values. Cluster quality often evaluated
percentage purity (Zhao & Karypis, 2003). nir number items i-th true
(gold standard) class assigned r-th cluster, n total number items,
k number clusters,
purity =

X
1 i=n
max (nri )
n i=1

words, number items belonging majority true class (i.e., represented
class cluster) summed across clusters divided total number items.
best scenario purity 1 approach 0 cluster quality deteriorates.
Since lack full AP coverage, results report directly comparable
studies used it. However, text-based models perfect coverage,
evaluated full set achieve purities 0.67 (Window2) 0.61 (Window2),
state-of-the-art levels comparable models, reported Section 2.1 above.
So, again, confidently claim improvements achieved multimodality
obtained comparing approach competitive purely textual models.
5.3.2 Results
Table 6 reports percentage purities AP clustering task. best automatically selected model (TunedFL) uses FL similarity estimation previous task,
similar SVD k (27 Window2 29 Window20) (0.5) parameters
36

fiMultimodal Distributional Semantics

Model
Text
Image
NaiveFL
NaiveSL
MixLDA
Textmixed
Imagemixed
TunedFL
TunedSL

Window2
AP
0.73
0.26
0.74
0.65
0.14
0.74
0.35
0.74
0.75

Window20
AP
0.65
0.26
0.64
0.66
0.14
0.67
0.29
0.69
0.69

Table 6: Percentage purities models AP. TunedFL model automatically
selected Battig data; TunedSL automatically tuned fixing SL similarity estimation.

ones found relatedness, suggesting particular parameter choice robust
could used out-of-the-box tasks well. TunedSL best SL-based method
tuning Battig set (same ks TunedFL, = 0.5 Window20 = 0.9
Window2).
Analogously previous semantic task, see Image model alone
level text models, although AP purities significantly chance
(p < 0.05 based simulated distributions random cluster assignment). Thus,
confirmation fact image-based vectors capture important aspects
meaning. previous task, MixLDA achieves poor results.
Looking text-based models enhanced visual information, see general
improvement performance almost multimodal combination strategies, except
NaiveFL Window20 NaiveSL Window2. Even Textmixed benefits
visual smoothing cases, outperformed TunedFL, whose performance
similar TunedSL, actually slightly better Window2. Interestingly, TunedSL outperforms Text Window2 despite fact single combination
strongly unbalanced towards textual similarity ( = 0.9), indicating visual information beneficial even textual information accounts lions share
composed estimate.
relatedness task, adding equal amount textual features instead
image-based ones help Window20 (0.66 purity 110K textual features)
even lowers performance Window2 (0.69 purity). Thus, improvement brought
visual features must attributed quality, quantity.
According two-tailed permutation test, even largest difference TunedFL
Text Window20 significant. might due brittleness purity
statistics leading high variance permutations, possibly suboptimal tuning.
Recall, respect, tuning phase performed rather different data
set (Battig) compared data set eventually evaluated models (AP).
37

fiBruni, Tran & Baroni

However, overall trends encouraging, line found
relatedness study.

6. Conclusion
paper provided extensive introduction new approach distributional semantics named Multimodal Distributional Semantics. multimodal
distributional semantic model integrates traditional text-based representation meaning
information coming vision. way, tries answer critique distributional models lack grounding, since base representation meaning entirely
linguistic input, neglecting statistical information inherent perceptual experience,
humans instead exploit. course, truly multimodal representation meaning
account entire spectrum human senses. hand, line
research still embryonic stage still shortage perceptual data
available techniques automatize processing. why, article,
focused analysis visual perceptual channel, disposal
large data sets effective methods analyze them.
particular, exploited ESP-Game data set, image documents
tagged words describing content. harvest visual information adopted
bag-of-visual-words technique, discretizes image content ways analogous
standard text-based distributional representations. introduced multimodal framework
optimizes text-image fusion data-driven fashion development data.
conducted number experiments assess quality obtained models.
first investigated general semantic properties purely image-based model,
assess overall quality well look information complementary present
text. found systematic differences two modalities, preference
encyclopedic properties text-based model perceptual properties case
image-based model. proceeded test selection models obtained combination
text- image-based representations via multimodal framework. used two
benchmarks word relatedness one benchmark word categorization
cases obtained systematic improvement performance multimodal models
compared models based standalone channels.
Still, looking numerical results, cannot deny improvement performance attained including visual information dramatic. Indeed, pessimistic
interpretation experiments could confirm hypothesis Louwerse
others (e.g., Louwerse, 2011; Louwerse & Connell, 2011; Tillman, Datla, Hutchinson, &
Louwerse, 2012) perceptual information already encoded, sufficient degree,
linguistic data, direct visual features dont bring much table. However, showed
various statistical validation tests important result, namely
adding visual information improves using text alone, robust reliable. think
realistic take-home message experiments reported, establishing
basic result mentioned, drawbacks overcome
work.
First all, deliberately used general semantic benchmarks state-of-the-art text
models, performance computational methods might getting close
38

fiMultimodal Distributional Semantics

ceiling. 0.78 correlation, best models still percentage points go
MEN (estimated upper bound based raters agreement: 0.84, see Section 5.2.1),
improvements bound quite small. Concerning AP benchmark, consider
difficult would even humans categorize casuarina samba among
trees. Indeed, error analysis TunedFL clustering results suggests factors
might lead better performance little vision. example,
model wrongly clusters branch (a social unit according AP) trees, merges
concepts melon peach (fruit AP) mandarin lime (trees). lack
contextual information, hard dispute model choices. Similarly, TunedFL
splits AP animal class cluster small domestic mammals (cats, dogs, kittens,
mice, puppies rats) cluster containing everything else (mostly larger mammals
cows elephants). Again, clustering procedure information
classes searching (e.g., animals general, small animals),
hard see performance could improved thanks better semantic features, visual
kinds. Moreover, data sets include abstract terms, specifically
designed test grounded aspects meaning, visual features might help
most. think made sense start investigation general benchmarks
semantics, opposed ad hoc test sets, show viability multimodal approach.
However, future want focus experimental challenges strengths
visually-enhanced models might emerge clearly. took first step direction
Bruni et al. (2012), focused specifically visual features help
processing literal metaphorical colours.
Another factor take account large-scale image data sets
techniques extract features infancy, might able
improve performance developing better image-based models. Regarding data
sets, explained Section 4.2.1 chose ESP-Game, obviously
sub-optimal many respects, discuss there. Regarding features,
mentioned beginning Section 4.2.2, recent advances image processing,
Fisher encoding, might lead better ways extract information contained images.
experiments, compared automatically tuned multimodal model
settings, showing overall stability superiority, two important caveats.
First, experiments good results already obtained using visual information
smooth text features, without using visual features directly (what called
Textmixed approach). Note already multimodal approach, visual
information crucially used improve quality textual dimensions, indeed
weve seen consistently outperforms using non-multimodally-smoothed text features.
Textmixed good full tuned model, simplicity makes
attractive approach.
Second, although automated tuning led us prefer Feature Level Scoring Level
fusion development sets, TunedSL clearly worse TunedFL one case
(with Window20 WS), suggesting that, least evaluation settings considered,
difference two fusion strategies crucial. However, comparing
naive versions strategies tuned ones across results, clear
tuning important obtain consistently good performance, confirming usefulness
general fusion architecture.
39

fiBruni, Tran & Baroni

conducted pilot experiment concreteness/abstractness factor, assess
impact meaning representation check good candidate new
weighted-fusion strategy plan investigate future. fact, current version
multimodal framework, parametrization combination strategy works
global level (i.e, words). could productive combine textual
visual information word-by-word basis, tune two modality contributions
meaning representation depending particular nature single word. Concrete
vs. abstract constitute neat binary distinction words, rather
thought ideal distinction offset less abrupt, real-world formulation,
takes account degree according certain word considered concrete
abstract. doubt words backdrop, squalor sharp evoke
perceptual cues gathered experience them, time
unequivocal amount abstractness accompanying them. plan refine
concreteness scoring method order make focus specifically imageable
components concreteness, expect relevant visual channel.
developments focus techniques extract image-based semantic
models. example, pilot study (Bruni et al., 2012), exploit new methods developed
computer vision improve object recognition capturing object location (Felzenszwalb,
Girshick, McAllester, & Deva Ramanan, 2010; de Sande, Uijlings, Gevers, & Smeulders,
2011). show possible extract better image-based semantic vectors first
localizing objects denoted words extracting visual information
object location surround independently. Interestingly, discovered
image-based semantic vectors extracted object surround effective
based object location tested word relatedness task. example,
fact pictures containing deers wolves depict similar surrounds tells us
creatures live similar environments, thus likely somewhat
related. seen distributional hypothesis transposed images: objects
semantically similar occur similar visual contexts. Nevertheless, work
considered proof concept, since experimented 20 words only. future
studies test larger number words.
obviously much room improvement, many exciting routes
explore, hope framework empirical results presented study
convinced reader multimodal distributional semantics promising avenue
pursue development human-like models meaning.

Acknowledgments
thank Jasper Uijlings valuable suggestions image analysis pipeline.
lot code many ideas came Giang Binh Tran, owe Gemma Boleda many
ideas useful comments. Peter Turney kindly shared abstractness score list
used Section 5.2.3 Yair Neuman generously helped preliminary analysis
impact abstractness multimodal models. Mirella Lapata kindly made
WordSim353 set used experiments Feng Lapata (2010) available us.
thank JAIR associated editor reviewers helpful suggestions constructive
40

fiMultimodal Distributional Semantics

criticism. Google partially funded project Google Research Award third
author. BLESS study Section 5.1.2 first presented Bruni et al. (2012).

References
Abdi, H., & Williams, L. (2010). Newman-Keuls Tukey test. Salkind, N., Frey, B., &
Dougherty, D. (Eds.), Encyclopedia Research Design, pp. 897904. Sage, Thousand
Oaks, CA.
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasa, M., & Soroa, A. (2009). study
similarity relatedness using distributional WordNet-based approaches.
Proceedings HLT-NAACL, pp. 1927, Boulder, CO.
Almuhareb, A., & Poesio, M. (2005). Concept learning categorization web.
Proceedings CogSci, pp. 103108, Stresa, Italy.
Andrews, M., Vigliocco, G., & Vinson, D. (2009). Integrating experiential distributional
data learn semantic representations. Psychological Review, 116 (3), 463498.
Baayen, H. (2008). Analyzing Linguistic Data: Practical Introduction Statistics using
R. Cambridge University Press, Cambridge, UK.
Barnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei, D., & Jordan, M. (2003). Matching words pictures. Journal Machine Learning Research, 3, 11071135.
Baroni, M., Barbu, E., Murphy, B., & Poesio, M. (2010). Strudel: distributional semantic
model based properties types. Cognitive Science, 34 (2), 222254.
Baroni, M., & Lenci, A. (2008). Concepts properties word spaces. Italian Journal
Linguistics, 20 (1), 5588.
Baroni, M., & Lenci, A. (2010). Distributional Memory: general framework corpusbased semantics. Computational Linguistics, 36 (4), 673721.
Baroni, M., & Lenci, A. (2011). BLESSed distributional semantic evaluation.
Proceedings EMNLP GEMS Workshop, pp. 110, Edinburgh, UK.
Barsalou, L. (2008). Grounded cognition. Annual Review Psychology, 59, 617645.
Berg, T., Berg, A., & Shih, J. (2010). Automatic attribute discovery characterization
noisy Web data. ECCV, pp. 663676, Crete, Greece.
Bergsma, S., & Goebel, R. (2011). Using visual information predict lexical preference.
Proceedings RANLP, pp. 399405, Hissar, Bulgaria.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
Bosch, A., Zisserman, A., & Munoz, X. (2007). Image classification using random forests
ferns. Proceedings ICCV, pp. 18, Rio de Janeiro, Brazil.
Bosch, A., Zisserman, A., & Munoz, X. (2008). Scene classification using hybrid generative/discriminative approach. IEEE Transactions Pattern Analysis Machine
Intelligence, 30 (4).
Bruni, E., Boleda, G., Baroni, M., & Tran, N. K. (2012). Distributional semantics
Technicolor. Proceedings ACL, pp. 136145, Jeju Island, Korea.
41

fiBruni, Tran & Baroni

Bruni, E., Bordignon, U., Liska, A., Uijlings, J., & Sergienya, I. (2013). Vsem: open
library visual semantics representation. Proceedings ACL, Sofia, Bulgaria.
Bruni, E., Tran, G. B., & Baroni, M. (2011). Distributional semantics text images.
Proceedings EMNLP GEMS Workshop, pp. 2232, Edinburgh, UK.
Bruni, E., Uijlings, J., Baroni, M., & Sebe, N. (2012). Distributional semantics eyes:
Using image analysis improve computational representations word meaning.
Proceedings ACM Multimedia, pp. 12191228, Nara, Japan.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39, 510
526.
Bullinaria, J., & Levy, J. (2012). Extracting semantic representations word cooccurrence statistics: Stop-lists, stemming SVD. Behavior Research Methods,
44, 890907.
Burgess, C. (2000). Theory operational definitions computational memory models:
response Glenberg Robertson. Journal Memory Language, 43 (3),
402408.
Caicedo, J., Ben-Abdallah, J., Gonzlez, F., & Nasraoui, O. (2012). Multimodal representation, indexing, automated annotation retrieval image collections via nonnegative matrix factorization. Neurocomputing, 76 (1), 5060.
Chatfield, K., Lempitsky, V., Vedaldi, A., & Zisserman, A. (2011). devil
details: evaluation recent feature encoding methods. Proceedings BMVC,
Dundee, UK.
Church, K., & Hanks, P. (1990). Word association norms, mutual information, lexicography. Computational Linguistics, 16 (1), 2229.
Clark, S. (2013). Vector space models lexical meaning. Lappin, S., & Fox, C. (Eds.),
Handbook Contemporary Semantics, 2nd ed. Blackwell, Malden, MA. press.
Coltheart, M. (1981). MRC psycholinguistic database. Quarterly Journal Experimental Psychology, 33.
Connolly, A., Gleitman, L., & Thompson-Schill, S. (2007). Effect congenital blindness
semantic representation everyday concepts. Proceedings National
Academy Sciences, 104 (20), 82418246.
Csurka, G., Dance, C., Fan, L., Willamowski, J., & Bray, C. (2004). Visual categorization
bags keypoints. Workshop Statistical Learning Computer Vision,
ECCV, pp. 122, Prague, Czech Republic.
Curran, J., & Moens, M. (2002). Improvements automatic thesaurus extraction.
Proceedings ACL Workshop Unsupervised Lexical Acquisition, pp. 5966,
Philadelphia, PA.
42

fiMultimodal Distributional Semantics

de Sande, K. V., Uijlings, J., Gevers, T., & Smeulders, A. (2011). Segmentation selective
search object recognition. Proceedings ICCV, pp. 18791886, Barcelona,
Spain.
de Vega, M., Glenberg, A., & Graesser, A. (Eds.). (2008). Symbols Embodiment: Debates
Meaning Cognition. Oxford University Press, Oxford, UK.
Deng, J., Dong, W., Socher, R., Li, L.-J., & Fei-Fei, L. (2009). Imagenet: large-scale
hierarchical image database. Proceedings CVPR, pp. 248255, Miami Beach,
FL.
Dumais, S. (2003). Data-driven approaches information access. Cognitive Science, 27,
491524.
Erk, K. (2012). Vector space models word meaning phrase meaning: survey..
Language Linguistics Compass, 6 (10), 635653.
Escalante, H. J., Hrnadez, C. A., Sucar, L. E., & Montes, M. (2008). Late fusion heterogeneous methods multimedia image retrieval. Proceedings ICMR, Vancouver,
Canada.
Evert, S. (2005). Statistics Word Cooccurrences. Dissertation, Stuttgart University.
Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells story: Generating sentences images.
Proceedings ECCV, Crete, Greece.
Felzenszwalb, P., Girshick, R., McAllester, D., & Deva Ramanan, D. (2010). Object detection discriminatively trained part based models. IEEE Transactions Pattern
Analysis Machine Intelligence, 32, 16271645.
Feng, Y., & Lapata, M. (2010). Visual information semantic representation. Proceedings HLT-NAACL, pp. 9199, Los Angeles, CA.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin,
E. (2002). Placing search context: concept revisited. ACM Transactions
Information Systems, 20 (1), 116131.
Firth, J. R. (1957). Papers Linguistics, 1934-1951. Oxford University Press, Oxford,
UK.
Fodor, J. (1975). Language Thought. Crowell Press, New York.
Glenberg, A., & Robertson, D. (2000). Symbol grounding meaning: comparison
high-dimensional embodied theories meaning. Journal Memory Language, 3 (43), 379401.
Grauman, K., & Darrell, T. (2005). pyramid match kernel: Discriminative classification
sets image features. Proceedings ICCV, pp. 14581465, Beijing, China.
Grauman, K., & Leibe, B. (2011). Visual Object Recognition. Morgan & Claypool, San
Francisco.
Grefenstette, G. (1994). Explorations Automatic Thesaurus Discovery. Kluwer, Boston,
MA.
43

fiBruni, Tran & Baroni

Griffin, L., Wahab, H., & Newell, A. (2013). Distributional learning appearance. PLoS
ONE, 8 (2). Published online: http://www.plosone.org/article/info:doi/10.
1371/journal.pone.0058074.
Griffiths, T., Steyvers, M., & Tenenbaum, J. (2007). Topics semantic representation.
Psychological Review, 114, 211244.
Hansen, T., Olkkonen, M., Walter, S., & Gegenfurtner, K. (2006). Memory modulates color
appearance. Nature Neuroscience, 9, 13671368.
Harnad, S. (1990). symbol grounding problem. Physica D: Nonlinear Phenomena,
42 (1-3), 335346.
Harris, Z. (1954). Distributional structure. Word, 10 (2-3), 14561162.
Johns, B., & Jones, M. (2012). Perceptual inference global lexical similarity. Topics
Cognitive Science, 4 (1), 103120.
Karypis, G. (2003). CLUTO: clustering toolkit. Tech. rep. 02-017, University Minnesota
Department Computer Science.
Kaschak, M., Madden, C., Therriault, D., Yaxley, R., Aveyard, M., Blanchard, A., & Zwaan,
R. (2005). Perception motion affects language processing. Cognition, 94, B79B89.
Kievit-Kylar, B., & Jones, M. (2011). Semantic Pictionary project. Proceedings
CogSci, pp. 22292234, Austin, TX.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011).
Baby talk: Understanding generating simple image descriptions. Proceedings
CVPR, Colorado Springs, MSA.
Landauer, T., & Dumais, S. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge. Psychological
Review, 104 (2), 211240.
Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags features: Spatial pyramid
matching recognizing natural scene categories. Proceedings CVPR, pp. 2169
2178, Washington, DC.
Leong, C. W., & Mihalcea, R. (2011). Going beyond text: hybrid image-text approach
measuring word relatedness. Proceedings IJCNLP, pp. 14031407.
Lloyd, S. (1982). Least squares quantization PCM. IEEE Transactions Information
Theory, 28, 129137.
Louwerse, M. (2011). Symbol interdependency symbolic embodied cognition. Topics
Cognitive Science, 3, 273302.
Louwerse, M., & Connell, L. (2011). taste words: Linguistic context perceptual
simulation predict modality words. Cognitive Science, 35, 381398.
Lowe, D. (1999). Object recognition local scale-invariant features. Proceedings
ICCV, pp. 11501157.
Lowe, D. (2004). Distinctive image features scale-invariant keypoints. International
Journal Computer Vision, 60 (2).
44

fiMultimodal Distributional Semantics

Lowe, W. (2001). Towards theory semantic space. Proceedings CogSci, pp. 576581,
Edinburgh, UK.
Lund, K., & Burgess, C. (1996). Producing high-dimensional semantic spaces lexical
co-occurrence. Behavior Research Methods, 28, 203208.
Manning, C., Raghavan, P., & Schtze, H. (2008). Introduction Information Retrieval.
Cambridge University Press, Cambridge, UK.
Manning, C., & Schtze, H. (1999). Foundations Statistical Natural Language Processing.
MIT Press, Cambridge, MA.
McDonald, S., & Brew, C. (2004). distributional model semantic context effects
lexical processing. Proceedings ACL, pp. 1724, Barcelona, Spain.
McRae, K., Cree, G., Seidenberg, M., & McNorgan, C. (2005). Semantic feature production
norms large set living nonliving things. Behavior Research Methods, 37 (4),
547559.
Miller, G., & Charles, W. (1991). Contextual correlates semantic similarity. Language
Cognitive Processes, 6 (1), 128.
Moore, D., & McCabe, G. (2005). Introduction Practice Statistics (5 edition).
Freeman, New York.
Murphy, G. (2002). Big Book Concepts. MIT Press, Cambridge, MA.
Nelson, D., McEvoy, C., & Schreiber, T. (1998). University South Florida word association, rhyme, word fragment norms. http://www.usf.edu/FreeAssociation/.
Nister, D., & Stewenius, H. (2006). Scalable recognition vocabulary tree. Proceedings 2006 IEEE Computer Society Conference Computer Vision Pattern
Recognition - Volume 2, CVPR 06, pp. 21612168.
Nowak, E., Jurie, F., & Triggs, B. (2006). Sampling strategies bag-of-features image
classification. Proceedings ECCV, pp. 490503, Graz, Austria.
Pad, S., & Lapata, M. (2007). Dependency-based construction semantic space models.
Computational Linguistics, 33 (2), 161199.
Pad, U., Pad, S., & Erk, K. (2007). Flexible, corpus-based modelling human plausibility
judgements. Proceedings EMNLP, pp. 400409, Prague, Czech Republic.
Pecher, D., Zeelenberg, R., & Raaijmakers, J. (1998). pizza prime coin? Perceptual
priming lexical decision pronunciation. Journal Memory Language, 38,
401418.
Perronnin, F., Sanchez, J., & Mensink, T. (2010). Improving fisher kernel large-scale
image classification. Proceedings ECCV, pp. 143156, Berlin, Heidelberg.
Pham, T.-T., Maillot, N., Lim, J.-H., & Chevallet, J.-P. (2007). Latent semantic fusion
model image retrieval annotation. Proceedings CIKM, pp. 439443,
Lisboa, Portugal.
Poesio, M., & Almuhareb, A. (2005). Identifying concept attributes using classifier.
Proceedings ACL Workshop Deep Lexical Semantics, pp. 1827, Ann Arbor,
MI.
45

fiBruni, Tran & Baroni

Pulvermueller, F. (2005). Brain mechanisms linking language action. Nature Reviews
Neuroscience, 6, 576582.
Radinsky, K., Agichtein, E., Gabrilovich, E., & Markovitch, S. (2011). word time:
computing word relatedness using temporal semantic analysis. Proceedings
WWW, pp. 337346, Hyderabad, India.
Rapp, R. (2003). Word sense discovery based sense descriptor dissimilarity. Proceedings 9th MT Summit, pp. 315322, New Orleans, LA.
Recchia, G., & Jones, M. (2012). semantic richness abstract concepts. Frontiers
Human Neuroscience, 6 (315).
Reisinger, J., & Mooney, R. J. (2010). Multi-prototype vector-space models word meaning. Proceedings NAACL, pp. 109117, Los Angeles, CA.
Riordan, B., & Jones, M. (2011). Redundancy perceptual linguistic experience:
Comparing feature-based distributional models semantic representation. Topics
Cognitive Science, 3 (2), 143.
Rothenhusler, K., & Schtze, H. (2009). Unsupervised classification dependency
based word spaces. Proceedings EACL GEMS Workshop, pp. 1724, Athens,
Greece.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates synonymy. Communications ACM, 8 (10), 627633.
Sahlgren, M. (2005). introduction random indexing. http://www.sics.se/~mange/
papers/RI_intro.pdf.
Sahlgren, M. (2006). Word-Space Model. Dissertation, Stockholm University.
Sahlgren, M. (2008). distributional hypothesis. Italian Journal Linguistics, 20 (1),
3353.
Schtze, H. (1997). Ambiguity Resolution Natural Language Learning. CSLI, Stanford,
CA.
Silberer, C., & Lapata, M. (2012). Grounded models semantic representation. Proceedings EMNLP-CoNLL, pp. 14231433, Jeju, Korea.
Sivic, J., & Zisserman, A. (2003). Video Google: text retrieval approach object matching videos. Proceedings ICCV, pp. 14701477, Nice, France.
Steyvers, M. (2010). Combining feature norms text data topic models. Acta
Psychologica, 133 (3), 234243.
Therriault, D., Yaxley, R., & Zwaan, R. (2009). role color diagnosticity object
recognition representation. Cognitive Processing, 10 (4), 335342.
Tillman, R., Datla, V., Hutchinson, S., & Louwerse, M. (2012). head toe: Embodiment statistical linguistic frequencies. Proceedings CogSci, pp.
24342439, Austin, TX.
Turney, P., Neuman, Y., Assaf, D., & Cohen, Y. (2011). Literal metaphorical sense
identification concrete abstract context. Proceedings EMNLP, pp.
680690, Edinburgh, UK.
46

fiMultimodal Distributional Semantics

Turney, P., & Pantel, P. (2010). frequency meaning: Vector space models semantics. Journal Artificial Intelligence Research, 37, 141188.
Van de Sande, K., Gevers, T., & Snoek, C. (2010). Evaluating color descriptors object
scene recognition. IEEE Transactions Pattern Analysis Machine Intelligence,
32 (9), 15821596.
Van Overschelde, J., Rawson, K., & Dunlosky, J. (2004). Category norms: updated
expanded version Battig Montague (1969) norms. Journal Memory
Language, 50, 289335.
Vedaldi, A., & Fulkerson, B. (2010). Vlfeat open portable library computer
vision algorithms. Proceedings ACM Multimedia, pp. 14691472, Firenze, Italy.
Von Ahn, L. (2006). Games purpose. Computer, 29 (6), 9294.
Vreeswijk, D. T., Huurnink, B., & Smeulders, A. W. (2011). Text image subject
classifiers: dense works better. Proceedings ACM Multimedia, pp. 14491452,
Scottsdale, AZ.
Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., & Gong, Y. (2010). Locality-constrained
linear coding image classification. Proceedings CVPR, pp. 33603367, San
Francisco, CA.
Weeds, J. (2003). Measures Applications Lexical Distributional Similarity. Ph.D.
thesis, Department Informatics, University Sussex.
Wittgenstein, L. (1953). Philosophical Investigations. Blackwell, Oxford, UK. Translated
G.E.M. Anscombe.
Yang, J., Jiang, Y.-G., Hauptmann, A., & Ngo, C.-W. (2007). Evaluating bag-of-visualwords representations scene classification. Wang, J. Z., Boujemaa, N., Bimbo,
A. D., & Li, J. (Eds.), Multimedia Information Retrieval, pp. 197206. ACM.
Zhao, Y., & Karypis, G. (2003). Criterion functions document clustering: Experiments
analysis. Tech. rep. 01-40, University Minnesota Department Computer
Science.

47


