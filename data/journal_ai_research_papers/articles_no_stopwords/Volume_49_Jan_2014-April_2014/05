Journal Artificial Intelligence Research 49 (2014) 601-633

Submitted 10/13; published 04/14

Algorithms Applications
Same-Decision Probability
Suming Chen
Arthur Choi
Adnan Darwiche

suming@cs.ucla.edu
aychoi@cs.ucla.edu
darwiche@cs.ucla.edu

Computer Science Department
University California, Los Angeles
Los Angeles, CA 90095

Abstract
making decisions uncertainty, optimal choices often difficult
discern, especially enough information gathered. Two key questions
regard relate whether one stop information gathering process commit
decision (stopping criterion), not, information gather next (selection
criterion). paper, show recently introduced notion, Same-Decision
Probability (SDP), useful stopping selection criterion, provide additional insight allow robust decision making variety scenarios.
query shown highly intractable, PPPP -complete, exemplary
class queries correspond computation certain expectations. propose first exact algorithm computing SDP, demonstrate effectiveness
several real synthetic networks. Finally, present new complexity results,
complexity computing SDP models Naive Bayes structure. Additionally,
prove computing non-myopic value information complete
complexity class computing SDP.

1. Introduction
Probabilistic graphical models often used model variety decision problems,
e.g., medical diagnosis (Pauker & Kassirer, 1980; Kahn, Roberts, Shaffer, & Haddawy,
1997; van der Gaag & Coupe, 1999), fault diagnosis (Lu & Przytula, 2006), classification
(Friedman, Geiger, & Goldszmidt, 1997; Ramoni & Sebastiani, 2001; Jordan, 2002), troubleshooting (Heckerman, Breese, & Rommelse, 1995), educational diagnosis (Butz, Hua, &
Maguire, 2004; Arroyo & Woolf, 2005; Millan, Descalco, Castillo, Oliveira, & Diogo, 2013),
intrusion detection (Kruegel, Mutz, Robertson, & Valeur, 2003; Modelo-Howard,
Bagchi, & Lebanon, 2008). similar applications, decision maker typically
position must decide tests perform observations make
order make better informed decision. Perhaps critically, decision maker must
decide stop making observations commit particular decision.
Same-Decision Probability (SDP) recently proposed Darwiche Choi
(2010), order help quantify robustness decision, context decisionmaking Bayesian networks. short, SDP probability would make
decision, perform observations yet made.
such, SDP treated measure decisions robustness respect
c
2014
AI Access Foundation. rights reserved.

fiChen, Choi & Darwiche

unknown variables, quantifying confidence would make decision, even
made observations.
paper, show apply SDP tool information gathering,
particular, way determine stop information gathering (as stopping criterion), not, pieces information gather next (as selection criterion).
compare SDP classical stopping selection criteria illustrative examples.
instance, demonstrate SDP distinguish stable unstable
decisions indistinguishable classical criteria. Additionally, show
scenarios classical criteria may call performing observations,
SDP indicates decision unlikely change.
Notably, SDP shown highly intractable, Choi, Xue, Darwiche
(2012), exact computation SDP limited toy examples,
variables, brute-force enumeration. paper, propose first exact
algorithm computing SDP. algorithm applied real-world networks
scope brute-force enumeration previously proposed approximation
algorithms, applied synthetic networks many 100 variables.
provide new complexity results SDP, highlight relative
intractability (even Naive Bayes networks), relationship broader class
expectation computation problems, emphasizing broader importance developing
effective algorithms SDP related problems.
paper thus structured follows. first introduce notation discuss
common stopping selection criteria Section 2. review previously introduced work SDP Section 3. Section 4, discuss SDP applied
stopping criterion selection criterion. Section 5, present novel
exact algorithm computing SDP discuss experimental results Section 6.
Section 7, present recent complexity results SDP. conclude
paper Section 8.

2. Related Work
making decisions uncertainty, may difficult finalize decision
presence unobserved variables. Given unobserved variables, two fundamental questions. first question whether, given current observations, decision
maker ready commit decision. refer stopping criterion
making decision. Assuming stopping criterion met, second question
additional observations made decision maker ready make decision. typically requires selection criterion based measure quantifying
observations value information (VOI). section, first introduce necessary
notation, review commonly used stopping selection criteria.
2.1 Notation
Throughout paper, use standard notation variables instantiations,
variables denoted upper case letters X instantiations lower case
letters x. Additionally, sets variables denoted bold upper case letters X
instantiations bold lower case letters x. assume state world
602

fiAlgorithms Applications Same-Decision Probability

described random variables X, evidence E X includes known variables,
hidden variables U X include unknown variables. definition, E U =
EU = X. often discuss ramifications observing subset hidden variables
H U decision making. Furthermore, use U denote main hypothesis
variable forms basis decision.1
2.2 Stopping Criterion
Given hidden variables model choice whether
observe subset, stopping criterion determines stop process
information gathering commit decision. Note concerned making
decision based hypothesis variable, state patients health.
stopping criterion, basic approach used variety domains commit
decision belief certain event crosses threshold, done
Pauker Kassirer (1980), Kruegel et al. (2003), Lu Przytula (2006). However,
approach may robust, observations may cause belief
event fall threshold. Van der Gaag Bodlaender (2011) note possibility
pose STOP problem, asks whether present evidence gathered
sufficient diagnosis, exists relevant evidence
gathered.
approaches involve ensuring uncertainty surrounding decision variable
sufficiently reduced. instance, Gao Koller (2011) stop information gathering
1) conditional entropy interest variable reduced beyond threshold 2)
margin first second likely states interest variable
threshold. case, clear threshold-based stopping criteria ubiquitous
decision making uncertainty.
Alternatively, several stopping criteria involve existence
budget, abstract quantity represent available resources
used information gathering. budget may representative number
observations allowed (Modelo-Howard et al., 2008; Munie & Shoham, 2008; Yu,
Krishnapuram, Rosales, & Rao, 2009; Chen, Low, Tan, Oran, Jaillet, Dolan, & Sukhatme,
2012a), terms monetary amount may spent observations varying
cost (Greiner, Grove, & Roth, 2002; Krause & Guestrin, 2009; Bilgic & Getoor, 2011).
context budget, general stopping criterion continue make observations
budget completely expended, done Modelo-Howard et al. (2008)
Munie Shoham (2008). Krause Guestrin (2009) Bilgic Getoor (2011) note
budget expended caveat value information
observation least cost observation.
2.3 Selection Criterion: Value Information
stopping criterion determine observations necessary, selection
criterion used determine variables selected observation.
Ideally, want observe variables give us additional information regards
1. work presented paper extended case multiple hypothesis variables,
focus case one hypothesis variable simplicity.

603

fiChen, Choi & Darwiche

decision variable. However, due resource constraints (such limited budget)
often possible. basic approach, common selection criterion select
observations minimize conditional entropy decision variable (Vomlel,
2004; Lu & Przytula, 2006; Krause & Guestrin, 2009; Yu et al., 2009; Zhang & Ji, 2010;
Gao & Koller, 2011; Ognibene & Demiris, 2013; Shann & Seuken, 2013). entropy
variable X defined as:
H(X) =

X

Pr (x) log Pr (x)

(1)

x

measure uncertainty variables state entropy variable
high, means much uncertainty value variable takes.2
uncertainty decision variables true state makes difficult make decision. Thus,
natural selection criterion observe variables minimize conditional entropy
decision variable, conditional entropy variable given variable X defined
as:
H(D | X) =

X

H(D | x)Pr (x)

(2)

x

conditional entropy thus expectation entropy would
observing X. similar selection criterion observe variables greatly
increase margin posterior probabilities first second most-likely
states decision variable (Krause & Guestrin, 2009).
selection criteria involve utilizing notion value information (VOI) order
quantify value various observations (Lindley, 1956; Stratonovich, 1965; Howard,
1966; Raiffa, 1968). VOI set variables depend various measures.
two example selection criteria discussed, measures would entropy
margins confidence. instance, observing variable X would reduce conditional
entropy H(D | X) observing variable X would (H(D | X) < H(D | X )),
value observing X would higher.
Krause Guestrin (2009) define general notion VOI based different
reward functions. particular, given arbitrary reward function R,3 hypothesis variable
D, evidence e, VOI observing hidden variables H is:
V(R, D, H, e) = ER(R, D, H, e) R(Pr (D | e))

(3)


ER(R, D, H, e) =

X

R(Pr (D | h, e))Pr (h | e)

(4)

h

expected reward observing variables H R(Pr (D | e)) reward
observed variables H. definition, reward function used Lu
2. information theory, logarithm typically assumed base-2 (Cover & Thomas, 1991),
assume throughout paper convenience.
3. reward function assumed take input probability distribution hypothesis variable,
Pr (D), return numeric value. discuss reward functions Section 7.2.

604

fiAlgorithms Applications Same-Decision Probability




+


Pr (D | E1 = +, E2 = +)
0.880952
0.119048

X1

X2

E1

E2

H1

H2

Figure 1: simple Bayesian network, sensor readings {E1 = +, E2 = +}. Variables H1
H2 represent health sensors E1 E2 . left posterior
decision variable D. Network CPTs found Appendix C Figure 13.

Przytula (2006) Krause Guestrin (2009) select variables order minimize
conditional entropy R(Pr (D | e)) = H(D | e), maximizing expected
reward observing variables H equivalent minimizing conditional entropy
H(D | H). possible reward functions involve utility-based reward functions
threshold-based reward functions (Munie & Shoham, 2008).4
Note vast majority selection criteria use myopic approach,
possible observations, one observation considered time, observation
highest VOI selected time. approach greedy short-sighted
optimal VOI computed computing non-myopically (Bilgic & Getoor,
2011). discuss usage non-myopic VOI Appendix A.1.

3. Same-Decision Probability
Same-Decision Probability (SDP) initially introduced Darwiche Choi (2010)
confidence measure threshold-based decisions Bayesian networks noisy
sensor readings. Prior formally defining SDP, first show example provide
intuition. Consider Bayesian network Figure 1, models scenario involving
hypothesis variable D, two noisy sensors E1 E2 influence belief
hypothesis d. Networks typically used compute belief hypothesis
given sensor readings, Pr (d | e). basis whether make decision often
depends whether posterior probability hypothesis surpasses
threshold (Hamscher, Console, & de Kleer, 1992; Heckerman et al., 1995; Kruegel
et al., 2003; Lu & Przytula, 2006).
Figure 1 shows particular reading two sensors resulting belief Pr (D = + |
E1 = +, E2 = +). Suppose threshold = 0.6, Pr (d | e) , would make
certain decision. Notice Figure 1 health sensors modeled variables
H1 H2 . sensor either truthful, stuck positive (readings always display +),
lying (readings show opposite value actual value) (Darwiche & Choi, 2010).
variables observed, could informed us trustworthiness
4. reward functions, see list provided Krause Guestrin (2009).

605

fiChen, Choi & Darwiche

H1

p
l

p
l

p
l

H2



p
p
p
l
l
l

Pr (h | e)
0.781071
0.096429
0.001071
0.096429
0.021429
0.001190
0.001071
0.001190
0.000119

Pr (d | h, e)
0.90
0.82
0.10
0.90
0.50
0.10
0.90
0.18
0.10

Table 1: Scenarios h sensor readings e = {E1 = +, E2 = +} network Figure 1,
H = {H1 , H2 }. Cases threshold = 0.6 bold. Note t, p, l
respectively represent truthful, stuck positive, lying sensor.

sensors E1 E2 thus allow us make better decision. want make
more-informed decision based probability Pr (d | h, e) instead making decision
based Pr (d | e).
Consider Table 1, enumerates possible health states sensors.
four cases probability hypothesis pass threshold (in bold),
leading decision. five scenarios, different decision would
made. SDP thus probability four scenarios decision
would made. example, SDP is:
0.781071 + 0.096429 + 0.096429 + 0.001071 = 0.975
indicating relatively robust decision.
Choi et al. (2012) define SDP formally as:
Definition 1 (Same-Decision Probability). Let N Bayesian network conditioned
evidence e, given hypothesis d, threshold , set
unobserved variables H. Suppose making decision confirmed threshold
Pr (d | e) . Same-Decision Probability scenario
X
[Pr (d | e, h) ]Pr (h | e),
(5)
SDP (d, H, e, ) =
h

[Pr (d | h, e) ] indicator function

1 Pr (d | e, h)
[Pr (d | h, e) ] =
0 otherwise.
SDP notably hard compute. Choi et al. (2012) prove computing SDP
general PPPP -complete.5 previous work SDP (Darwiche & Choi, 2010;
Choi et al., 2012), two options computing SDP
5. class PPPP thought counting variant NPPP class, contains polynomial
time hierarchy PH MAP problem complete (Park & Darwiche, 2004).

606

fiAlgorithms Applications Same-Decision Probability


+


Pr (D)
0.5
0.5



S1

S2

S3

S4

Figure 2: Bayesian network intrusion detection, CPTs given Table 2
1. approximate algorithm developed Choi et al. (2012). algorithm uses
augmented variable elimination algorithm produces potentially weak bound
based one-sided Chebyshev inequality.
2. naive brute-force method enumerates possible instantiations.

4. Applying Same-Decision Probability
investigate use SDP stopping criterion selection criterion.
contrast usage SDP traditional methods discussed Section 2, find
using SDP provide insight decision maker scenarios.
4.1 SDP Stopping Criterion
definition SDP, see calculating high SDP, contrast calculating
low SDP, would indicate higher degree readiness make decision, chances
decision changing given evidence gathering lower. section show
computing SDP provide additional insight thus distinguish scenarios
otherwise indistinguishable based standard stopping criteria.
threshold-based decision classical notion decision making uncertainty,
commonly used requires utilities elicited. Examples thresholdbased decisions prevalent educational diagnosis (Gertner, Conati, & VanLehn,
1998; Conati, Gertner, & VanLehn, 2002; Butz et al., 2004; Xenos, 2004; Arroyo & Woolf,
2005; Munie & Shoham, 2008), intrusion detection (Kruegel et al., 2003; Modelo-Howard
et al., 2008), fault diagnosis (Heckerman et al., 1995; Lu & Przytula, 2006), medical
diagnosis (Pauker & Kassirer, 1980; Kahn et al., 1997; van der Gaag & Coupe, 1999).
Consider sensor network Figure 2, may correspond intrusion detection
application discussed Kruegel et al. (2003). Here, hypothesis variable =
{+, } = + implying intrusion. Suppose commit decision, stop
performing observations, belief event = + surpasses threshold ,
say = 0.55. four sensors model, S1 , S2 , S3 S4 , whose readings may
affect decision.
Consider two following scenarios:
1. S1 = + S2 = +.
2. S3 = + S4 = +.
607

fiChen, Choi & Darwiche

S1 Pr (S1 | D)
+ +
0.55
+
0.45
+
0.45
0.55


S3 Pr (S3 | D)
+ +
0.60
+
0.40
+
0.40
0.60


S2 Pr (S2 | D)
+ +
0.55
0.45
+
+
0.45

0.55

S4 Pr (S4 | D)
+ +
0.65
0.35
+
+
0.35

0.65

Table 2: CPTs network Figure 2. Parameterization 1.
Since Pr (D = + | S1 = +, S2 = +) = 0.60 > 0.55 Pr (D = + | S3 = +, S4 = +) =
0.74 > 0.55, clear cases threshold crossed. deem
observations necessary based beliefs surpassing threshold.
Hence, using thresholds stopping criterion (as commonly done, see Kruegel
et al., 2003; Lu & Przytula, 2006; Gao & Koller, 2011), two scenarios identical
information gathered decision made.
viewpoint SDP, however, two scenarios different. particular, first scenario leads SDP 52.97%. means 47.03% chance
different decision would made observe two unobserved
sensors S3 S4 . second scenario, however, leads SDP 100%. is,
would certainty know would make decision observe
two unobserved sensors S1 S2 : matter readings S1 S2 could be,
beliefs event = + would always surpass threshold 0.55. Indeed,
see Table 2, sensors S1 S2 strong sensors S3 S4 ,
example, strong enough reverse decision.
example provides clear illustration utility SDP stopping criterion.
However, may argue clear second case, stop gathering
information Pr (D = + | S3 = +, S4 = +) = 0.74 larger margin threshold
Pr (D = + | S1 = +, S2 = +) = 0.60.6 However, show following example
deciding stop based solely margin robust. Consider
sensor network Figure 2 parameterizations sensor network shown Table 5
Table 6 (found Appendix C), respectively refer Case 1 Case 2.7
Note example, use threshold = 0.5.
cases, S3 = + S4 = + observed, Pr (D = + | S3 = +, S4 = +) .
particular,
1. Case 1: Pr (D = + | S3 = +, S4 = +) = 0.775.
6. Thus using aforementioned margins confidence stopping criterion used Gao Koller (2011).
7. Note exact numbers CPTs necessary grasp examples CPTs
provided readers may reconstruct networks.

608

fiAlgorithms Applications Same-Decision Probability

2. Case 2: Pr (D = + | S3 = +, S4 = +) = 0.599.
using previously discussed margin stopping criterion, would seem Case
1 could stop information gathering, whereas Case 2 information gathering
necessary. However, compute SDP cases insights
nature robustness settings. Case 1, find SDP 0.781, whereas
Case 2, find SDP 1.0 even though Case 1 margin higher,
greater chance decision would change given information.
demonstrates cannot use solely margin determine whether stop
information gathering.
clear examples SDP useful stopping criterion. First, SDP
pinpoint situations observations unnecessary would never
reverse decision consideration. Second, SDP identify situations
decision made robust, likely change upon making
observations. addition examples, Appendix A.2 show SDP
useful stopping criterion context utility-based decisions (e.g. influence diagrams).
4.2 SDP Selection Criterion
turn attention use SDP criterion deciding variables
observe next, assuming stopping criterion indicates observations
necessary. proposal based using VOI selection criterion (see Equation 3),
choosing SDP reward function. call SDP gain, formally
defined as:
Definition 2. Given Definition 4 SDP, SDP gain observing variables G
variables H defined expected SDP observing G H subtracted SDP
H:
G(G) = E(G, H, e, ) SDP (d, H, e, ),
(6)
expected SDP defined as:
E(G, H, e, ) =

X

SDP (d, H \ G, ge, )Pr (g|e)

(7)

g

defined decision made given current evidence.
Note observe variables G H expected SDP 1.0,
indicates observing G making decision, remaining variables H \ G
rendered completely redundant observation effect decision.
goal using SDP gain selection criterion observe variables which,
average, allow stable decision given collected observations.
next provide example using SDP selection criterion, contrasting
two selection criteria: One based reducing entropy hypothesis variable
D, another based maximizing gap decision probability Pr (d|e)
given threshold (Krause & Guestrin, 2009). criteria motivated
reducing uncertainty, show indeed lead less stable decisions
SDP used.
609

fiChen, Choi & Darwiche




+


Pr (D)
0.5
0.5
S1

S2

Figure 3: Bayesian network CPTs given Appendix C.

example given Bayesian network Figure 3, hypothesis
variable S1 /S2 sensors. decision triggered Pr (D = + | e) .80,
evidence e sensors S1 S2 . observations (empty evidence e), SDP
0.595, suggesting observations may needed. Assuming limited number
observations (Heckerman et al., 1995), using myopic approach observing one
variable time (Dittmer & Jensen, 1997), need select next variable
observe.
Note maximizing VOI negative entropy reward function amounts
maximizing mutual information, H(D, X) = H(D) H(D | X) (Cover & Thomas, 1991;
Krause & Guestrin, 2005). mutual information variable sensor S2
0.53 whereas mutual information sensor S1 0.278. Hence, observing
S2 reduce entropy most. terms margin confidence, another reward
function used Krause Guestrin (2009), observing S2 average lead 0.7
margin states = + = , whereas observing S1 lead 0.6
margin two states.
However, compute corresponding SDP gains, G(S1 ) G(S2 ), find
observing S1 will, average, lead improving decision stability most. particular, observing S1 would give us SDP either 1 0.81 expected SDP
0.905, whereas observing S2 would give us SDP either 0.7625, 0.5, 1
expected SDP 0.805. Therefore, G(S1 ) = 0.31 G(S2 ) = 0.21. Hence, observing S1
average allow us make decision less likely change due additional
information (beyond S1 ).
intuition occurs although observing S2 leads greater information gain observing S1 , superfluous information. Note Pr (D = + | S2 = ) =
0.0625, whereas Pr (D = + | S1 = ) = 0.2. Clearly, see observing S2 lead
skewed distribution minimal conditional entropy. However, context
threshold-based decisions, make decision based solely whether Pr (D = + | e)
threshold, meaning may put much emphasis much
threshold Pr (D = + | e) is. case, although observing S2
average lead extreme distribution, observing S2 = leads making extremely
nonrobust decision (a decision would change 50% time observation S1 ).
Observing S1 making decision leads much robust decision. example
demonstrates usefulness SDP selection criterion threshold-based decisions,
SDP used select observations lead robust decisions.
610

fiAlgorithms Applications Same-Decision Probability


+


Pr (D)
0.3
0.7



E1

H1

H3

H2

Figure 4: Naive Bayes network (CPTs defined Appendix C).

5. Computing Same-Decision Probability
Computing SDP involves computing expectation hidden variables H.
naive brute-force algorithm would enumerate check whether Pr (d | h, e)
instantiations h H. present algorithm save us need explore
every possible instantiation h. make algorithm easier understand, first
describe compute SDP Naive Bayes network. trivial problem
show Section 7 computing SDP Naive Bayes network NP-hard.
generalize algorithm arbitrary networks.
5.1 Computing SDP Naive Bayes Networks
find convenient implement test Pr (d | h, e) log-odds
domain, where:
log O(d | h, e) = log

Pr (d | h, e)
Pr (d | h, e)

(8)


define log-odds threshold = log 1T
and, equivalently, test whether
log O(d | h, e) .
Naive Bayes network class variable, H E leaf variables,
Q H, posterior log-odds observing partial instantiation q = {h1 , . . . , hj }
written as:

log O(d | q, e) = log O(d | e) +

j
X

w hi

(9)

i=1

whi weight evidence hi defined as:
whi = log

Pr (hi | d, e)
Pr (hi | d, e)

(10)

weight evidence whi contribution evidence hi quantity
log O(d | q, e) (Chan & Darwiche, 2003). Note weights computed time
space linear |H| using floating point representation.8 Table 3 depicts weights
evidence network Figure 4.
8. Additionally, note Equation 10, since Naive Bayes networks Hi d-separated E given
d, term e dropped equation. leave term general networks, Hi
may d-separated E.

611

fiChen, Choi & Darwiche


1
2
3

w hi
3.0
1.22
1.22

w hi
-2.17
-1.22
-1.22

Table 3: Weights evidence attributes Figure 4.
H1
0.0
H2
3.0

H2
-2.17

H3
4.22
5.44

H3
1.78
3.0

3.0

H3
0.95
0.56

0.27

2.17

H3
3.39
2.17

4.61

Figure 5: search tree network Figure 4. solid line indicates + dashed
line indicates . quantity log O(d | q, e) displayed next node q
tree. Nodes log O(d | q, e) = 0 shown bold.

One compute SDP enumerating instantiations variables H
using Equation 9 test whether log O(d | h, e) . Figure 5 depicts search tree
Naive Bayes network Figure 4, used purpose. leaves
tree correspond instantiations h variables H. generally, every node
tree corresponds instantiation q, Q H.
brute-force computation SDP would entail:
1. Initializing total SDP 0.
2. Visiting every leaf node h search tree.
3. Checking whether log O(d | h, e) so, adding Pr (h | e) total SDP.
Figure 5 depicts quantity log O(d | q, e) node q tree, indicating five
leaf nodes (i.e., five instantiations variables H) indeed contribute SDP.
state key observation underlying proposed algorithm. Consider node
corresponding instantiation H1 = + search tree, log O(d | H1 = +, e) = 3.0.
four completions h instantiation (i.e., four leaf nodes it)
log O(d | h, e) = 0. Hence, really need visit leaves add
contributions Pr (h|e) individually SDP. Instead, simply add Pr (H1 = +|e)
SDP, equals sum Pr (h|e) leaves. importantly,
detect leaves contribute SDP computing lower bound using
weights depicted Table 3. is, two weights variable H2 , minimum
1.22. Moreover, two weights variable H3 , minimum
1.22. Hence, lowest contribution log-odds made leaf node H1 = +
612

fiAlgorithms Applications Same-Decision Probability

H1
0.0
3.0

H2
-2.17
H3
0.95
0.27

3.39

2.17

Figure 6: reduced search tree network Figure 5.
1.22 1.22 = 2.44. Adding contribution current log-odds 3.0
lead log-odds .56, still passes given threshold.
similar technique used compute upper bounds, allowing us detect nodes
search tree leaf contribute SDP. Consider example
node corresponding instantiation H1 = , H2 = , log O(d | H1 = , H2 =
, e) = 3.39. Neither leaves node contribute SDP
log-odds pass threshold. detected considering weights
evidence variable H3 computing maximum weights (1.22). Adding
current log-odds 3.39 gives 2.17, still threshold. Hence,
leaf node H1 = , H2 = contribute SDP part search tree
pruned.
apply pruning technique based lower upper bounds, actually
end exploring portion tree shown Figure 6. pseudocode
final algorithm shown Algorithm 1. Note takes linear time compute
upper lower bounds. Additionally, note specific ordering H
search tree constructed directly linked amount pruning. use ordering
heuristic ranks query variable Hi difference corresponding upper
lower bound H ordered greatest difference lowest difference allow
earlier pruning.
5.2 Computing SDP Arbitrary Networks
generalize algorithm arbitrary networks viewing networks Naive
Bayes networks aggregate attributes. this, first need following notion.
Definition 3. partition H given E set S1 , . . . , Sk that: Si H;
Si Sj = ; S1 . . . Sk = H; Si independent (d-separated) Sj , 6= j, given
E.
Figure 7 depicts example partition.
intuition behind partition allows us view arbitrary network
Naive Bayes network, class variable aggregate attributes S1 , . . . , Sk . is,
aggregate attribute Si viewed variable states si , allowing us view
instantiation h set values s1 , . . . , sk . have:

613

fiChen, Choi & Darwiche

Algorithm 1 Computing SDP Naive Bayes network. Note: q = {h1 , . . . , hj },
P
wq defined ji=1 whi .

input:
N : Naive Bayes network class variable
H: attributes {H1 , . . . , Hk }
: log-odds threshold
e: evidence
output: Same-Decision Probability p

main:
global p 0.0 (initial probability)
q {} (initial instantiation empty set)
depth 0 (initial depth search tree)
DFS SDP(q, H, depth)
return p
1: procedure DFS SDP(q, H, depth)
P
2:
U pperBound log O(d | e) + wq + ki=depth+1 maxhi whi
P
3:
LowerBound log O(d | e) + wq + ki=depth+1 minhi whi
4:
(U pperBound < ) return
5:
else (LowerBound )
6:
add Pr (q | e) p, return
7:
else
8:
depth < k
9:
value hdepth+1 attribute Hdepth+1
10:
DFS SDP(qhdepth+1 , H \ Hdepth+1 , depth + 1)
Proposition 1. partial instantiation q = {s1 , . . . , sj },
log O(d | q, e) = log O(d | e) +

j
X

w si ,

(11)

i=1


wsi = log

Pr (si , | d, e)
Pr (si | d, e)

Proof.
Pr (d | q, e)
Pr (d | q, e)
Pr (d | e)Pr (s1 | d, e) . . . Pr (sj | d, e)
= log
Pr (d | e)Pr (s1 | d, e) . . . Pr (sj | d, e)

log O(d | q, e) = log

= log O(d | e) +

j
X
i=1

614

w si

(12)

fiAlgorithms Applications Same-Decision Probability

E1

X1

H1

H4



X2

H3

H5

E2

X3

H6

H2

Figure 7: partition H given E is: S1 = {H1 , H2 , H3 } S2 = {H4 }, S3 =
{H5 , H6 }.

Since Equations 11 12 analogous Equations 9 10, use Algorithm 1 arbitrary network. usage, however, requires auxiliary computations
needed readily available Naive Bayes networks. discuss
computations next.
5.2.1 Finding Partition
first need compute partition S1 , . . . , Sk , done pruning network
structure follows: delete edges outgoing nodes evidence E hypothesis D,
delete (successively) leaf nodes neither H, E D. identify
components X1 , . . . , Xk resulting network define non-empty Si = H Xi
element partition. guarantees original network structure, Si
d-separated Sj E 6= j (see (Darwiche, 2009)). Figure 7, network
pruning leads components X1 = {X1 , X2 , E2 , H1 , H2 , H3 }, X2 = {D, E1 , H4 }
X3 = {X3 , H5 , H6 }.
5.2.2 Computing Posterior Log-Odds, Probability Weights Evidence
quantities O(d | e), Pr (q | e) wsi , referenced Lines 2, 3, 6
algorithm, simple closed forms Naive Bayes networks. arbitrary networks,
however, computing quantities requires inference using algorithm
variable elimination described Darwiche (2009). Note network pruning
deleting edges removing leaf nodes, discussed above, guarantees
factor used variable elimination variables component Xi . Hence,
variable elimination applied component Xi isolation, sufficient
obtain needed quantities.
615

fiChen, Choi & Darwiche

5.2.3 Computing Min Max Evidence Weights
finally show compute upper lower bounds, maxsi wsi minsi wsi ,
referenced Lines 2 3 algorithm. quantities
computed using variable elimination, applied component Xi isolation.
case, however, must eliminate variables Xi \ Si first variables Si . Moreover,
first set variables summed-out, second set variables maxd-out
mind-out, depending whether need maxsi wsi minsi wsi . Finally, elimination
process applied twice, evidence d, e second time evidence d, e.
precisely, every component Xi set factors case =
= d. Using variable ordering, perform variable elimination
sets factors eliminate

left
Q iany nonquery (intermediary) variables
Q



set factors = Pr (Si , d, e), set factors = Pr (Si , d, e).
Since elimination order same, thus one-to-one matching

Pr (ei ,S )
factors sets, define new set factors = id = Pr id (ei ,Si ) .






calculate wsi wsi respectively maximizing minimizing variables.
Note summing variables maximizing variables variable elimination
algorithm used Dechter (1999) order solve MAP. algorithm differs
perform maximization minimization (to calculate wsi wsi ),
set factors instead factors (di di ) result simply summing
intermediary variables.
Note similarly Dechter (1999), first summing variables
performing maximization (and minimization case), elimination order
case constrained, meaning may forced use poor ordering variable
elimination results high treewidth.
5.3 Complexity Analysis
Let n number variables network, h = |H|, w = maxi wi ,
wi width constrained elimination order used
component Xi . best-case

time complexityof algorithm n exp w worst-case time complexity
n exp (w + h) . intuition behind bounds computing
maximum
minimum weights aggregate attribute takes time n exp w . bounds
complexity computing O(d|e), Pr (q|e) corresponding weights wsi . Moreover,
depending weights
threshold , traversing search tree take anywhere
constant time exp h . Since depth-first
search implemented linear

space, space complexity n exp w .

6. Experimental Results
performed several experiments real synthetic networks test performance algorithm across wide variety network structures, ranging simple
Naive Bayes networks highly connected networks. Real networks either learned
datasets provided UCI Machine Learning Repository (Bache & Lichman, 2013)
616

fiAlgorithms Applications Same-Decision Probability

Network
car
emdec6g
tcc4e
ttt
caa
voting
nav
fire
chess

source
UCI
HRL
HRL
UCI
CRESST
UCI
CRESST
CRESST
UCI

|H|
|h|
6
144
8
256
9
512
9
19683
14
16384
16
65536
20 1572864
24 16777216
30 1610612736

naive
0.131
0.407
0.470
6.234
6.801
21.35
642.88



approx
0.118
0.245
0.257
0.133
0.145
0.176
0.856
0.183
*

new
0.049
0.294
0.149
0.091
0.167
0.128
0.178
0.508
15.53

Table 4: Algorithm comparison real networks. show time, seconds, takes
algorithm, naive, approx, new compute SDP different networks.
Note indicates computation complete 20 minute
time limit constrained. Moreover, * indicates sufficient
memory complete computation.

provided HRL Laboratories CRESST.9 majority real networks used
diagnostic networks, made clear variable selected decision
variable would either knowledge fault variable. unclear cases,
decision variable picked random. query evidence variables selected
random real networks.
Besides algorithm, two options available compute SDP: 1.
naive method brute-force computation enumerating possible instantiations
2. approximate algorithm developed Choi et al. (2012). compare algorithm
two approaches, compute SDP real networks.
network selected least 80% total network variables query variables
could emphasize size query set greatly influences computation
time. computation given 20 minutes complete. believe value
threshold greatly affect running time, computed SDP thresholds =
[0.01, 0.1, 0.2, . . . , 0.8, 0.9, 0.99] took worst-case time. results experiments
three algorithms shown Table 4. Note |H| number query
variables |h| number instantiations naive algorithm must enumerate over.
Moreover, indicates computation complete 20 minute time limit
* indicates sufficient memory complete computation. networks
{car,ttt,voting,nav,chess} Naive Bayes networks whereas networks {caa,fire}
polytree networks others general networks.
Given real networks tested algorithm on, clear algorithm
outperforms naive implementation approximate algorithm Naive
Bayes networks polytree networks. Note approximation algorithm based
variable elimination use certain constrained orders. Naive Bayes
9. http://www.cse.ucla.edu/

617

fiChen, Choi & Darwiche

4000
3500

Average Explored Instantiations Running Time
Number instantiations (x 10e3)

3000
2500
2000
1500
1000
500
01

Time (s)
2

3

4
5
Number subnetworks

6

7

8

Figure 8: Synthetic network average running time average number instantiations
explored number connected components.

network hypothesis root, approximation algorithm forced
use particularly poor ordering, explains failure chess network.
analyze general network structure selected threshold affects
performance algorithm, generated synthetic networks 100 variables
varying treewidth using BNGenerator (Ide, Cozman, & Ramos, 2004). network,
randomly selected decision variable, 25 query variables, evidence variables.10
generated partition network grouped networks size obtained
partition (k). goal test algorithms running time ability prune
search-space depends k. average time average number instantiations
explored shown Figure 8.
general, see k increases, number instantiations explored
algorithm decreases runtime improves. network becomes similar
Naive Bayes structure increasing k. Moreover, larger k is, levels
search tree, means algorithm opportunities prune.
worst case, network may unable disconnected (k = 1). However, even
case algorithm still, average, efficient compared brute-force
implementation cases, computing maximum minimum weight
observing H, find exist h change decision.
found that, given time limit 2 hours, brute-force algorithm could solve
synthetic networks, whereas approach solved 70% networks.
test threshold affects computation time. Here, calculate posterior
probability decision variable run repeatedly algorithm thresholds
varying increments away. average running time increments seen
Figure 9. evident threshold set away initial
10. synthetic networks binary, brute-force approach would need explore 225 instantiations.

618

fiAlgorithms Applications Same-Decision Probability

1800
1600

Average Explored Instantiations Running Time
Number instantiations (x 10e3)

1400
1200
1000
800
600
400
200
00.0

Time (s)
0.1

0.2
0.3
0.4
0.5
Threshold distance initial posterior

0.6

Figure 9: Synthetic network average running time average number instantiations
explored threshold distance initial posterior probability.

posterior probability, algorithm finishes much faster, perhaps expected since
usage extreme thresholds would allow search space pruning.
Overall, experimental results show algorithm able solve many SDP
problems reach existing methods. confirm algorithm
completes much faster network disconnected threshold far
away initial posterior probability decision variable.

7. Complexity Computing Same-Decision Probability
present new complexity results SDP. first prove complexity
computing SDP Naive Bayes structures NP-hard. show general
complexity computing SDP lies complexity class general expectation
computation problem applicable wide variety queries graphical models,
computation non-myopic value information.
7.1 Computing SDP Naive Bayes
SDP known PPPP -complete (Choi et al., 2012). show SDP remains
hard Naive Bayes networks.
Theorem 1. Computing Same-Decision Probability Naive Bayes network NPhard.
Proof. reduce number partition problem defined Karp (1972) computing
SDP Naive Bayes model. Suppose given set positive P
integers c1P
, . . . , cn ,
wish determine whether exists {1, . . . , n} jI ci = j6I cj .
solve considering Naive Bayes network binary class variable
uniform probability, binary attributes H1 , . . . , Hn CPTs leading weights
619

fiChen, Choi & Darwiche

evidence wHi =T = ci wHi =F = ci . construction CPTs done
solving following system equations:
Pr (Hi = | = )
Pr (Hi = | = F )
Pr (Hi = F | = )
ci = log
Pr (Hi = F | = F )
1 = Pr (Hi = | = ) + Pr (Hi = F | = )
ci = log

1 = Pr (Hi = | = F ) + Pr (Hi = F | = F )

leave exact derivations (see Exercise 3.27 Darwiche, 2009). get
result that:
Pr (Hi = | = F ) = Pr (Hi = F | = ) =

2ci

1
+1

Pr (Hi = | = ) = Pr (Hi = F | = F ) = 1

2ci

1
+1

Note given CPTs defined wHi =T = ci wHi =F =
ci , set integers partitioned instantiation h = {h1 , . . . , hn }
P
n
i=1 whi = 0 since would include indices hi = case.
First,
PnThe Naive Bayes network satisfies number properties shall use
Pnext.
n
whi either 0, 1, 1 since weights whi integers. Next, i=1 whi = c,
i=1P
ni=1 whi = c hi 6= hi . Finally, Pr (h1 , . . . , hn ) = Pr (h1 , . . . , hn ) hi 6= hi ,
uniform probability distribution leaf Hi defined
symmetric CPT.
Consider following SDP (the last step based properties):
SDP (D = T, {H1 , . . . , Hn }, {}, 2/3)
X
[Pr (D = | h1 , . . . , hn ) 2/3]Pr (h1 , . . . , hn )
=
h1 ,...,hn

=

X

[log O(D = | h1 , . . . , hn ) 1]Pr (h1 , . . . , hn )

h1 ,...,hn

=

X

h1 ,...,hn

"

1 X
=
2

n
X

h1 ,...,hn



Pn

i=1 whi

#

whi 1 Pr (h1 , . . . , hn )

i=1

"

n
X

#

whi 6= 0 Pr (h1 , . . . , hn )

i=1

= 0 instantiation h1 , . . . , hn iff
" n
#
X X
whi 6= 0 Pr (h1 , . . . , hn ) < 1
h1 ,...,hn

i=1

620

fiAlgorithms Applications Same-Decision Probability

Hence, partitioning problem solved iff
SDP (D = T, {H1 , . . . , Hn }, {}, 2/3) < 1/2

7.2 Complexity Computing Non-myopic VOI
SDP shown PPPP -complete problem Choi et al. (2012). class PPPP
essentially counting variant NPPP class, contains polynomial hierarchy
PH MAP problem complete (Park & Darwiche, 2004). show
section general problem computing expectations PPPP -complete,
non-myopic VOI SDP instance expectation. Thus, development
algorithms compute SDP beneficial problems PPPP class,
turn benefits computing assortment expectations, including non-myopic VOI.
proposed expectation computation based using reward function R
properties review next. particular, function R assumed map
probability distribution Pr (D | e) numeric value. assume minimum
l maximum u range polytime computable. assumptions
limitingfor example, entropy utility expressed using reward functions
fall category (Krause & Guestrin, 2009).
consider following computation expectations.
D-EPT: Given polynomial-time computable reward function R, hypothesis variable
D, unobserved variables H, evidence e, real number N , distribution Pr induced
Bayesian network variables X,11 expectation decision problem asks:
E=

X

R(Pr (D | h, e))Pr (h | e)

h

greater N ?
Note SDP falls special case reward function R SDP indicator
function (see Definition 4). example, definition used Choi et al. (2012),
decision function outputs one two decisions depending whether Pr (d|e) >
value threshold .
following theorems, proofs Appendix B.
Theorem 2. D-EPT PPPP -hard.
Theorem 3. D-EPT PPPP .
shows D-EPT PPPP -complete implies computational problems
computing non-myopic VOI using variety reward functions PPPP complete.
11. proof holds influence diagrams constrained one decision node.

621

fiChen, Choi & Darwiche

8. Conclusion
paper, discussed commonly used information gathering criteria
graphical models value information reviewed recently introduced
notion Same-Decision Probability (SDP). paper, proposed usage
SDP decision making tool showing concrete examples usefulness
stopping criterion selection criterion. stopping criterion, SDP allow
us determine observations necessary. selection criterion, usage
SDP allow us select observations allow us increase decision robustness.
justified usage SDP, proposed exact algorithm
computation. Experimental results show algorithm comparable running time
previous approximate algorithm much faster naive brute-force
algorithm. Finally, presented several new complexity results.

Acknowledgements
paper combines extends work presented Chen, Choi, Darwiche (2012b,
2013). work partially supported ONR grant #N00014-12-1-0423, NSF
grant #IIS-1118122, NSF grant #IIS-0916161. would thank National
Center Research Evaluation, Standards, & Student Testing Hughes Research Lab
contributing sample diagnostic networks.

Appendix A. Miscellaneous Topics
section go details notions mentioned earlier
paper. particular, continue discussion Section 2.3 go notion
non-myopic value information. Additionally, continue left
Section 4.1 expand upon notion SDP stopping criterion context
utility-based decisions.
Appendix A.1 Non-myopic Value Information
Myopic value information often used many applications easy compute
(Dittmer & Jensen, 1997; Vomlel, 2004; Gao & Koller, 2011). However, problem
myopic selection optimal, times whole greater sum
parts, individual observation set H seemingly may provide significant
value, VOI observing H high. instance, take function
= X1 X2 , alone neither X1 X2 useful, together determinative
(Bilgic & Getoor, 2011). computing non-myopic VOI optimal
VOI obtained.
Due aforementioned problems using myopic VOI, recently, researchers
recently suggested using non-myopic VOI instead myopic VOI proposed various methods compute non-myopic VOI (Heckerman, Horvitz, & Middleton,
1993; Liao & Ji, 2008; Krause & Guestrin, 2009; Zhang & Ji, 2010; Bilgic & Getoor, 2011).
Computing non-myopic VOI hidden variables H difficult involves com622

fiAlgorithms Applications Same-Decision Probability

puting expectation possible values H, quickly becomes intractable
H becomes larger.
Existing algorithms computing non-myopic VOI approximate algorithms
(Heckerman et al., 1993; Liao & Ji, 2008) relatively limited algorithms restricted
tree networks leaf variables (Krause & Guestrin, 2009). Bilgic Getoor
(2011) developed Value Information Lattice (VOILA), framework
subsets hidden variables H examined, optimal subset features
found increase classification accuracy meeting budget constraint.
Appendix A.2 SDP Stopping Criterion Utility-based Decisions
cases expected utility different decisions, well cost reducing
uncertainty (making observations), quantified. common decision-theoretic
setting (Howard, 1966; Howard & Matheson, 1984), influence diagrams commonly
used. Influence diagrams seen Bayesian networks incorporate decision
utility nodes (Howard & Matheson, 1984; Zhang, 1998; Kjrulff & Madsen, 2008).
selection criterion decision-theoretic setting clear: observations lead
greatest increase expected utility selected. usage utilities observation
costs prevalent; however, numerous researchers noted difficulty coming
actual numerical quantities (Glasziou & Hilden, 1989; Lu & Przytula, 2006;
Bilgic & Getoor, 2011).
show SDP used stopping criterion decision-theoretic context expected-utility decisions influence diagrams (Howard & Matheson, 1984),
extend definition SDP general setting allow applications.
particular, assume F polytime computable decision function outputs
decision based distribution Pr (D | e). instance, decision function
commonly used classification select class highest posterior probability arg maxd Pr (d | e) (Friedman et al., 1997), whereas threshold-based decisions,
decision function would simply select decision Pr (D = | e) .
SDP thus defined probability decision would made
hidden states variables H known (Chen et al., 2012b).
Definition 4 (Same-Decision Probability Generalized ). Given decision function F,
hypothesis variable D, unobserved variables H, evidence e, Same-Decision Probability (SDP) defined
X
[F(Pr (D | h, e))]h Pr (h | e)
(13)
SDP (F, D, H, e) =
h

[F(Pr (D | h, e))]h indicator function

1 F(Pr (D | h, e)) = F(Pr (D | e))
=
0 otherwise.
original SDP definition, however, assumed binary variable,
F(Pr (D | e)) = Pr (d | e) threshold (Darwiche & Choi, 2010).
consider use SDP stopping criterion context expectedutility decisions influence diagrams (Howard & Matheson, 1984). particular,
623

fiChen, Choi & Darwiche

Q

C




P

Figure 10: influence diagram investment problem.
show using SDP, distinguish high-risk, high-reward scenarios lowrisk, low-reward scenarios otherwise indistinguishable consider usage
VOI/utilities alone.
Consider influence diagram Figure 10, consists Bayesian network
three variables (C, Q S), decision node I, utility node P direct
function utility function u. influence diagram models investment problem
venture capital firm deciding whether invest amount $5 million
tech startup (I = ) allowing money collect interest bank (I = F ).
example, profit investment (P ) depends decision (I) success
company (S), turn depends two factors: (1) whether existing competitor
companies successful (C) (2) whether co-founders startup high
quality, original idea (Q). C Q unobserved initially independent
other. Variable latent hypothesis variable case thus cannot observed.
Variables C Q, however, observed price.
goal choose decision = maximum expected utility:
X
EU (i | e) =
Pr (s | e)u(i, s),


u(i, s) utility decision = given evidence e variables C Q.
Figures 11 12 contain two different parameterizations influence diagram
Figure 10. refer different scenarios investment problem.
scenarios, given evidence variables C Q, best decision = F ,
expected utility $500K. decision maker may commit decision decide
observe variables C Q, hope finding better decision light
additional information. classical stopping criterion compute maximum
expected utility given observe variables C Q (Heckerman et al., 1993; Dittmer
& Jensen, 1997):
X
max
EU (i | c, q)Pr (c, q).


c,q

scenarios, maximum expected utility comes $1, 180K, showing
observations may lead better decision.12
12. According formulation Krause Guestrin (2009), computed VOI variables
C Q using reward function.

624

fiAlgorithms Applications Same-Decision Probability

Q

F

Pr (Q)
0.4
0.6

C

F

Pr (C)
0.6
0.4

Q


F
F

Pr (S = | .)
0.60
0.90
0.20
0.30

C

F

F



F
F



F

F

u(I, S)
$5 106
$5 106
$5 105
$5 105

Figure 11: parameterization influence diagram Figure 10.

Q

F

Pr (Q)
0.1
0.9

C

F

Pr (C)
0.9
0.1

Q


F
F

Pr (S = | .)
0.05
0.98
0.01
0.05

C

F

F



F
F



F

F

u(I, S)
$7 107
$5 106
$5 105
$5 105

Figure 12: parameterization influence diagram Figure 10.
point, two scenarios indistinguishable viewpoint
classical decision making tools. Remember Krause Guestrin (2009) Bilgic
Getoor (2011) remark budget observations expended long
value information observation greater cost observation. According
selection criteria, variables thus observed, expected
financial gain could well increase.
SDP, however, finds two scenarios different. particular,
respect variables C Q, SDP 60% first scenario 99% second
scenario. is, even though stand make better decision scenarios upon
observing variables C Q (at least respect financial gain), even though
expected benefit observations scenarios, unlikely
would change current decision = F second scenario comparison
first. Hence, given additional information provided SDP, decision maker may
act quite differently two scenarios. Indeed, take closer look second
scenario, state world (when = ) deciding invest would yield
large financial gain. However, chance state manifesting extremely
625

fiChen, Choi & Darwiche

small (analogous lottery), meaning risk-conscious decision maker may
averse gamble second scenario even waste resources observe variables
C D. Note example assumed utility incorporate
risk-factor, rational decision maker would always choose gather
information despite low probability changing current decision.
illustrates usefulness SDP stopping criterion context expectedutility decisions influence diagrams. Namely, using SDP, distinguish
two different scenarios, otherwise indistinguishable consider utilities
alone.

Appendix B. Proofs
section provide proofs Theorems 2 3.
Proof Theorem 2. show D-EPT PPPP -hard reduction following decision problem D-SDP, corresponds originally proposed notion same-decision
probability threshold-based decisions (Darwiche & Choi, 2010).
D-SDP: Given decision based probability Pr (d | e) surpassing threshold , set
unobserved variables H, probability p, same-decision probability:
X
[Pr (d | h, e) ]Pr (h | e)
(14)
h

greater p?
Here, [.] denotes indicator function evaluates 1 enclosed expression
satisfied, 0 otherwise. D-SDP shown PPPP -complete Choi et al. (2012).
same-decision probability corresponds expectation respect distribution Pr (H | e), using reward function:

1 Pr (d | h, e)
R(Pr (D | h, e)) =
0 otherwise.
Thus same-decision probability iff expectation .
Proof Theorem 3. show D-EPT PPPP , provide probabilistic polynomialtime algorithm, access PP oracle, answers decision problem D-EPT
correctly probability greater 21 . proof generalizes simplifies proof
given Choi et al. (2012) D-SDP.
Consider following probabilistic algorithm determines E > N :
1. Sample complete instantiation x Bayesian network, probability Pr (x).
linear time, using forward sampling (Henrion, 1986).
2. x compatible e, use PP-oracle compute = R(Pr (D | h, e)).
First, reward function R computed polynomial time, definition.
Second, Pr (D | h, e) computed using PP-oracle, since inference #Pcomplete (Roth, 1996), since PPP = P#P .
626

fiAlgorithms Applications Same-Decision Probability

3. Define function a(t) = 12 + 21 tN
ul , defines probability used probabilistic
algorithm guess whether E > N (see Lemma 1).
4. Declare E > N probability:
a(t) x compatible e;


1
2

x compatible e.

probability declaring E > N is:
r=

X
h

greater

1
2

1
a(t)Pr (h, e) + (1 Pr (e))
2

(15)

iff following set equivalent statements hold:
X

a(t)Pr (h, e) >

h

X

a(t)Pr (h | e) >

h

1
2


1
1tN
Pr (h | e) >
+
2 2 ul
2


X 1tN
Pr (h | e) > 0
2 ul
h
X
(t N )Pr (h | e) > 0

X 1
h

Pr (e)
2

h

X

R(Pr (D | h, e))Pr (h | e) > N.

h

Thus r >

1
2

iff E > N .

Lemma 1. function a(t) =

1
2

+

1 tN
2 ul

maps reward probability [0, 1].

Proof. Values u l given, denote upper lower bounds reward t,
threshold N . Thus tN
ul [1, 1].
Note a(t) denotes probability used algorithm declare whether E > N ,
higher lower depending value reward = R(Pr (D | h, e)).

Appendix C. Conditional Probability Tables
section provide conditional probability tables networks Figures 1, 2, 3,
4.

627

fiChen, Choi & Darwiche


+


Pr (D)
0.5
0.5

Hi


p
p
n
n
l
l


+
+



Xi
+

+

+

+


X1
+

+


Ei
+
+
+
+
+
+
+
+

Pr (X1 | D)
0.9
0.1
0.1
0.9

X1
+
+



Pr (Ei | Hi , Xi )
1.0
0.0
1.0
1.0
0.0
0.0
0.0
1.0

Hi

p
n
l

X2
+

+


Pr (X2 | X1 )
0.9
0.1
0.1
0.9

Pr (Hi )
0.81
0.09
0.09
0.01

Figure 13: CPTs Bayesian network given Figure 1. Note
CPTs variables Ei , lines case Ei = + given, since
Pr (Ei = |Hi , Xi ) = 1 Pr (Ei = +|Hi , Xi ).

S1 Pr (S1 | D)
+ +
0.65
+
0.35
0.30
+

0.70

S3 Pr (S3 | D)
+ +
0.65
+
0.35
0.35
+

0.65

S2 Pr (S2 | D)
+ +
0.60
+
0.40
+
0.30

0.70

S4 Pr (S4 | D)
+ +
0.65
+
0.35
+
0.35

0.65

Table 5: CPTs network Figure 2. Parameterization 2.

628

fiAlgorithms Applications Same-Decision Probability

S1 Pr (S1 | D)
+ +
0.50
0.50
+
+
0.40

0.60

S3 Pr (S3 | D)
+ +
0.55
0.45
+
+
0.45

0.55

S2 Pr (S2 | D)
+ +
0.50
+
0.50
+
0.40

0.60

S4 Pr (S4 | D)
+ +
0.55
+
0.45
+
0.45

0.55

Table 6: CPTs network Figure 2. Parameterization 3.

S2 Pr (S2 | D)
+ +
0.75
+
0.2
0.05
+
+
0.05

0.2

0.75

S1 Pr (S1 | D)
+ +
0.8
0.2
+
+
0.2

0.8

Table 7: CPTs Bayesian network Figure 3.

H1 Pr (H1 | D)
+ +
0.80
+
0.20
+
0.10

0.90

H2 Pr (H2 | D)
+ +
0.70
+
0.30
+
0.30

0.70

Table 8: CPTs network Figure 4. Pr (H3 | D), Pr (E1 | D) Pr (H2 |D)
equal.

629

fiChen, Choi & Darwiche

References
Arroyo, I., & Woolf, B. (2005). Inferring learning attitudes Bayesian network
log file data. Proceedings 12th International Conference Artificial
Intelligence Education, pp. 3340.
Bache, K., & Lichman, M. (2013). UCI machine learning repository..
Bilgic, M., & Getoor, L. (2011). Value information lattice: Exploiting probabilistic independence effective feature subset acquisition. Journal Artificial Intelligence
Research (JAIR), 41, 6995.
Butz, C. J., Hua, S., & Maguire, R. B. (2004). web-based intelligent tutoring system
computer programming. Web Intelligence, pp. 159165. IEEE Computer Society.
Chan, H., & Darwiche, A. (2003). Reasoning Bayesian network classifiers. Proceedings 19th Conference Uncertainty Artificial Intelligence, pp. 107115.
Chen, J., Low, K. H., Tan, C. K.-Y., Oran, A., Jaillet, P., Dolan, J., & Sukhatme, G.
(2012a). Decentralized data fusion active sensing mobile sensors modeling
predicting spatiotemporal traffic phenomena. Proceedings Twenty-Eighth
Conference Annual Conference Uncertainty Artificial Intelligence (UAI-12), pp.
163173, Corvallis, Oregon. AUAI Press.
Chen, S., Choi, A., & Darwiche, A. (2012b). Same-Decision Probability: new tool
decision making. Proceedings Sixth European Workshop Probabilistic
Graphical Models, pp. 5158.
Chen, S., Choi, A., & Darwiche, A. (2013). exact algorithm computing SameDecision Probability. Proceedings 23rd International Joint Conference
Artificial Intelligence, pp. 25252531.
Choi, A., Xue, Y., & Darwiche, A. (2012). Same-Decision Probability: confidence measure threshold-based decisions. International Journal Approximate Reasoning
(IJAR), 2, 14151428.
Conati, C., Gertner, A., & VanLehn, K. (2002). Using Bayesian networks manage uncertainty student modeling. User Modeling User-Adapted Interaction, 12 (4),
371417.
Cover, T. M., & Thomas, J. A. (1991). Elements Information Theory. Wiley-Interscience.
Darwiche, A. (2009). Modeling Reasoning Bayesian Networks (1st edition). Cambridge University Press.
Darwiche, A., & Choi, A. (2010). Same-Decision Probability: confidence measure
threshold-based decisions noisy sensors. Proceedings Fifth European
Workshop Probabilistic Graphical Models, pp. 113120.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (1), 4185.
Dittmer, S., & Jensen, F. (1997). Myopic value information influence diagrams. Proceedings Thirteenth Conference Annual Conference Uncertainty Artificial
Intelligence (UAI-97), pp. 142149.
630

fiAlgorithms Applications Same-Decision Probability

Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classifiers. Machine
Learning, 29 (2-3), 131163.
Gao, T., & Koller, D. (2011). Active classification based value classifier. Advances
Neural Information Processing Systems (NIPS 2011).
Gertner, A. S., Conati, C., & VanLehn, K. (1998). Procedural help Andes: Generating hints using Bayesian network student model. Proceedings National
Conference Artificial Intelligence, pp. 106111.
Glasziou, P., & Hilden, J. (1989). Test selection measures. Medical Decision Making, 9 (2),
133141.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.
Artificial Intelligence, 139 (2), 137174.
Hamscher, W., Console, L., & de Kleer, J. (Eds.). (1992). Readings Model-Based Diagnosis. Morgan Kaufmann Publishers Inc.
Heckerman, D., Breese, J. S., & Rommelse, K. (1995). Decision-theoretic troubleshooting.
Communications ACM, 38 (3), 4957.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). approximate nonmyopic computation value information. IEEE Transactions Pattern Analysis Machine
Intelligence, 15 (3), 292298.
Henrion, M. (1986). Propagating uncertainty Bayesian networks probabilistic logic
sampling. Proceedings Second Annual Conference Uncertainty Artificial
Intelligence (UAI-86), pp. 149163.
Howard, R. A. (1966). Information value theory. IEEE Transactions Systems Science
Cybernetics, 2 (1), 2226.
Howard, R. A., & Matheson, J. E. (Eds.). (1984). Readings Principles Applications Decision Analysis. Strategic Decision Group.
Ide, J. S., Cozman, F. G., & Ramos, F. T. (2004). Generating random Bayesian networks
constraints induced width. Proceedings 16th European Conference
Artificial Intelligence, pp. 323327.
Jordan, A. (2002). discriminative vs. generative classifiers: comparison logistic
regression naive Bayes. Advances Neural Information Processing Systems, 14,
841.
Kahn, C. E., Roberts, L. M., Shaffer, K. A., & Haddawy, P. (1997). Construction
Bayesian network mammographic diagnosis breast cancer. Computers Biology
Medicine, 27 (1), 1929.
Karp, R. M. (1972). Reducibility among combinatorial problems. Complexity Computer Computations. Springer.
Kjrulff, U. B., & Madsen, A. L. (2008). Bayesian Networks Influence Diagrams:
Guide Construction Analysis. Springer.
Krause, A., & Guestrin, C. (2005). Near-optimal nonmyopic value information graphical models. 21st Conference Uncertainty Artificial Intelligence, pp. 324331.
631

fiChen, Choi & Darwiche

Krause, A., & Guestrin, C. (2009). Optimal value information graphical models.
Journal Artificial Intelligence Research (JAIR), 35, 557591.
Kruegel, C., Mutz, D., Robertson, W., & Valeur, F. (2003). Bayesian event classification
intrusion detection. Proceedings Annual Computer Security Applications
Conference (ACSAC).
Liao, W., & Ji, Q. (2008). Efficient non-myopic value-of-information computation influence diagrams. International Journal Approximate Reasoning, 49 (2), 436450.
Lindley, D. V. (1956). measure information provided experiment. Annals
Mathematical Statistics, 27 (4), 9861005.
Lu, T.-C., & Przytula, K. W. (2006). Focusing strategies multiple fault diagnosis.
Proceedings 19th International FLAIRS Conference, pp. 842847.
Millan, E., Descalco, L., Castillo, G., Oliveira, P., & Diogo, S. (2013). Using Bayesian
networks improve knowledge assessment. Computers & Education, 60 (1), 436447.
Modelo-Howard, G., Bagchi, S., & Lebanon, G. (2008). Determining placement intrusion detectors distributed application Bayesian network modeling.
Proceedings 11th International Symposium Recent Advances Intrusion
Detection, pp. 271290.
Munie, M., & Shoham, Y. (2008). Optimal testing structured knowledge. AAAI08:
Proceedings 23rd National Conference Artificial intelligence, pp. 10691074.
Ognibene, D., & Demiris, Y. (2013). Towards active event recognition. Proceedings
23rd International Joint Conference Artificial Intelligence, pp. 24952501.
Park, J. D., & Darwiche, A. (2004). Complexity results approximation strategies
MAP explanations. Journal Artificial Intelligence Research (JAIR), 21, 101133.
Pauker, S. G., & Kassirer, J. P. (1980). threshold approach clinical decision making..
New England Journal Medicine, 302 (20), 110917.
Raiffa, H. (1968). Decision Analysis Introductory Lectures Choices Uncertainty.
Addison-Wesley.
Ramoni, M., & Sebastiani, P. (2001). Robust Bayes classifiers. Artificial Intelligence, 125 (12), 209226.
Roth, D. (1996). hardness approximate reasoning. Artificial Intelligence, 82, 273
302.
Shann, M., & Seuken, S. (2013). active learning approach home heating
smart grid. Proceedings 23rd International Joint Conference Artificial
Intelligence, pp. 28922899.
Stratonovich, R. (1965). value information. Izvestiya USSR Academy Sciences,
Technical Cybernetics, 5, 312.
van der Gaag, L. C., & Coupe, V. M. H. (1999). Sensitivity analysis threshold decision
making Bayesian belief networks. AI*IA, pp. 3748.
Vomlel, J. (2004). Bayesian networks educational testing. International Journal
Uncertainty, Fuzziness Knowledge-Based Systems, 12 (supp01), 83100.
632

fiAlgorithms Applications Same-Decision Probability

Xenos, M. (2004). Prediction assessment student behaviour open distance
education computers using Bayesian networks. Computers & Education, 43 (4),
345359.
Yu, S., Krishnapuram, B., Rosales, R., & Rao, R. B. (2009). Active sensing. International
Conference Artificial Intelligence Statistics, pp. 639646.
Zhang, N. L. (1998). Probabilistic inference influence diagrams. Computational Intelligence, pp. 514522.
Zhang, Y., & Ji, Q. (2010). Efficient sensor selection active information fusion. IEEE
Transactions Systems, Man, Cybernetics, Part B, 40 (3), 719728.

633


