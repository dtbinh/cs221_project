journal artificial intelligence

submitted published

applications
decision probability
suming chen
arthur choi
adnan darwiche

suming cs ucla edu
aychoi cs ucla edu
darwiche cs ucla edu

computer science department
university california los angeles
los angeles ca

abstract
making decisions uncertainty optimal choices often difficult
discern especially enough information gathered two key questions
regard relate whether one stop information gathering process commit
decision stopping criterion information gather next selection
criterion recently introduced notion decision
probability sdp useful stopping selection criterion provide additional insight allow robust decision making variety scenarios
query shown highly intractable pppp complete exemplary
class queries correspond computation certain expectations propose first exact computing sdp demonstrate effectiveness
several real synthetic networks finally present complexity
complexity computing sdp naive bayes structure additionally
prove computing non myopic value information complete
complexity class computing sdp

introduction
probabilistic graphical often used model variety decision
e g medical diagnosis pauker kassirer kahn roberts shaffer haddawy
van der gaag coupe fault diagnosis lu przytula classification
friedman geiger goldszmidt ramoni sebastiani jordan troubleshooting heckerman breese rommelse educational diagnosis butz hua
maguire arroyo woolf millan descalco castillo oliveira diogo
intrusion detection kruegel mutz robertson valeur modelo howard
bagchi lebanon similar applications decision maker typically
position must decide tests perform observations make
order make better informed decision perhaps critically decision maker must
decide stop making observations commit particular decision
decision probability sdp recently proposed darwiche choi
order help quantify robustness decision context decisionmaking bayesian networks short sdp probability would make
decision perform observations yet made
sdp treated measure decisions robustness respect
c

ai access foundation rights reserved

fichen choi darwiche

unknown variables quantifying confidence would make decision even
made observations
apply sdp tool information gathering
particular way determine stop information gathering stopping criterion pieces information gather next selection criterion
compare sdp classical stopping selection criteria illustrative examples
instance demonstrate sdp distinguish stable unstable
decisions indistinguishable classical criteria additionally
scenarios classical criteria may call performing observations
sdp indicates decision unlikely change
notably sdp shown highly intractable choi xue darwiche
exact computation sdp limited toy examples
variables brute force enumeration propose first exact
computing sdp applied real world networks
scope brute force enumeration previously proposed approximation
applied synthetic networks many variables
provide complexity sdp highlight relative
intractability even naive bayes networks relationship broader class
expectation computation emphasizing broader importance developing
effective sdp related
thus structured follows first introduce notation discuss
common stopping selection criteria section review previously introduced work sdp section section discuss sdp applied
stopping criterion selection criterion section present novel
exact computing sdp discuss experimental section
section present recent complexity sdp conclude
section

related work
making decisions uncertainty may difficult finalize decision
presence unobserved variables given unobserved variables two fundamental questions first question whether given current observations decision
maker ready commit decision refer stopping criterion
making decision assuming stopping criterion met second question
additional observations made decision maker ready make decision typically requires selection criterion measure quantifying
observations value information voi section first introduce necessary
notation review commonly used stopping selection criteria
notation
throughout use standard notation variables instantiations
variables denoted upper case letters x instantiations lower case
letters x additionally sets variables denoted bold upper case letters x
instantiations bold lower case letters x assume state world


fialgorithms applications decision probability

described random variables x evidence e x includes known variables
hidden variables u x include unknown variables definition e u
eu x often discuss ramifications observing subset hidden variables
h u decision making furthermore use u denote main hypothesis
variable forms basis decision
stopping criterion
given hidden variables model choice whether
observe subset stopping criterion determines stop process
information gathering commit decision note concerned making
decision hypothesis variable state patients health
stopping criterion basic used variety domains commit
decision belief certain event crosses threshold done
pauker kassirer kruegel et al lu przytula however
may robust observations may cause belief
event fall threshold van der gaag bodlaender note possibility
pose stop asks whether present evidence gathered
sufficient diagnosis exists relevant evidence
gathered
approaches involve ensuring uncertainty surrounding decision variable
sufficiently reduced instance gao koller stop information gathering
conditional entropy interest variable reduced beyond threshold
margin first second likely states interest variable
threshold case clear threshold stopping criteria ubiquitous
decision making uncertainty
alternatively several stopping criteria involve existence
budget abstract quantity represent available resources
used information gathering budget may representative number
observations allowed modelo howard et al munie shoham yu
krishnapuram rosales rao chen low tan oran jaillet dolan sukhatme
terms monetary amount may spent observations varying
cost greiner grove roth krause guestrin bilgic getoor
context budget general stopping criterion continue make observations
budget completely expended done modelo howard et al
munie shoham krause guestrin bilgic getoor note
budget expended caveat value information
observation least cost observation
selection criterion value information
stopping criterion determine observations necessary selection
criterion used determine variables selected observation
ideally want observe variables give us additional information regards
work presented extended case multiple hypothesis variables
focus case one hypothesis variable simplicity



fichen choi darwiche

decision variable however due resource constraints limited budget
often possible basic common selection criterion select
observations minimize conditional entropy decision variable vomlel
lu przytula krause guestrin yu et al zhang ji
gao koller ognibene demiris shann seuken entropy
variable x defined
h x

x

pr x log pr x



x

measure uncertainty variables state entropy variable
high means much uncertainty value variable takes
uncertainty decision variables true state makes difficult make decision thus
natural selection criterion observe variables minimize conditional entropy
decision variable conditional entropy variable given variable x defined

h x

x

h x pr x



x

conditional entropy thus expectation entropy would
observing x similar selection criterion observe variables greatly
increase margin posterior probabilities first second likely
states decision variable krause guestrin
selection criteria involve utilizing notion value information voi order
quantify value observations lindley stratonovich howard
raiffa voi set variables depend measures
two example selection criteria discussed measures would entropy
margins confidence instance observing variable x would reduce conditional
entropy h x observing variable x would h x h x
value observing x would higher
krause guestrin define general notion voi different
reward functions particular given arbitrary reward function r hypothesis variable
evidence e voi observing hidden variables h
v r h e er r h e r pr e




er r h e

x

r pr h e pr h e



h

expected reward observing variables h r pr e reward
observed variables h definition reward function used lu
information theory logarithm typically assumed base cover thomas
assume throughout convenience
reward function assumed take input probability distribution hypothesis variable
pr return numeric value discuss reward functions section



fialgorithms applications decision probability







pr e e



x

x

e

e

h

h

figure simple bayesian network sensor readings e e variables h
h represent health sensors e e left posterior
decision variable network cpts found appendix c figure

przytula krause guestrin select variables order minimize
conditional entropy r pr e h e maximizing expected
reward observing variables h equivalent minimizing conditional entropy
h h possible reward functions involve utility reward functions
threshold reward functions munie shoham
note vast majority selection criteria use myopic
possible observations one observation considered time observation
highest voi selected time greedy short sighted
optimal voi computed computing non myopically bilgic getoor
discuss usage non myopic voi appendix

decision probability
decision probability sdp initially introduced darwiche choi
confidence measure threshold decisions bayesian networks noisy
sensor readings prior formally defining sdp first example provide
intuition consider bayesian network figure scenario involving
hypothesis variable two noisy sensors e e influence belief
hypothesis networks typically used compute belief hypothesis
given sensor readings pr e basis whether make decision often
depends whether posterior probability hypothesis surpasses
threshold hamscher console de kleer heckerman et al kruegel
et al lu przytula
figure shows particular reading two sensors resulting belief pr
e e suppose threshold pr e would make
certain decision notice figure health sensors modeled variables
h h sensor truthful stuck positive readings display
lying readings opposite value actual value darwiche choi
variables observed could informed us trustworthiness
reward functions see list provided krause guestrin



fichen choi darwiche

h

p
l

p
l

p
l

h



p
p
p
l
l
l

pr h e










pr h e










table scenarios h sensor readings e e e network figure
h h h cases threshold bold note p l
respectively represent truthful stuck positive lying sensor

sensors e e thus allow us make better decision want make
informed decision probability pr h e instead making decision
pr e
consider table enumerates possible health states sensors
four cases probability hypothesis pass threshold bold
leading decision five scenarios different decision would
made sdp thus probability four scenarios decision
would made example sdp

indicating relatively robust decision
choi et al define sdp formally
definition decision probability let n bayesian network conditioned
evidence e given hypothesis threshold set
unobserved variables h suppose making decision confirmed threshold
pr e decision probability scenario
x
pr e h pr h e

sdp h e
h

pr h e indicator function

pr e h
pr h e
otherwise
sdp notably hard compute choi et al prove computing sdp
general pppp complete previous work sdp darwiche choi
choi et al two options computing sdp
class pppp thought counting variant nppp class contains polynomial
time hierarchy ph map complete park darwiche



fialgorithms applications decision probability





pr













figure bayesian network intrusion detection cpts given table
approximate developed choi et al uses
augmented variable elimination produces potentially weak bound
one sided chebyshev inequality
naive brute force method enumerates possible instantiations

applying decision probability
investigate use sdp stopping criterion selection criterion
contrast usage sdp traditional methods discussed section
sdp provide insight decision maker scenarios
sdp stopping criterion
definition sdp see calculating high sdp contrast calculating
low sdp would indicate higher degree readiness make decision chances
decision changing given evidence gathering lower section
computing sdp provide additional insight thus distinguish scenarios
otherwise indistinguishable standard stopping criteria
threshold decision classical notion decision making uncertainty
commonly used requires utilities elicited examples thresholdbased decisions prevalent educational diagnosis gertner conati vanlehn
conati gertner vanlehn butz et al xenos arroyo woolf
munie shoham intrusion detection kruegel et al modelo howard
et al fault diagnosis heckerman et al lu przytula medical
diagnosis pauker kassirer kahn et al van der gaag coupe
consider sensor network figure may correspond intrusion detection
application discussed kruegel et al hypothesis variable
implying intrusion suppose commit decision stop
performing observations belief event surpasses threshold
say four sensors model whose readings may
affect decision
consider two following scenarios




fichen choi darwiche

pr









pr









pr









pr









table cpts network figure parameterization
since pr pr
clear cases threshold crossed deem
observations necessary beliefs surpassing threshold
hence thresholds stopping criterion commonly done see kruegel
et al lu przytula gao koller two scenarios identical
information gathered decision made
viewpoint sdp however two scenarios different particular first scenario leads sdp means chance
different decision would made observe two unobserved
sensors second scenario however leads sdp
would certainty know would make decision observe
two unobserved sensors matter readings could
beliefs event would surpass threshold indeed
see table sensors strong sensors
example strong enough reverse decision
example provides clear illustration utility sdp stopping criterion
however may argue clear second case stop gathering
information pr larger margin threshold
pr however following example
deciding stop solely margin robust consider
sensor network figure parameterizations sensor network shown table
table found appendix c respectively refer case case
note example use threshold
cases observed pr
particular
case pr
thus aforementioned margins confidence stopping criterion used gao koller
note exact numbers cpts necessary grasp examples cpts
provided readers may reconstruct networks



fialgorithms applications decision probability

case pr
previously discussed margin stopping criterion would seem case
could stop information gathering whereas case information gathering
necessary however compute sdp cases insights
nature robustness settings case sdp whereas
case sdp even though case margin higher
greater chance decision would change given information
demonstrates cannot use solely margin determine whether stop
information gathering
clear examples sdp useful stopping criterion first sdp
pinpoint situations observations unnecessary would never
reverse decision consideration second sdp identify situations
decision made robust likely change upon making
observations addition examples appendix sdp
useful stopping criterion context utility decisions e g influence diagrams
sdp selection criterion
turn attention use sdp criterion deciding variables
observe next assuming stopping criterion indicates observations
necessary proposal voi selection criterion see equation
choosing sdp reward function call sdp gain formally
defined
definition given definition sdp sdp gain observing variables g
variables h defined expected sdp observing g h subtracted sdp
h
g g e g h e sdp h e

expected sdp defined
e g h e

x

sdp h g ge pr g e



g

defined decision made given current evidence
note observe variables g h expected sdp
indicates observing g making decision remaining variables h g
rendered completely redundant observation effect decision
goal sdp gain selection criterion observe variables
average allow stable decision given collected observations
next provide example sdp selection criterion contrasting
two selection criteria one reducing entropy hypothesis variable
another maximizing gap decision probability pr e
given threshold krause guestrin criteria motivated
reducing uncertainty indeed lead less stable decisions
sdp used


fichen choi darwiche







pr






figure bayesian network cpts given appendix c

example given bayesian network figure hypothesis
variable sensors decision triggered pr e
evidence e sensors observations empty evidence e sdp
suggesting observations may needed assuming limited number
observations heckerman et al myopic observing one
variable time dittmer jensen need select next variable
observe
note maximizing voi negative entropy reward function amounts
maximizing mutual information h x h h x cover thomas
krause guestrin mutual information variable sensor
whereas mutual information sensor hence observing
reduce entropy terms margin confidence another reward
function used krause guestrin observing average lead
margin states whereas observing lead
margin two states
however compute corresponding sdp gains g g
observing average lead improving decision stability particular observing would give us sdp expected sdp
whereas observing would give us sdp
expected sdp therefore g g hence observing
average allow us make decision less likely change due additional
information beyond
intuition occurs although observing leads greater information gain observing superfluous information note pr
whereas pr clearly see observing lead
skewed distribution minimal conditional entropy however context
threshold decisions make decision solely whether pr e
threshold meaning may put much emphasis much
threshold pr e case although observing
average lead extreme distribution observing leads making extremely
nonrobust decision decision would change time observation
observing making decision leads much robust decision example
demonstrates usefulness sdp selection criterion threshold decisions
sdp used select observations lead robust decisions


fialgorithms applications decision probability





pr





e

h

h

h

figure naive bayes network cpts defined appendix c

computing decision probability
computing sdp involves computing expectation hidden variables h
naive brute force would enumerate check whether pr h e
instantiations h h present save us need explore
every possible instantiation h make easier understand first
describe compute sdp naive bayes network trivial
section computing sdp naive bayes network np hard
generalize arbitrary networks
computing sdp naive bayes networks
convenient implement test pr h e log odds
domain
log h e log

pr h e
pr h e




define log odds threshold log
equivalently test whether
log h e
naive bayes network class variable h e leaf variables
q h posterior log odds observing partial instantiation q h hj
written

log q e log e

j
x

w hi





whi weight evidence hi defined
whi log

pr hi e
pr hi e



weight evidence whi contribution evidence hi quantity
log q e chan darwiche note weights computed time
space linear h floating point representation table depicts weights
evidence network figure
additionally note equation since naive bayes networks hi separated e given
term e dropped equation leave term general networks hi
may separated e



fichen choi darwiche






w hi




w hi




table weights evidence attributes figure
h

h


h


h



h





h







h





figure search tree network figure solid line indicates dashed
line indicates quantity log q e displayed next node q
tree nodes log q e shown bold

one compute sdp enumerating instantiations variables h
equation test whether log h e figure depicts search tree
naive bayes network figure used purpose leaves
tree correspond instantiations h variables h generally every node
tree corresponds instantiation q q h
brute force computation sdp would entail
initializing total sdp
visiting every leaf node h search tree
checking whether log h e adding pr h e total sdp
figure depicts quantity log q e node q tree indicating five
leaf nodes e five instantiations variables h indeed contribute sdp
state key observation underlying proposed consider node
corresponding instantiation h search tree log h e
four completions h instantiation e four leaf nodes
log h e hence really need visit leaves add
contributions pr h e individually sdp instead simply add pr h e
sdp equals sum pr h e leaves importantly
detect leaves contribute sdp computing lower bound
weights depicted table two weights variable h minimum
moreover two weights variable h minimum
hence lowest contribution log odds made leaf node h


fialgorithms applications decision probability

h



h

h







figure reduced search tree network figure
adding contribution current log odds
lead log odds still passes given threshold
similar technique used compute upper bounds allowing us detect nodes
search tree leaf contribute sdp consider example
node corresponding instantiation h h log h h
e neither leaves node contribute sdp
log odds pass threshold detected considering weights
evidence variable h computing maximum weights adding
current log odds gives still threshold hence
leaf node h h contribute sdp part search tree
pruned
apply pruning technique lower upper bounds actually
end exploring portion tree shown figure pseudocode
final shown note takes linear time compute
upper lower bounds additionally note specific ordering h
search tree constructed directly linked amount pruning use ordering
heuristic ranks query variable hi difference corresponding upper
lower bound h ordered greatest difference lowest difference allow
earlier pruning
computing sdp arbitrary networks
generalize arbitrary networks viewing networks naive
bayes networks aggregate attributes first need following notion
definition partition h given e set sk si h
si sj sk h si independent separated sj j given
e
figure depicts example partition
intuition behind partition allows us view arbitrary network
naive bayes network class variable aggregate attributes sk
aggregate attribute si viewed variable states si allowing us view
instantiation h set values sk



fichen choi darwiche

computing sdp naive bayes network note q h hj
p
wq defined ji whi

input
n naive bayes network class variable
h attributes h hk
log odds threshold
e evidence
output decision probability p

main
global p initial probability
q initial instantiation empty set
depth initial depth search tree
dfs sdp q h depth
return p
procedure dfs sdp q h depth
p

u pperbound log e wq ki depth maxhi whi
p

lowerbound log e wq ki depth minhi whi

u pperbound return

else lowerbound

add pr q e p return

else

depth k

value hdepth attribute hdepth

dfs sdp qhdepth h hdepth depth
proposition partial instantiation q sj
log q e log e

j
x

w si






wsi log

pr si e
pr si e

proof
pr q e
pr q e
pr e pr e pr sj e
log
pr e pr e pr sj e

log q e log

log e

j
x




w si



fialgorithms applications decision probability

e

x

h

h



x

h

h

e

x

h

h

figure partition h given e h h h h
h h

since equations analogous equations use arbitrary network usage however requires auxiliary computations
needed readily available naive bayes networks discuss
computations next
finding partition
first need compute partition sk done pruning network
structure follows delete edges outgoing nodes evidence e hypothesis
delete successively leaf nodes neither h e identify
components x xk resulting network define non empty si h xi
element partition guarantees original network structure si
separated sj e j see darwiche figure network
pruning leads components x x x e h h h x e h
x x h h
computing posterior log odds probability weights evidence
quantities e pr q e wsi referenced lines
simple closed forms naive bayes networks arbitrary networks
however computing quantities requires inference
variable elimination described darwiche note network pruning
deleting edges removing leaf nodes discussed guarantees
factor used variable elimination variables component xi hence
variable elimination applied component xi isolation sufficient
obtain needed quantities


fichen choi darwiche

computing min max evidence weights
finally compute upper lower bounds maxsi wsi minsi wsi
referenced lines quantities
computed variable elimination applied component xi isolation
case however must eliminate variables xi si first variables si moreover
first set variables summed second set variables maxd
mind depending whether need maxsi wsi minsi wsi finally elimination
process applied twice evidence e second time evidence e
precisely every component xi set factors case
variable ordering perform variable elimination
sets factors eliminate

left
q iany nonquery intermediary variables
q



set factors pr si e set factors pr si e
since elimination order thus one one matching

pr ei
factors sets define set factors id pr id ei si






calculate wsi wsi respectively maximizing minimizing variables
note summing variables maximizing variables variable elimination
used dechter order solve map differs
perform maximization minimization calculate wsi wsi
set factors instead factors di di simply summing
intermediary variables
note similarly dechter first summing variables
performing maximization minimization case elimination order
case constrained meaning may forced use poor ordering variable
elimination high treewidth
complexity analysis
let n number variables network h h w maxi wi
wi width constrained elimination order used
component xi best case

time complexityof n exp w worst case time complexity
n exp w h intuition behind bounds computing
maximum
minimum weights aggregate attribute takes time n exp w bounds
complexity computing e pr q e corresponding weights wsi moreover
depending weights
threshold traversing search tree take anywhere
constant time exp h since depth first
search implemented linear

space space complexity n exp w

experimental
performed several experiments real synthetic networks test performance across wide variety network structures ranging simple
naive bayes networks highly connected networks real networks learned
datasets provided uci machine learning repository bache lichman


fialgorithms applications decision probability

network
car
emdec g
tcc e
ttt
caa
voting
nav
fire
chess

source
uci
hrl
hrl
uci
cresst
uci
cresst
cresst
uci

h
h
















naive










approx





















table comparison real networks time seconds takes
naive approx compute sdp different networks
note indicates computation complete minute
time limit constrained moreover indicates sufficient
memory complete computation

provided hrl laboratories cresst majority real networks used
diagnostic networks made clear variable selected decision
variable would knowledge fault variable unclear cases
decision variable picked random query evidence variables selected
random real networks
besides two options available compute sdp
naive method brute force computation enumerating possible instantiations
approximate developed choi et al compare
two approaches compute sdp real networks
network selected least total network variables query variables
could emphasize size query set greatly influences computation
time computation given minutes complete believe value
threshold greatly affect running time computed sdp thresholds
took worst case time experiments
three shown table note h number query
variables h number instantiations naive must enumerate
moreover indicates computation complete minute time limit
indicates sufficient memory complete computation networks
car ttt voting nav chess naive bayes networks whereas networks caa fire
polytree networks others general networks
given real networks tested clear
outperforms naive implementation approximate naive
bayes networks polytree networks note approximation
variable elimination use certain constrained orders naive bayes
http www cse ucla edu



fichen choi darwiche




average explored instantiations running time
number instantiations x e









time






number subnetworks







figure synthetic network average running time average number instantiations
explored number connected components

network hypothesis root approximation forced
use particularly poor ordering explains failure chess network
analyze general network structure selected threshold affects
performance generated synthetic networks variables
varying treewidth bngenerator ide cozman ramos network
randomly selected decision variable query variables evidence variables
generated partition network grouped networks size obtained
partition k goal test running time ability prune
search space depends k average time average number instantiations
explored shown figure
general see k increases number instantiations explored
decreases runtime improves network becomes similar
naive bayes structure increasing k moreover larger k levels
search tree means opportunities prune
worst case network may unable disconnected k however even
case still average efficient compared brute force
implementation cases computing maximum minimum weight
observing h exist h change decision
found given time limit hours brute force could solve
synthetic networks whereas solved networks
test threshold affects computation time calculate posterior
probability decision variable run repeatedly thresholds
varying increments away average running time increments seen
figure evident threshold set away initial
synthetic networks binary brute force would need explore instantiations



fialgorithms applications decision probability




average explored instantiations running time
number instantiations x e










time






threshold distance initial posterior



figure synthetic network average running time average number instantiations
explored threshold distance initial posterior probability

posterior probability finishes much faster perhaps expected since
usage extreme thresholds would allow search space pruning
overall experimental able solve many sdp
reach existing methods confirm
completes much faster network disconnected threshold far
away initial posterior probability decision variable

complexity computing decision probability
present complexity sdp first prove complexity
computing sdp naive bayes structures np hard general
complexity computing sdp lies complexity class general expectation
computation applicable wide variety queries graphical
computation non myopic value information
computing sdp naive bayes
sdp known pppp complete choi et al sdp remains
hard naive bayes networks
theorem computing decision probability naive bayes network nphard
proof reduce number partition defined karp computing
sdp naive bayes model suppose given set positive p
integers c p
cn
wish determine whether exists n ji ci j cj
solve considering naive bayes network binary class variable
uniform probability binary attributes h hn cpts leading weights


fichen choi darwiche

evidence whi ci whi f ci construction cpts done
solving following system equations
pr hi
pr hi f
pr hi f
ci log
pr hi f f
pr hi pr hi f
ci log

pr hi f pr hi f f

leave exact derivations see exercise darwiche get

pr hi f pr hi f

ci




pr hi pr hi f f

ci




note given cpts defined whi ci whi f
ci set integers partitioned instantiation h h hn
p
n
whi since would include indices hi case
first
pnthe naive bayes network satisfies number properties shall use
pnext
n
whi since weights whi integers next whi c
p
ni whi c hi hi finally pr h hn pr h hn hi hi
uniform probability distribution leaf hi defined
symmetric cpt
consider following sdp last step properties
sdp h hn
x
pr h hn pr h hn

h hn



x

log h hn pr h hn

h hn



x

h hn



x



n
x

h hn



pn

whi



whi pr h hn





n
x



whi pr h hn



instantiation h hn iff
n

x x
whi pr h hn
h hn





fialgorithms applications decision probability

hence partitioning solved iff
sdp h hn

complexity computing non myopic voi
sdp shown pppp complete choi et al class pppp
essentially counting variant nppp class contains polynomial hierarchy
ph map complete park darwiche
section general computing expectations pppp complete
non myopic voi sdp instance expectation thus development
compute sdp beneficial pppp class
turn benefits computing assortment expectations including non myopic voi
proposed expectation computation reward function r
properties review next particular function r assumed map
probability distribution pr e numeric value assume minimum
l maximum u range polytime computable assumptions
limitingfor example entropy utility expressed reward functions
fall category krause guestrin
consider following computation expectations
ept given polynomial time computable reward function r hypothesis variable
unobserved variables h evidence e real number n distribution pr induced
bayesian network variables x expectation decision asks
e

x

r pr h e pr h e

h

greater n
note sdp falls special case reward function r sdp indicator
function see definition example definition used choi et al
decision function outputs one two decisions depending whether pr e
value threshold
following theorems proofs appendix b
theorem ept pppp hard
theorem ept pppp
shows ept pppp complete implies computational
computing non myopic voi variety reward functions pppp complete
proof holds influence diagrams constrained one decision node



fichen choi darwiche

conclusion
discussed commonly used information gathering criteria
graphical value information reviewed recently introduced
notion decision probability sdp proposed usage
sdp decision making tool showing concrete examples usefulness
stopping criterion selection criterion stopping criterion sdp allow
us determine observations necessary selection criterion usage
sdp allow us select observations allow us increase decision robustness
justified usage sdp proposed exact
computation experimental comparable running time
previous approximate much faster naive brute force
finally presented several complexity

acknowledgements
combines extends work presented chen choi darwiche b
work partially supported onr grant n nsf
grant iis nsf grant iis would thank national
center evaluation standards student testing hughes lab
contributing sample diagnostic networks

appendix miscellaneous topics
section go details notions mentioned earlier
particular continue discussion section go notion
non myopic value information additionally continue left
section expand upon notion sdp stopping criterion context
utility decisions
appendix non myopic value information
myopic value information often used many applications easy compute
dittmer jensen vomlel gao koller however
myopic selection optimal times whole greater sum
parts individual observation set h seemingly may provide significant
value voi observing h high instance take function
x x alone neither x x useful together determinative
bilgic getoor computing non myopic voi optimal
voi obtained
due aforementioned myopic voi recently researchers
recently suggested non myopic voi instead myopic voi proposed methods compute non myopic voi heckerman horvitz middleton
liao ji krause guestrin zhang ji bilgic getoor
computing non myopic voi hidden variables h difficult involves com

fialgorithms applications decision probability

puting expectation possible values h quickly becomes intractable
h becomes larger
existing computing non myopic voi approximate
heckerman et al liao ji relatively limited restricted
tree networks leaf variables krause guestrin bilgic getoor
developed value information lattice voila framework
subsets hidden variables h examined optimal subset features
found increase classification accuracy meeting budget constraint
appendix sdp stopping criterion utility decisions
cases expected utility different decisions well cost reducing
uncertainty making observations quantified common decision theoretic
setting howard howard matheson influence diagrams commonly
used influence diagrams seen bayesian networks incorporate decision
utility nodes howard matheson zhang kjrulff madsen
selection criterion decision theoretic setting clear observations lead
greatest increase expected utility selected usage utilities observation
costs prevalent however numerous researchers noted difficulty coming
actual numerical quantities glasziou hilden lu przytula
bilgic getoor
sdp used stopping criterion decision theoretic context expected utility decisions influence diagrams howard matheson
extend definition sdp general setting allow applications
particular assume f polytime computable decision function outputs
decision distribution pr e instance decision function
commonly used classification select class highest posterior probability arg maxd pr e friedman et al whereas threshold decisions
decision function would simply select decision pr e
sdp thus defined probability decision would made
hidden states variables h known chen et al b
definition decision probability generalized given decision function f
hypothesis variable unobserved variables h evidence e decision probability sdp defined
x
f pr h e h pr h e

sdp f h e
h

f pr h e h indicator function

f pr h e f pr e

otherwise
original sdp definition however assumed binary variable
f pr e pr e threshold darwiche choi
consider use sdp stopping criterion context expectedutility decisions influence diagrams howard matheson particular


fichen choi darwiche

q

c




p

figure influence diagram investment
sdp distinguish high risk high reward scenarios lowrisk low reward scenarios otherwise indistinguishable consider usage
voi utilities alone
consider influence diagram figure consists bayesian network
three variables c q decision node utility node p direct
function utility function u influence diagram investment
venture capital firm deciding whether invest amount million
tech startup allowing money collect interest bank f
example profit investment p depends decision success
company turn depends two factors whether existing competitor
companies successful c whether co founders startup high
quality original idea q c q unobserved initially independent
variable latent hypothesis variable case thus cannot observed
variables c q however observed price
goal choose decision maximum expected utility
x
eu e
pr e u


u utility decision given evidence e variables c q
figures contain two different parameterizations influence diagram
figure refer different scenarios investment
scenarios given evidence variables c q best decision f
expected utility k decision maker may commit decision decide
observe variables c q hope finding better decision light
additional information classical stopping criterion compute maximum
expected utility given observe variables c q heckerman et al dittmer
jensen
x
max
eu c q pr c q


c q

scenarios maximum expected utility comes k showing
observations may lead better decision
according formulation krause guestrin computed voi variables
c q reward function



fialgorithms applications decision probability

q

f

pr q



c

f

pr c



q


f
f

pr





c

f

f



f
f



f

f

u





figure parameterization influence diagram figure

q

f

pr q



c

f

pr c



q


f
f

pr





c

f

f



f
f



f

f

u





figure parameterization influence diagram figure
point two scenarios indistinguishable viewpoint
classical decision making tools remember krause guestrin bilgic
getoor remark budget observations expended long
value information observation greater cost observation according
selection criteria variables thus observed expected
financial gain could well increase
sdp however finds two scenarios different particular
respect variables c q sdp first scenario second
scenario even though stand make better decision scenarios upon
observing variables c q least respect financial gain even though
expected benefit observations scenarios unlikely
would change current decision f second scenario comparison
first hence given additional information provided sdp decision maker may
act quite differently two scenarios indeed take closer look second
scenario state world deciding invest would yield
large financial gain however chance state manifesting extremely


fichen choi darwiche

small analogous lottery meaning risk conscious decision maker may
averse gamble second scenario even waste resources observe variables
c note example assumed utility incorporate
risk factor rational decision maker would choose gather
information despite low probability changing current decision
illustrates usefulness sdp stopping criterion context expectedutility decisions influence diagrams namely sdp distinguish
two different scenarios otherwise indistinguishable consider utilities
alone

appendix b proofs
section provide proofs theorems
proof theorem ept pppp hard reduction following decision sdp corresponds originally proposed notion decision
probability threshold decisions darwiche choi
sdp given decision probability pr e surpassing threshold set
unobserved variables h probability p decision probability
x
pr h e pr h e

h

greater p
denotes indicator function evaluates enclosed expression
satisfied otherwise sdp shown pppp complete choi et al
decision probability corresponds expectation respect distribution pr h e reward function

pr h e
r pr h e
otherwise
thus decision probability iff expectation
proof theorem ept pppp provide probabilistic polynomialtime access pp oracle answers decision ept
correctly probability greater proof generalizes simplifies proof
given choi et al sdp
consider following probabilistic determines e n
sample complete instantiation x bayesian network probability pr x
linear time forward sampling henrion
x compatible e use pp oracle compute r pr h e
first reward function r computed polynomial time definition
second pr h e computed pp oracle since inference pcomplete roth since ppp p p


fialgorithms applications decision probability

define function tn
ul defines probability used probabilistic
guess whether e n see lemma
declare e n probability
x compatible e





x compatible e

probability declaring e n
r

x
h

greater





pr h e pr e




iff following set equivalent statements hold
x

pr h e

h

x

pr h e

h






tn
pr h e

ul



x tn
pr h e
ul
h
x
n pr h e

x
h

pr e


h

x

r pr h e pr h e n

h

thus r




iff e n

lemma function






tn
ul

maps reward probability

proof values u l given denote upper lower bounds reward
threshold n thus tn
ul
note denotes probability used declare whether e n
higher lower depending value reward r pr h e

appendix c conditional probability tables
section provide conditional probability tables networks figures




fichen choi darwiche





pr



hi


p
p
n
n
l
l







xi









x





ei









pr x





x





pr ei hi xi









hi

p
n
l

x





pr x x





pr hi





figure cpts bayesian network given figure note
cpts variables ei lines case ei given since
pr ei hi xi pr ei hi xi

pr









pr









pr









pr









table cpts network figure parameterization



fialgorithms applications decision probability

pr









pr









pr









pr









table cpts network figure parameterization

pr













pr









table cpts bayesian network figure

h pr h









h pr h









table cpts network figure pr h pr e pr h
equal



fichen choi darwiche

references
arroyo woolf b inferring learning attitudes bayesian network
log file data proceedings th international conference artificial
intelligence education pp
bache k lichman uci machine learning repository
bilgic getoor l value information lattice exploiting probabilistic independence effective feature subset acquisition journal artificial intelligence
jair
butz c j hua maguire r b web intelligent tutoring system
computer programming web intelligence pp ieee computer society
chan h darwiche reasoning bayesian network classifiers proceedings th conference uncertainty artificial intelligence pp
chen j low k h tan c k oran jaillet p dolan j sukhatme g
decentralized data fusion active sensing mobile sensors modeling
predicting spatiotemporal traffic phenomena proceedings twenty eighth
conference annual conference uncertainty artificial intelligence uai pp
corvallis oregon auai press
chen choi darwiche b decision probability tool
decision making proceedings sixth european workshop probabilistic
graphical pp
chen choi darwiche exact computing samedecision probability proceedings rd international joint conference
artificial intelligence pp
choi xue darwiche decision probability confidence measure threshold decisions international journal approximate reasoning
ijar
conati c gertner vanlehn k bayesian networks manage uncertainty student modeling user modeling user adapted interaction

cover thomas j elements information theory wiley interscience
darwiche modeling reasoning bayesian networks st edition cambridge university press
darwiche choi decision probability confidence measure
threshold decisions noisy sensors proceedings fifth european
workshop probabilistic graphical pp
dechter r bucket elimination unifying framework reasoning artificial
intelligence
dittmer jensen f myopic value information influence diagrams proceedings thirteenth conference annual conference uncertainty artificial
intelligence uai pp


fialgorithms applications decision probability

friedman n geiger goldszmidt bayesian network classifiers machine
learning
gao koller active classification value classifier advances
neural information processing systems nips
gertner conati c vanlehn k procedural help andes generating hints bayesian network student model proceedings national
conference artificial intelligence pp
glasziou p hilden j test selection measures medical decision making

greiner r grove j roth learning cost sensitive active classifiers
artificial intelligence
hamscher w console l de kleer j eds readings model diagnosis morgan kaufmann publishers inc
heckerman breese j rommelse k decision theoretic troubleshooting
communications acm
heckerman horvitz e middleton b approximate nonmyopic computation value information ieee transactions pattern analysis machine
intelligence
henrion propagating uncertainty bayesian networks probabilistic logic
sampling proceedings second annual conference uncertainty artificial
intelligence uai pp
howard r information value theory ieee transactions systems science
cybernetics
howard r matheson j e eds readings principles applications decision analysis strategic decision group
ide j cozman f g ramos f generating random bayesian networks
constraints induced width proceedings th european conference
artificial intelligence pp
jordan discriminative vs generative classifiers comparison logistic
regression naive bayes advances neural information processing systems

kahn c e roberts l shaffer k haddawy p construction
bayesian network mammographic diagnosis breast cancer computers biology
medicine
karp r reducibility among combinatorial complexity computer computations springer
kjrulff u b madsen l bayesian networks influence diagrams
guide construction analysis springer
krause guestrin c near optimal nonmyopic value information graphical st conference uncertainty artificial intelligence pp


fichen choi darwiche

krause guestrin c optimal value information graphical
journal artificial intelligence jair
kruegel c mutz robertson w valeur f bayesian event classification
intrusion detection proceedings annual computer security applications
conference acsac
liao w ji q efficient non myopic value information computation influence diagrams international journal approximate reasoning
lindley v measure information provided experiment annals
mathematical statistics
lu c przytula k w focusing strategies multiple fault diagnosis
proceedings th international flairs conference pp
millan e descalco l castillo g oliveira p diogo bayesian
networks improve knowledge assessment computers education
modelo howard g bagchi lebanon g determining placement intrusion detectors distributed application bayesian network modeling
proceedings th international symposium recent advances intrusion
detection pp
munie shoham optimal testing structured knowledge aaai
proceedings rd national conference artificial intelligence pp
ognibene demiris towards active event recognition proceedings
rd international joint conference artificial intelligence pp
park j darwiche complexity approximation strategies
map explanations journal artificial intelligence jair
pauker g kassirer j p threshold clinical decision making
england journal medicine
raiffa h decision analysis introductory lectures choices uncertainty
addison wesley
ramoni sebastiani p robust bayes classifiers artificial intelligence
roth hardness approximate reasoning artificial intelligence

shann seuken active learning home heating
smart grid proceedings rd international joint conference artificial
intelligence pp
stratonovich r value information izvestiya ussr academy sciences
technical cybernetics
van der gaag l c coupe v h sensitivity analysis threshold decision
making bayesian belief networks ai ia pp
vomlel j bayesian networks educational testing international journal
uncertainty fuzziness knowledge systems supp


fialgorithms applications decision probability

xenos prediction assessment student behaviour open distance
education computers bayesian networks computers education

yu krishnapuram b rosales r rao r b active sensing international
conference artificial intelligence statistics pp
zhang n l probabilistic inference influence diagrams computational intelligence pp
zhang ji q efficient sensor selection active information fusion ieee
transactions systems man cybernetics part b




