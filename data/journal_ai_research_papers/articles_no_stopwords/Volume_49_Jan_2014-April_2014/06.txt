Journal Artificial Intelligence Research 49 (2014) 527-568

Submitted 10/13; published 03/14

Large-Scale Optimization Evaluation Functions
Minimax Search
Kunihito Hoki

hoki@cs.uec.ac.jp

Department Communication Engineering Informatics
University Electro-Communications

Tomoyuki Kaneko

kaneko@acm.org

Department Graphics Computer Sciences
University Tokyo

Abstract
paper presents new method, Minimax Tree Optimization (MMTO), learn
heuristic evaluation function practical alpha-beta search program. evaluation
function may linear non-linear combination weighted features, weights
parameters optimized. control search results move decisions agree game records human experts, well-modeled objective function
minimized designed. Moreover, numerical iterative method used find local
minima objective function, forty million parameters adjusted
using small number hyper parameters. method applied shogi, major
variant chess evaluation function must handle larger state space
chess. Experimental results show large-scale optimization evaluation
function improves playing strength shogi programs, new method performs
significantly better methods. Implementation new method shogi
program Bonanza made substantial contributions programs first-place finish
2013 World Computer Shogi Championship. Additionally, present preliminary evidence
broader applicability method two-player games chess.

1. Introduction
Heuristic search powerful method artificial intelligence. 1997, chess-playing
computer Deep Blue defeated world chess champion Garry Kasparov (Campbell, Hoane,
& Hsu, 2002). computer decided moves making large number searches
minimax game tree using heuristic evaluation functions. framework
artificial intelligence, heuristic evaluation functions, well search methods,
crucial making strong computer players. Thus, researchers working various games
made substantial efforts quest create effective evaluation functions using machine learning techniques (Furnkranz, 2001). However, fully automated learning
heuristic evaluation functions remains challenging goal chess variants. example, developers reported majority features weights Deep Blue
created/tuned hand (Campbell et al., 2002). said recent top-level chess
programs tune parameters automatically, although yet find
publication describing methods use. Moreover, reinforcement learning
applied chess (Baxter, Tridgell, & Weaver, 2000; Veness, Silver, Uther, & Blair, 2009).
c
2014
AI Access Foundation. rights reserved.

fiHoki & Kaneko

However, best authors knowledge, evaluation functions learned
methods reported literature still weaker best hand-crafted functions
terms chess-playing strength.
paper, revisit idea behind earlier research learning chess evaluation
functions (Marsland, 1985; Hsu, Anantharaman, Campbell, & Nowatzyk, 1990; Tesauro,
2001) reformulate task optimization problem using alternative learning
method, called Minimax Tree Optimization (MMTO). objective optimize
full set parameters evaluation function search results match
desired move decisions, e.g., recorded moves grandmaster games. evaluation
functions learned iteration two procedures: (1) shallow heuristic search
training positions using current parameters (2) parameter update guided
approximation gradient objective function. achieve scalability stability,
introduce new combination optimization techniques: simplified loss function, gridadjacent update, equality constraint, l1 -regularization. One resulting merits
MMTO ensure existence local minimum within convenient range
parameters.
study demonstrates performance MMTO shogi, variant chess
evaluation functions need handle wider variety features positions Western
chess. Implementation MMTO shogi program Bonanza (described Section 4.6)
made substantial contribution programs first-place finish 2013 World Computer Shogi Championship. rules shogi, well survey approaches artificial
intelligence, described literature (Iida, Sakuta, & Rollason, 2002). Basic techniques, minimax search guided heuristic evaluation functions, effective
shogi chess. However, drop rule allows player reuse captured pieces
significantly changes properties: (1) number legal moves, well average
game length, greater chess, (2) endgame databases available, (3)
material balance less important chess, especially endgame. Thus,
performance shogi program dependent quality evaluation function.
experiments, first show full set parameters evaluation
functions optimized respect rate agreement training set.
that, examine performance various learned evaluation functions terms
rates agreement test positions win rates references. Scalability
demonstrated forty million parameters, far many tune
hand. features used piece values extended versions piece-square tables
commonly used learn evaluation functions chess (Tesauro, 2001; Baxter et al.,
2000; Veness et al., 2009). briefly examine performance MMTO chess
catch glimpse applicability MMTO games.
rest paper organized follows. next section reviews related research.
third section presents MMTO method. fourth section shows experimental
results, forty million parameters adjusted better performance, compares
performance method existing methods. last section presents
concluding remarks. paper incorporates extends previous work (Hoki &
Kaneko, 2012; Kaneko & Hoki, 2012).
528

fiLarge-Scale Optimization Evaluation Functions Minimax Search

2. Related Work
section reviews related research learning evaluation functions. First, describe
supervised learning methods use desired moves. Second, discuss learning
methods, including regression reinforcement learning. Third, briefly discuss difficulty supervised learning terms numerical optimization. Although machine learning
components besides evaluation functions game programs would interesting
research topic (Bjornsson & Marsland, 2002; Tsuruoka, Yokoyama, & Chikayama, 2002;
Coulom, 2007; Silver & Tesauro, 2009), review focuses research
done learning evaluation functions.
2.1 Learning Desired Moves Chess
Grandmaster games popular source information learning chess. Let us say
set positions P desired moves position P. Typically,
positions moves sampled grandmaster games. chess program
evaluation function e(p, w ), p game position feature weight vector w
contains parameters adjusted.
Let us assume evaluation function e(p, w ) partially differentiable respect
wi i. Here, wi i-th component w .
P example, function could
linear combination weighted features, i.e., e(p, w ) = wi (p), (p) i-th
feature value position p. aim learning find better weight vector w
strengthening play program. hypothesis behind kind learning
computer play agrees desired moves, better plays.
Let us begin simple intuitive goal: make results one-ply search agree
desired moves. simplicity, let us assume maximizing player moves first
root position p. one-ply search, move highest evaluation value
selected. Thus, w adjusted desired move highest evaluation
moves. goal formally written mathematical minimization problem
objective function:
P
w) =
JH
(w

X X

H (e(p.m, w ) e(p.dp , w )) .

(1)

pP mM0p

Here, p.m position move position p, dp desired move position p, M0p
set legal moves p excluding dp , H(x) Heaviside step function, i.e.,
H(x) equals 1 x 0, 0 otherwise. objective function counts number
moves evaluation value greater equal desired move,
better w found minimizing Eq. (1). Although several studies attempted
machine learning basis framework (Nitsche, 1982; van der Meulen, 1989;
Anantharaman, 1997), numerical procedures complicated, adjustment
large-scale vector w seemed present practical difficulties.
Marsland (1985) presented notable extension wherein continuous function used
conventional optimization techniques exploited. Here, continuous function
difference substituted non-continuous step function Eq. (1). interesting
529

fiHoki & Kaneko

modified function
w) =
J2P (w

X X

[max {0, e(p.m, w ) e(p.dp , w )}]2 .

(2)

pP mM0p

meaning function value different Eq. (1); i.e., function
count number moves evaluation value greater equal
w ) helps reduce function
desired move. However, gradient vector w J2P (w
value numerically. Marsland introduced inequality constraints order keep
evaluation right range. However, literature provide experimental
results practical chess programs.
second notable extension proposed early development chess machines
Deep Thought (Nowatzyk, 2000; Hsu et al., 1990). Here, positions compared
p.m, rather wp.m , is, one leaves principal variations (PVs), possibly
several plies p.m. extension carries least-square fitting evaluation
w ) does. Instead, biases
values. Therefore, max function J2P (w
p.dp
value e(w , w ) used least-square fitting, evaluation value
p .d
p .m
desired move dp , e(w p , w ) lower another move m, e(w
, w ).
third notable extension comparison training proposed Tesauro (2001).
Tesauro modified objective function
X X
p .d
P
p .m
w) =
Jct
(w
Tct (e(w p , w ) e(w
, w )),
pP mM0p

Tct (x) = [(R(x)) 1]2 ,

(3)

standard sigmoid function, R heuristic rescaling factor positive
differences, i.e., R(x) = x x 0, R(x) = cx constant c > 1 otherwise. Note
R(x) still continuous function. important property modified objective
function value derivative zero limit difference x goes
positive infinity, respectively one zero limit difference
x goes negative infinity. Therefore, Tct (x) Eq. (3) continuous approximation
H(x) Eq. (1). Note property explicitly stated Tesauro,
notably distinct work. number feature weights adjusted
method less two hundred. Tesauro mentioned application small-bit
integers, used adjust weights Deep Blue. However, neither
clarified procedure mentioned whether weights automatically adjusted
experiment.
Table 1 summarizes related work. existing methods possesses least
one three important properties optimization, i.e., continuity, minimax searches,
assured local minimum. However, none three properties. Also,
existing methods (Nowatzyk, 2000; Hsu et al., 1990; Tesauro, 2001) try
decrease functions iteration much possible. revisit issues
Section 2.3. hand, method, MMTO, scalability high-dimensional
learning. Moreover, empirically show decrease objective function value
leads increase playing strength. existing methods shown
property.
530

fiLarge-Scale Optimization Evaluation Functions Minimax Search

Method
(Nitsche, 1982)
(Marsland, 1985)
(van der Meulen, 1989)
(Hsu et al., 1990)
(Anantharaman, 1997)
Comparison training
MMTO

Continuity

Search

Assured local minimum












Yes


Yes

Yes
Yes
Yes
Yes



Yes
Yes

Yes


Yes

Table 1: Summary learning methods using desired moves training positions
adjust feature weights evaluation functions. first column name
method piece literature. second column describes continuity
objective functions respect feature weights. Yes means
continuity depends kind search method used. third column indicates
whether objective functions use minimax searches depths 1,
instead comparisons legal moves root position. fourth column
shows whether hyper parameters objective functions assure local
minimum found.

2.2 Methods Learning Evaluation Functions
Many researchers utilized information sources desired moves.
example, studies Othello dating 1990s compare desired moves
moves (Fawcett, 1993). However, practical famous machine learning
method yielded strong programs based regression desired value
using 1.5 million features (Buro, 1995, 2002). Othello, different evaluation functions
used game stages determined basis number discs play. Thus,
desired values training positions obtained complete endgame search
well heuristic search evaluation functions learned later game stages.
method successfully applied card games (Buro, Long, Furtak, & Sturtevant,
2009), chess variants. best authors knowledge, learning based
regression win/loss-labeled data yielded decent evaluation functions chess
variants. Except using desired moves, Buros method properties
similar listed Table 1; objective function continuity well assured
local minimum, method scalable. Gomboc, Buro, Marsland (2005) proposed
learn game records annotated human experts; however, feature weights
adjusted experiments small part full evaluation functions.
Reinforcement learning (Sutton & Barto, 1998), especially temporal difference learning,
famous success Backgammon (Tesauro, 2002), considered promising
way avoid difficulty finding desired values regression. approach
applied chess shown improve strength programs (Baxter
et al., 2000; Levinson & Weber, 2001; Veness et al., 2009). KnightCap program
achieved rating 2, 150 points Free Internet Chess Server (FICS1 )
1. Free Internet Chess Server, http://www.freechess.org, last access: 2013

531

fiHoki & Kaneko

Easy
(a)

Single minimum

Dicult
(b)

(c)

(d)

Smooth Non-dieren able

(e)

Narrow trough Non-con nuous

Figure 1: Example illustrating difficulties facing minimization procedure.
2, 575 points highest peak Internet Chess Club (ICC) (Baxter et al., 2000).
Another program achieved 2, 338 points highest peak ICC (Veness et al., 2009).
However, strong human players ratings 3, 000 points ICC,
difference means programs reached top level chess programs;
is, evaluation functions tuned reinforcement learning yet reached level
best-handcrafted evaluation functions chess. Moreover, number feature
weights adjusted order thousands. checkers, evaluation functions
trained temporal difference learning reportedly comparable best handcrafted
efforts (Schaeffer, Hlynka, & Jussila, 2001). reported player stronger
expert human checker players created using neural networks trained
evolutionary strategy (Chellapilla & Fogel, 1999). Here, features beyond piece
differentials given neural network priori.
Many machine learning techniques (Baxter et al., 2000; Veness et al., 2009)
applied shogi. However, despite efforts many programmers researchers, adjustment full weight vector evaluation function remains challenging goal.
studies published far adjusted piece values small part feature
weights evaluation functions (Beal & Smith, 2001; Ugajin & Kotani, 2010).
2.3 Learning Numerical Optimization
learning methods reviewed Section 2.1 objective functions decrease;
learning process extended numerical optimization using functions.
performance numerical optimization sensitive surface objective function.
Figure 1 shows properties particular sorts functions difficulties regarding
numerical minimization. easiest one among convex function (a); local
minimum exists, global minimum. Function (b) multiple local minima; however, still thought easy problem, various minimization algorithms
using gradients Hessian matrices effective it. would desirable design
learning method using, say, linear logistic regression, uses one two types
objective function (Buro, 2002).
contrast, non-differentiable functions (c) (e) often difficult
minimize differentiable ones. quadratic model, Hessian
approximation conjugated gradient method (Bertsekas & Bertsekas, 2008),
always appropriate functions. Function (d) difficult target,
important local minimum hidden inside deep narrow trough, quite difficult
find using numerical iteration methods. difficult example minimization
532

fiLarge-Scale Optimization Evaluation Functions Minimax Search

non-continuous function (e); even primitive iterative methods gradient decent
capable finding minimum. extreme case would function
analytical formula gradient unavailable. case, learning method would
able use partial derivatives, minima would obtained using
derivative-free methods, e.g., sampling methods (Bjornsson & Marsland, 2002; Coulom,
2012).
Theorems Appendix show minimax value continuous always
partially differentiable. Thus, existing methods incorporate minimax search
(Hsu et al., 1990; Tesauro, 2001) MMTO listed Table 1 type (c). Moreover,
certain forward pruning techniques may cause discontinuities. Therefore, even learning
methods type (e). overcome difficulty, MMTO well-modeled objective
function updates feature weights careful manner.

3. Minimax Tree Optimization
Minimax Tree Optimization (MMTO) extension comparison training reach
first intuitive goal embodied Eq. (1). purpose extension overcome
practical difficulties stabilize mathematical optimization procedure largescale feature weight vector w . Given set training positions P desired move dp
position p, MMTO optimizes weight vector w minimax search
w better agrees desired moves.
weight vector w improved iteration sub-procedures (see Figure 2).
iteration t, first step consists tree searches identify one leaves
PVs w (t) legal moves training positions P. PV leaf w (t) depends
feature weights w (t) evaluation function, new PV obtained
w (t) updated (We discuss issue Section 3.5). second step calculation
approximate partial derivatives, depends PV weight vector.
last step update weight vector. numerical stability, difference
w (t + 1) w (t)| must kept small distorted drastic changes
|w
partial derivatives. Section 3.4 shows grid-adjacent update ensures this.
3.1 Objective Function Minimized
objective function
P
w ) = J(P, w ) + JC (w
w ) + JR (w
w ),
JMmto
(w

(4)

first term J(P, w ) right side main part. terms JC
JR constraint regularization terms, respectively, defined Section 3.2.
first term
X X
J(P, w ) =
(s(p.dp , w ) s(p.m, w )) ,
(5)
pP mM0p

s(p, w ) minimax value identified tree search position p. (x)
1/(1 + exp(ax)), horizontally mirrored sigmoid function. slope (x)
controlled constant parameter > 0. large limit, (x) becomes Heaviside
533

fiHoki & Kaneko





wp.(t)

1. Perform game-tree search identify PV leaves
child positions
p.m position p training set P, w (t) weight vector
t-th iteration w (0) initial guess.
2. Calculate partial-derivative approximation well-modeled objective
p .m
w
function defined Section 3.1 using w
(t) (t). objective
function employs differentiable approximation H(x) (see Section 3.1),
well constraint regularization term (see Section 3.2).



3. Obtain new weight vector w (t+1) w (t) using grid-adjacent update
guided partial derivatives computed step 2 (see Section 3.4). Go
back step 1, terminate optimization objective function
value converges (see Section 4).



Figure 2: Minimax Tree Optimization: Iteration searches update using partial
derivatives

step function H(x). Thus, main differences first intuitive objective function
P (w
w ) Eq. (1) use (x) smooth approximation H(x) use
JH
w )
search result s(p, w ) instead raw evaluation e(p, w ). difference J2P (w
P
w
w
Eq. (2) Jct (w ) Eq. (3) J(P, ) simpler closer first intuitive one
w ) JR (w
w ) Eq (4).
Eq. (1). Moreover, none existing studies incorporate JC (w
minimax value s(p, w ) equals raw evaluation value e(wp , w ), e(p, w )
evaluation position p wp one PV leaves identified tree search rooted
p weight vector w . cases, derivatives s(p, w ) equal derivatives
e(wp , w ). reasons, PV leaves identified step 1 Figure 2.
3.2 Constraint Regularization Terms
computer programs chess variants, evaluation values typically represented
integers. Signed 16-bit integers especially preferred corresponding transposition tables memory efficient. Thus, restrict range absolute
value evaluation function e(p, w ). Moreover, search results change
w constant factor > 0, restriction
one uses scaled weight vector w
stabilizes numerical optimization procedure value uncertain.
w ) = 0 g(w
w 0 ) Eq. (4),
restriction, introduce constraint term JC (w
0
w ) = 0 equality constraint, 0 Lagrange multiplier.
subset w , g(w
w ) (see
addition constraint term, introduce regularization term JR (w
w ) = 1 |w
w 00 |, 1 > 0
last term Eq. (4)). use l1 -regularization JR (w
constant variable, w 00 subset w . l1 -regularization widely used deal highdimensional parameters, whereas l2 -regularization used avoid over-fitting (Tibshirani,
1996).

w0

534

fiLarge-Scale Optimization Evaluation Functions Minimax Search

P (w
w ) exists
constraint regularization terms ensure local minimum JMmto
finite range w . hand, depending P distribution dp ,
P (w
w ) Eq. (2), Jct
w ) Eq. (3).
property always true J(P, w ) itself, J2P (w
constraint l1 -regularization terms similar functionalities; i.e., restrict
range absolute value evaluation function e(p, w ). However, distinctions important practice l1 -regularization makes weight vector w 00 sparse
whereas constraint term not. Thus, regularization term suitable minor
features rarely seen, whereas constraint term suitable major features
appear often training set. Moreover, terms useful controlling
strength restriction. major feature values usually change often
minor feature values, magnitudes partial derivatives respect major feature
weights usually greater respect minor feature weights. adjust
strength l1 -regularization term weaker constraint term.
example, experiments used constraint term piece values
feature values, i.e., number pieces owned black/white, change single games
shogi. many weights penalized l1 -regularization. weight
w 0 , w 00 ).
controlled either constraint l1 -regularization term, i.e., w = (w
partial derivatives respect major minor feature weights differed several
orders magnitude, difficult stabilize optimization procedure means
single hyper parameter 1 .

3.3 Partial Derivative Approximation
iteration, feature weights updated basis partial derivatives
P (w
w ) defined Eq. (4). partial derivative, exists,
objective function JMmto
P



w) =
w) +
w ).
JMmto (w
J(P, w ) +
JC (w
JR (w
wi
wi
wi
wi

(6)


w ) right side treated intuitive manner; sgn(wi )1
last term w
JR (w

00
wi w , 0 otherwise. Function sgn(x) 1 x > 0, 0 x = 0, 1 x < 0.

w ) 0 wi
JC (w
/ w 0 . case wi w 0
partial derivative constraint term w

discussed Section 3.5.
partial derivative J(P, w ) always exist, minimax value
s(p, w ) always differentiable. Instead, use approximation,


J(P, w ) =
wi


X X
(s(p.dp , w ) s(p.m, w ))
wi
0

(7)



X X
p .d
e(w p , w ) e(wp.m , w )
wi
0

(8)

pP mMp

pP mMp

=

X X

p.d

0 (e(w p , w ) e(wp.m , w ))

pP mM0p


p .d p
e(w , w ) e(wp.m , w ) ,
wi

0 (x) = ddx (x). approximation Eq. (7) Eq. (8) makes computation
tractable, identify PV leaves step 1 Figure 2. stated Appendix A,
535

fiHoki & Kaneko

minimax value s(p, w ) found search continuous, therefore, function
J(P, w ) continuous. Moreover, approximate value equivalent partial
derivative unique PV exists position. Appendix discusses con
ditions w
s(p, w ) exists. Note found errors caused

approximation sufficiently small shogi application (Kaneko & Hoki, 2012).
Previous studies (Baxter et al., 2000; Tesauro, 2001) use approximation well.
3.4 Grid-Adjacent Update
numerical stability, grid-adjacent update step 3 (see Figure 2) used get
w (t + 1) w (t). Consider simple n-dimensional grid distance two
adjacent points h. Suppose h integer, e.g., h = 1. grid-adjacent update,
feature vector w (t) always one points grid, i-th component
wi (t + 1) adjacent wi (t):
wi (t + 1) = wi (t) h sgn(

P (w
w (t))
JMmto
).
wi

Thus, |wi (t+1)wi (t)| = |wi | = h 0 i. update decrease objective
P (w
P (w
w ) w
w w JMmto
w ) 0 errors approximation (see
function JMmto
Eq. (8)) negligible. Moreover, h must small enough update
p
p
change PV, i.e., w
(t) = w (t+1) majority positions p searched step 1.
Although MMTO focuses optimization weight vectors represented integers,
noted gradient descent update suitable even one uses floatingpoint feature weights. preliminary experiments indicate partial derivatives
J(P, w ) respect major minor feature weights differ seven orders
w proportional gradient vector may
magnitude. Thus, update vector w
appropriate updating minor feature weights small step. Thus, step
size component weight vector fixed grid-adjacent update,
might able controlled ways (see, e.g., Duchi, Hazan, & Singer, 2011).
3.5 Combination Techniques Practical Issues
MMTO combination above-described techniques. subsection discusses
practical issues combination alternatives; relate external constraints
learning (e.g., many weeks wait results), depend properties domain MMTO applied.
3.5.1 Lagrange Multiplier Grid-Adjacent Update
numerical stability, MMTO explores restricted parameter space constraint
w ) = 0. this, Lagrange multiplier 0 JC (w
w ) set
satisfied, i.e., JC (w
w)
J(P,w
0
median partial derivatives { wi | wi w } order maintain constraint
w 0 ) = 0 iteration. result, wi0 h n feature weights, h n feature
g(w
weights, 0 one feature weight, number feature weights w 0 2n + 1.
w ) constant iterations.
hand, 1 regularization term JR (w
536

fiLarge-Scale Optimization Evaluation Functions Minimax Search

3.5.2 Search Depth
game tree searches step 1 Figure 2 time-consuming step MMTO.
Tesauro (2001) shown use quiescence search yields better evaluation
functions. Thus, expected deeper searches MMTO yield better evaluation
functions. hand, must handle large amount training positions,
search time tends grow exponentially increase search depth. Therefore,
experiments use 1-ply standard search together quiescence search. Here,
quiescence search called every frontier node standard search. observed
evaluation functions learned shallow searches still effective playing games
deep searches (see Section 4.4). Similar results reported Tesauro.
3.5.3 Reuse PV Efficiency Learning
step 1 Figure 2 time-consuming part, worth considering omitting
assuming wp (t) = wp (t1) certain frequency. experiments, steps 2 3
repeated 32 times without running step 1. counted number iterations
run step 1. is, iteration ran single step 1 32 pairs steps 2 3.
number 32 would domain dependent set small enough update
change PV positions.
3.5.4 Pruning Trees
Pruning techniques dramatically reduce number searched nodes hence speed
learning. Fortunately, pruning introduce discontinuities objective
function. hand, pruning methods, including futility pruning (Schaeffer,
1986), may introduce discontinuities (see Appendix A.4). Therefore, robustness
whole learning procedure examined pruning techniques used.
far authors experience goes, objective function futility pruning seems
continuous (see Section 4.5).
3.5.5 Convergence Performance Measurement
termination criteria usually difficult determine iterative computations.
case learning shogi evaluation function, convergence objective function
MMTO seems significant criteria, rate agreement test set
Elo rating learned evaluation function converge converges. Note
rate agreement measured separate test set training set
order detect overfitting (see Section 4.3).
3.5.6 Duplication Positions Alternative Moves
Game records usually duplications positions desired moves opening
phase. Although ideal distributions positions desired moves unknown,
decided remove duplications training test sets simplicity.
is, use pair hposition, movei iteration. duplications
detected Zobrist hashing (1990). Note two different moves may
suggested position training test sets, objective function
537

fiHoki & Kaneko

becomes smaller tree search rooted position matches one moves.
result, conflicting goals move better move b vice versa
independently augmented objective function cancel
moves played position. experience, adaptation seems work
reasonably well shogi, best solution may depend target game.

4. Experiments
evaluated effectiveness MMTO experiments number feature
weights evaluation function varied thirteen forty million.
found MMTO works better comparison training intuitive modifications
terms rate agreement, speed convergence, game-playing strength.
w ) regularization term JR (w
w ) help inalso observed constraint term JC (w
crease performance evaluation functions terms rate agreement
test set. see numerical convergence, investigated surfaces objective
function MMTO limited number feature weights experimentally found
MMTO finds local minima reasonable range feature weights. Finally, carried
preliminary experiments chess well experiments data quality dependence.
4.1 Setup: Evaluation Functions, Features, Game Records
experiments described section used Bonanza, whose source code
available online (Hoki & Muramatsu, 2012). performance Bonanza major tournaments discussed Section 4.6. Bonanza uses techniques MMTO, PVS (Pearl,
1980; Marsland & Campbell, 1982; Reinefeld, 1983), capture search frontier nodes
quiescence search, transposition tables (Zobrist, 1990; Russell & Norvig, 2002), static exchange evaluation (Reul, 2010), killer history heuristics (Akl & Newborn, 1977; Schaeffer, 1989), null move pruning (Adelson-Velskiy, Arlazarov, & Donskoy, 1975; Heinz, 1999),
futility pruning (Schaeffer, 1986; Heinz, 1998), late move reductions (Romstad, 2010).
uses opening-book database randomly chose opening lines
self-play experiments. game records training test sets exclusively
chosen games played famous tournaments2 . 48, 566 game records
total. 30, 000 games played professional players using standard
time controls, i.e., one ten hours side byoyomi period (once time
2. abbreviated tournament name, number games used, date range games are: Juni,
12827, 19462010; Kisei, 3286, 19622010; Ryuo, 3279, 19872010; Osho, 2286, 19502010; Oui, 2017,
19592010; Ouza, 1849, 19522010; NHK-cup, 1745, 19512010; Ginga, 1735, 19912010; Kio, 1620,
19732010; Shinjino, 1332, 19692010; Zen-nihon-proshogi, 1160, 19822001; Hayazashi-shogi-senshuken,
945, 19722003; Judan, 764, 19621987; Meisho, 752, 19731989; Joryu-meijin, 608, 19742010; Meijin,
551, 19352010; All-star-kachinuki, 545, 19782003; Rating-senshuken, 476, 19872007; Asahi-open,
429, 20012007; Heisei-saikyo, 412, 19922007; Teno, 351, 19841992; Joryu-osho, 351, 19792010;
Kurashiki-touka, 314, 19932010; Nihon-series, 304, 19812010; 3-dan-league, 283, 19632009; Ladiesopen, 255, 19872007; Joryu-oui, 253, 19902010; Shoureikai, 217, 19412008; Gakusei-osho, 212, 1972
2006; Hayazashi-shinei, 206, 19822002; Gakusei-ouza, 191, 20012006; Asahi-amashogi, 187, 19802009;
Wakajishi, 183, 19531991; Kudan, 182, 19471961; Gakusei-meijin, 177, 19722006; Shogi-Renmei-cup,
172, 19671984; Tatsujin, 160, 19932010; Kinsho-cup, 156, 20022005; Amateur-meijin, 146, 19482009;
Kashima-cup, 119, 19962006; Grand-champion, 111, 19812008; Saikyosya-kettei, 101, 19541973; Miscellaneous, 5317, 16072010.

538

fiLarge-Scale Optimization Evaluation Functions Minimax Search

evaluation function
13
X



e
(p) fiA (p) wiA
i=1
X
B
B
B
eB
fkj
(p) fkj
(p) wkj
eC
eD

k,j
X

0 ,l
k,k
X

dimension
13
60, 876

C
C
C
fkk
0 l (p) fkk 0 l (p) wkk 0 l

2, 425, 950




fkjj 0 (p) fkjj
0 (p) wkjj 0

44, 222, 454

k,jj 0

Table 2: Dimensions evaluation functions. evaluation function linear combination weighted features. eA evaluates material balance, others
evaluate variety positional scores using extended piece-square tables.

expired, player move within sixty seconds). tournaments employed rapid
time controls 30 seconds per move top-level amateur players participants.
Table 2 shows four basic evaluation functions, eA material balance
others positional scores. experiments used sum functions, i.e.,
eA , eAB = eA + eB , eABC = eAB + eC , eABCD = eABC + eD . evaluation functions
anti-symmetric respect exchange black white: e(p, w ) = e(p, w ). Here,
p complete reversal black white sides position p; is, black plays
white white plays black3 . reversal, pieces owned black white
p regarded white black pieces p, respectively. Also, evaluation functions
symmetric respect right-and-left mirroring position: e(p, w ) = e(p, w ), p
mirror image p along file e.
function eA (p, w ) used evaluate material balance. 13 types
pieces shogi (Iida et al., 2002). feature fiA (p) represents number i-th
type owned black position p, wiA relative value i-th type piece.
partial derivative evaluation function respect wiA eA (p, w )/wiA =
fiA (p) fiA (p).
function eB (p, w B ) linear combination weighted two-piece-square features.
natural extensions one-piece-square features employed recent
machine learning studies chess evaluations (Baxter et al., 2000; Tesauro, 2001; Veness
et al., 2009). two-piece-square features used evaluate conditions
B (p) indicator function returns one
king another piece. feature fkj
conditions k j exist position p. Otherwise, returns zero. Condition
k represents location black king (there 81 squares), j represents
type, owner (black white), location piece. 1, 476 different
conditions j minor conditions merged. Thus, total number
kingpiece conditions 81 1, 476 = 119, 556 mirror symmetric conditions
merged.
3. Following shogi notation, black white refer players plays first second, respectively.

539

fiHoki & Kaneko

Similarly, functions eC (p, w C ) eD (p, w ) used evaluate kingking
C (p)
piece features kingpiecepiece features, respectively. indicator function fkk
0l
represents location two kings (k, k 0 ) condition (type location)
(p) represents location black king k
black piece l. indicator function fkjj
0
conditions two black white pieces (j, j 0 ).
Game tree searches required identify PV leaf positions MMTO obtain
best moves measure rate agreement. purposes, nominal depth 1
search used together quiescence search. normalize objective function
values, objective function values divided total number move pairs, Z P =
P
0
pP |Mp |. constraint function set


w )=
g(w

13
X

!
wiA

6, 500.

(9)

i=1

Also, accordance magnitude constraint, horizontally mirrored
sigmoid function (x) = 1/(1 + exp(ax)) set 0.0273 (x) would vary signifiw ) = 0.00625 (|w
wB| +
cantly x changed hundred. regularization term JR (w
C

w | + |w
w |). intuitive explanation penalty strength absolute value
|w
wi increased 160 improves relationship evaluation
values desired move another legal move. sums eB , eC , eD computed
using 32-bit integers, divided 25 order fit evaluation value
16-bit integer. Step h grid-adjacent update set smallest integer value 1.
4.2 Learning Piece Values
First, feature weights w = w evaluation function eA adjusted MMTO
comparison training, starting initial value wiA = 500 i. Tesauro
(2001) used floating-point feature weights conventional gradient descent method.
is, weight vector w updated
P
w ),
w (t) = w (t) rw Jct
(w

(10)

r constant training rate hand-tuned 0.5. components w used tree
search rounded nearest integer values. rescaling factor R Eq. (3) set
p .d
0.0025 accordance range difference | e(w p , w ) e(wp.m , w )| 50
5, 000. experiment 13 piece values adjusted w , 1, 000
game records used compose training set P. set 101,898 desired moves
Z P = 7, 936, 180 move pairs removing duplications handicapped games.
One problem observed comparison training slow learning: shown Figure 3, phase iterative procedure (from iteration 1 10) mainly adjusting
pawn value, partial differential value Eq. (3) pawns largest
phase. good pawn value found, phase II (from iteration 10 100)
mainly adjusting promoted rook promoted bishop values. values
highest second highest reasonable game play. long period time taken
w ) Eq. (3) scales poorly. general problem
phase II indicates JctP (w
gradient descent methods multiple degrees freedom (Nocedal & Wright, 2006),
540

fiLarge-Scale Optimization Evaluation Functions Minimax Search

pro_rook

2500

Phase II

Phase

Phase III
pro_bishop

Piece weight

2000
gold
bishop
rook
pro_pawn
pro_knight
silver
pro_silver
pro_lance
knight
lance

1500

1000

500

pawn
0

2

1

3

4 5 6

2

3

4 5 6

10

2

3

4 5 6

100

1000

Iteration

Figure 3: Results comparison training piece weights shogi. horizontal axis
plots number iterations logarithmic scale.

cope it, learning rate r cannot greater 0.5 accordance largest
partial derivative experiments.
second problem convergence: phase III (after iteration 100) Figure 3,
piece values keep increasing without changing ratio piece values, even though
relative ratios piece values room improvement. problem inherent
objective function comparison training, Eq. (3) explicit term
avoid it. extreme case training data satisfy inequality condition
p .d
e(w p , w ) e(wp.m , w ) moves position p, piece values diverge infinity
w ) minimized. fact, found training data
value JctP (w
experiment satisfied condition 94% pairs best another legal move.
Moreover, extreme case, training satisfy inequality
condition move position p Eq. (3), piece values shrink zero.
MMTO deals problems making grid-adjacent updates keeping
w ). weighted vector w converged
magnitudes constant constraint term JC (w
40 iterations (see Figure 4); value promoted rook 945
pawn 122. Note number iterations counted number step (1)s
throughout experiments.
4.3 Scalability Learning Practical Evaluation Functions
learning piece values, adjusted weight vectors positional scores.
time, large number training records used cope high-dimensional weight
vectors. main training set P 4, 467, 116 desired moves Z P = 368, 313, 024
541

fiHoki & Kaneko

pro_rook
pro_bishop
rook
bishop
gold
pro_knight
silver
pro_pawn
pro_lance
pro_silver
knight
lance
pawn

Piece weight

800

600

400

200
2

1

3

4

5 6 7 8

2

10
Iteration

3

4

5 6 7 8

100

Figure 4: Results MMTO piece weights.
move pairs removing duplications handicap games 47, 566 game records.
test set 103, 105 desired moves removing duplications handicap games
another 1, 000 game records. feature weights eAB adjusted MMTO
comparison training intuitive modifications. initial feature weights
w (0), w B (0)) used three methods; w (0) optimized MMTO
(w
previous experiment, w B (0) = 0. that, show scalability, feature weights
eABC eABCD optimized MMTO order. adjust feature weights
eABC , optimized feature weights eAB used initial feature weights w (0)
w B (0), 0 used w C (0). Similarly, eABCD , optimized feature weights
eABC used initial feature weights w (0), w B (0), w C (0), 0 used
w (0).
Comparison training eABC eABCD tested learning eAB yielded
small improvements. rate r Eq. (10) hand tuned 0.031. example
intuitive modifications stabilize iterative procedure, constant-step update
tested learning eAB . case, training rate r0 (t) substituted r

P
w (t))|,
r0 (t) = rc /|w (t) Jct
(w
constant-step modification conservatively updated w using constant step rc
hand tuned 1, 000. value r rc best five trials. Another
intuitive modification reuse PV, explained Section 3.5, PVs
used 32 times rate r Eq. (10) 0.01. rescaling factor R Eq. (3)
set 0.0025, value satisfactory previous experiment shown
Figure 3. Although three methods different, iterations consumed almost
amount time. time-consuming step experiments
game tree search identify PV leaf positions.
rate agreement test set shown Figure 5. Here, agreement means
legal move obtained highest value tree search desired move.
542

fiLarge-Scale Optimization Evaluation Functions Minimax Search

0

b2_c2_h6
b2_c2_b4

35

-20
d2_b2_f3
-40

b2_c2_b3
d2_f3_c4

-60

Positional weight

Agreement (%)

30

25

ABCD

MMTO (e
)
ABC
MMTO (e )
AB
MMTO (e )
AB
CT (e , reuse PV)
AB
CT (e , constant step)
AB
CT (e )

20

15

2

1

3

4

5 6 7 8

2

10

3

4

5 6 7 8

b2_c2_a3
d2_f3_b3

-80

b2_b1_c2
-100

b2_c2_a2

-120

-140
b2_d1_c2

2

100

Iteration

0

50

100
150
Iteration

200

Figure 5: (Left panel) Improvement rate agreement test set MMTO
comparison training (CT). (Right panel) Improvement feature weights
positional features eD . Feature weight b2 c2 b4 indicates black king
b2 two gold generals c2 b4. Similarly, feature weights b2 c2 h6,
b2 c2 b3, b2 c2 a3, b2 b1 c2, b2 c2 a2, b2 d1 c2 indicate two gold generals
king b2. Feature weight d2 b2 f3 indicates black king d2
opponents two gold generals b2 f3. Similarly, feature weights
d2 f3 b3 d2 f3 c4 indicate opponents two gold generals king
d2. Here, value divided 25 .

rate calculated excluding positions one legal move, positions
easy checkmate sequence identified shallow-depth
search. Tied values counted, either.
performances MMTO, comparison training, variations compared
case learning eAB . see figure agreement rates comparison
training constant-step modification unstable substantially lower
MMTO. see reuse-of-PV modification increases stability
reduces step length 0.031 0.01 reduces computation time learning
almost 32 times reduces number time-consuming PV updates.
MMTO full evaluation function eABCD highest rate (37%). largescale optimization weight vector wD increased level agreement 200 iterations.
543

fiHoki & Kaneko

without constraint
constraint

Pawn value

140
120
100
2

1

3 4 5 6

2

10
Iteration

3 4 5 6

2

100

Figure 6: Effect constraint term MMTO (eAB ).
computation took week using Intel X5690 workstation. agreement ratio
test set converged 100 iterations. However, feature weights converge.
w ) Eq. (9) improves stability MMTO
Figure 6 shows constraint JC (w
response pawn-value changes eAB learning. see value keeps
w ) turned converges 100 iterations
increasing JC (w
constraint turned on. One feature weights overflowed comparison training
eABC , another reason results eABC shown comparison
w ) little effect learning eAB ,
training. regularization term JR (w
improvement agreement rates MMTO mainly due use constraint
w ) grid-adjacent updates.
JC (w
w ) important optimizing larger weight vectors. FigThe regularization term JR (w
w ) Eq. (4) improves weight vector enlarged evaluation
ure 7 shows JR (w
function eABCD . Without regularization term, objective function value rate
agreement training set increase number iterations. However,
linear increment absolute value weight vectors, distorts
rate agreement test set 50-th iteration. 200-th iteration,
0.2% components w zero. hand, 96.3% components
w zero regularization term. results indicate MMTO without
regularization suffers overfitting training set large-scale weight vector
used. similar effect regularization occurs MMTO used eABC
learning, though effect smaller eABCD .
4.4 Improvements Strength
analyze relationship agreement rate strength, programs learned MMTO comparison training (Figure 5) play games
reference shogi program many times. reference program version GPS Shogi
released 2008 (Kaneko, 2009). open source program finalist past
world computer shogi championships. completely different evaluation function
majority parameters hand tuned. version GPS Shogi
serves reference program popular game server shogi programs4 . matches
follows: reference program (4 105 nodes/move) vs. four learned programs,
4. http://wdoor.c.u-tokyo.ac.jp/shogi/, last access: 2013 (in Japanese).

544

fiObjective function

Large-Scale Optimization Evaluation Functions Minimax Search

0.06
0.05
0.04
0.03
0.02

Agreement (%)

60

l1-regularization
without l1-regularization

55
50

training set

45

test set

40



10

B

| |+| |+| |

10

C

35
11
10

10

9

2

1

3

4 5 6

2

10

3

4 5 6

2

100

Iteration

Figure 7: Effect regularization term MMTO (eABCD ).
reference program (2105 nodes/move) vs. two learned programs evaluation
function eA eAB , reference program (8 105 nodes/move) vs. two learned
programs evaluation functions eABC eABCD . 500 games played
match. weight vectors obtained 1, 2, 4, 8, 16, 32, 48, 64, 80, 96, 112, 128, 144
iterations tested learning configuration. Thus, total 8 13 500 = 52, 000
games played. learned programs searched 4 105 nodes per move. programs
ran single thread searched similar numbers nodes second.
measured playing strength terms Elo rating, popular way
represent relative strength two-player games. winning probability two
players estimated 1/(1 + 10d/400 ), difference ratings.
example, rating player higher player B 150, winning
percentage player 70%. Here, ratings determined using maximum
likelihood estimation games.
Figure 8 shows Elo rating player. see MMTO eAB significantly outperformed comparison training initial feature weights.
MMTO used eAB , winning percentage reference (400k/move) stably increased 11.2% (1, 800 Elo points) 59.4% (2, 210 Elo points). contrast, comparison
545

fiElo Rating

Hoki & Kaneko

2500
2400
2300
2200
2100
2000
1900
1800
1700
1600
1500

MMTO (ABCD)
MMTO (ABC)
MMTO (AB)
Comparison training (AB)
Reference (800k)
Reference (400k)
Reference (200k)

1

10
Iteration

100

Figure 8: Improvements strength (Elo rating) achieved MMTO comparison training.

Opponent
Player 1
Player 2

Depth 2
94 2%
77 3%

Depth 3
89 3%
75 3%

Depth 4
82 3%
77 3%

Depth 5
85 3%
82 3%

Depth 6
78 3%
81 3%

Depth 7
81 3%
85 3%

Depth 8
78 3%
84 3%

Table 3: Winning percentages program learned game tree search various
depths. Opponent player 1 program search depth reduced
1, opponent player 2 program uses weight vector
learning.

training 19.5% (1, 910 Elo points) games. results shown Figs. 5
8 indicate MMTO outperforms comparison training.
large number features contributed playing strength programs
learned MMTO. Although eABC showed small improvement terms agreement
rate Elo rating, eABCD consistently yielded significant improvements two criteria. Thus, concluded MMTO scales well forty million features. Note
computational cost eABCD reasonably small practical game play.
number features appear position 2, 800 less even total
number features forty million. Also, summations Table 2 maintained incremental manner program makes unmakes move. sort
feature design similar famous Othello program (Buro, 2002). result,
Bonanza using eABCD searched 3 106 nodes/sec Intel Xeon X5680 12
threads. speed slower many chess programs, average
strong shogi programs. addition, found Bonanza using eABCD trained
MMTO played better comparable top shogi programs actual
tournaments. details discussed Section 4.6.
Two additional fixed-depth self-play experiments conducted see evaluation
functions trained using shallow searches (depth 1 quiescence search) effective
deep searches. Table 3 shows winning percentages learned program various
546

fiLarge-Scale Optimization Evaluation Functions Minimax Search

search depths game play. learned program eABCD evaluation function yielded
200-th iteration (Figure 5). winning percentages program
(player 1) search depth reduced 1 around 80%. Thus, see
deeper learned program searched, stronger program was. Tesauro (2001)
reported similar results using comparison training. addition, winning percentage
80% program (player 2) searched depth used eABC
200-th iteration. Thus, use eABCD trained 200 iterations effective
even program searched deeper. Here, winning percentages computed
thousand games (Seventy-six games less ending draws exceeding 300 moves
counted). Fifty megabytes memory assigned transposition table
program. uncertainties indicated 3 estimated conducting two-sided test
significance level 5% one-thousand games.
4.5 Numerical Stability Convergence
investigated continuity partial differentiability objective function
convergence feature weights empirical manner. forward pruning techniques game tree searches speed MMTO practical applications, methods
always maintain continuous search values, shown Appendix A.4. Moreover,
objective function contains large number search values. means difficult
estimate properties theoretical manner.
make empirical investigation manageable, used smallest evaluation
function eA deals thirteen shogi piece values. Moreover, reduced number
game records 1, 000; game records 98, 224 desired moves Z P = 7, 900, 993
move pairs removing duplications handicapped games.
4.5.1 Surface Objective Function
investigated function surface main part objective function J(P, w )
MMTO Eq. (4) generating contour maps millions sampling vectors w .
Note contour line (isovalue surface) curve along functions take
value. contour lines certain properties: gradient function
perpendicular lines, magnitude gradient large two lines
close together. addition, closed-loop contour line indicates location local
minimum maximum.
Two thirteen piece values weight vector w sampled order draw
contour maps two-dimensional functions interval 5 piece value.
remaining eleven pieces assigned reasonable values; 118 (pawn), 273 (lance), 318
(knight), 477 (silver general), 562 (gold general), 620 (bishop), 734 (rook), 485 (promoted
pawn), 387 (promoted lance), 445 (promoted knight), 343 (promoted silver), 781 (promoted
w ) ignored w
bishop), 957 (promoted rook). constraint term JC (w
w ) turned piece values.
could freely changed regularization term JR (w
nominal depth 1 search, together quiescence search, used.
analyzed two pairs: hgold, bishopi, hpawn, promoted lancei. Figure 9 shows
enlargement contour map J(P, wgold , wbishop ). contour interval 5 104 .
map computed ranges [200, 1100] bishop [100, 930]
547

fiHoki & Kaneko

(3) bishop=pro_bishop

(4) bishop=dragon
(5) gold=bishop

(2) bishop=rook

(1) bishop=silver

Gold general weight

(6) gold=pro_bishop

(8) gold=rook
700
600

x x

500
400
400

Objective function

(7) gold=pro_rook

800

0.164

(9) gold=silver
500

(1) (5)

600 700 800
Bishop weight

(2)

(9)

0.162
0.160

900

(5) (6)
(8)

(7)

(3) (4)

0.158
400 500 600 700 800
Bishop weight

500 600 700 800 900
Gold general weight

Figure 9: (Upper panel) Enlarged contour map J(P, wgold , wbishop ). dashed lines
indicate critical boundaries two-dimensional function
partially differentiable. two minima indicated x. (Bottom panel)
Cross sections contour map. left one shows intersection
map line wgold = 560, shows wbishop = 620.

gold general. Note function simply increases interesting
structures outside enlarged map. Figure 10 shows enlargement contour
map J(P, wpawn , wpro lance ). contour interval 1 103 . map computed
ranges [10, 500] pawn [200, 700] promoted lance.
see maps local minima within reasonable ranges
sudden changes function values. Although function depends large
number empirical search values, s(p, w ), approximately continuous amenable
optimization basis gradients approximated MMTO.
hand, maps illustrate three difficulties. first difficulty clear
edges contour lines. indicate function partially differentiable
points edges. dashed lines maps critical boundaries
profit loss ratio material exchanges inverts itself. example, silver usually
548

fiLarge-Scale Optimization Evaluation Functions Minimax Search

(1) pro_lance=lance (2) pro_lance=knight (3) pro_lance=silver
(4) pawn=silver

Pawn weight

400
(5) pawn=knight
300

(6) pawn=lance
(7) lance promotion=pawn

200
X

100

200

300

400

X

500

600

700

Objective function

Promoted lance weight
0.1584
0.1583
(2) (7) (3)
0.1582
0.1581 (1)
0.1580
0.1579
200 300 400 500 600 700
Promoted lance weight

0.20
0.19
0.18
0.17
0.16

(7) (6) (5)
(4)

100 200 300 400 500
Pawn weight

Figure 10: (Upper panel) Enlarged contour map J(P, wpawn , wpro lance ). dashed
lines indicate critical boundaries two-dimensional function
partially differentiable. two minima indicated x. (Bottom panel)
Cross sections contour map. left one shows intersection
map line wpawn = 125, shows wpro lance = 450.

less valuable bishop, capturing silver becomes profitable capturing
bishop bishop value smaller 477. boundary labeled bishop=silver
Figure 9. discussed Appendix A, function always partially differentiable
critical boundaries, multiple moves share best value. Note
boundaries theory, e.g., bishop=promoted knight boundary. Whether
boundary visible depends training set evaluation features. addition,
boundaries become winding curves non-linear evaluation function used instead
linear weighted sum.
second difficulty, scaling problem, illustrated Figure 10. map,
see scales two piece values differ two orders magnitude.
is, pawn-value variation five hundred changes function value 0.04, whereas
promoted-lance-value variation five hundred changes function value 4 104 .
difference scaling, surface along promoted lance almost flat.
property explains pawn value optimized earlier pieces
comparison training, shown Figure 3. property ill-scaling disadvantageous
comes optimizing promoted-lance value using naive gradient decent method.
549

fiHoki & Kaneko

Methods based second-order partial derivatives approximations Hessian matrix
resolve problem; however, behave poorly non-partially differentiable points
many boundaries. two difficulties point grid-adjacent update MMTO
effective.
third difficulty multiple local minima two maps.
means results MMTO depend initial values chance ending
local rather global minimum. investigate problem next
subsection 4.5.2.
4.5.2 Empirical Convergence Local-Minima Properties
previous subsection, examined two-dimensional cross sections function
J(P, w ). subsection, loosen restriction two thirteen dimensions,
sufficiently large express piece values shogi. aim experiment
catch glimpse global map numerical convergences arbitrary initial
guesses values pieces.
purpose, Monte Carlo sampling initial guess, w (0), carried
enumerate local minima analyze optimized vectors. ran 444 MMTO
randomized initial values. Here, uniformly distributed integer range [0, 32767]
assigned vector component, resulting vector scaled satisfy
w ) = 0.
equality condition g(w
Figure 11 shows cosine similarity objective-function value hundred 444
runs. Here, cosine similarity weight vector measured relative best vector
whose objective function smallest among 444 vectors 100 iterations.
majority runs, see function values weight vectors converged
numerically 50 iterations. Here, regard iteration procedure converged
function values similarities oscillate show neither steady increase
decrease 50-th 100-th iteration. Although convergence almost assured
MMTO thirteen piece values, would difficult achieve feature weights
optimized. example, Figure 5 shows convergence twothousand iterations using eABCD . 200 iterations took week Intel
X5690 workstation, could afford investigate convergence eABCD
current hardware. However, 200 iterations nonetheless achieved significant improvement
strength, shown Figure 8.
see trials MMTO ended multiple local minima.
Although multiplicity minima generally undesirable optimization,
other, favorable properties. first property run MMTO changed
weight-vector components sufficient amount. is, cosine similarity
444 optimized vectors localized range [0.925, 1], random
initial vectors widely spread (see top panel Figure 12). second property
weak correlation cosine similarities initial optimized
vectors. means starting better initial vector terms cosine similarity
beneficial (see top panel Figure 12). However, starting better initial
vector terms objective function value beneficial (see middle panel
Figure 12). third distribution local minima formed structures (see
550

fiLarge-Scale Optimization Evaluation Functions Minimax Search

0.95
0.90
0.85
0.80
0.98

Similarity

Cosine similarity weight vector

1.00

0.75
0.70

0.97
0.96
0.95

Objective function

0.65

Objective function

0.30

0.170
0.165

0.25
0.160
60 80
Iteration

0.20

2

1

3

4

5 6 7 89

10
Iteration

2

3

4

5 6 7 89

100

Figure 11: hundred runs MMTO weight vector w consisting thirteen piece
values. initial vectors set using pseudo-random numbers. inset
enlargement showing appearance numerical convergences.
top panel shows cosine similarities relative best weight vector.
bottom panel shows values objective function.

bottom panel Figure 12). is, lower local minimum is, similar
becomes best vector. Moreover, number local minima decreases weight
vector gets farther away best.
investigated dependence performance nominal search depth
step (1) shown Figure 2. Similar results terms convergence distribution
local minima obtained using deeper search nominal depth 2.
MMTO depth 2 consumes time MMTO depth 1, number
551

fiInitial objective function

Cosine similarity initial vector

Hoki & Kaneko

1.0
0.9
0.8
0.7
0.6
0.35

corr = 0.27

0.30

0.25

0.20
corr = -0.06

Optimized objective function

0.180
0.175
0.170
0.165
0.160
corr = -0.55
0.94

0.96

0.98

1.00

Cosine similarity optimized vector

Figure 12: Scatter plots 444 trials thirteen-dimensional weight vectors. vector
expresses thirteen piece values. cosine similarity vector measured
relative best vector. initial vector consists uniform pseudo-random
numbers, optimized one 100-th vector MMTO iterations
starting initial one. inset shows correlation coefficient
scatter plot.

random initial vectors reduced 78, number iterations reduced
sixty sake speed. majority runs, function values weight
vectors converged 50 iterations. Figure 13 shows strength (Elo rating) objective552

fiLarge-Scale Optimization Evaluation Functions Minimax Search

100
depth 1 (corr = -0.60)
depth 2 (corr = -0.86)

Elo rating

50
0
-50
-100
-150
0.150

0.155

0.160

0.165

0.170

0.175

0.180

Objective function

Figure 13: Scatter plots thirteen-dimensional weight vectors. 444 vectors indicated
crosses learned nominal depth 1 search step (1), 78
vectors indicated squares learned depth 2 search.

function value 78 runs depth 2 (squares) 444 runs depth 1 (crosses).
Here, Elo ratings identified using maximum likelihood estimation 894, 244
random-pairing games (5 104 nodes/move). Elo rating depth 1 17
average depth 2 41 average. Also, correlation coefficient
Elo rating objective function value depth 2 0.86 depth
1 0.60. Moreover, compared performance two best vectors gave
smallest objective function values. Here, computed winning probability
best results depth 1 2. player allowed use one second move,
one core Intel Xeon X5680 fifty megabytes memory assigned
transposition table. excluding two drawn games two games exceeding thousand
moves, obtained 43.6% winning rate program using best results
depth 2. results indicate MMTO better depth 2 depth 1.
4.6 Performance MMTO Tournament Conditions
MMTO invented developer Bonanza made one best programs
shogi. Moreover, ideas behind earlier versions MMTO published Japanese
(Hoki, 2006) adopted many developers dramatically changed shogi
programs.
One authors started developing Bonanza 2004, published program files
web 2005, published source codes web 2009 (Hoki, 2013). paper
gives detailed descriptions evaluation-function learning, whereas literature (Hoki
& Muramatsu, 2012) gives detailed descriptions game-tree pruning Bonanza.
addition learning method MMTO, Bonanza uses evaluation function eABCD
shown Table 2. earlier versions 2009 used subset eABCD modified
553

fiHoki & Kaneko

1
2
3
4
5

2006 May
Bonanza
YSS
KCC Shogi
TACOS
Gekisashi

2007 May
YSS
Tanase Shogi
Gekisashi
Bonanza
Bingo Shogi

2008 May
Gekisashi
Tanase Shogi
Bonanza
YSS
Bingo Shogi

2009 May
GPS Shogi
Otsuki Shogi
Monju
KCC Shogi
Bonanza

1
2
3
4
5

2010 May
Gekisashi
Shueso
GPS Shogi
Bonkras
Bonanza Feliz

2011 May
Bonkras
Bonanza
Shueso
Gekisashi
ponanza

2012 May
GPS Shogi
Puella
Tsutsukana
ponanza
Shueso

2013 May
Bonanza
ponanza
GPS Shogi
Gekisashi
NineDayFever

Table 4: Program names results recent World Computer Shogi Championship.
MMTO, earlier version variant MMTO, learning method
influenced MMTO used.

l2-regularization (Hoki, 2006). Subsequent versions fully evaluate eABCD learned l1regularization.
Table 4 shows results World Computer Shogi Championships. Since 2006,
performance Bonanza examined several computer shogi tournaments,
participant connects server program plays shogi time control
25 minutes side. Bonanza received first prize twice, second prize once, third
prize once. Moreover, players entitled Bonanza Feliz Monju used evaluation functions obtained MMTO. Thus, claim Bonanza uses MMTO,
plays better comparable top programs shogi, including commercial ones. method clearly plays level handcrafted shogi programs. Moreover,
descriptions learning shogi evaluation functions earlier version MMTO
published Hoki (2006) Japanese quickly recognized significant advances. fact, shogi program conventional handcrafted evaluation functions
broken top five last five years tournaments. One interesting case
results GPS Shogi (Kaneko, 2009), winner 2009 2012 tournaments,
source codes available online (Tanaka Kaneko, 2013). 2003 2008,
program uses handcrafted evaluation function 2009 used variant MMTO
results dramatically improved. variants MMTO used program differ
accordance content policy program. example, Tanase Shogi,
runner-up program 2008, used learning method based MMTO handcrafted
evaluation functions. Bonkras, ponanza, Puella , NineDayFever used variants MMTO. excellent results make clear MMTO outperforms conventional
programs use handcrafted evaluation functions played extremely well recent
shogi tournaments.
554

fiLarge-Scale Optimization Evaluation Functions Minimax Search

noted versions Bonanza add small amount randomness
grid-adjacent updates. However, omitted discussion using randomness
paper clear whether added randomness improved quality
evaluation function not. source codes various versions Bonanza available
online (Hoki, 2013) source code MMTO two files, learn1.c learn2.c.
4.7 Preliminary Experiments Chess
far, discussed performance MMTO shogi. expect MMTO
would effective two-player perfect information games provided certain
conditions met: (1) sufficient number game records available, (2) minimax
searches guided heuristic evaluations effective, (3) analytic partial derivatives
evaluation function respect variables available. example, MMTO
would yield interesting results applied game solved
means (e.g., van den Herik, Uiterwijk, & van Rijswijck, 2002). Also, would yield
interesting results game Go Monte-Carlo tree searches effective
minimax searches guided heuristic evaluation function (Kocsis & Szepesvari, 2006;
Gelly & Silver, 2011; Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener,
Perez, Samothrakis, & Colton, 2012; Gelly, Kocsis, Schoenauer Sebag, Silver, Szepesvari, &
Teytaud, 2012). Moreover, simpler learning method (e.g., regression method Othello,
Buro, 2002) would preferable MMTO, sufficiently effective.
conducted preliminary experiments chess catch glimpse applicability
MMTO games. Note already evaluation functions chess
outplay grandmasters, whereas none shogi. Thus, might difficult
improve well-crafted chess evaluation functions. experiment, chose opensource program (Crafty) fair implementation chess program (Hyatt, 2013).
original evaluation function tightly tuned simple multivariable
function. Thus, sake simplicity, modify way except add
new linear combination weighted two-pieces-square features. features used
evaluate conditions king another piece, eB Section 4.1.
mirror symmetric property described Section 4.1 applied features
pawn exists eighth rank counted. results, total number added
weights w B 39, 312. chess position possesses thirty fewer two-piecessquare features, additional computational time due modification became
almost negligible help pawn hash table lazy evaluation technique
come original.
training test sets composed using game records Free Internet
Chess Server (FICS). games played using standard time control server
two players ratings 2, 600 more. training set P 1, 267, 032 desired
moves Z P = 33, 619, 904 move pairs removing duplications 13, 440 game
records, whereas test set P 101, 982 desired moves Z P = 2, 755, 217 move pairs
removing duplications 1, 000 game records.
Figure 14 shows rate agreement test set number correct answers
chess problems iteration. Here, sigmoid function set 0.00341,
w B ) = 0.156|w
w B |.
equality constraint used, regularization term JR (w
555

fiHoki & Kaneko

Agreement (%)

35.2

1270

34.8

1260
1250

Agreement
Number correct answers

34.4
2

3

4

5

6

7 8 9

1

2

3

4

5

10

6

1240

7 8 9

100

Number correct answers

1280

Iteration

Figure 14: Improvement rate agreement test set (solid line) number
correct answers 2, 180 problems (dashed line) chess. two-piece-square
weights w B adjusted using MMTO.

Rating
Win

10101279
33 3%

12801489
35 3%

14901769
39 4%

17702049
43 4%

2050
42 4%

Table 5: Dependence strength (winning percentages) learned programs
quality (ratings players) training set. uncertainty indicated 3
estimated conducting two-sided test significance level 5% 1, 000
games.

total 2, 180 chess problems Encyclopedia Chess Middlegames (the second
section 879 problems), Win Chess (300 problems), Winning Chess Sacrifices
(1, 001 problems) used (Krogius, Livsic, Parma, & Taimanov, 1980; Reinfeld, 2001,
1969). learned program searched 5 104 nodes per problem eight megabytes
memory assigned transposition table. see agreement rate well
number correct answers tends improve number iterations grows, though
differences moderate. means MMTO found room improvement
well-implemented chess program. results indicate MMTO useful way
learn heuristic evaluation functions chess, especially one design evaluation
features suitable learning.
4.8 Data Quality Dependence
assess importance quality game records, conducted additional experiments using game records players various levels experience shogi. Here,
eABCD learned using results eABC Figure 5 initial value. results
summarized Table 5. training set composed records 47, 566
rapid time control (30 seconds per move) games played amateurs popular Internet
shogi site, Shogi Club 245 . first line table shows ratings amateur
players. second line shows winning percentages learned evaluation function
5. Shogi Club 24, http://www.shogidojo.com, last access: 2013.

556

fiLarge-Scale Optimization Evaluation Functions Minimax Search

evaluation function trained grandmaster-game records. Here, evaluation function learned 200 iterations. winning percentages computed
averaging results thousand games (About 15 drawn games games exceeding
300 moves counted). player allowed use one second one core
Intel Xeon X5680 move, fifty megabytes memory assigned
transposition table. Table 5 shows significance quality training set; use
game records stronger players made program stronger.

5. Conclusion
presented method, Minimax Tree Optimization (MMTO), uses game records
adjust full set feature weights evaluation function two-player game.
learning MMTO designed search results match desired moves,
e.g., recorded moves grandmaster games. MMTO consists two procedures: (1)
shallow heuristic search training positions using current feature weights
(2) update guided approximation gradient objective function.
new combination simple smooth approximation step function grid-adjacent
updates standard techniques, i.e., gradient guided optimization, constraints, regularization, contributed scalability stability MMTO led showing
substantial improvements existing methods.
performance MMTO demonstrated experiments shogi, variant chess
larger number legal moves. MMTO clearly outperformed existing methods.
addition, experimental results rate agreement playing strength indicate
MMTO adjust forty million parameters. Possible future work would automated
adjustment step length theoretical convergence analysis.

Acknowledgments
grateful Dr. Masakazu Muramatsu support work.

Appendix A. Notes Continuity Partial Differentiability
Minimax Value
saw Section 4.5 objective function MMTO piecewise smooth surface.
Appendix, theoretically discuss continuity partial differentiability
w ) respect w RN , w vector parameters
minimax value vp (w
evaluation function e(p, w ) p position. continuity minimax value
ensures continuity main part objective function MMTO defined Eq. (5).
partial differentiability analysis gives conditions approximation inside
MMTO described Section 3.3 valid. first analyze single minimax tree, assuming
tree known fixed. Then, extend discussion game-tree-search
methods possibly explore different trees different w .
Definition 1. evaluation function e(, ) (P, RN ) 7 R function, P set
positions target game, R set real numbers, RN N -dimensional
557

fiHoki & Kaneko

Euclidean space. evaluation function e(p, w ) continuous respect parameters w position p P w RN . Moreover, evaluation function
e(p, w ) partially differentiable respect component w w RN .
continuity partial differentiability evaluation function feasible assumptions. Note evaluation based ordinary piece-square table
properties, recent machine learning evaluation functions (Baxter et al.,
2000; Veness et al., 2009; Buro, 2002).
Definition 2. theoretical game graph G finite, directed acyclic, connected graph
representing possible transitions states target game, node (resp. edge)
represents position (resp. move). set nodes G corresponds P; V (G) = P.
minimax graph finite connected sub-graph G. convention, use term
minimax tree minimax graph even tree. denote set minimax
trees G T. node called maximizing (resp. minimizing) node corresponding
position maximizing (resp. minimizing) player move. destination edge
maximizing (resp. minimizing) node source edge minimizing
(resp. maximizing) node. clearly assume node n single position p,
denote evaluation function e(n, w ).
Let Lr,T set leaf nodes entire sub-tree Tr Tr rooted node r.
omit tree use Lr obvious. denote set immediate successors
(or children) node n tree Cn,T Cn . Note Cn = n leaf.
standard notation, node (or vertex) graph denoted n V (T ). However,
Appendix, omit V (.) write n obvious.
w ) value associated node n minimax
Definition 3. minimax value vn,T (w
tree defined recursively tree structure static evaluation function
e(n, w ), follows:

n leaf,
e(n, w )
w ) n non-leaf maximizing node,
w) =
maxcCn,T vc (w
vn,T (w
(11)

w
mincCn,T vc (w ) n non-leaf minimizing node.
w ) obvious. two minimax values b
omit tree use vn (w
maximizing (resp. minimizing) node, say better b > b (resp. b < a).
A.1 Continuity Minimax Value
continuity minimax value follows continuity evaluation function.
w ) continuous respect w minimax
Theorem 4. minimax value vn,T (w
w ) = vn,T (w
w 0 ), equivalently,
tree w RN . is, limw w
w 0 vn,T (w
w w 0 | < logically implies
w 0 RN > 0, exists > 0 |w
0
w ) vn,T (w
w )| < .
|vn,T (w
following assertion ordinary properties basic functions max
min common sense analysis. rather difficult, however, find suitable
reference containing it. therefore give proof useful subsequent
discussion.
558

fiLarge-Scale Optimization Evaluation Functions Minimax Search

x), ..., fk (x
x) continuous function
Proposition 5. Let k natural number f1 (x
x)) continuous function RN . Similarly, mini (fi (x
x))
RN 7 R. Then, maxi (fi (x
N
continuous function R .
x) continuous, x 0 RN > 0, exists
Proof. (x
x x0 | < implies |fi (x
x) (x
x0 )| < . Hence, choose = mini ,
> 0 |x
0
0
x x | < implies |fi (x
x) (x
x )| < = 1, . . . , k; is,
|x
x0 ) < (x
x) < (x
x0 ) + ,
(x

= 1, . . . , k.

Note ai < bi = 1, . . . , k obviously implies maxi ai < maxi bi . Thus,
inequalities obtain
x0 ) < max (x
x) < max (x
x0 ) + ,
max (x






is,

x) max (x
x0 )| < .
| max (x




x). proof similar mini (x
x).
implies continuity maxi (x
Let r root given tree . Now, prove Theorem 4 basis
mathematical induction leaf nodes Lr,T root r. is, leaf node
n Lr,T , minimax value continuous continuity evaluation
w ) = e(n, w ). internal node n, assume continuity holds
function; vn,T (w
child c Cn,T . induction hypothesis Proposition 5 ensure continuity
w ).
vn,T (w
A.2 Stability Principal Variations
subsection, showed continuity minimax values continuity
min max functions. Here, show best moves principal variations
stable changes leaves small enough. analyze stability order
discuss partial differentiability.
+
w ), hereafter called best children, denotes set
(w
Definition 6. symbol Cn,T
children node n tree minimax value n:
+
w ) = {c Cn,T |vc (w
w ) = vn (w
w )}.
(w
Cn,T
+

w ). Here, \ B denotes
w ); is, Cn,T \ Cn,T
(w
(w
denote rest children Cn,T
set difference, i.e., {e|e e
/ B}.

child considered best choice parent node minimax value
child parent node. two children share value,
w ) contains one child. Otherwise, number nodes Cn+ (w
w ) greater
Cn+ (w
one.
Definition 7. Let r root tree T. principal variation (abbreviated PV
w ) tree sub-tree obtained closure best children
short) (w
root:
w ) = {r},
0 (w
+

w ) = {c Cn,T
w ) | n i1 (w
w )} > 0,
(w
(w
w) =
(w


[

w ).
(w

i=0

559

fiHoki & Kaneko

n0 2

}
!
n1 2 n2 2 n3-1

!

n4 7 n5 2 n6-1

Figure 15: Example minimax tree (graph) transposition n5
+
+

w ) = Cn,T
w ) Cn,T
w ) = n (w
w ). Also, denote leaves
Note Cn,T
(w
(w
(w



w ) L (w
w ), is, (w
w ) Lr,T .
(w

Example 8. Figure 15 shows small minimax tree two best children root n0 ;
maximizing minimizing nodes denoted boxes circles, respectively. Here,
Cn+0 = {n1 , n2 } Cn+1 = {n5 }. principal variation tree {n0 , n1 , n2 , n5 }.
Lemma 9. internal node n tree w 0 RN , exists
w 1 w 0 | < n , set best
positive number n w 1 satisfying |w
1
0
children node n w subset one w :
+
+
w 0 ), w 1 s.t. |w
w 1 w 0 | < n .
w 1 ) Cn,T
(w
(w
Cn,T

w 0 ) empty, assertion trivial.
Proof. child values same, i.e., Cn (w
Otherwise, let 0 minimum absolute difference best value
w 0 ) vc (w
w 0 )| > 0. continuity minimax
values, i.e., 0 = mincCn (w
w 0 ) |vn (w
w 1 w 0 | < n ,
values ensures existence n w 1 satisfying |w
1
0
1
0
w ) vc (w
w )| < 0 /2 |vn (w
w ) vn (w
w )| < 0 /2. definition
maxcCn |vc (w

0
w ) satisfies
n triangle inequalities, c Cn,T (w
w 0 ) vn (w
w 0 )|
0 |vc (w
w 0 ) vc (w
w 1 )| + |vc (w
w 1 ) vn (w
w 0 )|
|vc (w
w 0 ) vc (w
w 1 )| + |vc (w
w 1 ) vn (w
w 1 )| + |vn (w
w 1 ) vn (w
w 0 )|
|vc (w


w 1 ) vn (w
w 1 )| + 0 + 0
< |vc (w
2
2
1
1
w ) vn (w
w )| + 0 .
= |vc (w
w 1 ) vn (w
w 1 )| > 0 0 = 0, namely, vc (w
w 1 ) 6= vn (w
w 1 ). implies definition
Thus, |vc (w
+
w 1 ).
(irrespective whether n max min node) c 6 Cn,T
(w
Definition 10. tree stability tree minimum value n among
nodes n , n positive number satisfying Lemma 9. Note minimum
value > 0 exists finite.
Example 11. reference Figure 15, suppose leaf value changes 0.1.
w 1 ) vn (w
w 0 )| 0.1 internal node n heights 1, 2,
Then, proven |vn (w
3 order: obvious n4 , n5 , n6 , proven n1 , n2 n3 ,
finally n0 . see neither n4 n6 become new best node result
change.
560

fiLarge-Scale Optimization Evaluation Functions Minimax Search


p

vn


u
0





w1)
vn (w

w0)
(w
j

w1)
vc (w
(c Cn+ )
/ h
w 1 ) (c
vc (w
/ Cn+ )
q



w 1 ) maximizing-node n, w 1 changes along i-th comFigure 16: Sketch vn (w
0
w 1 ) n equals vc (w
w 1 ) one old best
ponent wi + h w 0 . Here, vn (w
+
0
w ).
children c Cn (w

A.3 Partial Differentiability
show partial differentiability, well partial derivative, minimax
value node tree depends principal variations. denote right
left partial derivatives function RN 7 R point x 0
f 0
x ) =
(x
x+


f (x01 , . . . , x0i + h, . . . , x0N ) f (x01 , . . . , x0N )
,
h+0
h

(12)

f 0
x ) =
(x
x


f (x01 , . . . , x0i + h, . . . , x0N ) f (x01 , . . . , x0N )
.
h0
h

(13)

lim
lim

Let us pay attention single parameter xi changes h limit opf
0
erations. Hereafter, parameters held constant often omitted x
+ (x ),
x one-dimensional parameter interest. use symbol
analogy partial derivative order forget parameters
omitted.
w ) tree
Theorem 12. node n principal variation (w
w ) partial derivative evaluation
w RN , exists leaf la L (w

w ): vn+ (w
w ) = w
function equals right partial derivative vn (w
e(la , w ). Similarly,

w


w ) partial derivative evaluation function equals
exists leaf lb L (w

w ), vn (w
w ) = w
left partial derivative vn (w
e(lb , w ).

w


proof theorem, given end subsection, based stability
0 ) |w
w 1 w 0 | = |h|
best moves. assume w 1 = (w10 , . . . , wi0 + h, . . . , wN
sufficiently small Appendix A.3. Consequently, |h| < , node n
tree w 0 RN ,

1
(n:leaf)

e(n, w )
1 ) (n:maximizing node)
1
w
max
v
(w
+
0
c
w )=
w )
vn (w
cCn,T (w

min +
w 1 ) (n:minimizing node).
w 0 ) vc (w
cC
(w
n,T

561

(14)

fiHoki & Kaneko

w 1 ) changing h, n maxExample 13. Figure 16 sketches example vn (w
w 0 ) h = 0. value
imizing node. three best children value vn (w
continuously (not always linearly) changes h. best child depends sign
w 0 ) h less . minimax
h, always one c Cn+ (w
w 0 ) sufficiently less (by least 0 ) vn (w
w 0 )
values children c Cn (w
h = 0.
w ) given
next goal show right left partial derivatives vn (w
w ), respectively.
right left partial derivatives one best children Cn+ (w
following propositions describe ordinary properties right left limits
basic functions max min. Similar arguments found comprehensive textbook
calculus. give detailed proof here, however, rather difficult find
precisely assertion textbook.
Proposition 14. Let k natural number f1 (x), ..., fk (x) continuous
function R 7 R. Suppose functions value point x0 , i.e.,

0
maxi (x0 ) = mini (x0 ), right partial derivative x
+ (x ). Then,
0
right partial derivative minimum maximum (x) point x exists
equal minimum maximum right partial derivatives (x0 ), respectively.
maxi 0

mini 0

(x ) = max + (x0 ),
(x ) = min + (x0 ).
+
+
x
x
x
x
Proof. Let o(h) Landaus symbol, let us use denote residual terms converging
0
0
0 faster h, i.e., limh+0 o(h)
h = 0. Recall (x ) = f1 (x ) = 1, . . . , k.
positive h,


0
0
max (x + h) max (x ) = max (x ) + h + (x ) + o(h) max (x0 )




x



= max f1 (x0 ) + h + (x0 ) + o(h) f1 (x0 )

x


0
= h max + (x ) + o(h)
x
0

0

Eq. (12), function maxi (x) point x0 right partial derivative maxi
argument applies right partial derivative mini (x).


(x0 ).
x+

Proposition 15. Suppose functions value point x0

0
functions left derivative x
(x ). Then, left partial derivative minimum
maximum (x) point x0 equal maximum minimum left partial
derivatives (x0 ):
maxi 0

mini 0

(x ) = min (x0 ),
(x ) = max (x0 ).


x
x
x
x
562

fiLarge-Scale Optimization Evaluation Functions Minimax Search

Proof. Using similar algebra proof Proposition 14, find negative h,


0
0
0
max (x + h) max (x ) = h min (x ) + o(h).


x

0
Eq. (13), function maxi (x) point x0 left partial derivative mini x
(x ).
Note min max switched algebra negativity h.
argument applies left partial derivative mini (x).

Lemma 16. Let gi+ (n, w ) =

vn
w)
(w
wi+

(resp. gi (n, w ) =

vn
w )) right (resp.
(w
wi
w RN internal

left)

w ).
partial derivative minimax value vn (w
node
w ) tree T, exist right left partial derivatives
n principal variation (w
w ) respect = 1, . . . , N . right left partial
gi+ (n, w ) gi (n, w ) vn (w
derivatives are:

+
maxcC + (w
w ) gi (c, w ) (n: maximizing node)
n,T
+
gi (n, w ) =
+
mincC + (w
w ) gi (c, w ) (n: minimizing node)
n,T


mincC + (w
w ) gi (c, w ) (n: maximizing node)
n,T

gi (n, w ) =

(n: minimizing node).
maxcC + (w
w ) gi (c, w )
n,T

Proof. prove equalities basis mathematical induction leaf nodes
w ), definition evaluation function,
Lr,T root r. leaf n L (w
w ) clearly continuous partially differentiable respect
minimax value vn (w
component w RN . internal node n, assume, induction hypothesis,
right partial derivative gi+ (c, w ) left partial derivative gi (c, w ) exist
w 1 ) Cn+ (w
w 0 ) |h| < Eq. (14).
child c Cn,T . Recall Cn+ (w
induction hypothesis Proposition 14,
maxcCn+ (w
w ) vc
wi+

mincCn+ (w
vc
vc
w ) vc
w ),
w ) = min
w ).
(w
(w
+
+
+ (w
+
+
w
w
w
w)
w)
cCn (w
cCn (w




w ) = max
(w

Similarly, Proposition 15,
maxcCn+ (w
w ) vc
wi

mincCn+ (w
vc
vc
w ) vc
w ),
w ) = max
w ).
(w
(w


(w
+
+
w
w
w
w)
w)
cCn (w
cCn (w




w ) = min
(w

w ), obvious gi+ (n, w ) =
Now, prove Theorem 12. leaf n L (w

w ), Lemma 16 ensures left
= w
e(n, w ). internal node n (w

right partial derivatives gi+ (n, w ) gi (n, w ) given one best children.
w )
Thus, root r, always exist leaves la lb L (w
gi (n, w )

gi+ (r, w ) =


wi

gi (r, w ) =

e(la , w ),
563


wi

e(lb , w ).

(15)

fiHoki & Kaneko


g + (n, w 0 ) = 0
g (n, w 0 ) = 1

wi

e(a, w 0 ) = 1
e(a, w 0 ) = 0

n






g + (r, w 0 ) = g (r, w 0 ) = 0
w0) = 0
vr (w

r

c
&b


wi

e(c, w 0 ) = 0
e(c, w 0 ) = 0


wi

e(b, w 0 ) = 0
e(b, w 0 ) = 0

w ) exists w 0 , equal partial
Figure 17: Although partial derivative vr (w

0
derivative PV leaf wi e(a, w ).

w )
Remark 17. definition, gi+ (n, w 0 ) = gi (n, w 0 ), partial derivative vn (w
0

0
w ) satisfying
respect wi exists point w leaf l L (w


w0) =
vn (w
e(l, w 0 ).
wi
wi

(16)

w ) respect
Remark 18. = 1, . . . , N , partial derivative minimax value vn (w

0
0

w 0 ).
wi exists w equals wi e(l, w ), l unique element L (w
w ) partial derivative
Remark 19. exists tree Tr minimax value vn (w
0
w 0 )| > 1)
respect wi w , even leaves l PV unique (|L (w

0
give different partial derivatives wi e(l, w ). example sketched Figure 17,
partial derivative 1 0 b c.
A.4 Game-Tree Search Pruning Techniques
Consider game tree search function takes root position r evaluationw ) minimax values
function parameters w inputs, yields minimax tree TrS (w
(w
w
w
vn,Tr (w
(w
)


n


).

call

game-tree
search

static,
provided yields
w)
r


0
w )) = V (Tr (w
w )), root r. Then,
constant tree respect w , i.e., V (Tr (w
w ) yielded static game-tree
theorems 4 12 apply minimax value vr,TrS (w
search. example, fixed-depth minimax search minimax search considering limited
types moves (e.g., capture promotion) static game-tree search. minimax search
stand pat used quiescence search (Beal, 1990) static, too. Note stand
pat node n equivalent virtual move adding evaluation function e(n, w )
w ) Eq. (11), even n leaf node.
candidate node value vn (w
pruning techniques incorporated, part tree pruned explored.
0
w ) TrS (w
w ) yielded
Consider static search S, pruning 0 , tree TrS (w
0
. call pruning conservative, provided yields minimax value
w ) = vr,T S0 (w
w ). Theorem 4 applies minimax
root r w RN : vr,TrS (w
w ) (w
r
w
value root r, vr,T S0 (w
(w
),
yielded



static
game-tree search conservative
w)
r
pruning. Standard pruning (Knuth & Moore, 1975) conservative pruning. However,
many pruning techniques, e.g., static exchange evaluation (Reul, 2010), (extended) futility
pruning (Heinz, 1998), null move pruning (Adelson-Velskiy et al., 1975), late move
reductions (Romstad, 2010), prune sub-tree without prove sub564

fiLarge-Scale Optimization Evaluation Functions Minimax Search

tree irrelevant minimax value root. Thus, pruning techniques
generally conservative.
A.5 Summary
minimax value root tree explored game-tree search wellconfigured pruning techniques continuous. result suggests continuity
objective function MMTO Eq. (4), empirically observed Section 4.5.
partial differentiability, Theorem 12 suggest feasible consider leaves
principal variations search tree. one principal variation, stated
Remark 18, use partial derivative unique leaf introduced Section 3.3
correct. Otherwise, i.e., multiple principal variations, partial derivative
may exist different partial derivative one leaves, stated
Remark 19. Although frequency cases depends target game
evaluation features, almost negligible experiments discussed previous work
(Kaneko & Hoki, 2012).

References
Adelson-Velskiy, G. M., Arlazarov, V. L., & Donskoy, M. V. (1975). methods
controlling tree search chess programs. Artificial Intelligence, 6 (4), 361 371.
Akl, S. G., & Newborn, M. M. (1977). principal continuation killer heuristic.
Proceedings 1977 Annual Conference, ACM 77, pp. 466473, New York, NY,
USA. ACM.
Anantharaman, T. (1997). Evaluation tuning computer chess: Linear discriminant methods. ICCA Journal, 20 (4), 224242.
Baxter, J., Tridgell, A., & Weaver, L. (2000). Learning play chess using temporaldifferences. Machine Learning, 40 (3), 242263.
Beal, D. F. (1990). generalised quiescence search algorithm. Artificial Intelligence, 43,
8598.
Beal, D. F., & Smith, M. C. (2001). Temporal difference learning applied game playing
results application shogi. Theoretical Computer Science, 252 (1-2), 105
119.
Bertsekas, D. P., & Bertsekas, D. P. (2008). Nonlinear Programming (2nd edition). Athena
Scientific.
Bjornsson, Y., & Marsland, T. A. (2002). Learning control search extensions. Caulfield,
H. J., Chen, S.-H., Cheng, H.-D., Duro, R. J., Honavar, V., Kerre, E. E., Lu, M.,
Romay, M. G., Shih, T. K., Ventura, D., Wang, P. P., & Yang, Y. (Eds.), JCIS, pp.
446449. JCIS / Association Intelligent Machinery, Inc.
Browne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P., Rohlfshagen, P., Tavener,
S., Perez, D., Samothrakis, S., & Colton, S. (2012). survey monte carlo tree
search methods. Computational Intelligence AI Games, IEEE Transactions
on, 4 (1), 143.
565

fiHoki & Kaneko

Buro, M. (2002). Improving heuristic mini-max search supervised learning. Artificial
Intelligence, 134 (12), 8599.
Buro, M., Long, J. R., Furtak, T., & Sturtevant, N. R. (2009). Improving state evaluation,
inference, search trick-based card games. IJCAI, pp. 14071413.
Buro, M. (1995). Statistical feature combination evaluation game positions.
Journal Artificial Intelligence Research, 3, 373382.
Campbell, M., Hoane, Jr., A. J., & Hsu, F.-h. (2002). Deep Blue. Artificial Intelligence,
134 (12), 5783.
Chellapilla, K., & Fogel, D. (1999). Evolving neural networks play checkers without
relying expert knowledge. Neural Networks, IEEE Transactions on, 10 (6), 1382
1391.
Coulom, R. (2007). Computing Elo Ratings move patterns game go. ICGA
Journal, 30 (4), 198208.
Coulom, R. (2012). Clop: Confident local optimization noisy black-box parameter tuning.
Herik, H., & Plaat, A. (Eds.), Advances Computer Games 13, No. 7168 LNCS,
pp. 146157. Springer-Verlag.
Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods online learning
stochastic optimization. Journal Machine Learning Research, 12, 21212159.
Fawcett, T. E. (1993). Feature Discovery Problem Solving Systems. Ph.D. thesis, Department Computer Science, University Massachusetts, Amherst.
Furnkranz, J. (2001). Machine learning games: survey. Machines learn play
games, pp. 1159. Nova Science Publishers, Commack, NY, USA.
Gelly, S., Kocsis, L., Schoenauer M., Sebag, M., Silver, D., Szepesvari, C., & Teytaud, O.
(2012). grand challenge computer go: Monte carlo tree search extensions.
Commun. ACM, 55 (3), 106113.
Gelly, S., & Silver, D. (2011). Monte-carlo tree search rapid action value estimation
computer go. Artificial Intelligence, 175 (11), 18561875.
Gomboc, D., Buro, M., & Marsland, T. A. (2005). Tuning evaluation functions maximizing concordance. Theoretical Computer Science, 349 (2), 202229.
Heinz, E. A. (1998). Extended futility pruning. ICCA Journal, 21 (2), 7583.
Heinz, E. A. (1999). Adaptive null-move pruning. ICCA Journal, 22 (3), 123132.
Hoki, K. Bonanza computer shogi program.. http://www.geocities.jp/bonanza_
shogi/ Last access: 2013. Japanese.
Hoki, K. (2006). Optimal control minimax search results learn positional evaluation.
11th Game Programming Workshop (GPW2006), pp. 7883, Kanagawa, Japan.
Japanese.
Hoki, K., & Kaneko, T. (2012). global landscape objective functions optimization shogi piece values game-tree search. van den Herik, H. J., &
Plaat, A. (Eds.), Advances Computer Games 13, No. 7168 LNCS, pp. 184195.
Springer-Verlag.
566

fiLarge-Scale Optimization Evaluation Functions Minimax Search

Hoki, K., & Muramatsu, M. (2012). Efficiency three forward-pruning techniques shogi:
Futility pruning, null-move pruning, late move reduction (LMR). Entertainment
Computing, 3 (3), 5157.
Hsu, F.-h., Anantharaman, T. S., Campbell, M. S., & Nowatzyk, A. (1990). Deep Thought.
Marsland, T. A., & Schaeffer, J. (Eds.), Computers, Chess, Cognition, pp.
5578. Springer-Verlag.
Iida, H., Sakuta, M., & Rollason, J. (2002). Computer shogi. Artificial Intelligence, 134 (1
2), 121144.
Kaneko, T. (2009). Recent improvements computer shogi GPS-Shogi. IPSJ Magazine, 50 (9), 878886. Japanese.
Kaneko, T., & Hoki, K. (2012). Analysis evaluation-function learning comparison
sibling nodes. van den Herik, H. J., & Plaat, A. (Eds.), Advances Computer
Games 13, No. 7168 LNCS, pp. 158169. Springer-Verlag.
Knuth, D. E., & Moore, R. W. (1975). analysis alpha-beta pruning. Artificial
Intelligence, 6 (4), 293326.
Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. Machine Learning: ECML 2006, Vol. 4212, pp. 282293. Springer.
Krogius, N., Livsic, A., Parma, B., & Taimanov, M. (1980). Encyclopedia Chess Middlegames: Combinations. Chess Informant.
Levinson, R., & Weber, R. (2001). Chess neighborhoods, function combination, reinforcement learning. Marsland, T. A., & Frank, I. (Eds.), Computer Games,
No. 2063 LNCS, pp. 133150. Springer-Verlag.
Marsland, T. A. (1985). Evaluation function factors. ICCA Journal, 8 (2), 4757.
Marsland, T. A., & Campbell, M. (1982). Parallel search strongly ordered game trees.
ACM Computing Surveys, 14 (4), 533551.
Nitsche, T. (1982). learning chess program. Advances Computer Chess 3, pp.
113120. Pergamon Press.
Nocedal, J., & Wright, S. (2006). Numerical Optimization. Springer-Verlag.
Nowatzyk, A. (2000). http://tim-mann.org/DT_eval_tune.txt.
Pearl, J. (1980). Scout: simple game-searching algorithm proven optimal properties.
Proceedings First Annual National Conference Artificial Intelligence,
pp. 143145.
Reinefeld, A. (1983). improvement scout tree search algorithm. ICCA Journal,
6 (4), 414.
Reinfeld, F. (1969). 1001 Winning Chess Sacrifices Combinations. Wilshire Book
Company.
Reinfeld, F. (2001). Win Chess (Dover Books Chess). Dover Publications.
Reul, F. (2010). Static exchange evaluation -approach. ICGA Journal, 33 (1), 317.
567

fiHoki & Kaneko

Romstad, T. Introduction Late Move Reductions. http://www.glaurungchess.com/
lmr.html, Last access: 2010.
Russell, S. J., & Norvig, P. (2002). Artificial Intelligence: Modern Approach (2nd Edition).
Prentice Hall.
Schaeffer, J. (1986). Experiments search knowledge. Ph.D. Thesis, Department
Computing Science, University Waterloo, Canada.
Schaeffer, J. (1989). history heuristic alpha-beta search enhancements practice.
IEEE Transactions Pattern Analysis Machine Intelligence, PAMI-11 (1), 1203
1212.
Schaeffer, J., Hlynka, M., & Jussila, V. (2001). Temporal difference learning applied
high-performance game-playing program. IJCAI01: Proceedings 17th
international joint conference Artificial intelligence, pp. 529534, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Silver, D., & Tesauro, G. (2009). Monte-carlo simulation balancing. ICML 09: Proceedings 26th Annual International Conference Machine Learning, pp. 945952.
ACM.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction (Adaptive
Computation Machine Learning). MIT Press.
Tanaka, T., & Kaneko, T. GPS Shogi.. http://gps.tanaka.ecc.u-tokyo.ac.jp/
gpsshogi/ Last access: 2013. Japanese.
Tesauro, G. (2001). Comparison training chess evaluation functions. Machines
Learn Play Games, pp. 117130. Nova Science Publishers.
Tesauro, G. (2002). Programming backgammon using self-teaching neural nets. Artificial
Intelligence, 134 (12), 181199.
Tibshirani, R. (1996). Regression shrinkage selection via lasso. J. Royal. Statist.
Soc B, 58 (1), 267288.
Tsuruoka, Y., Yokoyama, D., & Chikayama, T. (2002). Game-tree search algorithm based
realization probability. ICGA Journal, 25 (3), 145152.
Ugajin, T., & Kotani, Y. (2010). Learning evaluation function based tree strap shogi.
15th Game Programming Workshop, pp. 114118. Japanese.
van den Herik, H. J., Uiterwijk, J. W. H. M., & van Rijswijck, J. (2002). Games solved:
future. Artif. Intell., 134 (1-2), 277311.
van der Meulen, M. (1989). Weight assessment evaluation functions. Beal, D. (Ed.),
Advances in. Computer Chess 5, pp. 8189.
Veness, J., Silver, D., Uther, W., & Blair, A. (2009). Bootstrapping game tree search.
Advances Neural Information Processing Systems 22, pp. 19371945.
Zobrist, A. L. (1990). new hashing method application game playing. ICCA
Journal, 13 (2), 6973.

568


