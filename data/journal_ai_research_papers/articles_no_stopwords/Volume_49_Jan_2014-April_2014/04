Journal Artificial Intelligence Research 49 (2014) 733-773

Submitted 07/13; published 04/14

Comparative Evaluation Link-Based Approaches
Candidate Ranking Link-to-Wikipedia Systems
Norberto Fernandez Garca
Jesus Arias Fisteus
Luis Sanchez Fernandez

berto@it.uc3m.es
jaf@it.uc3m.es
luiss@it.uc3m.es

Telematics Engineering Department
Universidad Carlos III de Madrid
Avda. Universidad, 30, E-28911
Leganes, Madrid, Spain.

Abstract
recent years, task automatically linking pieces text (anchors) mentioned
document Wikipedia articles represent meaning anchors received
extensive research attention. Typically, link-to-Wikipedia systems try find set
Wikipedia articles candidates represent meaning anchor and, later,
rank candidates select appropriate one. ranking process
systems rely context information obtained document anchor
mentioned and/or Wikipedia. paper center attention use
Wikipedia links context information. particular, offer review several candidate
ranking approaches state-of-the-art rely Wikipedia link information.
addition, provide comparative empirical evaluation different approaches
five different corpora: TAC 2010 corpus four corpora built actual Wikipedia
articles news items.

1. Introduction
Due important volume information contained Wikipedia, open
nature content, on-line encyclopedia adopted recent times useful
resource computational linguistics tasks name translation (Lin, Snover, & Ji, 2011),
named entity recognition (Nothman, Murphy, & Curran, 2009), etc.
development automatic link discovery systems (Erbs, Zesch, & Gurevych, 2011)
another area research Wikipedia important impact. task
discovering links Wikipedia articles addressed, slight variants
different names, different communities. instance, Hachey et al. (2013) distinguish
named entity linking, addressed context Knowledge Base Population
(KBP) track (National Institute Standards Technology, 2014b) Text Analysis
Conference (TAC) (National Institute Standards Technology, 2014a), wikification, addressed Link-the-Wiki track Initiative Evaluation XML
retrieval (INEX) (INEX, 2014). cases goal automatically find Wikipedia
articles represent meaning certain piece text document define
link Wikipedia using anchor piece text. However, differences
aspects anchors considered (only named entities named entity linking, named
c
2014
AI Access Foundation. rights reserved.

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

entities common terms wikification) whether Wikipedia considered
complete source knowledge (wikification) (named entity linking).
Henceforth, simply refer link-to-Wikipedia general task discovering
links Wikipedia, includes wikification named entity linking particular
cases.
According Erbs et al. (2011), task discovering links divided series
steps. include: identifying anchors linked, searching candidate link
targets anchor, selecting best candidate results searching
step. common link-to-Wikipedia approaches address steps independently
sequentially (though examples steps independent,
Cucerzan, 2012 Sil, 2013).
Due important role (Ji, Grishman, & Dang, 2011), context paper
center attention last aforementioned processes. referred
disambiguation Hachey et al. (2011). However, selecting best link target
usually involves creating ranking candidates choose one highest
rank, authors refer process target ranking (Erbs et al., 2011) candidate
ranking (Guo, Tang, Che, Liu, & Li, 2011; Ploch, Hennig, de Luca, & Albayrak, 2011; Ji
et al., 2011). article, adopt term candidate ranking.
order select best Wikipedia article link given anchor, candidate
ranking process relies context information provided set features.
features extracted document anchor placed (the context document)
and/or different Wikipedia articles considered candidates become link target.
According Erbs et al. (2011) features classified three groups: (1)
extracted text document/articles, (2) extracted titles;
(3) based existing links. latter subject study paper.
Traditional research area computational linguistics shown effectiveness using WordNet (Miller, 1995) graph links tasks computing semantic relatedness (Budanitsky & Hirst, 2006) performing word sense disambiguation (Navigli &
Lapata, 2010). case link discovery, Erbs et al. (2011) indicate that, enough
training information available, link-based approaches outperform text-based ones.
Taking account, surprising find many link-to-Wikipedia approaches
use features candidate ranking based information links. examples work Milne Witten (2008b), Pilz (2010), Radford et al. (2010), Fernandez
et al. (2010), Guo et al. (2011), Ploch et al. (2011) Ratinov et al. (2011).
Given ample variety link-based features candidate ranking described
state art, comparative analysis different alternatives useful decide
approach (or approaches) considered designing link-to-Wikipedia
systems. However, using results published state art difficult
compare across systems ranking performance different link-based approaches.
First, link-to-Wikipedia systems usually evaluated end-to-end setup, is,
evaluation involves ranking stage, candidate searching
candidate selection processes. Thus, impact performance different system
components mixed. Second, general, link-to-Wikipedia systems rely
link-based features rank candidates, combine features
types. Thus, effects different contributions ranking process mixed.
734

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Taking account, main goals paper twofold: (1) offer overview
link-based approaches candidate ranking link-to-Wikipedia systems; and, (2) perform
empirical evaluation compare approaches. order address aforementioned difficulties, we: (1) focus analysis candidate ranking stage, isolating
much possible candidate search/selection stages; (2) consider linkbased features, combined based text titles. similar comparison is,
knowledge authors, available time writing.
rest paper organized follows: section 2 presents definitions
formal description problem addressed. Section 3 outlines different linkbased approaches compared. Section 4 describes setup empirical evaluation
carried out, well results. Section 5 offers overview related work.
Finally, section 6 closes paper concluding remarks future lines work.

2. Definitions Problem Formalization
section introduce definitions nomenclature helpful
rest article.
textual document mentions anchor going linked Wikipedia
named context document represented dc .
set Wikipedia articles denoted W , whereas particular Wikipedia
articles represented wi , = 1, . . . , |W |. case consider set W
Wikipedia pages belong Main namespace (Wikipedia, 2014b)
represent non-ambiguous concepts (that is, disambiguation pages filtered out).
denote C(dc , a) set Wikipedia articles {c1 , c2 , . . . , c|C(dc ,a)| }, ck W
selected candidates fit meaning dc .
link l defined duple l = (src(l), dest(l)), src(l) represents
document source link dest(l) document pointed
link, is, destination.
denote F (d) set documents destination forward links
d, is: F (d) = {f | l, src(l) = dest(l) = f }. Similarly, represent
B(d) set documents source backward links d, is: B(d) =
{b | l, src(l) = b dest(l) = d}. paper, use information provided
Wikipedia links. Thus, consider Wikipedia articles members F (d)
B(d). Note F (d) B(d) sets and, thus, consider
duplicates. However, might happen document several links pointing
destination. order represent information, denote number links
source document destination document n(s, d) .
Taking account aforementioned definitions, candidate ranking process
addressed context paper may formalized follows:
Definition 1. Given context document dc , mentions anchor a, set
candidate Wikipedia articles C(dc , a), candidate ranking task consists ordering
members C(dc , a) according rating. rating measures suitability
candidate represent meaning anchor. candidate ci C(dc , a)
highest rank, fits best meaning anchor context document
dc , selected define new link (dc , ci ).
735

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

aspects stressed definition:
introduce restriction nature anchors linked.
particular, may represent either named entities (persons, organizations, etc.)
common terms.
previous related work (Cucerzan, 2007; Mihalcea & Csomai, 2007; Han,
Sun, & Zhao, 2011), address paper scenario adequate
Wikipedia article linked exist.

3. Overview Approaches
describe different candidate ranking approaches evaluated
paper. common source context information links.
particular, links considered available context document, dc , well
constitute link structure Wikipedia, including links to/from
candidate Wikipedia articles, ci C(dc , a).
However, approaches considered use link information
manner. particular, classify two families, name (1) bag-oflinks approaches, (2) graph approaches. main difference
second group links used build graph structure, later analyzed
select best candidate. case approaches first group.
3.1 Bag-of-Links Approaches
section present set approaches characteristic common:
rely building graph link context information rank candidates.
Nevertheless, approaches family differences way address
task. particular, distinguish least three alternative groups:
approaches rely similarity metrics compute similarity score
context document candidate, later select candidate
highest score (the similar one).
Another alternative rely popularity metrics, simply try compute
popularity score candidate. popular candidate selected.
approaches rely information provided context document.
ranking process modeled statistical problem. Statistical methods
used select likely candidate, given context information.
accordance classification, following sections describe group approaches.
3.1.1 Similarity Metrics
first similarity metrics consider relatedness, computed basis
Wikipedia Link-based Measure (Milne & Witten, 2008a). relatedness used feature
736

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

link-to-Wikipedia approaches Milne Witten (2008b), Han Zhao
(2009), Kulkarni et al. (2009), Pilz (2010), Fahrni et al. (2011), Han et al. (2011) Ratinov
et al. (2011).
Basically, relatedness allows compute similarity two Wikipedia documents wi , wj links common. original definition Milne
Witten (2008a), computed as:
RB (wi , wj ) =

log(max{|B(wi )|, |B(wj )|}) log(|B(wi ) B(wj )|)
log(|W |) log(min{|B(wi )|, |B(wj )|})

(1)

According Milne Witten (2008a), relatedness metrics based Normalized Google Distance (NGD), defined Cilibrasi Vitanyi (2007). NGD based
intuition terms similar related meaning co-occur frequently
documents. Thus, given pair terms, Google search engine used obtain
pages mention terms. Pages mention indicate relatedness,
pages one suggest unrelatedness. indicated Milne Witten
(2008a), relatedness metrics, defined equation 1, shares inspiring principle, uses Wikipedia links instead Google search results account mentions.
distance metrics, relatedness values expected smaller similar
Wikipedia articles are. However, easy transform distance metrics
similarity metrics following approach Gracia Mena (2008), requires
computation of:
simRB (wi , wj ) = e2RB (wi ,wj )

(2)

second similarity metrics Wikipedia articles considered based
computing Pointwise Mutual Information (PMI) sets links articles
compared. instance, used Ratinov et al. (2011) link-to-Wikipedia
task. defined work as:
PB (wi , wj ) =

|B(wi ) B(wj )|/|W |
(|B(wi )|/|W |)(|B(wj )|/|W |)

(3)

Note definitions equations (1) (3) rely backlinks (B(x)) computation. However, indicated Ratinov et al. (2011), relatedness PMI
computed using outgoing links document. paper explore
compare alternatives denote relatedness similarity PMI computed
forward links simRF PF respectively.
Taking aforementioned definitions account, Wikipedia article {c1 , . . . , ck }
C(dc , a) compute relatedness PMI Wikipedia articles linked
dc , is, fj F (dc ). Combining different values obtain final
relatedness PMI ci dc . According Ratinov et al. (2011) several ways
combine values may followed, taking average maximum.
explore different possibilities paper. particular, relatedness:
737

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

RelFA (ci , dc ) =

RelB
(ci , dc ) =

1
|F (dc )|
1
|F (dc )|

RelFM (ci , dc )

=


RelB
(ci , dc ) =

X

simRF (ci , fj )

(4)

X

simRB (ci , fj )

(5)

max

simRF (ci , fj )

(6)

max

simRB (ci , fj )

(7)

fj F (dc )

fj F (dc )
fj F (dc )
fj F (dc )

Whereas Pointwise Mutual Information computed as:
P IFA (ci , dc ) =

1
|F (dc )|

1

P IB
(ci , dc ) =
|F (dc )|
P IFM (ci , dc )

=


P IB
(ci , dc ) =

X

PF (ci , fj )

(8)

X

PB (ci , fj )

(9)

max

PF (ci , fj )

(10)

max

PB (ci , fj )

(11)

fj F (dc )

fj F (dc )
fj F (dc )
fj F (dc )

Another well-known approach compute document similarity within natural language processing information retrieval communities cosine similarity. Basically,
vector built represent context document candidate article. Then, similarity context document candidate computed cosine
angle respective vectors. Several approaches state art (Bunescu
& Pasca, 2006; Fader, Soderland, & Etzioni, 2009; Nguyen & Cao, 2010; Fahrni et al.,
2011; Ploch et al., 2011; Ratinov et al., 2011) use cosine similarity. However,
differences features used build vector representations
documents.
case, requirement considering solely links context information.
Thus, document represented using links mentioned
document. similar approach model documents compute cosine similarity
used, instance, Fahrni et al. (2011) Ploch et al. (2011).
particular, document represented vector vd R|W | . component
vd,i , = 1, . . . , |W | vector vd computed traditional term frequency
(TF), inverse document frequency (IDF) product (Manning, Raghavan, & Schtze, 2008)
follows:
vd,i = F (d, wi ) IDF (wi ) = P

|W |
n(d, wi )
log
|B(wi )|
wj F (d) n(d, wj )

(12)

Note F (d) contain certain Wikipedia article wi , n(d, wi ) = 0,
F (d, wi ) = 0 and, thus, vd,i = 0. Due this, vector vd expected sparse.
738

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Given two documents compared (for instance, dc Wikipedia article ci
C(dc , a)), cosine similarity metrics computed cosine angle
vectors two documents, follows:
simcos (vci , vdc ) =

vdc vci
||vdc ||2 ||vci ||2

(13)

Finally, Radford et al. (2010) suggest metrics based Wikipedia link structure,
interpreted similarity metrics. order compute metrics,
following equation computed candidate ci C(dc , a):
simR (ci , dc ) = log(|B(ci ) Ldc | + 1) + 1

(14)

Ldc set built union backlinks Wikipedia articles
linked dc :
Ldc =

[

B(fi )

(15)

F (dc )

aforementioned similarity metrics trivially used address candidate
ranking process. candidate ci C(dc , a) selected link destination
maximal similarity context document dc :
arg max{simf (ci , dc )}
ci

(16)

, RelM , P , P ,
simf represents one functions: RelFA , RelFM , RelB
B
F
F
, P , sim
P IB
cos , simR .
B

3.1.2 Popularity Metrics
Algorithms based popularity metrics constitute second group bag-of-links
family.
first approach could used compute popularity certain candidate,
ci C(dc , a), simply counting number Wikipedia articles link it, is,
indegree, |B(ci )| or, alternatively, number Wikipedia articles linked it,
outdegree, |F (ci )|. metrics considered, instance, work Dredze et al.
(2010), Guo et al. (2011) Cao et al. (2011).
Fader et al. (2009) describe popularity score based incoming links
Wikipedia candidate ci :
|B(ci )|
))
(17)

parameter set = 15 (Fader et al., 2009).
Finally, degree centrality certain Wikipedia candidate article ci
considered bag-of-links popularity metrics. Hachey et al. (2011) define degree
centrality as:
popF (ci ) = (1 + log(1 +

D(ci ) =

|B(ci )|
|W | 1

739

(18)

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

Note that, indicated beginning section, aforementioned popularity
metrics take account information provided context document.
depend information obtained candidates.
aforementioned metrics used rank candidates popularity. Then,
popular candidate ci C(dc , a) selected link destination. Taking
account functions |B(ci )| involved aforementioned approaches (linear,
logarithm) monotonically increasing functions, order (ranking) provided
cases same. Due this, context paper, consider evaluation
indegree outdegree only:

arg max{indegree(ci )}

(19)

arg max{outdegree(ci )}

(20)

ci

ci

3.1.3 Statistical Techniques
candidate ranking process addressing context paper
mathematically modeled using statistical techniques, suggested work
Fader et al. (2009) Han Sun (2011).
particular case, considering set Wikipedia articles linked dc , F (dc ) =
{f1 , . . . , f|F (dc )| } input features, destination computed selecting
Wikipedia article ci C(dc , a) maximizes conditional probability:
P (ci /f1 , . . . , f|F (dc )| ) F (dc )

(21)

number features |F (dc )| considered relatively large, estimating values
conditional probability equation (21) ci would complex problem. Due
this, practice, problem reformulated make treatable. particular:
(1) Bayes rule used reverse conditional probability equation (21); and, (2)
assumed features (links F (dc ) case) conditionally independent
(Naive Bayes assumption).
result problem reformulation known state art Naive
Bayes classifier (Manning et al., 2008). specific scenario, classifier
able distinguish classes (the different ci C(dc , a)) likely
anchor a.
Mathematically, expression use select best ci using maximum
posteriori decision rule (Manning et al., 2008) is:
|F (dc )|

arg max{N B(ci , dc )} = arg max{log P (ci ) +
ci

ci

X

n(dc , fj ) log P (fj /ci )}

(22)

j=1

logarithm function used avoid underflows (as suggested Manning et al.,
2008).
order compute values equation (22) ci , need know value
two probabilities: (1) prior probability class ci , P (ci ); and, (2) conditional
740

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

probabilities P (fj /ci ). estimate two probabilities follow approach described
Manning et al. (2008):
P
bj B(ci ) n(bj , ci )
P
(23)
P (ci ) = P
wi W
fj F (wi ) n(wi , fj )

is, P (ci ) represents Maximum Likelihood Estimate (MLE) probability
certain document contains link ci , computed dividing number actual
links ci total number links Wikipedia:
P
1 + bi B(ci ) n(bi , fj )
P
P (fj /ci ) = P
(24)
wj W (1 +
bi B(ci ) n(bi , wj ))

case, P (fj /ci ) represents probability anchor linking fj
document already contains link ci . Again, MLE used conditional
probabilities and, thus, probabilities computed dividing number links
fj documents contain link ci total number links documents
contain link ci . seen MLE smoothed using Laplace smoothing
avoid zeros.
3.2 Graph Approaches
second family link-based approaches candidate ranking consider graph
approaches, rely building graph processing select best candidate.
3.2.1 PageRank Personalized PageRank
first algorithm consider within graph family PageRank, first defined
Page et al. (1999), widely known due use part Google search engine.
Examples application PageRank method candidate ranking found
instance work Fernandez et al. (2010), Dredze et al. (2010) Hachey et al.
(2011).
Basically, PageRank algorithm used compute popularity
certain page, taking account popularity number pages link it. Using
mathematical formulation described Brin Page (1998) particular scenario
addressing paper, compute popularity P R(wi ) Wikipedia
article wi , need solve following equation:
P R(wi ) =

(1 d)
+d[
|W |

X

wj B(wi )

1
P R(wj ) ]
|F (wj )|

(25)

damping factor set 0 1, typically set
0.85 according Brin Page (1998) Hachey et al. (2011).
Note that, according equation (25), taking account links to/from
Wikipedia, computing PageRank general scenario requires complete information link structure Web, computationally expensive problem.
simplification assumed Fernandez et al. (2010) Hachey et al. (2011).
741

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

Note that, happens popularity metrics described section 3.1.2,
PageRank metrics depend context information dc , graph
built link structure Wikipedia. However, context information dc
included process using variant algorithm known Personalized PageRank
Topic-Sensitive PageRank (Haveliwala, 2003). algorithm used instance Yeh
et al. (2009) define semantic relatedness metrics.
main difference classical PageRank Personalized PageRank that,
instead relying uniform damping vector, biased give relevance
given set resources (Haveliwala, 2003). particular case, resources
articles linked dc , is, members F (dc ). practice, equation (25) adapted
follows compute Personalized PageRank:

P P R(wi , dc ) =









d[

X

1
P P R(wj , dc )] wi
/ F (dc )
|F (wj )|

X

1
P P R(wj , dc )] wi F (dc )
|F (wj )|

wj B(wi )






(1 d) F (dc , wi ) + [

wj B(wi )

F (dc , wi ) represents term frequency link wi context
document dc , computed indicated equation (12).
PageRank Personalized PageRank values computed, used
rank candidates. article ci C(dc , a) highest P R(ci ) P P R(ci , dc )
value selected link destination:

arg max{P R(ci )}

(26)

arg max{P P R(ci , dc )}

(27)

ci

ci

3.2.2 Random Walk
Several works state art (Gentile et al., 2009; Fernandez et al., 2010; Han et al.,
2011; Ploch et al., 2011; Jimenez et al., 2013) define techniques link several anchors
context document time. Usually, approaches address candidate
ranking process building graph computing random walk (Spitzer, 1976)
graph rank nodes.
Though approaches share underlying principle, differences
mainly two aspects: nature nodes considered part
graph nature edges. instance, Gentile et al. (2009) indicate
nodes represent either concepts (candidates) features (like words title
certain candidate), edges link candidates specific features. Han et al.
(2011) define nodes anchor linked candidates. edges link
anchor candidates candidates among basis
relatedness (see section 3.1.1). work Fernandez et al. (2010) nodes include
candidates, edges defined basis information co-occurrence
742

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

candidates Wikipedia articles. similar approach used Ploch et al. (2011),
including nodes candidates edges defined basis Wikipedia links.
Note PageRank metrics interpreted random walk (Page et al.,
1999). However, PageRank, described section 3.2.1, operates graph
whole link structure Wikipedia, approaches section build graphs,
typically much smaller tailored concrete scenario addressed.
approaches Gentile et al. (2009) Han et al. (2011) rely text-based features
build graphs: work Gentile et al. (2009) features used nodes
graph, whereas Han et al. (2011) use text-based similarity metrics compute
weights edges connecting anchor candidates. Due this, context
paper evaluate approaches Fernandez et al. (2010) Ploch et al. (2011),
rely link information.
indicated above, Fernandez et al. (2010) Ploch et al. (2011) designed
approaches link time several anchors context document. Thus,
need adapt approaches scenario addressed paper,
anchor considered. so, element F (dc ) treated single-element
pseudo-candidate set anchor ai dc = 1, . . . , |F (dc )|.
compute score candidate ci C(dc , a) according Ploch et al. (2011)
(that name RWP (ci , dc ))) build graph nodes candidates
pseudo-candidates (that is, elements C(dc , a) plus Wikipedia articles linked
F (dc )). edge two nodes appears link Wikipedia
articles represented nodes. graph built, PageRank algorithm
applied graph. score assigned node PageRank value.
similar approach used case Fernandez et al. (2010). Again, nodes
include candidates pseudo-candidates, case edges represent cooccurrences. particular, edge node wi node wj when:
1. least third Wikipedia article wk links wi wj , is,
wi , wj F (wk ). edges assigned weights according to:
weightC (wi wj ) =

|B(wi ) B(wj )|
|B(wi )|

(28)

2. direct link exists wi wj , is, wj F (wi ). edges assigned
weights follows:
weightL (wi wj ) = Fij IDFj = P

n(wi , wj )
|W |
log
|B(wj )|
wk F (wi ) n(wi , wk )

(29)

two nodes wi wj match conditions above, is, directly
linked co-occur third article wk , single edge created combines
contributions follows:

weight(wi wj ) =

kL
kC
weightC (wi wj ) +
weightL (wi wj )
kC + kL
kC + kL
743

(30)

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

kL kC configuration parameters. case use values
kL = 0.55 kC = 0.25 suggested Fernandez et al. (2010).
weighted, directed graph built, PageRank computed graph.
score candidate ci C(dc , a), named RWF (ci , dc ), PageRank value
candidate node graph. scores candidates computed,
candidate highest score selected best one:
arg max{RWP (ci , dc )}

(31)

arg max{RWF (ci , dc )}

(32)

ci

ci

Note that, approaches listed section 3, might happen different
members candidates set obtain weight and, thus, would tie
ranking. used frequently linked (MFL) algorithm break potential
ties. algorithm simply assigns weight candidate according total number
incoming links, is:
F L(ci , dc ) =

X

n(bj , ci )

(33)

bj B(ci )

4. Comparative Evaluation
section reports results evaluation different approaches described
section 3. organized follows: experimental setup (Wikipedia dataset, corpora,
etc.) used evaluation outlined section 4.1, whereas section 4.2 reports
quantitative results well analysis interpretation results.
4.1 Experimental Setup
order evaluate approaches described section 3, need set elements: (1)
corpora queries evaluate approaches; (2) information Wikipedia link
structure used input different approaches; (3) adequate metrics
measure compare performance approach. next sections describe
three elements briefly:
4.1.1 Corpora Queries
order evaluate different approaches, need corpora containing link-to-Wikipedia
queries. According definition problem (see section 2) queries
corpora provide:
anchor going linked.
context document dc anchor appears. links document
provide context information used algorithms.
set candidates, C(dc , a), Wikipedia articles potential targets
anchor.
744

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

golden standard indicates correct answer (candidate C(dc , a) ranked
top) query. golden standard used compute performance
algorithms evaluated.
state art, distinguish different approaches regarding corpora
use empirical evaluation. first approach build specific corpora. followed
early work (Bunescu & Pasca, 2006; Cucerzan, 2007; Mihalcea & Csomai, 2007), well
recent work (Milne & Witten, 2008b; Nguyen & Cao, 2010; Pilz, 2010).
common approach within first group use corpus subset Wikipedia articles
compare links suggested automatic algorithms provided Wikipedia
editors (see instance Bunescu & Pasca, 2006; Cucerzan, 2007; Milne & Witten, 2008b;
Nguyen & Cao, 2010 Pilz, 2010). methodology used context
INEX Link-the-wiki track (Huang, Xu, Trotman, & Geva, 2008). second alternative
use already available corpora, TAC/KBP corpus (used instance Han &
Sun, 2011 Hachey et al., 2011), corpora defined Cucerzan (2007) (used
instance Gentile et al., 2009 Ratinov et al., 2011).
context paper, adopt approaches. particular, use
following corpora comparative evaluation:
Cucerzan work Cucerzan (2007) authors use two different corpora, one
built Wikipedia articles manually annotated MSNBC (MSNBC,
2014) news items. used corpora build own. order so,
proceeded follows:
1. documents Cucerzan corpora contain set pairs {anchor, Wikipedia
article}, one representing potential link-to-Wikipedia query. select
randomly 250 pairs Cucerzans corpora. pairs provide us
anchor linked golden standard (correct answer
query).
2. typical approach among systems TAC/KBP generate candidate
set, C(dc , a), rely information retrieval techniques (Ji et al., 2011).
paper adopt approach. However, difference TAC/KBP
scenario, evaluation involves stages entity linking, center
evaluation candidate ranking stage. Due this, interested
isolating much possible stage potential bad performance
particular candidate search implementation. is, interested
analyzing performance different candidate ranking approaches assuming
candidate search stage ideal, sense always returns
correct candidate among candidate set. Obviously, exist
ideal candidate searcher. Thus, practice, rely state art
search engine (Google) append correct answer candidate set
case found search engine. particular, query Google
search engine text anchor site:en.wikipedia.org restriction,
filtering top-10 Google results Wikipedia pages included
Main namespace. case correct Wikipedia article linked
745

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

included within Google result set, appended end, though
happens limited number queries: Curcerzan Wikipedia
corpus correct candidate added 9 250 queries (3.6%),
Curcerzan news corpus added 14 250 queries (5.6%).
3. Finally, rest pairs {anchor, Wikipedia article} included
document query selected, obtain Wikipedia article
component used context information (links F (dc )), filtering
links articles included candidate set order avoid bias.
Ad-hoc corpora Two ad-hoc corpora used evaluation. One corpus
(Wikipedia random corpus) built following methodology suggested
previous works state art, is, selecting set 500 Wikipedia articles
using Random article page (Wikipedia, 2014a).
second ad-hoc corpus (Wikinews corpus) built using documents
English Wikinews site (Wikinews, 2014b). documents represent news items.
usually annotated human editors Wikipedia links. case 500
news items selected Random article functionality Wikinews (Wikinews,
2014a).
document total set 1000 documents two ad-hoc corpora,
built link-to-Wikipedia query using following procedure:
1. Wikipedia link randomly selected document.
2. selected link obtain anchor golden standard (link
destination Wikipedia).
3. use anchor Google search engine build candidate set,
indicated case Cucerzan corpora. Again, append correct
candidate included Google result set. particular,
correct candidate added 14 500 queries (2.8%) case
Wikinews corpus 36 500 queries (7.2%) Wikipedia random
corpus.
4. query context information obtained rest links
document, filtering linking members candidate set order
avoid bias.
TAC2010 TAC 2010 dataset includes total 2250 entity linking queries and,
one, provides anchor linked, context document dc
golden standard. used dataset basis build last corpus involved
evaluation. order so, proceeded follows:
1. total set 2250 queries, 1230 golden standard NIL answer,
is, Wikipedia article link cases. Thus, correct
candidate instance exists and, due this, difficult take advantage
queries evaluate candidate ranking process. Taking account,
discard queries keep remaining 1020.
746

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

2. documents TAC 2010 corpus contain links. Thus, use
following procedure order obtain context links needed
algorithms:
query, analyze context document dc using natural language
processing techniques. particular, extract named entities (persons,
locations organizations) text using Stanford NER tool (Finkel,
Grenager, & Manning, 2005).
Then, link detected entities Wikipedia using Google. particular,
query Google search engine text named entity
site:en.wikipedia.org restriction, filtering top-10 Google results
Wikipedia pages included Main namespace, assigning
link top result filtered list. discard named entities
Google results found. links named entities Wikipedia defined
procedure used context information candidate ranking,
filtering context links members candidate set
order avoid bias.
discard queries context information available, is,
NER tool find named entities dc , filtered
process linking Wikipedia. results total 1012 valid
queries.
3. candidate set C(dc , a) query obtained using procedure
previous corpora: using Google search anchor appending
correct candidate case found (which happens 70 1012 queries
(6.9%)).
summarize, carry evaluation five different corpora (Cucerzan news,
Cucerzan Wikipedia, Wikipedia random, Wikinews TAC 2010)1 , add
total 2512 link-to-Wikipedia queries. Boxplot diagrams representing distributions
corpus number candidates (|C(dc , a)|) per query, number links
(|F (dc )|) per query, shown Figures 1 2 respectively.
Note appending correct candidate candidates set needed 143
2513 queries. indicates Google performs quite well candidate searcher
case, candidate recall near 95% (considering first 10 results).
put result context, indicate Hachey et al. (2013) compare several
candidate search approaches TAC 2009 dataset report candidate recall
75% limited maximum 10 results. However results similar
Lehmann et al. (2010), authors use Google search combined
set additional techniques report 97% candidate recall TAC 2009 dataset.
4.1.2 Wikipedia Link Structure
approaches described section 3 require information Wikipedia link structure carry candidate ranking process. case, information
1. corpora available download at: http://www.it.uc3m.es/berto/link-to-wikipedia/survey/

747

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

2

4

6

8

10

Distribution number candidates per query corpus

Cucerzan News

Cucerzan Wiki.

Wiki. Random

Wikinews

TAC2010

Figure 1: Boxplot diagram number candidates (|C(dc , a)|) per query
corpus.

0

50

100

150

200

250

300

350

Distribution number context links per query corpus

Cucerzan News

Cucerzan Wiki.

Wiki. Random

Wikinews

TAC2010

Figure 2: Boxplot diagram number links context (|F (dc )|) per query
corpus.

748

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

obtained dump Wikipedia page links provided DBpedia (Bizer et al., 2009)
version 3.82 , generated full Wikipedia dump dated June 2012.
links dump preprocessed follows:
redirections resolved, using redirections mapping table DBpedia
3.83 .
indicated section 2, consider Wikipedia pages belong
Main namespace. Thus, links from/to pages namespaces (like Talk pages,
User pages, etc.) removed.
Using information provided disambiguation map DBpedia 3.84 ,
links from/to disambiguation pages removed.
inner links (from article itself) filtered out.
evaluation corpora described section 4.1.1 include cases Wikipedia
articles. separate input data evaluation data, links source
destination one Wikipedia articles included evaluation corpora
filtered out.
4.1.3 Performance Metrics
measure compare performance considered approaches, need
adequate metrics. well-known evaluation metrics link-to-Wikipedia approaches
accuracy, used instance Bunescu Pasca (2006), Cucerzan (2007), Hachey et al.
(2011), Ratinov et al. (2011) Hachey et al. (2013). accuracy computed
percentage queries candidate selected algorithm correct one,
or, formally:
Accuracy =

1 X
S(q)
|Q|

(34)

qQ

Q represents set evaluation queries, q particular query set, S(q)
function S(q) = 1 candidate article ranked top query q
correct answer S(q) = 0 otherwise.
However, center evaluation candidate ranking stage link-toWikipedia task, accuracy presents limitation: take account actual
position correct answer within ranking produced algorithm. example,
one algorithm ranks correct answer query 2nd position, whereas another
algorithm ranks 8th position, contribution query accuracy
zero cases, despite first algorithm ranked correct answer higher.
scenarios link-to-Wikipedia approaches work human-supervision (for instance, systems used within production process news agency (Fernandez
2. http://downloads.dbpedia.org/3.8/en/page links en.nt.bz2 (April, 2014)
3. http://downloads.dbpedia.org/3.8/en/redirects transitive en.nt.bz2 (April, 2014)
4. http://downloads.dbpedia.org/3.8/en/disambiguations en.nt.bz2 (April, 2014)

749

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

et al., 2006) add metadata news items) particular order candidates relevant, case top-ranked candidate correct one, human supervisor
continue reading ranked list candidates select another option. Obviously,
nearer top correct candidate list suggestions, better.
Taking account, decided report performance using accuracy,
two position-based discounting schemes measure overall quality ranked
list results:
Mean Reciprocal Rank (MRR) used instance evaluation question
answering systems (Voorhees, 1999). MRR set evaluation queries Q
computed as:
RR(Q) =

1 X 1
|Q|
r(q)

(35)

qQ

r(q) represents position correct candidate rank query
q Q.
shown equation 35, MRR penalizes differences position severely.
Taking account, report results Discounted Cumulative
Gain certain level K (DCG@K), introduces smoother penalization
position. DCG@K computed as:
k
1 X X 2R(q,i) 1
DCG@K(Q) =
|Q|
log2 (1 + i)

(36)

qQ i=1

Q represents set evaluation queries, q particular query set,
R(q, i) relevance score given candidate article position query q.
adopt binary relevance model and, thus, R(q, i) = 1 candidate position
correct answer R(q, i) = 0 otherwise. Furthermore, consider
single candidate relevant query. Taking account, DCG@K
equivalent normalized version, Normalized Discounted Cumulative Gain
K (NDCG@K) (Manning et al., 2008), equation 36 simplified into:

1 X
DCG@K(Q) =
f (q, k) f (q, k) =
|Q|
qQ

(

1
log2 (1+r(q))

r(q) <= k

0

r(q) > k

(37)

r(q) represents position correct candidate rank query q.
seen equation 37, bigger value r(q) (that is, farther away
correct candidate top rank) lesser value term added
DCG@K. Note DCG@1 would equivalent accuracy defined
equation 34.
750

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Figure 3: Taxonomy different approaches considered evaluation.
4.2 Evaluation
section reports results empirical evaluation approaches.
structured presentation results three parts: section 4.2.1 compares individual
algorithms described section 3, section 4.2.2 analyzes combination different approaches use machine learning techniques and, finally, section 4.2.3 evaluates
impact changing search stage performance algorithms.
4.2.1 Comparison Individual Approaches
approaches described section 3 (summarized taxonomy shown Figure 3)
evaluated corpora described section 4.1.1. Table 1 reports accuracy
obtained approach different evaluation corpora. highlighted
boldface best accuracy among link-based evaluated approaches particular
corpus.
Table 1 includes column (Overall) reports results obtained corpus
generated aggregating queries. last column, Confidence Interval (Overall),
reports 95% confidence interval accuracy Overall case, computed using
bootstrap methods suggested Adibi, Cohen, Morrison (2004).
seen Approach column Table 1, apart approaches considered section 3, include results two naive algorithms reference baselines:
(1) random algorithm, simply ranks candidates randomly; and, (2)
frequently linked (MFL) algorithm, described section 3 (see equation (33)).
report (see row Google) accuracy obtained using trivial ranker simply returns
751

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

candidates order defined corpus (that is,
order returned Google, correct candidate end found
Google)5 .
Cucerzan
Approach
Random
MFL
Google
RelFA
RelFM

RelB

RelB
P IFA
P IFM

P IB

P IB
simcos
simR
indegree
outdegree
NB
PR
PPR
RWP
RWF

Wiki.

News

Wiki

Random

0.133
0.700
0.844
0.568
0.416
0.716
0.576
0.204
0.140
0.272
0.192
0.440
0.760
0.700
0.532
0.772
0.684
0.692
0.776
0.760

0.149
0.630
0.883
0.458
0.406
0.651
0.542
0.233
0.229
0.321
0.285
0.486
0.687
0.647
0.462
0.719
0.623
0.687
0.663
0.715

0.153
0.571
0.795
0.489
0.365
0.717
0.597
0.289
0.252
0.421
0.383
0.483
0.687
0.581
0.327
0.729
0.565
0.697
0.647
0.643

TAC
Wikinews
0.155
0.676
0.914
0.436
0.338
0.694
0.526
0.206
0.174
0.318
0.274
0.516
0.734
0.670
0.472
0.752
0.668
0.702
0.752
0.744

2010
0.118
0.668
0.748
0.495
0.227
0.738
0.418
0.081
0.055
0.237
0.167
0.421
0.725
0.671
0.579
0.766
0.635
0.553
0.732
0.740

Overall
0.137
0.650
0.813
0.486
0.313
0.715
0.503
0.174
0.144
0.301
0.245
0.461
0.719
0.653
0.491
0.752
0.631
0.639
0.717
0.721

Confidence
Interval
(Overall)
(0.124, 0.151)
(0.631, 0.668)
(0.798, 0.828)
(0.466, 0.505)
(0.295, 0.331)
(0.696, 0.732)
(0.483, 0.522)
(0.160, 0.189)
(0.130, 0.157)
(0.283, 0.319)
(0.228, 0.262)
(0.441, 0.480)
(0.701, 0.736)
(0.634, 0.671)
(0.471, 0.510)
(0.734, 0.768)
(0.612, 0.650)
(0.619, 0.657)
(0.698, 0.734)
(0.703, 0.738)

Table 1: Accuracy obtained different approaches evaluation corpora.
Figure 4 shows DCG@K different values K Overall aggregated corpus.
MRR values different evaluation corpora reported Table 2, where, again,
highlighted boldface best MRR among link-based evaluated approaches
particular corpus. Furthermore, order provide detailed idea
differences among methods, show Figure 5 percentage queries
correct candidate ranked position K (with K 1 10) algorithm.
provide empirical results run-time different algorithms. particular, average run-time per query (in seconds) measured Linux
2.6.32, Intel Core i7 2.80GHz PC 16GB RAM one second approaches except RWF P P R, run closer 4 571 seconds per query respectively. relatively large response time P P R due fact algorithm uses
context information personalize PageRank damping vector. Taking account that,
general, query different context, means need run PageRank
5. Note Google case, queries correct candidate appended result set
accounted errors computing accuracy.

752

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Approach
Random
MFL
Google
RelFA
RelFM

RelB

RelB
P IFA
P IFM

P IB

P IB
simcos
simR
indegree
outdegree
NB
PR
PPR
RWP
RWF

Cucerzan
News
Wiki
0.347 0.370
0.806 0.763
0.894 0.922
0.727 0.651
0.607 0.600
0.830 0.787
0.727 0.710
0.428 0.458
0.356 0.440
0.500 0.543
0.415 0.502
0.650 0.671
0.850 0.813
0.806 0.773
0.700 0.648
0.859 0.834
0.799
0.76
0.812 0.801
0.864 0.796
0.854 0.821

Wiki.
Random
0.374
0.726
0.858
0.667
0.573
0.830
0.746
0.501
0.463
0.622
0.570
0.666
0.805
0.729
0.550
0.832
0.719
0.810
0.778
0.774

Wikinews
0.379
0.794
0.939
0.635
0.548
0.820
0.705
0.435
0.403
0.547
0.503
0.699
0.837
0.793
0.655
0.848
0.786
0.810
0.847
0.837

TAC
2010
0.326
0.791
0.828
0.688
0.473
0.847
0.643
0.318
0.271
0.490
0.414
0.620
0.838
0.794
0.730
0.861
0.774
0.734
0.837
0.836

Overall
0.353
0.778
0.872
0.673
0.534
0.831
0.691
0.403
0.361
0.534
0.472
0.653
0.830
0.780
0.668
0.850
0.767
0.778
0.826
0.824

Table 2: MRR obtained different approaches evaluation corpora.
computation whole Wikipedia graph query corpus, process
time consuming6 . Note, however, used Python implementation
optimized and, thus, results provided reference.
contextualize results reported Table 1, indicate TAC 2010
corpus using evaluation practically equivalent (apart 8 queries
removed due lack context information, indicated section 4.1.1) Non-NIL
queries TAC 2010 dataset. Due this, results reported column TAC 2010
Table 1 roughly compared (less 1% error) TAC 2010 Non-NIL
accuracy reported papers state art. instance, best performing
approach TAC 2010 (Lehmann et al., 2010) reported accuracy Non-NIL queries
80.6%. Note, however, cautious comparisons, results
reporting would equivalent obtained end-to-end system using
ideal candidate search stage (we always append correct candidate) without
candidate selection process (we report results candidate ranking stage).
Analyzing results reported Table 1, first conclusion may drawn
overall accuracy achieved using Google ranking better obtained
evaluated approaches. However, observe results obtained
6. According Bianchini, Gori, Scarselli (2005), computation depends linearly number
edges Wikipedia graph.

753

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

Algorithms
RelAF

0.75

RelM
F
RelAB
RelM
B
PMIAF

DCG@K

PMIM
F
PMIAB
PMIM
B
simCOS

0.50

simR
indegree
outdegree
NB
PR
PPR
RWP
RWF

0.25

1

2

3

4

5

6

7

8

9

10

K

Figure 4: DCG@K values different algorithms considered evaluated
Overall corpus generated aggregating queries.

individual corpus, note using Google always best approach.
particular, accuracy N B TAC 2010 corpus slightly better achieved
Google.
interpret findings, take account previous work area
(notably Chang et al., 2010) already pointed using Google produces
relatively good results entity linking task (accuracy near 78% TAC 2009 experimental setup). sense, overall performance obtained Google completely
unexpected. degradation performance TAC 2010 case partially explained
754

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

100%

90%

80%
Position (K)

Percentage queries

70%

10
9

60%

8
7
6

50%

5
4

40%

3
2

30%

1

20%

10%

RWF

RWP

PPR

PR

NB

outdegree

indegree

simR

simCOS

PMIM
B

PMIAB

PMIM
F

PMIAF

RelM
B

RelAB

RelM
F

RelAF

0%

Algorithms

Figure 5: Percentage queries correct candidate ranked position K (with
K 1 10) different algorithms compared.

755

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

fact corpus specifically built entity linking task using careful
targeted process7 . Due this, queries TAC 2010 corpus expected
challenging. instance, common case8 within corpus groups queries
sharing anchor linked, different correct answers depending
particular query. case, queries share anchor, share Google
ranking and, thus, top ranked candidate but, correct answer changes
queries, using always Google top ranked candidate answer introduces errors.
Note using Google taking advantage context information,
expected valuable decide best link anchor, especially
queries challenging.
second conclusion naive popularity metrics indegree (or F L,
whose performance similar indegree) produce reasonably accurate results.
aspect consistent previous work state art (Ji & Grishman, 2011)
authors indicate naive candidate ranking approaches based web popularity
achieve accuracies around 71% TAC 2010 corpus.
Another aspect highlighted that, consistently across evaluation corpora,
indegree metrics produces better accuracy outdegree, indicates
number backlinks offers better representation popularity Wikipedia article
number outgoing links. One aspect may explain, least partially,
difference performance fact number outgoing links may high due
several reasons. Wikipedia article long (which indicates received
extensive attention Wikipedia contributors is, sense, popular) expect
links shorter articles. However, cases
Wikipedia article contain many outgoing links. case, instance, articles
represent hub links, List articles.
test hypothesis outdegree metrics introduces bias favor hubs
List articles, compared number queries candidate ranked
top indegree outdegree list (its title starts List ) different corpora.
results shown Table 3, x, proportion queries candidate
ranked top list, whereas proportion queries candidate
ranked top list correct answer. is:
Queries top ranked candidate list
Total number queries corpus

(38)

Queries top ranked candidate list correct
Queries top ranked candidate list

(39)

x=

y=

seen Table 3, outdegree metrics ranks list pages frequently
top indegree metrics. seen that, cases,
candidate ranked top list, correct answer. particularity explains
significant part difference overall results indegree outdegree.
7. indicated TAC KBP 2010 task definition document, available at:
http://www.it.uc3m.es/berto/link-to-wikipedia/survey/KBP2010 TaskDefinition.pdf (April, 2014)
8. identified total 144 queries (approximately 14%) following pattern.

756

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Approach
Name
Param
x
indegree

x
outdegree


Cucerzan
News Wiki
0.004 0.004
1.0
1.0
0.06 0.088
1.0
1.0

Wiki.
Random
0.022
0.818
0.138
0.956

Wikinews
0.006
0.667
0.078
0.974

TAC
2010
0.014
1.0
0.049
1.0

Overall
0.012
0.903
0.078
0.979

Table 3: Comparison indegree outdegree regarding tendency rank
Wikipedia list top.

difference performance using backlinks forward links
noticed similarity metrics, approaches relying backlink information
, RelM , P , P ) produce better results corresponding metrics work(RelB
B
B
B
ing forward links (RelFA , RelFM , P IFA , P IFM ).
According results Table 1, pointed that, among linkbased approaches evaluated, taking advantage context information is, general,
beneficial. support conclusion compare results F L N B. Note
N B uses prior P (ci ) (see equations (22) (23)), basically normalized
version F L. However, N B combines prior probabilities P (fj /ci ),
capture context information. seen Tables 1 2, result combination N B produces better results F L. Note none alternatives
use popularity information included among top-5 link-based evaluated
). However, using context
approaches higher accuracy (N B, RWF , simR , RWP , RelB
information sufficient condition ensure good performance, reflected
results PMI variants.
Another conclusion reached that, cases relatedness PMI
metrics, averaging pairwise similarities candidate articles F (dc )
, P , RelA P ) produces, consistently across corpora, better
(as done RelB
B
F
F
, P , RelM P ).
accuracy relying maximum (as done RelB
B
F
F
possible explanation result relying maximum similarity
take account one elements F (dc ) (the one maximizes similarity)
represent semantics document dc , whereas, averaging, elements
F (dc ) contribute final similarity value. reasonable think set
forward links dc provides accurate representation semantics context
document single link document.

objective testing intuition, implemented two new variants RelB
, name RelM (P ) P (P ). order obtain RelM (P )(c , )
P IB
c
B
B
B
scores compute simRB (ci , fj ) fj F (dc ) equation (7). However, instead
selecting maximum value, done equation (7), select certain percentage
P top values average them. instance, |F (dc )| = 10 P = 50%,
select top 5 simRB (ci , fj ) values average them. Note that, approach,
. obtain
P = 100% scores would equivalent obtained RelB
(P )(c , ) scores proceed similar way, using P (c , f ) values
P IB
c
B j
(P )
(see equation (11)) instead simRB (ci , fj ) ones. evaluated RelB
757

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

0.7

RelAB

Accuracy

0.6

RelM
B
0.5

Algorithms
PMIM
B ( P)
RelM
B ( P)

0.4

PMIAB
0.3

PMIM
B

10%

30%

50%

70%

90%

Percentage
(P ) P (P ) approaches different values
Figure 6: Accuracy values RelB
B
percentage P evaluated Overall corpus generated aggregating
queries.

(P ) variants overall aggregated corpus different values percentage P .
P IB
Figure 6 reports accuracy obtained new variants. included
, P , RelA
references horizontal lines representing overall accuracy RelB
B
B

P IB . seen, increasing context information improves results.
related relatedness PMI metrics fact results obtained
PMI variants quite poor compared equivalent relatedness variants.
(0.715) P (0.301).
example, see difference overall accuracy RelB
B

758

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

inspection results P revealed that, using absolute values instead
logarithmic values (as relatedness), PMI sensitive outliers. order
verify quantify observation, decided compare results PMI two
alternatives:
implemented logarithmically smoothed version averaging PMI variants
adapting equations (8) (9) follows:

logP IFA (ci , dc ) =

logP IB
(ci , dc ) =

1
|F (dc )|
1
|F (dc )|

X

log[PF (ci , fj )]

(40)

X

log[PB (ci , fj )]

(41)

fj F (dc )

fj F (dc )

used symmetric conditional probability (SCP), introduced da Silva
Lopes (1999). SCP two Wikipedia documents wi , wj computed as:
SB (wi , wj ) =

|B(wi ) B(wj )|2
|B(wi )||B(wj )|

(42)

adapted use forward links as:
SF (wi , wj ) =

|F (wi ) F (wj )|2
|F (wi )||F (wj )|

(43)

Using equations (42) (43) following two metrics implemented:
SCPFA (ci , dc ) =
SCPBA (ci , dc ) =

1
|F (dc )|
1
|F (dc )|

X

[SF (ci , fj )]

X

[SB (ci , fj )]

(44)

fj F (dc )

(45)

fj F (dc )

run approaches corpora, report results Table 4 (accuracies)
Table 5 (MRR). seen comparing results Table 4
Table 1, significant increase performance achieved using
P IFA P IB
logarithmically smoothed version P I. seen accuracies reported
similar
SCPFA SCPBA better P IFA P IB
, respectively.
results relatedness variants RelFA RelB
4.2.2 Combining Individual Approaches
want explore possibility combining results different link-based
approaches test whether better results obtained not. approach
follow combine alternatives described section 3 based supervised machine
learning techniques. particular, use learning rank (Joachims, 2002) method.
759

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

Cucerzan
Approach
logP IFA

logP IB
SCPFA
SCPBA

Wiki.

News

Wiki

Random

0.392
0.552
0.500
0.728

0.357
0.550
0.454
0.634

0.423
0.669
0.441
0.693

TAC
Wikinews
0.370
0.590
0.404
0.658

2010
0.396
0.674
0.286
0.550

Overall
0.392
0.632
0.378
0.626

Confidence
Interval
(Overall)
(0.372, 0.411)
(0.612, 0.650)
(0.359, 0.397)
(0.607, 0.645)

Table 4: Accuracy obtained logarithmically smoothed P variants SCP based metrics evaluation corpora.

Approach
logP IFA

logP IB

SCPF
SCPBA

Cucerzan
News Wiki
0.604 0.574
0.735 0.714
0.681 0.648
0.835 0.788

Wiki.
Random
0.623
0.800
0.637
0.815

Wikinews
0.580
0.752
0.610
0.797

TAC
2010
0.607
0.805
0.545
0.741

Overall
0.601
0.778
0.600
0.781

Table 5: MRR obtained logarithmically smoothed P variants SCP -based
metrics evaluation corpora.

Though several learning rank algorithms available state art (Liu, 2009),
decided rely ListN et method described Cao et al. (2007). decision
backed results reported Chen Ji (2011), several alternatives
evaluated compared context entity linking problem. particular,
took advantage open source implementation ListN et provided University
Massachusetts RankLib package (Van B. Dang, 2014).
Basically, use scores returned individual approaches section 3 features
taken account ListN et algorithm. values features normalized
range [0, 1] avoid bias might favor them.
tested three different combinations approaches. first variant (that
name ListN etAll ) combines link-based approaches evaluation (that is,
algorithms included Table 1 except Google naive references F L
Random). second variant (ListN etT op ) combines top-5 best performing linkbased algorithms evaluation (according Overall accuracy Table 1, is, N B,
). Finally, third case (ListN et
RWF , simR , RWP , RelB
op+Google ) combines top5 best performing link-based algorithms Google baseline. cases,
used configuration parameters ListN et suggested RankLib
implementation (1500 epochs learning rate 105 ).
order report accuracy, MRR DCG@K ListN et variants, use
results obtained averaging 10 repetitions 10-fold cross validation particular
corpus analyzed. Table 6 reports accuracy different corpora combinations considered, Table 7 reports MRR results
760

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Approach
ListN etAll
ListN etT op
ListN etT op+Google

Cucerzan
News Wiki
0.780 0.716
0.824 0.786
0.854 0.864

Wiki.
Random
0.742
0.769
0.850

Wikinews
0.738
0.803
0.877

TAC
2010
0.678
0.793
0.816

Overall
0.705
0.797
0.846

Table 6: Accuracy obtained combining approaches section 3 ListN et
evaluation corpora.

Approach
ListN etAll
ListN etT op
ListN etT op+Google

Cucerzan
News Wiki
0.867 0.834
0.888 0.879
0.916 0.930

Wiki.
Random
0.847
0.865
0.916

Wikinews
0.848
0.885
0.929

TAC
2010
0.812
0.882
0.894

Overall
0.828
0.882
0.913

Table 7: MRR obtained combining approaches section 3 ListN et
evaluation corpora.

combinations. Figure 7 compares DCG@K achieved ListN et variants Overall
case top-5 link-based evaluated approaches.
compare accuracy values reported Table 6 Table 1.
overall case, best result obtained ListN etT op+Google . ListN etT op combination
shows lower performance Google reference, outperforms N B (the best
individual algorithms comparison). Regarding ListN etAll variant, accuracy
lower obtained Google N B. Similar conclusions reached
Figure 7 DCG@K metrics overall case. conclusions suggest
particular combinations features positive impact results.
However, cautious results, because, indicated Vanwinckelen (2012), repeated cross validation assumed provide perfectly precise
estimates models predictive accuracy. fact, Vanwinckelen (2012) recommend reporting confidence intervals making significance claims repeated cross validation. report that, though popular among researchers, practice contribute
misleading interpretations.
4.2.3 Effect Changes Search Stage
indicated section 4.1.1, order isolate results candidate ranking algorithms evaluated potential bad performance particular candidate search
implementation, would need rely ideal candidate search stage, sense
always returns correct answer among candidate set. Obviously,
exist ideal candidate searcher. Thus, practice, try mimic behavior,
relied state art search engine (Google) appended correct answer
candidate set case found search engine.
761

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

0.90

Algorithms
RelAB

0.85

simR

DCG@K

NB
RWP
RWF
ListNetAll

0.80

ListNetTop
ListNetTop+Google

0.75

0.70
1

2

3

4

5

6

7

8

9

10

K

Figure 7: DCG@K values top-5 link-based evaluated approaches ListN et variants evaluated Overall corpus generated aggregating
queries.

762

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

However, interested evaluating impact results achieved
different algorithms aforementioned conditions change restrictive
setup. order so, proceeded follows:
used information retrieval library Apache Lucene (Apache Software Foundation, 2014) build index titles DBpedia 3.8 pages. title
processed StandardAnalyzer Lucene.
1012 queries TAC 2010 corpus carried following
process:
anchor query used search Lucene index candidates, C(dc , a). result set limited top-10 entries, previous experiments. However, contrary previous experiments,
Lucene return correct answer within result set, append
it.
documents TAC 2010 corpus contain context links,
automatically generated using similar approach one described
section 4.1.1: named entities obtained context documents using
Stanford NER resolved links querying Lucene text
named entity assigning link destination top result search
engine (as usual, filtering links articles included within
candidate set).
Using aforementioned procedure built new version TAC 2010 corpus
annotated Lucene. Thus, two variants TAC 2010:
TAC 2010 Google, Google used candidate searcher correct
candidate appended Google results set case found.
version used experiments previous sections.
TAC 2010 Lucene, version built following procedure described
section.
run evaluated approaches, well references Random F L,
TAC 2010 Lucene corpus. accuracies achieved different algorithms
95% confidence intervals shown Figure 8. ease comparison, depicted
figure accuracies TAC 2010 Google corpus. included
(with name Search) accuracy achieved candidate ranking provided
search engine (either Google Lucene) directly used.
first aspect noted results reported Figure 8 that, surprisingly,
accuracies obtained different approaches using Lucene search are, general,
lower. Note Lucene case including correct candidate
candidates set. Thus, many queries (363 cases, almost 36% total queries)
impossible candidate rankers rank correct candidate top.
Another issue highlighted that, look top-5 best performing link-based
, greatly reduced
approaches Table 1: N B, RWF , simR , RWP , RelB
763

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

0.8

Accuracy

0.6

Case
Google

0.4

Lucene

0.2

RWF

RWP

PPR

PR

NB

indegree

outdegree

simR

simCOS

PMIM
B

PMIAB

PMIM
F

PMIAF

RelM
B

RelAB

RelM
F

RelAF

Search

MFL

Random

0.0

Algorithms

Figure 8: Accuracy obtained different approaches TAC 2010 Google TAC
2010 Lucene corpora.

performance TAC 2010 Lucene corpus. fact, though N B top performing
corpus, statistically significant difference popularity approaches
indegree PageRank (P R). possible explanation result TAC 2010
corpus context information automatically generated search engine
supervised. Thus, expect noisy. noise affects N B, RWF , simR ,
, rely context information take decisions, impact
RWP , RelB
indegree P R, rely context information. Note that, though noise
context information affects cases Google Lucene used search
764

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

engines, better performance Google (note results Search) makes much worse
Lucene case.

5. Related Work
foundations link-to-Wikipedia task found two different research communities. First, area related traditional computational linguistics tasks
cross-document co-reference resolution (Bagga & Baldwin, 1998), word sense disambiguation (Navigli, 2009). main difference respect traditional tasks
Wikipedia used source knowledge instead lexicons WordNet (Miller,
1995) typically used former work (see instance Li, Szpakowicz, & Matwin, 1995).
Second, link-to-Wikipedia task related link prediction task link mining (Getoor & Diehl, 2005), though case goal mainly decide whether two
objects (for instance, two actors social network, actor event) linked
not, instead finding best link destination Wikipedia particular anchor
text document.
Traditionally, works Bunescu Pasca (2006) Cucerzan (2007)
considered seminal area. Since publication papers problem
linking anchors text document Wikipedia articles addressed several
works, referenced section 3.
Two initiatives especially relevant sense: Knowledge Base Population (KBP) track Text Analysis Conference (TAC), Link-the-Wiki track
Initiative Evaluation XML retrieval (INEX). initiatives share goal:
offer common environment (corpora, performance metrics, etc) allow fair comparative evaluation different techniques and, thus, foster area research. However,
indicated introductory section, approach link-to-Wikipedia task slight
differences. case KBP, final aim automatically populate Knowledge
Base (KB) built Wikipedia information named entities. Thus, linkto-Wikipedia variant (named entity linking) focused entities covers case
good Wikipedia target exists link, case indicates need add
new entry KB. case Link-the-Wiki INEX track, focus set
keeping links date rich dynamic hypermedia document collection (such
wiki). Therefore, link-to-Wikipedia variant (wikification) covers common terms
named entities anchors linked, pay special attention case
good Wikipedia target exists, case link needs created.
cases, overview papers published organizers events (Huang
et al., 2008, 2009, 2010; Ji et al., 2010, 2011; Ji & Grishman, 2011) offer good source
references area. However, comparisons provided works refer
systems taking part TAC/KBP INEX, external work. Furthermore,
indicated introductory section, link-to-Wikipedia systems combine, general,
variety techniques features different types (based text, links, etc.) address
task. results reported overviews refer usually full systems,
difficult analyze compare performance individual techniques part
systems. goal work analysis comparison link-based
techniques.
765

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

surveys related task link-to-Wikipedia and, thus, relevant purposes paper Navigli Lapata (2010), Chen Ji (2011), Hachey
et al. (2013).
Chen Ji (2011) evaluate several supervised candidate rankers named entity
linking, compare reference unsupervised approaches: naive algorithm
three different similarity metrics based textual features. main goal comparison
assess machine learning mechanism (maximum entropy, SVM, SV rank
ListNet) top performing. Thus, results reported Chen Ji (2011)
paper complementary, because, indicated section 4.2.2, different
approaches analyzed used features supervised systems. Obviously,
requires know supervised techniques work better (Chen & Ji, 2011),
know link-based techniques better, goal paper.
Hachey et al. (2013) re-implement compare three different named entity linking
systems state art. However, main goal work different ours,
aim Hachey et al. (2013) analyze impact candidate searching
candidate ranking stages final performance entity linking system.
Navigli Lapata (2010) compare several metrics based graph connectivity, including considered paper, PageRank indegree. However,
work different scope ours: centered different task (word sense
disambiguation), uses different data source (WordNet).
knowledge authors, previous overview comparison different linkbased approaches candidate ranking link-to-Wikipedia systems, proposed
paper, available time writing.

6. Conclusions Future Lines
paper presented overview link-based approaches candidate ranking
link-to-Wikipedia systems. Apart overview, comparative analysis
different approaches carried out. structured analysis three parts:
first part devoted compare performance individual approaches
according three metrics (accuracy, DCG@K MRR) five different corpora
(Cucerzan news, Cucerzan Wikipedia, random Wikipedia articles, random Wikinews
articles TAC 2010). results part analysis indicate that, though
naive approaches based popularity candidates perform reasonably
well, taking advantage context information is, general, beneficial linkbased approaches. found using information backlinks
obtain better results using forward links techniques.
second part analysis combined different approaches using
ListNet. main conclusion part that, according results obtained,
combining algorithms produce positive effects performance.
Finally, third part analysis devoted evaluate impact
candidate search stage candidate ranking results, impact found
significant.
766

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Regarding potential future lines development work described paper,
first aspect consider evaluate impact quality context links
performance algorithms. want analyze effect ignoring links
might introducing noise ranking process, Lists. opposite case,
interested measuring impact including links pages namespaces,
Categories, considered paper. sense, taking Categories
account open door use semantic relatedness measures based
information, described Ponzetto Strube (2007).
According results paper, using ListNet combine algorithms produce
positive effects performance cases. However, exhaustive analysis different
combinations carried out. Thus, another potential line development could
exploring combinations algorithms, either taking advantage proposals
mechanisms feature selection learning rank (Geng, Liu, Qin, & Li, 2007)
empirically.
analyzed different algorithms perspective performance
link-to-Wikipedia task. computational complexity aspects addressed.
exhaustive analysis different algorithms along line left future work.
suggested section 4.1.3, link-to-Wikipedia systems integrated content
production workflows, interact human supervisors. Assessing
impact human factor final performance systems constitute
area future research.
Finally, though paper centered attention candidate ranking
stage, link-to-Wikipedia systems usually include processing stages: identifying
anchors linked, searching candidate links anchors, deciding whether
link suggested (detect NIL answers). end-to-end evaluation including
additional processing stages interesting line continue work reported
paper.

Acknowledgements
memoriam Concepcion Garca Alonso (1943-2012) passengers passed
away Angrois railway accident (24/Jul/2013).

References
Adibi, J., Cohen, P. R., & Morrison, C. T. (2004). Measuring confidence intervals link
discovery: bootstrap approach. Proceedings ACM Special Interest Group
Knowledge Discovery Data Mining (ACM-SIGKDD-04.
Apache Software Foundation (2014). Apache Lucene - Welcome Apache Lucene. Available
at: http://lucene.apache.org/.
Bagga, A., & Baldwin, B. (1998). Entity-based cross-document coreferencing using
Vector Space Model. Proceedings 17th international conference Computational linguistics - Volume 1, COLING 98, pp. 7985, Stroudsburg, PA, USA.
Association Computational Linguistics.
767

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

Bianchini, M., Gori, M., & Scarselli, F. (2005). Inside PageRank. ACM Trans. Internet
Technol., 5 (1), 92128.
Bizer, C., Lehmann, J., Kobilarov, G., Auer, S., Becker, C., Cyganiak, R., & Hellmann, S.
(2009). DBpedia - crystallization point Web Data. Web Semant., 7 (3),
154165.
Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine.
Comput. Netw. ISDN Syst., 30 (1-7), 107117.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based Measures Lexical Semantic Relatedness. Comput. Linguist., 32 (1), 1347.
Bunescu, R. C., & Pasca, M. (2006). Using Encyclopedic Knowledge Named entity
Disambiguation. Proceedings 11st Conference European Chapter
Association Computational Linguistics, EACL.
Cao, Y., Lin, C., & Zheng, G. (2011). MSRA TAC 2011: Entity Linking. Proceedings
Knowledge Base Population (KBP) track 4th Text Analysis Conference
(TAC). National Institute Standards Technololgy (NIST).
Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., & Li, H. (2007). Learning rank: pairwise
approach listwise approach. ICML 07: Proceedings 24th international
conference Machine learning, pp. 129136, New York, NY, USA. ACM.
Chang, A., Spitkovsky, V., Yeh, E., Aguirre, E., & Manning, C. (2010). Stanford-UBC Entity
Linking TAC-KBP. Proceedings Knowledge Base Population (KBP) track
3rd Text Analysis Conference (TAC).
Chen, Z., & Ji, H. (2011). Collaborative ranking: case study entity linking. Proceedings Conference Empirical Methods Natural Language Processing, EMNLP
11, pp. 771781, Stroudsburg, PA, USA. Association Computational Linguistics.
Cilibrasi, R. L., & Vitanyi, P. M. B. (2007). Google Similarity Distance. IEEE Trans.
Knowl. Data Eng., 19 (3), 370383.
Cucerzan, S. (2007). Large-Scale Named Entity Disambiguation Based Wikipedia Data.
Proceedings EMNLP-CoNLL 2007, pp. 708716.
Cucerzan, S. (2012). MSR System Entity Linking TAC 2012. Proceedings
Knowledge Base Population (KBP) track 5th Text Analysis Conference
(TAC).
da Silva, J. F., & Lopes, G. P. (1999). local maxima method fair dispersion normalization extracting multi-word units corpora. Sixth Meeting Mathematics
Language.
Dredze, M., McNamee, P., Rao, D., Gerber, A., & Finin, T. (2010). Entity disambiguation
knowledge base population. Proceedings 23rd International Conference
Computational Linguistics, COLING 10, pp. 277285, Stroudsburg, PA, USA.
Association Computational Linguistics.
Erbs, N., Zesch, T., & Gurevych, I. (2011). Link Discovery: Comprehensive Analysis.
Proceedings 2011 IEEE Fifth International Conference Semantic Computing,
ICSC 11, pp. 8386, Washington, DC, USA. IEEE Computer Society.
768

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Fader, A., Soderland, S., & Etzioni, O. (2009). Scaling Wikipedia-based Named Entity
Disambiguation Arbitrary Web Text. Proceedings WikiAI 09 - IJCAI
Workshop: User Contributed Knowledge Artificial Intelligence: Evolving Synergy, Pasadena, CA, USA.
Fahrni, A., Nastase, V., & Strube, M. (2011). HITS Cross-lingual Entity Linking System
TAC 2011: One Model Languages. Proceedings Knowledge Base
Population (KBP) track 4th Text Analysis Conference (TAC). National Institute
Standards Technololgy (NIST).
Fernandez, N., Fisteus, J., Sanchez, L., & Martin, E. (2010). WebTLab: Cooccurence
based Approach KBP 2010 Entity-Linking Task. Proceedings Knowledge
Base Population (KBP) track 3rd Text Analysis Conference (TAC). National
Institute Standards Technololgy (NIST).
Fernandez, N., Blazquez, J. M., Fisteus, J. A., Sanchez, L., Sintek, M., Bernardi, A., Fuentes,
M., Marrara, A., & Ben-Asher, Z. (2006). NEWS: Bringing Semantic Web Technologies News Agencies. Semantic Web - ISWC 2006, Vol. 4273 Lecture
Notes Computer Science, pp. 778791. Springer Berlin Heidelberg.
Finkel, J. R., Grenager, T., & Manning, C. (2005). Incorporating Non-local Information
Information Extraction Systems Gibbs Sampling. Proceedings 43nd
Annual Meeting Association Computational Linguistics (ACL 2005), pp.
363370.
Geng, X., Liu, T.-Y., Qin, T., & Li, H. (2007). Feature selection ranking. Proceedings
30th annual international ACM SIGIR conference Research development
information retrieval, SIGIR 07, pp. 407414, New York, NY, USA. ACM.
Gentile, A., Zhang, Z., Xia, L., & Iria, J. (2009). Graph-based Semantic Relatedness
Named Entity Disambiguation. Proceeding 1st International Conference
Software, Services Semantic Technologies (S3T).
Getoor, L., & Diehl, C. P. (2005). Link mining: survey. SIGKDD Explor. Newsl., 7 (2),
312.
Gracia, J., & Mena, E. (2008). Web-Based Measure Semantic Relatedness. Proceedings
9th international conference Web Information Systems Engineering, WISE
08, pp. 136150.
Guo, Y., Tang, G., Che, W., Liu, T., & Li, S. (2011). HIT Approaches Entity Linking
TAC 2011. Proceedings Knowledge Base Population (KBP) track 4th
Text Analysis Conference (TAC). National Institute Standards Technololgy
(NIST).
Hachey, B., Radford, W., & Curran, J. R. (2011). Graph-based named entity linking
Wikipedia. Proceedings 12th international conference Web information
system engineering, WISE11, pp. 213226, Berlin, Heidelberg. Springer-Verlag.
Hachey, B., Radford, W., Nothman, J., Honnibal, M., & Curran, J. R. (2013). Evaluating
Entity Linking Wikipedia. Artificial Intelligence, 194 (0), 130 150.
Han, X., & Sun, L. (2011). generative entity-mention model linking entities
knowledge base. Proceedings 49th Annual Meeting Association
769

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

Computational Linguistics: Human Language Technologies - Volume 1, HLT 11, pp.
945954, Stroudsburg, PA, USA. Association Computational Linguistics.
Han, X., Sun, L., & Zhao, J. (2011). Collective entity linking web text: graph-based
method. Proceedings 34th international ACM SIGIR conference Research
development Information Retrieval, SIGIR 11, pp. 765774, New York, NY,
USA. ACM.
Han, X., & Zhao, J. (2009). Named entity disambiguation leveraging Wikipedia semantic
knowledge. Proceedings 18th ACM conference Information knowledge
management, CIKM 09, pp. 215224, New York, NY, USA. ACM.
Haveliwala, T. H. (2003). Topic-Sensitive PageRank: Context-Sensitive Ranking Algorithm Web Search. IEEE Trans. Knowl. Data Eng., 15 (4), 784796.
Huang, D. W., Xu, Y., Trotman, A., & Geva, S. (2008). Overview INEX 2007 Link
Wiki Track. Fuhr, N., Kamps, J., Lalmas, M., & Trotman, A. (Eds.), Focused
Access XML Documents, pp. 373387. Springer-Verlag, Berlin, Heidelberg.
Huang, D. W. C., Geva, S., & Trotman, A. (2009). Overview INEX 2008 Link
Wiki Track. Geva, S., Kamps, J., & Trotman, A. (Eds.), Advances Focused
Retrieval, Vol. 5631 Lecture Notes Computer Science, pp. 314325. Springer
Berlin Heidelberg.
Huang, W., Geva, S., & Trotman, A. (2010). Overview INEX 2009 Link Wiki
Track. Geva, S., Kamps, J., & Trotman, A. (Eds.), Focused Retrieval Evaluation, Vol. 6203 Lecture Notes Computer Science, pp. 312323. Springer Berlin
Heidelberg.
INEX (2014). INEX 2014 main page. Available at:
https://inex.mmci.uni-saarland.de/.
Ji, H., & Grishman, R. (2011). Knowledge base population: Successful approaches
challenges. Proceedings 49th Annual Meeting Association Computational Linguistics (ACL), pp. 11481158.
Ji, H., Grishman, R., & Dang, H. T. (2011). Overview TAC2011 Knowledge Base
Population Track. Proceedings Knowledge Base Population (KBP) track
4th Text Analysis Conference (TAC).
Ji, H., Grishman, R., Dang, H. T., Griffitt, K., & Ellis, J. (2010). Overview TAC2010
Knowledge Base Population Track. Proceedings Knowledge Base Population
(KBP) track 3rd Text Analysis Conference (TAC).
Jimenez, M., Fernandez, N., Fisteus, J., & Sanchez, L. (2013). WikiIdRank++: extensions
improvements WikiIdRank system entity linking. International Journal
Artificial Intelligence Tools, 22 (3).
Joachims, T. (2002). Optimizing search engines using clickthrough data. Proceedings
eighth ACM SIGKDD international conference Knowledge discovery data
mining, KDD 02, pp. 133142, New York, NY, USA. ACM.
Kulkarni, S., Singh, A., Ramakrishnan, G., & Chakrabarti, S. (2009). Collective annotation
Wikipedia entities web text. Proceedings 15th ACM SIGKDD inter770

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

national conference Knowledge discovery data mining, KDD 09, pp. 457466,
New York, NY, USA. ACM.
Lehmann, J., Monahan, S., Nezda, L., Jung, A., & Shi, Y. (2010). LCC Approaches
Knowledge Base Population TAC 2010. Proceedings Knowledge Base
Population (KBP) track 3rd Text Analysis Conference (TAC).
Li, X., Szpakowicz, S., & Matwin, S. (1995). WordNet-based algorithm word sense
disambiguation. Proceedings 14th international joint conference Artificial
intelligence - Volume 2, IJCAI95, pp. 13681374, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Lin, W.-P., Snover, M., & Ji, H. (2011). Unsupervised language-independent name translation mining Wikipedia infoboxes. Proceedings First Workshop
Unsupervised Learning NLP, EMNLP 11, pp. 4352, Stroudsburg, PA, USA. Association Computational Linguistics.
Liu, T.-Y. (2009). Learning Rank Information Retrieval. Found. Trends Inf. Retr.,
3 (3), 225331.
Manning, C. D., Raghavan, P., & Schtze, H. (2008). Introduction Information Retrieval.
Cambridge University Press, New York, NY, USA.
Mihalcea, R., & Csomai, A. (2007). Wikify!: linking documents encyclopedic knowledge.
Proceedings sixteenth ACM conference Conference information
knowledge management, CIKM 07, pp. 233242, New York, NY, USA. ACM.
Miller, G. A. (1995). WordNet: lexical database English. Commun. ACM, 38 (11),
3941.
Milne, D., & Witten, I. H. (2008a). effective, low-cost measure semantic relatedness obtained Wikipedia links. Proceedings first AAAI Workshop
Wikipedia Artificial Intelligence (WIKIAI08).
Milne, D., & Witten, I. H. (2008b). Learning link Wikipedia. Proceedings
17th ACM conference Information knowledge management, CIKM 08, pp.
509518, New York, NY, USA. ACM.
MSNBC (2014). MSNBC: news, video progressive community. Available at:
http://www.msnbc.msn.com.
National Institute Standards Technology (2014a). Text Analysis Conference (TAC).
Available at: http://www.nist.gov/tac/.
National Institute Standards Technology (2014b). Text Analysis Conference (TAC)
KBP 2014 Tracks. Available at: http://www.nist.gov/tac/2014/KBP/.
Navigli, R. (2009). Word sense disambiguation: survey. ACM Comput. Surv., 41 (2),
10:110:69.
Navigli, R., & Lapata, M. (2010). Experimental Study Graph Connectivity
Unsupervised Word Sense Disambiguation. IEEE Trans. Pattern Anal. Mach. Intell.,
32 (4), 678692.
771

fiFernandez Garca, Arias Fisteus & Sanchez Fernandez

Nguyen, H., & Cao, T. (2010). Exploring Wikipedia Text Features Named Entity
Disambiguation. Nguyen, N., Le, M., & Swiatek, J. (Eds.), Intelligent Information
Database Systems, Vol. 5991 Lecture Notes Computer Science, pp. 1120.
Springer Berlin / Heidelberg.
Nothman, J., Murphy, T., & Curran, J. R. (2009). Analysing Wikipedia gold-standard
corpora NER training. Proceedings 12th Conference European
Chapter Association Computational Linguistics, EACL 09, pp. 612620,
Stroudsburg, PA, USA. Association Computational Linguistics.
Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). PageRank Citation Ranking:
Bringing Order Web. Technical report 1999-66, Stanford InfoLab.
Pilz, A. (2010). Entity Disambiguation using Link based Relations extracted
Wikipedia. First Workshop Automated Knowledge Base Construction (AKBC
2010), Grenoble, France.
Ploch, D., Hennig, L., de Luca, E. W., & Albayrak, S. (2011). DAI Approaches
TAC-KBP 2011 Entity Linking Task. Proceedings Knowledge Base Population (KBP) track 4th Text Analysis Conference (TAC). National Institute
Standards Technololgy (NIST).
Ponzetto, S. P., & Strube, M. (2007). Knowledge derived Wikipedia computing
semantic relatedness. J. Artif. Int. Res., 30 (1), 181212.
Radford, W., Hachey, B., Nothma, J., Honnibal, M., & Curran, J. (2010). CMCRC
TAC 2010: Document-level Entity Linking graph-based re-ranking. Proceedings 3rd Text Analysis Conference (TAC), National Institute Standards
Technology, NIST, Maryland, USA.
Ratinov, L., Roth, D., Downey, D., & Anderson, M. (2011). Local global algorithms
disambiguation Wikipedia. Proceedings 49th Annual Meeting
Association Computational Linguistics: Human Language Technologies - Volume
1, HLT 11, pp. 13751384, Stroudsburg, PA, USA. Association Computational
Linguistics.
Sil, A. (2013). Exploring Re-ranking Approaches Joint Named-entityrecognition
Linking. Proceedings Sixth Workshop Ph.D. Students Information
Knowledge Management, PIKM 13, pp. 1118.
Spitzer, F. (1976). Principles Random Walk (2nd Edition). Springer.
Van B. Dang (2014). RankLib (software package). Available at:
http://people.cs.umass.edu/vdang/ranklib.html.
Vanwinckelen, Gitte; Blockeel, H. (2012). estimating model accuracy repeated
cross-validation. Proceedings 21st Belgian-Dutch Conference Machine
Learning, pp. 3944.
Voorhees, E. (1999). TREC-8 Question Answering Track Report. Proceedings 8th
Text Retrieval Conference, pp. 7782.
Wikinews (2014a). Wikinews Random Page Generator. Available at:
http://en.wikinews.org/wiki/Special:Random.
772

fiEvaluation Link-Based Approaches Candidate Ranking Link-to-Wikipedia

Wikinews (2014b). Wikinews, free news source. Available at:
http://en.wikinews.org/.
Wikipedia (2014a). Wikipedia Random Page Generator. Available at:
http://en.wikipedia.org/wiki/Special:Random.
Wikipedia (2014b). Wikipedia:Namespace - Wikipedia, free encyclopedia. Available at:
http://en.wikipedia.org/wiki/Wikipedia:Namespace.
Yeh, E., Ramage, D., Manning, C. D., Aguirre, E., & Soroa, A. (2009). WikiWalk: random
walks Wikipedia semantic relatedness. Proceedings 2009 Workshop
Graph-based Methods Natural Language Processing, TextGraphs-4, pp. 4149,
Stroudsburg, PA, USA. Association Computational Linguistics.

773


