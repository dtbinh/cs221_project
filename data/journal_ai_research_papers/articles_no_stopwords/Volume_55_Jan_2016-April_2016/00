journal artificial intelligence

submitted published

distributed representation framework
cross lingual transfer parsing
jiang guo
wanxiang che

jguo ir hit edu cn
car ir hit edu cn

center social computing information retrieval
harbin institute technology
harbin heilongjiang china

david yarowsky

yarowsky jhu edu

center language speech processing
johns hopkins university
baltimore md usa

haifeng wang

wanghaifeng baidu com

baidu inc beijing china

ting liu

tliu ir hit edu cn

center social computing information retrieval
harbin institute technology
harbin heilongjiang china

abstract
investigates cross lingual transfer parsing aiming inducing dependency parsers low resource languages training data resource rich
language e g english existing model transfer approaches typically dont include lexical features transferable across languages bridge lexical feature gap
distributed feature representations composition provide two
inducing cross lingual distributed representations words map vocabularies two different languages common vector space consequently lexical features non lexical
features used model cross lingual transfer furthermore framework flexible
enough incorporate additional useful features cross lingual word clusters combined
contributions achieve average relative error reduction labeled attachment score
compared delexicalized parser trained english universal treebank transferred
three languages significantly outperforms state art delexicalized augmented projected cluster features identical data finally demonstrate
boosted minimal supervision e g annotated sentences target languages great significance practical usage

introduction
dependency parsing one long standing central natural language processing nlp goal dependency parsing induce implicit tree structures natural language
sentence following dependency grammar highly beneficial downstream
tasks question answering machine translation knowledge mining representation
majority work dependency parsing dedicated resource rich languages english chinese languages exists large scale annotated treebanks used
ai access foundation rights reserved

fig uo c yarowsky wang l iu

supervised training dependency parsers penn treebank marcus marcinkiewicz
santorini xue xia chiou palmer however languages
world even labeled training data parsing labor intensive
time consuming manually annotate treebanks languages fact given rise
range unsupervised methods klein manning transfer methods hwa resnik weinberg cabezas kolak mcdonald petrov hall linguistic
structure prediction
considering unsupervised methods fall far behind transfer methods terms
accuracy well difficulty evaluation focus transfer methods study
attempt build parsers low resource languages exploiting treebanks resource rich
languages two approaches linguistic transfer general namely data transfer
model transfer data transfer methods emphasizes creation artificial training data
used supervised training target language side appealing property
learn language specific linguistic structures effectively major drawbacks
requirement parallel data noise automatically created training data introduced
word alignment projection hand model transfer methods build
source language side used directly parsing target languages without need
creating annotated data target languages
falls latter category major obstacle transferring parsing system
one language another lexical features e g words directly transferable
across languages address challenge mcdonald et al built delexicalized parser parser non lexical features delexicalized parser makes sense pos
tag features significantly predictive unlabeled dependency parsing however labeled
dependency parsing especially semantic oriented dependencies stanford typed dependencies de marneffe et al de marneffe manning non lexical features
predictive enough tackstrom mcdonald uszkoreit proposed learn cross lingual
word clusters multilingual paralleled unlabeled data word alignments apply
clusters features semi supervised delexicalized parsing word clusters thought
kind coarse grained representations words thus partially fills gap lexical
features cross lingual learning dependency parsing
proposes novel cross lingual dependency parsing
pure distributed feature representations contrast discrete feature representations used
traditional dependency parsers distributed representations map symbolic features continuous
representation space shared across languages therefore model ability
utilize lexical non lexical features naturally specifically framework contains two
primary components
neural network dependency parser expect non linear model dependency
parsing study distributed feature representations shown effective non linear architectures linear architectures wang manning chen
manning proposed transition dependency parser neural network
architecture simple works well benchmark datasets briefly model simply replaces predictor transition dependency parser well designed neural
network classifier provide explanations merits model section
well adapt cross lingual task


fir epresentation l earning c ross l ingual ransfer parsing

cross lingual word representation learning key filling lexical feature gap
project representations features different languages common vector
space preserving translational equivalence study compare two approaches
learning cross lingual word representations section first named
robust projection second canonical correlation analysis
approaches simple implement scalable large data
another drawback model transfer methods focus universal structures across languages thus lack ability recovering target language specific
structures therefore necessary conduct target language adaptation top transferred introduce practical straightforward solution incorporating minimal
supervision target languages section
evaluate universal multilingual treebanks v mcdonald et al
case studies include transferring english en german de spanish es french
fr experiments incorporating lexical features performance cross lingual
dependency parsing improved significantly embedding cross lingual cluster features tackstrom et al achieve average relative error reduction labeled
attachment score las compared delexicalized parsers significantly outperforms delexicalized mcdonald et al augmented cluster features identical
data addition small amount labeled training data e g sentences target language side parameter adaptation minimal supervision performance
cross lingual transfer system boosted recalls language specific dependency
structures improved dramatically
original major contributions include
propose novel flexible cross lingual learning framework dependency parsing
distributed representations effectively incorporate lexical nonlexical features
present two novel effective approaches inducing cross lingual word representation
bridge lexical feature gap cross lingual dependency parsing transfer
cross lingual word cluster features effectively embedded model
leading significant additive improvements
cross lingual transfer systems easily effectively adapted
target languages minimal supervision demonstrating great potential practical usage

background
section describes necessary background crucial understanding transfer
parsing framework
article thoroughly revised extended version work guo che yarowsky wang liu
provide detailed linguistic methodological background cross lingual parsing additional extensions
primarily include experiments analysis target language adaptation minimal supervision system
made publicly available https github com jiangfeng acl clnndep



fig uo c yarowsky wang l iu

punct
root

dobj
nsubj

root


pron

amod


verb

good
adj

control
noun




figure example labeled dependency tree

dependency parsing
given input sentence x w w wn wi ith word x goal dependency
parsing build dependency tree denoted h l h n
n l l h l indicates directed arc head word wh modifier wm
dependency label l l label set figure
mainstream proposed dependency parsing described
graph transition mcdonald nivre graph
eisner mcdonald crammer pereira view parsing finding
highest scoring tree directed graph score dependency tree typically factored
scores small independent structures way factorization defines order
model complexity inference process mcdonald pereira carreras
koo collins instance first order factored dependency arcs
thus known arc factored higher order would consider expressive
substructures sibling grandchild structures transition instead aim
predict transition sequence initial parser state terminal states conditioned
parsing history yamada matsumoto nivre nivre hall nilsson
lot interest since fast linear time projective parsing incorporate
rich non local features zhang nivre
considered past simple transition parsing greedy decoding
local training accurate graph parsers globally trained use exact
inference however chen manning showed greedy transition
parsers significantly improved well designed neural network architecture considered paradigm parsing pure distributed
feature representations recently architecture improved different ways
example weiss alberti collins petrov combined neural network structured
perceptron use beam search decoding achieving state art performance dyer ballesteros ling matthews smith instead explored novel techniques learning
better representations parser states utilizing long short term memory networks lstm
work includes zhou zhang huang chen applied structured learning
beam search decoding neural network model study choose original
chen mannings architecture without losing generality build basic dependency parsing
cross lingual transfer


fir epresentation l earning c ross l ingual ransfer parsing

distributed representations nlp
recent years seen numerous attempts learning distributed representations different natural language objects morphemes words phrases sentences documents
distributed representations symbolic units embedded dense continuous lowdimensional vector space thus often referred embeddings
distributed representation attractive nlp several reasons first provides straightforward way measuring similarities natural language objects distributed
representations easily tell two words phrases documents similar semantic
even aspects simply measuring cosine distance vectors
second learned large scale unannotated data general thus highly beneficial downstream applications source alleviate data sparsity
straightforward way applying distributed representations nlp tasks fed distributed
feature representations existing supervised nlp systems augmented features semisupervised fashion turian ratinov bengio despite simplicity effectiveness
shown potential distributed representations cannot fully exploited generalized linear adopted traditional nlp systems wang manning
one remedy discretize distributed feature representations convert continuous dense low dimensional vectors traditional discrete sparse high dimensional
space studied guo che wang liu however believe non linear system
e g neural network powerful promising solution decent progress already
made paradigm nlp tasks neural sequence labeling collobert
et al dependency parsing chen manning sentence classification kim
machine translation sutskever vinyals le
third provides kind representation shared across languages tasks
even diverse modalities data resources property motivated lines multilingual representation learning klementiev et al chandar p et al hermann
blunsom multi task learning collobert weston multi modal learning srivastava salakhutdinov primary motivation work facilitates
cross lingual transfer parsing via multilingual distributed representation learning words

cross lingual dependency parsing
section first describe primary transition dependency parsing model utilizing
neural networks details cross lingual transfer
neural network architecture transition dependency parsing
section first briefly describe transition dependency parsing arc standard
parsing revisit neural network architecture transition dependency
parsing proposed chen manning
discussed section transition parsing generates dependency tree predicting transition sequence initial parser state terminal state several transition
parsing presented literature arc standard arc eager projective parsing nivre list nivre
two terminologies used interchangeably



fig uo c yarowsky wang l iu

swap nivre non projective parsing different different
transition actions take arc standard example parsing state typically known
configuration represented tuple consisting stack buffer b partially derived forest e set dependency arcs given input word sequence x w w wn
initial configuration represented w w w wn b terminal configuration w b w pseudo word indicating root whole dependency
tree denoting si ith element stack bi ith element buffer arc standard system defines three types transition actions l eft rc r
r ight rc r hift r dependency relation
r

l eft rc r extend arc
head modifier
remove stack
r

r ight rc r extend arc
head modifier
pop stack
hift move b buffer stack precondition b empty
typical greedy arc standard parsing build multi class classifier e g
support vector machines maximum entropy predicting transition action given feature vector extracted specific configuration conventional feature engineering suffers
sparsity incompleteness expensive feature computation chen manning
neural network model provides effective solution
architecture neural network dependency parsing model illustrated figure unlike high dimensional sparse discrete features used traditional parsing
neural network model apply distributed feature representations primarily three types
information extracted configuration chen mannings model word features pos
features relation features respectively study add non local features including distance features indicating distance two items valency features indicating
number children given item zhang nivre distance valency features
discretized buckets features projected embedding layer via corresponding lookup tables e embedding matrices estimated training
process complete feature templates used system shown table
feature compositions performed hidden layer via cube activation function
h g x w xw xt xr xd xv b
w weight matrix input layer hidden layer b bias vector
feature compositions important dependency parsing nlp general
researchers used cost intensive manual feature engineering design large set feature
templates however cannot cover potentially useful features lei xin zhang
barzilay jaakkola showed full feature representation derived
kronecker product multiple views features tensor model representing
tensor low rank form c andecomp parafac cp tensor decomposition kolda
bader number parameters effectively reduced thus suitable tasks
limited training data cao khudanpur
b top head element stack buffer



fir epresentation l earning c ross l ingual ransfer parsing

softmax layer




hidden layer




transition actions


hidden representation


input layer



feature extraction





words

clusters

lexical features
root

parsing configurations

stack
verb

lookup tables



pos tags





relations

distance
valency

non lexical features
good adj

buffer
control noun



nsubj
pron

figure neural network model dependency parsing cluster features introduced
section
type

feature templates
w

eswi eb


word

w
w
w
w
elc
erc
elc
erc





w
w
elc lc
erc rc





est eb


pos





elc
erc
elc
erc







elc lc
erc rc




relation

r
r
r
r
elc
erc
elc
erc





r
r
elc lc
erc rc




distance



es
es

b

valency

eslv eslv esrv

table feature templates neural network model transition dependency parsing
w c r lv rv
ep
indicates feature embeddings element position p lc
rc first child left right lc rc second child left right

indicates lexical features indicates non lexical features
suggest cube activation function g x x viewed special case
low rank tensor verification g x expanded
g w x wm xm b
wi wj wk xi xj xk b wi wj xi xj

j k

j



fig uo c yarowsky wang l iu

treat bias term b x x weight corresponding feature
combination xi xj xk wrote wi wj wk exactly rank component tensor low rank form cp tensor decomposition consequently cube activation function
implicitly derives full feature combinations fact add many features possible
input layer improve parsing accuracy section brown cluster
features readily incorporated model
composed features propagated output layer generating probabilistic distribution output labels e transition actions via softmax activation function
sof tmax w h use following objective function train model
j

n

crossent di yi
n


crossent p q cross entropy two distributions p q
crossent p q pk ln qk
k

parameters trained back propagation model typically consists
embedding matrices weights network however cases may exclude
word embedding matrix e w indicates word embeddings constrained fixed
e without updating training
cross lingual transfer
idea cross lingual transfer parser examined straightforward contrast
traditional approaches discard rich lexical features delexicalizing transferring
one language another model transferred full model trained
source language side e english
since non lexical feature pos relation distance valency embeddings directly transferable languages key component framework cross lingual learning
lexical feature embeddings e word embeddings cross lingual word embeddings
induced first learn dependency parser source language side parser
directly used parsing target language data
u niversal ependencies
discussed previously cross lingual model transfer assumes universal grammatical structures
identified multiple languages therefore evaluated test set target language
unlabeled attachment score uas labeled attachment score las performance
transfer parsing rely heavily multilingual consistency annotation schemes generally
syntactic annotation schemes differ head finding rules e g choice lexical versus functional head dependency relation labels e syntactic tagset challenging task
construct multilingual treebanks consistent annotations initial cross lingual parsing studies conll shared task datasets buchholz marsi broadly used however
inconsistencies occur head finding rules syntactic tagset across languages
made difficult evaluate cross lingual parsers


fir epresentation l earning c ross l ingual ransfer parsing

order overcome difficulties collection multilingual treebanks homogeneous syntactic dependency annotation presented recently namely universal dependency treebanks udt mcdonald et al universal annotation scheme created
harmonizing available treebanks slightly different variants stanford typed dependencies de marneffe et al along universal part speech tags petrov das mcdonald dataset greatly facilitates multilingual syntactic analysis
makes possible use las evaluation fact udt already used standard
dataset benchmarking cross lingual transfer parsing xia tiedemann
zhang barzilay duong cohn bird cook b rasooli collins
efforts towards universal dependencies include recent universal dependencies project ud hamledt zeman et al conduct experiments
udt v dataset without losing generality
p rojective vs n projective parsing
non projectivity common phenomenon multilingual dependency parsing term nonprojectivity indicates dependency tree crossing arcs often appear morphologically rich languages proposed graph transitionbased parsing produce non projective trees example arc standard
section readily extended adding swap action handle non projectivity
gives expected linear worst case n complexity nivre strategies include
list nivre adapted covington covington combination list swap choi
mccallum unfortunately systematically comparison different
literature far
study however focus projective parsing non projective
trees source language english training data consequently non projectivities target languages handled moment

cross lingual word representation learning
prior introducing approaches cross lingual word representation learning briefly review
basic model learning monolingual word embeddings constitutes subprocedure
cross lingual approaches
continuous bag words model
recent years approaches studied learning word embeddings largescale plain texts approaches generally derived called distributional hypothesis firth shall know word company keeps study consider
continuous bag words cbow model mikolov chen corrado dean imple https universaldependencies github io docs
https github com ryanmcd uni dep tb
note target languages address non projectivity pervasive specifically proportion projective trees presented training corpus respectively de es
fr



fig uo c yarowsky wang l iu

mented open source toolkit word vec basic principle cbow model predict
individual word sequence given bag context words within fixed window size
input log linear classifier model avoids non linear transformation hidden
layers hence trained high efficiency
large window size grouped words resulting word embeddings topically similar whereas small window size grouped words syntactically similar bansal gimpel livescu set window size parsing task
next introduce inducing bilingual word embeddings general expect
bilingual word embeddings preserve translational equivalences example cooking english close translation kochen german embedding space
robust alignment projection
first method inducing cross lingual word embeddings two stages first learn word
embeddings source language corpora monolingual case project
monolingual word embeddings target language word alignments
given sentence aligned parallel corpus first conduct unsupervised bidirectional word
alignment collect alignment dictionary specifically word aligned sentence pair
keep alignments conditional alignment probability exceeding threshold
discard others specifically let wit wjs ci j nt j ns
alignment dictionary ci j number times ith target word wit aligned
j th source word wjs ns nt vocabulary sizes use shorthand j
denote word pair projection formalized weighted average
embeddings translation words
ci j
v wit
v wjs


c

j
ci j ci j v w embedding w
obviously simple projection method one drawback assigns word embeddings
target language words occur word aligned data typically smaller
monolingual datasets therefore order improve robustness projection utilize
morphology inspired mechanism propagate embeddings vocabulary words oft
vocabulary oov words specifically oov word woov
extract list candidate
words similar terms edit distance levenshtein distance set averaged

vector embedding woov
formally

v woov
avg v w
w c


c ww editdist woov
w



reduce noise choose small edit distance threshold
process robust projection viewed two stage graph propagation
illustrated figure left panel embeddings first propagated source language words
target language words appear bilingual lexicons next monolingual propagation
performed obtain oov word embeddings target language edit distance metric
http code google com p word vec



fir epresentation l earning c ross l ingual ransfer parsing



source language




bilingual lexicon
weighted

target language

parallel data
wiktionary
panlex


































cca




vocabulary words
vocabulary words





figure illustration robust projection left cca right inducing cross lingual word
embeddings

canonical correlation analysis
second consider similar faruqui dyer uses cca
improve monolingual word embeddings multilingual correlation cca way measuring
linear relationship multidimensional variables two multidimensional variables
cca aims two projection matrices map original variables basis lowerdimensional correlation two variables maximized
refer readers work hardoon szedmak shawe taylor theoretical
foundations specifics cca lets treat cca black box see cca
applied inducing bilingual word embeddings suppose already two pre trained
monolingual word embeddings e g english german rn rn
first step extract one one alignment dictionary alignment dictionary
ast indicating every word translated one word vice
versa
process illustrated figure right panel denoting dimension resulting word
embeddings min first derive two projection matrices v rd w rd
respectively cca
v w cca



v w used project entire vocabulary
v

w



rn rn resulting word embeddings cross lingual task
worth trying observed slight performance degradation experimental setting



fig uo c yarowsky wang l iu

pros cons
contrary robust projection cca assigns embeddings every word monolingual vocabulary however one potential limitation cca assumes linear transformation
word embeddings difficult satisfy mean time training source language
parser cca cross lingual word embeddings constrained e w fixed
mentioned section otherwise translational equivalence broken robust projection however doesnt limitation discussion experiments
presented section
note approaches generalized lower resource languages parallel bitexts
available way dictionary readily obtained bilingual lexicon
induction approaches mann yarowsky koehn knight haghighi liang bergkirkpatrick klein online resources wiktionary panlex

experiments
section describes experiments first describe data settings used experiments
data settings
pre training word embeddings use wmt monolingual news corpora
english german spanish french combined wmt wmt monolingual news corpora got word alignment counts fast align toolkit cdec dyer
et al parallel news commentary corpora wmt combined europarl corpus english german spanish french
training neural network dependency parser set number hidden units
dimension embeddings different features shown table

dim

word


pos


label


distance


valency


cluster


table dimensions types feature embeddings
mini batch adaptive stochastic gradient descent adagrad duchi hazan singer
used optimization cca use implementation faruqui dyer

employ universal dependency treebanks udt v reliable evaluation
cross lingual dependency parsing universal multilingual treebanks annotated
universal pos tagset petrov et al contains pos tags well
universal dependencies defines dependency relations follow standard split
treebanks languages






https www wiktionary org
http panlex org
http www statmt org wmt
http www statmt org wmt
http www statmt org europarl



fir epresentation l earning c ross l ingual ransfer parsing

baseline systems
compare following systems
first baseline evaluate delexicalized transfer neural network parser
elex use non lexical features figure investigate effect
non local features distance valency delexicalized systems include
non local features referred elex basic
compare delexicalized parser presented mcdonald et al
c used perceptron transition parser beam size
along richer non local features zhang nivre implementation
framework zpar zhang clark referred c
furthermore consider strong baseline system proposed tackstrom et al
utilized cross lingual word cluster features enhance perceptron delexicalized
parser c cluster use alignment dictionary described section
induce cross lingual word clusters implement p rojected clustering described work tackstrom et al assigns target word cluster
often aligned
c wit arg max ci j c wjs k
k

j

obviously method drawback words occur alignment dictionary oov cannot assigned cluster therefore use strategy described
section likely clusters oov words instead computing average
embeddings solve argmax

arg max
c woov
k

c w k

w c




w
c weditdist woov

set constantly instead clustering model uszkoreit brants use
brown clustering induce hierarchical word clusters word represented
bit string use word cluster feature templates tackstrom et al set
number brown clusters
experimental
parsing trained development data english early stopping
table lists cross lingual transfer experiments dependency parsing table
summarizes experimental gains detailed table
first examine benefit brought non local distance valency features observed
comparison elex basic elex marginal improvements obtained de
fr significant improvements es therefore adopted features
following experiments
delexicalized system obtains slightly lower performance reported mcdonald
et al c used greedy decoding local training implementation
mcdonald et al work attains comparable performance c languages consider study cross lingual word embeddings alignment projection
cca obtain statistically significant improvements delexicalized system


fig uo c yarowsky wang l iu

elex basic
elex
p roj
p roj cluster
cca
cca cluster

unlabeled attachment score uas
en
de
es
fr
avg







labeled attachment score las
en
de
es
fr
avg







c





















c
c cluster































table cross lingual transfer dependency parsing english test dataset universal multilingual treebanks measured unlabeled attachment score uas
labeled attachment score las elex basic delexicalized model without nonlocal features distance valency denotes implementation c since
model varies different target languages cca indicates
averaged uas las

experimental contribution
p roj
vs elex
cca
vs elex
p roj
vs c
cca
vs c
p roj cluster
vs p roj
cca cluster
vs cca
c cluster vs c
p roj cluster
vs elex
cca cluster
vs elex
p roj cluster
vs c
cca cluster
vs c
p roj cluster
vs c cluster
cca cluster
vs c cluster

de es fr avg relative














table summary experimental gains detailed table absolute las gain
relative error reduction gains statistically significant malteval nilsson
nivre p

uas las interestingly notice p roj consistently outperforms cca significant
margin comparable c cluster analysis observation conducted section


fir epresentation l earning c ross l ingual ransfer parsing

type
cluster

feature templates
c
esc eb


c
c
c
c
elc si erc
elc
erc





c
c
elc lc
erc rc




table word cluster feature templates

framework flexible incorporating richer features simply embedding
continuous vectors thus embed cross lingual word cluster features model
together proposed cross lingual word embeddings cluster feature templates shown
table similar pos tag feature templates shown table significant
additive improvements obtained p roj cca embedding cluster features
compared delexicalized system relative error reduced uas
las combined system outperforms c cluster significantly
e ffect robust p rojection
since p roj induction cross lingual word clusters use edit distance measure
oov words would see affects performance parsing
intuitively higher coverage projected words test dataset promote parsing
performance verify conduct experiments settings
p roj cluster model robust projection examine effect edit distances ranging
shown table improvements observed languages
robust projection edit distance measure especially fr highest coverage gain
obtained robust projection observe slightly improvements de es
edit distance performance starts degrade gets larger reasonable since
larger edit distance increases word coverage introduces noise

simple

de

es

fr

coverage
uas
las
coverage
uas
las
coverage
uas
las






















robust











table effect robust projection














fig uo c yarowsky wang l iu

e ffect f ine uning w ord e mbeddings
another reason effectiveness p roj cca lies fine tuning word embeddings
training parser
cca viewed joint method inducing cross lingual word embeddings
training source language dependency parser cross lingual word embeddings derived
cca en word embeddings fixed otherwise translational equivalence
broken however p roj limitation word embeddings updated
non lexical feature embeddings order obtain accurate dependency parser refer
procedure fine tuning process word embeddings verify benefits fine tuning
conduct experiments see relative loss word embeddings fixed training
shown table indicates fine tuning indeed offers considerable help

de
es
fr

uas
las
uas
las
uas
las

fixed







fine tuning















table effect fine tuning word embeddings

compare existing bilingual word embeddings
section compare bilingual embeddings several previous approaches context dependency parsing best knowledge first work evaluation
bilingual word embeddings syntactic tasks
approaches consider include multi task learning klementiev et al
mtl bilingual auto encoder chandar p et al b iae bilingual compositional vector model hermann blunsom b icvm bilingual bag words
gouws et al b ilbowa
mtl b iae adopt released word embeddings directly due inefficiency
training b icvm b ilbowa run systems dataset previous
experiments summarized table
cca p roj consistently outperforms approaches languages p roj performs best inferior performance mtl b iae partly due low word coverage
example cover words universal de test treebank whereas cca
p roj covers moreover b iae b icvm b ilbowa introduce sentence level translational equivalence objectives regularizers learning bilingual word embeddings
approaches advantageous dont assume require word alignment however word toword translational equivalence cannot well preserved way
mtl embeddings normalized training
b icvm uses bilingual parallel dataset



fir epresentation l earning c ross l ingual ransfer parsing

mtl klementiev et al
b iae chandar p et al
b icvm hermann blunsom
b ilbowa gouws et al
cca
p roj

de
uas
las







es
uas







las







fr
uas
las







table comparison existing bilingual word embeddings mtl b iae use
released bilingual word embeddings

target word es

china
china

problemas


septiembre
september

p roj
india
russia
taiwan
chinese

difficulties
troubles
issues
october
august
january
december

cca
russia
indonesia
beijing
chinese

woes
troubles
dilemmas
december
july
october
june

neighboring words en
mtl
b iae
china
korea
independent india
sumitomo
chinese
malaysian
brazil
events

sanctions
greatly
conditions
highlighted
laws
scale
december
month
february
april
july
scheduled
march
november

b icvm
chinese
chinois
sino

problematic
problematical
difficulties
troubles
th


eleventh

b ilbowa
helsinki
bulgarians
constituting
market
deficiencies
situations
omissions
attentively

p
twelve


table target words spanish similar words english induced
approaches

verify assumption taking en es case study manually inspect
similar words cosine similarity english given set words spanish table
observe semantic syntactic shifting k nearest neighbors prediction b iae
b icvm b ilbowa whereas p roj cca give translational equivalent predictions
example b icvm yields adjective problematical target noun problemas b ilbowa yields
semantic related word market china general p roj robust behaving
consistently well sampled words
worth noting dont assume require bilingual parallel data cca p roj
need practice bilingual lexicon paired languages especially important
generalizing approaches lower resource languages parallel texts available


fig uo c yarowsky wang l iu

target language adaptation minimal supervision
important us distinguish linguistic structures learned via cross lingual transfer
versus learned basis monolingual information language
parsed intuitively cross lingual approaches learn common dependency structures
shared source target language however many languages
specialized language specific syntactic characteristics learned data
target language
take adjective noun order example spanish french adjectives often appears
nouns thus forming right directed arc labeled amod whereas english amod
adjectival modifier arcs mostly left directed illustrated figure another example
subject verb object order german verbs often appear end sentence v position
causes much left directed dobj direct object arcs english figure
differences clearly observed universal treebanks table shows significant
distribution divergence left directed right directed arcs dobj amod relations
treebanks different languages
relation dobj language en vs de
dobj
dobj
ratio
en



de



relation amod language en vs es fr
amod amod
ratio
en



es



fr




table distribution divergences left directed right directed arcs dobj relation en
de top amod relation en es fr bottom

amod

amod

noun

adj

noun

adj

spanish

consejo

superior

conflictos

sociales

adj

noun

adj

noun

english

superior

council

social

conflicts

amod

amod

figure reverse direction amod relation spanish english french adjectives following nouns



fir epresentation l earning c ross l ingual ransfer parsing

root
advmod

det

dobj

adv

det

noun

verb

de

endlich

den

richtigen

gefunden

en

finally

found



right man

adv

verb

det

noun

advmod

det
dobj
root

figure reverse direction dobj relation german english
therefore section investigate much cross lingual transfer model improved annotating small amount labeled training data target language side even though
building large scale treebanks low resource languages supervised learning costly annotating dependency structures small amount sentences e g difficult
still conduct experiments universal dependency treebanks provide labeled
training data multiple languages language studied de es fr incrementally
augment amount labeled sentences step adapt parameters cross lingual transfer model specific target language theoretically since target
language treebanks contain non projective trees would make sense apply non projective
e g swap target language adaptation way however w
trained scratch doesnt good performance experiments since minimally supervised data small consequently still rely arc standard
adaption process almost training source language parser described
section except word embedding matrix e w fixed rest parameters
e l v c w w b optimized augmented labeled data target language
taking equation objective function development data used process thus
simply perform parameter updating iterations
addition built another strong baseline system employs augmented labeled
training data supervised learning system utilize word embeddings brown
clusters features derived separately language
shown figure really promising p roj cluster cca cluster
systems consistently outperform delexicalized system supervised system significant margin p roj cluster cca cluster general achieve comparable performances
cca cluster slightly better
worthy noting performances p roj cluster cca cluster boosted
augmenting sentences take de example uas increased
las nearly equal effect sentences
supervised learning observation demonstrates great potential cross lingual transfer
system practical usage












g uo c yarowsky wang l iu

































uas









uas



uas































proj cluster
cca cluster
delexicalized
supervised

























































































las







las







las









proj cluster
cca cluster
delexicalized
supervised

labeled training data fr







labeled training data es





labeled training data de











proj cluster
cca cluster
delexicalized
supervised


























labeled training data de







proj cluster
cca cluster
delexicalized
supervised

proj cluster
cca cluster
delexicalized
supervised









proj cluster
cca cluster
delexicalized
supervised

















labeled training data es















labeled training data fr

figure target language adaptation incrementally augmenting labeled training data sentences fine tune cross lingual transfer model performances evaluated
uas top las bottom note points whose x coordinates represent
cross lingual transfer performance labeled training data used

analysis primary hypothesis incorporating data target language model
able learn special syntactic patterns consistent source language
verify study influence target language adaptation two special relations
dobj de amod es fr measuring precision recall changes use
target language sentences shown respectively table table
observe great improvements recall relations indicates model indeed gains
ability learning target language specific dependency structures supervision
sentences

related studies
cross lingual annotation projection method pioneered yarowsky ngai wicentowski shallow nlp tasks pos tagging ner etc later applied dependency
parsing hwa et al smith eisner zhao et al jiang et al tiedemann
work along line dedicated improving robustness syntactic pro

fir epresentation l earning c ross l ingual ransfer parsing

relation dobj language de
precision recall
proj cluster







cca cluster








table effect minimal supervision sentences dobj
relation amod language es fr
es
fr
precision recall precision recall
proj cluster












cca cluster













table effect minimal supervision sentences amod

jection alleviating noise errors introduced word alignment projection typical
approaches include soft projection li zhang chen treebank translation tiedemann agic nivre distribution transfer xia recently proposed
density driven projection rasooli collins worth mentioning remarkable
achieved annotation projection methods tiedemann rasooli collins
due large part parsers trained target language side
cross lingual model transfer learning cross lingual feature representations promising direction typical approaches include cross lingual word clustering tackstrom et al
employed baseline system projection features durrett pauls klein kozhevnikov titov derived linear projection maps target instances
source side feature representations extent similar cca xiao
guo learned cross lingual word embeddings applied mstparser linguistic
transfer inspired work sgaard et al obtained multi source unified word embeddings via inverted indexing wikipedia applied nlp tasks however
didnt significant improvements parsing nevertheless idea utilizing multisource information learning cross lingual word embeddings makes great sense recently
duong et al b utilized neural network architecture parameter sharing
parsers different languages however requires annotated treebanks
target language side makes distinct transfer parsing framework addition
representation learning attempts made integrate monolingual linguistic features parsing manually constructed universal dependency parsing rules naseem


fig uo c yarowsky wang l iu

chen barzilay johnson manually specified typological features naseem barzilay
globerson zhang barzilay
neural networks dependency parsing best knowledge mayberry miikkulainen presented first work explored neural networks
shift reduce constituent parsing used one hot feature representations henderson
used simple synchrony network predict parse decisions constituency parser
first use neural networks broad coverage penn treebank parser titov henderson applied incremental sigmoid belief networks constituent parsing garg
henderson later extended work transition dependency parsing temporal restricted boltzman machine parsers however much less scalable practice
earlier progress made deep learning parsing includes work collobert
socher et al constituent parsing stenetorp built recursive neural
networks transition dependency parsing

conclusion
proposes novel framework distributed representations cross lingual dependency parsing two proposed induction cross lingual word representations
namely robust projection cca bridge lexical feature gap
experiments cross lingual word embeddings derived
transferred parsing performance improved significantly delexicalized system
notable observation projection method performs significantly better cca additionally framework flexibly able incorporate cross lingual word cluster features
significant gains use combined system significantly outperforms delexicalized systems languages average error reduction las
significantly outperforms mcdonald et al augmented projected word
cluster features
furthermore performance cross lingual transfer system specific target language boosted minimal supervision language great
significance practical usage

acknowledgments
grateful manaal faruqui providing bilingual resources thank ryan mcdonald
pointing evaluation issue experiment thank sharon busching
proofreading anonymous reviewers insightful comments suggestions work
supported national key basic program china via grant cb
national natural science foundation china nsfc via grant
corresponding author wanxiang che e mail car ir hit edu cn

references
bansal gimpel k livescu k tailoring continuous word representations dependency parsing proceedings nd annual meeting association computa

fir epresentation l earning c ross l ingual ransfer parsing

tional linguistics short papers pp baltimore maryland association
computational linguistics
brown p f desouza p v mercer r l pietra v j lai j c class n gram
natural language computational linguistics
buchholz marsi e conll x shared task multilingual dependency parsing
proceedings tenth conference computational natural language learning conllx pp york city association computational linguistics
cao khudanpur online learning tensor space proceedings nd
annual meeting association computational linguistics long papers
pp baltimore maryland association computational linguistics
carreras x experiments higher order projective dependency parser proceedings
conll shared task session emnlp conll pp prague czech
republic association computational linguistics
chandar p lauly larochelle h khapra ravindran b raykar v c saha
autoencoder learning bilingual word representations advances
neural information processing systems pp curran associates inc
chen manning c fast accurate dependency parser neural networks
proceedings conference empirical methods natural language processing
emnlp pp doha qatar association computational linguistics
choi j mccallum transition dependency parsing selectional branching proceedings st annual meeting association computational linguistics long papers pp sofia bulgaria association computational
linguistics
collobert r deep learning efficient discriminative parsing proceedings th
international conference artificial intelligence statistics aistats pp
fort lauderdale fl usa jmlr org
collobert r weston j unified architecture natural language processing deep
neural networks multitask learning proceedings th international conference
machine learning icml pp helsinki finland acm
collobert r weston j bottou l karlen kavukcuoglu k kuksa p natural
language processing almost scratch journal machine learning

covington fundamental dependency parsing proceedings
th annual acm southeast conference pp
de marneffe c maccartney b manning c et al generating typed dependency
parses phrase structure parses proceedings fifth international conference
language resources evaluation lrec pp genoa italy european
language resources association elra
de marneffe c manning c stanford typed dependencies representation
coling proceedings workshop cross framework cross domain parser
evaluation pp manchester uk association computational linguistics


fig uo c yarowsky wang l iu

duchi j hazan e singer adaptive subgradient methods online learning
stochastic optimization journal machine learning
duong l cohn bird cook p low resource dependency parsing cross lingual
parameter sharing neural network parser proceedings rd annual meeting
association computational linguistics th international joint conference
natural language processing short papers pp beijing china
association computational linguistics
duong l cohn bird cook p b neural network model low resource universal dependency parsing proceedings conference empirical methods
natural language processing pp lisbon portugal association computational
linguistics
durrett g pauls klein syntactic transfer bilingual lexicon proceedings joint conference empirical methods natural language processing
computational natural language learning pp jeju island korea association
computational linguistics
dyer c ballesteros ling w matthews smith n transition dependency parsing stack long short term memory proceedings rd annual meeting
association computational linguistics th international joint conference
natural language processing long papers pp beijing china association computational linguistics
dyer c lopez ganitkevitch j weese j ture f blunsom p setiawan h eidelman v
resnik p cdec decoder alignment learning framework finite state
context free translation proceedings acl system demonstrations pp
uppsala sweden association computational linguistics
eisner j three probabilistic dependency parsing exploration
proceedings th conference computational linguistics pp
copenhagen denmark association computational linguistics
faruqui dyer c improving vector space word representations multilingual
correlation proceedings th conference european chapter association computational linguistics pp gothenburg sweden association
computational linguistics
firth j r synopsis linguistic theory studies linguistic analysis pp
blackwell
garg n henderson j temporal restricted boltzmann machines dependency parsing
proceedings th annual meeting association computational linguistics
human language technologies pp portland oregon usa association computational linguistics
gouws bengio corrado g bilbowa fast bilingual distributed representations
without word alignments proceedings nd international conference machine
learning icml pp lille france
guo j che w wang h liu revisiting embedding features simple semisupervised learning proceedings conference empirical methods natural


fir epresentation l earning c ross l ingual ransfer parsing

language processing emnlp pp doha qatar association computational
linguistics
guo j che w yarowsky wang h liu cross lingual dependency parsing
distributed representations proceedings rd annual meeting association computational linguistics th international joint conference natural
language processing long papers pp beijing china association
computational linguistics
haghighi liang p berg kirkpatrick klein learning bilingual lexicons
monolingual corpora proceedings acl hlt pp columbus ohio
association computational linguistics
hardoon r szedmak shawe taylor j canonical correlation analysis
overview application learning methods neural computation
henderson j discriminative training neural network statistical parser proceedings nd meeting association computational linguistics acl main
pp barcelona spain
hermann k blunsom p multilingual compositional distributed semantics proceedings nd annual meeting association computational
linguistics long papers pp baltimore maryland association computational linguistics
hwa r resnik p weinberg cabezas c kolak bootstrapping parsers via
syntactic projection across parallel texts natural language engineering
jiang w liu q lv relaxed cross lingual projection constituent syntax
proceedings conference empirical methods natural language processing
pp edinburgh scotland uk association computational linguistics
kim convolutional neural networks sentence classification proceedings
conference empirical methods natural language processing emnlp pp doha qatar association computational linguistics
klein manning c corpus induction syntactic structure dependency constituency proceedings nd meeting association computational linguistics acl main pp barcelona spain
klementiev titov bhattarai b inducing crosslingual distributed representations
words proceedings coling pp mumbai india coling
organizing committee
koehn p knight k learning translation lexicon monolingual corpora proceedings acl workshop unsupervised lexical acquisition pp philadelphia pennsylvania usa association computational linguistics
kolda g bader b w tensor decompositions applications siam review

koo collins efficient third order dependency parsers proceedings
th annual meeting association computational linguistics pp uppsala
sweden association computational linguistics


fig uo c yarowsky wang l iu

kozhevnikov titov cross lingual model transfer feature representation
projection proceedings nd annual meeting association computational
linguistics short papers pp baltimore maryland association
computational linguistics
lei xin zhang barzilay r jaakkola low rank tensors scoring
dependency structures proceedings nd annual meeting association
computational linguistics long papers pp baltimore maryland
association computational linguistics
li z zhang chen w soft cross lingual syntax projection dependency parsing proceedings coling th international conference computational
linguistics technical papers pp dublin ireland dublin city university association computational linguistics
x xia f unsupervised dependency parsing transferring distribution via
parallel guidance entropy regularization proceedings nd annual meeting
association computational linguistics long papers pp
baltimore maryland association computational linguistics
mann g yarowsky multipath translation lexicon induction via bridge languages
proceedings second meeting north american chapter association
computational linguistics language technologies naacl pp pittsburgh
pennsylvania association computational linguistics
marcus p marcinkiewicz santorini b building large annotated corpus
english penn treebank computational linguistics
mayberry r miikkulainen r sardsrn neural network shift reduce parser
proceedings sixteenth international joint conference artificial intelligence pp
morgan kaufmann publishers inc
mcdonald r crammer k pereira f online large margin training dependency
parsers proceedings rd annual meeting association computational
linguistics acl pp ann arbor michigan association computational linguistics
mcdonald r nivre j characterizing errors data driven dependency parsing
proceedings joint conference empirical methods natural
language processing computational natural language learning emnlp conll pp
prague czech republic association computational linguistics
mcdonald r nivre j quirmbach brundage goldberg das ganchev k hall k
petrov zhang h tackstrom bedini c bertomeu castello n lee j
universal dependency annotation multilingual parsing proceedings st annual
meeting association computational linguistics short papers pp
sofia bulgaria association computational linguistics
mcdonald r petrov hall k multi source transfer delexicalized dependency
parsers proceedings conference empirical methods natural language
processing pp edinburgh scotland uk association computational linguistics


fir epresentation l earning c ross l ingual ransfer parsing

mcdonald r pereira f c online learning approximate dependency parsing
proceedings st conference european chapter association computational linguistics pp trento italy association computer
linguistics
mikolov chen k corrado g dean j efficient estimation word representations
vector space international conference learning representations iclr workshop
naseem barzilay r globerson selective sharing multilingual dependency
parsing proceedings th annual meeting association computational
linguistics long papers pp jeju island korea association computational linguistics
naseem chen h barzilay r johnson universal linguistic knowledge
guide grammar induction proceedings conference empirical methods
natural language processing pp cambridge association computational linguistics
nilsson j nivre j malteval evaluation visualization tool dependency
parsing proceedings sixth international language resources evaluation
lrec pp marrakech morocco european language resources association
elra
nivre j efficient projective dependency parsing proceedings
th international workshop parsing technologies iwpt pp nancy france
association computational linguistics
nivre j incrementality deterministic dependency parsing proceedings workshop incremental parsing bringing engineering cognition together pp
barcelona spain association computational linguistics
nivre j deterministic incremental dependency parsing computational
linguistics
nivre j non projective dependency parsing expected linear time proceedings
joint conference th annual meeting acl th international joint
conference natural language processing afnlp pp suntec singapore
association computational linguistics
nivre j hall j nilsson j memory dependency parsing hlt naacl
workshop eighth conference computational natural language learning conll
pp boston massachusetts usa association computational linguistics
petrov das mcdonald r universal part speech tagset proceedings
eighth international conference language resources evaluation lrec
pp istanbul turkey european language resources association elra
rasooli collins density driven cross lingual transfer dependency parsers
proceedings conference empirical methods natural language processing pp lisbon portugal association computational linguistics
smith eisner j parser adaptation projection quasi synchronous grammar
features proceedings conference empirical methods natural language
processing pp singapore association computational linguistics


fig uo c yarowsky wang l iu

socher r bauer j manning c andrew n parsing compositional vector
grammars proceedings st annual meeting association computational
linguistics long papers pp sofia bulgaria association computational linguistics
sgaard agic v martnez alonso h plank b bohnet b johannsen inverted
indexing cross lingual nlp proceedings rd annual meeting association
computational linguistics th international joint conference natural language processing long papers pp beijing china association
computational linguistics
srivastava n salakhutdinov r r multimodal learning deep boltzmann machines advances neural information processing systems pp curran
associates inc
stenetorp p transition dependency parsing recursive neural networks deep
learning workshop nips lake tahoe nevada usa
sutskever vinyals le q v sequence sequence learning neural networks advances neural information processing systems pp curran
associates inc
tackstrom mcdonald r uszkoreit j cross lingual word clusters direct transfer
linguistic structure proceedings conference north american chapter
association computational linguistics human language technologies pp
montreal canada association computational linguistics
tiedemann j rediscovering annotation projection cross lingual parser induction
proceedings coling th international conference computational linguistics technical papers pp dublin ireland dublin city university association computational linguistics
tiedemann j cross lingual dependency parsing universal dependencies predicted
pos labels
tiedemann j agic v nivre j treebank translation cross lingual parser induction

titov henderson j fast robust multilingual dependency parsing generative
latent variable model proceedings conll shared task session emnlp conll
pp prague czech republic association computational linguistics
turian j ratinov l bengio word representations simple general method
semi supervised learning proceedings th annual meeting association
computational linguistics pp uppsala sweden association computational linguistics
uszkoreit j brants distributed word clustering large scale class language
modeling machine translation proceedings acl hlt pp columbus
ohio association computational linguistics
wang manning c effect non linear deep architecture sequence labeling
proceedings sixth international joint conference natural language processing
pp nagoya japan asian federation natural language processing


fir epresentation l earning c ross l ingual ransfer parsing

weiss alberti c collins petrov structured training neural network
transition parsing proceedings rd annual meeting association
computational linguistics th international joint conference natural language
processing long papers pp beijing china association computational linguistics
xiao guo distributed word representation learning cross lingual dependency
parsing proceedings eighteenth conference computational natural language
learning pp ann arbor michigan association computational linguistics
xue n xia f chiou f palmer penn chinese treebank phrase structure
annotation large corpus natural language engineering
yamada h matsumoto statistical dependency analysis support vector machines
proceedings th international workshop parsing technologies iwpt pp
nancy france association computational linguistics
yarowsky ngai g wicentowski r inducing multilingual text analysis tools via
robust projection across aligned corpora proceedings first international conference
human language technology pp san diego ca usa association
computational linguistics
zeman dusek marecek popel ramasamy l stepanek j zabokrtsky z
hajic j hamledt harmonized multi language dependency treebank language
resources evaluation
zhang barzilay r hierarchical low rank tensors multilingual transfer parsing
proceedings conference empirical methods natural language processing
pp lisbon portugal association computational linguistics
zhang clark syntactic processing generalized perceptron beam
search computational linguistics
zhang nivre j transition dependency parsing rich non local features
proceedings th annual meeting association computational linguistics human language technologies pp portland oregon usa association
computational linguistics
zhao h song kit c zhou g cross language dependency parsing bilingual
lexicon proceedings joint conference th annual meeting acl
th international joint conference natural language processing afnlp pp
suntec singapore association computational linguistics
zhou h zhang huang chen j neural probabilistic structured prediction
model transition dependency parsing proceedings rd annual meeting
association computational linguistics th international joint conference
natural language processing long papers pp beijing china
association computational linguistics




