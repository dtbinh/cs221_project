Journal Artificial Intelligence Research 55 (2016) 995-1023

Submitted 08/15; published 04/16

Distributed Representation-Based Framework
Cross-Lingual Transfer Parsing
Jiang Guo
Wanxiang Che

JGUO @ IR . HIT. EDU . CN
CAR @ IR . HIT. EDU . CN

Research Center Social Computing Information Retrieval
Harbin Institute Technology
Harbin, Heilongjiang, China

David Yarowsky

YAROWSKY @ JHU . EDU

Center Language Speech Processing
Johns Hopkins University
Baltimore, MD, USA

Haifeng Wang

WANGHAIFENG @ BAIDU . COM

Baidu Inc., Beijing, China

Ting Liu

TLIU @ IR . HIT. EDU . CN

Research Center Social Computing Information Retrieval
Harbin Institute Technology
Harbin, Heilongjiang, China

Abstract
paper investigates problem cross-lingual transfer parsing, aiming inducing dependency parsers low-resource languages using training data resource-rich
language (e.g., English). Existing model transfer approaches typically dont include lexical features, transferable across languages. paper, bridge lexical feature gap
using distributed feature representations composition. provide two algorithms
inducing cross-lingual distributed representations words, map vocabularies two different languages common vector space. Consequently, lexical features non-lexical
features used model cross-lingual transfer. Furthermore, framework flexible
enough incorporate additional useful features cross-lingual word clusters. combined
contributions achieve average relative error reduction 10.9% labeled attachment score
compared delexicalized parser, trained English universal treebank transferred
three languages. significantly outperforms state-of-the-art delexicalized models augmented projected cluster features identical data. Finally, demonstrate models
boosted minimal supervision (e.g., 100 annotated sentences) target languages, great significance practical usage.

1. Introduction
Dependency Parsing one long-standing central problems natural language processing (NLP). goal dependency parsing induce implicit tree structures natural language
sentence following dependency grammar, highly beneficial various downstream
tasks, question answering, machine translation knowledge mining/representation.
majority work dependency parsing dedicated resource-rich languages, English Chinese. languages, exists large-scale annotated treebanks used
2016 AI Access Foundation. rights reserved.

fiG UO , C , YAROWSKY, WANG & L IU

supervised training dependency parsers, Penn Treebank (Marcus, Marcinkiewicz,
& Santorini, 1993; Xue, Xia, Chiou, & Palmer, 2005). However, languages
world, even labeled training data parsing, labor intensive
time consuming manually annotate treebanks languages. fact given rise
range research unsupervised methods (Klein & Manning, 2004), transfer methods (Hwa, Resnik, Weinberg, Cabezas, & Kolak, 2005; McDonald, Petrov, & Hall, 2011) linguistic
structure prediction.
Considering unsupervised methods fall far behind transfer methods terms
accuracy, well difficulty evaluation, focus transfer methods study.
attempt build parsers low-resource languages exploiting treebanks resource-rich
languages. two approaches linguistic transfer general, namely data transfer
model transfer. Data transfer methods emphasizes creation artificial training data
used supervised training target language side. appealing property
learn language-specific linguistic structures effectively. major drawbacks
requirement parallel data noise automatically created training data introduced
word alignment-based projection. hand, model transfer methods build models
source language side, used directly parsing target languages without need
creating annotated data target languages.
paper falls latter category. major obstacle transferring parsing system
one language another lexical features (e.g., words) directly transferable
across languages. address challenge, McDonald et al. (2011) built delexicalized parser parser non-lexical features. delexicalized parser makes sense POS
tag features significantly predictive unlabeled dependency parsing. However, labeled
dependency parsing, especially semantic-oriented dependencies Stanford typed dependencies (De Marneffe et al., 2006; De Marneffe & Manning, 2008), non-lexical features
predictive enough. Tackstrom, McDonald, Uszkoreit (2012) proposed learn cross-lingual
word clusters multilingual paralleled unlabeled data word alignments, apply
clusters features semi-supervised delexicalized parsing. Word clusters thought
kind coarse-grained representations words. Thus, approach partially fills gap lexical
features cross-lingual learning dependency parsing.
paper proposes novel approach cross-lingual dependency parsing based
pure distributed feature representations. contrast discrete feature representations used
traditional dependency parsers, distributed representations map symbolic features continuous
representation space, shared across languages. Therefore, model ability
utilize lexical non-lexical features naturally. Specifically, framework contains two
primary components:
neural network-based dependency parser. expect non-linear model dependency
parsing study, distributed feature representations shown effective non-linear architectures linear architectures (Wang & Manning, 2013). Chen
Manning (2014) proposed transition-based dependency parser using neural network
architecture, simple works well benchmark datasets. Briefly, model simply replaces predictor transition-based dependency parser well-designed neural
network classifier. provide explanations merits model Section 3,
well adapt cross-lingual task.
996

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Cross-lingual word representation learning. key filling lexical feature gap
project representations features different languages common vector
space, preserving translational equivalence. study compare two approaches
learning cross-lingual word representations Section 4. first approach named
robust projection, second approach based canonical correlation analysis.
approaches simple implement scalable large data.
Another drawback model transfer methods focus universal structures across various languages, thus lack ability recovering target language-specific
structures. Therefore, necessary conduct target language adaptation top transferred models. introduce practical straightforward solution incorporating minimal
supervision target languages (Section 6).
evaluate models universal multilingual treebanks v2.0 (McDonald et al., 2013).
Case studies include transferring English (EN) German (DE), Spanish (ES) French
(FR). Experiments show incorporating lexical features, performance cross-lingual
dependency parsing improved significantly. embedding cross-lingual cluster features (Tackstrom et al., 2012), achieve average relative error reduction 10.9% labeled
attachment score (LAS), compared delexicalized parsers. significantly outperforms delexicalized models McDonald et al. augmented cluster features identical
data. addition, show using small amount labeled training data (e.g., 100 sentences) target language side parameter adaptation (minimal supervision), performance
cross-lingual transfer system boosted, recalls language-specific dependency
structures improved dramatically.1
original major contributions paper include:
propose novel flexible cross-lingual learning framework dependency parsing
based distributed representations, effectively incorporate lexical nonlexical features.
present two novel effective approaches inducing cross-lingual word representation
bridge lexical feature gap cross-lingual dependency parsing transfer.
show cross-lingual word cluster features effectively embedded model,
leading significant additive improvements.
show cross-lingual transfer systems easily effectively adapted
target languages minimal supervision, demonstrating great potential practical usage.

2. Background
section describes necessary background crucial understanding transfer
parsing framework.
1. article thoroughly revised extended version work Guo, Che, Yarowsky, Wang, Liu (2015).
provide detailed linguistic methodological background cross-lingual parsing. Additional extensions
primarily include experiments analysis target language adaptation minimal supervision. system
made publicly available at: https://github.com/jiangfeng1124/acl15-clnndep.

997

fiG UO , C , YAROWSKY, WANG & L IU

punct
root

dobj
nsubj

ROOT


PRON

amod


VERB

good
ADJ

control
NOUN

.
.

Figure 1: example labeled dependency tree.

2.1 Dependency Parsing
Given input sentence x = w0 w1 ...wn wi ith word x, goal dependency
parsing build dependency tree, denoted = {(h, m, l) 0 h n; 0 <
n, l L}, (h, m, l) indicates directed arc head word wh modifier wm
dependency label l, L label set (Figure 1).
mainstream models proposed dependency parsing described
either graph-based models transition-based models (McDonald & Nivre, 2007). Graph-based
models (Eisner, 1996; McDonald, Crammer, & Pereira, 2005) view parsing problem finding
highest scoring tree directed graph. score dependency tree typically factored
scores small independent structures. way factorization defines order
model complexity inference process (McDonald & Pereira, 2006; Carreras,
2007; Koo & Collins, 2010). instance, first-order models factored dependency arcs,
thus known arc-factored models. Higher-order models would consider expressive
substructures sibling grandchild structures. Transition-based models instead aim
predict transition sequence initial parser state terminal states, conditioned
parsing history (Yamada & Matsumoto, 2003; Nivre, 2003; Nivre, Hall, & Nilsson, 2004).
approach lot interest since fast (linear time projective parsing) incorporate
rich non-local features (Zhang & Nivre, 2011).
considered past simple transition-based parsing using greedy decoding
local training accurate graph-based parsers globally trained use exact
inference algorithms. However, Chen Manning (2014) showed greedy transition-based
parsers significantly improved well-designed neural network architecture. approach considered new paradigm parsing, based pure distributed
feature representations. recently, architecture improved different ways.
example, Weiss, Alberti, Collins, Petrov (2015) combined neural network structured
perceptron, use beam-search decoding, achieving new state-of-the-art performance. Dyer, Ballesteros, Ling, Matthews, Smith (2015) instead explored novel techniques learning
better representations parser states utilizing long short-term memory networks (LSTM).
work includes Zhou, Zhang, Huang, Chen (2015) applied structured learning
beam-search decoding neural network model. study, choose original
Chen & Mannings architecture, without losing generality, build basic dependency parsing
models cross-lingual transfer.
998

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

2.2 Distributed Representations NLP
Recent years seen numerous attempts learning distributed representations different natural language objects, morphemes, words phrases, sentences documents. Using
distributed representations, symbolic units embedded dense, continuous lowdimensional vector space, thus often referred embeddings.2
Distributed representation attractive NLP several reasons. First, provides straightforward way measuring similarities natural language objects. distributed
representations, easily tell two words/phrases/documents similar semantic
even aspects simply measuring cosine distance vectors.
Second, learned large-scale unannotated data general, thus highly beneficial various downstream applications source alleviate data sparsity.
straightforward way applying distributed representations NLP tasks fed distributed
feature representations existing supervised NLP systems augmented features, semisupervised fashion (Turian, Ratinov, & Bengio, 2010). Despite simplicity effectiveness,
shown potential distributed representations cannot fully exploited generalized linear models adopted traditional NLP systems (Wang & Manning,
2013). One remedy discretize distributed feature representations, convert continuous, dense low-dimensional vectors traditional discrete, sparse high-dimensional
space, studied Guo, Che, Wang, Liu (2014). However, believe non-linear system
(e.g., neural network) powerful promising solution. decent progress already
made paradigm NLP various tasks, neural sequence labeling (Collobert
et al., 2011), dependency parsing (Chen & Manning, 2014), sentence classification (Kim, 2014)
machine translation (Sutskever, Vinyals, & Le, 2014).
Third, provides kind representation shared across languages, tasks
even diverse modalities data resources. property motivated lines research multilingual representation learning (Klementiev et al., 2012; Chandar P et al., 2014; Hermann &
Blunsom, 2014), multi-task learning (Collobert & Weston, 2008) multi-modal learning (Srivastava & Salakhutdinov, 2012). primary motivation work facilitates
cross-lingual transfer parsing via multilingual distributed representation learning words.

3. Cross-Lingual Dependency Parsing
section, first describe primary transition-based dependency parsing model utilizing
neural networks, details cross-lingual transfer.
3.1 Neural Network Architecture Transition-Based Dependency Parsing
section, first briefly describe transition-based dependency parsing arc-standard
parsing algorithm. revisit neural network architecture transition-based dependency
parsing proposed Chen Manning (2014).
discussed Section 2.1, transition-based parsing generates dependency tree predicting transition sequence initial parser state terminal state. Several transition-based
parsing algorithms presented literature, arc-standard arc-eager algorithms projective parsing (Nivre, 2003, 2004), list-based algorithm (Nivre, 2008)
2. paper, two terminologies used interchangeably.

999

fiG UO , C , YAROWSKY, WANG & L IU

swap-based algorithm (Nivre, 2009) non-projective parsing. Different algorithms different
transition actions. Take arc-standard algorithm example, parsing state (typically known
configuration) represented tuple consisting stack S, buffer B, partially derived forest (i.e., set dependency arcs) A. Given input word sequence x = w1 w2 , ..., wn ,
initial configuration represented as: [w0 ]S , [w1 w2 , ..., wn ]B , , terminal configuration [w0 ]S , []B , A, w0 pseudo word indicating root whole dependency
tree. Denoting Si (i = 0, 1, ...) ith element stack, Bi (i = 0, 1, ...) ith element buffer,3 arc-standard system defines three types transition actions: L EFT-A RC(r),
R IGHT-A RC(r), HIFT, r dependency relation.
r

L EFT-A RC(r): extend new arc (S1
S0 ) (S0 head S1 modifier)
remove S1 stack.
r

R IGHT-A RC(r): extend new arc (S1
S0 ) (S1 head S0 modifier)
pop S0 stack.
HIFT: move B0 buffer stack. Precondition B empty.
typical approach greedy arc-standard parsing build multi-class classifier (e.g.,
support vector machines, maximum entropy models) predicting transition action given feature vector extracted specific configuration. conventional feature engineering suffers
problem sparsity, incompleteness expensive feature computation (Chen & Manning,
2014), neural network model provides effective solution.
architecture neural network based dependency parsing model illustrated Figure 2. Unlike high-dimensional, sparse discrete features used traditional parsing models,
neural network model, apply distributed feature representations. Primarily, three types
information extracted configuration Chen & Mannings model: word features, POS
features relation features respectively. study, add non-local features including distance features indicating distance two items, valency features indicating
number children given item (Zhang & Nivre, 2011). distance valency features
discretized buckets. features projected embedding layer via corresponding lookup tables (i.e., embedding matrices), estimated training
process. complete feature templates used system shown Table 1.
Then, feature compositions performed hidden layer via cube activation function:
h = g(x) = (W1 [xw , xt , xr , xd , xv ] + b1 )3
W1 weight matrix input layer hidden layer, b1 bias vector.
Feature compositions important dependency parsing NLP general.
Researchers used cost-intensive manual feature engineering design large set feature
templates. However, approach cannot cover potentially useful features. Lei, Xin, Zhang,
Barzilay, Jaakkola (2014) showed full feature representation derived
Kronecker product multiple views features, results tensor model. representing
tensor low-rank form using C ANDECOMP /PARAFAC (CP) tensor decomposition (Kolda &
Bader, 2009), number parameters effectively reduced, thus suitable tasks
limited training data (Cao & Khudanpur, 2014).
3. S0 /B0 top/head element stack/buffer.

1000

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Softmax Layer:
= ( )



Hidden Layer:
= = ( + )3



Transition Actions


Hidden Representation


Input Layer:

= [ , , , , , ]

Feature Extraction





Words

Clusters

Lexical features
ROOT

Parsing Configurations

Stack
has_VERB

Lookup Tables
1


POS tags



,

Relations

Distance,
Valency

Non-lexical features
good_ADJ

Buffer
Control_NOUN

._.

nsubj
He_PRON

Figure 2: Neural network model dependency parsing. Cluster features introduced
Section 5.2 5.3.
Type

Feature Templates
w
, = 0, 1, 2
ESwi , EB


Word

w
w
w
w
Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)
i)
w
w
Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

, = 0, 1, 2
ESt , EB


POS





Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)
i)


Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

Relation

r
r
r
r
Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)
i)
r
r
Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

Distance



ES
, ES
0 ,S1
0 ,B0

Valency

ESlv0 , ESlv1 , ESrv1

Table 1: Feature templates neural network model transition-based dependency parsing.
{w,c,t,r,d,lv,rv}
Ep
indicates various feature embeddings element position p. lc1
(rc1) first child left (right) lc2 (rc2) second child left (right).

indicates lexical features, indicates non-lexical features.
suggest cube activation function g(x) = x3 viewed special case
low-rank tensor. verification, g(x) expanded as:
g(w1 x1 + ... + wm xm + b) =
(wi wj wk )xi xj xk + b(wi wj )xi xj + ...

i,j,k

i,j

1001

fiG UO , C , YAROWSKY, WANG & L IU

treat bias term b x0 x0 = 1, weight corresponding feature
combination xi xj xk wrote wi wj wk , exactly rank-1 component tensor low-rank form using CP tensor decomposition. Consequently, cube activation function
implicitly derives full feature combinations. fact, add many features possible
input layer improve parsing accuracy. show Section 5.2 Brown-cluster
features readily incorporated model.
composed features propagated output layer, generating probabilistic distribution output labels (i.e., transition actions) via softmax activation function: =
sof tmax(W2 h). use following objective function train model:
J () =

1 N
2
CrossEnt(di , yi ) +
N i=0
2

CrossEnt(p, q) cross-entropy two distributions p q:
CrossEnt(p, q) = pk ln qk
k

parameters trained using back-propagation. model, typically consists
embedding matrices weights network. However, cases, may exclude
word embedding matrix E w , indicates word embeddings constrained fixed
(i.e., without updating) training.
3.2 Cross-Lingual Transfer
idea cross-lingual transfer using parser examined straightforward. contrast
traditional approaches discard rich lexical features (delexicalizing) transferring
models one language another, model transferred using full model trained
source language side (i.e., English).
Since non-lexical feature (POS, relation, distance, valency) embeddings directly transferable languages, key component framework cross-lingual learning
lexical feature embeddings (i.e., word embeddings). cross-lingual word embeddings
induced, first learn dependency parser source language side. that, parser
directly used parsing target language data.
3.2.1 U NIVERSAL EPENDENCIES
discussed previously, cross-lingual model transfer assumes universal grammatical structures
identified multiple languages. Therefore, evaluated test set target language
either unlabeled attachment score (UAS) labeled attachment score (LAS), performance
transfer parsing rely heavily multilingual consistency annotation schemes. Generally
syntactic annotation schemes differ head-finding rules (e.g., choice lexical versus functional head) dependency relation labels (i.e., syntactic tagset). challenging task
construct multilingual treebanks consistent annotations. initial cross-lingual parsing studies, CoNLL shared task datasets (Buchholz & Marsi, 2006) broadly used. However,
inconsistencies occur head-finding rules syntactic tagset across languages,
made difficult evaluate cross-lingual parsers.
1002

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

order overcome difficulties, new collection multilingual treebanks homogeneous syntactic dependency annotation presented recently, namely Universal Dependency Treebanks (UDT) (McDonald et al., 2013). universal annotation scheme created
harmonizing available treebanks slightly different variants Stanford typed dependencies (De Marneffe et al., 2006), along universal Part-of-Speech tags (Petrov, Das, & McDonald, 2012). dataset greatly facilitates research multilingual syntactic analysis,
makes possible use LAS evaluation. fact, UDT already used standard
dataset benchmarking research cross-lingual transfer parsing (Ma & Xia, 2014; Tiedemann,
2014; Zhang & Barzilay, 2015; Duong, Cohn, Bird, & Cook, 2015a, 2015b; Rasooli & Collins,
2015). efforts towards universal dependencies include recent Universal Dependencies project (UD) 4 HamleDT (Zeman et al., 2014). paper, conduct experiments
UDT (v2.0) 5 dataset without losing generality.
3.2.2 P ROJECTIVE VS . N - PROJECTIVE PARSING
Non-projectivity common phenomenon multilingual dependency parsing. term nonprojectivity indicates dependency tree crossing-arcs, often appear morphologically rich languages. Various algorithms proposed graph-based transitionbased parsing algorithms produce non-projective trees. example, arc-standard algorithm
(Section 3.1) readily extended adding swap action handle non-projectivity,
gives expected linear worst-case O(n2 ) complexity (Nivre, 2009). strategies include
list-based algorithm (Nivre, 2008) adapted Covington algorithm (Covington, 2001), combination list-based swap-based algorithm (Choi &
McCallum, 2013). Unfortunately, systematically comparison different
algorithms literature far.
study, however, focus projective parsing non-projective
trees source language (English) training data. Consequently, non-projectivities target languages handled moment.6

4. Cross-Lingual Word Representation Learning
Prior introducing approaches cross-lingual word representation learning, briefly review
basic model learning monolingual word embeddings, constitutes subprocedure
cross-lingual approaches.
4.1 Continuous Bag-of-Words Model
recent years, various approaches studied learning word embeddings largescale plain texts. approaches generally derived so-called distributional hypothesis (Firth, 1957): shall know word company keeps. study, consider
Continuous Bag-of-Words (CBOW) model (Mikolov, Chen, Corrado, & Dean, 2013) imple4. https://universaldependencies.github.io/docs/
5. https://github.com/ryanmcd/uni-dep-tb
6. Note target languages address paper, non-projectivity pervasive. Specifically, proportion projective trees presented training corpus respectively 91% DE, 94% ES, 88%
FR.

1003

fiG UO , C , YAROWSKY, WANG & L IU

mented open-source toolkit word2vec.7 basic principle CBOW model predict
individual word sequence given bag context words within fixed window size
input, using log-linear classifier. model avoids non-linear transformation hidden
layers, hence trained high efficiency.
large window size, grouped words using resulting word embeddings topically similar; whereas small window size, grouped words syntactically similar (Bansal, Gimpel, & Livescu, 2014). set window size 1 parsing task.
Next, introduce approach inducing bilingual word embeddings. general, expect
bilingual word embeddings preserve translational equivalences. example, cooking (English) close translation: kochen (German) embedding space.
4.2 Robust Alignment-Based Projection
first method inducing cross-lingual word embeddings two stages. First, learn word
embeddings source language (S) corpora monolingual case, project
monolingual word embeddings target language (T), based word alignments.
Given sentence-aligned parallel corpus D, first conduct unsupervised bidirectional word
alignment, collect alignment dictionary. Specifically, word-aligned sentence pair
D, keep alignments conditional alignment probability exceeding threshold = 0.95
discard others. Specifically, let = {(wiT , wjS , ci,j ), = 1, 2, ..., NT ; j = 1, 2, ..., NS }
alignment dictionary, ci,j number times ith target word wiT aligned
j th source word wjS . NS NT vocabulary sizes. use shorthand (i, j)
denote word pair . projection formalized weighted average
embeddings translation words:
ci,j
v(wiT ) =
v(wjS )
(1)

c
i,
(i,j)AT
ci, = j ci,j , v(w) embedding w.
Obviously, simple projection method one drawback: assigns word embeddings
target language words occur word aligned data, typically smaller
monolingual datasets. Therefore, order improve robustness projection, utilize
morphology-inspired mechanism, propagate embeddings in-vocabulary words out-ofT
vocabulary (OOV) words. Specifically, OOV word woov
, extract list candidate
words similar terms edit distance (Levenshtein distance), set averaged

vector embedding woov
. formally,

v(woov
) = Avg (v(w ))
w C


C = {ww EditDist(woov
, w) }

(2)

reduce noise, choose small edit distance threshold = 1.
process robust projection viewed two-stage graph-propagation algorithm,
illustrated Figure 3 (left panel). Embeddings first propagated source language words
target language words appear bilingual lexicons. Next, monolingual propagation
performed obtain OOV word embeddings target language, using edit distance metric.
7. http://code.google.com/p/word2vec/

1004

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

1

Source Language



1
Bilingual Lexicon
(weighted)

Target Language

Parallel data
Wiktionary
PanLex




2



1





2













2

2



CCA
1



In-Vocabulary words
Out-of-Vocabulary words





Figure 3: Illustration robust projection (left) CCA (right) inducing cross-lingual word
embeddings.

4.3 Canonical Correlation Analysis
second approach consider similar Faruqui Dyer (2014), uses CCA
improve monolingual word embeddings multilingual correlation. CCA way measuring
linear relationship multidimensional variables. two multidimensional variables,
CCA aims find two projection matrices map original variables new basis (lowerdimensional), correlation two variables maximized.
refer readers work Hardoon, Szedmak, Shawe-Taylor (2004) theoretical
foundations algorithm specifics CCA. lets treat CCA black box, see CCA
applied inducing bilingual word embeddings. Suppose already two pre-trained
monolingual word embeddings (e.g., English German): Rn1 d1 Rn2 d2 .
first step, extract one-to-one alignment dictionary alignment dictionary
AST .8 Here, , indicating every word translated one word , vice
versa.
process illustrated Figure 3 (right panel). Denoting dimension resulting word
embeddings min(d1 , d2 ). First, derive two projection matrices V Rd1 , W Rd2
respectively using CCA:
V, W = CCA( , )

(3)

Then, V W used project entire vocabulary :
= V,

= W

(4)

Rn1 Rn2 resulting word embeddings cross-lingual task.
8. worth trying, observed slight performance degradation experimental setting.

1005

fiG UO , C , YAROWSKY, WANG & L IU

4.4 Pros Cons
Contrary robust projection approach, CCA assigns embeddings every word monolingual vocabulary. However, one potential limitation CCA assumes linear transformation
word embeddings, difficult satisfy. mean time, training source language
parser using CCA cross-lingual word embeddings, constrained E w fixed,
mentioned Section 3.1, otherwise, translational equivalence broken. robust projection approach, however, doesnt limitation. discussion experiments
presented Section 5.3.2.
Note approaches generalized lower-resource languages parallel bitexts
available. way, dictionary readily obtained either using bilingual lexicon
induction approaches (Mann & Yarowsky, 2001; Koehn & Knight, 2002; Haghighi, Liang, BergKirkpatrick, & Klein, 2008), online-resources Wiktionary9 Panlex.10

5. Experiments
section describes experiments. first describe data settings used experiments, results.
5.1 Data Settings
pre-training word embeddings, use WMT-2011 monolingual news corpora
English, German Spanish.11 French, combined WMT-2011 WMT-2012 monolingual news corpora.12 got word alignment counts using fast-align toolkit cdec (Dyer
et al., 2010) parallel news commentary corpora (WMT 2006-10) combined Europarl corpus English{German, Spanish, French}.13
training neural network dependency parser, set number hidden units
400. dimension embeddings different features shown Table 2.

Dim.

Word
50

POS
50

Label
50

Distance
5

Valency
5

Cluster
8

Table 2: Dimensions various types feature embeddings.
Mini-batch adaptive stochastic gradient descent (AdaGrad) (Duchi, Hazan, & Singer, 2011)
used optimization. CCA approach, use implementation Faruqui Dyer
(2014).
employ universal dependency treebanks (UDT v2.0) reliable evaluation
approach cross-lingual dependency parsing. universal multilingual treebanks annotated
using universal POS tagset (Petrov et al., 2012) contains 12 POS tags, well
universal dependencies defines 40 dependency relations. follow standard split
treebanks languages.
9.
10.
11.
12.
13.

https://www.wiktionary.org/
http://panlex.org/
http://www.statmt.org/wmt11/
http://www.statmt.org/wmt12/
http://www.statmt.org/europarl/

1006

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

5.2 Baseline Systems
compare approach following systems.
first baseline, evaluate delexicalized transfer neural network-based parser
[D ELEX], use non-lexical features (Figure 2). investigate effect
non-local features (distance, valency). delexicalized systems include
non-local features referred [D ELEX (basic)].
compare approach delexicalized parser presented McDonald et al.
(2013) [M C D13], used perceptron-based transition-based parser beam size 8,
along richer non-local features (Zhang & Nivre, 2011). re-implementation approach
framework Zpar (Zhang & Clark, 2011) referred [M C D13 ].
Furthermore, consider strong baseline system proposed Tackstrom et al. (2012),
utilized cross-lingual word cluster features enhance perceptron-based delexicalized
parser [M C D13 +Cluster]. use alignment dictionary described Section 4.2
induce cross-lingual word clusters. re-implement P ROJECTED clustering approach described work Tackstrom et al., assigns target word cluster
often aligned:
c(wiT ) = arg max ci,j 1[c(wjS ) = k]
k

(i,j)AT

Obviously, method drawback words occur alignment dictionary (OOV) cannot assigned cluster. Therefore, use strategy described
Section 4.2 find likely clusters OOV words. Instead computing average
embeddings, solve argmax problem:

) = arg max
c(woov
k

1[c(w ) = k]

w C

(5)


, w) }
C = {wEditDist(woov

set 1 constantly. Instead clustering model Uszkoreit Brants (2008), use
Brown clustering (1992) induce hierarchical word clusters, word represented
bit-string. use word cluster feature templates Tackstrom et al. (2012), set
number Brown clusters 256.
5.3 Experimental Results
parsing models trained using development data English early-stopping.
Table 3 lists results cross-lingual transfer experiments dependency parsing. Table 4
summarizes experimental gains detailed Table 3.
first examine benefit brought non-local distance valency features. observed
comparison ELEX (basic) ELEX, marginal improvements obtained DE
FR, significant improvements ES. Therefore, adopted features
following experiments.
delexicalized system obtains slightly lower performance reported McDonald
et al. (2013) (M C D13), used greedy decoding local training. re-implementation
McDonald et al.s work attains comparable performance C D13. languages consider study, using cross-lingual word embeddings either alignment-based projection
CCA, obtain statistically significant improvements delexicalized system,
1007

fiG UO , C , YAROWSKY, WANG & L IU

ELEX (basic)
ELEX
P ROJ
P ROJ+Cluster
CCA
CCA+Cluster

Unlabeled Attachment Score (UAS)
EN
DE
ES
FR
AVG
83.63 56.85 67.28 68.70 64.28
83.67 57.01 68.05 68.85 64.64
91.96 60.07 71.42 71.36 67.62
92.33 60.35 71.90 72.93 68.39
90.62 59.42 68.87 69.58 65.96
92.03 60.66 71.33 70.87 67.62

Labeled Attachment Score (LAS)
EN
DE
ES
FR
AVG
79.37 47.06 56.43 57.73 53.74
79.42 47.12 56.99 57.78 53.96
90.48 49.94 61.76 61.55 57.75
90.91 51.54 62.28 63.12 58.98
88.88 49.32 59.65 59.50 56.16
90.49 51.29 61.69 61.50 58.16

C D13

83.33

58.50

68.07

70.14

65.57

78.54

48.11

56.86

58.20

54.39

C D13
C D13 +Cluster

84.44
90.21

57.30
60.55

68.15
70.43

69.91
72.01

65.12
67.66

80.30
88.28

47.34
50.20

57.12
60.96

58.80
61.96

54.42
57.71

Table 3: Cross-lingual transfer dependency parsing English test dataset 4 universal multilingual treebanks. Results measured unlabeled attachment score (UAS)
labeled attachment score (LAS). ELEX (basic) delexicalized model without nonlocal features (distance, valency). denotes re-implementation C D13. Since
model varies different target languages CCA-based approach, indicates
averaged UAS/LAS.

Experimental Contribution
P ROJ
vs. ELEX
CCA
vs. ELEX
P ROJ
vs. C D13
CCA
vs. C D13
P ROJ+Cluster
vs. P ROJ
CCA+Cluster
vs. CCA
C D13 +Cluster vs. C D13
P ROJ+Cluster
vs. ELEX
CCA+Cluster
vs. ELEX
P ROJ+Cluster
vs. C D13
CCA+Cluster
vs. C D13
P ROJ+Cluster
vs. C D13 +Cluster
CCA+Cluster
vs. C D13 +Cluster

DE/ES/FR Avg. (Relative)
+3.79 (8.2%)
+2.19 (4.8%)
+3.33 (7.3%)
+1.74 (3.8%)
+1.23 (2.9%)
+2.00 (4.6%)
+3.29 (7.2%)
+5.02 (10.9%)
+4.20 (9.1%)
+4.46 (9.8%)
+3.74 (8.2%)
+1.27 (3.0%)
+0.45 (1.1%)

Table 4: Summary experimental gains detailed Table 3, absolute LAS gain
relative error reduction. gains statistically significant using MaltEval (Nilsson
& Nivre, 2008) p < 0.01.

UAS LAS. Interestingly, notice P ROJ consistently outperforms CCA significant
margin, comparable C D13 +Cluster. analysis observation conducted Section 5.3.1 5.3.2.
1008

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Type
Cluster

Feature Templates
c
ESc , EB
, = 0, 1, 2

c
c
c
c
Elc1(Si ) , Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)

c
c
Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

Table 5: Word cluster feature templates.

framework flexible incorporating richer features simply embedding
continuous vectors. Thus embed cross-lingual word cluster features model,
together proposed cross-lingual word embeddings. cluster feature templates shown
Table 5, similar POS tag feature templates. shown Table 3, significant
additive improvements obtained P ROJ CCA embedding cluster features.
Compared delexicalized system, relative error reduced 13.1% UAS,
12.6% LAS. combined system outperforms C D13 +Cluster significantly .
5.3.1 E FFECT ROBUST P ROJECTION
Since P ROJ induction cross-lingual word clusters, use edit distance measure
OOV words, would see affects performance parsing.
Intuitively, higher coverage projected words test dataset promote parsing
performance more. verify this, conduct experiments settings using
P ROJ+Cluster model. robust projection, examine effect edit distances ranging
1 3. Results shown Table 6. Improvements observed languages using
robust projection edit distance measure, especially FR, highest coverage gain
obtained robust projection. observe slightly improvements DE ES using
edit distance 2. performance starts degrade gets larger. reasonable, since
larger edit distance increases word coverage, introduces noise.

Simple

DE

ES

FR

coverage
UAS
LAS
coverage
UAS
LAS
coverage
UAS
LAS

91.37
59.74
50.84
94.51
70.97
61.34
90.83
71.17
61.72

=1
94.70
60.35
51.54
96.67
71.90
62.28
97.60
72.93
63.12

Robust
=2
96.50
60.53
51.70
97.75
72.00
62.34
98.33
72.79
63.02

Table 6: Effect robust projection.

1009

=3
97.47
60.53
51.69
98.47
71.93
62.27
98.58
72.70
62.94

fiG UO , C , YAROWSKY, WANG & L IU

5.3.2 E FFECT F INE -T UNING W ORD E MBEDDINGS
Another reason effectiveness P ROJ CCA lies fine-tuning word embeddings
training parser.
CCA viewed joint method inducing cross-lingual word embeddings.
training source language dependency parser cross-lingual word embeddings derived
CCA, EN word embeddings fixed. Otherwise, translational equivalence
broken. However, P ROJ, limitation. Word embeddings updated
non-lexical feature embeddings, order obtain accurate dependency parser. refer
procedure fine-tuning process word embeddings. verify benefits fine-tuning,
conduct experiments see relative loss word embeddings fixed training. Results
shown Table 7, indicates fine-tuning indeed offers considerable help.

DE
ES
FR

UAS
LAS
UAS
LAS
UAS
LAS

Fixed
59.74
49.44
70.10
61.31
70.65
60.69

Fine-tuning
60.07
49.94
71.42
61.76
71.36
61.50


+0.33
+0.50
+1.32
+0.45
+0.71
+0.81

Table 7: Effect fine-tuning word embeddings.

5.4 Compare Existing Bilingual Word Embeddings
section, compare bilingual embeddings several previous approaches context dependency parsing. best knowledge, first work evaluation
bilingual word embeddings syntactic tasks.
approaches consider include multi-task learning approach (Klementiev et al., 2012)
[MTL], bilingual auto-encoder approach (Chandar P et al., 2014) [B IAE], bilingual compositional vector model (Hermann & Blunsom, 2014) [B ICVM], bilingual bag-of-words
approach (Gouws et al., 2015) [B ILBOWA].
MTL B IAE, adopt released word embeddings directly due inefficiency
training.14 B ICVM B ILBOWA, re-run systems dataset previous
experiments.15 Results summarized Table 8.
CCA P ROJ consistently outperforms approaches languages, P ROJ performs best. inferior performance MTL B IAE partly due low word coverage.
example, cover 31% words universal DE test treebank, whereas CCA
P ROJ covers 70%. Moreover, B IAE, B ICVM B ILBOWA introduce sentence-level translational equivalence objectives regularizers learning bilingual word embeddings.
approaches advantageous dont assume/require word alignment. However, word-toword translational equivalence cannot well preserved way.
14. MTL embeddings normalized training.
15. B ICVM uses bilingual parallel dataset.

1010

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

MTL (Klementiev et al., 2012)
B IAE (Chandar P et al., 2014)
B ICVM (Hermann & Blunsom, 2014)
B ILBOWA (Gouws et al., 2015)
CCA
P ROJ

DE
UAS
LAS
56.93 46.22
53.74 43.68
56.30 46.99
54.51 44.95
59.42 49.32
60.07 49.94

ES
UAS
67.71
58.81
67.78
67.23
68.87
71.42

LAS
58.43
46.66
58.08
56.16
59.65
61.76

FR
UAS
LAS
67.51 57.27
60.10 49.47
69.13 58.13
64.82 52.73
69.58 59.50
71.36 61.55

Table 8: Comparison existing bilingual word embeddings. MTL B IAE, use
released bilingual word embeddings.

Target Word (ES)

china
(china)

problemas
(problems)

septiembre
(september)

P ROJ
india
russia
taiwan
chinese
problem
difficulties
troubles
issues
october
august
january
december

CCA
russia
indonesia
beijing
chinese
problems
woes
troubles
dilemmas
december
july
october
june

Neighboring Words (EN)
MTL
B IAE
china
korea
independent india
sumitomo
chinese
malaysian
brazil
events
problem
sanctions
greatly
conditions
highlighted
laws
scale
december
month
february
april
july
scheduled
march
november

B ICVM
chinese
chinois
sino
33.55
problematic
problematical
difficulties
troubles
11th
11.00
11
eleventh

B ILBOWA
helsinki
bulgarians
constituting
market
deficiencies
situations
omissions
attentively
a.m
p.m
twelve
1998-1999

Table 9: Target words Spanish 4 similar words English, induced various
approaches.

verify assumption, taking EN/ES case study. manually inspect 4
similar words (by cosine similarity) English given set words Spanish (Table 9).
observe semantic syntactic shifting k-nearest neighbors prediction B IAE,
B ICVM B ILBOWA, whereas P ROJ CCA give translational equivalent predictions.
example, B ICVM yields adjective problematical target noun problemas; B ILBOWA yields
semantic-related word market china. general, P ROJ robust approach, behaving
consistently well sampled words.
worth noting dont assume/require bilingual parallel data CCA P ROJ.
need practice bilingual lexicon paired languages. especially important
generalizing approaches lower-resource languages, parallel texts available.
1011

fiG UO , C , YAROWSKY, WANG & L IU

6. Target-Language Adaptation Minimal Supervision
important us distinguish linguistic structures learned via cross-lingual transfer
versus learned basis monolingual information language
parsed. Intuitively, cross-lingual approaches learn common dependency structures
shared source target language. However, many languages,
specialized (language-specific) syntactic characteristics learned data
target language.
Take adjective-noun order example, Spanish French, adjectives often appears
nouns, thus forming right-directed arc labeled amod, whereas English, amod
(adjectival modifier) arcs mostly left-directed, illustrated Figure 4. Another example
subject-verb-object order. German, verbs often appear end sentence V2 position,
causes much left-directed dobj (direct object) arcs English (Figure 5).
differences clearly observed universal treebanks. Table 10 shows significant
distribution divergence left-directed right-directed arcs dobj amod relations
treebanks different languages.
Relation: dobj; Language: EN vs. DE
dobj
dobj
ratio
EN
38,395
764
50.3 : 1
DE
4,277
3,457
1.2 : 1
Relation: amod; Language: EN vs. ES, FR
amod amod
ratio
EN
1,667
57,864
1 : 34.7
ES
14,876
5,205
2.9 : 1
FR
12,919
4,910
2.6 : 1

Table 10: Distribution divergences left-directed right-directed arcs dobj relation EN
DE (top), amod relation EN ES/FR (bottom).

amod

amod

NOUN

ADJ

NOUN

ADJ

Spanish:

Consejo

Superior

conflictos

sociales

ADJ

NOUN

ADJ

NOUN

English:

Superior

Council

social

conflicts

amod

amod

Figure 4: Reverse direction amod relation Spanish English. French adjectives following nouns.

1012

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

root
advmod

det

dobj

ADV

DET

NOUN

VERB

DE:

endlich

den

richtigen

gefunden

EN:

finally

found



right man

ADV

VERB

DET

NOUN

advmod

det
dobj
root

Figure 5: Reverse direction dobj relation German English.
Therefore, section, investigate much cross-lingual transfer model improved annotating small amount labeled training data target language side. Even though
building large-scale treebanks low-resource languages supervised learning costly, annotating dependency structures small amount sentences (e.g., 100) difficult.
still conduct experiments universal dependency treebanks, provide labeled
training data multiple languages. language studied (DE, ES, FR), incrementally
augment amount labeled sentences 100 1,000 step 100, adapt parameters cross-lingual transfer model specific target language. Theoretically, since target
language treebanks contain non-projective trees, would make sense apply non-projective
algorithms (e.g., swap-based) target language adaptation. way, however, W2
re-trained scratch, doesnt show good performance experiments since minimally supervised data small. Consequently, still rely arc-standard algorithm
adaption. process almost training source language parser described
Section 3, except word embedding matrix E w fixed, rest parameters
(E {t,l,d,v,c} , W1 , W2 , b1 ) optimized using augmented labeled data target language,
taking Equation 3.1 objective function. development data used process, thus
simply perform parameter updating 2,000 iterations.
addition, built another strong baseline system employs augmented labeled
training data supervised learning. system, utilize word embeddings Brown
clusters features, derived separately language.
shown Figure 6, results really promising. P ROJ+Cluster CCA+Cluster
systems consistently outperform delexicalized system supervised system significant margin. P ROJ+Cluster CCA+Cluster general achieve comparable performances,
CCA+Cluster slightly better.
worthy noting performances P ROJ+Cluster CCA+Cluster boosted
augmenting 100 sentences. Take DE example, UAS increased 60.35% 68.91%,
LAS 51.54% 61.54%, nearly equal effect using 1,000 sentences
supervised learning. observation demonstrates great potential cross-lingual transfer
system practical usage.
1013



85

85

75

80

G UO , C , YAROWSKY, WANG & L IU










80






















UAS







75

UAS

65

UAS






60







75

70




80







70

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

0

200

400

600

800

1000

0

200






400

600

800

1000

200

600

800

1000












75

75























60







70

70



LAS

50



65

LAS



65

55

LAS

400

80





PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

Labeled training data (FR)



65

0

Labeled training data (ES)

80

70

Labeled training data (DE)





65



65

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

45

50



70

55






0

200

400

600

800

Labeled training data (DE)

1000



60

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

55

60



55

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

40

45



0

200

400

600

800

Labeled training data (ES)

1000

0

200

400

600

800

1000

Labeled training data (FR)

Figure 6: Target-language adaptation incrementally augmenting labeled training data (sentences) fine-tune cross-lingual transfer model. Performances evaluated using
UAS (top) LAS (bottom). Note points whose x coordinates 0 represent
cross-lingual transfer performance, labeled training data used.

Analysis. primary hypothesis incorporating data target language, model
able learn special syntactic patterns consistent source language.
verify this, study influence target-language adaptation two special relations:
dobj (DE) amod (ES, FR), measuring precision recall changes use
100 target language sentences. Results shown respectively Table 11 Table 12.
observe great improvements recall relations, indicates model indeed gains
ability learning target-language-specific dependency structures supervision
100 sentences.

7. Related Studies
cross-lingual annotation projection method pioneered Yarowsky, Ngai, Wicentowski (2001) shallow NLP tasks (POS tagging, NER, etc.), later applied dependency
parsing (Hwa et al., 2005; Smith & Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann,
2014). work along line dedicated improving robustness syntactic pro1014

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Relation: dobj; Language: DE
Precision Recall
PROJ+Cluster
41.45
31.09
+100
41.90
51.40

0.45 20.31
CCA+Cluster
39.47
31.74
+100
43.59
57.57

4.12 25.83

Table 11: Effect minimal supervision (100 sentences) dobj.
Relation: amod; Language: ES, FR
ES
FR
Precision Recall Precision Recall
PROJ+Cluster
94.97
80.05
92.94
81.70
+100
91.60
92.52
93.61
95.75

3.37 12.47
0.67 14.05
CCA+Cluster
93.37
77.31
92.08
72.22
+100
91.85
92.77
92.77
96.41

1.52 15.46
0.69 24.19

Table 12: Effect minimal supervision (100 sentences) amod.

jection alleviating noise errors introduced word alignment-based projection. Typical
approaches include soft projection (Li, Zhang, & Chen, 2014), treebank translation (Tiedemann, Agic, & Nivre, 2014), distribution transfer (Ma & Xia, 2014), recently proposed
density-driven projection (Rasooli & Collins, 2015). worth mentioning remarkable results
achieved annotation projection methods (Tiedemann, 2015; Rasooli & Collins,
2015), due large part parsers trained target language side.
cross-lingual model transfer, learning cross-lingual feature representations promising direction. Typical approaches include cross-lingual word clustering (Tackstrom et al., 2012)
employed paper baseline system, projection features (Durrett, Pauls, & Klein, 2012). Kozhevnikov Titov (2014) derived linear projection maps target instances
source-side feature representations, extent similar CCA approach. Xiao
Guo (2014) learned cross-lingual word embeddings applied MSTParser linguistic
transfer, inspired work. Sgaard et al. (2015) obtained multi-source unified word embeddings via inverted indexing Wikipedia, applied various NLP tasks. However,
results didnt show significant improvements parsing. Nevertheless, idea utilizing multisource information learning cross-lingual word embeddings makes great sense. recently,
Duong et al. (2015a, 2015b) utilized neural network architecture parameter sharing
parsers different languages. However, approach requires annotated treebanks
target language side, makes distinct transfer parsing framework. addition
representation learning, attempts made integrate monolingual linguistic features parsing models, manually constructed universal dependency parsing rules (Naseem,
1015

fiG UO , C , YAROWSKY, WANG & L IU

Chen, Barzilay, & Johnson, 2010) manually specified typological features (Naseem, Barzilay,
& Globerson, 2012; Zhang & Barzilay, 2015).
Using neural networks dependency parsing new approach. best knowledge, Mayberry Miikkulainen (1999) presented first work explored neural networks
shift-reduce constituent-based parsing. used one-hot feature representations. Henderson
(2004) used simple synchrony network predict parse decisions constituency parser,
first use neural networks broad-coverage Penn Treebank parser. Titov Henderson (2007) applied Incremental Sigmoid Belief Networks constituent-based parsing. Garg
Henderson (2011) later extended work transition-based dependency parsing using Temporal Restricted Boltzman Machine. parsers, however, much less scalable practice.
Earlier progress made using deep learning parsing includes work Collobert (2011)
Socher et al. (2013) constituent-based parsing, Stenetorp (2013) built recursive neural
networks transition-based dependency parsing.

8. Conclusion
paper proposes novel framework based distributed representations cross-lingual dependency parsing. Two algorithms proposed induction cross-lingual word representations,
namely robust projection CCA, bridge lexical feature gap.
Experiments show using cross-lingual word embeddings derived either approach,
transferred parsing performance improved significantly delexicalized system.
notable observation projection method performs significantly better CCA. Additionally, framework flexibly able incorporate cross-lingual word cluster features,
significant gains use. combined system significantly outperforms delexicalized systems languages, average 10.9% error reduction LAS,
significantly outperforms models McDonald et al. (2013) augmented projected word
cluster features.
Furthermore, show performance cross-lingual transfer system specific target language boosted minimal supervision language, great
significance practical usage.

Acknowledgments
grateful Manaal Faruqui providing bilingual resources. thank Ryan McDonald
pointing evaluation issue experiment. thank Sharon Busching
proofreading anonymous reviewers insightful comments suggestions. work
supported National Key Basic Research Program China via grant 2014CB340503
National Natural Science Foundation China (NSFC) via grant 61133012 61370164.
Corresponding author: Wanxiang Che, E-mail: car@ir.hit.edu.cn.

References
Bansal, M., Gimpel, K., & Livescu, K. (2014). Tailoring continuous word representations dependency parsing. Proceedings 52nd Annual Meeting Association Computa1016

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

tional Linguistics (Volume 2: Short Papers), pp. 809815, Baltimore, Maryland. Association
Computational Linguistics.
Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-based n-gram
models natural language. Computational linguistics, 18(4), 467479.
Buchholz, S., & Marsi, E. (2006). Conll-x shared task multilingual dependency parsing.
Proceedings Tenth Conference Computational Natural Language Learning (CoNLLX), pp. 149164, New York City. Association Computational Linguistics.
Cao, Y., & Khudanpur, S. (2014). Online learning tensor space. Proceedings 52nd
Annual Meeting Association Computational Linguistics (Volume 1: Long Papers),
pp. 666675, Baltimore, Maryland. Association Computational Linguistics.
Carreras, X. (2007). Experiments higher-order projective dependency parser. Proceedings
CoNLL Shared Task Session EMNLP-CoNLL 2007, pp. 957961, Prague, Czech
Republic. Association Computational Linguistics.
Chandar P, S., Lauly, S., Larochelle, H., Khapra, M., Ravindran, B., Raykar, V. C., & Saha, A.
(2014). autoencoder approach learning bilingual word representations. Advances
Neural Information Processing Systems 27, pp. 18531861. Curran Associates, Inc.
Chen, D., & Manning, C. (2014). fast accurate dependency parser using neural networks.
Proceedings 2014 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 740750, Doha, Qatar. Association Computational Linguistics.
Choi, J. D., & McCallum, A. (2013). Transition-based dependency parsing selectional branching. Proceedings 51st Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 10521062, Sofia, Bulgaria. Association Computational
Linguistics.
Collobert, R. (2011). Deep learning efficient discriminative parsing. Proceedings 14th
International Conference Artificial Intelligence Statistics (AISTATS), pp. 224232,
Fort Lauderdale, FL, USA. JMLR.org.
Collobert, R., & Weston, J. (2008). unified architecture natural language processing: Deep
neural networks multitask learning. Proceedings 25th International Conference
Machine Learning, ICML 08, pp. 160167, Helsinki, Finland. ACM.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural
language processing (almost) scratch. Journal Machine Learning Research, 12, 2493
2537.
Covington, M. A. (2001). fundamental algorithm dependency parsing. Proceedings
39th annual ACM southeast conference, pp. 95102.
De Marneffe, M.-C., MacCartney, B., Manning, C. D., et al. (2006). Generating typed dependency
parses phrase structure parses. Proceedings Fifth International Conference
Language Resources Evaluation (LREC06), pp. 449454, Genoa, Italy. European
Language Resources Association (ELRA).
De Marneffe, M.-C., & Manning, C. D. (2008). stanford typed dependencies representation.
COLING 2008: Proceedings workshop Cross-Framework Cross-Domain Parser
Evaluation, pp. 18, Manchester, UK. Association Computational Linguistics.
1017

fiG UO , C , YAROWSKY, WANG & L IU

Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods online learning
stochastic optimization. Journal Machine Learning Research, 12, 21212159.
Duong, L., Cohn, T., Bird, S., & Cook, P. (2015a). Low resource dependency parsing: Cross-lingual
parameter sharing neural network parser. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference
Natural Language Processing (Volume 2: Short Papers), pp. 845850, Beijing, China.
Association Computational Linguistics.
Duong, L., Cohn, T., Bird, S., & Cook, P. (2015b). neural network model low-resource universal dependency parsing. Proceedings 2015 Conference Empirical Methods
Natural Language Processing, pp. 339348, Lisbon, Portugal. Association Computational
Linguistics.
Durrett, G., Pauls, A., & Klein, D. (2012). Syntactic transfer using bilingual lexicon. Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing
Computational Natural Language Learning, pp. 111, Jeju Island, Korea. Association
Computational Linguistics.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-based dependency parsing stack long short-term memory. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference
Natural Language Processing (Volume 1: Long Papers), pp. 334343, Beijing, China. Association Computational Linguistics.
Dyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blunsom, P., Setiawan, H., Eidelman, V.,
& Resnik, P. (2010). cdec: decoder, alignment, learning framework finite-state
context-free translation models. Proceedings ACL 2010 System Demonstrations, pp.
712, Uppsala, Sweden. Association Computational Linguistics.
Eisner, J. M. (1996). Three new probabilistic models dependency parsing: exploration.
Proceedings 16th conference Computational linguistics-Volume 1, pp. 340345,
Copenhagen, Denmark. Association Computational Linguistics.
Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual
correlation. Proceedings 14th Conference European Chapter Association Computational Linguistics, pp. 462471, Gothenburg, Sweden. Association
Computational Linguistics.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies linguistic analysis, pp.
132. Blackwell.
Garg, N., & Henderson, J. (2011). Temporal restricted boltzmann machines dependency parsing.
Proceedings 49th Annual Meeting Association Computational Linguistics:
Human Language Technologies, pp. 1117, Portland, Oregon, USA. Association Computational Linguistics.
Gouws, S., Bengio, Y., & Corrado, G. (2015). Bilbowa: Fast bilingual distributed representations
without word alignments. Proceedings 32nd International Conference Machine
Learning (ICML), pp. 748756, Lille, France.
Guo, J., Che, W., Wang, H., & Liu, T. (2014). Revisiting embedding features simple semisupervised learning. Proceedings 2014 Conference Empirical Methods Natural
1018

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Language Processing (EMNLP), pp. 110120, Doha, Qatar. Association Computational
Linguistics.
Guo, J., Che, W., Yarowsky, D., Wang, H., & Liu, T. (2015). Cross-lingual dependency parsing
based distributed representations. Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural
Language Processing (Volume 1: Long Papers), pp. 12341244, Beijing, China. Association
Computational Linguistics.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons
monolingual corpora. Proceedings ACL-08: HLT, pp. 771779, Columbus, Ohio.
Association Computational Linguistics.
Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis:
overview application learning methods. Neural computation, 16(12), 26392664.
Henderson, J. (2004). Discriminative training neural network statistical parser. Proceedings 42nd Meeting Association Computational Linguistics (ACL04), Main
Volume, pp. 95102, Barcelona, Spain.
Hermann, K. M., & Blunsom, P. (2014). Multilingual models compositional distributed semantics. Proceedings 52nd Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 5868, Baltimore, Maryland. Association Computational Linguistics.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers via
syntactic projection across parallel texts. Natural language engineering, 11(03), 311325.
Jiang, W., Liu, Q., & Lv, Y. (2011). Relaxed cross-lingual projection constituent syntax.
Proceedings 2011 Conference Empirical Methods Natural Language Processing,
pp. 11921201, Edinburgh, Scotland, UK. Association Computational Linguistics.
Kim, Y. (2014). Convolutional neural networks sentence classification. Proceedings
2014 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 17461751, Doha, Qatar. Association Computational Linguistics.
Klein, D., & Manning, C. (2004). Corpus-based induction syntactic structure: Models dependency constituency. Proceedings 42nd Meeting Association Computational Linguistics (ACL04), Main Volume, pp. 478485, Barcelona, Spain.
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations
words. Proceedings COLING 2012, pp. 14591474, Mumbai, India. COLING
2012 Organizing Committee.
Koehn, P., & Knight, K. (2002). Learning translation lexicon monolingual corpora. Proceedings ACL-02 Workshop Unsupervised Lexical Acquisition, pp. 916, Philadelphia, Pennsylvania, USA. Association Computational Linguistics.
Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions applications. SIAM review, 51(3),
455500.
Koo, T., & Collins, M. (2010). Efficient third-order dependency parsers. Proceedings
48th Annual Meeting Association Computational Linguistics, pp. 111, Uppsala,
Sweden. Association Computational Linguistics.
1019

fiG UO , C , YAROWSKY, WANG & L IU

Kozhevnikov, M., & Titov, I. (2014). Cross-lingual model transfer using feature representation
projection. Proceedings 52nd Annual Meeting Association Computational
Linguistics (Volume 2: Short Papers), pp. 579585, Baltimore, Maryland. Association
Computational Linguistics.
Lei, T., Xin, Y., Zhang, Y., Barzilay, R., & Jaakkola, T. (2014). Low-rank tensors scoring
dependency structures. Proceedings 52nd Annual Meeting Association
Computational Linguistics (Volume 1: Long Papers), pp. 13811391, Baltimore, Maryland.
Association Computational Linguistics.
Li, Z., Zhang, M., & Chen, W. (2014). Soft cross-lingual syntax projection dependency parsing. Proceedings COLING 2014, 25th International Conference Computational
Linguistics: Technical Papers, pp. 783793, Dublin, Ireland. Dublin City University Association Computational Linguistics.
Ma, X., & Xia, F. (2014). Unsupervised dependency parsing transferring distribution via
parallel guidance entropy regularization. Proceedings 52nd Annual Meeting
Association Computational Linguistics (Volume 1: Long Papers), pp. 13371348,
Baltimore, Maryland. Association Computational Linguistics.
Mann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridge languages.
Proceedings Second Meeting North American Chapter Association
Computational Linguistics Language Technologies, NAACL 01, pp. 18, Pittsburgh,
Pennsylvania. Association Computational Linguistics.
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building large annotated corpus
english: penn treebank. Computational linguistics, 19(2), 313330.
Mayberry, M. R., & Miikkulainen, R. (1999). Sardsrn: neural network shift-reduce parser.
Proceedings Sixteenth International Joint Conference Artificial Intelligence, pp.
820827. Morgan Kaufmann Publishers Inc.
McDonald, R., Crammer, K., & Pereira, F. (2005). Online large-margin training dependency
parsers. Proceedings 43rd Annual Meeting Association Computational
Linguistics (ACL05), pp. 9198, Ann Arbor, Michigan. Association Computational Linguistics.
McDonald, R., & Nivre, J. (2007). Characterizing errors data-driven dependency parsing
models. Proceedings 2007 Joint Conference Empirical Methods Natural
Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp.
122131, Prague, Czech Republic. Association Computational Linguistics.
McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K., Hall, K.,
Petrov, S., Zhang, H., Tackstrom, O., Bedini, C., Bertomeu Castello, N., & Lee, J. (2013).
Universal dependency annotation multilingual parsing. Proceedings 51st Annual
Meeting Association Computational Linguistics (Volume 2: Short Papers), pp. 92
97, Sofia, Bulgaria. Association Computational Linguistics.
McDonald, R., Petrov, S., & Hall, K. (2011). Multi-source transfer delexicalized dependency
parsers. Proceedings 2011 Conference Empirical Methods Natural Language
Processing, pp. 6272, Edinburgh, Scotland, UK. Association Computational Linguistics.
1020

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

McDonald, R. T., & Pereira, F. C. (2006). Online learning approximate dependency parsing
algorithms. Proceedings 11st Conference European Chapter Association Computational Linguistics, pp. 8188, Trento, Italy. Association Computer
Linguistics.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation word representations
vector space. International Conference Learning Representations (ICLR) Workshop.
Naseem, T., Barzilay, R., & Globerson, A. (2012). Selective sharing multilingual dependency
parsing. Proceedings 50th Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 629637, Jeju Island, Korea. Association Computational Linguistics.
Naseem, T., Chen, H., Barzilay, R., & Johnson, M. (2010). Using universal linguistic knowledge
guide grammar induction. Proceedings 2010 Conference Empirical Methods
Natural Language Processing, pp. 12341244, Cambridge, MA. Association Computational Linguistics.
Nilsson, J., & Nivre, J. (2008). Malteval: evaluation visualization tool dependency
parsing.. Proceedings Sixth International Language Resources Evaluation
(LREC08), pp. 161166, Marrakech, Morocco. European Language Resources Association
(ELRA).
Nivre, J. (2003). efficient algorithm projective dependency parsing. Proceedings
8th International Workshop Parsing Technologies (IWPT), pp. 149160, Nancy, France.
Association Computational Linguistics.
Nivre, J. (2004). Incrementality deterministic dependency parsing. Proceedings Workshop Incremental Parsing: Bringing Engineering Cognition Together, pp. 5057,
Barcelona, Spain. Association Computational Linguistics.
Nivre, J. (2008). Algorithms deterministic incremental dependency parsing. Computational
Linguistics, 34(4), 513553.
Nivre, J. (2009). Non-projective dependency parsing expected linear time. Proceedings
Joint Conference 47th Annual Meeting ACL 4th International Joint
Conference Natural Language Processing AFNLP, pp. 351359, Suntec, Singapore.
Association Computational Linguistics.
Nivre, J., Hall, J., & Nilsson, J. (2004). Memory-based dependency parsing. HLT-NAACL 2004
Workshop: Eighth Conference Computational Natural Language Learning (CoNLL-2004),
pp. 4956, Boston, Massachusetts, USA. Association Computational Linguistics.
Petrov, S., Das, D., & McDonald, R. (2012). universal part-of-speech tagset. Proceedings
Eighth International Conference Language Resources Evaluation (LREC-2012),
pp. 20892096, Istanbul, Turkey. European Language Resources Association (ELRA).
Rasooli, M. S., & Collins, M. (2015). Density-driven cross-lingual transfer dependency parsers.
Proceedings 2015 Conference Empirical Methods Natural Language Processing, pp. 328338, Lisbon, Portugal. Association Computational Linguistics.
Smith, D. A., & Eisner, J. (2009). Parser adaptation projection quasi-synchronous grammar
features. Proceedings 2009 Conference Empirical Methods Natural Language
Processing, pp. 822831, Singapore. Association Computational Linguistics.
1021

fiG UO , C , YAROWSKY, WANG & L IU

Socher, R., Bauer, J., Manning, C. D., & Andrew Y., N. (2013). Parsing compositional vector
grammars. Proceedings 51st Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 455465, Sofia, Bulgaria. Association Computational Linguistics.
Sgaard, A., Agic, v., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015). Inverted
indexing cross-lingual nlp. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 17131722, Beijing, China. Association
Computational Linguistics.
Srivastava, N., & Salakhutdinov, R. R. (2012). Multimodal learning deep boltzmann machines. Advances Neural Information Processing Systems 25, pp. 22222230. Curran
Associates, Inc.
Stenetorp, P. (2013). Transition-based dependency parsing using recursive neural networks. Deep
Learning Workshop NIPS, Lake Tahoe, Nevada, USA.
Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence sequence learning neural networks. Advances Neural Information Processing Systems 27, pp. 31043112. Curran
Associates, Inc.
Tackstrom, O., McDonald, R., & Uszkoreit, J. (2012). Cross-lingual word clusters direct transfer
linguistic structure. Proceedings 2012 Conference North American Chapter
Association Computational Linguistics: Human Language Technologies, pp. 477
487, Montreal, Canada. Association Computational Linguistics.
Tiedemann, J. (2014). Rediscovering annotation projection cross-lingual parser induction.
Proceedings COLING 2014, 25th International Conference Computational Linguistics: Technical Papers, pp. 18541864, Dublin, Ireland. Dublin City University Association Computational Linguistics.
Tiedemann, J. (2015). Cross-lingual dependency parsing universal dependencies predicted
PoS labels., 340349.
Tiedemann, J., Agic, v., & Nivre, J. (2014). Treebank translation cross-lingual parser induction.,
130140.
Titov, I., & Henderson, J. (2007). Fast robust multilingual dependency parsing generative
latent variable model. Proceedings CoNLL Shared Task Session EMNLP-CoNLL
2007, pp. 947951, Prague, Czech Republic. Association Computational Linguistics.
Turian, J., Ratinov, L.-A., & Bengio, Y. (2010). Word representations: simple general method
semi-supervised learning. Proceedings 48th Annual Meeting Association
Computational Linguistics, pp. 384394, Uppsala, Sweden. Association Computational Linguistics.
Uszkoreit, J., & Brants, T. (2008). Distributed word clustering large scale class-based language
modeling machine translation. Proceedings ACL-08: HLT, pp. 755762, Columbus,
Ohio. Association Computational Linguistics.
Wang, M., & Manning, C. D. (2013). Effect non-linear deep architecture sequence labeling.
Proceedings Sixth International Joint Conference Natural Language Processing,
pp. 12851291, Nagoya, Japan. Asian Federation Natural Language Processing.
1022

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Weiss, D., Alberti, C., Collins, M., & Petrov, S. (2015). Structured training neural network
transition-based parsing. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural Language
Processing (Volume 1: Long Papers), pp. 323333, Beijing, China. Association Computational Linguistics.
Xiao, M., & Guo, Y. (2014). Distributed word representation learning cross-lingual dependency
parsing. Proceedings Eighteenth Conference Computational Natural Language
Learning, pp. 119129, Ann Arbor, Michigan. Association Computational Linguistics.
Xue, N., Xia, F., Chiou, F.-D., & Palmer, M. (2005). penn chinese treebank: Phrase structure
annotation large corpus. Natural language engineering, 11(02), 207238.
Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis support vector machines.
Proceedings 8th International Workshop Parsing Technologies (IWPT), pp. 195
206, Nancy, France. Association Computational Linguistics.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis tools via
robust projection across aligned corpora. Proceedings first international conference
Human language technology research, pp. 18, San Diego, CA, USA. Association
Computational Linguistics.
Zeman, D., Dusek, O., Marecek, D., Popel, M., Ramasamy, L., Stepanek, J., Zabokrtsky, Z., &
Hajic, J. (2014). Hamledt: Harmonized multi-language dependency treebank. Language
Resources Evaluation, 48(4), 601637.
Zhang, Y., & Barzilay, R. (2015). Hierarchical low-rank tensors multilingual transfer parsing.
Proceedings 2015 Conference Empirical Methods Natural Language Processing,
pp. 18571867, Lisbon, Portugal. Association Computational Linguistics.
Zhang, Y., & Clark, S. (2011). Syntactic processing using generalized perceptron beam
search. Computational Linguistics, 37(1), 105151.
Zhang, Y., & Nivre, J. (2011). Transition-based dependency parsing rich non-local features.
Proceedings 49th Annual Meeting Association Computational Linguistics: Human Language Technologies, pp. 188193, Portland, Oregon, USA. Association
Computational Linguistics.
Zhao, H., Song, Y., Kit, C., & Zhou, G. (2009). Cross language dependency parsing using bilingual
lexicon. Proceedings Joint Conference 47th Annual Meeting ACL
4th International Joint Conference Natural Language Processing AFNLP, pp.
5563, Suntec, Singapore. Association Computational Linguistics.
Zhou, H., Zhang, Y., Huang, S., & Chen, J. (2015). neural probabilistic structured-prediction
model transition-based dependency parsing. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference
Natural Language Processing (Volume 1: Long Papers), pp. 12131222, Beijing, China.
Association Computational Linguistics.

1023


