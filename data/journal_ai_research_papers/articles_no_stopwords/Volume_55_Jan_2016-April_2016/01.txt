Journal Artificial Intelligence Research 55 (2016) 11351178

Submitted 12/15; published 04/16

Exploiting Causality Selective Belief Filtering
Dynamic Bayesian Networks
Stefano V. Albrecht

svalb@cs.utexas.edu

Department Computer Science
University Texas Austin
Austin, TX 78712, USA

Subramanian Ramamoorthy

s.ramamoorthy@ed.ac.uk

School Informatics
University Edinburgh
Edinburgh, EH8 9AB, UK

Abstract
Dynamic Bayesian networks (DBNs) general model stochastic processes
partially observed states. Belief filtering DBNs task inferring belief state (i.e.
probability distribution process states) based incomplete noisy observations.
hard problem complex processes large state spaces. article,
explore idea accelerating filtering task automatically exploiting causality
process. consider specific type causal relation, called passivity, pertains
state variables cause changes variables. present Passivity-based
Selective Belief Filtering (PSBF) method, maintains factored belief representation
exploits passivity perform selective updates belief factors. PSBF produces
exact belief states certain assumptions approximate belief states otherwise,
approximation error bounded degree uncertainty process. show
empirically, synthetic processes varying sizes degrees passivity, PSBF
faster several alternative methods achieving competitive accuracy. Furthermore,
demonstrate passivity occurs naturally complex system multi-robot
warehouse, PSBF exploit accelerate filtering task.

1. Introduction
Dynamic Bayesian networks (DBNs) (Dean & Kanazawa, 1989) general model
stochastic processes partially observed states. topology DBN compact
specification variables process interact transitions (cf. Figure 1). Given
possible incompleteness noise observations, may generally possible
infer state process absolute certainty. Instead, may infer beliefs
process state based history observations, form probability distribution
state space process. often called belief state task calculating
belief states commonly referred belief filtering.
number exact approximate inference methods exist Bayesian networks (see,
e.g., Koller & Friedman, 2009; Pearl, 1988) used filtering DBNs,
applying unrolled DBN + 1 slice repeated observed
time step, via successive update current posterior (belief state) used
c
2016
AI Access Foundation. rights reserved.

fiAlbrecht & Ramamoorthy

xt1

xt+1
1

y1t+1

xt2

xt+1
2

y2t+1



t+1

Figure 1: Example dynamic Bayesian network (DBN) two state variables two
observation variables. xti xt+1
variables represent process states time

+ 1, respectively, yit+1 variables (shaded) represent observation time + 1.
arrows describe variables interact.

prior next time step (see Murphy, 2002). However, clear
unrolled variant becomes intractable network grows unboundedly time. Even
successive update, exact methods become intractable high-dimensional process
states approximate methods may propagate growing errors time. Therefore, filtering
methods developed utilise special structure DBNs maintain errors
propagated time. (We defer detailed discussion methods Section 2.)
Often, key developing efficient filtering methods identify structure
process leveraged inference. article, interested application
DBNs representations actions partially observed decision processes,
POMDPs (Kaelbling, Littman, & Cassandra, 1998; Sondik, 1971) many variants.
DBNs used represent effects actions decision process, specifying
variables interact information decision maker observes. many cases,
decision processes exhibit high degrees causal structure (Pearl, 2000), mean
change one part process may cause change another part. experience
processes causal structure may used make filtering task
tractable, tell us beliefs need revised certain aspects
process state. example, variable x2 Figure 1 changes value variable x1
changed value (i.e. change x1 causes change x2 ), seems intuitive use
causal relation deciding whether revise ones belief x2 . Unfortunately,
current filtering methods take causal structure account.
refer type causal relation (between x1 x2 ) passivity. Intuitively,
say state variable xi passive given action if, executing action,
subset state variables directly affect xi (i.e. xi parents DBN)
xi may change value least one variables subset changed
value. worth pointing passivity occurs naturally frequently many
planning domains, especially robotic physical systems (Mainzer, 2010).
following example1 illustrates simple robot arm:

1. mark end example solid black square.

1136

fiExploiting Causality Selective Belief Filtering DBNs

3
2

2

1

3



XA

B

XB

1

(a) Robot arm gripper

C
(b) Holding blocks B

Figure 2: Robot arm three rotational joints gripper. variables represent
absolute orientations corresponding joints.
Example 1 (Robot arm). Consider robot arm three rotational joints gripper,
shown Figure 2a. joints denoted 1 , 2 , 3 may take values
discrete set {0 , 1 , ..., 359 } indicate absolute orientations (e.g. = 0 means
joint points exactly right, = 180 means points left).
joint i, let two actions CWi CCWi rotate joint 1 clockwise
counter-clockwise, respectively. uncertainty system could due stochastic
joint movements unreliable sensor readings joint orientations.
action CWi CCWi , variable passive value directly
modified action. However, variables j6=i passive change
values corresponding preceding variable j1 changed value, since changed
orientation joint j 1 causes changed orientation joint j (recall orientations
absolute). Note accounts chains causal effects, indicated
arrows: orientation joint 3 changes orientation joint 1 changes, since joint
1 causes joint 2 change, turn causes joint 3 change.
examples passivity seen context object manipulation,
blocks planning domain (e.g. Pasula, Zettlemoyer, & Kaelbling, 2007). Figure 2b
shows arm holding blocks B A, top B. Here, position B (XB )
passive respect joint orientations since change orientations
changed. Furthermore, causal chain joint orientations position
block (XA ), since position change Bs position changes.

passivity exploited accelerate filtering task example?
fact state variables passive means aspects state may remain
unchanged, depending action choose. example, choose rotate joint
3, fact joints 1 2 passive means unaffected action.
Thus, seems redundant revise beliefs orientations joints 1 2. However,
precisely current filtering methods (cf. Section 2).
concretely, assume use factored belief representation P (1 , 2 , 3 ) = P (1 , 2 )
P (2 , 3 ) choose rotate 3 direction. Then, easy see need
update factor P (2 , 3 ), since 3 changes value, factor P (1 , 2 ), since
variables 1 , 2 passive. Since parents 1 , 2 (if any) change
values, know 1 , 2 change values either. show later, skipping
1137

fiAlbrecht & Ramamoorthy

P (1 , 2 ) result loss information cases, similarly chains
causal connections (cf. Example 1). complex example planning domain
involving passivity, exploited, discussed Section 6.2.
addition guiding belief revision, several features make passivity
interesting example causal relation: First all, passivity latent causal relation,
meaning readily extracted process dynamics without additional
annotation expert. (In Section 4, give procedure identifies passive variables
based conditional probability tables.) Furthermore, passivity deterministic
relation since passive variables may stochastic behaviour changing
values. Finally, passivity relatively simple example causal relation, idea
exploiting passivity order accelerate filtering task intuitive. Yet, best
knowledge, formalised explored rigorously before.
purpose present article formalise evaluate idea automatically
exploiting causal structure efficient belief filtering DBNs, using passivity concrete
example causal relation. Specifically, hypothesis large processes
high degrees passivity, structure exploited accelerate filtering task.
discussing related work Section 2 technical preliminaries Section 3,
contributions grouped following parts:
Section 4, give formally concise definition passivity discuss various
aspects definition. definition assumes decision process specified
set dynamic Bayesian networks (one action). discuss nonexample passivity, mean variables appear passive really
passive. Finally, give simple procedure detect passive variables
based conditional probability tables.
Section 5, present Passivity-based Selective Belief Filtering (PSBF) method.
Following idea outlined above, PSBF uses factored belief representation
belief factors defined clusters correlated state variables. PSBF follows
2-step update procedure wherein belief state first propagated
process dynamics (the transition step) conditioned observation (the
observation step). interesting novelty PSBF way performs
transition step: rather updating belief factors, PSBF updates
factors whose variables suspects changed, possible exploiting
passivity (to made precise shortly). Similarly, observation step, PSBF updates
belief factors determines structurally connected
observation, uses parts observation relevant
belief factor, thus allowing efficient incorporation observations. PSBF
produces exact belief states certain assumptions approximate belief states
otherwise. discuss computational complexity error bounds PSBF.
Section 6, evaluate PSBF two experimental domains: first evaluate PSBF
synthetic (i.e. randomly generated) processes varying sizes degrees passivity.
process sizes vary one thousand one trillion states, passivity
degrees vary 25% 100% passivity. results show PSBF faster
several alternative methods maintaining competitive accuracy. particular,
1138

fiExploiting Causality Selective Belief Filtering DBNs

results indicate computational gains grow significantly degree
passivity size process. evaluate PSBF complex simulation
multi-robot warehouse system style Kiva (Wurman, DAndrea, & Mountz,
2008). show passivity occurs system PSBF exploit
accelerate filtering task, outperforming alternative methods.
Finally, discuss strengths weaknesses PSBF Section 7, conclude
work Section 8. proofs found appendix.

2. Related Work
exists substantial body work belief filtering partially observed stochastic
processes. section, review filtering methods utilise special structure
DBNs situate work within related literature.
2.1 Approximate Belief Filtering DBNs
Several authors proposed filtering methods wherein belief state represented set
state samples. Specifically, probability process state normalised
frequency state samples correspond s. methods commonly
referred particle filters (PF); see work Doucet, de Freitas, Gordon (2001)
survey. common variant PF (Gordon, Salmond, & Smith, 1993), filtering
task consists propagating current state samples process dynamics
subsequent resampling step based probabilities new state samples
would produced observation. Two interesting features PF
applied processes discrete continuous variables, approximation
error converges zero increase number state samples.
known problem PF fact number samples needed acceptable
approximations grow drastically variance process dynamics (as shown
experiments; cf. Section 6). Rao-Blackwellised PF (RBPF) (Doucet, De Freitas,
Murphy, & Russell, 2000) developed address problem. RBPF assumes
state variables grouped sets R X distribution X
efficiently calculated R filtering. Hence, sample RBPF consists
sample R corresponding marginal distribution X. RBPF useful
variance R relatively low variance X high, since reduces number
samples needed acceptable approximations.
Boyen Koller (1999, 1998) recognised process consists several independent
weakly interacting subcomponents, belief state represented efficiently
product smaller beliefs individual subcomponents. seminal contribution show approximation error due factored representation essentially
bounded degree uncertainty (or mixing rates) process. precisely,
prove relative entropy (or KL divergence; Kullback & Leibler, 1951) two belief states contracts exponential rate propagated stochastic transition
process. Based observation, propose filtering method (BK) wherein belief
state represented factored form belief factors updated using exact inference method, junction tree algorithm (Lauritzen & Spiegelhalter, 1988). Since
1139

fiAlbrecht & Ramamoorthy

internal cliques used junction tree algorithm may correspond belief
state representation BK, final projection step typically performed
original factorisation restored. performance method depends crucially whether relevant correlations state variables captured small
clusters, whether projection step performed efficiently.
Factored particle filtering (FP) (Ng, Peshkin, & Pfeffer, 2002) addresses main drawbacks PF (many samples needed) BK (small clusters required) approximating
belief factors using set factored state samples. samples factored sense
assign values variables corresponding factor. allows FP
represent belief factors large BK, reduces number samples
needed due smaller number variables factor. authors provide different methods updating factored state samples, generic idea first perform
join operation full state samples reconstructed factored samples,
updated standard PF. updated samples projected
factored form using project operation. main drawback FP join
project operations essentially correspond standard relational database operations,
expensive.
Murphy Weiss (2001) propose filtering method called factored frontier (FF).
uses fully factored representation belief states; is, belief state product
marginals individual state variable. allows compact representation
beliefs. algorithm works moving set state variables (the frontier) forward
backward DBN topology. requires certain variable ordering,
difficult attain intra-correlations state variables (i.e. edges within + 1
slice DBN) allowed. authors show method equivalent single
iteration loopy belief propagation (LBP) (Pearl, 1988). Thus, similar LBP,
applied successive iterations improve approximation accuracy.
None works discussed explicitly address question causal relations
state variables exploited accelerate filtering task, or, alternatively,
filtering methods proposed therein implicitly benefit causal structure. method,
PSBF, related BK FP PSBF, too, uses factored belief representation,
belief factors defined clusters correlated state variables. Therefore,
analysis approximation errors Boyen Koller (1998) applies PSBF,
show Section 5 well experiments. However, contrast BK FP, PSBF
perform inference complete factorisation, rather individual
factors. consequence, PSBF require join project operation, one
main disadvantages BK FP.
2.2 Belief Filtering Decision Processes
methods discussed preceding subsection used belief filtering decision
processes, including POMDPs (Kaelbling et al., 1998; Sondik, 1971). regard,
methods viewed pure filters concerned belief filtering
control decision process. contrast combined filtering
methods, interleave filtering control tasks decision processes make
specific assumptions regarding solutions thereof. exists large body literature
1140

fiExploiting Causality Selective Belief Filtering DBNs

combined methods, including reachability-based methods (Hauskrecht, 2000; Washington,
1997), grid-based methods (Zhou & Hansen, 2001; Brafman, 1997; Lovejoy, 1991), pointbased methods (Smith & Simmons, 2005; Pineau, Gordon, & Thrun, 2003), compression
methods (Roy, Gordon, & Thrun, 2005; Poupart & Boutilier, 2002).
potential advantage combined methods access additional
structure may, therefore, utilise synergies filtering control tasks. One
synergy use decision quality guide belief filtering, rather metrics
relative entropy. Poupart Boutilier (2001, 2000) propose filtering method, called
value-directed approximation, chooses different approximation schemes different
decisions minimise expected loss decision quality (i.e. accumulated rewards).
method assumes POMDP solved exactly value function
provided form -vectors represent available actions POMDP.
Based value function, algorithm computes switching set alternative
plans determine error bounds approximation schemes. used search
optimal approximation scheme tree-based manner, search traverses
approximate exact schemes.
idea using decision quality guide belief filtering appealing, method
involves series optimisation problems exhaustive tree search,
costly complex systems. advantage pure filtering methods, including proposed
method PSBF, filter processes complex combined methods,
multi-robot warehouse system studied Section 6. actual control task
done via domain-specific solutions (cf. Section 6.2.1).
2.3 Substructure Parameterisation
Bayesian networks, hence DBNs, allow compact parameterisation (i.e. specification
probabilities) efficient inference via conditional independence relations. addition,
considerable work identifying substructure parameterisation
simplify knowledge acquisition enhance inference (Koller & Friedman, 2009;
Boutilier, Dean, & Hanks, 1999). property studied work, passivity, one example
substructure parameterisation. notable examples include causal independence (e.g. Heckerman & Breese, 1994; Heckerman, 1993) context-specific independence
(Boutilier, Friedman, Goldszmidt, & Koller, 1996).
Causal independence assumption effects individual causes common
variable (i.e. parents variable) independent one another. allows
compact parameterisation via operators noisy-or (Srinivas, 1993; Pearl, 1988),
used enhance inference (Zhang & Poole, 1996). Note passivity
conceptually much simpler property causal independence, passivity neither
concerned strength individual causes extent depend
other. Moreover, passivity read directly parameterisation (cf. Section 4.3)
whereas causal independence usually imposed designer.
Context-specific independence (CSI) property states variable independent parents given certain assignment values (i.e. context)
parents. Non-local CSI statements follow similarly d-separation (Geiger, Verma,
& Pearl, 1989). allow reduction parameters (Boutilier et al., 1996)
1141

fiAlbrecht & Ramamoorthy

enhancement inference (Poole & Zhang, 2003). discuss Section 4, passivity viewed special kind CSI applied DBNs, parents respect
variable passive provide context CSI. However, contrast CSI,
passivity assume context actually observed.

3. Technical Preliminaries
section introduces basic concepts notation used work. begin
brief discussion decision processes provide context work, followed
discussion dynamic Bayesian networks model perform inference.
3.1 Decision Processes, Belief States, Exact Updates
consider stochastic decision process wherein, time t, process state
st decision maker, agent, choosing action . executing st ,

process transitions state st+1 probability (st , st+1 ) agent receives

observation ot+1 probability (st+1 , ot+1 ). assume factored representations
state space observation space O, = X1 ... Xn = Y1 ... Ym ,
domains Xi , Yj finite. notation si used denote value Xi
state S, analogously oj O. Moreover, assume process
time-invariant, meaning independent t. framework compatible
many decision models used artificial intelligence literature, including POMDPs
(Kaelbling et al., 1998; Sondik, 1971) many variants.
agent chooses action based belief state bt (also known information state),
represents agents beliefs likelihood states time t. Formally,
belief state probability distribution state space process. Belief filtering
task calculating belief state based history observations. Ideally,
resulting belief state exact retains relevant information past
observations (this sometimes referred sufficient statistic; cf. Astrom, 1965).
exact update rule simple procedure produces exact belief states:
Definition 1 (Exact update rule). exact update rule defined follows: taking
action observing ot+1 , belief state bt updated bt+1 via
bt+1 (s0 ) =

X



bt (s) (s, s0 )

(1)

sS


bt+1 (s0 ) = bt+1 (s0 ) (s0 , ot+1 )

(2)

normalisation constant.
sometimes refer step bt bt+1 transition step step bt+1 bt+1
observation step. Unfortunately, space complexity storing exact belief states
time complexity updating using exact update rule exponential
number state variables, making infeasible complex systems large state
spaces. Hence, efficient approximate methods required.
1142

fiExploiting Causality Selective Belief Filtering DBNs

3.2 Dynamic Bayesian Networks
dynamic Bayesian network (DBN) (Dean & Kanazawa, 1989) Bayesian network
special temporal semantics specifies stochastic process transitions one
state another. DBNs used model effects actions stochastic decision
process. Specifically, compact representation transition function
observation function Oa action a:
Definition 2 (DBN). dynamic Bayesian network action a, denoted , acyclic
directed graph consisting of:




t+1 xt , xt+1 X ,
State variables X = xt1 , ..., xtn X t+1 = xt+1


1 , ..., xn
representing states process time + 1, respectively.


t+1 t+1 , representing obser Observation variables t+1 = y1t+1 , ..., ym
j
j
vation received time + 1.




Directed edges Ea X X t+1 X t+1 X t+1 X t+1 t+1 t+1 t+1 ,
specifying network topology dependencies variables.
Conditional probability distributions Pa (z | paa (z)) variable z X t+1 t+1 ,
specifying probability z assumes certain value given specific assignment
parents paa (z) = {z 0 | (z 0 , z) Ea }. convenience, define pata (Z) =
t+1 pa (Z), pa (Z) =
X paa (Z) pat+1


zZ paa (z).
(Z) = X
edges Ea distributions Pa define functions


0

(s, ) =

n


0
Pa xt+1
= s0i | paa (xt+1

) - (s, )



(3)

i=1

(s0 , o) =






Pa yjt+1 = oj | paa (yjt+1 ) - (s0 , o)

(4)

j=1
t+1
0
use notation paa (xt+1

) - (s, ) specify parents xi
t+1
0
X , respectively, assume corresponding values . Formally,
t+1
t+1

xtl pata (xit+1 ) xt+1
pat+1
= s0l0 . Similarly, use
(xi ), xl = sl xl0
l0
t+1
0
notation paa (yj ) - (s , o) specify parents yjt+1 X t+1 t+1 ,
respectively, assume corresponding values s0 o.

Xt

Example 2 (DBN representation robot arm). represent robot arm Example 1 set DBNs, one DBN action
{CW
, CCWi }.
at+1
t+1





state observation
variables
= 1 , 2t+1 , 3t+1 ,
n
DBNs X = 1 , 2 , 3 , X

t+1 = 1t+1 , 2t+1 , 3t+1 . make example realistic, let us assume
joint orientations bounded relative orientation immediately preceding joint
(e.g. form cone), first joint bounded relative ground.
means joint movement depends well preceding joint orientation, shown Figure 3. Moreover, joint orientations correlated (i.e. edges within
1143

fiAlbrecht & Ramamoorthy

1t

1t+1

1t+1

2t

2t+1

2t+1

3t

3t+1

3t+1

Xt

X t+1

t+1

Figure 3: DBN representation robot arm.
X t+1 ) joint exceed bound given preceding joint. Finally, observation variables depend solely corresponding joint variable. actions
example would differ variable distributions Pa .

3.3 Additional Definitions
useful define following:
xt+1

binary order defined X X t+1 xti xtj xt+1

j
t+1

1 < j n, xi xj 1 i, j n.
Given set Z X X t+1 , write Z denote tuple contains variables
Z, ordered .
Given ordered tuple Z = (zi1 , ..., zi|Z| ), define set S(Z) = Xi1 .... Xi|Z|
contain value tuples variables Z.
Given value tuple sZ = (si1 , ..., si|Z| ) S(Z), use notation Z - sZ
abbreviation zil = sil zil Z (i.e. variables Z assume
corresponding values sZ ).

4. Passivity
section introduces formal definition passivity, used basis
remainder article. provide simple procedure detect passive
variables process dynamics.
4.1 Formal Definition
outlined Section 1, state variable xt+1
called passive action exists

(in DBN ) xt+1 may change value
subset xt+1

parents

X


1144

fiExploiting Causality Selective Belief Filtering DBNs

least one variables subset changed value. Conversely, xt+1


change variables subset change. Formally, define passivity follows:
t+1

Definition 3 (Passivity). Let action given DBN

. state variable xi
t+1


called passive exists
set a,i paa (xi ) \ xi that:

t+1
(i) xtj a,i : xt+1
Ea
j , xi

(ii) two states st st+1 (st , st+1 ) > 0 :


sti = st+1
xtj a,i : stj = st+1
j


(5)

state variable passive called active.
set a,i corresponds subset variables described above: contains
variables directly affect xit+1 (i.e. parents xt+1
X ) xt+1
may


change value variables a,i changed value. sometimes say
variable xt+1
passive respect another variable xtj case

xtj a,i . Furthermore, omit obvious context.
Clause (i) Definition 3 requires xt+1
intra-correlated variables a,i ;

specifically, edge xt+1

xt+1
xtj a,i . example, see
j

Figure 1 assumed variable xt+1
passive respect variable
2
xt1 . (We discuss purpose clause next subsection.) Clause (ii) defines
core semantics passivity requiring xt+1
remains unchanged variables

a,i remain unchanged. Note means distribution Pa xt+1
may specify

deterministic stochastic behaviour variables a,i change values.
includes xt+1
may change value all.

state variable xit+1 passive even parents X , none
xti . case, set a,i would empty clause (i) well premise (5)
would trivially hold true. However, variable passive change
value circumstances. words, would constant.
case, one consider removing variable state description order reduce
computational costs.
noted Section 2.3, passivity shown special kind context-specific
independence (CSI) (Boutilier et al., 1996) applied DBNs. Here, associated set a,i
passive variable xt+1
provides context: given assignment values xtj a,i (i.e.

t+1
context) xtj = xt+1
independent xtk , xt+1
xtk pata (xt+1
j , xi
) \ a,i
k
k 6= i. However, besides similarity, important difference passivity
CSI, passivity actually assume context observed. Thus,
passivity viewed kind CSI unobserved contexts. become clear
Section 5, describe filtering method exploits passivity.
4.2 Non-Example Passivity
purpose clause (i) definition passivity? all, discussed
previously, clause (ii) captures core idea passivity, variable may
change value variables respect passive changed value.
1145

fiAlbrecht & Ramamoorthy

xt1

xt+1
1

xt2

xt+1
2

Figure 4: Example process clause (ii) insufficient.
However, may seem intuitive clause (ii) sufficient passivity,
fact processes clause (ii) alone suffice. words, clause (ii)
necessary sufficient passivity. illustrate following example:
Example 3 (Non-example passivity). Consider process two binary state variables,
x1 , x2 , single action, a, shown Figure 4. (We omit observation variables
clarity.) dynamics process xt+1
takes value xt2 xt+1
takes
1
2
value xt1 (i.e. x1 x2 swap values time step). process,
state variables satisfy clause (ii) Definition 3: set x01 = x02 (i.e. initial values),
(st , st+1 ) positive states st = st+1 , hence (5) true. set x01 6= x02 ,
(st , st+1 ) positive states st , st+1 sti 6= st+1
, {1, 2}, hence (5)
trivially true since premise false.

Despite satisfying clause (ii), state variables xt+1
xt+1
Example 3
1
2
fact passive, following two reasons: Firstly, passivity causal relation
must imply causal order (Pearl, 2000). However, causal order
x1 x2 , edge xt+1
xt+1
1
2 . Secondly, passivity means
variable may change value another variable respect passive (a
variable a,i ) changed value. words, whether passive variable xt+1

may change value depends past values a,i (at time t) new values
a,i (at time + 1). However, variables Example 3 depend values
time t, hence values time + 1 predetermined depend whether
variables a,i change values.
first issue, namely causal order, addressed adding corresponding edges X t+1 . instance, Example 3 could add edge xt+1
xt+1
1
2
establish causal order. However, generally solve second issue,
every passive variable xit+1 must depend past new values variables a,i . words, xit+1 must inter-correlated well intra-correlated
variables a,i . former given definition (since every variable a,i
parent xt+1
) latter precisely required clause (i) Definition 3.
Therefore, clauses (i) (ii) together define formal meaning passivity.
4.3 Detecting Passive Variables
mentioned Section 1, passivity latent causal property sense
extracted process dynamics without additional information, additional
assumptions regarding representation variable distributions. order determine
1146

fiExploiting Causality Selective Belief Filtering DBNs


Algorithm 1 Passive(xt+1
, )

1:


Input: state variable xt+1
, DBN

Output: a,i xt+1
passive , else false


t+1
3: Q OrderedQueue P pata (xi ) \ xti
// ascending order |a,i |
2:

4:

Q 6=

5:

a,i NextElement(Q)

6:

Q Q \ {a,i }

7:

xtj a,i


t+1
xt+1
6 Ea
j , xi

8:
9:
10:
11:
12:
13:
14:
15:
16:

Go line 4 // clause (i) violated

a,i paa (xt+1
) \ a,i xti

n

t+1

t+1

x
|
x
a,i
j
a,i
j
S(a,i ), S(a,i ), si Xi


t+1
=s ,
Pa xt+1
=

|
x

,


,




a,i
a,i


< 1


a,i
Go line 4 // clause (ii) violated
return a,i
return false

variable xit+1 passive , one find set a,i clauses Definition 3
satisfied. simple procedure representation variable
distributions given Algorithm 1. algorithm takes inputs variable xt+1


t+1


DBN , checks whether xi passive searching set a,i satisfies
clauses Definition 3. Note power set P line 3 includes empty set ,
hence accounts a,i = . Lines 7 9 check clause (i) satisfied lines
10 14 check clause (ii) satisfied. Line 13 essentially checks (5) holds true.
clauses satisfied, xt+1
passive respect variables a,i ,

algorithm returns set a,i . Otherwise, algorithm returns logical false.2
time complexity Algorithm 1 exponential worst case, xt+1


passive. Specifically, time requirements line 4 grow exponentially number
parents xt+1
X , time requirements line 12 grow exponentially

cardinality a,i a,i . However, time requirements reduced significantly
committing specific representations variable distributions Pa . example,
distributions represented tabular form, one utilise arrays indices
perform sweeping tests (5), i.e. line 13. Moreover, important realise
algorithm needs performed state variable, prior start
2. Strictly speaking, Algorithm 1 checks property stronger passivity
check (st , st+1 ) > 0 (cf. clause (ii)) line 12. However, algorithm modified include
check. omit exposition order highlight core ideas behind algorithm.

1147

fiAlbrecht & Ramamoorthy

process demand. since passivity invariant process states.
words, variable passive , always passive . Therefore, suffices
check advance passivity.
Note set a,i necessarily unique. example, consider
variable xt+1
1

passive respect variables xt2 xt3 , i.e. a,1 = xt2 , xt3 , assume
xt+1
changes
x3t+1 changes(i.e.
2

change time). Then,
0

00

easy verify a,1 = x2 a,1 = x3 satisfy clauses (i) (ii), hence
a,1 , 0a,1 , 00a,1 valid sets definition passivity. guiding principle
cases Occams razor, which, intuitively speaking, states simplest explanation
suffices. case, means suffices use smallest set a,i terms
cardinality |a,i |. (Hence, line 3 Algorithm 1 sorts queue Q ascending order
|a,i |.) rationale exist multiple causal explanations passive variable
xt+1
, one involving fewest key variables favoured since reduces
(compared alternative explanations) number cases would
revise beliefs xit+1 . earlier example, accept a,1 causal explanation
t+1
xt+1
every time xt+1
xt+1
may
1 , would revise beliefs x1
2
3
0
changed values. However, accept a,1 causal explanation, would
revise belief x1t+1 xt+1
may changed value. difference
2
become obvious Section 5.2, explains passivity exploited
reduce computational costs.

5. Passivity-based Selective Belief Filtering
section presents Passivity-based Selective Belief Filtering (PSBF) method,
exploits passivity efficient filtering. discussed Section 3, assume process
specified set dynamic Bayesian networks contains one DBN
action A. Therefore, whenever refer action (e.g. , , Pa , paa ),
assumed context .
PSBF follows general two-step update procedure belief state first
propagated process dynamics (transition step) conditioned
observation (observation step). Thus, natural divide exposition PSBF
three parts: (1) belief state representation, (2) transition step, (3) observation
step. discussed Sections 5.1, 5.2, 5.3, respectively. summary PSBF
given Section 5.4. discuss computational complexity error bounds
PSBF Sections 5.5 5.6, respectively.
5.1 Belief State Representation
Recall Section 1 principal idea behind PSBF maintain separate beliefs
individual aspects process, exploit passivity order perform selective
updates separate beliefs. union individual aspects constitutes complete
state description process. Therefore, belief state represented product
separate beliefs individual aspects.
capture informal notion individual aspects formally form clusters,
defined follows:
1148

fiExploiting Causality Selective Belief Filtering DBNs

C1
1t

1t+1

C1

C1
1t

1t+1

1t+1

1t+1

1t

1t+1

2t+1

2t

2t+1

2t+1

2t

2t+1

2t+1

3t+1

3t

3t+1

3t+1

3t

3t+1

3t+1

1t+1

C2
2t

2t+1
C3

3t

3t+1

C2
(a) C1 , C2 , C3

(b) C1

(c) C1 , C2

Figure 5: Three clusterings robot arm DBN.
Definition 4 (Cluster). clustering X t+1 set C = {C1 , ..., CK } satisfies
k : Ck X t+1 C1 ... CK = X t+1 . refer elements Ck C clusters.
underlying idea behind concept clusters variables cluster Ck
connected important sense. Specifically, two variables common
cluster, exists relation variables regarding likelihood
values may assume. words, variables correlated X t+1 .
number K concrete choice clusters Ck specified user
generated automatically. example, may specified manually domain expert
familiar structure modelled system, generated automatically using
methods ones described Section 6.1. stressed, however,
order reduce computational costs, advisable follow general rule small
possible, large necessary choosing clusters (see Section 5.5 discussion
computational complexity). Therefore, two variables strongly correlated,
presumably common cluster, whereas weakly
correlated (weakly meaning correlation ignored safely),
separate clusters order reduce computational costs. illustrated
following example:
Example 4 (Clusters robot DBN). Recall robot arm DBN Example 2, specifit+1 given three clusters
cally Figure
cluster
state
t+1 3. One way
t+1
t+1
variables X
C1 = 1
, C2 = 2
, C3 = 3
, shown Figure 5a. clustering
efficient since minimises size cluster. However, clusters fail capture
important correlation joint orientation restricted preceding joint
orientation
i1 . Another

way cluster state variables given single cluster
C1 = 1t+1 , 2t+1 , 3t+1 , shown Figure 5b. clustering captures correlations
variables. However, largest possible cluster

and, therefore,

least effi
cient one. compromise given two clusters C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 ,
shown Figure 5c. clustering captures correlation joint orientations immediately preceding joint orientations, efficient
previous clustering since smaller clusters.

1149

fiAlbrecht & Ramamoorthy

Given definition clusters, capture informal notion separate beliefs
form belief factors:
Definition 5 (Belief factor). Given cluster Ck , corresponding belief factor bk
probability distribution set S(Ck ).
Intuitively, belief factor bk represents agents beliefs likelihood values
variables corresponding cluster Ck . analogy view belief factor
smaller belief state, view b full belief state combination
smaller belief states. However, distinguish two, refer b simply belief
state bk belief factor.
Finally, given clusters Ck corresponding belief factors bk , belief state b
represented factored form
b(s) =

K


bk (sk )

k=1



use notation sk refer tuple (si )xt+1 Ck . (E.g., Ck = xt+1
, xt+1
2
3

= (s1 , s2 , s3 , s4 ), sk = (s2 , s3 ).)
5.2 Exploiting Passivity Transition Step
order perform selective updates belief factors bk , require procedure
performs transition step independently factor.3 obtain procedure
introducing two assumptions allow us modify transition step (1) exact
update rule. assumptions guarantee transition step performed exactly,
sense (1). However, discuss shortly, assumptions violated obtain
approximate belief states.
first assumption, (A1), states clusters must uncorrelated (i.e.
edges X t+1 clusters), second assumption, (A2), states clusters
must disjoint. Formally, defined follows:
t+1
(A1) : xt+1
Ck pat+1
(xi ) Ck


(A2) k 6= k 0 : Ck Ck0 =
Note neither assumption implies other. is, may case (A1)
satisfied (A2) violated, vice versa. Assuming (A1) (A2),
reformulate (1)
X


0
bt+1
Tka (s, s0k )
btk0 (sk0 )
(6)
k (sk ) = 1
S(pat (Ck ))


k0 :[xt+1
Ck0 : xti pat (Ck )]



1 normalisation constant


0
Tka (s, s0k ) =
Pa xt+1
= (s0k )i | paa (xt+1

) - (s, sk ) .
xt+1
Ck


3. advantage belief factors updated parallel, useful feature
considering many platforms use parallel processing techniques.

1150

fiExploiting Causality Selective Belief Filtering DBNs

procedure performs transition step independently belief factor bk , hence
updated order parallel.
Assumption (A1) allows us bring (1) form updates belief
factors bk independently other. Specifically, (A1) allows us define cluster-based
transition function Tka , turn enables summation (6). Assumption (A2),
hand, guarantees product (6) correct. particular, may
case |sk0 | < |Ck0 | (i.e. fewer elements sk0 Ck0 ) variables
Ck0 patat (Ck ) (i.e. xt+1
Ck0 xti
/ patat (Ck )). cases, btk0

t+1
taken marginal distribution variables xi Ck0 xti patat (Ck ),
(A2) guarantees marginalisation introduces errors.
mentioned previously, assumption may violated obtain approximate belief
states. However, important distinction (A1) (A2) regard:
(A2) violated, (6) still well-defined sense still executed,
except product (6) may degrade accuracy results. contrast
(A1), structural requirement Tka sense Tka ill-defined without (A1).
since, (A1) violated, variables Ck may parents X t+1
0
Ck , case paa (xt+1
) - (s, sk ) would ill-defined. Thus, (A1) violated,
enforce modifying distributions Pa xt+1
Ck marginalise

t+1
variables pat+1
(x
)




C
,


clusters
C
. means

k
k


variable separate distribution every cluster contains variable, thereby
possibly introducing approximation error.
Given modified transition step (6), exploit passivity perform selective
updates belief factors bk . Recall Section 4.1 variable xt+1
passive

t+1

exists set a,i variables xi may change value
variables a,i changed value. causal connection used decide
whether values variables cluster Ck may changed, case
corresponding belief factor bk updated. Theorem 1 provides formal foundation:


Theorem 1. (A1) (A2) hold, xt+1
Ck passive ,


: bt+1
k (sk ) = bk (sk ).

Proof. Proof Appendix A.
Theorem 1 states clusters C1 , ..., CK disjoint uncorrelated,

variables cluster Ck passive , transition step corresponding
belief factor btk bt+1
omitted without loss information.
k
Theorem 1 translate situations (A1) (A2), both, violated?
key assumption (A1), states clusters must uncorrelated.
discussed earlier, enforce modifying variable distributions Pa cluster.
However, passive variable xit+1 Ck correlated (passive active) variable
t+1
t+1
xt+1
Ck0 , xt+1
pat+1
distribution Pa
(xi ), marginalising xj
j
j
t+1
t+1
xi typically cause xi lose passivity, sense would longer satisfy
clauses Definition 3. Consequently, would always perform transition
step Ck , even unmodified variables Ck passive. problematic
unnecessary computations, modified distributions
introduce error every time transition step performed.
1151

fiAlbrecht & Ramamoorthy

1t

1t+1

1t+1
C1

2t

2t+1

2t+1
C2

3t

3t+1

3t+1

Xt

X t+1

t+1

Figure 6: Robot arm DBN implementing action CW3 . Dashed circles mark passive state
variables. coloured ellipses represent clusters C1 C2 .
alleviate effect, one check chance unmodified variables
cluster would change values. shown case whenever
causal path active variable variable cluster:
Definition 6 (Causal path). causal path , active variable xt+1
another

t+1
t+1 (Q)
t+1
(1)
(2)
(Q)
(1)
variable xj , sequence hx , x , ..., x x = xi , x
= xj ,
1 q < Q :
(i) x(q) X t+1
(ii) x(q) , x(q+1) Ea
(iii) x(q+1) passive respect x(q)
Intuitively, causal path defines chain causal effects (such joints 1 3
Example 1): since active variable x(1) may changed value x(2) passive
respect x(1) , x(2) may changed value; since x(2) may changed
value x(3) passive respect x(2) , x(3) may changed value, etc.
Hence, absence observing changes, mere existence causal path
x(1) x(Q) reason revise beliefs x(Q) . Therefore, general update rule,
omit transition step btk bt+1
unmodified variables cluster Ck passive
k


, causal path active variable variable Ck .
demonstrated following example:
Example 5 (PSBF update rule robot arm DBN). Let us consider robot arm
previous examples. Figure 6 shows DBN implements action CW3 .
action rotates joint 3 robot arm 1 clock-wise (i.e. joint orientation 3t+1
direct target action). Therefore, variable 3t+1 active variables
1t+1 2t+1 passive (shown dashed circles).

use clustering C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 reasons given Example 4. Since 1t+1 parent 2t+1 , PSBF enforce assumption (A1)
1152

fiExploiting Causality Selective Belief Filtering DBNs

Algorithm 2 SkippableClusters(C, )
1:

Input: clustering C = {C1 , ..., CK }, DBN

2:

Output: set clusters C C skipped transition step

3:

C C

4:

Q OrderedQueue(X t+1 )

5:

C 6= Q 6=

6:
7:
8:
9:
10:
11:
12:
13:
14:

xt+1
NextElement(Q)



Q Q \ xt+1


Passive(xt+1
, )


C C \ Ck C | xt+1
Ck


xt+1
Q
j
t+1

CausalPath(xt+1
, xj , )
n

C C \ Ck C | xt+1
Ck
j
n

Q Q \ xt+1
j

return C

marginalising 1t+1 variable distribution Pa 2t+1 cluster C2 . modified variable distribution loses passivity property (both clauses Definition 3
violated), unmodified distribution 1t+1 still passive.
performing transition step, PSBF update belief factor b2
corresponding cluster C2 contains active variable 3t+1 . However, since variables
cluster C1 passive (there modified variables C1 ), since causal
path 3t+1 variable C1 , PSBF omit update belief factor b1 .
Intuitively, makes sense since change orientation joint 3 cannot cause
change orientations preceding joints. Note corresponds saving
50% transition step.


Algorithm 2 defines procedure utilises rule find clusters
transition step skipped. algorithm takes inputs clustering C DBN
, returns set C skippable clusters. essentially searches active
variables xt+1
removes clusters Ck C contain variables

causal path xit+1 . function OrderedQueue(X t+1 ) returns ordered
queue Q variables X t+1 . performance Algorithm 2 depends order
queue. experiments, obtained good performance ordering variables
descending order number outgoing edges. function NextElement(Q) returns

next element queue; function Passive(xt+1
, ) defined Algorithm 1;
t+1 t+1

function CausalPath(xi , xj , ) returns logical true
1153

fiAlbrecht & Ramamoorthy

causal path xt+1
xjt+1 .4 Note that, given invariance passivity process

states (cf. Section 4.1), suffices call Algorithm 2 (in advance needed)
determine clusters omit transition step.
5.3 Efficient Incorporation Observations
PSBF perform observation step similarly exact update rule (2),
conditions propagated belief state bt+1 observation ot+1 obtain fully updated
belief state bt+1 . However, given factored belief state representation used PSBF,
require procedure respects factorisation observation step. Assuming
(A1) (A2) hold, bring (2) form updates belief factors bk
independently
X


t+1 0
0
0
bt+1
(s, ot+1 )
bt+1
(7)
k (sk ) = 2 bk (sk )
k0 (sk )
t+1 )) : = s0 k 0 6= k : C 0 pat+1 (Y t+1 ) 6=
S(pat+1
k
k
(Y

k




2 normalisation constant. Note that, analogously (6), variables
t+1 ), bt+1 taken marginal distribution
Ck0 pat+1
k0
(Y
t+1
t+1
Ck0 paat (Y
). Assumption (A2) guarantees marginalisation introduces
errors. (A1) (A2) hold, transition step (6) observation step (7)
produce exact belief states sense (1) (2), regardless many clusters
skipped transition step (cf. Theorem 1).
observation step (7) updates belief states uses observation variables
process. words, ignores internal structure observation variables.
However, clear variables cluster Ck marginally independent
observation variables t+1 (this determined using d-separation (Geiger et al., 1989),
simply checking directed path Ck t+1 ), need
perform observation step corresponding belief factor bk . expressed
formally Theorem 2:


Theorem 2. xt+1
Ck marginally independent yjt+1 t+1 ,

t+1
: bt+1
k (sk ) = bk (sk ).

Proof. Proof Appendix B.
Theorem 2 states variables Ck independent t+1 ,
observation step bk skipped. However, even Ck independent t+1 , may
case variables Ck depend subset Yk t+1 observation
variables. Clearly, cases, suffices use Yk rather t+1 observation
step. account this, first note variables t+1 may correlated
other. preserve correlations, subdivide t+1 clusters Cl t+1
introduce following assumptions:


(A3) : yjt+1 Cl paa (yjt+1 ) t+1 Cl
(A4) l 6= l0 : Cl Cl0 =
4. simple way implement function modify standard graph search method (such breath-first
search) check (iii) Definition 6, apply variables X t+1 edges Ea .

1154

fiExploiting Causality Selective Belief Filtering DBNs

Assumptions (A3) (A4) analogous (A1) (A2), respectively, essentially
serve purposes observation step. distinguish clusters Ck Cl ,
sometimes refer former state cluster latter observation cluster.
Assuming (A3) (A4) hold, redefine observation step
X


t+1 0
0

t+1
0
bt+1
(s
)
=

b
(s
)

(s,

)
bt+1
(8)
2
k
k
l
k
k
k0 (sk )
t+1
0
0
l: Cl Yk 6= S(pat+1
(Cl )) : sk = sk k 6= k : Ck0 pa (Cl ) 6=



al (s, ot+1
l ) =







t+1
t+1
Pa yjt+1 =(ot+1
)
|
pa
(y
)
(s,

)
j
j
l
l

yjt+1 Cl

Yk t+1 set observation variables marginally independent
variables Ck .
Given Theorem 2, one see (8) equivalent (7) observation variables
clustered (or, equivalently, single observation cluster Cl = t+1 ). However,
important note observation variables clustered (i.e. multiple
observation clusters Cl ), (8) notQnecessarily
equivalent
P
P(7).
QmTo see this, helpful
compare abstract formulations

(o
)
b

j=1
j

j=1 (oj ) bs ,
former corresponds (8) latter (7). Therein, (o1 , ..., om ) observation, bs
probability state S, (oj ) probability observing yj = oj
s. abstract formulations equivalent = 1 bs = 1 s,
cases may equivalent. Nonetheless, fix number observation
variables m, (8) approximates (7) closely increase number state variables
n. experiments indicate often suffices use state variables
observation variables order obtain good approximations.
Finally, show suffices perform observation step bk using
clusters Cl whose variables independent variables Ck , observe (8)
fact repeated application (7) every Cl , updated belief factor bt+1

k
t+1
used place bk subsequent application. Since every application
form (7) (with t+1 = Cl ), conclude Theorem 2 holds, hence observation
step skipped clusters Cl independent Ck .
5.4 Summary PSBF
preceding sections summarised follows:
Representation: belief state bt represented product K belief factors btk ,
Q


bt (s) = K
k=1 bk (s). belief factor bk probability distribution
set S(Ck ), Ck X t+1 cluster correlated state variables.
Transition step: transition step btk bt+1
performed using (6), clusters
k


Ck include active variables , causal path

active variable . clusters skipped.
Observation step: observation step bt+1
bt+1
performed using (8),
k
k
clusters Ck dependent observation variables t+1 , using
observation clusters Cl relevant Ck . clusters skipped.
1155

fiAlbrecht & Ramamoorthy

Algorithm 3 PSBF(at , ot+1 , (btk )Ck C | C, C, (a )aA )
1:

Input: action , observation ot+1 , belief factors (btk )Ck C

2:

Parameters: state clustering C, observation clustering C, DBNs (a )aA

3:

Output: updated belief factors (bt+1
k )Ck C

4:

// Transition step

5:

C SkippableClusters(C, )

6:

Ck C

7:
8:
9:
10:
11:



Ck C
bt+1
btk
k
else
s0k S(Ck )
X

0
bt+1
Tka (s, s0k )
btk0 (sk0 )
k (sk ) 1
S(patat (Ck ))

12:

k0 :[xt+1
Ck0 : xti patat (Ck )]


// Observation step

Ck C
n


14:
Yk yjt+1 t+1 | directed path Ck yjt+1
13:

15:

Yk =

16:

bt+1
bt+1
k
k

17:
18:
19:

else
s0k S(Ck )
t+1 0
0
bt+1
k (sk ) 2 bk (sk )



X



al (s, ot+1 )



0
bt+1
k0 (sk )

(Cl ) 6=
Cl C : Cl Yk 6= S(pat+1
(Cl )) : sk = s0k k0 6= k : Ck0 pat+1



20:

return

(bt+1
k )Ck C

Algorithm 3 provides procedural specification PSBF. algorithm takes inputs
action time t, , subsequent observation time + 1, ot+1 , belief factors
time t, btk . internal parameters state clustering C, observation clustering
C, set DBNs (a )aA define process. Lines 4 11 implement
transition step lines 12 19 implement observation step. Note suffices
execute lines 5 14 advance (or demand) remember results
future reference. algorithm returns updated belief factors bt+1
k .
1156

fiExploiting Causality Selective Belief Filtering DBNs

5.5 Space Time Complexity
belief factor bk one elementP
bk (sk ) sk S(Ck ).5 Thus, total space required
maintain K belief factors bk K
k=1 |S(Ck )|. Furthermore, size set S(Ck ) grows
exponentially number variables Ck , hence dominant growth factor
space requirement given largest cluster Ck |Ck | = maxk0 |Ck0 |. Therefore,
space complexity PSBF O(exp maxk |Ck |), hence representation feasible
reasonably small clusters Ck .
Similarly, numberPof operations required perform transition observation
steps order 2 K
k=1 |S(Ck )| worst case (i.e. clusters need updated
steps). Specifically, line 11 line 19 Algorithm 3 executed
every sk Ck . dominant growth factor given largest cluster Ck , hence
time complexity PSBF O(2 exp maxk |Ck |) = O(exp maxk |Ck |). Note
assumes analysis performed lines 5 14 Algorithm 3 done advance.
time complexity worst case, clusters need updated
transition observation steps. difficult derive time complexity
average case unclear average case terms passivity. Even
stipulate certain average degree passivity (e.g. 50% variables passive), would
still difficult make general statement time requirements since depends
crucially passive variables distributed across clusters. example, even
process average 90% passivity, one active variable cluster
every cluster would need updated transition step. Thus, general
statement make regards passivity time complexity PSBF
refined O(exp maxCk CT CO |Ck |), CT CO include clusters
need updated transition observation step, respectively.
5.6 Error Bounds
five possible sources approximation errors PSBF:
clusters correlated (i.e. (A1) (A3) violated)
clusters overlapping (i.e. (A2) (A4) violated)
Generally (8) multiple observation clusters Cl used
first two cases, approximation error depends amount correlation
overlap. little correlation overlap clusters,
approximation error expected small. Conversely, clusters strongly
correlated overlapping, approximation error expected large.
Boyen Koller (1998) provide useful analysis error bound filtering
method uses factored belief state representation. Since PSBF uses factored
representation, analysis applies directly PSBF. purpose section
restate main result analysis context work.
analysis uses concept relative entropy (Kullback & Leibler, 1951)
measure similarity belief states:
5. practice, suffices store |S(Ck )| 1 elements, irrelevant analysis.

1157

fiAlbrecht & Ramamoorthy

Definition 7 (Relative entropy). Let two probability distributions defined
set X. relative entropy defined
KL(||) =

X
xX

(x) ln

(x)
(x)

(x) > 0 (x) > 0.
Similar Boyen Koller (1998), define approximation error incurred PSBF
relative exact belief state. However, since consider decision process multiple
actions (represented DBNs ), define error action respectively:
Definition 8 (Approximation error). Let b exact belief state b approximation PSBF. taking action a, let b0 exact update b (using (1) (2))
b0 PSBF-update b (using (6) (8)). Furthermore, let b0 exact update
b (using (1) (2)). say PSBF incurs error relative b0
KL(b0 ||b0 ) KL(b0 ||b0 ) .
analysis relies concept mixing rates. Intuitively, mixing rate
DBN quantifies degree stochasticity . depends mixing rates ka
individual clusters Ck :
Definition 9 (Mixing rate). mixing rate cluster Ck X t+1 defined
ka = 0min
00

,s

X



min Tka (s0 , s), Tka (s00 , s) .

sS(Ck )

Ck satisfy (A1) (A2), observation variables t+1 one observation
cluster, mixing rate given = (mink ka /r)q cluster Ck
depends r influences q clusters Ck0 6=k (Boyen & Koller, 1998).
worst case (that is, (A1A4) violated), minimal mixing rate given ka
single cluster Ck = X t+1 .
Finally, main result work Boyen Koller (1998), restated
context work Theorem 3, essentially states approximation error PSBF
(measured terms relative entropy) bounded mixing rates process:
Theorem 3 (Boyen & Koller, 1998). Let bt exact belief state bt approximation PSBF using clusters Ck . Then, states ~s = (s0 , ..., st ) actions
~a = (a0 , ..., at1 ),
h
max

~a
Eo1 ,...,ot KL(bt ||bt )
mina ~a
expectation E Q
taken possible sequences observations o1 , ..., ot
1

+1 , +1 ), defined above.
probabilities P (o , ..., ) = t1
=0 (s
1158

fiExploiting Causality Selective Belief Filtering DBNs

Process size

# x vars (n)

# vars (m)

# states (|S|)

# obs. (|O|)



10

3

> one thousand

8



20

6

> one million

64

L

30

9

> one billion

512

XL

40

12

> one trillion

4096

Table 1: Synthetic process sizes. variables binary.

6. Experimental Evaluation
evaluated PSBF two experimental domains: Section 6.1, evaluated PSBF
synthetic (i.e. randomly generated) processes varying sizes degrees passivity.
Section 6.2, evaluated PSBF simulation multi-robot warehouse system. brief
summary experimental results given Section 6.3.
6.1 Synthetic Processes
first evaluated PSBF series synthetic processes. PSBF compared selection
alternative methods, including PF (Gordon et al., 1993), RBPF (Doucet et al., 2000), BK
(Boyen & Koller, 1998), (Murphy & Weiss, 2001); see Section 2 discussion
methods. algorithms implemented Matlab 7.13, used Matlab
toolbox BNT (Murphy, 2001) implement BK FF.
6.1.1 Specification Synthetic Processes
generated synthetic processes four different sizes specified Table 1.
process generated follows:
First, variable xit+1 chosen passive probability p, case
add edge (xti , xt+1
). refer p degree passivity. sample

t+1
edges X /X
X t+1 , generate mixture Gaussians G using Algorithm 4 (see
Appendix C). Figure 7 shows example G generated process size M. set
G used produce areas correlated variables (i.e. Gaussians),
constitute natural candidates state clusters.
Let vector maximum densities Gaussian G, let
vector densities value N. Then, every combination j, edge (xti , xt+1
j )
2
added probability equal maximum element j / , operators
point-wise. xt+1
chosen passive, edge (xti , xt+1

j ) added
t+1 t+1
t+1 t+1
< j. case, add edge (xi , xj ). Edges (xi , xj ) added similarly
t+1
< j,6 add edge (xti , xt+1
j ) passive xj . ensure every
variable effect generated process, xti connected least one xt+1
j
t+1
X t+1 (adding
(adding (xti , xt+1
)

necessary)


x


least
one
parent

X

j
6. condition < j cases ensure resulting DBN acyclic.

1159

fiAlbrecht & Ramamoorthy

0.35
0.3

Density

0.25
0.2
0.15
0.1
0.05
0

2

4

6

8

10

12

14

16

18

20

i, j

Figure 7: Example mixture Gaussians generated process size consisting
t/t+1
three Gaussians. closer two variables xi
xt+1
peak common
j
Gaussian, higher probability edge added them.
t+1 t+1
(xtj , xt+1
j ) necessary). Finally, edges (xi , yj ) added probability 0.1,
i, j, ensuring yjt+1 least one parent X t+1 .
variables process binary. Passive variables assumed passive
respect parents X . distributions Pa xt+1
X t+1 generated

t+1
uniformly randomly without bias. passive variables xi , modify Pa satisfy clause
(ii) Definition 3. distributions Pa yjt+1 t+1 generated probability
sampled uniformly either [0.0, 0.2] [0.8, 1.0], obtain meaningful observations.
Finally, every process consists two actions. obtained randomly choosing
one three variables xt+1
whose distributions Pa resampled

edges X added probability 0.1 (passive variables chosen way longer
passive). simulations, actions chosen uniformly randomly.
process starts random initial state, algorithms tested
sequence processes, initial states, chosen actions, random numbers.

6.1.2 Clustering Methods
used three different clustering methods, denoted hpci, hmorali, hmodisi. methods
applied variables X t+1 without edges involving X t+1 :
hpci drops directions edges (i.e. edge xt+1
xt+1
ads reverse

j
t+1
t+1
edge xj xi ) puts variables (undirected) path
one cluster. definition, resulting clusters satisfy assumptions (A1A4).
hmorali connects parents variable drops directions (it moralises
variables) extracts clusters fully connected variables (maximum cliques).
resulting clusters may satisfy assumptions (A1A4).
hmodisi similar hmorali truncates resulting clusters make disjoint
(clusters removed become subset another cluster). definition,
resulting clusters satisfy (A2/A4), necessarily (A1/A3).
example, consider Figure 5 Section 5.1. Here, hpci would produce cluster
C1 Figure 5b, since variables connected undirected path. Furthermore,
1160

fiExploiting Causality Selective Belief Filtering DBNs

hmorali would produce two clusters C1 C2 Figure 5c, correspond
two maximum cliques moralising variables X t+1 . Finally, hmodisi would produce
cluster C1 Figure 5c cluster C3 Figure 5a.
PSBF used clustering method generate clusters state variables (Ck )
observation variables (Cl ). Moreover, PSBF enforced (A1/A3) whenever necessary
modifying variable distributions described Section 5.1.
6.1.3 Accuracy
order compare accuracy tested algorithms, computed relative entropy
(cf. Definition 7) exact belief states obtained using exact update rule (cf. Definition 1)
approximate belief states produced tested algorithms. However, since exact
belief states relative entropy hard compute large processes, able
compare accuracy algorithms processes size only. algorithms initialised
uniform belief states, uniformly sampled particles.
first compared accuracy PSBF BK, since use factorisation
belief state representations. Figure 8 shows relative entropy PSBF BK averaged 1000 processes 0%, 20%, 40%, 60%, 80%, 100% passivity, respectively.
results show PSBF hpc/modisi produced lower relative entropy (i.e. higher accuracy) BK hpc/modisi, PSBF hmorali produced relative entropy comparable
BK hmorali. indicates violations (A2/A4) introduce smaller errors
violations (A1/A3). Note PSBF BK convergent behaviour
relative entropy, shows approximation error due factorisation
bounded, discussed Section 5.6. interesting since PSBF BK obtain approximation errors factorisation different ways: PSBF loses accuracy modifying
variable distributions ensure state clusters independent (cf. Section 5.2),
BK loses accuracy marginalising original factorisation inference (i.e.
projection step; cf. Section 2.1). Nevertheless, shown results, resulting
approximation errors bounded cases, similar convergence.
Note relative entropy methods increased degree passivity
process. explained fact higher passivity implies higher determinacy
and, therefore, lower mixing rates (cf. Definition 9), crucial factor error
bounds PSBF BK (cf. Theorem 3). Finally, note PSBF produce exact
belief states (i.e. zero relative entropy) using hpci clustering, despite fact
clusters generated hpci satisfy assumptions (A1A4). However, discussed detail
Sections 5.3 5.6, another possible source approximation errors multiple observation
clusters used, often case using hpci produce observation clusters.
compare accuracy PF/RBPF PSBF/BK, number samples used
PF/RBPF chosen automatically process required approximately
much time per belief update PSBF hmorali BK hmorali, respectively.
experiments, meant PF (RBPF) able process 100 300 (20
50) samples. However, since process 1000 states, nearly enough
represent uniform belief state. Hence, PF/RBPF produced much higher relative entropy
PSBF/BK. Moreover, fact processes high variance means
PF/RBPF would require many samples achieve accuracy PSBF/BK (as
1161

fiAlbrecht & Ramamoorthy

2

0.6
0.4

BK (pc)

PSBF (pc)

BK (moral)

PSBF (moral)

BK (modis)

PSBF (modis)

Relative entropy

Relative entropy

0.8

0.2
0

0

500

1000

1500
2000
Transition

2500

1.5
1
0.5
0

3000

0

500

(a) 0% passivity

1000

1500
2000
Transition

2500

3000

2500

3000

2500

3000

(b) 20% passivity

5
Relative entropy

Relative entropy

3

2

1

4
3
2
1

0

0

500

1000

1500
2000
Transition

2500

0

3000

0

500

8

6

4

2

0

1500
2000
Transition

(d) 60% passivity

Relative entropy

Relative entropy

(c) 40% passivity

1000

0

500

1000

1500
2000
Transition

2500

3000

(e) 80% passivity

6
4
2
0

0

500

1000

1500
2000
Transition

(f) 100% passivity

Figure 8: Accuracy results PSBF BK. Plots show relative entropy exact
algorithms belief states (lower better). Results averaged 1000 processes size
(n = 10, = 3), average 0%100% non-target variables passive (cf.
Section 6.1.1). PSBF/BK used clustering methods hpci, hmorali, hmodisi.
shown next section). One would expect latter issue alleviated use
exact inference RBPF (cf. Section 2.1). However, case much
variance process captured marginal distributions used particles
RBPF. contrast, synthetic processes exhibit high variance across variables,
1162

fiExploiting Causality Selective Belief Filtering DBNs

automatic grouping7 state variables sampled exact variables still contained
much variance sampled variables. Hence, RBPF required significantly samples
number could process time provided.
Finally, order compare accuracy PSBF/BK, number iterations
used (more precisely, number iterations loopy belief propagation; cf. Murphy &
Weiss, 2001) chosen automatically process required approximately
much time per belief update PSBF hmorali BK hmorali, respectively. However,
often able perform several iterations provided time, resulting
relative entropy substantially higher PSBF/BK. problem
designed specific class DBN topologies, namely containing
edges within X t+1 (called regular DBNs Murphy & Weiss, 2001). allows
use fully factored representation belief states, variable
belief factor. However, processes used experiments high intra-correlation
state variables (i.e. many edges X t+1 ), especially increasing passivity.
correlations cannot captured belief state representation FF, resulting
significantly higher relative entropy PSBF/BK.
6.1.4 Timing
measured computation times processes sizes S, M, L, XL passivities 25%,
50%, 75%, 100%, respectively. PSBF BK used hmorali clustering, seemed
appropriate fair comparison since produced consistently similar accuracy
algorithms. number samples used PF chosen automatically process
PF achieved average accuracy approximately good PSBF
BK, respectively, final 20% process. involved computing exact belief
states relative entropies, able use PF processes size only. omit
RBPF section shown previous section unsuitable
processes consider. PSBF tested 1, 2, 4 parallel processes,
allocated approximately number belief factors.
Figures 9a 9d show times 1000 transitions averaged 1000 processes,
Figure 9e shows average percentage belief factors updated transition
observation steps PSBF. timing reported PSBF includes time taken
modify variable distributions (in case overlapping clusters) detect skippable clusters
transition observation steps, done advance
action. results show PSBF able minimise time requirements significantly
exploiting passivity. First, note marginal gains 25% 50%
passivity, despite fact PSBF updated 14% fewer clusters transition step.
clusters mostly small. However, significant gains
50% 75% passivity average speed-ups 11% (S), 14% (M), 15% (L), 18% (XL),
7. open question group state variables sampled exact variables (Doucet et al.,
2000). used simple heuristic whereby set sampled variables contained variables xt+1


parents X t/t+1 none xti . remaining variables X t+1 constituted set
exact variables. ensure resulting grouping valid actions (i.e. DBNs) process,
considered edges involved DBNs; is, performed grouping union Ea
a. Moreover, improve efficiency, subdivided set exact variables clusters
variables connected undirected edges X t+1 without edges involving sampled variables.

1163

fi100
PF:BK
PF:PSBF
BK

50

0

25%

50%
75%
Passivity

400

200

150

PSBF1 100
PSBF2
PSBF4
50

100%

(a) (n=10, m=3)

0

25%

50%
75%
Passivity

Seconds 1000 transitions

150

Seconds 1000 transitions

Seconds 1000 transitions

Seconds 1000 transitions

Albrecht & Ramamoorthy

350
300
250
200
150
100
50

100%

0

(b) (n=20, m=6)

25%

100%

(c) L (n=30, m=9)

100
% updated belief factors

50%
75%
Passivity

700
600
500
400
300
200
100
0

25%

50%
75%
Passivity

100%

(d) XL (n=40, m=12)

(trans)
(obs)
(trans)
(obs)

80
60
40
L (trans)
L (obs)
XL (trans)
XL (obs)

20
0

25%

50%
75%
Passivity

100%

(e) Updated belief factors

Figure 9: Timing results. (ad) Average number seconds required 1000 transitions
UNIX dual-core machine 2.4 GHz, sizes S, M, L, XL. Passivity p% means
average p% non-target variables passive (cf. Section 6.1.1). PSBF BK
used hmorali clustering. PF optimised binary variables used number samples
achieve accuracy PSBF BK, respectively. PSBF run 1 (PSBF-1), 2
(PSBF-2), 4 (PSBF-4) parallel processes. (e) Average percentage belief factors
updated transition observation steps, respectively.
75% 100% passivity average speed-ups 11% (S), 33% (M), 46% (L),
49% (XL). shows computational gains grow significantly
degree passivity size process.
results show PSBF consistently outperformed BK process sizes.
two main computational savings PSBF relative BK: firstly, skipping belief
factors transition observation steps, secondly, perform
potentially expensive projection step restore original factorisation inference.
However, times algorithms grew exponentially size process,
note relative difference PSBF BK decreased significantly lower
degrees passivity. instance Free Lunch (see Section 7 discussion),
means PSBF performs best processes high passivity suffer
performance processes lack passivity. Specifically, computational overhead
modifying variable distributions detecting skippable belief factors amortise
1164

fiExploiting Causality Selective Belief Filtering DBNs

effectively large processes low passivity. Furthermore, low passivity, PSBF
often perform full transition observation steps (i.e. update belief factors
step), costly large processes.
BK PF affected passivity? surprisingly, performance BK
nearly unaffected increasing degrees passivity. junction tree algorithm used
BK benefited marginally increased sparsity process, computational
gains minimal. first unable use PF required many samples
(between 10k 200k) achieve comparable accuracy PSBF/BK, due
high variance processes. order investigate effect passivity PF,
implemented version PF strictly optimised binary variables. Interestingly,
found passivity adverse effect performance PF, requiring use
exponentially samples increased passivity (see Figure 9a). makes sense
view PF factored approximation method (such PSBF BK) means
analysis Section 5.6 applies. However, PF puts variables single cluster
(since actually factored method), mixing rate process much lower
PSBF BK (as discussed Section 5.6) and, thus, error bounds less
tight. compensate this, PF requires significantly samples increased passivity.
6.2 Multi-robot Warehouse System
section, demonstrate passivity occur naturally complex system
PSBF exploit accelerate filtering task. end, consider
multi-robot warehouse system style Kiva (Wurman et al., 2008),
robots task transport goods within warehouse (cf. Figure 10a).
6.2.1 Specification Warehouse System
Figure 10b shows initial state warehouse simulation. warehouse consists
2 workstations (W1, W2), 4 robots (R1R4), 16 inventory pods (I1I16). robot
move forward backward, turn left right, load unload inventory pod (if
positioned pod), nothing. Kiva, robots move inventory pods
unless carrying pod, case pods become obstacles. move
turn operations stochastic robot may move/turn far (3% chance)
nothing (2% chance). robot possesses two sensors, one telling inventory
pod loaded (if any) one direction facing. direction sensor noisy
random direction may reported (3% chance).
robot maintains list tasks form Bring inventory pod workstation
W (yellow area around W) Bring inventory pod position (x,y). tasks
executed depends control mode, use two simulations:8
8. control modes ad hoc often make suboptimal decisions. However, found current
solution techniques (DEC-)POMDPs, including approximate methods, infeasible setting.
Nonetheless, quality decisions made control modes largely depends accuracy
belief states, hence important belief states updated accurately. Therefore,
control modes sufficient purposes.

1165

fiAlbrecht & Ramamoorthy

(a) Kiva warehouse system

(b) Initial state simulation

Figure 10: (a) Kiva warehouse system (image reproduced DAndrea & Wurman, 2008).
Robots (orange coloured) transport shelfs goods workstations. (b) Initial
state warehouse simulation. warehouse consists 2 workstations (W1, W2), 4
robots (R1R4), 16 inventory pods (I1I16).
Centralised mode: central controller maintains belief state bt state
warehouse system. time t, samples 100 states bt removes

duplicate states, resulting set
P t= {s1 , s2 , ...}. resamples state



probabilities w(s ) = b (s )/ q b (sq ). Based current task
robot, performs search (Hart, Nilsson, & Raphael, 1968) (with Manhattan
distance) space joint actions find optimal action robot.
executing actions, robots send sensor readings controller,
controller updates belief state using sensor readings.
Decentralised mode: robot maintains belief state communication robots. knowledge robots
current tasks, communicated task allocation module. time t,
robot samples set state done centralised mode. Treating
robots static obstacles, performs search based current
task find action . repeated robot r states sq S,
resulting actions ar,q used
P obtain distributions r : [0, 1] (A
set actions) r (a) = q : ar,q =a w(sq ). robot executes action updates belief state using sensor readings distributions r
average robots actions.
tasks generated external scheduler time intervals sampled U [1, 10].
generated task assigned one robots sequential auction (Dias, Zlot,
Kalra, & Stentz, 2006). robots bids calculated total number steps needed
solve current tasks auctioned task (in simplified model
robots removed), averaged states S. robot lowest bid
assigned task.
1166

fiExploiting Causality Selective Belief Filtering DBNs

Figure 11: Example DBN smaller warehouse system consisting one inventory
pod (I1) two robots (R1, R2). DBN implements joint action R1 moves
R2 turns. Dashed circles mark passive state variables. coloured areas represent
state clusters C1 C8 .

6.2.2 DBN Topology Clustering
Figure 11 shows example DBN smaller warehouse one inventory pod two
robots. inventory pod represented two variables, I.x I.y, correspond
x position inventory pod. robot R represented four variables:
R.x/R.y x/y position, R.d direction, R.s status. status
robot R either R.s=0 (unloaded) R.s=I (loaded inventory pod I). Constants
size warehouse positions workstations omitted DBN.
four types clusters: I-clusters (C1C4) preserve correlation
R loaded I, must always position R (there two I-clusters
(I,R) pair); R-clusters (C5) S-clusters (C6), respectively, preserve
correlation two robots position carry inventory pod
(there one R/S-cluster (Ra,Rb) pair > b); And, finally, D-clusters (C7,
C8). PSBF uses singleton observation clusters (i.e. one cluster observation variable).
differences DBNs centralised decentralised modes
(Figure 11 uses centralised mode). centralised mode, one DBN
action combination robots. Since controller observes R.s noise-free, add
edges R.x/R.y I.x/I.y R.s=I remove otherwise simplify inference
(thus, Figure 11, R1 loaded I1 R2 unloaded). decentralised mode,
robot observes sensor readings, hence add remove edges
itself, edges robots must permanently added. means
robots status variables (R.s) must linked I.x/I.y and, therefore, included
I-clusters (to preserve correlation must position R R
loaded I). Moreover, since robot knows action, one DBN
1167

fiSeconds per transition

Albrecht & Ramamoorthy

Centralised
180 Decentralised
160
140
120
100
80
60
BK

PSBF

PF

Figure 12: Results warehouse simulation, using centralised decentralised
control modes. Timing measured UNIX dual-core machine 2.4 GHz averaged
20 different simulations 100 transitions each.
actions, variables associated robots active (the
distributions r defined previous section used average actions).
6.2.3 Results
implemented PSBF, BK, PF C#, using framework Infer.NET (Minka, Winn,
Guiver, & Knowles, 2012) implement BK. allowed BK exploit sparsity
process offered improved memory handling. PSBF optimised sparsity (6)
(8), respectively, summing states btk0 / bt+1
k0 positive. PF naturally
benefits sparsity allows concentrate samples fewer states. number
samples used PF set way controller decisions invariant
random numbers used sampling process PF. done ensure
results repeatable. Finally, maintain sparsity process, probability
belief states lower 0.01 set 0. tested algorithms initialised exact
belief state, shown Figure 10b.
Figure 12 shows time per transition averaged 20 different simulations 100
transitions each. timing reported PSBF includes time needed modify variable
distributions (for overlapping clusters) detect skippable belief factors transition
observation steps, done demand every previously unseen
DBN. centralised mode, PSBF able outperform BK average 49%
PF 36%. PF needed 20,000 samples produce consistent (i.e. repeatable) results.
decentralised mode, PSBF outperformed BK average 17% PF 32%. PF
needed 45,000 samples produce consistent results, due increased variance
process. differences statistically significant, based paired t-tests 5%
significance level. Note PSBF BK slower decentralised mode since
corresponding DBNs much higher inter-connectivity. addition, PSBF updated
belief factors since active variables.
expected, PSBF able exploit high degree passivity process
accelerate filtering task. many cases, meant PSBF needed update less
half belief factors. Precisely many belief factors updated depends
1168

fiExploiting Causality Selective Belief Filtering DBNs

performed action. illustrate this, consider smaller warehouse DBN shown Figure 11
(for centralised mode), R1 moving R2 turning. Here, R1.x, R1.y,
R2.d active variables variables passive (dashed circles), corresponding
passivity 70%. DBN, PSBF updates belief factors corresponding clusters
C1, C2, C5, C8, since contain active variables, updates belief
factors C3 C4, since directed paths active variables (R1.x R1.y)
them. Therefore, factors updated C6 C7.
consider full warehouse experiment, contains 16 inventory pods 4 robots,
resulting 48 variables 128 I-clusters, 6 R-clusters, 6 S-clusters, 4 D-clusters.
Assume similar situation one robot moves inventory pod, say R4 I1,
R13 turn. case, PSBF updates 3 6 R-clusters (those containing
R4), 0 6 S-clusters (since status change), 3 4 D-clusters (for R13), 38 128
I-clusters (32 I-clusters containing R4 plus 6 I-clusters R13 I1), amounting
total saving 69.44% belief factors need updated.
number states warehouse system (including invalid states) exceeded 1045
states. Therefore, unable compare accuracy tested algorithms terms
relative entropy. Instead, compared accuracy based results task
auctions number completed tasks end simulation. gives
good indication algorithms accuracy, since outcome auction
number completed tasks depend accuracy belief states. centralised
mode, algorithms generated 95% identical task auctions completed 15.7 (BK),
15.5 (PSBF), 15.2 (PF) tasks average. decentralised mode, generated
93% identical auctions completed 12.1 (BK), 12.2 (PSBF), 11.7 (PF) tasks
average. modes, none differences statistically significant. Therefore,
indicates PSBF achieved accuracy similar BK PF.
6.3 Summary Experimental Evaluation
experimental results show PSBF produces belief states competitive accuracy:
synthetic processes, PSBF achieved accuracy average better
comparable accuracy alternative methods. warehouse system, PSBF
able complete statistically equivalent number tasks compared
methods, indicates accuracy equivalent comparable.
Furthermore, experimental results show PSBF performed belief updates
significantly faster alternative methods: synthetic processes, PSBF using
parallel processes outperformed BK 64% largest process (XL), PF took
much time achieve accuracy comparable PSBF. particular, results show
computational gains grow significantly degree passivity
size process. warehouse system, PSBF outperformed alternative methods
49%, substantial saving considering size state space (more
1045 states). Furthermore, computational gains much higher centralised
control mode decentralised control mode, since latter significantly
lower degree passivity. Therefore, shows high degrees passivity bear
great potential filtering task.
1169

fiAlbrecht & Ramamoorthy

7. Free Lunch PSBF
view belief filtering method generally suited types processes.
Instead, method assumes certain structure process (explicitly implicitly)
attempts exploit order render filtering task tractable. Typically,
methods tailored way respect structure perform well
structure present process, suffer significant loss performance structure
absent. instance, PF works best processes low degrees uncertainty, since
means fewer state samples needed acceptable approximations.
hand, number samples needed acceptable approximations grow substantially
degree uncertainty process (as shown experiments). another
example, BK works best processes little correlation state variables, since
means belief factors small processed efficiently. However,
many variables strongly correlated, BK typically becomes infeasible.
Therefore, structural assumptions taken account choosing
filtering method specific process.
formal account view given Free Lunch theorems (Wolpert
& Macready, 1997, 1995) state that, intuitively speaking, two algorithms
equivalent performance averaged possible instances problem.
words, classes problem instances algorithm better performance
algorithm B, must classes problem instances
worse performance B. Then, question is: class problem instances (that
is, processes) PSBF expected achieve good performance? class essentially
described following three criteria:
Degree passivity PSBF attempts accelerate filtering task omitting
transition step many belief factors possible. depends passivity
variables state clusters. ideal case, process exhibits high degree
passivity PSBF omit transition step many belief factors.
worst case, process passive variables all, PSBF update
belief factors transition step. However, discussed Section 5.5, high degree
passivity necessarily sufficient infer many clusters skipped
transition step, since passive variables could distributed way
cluster skipped (e.g. passive variables distributed uniformly
amongst state clusters). Therefore, optimal case, passivity concentrated
correlated state variables passive variables end clusters.
Size state clusters space time complexity belief state representation
PSBF exponential size largest state cluster (cf. Section 5.5). Therefore,
ideal case, relevant variable correlations captured small state
clusters cost storing belief factors performing update procedures
small. worst case, large state clusters required retain variable
correlations cost storing updating belief factors large. Another reason
state clusters small way PSBF performs
transition step. One pre-requisite omitting transition step belief factor
variables corresponding cluster passive. many variables
1170

fiExploiting Causality Selective Belief Filtering DBNs

one cluster, less likely variables cluster passive, and,
therefore, less likely cluster skipped.
Structure observations third criterion, though arguably less important
criteria, structure observations (i.e. way observation
variables depend state variables) size observation clusters (Cl ).
PSBF attempts accelerate observation step skipping state
clusters whose variables structurally independent observation, and,
cluster cannot skipped, incorporating observation clusters
relevant update. Therefore, ideal case, fraction state clusters
depend observation, relevant correlations observation variables
captured small observation clusters. worst case, state clusters
depend observation sense, structure observation
allow efficient clustering.
Thus, summary, PSBF suitable processes high degrees passivity
relevant variable correlations captured small state observation
clusters. hand, PSBF may suitable low degrees
passivity, large state observation clusters necessary retain relevant
variable correlations process.
addition identifying class processes filtering method suitable,
important justify practical relevance class. work, interested
robotic physical decision processes (as shown examples experiments).
systems typically exhibit number features: First all, robotic systems usually
causal structure (e.g. Mainzer, 2010; Pearl, 2000). Passivity, specific type
causality, observed many robotic systems, including robot arm used
examples multi-robot warehouse system Section 6.2. Furthermore, robotic systems
typically modular structure, module responsible specific
subtask may interact modules. modular structure often allows
efficient clustering, sense module corresponds cluster correlated state
variables. Finally, sensors used robotic systems typically provide information
certain aspects system, components system may benefit
sensor information. words, independencies state
observation variables. features correspond criteria (above) specify
class processes PSBF suitable filtering method. Therefore, believe
class practically justified.

8. Conclusion
Inferring state stochastic process difficult technical challenge complex
systems large state spaces. key developing efficient solutions identify special
structure process, e.g. topology parameterisation dynamic Bayesian
networks, leveraged render filtering task tractable.
end, present article explored idea automatically detecting exploiting
causal structure order accelerate belief filtering task. considered specific type
causal relation, termed passivity, pertains state variables cause changes
1171

fiAlbrecht & Ramamoorthy

state variables. demonstrate potential exploiting passivity, developed novel
filtering method, PSBF, uses factored belief state representation exploits passivity
perform selective updates belief factors. PSBF produces exact belief states
certain assumptions approximate belief states otherwise. showed empirically,
synthetic processes varying sizes degrees passivity well example
complex multi-robot system, PSBF faster several alternative methods
achieving competitive accuracy. particular, results showed computational
gains grow significantly size process degree passivity.
work demonstrates system exhibits much causal structure,
great potential exploiting structure render filtering task tractable.
particular, experiments support initial hypothesis factored beliefs passivity
useful combination large processes. insight relevant complex processes
high degrees causality, robots used homes, offices, industrial factories,
filtering task may constitute major impediment due often large state
space system.
several potential directions future work. example, would useful
know definition passivity could relaxed variables fall
definition, principal idea behind PSBF still applicable. One
relaxation could form approximate passivity, allows small probabilities
passive variables change values even relevant parents remain unchanged.
addition, would interesting know idea performing selective updates
belief factors (via passivity) could applied existing methods use
factored belief state representation (cf. Section 2.1). Finally, another useful avenue future
work would formulate additional types causal relations exploited
ways similar PSBF exploits passivity, perhaps ways that.

Acknowledgements
article result long debate presented topic, process benefited
number discussions suggestions. particular, authors wish thank
anonymous reviewers NIPS12 UAI13 conferences well Journal AI
Research; attendees workshop Advances Causal Inference held UAI15;
colleagues School Informatics University Edinburgh. Furthermore,
authors acknowledge financial support German National Academic Foundation,
UK Engineering Physical Sciences Research Council (grant number EP/H012338/1),
European Commission (TOMSY Grant Agreement 270436).

1172

fiExploiting Causality Selective Belief Filtering DBNs

Appendix A. Proof Theorem 1
prove Theorem 1, useful first establish following lemma:
Lemma 1. (A1) holds xt+1
Ck passive ,

s, s0 : Tka (s, s0k ) = 1 sk = s0k .
Proof.
: fact (A1) means a,i Ck xt+1
Ck . Since xt+1
Ck


passive , follows xtj a,i passive , a,i . Therefore, given
Tka (s, s0k ) = 1 clause (ii) Definition 3, follows sk = s0k .
: Follows directly (A1) fact xt+1
Ck passive .


Using Lemma 1, give compact proof Theorem 1:


Theorem 2. (A1) (A2) hold, xt+1
Ck passive ,


: bt+1
k (sk ) = bk (sk ).

Proof.
0
bt+1
k (sk )

=

1

X



Tka (s, s0k )

S(pat (Ck ))

=

1

X

btk0 (sk0 )

k0 :[xt+1
Ck0 : xti pat (Ck )]




Lem1







Tka (s, s0k )



btk0 (sk0 )

S(pat (Ck )):sk =s0k k0 :[xt+1
Ck0 : xti pat (Ck )]




=

1 btk (sk )

X



Tka (s, s0k )



btk0 (sk0 )

S(pat (Ck )):sk =s0k k0 6= k:[xt+1
Ck0 : xti pat (Ck )]




{z

|

(A1)

= 1

=

1 btk (sk )

=

btk (sk ). (1 = 1 since btk normalised)

1173

}

fiAlbrecht & Ramamoorthy

Appendix B. Proof Theorem 2
prove Theorem 2, first note following proposition:



Proposition 1. xt+1
Ck marginally independent yjt+1 t+1 ,


s, s0 : k0 6=k sk0 = s0k0 (s, ot ) = (s0 , ot ).

proposition follows directly definition.

Using Proposition 1, give compact proof Theorem 2:


Theorem 2. xt+1
Ck marginally independent yjt+1 t+1 ,

t+1
: bt+1
k (sk ) = bk (sk ).

Proof.
X

t+1 0
0
bt+1
k (sk ) = 2 bk (sk )





(s, ot+1 )

0
bt+1
k0 (sk )

t+1 )) : = s0 k 0 6= k : C 0 pat+1 (Y t+1 ) 6=
S(pat+1
k
k

(Y
k




|

Prop1

{z

= constant , independent

=

bt+1 (s0k )
P k t+1 00
s00 bk (sk )
k

=

bt+1 (s0k )
P k t+1 00
s00 bk (sk )
k

0
= bt+1
k (sk ).

1174

}
s0k

fiExploiting Causality Selective Belief Filtering DBNs

Appendix C. Mixture Gaussians
Algorithm 4 provides simple procedure randomly generates mixture Gaussians
(i.e. set normal distributions) synthetic processes Section 6.1. algorithm
takes input number n state variables returns set G Gaussians whose means
set {1, ..., n}. number Gaussians, means, variances
chosen automatically achieve good coverage state variables minimising
(visual) overlap Gaussians. See Figure 7 example.

Algorithm 4 MixtureOfGaussians(n)
1:

Input: number state variables n

2:

Parameters: 4, min 5 , max

3:

Output: mixture Gaussians G

4:

G

5:

R {(1, ..., n)}

6:

R 6=

n
10

7:

R next element R

8:

R R \ {R}

9:

R(drand |R|e) // rand returns random number (0, 1)

10:

1 min[ R(1), R(|R|) ]

11:
12:

min[max , max[min , rand ]]


G G (, 2 ) // mean variance Gaussian

13:

R (R(1), R(2), ..., R(p)) R(p) <

14:

R+ (R(q), R(q + 1), ..., R(|R|)) R(q) > +

15:

R 6=

16:
17:
18:
19:

R R {R }
R+ 6=

R R {R+ }
return G

1175

fiAlbrecht & Ramamoorthy

References
Astrom, K. (1965). Optimal control Markov processes incomplete state information.
Journal Mathematical Analysis Applications, 10, 174205.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: structural assumptions computational leverage. Journal Artificial Intelligence Research, 11 (1),
194.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence Bayesian networks. Proceedings 12th Conference Uncertainty
Artificial Intelligence, pp. 115123.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings 14th Conference Uncertainty Artificial Intelligence, pp. 3342.
Boyen, X., & Koller, D. (1999). Exploiting architecture dynamic systems. Proceedings
16th National Conference Artificial Intelligence, pp. 313320.
Brafman, R. (1997). heuristic variable grid solution method POMDPs. Proceedings
14th National Conference Artificial Intelligence, pp. 727733.
DAndrea, R., & Wurman, P. (2008). Future challenges coordinating hundreds autonomous vehicles distribution facilities. Proceedings IEEE International
Conference Technologies Practical Robot Applications, pp. 8083.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5, 142150.
Dias, M., Zlot, R., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination:
survey analysis. Proceedings IEEE, 94 (7), 12571270.
Doucet, A., de Freitas, N., & Gordon, N. (2001). Sequential Monte Carlo Methods Practice.
Springer Science & Business Media.
Doucet, A., De Freitas, N., Murphy, K., & Russell, S. (2000). Rao-Blackwellised particle
filtering dynamic Bayesian networks. Proceedings 16th Conference
Uncertainty Artificial Intelligence, pp. 176183.
Geiger, D., Verma, T., & Pearl, J. (1989). d-separation: theorems algorithms.
Proceedings 5th Conference Uncertainty Artificial Intelligence, pp. 139
148.
Gordon, N., Salmond, D., & Smith, A. (1993). Novel approach nonlinear/non-Gaussian
Bayesian state estimation. IEE Proceedings F (Radar Signal Processing), Vol.
140, pp. 107113.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions Systems Science Cybernetics,
Vol. 4, pp. 100107.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes. Journal Artificial Intelligence Research, 13, 3394.
1176

fiExploiting Causality Selective Belief Filtering DBNs

Heckerman, D. (1993). Causal independence knowledge acquisition inference.
Proceedings 9th Conference Uncertainty Artificial Intelligence, pp. 122
127.
Heckerman, D., & Breese, J. (1994). new look causal independence. Proceedings
10th Conference Uncertainty Artificial Intelligence, pp. 286292.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially
observable stochastic domains. Artificial intelligence, 101 (1), 99134.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques.
MIT Press.
Kullback, S., & Leibler, R. (1951). information sufficiency. Annals Mathematical Statistics, 22 (1), 7986.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical
structures application expert systems. Journal Royal Statistical
Society. Series B (Methodological), 50 (2), 157224.
Lovejoy, W. (1991). Computationally feasible bounds partially observed Markov decision
processes. Operations Research, 39, 162175.
Mainzer, K. (2010). Causality natural, technical, social systems. European Review,
18, 433454.
Minka, T., Winn, J., Guiver, J., & Knowles, D. (2012). Infer.NET 2.5.. Microsoft Research
Cambridge. http://research.microsoft.com/infernet.
Murphy, K. (2001). Bayes net toolbox Matlab. Computing Science Statistics,
33 (2), 10241034. https://code.google.com/p/bnt/.
Murphy, K., & Weiss, Y. (2001). factored frontier algorithm approximate inference
DBNs. Proceedings 17th Conference Uncertainty Artificial Intelligence,
pp. 378385.
Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference Learning.
Ph.D. thesis, University California, Berkeley.
Ng, B., Peshkin, L., & Pfeffer, A. (2002). Factored particles scalable monitoring.
Proceedings 18th Conference Uncertainty Artificial Intelligence, pp. 370
377.
Pasula, H., Zettlemoyer, L., & Kaelbling, L. (2007). Learning symbolic models stochastic
domains. Journal Artificial Intelligence Research, 29, 309352.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.
Pearl, J. (2000). Causality: Models, Reasoning, Inference. Cambridge University Press.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm
POMDPs. Proceedings 18th International Joint Conference Artificial
Intelligence, Vol. 18, pp. 10251032.
Poole, D., & Zhang, N. (2003). Exploiting contextual independence probabilistic inference.
Journal Artificial Intelligence Research, 18, 263313.
1177

fiAlbrecht & Ramamoorthy

Poupart, P., & Boutilier, C. (2000). Value-directed belief state approximation POMDPs.
Proceedings 16th Conference Uncertainty Artificial Intelligence, pp.
497506.
Poupart, P., & Boutilier, C. (2001). Vector-space analysis belief-state approximation
POMDPs. Proceedings 17th Conference Uncertainty Artificial
Intelligence, pp. 445452.
Poupart, P., & Boutilier, C. (2002). Value-directed compression POMDPs. Advances
Neural Information Processing Systems, pp. 15471554.
Roy, N., Gordon, G., & Thrun, S. (2005). Finding approximate POMDP solutions
belief compression. Journal Artificial Intelligence Research, 23, 140.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysis
implementation. Proceedings 21st Conference Uncertainty Artificial
Intelligence, pp. 542549.
Sondik, E. (1971). Optimal Control Partially Observable Markov Processes. Ph.D.
thesis, Stanford University.
Srinivas, S. (1993). generalization noisy-or model. Proceedings 9th Conference
Uncertainty Artificial Intelligence, pp. 208215.
Washington, R. (1997). BI-POMDP: bounded, incremental partially-observable Markovmodel planning. Recent Advances AI Planning, pp. 440451. Springer.
Wolpert, D., & Macready, W. (1995). free lunch theorems search. Tech. rep. SFI-TR95-02-010, Santa Fe Institute.
Wolpert, D., & Macready, W. (1997). free lunch theorems optimization. IEEE
Transactions Evolutionary Computation, 1 (1), 6782.
Wurman, P., DAndrea, R., & Mountz, M. (2008). Coordinating hundreds cooperative,
autonomous vehicles warehouses. AI Magazine, 29 (1), 9.
Zhang, N., & Poole, D. (1996). Exploiting causal independence Bayesian network inference.
Journal Artificial Intelligence Research, 5, 301328.
Zhou, R., & Hansen, E. (2001). improved grid-based approximation algorithm
POMDPs. Proceedings 17th International Joint Conference Artificial
Intelligence, pp. 707716.

1178


