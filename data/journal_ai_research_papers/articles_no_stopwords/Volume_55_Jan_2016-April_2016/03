journal artificial intelligence

submitted published

automatic description generation images survey
datasets evaluation measures
raffaella bernardi

bernardi disi unitn

university trento italy

ruket cakici

ruken ceng metu edu tr

middle east technical university turkey

desmond elliott

elliott uva nl

university amsterdam netherlands

aykut erdem
erkut erdem
nazli ikizler cinbis

aykut cs hacettepe edu tr
erkut cs hacettepe edu tr
nazli cs hacettepe edu tr

hacettepe university turkey

frank keller

keller inf ed ac uk

university edinburgh uk

adrian muscat

adrian muscat um edu mt

university malta malta

barbara plank

bplank cst dk

university copenhagen denmark

abstract
automatic description generation natural images challenging
recently received large amount interest computer vision natural language processing communities survey classify existing approaches
conceptualize viz cast description generation retrieval visual multimodal representational
space provide detailed review existing highlighting advantages
disadvantages moreover give overview benchmark image datasets
evaluation measures developed assess quality machine generated
image descriptions finally extrapolate future directions area automatic image
description generation

introduction
past two decades fields natural language processing nlp computer
vision cv seen great advances respective goals analyzing generating
text understanding images videos fields share similar set methods rooted artificial intelligence machine learning historically developed
separately scientific communities typically interacted little
recent years however seen upsurge interest require
combination linguistic visual information lot everyday tasks nature
e g interpreting photo context newspaper article following instructions
conjunction diagram map understanding slides listening lecture
c

ai access foundation rights reserved

fibernardi et al

addition web provides vast amount data combines linguistic visual
information tagged photographs illustrations newspaper articles videos subtitles
multimodal feeds social media tackle combined language vision tasks
exploit large amounts multimodal data cv nlp communities moved
closer together example organizing workshops language vision
held regularly cv nlp conferences past years
language vision community automatic image description emerged
key task task involves taking image analyzing visual content generating
textual description typically sentence verbalizes salient aspects
image challenging cv point view description could principle
talk visual aspect image mention objects attributes
talk features scene e g indoor outdoor verbalize people
objects scene interact challenging still description could even refer
objects depicted e g talk people waiting train even
train visible arrived yet provide background knowledge
cannot derived directly image e g person depicted mona lisa
short good image description requires full image understanding therefore
description task excellent test bed computer vision systems one much
comprehensive standard cv evaluations typically test instance accuracy
object detectors scene classifiers limited set classes
image understanding necessary sufficient producing good description
imagine apply array state art detectors image localize objects
e g felzenszwalb girshick mcallester ramanan girshick donahue darrell
malik determine attributes e g lampert nickisch harmeling berg
berg shih parikh grauman compute scene properties e g oliva
torralba lazebnik schmid ponce recognize human object interactions e g prest schmid ferrari yao fei fei would
long unstructured list labels detector outputs would unusable image
description good image description contrast comprehensive concise
talk important things image formally correct
e consists grammatically well formed sentences
nlp point view generating description natural language generation nlg task nlg turn non linguistic representation
human readable text classically non linguistic representation logical form
database query set numbers image description input image representation e g detector outputs listed previous paragraph nlg
model turn sentences generating text involves series steps traditionally
referred nlp pipeline reiter dale need decide aspects
input talk content selection need organize content text
verbalize surface realization surface realization turn requires choosing right words lexicalization pronouns appropriate referential expression
generation grouping related information together aggregation
words automatic image description requires full image understanding
sophisticated natural language generation makes interesting


fiautomatic description generation images survey

task embraced cv nlp communities note
description task become even challenging take account
good descriptions often user specific instance art critic require different
description librarian journalist even photograph briefly
touch upon issue talk difference descriptions captions
section discuss future directions section
given automatic image description interesting task driven
existence mature cv nlp methods availability relevant datasets large
image description literature appeared last five years aim survey
article give comprehensive overview literature covering datasets
evaluation metrics
sort existing literature three categories image description
used first group follows classical pipeline outlined
first detect predict image content terms objects attributes scene types
actions set visual features use content information
drive natural language generation system outputs image description
term approaches direct generation
second group cast retrieval create
description novel image search images database similar
novel image build description novel image descriptions
set similar images retrieved novel image described simply
reusing description similar retrieved image transfer synthesizing
novel description description set similar images retrieval
subdivided type use represent images
compute similarity first subgroup uses visual space retrieve images
second subgroup uses multimodal space represents images text jointly
overview reviewed survey category
fall see table
generating natural language descriptions videos presents unique challenges
image description additionally requires analyzing objects
attributes actions temporal dimension aim solve description generation videos proposed literature e g khan zhang
gotoh guadarrama krishnamoorthy malkarnenkar venugopalan mooney darrell
saenko krishnamoorthy malkarnenkar mooney saenko guadarrama
rohrbach qiu titov thater pinkal schiele thomason venugopalan guadarrama saenko mooney rohrbach rohrback tandon schiele yao torabi
cho ballas pal larochelle courville zhu kiros zemel salakhutdinov urtasun
torralba fidler however existing work description generation used
static images focus survey
survey article first group automatic image description three
categories outlined provide comprehensive overview
though image description approaches circumvent nlg aspect transferring human authored
descriptions see sections
interesting intermediate involves annotation image streams sequences sentences see work park kim



fibernardi et al

category section examine available multimodal image datasets used
training testing description generation section furthermore review
evaluation measures used gauge quality generated descriptions
section finally section discuss future directions including possible
tasks related image description visual question answering

image description
generating automatic descriptions images requires understanding humans
describe images image description analyzed several different dimensions shatford jaimes chang follow hodosh young hockenmaier
assume descriptions interest survey article ones
verbalize visual conceptual information depicted image e descriptions
refer depicted entities attributes relations actions
involved outside scope automatic image description non visual descriptions
give background information refer objects depicted image e g
location image taken took picture relevant
standard approaches image description perceptual descriptions capture
global low level visual characteristics images e g dominant color image
type media photograph drawing animation etc
following subsections give comprehensive overview state art approaches description generation table offers high level summary field
three categories outlined introduction direct generation retrieval visual space retrieval model multimodal space
description generation visual input
general studies group first predict likely meaning
given image analyzing visual content generate sentence reflecting
meaning category achieve following general pipeline
architecture
computer vision techniques applied classify scene type detect objects present image predict attributes relationships hold
recognize actions taking place
followed generation phase turns detector outputs words
phrases combined produce natural language description
image techniques natural language generation e g templates n grams
grammar rules
approaches reviewed section perform explicit mapping images
descriptions differentiates studies described section
incorporate implicit vision language illustration sample model
shown figure explicit pipeline architecture tailored hand
constrains generated descriptions relies predefined sets semantic classes
scenes objects attributes actions moreover architecture crucially assumes


fiautomatic description generation images survey

reference

generation

farhadi et al
kulkarni et al
li et al
ordonez et al
yang et al
gupta et al
kuznetsova et al
mitchell et al
elliott keller
hodosh et al
gong et al
karpathy et al
kuznetsova et al
mason charniak
patterson et al
socher et al
verma jawahar
yatskar et al
chen zitnick
donahue et al
devlin et al
elliott de vries
fang et al
jia et al
karpathy fei fei
kiros et al
lebret et al
lin et al
mao et al
ortiz et al
pinheiro et al
ushiku et al
vinyals et al
xu et al
yagcioglu et al

retrieval
visual space multimodal space
x

x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x

x
x
x

x
x
x
x
x
x
x
x
x

x
x
x
x
x
x
x
x
x

x
x
x

table overview existing approaches automatic image description
categorized literature approaches directly generate description image
section approaches retrieve images via visual similarity transfer description image section approaches frame task retrieving
descriptions images multimodal space section



fibernardi et al

figure automatic image description generation system proposed kulkarni et al

accuracy detectors semantic class assumption met
practice
approaches description generation differ along two main dimensions image
representations derive descriptions b address sentence generation terms representations used existing conceptualized
images number different ways relying spatial relationships farhadi et al
corpus relationships yang et al spatial visual attributes kulkarni
et al another group papers utilizes abstract image representation
form meaning tuples capture different aspects image objects detected
attributes detections spatial relations scene type
farhadi et al yang et al kulkarni et al li et al mitchell et al
recently yatskar et al proposed generate descriptions denselylabeled images incorporate object attribute action scene annotations similar
spirit work fang et al rely prior labeling objects
attributes etc rather authors train word detectors directly images
associated descriptions multi instance learning weakly supervised
training object detectors words returned detectors fed
language model sentence generation followed ranking step
first framework explicitly represent structure image relates
structure description visual dependency representations vdr method
proposed elliott keller vdr captures spatial relations
objects image form dependency graph graph related
syntactic dependency tree description image initial work vdrs
relied corpus manually annotated vdrs training recent approaches
induce vdrs automatically output object detector elliott de vries
labels present abstract scenes ortiz et al idea explicitly
representing image structure description generation picked
vdrs proven useful description generation image retrieval elliott
lavrenko keller
abstract scenes schematic images typically constructed clip art employed avoid
need object detector labels positions objects know example zitnick
parikhs dataset see section details



fiautomatic description generation images survey

lin et al parse images scene graphs similar vdrs
represent relations objects scene generate scene
graphs semantic grammar
existing approaches vary along second dimension viz
sentence generation one end scale approaches use
n gram language examples include works kulkarni et al
li et al generate descriptions n gram language trained
subset wikipedia approaches first determine attributes relationships
regions image regionprepositionregion triples n gram language
model used compose image description fluent given language model
fang et al similar uses maximum entropy language model
instead n gram model generate descriptions gives authors flexibility
handling output word detectors core model
recent image description work recurrent neural networks rnns
regarded relying language modeling classical rnn language model captures
probability generating given word string given words generated far
image description setup rnn trained generate next word given
string far set image features setting rnn therefore
purely language model case n gram model instance hybrid
model relies representation incorporates visual linguistic features
return detail section
second set approaches use sentence templates generate descriptions
typically manually pre defined sentence frames open slots need filled
labels objects relations attributes instance yang et al fill
sentence template selecting likely objects verbs prepositions scene types
hidden markov model verbs generated finding likely pairing
object labels gigaword external corpus generation model elliott
keller parses image vdr traverses vdrs fill slots
sentence templates performs limited content selection
learning associations vdrs syntactic dependency trees training time
associations allow select appropriate verb description test time
approaches used linguistically sophisticated approaches generation
mitchell et al generate syntactically well formed sentence fragments
recombine tree substitution grammar related pursued
kuznetsova et al tree fragments learnt training set existing
descriptions fragments combined test time form descriptions
another linguistically expressive model recently proposed ortiz et al
authors model image description machine translation vdrsentence pairs
perform explicit content selection surface realization integer linear program
linguistic constraints
systems presented far aimed directly generating novel descriptions however
argued hodosh et al framing image description natural language generation nlg task makes difficult objectively evaluate quality novel descriptions
note graphs used image retrieval johnson krishna stark li shamma bernstein
fei fei schuster krishna chang fei fei manning



fibernardi et al

figure description model retrieval visual space proposed ordonez
et al
introduces number linguistic difficulties detract attention underlying image understanding hodosh et al time evaluation
generation systems known difficult reiter belz hodosh et al therefore propose makes possible evaluate mapping images
sentences independently generation aspect follow
conceptualize image description retrieval associate image
description retrieving ranking set similar images candidate descriptions
candidate descriptions used directly description transfer
novel description synthesized candidates description generation
retrieval images ranking descriptions carried two ways
visual space multimodal space combines textual visual
information space following subsections survey work follows two
approaches
description retrieval visual space
studies group pose automatically generating description
image retrieving images similar query image e image
described illustrated figure words systems exploit similarity
visual space transfer descriptions query images compared
generate descriptions directly section retrieval typically require large amount
training data order provide relevant descriptions
terms algorithmic components visual retrieval approaches typically follow
pipeline three main steps
represent given query image specific visual features
retrieve candidate set images training set similarity measure
feature space used
rank descriptions candidate images making use visual
textual information contained retrieval set alternatively combine
fragments candidate descriptions according certain rules schemes
one first model follow im text model ordonez et al
gist oliva torralba tiny image torralba fergus freeman


fiautomatic description generation images survey

descriptors employed represent query image determine visually similar
images first retrieval step retrieval consider
step baseline ranking step range detectors e g object stuff
pedestrian action detectors scene classifiers specific entities mentioned
candidate descriptions first applied images better capture visual content
images represented means detector classifier responses finally
ranking carried via classifier trained semantic features
model proposed kuznetsova et al first runs detectors classifiers used ranking step im text model query image extract
represent semantic content instead performing single retrieval combining
responses detectors classifiers im text model carries
separate image retrieval step visual entity present query image collect related phrases retrieved descriptions instance dog detected given
image retrieval process returns phrases referring visually similar dogs
training set specifically step used collect three different kinds phrases
noun verb phrases extracted descriptions training set
visual similarity object regions detected training images query
image similarly prepositional phrases collected stuff detection query
image measuring visual similarity detections query training
images appearance geometric arrangements prepositional phrases
additionally collected scene context detection measuring global scene similarity computed query training images finally description generated
collected phrases detected object via integer linear programming ilp
considers factors word ordering redundancy etc
method gupta et al another phrase retrieve
visually similar images authors employ simple rgb hsv color histograms
gabor haar descriptors gist sift lowe descriptors image features instead visual object detectors scene classifiers rely
textual information descriptions visually similar images extract
visual content input image specifically candidate descriptions segmented phrases certain type subject verb subject prep object
verb prep object attribute object etc best describe input image determined according joint probability model image similarity google search counts image represented triplets form
attribute object verb verb prep attribute object object prep object
end description generated three top scoring triplets fixed
template increase quality descriptions authors apply syntactic
aggregation subject predicate grouping rules generation step
patterson et al first present large scale scene attribute dataset
computer vision community dataset includes images scene
categories annotated certain attributes list discriminative
attributes related materials surface properties lighting affordances spatial layout
allows train attribute classifiers dataset authors
demonstrate responses attribute classifiers used global
image descriptor captures semantic content better standard global image


fibernardi et al

descriptors gist application extended baseline model im text
replacing global features automatically extracted scene attributes giving better
image retrieval description
mason charniaks description generation differs
discussed formulates description generation extractive summarization
selects output description considering textual information
final ranking step particular authors represented images scene
attributes descriptor patterson et al visually similar images identified training set next step conditional probabilities observing word
description query image estimated via non parametric density estimation
descriptions retrieved images final output description determined two different extractive summarization techniques one depending
sumbasic model nenkova vanderwende kullback leibler
divergence word distributions query candidate descriptions
yagcioglu et al proposed average query expansion
compositional distributed semantics represent images use features extracted
recently proposed visual geometry group convolutional neural network vgg cnn
chatfield simonyan vedaldi zisserman features activations
last layer deep neural network trained imagenet proven
effective many computer vision original query expanded
average distributed representations retrieved descriptions weighted
similarity input image
devlin et al utilizes cnn activations global image
descriptor performs k nearest neighbor retrieval determine images
training set visually similar query image selects description
candidate descriptions associated retrieved images best describes
images similar query image approaches mason
charniak yagcioglu et al differs terms
represent similarity description select best candidate
whole set specifically propose compute description similarity
n gram overlap f score descriptions suggest choose output
description finding description corresponds description highest
mean n gram overlap candidate descriptions k nearest neighbor centroid
description estimated via n gram similarity measure
description retrieval multimodal space
third group studies casts image description generation retrieval
multimodal space hodosh et al socher et al karpathy et al
intuition behind illustrated figure overall
characterized follows

learn common multimodal space visual textual data training
set imagedescription pairs


fiautomatic description generation images survey

given query use joint representation space perform cross modal image
sentence retrieval

figure image descriptions retrieval task proposed works hodosh et al
socher et al karpathy et al
contrast retrieval work visual space section
unimodal image retrieval followed ranking retrieved descriptions image
sentence features projected common multimodal space multimodal
space used retrieve descriptions given image advantage
allows bi directional e common space used
direction retrieving appropriate image query sentence
section first discuss seminal hodosh et al description
retrieval present recent approaches combine retrieval
form natural language generation hodosh et al map images sentences
common space joint space used image search
plausible image given sentence image annotation sentence describes
image well see figure earlier study authors proposed learn common meaning space farhadi et al consisting triple representation form
hobject action scenei representation thus limited set pre defined discrete
slot fillers given training information instead hodosh et al use kcca
kernelized version cca canonical correlation analysis hotelling learn
joint space cca takes training dataset image sentence pairs e dtrain hi si
thus input two different feature spaces finds linear projections newly induced common space kcca kernel functions map original items higher order
space order capture patterns needed associate image text kcca
shown previously successful associating images hardoon szedmak shawetaylor image regions socher fei fei individual words set
tags
hodosh et al compare kcca nearest neighbor nn baseline
uses unimodal text image spaces without constructing joint space drawback
kcca applicable smaller datasets requires two kernel matrices
source http nlp cs illinois edu hockenmaiergroup framing image description



fibernardi et al

kept memory training becomes prohibitive large datasets
attempts made circumvent computational burden kcca e g
resorting linear hodosh hockenmaier alternatively sun gan
nevatia used automatically discovered concepts images form semantic
space performed sentence retrieval accordingly however recent work description
retrieval instead utilized neural networks construct joint space image description
generation
socher et al use neural networks building sentence image vector representations mapped common embedding space novelty
work use compositional sentence vector representations first image word
representations learned single modalities finally mapped common
multimodal space particular use dt rnn dependency tree recursive neural
network composing language vectors abstract word order syntactic difference semantically irrelevant dimensional word embeddings
image space authors use nine layer neural network trained imagenet data
unsupervised pre training image embeddings derived taking output
last layer dimensions two spaces projected multi modal space
max margin objective function intuitively trains pairs correct image
sentence vectors high inner product authors model outperforms previously used kcca approaches work hodosh hockenmaier

karpathy et al extend previous multi modal embeddings model rather
directly mapping entire images sentences common embedding space
model embeds fine grained units e fragments images objects sentences
dependency tree fragments common space final model integrates global
sentence image level well finer grained information outperforms previous
approaches dt rnn socher et al similar pursued
pinheiro et al propose bilinear phrase model learns mapping
image representations sentences constrained language model used
generate representation conceptually related pursued ushiku
et al authors use common subspace model maps feature vectors
associated phrase nearby regions space generation beamsearch decoder templates used
description generation systems difficult evaluate therefore studies reviewed
treat retrieval ranking task hodosh et al socher et al
valuable enables comparative evaluation
retrieval ranking limited availability existing datasets descriptions
alleviate recent developed extensions multimodal
spaces able rank sentences generate chen zitnick
donahue et al karpathy fei fei kiros et al lebret et al
mao et al vinyals et al xu et al
kiros et al introduced general encoder decoder framework image description
ranking generation illustrated figure intuitively method works follows
encoder first constructs joint multimodal space space used rank
images descriptions second stage decoder uses shared multimodal


fiautomatic description generation images survey

figure encoder decoder model proposed kiros et al

representation generate novel descriptions model directly inspired recent
work machine translation encodes sentences longshort term memory lstm
recurrent neural network image features deep convolutional network cnn
lstm extension recurrent neural network rnn incorporates builtin memory store information exploit long range context kiros et al
encoder decoder model vision space projected embedding space lstm
hidden states pairwise ranking loss minimized learn ranking images
descriptions decoder neural network language model able generate novel
descriptions multimodal space
another work carried time similar latter
described donahue et al authors propose model
lstm neural architecture however rather projecting vision space
embedding space hidden states model takes copy static image
previous word directly input fed stack four lstms another
lstm model proposed jia et al added semantic image information
additional input lstm model kiros et al outperforms prior
dt rnn model socher et al turn donahue et al report outperform
work kiros et al task image description retrieval subsequent work
includes rnn architectures mao et al vinyals et al
similar one proposed kiros et al achieve comparable
standard datasets mao wei yang wang huang yuille b propose
interesting extension mao et al model learning novel visual concepts
karpathy fei fei improve previous proposing deep visualsemantic alignment model simpler architecture objective function key
insight assume parts sentence refer particular unknown regions
image model tries infer alignments segments sentences regions
images convolutional neural networks image regions bidirectional
rnn sentences structured objective aligns two modalities words
image regions mapped common multimodal embedding multimodal
recurrent neural network architecture uses inferred alignments learn generate


fibernardi et al

novel descriptions image used condition first state recurrent
neural network generates image descriptions
another model generate novel sentences proposed chen zitnick
contrast previous work model dynamically builds visual representation
scene description generated word read generated
visual representation updated reflect information accomplish
simple rnn model achieves comparable better prior studies
except recently proposed deep visual semantic alignment model karpathy fei fei
model xu et al closely related uses rnn
architecture visual representations dynamically updated xu et al
model incorporates attentional component gives way determining
regions image salient focus description regions
resulting improvement description accuracy makes possible analyze
model behavior visualizing regions attended word
generated model
general rnn ranking generation followed lebret
et al main innovation linguistic side employ bilinear
model learn common space image features syntactic phrases noun phrases verb
phrases prepositional phrases markov model utilized generate sentences
phrase embedding visual side standard cnn features used
elegant modeling framework whose performance broadly comparable
state art
finally two important directions less explored portability weakly supervised learning verma jawahar evaluate portability bi directional
model topic showing performance significantly degrades highlight importance cross dataset image description retrieval evaluation another interesting observation require training set fully annotated
image sentence pairs however obtaining data large quantities prohibitively expensive gong et al propose weak supervision transfers
knowledge millions weakly annotated images improve accuracy description
retrieval
comparison existing approaches
discussion previous subsections makes clear image
description particular strengths weaknesses example methods
cast task generation section advantage types
approaches produce novel sentences describe given image however
success relies heavily accurately estimate visual content
well able verbalize content particular explicitly employ computer
vision techniques predict likely meaning given image methods
limited accuracy practice hence fail identify important objects
attributes valid description generated another difficulty lies
final description generation step sophisticated natural language generation crucial


fiautomatic description generation images survey

guarantee fluency grammatical correctness generated sentences come
price considerable algorithmic complexity
contrast image description methods cast retrieval
visual space transfer retrieved descriptions novel image section
produce grammatically correct descriptions guaranteed design
systems fetch human generated sentences visually similar images main issue
requires large amounts images human written descriptions
accuracy grammaticality descriptions reduces size
training set decreases training set needs diverse addition
large order visual retrieval approaches produce image descriptions
adequate novel test images devlin et al though mitigated
synthesizing novel description retrieved ones see section
approaches cast image description retrieval multimodal space
section advantage generating human descriptions
able retrieve appropriate ones pre defined large pool descriptions
however ranking descriptions requires cross modal similarity metric compares
images sentences metrics difficult define compared unimodal
image image similarity metrics used retrieval work visual space
additionally training common space images sentences requires large training
set images annotated human generated descriptions plus side
multimodal embedding space used reverse e retrieving
appropriate image query sentence something generation
visual retrieval approaches capable

datasets evaluation
wide range datasets automatic image description images
datasets associated textual descriptions differ certain
aspects size format descriptions descriptions collected review common approaches collecting datasets datasets
evaluation measures comparing generated descriptions ground truth texts
datasets summarized table examples images descriptions given
figure readers refer dataset survey ferraro mostafazadeh huang
vanderwende devlin galley mitchell analysis similar provides
basic comparison existing language vision datasets limited
automatic image description reports simple statistics quality metrics
perplexity syntactic complexity abstract concrete word ratios
image description datasets
pascal k sentence dataset rashtchian et al dataset commonly
used benchmark evaluating quality description generation systems
medium scale dataset consists images selected pascal
object recognition dataset everingham van gool williams winn zisserman
includes objects different visual classes humans animals vehicles


fibernardi et al

images

texts

judgments

objects

pascal k rashtchian et al
vlt k elliott keller
flickr k hodosh hockenmaier
flickr k young et al
abstract scenes zitnick parikh
iapr tc grubinger et al
ms coco lin et al


















partial
yes



collected

partial
partial


complete
segmented
partial

bbc news feng lapata
sbu captions ordonez et al
deja image captions chen et al







varies


collected






table image datasets automatic description generation split
overview image description datasets top caption datasets bottom see
main text explanation distinction
image associated five descriptions generated humans amazon mechanical
turk amt service
visual linguistic treebank vlt k elliott keller makes use images
pascal action recognition dataset augments images three twosentence descriptions per image descriptions collected amt specific
instructions verbalize main action depicted image actors involved first
sentence mentioning important background objects second sentence
subset images visual linguistic treebank object annotation
available form polygons around objects mentioned descriptions
subset manually created visual dependency representations see section
included three vdrs per images e total
flickr k dataset hodosh et al extended version flickr k
dataset young et al contain images flickr comprising approximately
images respectively images two datasets selected
user queries specific objects actions datasets contain five descriptions per image collected amt workers strategy similar pascal k
dataset
abstract scenes dataset zitnick parikh zitnick parikh vanderwende
consists clip art images descriptions images created
amt workers asked place fixed vocabulary clip art objects
scene choosing descriptions sourced worker created
scenes authors provided descriptions two different forms first
group contains single sentence description image second group includes two
alternative descriptions per image two descriptions consist three simple
sentences sentence describing different aspect scene main advantage
dataset affords opportunity explore image description generation without
kuznetsova et al ran human judgments study images dataset



fiautomatic description generation images survey

one jet lands airport another takes
next
two airplanes parked airport
two jets taxi past
two parked jet airplanes facing opposite directions
two passenger planes grassy plain

several people chairs small child
watching one play trumpet
man playing trumpet front little boy
people sitting sofa man playing
instrument entertainment

pascal k

b vlt k

man snowboarding structure snowy
hill
snowboarder jumps air snowy
hill
snowboarder wearing green pants trick
high bench
someone yellow pants ramp
snow
man performing trick snowboard high
air

yellow building white columns background
two palm trees front house
cars parking front house
woman child walking square

c flickr k

iapr tc

cat anxiously sits park stares
unattended hot dog someone left
yellow bench

blue smart car parked parking lot
vehicles wet wide city street
several cars motorcycle snow covered street
many vehicles drive icy street
small smart car driving city

e abstract scenes

f ms coco

figure example images descriptions benchmark image datasets



fibernardi et al

need automatic object recognition thus avoiding associated noise
recent version dataset created part visual question answering
vqa dataset antol agrawal lu mitchell batra zitnick parikh contains
different scene images realistic human five single sentence
descriptions
iapr tc dataset introduced grubinger et al one earliest
multi modal datasets contains images descriptions images originally retrieved via search engines google bing yahoo descriptions
produced multiple languages predominantly english german image
associated one five descriptions description refers different aspect
image applicable dataset contains complete pixel level segmentation
objects
ms coco dataset lin et al currently consists images five
different descriptions per image images dataset annotated object categories means bounding boxes around instances one categories
available images ms coco dataset widely used image description something facilitated standard evaluation server recently
become available extensions ms coco currently development including
addition questions answers antol et al
one lin et al uses nyu dataset silberman kohli hoiem
fergus contains indoor scenes object segmentation
dataset augmented five descriptions per image lin et al
image caption datasets
image descriptions verbalize seen image e refer objects
actions attributes depicted mention scene type etc captions hand
typically texts associated images verbalize information cannot seen
image caption provides personal cultural historical context image
panofsky images shared social networking photo sharing websites
accompanied descriptions captions mixtures types text images
newspaper museum typically contain cultural historical texts e captions
descriptions
bbc news dataset feng lapata one earliest collections
images co occurring texts feng lapata harvested news articles
british broadcasting corporation news website constraint article
includes image caption






source http nlp cs illinois edu hockenmaiergroup pascal sentences index html
source http github com elliottd vlt
source https illinois edu fb sec
source http imageclef org photodata
source http microsoft com en us um people larryz clipart semanticclassesrender
classes v html
source http mscoco org explore
source http mscoco org dataset captions eval



fiautomatic description generation images survey

sbu captions dataset introduced ordonez et al differs
previous datasets web scale dataset containing approximately one million
captioned images compiled data available flickr user provided image
descriptions images downloaded filtered flickr constraint
image contained least one noun one verb predefined control lists resulting
dataset provided csv file urls
deja image captions dataset chen et al contains images
near identical captions harvested flickr million images downloaded
flickr calendar year set nouns queries image
captions normalized lemmatization stop word removal create corpus
near identical texts instance sentences bird flies blue sky bird
flying blue sky normalized bird fly blue sky chen et al image
caption pairs retained captions repeated one user normalized
form

collecting datasets
collecting imagetext datasets typically performed crowd sourcing harvesting data web images datasets sourced
existing task computer vision community pascal challenge everingham
et al used pascal k vlt k datasets directly flickr
case flickr k k ms coco sbu captions deja image captions datasets
crowdsourced case abstract scenes dataset texts imagedescription
datasets usually crowd sourced amazon mechanical turk crowdflower whereas
texts imagecaption datasets harvested photo sharing sites
flickr news providers captions usually collected without financial incentive
written people sharing images journalists
crowd sourcing descriptions images involves defining simple task
performed untrained workers examples task guidelines used hodosh et al
elliott keller given figure instances care taken
clearly inform potential workers expectations task particular
explicit instructions given descriptions written examples
good texts provided addition hodosh et al provided extensive examples
explain would constitute unsatisfactory texts options available control
quality collected texts minimum performance rate workers common
choice pre task selection quiz may used determine whether workers
sufficient grasp english language hodosh et al
issue remuneration crowd sourced workers controversial higher payments lead better quality crowd sourced environment mason watts
rashtchian et al paid description elliott keller paid
average seconds work produce two sentence description best
knowledge information available datasets


fibernardi et al

mechanical turk interface used collect flickr k dataset

b mechanical turk interface used collect vlt k dataset

figure examples mechanical turk interfaces collecting descriptions

evaluation measures
evaluating output natural language generation nlg system fundamentally
difficult task dale white reiter belz common way assess
quality automatically generated texts subjective evaluation human experts
source appendix work hodosh et al



fiautomatic description generation images survey

nlg produced text typically judged terms grammar content indicating
syntactically correct relevant text respectively fluency generated
text sometimes tested well especially surface realization technique involved
generation process automatically generated descriptions images
evaluated nlg techniques typically judges provided image
well description evaluation tasks subjective human evaluations
machine generated image descriptions often performed mechanical turk help
questions far following likert scale questions used test datasets
user groups sizes
description accurately describes image kulkarni et al li et al
mitchell et al kuznetsova et al elliott keller hodosh et al

description grammatically correct yang et al mitchell et al
kuznetsova et al elliott keller inter alia
description incorrect information mitchell et al
description relevant image li et al yang et al
description creatively constructed li et al
description human mitchell et al
another evaluating descriptions use automatic measures
bleu papineni roukos ward zhu rouge lin hovy translation
error rate feng lapata meteor denkowski lavie cider vedantam lawrence zitnick parikh measures originally developed evaluate output machine translation engines text summarization systems
exception cider developed specifically image description evaluation
measures compute score indicates similarity system output
one human written reference texts e g ground truth translations summaries
evaluation subject much discussion critique kulkarni
et al hodosh et al elliott keller kulkarni et al found weakly
negative correlation human judgments unigram bleu pascal k
dataset pearsons hodosh et al studied cohens correlation
expert human judgments binarized unigram bleu unigram rouge retrieved
descriptions flickr k dataset found best agreement humans
bleu rouge system retrieved sentences originally associated images agreement dropped one reference sentence
available reference sentences disjoint proposal sentences
concluded neither measure appropriate image description evaluation
subsequently proposed imagesentence ranking experiments discussed detail elliott keller analyzed correlation human judgments automatic
evaluation measures retrieved system generated image descriptions flickr k
vlt k datasets showed sentence level unigram bleu point


fibernardi et al

time de facto standard measure image description evaluation weakly
correlated human judgments meteor banerjee lavie less frequently used
translation evaluation measure exhibited highest correlation human judgments
however kuznetsova et al found unigram bleu strongly correlated
human judgments meteor image caption generation
first large scale image description evaluation took place ms coco
captions challenge featuring teams dataset training images
images withheld test dataset number reference texts testing
image five insight measures may benefit
larger reference sets vedantam et al automatic evaluation measures
used image description systems outperformed humanhuman upper bound
whether five reference descriptions provided however none systems
outperformed humanhuman evaluation judgment elicitation task used meteor
found robust measure systems beating human text one
two submissions depending number references systems outperformed
humans seven five times measured cider according rouge bleu
system nearly outperformed humans confirming unsuitability
evaluation measures
description generation cross modal retrieval perspective hodosh hockenmaier hodosh et al socher et al
gong et al karpathy et al verma jawahar able use measures information retrieval median rank mrank precision k k
recall k r k evaluate descriptions return addition text similarity
measures reported evaluation paradigm first proposed hodosh et al
reported high correlation human judgments imagesentence ranking
evaluations
table summarize image description approaches discussed survey
list datasets evaluation measures employed approaches
seen recent systems starting converged use
large description datasets flickr k k ms coco employ evaluation measures
perform well terms correlation human judgments meteor cider however
use bleu despite limitations still widespread use human evaluation
means universal literature

future directions
survey demonstrates cv nlp communities witnessed upsurge
interest automatic image description systems help recent advances deep
learning images text substantial improvements quality automatically generated descriptions registered nevertheless series challenges
image description remain following discuss future directions
line likely benefit
source http mscoco org dataset cap
calculated collecting additional human written description compared
reference descriptions



fiautomatic description generation images survey

reference



farhadi et al
kulkarni et al
li et al
ordonez et al
yang et al

multretrieval
generation
generation
visretrieval
generation

datasets

gupta et al
kuznetsova et al
mitchell et al
elliott keller
hodosh et al

pascal k
pascal k
pascal k
sbu
iapr
flickr k k
coco
visretrieval
pascal k iapr
visretrieval
sbu
generation
pascal k
generation
vlt k
multretrieval pascal k flickr k

gong et al
karpathy et al
kuznetsova et al
mason charniak
patterson et al
socher et al
verma jawahar
yatskar et al
chen zitnick

multretrieval
multretrieval
generation
visretrieval
visretrieval
multretrieval
multretrieval
generation
multretrieval

donahue et al

multretrieval

devlin et al
elliott de vries
fang et al

visretrieval
generation
generation

jia et al
generation
karpathy fei fei multretrieval
kiros et al
lebret et al
lin et al
mao et al
ortiz et al
pinheiro et al
ushiku et al

multretrieval
multretrieval
generation
multretrieval
generation
multretrieval
generation

vinyals et al

multretrieval

xu et al
yagcioglu et al

multretrieval
visretrieval

measures
bleu
human bleu
human bleu

bleu rouge meteor
cider r k
human bleu rouge
human bleu
human
human bleu
human bleu rouge
mrank r k
sbu flickr k
r k
flickr k k coco
bleu meteor cider
sbu
human bleu meteor
sbu
human bleu
sbu
bleu
pascal k
mrank r k
iapr sbu pascal k bleu rouge p k
data
human bleu
flickr k k coco
bleu meteor cider
mrank r k
flickr k coco
human bleu mrank
r k
coco
bleu meteor
vlt k pascal k
bleu meteor
coco
human bleu rouge
meteor cider
flickr k k coco
bleu meteor
flickr k k coco
bleu meteor cider
mrank r k
flickr k k
r k
flickr k coco
bleu r k
nyu
rouge
iapr flickr k coco bleu mrank r k
abstract scenes
human bleu meteor
coco
bleu
pascal k iapr sbu bleu
coco
pascal k
sbu bleu meteor cider
flickr k k
mrank r k
flickr k k coco
bleu meteor
flickr k k coco
human bleu meteor
cider

table overview approaches datasets evaluation measures reviewed
survey organised chronological order



fibernardi et al

datasets
earliest work image description used relatively small datasets farhadi et al
kulkarni et al elliott keller recently introduction flickr k
ms coco large datasets enabled training complex
neural networks still area likely benefit larger diversified datasets
share common unified comprehensive vocabulary vinyals et al argue
collection process quality descriptions datasets affect performance
significantly make transfer learning datasets effective expected
learning model ms coco applying datasets collected
different settings sbu captions pascal k leads degradation bleu
performance surprising since ms coco offers much larger amount training
data pascal k vinyals et al put largely due differences
vocabulary quality descriptions learning approaches likely suffer
situations collecting larger comprehensive datasets developing
generic approaches capable generating naturalistic descriptions across domains
therefore open challenge
supervised likely take advantage carefully collected large
datasets lowering amount supervision exchange access larger unsupervised
data interesting avenue future leveraging unsupervised data
building richer representations description another open challenge
context
measures
designing automatic measures mimic human judgments evaluating suitability image descriptions perhaps urgent need area image description
elliott keller need dramatically observed latest evaluation ms coco challenge according existing measures including latest cider
measure vedantam et al several automatic methods outperform human upper bound upper bound indicates similar human descriptions
counterintuitive nature confirmed fact human judgments used evaluation output even best system judged worse
human generated description time fang et al however since
conducting human judgment experiments costly major need improved automatic measures highly correlated human judgments figure plots
epanechnikov probability density estimate non parametric optimal estimator bleu
meteor rouge cider scores per subjective judgment flickr k dataset human judgments obtained human experts hodosh et al bleu
confirmed unable sufficiently discriminate lowest three human
judgments meteor cider signs moving towards useful separation
diversity originality
current often rely direct representations descriptions see training time making descriptions generated test time similar many


fiautomatic description generation images survey

human judgement

human judgement


perfect
minor mistakes
aspects
relation



















perfect
minor mistakes
aspects
relation

















bleu









meteor
human judgement
perfect
minor mistakes
aspects
relation























perfect
minor mistakes
aspects
relation





human judgement























cider

rouge

figure probability density estimates bleu meteor rouge cider scores
human judgments flickr k dataset axis shows probability density
x axis score computed measure



fibernardi et al

repetitions limits diversity generated descriptions making difficult reach
human levels performance situation demonstrated devlin et al
best model able generate unique descriptions systems generate diverse original descriptions repeat already
seen infer underlying semantics therefore remain open challenge chen
zitnick related approaches take step towards addressing limitations
coupling description visual representation generation
jas parikh introduces notion image specificity arguing domain image descriptions uniform certain images specific others
descriptions non specific images tend vary lot people tend describe nonspecific scene different aspects notion effects description systems
measures investigated detail

tasks
another open challenge visual question answering vqa natural language
question answering text significant goal nlp long
time e g liang jordan klein fader zettlemoyer etzioni richardson burges renshaw fader zettlemoyer etzioni answering questions
images task recently emerged towards achieving goal malinowski
fritz propose bayesian framework connects natural language questionanswering visual information extracted image parts recently image
question answering methods neural networks developed gao mao
zhou huang yuille ren kiros zemel malinowski rohrbach fritz
lu li following effort several datasets task
released daquar malinowski fritz compiled scene depth images
mainly focuses questions type quantity color objects cocoqa ren et al constructed converting image descriptions vqa format
subset images ms coco dataset freestyle multilingual image question answering fm iqa dataset gao et al visual madlibs dataset yu park
berg berg vqa dataset antol et al built images
ms coco time question answer pairs collected via human annotators
freestyle paradigm emerging field likely flourish near future ultimate goal vqa build systems pass recently developed
visual turing test able answer arbitrary questions images
precision human observer malinowski fritz b geman geman hallonquist
younes
multilingual repositories image description interesting direction
explore currently among available benchmark datasets iapr tc
dataset grubinger et al multilingual descriptions english german
future work investigate whether transferring multimodal features monolingual description improved descriptions compared monolingual baselines


fiautomatic description generation images survey

would interesting study different tasks multilingual multimodal
setting larger syntactically diverse multilingual description corpora
overall image understanding ultimate goal computer vision natural language generation one ultimate goals nlp image description
goals interconnected topic therefore likely benefit individual advances
two fields

conclusions
survey discuss recent advances automatic image description closely related
review analyze large body existing work highlighting common
characteristics differences existing particular categorize
related work three groups direct description generation images retrieval
images visual space iii retrieval images multimodal joint visual
linguistic space addition provided brief review existing corpora
automatic evaluation measures discussed future directions vision language

compared traditional keyword image annotation object recognition
attribute detection scene labeling etc automatic image description systems produce
human explanations visual content providing complete picture scene
advancements field could lead intelligent artificial vision systems
make inferences scenes generated grounded image descriptions
therefore interact environments natural manner could
direct impact technological applications visually impaired people
benefit accessible interfaces
despite remarkable increase number image description systems recent
years experimental suggest system performance still falls short human performance similar challenge lies automatic evaluation systems reference
descriptions measures tools currently use sufficiently highly correlated human judgments indicating need measures deal
complexity image description adequately

acknowledgments
thank anonymous reviewers useful comments work partially supported european commission ict cost action iv l net european network integrating vision language ic rc ae ee nic
funded scientific technological council turkey tubitak grant e fk would acknowledge erc funding starting grant
synchronous linguistic visual processing de supported ercim
abcde fellowship
multimodal translation shared task workshop machine translation use
english german translated version flickr k corpora see http www statmt org wmt
multimodal task html details



fibernardi et al

references
antol agrawal lu j mitchell batra zitnick c l parikh
vqa visual question answering international conference computer vision
banerjee lavie meteor automatic metric mt evaluation
improved correlation human judgments annual meeting association computational linguistics workshop intrinsic extrinsic evaluation
measures mt summarization
berg l berg c shih j automatic attribute discovery characterization noisy web data european conference computer vision
chatfield k simonyan k vedaldi zisserman return devil
details delving deep convolutional nets british machine vision conference
chen j kuznetsova p warren choi deja image captions corpus
expressive descriptions repetition north american chapter association
computational linguistics
chen x zitnick c l minds eye recurrent visual representation image
caption generation ieee conference computer vision pattern recognition
dale r white e eds workshop shared tasks comparative
evaluation natural language generation position papers
denkowski lavie meteor universal language specific translation evaluation target language conference european chapter association computational linguistics workshop statistical machine translation
devlin j cheng h fang h gupta deng l x zweig g mitchell
language image captioning quirks works
annual meeting association computational linguistics
donahue j hendricks l guadarrama rohrbach venugopalan saenko
k darrell long term recurrent convolutional networks visual recognition description ieee conference computer vision pattern recognition
elliott de vries p describing images inferred visual dependency
representations annual meeting association computational linguistics
elliott keller f image description visual dependency representations conference empirical methods natural language processing
elliott keller f comparing automatic evaluation measures image
description annual meeting association computational linguistics
elliott lavrenko v keller f query example image retrieval
visual dependency representations international conference computational
linguistics
everingham van gool l williams c k winn j zisserman
pascal visual object classes voc challenge international journal computer
vision


fiautomatic description generation images survey

fader zettlemoyer l etzioni paraphrase driven learning open question answering annual meeting association computational linguistics
fader zettlemoyer l etzioni open question answering curated
extracted knowledge bases acm sigkdd conference knowledge discovery
data mining
fang h gupta iandola f srivastava r deng l dollar p gao j x
mitchell platt j zitnick c l zweig g captions visual
concepts back ieee conference computer vision pattern recognition
farhadi hejrati sadeghi young p rashtchian c hockenmaier j
forsyth every picture tells story generating sentences images
european conference computer vision
felzenszwalb p f girshick r b mcallester ramanan object detection discriminatively trained part ieee transactions pattern
analysis machine intelligence
feng lapata automatic image annotation auxiliary text information annual meeting association computational linguistics
feng lapata automatic caption generation news images ieee
transactions pattern analysis machine intelligence
ferraro f mostafazadeh n huang vanderwende l devlin j galley
mitchell survey current datasets vision language
conference empirical methods natural language processing
gao h mao j zhou j huang z yuille talking machine
dataset methods multilingual image question answering international
conference learning representations
geman geman hallonquist n younes l visual turing test computer
vision systems proceedings national academy sciences
girshick r donahue j darrell malik j rich feature hierarchies accurate object detection semantic segmentation ieee conference computer
vision pattern recognition
gong wang l hodosh hockenmaier j lazebnik improving imagesentence embeddings large weakly annotated photo collections european
conference computer vision
grubinger clough p muller h deselaers iapr tc benchmark
evaluation resource visual information systems international conference
language resources evaluation
guadarrama krishnamoorthy n malkarnenkar g venugopalan mooney r darrell saenko k youtube text recognizing describing arbitrary
activities semantic hierarchies zero shot recognition international conference computer vision
gupta verma jawahar c v choosing linguistics vision describe
images aaai conference artificial intelligence


fibernardi et al

hardoon r szedmak shawe taylor j canonical correlation analysis
overview application learning methods neural computation

hodosh hockenmaier j sentence image description scalable
explicit ieee conference computer vision pattern recognition
workshops
hodosh young p hockenmaier j framing image description ranking task data evaluation metrics journal artificial intelligence

hotelling h relations two sets variates biometrika
jaimes chang f conceptual framework indexing visual information
multiple levels ist spie internet imaging
jas parikh image specificity ieee conference computer vision
pattern recognition
jia x gavves e fernando b tuytelaars guiding long short term
memory model image caption generation international conference computer vision
johnson j krishna r stark li l j shamma bernstein fei fei l
image retrieval scene graphs ieee conference computer vision
pattern recognition
karpathy fei fei l deep visual semantic alignments generating image
descriptions ieee conference computer vision pattern recognition
karpathy joulin fei fei l deep fragment embeddings bidirectional
image sentence mapping advances neural information processing systems
khan u g zhang l gotoh towards coherent natural language description video streams international conference computer vision workshops
kiros r salakhutdinov r zemel r unifying visual semantic embeddings
multimodal neural language advances neural information processing systems deep learning workshop
krishnamoorthy n malkarnenkar g mooney r saenko k guadarrama
generating natural language video descriptions text mined knowledge
annual conference north american chapter association computational linguistics human language technologies
kulkarni g premraj v dhar li choi berg c berg l baby
talk understanding generating simple image descriptions ieee conference
computer vision pattern recognition
kuznetsova p ordonez v berg c berg l choi collective
generation natural image descriptions annual meeting association
computational linguistics


fiautomatic description generation images survey

kuznetsova p ordonezz v berg l choi treetalk composition
compression trees image descriptions conference empirical methods
natural language processing
lampert c h nickisch h harmeling learning detect unseen object
classes class attribute transfer ieee conference computer vision
pattern recognition
lazebnik schmid c ponce j beyond bags features spatial pyramid
matching recognizing natural scene categories ieee conference computer
vision pattern recognition
lebret r pinheiro p collobert r phrase image captioning
international conference machine learning
li kulkarni g berg l berg c choi composing simple image
descriptions web scale n grams signll conference computational
natural language learning
liang p jordan klein learning dependency compositional
semantics computational linguistics
lin c hovy e automatic evaluation summaries n gram cooccurrence statistics annual conference north american chapter
association computational linguistics human language technologies
lin fidler kong c urtasun r generating multi sentence natural
language descriptions indoor scenes british machine vision conference
lin maire belongie hays j perona p ramanan dollar p zitnick
c l microsoft coco common objects context european conference
computer vision
lowe distinctive image features scale invariant keypoints international
journal computer vision
l lu z li h learning answer questions image convolutional
neural network aaai conference artificial intelligence
malinowski fritz multi world question answering
real world scenes uncertain input advances neural information processing systems
malinowski fritz b towards visual turing challenge advances
neural information processing systems workshop learning semantics
malinowski rohrbach fritz ask neurons neural
answering questions images international conference computer vision
mao j xu w yang wang j yuille l deep captioning multimodal recurrent neural networks rnn international conference learning
representations


fibernardi et al

mao j wei x yang wang j huang z yuille l b learning
child fast novel visual concept learning sentence descriptions images
international conference computer vision
mason r charniak e nonparametric method data driven image captioning annual meeting association computational linguistics
mason w watts j financial incentives performance crowds
acm sigkdd workshop human computation
mitchell han x dodge j mensch goyal berg c yamaguchi k
berg l stratos k daume iii h iii midge generating image
descriptions computer vision detections conference european chapter
association computational linguistics
nenkova vanderwende l impact frequency summarization tech
rep microsoft
oliva torralba modeling shape scene holistic representation
spatial envelope international journal computer vision
ordonez v kulkarni g berg l im text describing images million
captioned photographs advances neural information processing systems
ortiz l g wolff c lapata learning interpret describe
abstract scenes conference north american chapter association
computational linguistics
panofsky e studies iconology oxford university press
papineni k roukos ward zhu w j bleu method automatic evaluation machine translation annual meeting association
computational linguistics
parikh grauman k relative attributes international conference
computer vision
park c kim g expressing image stream sequence natural
sentences advances neural information processing systems
patterson g xu c su h hays j sun attribute database beyond categories deeper scene understanding international journal computer vision

pinheiro p lebret r collobert r simple image description generator via
linear phrase model international conference learning representations
workshop
prest schmid c ferrari v weakly supervised learning interactions
humans objects ieee transactions pattern analysis machine
intelligence
rashtchian c young p hodosh hockenmaier j collecting image annotations amazons mechanical turk north american chapter association
computational linguistics human language technologies workshop creating
speech language data amazons mechanical turk


fiautomatic description generation images survey

reiter e belz investigation validity metrics automatically evaluating natural language generation systems computational linguistics

reiter e dale r building natural language generation systems cambridge
university press
ren kiros r zemel r image question answering visual semantic embedding model dataset international conference machine learningt
deep learning workshop
richardson burges c j renshaw e mctest challenge dataset
open domain machine comprehension text conference empirical methods
natural language processing
rohrbach rohrback tandon n schiele b dataset movie description international conference computer vision
rohrbach qiu w titov thater pinkal schiele b translating
video content natural language descriptions international conference
computer vision
schuster krishna r chang fei fei l manning c generating
semantically precise scene graphs textual descriptions improved image retrieval conference empirical methods natural language processing vision
language workshop
shatford analyzing subject picture theoretical cataloging
classification quarterly
silberman n kohli p hoiem fergus r indoor segmentation support
inference rgbd images european conference computer vision
socher r fei fei l connecting modalities semi supervised segmentation
annotation im ages unaligned text corpora ieee conference
computer vision pattern recognition
socher r karpathy le q v manning c ng grounded compositional semantics finding describing images sentences transactions
association computational linguistics
sun c gan c nevatia r automatic concept discovery parallel text
visual corpora international conference computer vision
thomason j venugopalan guadarrama saenko k mooney r integrating language vision generate natural language descriptions videos
wild international conference computational linguistics
torralba fergus r freeman w million tiny images large data
set nonparametric object scene recognition ieee transactions pattern
analysis machine intelligence
ushiku yamaguchi mukuta harada common subspace model
similarity phrase learning caption generation images international
conference computer vision


fibernardi et al

vedantam r lawrence zitnick c parikh cider consensus image description evaluation ieee conference computer vision pattern
recognition
verma jawahar c v im text text im associating images texts
cross modal retrieval british machine vision conference
vinyals toshev bengio erhan tell neural image
caption generator ieee conference computer vision pattern recognition
xu k ba j kiros r cho k courville salakhutdinov r zemel r bengio
attend tell neural image caption generation visual attention
international conference machine learning
yagcioglu erdem e erdem cakici r distributed representation
query expansion image captioning annual meeting
association computational linguistics
yang teo c l daume iii h aloimonos corpus guided sentence generation natural images conference empirical methods natural language
processing
yao b fei fei l grouplet structured image representation recognizing
human object interactions ieee conference computer vision pattern
recognition
yao l torabi cho k ballas n pal c larochelle h courville
describing videos exploiting temporal structure international conference
computer vision
yatskar galley vanderwende l zettlemoyer l see evil say
evil description generation densely labeled images joint conference
lexical computation semantics
young p lai hodosh hockenmaier j image descriptions visual
denotations similarity metrics semantic inference event descriptions
transactions association computational linguistics
yu l park e berg c berg l visual madlibs fill blank
description generation question answering international conference computer vision
zhu kiros r zemel r salakhutdinov r urtasun r torralba fidler
aligning books movies towards story visual explanations watching
movies reading books international conference computer vision
zitnick c l parikh vanderwende l learning visual interpretation
sentences international conference computer vision
zitnick c l parikh bringing semantics focus visual abstraction
ieee conference computer vision pattern recognition




