Journal Articial Intelligence Research 55 (2016) 1091-1133

Submitted 09/2015; published 04/2016

Semantic Visualization
Neighborhood Graph Regularization
Tuan M. V. Le
Hady W. Lauw

vmtle.2012@phdis.smu.edu.sg
hadywlauw@smu.edu.sg

School Information Systems
Singapore Management University
80 Stamford Road, Singapore 178902

Abstract
Visualization high-dimensional data, text documents, useful map
similarities among various data points. high-dimensional space, documents
commonly represented bags words, dimensionality equal vocabulary
size. Classical approaches document visualization directly reduce visualizable
two three dimensions. Recent approaches consider intermediate representation
topic space, word space visualization space, preserves semantics
topic modeling. aiming good model parameters
observed data, previous approaches considered local consistency among data
instances. consider problem semantic visualization jointly modeling topics
visualization intrinsic document manifold, modeled using neighborhood graph.
document topic distribution visualization coordinate. Specically,
propose unsupervised probabilistic model, called Semafore, aims preserve
manifold lower-dimensional spaces neighborhood regularization framework
designed semantic visualization task. validate ecacy Semafore,
comprehensive experiments number real-life text datasets news articles
Web pages show proposed methods outperform state-of-the-art baselines
objective evaluation metrics.

1. Introduction
Text documents come various avors, Web pages, news articles, blog posts, emails,
messages social media Twitter. much English, increasing
amounts content various languages well. backdrop growth volume, diversity, complexity various corpora, need useful tools analyze
wealth text content. One form analysis look paper visualization. dierent types visualizations, temporal longitudinal,
networked, natures. interested form visualization
represent collection documents coordinates low-dimensional space,
learn similarities dierences among documents based distances
visualization space.
Visualization high-dimensional data important exploratory data analysis task,
actively studied various academic communities. HCI community
interested presentation information, well interface aspects (Chi, 2000),
machine learning community interested quality dimensionality reduction
c
2016
AI Access Foundation. rights reserved.

fiLe & Lauw

(Van der Maaten & Hinton, 2008), i.e., transform high-dimensional representation lower-dimensional representation shown scatterplot.
visualization form simple, widely applicable across various domains.
Consider therefore problem visualizing documents scatterplot. Commonly,
document represented bag words, i.e., vector word counts. highdimensional representation would reduced coordinates visualizable 2D (or 3D)
space. One pioneering technique Multidimensional Scaling (MDS) (Kruskal, 1964).
goal preserve distances high-dimensional space low-dimensional embedding. applied documents, visualization technique generic high-dimensional
data, e.g., MDS, may necessarily preserve topical semantics. Words often ambiguous, issues polysemy, word carries multiple senses,
synonymy, dierent words carry sense. dimensions original representation (which words) may accurately capture ambiguity, aects
quality reduced representation (which visualization space) well.
model semantics documents way resolve ambiguity,
current popular approach topic modeling, PLSA (Hofmann, 1999) LDA
(Blei, Ng, & Jordan, 2003). document associated probability distribution
set topics. topic probability distribution words vocabulary.
way, polysemous words separated dierent topics, synonymous words
grouped topic.
Topic modeling another form dimensionality reduction: word space
topic space. word space refers documents original representation, usually
bag words. topic space refers simplex topic distributions. documents
probability distribution topics eectively representation document
topic space. However, topic model designed visualization.
one possible visualization plot documents topic distributions simplex, 2D
visualization space could express three topics, limiting.
Given success modeling semantics documents, therefore ask question
whether best forms dimensionality reductions (visualization
topic modeling) documents. end goal arrive visualization documents
consistent semantic representation (topics), well original
representation (words). coupling distinct task topic modeling visualization
respectively, enables novel capabilities. one thing, topic modeling helps create
richer visualization, associate coordinate visualization space
topic word distributions, providing semantics visualization space.
another, tight integration potentially allows visualization serve way
explore tune topic models, allowing users introduce feedback (Hu, Boyd-Graber,
Satino, & Smith, 2014) model visual interface (Choo, Lee, Reddy, & Park,
2013). capabilities support several use case scenarios. One potential use case
document organizer system. visualization could potentially help assigning categories
documents, showing closely related documents labeled. Another
augmented retrieval system. Given query, results may include relevant
documents, similar documents (neighbors visualization).
1092

fiSemantic Visualization Neighborhood Graph Regularization

1.1 Problem Statement
refer task jointly modeling topics visualization semantic visualization.
input set documents D. specied number topics Z visualization
dimensionality (assumed 2D, without losing generality), goal derive,
every document D, latent coordinate visualization space, probability
distribution Z topics. focus documents description,
approach would apply visualization data types latent factor modeling,
i.e., topic model, makes sense.
straightforward way undergo two-step reductions. rst reduction,
original representation documents reduced topic distributions using topic modeling. second reduction, documents topic distributions reduced
visualization coordinates. approach may value compared direct reduction word space visualization space. However, ideal, disjoint
reductions could mean errors may propagate rst second reduction,
resulting visualization may faithfully capture original representation.
better way solve problem join two reductions single, joint
process produces topic distributions visualization coordinates. approach
rst pioneered PLSV (Iwata, Yamada, & Ueda, 2008), showed
joint approach outperformed disjoint approach. PLSV derives latent parameters
maximizing likelihood observing documents. goal concerned
error model observation.
literature, found algorithms ensure smoothness tend perform
better learning tasks (Zhou, Bousquet, Lal, Weston, & Scholkopf, 2004). Smoothness
concerns preserving observed proximity documents. objective arises naturally assumption intrinsic geometry data low-rank, non-linear
subspace within high-dimensional space. Therefore, preserving neighborhood structure
important learning tasks. assumption well-accepted machine learning
community (Laerty & Wasserman, 2007), nds application supervised
unsupervised learning (Belkin & Niyogi, 2003; Zhou et al., 2004; Zhu, Ghahramani, Lafferty, et al., 2003). Recently, preponderance evidence assumption
applies text data particular (Cai, Mei, Han, & Zhai, 2008; Cai, Wang, & He, 2009;
Huh & Fienberg, 2012). therefore propose incorporate assumption new
unsupervised, semantic visualization model.
1.2 Overview
propose unsupervised probabilistic model jointly derives topic distributions
visualization coordinates intrinsic geometry data. proposed model called
Semafore, stands SEmantic visualization MAniFOld REgularization.
build neighborhood regularization framework semantic visualization model.
framework involves new issues resolve, including regularization function,
space regularization take place.
model evaluated series real-life, publicly available datasets,
benchmark datasets used document classication task. advantage statistical
method, ours, dependent specic language. Two datasets
1093

fiLe & Lauw

English, one Brazilian Portuguese. model unsupervised (class
label neither required used learning), objectively quantify visualization quality, leverage class label information. common assumption documents
class expected neighbors original space (Belkin, Niyogi, &
Sindhwani, 2006; Zhou et al., 2004; Zhu et al., 2003), suggests
close visualization space. investigate eectiveness Semafore placing
documents class nearby visualization space, systematically compare
existing baselines without one properties, namely: joint modeling
topic visualization, neighborhood regularization.
1.3 Contributions
visualization topic modeling are, separately, well-studied problems, interface
two, semantic visualization, relatively new problem, previous
work. work, make following contributions.
propose incorporating neighborhood structure semantic visualization.
respect, propose probabilistic model Semafore, two integrated components. One kernelized semantic visualization model, enabling substitution
kernel functions relate visualization coordinates topic distributions (see
Section 3.3). neighborhood graph regularization framework semantic
visualization described Section 4.1.
Realizing neighborhood graph regularization involves exploration
incorporate appropriate forms neighborhood structure. respect,
investigate eects neighborhood graph construction techniques knearest neighbors (k-NN), -ball, disjoint minimum spanning trees (DMST),
well dierent edge weight estimations heat-kernel (see Section 4.2)
context semantic visualization.
Section 5, describe requisite learning algorithms based maximum
posteriori (MAP) estimation using expectation-maximization (EM), order
parameters various regularization functions kernels propose.
nal contribution evaluation Semafores eectiveness series reallife, public datasets described Section 6, shows Semafore outperforms
existing baselines well-established objective visualization metric.
prior work (Le & Lauw, 2014b), proposed problem described preliminary model. extended article, signicant technical changes provide
signicantly comprehensive discussion model. instance, discuss
Student-t kernel, addition previously introduced Gaussian kernel. Furthermore, investigate ecacies dierent neighborhood graph constructions, including
-ball DMST graphs, addition previously introduced kNN graph.
graph weights enhanced investigation heat kernel, addition
simple-minded binary scheme previously. discussed Section 6.3, enhancements
collectively result statistically signicant improvements previous model. Beyond
1094

fiSemantic Visualization Neighborhood Graph Regularization

technical enhancements, provide comprehensive model analysis empirical validation, including richer quantitative qualitative discussions visualizations
resulting topic models, well metric measure topic interpretability based
pairwise mutual information.

2. Related Work
section, discuss dierent aspects work, identify related papers
literature, point key conceptual dierences.
2.1 Visualization Dimensionality Reduction
One way perform visualization using generic dimensionality reduction technique.
techniques come several avors, depending objective. Principal component
analysis (PCA) (Jollie, 2005) identies components explain variance
data. Related PCA singular value decomposition (SVD) (Golub & Van Loan,
2012). Comparatively, independent component analysis (ICA) (Comon, 1994) identies
components independent one another, whereas linear discriminant analysis
(Fishers LDA) (Fisher, 1936) identies components discriminate
known class labels. generic, techniques frequently applied feature
extraction, optimized visualization. focus properties
components (e.g., orthogonality, independence) rather intrinsic relationship
among data instances. Furthermore, based linear projections, may
capture non-linearities data well.
Another category techniques, directly related visualization,
embedding approach. aims preserve high-dimensional similarities dierences
low-dimensional embedding. One pioneering work multidimensional scaling
(MDS) (Kruskal, 1964). Given set pairwise distances ij data points j,
MDS determines coordinates xi xj respectively, embedded visualization
distance ||xi xj || approximates ij much possible. MDS, distance
preserved ij frequently linear distance, measuring distance along straight line
two points input space. Instead linear distance, Isomap (Tenenbaum,
De Silva, & Langford, 2000) seeks preserve geodesic distance, nding shortest paths
graph edges connecting neighboring data points. LLE (Roweis & Saul, 2000) seeks
preserve linear distances, among neighboring points avoiding need
estimate pairwise distances widely separated data points. Recently
works applying similar concept embedding using probabilistic modeling,
PE (Iwata, Saito, Ueda, Stromsten, Griths, & Tenenbaum, 2007), SNE (Hinton & Roweis,
2002), t-SNE (Van der Maaten & Hinton, 2008), GTM (Bishop, Svensen, & Williams,
1998). Yet others based semi-denite programming (Shaw & Jebara, 2007, 2009).
Alternatively, several embedding techniques aim preserve relationship among
data instances, rather properties local minima (Kim & Torre, 2010).
Importantly, techniques optimized semantic visualization,
model topics all. coordinates reect semantic meaning,
reecting optimization objective.
1095

fiLe & Lauw

related works far seek address semantic visualization
task directly. closest previous work topic modeling visualization
single generative process Probabilistic Latent Semantic Visualization (PLSV) (Iwata
et al., 2008), shows joint approach outperforms separate approach.
PLSV builds upon foundation topic modeling technique Probabilistic Latent
Semantic Analysis (PLSA) (Hofmann, 1999) incorporating visualization coordinates,
build upon foundation PLSV incorporating RBF kernels (Section 3.3)
neighborhood structure (Section 4).
related works share similar objective, share
paradigm visualization topic modeling. instance, LDA-SOM (Millar, Peterson, &
Mendenhall, 2009) rst conducts topic modeling using Latent Dirichlet Allocation (LDA)
(Blei et al., 2003), separately embeds documents topic distributions
Self-Organizing Map (SOM) (Kohonen, 1990). However, joint model,
SOM uses dierent visualization space Euclidean space interested
in. another instance, SSE (Le & Lauw, 2014a) builds Spherical Admixture
Model (SAM) (Reisinger, Waters, Silverthorn, & Mooney, 2010) belonging class
spherical topic models targeted spherical (unit vector) reprepresentations topics
documents, directly comparable equivalent simplex representation
multinomial modeling (probability distribution words) adopted work well
PLSV.
semantic visualization, refer task joining visualization topic modeling. related, dierent, task topic visualization, objective visualize
topics, terms keywords dominant topic (Chaney & Blei, 2012;
Chuang, Manning, & Heer, 2012), topics dominant corpus (Wei, Liu, Song,
Pan, Zhou, Qian, Shi, Tan, & Zhang, 2010), topics related one another
(Gretarsson, Odonovan, Bostandjiev, Hollerer, Asuncion, Newman, & Smyth, 2012).
2.2 Topic Modeling
Topic model involves statistical modeling text (documents words) order discover
abstract concepts topics occur corpus. Beginning latent semantic
indexing (Dumais, Furnas, Landauer, Deerwester, Deerwester, et al., 1995), topic model
evolves modern probabilistic treatments, Probabilistic Latent Semantic
Analysis (PLSA) (Hofmann, 1999) Latent Dirichlet Allocation (LDA) (Blei et al.,
2003). Intuitively, topic captures collection words tend co-occur
describe concept. appeal producing highly interpretable
statistical models let users make semantic sense corpus. text-only
document corpora, topic models applied cases links observed
addition text (McCallum, Wang, & Corrada-Emmanuel, 2007).
Meanwhile, assumption intrinsic geometry data non-linear low
dimensional subspace within high-dimensional space nds application supervised
unsupervised (Belkin & Niyogi, 2003) learning algorithms. especially prevalent
semi-supervised learning (Zhou et al., 2004; Zhu et al., 2003) way bridge labeled
unlabeled data. Regularization technique realize assumption long
history (Belkin et al., 2006). specic form regularization function varies among
1096

fiSemantic Visualization Neighborhood Graph Regularization

applications. study assumption unsupervised topic models begins
LapPLSI (Cai et al., 2008), introduces regularization PLSA (Hofmann, 1999),
minimizing Euclidean distance neighboring documents topic distributions.
Follow-up work introduce distance functions (Cai et al., 2009; Wu, Bu, Chen, Zhu,
Zhang, Liu, Wang, & Cai, 2012). previous work focus maintaining proximity
similar documents, DTM (Huh & Fienberg, 2012) adds new criterion maintain
distance among dierent documents. work dierent need
contend visualization aspects, topic modeling.
2.3 Semantic Similarity
topic models, alternative mechanisms learn semantic relationship
documents. One way measuring semantic similarity among documents
words. instance, vector space model, documents may represented term
vector, similarity may expressed terms cosine similarity (Turney, Pantel,
et al., 2010). word occurrences alone, could additional signals
semantic similarity. instance, working Wikipedia corpus, categories
links took account determine similarity among articles (Gabrilovich
& Markovitch, 2009; Ponzetto & Strube, 2007). work diers several
important respects. First, objective similarity value per se, rather
determining lower-dimensional embedding coordinates, would allow visualization
one application. Second, method based probabilistic modeling latent variables,
akin topic modeling, instead operating vector space model representation
documents.

3. Semantic Visualization
introduce problem formulation semantic visualization Section 3.1. focus
paper eects neighborhood graph structure semantic visualization
task. gure clearest way showcase eects design neighborhood
preservation framework existing generative process, PLSV (Iwata
et al., 2008), review Section 3.2. Section 3.3, describe innovation
semantic visualization model, abstraction mapping
topic space visualization space using radial basis function (RBF) kernels.
allows exploration various kernels, identify two exploration.
ease following discussion, include table notations Table 1.
3.1 Problem
task semantic visualization, input corpus documents = {d1 , . . . , dN }.
Every dn bag words, wnm denotes mth word dn . total number
words dn Mn . objective learn, dn , latent distribution Z
topics {P(z|dn )}Z
z=1 . topic z associated parameter z , probability
distribution {P(w|z )}wW words vocabulary W . words highest
probabilities given topic capture semantic topic.
1097

fiLe & Lauw

Notation
dn
xn
Mn
z
z
z
W
N
Z





Description
specic document
latent coordinate dn visualization space
number words document dn
specic topic
coordinate topic z visualization space
word distribution topic z
vocabulary (the set words lexicon)
total number documents corpus
total number topics (user-dened)
collection xn documents
collection z topics
collection z topics
collective set parameters {, , }

Table 1: Notations.
semantic visualization, additional objective semantic visualization,
learn, document dn , latent coordinate xn low-dimensionality
visualization space. Similarly, topic z associated latent coordinate z
visualization space. document dn topic distribution expressed terms
Euclidean distance coordinate xn dierent topic coordinates
= {z }Z
z=1 . Intuitively, closer xn topics z , higher P(z|dn )
probability topic z document dn .
following sections, systematically describe various components
solution. generative process links latent variables (coordinates) words
documents described Section 3.2. specic relationship documents
topics coordinates constitutes specic mapping function, model RBF
kernel Section 3.3. following Section 4, discuss incorporate neighborhood
structure semantic visualization.
3.2 Generative Process
describe generative process documents based topics visualization
coordinates. review PLSV whose graphical model shown Figure 1.
eventual complete model generalization model, involving enhancements
kernelization (Section 3.3) neighborhood structure preservation (Section 4).
generative process follows:
1. topic z = 1, . . . , Z:
(a) Draw zs word distribution: z Dirichlet()
(b) Draw zs coordinate: z Normal(0, 1 I)
2. document dn , n = 1, . . . , N :
(a) Draw dn coordinate: xn Normal(0, 1 I)
1098

fiSemantic Visualization Neighborhood Graph Regularization

N



Mn w

x

Z

z









Figure 1: Graphical model PLSV.
(b) word wnm dn :
i. Draw topic: z Multi({P(z|xn , )}Z
z=1 )
ii. Draw word: wnm Multi(z )
Here, Dirichlet prior, identity matrix, control variance
Z
Z
Normal distributions. parameters = {xn }N
n=1 , = {z }z=1 , = {z }z=1 , collectively
denoted = , , , learned documents based maximum posteriori
estimation. log likelihood function shown Equation 1.
L(|D) =

Mn
N

n=1 m=1

log

Z


P(z|xn , )P(wnm |z )

(1)

z=1

reiterate focus incorporating neighborhood graph structure
semantic visualization. building neighborhood graph regularization framework
existing generative process, i.e., PLSV, clearly observe improvement
PLSV arises neighborhood graph regularization. sense, work
tradition introducing neighborhood graph regularization probabilistic topic
modeling (Huh & Fienberg, 2012; Cai et al., 2008, 2009), contributions relate
neighborhood graph regularization, rather generative process. said,
one signicant dierence PLSV, exibility allowing various kernel
functions, discuss next.
3.3 RBF Kernels
Step 2(b)i generative process, topic z word drawn
distribution {P(z|xn , )}Z
z=1 . distribution relates coordinates topics
visualization space = {z }Z
z=1 coordinate xn document dn
documents topic distribution {P(z|dn )}Z
z=1 .
relationship formulated mapping problem want nd
function G maps point visualization space point topic space. However,
form G cannot known exactly visualization space topic space
latent spaces G may dierent across dierent domains. Therefore, compute
topic distributions, need way approximate G.
build function approximation unknown function G, use abstraction
Radial Basis Function (RBF) neural networks (Bishop, 1995) feedforward multilayered RBF neural networks one hidden layer serve universal approximator
1099

fiLe & Lauw

K nz




Z
Z










/xn

Figure 2: Topic distribution expressed function visualization coordinates using
Radial Basis Function (RBF) network.

arbitrary continuous functions (Park & Sandberg, 1991). property provides
condence model would ability approximate existing relationship
visualization space topic space arbitrary precision. Unlike PLSV (Iwata
et al., 2008) dened specic mapping function, approach generalizes semantic visualization model dening mapping problem terms kernelization,
admits several mapping functions within family RBF kernels.
context, Radial Basis Function (Buhmann, 2000) relate coordinate variables
based distances denes kernel function (||xn z ||) terms far data
point (e.g., xn ) center (e.g., z ). kernel function may take various forms,
e.g., Gaussian, multi-quadric, inverse quadratic, polyharmonic spline. express P(z|dn )
function xn , consider normalized architecture RBF network, three
layers shown Figure 2. input layer consists one input node (xn ). hidden
layer consists Z number normalized RBF activation functions. centered
z computes Z (||xn z ||) . linear output layer consists Z output nodes.
z =1

(||xn z ||)

output node yz (xn ) corresponds P(z|dn ), linear combination RBF
functions, shown Equation 2. Here, wz,z weight inuence RBF function

z P(z|dn ), constraint Z
z =1 wz,z = 1.
Z
P(z|dn ) = yz (xn ) =

z =1 wz,z (||xn z ||)
Z
z =1 (||xn z ||)

(2)

Equation 2 general form, instantiate specic mapping function,
need determine assignment wz,z form function . wz,z ,
experiment special case wz,z = 1 z = z 0 otherwise.
kernel function , one variation consider Gaussian, yields function Equation 3, refers collective set z s. Note set variance
Gaussian 1. However, true value really important dierent variance
value produces re-scaled visualization scaling factor equal variance.
1100

fiSemantic Visualization Neighborhood Graph Regularization

exp( 12 ||xn z ||2 )
P(z|dn )Gaussian = P(z|xn , )Gaussian = Z
1
2
z =1 exp( 2 ||xn z || )

(3)

Another variation considered Student-t. distribution used
t-SNE (Van der Maaten & Hinton, 2008) context non-semantic, direct embedding mitigate eects crowding. Due mismatched dimensionalities, points
crunched together center visualization, prevents gaps forming
clusters. Therefore, hypothesize using Student-t radial basis function, yields function Equation 4, help improve performance
model crowding becomes issue. Note Student-t distribution one degree
freedom yields radial basis function form similar inverse quadratic.
(1 + ||xn z ||2 )1
P(z|dn )Studentt = P(z|xn , )Studentt = Z
2 1
z =1 (1 + ||xn z || )

(4)

Gaussian function (Equation 3) used previously baseline PLSV
(Iwata et al., 2008) compare to. inclusion helps establish parity
comparative purposes, investigate eectiveness alternative Student-t
kernel (described above), well neighborhood regularization (described
next section).

4. Neighborhood Graph Regularization Framework
recent works (Cai et al., 2008, 2009; Huh & Fienberg, 2012) trying preserve
local neighborhood structure learning low-dimensional topic representations documents. works assume documents sampled nonlinear low-dimensional
subspace embedded high-dimensional space. Therefore, local neighborhood
structure important revealing hidden topics documents preserved
learning topic representations documents (Bai, Guo, Lan, & Cheng, 2014).
generative process semantic visualization described Section 3, document parameters sampled independently, may necessarily reect underlying local neighborhood structure. therefore seek realize assumption semantic visualization.
particular, assume two documents di dj close original space,
parameters j low-rank representation similar well. Coupled
kernelized semantic visualization model described Section 3, neighborhood
preservation approach described section constitutes proposed model, Semafore,
stands SEmantic visualization MAniFOld REgularization.
4.1 Neighborhood Regularization
neighborhood structure represented neighborhood graph. Given set
data points Euclidean space, neighborhood graph constructed input
data points vertices. denition, edges symmetric, i.e., ij = ji , weighted.
collection edge weights collectively denoted = {ij }.
moment, assume neighborhood graph, address
issue neighborhood graph may incorporated semantic visualiza1101

fiLe & Lauw

tion framework. actuality, neighborhood graph construction important
component, whose construction described detail Section 4.2.
One eective means incorporate neighborhood structure learning model
regularization framework (Belkin et al., 2006). leads re-design
log-likelihood function Equation 1 new regularized function L (Equation 5),
consists parameters (visualization coordinates topic distributions),
documents neighborhood structure.
L(|D, ) = L(|D) + R(|)

(5)

rst component L log-likelihood function Equation 1, reects
latent parameters observation D. second component R
regularization function, reects consistency latent parameters
neighboring documents neighborhood structure . regularization parameter,
commonly found neighborhood based algorithms (Belkin et al., 2006; Cai et al., 2008,
2009), controls extent regularization (we experiment dierent
experiments).
4.1.1 Proposed Regularization Function
turn denition R function. intuition data points
close high-dimensional space, close low-rank representations, i.e., local consistency, known smoothness. One function satises
R+ Equation 6. Here, F distance function operates low-rank space.
Minimizing R+ leads minimizing distance F(i , j ) neighbors (ij = 1).
R+ (|) =

N


ij F(i , j )

(6)

i,j=1;i=j

level local consistency still insucient, regulate
non-neighbors (i.e., ij = 0) behave. instance, prevent non-neighbors
similar low-rank representations. Another valid objective visualization keep
non-neighbors apart, satised another objective function R Equation 7. R
minimized two non-neighbors di dj (i.e., ij = 0) distant low-rank
representations. addition 1 F prevent division-by-zero error.
R (|) =

N

i,j=1;i=j;ij =0

1 ij
F(i , j ) + 1

(7)

hypothesize neither objective eective own. complete objective
would capture spirits keeping neighbors close, keeping non-neighbors apart.
Therefore, put Equation 6 Equation 7 together using summation maximize
objective function shown Equation 8. Note coecient 12 Equation 8
simplifying formula derivative R (|).
1
R (|) = (R+ (|) + R (|))
2
1102

(8)

fiSemantic Visualization Neighborhood Graph Regularization

2

d1

1

d2

I1

I2

0
-2

-1

0
-1

1

2

3

4

d3

-2

Figure 3: Example topic distribution may dierent visualization coordinates. points red line topic distributions.

Summation preserves absolute magnitude distance, helps improve
visualization task keeping non-neighbors separated visualizable Euclidean space.
Taking product unsuitable, constrains ratio distances neighbors distances non-neighbors. may result crowding eect,
many documents clustered together, relative ratio may maintained,
absolute distances visualization space could small.
proposed regularization function above, possible consider
regularization functions. instance, experimented modifying
regularization function adapted Discriminative Topic Model (DTM) (Huh & Fienberg,
2012), addressed topic modeling semantic visualization. Note
original DTM formulation, distance function F(i , j ) operates topic space,
adapt semantic visualization redening distance function F(i , j )
operate visualization space instead. modied DTM formulation shown
underperform proposed regularization function (Le & Lauw, 2014b).
4.1.2 Enforcing Neighborhood Structure: Visualization vs. Topic Space
turn denition F(1 , 2 ). neighborhood-based models (Belkin et al.,
2006; Cai et al., 2008, 2009), one low-rank representative space. semantic
visualization, two: topic visualization spaces. look
enforce neighborhood graph structure.
rst glance, seem equivalent. all, representations
documents. However, necessarily case. Consider simple example two
topics z1 z2 visualization coordinates 1 = (0, 0) 2 = (2, 0) respectively.
Meanwhile, three documents {d1 , d2 , d3 } coordinates x1 = (1, 1), x2 = (1, 1),
x3 = (1, 1). two documents coordinates,
topic distributions. example, x1 x2 equidistant 1 2 ,
therefore according Equation 3, topic distribution P(z1 |d1 ) =
P(z1 |d2 ) = 0.5, P(z2 |d1 ) = P(z2 |d2 ) = 0.5. two documents topic
distributions, may necessarily coordinates. d3
1103

fiLe & Lauw

topic distribution d1 d2 , dierent coordinate. fact, coordinate
form (1, ?) topic distribution. example illustrated Figure 3.
suggests enforcing neighborhood structure topic space may necessarily lead data points closer visualization space. postulate
regularizing visualization space eective. advantages computational eciency so, describe shortly. Therefore,
dene F(i , j ) squared Euclidean distance ||xi xj ||2 corresponding
visualization coordinates.
4.2 Neighborhood Graph
discuss neighborhood graph may approximated, concerns two
issues graph edges dened, well weighted. neighborhood graph constructed original data space represent document
tf-idf vector (Manning, Raghavan, Schutze, et al., 2008). experiment dierent
vector representations, including word counts term frequencies, nd tf-idf give
best results. distance two document vectors measured using Euclidean
distance.
4.2.1 Graph Construction
research studies properties methods construction neighborhood graphs (Zemel & Carreira-Perpinan, 2004; Carey & Mahadevan, 2014). Since
construction neighborhood graph critical step may aect performance
various graph-based algorithms, problem research issue independent interest. scope exploring well-established graph construction techniques
may apply case semantic visualization. investigate various graph
construction methods empirically Section 6.
following, briey review two categories graph construction methods.
1. Neighborhood-based Graphs. formulation, edges formed data points
deemed suciently close other. admits dierent denitions
sucient closeness. common denitions found literature include
two below.
(a) -ball: neighborhood graph contains edge connecting two documents di
dj , di dj distance less threshold .
(b) k-nearest neighbors (k-NN) graph: neighborhood graph contains edge
connecting two documents di dj , di set Nk (dj ) knearest
neighbors dj , dj set Nk (di ).
-ball k-NN strongly data-dependent parameters (i.e., k)
straightforward choose best value parameters. Neither guarantees
graph would connected. need carefully selected tuned,
extent aect balance contribution neighbors
R+ non-neighbors R neighborhood regularization R Equation 8.
1104

fiSemantic Visualization Neighborhood Graph Regularization

Appendix A, explore empirically graph parameters help maintain
balance within neighborhood regularization function.
-ball suers another issue tends produce many edges points
located high-density regions, thus little restriction maximum degree
vertex. k-NN suer problem one commonly
used types graphs.
subsequent development experiments, experiment -ball
k-NN graph may variance performance dierent graph
construction techniques dierent datasets (Hein, Audibert, & Luxburg, 2007; Ting,
Huang, & Jordan, 2010; Coifman & Lafon, 2006).
2. Minimum Spanning Tree-based Graphs. -ball k-NN quite sensitive
noise sparsity, graph construction based combining multiple minimum
spanning trees help reduce sensitivity noise output graph (Zemel
& Carreira-Perpinan, 2004). two variations based approach.
(a) Perturbed Minimum Spanning Trees (PMST): PMST builds neighborhood
graph generating > 1 perturbed copies whole dataset according
local noise model tting MST perturbed copy. weight
eij [0, 1] assigned edge points xi xj equal
average number times edge appears trees.
(b) Disjoint Minimum Spanning Trees (DMST): DMST produces neighborhood
graph nding deterministic collection r minimum spanning trees
satises property tree collection uses edge trees.
neighborhood graph union edges trees contains r(N 1)
edges.
representative category, use DMST, deterministic easier
construct PMST showing similar ecacies.
4.2.2 Graph Weighting
next issue assign weights edges neighborhood graph.
respect, consider two variations edge weights.
1. Simple Minded :

ij =

1,
0,

di dj connected,
otherwise.

(9)

simplest approach use binary weighting assign weights
edges. However, approach assign uniform weights edges
sensitive errors, cli eect 1 immediately 0. Moreover,
since weights smoothed, could result loss information.
hypothesize among connected nodes, may still dierences
terms degrees similarity, expressed mutual distances.
motivates second approach below.
1105

fiLe & Lauw

2. Heat Kernel :

ij =

exp(
0,

||di dj ||2
),


di dj connected,
otherwise.

(10)

alternative approach using Heat Kernel function (Belkin & Niyogi, 2001;
Jebara, Wang, & Chang, 2009). Heat Kernel advantage Simple Minded
allowing smoother weights edges, helps address issues sensitivity
loss information. However, Simple Minded parameterized, Heat
Kernel one parameter needs determined (i.e., ). Note = ,
Heat Kernel degenerates Simple Minded, i.e., former general
formulation. exact value important model would
eectively absorbed regularization parameter. simplicity, set = 2.

5. Model Fitting
discuss parameters model described Sections 3 4
learned. One well-accepted framework learn model parameters using maximum posteriori (MAP) estimation Expectation-Maximization EM algorithm (Dempster,
Laird, & Rubin, 1977).
model, regularized conditional expectation complete-data log likelihood MAP estimation priors is:
Q(|) =
+

Mn
N
Z




P(z|n, m, ) log P(z|xn , )P(wnm |z )

n=1 m=1 z=1
N


Z


n=1

z=1

log(P(xn )) +

log(P(z )) +

Z


log(P(z ))

z=1

+ R(|),
current estimate. P(z|n, m, ) class posterior probability nth
document mth word current estimate. P(z ) symmetric Dirichlet prior
parameter word probability z . P(xn ) P(z ) Gaussian priors zero
mean spherical covariance document coordinates xn topic coordinates z .
set hyper-parameters = 0.01, = 0.1N = 0.1Z following PLSV (Iwata
et al., 2008).
E-step, P(z|n, m, ) updated follows:
P(z|n, m, ) = Z

P(z|xn , )P(wnm |z )

z =1 P(z

|x

n , )P(wnm |z )

.

M-step, maximizing Q(|) w.r.t zw , next estimate word probability
zw follows:
N Mn
m=1 I(wnm = w)P(z|n, m, ) +
n=1
zw = W
,
N Mn

m=1 I(wnm = w )P(z|n, m, ) + W
w =1
n=1
1106

fiSemantic Visualization Neighborhood Graph Regularization

I(.) indicator function. z xn cannot solved closed form,
estimated maximizing Q(|) using quasi-Newton (Liu & Nocedal, 1989).
computation fo gradients Q(|) w.r.t z xn depend specic
kernel used (see Section 3.3).
Gaussian kernel, following gradients:
n


Q(|)
=
P(z|xn , ) P(z|n, m, ) (z xn ) z ,
z

N

Q(|)
=
xn



n=1 m=1
Mn
Z




m=1 z=1


R(|)
.
P(z|xn , ) P(z|n, m, ) (xn z ) xn +
xn

Student-t kernel, following gradients:

N Mn
2 P(z|xn , ) P(z|n, m, ) (z xn )
Q(|)
=
z ,
z
1 + ||xn z ||2
n=1 m=1


Mn
Z

2 P(z|xn , ) P(z|n, m, ) (xn z )
Q(|)
R(|)
=
xn +
.
2
xn
1 + ||xn z ||
xn
m=1 z=1

gradient R(|) w.r.t. xn computed depending form regularization function R(|). use proposed regularization function R (|)
described Section 4.1.1, following gradient:
R (|)
R(|)
=
xn
xn



(xn xj )
1
=
4nj (xn xj )
4(1 nj )
.
2
2
(F(n , j ) + 1)
j=1;j=n

j=1;j=n

mentioned earlier, eciency advantage regularizing visualization space. R(|) contain variable z regularization visualization
O(N 2 ). contrast, regularizaspace. complexity computing R(|)
xn
tion topic space, take gradient R(|) w.r.t z . contributes
. Therefore, regularizatowards greater complexity O(Z 2 N 2 ) compute R(|)
z
tion topic space would run much slower visualization space.

6. Experiments
main objective experiments evaluate eectiveness neighborhood regularization semantic visualization model. describing experimental setup,
rst examine dierent design choices model relating kernel, graph construction, regularization function. Thereafter, compare Semafore baseline
methods aim address visualization topic modeling, quantitatively
qualitatively, rst terms visualization terms topic modeling.
1107

fiLe & Lauw

6.1 Experimental Setup
section, give description benchmark datasets well suitable metrics
used evaluation.
6.1.1 Datasets
use three real-life, publicly available datasets (Cardoso-Cachopo, 2007) evaluation.
20N ews contains newsgroup articles (in English) 20 classes.
Reuters8 contains newswire articles (in English) 8 classes.
Cade12 contains web pages (in Brazilian Portuguese) classied 12 classes.
benchmark datasets used document classication. task fully
unsupervised, ground-truth class labels useful objective evaluation.
create balanced classes sampling fty documents class, following practice
PLSV (Iwata et al., 2008). results in, one sample, 1000 documents 20N ews,
400 Reuters8, 600 Cade12. vocabulary sizes 5.4K 20N ews, 1.9K
Reuters8, 7.6K Cade12. algorithms probabilistic, generate samples
dataset. sample, conduct independent runs. Therefore, result
reported setting average total 25 runs.
6.1.2 Metrics
suitable metric, return fundamental principle good visualization
preserve relationship documents (in high-dimensional space)
lower-dimensional visualization space. User studies, even well-designed, could
overly subjective may repeatable across dierent users reliably. Therefore,
objective evaluation, rely two types quantitative analysis:
Classication: evaluation relies ground-truth class labels found
datasets. well-established practice many clustering visualization
works machine learning. basis evaluation reasonable assumption
documents class related documents dierent classes.
Therefore good visualization would place documents class neighbors
visualization.
document dn , hide true class cn , generate prediction
class Ct (n) taking majority class among t-nearest neighbors, determined Euclidean distance visualization space. Classication accuracy
Classif ication Acc(t) dened fraction documents whose predicted class
Ct (n) matches true class cn . specically, have:
N
1
(Ct (n) = cn ),
Classif ication Acc(t) =
N
n=1

1108

fiSemantic Visualization Neighborhood Graph Regularization

delta function equals 1 prediction matches 0 otherwise.
metric used PLSV (Iwata et al., 2008). accuracy computed
based documents coordinates, trends produced computed based
topic distributions (due coupling kernels described Section 3.3).
Neighborhood Preservation: evaluation rely ground-truth class
labels local neighborhood structure input data. assumption
good visualization would able preserve local structure input
data much possible. two documents neighbors input data,
still neighbors visualization space.
every document dn , compute sets t-nearest neighbors Yt (n) Xt (n)
document dn input data visualization respectively. neighborhood
preservation accuracy P reservation Acc(t) dened average fraction
overlap size Yt (n) Xt (n) size Yt (n) (i.e. t), n = 1, . . . , N .
specically, have:

P reservation Acc(t) =

N
1 |Yt (n) Xt (n)|
,
N

n=1

|Yt (n) Xt (n)| size overlap set Yt (n) Xt (n).
similar measure found literature (Akkucuk & Carroll, 2006),
called rate agreement local structure agreement rate used
measure well local structure preserved input data
low dimensional embedding. used tuning parameters non-linear
dimensionality reduction method (Chen & Buja, 2009).
subsequent experiments, let vary range [5, 50] step size
5 report accuracies. Since dierent methods may behave dierently dierent
ts, choosing specic comparison may unfair methods. Moreover,
method consistently well dierent ts would smoother local
structure. Therefore, comparing various methods, present preservation
classication accuracies averaged across [5, 50], denoted P reservation Acc(Avg)
Classif ication Acc(Avg) respectively.
6.2 Parameter Study
section, study eects graph parameters model. Specically,
parameters concern graph construction, including number neighbors k k-NN
graph, distance threshold -ball graph, number minimum spanning trees
r DMST. type graph, use Simple Minded weight. following
gures, regularization function R = 10 number topics Z = 20.
use neighborhood preservation accuracy P reservation Acc(t) show eects
graph parameters metric need ground-truth class labels,
always available tuning graph parameters.
1109

fiLe & Lauw




















































W

W



W



E















Z















Figure 4: Preservation accuracy Semafore using k-NN graph dierent neighborhood size k (a) 20N ews, (b) Reuters8, (c) Cade12.











































W

W

W













E















Z















Figure 5: Preservation accuracy Semafore using DMST graph dierent number minimum spanning trees r (a) 20N ews, (b) Reuters8, (c) Cade12.





































E











W

W



W



















Z





















Figure 6: Preservation accuracy Semafore using -ball graph dierent values
distance threshold (a) 20N ews, (b) Reuters8, (c) Cade12.

1110

fiSemantic Visualization Neighborhood Graph Regularization

Figure 4, show performance model dierent neighborhood size k
k-NN graph dierent datasets. every k, vary plot P reservation Acc(t).
Figure 4 shows optimum k 20N ews, Reuters8, Cade12 10, 10, 5
respectively. compute average accuracy P reservation Acc(Avg) conrms
optima indeed k values. on, use k=10 20N ews
Reuters8, k=5 Cade12 k-NN graph used.
DMST graph, plot P reservation Acc(t) dierent number minimum
spanning trees r dierent datasets Figure 5. dicult see r best
gure dierences much. P reservation Acc(Avg)
computed shows three datasets, optimum r=5,6,7.
Subsequently, use r=6 DMST graphs three datasets.
-ball graph, Figure 6 plot P reservation Acc(t) dierent values
range [1.32, 1.40]. choose range =1.32 =1.40 roughly give
average number neighbors 5 100 respectively. P reservation Acc(Avg) shows
optimum 20N ews, Reuters8, Cade12 1.34, 1.35, 1.33 respectively.
6.3 Model Analysis
section, study various design choices involved designing Semafore
model, nally concluding eventual synthesis design choices used
comparison baselines. keep discussion focused organized,
following sub-section, vary single design choice, order isolate eects.
unvaried, model following setup default: number topics Z = 20,
graph construction method k-NN, graph weighting method simple minded,
RBF kernel Gaussian, regularization function R = 10.
6.3.1 Neighborhood Graph Construction
investigate three graph construction methods: k-NN, -ball DMST, representatives neighborhood-based minimum spanning tree-based methods respectively.
graph, parameter tuned shown Section 6.2. regularization
parameter , try dierent settings dataset. happens = 10
performs best graph construction methods across three datasets.
Figure 7, run Semafore dierent types graph three datasets
report P reservation Acc(Avg) dierent number topics Z. results show
dierent types graph behave dierently dierent datasets. 20N ews, -ball
DMST give model highest performance. Since dierence two
statistically signicant, choose use DMST subsequent experiments 20N ews.
Reuters8, since -ball outperforms others (signicant 0.05 level), going
default choice subsequent experiments. Cade12, choice DMST,
slightly better k-NN (statistically signicant Z = 10, 40, 50).
6.3.2 Neighborhood Graph Weighting
compare two variations graph weighting methods, namely: Simple Minded
Heat Kernel methods. experiment, use k-NN graph specic ks dierent
1111

fiLe & Lauw







D^d





W



W

W

EE











Ed
E











Ed



Z





Ed


Figure 7: eects dierent graph construction methods models performance.







,<t



W



W

W

^Dt











Ed
E












Ed
Z





Ed


Figure 8: eects dierent graph weighting schemes models performance.
graph used experiment k-NN graph specic ks dierent
datasets studied Section 6.2.











Ed
E

^<



W



W

W

'<











Ed
Z












Ed


Figure 9: eects Gaussian Student-t RBF kernels models performance.
1112

fiSemantic Visualization Neighborhood Graph Regularization

Regularization function
Graph construction
Graph weighting
RBF kernel

20N ews
R
DMST
Heat Kernel
Student-t

Reuters8
R
-ball
Heat Kernel
Student-t

Cade12
R
DMST
Simple Minded
Student-t

Table 2: Synthesized Model Dataset.
datasets studied Section 6.2. regularization parameter set 10 trying
various settings picking best one.
Figure 8, compare Simple Minded method Heat Kernel method see
inuences model dierent number topics Z. observe Heat Kernel
signicantly consistently better Simple Minded method across cases
20N ews Reuters8. dierence statistically signicant 0.01 level. One
explanation Heat Kernel assigns smoother weights graph edges, thus
robust Simple Minded. Cade12, Simple Minded slightly better, though
dierences statistically signicant 0.05 level Z = 40. Subsequently,
use Heat Kernel 20N ews Reuters8, Simple Minded Cade12 part
nal synthesis.
6.3.3 RBF Kernel
described Section 3.3, express topic distributions function visualization
coordinates using RBF network abstraction. section, show dierent
RBF kernels aect models performance. two kernels exploring Gaussian
(Equation 3) Student-t (Equation 4). tune regularization term kernel
see best one two kernels = 10.
Figure 9 shows results dierent number topics Z. Student-t kernel slight
edge Gaussian kernel consistently across dierent number topics. dierence
small, statistically signicant (at 0.05 level) majority cases (for 20N ews
Z = 10, 20, 30, 50, Reuters8 Z = 30, Cade12 Z = 10, 30, 50). slight
improvement could sign crowding problem exist model. Student-t
kernel would even useful extreme crowding issues,
number documents visualized even larger. Subsequently, due
slight edge, use Student-t part nal synthesis. see shortly, using
Student-t within synthesized model results signicant improvement overall.
6.3.4 Synthesised Semafore Model
Based model analysis preceding paragraphs, combine design choices
nal synthesis model called Semafore. synthesized model slightly dierent
dierent datasets, listed Table 2.
conduct another set experiments verify synthesized models
would produce noticeable improvement earlier version (kNN + Simple Minded
+ Gaussian Kernel) appeared earlier work (Le & Lauw, 2014b), underlining
1113

fiLe & Lauw

EE^D'<
,<^<













Ed










E





Ed
Z

EE^D'<
D^d^D^<

W


W


W


EE^D'<
D^d,<^<













Ed


Figure 10: synthesized models dierent properties compared earlier version
(kNN + Simple Minded + Gaussian Kernel) appeared earlier work
(Le & Lauw, 2014b).

utility subsequent enhancements. Figure 10 shows indeed case.
Based standard deviations shown gures, improvements clear
20N ews Reuters8 clear Cade12. Paired samples t-test indicate
improvement signicant 0.05 level lower cases, except cases
Z = 10, 20 Cade12. use synthesized models comparisons
baseline methods following section.
6.4 Comparison Visualizations
compare proposed model several baselines. First, outline set
comparative methods. Thereafter, discuss quantitative evaluation (in terms accuracy),
well qualitative evaluation (in terms example visualizations). Finally, show
gains visualization quality come expense topic modeling.
semantic visualization seeks ensure consistency topic model visualization, comparison focuses methods producing topics visualization coordinates
listed Table 3.
Semafore proposed method incorporates neighborhood structure
semantic visualization.
PLSV (Iwata et al., 2008) state-of-the-art, representing joint approach
without neighborhood structure preservation.
PE (LDA) represents pipeline approach involving topic modeling LDA (Blei
et al., 2003), followed visualizing documents topic distributions PE (Iwata
et al., 2007). pipeline better LDA/MDS appeared earlier
work (Le & Lauw, 2014b). pipeline methods, shown inferior PLSV
(Iwata et al., 2008), reproduced avoid duplication.
1114

fiSemantic Visualization Neighborhood Graph Regularization

Visualization

Topic model

Joint model

Neighborhood

Semafore
PLSV
PE (LDA)
t-SNE (LDA)
Table 3: Comparative Methods.
t-SNE (LDA) another pipeline approach rst uses LDA (Blei et al., 2003)
learn topic model use t-SNE (Van der Maaten & Hinton, 2008) visualize
documents topic distributions.
completeness, conduct experiments comparing method t-SNE
Laplacian EigenMaps (LE) (Belkin & Niyogi, 2003) (direct visualization, without topic
modeling). keep discussion focused, show Appendix B,
consider t-SNE LE comparative baselines two methods model
visualization, topics.
6.4.1 Accuracy
section, compare model several baselines terms classication
accuracy (Figure 11) neighborhood preservation accuracy (Figure 12). two
gures, standard deviations Semafore shown.
Classcation Accuracy. Figure 11(a), 11(c) 11(e) show Classf ication Acc(t)
dierent ts Z = 20 20N ews, Reuters8, Cade12 respectively. t,
comparison shows outperformance Semafore baselines consistently. four
methods show behavior performances decrease increases.
increases, may lose accuracy predicting labels documents near border
cluster.
Now, vary number topics Z. Figure 11(b), show performance
Classf ication Acc(Avg) 20N ews. Figure 11(d) 11(f) show Reuters8
Cade12 respectively. gures, draw following observations
comparative methods:
Semafore performs best datasets across various numbers topics (Z).
Semafore beats PLSV 25% 51% 20N ews, 613% Reuters8,
2232% Cade12. margins performance respect PLSV statistically signicant 0.01 signicant level lower cases. eectively showcases
utility neighborhood regularization enhancing quality visualization.
preserving local consistency, Semafore achieves good accuracy even small
number topics (e.g., 10).
PLSV performs better PE (LDA) t-SNE (LDA), shows
utility joint, instead separate, modeling topics visualization.
PE (LDA) t-SNE (LDA) worse PLSV embeds documents
using two-step reductions optimize separately two dierent objective functions.
1115

fiLe & Lauw

Therefore, errors previous step may propagate next, without
opportunity correction. may cause distortions visualization.
cases, PLSV, PE (LDA) t-SNE (LDA) tend decreasing accuracies number topics increases. may number topic
increases, topic distributions word probabilities may overt data
thus accuracy reduced. contrast, Semafore shows quite stable performance across dierent numbers topics. may explained utility
neighborhood regularization, helps prevent overtting number
topics increases.
Neighborhood Preservation Accuracy. better classication accuracy,
Semafore preserves well local structure input data visualization space.
P reservation Acc(t) results Figure 12(a), 12(c) 12(e) show Semafore
consistently better baselines terms neighborhood preservation across
dierent ts dierent datasets. Figure 12(b), 12(d) 12(f), vary number
topics Z report P reservation Acc(Avg) results. Semafore beats PLSV
41% 76% 20N ews, 2436% Reuters8, 2945% Cade12 terms
neighborhood preservation accuracy. improvements Semafore PLSV
statistically signicant 0.01 signicant level lower cases.
accuracy results based visualization coordinates. computed accuracies based topic distributions, similar trends.
6.4.2 Visualizations
provide intuitive appreciation, briey describe qualitative comparison visualizations. method dataset, visualization shown scatterplot (best
seen color). document coordinate, assigned shape color based
class. topic coordinate, drawn black, hollow circle. legend
provided, mapping symbol corresponding class label.
Note illustrative, rather comparative discussion, objective
evaluation rely eyeballing alone. However, shown quantitative
results preceding section, section, focus qualitative study
output visualizations.
20News. Figure 13 shows visualization 20N ews dataset. Semafores Figure 13(a)
shows dierent classes well separated. distinct clusters blue squares
purple diamonds top hockey baseball classes respectively, clusters
orange triangles pink asterisks bottom cryptography medicine, etc.
Beyond individual classes, visualization places related classes nearby. Computerrelated classes found lower left. Politics religion lower right.
Comparatively, Figure 13(b) PLSV shows crowding center. instance,
motorcycle (green dashes) autos (red dashes) mixed center without good
separation. Figure 13(c) PE (LDA) worse. PE (LDA) give good separation
similar classes. mixes autos (red dashes) space (green circles) together
center. Medicine (pink asterisks) mixed classes PE (LDA)
Semafore PLSV give good separation it. Figure 13(d) visualization tSNE (LDA). Although t-SNE (LDA) separate well hockey (blue squares) baseball
1116

fiSemantic Visualization Neighborhood Graph Regularization

W>^s


















W>




^









Es





W>











Zs

Zs







^





W>^s




















^E>




Ed





W>^s






Ed
Es



W>




^

^E>




^E>











Ed


Figure 11: Classication Accuracy Comparison.

1117





fiLe & Lauw

W>^s


















W>
W

W

^









Es

W

W>








Zs

Zs







^





W>^s



















^E>




Ed



W

W>^s











Ed
Es



W>
W

W

^

^E>




^E>










Ed


Figure 12: Preservation Accuracy Comparison.

1118





fiSemantic Visualization Neighborhood Graph Regularization

(purple diamonds) classes, able detect semantic similarities (as baseball
hockey sports). addition, still mixes documents dierent classes
together center upper right.
Reuters8. Figure 14 shows visualization outputs Reuters8 dataset. Semafore
Figure 14(a) better separating eight classes distinct clusters. anticlockwise direction top, navy blue diamonds (money-fx ), red dashes
(interest), red squares (crude), light blue pluses (earn), green triangles (acq), purple crosses
(ship), blue asterisks (grain), nally orange circles (trade).
comparison, PLSV Figure 14(b) shows several classes intermixed
center, including red dashes (interest), orange circles (trade), navy blue diamonds
(money-fx ). PE (LDA) Figure 14(c) worse mixes dierentiated classes
red dashes (interest) navy blue diamonds (money-fx ) together. t-SNE (LDA)
Figure 14(d) seems better cluster separation still mix documents dierent
classes together red squares (crude) green triangles (acq) upper right.
Green triangles (acq) mix light blue pluses (earn) left visualization
t-SNE (LDA).
Cade12. Figure 15 shows visualization outputs Cade12.
challenging dataset. Even so, Semafore Figure 15(a) still achieves better separation
classes, compared PLSV Figure 15(b). Particularly, Semafore gives
better separation esportes (green triangles) well compras-on-line (orange circles)
PLSV PE (LDA). t-SNE (LDA) shows quite good clusters esportes (green
triangles) well compras-on-line (orange circles) merges many dierent
classes together clusters right upper right.
6.5 Comparison Topic Models
One question whether Semafores gain visualization quality closest baseline
PLSV expense quality topic model. investigate this,
compare topic models Semafore PLSV, share core generative process.
parity, comparison, include joint models, whereby visualization
coordinates aect topic models well.
metric use measure quality topic models pairwise mutual information
PMI. measures topic interpretability, based coocurrence frequencies top words
topic large external corpus. Although metrics perplexity heldout likelihood show generalization ability learned topic model unseen test
data, traditional metrics capture whether topics coherent (Chang, Gerrish,
Wang, Boyd-Graber, & Blei, 2009). Therefore, comparison, rely PMI,
measure quality topic words terms interpretability human.
human subjects, interpretability closely related coherence (Newman, Lau, Grieser, &
Baldwin, 2010), i.e., much top keywords topic associated
other. extensive study evaluation methods coherence, Newman et al. (2010)
identify Pointwise Mutual Information (PMI) best measure, terms
greatest correlation human judgments.
PMI based term cooccurrences. pair words wi wj , PMI dened
p(wi ,wj )
log p(wi )p(w
. topic, average pairwise PMIs among top 10 words
j)
1119

fiLe & Lauw

^

W>^s

W>

^E>
>











































Figure 13: Visualization documents 20N ews number topics Z = 20. point
represents document shape color represent document class.
topic drawn black, hollow circle.
1120

fiSemantic Visualization Neighborhood Graph Regularization

^

W>^s

W>

^E>
>



















Figure 14: Visualization documents Reuters8 number topics Z = 20.
point represents document shape color represent document class.
topic drawn black, hollow circle.

1121

fiLe & Lauw

^

W>^s

W>

^E>
>



























Figure 15: Visualization documents Cade12 number topics Z = 20. point
represents document shape color represent document class.
topic drawn black, hollow circle.

1122

fiSemantic Visualization Neighborhood Graph Regularization

W>^s

^






WD/^

WD/^

^






W>^s









Ed








Ed



Z

E

Figure 16: Topic Interpretability Semafore PLSV terms PMI Score (higher
better).

topic. topic model, average PMI across topics. Intuitively, PMI higher
(better), topic features words highly correlated one another.
Key PMI use external corpus estimate p(wi , wj ) p(wi ). Following
Newman et al. (2009), use Google Web 1T 5-gram Version 1 (Brants & Franz, 2006),
huge corpus n-grams generated 1 trillion word tokens. p(wi ) estimated
frequencies 1-grams. recommended Newman et al., p(wi , wj ) estimated
frequencies 5-grams. obtain PMI English-based 20N ews Reuters8,
Cade12 possess large-scale n-gram corpus specically
Brazilian Portuguese.
Figure 16, plot PMI score various number topics Z. Semafore performs
better PLSV across topics settings. Figure 16(a) 20News, except
case Z = 10, cases Semafores outperformance signicant 0.05 level
lower. Figure 16(b) Reuters8, cases Semafores outperformance signicant
0.05 level lower except Z = 30. results show Semafore improves
visualization sacricing topic interpretability learned topics.
greater appreciation quality output topic models, Appendix C,
show several examples topic models Z = 20, Semafore PLSV, terms
top keywords highest probabilities topic.

7. Conclusion
paper, address semantic visualization problem, jointly conducts topic
modeling visualization documents. propose new framework incorporate
neighborhood structure within probabilistic semantic visualization model called Semafore.
model carefully designed reect context semantic visualization, leading
number design choices related RBF kernel mapping topic visualiza1123

fiLe & Lauw

tion spaces, approximation neighborhood graph construction weighting,
well appropriate regularization functions spaces. Experiments real-life
datasets show Semafore signicantly outperforms baselines terms visualization quality accuracy, similar, slightly better topic model.
provides evidence neighborhood structure, together joint modeling topics
visualization, important semantic visualization.

Appendix A. Balancing Contributions Neighbors Non-neighbors
Regularization
mentioned Section 4.2, balance contribution neighbors R+
non-neighbors R neighborhood regularization R Equation 8 may require careful
tuning graph parameters (i.e., k). example, case using k-NN graph
N total number documents, would kN terms neighbor regularization
R+ , (N k)N terms non-neighbor regularization R . Supposing N increases
signicantly, might imbalance k remain unchanged. Therefore, N
changes, k tuned accordingly maintain balance. simplistic
point, ratio kN (N k)N would remain roughly N
k grow similar factors. practice, recommend tuning k carefully.
run additional experiments validate argument 20N ews dataset.
basic point N changes, k tuned still show signicant improvement
due neighborhood graph regularization. closest baseline PLSV, empirically terms classication accuracy, well conceptually PLSV shares similar
generative process dierent kernel without neighborhood regularization.
Hence, compare performance method Semafore (with k-NN graph, heat
kernel weighting, Student-t kernel) PLSV various data sizes Z = 20 topics.
Figure 17(a) dataset size N = 500, Semafore runs k = 10.
Figure 17(b) dataset size N = 1000, Semafore runs k = 10.
Figure 17(c) dataset size N = 5000, Semafore runs k = 50.
note 10X dierence smallest largest datasets.
Yet relative outperformance Semafore PLSV around 15% 20% evident
across three datasets. supports case k tuned produce positive
eect using neighborhood graph regularization.

Appendix B. Additional Comparisons
mentioned Section 6.4, completeness, include additional comparisons
visualization methods aim topic modeling. particular, include two
methods. First, include t-SNE (Van der Maaten & Hinton, 2008), used
composite t-SNE (LDA). Second, include Laplacian EigenMaps (LE) (Belkin &
Niyogi, 2003), takes input neighborhood graph. Figure 18 Figure 19
show classication accuracy preservation accuracy Semafore , t-SNE LE
1124

fiSemantic Visualization Neighborhood Graph Regularization














^



W>^s

^


W>^s




^




























E

E

W>^s











E

Figure 17: Classication accuracy comparison 20N ews various data sizes (Z = 20).

^

^E
















>













































E













Z



Figure 18: Classication accuracy comparison.

^

^E







W


W

W



>
















E
























Z

Figure 19: Preservation accuracy comparison.

1125










fiLe & Lauw

varying t. Semafore outperforms LE cases. t-SNE, Semafore outperforms t-SNE Reuters8. However, 20N ews Cade12, dicult tell
whether Semafore t-SNE better. t-SNE tends decreasing accuracy increases. expected t-SNE known focus preserving local structure
(Van der Maaten & Hinton, 2008). small, basically consider local
structure visualization. increases, consider global structure
visualization Semafore outperforms t-SNE signicantly. Overall, Semafore
stable t-SNE changes, indicates Semafore tries balance preserving
local global structure better t-SNE. emphasize comparison
information purpose only, regard t-SNE LE comparative baselines.

Appendix C. Topic Model Examples
showcase topic models derived Semafore PLSV. 20N ews, Table 4 shows
topics Semafore, Table 5 shows topics PLSV. Reuters8, Table 6
shows topics Semafore, Table 7 shows topics PLSV. Cade12, Table 8
shows topics Semafore, Table 9 shows topics PLSV.
method, show list twenty topics. topic, produce
top ten words highest probabilities. shown top words, topics
correspond strongly classes. example, topic s19 Table 4 20N ews
Christianity, corresponds soc.religion.christian class. Topic s4
cars motorcycles, corresponding rec.autos rec.motorcycles. Topic s12 probably
concerning categories rec.sport.baseball and/or rec.sport.hockey.
Overall, observe quality topic words comparable across comparative methods. Note direct correspondence topics dierent
methods (e.g., rst topic Semafore may correspond rst topic PLSV).
manual inspection, see related topics, e.g., s4 p6 ,
s12 p7 . However, sets topics set keywords topic
identical. borne slight dierence terms PMI scores.
qualitative study helps show Semafore improves visualization quality,
still maintaining least quality topic words, better. supports
conclusion reached quantitative comparisons main manuscript.

1126

fiSemantic Visualization Neighborhood Graph Regularization

Table 4: Semafores Topic Model 20N ews (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
space, system, -rcb-, book, computer, university, list, post, price, science
article, year, good, write, guy, well, time, head, question, leave
gun, law, kuwait, people, death, fbus, article, control, weapon, child
window, le, program, widget, application, type, will, resource, call, function
car, bike, speed, engine, drive, lock, turn, mile, front, change
will, power, place, work, rate, write, sound, lead, good, interested
write, article, thing, time, people, better, start, problem, will, good
write, time, people, friend, pay, public, article, tax, opinion, money
people, claim, write, system, person, moral, evidence, objective, read, state
image, datum, graphic, send, le, format, package, software, mail, include
armenian, re, jew, child, kill, start, people, turkish, door, israel
system, board, will, datum, time, work, tape, test, copy, command
game, team, year, player, win, play, will, hit, season, hockey
will, post, space, good, time, include, cost, option, launch, people
drive, card, window, appear, disk, ram, driver, memory, work, color
mr., president, stephanopoulo, state, group, consider, party, question, issue, press
write, article, well, will, thing, work, point, include, time, help
key, article, chip, food, write, people, government, encryption, thing, algorithm
price, buy, apple, computer, dealer, t, model, problem, sell, monitor
god, jesus, will, christian, religion, faith, truth, bible, belief, church

Table 5: PLSVs Topic Model 20N ews (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
write, people, christian, belief, time, faith, god, religion, life,
god, will, jesus, kuwait, atheist, church, christian, man, religion, sin
armenian, appear, art, turkish, tartar, 1st, village, armenia, 1.40, genocide
will, key, write, time, article, government, system, thing, chip, hit
mr., stephanopoulo, president, will, party, state, door, time, meeting, open
write, re, article, gun, system, -rcb-, start, people, fbus, claim
car, will, bike, engine, drive, well, dealer, battery, change, front
game, win, year, will, team, play, season, good, goal, playo
player, team, write, hockey, game, fan, article, year, will, guy
space, system, datum, will, april, nasa, security, university, computer, list
graphic, image, le, ftp, send, format, package, system, datum, object
image, datum, program, window, version, le, software, tool, support, user
drive, jumper, master, ndet loop, slave, rate, gun, function, crime, set
window, le, card, will, program, color, driver, support, disk, bit
people, write, state, article, law, government, country, rights, jew,
write, article, thing, people, good, will, time, lot, year, day
work, drive, tape, scsus, problem, simm, controller, write, memory, article
widget, -rcb-, window, -lcb-, application, resource, set, visual, type, le
price, will, write, system, computer, article, apple, chip, monitor, board
will, vote, comp, newsgroup, suit, problem, os2, sco, post, mail

1127

fiLe & Lauw

Table 6: Semafores Topic Model Reuters8 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
company, pipeline, raise, crude, march, spokesman, renery, capacity, corp, post
pct, bank, day, stg, today, reuter, money, market, mln, bill
oer, share, company, board, group, acquire, stock, dlr, acquisition, receive
exchange, currency, dollar, west, nance, baker, monetary, germany, continue, interest
share, reuter, dlr, mln, buy, company, corp, pay, stock, group
price, opec, market, bpd, ocial, february, month, output, saudus, january
rate, bank, pct, cut, fund, prime, point, reserve, issue, lower
billion, foreign, import, increase, dlr, trade, economic, export, will, country
bank, billion, market, government, fall, stock, economy, rise, surplus, decit
will, company, sell, pct, vessel, operation, week, billion, shipping, unit
strike, port, union, spokesman, cargo, employer, worker, sector, redundancy, court
oil, export, dlr, industry, year, pct, future, company, report, price
reuter, pct, report, national, week, brazil, today, increase, pay, april
trade, japan, japanese, reagan, state, tari, unite, market, washington, ocial
grain, mln, soviet, crop, tonne, year, usda, production, fall, analyst
trade, talk, gulf, gatt, bill, yeutter, round, reuter, call, negotiation
certicate, reuter, cost, government, program, agreement, agriculture, will, study, loan
year, ocial, import, will, state, price, government, china, land, rise
mln, ct, loss, net, shr, dlr, prot, qtr, reuter, year
oil, mln, will, barrel, dlr, crude, source, level, petroleum, day

Table 7: PLSVs Topic Model Reuters8 (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
will, oil, company, reuter, industry, canada, price, shell, raise, sell
rate, currency, dollar, exchange, baker, west, will, bank, reuter, treasury
bank, pct, day, import, year, rate, export, february, expect, reuter
share, company, corp, oer, stock, board, will, reuter, dlr, buy
rate, bank, pct, prime, cut, point, interest, market, lower, savings
market, bank, stock, price, japan, ministry, rise, ocial, gulf, bond
reuter, pct, week, report, year, march, mark, american, commission, gure
mln, ct, loss, net, dlr, shr, year, prot, qtr, reuter
mln, pct, billion, stg, dlr, reuter, market, january, revise, rise
billion, dlr, rate, market, surplus, currency, reserve, trading, dollar, foreign
oil, opec, price, bpd, pipeline, mln, crude, ocial, dlr, output
crude, dlr, barrel, corp, capacity, renery, oil, company, oer, group
reuter, ocial, state, cut, gulf, government, today, action, force, tell
oil, government, indonesium, price, foreign, bank, billion, reserve, company, industry
certicate, company, mln, year, grain, cooperative, program, dlr, government, cost
year, trade, agriculture, reuter, grain, agreement, gatt, yeutter, nancial, agricultural
strike, port, union, spokesman, employer, brazil, cargo, worker, redundancy, sector
trade, japan, japanese, reagan, tari, unite, washington, state, nakasone, semiconductor
grain, mln, crop, tonne, soviet, year, ocial, china, pct, oer
trade, country, minister, talk, state, meeting, economic, exchange, issue, baldrige

1128

fiSemantic Visualization Neighborhood Graph Regularization

Table 8: Semafores Topic Model Cade12 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
sp, aulas, tecnologia, rj, sao, area, janeiro, particulares, areas, sica
terra, jun, gif, busca, virtual, brasil, forum, tempo, noticias, revistas
trabalho, seguranca, saude, medicina, ocupacional, prevencao, ppra, pcmso, imagem, imagens
peixes, cade, lazer, pesca, agua, rio, praia, hotel, sao, doce
agar, vida, personal, sica, base, tratamento, tem, pode, sistema, trainer
sao, br, rio, sul, criancas, www, escola, mail, http, atendimento
links, page, home, fotos, pagina, dicas, download, tenis, informacoes, jogos
internet, informatica, acesso, mg, br, servicos, provedor, mail, revista, horizonte
servicos, sao, paulo, entregas, entrega, sp, cesta, express, empresa, servico
pesca, sp, grupo, brasil, eventos, video, mg, informacoes, turismo, danca
astronomia, pagina, jose, foi, bem, espaco, tem, veja, losoa, correio
mp, banda, musicas, rock, musica, page, letras, bandas, pagina, site
historia, cultura, mundo, site, page, brasil, informacoes, rs, livro, arte
noticias, jornal, cidade, sp, sao, regiao, demolay, ordem, rio, capitulo
empresas, informacoes, informacao, dados, atraves, textos, mail, equipe, unicamp, centro
engenharia, servicos, projetos, empresa, consultoria, quimica, instituto, pesquisa, rio, manutencao
site, informacoes, brasil, associacao, educacao, pagina, organizacao, centro, brasileira, direitos
software, web, empresa, sistemas, sistema, br, marketing, desenvolvimento, windows, dados
virtual, online, venda, produtos, cade, shopping, internet, loja, compras, cursos
futebol, informacoes, fotos, clube, historia, paulo, sao, quake, pagina, cade

Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
engenharia, projetos, servicos, trabalho, empresa, consultoria, seguranca, sp, medicina, sao
sao, ong, rio, instituto, personal, educacao, organizacao, sp, paulo, ns
sao, br, desenvolvimento, sistema, tratamento, mail, sistemas, clientes, informacoes, empresa
aulas, formula, quimica, particulares, informacoes, matematica, pilotos, fotos, sica, site
jornal, tenis, noticias, esportes, sp, informacoes, sao, esporte, fotos, links
musica, page, rock, bandas, links, home, pagina, musicas, music, fotos
pesca, demolay, sp, peixes, sao, fotos, ordem, capitulo, paulo, jitsu
mp, musicas, nacionais, agar, internacionais, rock, formato, site, page, pagina
pesquisa, tecnologia, informacoes, cade, ciencia, geograa, pesquisas, area, instituto, pagina
site, pagina, internet, mail, clique, veja, br, pode, foi, links
astronomia, informacoes, cultura, site, pagina, brasil, home, page, fotos, historia
banda, fotos, rock, letras, page, musicas, pagina, site, home, mp
internet, provedor, acesso, mg, informatica, software, servicos, belo, horizonte, manutencao
futebol, clube, sao, paulo, campeonato, historia, informacoes, pagina, turismo, tricolor
noticias, terra, internet, brasil, informatica, online, jornal, virtual, servicos, busca
links, page, quake, home, pagina, fotos, dicas, mp, download, informacoes
grupo, banda, karate, pagina, page, informacoes, fotos, home, rio, historia
produtos, virtual, shopping, cade, venda, online, sao, rio, loja, compras
br, sao, informacoes, marketing, mail, empresa, internet, www, fax, site
vida, dia, sao, foi, terra, panico, jose, tem, planetas, grande

Table 9: PLSVs Topic Model Cade12 (for 20 topics)

1129

fiLe & Lauw

References
Akkucuk, U., & Carroll, J. D. (2006). PARAMAP vs. Isomap: comparison two nonlinear
mapping algorithms. Journal Classication, 23 (2), 221254.
Bai, L., Guo, J., Lan, Y., & Cheng, X. (2014). Local Linear Matrix Factorization
Document Modeling. Advances Information Retrieval, pp. 398411. Springer.
Belkin, M., & Niyogi, P. (2001). Laplacian eigenmaps spectral techniques embedding clustering. Advances Neural Information Processing Systems (NIPS),
Vol. 14, pp. 585591.
Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps dimensionality reduction data
representation. Neural Computation, 15 (6), 13731396.
Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold regularization: geometric framework learning labeled unlabeled examples. Journal Machine Learning
Research (JMLR), 7, 23992434.
Bishop, C. M. (1995). Neural Networks Pattern Recognition. Oxford University Press.
Bishop, C. M., Svensen, M., & Williams, C. K. (1998). GTM: generative topographic
mapping. Neural Computation, 10 (1), 215234.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal
Machine Learning Research (JMLR), 3, 9931022.
Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,
Philadelphia.
Buhmann, M. D. (2000). Radial basis functions. Acta Numerica 2000, 9.
Cai, D., Mei, Q., Han, J., & Zhai, C. (2008). Modeling hidden topics document manifold.
Proceedings ACM Conference Information Knowledge Management
(CIKM).
Cai, D., Wang, X., & He, X. (2009). Probabilistic dyadic data analysis local global
consistency. Proceedings International Conference Machine Learning
(ICML).
Cardoso-Cachopo, A. (2007). Improving Methods Single-label Text Categorization. PhD
Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa.
Carey, C., & Mahadevan, S. (2014). Manifold Spanning Graphs. Twenty-Eighth AAAI
Conference Articial Intelligence.
Chaney, A. J.-B., & Blei, D. M. (2012). Visualizing Topic Models. Proceedings
International AAAI Conference Web Social Media (ICWSM).
Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J. L., & Blei, D. M. (2009). Reading
tea leaves: humans interpret topic models. Advances Neural Information
Processing Systems, pp. 288296.
Chen, L., & Buja, A. (2009). Local multidimensional scaling nonlinear dimension reduction, graph drawing, proximity analysis. Journal American Statistical
Association, 104 (485), 209219.
1130

fiSemantic Visualization Neighborhood Graph Regularization

Chi, E. H.-h. (2000). taxonomy visualization techniques using data state reference model. Proceedings IEEE Symposium Information Visualization
(InfoVis), pp. 6975.
Choo, J., Lee, C., Reddy, C. K., & Park, H. (2013). UTOPIAN: User-driven topic modeling based interactive nonnegative matrix factorization. IEEE Transactions
Visualization Computer Graphics, 19 (12), 19922001.
Chuang, J., Manning, C. D., & Heer, J. (2012). Termite: visualization techniques assessing textual topic models. Proceedings International Working Conference
Advanced Visual Interfaces (AVI), pp. 7477.
Coifman, R. R., & Lafon, S. (2006). Diusion maps. Applied Computational Harmonic
Analysis, 21 (1), 5 30.
Comon, P. (1994). Independent component analysis, new concept?. Signal Processing,
36 (3), 287314.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete
data via EM algorithm. Journal Royal Statistical Society, Series B, 39 (1),
138.
Dumais, S., Furnas, G., Landauer, T., Deerwester, S., Deerwester, S., et al. (1995). Latent
semantic indexing. Proceedings Text Retrieval Conference.
Fisher, R. A. (1936). use multiple measurements taxonomic problems. Annals
Eugenics, 7 (2), 179188.
Gabrilovich, E., & Markovitch, S. (2009). Wikipedia-based semantic interpretation
natural language processing. Journal Articial Intelligence Research (JAIR), 34 (2),
443.
Golub, G. H., & Van Loan, C. F. (2012). Matrix Computations, Vol. 3. JHU Press.
Gretarsson, B., Odonovan, J., Bostandjiev, S., Hollerer, T., Asuncion, A., Newman, D., &
Smyth, P. (2012). TopicNets: Visual analysis large text corpora topic modeling.
ACM Transactions Intelligent Systems Technology (TIST), 3 (2), 23.
Hein, M., Audibert, J.-y., & Luxburg, U. V. (2007). Graph Laplacians Convergence
Random Neighborhood Graphs. Journal Machine Learning Research, pp.
13251368.
Hinton, G. E., & Roweis, S. T. (2002). Stochastic neighbor embedding. Advances
Neural Information Processing Systems (NIPS), pp. 833840.
Hofmann, T. (1999). Probabilistic latent semantic indexing. Proceedings International ACM SIGIR Conference Research Development Information
Retrieval (SIGIR), pp. 5057.
Hu, Y., Boyd-Graber, J., Satino, B., & Smith, A. (2014). Interactive topic modeling.
Machine Learning, 95 (3), 423469.
Huh, S., & Fienberg, S. E. (2012). Discriminative topic modeling based manifold learning.
ACM Transactions Knowledge Discovery Data (TKDD), 5 (4), 20.
1131

fiLe & Lauw

Iwata, T., Saito, K., Ueda, N., Stromsten, S., Griths, T. L., & Tenenbaum, J. B. (2007).
Parametric embedding class visualization. Neural Computation, 19 (9), 25362556.
Iwata, T., Yamada, T., & Ueda, N. (2008). Probabilistic latent semantic visualization: topic
model visualizing documents. Proceedings ACM SIGKDD International
Conference Knowledge Discovery Data Mining (KDD), pp. 363371.
Jebara, T., Wang, J., & Chang, S.-F. (2009). Graph construction b-matching semisupervised learning. Proceedings 26th Annual International Conference
Machine Learning, pp. 441448. ACM.
Jollie, I. (2005). Principal Component Analysis. Wiley Online Library.
Kim, M., & Torre, F. (2010). Local minima embedding. Proceedings International
Conference Machine Learning (ICML), pp. 527534.
Kohonen, T. (1990). self-organizing map. Proceedings IEEE, 78 (9), 14641480.
Kruskal, J. B. (1964). Multidimensional scaling optimizing goodness nonmetric
hypothesis. Psychometrika, 29 (1), 127.
Laerty, J. D., & Wasserman, L. (2007). Statistical Analysis Semi-Supervised Regression.
Advances Neural Information Processing Systems (NIPS), pp. 801808.
Le, T., & Lauw, H. W. (2014a). Semantic visualization spherical representation.
Proceedings ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 10071016. ACM.
Le, T. M., & Lauw, H. W. (2014b). Manifold learning jointly modeling topic
visualization. Proceedings AAAI Conference Articial Intelligence.
Liu, D. C., & Nocedal, J. (1989). limited memory BFGS method large scale
optimization. Mathematical Programming, 45, 503528.
Manning, C. D., Raghavan, P., Schutze, H., et al. (2008). Introduction Information
Retrieval, Vol. 1. Cambridge University Press Cambridge.
McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic role discovery
social networks experiments enron academic email.. Journal Articial
Intelligence Research (JAIR), 30, 249272.
Millar, J. R., Peterson, G. L., & Mendenhall, M. J. (2009). Document Clustering
Visualization Latent Dirichlet Allocation Self-Organizing Maps. FLAIRS
Conference, Vol. 21, pp. 6974.
Newman, D., Karimi, S., & Cavedon, L. (2009). External evaluation topic models.
Australasian Document Computing Symposium (ADCS).
Newman, D., Lau, J. H., Grieser, K., & Baldwin, T. (2010). Automatic evaluation topic
coherence. Human Language Technologies: 2010 Annual Conference
North American Chapter Association Computational Linguistics, pp. 100
108.
Park, J., & Sandberg, I. W. (1991). Universal approximation using radial-basis-function
networks. Neural Computation, 3 (2), 246257.
1132

fiSemantic Visualization Neighborhood Graph Regularization

Ponzetto, S. P., & Strube, M. (2007). Knowledge derived Wikipedia computing
semantic relatedness.. Journal Articial Intelligence Research (JAIR), 30, 181212.
Reisinger, J., Waters, A., Silverthorn, B., & Mooney, R. J. (2010). Spherical topic models.
Proceedings International Conference Machine Learning (ICML), pp.
903910.
Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction locally linear
embedding. Science, 290 (5500), 23232326.
Shaw, B., & Jebara, T. (2007). Minimum volume embedding. Proceedings International Conference Articial Intelligence Statistics (AISTATS), pp. 460467.
Shaw, B., & Jebara, T. (2009). Structure preserving embedding. Proceedings
International Conference Machine Learning (ICML), pp. 937944. ACM.
Tenenbaum, J. B., De Silva, V., & Langford, J. C. (2000). global geometric framework
nonlinear dimensionality reduction. Science, 290 (5500), 23192323.
Ting, D., Huang, L., & Jordan, M. I. (2010). Analysis Convergence Graph Laplacians. Proceedings International Conference Machine Learning (ICML).
Turney, P. D., Pantel, P., et al. (2010). frequency meaning: Vector space models
semantics. Journal Articial Intelligence Research (JAIR), 37 (1), 141188.
Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine
Learning Research (JMLR), 9 (2579-2605), 85.
Wei, F., Liu, S., Song, Y., Pan, S., Zhou, M. X., Qian, W., Shi, L., Tan, L., & Zhang,
Q. (2010). Tiara: visual exploratory text analytic system. Proceedings
ACM SIGKDD International Conference Knowledge Discovery Data Mining
(KDD), pp. 153162.
Wu, H., Bu, J., Chen, C., Zhu, J., Zhang, L., Liu, H., Wang, C., & Cai, D. (2012). Locally
discriminative topic modeling. Pattern Recognition, 45 (1), 617625.
Zemel, R. S., & Carreira-Perpinan, M. A. (2004). Proximity graphs clustering
manifold learning. Advances Neural Information Processing Systems (NIPS),
pp. 225232.
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Scholkopf, B. (2004). Learning local
global consistency. Advances Neural Information Processing Systems (NIPS),
16 (16).
Zhu, X., Ghahramani, Z., Laerty, J., et al. (2003). Semi-supervised learning using Gaussian
elds harmonic functions. Proceedings International Conference
Machine Learning (ICML), Vol. 3, pp. 912919.

1133


