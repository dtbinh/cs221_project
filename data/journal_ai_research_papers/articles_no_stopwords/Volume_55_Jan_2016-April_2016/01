journal artificial intelligence

submitted published

exploiting causality selective belief filtering
dynamic bayesian networks
stefano v albrecht

svalb cs utexas edu

department computer science
university texas austin
austin tx usa

subramanian ramamoorthy

ramamoorthy ed ac uk

school informatics
university edinburgh
edinburgh eh ab uk

abstract
dynamic bayesian networks dbns general model stochastic processes
partially observed states belief filtering dbns task inferring belief state e
probability distribution process states incomplete noisy observations
hard complex processes large state spaces article
explore idea accelerating filtering task automatically exploiting causality
process consider specific type causal relation called passivity pertains
state variables cause changes variables present passivity
selective belief filtering psbf method maintains factored belief representation
exploits passivity perform selective updates belief factors psbf produces
exact belief states certain assumptions approximate belief states otherwise
approximation error bounded degree uncertainty process
empirically synthetic processes varying sizes degrees passivity psbf
faster several alternative methods achieving competitive accuracy furthermore
demonstrate passivity occurs naturally complex system multi robot
warehouse psbf exploit accelerate filtering task

introduction
dynamic bayesian networks dbns dean kanazawa general model
stochastic processes partially observed states topology dbn compact
specification variables process interact transitions cf figure given
possible incompleteness noise observations may generally possible
infer state process absolute certainty instead may infer beliefs
process state history observations form probability distribution
state space process often called belief state task calculating
belief states commonly referred belief filtering
number exact approximate inference methods exist bayesian networks see
e g koller friedman pearl used filtering dbns
applying unrolled dbn slice repeated observed
time step via successive update current posterior belief state used
c

ai access foundation rights reserved

fialbrecht ramamoorthy

xt

xt




xt

xt








figure example dynamic bayesian network dbn two state variables two
observation variables xti xt
variables represent process states time

respectively yit variables shaded represent observation time
arrows describe variables interact

prior next time step see murphy however clear
unrolled variant becomes intractable network grows unboundedly time even
successive update exact methods become intractable high dimensional process
states approximate methods may propagate growing errors time therefore filtering
methods developed utilise special structure dbns maintain errors
propagated time defer detailed discussion methods section
often key developing efficient filtering methods identify structure
process leveraged inference article interested application
dbns representations actions partially observed decision processes
pomdps kaelbling littman cassandra sondik many variants
dbns used represent effects actions decision process specifying
variables interact information decision maker observes many cases
decision processes exhibit high degrees causal structure pearl mean
change one part process may cause change another part experience
processes causal structure may used make filtering task
tractable tell us beliefs need revised certain aspects
process state example variable x figure changes value variable x
changed value e change x causes change x seems intuitive use
causal relation deciding whether revise ones belief x unfortunately
current filtering methods take causal structure account
refer type causal relation x x passivity intuitively
say state variable xi passive given action executing action
subset state variables directly affect xi e xi parents dbn
xi may change value least one variables subset changed
value worth pointing passivity occurs naturally frequently many
domains especially robotic physical systems mainzer
following example illustrates simple robot arm

mark end example solid black square



fiexploiting causality selective belief filtering dbns












xa

b

xb



robot arm gripper

c
b holding blocks b

figure robot arm three rotational joints gripper variables represent
absolute orientations corresponding joints
example robot arm consider robot arm three rotational joints gripper
shown figure joints denoted may take values
discrete set indicate absolute orientations e g means
joint points exactly right means points left
joint let two actions cwi ccwi rotate joint clockwise
counter clockwise respectively uncertainty system could due stochastic
joint movements unreliable sensor readings joint orientations
action cwi ccwi variable passive value directly
modified action however variables j passive change
values corresponding preceding variable j changed value since changed
orientation joint j causes changed orientation joint j recall orientations
absolute note accounts chains causal effects indicated
arrows orientation joint changes orientation joint changes since joint
causes joint change turn causes joint change
examples passivity seen context object manipulation
blocks domain e g pasula zettlemoyer kaelbling figure b
shows arm holding blocks b top b position b xb
passive respect joint orientations since change orientations
changed furthermore causal chain joint orientations position
block xa since position change bs position changes

passivity exploited accelerate filtering task example
fact state variables passive means aspects state may remain
unchanged depending action choose example choose rotate joint
fact joints passive means unaffected action
thus seems redundant revise beliefs orientations joints however
precisely current filtering methods cf section
concretely assume use factored belief representation p p
p choose rotate direction easy see need
update factor p since changes value factor p since
variables passive since parents change
values know change values later skipping


fialbrecht ramamoorthy

p loss information cases similarly chains
causal connections cf example complex example domain
involving passivity exploited discussed section
addition guiding belief revision several features make passivity
interesting example causal relation first passivity latent causal relation
meaning readily extracted process dynamics without additional
annotation expert section give procedure identifies passive variables
conditional probability tables furthermore passivity deterministic
relation since passive variables may stochastic behaviour changing
values finally passivity relatively simple example causal relation idea
exploiting passivity order accelerate filtering task intuitive yet best
knowledge formalised explored rigorously
purpose present article formalise evaluate idea automatically
exploiting causal structure efficient belief filtering dbns passivity concrete
example causal relation specifically hypothesis large processes
high degrees passivity structure exploited accelerate filtering task
discussing related work section technical preliminaries section
contributions grouped following parts
section give formally concise definition passivity discuss
aspects definition definition assumes decision process specified
set dynamic bayesian networks one action discuss nonexample passivity mean variables appear passive really
passive finally give simple procedure detect passive variables
conditional probability tables
section present passivity selective belief filtering psbf method
following idea outlined psbf uses factored belief representation
belief factors defined clusters correlated state variables psbf follows
step update procedure wherein belief state first propagated
process dynamics transition step conditioned observation
observation step interesting novelty psbf way performs
transition step rather updating belief factors psbf updates
factors whose variables suspects changed possible exploiting
passivity made precise shortly similarly observation step psbf updates
belief factors determines structurally connected
observation uses parts observation relevant
belief factor thus allowing efficient incorporation observations psbf
produces exact belief states certain assumptions approximate belief states
otherwise discuss computational complexity error bounds psbf
section evaluate psbf two experimental domains first evaluate psbf
synthetic e randomly generated processes varying sizes degrees passivity
process sizes vary one thousand one trillion states passivity
degrees vary passivity psbf faster
several alternative methods maintaining competitive accuracy particular


fiexploiting causality selective belief filtering dbns

indicate computational gains grow significantly degree
passivity size process evaluate psbf complex simulation
multi robot warehouse system style kiva wurman dandrea mountz
passivity occurs system psbf exploit
accelerate filtering task outperforming alternative methods
finally discuss strengths weaknesses psbf section conclude
work section proofs found appendix

related work
exists substantial body work belief filtering partially observed stochastic
processes section review filtering methods utilise special structure
dbns situate work within related literature
approximate belief filtering dbns
several authors proposed filtering methods wherein belief state represented set
state samples specifically probability process state normalised
frequency state samples correspond methods commonly
referred particle filters pf see work doucet de freitas gordon
survey common variant pf gordon salmond smith filtering
task consists propagating current state samples process dynamics
subsequent resampling step probabilities state samples
would produced observation two interesting features pf
applied processes discrete continuous variables approximation
error converges zero increase number state samples
known pf fact number samples needed acceptable
approximations grow drastically variance process dynamics shown
experiments cf section rao blackwellised pf rbpf doucet de freitas
murphy russell developed address rbpf assumes
state variables grouped sets r x distribution x
efficiently calculated r filtering hence sample rbpf consists
sample r corresponding marginal distribution x rbpf useful
variance r relatively low variance x high since reduces number
samples needed acceptable approximations
boyen koller recognised process consists several independent
weakly interacting subcomponents belief state represented efficiently
product smaller beliefs individual subcomponents seminal contribution approximation error due factored representation essentially
bounded degree uncertainty mixing rates process precisely
prove relative entropy kl divergence kullback leibler two belief states contracts exponential rate propagated stochastic transition
process observation propose filtering method bk wherein belief
state represented factored form belief factors updated exact inference method junction tree lauritzen spiegelhalter since


fialbrecht ramamoorthy

internal cliques used junction tree may correspond belief
state representation bk final projection step typically performed
original factorisation restored performance method depends crucially whether relevant correlations state variables captured small
clusters whether projection step performed efficiently
factored particle filtering fp ng peshkin pfeffer addresses main drawbacks pf many samples needed bk small clusters required approximating
belief factors set factored state samples samples factored sense
assign values variables corresponding factor allows fp
represent belief factors large bk reduces number samples
needed due smaller number variables factor authors provide different methods updating factored state samples generic idea first perform
join operation full state samples reconstructed factored samples
updated standard pf updated samples projected
factored form project operation main drawback fp join
project operations essentially correspond standard relational database operations
expensive
murphy weiss propose filtering method called factored frontier
uses fully factored representation belief states belief state product
marginals individual state variable allows compact representation
beliefs works moving set state variables frontier forward
backward dbn topology requires certain variable ordering
difficult attain intra correlations state variables e edges within
slice dbn allowed authors method equivalent single
iteration loopy belief propagation lbp pearl thus similar lbp
applied successive iterations improve approximation accuracy
none works discussed explicitly address question causal relations
state variables exploited accelerate filtering task alternatively
filtering methods proposed therein implicitly benefit causal structure method
psbf related bk fp psbf uses factored belief representation
belief factors defined clusters correlated state variables therefore
analysis approximation errors boyen koller applies psbf
section well experiments however contrast bk fp psbf
perform inference complete factorisation rather individual
factors consequence psbf require join project operation one
main disadvantages bk fp
belief filtering decision processes
methods discussed preceding subsection used belief filtering decision
processes including pomdps kaelbling et al sondik regard
methods viewed pure filters concerned belief filtering
control decision process contrast combined filtering
methods interleave filtering control tasks decision processes make
specific assumptions regarding solutions thereof exists large body literature


fiexploiting causality selective belief filtering dbns

combined methods including reachability methods hauskrecht washington
grid methods zhou hansen brafman lovejoy pointbased methods smith simmons pineau gordon thrun compression
methods roy gordon thrun poupart boutilier
potential advantage combined methods access additional
structure may therefore utilise synergies filtering control tasks one
synergy use decision quality guide belief filtering rather metrics
relative entropy poupart boutilier propose filtering method called
value directed approximation chooses different approximation schemes different
decisions minimise expected loss decision quality e accumulated rewards
method assumes pomdp solved exactly value function
provided form vectors represent available actions pomdp
value function computes switching set alternative
plans determine error bounds approximation schemes used search
optimal approximation scheme tree manner search traverses
approximate exact schemes
idea decision quality guide belief filtering appealing method
involves series optimisation exhaustive tree search
costly complex systems advantage pure filtering methods including proposed
method psbf filter processes complex combined methods
multi robot warehouse system studied section actual control task
done via domain specific solutions cf section
substructure parameterisation
bayesian networks hence dbns allow compact parameterisation e specification
probabilities efficient inference via conditional independence relations addition
considerable work identifying substructure parameterisation
simplify knowledge acquisition enhance inference koller friedman
boutilier dean hanks property studied work passivity one example
substructure parameterisation notable examples include causal independence e g heckerman breese heckerman context specific independence
boutilier friedman goldszmidt koller
causal independence assumption effects individual causes common
variable e parents variable independent one another allows
compact parameterisation via operators noisy srinivas pearl
used enhance inference zhang poole note passivity
conceptually much simpler property causal independence passivity neither
concerned strength individual causes extent depend
moreover passivity read directly parameterisation cf section
whereas causal independence usually imposed designer
context specific independence csi property states variable independent parents given certain assignment values e context
parents non local csi statements follow similarly separation geiger verma
pearl allow reduction parameters boutilier et al


fialbrecht ramamoorthy

enhancement inference poole zhang discuss section passivity viewed special kind csi applied dbns parents respect
variable passive provide context csi however contrast csi
passivity assume context actually observed

technical preliminaries
section introduces basic concepts notation used work begin
brief discussion decision processes provide context work followed
discussion dynamic bayesian networks model perform inference
decision processes belief states exact updates
consider stochastic decision process wherein time process state
st decision maker agent choosing action executing st

process transitions state st probability st st agent receives

observation ot probability st ot assume factored representations
state space observation space x xn ym
domains xi yj finite notation si used denote value xi
state analogously oj moreover assume process
time invariant meaning independent framework compatible
many decision used artificial intelligence literature including pomdps
kaelbling et al sondik many variants
agent chooses action belief state bt known information state
represents agents beliefs likelihood states time formally
belief state probability distribution state space process belief filtering
task calculating belief state history observations ideally
resulting belief state exact retains relevant information past
observations sometimes referred sufficient statistic cf astrom
exact update rule simple procedure produces exact belief states
definition exact update rule exact update rule defined follows taking
action observing ot belief state bt updated bt via
bt

x



bt



ss


bt bt ot



normalisation constant
sometimes refer step bt bt transition step step bt bt
observation step unfortunately space complexity storing exact belief states
time complexity updating exact update rule exponential
number state variables making infeasible complex systems large state
spaces hence efficient approximate methods required


fiexploiting causality selective belief filtering dbns

dynamic bayesian networks
dynamic bayesian network dbn dean kanazawa bayesian network
special temporal semantics specifies stochastic process transitions one
state another dbns used model effects actions stochastic decision
process specifically compact representation transition function
observation function oa action
definition dbn dynamic bayesian network action denoted acyclic
directed graph consisting




xt xt x
state variables x xt xtn x xt


xn
representing states process time respectively


representing obser observation variables ym
j
j
vation received time




directed edges ea x x x x x
specifying network topology dependencies variables
conditional probability distributions pa z paa z variable z x
specifying probability z assumes certain value given specific assignment
parents paa z z z z ea convenience define pata z
pa z pa z
x paa z pat


zz paa z
z x
edges ea distributions pa define functions






n



pa xt
paa xt
















pa yjt oj paa yjt



j


use notation paa xt

specify parents xi


x respectively assume corresponding values formally



xtl pata xit xt
pat
l similarly use
xi xl sl xl
l


notation paa yj specify parents yjt x
respectively assume corresponding values

xt

example dbn representation robot arm represent robot arm example set dbns one dbn action
cw
ccwi







state observation
variables

n
dbns x x

make example realistic let us assume
joint orientations bounded relative orientation immediately preceding joint
e g form cone first joint bounded relative ground
means joint movement depends well preceding joint orientation shown figure moreover joint orientations correlated e edges within


fialbrecht ramamoorthy



















xt

x



figure dbn representation robot arm
x joint exceed bound given preceding joint finally observation variables depend solely corresponding joint variable actions
example would differ variable distributions pa

additional definitions
useful define following
xt

binary order defined x x xti xtj xt

j


j n xi xj j n
given set z x x write z denote tuple contains variables
z ordered
given ordered tuple z zi zi z define set z xi xi z
contain value tuples variables z
given value tuple sz si si z z use notation z sz
abbreviation zil sil zil z e variables z assume
corresponding values sz

passivity
section introduces formal definition passivity used basis
remainder article provide simple procedure detect passive
variables process dynamics
formal definition
outlined section state variable xt
called passive action exists

dbn xt may change value
subset xt

parents

x




fiexploiting causality selective belief filtering dbns

least one variables subset changed value conversely xt


change variables subset change formally define passivity follows


definition passivity let action given dbn

state variable xi



called passive exists
set paa xi xi


xtj xt
ea
j xi

ii two states st st st st


sti st
xtj stj st
j




state variable passive called active
set corresponds subset variables described contains
variables directly affect xit e parents xt
x xt
may


change value variables changed value sometimes say
variable xt
passive respect another variable xtj case

xtj furthermore omit obvious context
clause definition requires xt
intra correlated variables

specifically edge xt

xt
xtj example see
j

figure assumed variable xt
passive respect variable

xt discuss purpose clause next subsection clause ii defines
core semantics passivity requiring xt
remains unchanged variables

remain unchanged note means distribution pa xt
may specify

deterministic stochastic behaviour variables change values
includes xt
may change value

state variable xit passive even parents x none
xti case set would empty clause well premise
would trivially hold true however variable passive change
value circumstances words would constant
case one consider removing variable state description order reduce
computational costs
noted section passivity shown special kind context specific
independence csi boutilier et al applied dbns associated set
passive variable xt
provides context given assignment values xtj e


context xtj xt
independent xtk xt
xtk pata xt
j xi

k
k however besides similarity important difference passivity
csi passivity actually assume context observed thus
passivity viewed kind csi unobserved contexts become clear
section describe filtering method exploits passivity
non example passivity
purpose clause definition passivity discussed
previously clause ii captures core idea passivity variable may
change value variables respect passive changed value


fialbrecht ramamoorthy

xt

xt


xt

xt


figure example process clause ii insufficient
however may seem intuitive clause ii sufficient passivity
fact processes clause ii alone suffice words clause ii
necessary sufficient passivity illustrate following example
example non example passivity consider process two binary state variables
x x single action shown figure omit observation variables
clarity dynamics process xt
takes value xt xt
takes


value xt e x x swap values time step process
state variables satisfy clause ii definition set x x e initial values
st st positive states st st hence true set x x
st st positive states st st sti st
hence
trivially true since premise false

despite satisfying clause ii state variables xt
xt
example


fact passive following two reasons firstly passivity causal relation
must imply causal order pearl however causal order
x x edge xt
xt

secondly passivity means
variable may change value another variable respect passive
variable changed value words whether passive variable xt

may change value depends past values time values
time however variables example depend values
time hence values time predetermined depend whether
variables change values
first issue namely causal order addressed adding corresponding edges x instance example could add edge xt
xt


establish causal order however generally solve second issue
every passive variable xit must depend past values variables words xit must inter correlated well intra correlated
variables former given definition since every variable
parent xt
latter precisely required clause definition
therefore clauses ii together define formal meaning passivity
detecting passive variables
mentioned section passivity latent causal property sense
extracted process dynamics without additional information additional
assumptions regarding representation variable distributions order determine


fiexploiting causality selective belief filtering dbns


passive xt





input state variable xt
dbn

output xt
passive else false



q orderedqueue p pata xi xti
ascending order




q



nextelement q



q q



xtj



xt
ea
j xi











go line clause violated

paa xt
xti

n





x

x

j

j
si xi




pa xt



x

















go line clause ii violated
return
return false

variable xit passive one set clauses definition
satisfied simple procedure representation variable
distributions given takes inputs variable xt





dbn checks whether xi passive searching set satisfies
clauses definition note power set p line includes empty set
hence accounts lines check clause satisfied lines
check clause ii satisfied line essentially checks holds true
clauses satisfied xt
passive respect variables

returns set otherwise returns logical false
time complexity exponential worst case xt


passive specifically time requirements line grow exponentially number
parents xt
x time requirements line grow exponentially

cardinality however time requirements reduced significantly
committing specific representations variable distributions pa example
distributions represented tabular form one utilise arrays indices
perform sweeping tests e line moreover important realise
needs performed state variable prior start
strictly speaking checks property stronger passivity
check st st cf clause ii line however modified include
check omit exposition order highlight core ideas behind



fialbrecht ramamoorthy

process demand since passivity invariant process states
words variable passive passive therefore suffices
check advance passivity
note set necessarily unique example consider
variable xt


passive respect variables xt xt e xt xt assume
xt
changes
x changes e


change time




easy verify x x satisfy clauses ii hence
valid sets definition passivity guiding principle
cases occams razor intuitively speaking states simplest explanation
suffices case means suffices use smallest set terms
cardinality hence line sorts queue q ascending order
rationale exist multiple causal explanations passive variable
xt
one involving fewest key variables favoured since reduces
compared alternative explanations number cases would
revise beliefs xit earlier example accept causal explanation

xt
every time xt
xt
may
would revise beliefs x



changed values however accept causal explanation would
revise belief x xt
may changed value difference

become obvious section explains passivity exploited
reduce computational costs

passivity selective belief filtering
section presents passivity selective belief filtering psbf method
exploits passivity efficient filtering discussed section assume process
specified set dynamic bayesian networks contains one dbn
action therefore whenever refer action e g pa paa
assumed context
psbf follows general two step update procedure belief state first
propagated process dynamics transition step conditioned
observation observation step thus natural divide exposition psbf
three parts belief state representation transition step observation
step discussed sections respectively summary psbf
given section discuss computational complexity error bounds
psbf sections respectively
belief state representation
recall section principal idea behind psbf maintain separate beliefs
individual aspects process exploit passivity order perform selective
updates separate beliefs union individual aspects constitutes complete
state description process therefore belief state represented product
separate beliefs individual aspects
capture informal notion individual aspects formally form clusters
defined follows


fiexploiting causality selective belief filtering dbns

c




c

c










































c



c





c
c c c

b c

c c c

figure three clusterings robot arm dbn
definition cluster clustering x set c c ck satisfies
k ck x c ck x refer elements ck c clusters
underlying idea behind concept clusters variables cluster ck
connected important sense specifically two variables common
cluster exists relation variables regarding likelihood
values may assume words variables correlated x
number k concrete choice clusters ck specified user
generated automatically example may specified manually domain expert
familiar structure modelled system generated automatically
methods ones described section stressed however
order reduce computational costs advisable follow general rule small
possible large necessary choosing clusters see section discussion
computational complexity therefore two variables strongly correlated
presumably common cluster whereas weakly
correlated weakly meaning correlation ignored safely
separate clusters order reduce computational costs illustrated
following example
example clusters robot dbn recall robot arm dbn example specifit given three clusters
cally figure
cluster
state
one way


variables x
c
c
c
shown figure clustering
efficient since minimises size cluster however clusters fail capture
important correlation joint orientation restricted preceding joint
orientation
another

way cluster state variables given single cluster
c shown figure b clustering captures correlations
variables however largest possible cluster

therefore

least effi
cient one compromise given two clusters c c
shown figure c clustering captures correlation joint orientations immediately preceding joint orientations efficient
previous clustering since smaller clusters



fialbrecht ramamoorthy

given definition clusters capture informal notion separate beliefs
form belief factors
definition belief factor given cluster ck corresponding belief factor bk
probability distribution set ck
intuitively belief factor bk represents agents beliefs likelihood values
variables corresponding cluster ck analogy view belief factor
smaller belief state view b full belief state combination
smaller belief states however distinguish two refer b simply belief
state bk belief factor
finally given clusters ck corresponding belief factors bk belief state b
represented factored form
b

k


bk sk

k



use notation sk refer tuple si xt ck e g ck xt
xt



sk
exploiting passivity transition step
order perform selective updates belief factors bk require procedure
performs transition step independently factor obtain procedure
introducing two assumptions allow us modify transition step exact
update rule assumptions guarantee transition step performed exactly
sense however discuss shortly assumptions violated obtain
approximate belief states
first assumption states clusters must uncorrelated e
edges x clusters second assumption states clusters
must disjoint formally defined follows

xt
ck pat
xi ck


k k ck ck
note neither assumption implies may case
satisfied violated vice versa assuming
reformulate
x



bt
tka k
btk sk

k sk
pat ck


k xt
ck xti pat ck



normalisation constant



tka k
pa xt
k paa xt

sk
xt
ck


advantage belief factors updated parallel useful feature
considering many platforms use parallel processing techniques



fiexploiting causality selective belief filtering dbns

procedure performs transition step independently belief factor bk hence
updated order parallel
assumption allows us bring form updates belief
factors bk independently specifically allows us define cluster
transition function tka turn enables summation assumption
hand guarantees product correct particular may
case sk ck e fewer elements sk ck variables
ck patat ck e xt
ck xti
patat ck cases btk


taken marginal distribution variables xi ck xti patat ck
guarantees marginalisation introduces errors
mentioned previously assumption may violated obtain approximate belief
states however important distinction regard
violated still well defined sense still executed
except product may degrade accuracy contrast
structural requirement tka sense tka ill defined without
since violated variables ck may parents x

ck case paa xt
sk would ill defined thus violated
enforce modifying distributions pa xt
ck marginalise


variables pat
x





c



clusters
c
means

k
k


variable separate distribution every cluster contains variable thereby
possibly introducing approximation error
given modified transition step exploit passivity perform selective
updates belief factors bk recall section variable xt
passive



exists set variables xi may change value
variables changed value causal connection used decide
whether values variables cluster ck may changed case
corresponding belief factor bk updated theorem provides formal foundation


theorem hold xt
ck passive


bt
k sk bk sk

proof proof appendix
theorem states clusters c ck disjoint uncorrelated

variables cluster ck passive transition step corresponding
belief factor btk bt
omitted without loss information
k
theorem translate situations violated
key assumption states clusters must uncorrelated
discussed earlier enforce modifying variable distributions pa cluster
however passive variable xit ck correlated passive active variable


xt
ck xt
pat
distribution pa
xi marginalising xj
j
j


xi typically cause xi lose passivity sense would longer satisfy
clauses definition consequently would perform transition
step ck even unmodified variables ck passive problematic
unnecessary computations modified distributions
introduce error every time transition step performed


fialbrecht ramamoorthy






c






c







xt

x



figure robot arm dbn implementing action cw dashed circles mark passive state
variables coloured ellipses represent clusters c c
alleviate effect one check chance unmodified variables
cluster would change values shown case whenever
causal path active variable variable cluster
definition causal path causal path active variable xt
another


q



q

variable xj sequence hx x x x xi x
xj
q q
x q x
ii x q x q ea
iii x q passive respect x q
intuitively causal path defines chain causal effects joints
example since active variable x may changed value x passive
respect x x may changed value since x may changed
value x passive respect x x may changed value etc
hence absence observing changes mere existence causal path
x x q reason revise beliefs x q therefore general update rule
omit transition step btk bt
unmodified variables cluster ck passive
k


causal path active variable variable ck
demonstrated following example
example psbf update rule robot arm dbn let us consider robot arm
previous examples figure shows dbn implements action cw
action rotates joint robot arm clock wise e joint orientation
direct target action therefore variable active variables
passive shown dashed circles

use clustering c c reasons given example since parent psbf enforce assumption


fiexploiting causality selective belief filtering dbns

skippableclusters c


input clustering c c ck dbn



output set clusters c c skipped transition step



c c



q orderedqueue x



c q











xt
nextelement q



q q xt


passive xt



c c ck c xt
ck


xt
q
j


causalpath xt
xj
n

c c ck c xt
ck
j
n

q q xt
j

return c

marginalising variable distribution pa cluster c modified variable distribution loses passivity property clauses definition
violated unmodified distribution still passive
performing transition step psbf update belief factor b
corresponding cluster c contains active variable however since variables
cluster c passive modified variables c since causal
path variable c psbf omit update belief factor b
intuitively makes sense since change orientation joint cannot cause
change orientations preceding joints note corresponds saving
transition step


defines procedure utilises rule clusters
transition step skipped takes inputs clustering c dbn
returns set c skippable clusters essentially searches active
variables xt
removes clusters ck c contain variables

causal path xit function orderedqueue x returns ordered
queue q variables x performance depends order
queue experiments obtained good performance ordering variables
descending order number outgoing edges function nextelement q returns

next element queue function passive xt
defined


function causalpath xi xj returns logical true


fialbrecht ramamoorthy

causal path xt
xjt note given invariance passivity process

states cf section suffices call advance needed
determine clusters omit transition step
efficient incorporation observations
psbf perform observation step similarly exact update rule
conditions propagated belief state bt observation ot obtain fully updated
belief state bt however given factored belief state representation used psbf
require procedure respects factorisation observation step assuming
hold bring form updates belief factors bk
independently
x





bt
ot
bt

k sk bk sk
k sk
k k c pat
pat
k
k


k




normalisation constant note analogously variables
bt taken marginal distribution
ck pat
k



ck paat
assumption guarantees marginalisation introduces
errors hold transition step observation step
produce exact belief states sense regardless many clusters
skipped transition step cf theorem
observation step updates belief states uses observation variables
process words ignores internal structure observation variables
however clear variables cluster ck marginally independent
observation variables determined separation geiger et al
simply checking directed path ck need
perform observation step corresponding belief factor bk expressed
formally theorem


theorem xt
ck marginally independent yjt


bt
k sk bk sk

proof proof appendix b
theorem states variables ck independent
observation step bk skipped however even ck independent may
case variables ck depend subset yk observation
variables clearly cases suffices use yk rather observation
step account first note variables may correlated
preserve correlations subdivide clusters cl
introduce following assumptions


yjt cl paa yjt cl
l l cl cl
simple way implement function modify standard graph search method breath first
search check iii definition apply variables x edges ea



fiexploiting causality selective belief filtering dbns

assumptions analogous respectively essentially
serve purposes observation step distinguish clusters ck cl
sometimes refer former state cluster latter observation cluster
assuming hold redefine observation step
x







bt




b






bt


k
k
l
k
k
k sk



l cl yk pat
cl sk sk k k ck pa cl



al ot
l









pa yjt ot


pa





j
j
l
l

yjt cl

yk set observation variables marginally independent
variables ck
given theorem one see equivalent observation variables
clustered equivalently single observation cluster cl however
important note observation variables clustered e multiple
observation clusters cl notqnecessarily
equivalent
p
p
qmto see helpful
compare abstract formulations



b

j
j

j oj bs
former corresponds latter therein om observation bs
probability state oj probability observing yj oj
abstract formulations equivalent bs
cases may equivalent nonetheless fix number observation
variables approximates closely increase number state variables
n experiments indicate often suffices use state variables
observation variables order obtain good approximations
finally suffices perform observation step bk
clusters cl whose variables independent variables ck observe
fact repeated application every cl updated belief factor bt

k

used place bk subsequent application since every application
form cl conclude theorem holds hence observation
step skipped clusters cl independent ck
summary psbf
preceding sections summarised follows
representation belief state bt represented product k belief factors btk
q


bt k
k bk belief factor bk probability distribution
set ck ck x cluster correlated state variables
transition step transition step btk bt
performed clusters
k


ck include active variables causal path

active variable clusters skipped
observation step observation step bt
bt
performed
k
k
clusters ck dependent observation variables
observation clusters cl relevant ck clusters skipped


fialbrecht ramamoorthy

psbf ot btk ck c c c aa


input action observation ot belief factors btk ck c



parameters state clustering c observation clustering c dbns aa



output updated belief factors bt
k ck c



transition step



c skippableclusters c



ck c









ck c
bt
btk
k
else
k ck
x


bt
tka k
btk sk
k sk
patat ck



k xt
ck xti patat ck


observation step

ck c
n



yk yjt directed path ck yjt




yk



bt
bt
k
k





else
k ck


bt
k sk bk sk



x



al ot




bt
k sk

cl
cl c cl yk pat
cl sk k k k ck pat





return

bt
k ck c

provides procedural specification psbf takes inputs
action time subsequent observation time ot belief factors
time btk internal parameters state clustering c observation clustering
c set dbns aa define process lines implement
transition step lines implement observation step note suffices
execute lines advance demand remember
future reference returns updated belief factors bt
k


fiexploiting causality selective belief filtering dbns

space time complexity
belief factor bk one elementp
bk sk sk ck thus total space required
maintain k belief factors bk k
k ck furthermore size set ck grows
exponentially number variables ck hence dominant growth factor
space requirement given largest cluster ck ck maxk ck therefore
space complexity psbf exp maxk ck hence representation feasible
reasonably small clusters ck
similarly numberpof operations required perform transition observation
steps order k
k ck worst case e clusters need updated
steps specifically line line executed
every sk ck dominant growth factor given largest cluster ck hence
time complexity psbf exp maxk ck exp maxk ck note
assumes analysis performed lines done advance
time complexity worst case clusters need updated
transition observation steps difficult derive time complexity
average case unclear average case terms passivity even
stipulate certain average degree passivity e g variables passive would
still difficult make general statement time requirements since depends
crucially passive variables distributed across clusters example even
process average passivity one active variable cluster
every cluster would need updated transition step thus general
statement make regards passivity time complexity psbf
refined exp maxck ct co ck ct co include clusters
need updated transition observation step respectively
error bounds
five possible sources approximation errors psbf
clusters correlated e violated
clusters overlapping e violated
generally multiple observation clusters cl used
first two cases approximation error depends amount correlation
overlap little correlation overlap clusters
approximation error expected small conversely clusters strongly
correlated overlapping approximation error expected large
boyen koller provide useful analysis error bound filtering
method uses factored belief state representation since psbf uses factored
representation analysis applies directly psbf purpose section
restate main analysis context work
analysis uses concept relative entropy kullback leibler
measure similarity belief states
practice suffices store ck elements irrelevant analysis



fialbrecht ramamoorthy

definition relative entropy let two probability distributions defined
set x relative entropy defined
kl

x
xx

x ln

x
x

x x
similar boyen koller define approximation error incurred psbf
relative exact belief state however since consider decision process multiple
actions represented dbns define error action respectively
definition approximation error let b exact belief state b approximation psbf taking action let b exact update b
b psbf update b furthermore let b exact update
b say psbf incurs error relative b
kl b b kl b b
analysis relies concept mixing rates intuitively mixing rate
dbn quantifies degree stochasticity depends mixing rates ka
individual clusters ck
definition mixing rate mixing rate cluster ck x defined
ka min




x



min tka tka

ss ck

ck satisfy observation variables one observation
cluster mixing rate given mink ka r q cluster ck
depends r influences q clusters ck k boyen koller
worst case violated minimal mixing rate given ka
single cluster ck x
finally main work boyen koller restated
context work theorem essentially states approximation error psbf
measured terms relative entropy bounded mixing rates process
theorem boyen koller let bt exact belief state bt approximation psbf clusters ck states st actions

h
max


eo ot kl bt bt
mina
expectation e q
taken possible sequences observations ot


defined
probabilities p



fiexploiting causality selective belief filtering dbns

process size

x vars n

vars

states

obs







one thousand









one million



l





one billion



xl





one trillion



table synthetic process sizes variables binary

experimental evaluation
evaluated psbf two experimental domains section evaluated psbf
synthetic e randomly generated processes varying sizes degrees passivity
section evaluated psbf simulation multi robot warehouse system brief
summary experimental given section
synthetic processes
first evaluated psbf series synthetic processes psbf compared selection
alternative methods including pf gordon et al rbpf doucet et al bk
boyen koller murphy weiss see section discussion
methods implemented matlab used matlab
toolbox bnt murphy implement bk
specification synthetic processes
generated synthetic processes four different sizes specified table
process generated follows
first variable xit chosen passive probability p case
add edge xti xt
refer p degree passivity sample


edges x x
x generate mixture gaussians g see
appendix c figure shows example g generated process size set
g used produce areas correlated variables e gaussians
constitute natural candidates state clusters
let vector maximum densities gaussian g let
vector densities value n every combination j edge xti xt
j

added probability equal maximum element j operators
point wise xt
chosen passive edge xti xt

j added


j case add edge xi xj edges xi xj added similarly

j add edge xti xt
j passive xj ensure every
variable effect generated process xti connected least one xt
j

x adding
adding xti xt


necessary


x


least
one
parent

x

j
condition j cases ensure resulting dbn acyclic



fialbrecht ramamoorthy




density




























j

figure example mixture gaussians generated process size consisting

three gaussians closer two variables xi
xt
peak common
j
gaussian higher probability edge added

xtj xt
j necessary finally edges xi yj added probability
j ensuring yjt least one parent x
variables process binary passive variables assumed passive
respect parents x distributions pa xt
x generated


uniformly randomly without bias passive variables xi modify pa satisfy clause
ii definition distributions pa yjt generated probability
sampled uniformly obtain meaningful observations
finally every process consists two actions obtained randomly choosing
one three variables xt
whose distributions pa resampled

edges x added probability passive variables chosen way longer
passive simulations actions chosen uniformly randomly
process starts random initial state tested
sequence processes initial states chosen actions random numbers

clustering methods
used three different clustering methods denoted hpci hmorali hmodisi methods
applied variables x without edges involving x
hpci drops directions edges e edge xt
xt
ads reverse

j


edge xj xi puts variables undirected path
one cluster definition resulting clusters satisfy assumptions
hmorali connects parents variable drops directions moralises
variables extracts clusters fully connected variables maximum cliques
resulting clusters may satisfy assumptions
hmodisi similar hmorali truncates resulting clusters make disjoint
clusters removed become subset another cluster definition
resulting clusters satisfy necessarily
example consider figure section hpci would produce cluster
c figure b since variables connected undirected path furthermore


fiexploiting causality selective belief filtering dbns

hmorali would produce two clusters c c figure c correspond
two maximum cliques moralising variables x finally hmodisi would produce
cluster c figure c cluster c figure
psbf used clustering method generate clusters state variables ck
observation variables cl moreover psbf enforced whenever necessary
modifying variable distributions described section
accuracy
order compare accuracy tested computed relative entropy
cf definition exact belief states obtained exact update rule cf definition
approximate belief states produced tested however since exact
belief states relative entropy hard compute large processes able
compare accuracy processes size initialised
uniform belief states uniformly sampled particles
first compared accuracy psbf bk since use factorisation
belief state representations figure shows relative entropy psbf bk averaged processes passivity respectively
psbf hpc modisi produced lower relative entropy e higher accuracy bk hpc modisi psbf hmorali produced relative entropy comparable
bk hmorali indicates violations introduce smaller errors
violations note psbf bk convergent behaviour
relative entropy shows approximation error due factorisation
bounded discussed section interesting since psbf bk obtain approximation errors factorisation different ways psbf loses accuracy modifying
variable distributions ensure state clusters independent cf section
bk loses accuracy marginalising original factorisation inference e
projection step cf section nevertheless shown resulting
approximation errors bounded cases similar convergence
note relative entropy methods increased degree passivity
process explained fact higher passivity implies higher determinacy
therefore lower mixing rates cf definition crucial factor error
bounds psbf bk cf theorem finally note psbf produce exact
belief states e zero relative entropy hpci clustering despite fact
clusters generated hpci satisfy assumptions however discussed detail
sections another possible source approximation errors multiple observation
clusters used often case hpci produce observation clusters
compare accuracy pf rbpf psbf bk number samples used
pf rbpf chosen automatically process required approximately
much time per belief update psbf hmorali bk hmorali respectively
experiments meant pf rbpf able process
samples however since process states nearly enough
represent uniform belief state hence pf rbpf produced much higher relative entropy
psbf bk moreover fact processes high variance means
pf rbpf would require many samples achieve accuracy psbf bk


fialbrecht ramamoorthy






bk pc

psbf pc

bk moral

psbf moral

bk modis

psbf modis

relative entropy

relative entropy














transition














passivity





transition













b passivity


relative entropy

relative entropy






















transition























transition

passivity

relative entropy

relative entropy

c passivity











transition





e passivity














transition

f passivity

figure accuracy psbf bk plots relative entropy exact
belief states lower better averaged processes size
n average non target variables passive cf
section psbf bk used clustering methods hpci hmorali hmodisi
shown next section one would expect latter issue alleviated use
exact inference rbpf cf section however case much
variance process captured marginal distributions used particles
rbpf contrast synthetic processes exhibit high variance across variables


fiexploiting causality selective belief filtering dbns

automatic grouping state variables sampled exact variables still contained
much variance sampled variables hence rbpf required significantly samples
number could process time provided
finally order compare accuracy psbf bk number iterations
used precisely number iterations loopy belief propagation cf murphy
weiss chosen automatically process required approximately
much time per belief update psbf hmorali bk hmorali respectively however
often able perform several iterations provided time resulting
relative entropy substantially higher psbf bk
designed specific class dbn topologies namely containing
edges within x called regular dbns murphy weiss allows
use fully factored representation belief states variable
belief factor however processes used experiments high intra correlation
state variables e many edges x especially increasing passivity
correlations cannot captured belief state representation resulting
significantly higher relative entropy psbf bk
timing
measured computation times processes sizes l xl passivities
respectively psbf bk used hmorali clustering seemed
appropriate fair comparison since produced consistently similar accuracy
number samples used pf chosen automatically process
pf achieved average accuracy approximately good psbf
bk respectively final process involved computing exact belief
states relative entropies able use pf processes size omit
rbpf section shown previous section unsuitable
processes consider psbf tested parallel processes
allocated approximately number belief factors
figures times transitions averaged processes
figure e shows average percentage belief factors updated transition
observation steps psbf timing reported psbf includes time taken
modify variable distributions case overlapping clusters detect skippable clusters
transition observation steps done advance
action psbf able minimise time requirements significantly
exploiting passivity first note marginal gains
passivity despite fact psbf updated fewer clusters transition step
clusters mostly small however significant gains
passivity average speed ups l xl
open question group state variables sampled exact variables doucet et al
used simple heuristic whereby set sampled variables contained variables xt


parents x none xti remaining variables x constituted set
exact variables ensure resulting grouping valid actions e dbns process
considered edges involved dbns performed grouping union ea
moreover improve efficiency subdivided set exact variables clusters
variables connected undirected edges x without edges involving sampled variables




pf bk
pf psbf
bk









passivity







psbf
psbf
psbf




n







passivity

seconds transitions



seconds transitions

seconds transitions

seconds transitions

albrecht ramamoorthy













b n





c l n


updated belief factors



passivity














passivity



xl n

trans
obs
trans
obs




l trans
l obs
xl trans
xl obs








passivity



e updated belief factors

figure timing ad average number seconds required transitions
unix dual core machine ghz sizes l xl passivity p means
average p non target variables passive cf section psbf bk
used hmorali clustering pf optimised binary variables used number samples
achieve accuracy psbf bk respectively psbf run psbf
psbf psbf parallel processes e average percentage belief factors
updated transition observation steps respectively
passivity average speed ups l
xl shows computational gains grow significantly
degree passivity size process
psbf consistently outperformed bk process sizes
two main computational savings psbf relative bk firstly skipping belief
factors transition observation steps secondly perform
potentially expensive projection step restore original factorisation inference
however times grew exponentially size process
note relative difference psbf bk decreased significantly lower
degrees passivity instance free lunch see section discussion
means psbf performs best processes high passivity suffer
performance processes lack passivity specifically computational overhead
modifying variable distributions detecting skippable belief factors amortise


fiexploiting causality selective belief filtering dbns

effectively large processes low passivity furthermore low passivity psbf
often perform full transition observation steps e update belief factors
step costly large processes
bk pf affected passivity surprisingly performance bk
nearly unaffected increasing degrees passivity junction tree used
bk benefited marginally increased sparsity process computational
gains minimal first unable use pf required many samples
k k achieve comparable accuracy psbf bk due
high variance processes order investigate effect passivity pf
implemented version pf strictly optimised binary variables interestingly
found passivity adverse effect performance pf requiring use
exponentially samples increased passivity see figure makes sense
view pf factored approximation method psbf bk means
analysis section applies however pf puts variables single cluster
since actually factored method mixing rate process much lower
psbf bk discussed section thus error bounds less
tight compensate pf requires significantly samples increased passivity
multi robot warehouse system
section demonstrate passivity occur naturally complex system
psbf exploit accelerate filtering task end consider
multi robot warehouse system style kiva wurman et al
robots task transport goods within warehouse cf figure
specification warehouse system
figure b shows initial state warehouse simulation warehouse consists
workstations w w robots r r inventory pods robot
move forward backward turn left right load unload inventory pod
positioned pod nothing kiva robots move inventory pods
unless carrying pod case pods become obstacles move
turn operations stochastic robot may move turn far chance
nothing chance robot possesses two sensors one telling inventory
pod loaded one direction facing direction sensor noisy
random direction may reported chance
robot maintains list tasks form bring inventory pod workstation
w yellow area around w bring inventory pod position x tasks
executed depends control mode use two simulations
control modes ad hoc often make suboptimal decisions however found current
solution techniques dec pomdps including approximate methods infeasible setting
nonetheless quality decisions made control modes largely depends accuracy
belief states hence important belief states updated accurately therefore
control modes sufficient purposes



fialbrecht ramamoorthy

kiva warehouse system

b initial state simulation

figure kiva warehouse system image reproduced dandrea wurman
robots orange coloured transport shelfs goods workstations b initial
state warehouse simulation warehouse consists workstations w w
robots r r inventory pods
centralised mode central controller maintains belief state bt state
warehouse system time samples states bt removes

duplicate states resulting set
p resamples state



probabilities w b q b sq current task
robot performs search hart nilsson raphael manhattan
distance space joint actions optimal action robot
executing actions robots send sensor readings controller
controller updates belief state sensor readings
decentralised mode robot maintains belief state communication robots knowledge robots
current tasks communicated task allocation module time
robot samples set state done centralised mode treating
robots static obstacles performs search current
task action repeated robot r states sq
resulting actions ar q used
p obtain distributions r
set actions r q ar q w sq robot executes action updates belief state sensor readings distributions r
average robots actions
tasks generated external scheduler time intervals sampled u
generated task assigned one robots sequential auction dias zlot
kalra stentz robots bids calculated total number steps needed
solve current tasks auctioned task simplified model
robots removed averaged states robot lowest bid
assigned task


fiexploiting causality selective belief filtering dbns

figure example dbn smaller warehouse system consisting one inventory
pod two robots r r dbn implements joint action r moves
r turns dashed circles mark passive state variables coloured areas represent
state clusters c c

dbn topology clustering
figure shows example dbn smaller warehouse one inventory pod two
robots inventory pod represented two variables x correspond
x position inventory pod robot r represented four variables
r x r x position r direction r status status
robot r r unloaded r loaded inventory pod constants
size warehouse positions workstations omitted dbn
four types clusters clusters c c preserve correlation
r loaded must position r two clusters
r pair r clusters c clusters c respectively preserve
correlation two robots position carry inventory pod
one r cluster ra rb pair b finally clusters c
c psbf uses singleton observation clusters e one cluster observation variable
differences dbns centralised decentralised modes
figure uses centralised mode centralised mode one dbn
action combination robots since controller observes r noise free add
edges r x r x r remove otherwise simplify inference
thus figure r loaded r unloaded decentralised mode
robot observes sensor readings hence add remove edges
edges robots must permanently added means
robots status variables r must linked x therefore included
clusters preserve correlation must position r r
loaded moreover since robot knows action one dbn


fiseconds per transition

albrecht ramamoorthy

centralised
decentralised






bk

psbf

pf

figure warehouse simulation centralised decentralised
control modes timing measured unix dual core machine ghz averaged
different simulations transitions
actions variables associated robots active
distributions r defined previous section used average actions

implemented psbf bk pf c framework infer net minka winn
guiver knowles implement bk allowed bk exploit sparsity
process offered improved memory handling psbf optimised sparsity
respectively summing states btk bt
k positive pf naturally
benefits sparsity allows concentrate samples fewer states number
samples used pf set way controller decisions invariant
random numbers used sampling process pf done ensure
repeatable finally maintain sparsity process probability
belief states lower set tested initialised exact
belief state shown figure b
figure shows time per transition averaged different simulations
transitions timing reported psbf includes time needed modify variable
distributions overlapping clusters detect skippable belief factors transition
observation steps done demand every previously unseen
dbn centralised mode psbf able outperform bk average
pf pf needed samples produce consistent e repeatable
decentralised mode psbf outperformed bk average pf pf
needed samples produce consistent due increased variance
process differences statistically significant paired tests
significance level note psbf bk slower decentralised mode since
corresponding dbns much higher inter connectivity addition psbf updated
belief factors since active variables
expected psbf able exploit high degree passivity process
accelerate filtering task many cases meant psbf needed update less
half belief factors precisely many belief factors updated depends


fiexploiting causality selective belief filtering dbns

performed action illustrate consider smaller warehouse dbn shown figure
centralised mode r moving r turning r x r
r active variables variables passive dashed circles corresponding
passivity dbn psbf updates belief factors corresponding clusters
c c c c since contain active variables updates belief
factors c c since directed paths active variables r x r
therefore factors updated c c
consider full warehouse experiment contains inventory pods robots
resulting variables clusters r clusters clusters clusters
assume similar situation one robot moves inventory pod say r
r turn case psbf updates r clusters containing
r clusters since status change clusters r
clusters clusters containing r plus clusters r amounting
total saving belief factors need updated
number states warehouse system including invalid states exceeded
states therefore unable compare accuracy tested terms
relative entropy instead compared accuracy task
auctions number completed tasks end simulation gives
good indication accuracy since outcome auction
number completed tasks depend accuracy belief states centralised
mode generated identical task auctions completed bk
psbf pf tasks average decentralised mode generated
identical auctions completed bk psbf pf tasks
average modes none differences statistically significant therefore
indicates psbf achieved accuracy similar bk pf
summary experimental evaluation
experimental psbf produces belief states competitive accuracy
synthetic processes psbf achieved accuracy average better
comparable accuracy alternative methods warehouse system psbf
able complete statistically equivalent number tasks compared
methods indicates accuracy equivalent comparable
furthermore experimental psbf performed belief updates
significantly faster alternative methods synthetic processes psbf
parallel processes outperformed bk largest process xl pf took
much time achieve accuracy comparable psbf particular
computational gains grow significantly degree passivity
size process warehouse system psbf outperformed alternative methods
substantial saving considering size state space
states furthermore computational gains much higher centralised
control mode decentralised control mode since latter significantly
lower degree passivity therefore shows high degrees passivity bear
great potential filtering task


fialbrecht ramamoorthy

free lunch psbf
view belief filtering method generally suited types processes
instead method assumes certain structure process explicitly implicitly
attempts exploit order render filtering task tractable typically
methods tailored way respect structure perform well
structure present process suffer significant loss performance structure
absent instance pf works best processes low degrees uncertainty since
means fewer state samples needed acceptable approximations
hand number samples needed acceptable approximations grow substantially
degree uncertainty process shown experiments another
example bk works best processes little correlation state variables since
means belief factors small processed efficiently however
many variables strongly correlated bk typically becomes infeasible
therefore structural assumptions taken account choosing
filtering method specific process
formal account view given free lunch theorems wolpert
macready state intuitively speaking two
equivalent performance averaged possible instances
words classes instances better performance
b must classes instances
worse performance b question class instances
processes psbf expected achieve good performance class essentially
described following three criteria
degree passivity psbf attempts accelerate filtering task omitting
transition step many belief factors possible depends passivity
variables state clusters ideal case process exhibits high degree
passivity psbf omit transition step many belief factors
worst case process passive variables psbf update
belief factors transition step however discussed section high degree
passivity necessarily sufficient infer many clusters skipped
transition step since passive variables could distributed way
cluster skipped e g passive variables distributed uniformly
amongst state clusters therefore optimal case passivity concentrated
correlated state variables passive variables end clusters
size state clusters space time complexity belief state representation
psbf exponential size largest state cluster cf section therefore
ideal case relevant variable correlations captured small state
clusters cost storing belief factors performing update procedures
small worst case large state clusters required retain variable
correlations cost storing updating belief factors large another reason
state clusters small way psbf performs
transition step one pre requisite omitting transition step belief factor
variables corresponding cluster passive many variables


fiexploiting causality selective belief filtering dbns

one cluster less likely variables cluster passive
therefore less likely cluster skipped
structure observations third criterion though arguably less important
criteria structure observations e way observation
variables depend state variables size observation clusters cl
psbf attempts accelerate observation step skipping state
clusters whose variables structurally independent observation
cluster cannot skipped incorporating observation clusters
relevant update therefore ideal case fraction state clusters
depend observation relevant correlations observation variables
captured small observation clusters worst case state clusters
depend observation sense structure observation
allow efficient clustering
thus summary psbf suitable processes high degrees passivity
relevant variable correlations captured small state observation
clusters hand psbf may suitable low degrees
passivity large state observation clusters necessary retain relevant
variable correlations process
addition identifying class processes filtering method suitable
important justify practical relevance class work interested
robotic physical decision processes shown examples experiments
systems typically exhibit number features first robotic systems usually
causal structure e g mainzer pearl passivity specific type
causality observed many robotic systems including robot arm used
examples multi robot warehouse system section furthermore robotic systems
typically modular structure module responsible specific
subtask may interact modules modular structure often allows
efficient clustering sense module corresponds cluster correlated state
variables finally sensors used robotic systems typically provide information
certain aspects system components system may benefit
sensor information words independencies state
observation variables features correspond criteria specify
class processes psbf suitable filtering method therefore believe
class practically justified

conclusion
inferring state stochastic process difficult technical challenge complex
systems large state spaces key developing efficient solutions identify special
structure process e g topology parameterisation dynamic bayesian
networks leveraged render filtering task tractable
end present article explored idea automatically detecting exploiting
causal structure order accelerate belief filtering task considered specific type
causal relation termed passivity pertains state variables cause changes


fialbrecht ramamoorthy

state variables demonstrate potential exploiting passivity developed novel
filtering method psbf uses factored belief state representation exploits passivity
perform selective updates belief factors psbf produces exact belief states
certain assumptions approximate belief states otherwise showed empirically
synthetic processes varying sizes degrees passivity well example
complex multi robot system psbf faster several alternative methods
achieving competitive accuracy particular showed computational
gains grow significantly size process degree passivity
work demonstrates system exhibits much causal structure
great potential exploiting structure render filtering task tractable
particular experiments support initial hypothesis factored beliefs passivity
useful combination large processes insight relevant complex processes
high degrees causality robots used homes offices industrial factories
filtering task may constitute major impediment due often large state
space system
several potential directions future work example would useful
know definition passivity could relaxed variables fall
definition principal idea behind psbf still applicable one
relaxation could form approximate passivity allows small probabilities
passive variables change values even relevant parents remain unchanged
addition would interesting know idea performing selective updates
belief factors via passivity could applied existing methods use
factored belief state representation cf section finally another useful avenue future
work would formulate additional types causal relations exploited
ways similar psbf exploits passivity perhaps ways

acknowledgements
article long debate presented topic process benefited
number discussions suggestions particular authors wish thank
anonymous reviewers nips uai conferences well journal ai
attendees workshop advances causal inference held uai
colleagues school informatics university edinburgh furthermore
authors acknowledge financial support german national academic foundation
uk engineering physical sciences council grant number ep h
european commission tomsy grant agreement



fiexploiting causality selective belief filtering dbns

appendix proof theorem
prove theorem useful first establish following lemma
lemma holds xt
ck passive

tka k sk k
proof
fact means ck xt
ck since xt
ck


passive follows xtj passive therefore given
tka k clause ii definition follows sk k
follows directly fact xt
ck passive


lemma give compact proof theorem


theorem hold xt
ck passive


bt
k sk bk sk

proof

bt
k sk





x



tka k

pat ck





x

btk sk

k xt
ck xti pat ck




lem







tka k



btk sk

pat ck sk k k xt
ck xti pat ck






btk sk

x



tka k



btk sk

pat ck sk k k k xt
ck xti pat ck




z









btk sk



btk sk since btk normalised





fialbrecht ramamoorthy

appendix b proof theorem
prove theorem first note following proposition



proposition xt
ck marginally independent yjt


k k sk k ot ot

proposition follows directly definition

proposition give compact proof theorem


theorem xt
ck marginally independent yjt


bt
k sk bk sk

proof
x



bt
k sk bk sk





ot


bt
k sk

k k c pat
pat
k
k


k






prop

z

constant independent



bt k
p k
bk sk
k



bt k
p k
bk sk
k


bt
k sk




k

fiexploiting causality selective belief filtering dbns

appendix c mixture gaussians
provides simple procedure randomly generates mixture gaussians
e set normal distributions synthetic processes section
takes input number n state variables returns set g gaussians whose means
set n number gaussians means variances
chosen automatically achieve good coverage state variables minimising
visual overlap gaussians see figure example

mixtureofgaussians n


input number state variables n



parameters min max



output mixture gaussians g



g



r n



r

n




r next element r



r r r



r drand r e rand returns random number



min r r r




min max max min rand


g g mean variance gaussian



r r r r p r p



r r q r q r r r q



r






r r r
r

r r r
return g



fialbrecht ramamoorthy

references
astrom k optimal control markov processes incomplete state information
journal mathematical analysis applications
boutilier c dean hanks decision theoretic structural assumptions computational leverage journal artificial intelligence

boutilier c friedman n goldszmidt koller context specific independence bayesian networks proceedings th conference uncertainty
artificial intelligence pp
boyen x koller tractable inference complex stochastic processes
proceedings th conference uncertainty artificial intelligence pp
boyen x koller exploiting architecture dynamic systems proceedings
th national conference artificial intelligence pp
brafman r heuristic variable grid solution method pomdps proceedings
th national conference artificial intelligence pp
dandrea r wurman p future challenges coordinating hundreds autonomous vehicles distribution facilities proceedings ieee international
conference technologies practical robot applications pp
dean kanazawa k model reasoning persistence causation
computational intelligence
dias zlot r kalra n stentz market multirobot coordination
survey analysis proceedings ieee
doucet de freitas n gordon n sequential monte carlo methods practice
springer science business media
doucet de freitas n murphy k russell rao blackwellised particle
filtering dynamic bayesian networks proceedings th conference
uncertainty artificial intelligence pp
geiger verma pearl j separation theorems
proceedings th conference uncertainty artificial intelligence pp

gordon n salmond smith novel nonlinear non gaussian
bayesian state estimation iee proceedings f radar signal processing vol
pp
hart p nilsson n raphael b formal basis heuristic determination
minimum cost paths ieee transactions systems science cybernetics
vol pp
hauskrecht value function approximations partially observable markov
decision processes journal artificial intelligence


fiexploiting causality selective belief filtering dbns

heckerman causal independence knowledge acquisition inference
proceedings th conference uncertainty artificial intelligence pp

heckerman breese j look causal independence proceedings
th conference uncertainty artificial intelligence pp
kaelbling l littman cassandra acting partially
observable stochastic domains artificial intelligence
koller friedman n probabilistic graphical principles techniques
mit press
kullback leibler r information sufficiency annals mathematical statistics
lauritzen spiegelhalter local computations probabilities graphical
structures application expert systems journal royal statistical
society series b methodological
lovejoy w computationally feasible bounds partially observed markov decision
processes operations
mainzer k causality natural technical social systems european review

minka winn j guiver j knowles infer net microsoft
cambridge http microsoft com infernet
murphy k bayes net toolbox matlab computing science statistics
https code google com p bnt
murphy k weiss factored frontier approximate inference
dbns proceedings th conference uncertainty artificial intelligence
pp
murphy k dynamic bayesian networks representation inference learning
ph thesis university california berkeley
ng b peshkin l pfeffer factored particles scalable monitoring
proceedings th conference uncertainty artificial intelligence pp

pasula h zettlemoyer l kaelbling l learning symbolic stochastic
domains journal artificial intelligence
pearl j probabilistic reasoning intelligent systems networks plausible inference morgan kaufmann
pearl j causality reasoning inference cambridge university press
pineau j gordon g thrun point value iteration anytime
pomdps proceedings th international joint conference artificial
intelligence vol pp
poole zhang n exploiting contextual independence probabilistic inference
journal artificial intelligence


fialbrecht ramamoorthy

poupart p boutilier c value directed belief state approximation pomdps
proceedings th conference uncertainty artificial intelligence pp

poupart p boutilier c vector space analysis belief state approximation
pomdps proceedings th conference uncertainty artificial
intelligence pp
poupart p boutilier c value directed compression pomdps advances
neural information processing systems pp
roy n gordon g thrun finding approximate pomdp solutions
belief compression journal artificial intelligence
smith simmons r point pomdp improved analysis
implementation proceedings st conference uncertainty artificial
intelligence pp
sondik e optimal control partially observable markov processes ph
thesis stanford university
srinivas generalization noisy model proceedings th conference
uncertainty artificial intelligence pp
washington r bi pomdp bounded incremental partially observable markovmodel recent advances ai pp springer
wolpert macready w free lunch theorems search tech rep sfi tr santa fe institute
wolpert macready w free lunch theorems optimization ieee
transactions evolutionary computation
wurman p dandrea r mountz coordinating hundreds cooperative
autonomous vehicles warehouses ai magazine
zhang n poole exploiting causal independence bayesian network inference
journal artificial intelligence
zhou r hansen e improved grid approximation
pomdps proceedings th international joint conference artificial
intelligence pp




