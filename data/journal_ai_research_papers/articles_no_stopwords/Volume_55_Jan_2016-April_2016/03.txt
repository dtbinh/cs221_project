Journal Artificial Intelligence Research 55 (2016) 409-442

Submitted 07/15; published 02/16

Automatic Description Generation Images: Survey
Models, Datasets, Evaluation Measures
Raffaella Bernardi

bernardi@disi.unitn.it

University Trento, Italy

Ruket Cakici

ruken@ceng.metu.edu.tr

Middle East Technical University, Turkey

Desmond Elliott

d.elliott@uva.nl

University Amsterdam, Netherlands

Aykut Erdem
Erkut Erdem
Nazli Ikizler-Cinbis

aykut@cs.hacettepe.edu.tr
erkut@cs.hacettepe.edu.tr
nazli@cs.hacettepe.edu.tr

Hacettepe University, Turkey

Frank Keller

keller@inf.ed.ac.uk

University Edinburgh, UK

Adrian Muscat

adrian.muscat@um.edu.mt

University Malta, Malta

Barbara Plank

bplank@cst.dk

University Copenhagen, Denmark

Abstract
Automatic description generation natural images challenging problem
recently received large amount interest computer vision natural language processing communities. survey, classify existing approaches based
conceptualize problem, viz., models cast description either generation problem retrieval problem visual multimodal representational
space. provide detailed review existing models, highlighting advantages
disadvantages. Moreover, give overview benchmark image datasets
evaluation measures developed assess quality machine-generated
image descriptions. Finally extrapolate future directions area automatic image
description generation.

1. Introduction
past two decades, fields natural language processing (NLP) computer
vision (CV) seen great advances respective goals analyzing generating
text, understanding images videos. fields share similar set methods rooted artificial intelligence machine learning, historically developed
separately, scientific communities typically interacted little.
Recent years, however, seen upsurge interest problems require
combination linguistic visual information. lot everyday tasks nature,
e.g., interpreting photo context newspaper article, following instructions
conjunction diagram map, understanding slides listening lecture.
c
2016
AI Access Foundation. rights reserved.

fiBernardi et al.

addition this, web provides vast amount data combines linguistic visual
information: tagged photographs, illustrations newspaper articles, videos subtitles,
multimodal feeds social media. tackle combined language vision tasks
exploit large amounts multimodal data, CV NLP communities moved
closer together, example organizing workshops language vision
held regularly CV NLP conferences past years.
new language-vision community, automatic image description emerged
key task. task involves taking image, analyzing visual content, generating
textual description (typically sentence) verbalizes salient aspects
image. challenging CV point view, description could principle
talk visual aspect image: mention objects attributes,
talk features scene (e.g., indoor/outdoor), verbalize people
objects scene interact. challenging still, description could even refer
objects depicted (e.g., talk people waiting train, even
train visible arrived yet) provide background knowledge
cannot derived directly image (e.g., person depicted Mona Lisa).
short, good image description requires full image understanding, therefore
description task excellent test bed computer vision systems, one much
comprehensive standard CV evaluations typically test, instance, accuracy
object detectors scene classifiers limited set classes.
Image understanding necessary, sufficient producing good description.
Imagine apply array state-of-the-art detectors image localize objects
(e.g., Felzenszwalb, Girshick, McAllester, & Ramanan, 2010; Girshick, Donahue, Darrell,
& Malik, 2014), determine attributes (e.g., Lampert, Nickisch, & Harmeling, 2009; Berg,
Berg, & Shih, 2010; Parikh & Grauman, 2011), compute scene properties (e.g., Oliva &
Torralba, 2001; Lazebnik, Schmid, & Ponce, 2006), recognize human-object interactions (e.g., Prest, Schmid, & Ferrari, 2012; Yao & Fei-Fei, 2010). result would
long, unstructured list labels (detector outputs), would unusable image
description. good image description, contrast, comprehensive concise
(talk important things image), formally correct,
i.e., consists grammatically well-formed sentences.
NLP point view, generating description natural language generation (NLG) problem. task NLG turn non-linguistic representation
human-readable text. Classically, non-linguistic representation logical form,
database query, set numbers. image description, input image representation (e.g., detector outputs listed previous paragraph), NLG
model turn sentences. Generating text involves series steps, traditionally
referred NLP pipeline (Reiter & Dale, 2006): need decide aspects
input talk (content selection), need organize content (text
planning) verbalize (surface realization). Surface realization turn requires choosing right words (lexicalization), using pronouns appropriate (referential expression
generation), grouping related information together (aggregation).
words, automatic image description requires full image understanding,
sophisticated natural language generation. makes interesting
410

fiAutomatic Description Generation Images: Survey

task embraced CV NLP communities.1 Note
description task become even challenging take account
good descriptions often user-specific. instance, art critic require different
description librarian journalist, even photograph. briefly
touch upon issue talk difference descriptions captions
Section 3 discuss future directions Section 4.
Given automatic image description interesting task, driven
existence mature CV NLP methods availability relevant datasets, large
image description literature appeared last five years. aim survey
article give comprehensive overview literature, covering models, datasets,
evaluation metrics.
sort existing literature three categories based image description
models used. first group models follows classical pipeline outlined above:
first detect predict image content terms objects, attributes, scene types,
actions, based set visual features. Then, models use content information
drive natural language generation system outputs image description.
term approaches direct generation models.
second group models cast problem retrieval problem. is, create
description novel image, models search images database similar
novel image. build description novel image based descriptions
set similar images retrieved. novel image described simply
reusing description similar retrieved image (transfer), synthesizing
novel description based description set similar images. Retrieval-based models
subdivided based type approach use represent images
compute similarity. first subgroup models uses visual space retrieve images,
second subgroup uses multimodal space represents images text jointly.
overview models reviewed survey, category
fall into, see Table 1.
Generating natural language descriptions videos presents unique challenges
image-based description, additionally requires analyzing objects
attributes actions temporal dimension. Models aim solve description generation videos proposed literature (e.g., Khan, Zhang, &
Gotoh, 2011; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell,
& Saenko, 2013; Krishnamoorthy, Malkarnenkar, Mooney, Saenko, & Guadarrama, 2013;
Rohrbach, Qiu, Titov, Thater, Pinkal, & Schiele, 2013; Thomason, Venugopalan, Guadarrama, Saenko, & Mooney, 2014; Rohrbach, Rohrback, Tandon, & Schiele, 2015; Yao, Torabi,
Cho, Ballas, Pal, Larochelle, & Courville, 2015; Zhu, Kiros, Zemel, Salakhutdinov, Urtasun,
Torralba, & Fidler, 2015). However, existing work description generation used
static images, focus survey.2
survey article, first group automatic image description models three
categories outlined provide comprehensive overview models
1. Though image description approaches circumvent NLG aspect transferring human-authored
descriptions, see Sections 2.2 2.3.
2. interesting intermediate approach involves annotation image streams sequences sentences, see work Park Kim (2015).

411

fiBernardi et al.

category Section 2. examine available multimodal image datasets used
training testing description generation models Section 3. Furthermore, review
evaluation measures used gauge quality generated descriptions
Section 3. Finally, Section 4, discuss future research directions, including possible
new tasks related image description, visual question answering.

2. Image Description Models
Generating automatic descriptions images requires understanding humans
describe images. image description analyzed several different dimensions (Shatford, 1986; Jaimes & Chang, 2000). follow Hodosh, Young, Hockenmaier (2013)
assume descriptions interest survey article ones
verbalize visual conceptual information depicted image, i.e., descriptions
refer depicted entities, attributes relations, actions
involved in. Outside scope automatic image description non-visual descriptions,
give background information refer objects depicted image (e.g.,
location image taken took picture). Also, relevant
standard approaches image description perceptual descriptions, capture
global low-level visual characteristics images (e.g., dominant color image
type media photograph, drawing, animation, etc.).
following subsections, give comprehensive overview state-of-the-art approaches description generation. Table 1 offers high-level summary field, using
three categories models outlined introduction: direct generation models, retrieval models visual space, retrieval model multimodal space.
2.1 Description Generation Visual Input
general approach studies group first predict likely meaning
given image analyzing visual content, generate sentence reflecting
meaning. models category achieve using following general pipeline
architecture:
1. Computer vision techniques applied classify scene type, detect objects present image, predict attributes relationships hold
them, recognize actions taking place.
2. followed generation phase turns detector outputs words
phrases. combined produce natural language description
image, using techniques natural language generation (e.g., templates, n-grams,
grammar rules).
approaches reviewed section perform explicit mapping images
descriptions, differentiates studies described Section 2.2 2.3,
incorporate implicit vision language models. illustration sample model
shown Figure 1. explicit pipeline architecture, tailored problem hand,
constrains generated descriptions, relies predefined sets semantic classes
scenes, objects, attributes, actions. Moreover, architecture crucially assumes
412

fiAutomatic Description Generation Images: Survey

Reference

Generation

Farhadi et al. (2010)
Kulkarni et al. (2011)
Li et al. (2011)
Ordonez et al. (2011)
Yang et al. (2011)
Gupta et al. (2012)
Kuznetsova et al. (2012)
Mitchell et al. (2012)
Elliott Keller (2013)
Hodosh et al. (2013)
Gong et al. (2014)
Karpathy et al. (2014)
Kuznetsova et al. (2014)
Mason Charniak (2014)
Patterson et al. (2014)
Socher et al. (2014)
Verma Jawahar (2014)
Yatskar et al. (2014)
Chen Zitnick (2015)
Donahue et al. (2015)
Devlin et al. (2015)
Elliott de Vries (2015)
Fang et al. (2015)
Jia et al. (2015)
Karpathy Fei-Fei (2015)
Kiros et al. (2015)
Lebret et al. (2015)
Lin et al. (2015)
Mao et al. (2015a)
Ortiz et al. (2015)
Pinheiro et al. (2015)
Ushiku et al. (2015)
Vinyals et al. (2015)
Xu et al. (2015)
Yagcioglu et al. (2015)

Retrieval
Visual Space Multimodal Space
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X

X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X

X
X
X

Table 1: overview existing approaches automatic image description.
categorized literature approaches directly generate description image
(Section 2.1), approaches retrieve images via visual similarity transfer description new image (Section 2.2), approaches frame task retrieving
descriptions images multimodal space (Section 2.3).

413

fiBernardi et al.

Figure 1: automatic image description generation system proposed Kulkarni et al.
(2011).
accuracy detectors semantic class, assumption always met
practice.
Approaches description generation differ along two main dimensions: (a) image
representations derive descriptions from, (b) address sentence generation problem. terms representations used, existing models conceptualized
images number different ways, relying spatial relationships (Farhadi et al., 2010),
corpus-based relationships (Yang et al., 2011), spatial visual attributes (Kulkarni
et al., 2011). Another group papers utilizes abstract image representation
form meaning tuples capture different aspects image: objects detected,
attributes detections, spatial relations them, scene type
(Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al.,
2012). recently, Yatskar et al. (2014) proposed generate descriptions denselylabeled images, incorporate object, attribute, action, scene annotations. Similar
spirit work Fang et al. (2015), rely prior labeling objects,
attributes, etc. Rather, authors train word detectors directly images
associated descriptions using multi-instance learning (a weakly supervised approach
training object detectors). words returned detectors fed
language model sentence generation, followed re-ranking step.
first framework explicitly represent structure image relates
structure description Visual Dependency Representations (VDR) method
proposed Elliott Keller (2013). VDR captures spatial relations
objects image form dependency graph. graph related
syntactic dependency tree description image.3 initial work using VDRs
relied corpus manually annotated VDRs training, recent approaches
induce VDRs automatically based output object detector (Elliott & de Vries,
2015) labels present abstract scenes (Ortiz et al., 2015).4 idea explicitly
representing image structure using description generation picked
3. VDRs proven useful description generation, image retrieval (Elliott,
Lavrenko, & Keller, 2014).
4. Abstract scenes schematic images, typically constructed using clip-art. employed avoid
need object detector, labels positions objects know. example Zitnick
Parikhs (2013) dataset, see Section 3 details.

414

fiAutomatic Description Generation Images: Survey

Lin et al. (2015), parse images scene graphs, similar VDRs
represent relations objects scene. generate scene
graphs using semantic grammar.5
Existing approaches vary along second dimension, viz., approach
sentence generation problem. one end scale, approaches use
n-gram-based language models. Examples include works Kulkarni et al. (2011)
Li et al. (2011), generate descriptions using n-gram language models trained
subset Wikipedia. approaches first determine attributes relationships
regions image regionprepositionregion triples. n-gram language
model used compose image description fluent, given language model.
approach Fang et al. (2015) similar, uses maximum entropy language model
instead n-gram model generate descriptions. gives authors flexibility
handling output word detectors core model.
Recent image description work using recurrent neural networks (RNNs)
regarded relying language modeling. classical RNN language model: captures
probability generating given word string, given words generated far.
image description setup, RNN trained generate next word given
string far, set image features. setting, RNN therefore
purely language model (as case n-gram model, instance), hybrid
model relies representation incorporates visual linguistic features.
return detail Section 2.3.
second set approaches use sentence templates generate descriptions.
(typically manually) pre-defined sentence frames open slots need filled
labels objects, relations, attributes. instance, Yang et al. (2011) fill
sentence template selecting likely objects, verbs, prepositions, scene types
based Hidden Markov Model. Verbs generated finding likely pairing
object labels Gigaword external corpus. generation model Elliott
Keller (2013) parses image VDR, traverses VDRs fill slots
sentence templates. approach performs limited content selection
learning associations VDRs syntactic dependency trees training time;
associations allow select appropriate verb description test time.
approaches used linguistically sophisticated approaches generation.
Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments
recombine using tree-substitution grammar. related approach pursued
Kuznetsova et al. (2014), tree-fragments learnt training set existing
descriptions fragments combined test time form new descriptions.
Another linguistically expressive model recently proposed Ortiz et al. (2015).
authors model image description machine translation VDRsentence pairs
perform explicit content selection surface realization using integer linear program
linguistic constraints.
systems presented far aimed directly generating novel descriptions. However,
argued Hodosh et al. (2013), framing image description natural language generation (NLG) task makes difficult objectively evaluate quality novel descriptions
5. Note graphs used image retrieval Johnson, Krishna, Stark, Li, Shamma, Bernstein,
Fei-Fei (2015) Schuster, Krishna, Chang, Fei-Fei, Manning (2015).

415

fiBernardi et al.

Figure 2: description model based retrieval visual space proposed Ordonez
et al. (2011).
introduces number linguistic difficulties detract attention underlying image understanding problem (Hodosh et al., 2013). time, evaluation
generation systems known difficult (Reiter & Belz, 2009). Hodosh et al. therefore propose approach makes possible evaluate mapping images
sentences independently generation aspect. Models follow approach
conceptualize image description retrieval problem: associate image
description retrieving ranking set similar images candidate descriptions.
candidate descriptions either used directly (description transfer)
novel description synthesized candidates (description generation).
retrieval images ranking descriptions carried two ways:
either visual space multimodal space combines textual visual
information space. following subsections, survey work follows two
approaches.
2.2 Description Retrieval Visual Space
studies group pose problem automatically generating description
image retrieving images similar query image (i.e., new image
described); illustrated Figure 2. words, systems exploit similarity
visual space transfer descriptions query images. Compared models
generate descriptions directly (Section 2.1), retrieval models typically require large amount
training data order provide relevant descriptions.
terms algorithmic components, visual retrieval approaches typically follow
pipeline three main steps:
1. Represent given query image specific visual features.
2. Retrieve candidate set images training set based similarity measure
feature space used.
3. Re-rank descriptions candidate images making use visual
and/or textual information contained retrieval set, alternatively combine
fragments candidate descriptions according certain rules schemes.
One first model follow approach Im2Text model Ordonez et al.
(2011). GIST (Oliva & Torralba, 2001) Tiny Image (Torralba, Fergus, & Freeman, 2008)
416

fiAutomatic Description Generation Images: Survey

descriptors employed represent query image determine visually similar
images first retrieval step. retrieval-based models consider result
step baseline. re-ranking step, range detectors (e.g., object, stuff,
pedestrian, action detectors) scene classifiers specific entities mentioned
candidate descriptions first applied images better capture visual content,
images represented means detector classifier responses. Finally,
re-ranking carried via classifier trained semantic features.
model proposed Kuznetsova et al. (2012) first runs detectors classifiers used re-ranking step Im2Text model query image extract
represent semantic content. Then, instead performing single retrieval combining
responses detectors classifiers Im2Text model does, carries
separate image retrieval step visual entity present query image collect related phrases retrieved descriptions. instance, dog detected given
image, retrieval process returns phrases referring visually similar dogs
training set. specifically, step used collect three different kinds phrases.
Noun verb phrases extracted descriptions training set based
visual similarity object regions detected training images query
image. Similarly, prepositional phrases collected stuff detection query
image measuring visual similarity detections query training
images based appearance geometric arrangements. Prepositional phrases
additionally collected scene context detection measuring global scene similarity computed query training images. Finally, description generated
collected phrases detected object via integer linear programming (ILP)
considers factors word ordering, redundancy, etc.
method Gupta et al. (2012) another phrase-based approach. retrieve
visually similar images, authors employ simple RGB HSV color histograms,
Gabor Haar descriptors, GIST SIFT (Lowe, 2004) descriptors image features. Then, instead using visual object detectors scene classifiers, rely
textual information descriptions visually similar images extract
visual content input image. Specifically, candidate descriptions segmented phrases certain type (subject, verb), (subject, prep, object),
(verb, prep, object), (attribute, object), etc. best describe input image determined according joint probability model based image similarity Google search counts, image represented triplets form
{((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}.
end, description generated using three top-scoring triplets based fixed
template. increase quality descriptions, authors apply syntactic
aggregation subject predicate grouping rules generation step.
Patterson et al. (2014) first present large-scale scene attribute dataset
computer vision community. dataset includes 14,340 images 707 scene
categories, annotated certain attributes list 102 discriminative
attributes related materials, surface properties, lighting, affordances, spatial layout.
allows train attribute classifiers dataset. paper, authors
demonstrate responses attribute classifiers used global
image descriptor captures semantic content better standard global image
417

fiBernardi et al.

descriptors GIST. application, extended baseline model Im2Text
replacing global features automatically extracted scene attributes, giving better
image retrieval description results.
Mason Charniaks (2014) description generation approach differs models
discussed formulates description generation extractive summarization
problem, selects output description considering textual information
final re-ranking step. particular, authors represented images using scene
attributes descriptor Patterson et al. (2014). visually similar images identified training set, next step, conditional probabilities observing word
description query image estimated via non-parametric density estimation
using descriptions retrieved images. final output description determined using two different extractive summarization techniques, one depending
SumBasic model (Nenkova & Vanderwende, 2005) based Kullback-Leibler
divergence word distributions query candidate descriptions.
Yagcioglu et al. (2015) proposed average query expansion approach based
compositional distributed semantics. represent images, use features extracted
recently proposed Visual Geometry Group convolutional neural network (VGG-CNN;
Chatfield, Simonyan, Vedaldi, & Zisserman, 2014). features activations
last layer deep neural network trained ImageNet, proven
effective many computer vision problems. Then, original query expanded
average distributed representations retrieved descriptions, weighted
similarity input image.
approach Devlin et al. (2015) utilizes CNN activations global image
descriptor performs k-nearest neighbor retrieval determine images
training set visually similar query image. selects description
candidate descriptions associated retrieved images best describes
images similar query image, approaches Mason
Charniak (2014) Yagcioglu et al. (2015). approach differs terms
represent similarity description select best candidate
whole set. Specifically, propose compute description similarity based
n-gram overlap F-score descriptions. suggest choose output
description finding description corresponds description highest
mean n-gram overlap candidate descriptions (k-nearest neighbor centroid
description) estimated via n-gram similarity measure.
2.3 Description Retrieval Multimodal Space
third group studies casts image description generation retrieval problem,
multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).
intuition behind models illustrated Figure 3, overall approach
characterized follows:

1. Learn common multimodal space visual textual data using training
set imagedescription pairs.
418

fiAutomatic Description Generation Images: Survey

2. Given query, use joint representation space perform cross-modal (image
sentence) retrieval.

Figure 3: Image descriptions retrieval task proposed works Hodosh et al.
(2013), Socher et al. (2014), Karpathy et al. (2014)6 .
contrast retrieval models work visual space (Section 2.2),
unimodal image retrieval followed ranking retrieved descriptions, image
sentence features projected common multimodal space. Then, multimodal
space used retrieve descriptions given image. advantage approach
allows bi-directional models, i.e., common space used
direction, retrieving appropriate image query sentence.
section, first discuss seminal paper Hodosh et al. (2013) description
retrieval, present recent approaches combine retrieval approach
form natural language generation. Hodosh et al. map images sentences
common space. joint space used image search (find
plausible image given sentence) image annotation (find sentence describes
image well), see Figure 3. earlier study authors proposed learn common meaning space (Farhadi et al., 2010) consisting triple representation form
hobject, action, scenei. representation thus limited set pre-defined discrete
slot fillers, given training information. Instead, Hodosh et al. use KCCA,
kernelized version CCA, Canonical Correlation Analysis (Hotelling, 1936), learn
joint space. CCA takes training dataset image-sentence pairs, i.e., Dtrain = {hi, si},
thus input two different feature spaces, finds linear projections newly induced common space. KCCA, kernel functions map original items higher-order
space order capture patterns needed associate image text. KCCA
shown previously successful associating images (Hardoon, Szedmak, & ShaweTaylor, 2004) image regions (Socher & Fei-Fei, 2010) individual words set
tags.
Hodosh et al. (2013) compare KCCA approach nearest-neighbor (NN) baseline
uses unimodal text image spaces, without constructing joint space. drawback
KCCA applicable smaller datasets, requires two kernel matrices
6. Source http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/

419

fiBernardi et al.

kept memory training. becomes prohibitive large datasets.
attempts made circumvent computational burden KCCA, e.g.,
resorting linear models (Hodosh & Hockenmaier, 2013). Alternatively, Sun, Gan,
Nevatia (2015) used automatically discovered concepts images form semantic
space, performed sentence retrieval accordingly. However, recent work description
retrieval instead utilized neural networks construct joint space image description
generation.
Socher et al. (2014) use neural networks building sentence image vector representations mapped common embedding space. novelty
work use compositional sentence vector representations. First, image word
representations learned single modalities, finally mapped common
multimodal space. particular, use DT-RNN (Dependency Tree Recursive Neural
Network) composing language vectors abstract word order syntactic difference semantically irrelevant. results 50-dimensional word embeddings.
image space, authors use nine layer neural network trained ImageNet data,
using unsupervised pre-training. Image embeddings derived taking output
last layer (4,096 dimensions). two spaces projected multi-modal space
max-margin objective function intuitively trains pairs correct image
sentence vectors high inner product. authors show model outperforms previously used KCCA approaches work Hodosh Hockenmaier
(2013).
Karpathy et al. (2014) extend previous multi-modal embeddings model. Rather
directly mapping entire images sentences common embedding space,
model embeds fine-grained units, i.e., fragments images (objects) sentences
(dependency tree fragments), common space. final model integrates global
(sentence image-level) well finer-grained information outperforms previous
approaches, DT-RNN (Socher et al., 2014). similar approach pursued
Pinheiro et al. (2015), propose bilinear phrase-based model learns mapping
image representations sentences. constrained language model used
generate representation. conceptually related approach pursued Ushiku
et al. (2015): authors use common subspace model maps feature vectors
associated phrase nearby regions space. generation, beamsearch based decoder templates used.
Description generation systems difficult evaluate, therefore studies reviewed
treat problem retrieval ranking task (Hodosh et al., 2013; Socher et al.,
2014). approach valuable enables comparative evaluation,
retrieval ranking limited availability existing datasets descriptions.
alleviate problem, recent models developed extensions multimodal
spaces; able rank sentences, generate (Chen & Zitnick,
2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015;
Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).
Kiros et al. (2015) introduced general encoder-decoder framework image description
ranking generation, illustrated Figure 4. Intuitively method works follows.
encoder first constructs joint multimodal space. space used rank
images descriptions. second stage (decoder) uses shared multimodal
420

fiAutomatic Description Generation Images: Survey

Figure 4: encoder-decoder model proposed Kiros et al. (2015).

representation generate novel descriptions. model, directly inspired recent
work machine translation, encodes sentences using LongShort Term Memory (LSTM)
recurrent neural network, image features using deep convolutional network (CNN).
LSTM extension recurrent neural network (RNN) incorporates builtin memory store information exploit long range context. Kiros et al.s (2015)
encoder-decoder model, vision space projected embedding space LSTM
hidden states; pairwise ranking loss minimized learn ranking images
descriptions. decoder, neural-network-based language model, able generate novel
descriptions multimodal space.
Another work carried time similar latter
described paper Donahue et al. (2015). authors propose model
based LSTM neural architecture. However, rather projecting vision space
embedding space hidden states, model takes copy static image
previous word directly input, fed stack four LSTMs. Another
LSTM-based model proposed Jia et al. (2015), added semantic image information
additional input LSTM. model Kiros et al. (2015) outperforms prior
DT-RNN model (Socher et al., 2014); turn, Donahue et al. report outperform
work Kiros et al. (2015) task image description retrieval. Subsequent work
includes RNN-based architectures Mao et al. (2015a) Vinyals et al. (2015),
similar one proposed Kiros et al. (2015) achieve comparable
results standard datasets. Mao, Wei, Yang, Wang, Huang, Yuille (2015b) propose
interesting extension Mao et al.s (2015a) model learning novel visual concepts.
Karpathy Fei-Fei (2015) improve previous models proposing deep visualsemantic alignment model simpler architecture objective function. key
insight assume parts sentence refer particular unknown regions
image. model tries infer alignments segments sentences regions
images based convolutional neural networks image regions, bidirectional
RNN sentences structured objective aligns two modalities. Words
image regions mapped common multimodal embedding. multimodal
recurrent neural network architecture uses inferred alignments learn generate
421

fiBernardi et al.

novel descriptions. Here, image used condition first state recurrent
neural network, generates image descriptions.
Another model generate novel sentences proposed (Chen & Zitnick, 2015).
contrast previous work, model dynamically builds visual representation
scene description generated. is, word read generated
visual representation updated reflect new information. accomplish
simple RNN. model achieves comparable better results prior studies,
except recently proposed deep visual-semantic alignment model (Karpathy & Fei-Fei,
2015). model Xu et al. (2015) closely related uses RNN-based
architecture visual representations dynamically updated. Xu et al.s (2015)
model incorporates attentional component, gives way determining
regions image salient, focus description regions.
resulting improvement description accuracy, makes possible analyze
model behavior visualizing regions attended word
generated model.
general RNN-based ranking generation approach followed Lebret
et al. (2015). Here, main innovation linguistic side: employ bilinear
model learn common space image features syntactic phrases (noun phrases, verb
phrases, prepositional phrases). Markov model utilized generate sentences
phrase embedding. visual side, standard CNN-based features used.
results elegant modeling framework, whose performance broadly comparable
state art.
Finally, two important directions less explored are: portability weakly supervised learning. Verma Jawahar (2014) evaluate portability bi-directional
model based topic models, showing performance significantly degrades. highlight importance cross-dataset image description retrieval evaluation. Another interesting observation models require training set fully-annotated
image-sentence pairs. However, obtaining data large quantities prohibitively expensive. Gong et al. (2014) propose approach based weak supervision transfers
knowledge millions weakly annotated images improve accuracy description
retrieval.
2.4 Comparison Existing Approaches
discussion previous subsections makes clear approach image
description particular strengths weaknesses. example, methods
cast task generation problem (Section 2.1) advantage types
approaches produce novel sentences describe given image. However,
success relies heavily accurately estimate visual content
well able verbalize content. particular, explicitly employ computer
vision techniques predict likely meaning given image; methods
limited accuracy practice, hence fail identify important objects
attributes, valid description generated. Another difficulty lies
final description generation step; sophisticated natural language generation crucial
422

fiAutomatic Description Generation Images: Survey

guarantee fluency grammatical correctness generated sentences. come
price considerable algorithmic complexity.
contrast, image description methods cast problem retrieval
visual space problem transfer retrieved descriptions novel image (Section 2.2)
always produce grammatically correct descriptions. guaranteed design,
systems fetch human-generated sentences visually similar images. main issue
approach requires large amounts images human-written descriptions.
is, accuracy (but grammaticality) descriptions reduces size
training set decreases. training set needs diverse (in addition
large), order visual retrieval-based approaches produce image descriptions
adequate novel test images (Devlin et al., 2015). Though problem mitigated
re-synthesizing novel description retrieved ones (see Section 2.2).
Approaches cast image description retrieval multimodal space problem
(Section 2.3) advantage generating human-like descriptions
able retrieve appropriate ones pre-defined large pool descriptions.
However, ranking descriptions requires cross-modal similarity metric compares
images sentences. metrics difficult define, compared unimodal
image-to-image similarity metrics used retrieval models work visual space.
Additionally, training common space images sentences requires large training
set images annotated human-generated descriptions. plus side,
multimodal embedding space used reverse problem, i.e., retrieving
appropriate image query sentence. something generation-based
visual retrieval-based approaches capable of.

3. Datasets Evaluation
wide range datasets automatic image description research. images
datasets associated textual descriptions differ certain
aspects size, format descriptions descriptions collected. review common approaches collecting datasets, datasets themselves,
evaluation measures comparing generated descriptions ground-truth texts.
datasets summarized Table 2, examples images descriptions given
Figure 5. readers refer dataset survey Ferraro, Mostafazadeh, Huang,
Vanderwende, Devlin, Galley, Mitchell (2015) analysis similar ours. provides
basic comparison existing language vision datasets. limited
automatic image description, reports simple statistics quality metrics
perplexity, syntactic complexity, abstract concrete word ratios.
3.1 Image-Description Datasets
Pascal1K sentence dataset (Rashtchian et al., 2010) dataset commonly
used benchmark evaluating quality description generation systems.
medium-scale dataset, consists 1,000 images selected Pascal 2008
object recognition dataset (Everingham, Van Gool, Williams, Winn, & Zisserman, 2010)
includes objects different visual classes, humans, animals, vehicles.
423

fiBernardi et al.

Images

Texts

Judgments

Objects

Pascal1K (Rashtchian et al., 2010)
VLT2K (Elliott & Keller, 2013)
Flickr8K (Hodosh & Hockenmaier, 2013)
Flickr30K (Young et al., 2014)
Abstract Scenes (Zitnick & Parikh, 2013)
IAPR-TC12 (Grubinger et al., 2006)
MS COCO (Lin et al., 2014)

1,000
2,424
8,108
31,783
10,000
20,000
164,062

5
3
5
5
6
15
5


Partial
Yes



Collected

Partial
Partial


Complete
Segmented
Partial

BBC News (Feng & Lapata, 2008)
SBU1M Captions (Ordonez et al., 2011)
Deja-Image Captions (Chen et al., 2015)

3,361
1,000,000
4,000,000

1
1
Varies


Collected7






Table 2: Image datasets automatic description generation models. split
overview image description datasets (top) caption datasets (bottom) see
main text explanation distinction.
image associated five descriptions generated humans Amazon Mechanical
Turk (AMT) service.
Visual Linguistic Treebank (VLT2K; Elliott & Keller, 2013) makes use images
Pascal 2010 action recognition dataset. augments images three, twosentence descriptions per image. descriptions collected AMT specific
instructions verbalize main action depicted image actors involved (first
sentence), mentioning important background objects (second sentence).
subset 341 images Visual Linguistic Treebank, object annotation
available (in form polygons around objects mentioned descriptions).
subset, manually created Visual Dependency Representations (see Section 2.1)
included (three VDRs per images, i.e., total 1023).
Flickr8K dataset (Hodosh et al., 2013) extended version Flickr30K
dataset (Young et al., 2014) contain images Flickr, comprising approximately 8,000
30,000 images, respectively. images two datasets selected
user queries specific objects actions. datasets contain five descriptions per image collected AMT workers using strategy similar Pascal1K
dataset.
Abstract Scenes dataset (Zitnick & Parikh, 2013; Zitnick, Parikh, & Vanderwende,
2013) consists 10,000 clip-art images descriptions. images created
AMT, workers asked place fixed vocabulary 80 clip-art objects
scene choosing. descriptions sourced worker-created
scenes. authors provided descriptions two different forms. first
group contains single sentence description image, second group includes two
alternative descriptions per image. two descriptions consist three simple
sentences sentence describing different aspect scene. main advantage
dataset affords opportunity explore image description generation without
7. Kuznetsova et al. (2014) ran human judgments study 1,000 images dataset.

424

fiAutomatic Description Generation Images: Survey

1. One jet lands airport another takes
next it.
2. Two airplanes parked airport.
3. Two jets taxi past other.
4. Two parked jet airplanes facing opposite directions.
5. two passenger planes grassy plain

1. several people chairs small child
watching one play trumpet
2. man playing trumpet front little boy.
3. People sitting sofa man playing
instrument entertainment.

(a) Pascal1K8

(b) VLT2K9

1. man snowboarding structure snowy
hill.
2. snowboarder jumps air snowy
hill.
3. snowboarder wearing green pants trick
high bench
4. Someone yellow pants ramp
snow.
5. man performing trick snowboard high
air.

1. yellow building white columns background
2. two palm trees front house
3. cars parking front house
4. woman child walking square

(c) Flickr8K10

(d) IAPR-TC1211

1. cat anxiously sits park stares
unattended hot dog someone left
yellow bench

1. blue smart car parked parking lot.
2. vehicles wet wide city street.
3. Several cars motorcycle snow covered street.
4. Many vehicles drive icy street.
5. small smart car driving city.

(e) Abstract Scenes12

(f) MS COCO13

Figure 5: Example images descriptions benchmark image datasets.

425

fiBernardi et al.

need automatic object recognition, thus avoiding associated noise.
recent version dataset created part visual question-answering
(VQA) dataset (Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, & Parikh, 2015). contains
50,000 different scene images realistic human models five single-sentence
descriptions.
IAPR-TC12 dataset introduced Grubinger et al. (2006) one earliest
multi-modal datasets contains 20,000 images descriptions. images originally retrieved via search engines Google, Bing Yahoo, descriptions
produced multiple languages (predominantly English German). image
associated one five descriptions, description refers different aspect
image, applicable. dataset contains complete pixel-level segmentation
objects.
MS COCO dataset (Lin et al., 2014) currently consists 123,287 images five
different descriptions per image. Images dataset annotated 80 object categories, means bounding boxes around instances one categories
available images. MS COCO dataset widely used image description, something facilitated standard evaluation server recently
become available14 . Extensions MS COCO currently development, including
addition questions answers (Antol et al., 2015).
One paper (Lin et al., 2015) uses NYU dataset (Silberman, Kohli, Hoiem, &
Fergus, 2012), contains 1,449 indoor scenes 3D object segmentation.
dataset augmented five descriptions per image Lin et al.
3.2 Image-Caption Datasets
Image descriptions verbalize seen image, i.e., refer objects,
actions, attributes depicted, mention scene type, etc. Captions, hand,
typically texts associated images verbalize information cannot seen
image. caption provides personal, cultural, historical context image
(Panofsky, 1939). Images shared social networking photo-sharing websites
accompanied descriptions captions, mixtures types text. images
newspaper museum typically contain cultural historical texts, i.e., captions
descriptions.
BBC News dataset (Feng & Lapata, 2008) one earliest collections
images co-occurring texts. Feng Lapata (2008) harvested 3,361 news articles
British Broadcasting Corporation News website, constraint article
includes image caption.
8.
9.
10.
11.
12.

Source http://nlp.cs.illinois.edu/HockenmaierGroup/pascal-sentences/index.html
Source http://github.com/elliottd/vlt
Source https://illinois.edu/fb/sec/1713398
Source http://imageclef.org/photodata
Source http://research.microsoft.com/en-us/um/people/larryz/clipart/SemanticClassesRender
/Classes_v1.html
13. Source http://mscoco.org/explore
14. Source http://mscoco.org/dataset/#captions-eval

426

fiAutomatic Description Generation Images: Survey

SBU1M Captions dataset introduced Ordonez et al. (2011) differs
previous datasets web-scale dataset containing approximately one million
captioned images. compiled data available Flickr user-provided image
descriptions. images downloaded filtered Flickr constraint
image contained least one noun one verb predefined control lists. resulting
dataset provided CSV file URLs.
Deja-Image Captions dataset (Chen et al., 2015) contains 4,000,000 images
180,000 near-identical captions harvested Flickr. 760 million images downloaded
Flickr calendar year 2013 using set 693 nouns queries. image
captions normalized lemmatization stop word removal create corpus
near-identical texts. instance, sentences bird flies blue sky bird
flying blue sky normalized bird fly blue sky (Chen et al., 2015). Image
caption pairs retained captions repeated one user normalized
form.

3.3 Collecting Datasets
Collecting new imagetext datasets typically performed crowd-sourcing harvesting data web. images datasets either sourced
existing task computer vision community Pascal challenge (Everingham
et al., 2010) used Pascal1K VLT2K datasets directly Flickr,
case Flickr8K/30K, MS COCO, SBU1M Captions, Deja-Image Captions datasets,
crowdsourced, case Abstract Scenes dataset. texts imagedescription
datasets usually crowd-sourced Amazon Mechanical Turk Crowdflower; whereas
texts imagecaption datasets harvested photo-sharing sites,
Flickr, news providers. Captions usually collected without financial incentive
written people sharing images, journalists.
Crowd-sourcing descriptions images involves defining simple task
performed untrained workers. Examples task guidelines used Hodosh et al.
(2013) Elliott Keller (2013) given Figure 6. instances, care taken
clearly inform potential workers expectations task. particular,
explicit instructions given descriptions written, examples
good texts provided. addition, Hodosh et al. provided extensive examples
explain would constitute unsatisfactory texts. options available control
quality collected texts: minimum performance rate workers common
choice; pre-task selection quiz may used determine whether workers
sufficient grasp English language (Hodosh et al., 2013).
issue remuneration crowd-sourced workers controversial, higher payments always lead better quality crowd-sourced environment (Mason & Watts,
2009). Rashtchian et al. (2010) paid $0.01/description, Elliott Keller (2013) paid $0.04
average 67 seconds work produce two-sentence description. best
knowledge, information available datasets.
427

fiBernardi et al.

(a) Mechanical Turk Interface used collect Flickr8K dataset15 .

(b) Mechanical Turk Interface used collect VLT2K dataset.

Figure 6: Examples Mechanical Turk interfaces collecting descriptions.

3.4 Evaluation Measures
Evaluating output natural language generation (NLG) system fundamentally
difficult task (Dale & White, 2007; Reiter & Belz, 2009). common way assess
quality automatically generated texts subjective evaluation human experts.
15. Source Appendix work Hodosh et al. (2013)

428

fiAutomatic Description Generation Images: Survey

NLG-produced text typically judged terms grammar content, indicating
syntactically correct relevant text is, respectively. Fluency generated
text sometimes tested well, especially surface realization technique involved
generation process. Automatically generated descriptions images
evaluated using NLG techniques. Typically, judges provided image
well description evaluation tasks. Subjective human evaluations
machine generated image descriptions often performed Mechanical Turk help
questions. far, following Likert-scale questions used test datasets
user groups various sizes.
description accurately describes image (Kulkarni et al., 2011; Li et al., 2011;
Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al.,
2013).
description grammatically correct (Yang et al., 2011; Mitchell et al., 2012;
Kuznetsova et al., 2012; Elliott & Keller, 2013, inter alia).
description incorrect information (Mitchell et al., 2012).
description relevant image (Li et al., 2011; Yang et al., 2011).
description creatively constructed (Li et al., 2011).
description human-like (Mitchell et al., 2012).
Another approach evaluating descriptions use automatic measures,
BLEU (Papineni, Roukos, Ward, & Zhu, 2002), ROUGE (Lin & Hovy, 2008), Translation
Error Rate (Feng & Lapata, 2013), Meteor (Denkowski & Lavie, 2014), CIDEr (Vedantam, Lawrence Zitnick, & Parikh, 2015). measures originally developed evaluate output machine translation engines text summarization systems,
exception CIDEr, developed specifically image description evaluation.
measures compute score indicates similarity system output
one human-written reference texts (e.g., ground truth translations summaries).
approach evaluation subject much discussion critique (Kulkarni
et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014). Kulkarni et al. found weakly
negative correlation human judgments unigram BLEU Pascal 1K
Dataset (Pearsons = -0.17 0.05). Hodosh et al. studied Cohens correlation
expert human judgments binarized unigram BLEU unigram ROUGE retrieved
descriptions Flickr8K dataset. found best agreement humans
BLEU ( = 0.72) ROUGE ( = 0.54) system retrieved sentences originally associated images. Agreement dropped one reference sentence
available, reference sentences disjoint proposal sentences.
concluded neither measure appropriate image description evaluation
subsequently proposed imagesentence ranking experiments, discussed detail below. Elliott Keller analyzed correlation human judgments automatic
evaluation measures retrieved system-generated image descriptions Flickr8K
VLT2K datasets. showed sentence-level unigram BLEU, point
429

fiBernardi et al.

time de facto standard measure image description evaluation, weakly
correlated human judgments. Meteor (Banerjee & Lavie, 2005), less frequently used
translation evaluation measure, exhibited highest correlation human judgments.
However, Kuznetsova et al. (2014) found unigram BLEU strongly correlated
human judgments Meteor image caption generation.
first large-scale image description evaluation took place MS COCO
Captions Challenge 2015,16 featuring 15 teams dataset 123,716 training images
41,000 images withheld test dataset. number reference texts testing
image either five 40, based insight measures may benefit
larger reference sets (Vedantam et al., 2015). automatic evaluation measures
used, image description systems outperformed humanhuman upper bound,17
whether five 40 reference descriptions provided. However, none systems
outperformed humanhuman evaluation judgment elicitation task used. Meteor
found robust measure, systems beating human text one
two submissions (depending number references); systems outperformed
humans seven five times measured CIDEr; according ROUGE BLEU,
system nearly always outperformed humans, confirming unsuitability
evaluation measures.
models approach description generation problem cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014;
Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) able use measures information retrieval, median rank (mRank), precision k (S@k),
recall k (R@k) evaluate descriptions return, addition text-similarity
measures reported above. evaluation paradigm first proposed Hodosh et al.,
reported high correlation human judgments imagesentence based ranking
evaluations.
Table 3, summarize image description approaches discussed survey,
list datasets evaluation measures employed approaches.
seen recent systems (starting 2014) converged use
large description datasets (Flickr8K/30K, MS COCO) employ evaluation measures
perform well terms correlation human judgments (Meteor, CIDEr). However,
use BLEU, despite limitations, still widespread; use human evaluation
means universal literature.

4. Future Directions
survey demonstrates, CV NLP communities witnessed upsurge
interest automatic image description systems. help recent advances deep
learning models images text, substantial improvements quality automatically generated descriptions registered. Nevertheless, series challenges
image description research remain. following, discuss future directions
line research likely benefit from.
16. Source http://mscoco.org/dataset/cap2015
17. Calculated collecting additional human-written description, compared
reference descriptions.

430

fiAutomatic Description Generation Images: Survey

Reference

Approach

Farhadi et al. (2010)
Kulkarni et al. (2011)
Li et al. (2011)
Ordonez et al. (2011)
Yang et al. (2011)

MultRetrieval
Generation
Generation
VisRetrieval
Generation

Datasets

Gupta et al. (2012)
Kuznetsova et al. (2012)
Mitchell et al. (2012)
Elliott Keller (2013)
Hodosh et al. (2013)

Pascal1K
Pascal1K
Pascal1K
SBU1M
IAPR,
Flickr8K/30K,
COCO
VisRetrieval
Pascal1K, IAPR
VisRetrieval
SBU1M
Generation
Pascal1K
Generation
VLT2K
MultRetrieval Pascal1K, Flickr8K

Gong et al. (2014)
Karpathy et al. (2014)
Kuznetsova et al. (2014)
Mason Charniak (2014)
Patterson et al. (2014)
Socher et al. (2014)
Verma Jawahar (2014)
Yatskar et al. (2014)
Chen Zitnick (2015)

MultRetrieval
MultRetrieval
Generation
VisRetrieval
VisRetrieval
MultRetrieval
MultRetrieval
Generation
MultRetrieval

Donahue et al. (2015)

MultRetrieval

Devlin et al. (2015)
Elliott de Vries (2015)
Fang et al. (2015)

VisRetrieval
Generation
Generation

Jia et al. (2015)
Generation
Karpathy Fei-Fei (2015) MultRetrieval
Kiros et al. (2015)
Lebret et al. (2015)
Lin et al. (2015)
Mao et al. (2015a)
Ortiz et al. (2015)
Pinheiro et al. (2015)
Ushiku et al. (2015)

MultRetrieval
MultRetrieval
Generation
MultRetrieval
Generation
MultRetrieval
Generation

Vinyals et al. (2015)

MultRetrieval

Xu et al. (2015)
Yagcioglu et al. (2015)

MultRetrieval
VisRetrieval

Measures
BLEU
Human, BLEU
Human, BLEU

BLEU, ROUGE, Meteor,
CIDEr, R@k
Human, BLEU, ROUGE
Human, BLEU
Human
Human, BLEU
Human, BLEU, ROUGE,
mRank, R@k
SBU1M, Flickr30K
R@k
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr
SBU1M
Human, BLEU, Meteor
SBU1M
Human, BLEU
SBU1M
BLEU
Pascal1K
mRank, R@k
IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k
data
Human, BLEU
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr,
mRank, R@k
Flickr30K, COCO
Human, BLEU, mRank,
R@k
COCO
BLEU, Meteor
VLT2K, Pascal1K
BLEU, Meteor
COCO
Human, BLEU, ROUGE,
Meteor, CIDEr
Flickr8K/30K, COCO
BLEU, Meteor
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr,
mRank, R@k
Flickr8K/30K
R@k
Flickr30K, COCO
BLEU, R@k
NYU
ROUGE
IAPR, Flickr30K, COCO BLEU, mRank, R@k
Abstract Scenes
Human, BLEU, Meteor
COCO
BLEU
Pascal1K, IAPR, SBU1M, BLEU
COCO
Pascal1K,
SBU1M, BLEU, Meteor, CIDEr,
Flickr8K/30K
mRank, R@k
Flickr8K/30K, COCO
BLEU, Meteor
Flickr8K/30K, COCO
Human, BLEU, Meteor,
CIDEr

Table 3: overview approaches, datasets, evaluation measures reviewed
survey organised chronological order.

431

fiBernardi et al.

4.1 Datasets
earliest work image description used relatively small datasets (Farhadi et al., 2010;
Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, introduction Flickr30K,
MS COCO large datasets enabled training complex models
neural networks. Still, area likely benefit larger diversified datasets
share common, unified, comprehensive vocabulary. Vinyals et al. (2015) argue
collection process quality descriptions datasets affect performance
significantly, make transfer learning datasets effective expected.
show learning model MS COCO applying datasets collected
different settings SBU1M Captions Pascal1K, leads degradation BLEU
performance. surprising, since MS COCO offers much larger amount training
data Pascal1K. Vinyals et al. put it, largely due differences
vocabulary quality descriptions. learning approaches likely suffer
situations. Collecting larger comprehensive datasets developing
generic approaches capable generating naturalistic descriptions across domains
therefore open challenge.
supervised algorithms likely take advantage carefully collected large
datasets, lowering amount supervision exchange access larger unsupervised
data interesting avenue future research. Leveraging unsupervised data
building richer representations description models another open research challenge
context.
4.2 Measures
Designing automatic measures mimic human judgments evaluating suitability image descriptions perhaps urgent need area image description
(Elliott & Keller, 2014). need dramatically observed latest evaluation results MS COCO Challenge. According existing measures, including latest CIDEr
measure (Vedantam et al., 2015), several automatic methods outperform human upper bound (this upper bound indicates similar human descriptions other).
counterintuitive nature result confirmed fact human judgments used evaluation, output even best system judged worse
human generated description time (Fang et al., 2015). However, since
conducting human judgment experiments costly, major need improved automatic measures highly correlated human judgments. Figure 7 plots
Epanechnikov probability density estimate (a non-parametric optimal estimator) BLEU,
Meteor, ROUGE, CIDEr scores per subjective judgment Flickr8K dataset. human judgments obtained human experts (Hodosh et al., 2013). BLEU
confirmed unable sufficiently discriminate lowest three human
judgments, Meteor CIDEr show signs moving towards useful separation.
4.3 Diversity Originality
Current algorithms often rely direct representations descriptions see training time, making descriptions generated test time similar. results many
432

fiAutomatic Description Generation Images: Survey

Human Judgement

Human Judgement
0.10

Perfect
Minor mistakes
aspects
relation

0.00

0.00

0.02

0.05

0.04

0.10

0.06

0.08

0.15

Perfect
Minor mistakes
aspects
relation

0

20

40

60

80

100

0

20

BLEU

40

60

80

100

Meteor
Human Judgement
Perfect
Minor mistakes
aspects
relation

0

0

1

100

2

3

200

4

5

300

6

Perfect
Minor mistakes
aspects
relation

400

7

Human Judgement

0.0

0.2

0.4

0.6

0.8

0.00

1.0

0.05

0.10

0.15

0.20

CIDEr

ROUGE

Figure 7: Probability density estimates BLEU, Meteor, ROUGE, CIDEr scores
human judgments Flickr8K dataset. y-axis shows probability density,
x-axis score computed measure.

433

fiBernardi et al.

repetitions limits diversity generated descriptions, making difficult reach
human levels performance. situation demonstrated Devlin et al. (2015),
show best model able generate 47.0% unique descriptions. Systems generate diverse original descriptions repeat already
seen, infer underlying semantics therefore remain open challenge. Chen
Zitnick (2015) related approaches take step towards addressing limitations
coupling description visual representation generation.
Jas Parikh (2015) introduces notion image specificity, arguing domain image descriptions uniform, certain images specific others.
Descriptions non-specific images tend vary lot people tend describe nonspecific scene different aspects. notion effects description systems
measures investigated detail.

4.4 Tasks
Another open challenge visual question-answering (VQA). natural language
question-answering based text significant goal NLP research long
time (e.g., Liang, Jordan, & Klein, 2012; Fader, Zettlemoyer, & Etzioni, 2013; Richardson, Burges, & Renshaw, 2013; Fader, Zettlemoyer, & Etzioni, 2014), answering questions
images task recently emerged. Towards achieving goal, Malinowski
Fritz (2014a) propose Bayesian framework connects natural language questionanswering visual information extracted image parts. recently, image
question answering methods based neural networks developed (Gao, Mao,
Zhou, Huang, & Yuille, 2015; Ren, Kiros, & Zemel, 2015; Malinowski, Rohrbach, & Fritz,
2015; Ma, Lu, & Li, 2016). Following effort, several datasets task
released: DAQUAR (Malinowski & Fritz, 2014a) compiled scene depth images
mainly focuses questions type, quantity color objects; COCOQA (Ren et al., 2015) constructed converting image descriptions VQA format
subset images MS COCO dataset; Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al., 2015), Visual Madlibs dataset (Yu, Park,
Berg, & Berg, 2015) VQA dataset (Antol et al., 2015), built images
MS COCO, time question-answer pairs collected via human annotators
freestyle paradigm. Research emerging field likely flourish near future. ultimate goal VQA build systems pass (recently developed)
Visual Turing Test able answer arbitrary questions images
precision human observer (Malinowski & Fritz, 2014b; Geman, Geman, Hallonquist, &
Younes, 2015).
multilingual repositories image description interesting direction
explore. Currently, among available benchmark datasets, IAPR-TC12
dataset (Grubinger et al., 2006) multilingual descriptions (in English German).
Future work investigate whether transferring multimodal features monolingual description models results improved descriptions compared monolingual baselines.
434

fiAutomatic Description Generation Images: Survey

would interesting study different models new tasks multilingual multimodal
setting using larger syntactically diverse multilingual description corpora.18
Overall, image understanding ultimate goal computer vision natural language generation one ultimate goals NLP. Image description
goals interconnected topic therefore likely benefit individual advances
two fields.

5. Conclusions
survey, discuss recent advances automatic image description closely related
problems. review analyze large body existing work highlighting common
characteristics differences existing research. particular, categorize
related work three groups: (i) direct description generation images, (i) retrieval
images visual space, (iii) retrieval images multimodal (joint visual
linguistic) space. addition, provided brief review existing corpora
automatic evaluation measures, discussed future directions vision language
research.
Compared traditional keyword-based image annotation (using object recognition,
attribute detection, scene labeling, etc.), automatic image description systems produce
human-like explanations visual content, providing complete picture scene.
Advancements field could lead intelligent artificial vision systems,
make inferences scenes generated grounded image descriptions
therefore interact environments natural manner. could
direct impact technological applications visually impaired people
benefit accessible interfaces.
Despite remarkable increase number image description systems recent
years, experimental results suggest system performance still falls short human performance. similar challenge lies automatic evaluation systems using reference
descriptions. measures tools currently use sufficiently highly correlated human judgments, indicating need measures deal
complexity image description problem adequately.

Acknowledgments
thank anonymous reviewers useful comments. work partially supported European Commission ICT COST Action iV&L Net: European Network Integrating Vision Language (IC1037). RC, AE, EE, NIC
funded Scientific Technological Research Council Turkey (TUBITAK) research grant 113E116. FK would acknowledge ERC funding starting grant
203427 Synchronous Linguistic Visual Processing. DE supported ERCIM
ABCDE Fellowship 2014-23.
18. Multimodal Translation Shared Task 2016 Workshop Machine Translation use
English German translated version Flickr30K corpora. See http://www.statmt.org/wmt16/
multimodal-task.html details.

435

fiBernardi et al.

References
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015).
Vqa: Visual question answering. International Conference Computer Vision.
Banerjee, S., & Lavie, A. (2005). METEOR: Automatic Metric MT Evaluation
Improved Correlation Human Judgments. Annual Meeting Association Computational Linguistics Workshop Intrinsic Extrinsic Evaluation
Measures MT and/or Summarization.
Berg, T. L., Berg, A. C., & Shih, J. (2010). Automatic attribute discovery characterization noisy web data. European Conference Computer Vision.
Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return devil
details: Delving deep convolutional nets. British Machine Vision Conference.
Chen, J., Kuznetsova, P., Warren, D., & Choi, Y. (2015). Deja image-captions: corpus
expressive descriptions repetition. North American Chapter Association
Computational Linguistics.
Chen, X., & Zitnick, C. L. (2015). Minds eye: recurrent visual representation image
caption generation. IEEE Conference Computer Vision Pattern Recognition.
Dale, R., & White, M. E. (Eds.). (2007). Workshop Shared Tasks Comparative
Evaluation Natural Language Generation: Position Papers.
Denkowski, M., & Lavie, A. (2014). Meteor Universal: Language Specific Translation Evaluation Target Language. Conference European Chapter Association Computational Linguistics Workshop Statistical Machine Translation.
Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., & Mitchell, M.
(2015). Language Models Image Captioning: Quirks Works.
Annual Meeting Association Computational Linguistics.
Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko,
K., & Darrell, T. (2015). Long-term recurrent convolutional networks visual recognition description. IEEE Conference Computer Vision Pattern Recognition.
Elliott, D., & de Vries, A. P. (2015). Describing images using inferred visual dependency
representations. Annual Meeting Association Computational Linguistics.
Elliott, D., & Keller, F. (2013). Image Description using Visual Dependency Representations. Conference Empirical Methods Natural Language Processing.
Elliott, D., & Keller, F. (2014). Comparing Automatic Evaluation Measures Image
Description. Annual Meeting Association Computational Linguistics.
Elliott, D., Lavrenko, V., & Keller, F. (2014). Query-by-Example Image Retrieval using
Visual Dependency Representations. International Conference Computational
Linguistics.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010).
PASCAL Visual Object Classes (VOC) Challenge. International Journal Computer
Vision, 88 (2), 303338.
436

fiAutomatic Description Generation Images: Survey

Fader, A., Zettlemoyer, L., & Etzioni, O. (2013). Paraphrase-driven learning open question answering. Annual Meeting Association Computational Linguistics.
Fader, A., Zettlemoyer, L., & Etzioni, O. (2014). Open question answering curated
extracted knowledge bases. ACM SIGKDD Conference Knowledge Discovery
Data Mining.
Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollar, P., Gao, J., He, X.,
Mitchell, M., Platt, J., Zitnick, C. L., & Zweig, G. (2015). captions visual
concepts back. IEEE Conference Computer Vision Pattern Recognition.
Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells story: Generating sentences images.
European Conference Computer Vision.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection discriminatively trained part-based models. IEEE Transactions Pattern
Analysis Machine Intelligence, 32 (9), 16271645.
Feng, Y., & Lapata, M. (2008). Automatic Image Annotation Using Auxiliary Text Information. Annual Meeting Association Computational Linguistics.
Feng, Y., & Lapata, M. (2013). Automatic caption generation news images. IEEE
Transactions Pattern Analysis Machine Intelligence, 35 (4), 797812.
Ferraro, F., Mostafazadeh, N., Huang, T., Vanderwende, L., Devlin, J., Galley, M., &
Mitchell, M. (2015). survey current datasets vision language research.
Conference Empirical Methods Natural Language Processing.
Gao, H., Mao, J., Zhou, J., Huang, Z., & Yuille, A. (2015). talking machine?
dataset methods multilingual image question answering. International
Conference Learning Representations.
Geman, D., Geman, S., Hallonquist, N., & Younes, L. (2015). Visual turing test computer
vision systems. Proceedings National Academy Sciences, 112 (12), 36183623.
Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies accurate object detection semantic segmentation. IEEE Conference Computer
Vision Pattern Recognition.
Gong, Y., Wang, L., Hodosh, M., Hockenmaier, J., & Lazebnik, S. (2014). Improving ImageSentence Embeddings Using Large Weakly Annotated Photo Collections. European
Conference Computer Vision.
Grubinger, M., Clough, P., Muller, H., & Deselaers, T. (2006). IAPR TC-12 benchmark:
new evaluation resource visual information systems. International Conference
Language Resources Evaluation.
Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., & Saenko, K. (2013). Youtube2text: Recognizing describing arbitrary
activities using semantic hierarchies zero-shot recognition. International Conference Computer Vision.
Gupta, A., Verma, Y., & Jawahar, C. V. (2012). Choosing linguistics vision describe
images. AAAI Conference Artificial Intelligence.
437

fiBernardi et al.

Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis:
overview application learning methods. Neural Computation, 16 (12),
26392664.
Hodosh, M., & Hockenmaier, J. (2013). Sentence-based image description scalable,
explicit models. IEEE Conference Computer Vision Pattern Recognition
Workshops.
Hodosh, M., Young, P., & Hockenmaier, J. (2013). Framing Image Description Ranking Task: Data, Models Evaluation Metrics. Journal Artificial Intelligence
Research, 47, 853899.
Hotelling, H. (1936). Relations two sets variates. Biometrika, 0, 321377.
Jaimes, A., & Chang, S.-F. (2000). conceptual framework indexing visual information
multiple levels. IST SPIE Internet Imaging.
Jas, M., & Parikh, D. (2015). Image specificity. IEEE Conference Computer Vision
Pattern Recognition.
Jia, X., Gavves, E., Fernando, B., & Tuytelaars, T. (2015). Guiding long-short term
memory model image caption generation. International Conference Computer Vision.
Johnson, J., Krishna, R., Stark, M., Li, L.-J., Shamma, D. A., Bernstein, M., & Fei-Fei, L.
(2015). Image retrieval using scene graphs. IEEE Conference Computer Vision
Pattern Recognition.
Karpathy, A., & Fei-Fei, L. (2015). Deep visual-semantic alignments generating image
descriptions. IEEE Conference Computer Vision Pattern Recognition.
Karpathy, A., Joulin, A., & Fei-Fei, L. (2014). Deep Fragment Embeddings Bidirectional
Image Sentence Mapping. Advances Neural Information Processing Systems.
Khan, M. U. G., Zhang, L., & Gotoh, Y. (2011). Towards coherent natural language description video streams. International Conference Computer Vision Workshops.
Kiros, R., Salakhutdinov, R., & Zemel, R. S. (2015). Unifying visual-semantic embeddings
multimodal neural language models. Advances Neural Information Processing Systems Deep Learning Workshop.
Krishnamoorthy, N., Malkarnenkar, G., Mooney, R., Saenko, K., & Guadarrama, S. (2013).
Generating Natural-Language Video Descriptions Using Text-Mined Knowledge.
Annual Conference North American Chapter Association Computational Linguistics: Human Language Technologies.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011). Baby
talk: Understanding generating simple image descriptions. IEEE Conference
Computer Vision Pattern Recognition.
Kuznetsova, P., Ordonez, V., Berg, A. C., Berg, T. L., & Choi, Y. (2012). Collective
Generation Natural Image Descriptions. Annual Meeting Association
Computational Linguistics.
438

fiAutomatic Description Generation Images: Survey

Kuznetsova, P., Ordonezz, V., Berg, T. L., & Choi, Y. (2014). TREETALK: Composition
compression trees image descriptions. Conference Empirical Methods
Natural Language Processing.
Lampert, C. H., Nickisch, H., & Harmeling, S. (2009). Learning detect unseen object
classes between-class attribute transfer. IEEE Conference Computer Vision
Pattern Recognition.
Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags features: Spatial pyramid
matching recognizing natural scene categories. IEEE Conference Computer
Vision Pattern Recognition.
Lebret, R., Pinheiro, P. O., & Collobert, R. (2015). Phrase-based image captioning.
International Conference Machine Learning.
Li, S., Kulkarni, G., Berg, T. L., Berg, A. C., & Choi, Y. (2011). Composing simple image
descriptions using web-scale n-grams. SIGNLL Conference Computational
Natural Language Learning.
Liang, P., Jordan, M. I., & Klein, D. (2012). Learning dependency-based compositional
semantics. Computational Linguistics, 39 (2), 389446.
Lin, C.-Y., & Hovy, E. (2008). Automatic evaluation summaries using n-gram cooccurrence statistics. Annual Conference North American Chapter
Association Computational Linguistics: Human Language Technologies.
Lin, D., Fidler, S., Kong, C., & Urtasun, R. (2015). Generating multi-sentence natural
language descriptions indoor scenes. British Machine Vision Conference.
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., & Zitnick,
C. L. (2014). Microsoft COCO: Common objects context. European Conference
Computer Vision.
Lowe, D. (2004). Distinctive image features scale-invariant keypoints. International
Journal Computer Vision, 60 (4), 91110.
Ma, L., Lu, Z., & Li, H. (2016). Learning answer questions image using convolutional
neural network. AAAI Conference Artificial Intelligence.
Malinowski, M., & Fritz, M. (2014a). multi-world approach question answering
real-world scenes based uncertain input. Advances Neural Information Processing Systems.
Malinowski, M., & Fritz, M. (2014b). Towards visual turing challenge. Advances
Neural Information Processing Systems Workshop Learning Semantics.
Malinowski, M., Rohrbach, M., & Fritz, M. (2015). Ask neurons: neural-based
approach answering questions images. International Conference Computer Vision.
Mao, J., Xu, W., Yang, Y., Wang, J., & Yuille, A. L. (2015a). Deep captioning multimodal recurrent neural networks (m-RNN). International Conference Learning
Representations.
439

fiBernardi et al.

Mao, J., Wei, X., Yang, Y., Wang, J., Huang, Z., & Yuille, A. L. (2015b). Learning
child: Fast novel visual concept learning sentence descriptions images.
International Conference Computer Vision.
Mason, R., & Charniak, E. (2014). Nonparametric Method Data-driven Image Captioning. Annual Meeting Association Computational Linguistics.
Mason, W. A., & Watts, D. J. (2009). Financial incentives performance crowds.
ACM SIGKDD Workshop Human Computation.
Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A. C., Yamaguchi, K.,
Berg, T. L., Stratos, K., Daume, III, H., & III (2012). Midge: generating image
descriptions computer vision detections. Conference European Chapter
Association Computational Linguistics.
Nenkova, A., & Vanderwende, L. (2005). impact frequency summarization. Tech.
rep., Microsoft Research.
Oliva, A., & Torralba, A. (2001). Modeling shape scene: holistic representation
spatial envelope. International Journal Computer Vision, 42 (3), 145175.
Ordonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 million
captioned photographs. Advances Neural Information Processing Systems.
Ortiz, L. M. G., Wolff, C., & Lapata, M. (2015). Learning Interpret Describe
Abstract Scenes. Conference North American Chapter Association
Computational Linguistics.
Panofsky, E. (1939). Studies Iconology. Oxford University Press.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic evaluation machine translation. Annual Meeting Association
Computational Linguistics.
Parikh, D., & Grauman, K. (2011). Relative attributes. International Conference
Computer Vision.
Park, C., & Kim, G. (2015). Expressing image stream sequence natural
sentences. Advances Neural Information Processing Systems.
Patterson, G., Xu, C., Su, H., & Hays, J. (2014). SUN Attribute Database: Beyond Categories Deeper Scene Understanding. International Journal Computer Vision,
108 (1-2), 5981.
Pinheiro, P., Lebret, R., & Collobert, R. (2015). Simple image description generator via
linear phrase-based model. International Conference Learning Representations
Workshop.
Prest, A., Schmid, C., & Ferrari, V. (2012). Weakly supervised learning interactions
humans objects. IEEE Transactions Pattern Analysis Machine
Intelligence, 34 (3), 601614.
Rashtchian, C., Young, P., Hodosh, M., & Hockenmaier, J. (2010). Collecting image annotations using amazons mechanical turk. North American Chapter Association
Computational Linguistics: Human Language Technologies Workshop Creating
Speech Language Data Amazons Mechanical Turk.
440

fiAutomatic Description Generation Images: Survey

Reiter, E., & Belz, A. (2009). investigation validity metrics automatically evaluating natural language generation systems. Computational Linguistics,
35 (4), 529588.
Reiter, E., & Dale, R. (2006). Building Natural Language Generation Systems. Cambridge
University Press.
Ren, M., Kiros, R., & Zemel, R. (2015). Image question answering: visual semantic embedding model new dataset. International Conference Machine Learningt
Deep Learning Workshop.
Richardson, M., Burges, C. J., & Renshaw, E. (2013). MCTest: challenge dataset
open-domain machine comprehension text. Conference Empirical Methods
Natural Language Processing.
Rohrbach, A., Rohrback, M., Tandon, N., & Schiele, B. (2015). dataset movie description. International Conference Computer Vision.
Rohrbach, M., Qiu, W., Titov, I., Thater, S., Pinkal, M., & Schiele, B. (2013). Translating
Video Content Natural Language Descriptions. International Conference
Computer Vision.
Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., & Manning, C. D. (2015). Generating
semantically precise scene graphs textual descriptions improved image retrieval. Conference Empirical Methods Natural Language Processing Vision
Language Workshop.
Shatford, S. (1986). Analyzing subject picture: theoretical approach. Cataloging
& Classification Quarterly, 6, 3962.
Silberman, N., Kohli, P., Hoiem, D., & Fergus, R. (2012). Indoor segmentation support
inference RGBD images. European Conference Computer Vision.
Socher, R., & Fei-Fei, L. (2010). Connecting modalities: Semi-supervised segmentation
annotation im- ages using unaligned text corpora. IEEE Conference
Computer Vision Pattern Recognition.
Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., & Ng, A. (2014). Grounded Compositional Semantics Finding Describing Images Sentences. Transactions
Association Computational Linguistics, 2, 207218.
Sun, C., Gan, C., & Nevatia, R. (2015). Automatic concept discovery parallel text
visual corpora. International Conference Computer Vision.
Thomason, J., Venugopalan, S., Guadarrama, S., Saenko, K., & Mooney, R. (2014). Integrating Language Vision Generate Natural Language Descriptions Videos
Wild. International Conference Computational Linguistics.
Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: large data
set nonparametric object scene recognition. IEEE Transactions Pattern
Analysis Machine Intelligence, 30 (11), 19581970.
Ushiku, Y., Yamaguchi, M., Mukuta, Y., & Harada, T. (2015). Common subspace model
similarity: Phrase learning caption generation images. International
Conference Computer Vision.
441

fiBernardi et al.

Vedantam, R., Lawrence Zitnick, C., & Parikh, D. (2015). Cider: Consensus-based image description evaluation. IEEE Conference Computer Vision Pattern
Recognition.
Verma, Y., & Jawahar, C. V. (2014). Im2Text Text2Im: Associating Images Texts
Cross-Modal Retrieval. British Machine Vision Conference.
Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show tell: neural image
caption generator. IEEE Conference Computer Vision Pattern Recognition.
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R., & Bengio, Y.
(2015). Show, attend tell: Neural image caption generation visual attention.
International Conference Machine Learning.
Yagcioglu, S., Erdem, E., Erdem, A., & Cakici, R. (2015). Distributed Representation
Based Query Expansion Approach Image Captioning. Annual Meeting
Association Computational Linguistics.
Yang, Y., Teo, C. L., Daume, III, H., & Aloimonos, Y. (2011). Corpus-guided sentence generation natural images. Conference Empirical Methods Natural Language
Processing.
Yao, B., & Fei-Fei, L. (2010). Grouplet: structured image representation recognizing
human object interactions. IEEE Conference Computer Vision Pattern
Recognition.
Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., & Courville, A. (2015).
Describing videos exploiting temporal structure. International Conference
Computer Vision.
Yatskar, M., Galley, M., Vanderwende, L., & Zettlemoyer, L. (2014). See Evil, Say
Evil: Description Generation Densely Labeled Images. Joint Conference
Lexical Computation Semantics.
Young, P., Lai, A., Hodosh, M., & Hockenmaier, J. (2014). image descriptions visual
denotations: New similarity metrics semantic inference event descriptions.
Transactions Association Computational Linguistics, 2, 6778.
Yu, L., Park, E., Berg, A. C., & Berg, T. L. (2015). Visual madlibs: Fill blank
description generation question answering. International Conference Computer Vision.
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.
(2015). Aligning books movies: Towards story-like visual explanations watching
movies reading books. International Conference Computer Vision.
Zitnick, C. L., Parikh, D., & Vanderwende, L. (2013). Learning visual interpretation
sentences. International Conference Computer Vision.
Zitnick, C. L., & Parikh, D. (2013). Bringing semantics focus using visual abstraction.
IEEE Conference Computer Vision Pattern Recognition.

442


