Journal Artificial Intelligence Research 30 (2007) 1-50

Submitted 11/06; published 9/07

Learning Semantic Definitions Online Information Sources
Mark James Carman
Craig A. Knoblock

mark@bradipo.net
knoblock@isi.edu

University Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292

Abstract
Internet contains large number information sources providing many types
data weather forecasts travel deals financial information. sources
accessed via Web-forms, Web Services, RSS feeds on. order make automated
use sources, need model semantically, writing semantic descriptions
Web Services tedious error prone. paper investigate problem
automatically generating models. introduce framework learning Datalog
definitions Web sources. order learn definitions, system actively invokes
sources compares data produce known sources information.
performs inductive logic search space plausible source definitions
order learn best possible semantic model new source. paper
perform empirical evaluation system using real-world Web sources. evaluation
demonstrates effectiveness approach, showing automatically learn
complex models real sources reasonable time. compare system
complex schema matching system, showing approach handle kinds
problems tackled latter.

1. Introduction
Recent years seen explosion quantity variety information available
online. One find shopping data (prices availability goods), geospatial data
(weather forecasts, housing information), travel data (flight pricing status), financial
data (exchange rates stock quotes), scratches surface
available. aim work make use vast store information.
amount information increased, reuse across Web portals
applications. Developers realised importance managing content separately
presentation, leading development XML self-describing data format.
Content XML far easier manipulate HTML, simplifying integration across
different sources. Standards emerged providing programmatic access data
(like SOAP REST) developers easily build programs (called Mash-Ups)
combine content different sites real-time. Many portals provide access
data even provide syntactic definitions (in WSDL) input output
data sources expect. Missing, however, semantic descriptions source
does, required order support automated data integration.
c
2007
AI Access Foundation. rights reserved.

fiCarman & Knoblock

1.1 Structured Querying
Given structured sources available, would combine data dynamically
answer specific user requests (as opposed statically case Mash-Ups). Dynamic
data requests expressed queries shown below. queries may
require access multiple (publicly available) data sources combine information ways
envisaged producers.
1. Tourism: Get prices availability three star hotels within 100 kilometers
Trento, Italy lie within 1 kilometer ski resort 1 meter snow.
2. Transportation: Determine time need leave work order catch bus
airport pick brother arriving Qantas flight 205.
3. Disaster Prevention: Find phone numbers people living within 1 mile
coast 200 feet elevation.
clear even small set examples powerful ability combine
data disparate sources be. order give queries automated system,
must first express formally query language SQL Datalog (Ullman,
1989). Datalog first query might look follows:
q(hotel, price) :accommodation(hotel, 3*, address), available(hotel, today, price),
distance(address, hTrento,Italyi, dist1), dist1 < 100km,
skiResort(resort, loc1), distance(address, loc1, dist2),
dist2 < 1km, snowCondiditions(resort, today, height), height > 1m.
expression states hotel price pairs generated looking three star hotels
relational table called accommodation, checking price tomorrow night
table called available. address hotel used calculate distance Trento,
must less 100 kilometers. query checks skiResort
within 1 kilometer hotel, snowConditions today show 1
meter snow.
1.2 Mediators
system capable generating plan answer query called Information
Mediator (Wiederhold, 1992). order generate plan, mediators look sources
relevant query. case, relevant sources might be:
1. Italian Tourism Website: find hotels near Trento, Italy.
2. Ski Search Engine: find ski resorts near hotel.
3. Weather Provider: find much snow fallen ski resort.
mediator know sources relevant, needs know information
source provides. XML defines syntax (formatting) used source, semantics
(intended meaning) information source provides must defined separately.
done using Local-as-View (LAV) source definitions Datalog (Levy, 2000).
Essentially, source definitions describe queries given mediator, return
data source provides. Example definitions shown below. first states
2

fiLearning Semantic Definitions Information Sources Internet

source hotelSearch takes four values input (inputs prefixed $-symbol),
returns list hotels lie within given distance input location.
hotel returns address well price room given date.
(Note source provides information hotels Italy.)
hotelSearch($location, $distance, $rating, $date, hotel, address, price) :country(location, Italy), accommodation(hotel, rating, address),
available(hotel, date, price), distance(address, location, dist1),
dist1 < distance.
findSkiResorts($address, $distance, resort, location) :skiResort(resort, location), distance(address, location, dist1),
dist1 < distance.
getSkiConditions($resort, $date, height) :snowCondiditions(resort, date, height).
order generate plan answering query, mediator performs process called
query reformulation (Levy, 2000), whereby transforms query new query
(in terms of) relevant information sources.1 (A source relevant refers
relations query.) resulting plan case shown below.
q(hotel, price) :hotelSearch(hTrento,Italyi, 100km, 3*, today, hotel, address, price),
findSkiResorts(address, 1km, resort, location),
getSkiConditions(resort, today, height), height > 1m.
work, questions interest are: definitions information
sources come precisely, happens want add new sources
system? possible generate source definitions automatically?
1.3 Discovering New Sources
example above, mediator knows set relevant sources use successfully
answer query. instead, one sources missing doesnt desired
scope (e.g. getSkiConditions doesnt provide data Trento), mediator first needs
discover source providing information. number variety information
sources increase, undoubtedly rely automated methods discovering
annotating semantic descriptions. order discover relevant sources, system
might inspect service registry2 (such defined UDDI), perform keywordbased search Web index (such Google del.icio.us). research community
looked problem discovering relevant services, developing techniques classifying
services different domains (such weather flights) using service metadata (He &
Kushmerick, 2003) clustering similar services together improve keyword-based search
(Dong, Halevy, Madhavan, Nemes, & Zhang, 2004). techniques, although useful,
sufficient automating service integration.
1. complexity query reformulation known exponential, although efficient algorithms
performing exist (Pottinger & Halevy, 2001).
2. Note technically, service different source. service interface providing access
multiple operations, may provide information. operation affect state
world (e.g. charging somebodys credit card), call information source.
paper, however, use term service refer information sources.

3

fiCarman & Knoblock

1.4 Labeling Service Inputs Outputs
relevant service discovered, problem shifts modeling semantically. Modeling sources hand laborious, automating process makes sense. Since different
services often provide similar overlapping data, possible use knowledge
previously modeled services learn descriptions new ones.
first step modeling source determine type data requires
input produces output. done assigning semantic types (like zipcode,
telephone number, temperature, on) attributes service. Semantic types
restrict possible values attribute subset corresponding primitive type.
research community investigated automating assignment process viewing
classification problem (He & Kushmerick, 2003). system, Kushmerick
trained Support Vector Machine (SVM) metadata describing different sources.
system, given source following:
getWeather($zip, temp)
uses labels getWeather, zip temp (and available metadata) assign
types input output attributes, e.g.: zip zipcode, temp temperature. Note
additional metadata often useful distinguishing possible assignments. (If,
example, name operation listEmployees, temp may referred
temporary employee rather temperature.)
subsequent work, researchers developed comprehensive system used
metadata output data classify service attributes (Lerman, Plangprasopchok, &
Knoblock, 2006). system, Logistic Regression based classifier first assigns semantic types input parameters. Examples input types used invoke
service, output given pattern-language based classifier, assigns types
output parameters. authors argue classification based data
metadata far accurate based metadata alone. Using example,
easy see why. Consider following tuples produced getWeather source:
h90292, 25 Ci, h10274, 15 Ci, h60610, 18 Ci, ...
Given data, classifier certain temp really refers temperature,
indeed even assign specific type, temperatureC (in Celsius).
problem determining semantic types services attributes
interesting, room improvement current techniques, assume
purposes work already solved.
1.5 Generating Definition
know parameter types, invoke service, still unable
make use data returns. that, need know output attributes
relate input (i.e. definition source). example, getWeather service
need know whether temperature returned current temperature,
predicted high temperature tomorrow average temperature time year.
relationships described following definitions:
getWeather($zip, temp) :- currentTemp(zip, temp).
getWeather($zip, temp) :- forecast(zip, tomorrow, temp).
getWeather($zip, temp) :- averageTemp(zip, today, temp).
4

fiLearning Semantic Definitions Information Sources Internet

relations used definitions would defined domain ontology (or schema).
paper describe system capable learning definitions
correct. system leverages knows domain, i.e. domain
ontology set known information sources, learn know, namely
relationship attributes newly discovered source.
1.6 Outline
paper presents comprehensive treatment methods learning semantic descriptions Web information sources. extends previous work subject (Carman &
Knoblock, 2007) presenting detailed descriptions methods enumerating
search space evaluating individual candidate definitions. provide additional
details regarding evaluation methodology results generated.
paper structured follows. start example motivate source
induction problem formulate problem concisely. discuss approach
learning definitions sources terms known sources information (section
3). give details search procedure generating candidate definitions (section 4)
evaluation procedure scoring candidates search (section 5).
describe extensions basic algorithm (section 6) discussing evaluation setup
experiments (section 7), demonstrate capabilities system. Finally,
contrast approach prior work.

2. Problem
describe detail problem learning definitions newly discovered services.
start concrete example meant learning source definition.
example four types data (semantic types), namely: zipcodes, distances, latitudes
longitudes. three known sources information. sources
definition Datalog shown below. first service, aptly named source1, takes
zipcode returns latitude longitude coordinates centroid. second
service calculates great circle distance (the shortest distance earths surface)
two pairs coordinates, third converts distance kilometres
miles multiplying input constant 1.609.
source1($zip, lat, long) :- centroid(zip, lat, long).
source2($lat1, $long1, $lat2, $long2, dist) :greatCircleDist(lat1, long1, lat2, long2, dist).
source3($dist1, dist2) :- multiply(dist1, 1.609, dist2).
goal example learn definition newly discovered service, called source4.
service takes two zipcodes input returns distance value output:
source4($zip1, $zip2, dist)
system describe uses type signature (input output type information)
search appropriate definition source. definition discovered case
might following conjunction calls individual sources:
source4($zip1, $zip2, dist):source1($zip1, lat1, long1), source1($zip2, lat2, long2),
source2($lat1, $long1, $lat2, $long2, dist2), source3($dist2, dist).
5

fiCarman & Knoblock

definition states sources output distance calculated input zipcodes,
giving zipcodes source1, taking resulting coordinates calculating
distance using source2, converting distance miles using
source3. test whether definition correct, system must invoke new
source definition see values generated agree other. following
table shows test:
$zip1
80210
60601
10005

$zip2
90266
15201
35555

dist (actual)
842.37
410.31
899.50

dist (predicted)
843.65
410.83
899.21

table, input zipcodes selected randomly set examples,
output source definition shown side side. Since output
values quite similar, system seen sufficient number examples,
confident found correct semantic definition source.
definition given terms source relations, could
written terms domain relations (the relations used define sources 1 3).
convert definition form, one simply needs replace source relation
definition follows:
source4($zip1, $zip2, dist):centroid(zip1, lat1, long1), centroid(zip2, lat2, long2),
greatCircleDist(lat1, long1, lat2, long2, dist2), multiply(dist1, 1.609, dist2).
Written way, new semantic definition makes sense intuitive level: source
simply calculating distance miles centroids two zipcodes.
2.1 Problem Formulation
given example Source Definition Induction Problem, describe
problem formally, so, introduce concepts notation. (We
note focus paper learning definitions information-providing,
opposed world-altering services.)
domain semantic data-type t, denoted D[t], (possibly infinite) set
constant values {c1 , c2 , ...}, constitute set values variables
type. example D[zipcode] = {90210, 90292, ...}
attribute pair hlabel, semantic data-typei, e.g. hzip1, zipcodei. type
attribute denoted type(a) corresponding domain D[type(a)] abbreviated
D[a].
scheme ordered (finite) set attributes ha1 , ..., unique labels,
n referred arity scheme. example scheme might hzip1 :
zipcode, zip2 : zipcode, dist : distancei. domain scheme A, denoted
D[A], Cartesian product domains attributes scheme {D[a1 ]
... D[an ]}, ai A.
tuple scheme element set D[A]. tuple represented
set name-value pairs, {zip1 = 90210, zip2 = 90292, dist = 8.15}
6

fiLearning Semantic Definitions Information Sources Internet

relation named scheme, airDistance(zip1, zip2, dist). Multiple relations may share scheme.
extension relation r, denoted E[r], subset tuples D[r]. example, E[airDistance] might table containing distance zipcodes
California. (Note extension relation may contain distinct tuples.)
database instance set relations R, denoted I[R], set extensions
{E[r1 ], ..., E[rn ]}, one relation r R.
query language L formal language constructing queries set relations.
denote set queries written using language L set
relations R returning tuples conforming scheme LR,A .
result set produced execution query q LR,A database instance
I[R] denoted EI [q].
source relation s, binding pattern s, distinguishes input
attributes output attributes. (The output attributes source denoted
complement binding pattern3 , sc s\s .)
view definition source query vs written query language LR,s .
Source Definition Induction Problem defined tuple:
hT, R, L, S, V,
set semantic data-types, R set relations, L query language,
set known sources, V set view definitions (one known source),
new source (also referred target).
semantic type must provided set examples values Et D[t].
(We require entire set D[t], domain many types may partially
unknown large enumerated.) addition, predicate eqt (t, t) available
checking equality values semantic type handle case multiple
serialisations variable represent value.
relation r R referred global relation domain predicate
extension virtual, meaning extension generated inspecting every
relevant data source. set relations R may include interpreted predicates,
, whose extension defined virtual.
language L used constructing queries could query language including
SQL XQuery (the XML Query Language). paper use form Datalog.
source extension E[s] complete set tuples
produced source (at given moment time). require corresponding view
definition vs V (written LR,s ) consistent source, that: E[s] EI [vs ],
(where I[R] current virtual database instance global relations). Note
require equivalence, sources may provide incomplete data.
view definition source modeled unknown. solution
Source Definition Induction Problem view definition v LR,s source
E[s ] EI [v ], view definition v 0 LR,s better describes
(provides tighter definition for) source , i.e.:
v 0 LR,s s.t. E[s ] EI [v 0 ] |EI [v 0 ]| < |EI [v ]|
3. \-symbol denotes set difference.

7

fiCarman & Knoblock

Given limited available computation bandwidth, note may possible
guarantee optimality condition holds particular solution; thus paper
simply strive find best solution possible.
2.2 Implicit Assumptions
number assumptions implicit problem formulation. first
exists system capable discovering new sources importantly classifying (to
good accuracy) semantic types input output. Systems capable
discussed section 1.4.
second assumption representation source relational
view definition. sources Internet provide tree structured XML data. may
always obvious best flatten data set relational tuples,
preserving intended meaning data. Consider travel booking site returns
set flight options ticket number, price list flight segments
constitute itinerary. One possibility converting data set tuples would
break ticket individual flight segment tuples (thereby obscuring
relationship price number flight segments). Another would create
one long tuple ticket room number flight segments (thereby
creating tuples many null values). case, obvious which, either,
options preferred. online data sources can, however, modeled quite
naturally relational sources; first tackling relational problem, develop
techniques later applied difficult semi-structured case.
third assumption set domain relations suffices describing source
modeled. instance, consider case domain model contains relations
describing financial data, new source provides weather forecasts. Obviously,
system would unable find adequate description behavior source
would learn model it. practical perspective, limitation big
problem, since user request data (write queries mediator) using relations
domain model anyway. (Thus source cannot described using
relations would needed answer user requests.) words, onus
domain modeler model sufficient relations able describe types
queries user able pose system consequently, types sources
available. said, interesting avenue future research would
investigate problem automating (at least part) process expanding scope
domain model (by adding attributes relations, inventing new ones), based
types sources discovered.
2.3 Problem Discussion
number questions arise problem formulation, first domain
model comes from. principle, set semantic types relations could come
many places. could taken standard data models different domains,
might simplest model possible aptly describes set known sources.
domain model may evolve time sources discovered appropriate
model found. Somewhat related question specific semantic types
8

fiLearning Semantic Definitions Information Sources Internet

ought be. example, sufficient one semantic type distance
one distinguish distance meters distance feet? Generally speaking,
semantic type created attribute syntactically dissimilar
attributes. example, phone number zipcode different syntax,
thus operations accept one types input unlikely accept other.
practice, one might create new semantic type whenever trained classifier recognise
type based syntax alone. general, semantic types are,
harder job system classifying attributes, easier job system
tasked learning definition source.
Another question considered definitions known sources come
from. Initially definitions would need written hand. system learns
definitions new sources, would added set known sources, making
possible learn ever complicated definitions.
order system learn definition new source, must able invoke
source thus needs examples input types. representative set
examples available, efficient accurate learning process be. initial
set examples need provided domain modeler. Then, system learns
time, generate large number examples different semantic types (as output
various sources), retained future use.
Information Integration research reached point mediator technology4
becoming mature practical. need involve human writing source
definitions is, however, Achilles Heel systems. gains flexibility come
ability dynamically reformulate user queries often partially offset
time skill required write definitions incorporating new sources. Thus system
capable learning definitions automatically could greatly enhance viability mediator
technology. motivation alone seems sufficient pursuing problem.

3. Approach
approach take learning semantic models information sources Web
twofold. Firstly, choose model sources using powerful language conjunctive
queries. Secondly, leverage set known sources order learn definition
new one. section discuss aspects detail.
3.1 Modeling Language
source definition language L hypothesis language new definitions
need learnt. often case machine learning, faced trade-off
respect expressiveness language. hypothesis language simple,
may able model real services using it. hand, language
overly complex, space possible hypotheses large learning
feasible. language choose conjunctive queries Datalog,
4. Influential Information Integration systems include TSIMMIS (Garcia-Molina, Hammer, Ireland, Papakonstantinou, Ullman, & Widom, 1995), SIMS (Arens, Knoblock, & Shen, 1996), InfoMaster (Duschka,
1997), Ariadne (Knoblock, Minton, Ambite, Ashish, Muslea, Philpot, & Tejada, 2001).

9

fiCarman & Knoblock

highly expressive relational query language. section argue less expressive
language sufficient purposes.
Researchers interested problem assigning semantics Web Services (He &
Kushmerick, 2003) investigated problem using Machine Learning techniques
classify services (based metadata characteristics) different semantic domains,
weather flights, operations provide different classes operation,
weatherForecast flightStatus. relational perspective, consider
different classes operations relations. instance, consider definition below:
source($zip, temp) :- weatherForecast(zip, tomorrow, temp).
source provides weather data selecting tuples relation called weatherForecast,
desired zipcode date equal tomorrow. query referred
select-project query evaluation performed using relational operators
selection projection. far good, able use simple classifier learn
simple definition source. limitation imposed restricted (select-project)
modeling language becomes obvious, however, consider slightly complicated
sources. Consider source provides temperature Fahrenheit well Celsius.
order model source using select-project query, would require
weatherForecast relation extended new attribute follows:
source($zip, tempC, tempF):- weatherForecast(zip, tomorrow, tempC, tempF).
attributes could conceivably returned weather forecast operation
(such dewpoint, humidity, temperature Kelvin, latitude, etc.), longer relation
need cover all. Better, case, would introduce second
relation convertCtoF makes explicit relationship temperature values.
If, addition, source limits output zipcodes California, reasonable definition
source might be:
source($zip, tempC, tempF):weatherForecast(zip, tomorrow, tempC), convertCtoF(tempC, tempF),
state(zip, California).
definition longer expressed language select-project queries,
involves multiple relations joins across them. Thus simple example, see
modeling services using simple select-project queries sufficient purposes.
need select-project-join queries, referred conjunctive queries.5 reader
already introduced examples conjunctive queries throughout previous
sections. Conjunctive queries form subset logical query language Datalog
described formally follows:
conjunctive query set relations R expression form:
q(X0 ) :- r1 (X1 ), r2 (X2 ), ..., rl (Xl ).
ri R relation Xi ordered set variable names size
arity(ri ).6 conjunct ri (Xi ) referred literal. set variables

query, denoted vars(q) = li=0 Xi , consists distinguished variables
X0 (from head query), existential variables vars(q)\X0 , (which
5. Evaluating select-project-join query requires additional relational operators: natural join rename.
6. Note conjunctive query expressed first order logicSas follows:
l
X0 s.t. r1 (X1 ) r2 (X2 ) ... rl (Xl ) q(X0 ) X0 = i=1 Xi

10

fiLearning Semantic Definitions Information Sources Internet

appear body). conjunctive query said safe

distinguished variables appear body, i.e. X0 li=1 Xi .
3.2 Expressive Languages
Modeling sources using conjunctive queries implies aggregate operators MIN
ORDER cannot used source definitions. functionality sources
described without operators. sources described poorly, however.
Consider hotel search service returns 20 closest hotels given location:
hotelSearch($loc, hotel, dist) :accommodation(hotel, loc1), distance(loc, loc1, dist).
According definition, source return hotels regardless distance. One
cannot express fact closest hotels returned. reason
including aggregate operators hypothesis language search space associated
learning definitions prohibitively large. (Thus leave aggregate operators future
work discussed section 9.2.)
Similarly, source definitions cannot contain disjunction, rules union recursive queries. Again, simplifying assumption holds information sources
greatly reduces search space. means however, weather service providing
forecasts cities US Canada would modeled as:
s($city, temp) :- forecast(city, country, tomorrow, temp).
Since definition restrict domain country attribute, confronted
request forecast Australia, mediator would proceed call service,
oblivious restriction attribute.
allow negation queries source definitions rarely
require it, including would needlessly complicate search. rare cases
negation particular predicate useful describing certain types sources,
negated predicate included (as distinct predicate) search. instance,
might use describe source, even though strictly speaking negation <.
3.3 Leveraging Known Sources
approach problem discovering semantic definitions new services
leverage set known sources learning new definition. Broadly speaking,
invoking known sources (in methodical manner) see combination
information provide matches information provided new source.
practical perspective, means order model newly discovered source semantically,
require overlap data produced new source set known
sources. One way understand consider new source producing weather data.
none known sources produce weather information, way
system learn whether new source producing historical weather data, weather
forecasts - even describing weather all. (In principle, one could try guess
service based type signature alone, would guarantee
definition correct, making little use mediator.) Given overlapping
data requirement, one might claim little benefit incorporating new sources.
detail reasons case below.
11

fiCarman & Knoblock

obvious benefit learning definitions new sources redundancy.
system able learn one source provides exactly information currently
available source, latter suddenly becomes unavailable, former used
place. example, mediator knows one weather source providing current
conditions learns second source provides similar data,
first goes whatever reason (perhaps access quota reached),
weather data still accessed second.
second perhaps interesting reason wanting learn definition
new source new source may provide data lies outside scope (or
simply present in) data provided sources. example, consider
weather service provides temperature values zipcodes United States.
consider second source provides weather forecasts cities worldwide. system
use first source learn definition second, amount information
available querying increases greatly.
Binding constraints service make accessing certain types information difficult
inefficient. case, discovering new source providing similar data
different binding pattern may improve performance. example, consider hotel
search service accepts zipcode returns set hotels along star rating:
hotelSearch($zip, hotel, rating, street, city, state):accommodation(hotel, rating, street, city, state, zip).
consider simple query names addresses five star hotels California:
q(hotel, street, city, zip):- accommodation(hotel, 5*, street, city, California, zip).
Answering query would require thousands calls known source, one every
zipcode California, mediator could answer query another
source providing zipcodes. contrast, system learnt definition new
source provides exactly data different binding pattern (such
one below), answering query would require one call source:
hotelsByState($state, $rating, hotel, street, city, zip):accommodation(hotel, rating, street, city, state, zip).
Often functionality complex source described terms composition
functionality provided simpler services. instance, consider motivating
example section 2, functionality provided new source calculate distance miles two zipcodes. functionality could achieved
performing four different calls available sources. case, definition learnt
system meant query regarding distance zipcodes could handled efficiently. general, learning definitions complicated sources
terms simpler ones, system benefit computation, optimisation caching
abilities services providing complex functionality.
Finally, newly discovered service may faster access known sources
providing similar data. instance, consider geocoding service takes address
returns latitude longitude coordinates location. variety
algorithms used calculate coordinates, unreasonable geocoding
services take long time (upwards one second) return result. system
able discover new source providing geocoding functionality, using
12

fiLearning Semantic Definitions Information Sources Internet

faster algorithm, could locate display many addresses map
amount time.

4. Inducing Definitions
section describe algorithm generating candidate definitions newly
discovered source. algorithm forms first phase generate test methodology
learning source definitions. defer discussion testing phase later paper.
start briefly discussing work relational rule learning describe
algorithm builds upon ideas.
4.1 Inductive Logic Programming
language conjunctive queries restricted form first-order logic. Machine
Learning community, systems capable learning models using first-order representations
referred Inductive Logic Programming (ILP) systems relational rule learners.
expressiveness modeling language, complexity learning much
higher propositional rule learners (also called attribute-value learners), form
bulk Machine Learning algorithms. Given relational modeling services, many
techniques developed ILP apply problem.
First Order Inductive Learner (foil) well known ILP search algorithm (CameronJones & Quinlan, 1994). capable learning first-order rules describe target predicate, represented set positive examples (tuples target relation,
denoted E + ) optionally set negative examples (E ). search viable
definition foil starts empty clause7 progressively adds literals body
(antecedent) rule, thereby making rule specific. process continues
definition (denoted h) covers positive examples negative examples:
E + EI [h] 6=

E EI [h] =

Usually set rules learnt manner removing positive examples covered
first rule repeating process. (The set rules interpreted union
query.) Search foil performed greedy best-first manner, guided information
gain-based heuristic. Many extensions basic algorithm exist, notably
combine declarative background knowledge search process focl (Pazzani &
Kibler, 1992). systems categorised performing top search
start empty clause (the general rule possible) progressively specialize
clause. Bottom approaches, hand, golem (Muggleton & Feng,
1990), perform specific general search starting positive examples target.
4.2 Search
describe actual search procedure use generate candidate definitions
new source. procedure based top-down search strategy used foil.
algorithm takes input type signature new source uses seed search
7. use terms clause query interchangeably refer conjunctive query Datalog. empty
clause query without literals body (right side) clause.

13

fiCarman & Knoblock

input : predicate signature
output: best scoring view definition vbest
invoke target set random inputs;
vbest empty clause ;
3 add vbest empty queue;
4 queue 6= time() < timeout i++ < limit
5
v0 best definition queue;
6
forall v1 expand(v0 )
7
insert v1 queue;
8
eval(v1 ) > 0
9
forall v2 constrain(v1 )
10
insert v2 queue;
11
eval(v2 ) eval(v1 ) v1 v2 ;
12
end
13
end
14
eval(v1 ) eval(vbest ) vbest v1 ;
15
end
16 end
17 return vbest ;
Algorithm 1: Best-first search space candidate source definitions.
1

2

candidate definitions. (We refer new source relation target predicate
set known source relations source predicates.) space candidate definitions
enumerated best-first manner, candidate tested see data returns
similar target. Pseudo-code describing procedure given Algorithm 1.8
first step algorithm invoke new source representative set
input tuples generate examples output tuples characterise functionality
source. set invocations must include positive examples (invocations output
tuples produced) possible, negative tuples (inputs output
returned). algorithms ability induce correct definition source depends
greatly number positive examples available. Thus minimum number positive
invocations source imposed, meaning algorithm may invoke
source repeatedly using different inputs sufficient positive invocations recorded.
Selecting appropriate input values successfully invoke service easier said
done. defer discussion issues difficulties involved successfully invoking
new source section 6.1, assume moment induction system able
generate table values represent functionality.
next step algorithm initialise search adding empty clause
queue definitions expand. rest algorithm simply best-first search
procedure. iteration highest scoring yet expanded definition (denoted
v0 ) removed queue expanded adding new predicate end
8. implementation algorithm used experiments section 7.3 available at:
http://www.isi.edu/publications/licensed-sw/eidos/index.html

14

fiLearning Semantic Definitions Information Sources Internet

clause (see next section example). candidate generated (denoted v1 )
added queue. algorithm progressively constrains candidate binding
variables newly added predicate, (see section 4.4). eval function (see section
5.3) evaluates quality candidate produced. procedure stops constraining
candidate change evaluation function (eval) drops zero.
compares v1 previous best candidate vbest updates latter accordingly.
principle algorithm terminate perfect candidate definition
discovered - one produces exactly data target. practice never
occurs sources incomplete (dont perfectly overlap other)
noisy. Instead algorithm terminates either queue becomes empty, time limit
reached maximum number iterations performed.
4.3 Example
run example process generating candidate definitions. Consider
newly discovered source, takes zipcode distance, returns
zipcodes lie within given radius (along respective distances). target
predicate representing source is:
source5($zip1, $dist1, zip2, dist2)
assume two known sources. first source definition
learnt example section 2, namely:
source4($zip1, $zip2, dist):centroid(zip1, lat1, long1), centroid(zip2, lat2, long2),
greatCircleDist(lat1, long1, lat2, long2, dist2), multiply(dist1, 1.609, dist2).
second source isnt actually source interpreted predicate:
(dist1, dist2).
search definition new source might proceed follows. first
definition generated empty clause:
source5($ , $ , , ).
null character ( ) represents fact none inputs outputs
restrictions placed values. Prior adding first literal (source predicate),
system check whether output attributes echo input values. case, given
semantic types, two possibilities need checked:
source5($zip1, $ , zip1, ).
source5($ , $dist1, , dist1).
Assuming neither possibilities true (i.e. improves score), literals
added one time refine definition. literal source predicate
assignment variable names attributes. new definition must created every
possible literal includes least one variable already present clause. (For
moment ignore issue binding constraints sources added.) Thus many
candidate definitions would generated, including following:
source5($zip1, $dist1, , ) :- source4($zip1, $ , dist1).
source5($zip1, $ , zip2, )
:- source4($zip1, $zip2, ).
source5($ , $dist1, , dist2) :- (dist1, dist2).
15

fiCarman & Knoblock

Note semantic types type signature target predicate limit greatly
number candidate definitions produced. system evaluates
candidates turn, selecting best one expansion. Assuming first
three best score, would expanded adding another literal, forming
complicated candidates following:
source5($zip1, $dist1, , dist2) :- source4($zip1, $ , dist1), (dist1, dist2).
process continues system discovers definition perfectly describes
source, forced backtrack literal improves score.
4.4 Iterative Expansion
sources used previous example relatively low arity. Internet
rarely case, many sources producing large number attributes type.
problem causes exponential number definitions possible
expansion step. Consider instance stock price service, provides current,
high, low, market-opening market-closing prices given ticker symbol. type
signature service would be:
stockprice($ticker, price, price, price, price, price)
definition predicate added already contains k distinct price
variables, number ways price attributes new relation
P
assigned variable names 5i=0 5i k , prohibitively large even moderate k.9
limit search space case high-arity predicates, first generate candidates
minimal number bound variables new literal progressively constrain
best performing definitions within expansion. (High arity predicates
handled similar fashion foil, Quinlan Cameron-Jones, 1993.) example,
consider using source learn definition new source signature:
source6($ticker, price, price)
start adding literals empty definition before. time though, instead
generating literal every possible assignment variable names attributes
relation, generate simplest assignments binding constraints
met. (This expand procedure referred Algorithm 1.) example,
ticker symbol input stockprice source would need bound, generating single
definition:
source6($tic, , ) :- stockprice($tic, , , , , ).
definition would evaluated, constrained definitions generated
equating variable literal variables clause. (This
constrain procedure Algorithm 1.) Two definitions shown below:
source6($tic, pri1, ) :- stockprice($tic, pri1, , , , ).
source6($tic, pri1, ) :- stockprice($tic, , pri1, , , ).
best definitions would selected constrained further, generating
definitions as:

9. Intuitively, one assign variable
names attributes using k labels k different ways. One
choose 5 attributes 5i ways, one = 0, 1, .., 5. See Weber, Tausend,
Stahl (1995) detailed discussion size hypothesis space ILP.

16

fiLearning Semantic Definitions Information Sources Internet

source6($tic, pri1, pri2) :- stockprice($tic, , pri1, pri2, , ).
source6($tic, pri1, pri2) :- stockprice($tic, , pri1, , pri2, ).
way, best scoring literal found without need iterate
possible assignments variables attributes.
4.5 Domain Predicates vs. Source Predicates
examples sections 4.3 4.4, decision perform search source
predicates rather domain predicates made arbitrary fashion.10
section justify decision. one perform search domain predicates
rather source predicates, testing definition would require additional
query reformulation step. example, consider following candidate definition
source5 containing domain predicate centroid:
source5($zip1, $ , , ) :- centroid(zip1, , ).
order evaluate candidate, system would need first treat definition
query reformulate set rewritings (that together form union query)
various sources follows:
source5($zip1, $ , , ) :- source4($zip1, $ , ).
source5($zip1, $ , , ) :- source4($ , $zip1, ).
union query executed available sources (in case
source4) see tuples candidate definition returns. practice however,
definitions known sources contain multiple literals (as normally do)
domain relations high-arity (as often are), search space
conjunctions domain predicates often much larger corresponding search
space conjunctions source predicates. multiple conjunctions
domain predicates (candidate definitions) end reformulating conjunction
source predicates (union queries). example, consider following candidate definitions
written terms domain predicates:
source5($zip1, $ , , ) :- centroid(zip1, lat, ), greatCircleDist(lat, , , , ).
source5($zip1, $ , , ) :- centroid(zip1, , lon), greatCircleDist( , lon, , , ).
source5($zip1, $ , , ) :- centroid(zip1, lat, lon), greatCircleDist(lat, lon, , , ).
three candidates would reformulate query sources (shown below),
thus indistinguishable given sources available.
source5($zip1, $ , , ) :- source4($zip1, $ , ).
general, number candidate definitions map reformulation
exponential number hidden variables present definitions known sources.
reason, simplify problem search space conjunctions source
predicates. sense, performing search source predicates seen
introducing similarity heuristic focuses search toward definitions similar
structure definitions available sources. note definitions produced
(and will) later converted queries global predicates unfolding
10. note difference domain, source interpreted predicates. Domain predicates
invented domain expert use modeling particular information domain. define common
schema used describing information different sources. Source predicates represent
sources available system. Interpreted predicates, (such ), special type domain
predicate, treated source predicates, since meaning interpreted (understood).

17

fiCarman & Knoblock

possibly tightening remove redundancies. discuss process tightening
unfoldings section 6.6.
4.6 Limiting Search
search space generated top-down search algorithm may large even
small number sources. use semantic types limits greatly ways
variables within definition equated (aka join paths) thus goes long
way reduce size search space. Despite reduction, number sources
available increases, search space becomes large techniques limiting must
used. employ standard (and standard) ILP techniques limiting
space. limitations often referred inductive search bias language bias
(Nedellec, Rouveirol, Ade, Bergadano, & Tausend, 1996).
obvious way limit search restrict number source predicates
occur definition. Whenever definition reaches maximum length, backtracking
performed, allowing search escape local minima may result
greedy enumeration. assumption shorter definitions probable
longer ones, makes sense since service providers likely provide data
simplest form possible. Moreover, simpler definition learnt, useful
mediator, decide trade completeness (the ability express longer
definitions) improved accuracy shorter definitions.
second restriction placed candidate definitions limit number times
source predicate appears given candidate. makes sense
definitions real services tend contain many repeated predicates. Intuitively,
services provide raw data without performing many calculations upon it.
Repeated use predicate definition useful describing form
calculation raw data itself. (Exceptions rule exist, example predicates
representing unit conversion functionality Fahrenheit Celsius, may necessarily
occur multiple times definition source.)
third restriction limits complexity definitions generated reducing
number literals contain variables head clause. Specifically,
limits level existential quantification (sometimes referred depth,
Muggleton Feng, 1990) variable clause. level defined zero
distinguished variables (those appearing head clause). existential
variables defined recursively one plus lowest level variable appearing
literal. example, candidate definition shown maximum existential
quantification level three shortest path last literal head literal
(via join variables) passes two literals:
source5($zip1, $ , , ) :- source4($zip1, $ , d1), source3($d1, d2), source3($d2, ).
effect bias concentrate search around simpler highly connected
definitions, literal closely linked input output source.
fourth restriction placed source definitions executable.
specifically, possible execute left right, meaning inputs
source appear either target predicate (head clause) one
literals left literal. example, two candidate definitions shown below,
18

fiLearning Semantic Definitions Information Sources Internet

first executable. second definition not, zip2 used input
source4 first literal, without first bound value head clause:
source5($zip1, $ , zip2, ) :- source4($zip1, $zip2, ).
source5($zip1, $ , , ) :- source4($zip1, $zip2, dist1), source4($zip2, $zip1, dist1).
restriction serves two purposes. Firstly, biases, limits size
search space. Secondly, makes easier evaluate definitions produced. theory,
one could still evaluate second definition generating lots input values
zip2, would require lot invocations minimal gain.
last restriction reduces search space limiting number times
variable appear given literal body clause. Definitions
variable appears multiple times given literal, following example
returns distance zipcode itself, common practice:
source5($zip1, $ , , dist2) :- source4($zip1, $zip1, dist2).
Explicitly preventing definitions generated makes sense sources
requiring rare, better reduce search space exponentially
ignoring them, explicitly check time.

5. Scoring Definitions
proceed problem evaluating candidate definitions generated
search. basic idea compare output produced source output
produced definition input. similar set tuples produced,
higher score candidate. score averaged set different
input tuples see well candidate definition describes new source.
motivating example section 2, source definition learnt
(the definition repeated below) produced one output tuple hdisti every input
tuple hzip1, zip2i:
source4($zip1, $zip2, dist):centroid(zip1, lat1, long1), centroid(zip2, lat2, long2),
greatCircleDist(lat1, long1, lat2, long2, dist2),
multiply(dist1, 1.6093, dist2).
fact made simple compare output service output induced
definition. general however, source modeled (and candidate definitions
modeling it) may produce multiple output tuples input tuple. Take example
source5 section 4.3, produces set output tuples hzip2, dist2i containing
zipcodes lie within given radius input zipcode hzip1, dist1i.
cases, system needs compare set output tuples set produced
definition see tuples same. Since new source
existing sources may complete, two sets may simply overlap, even candidate
definition correctly describes new source. Assuming count number
tuples same, need measure tells us well candidate hypothesis
describes data returned source. One measure following:
score(s, v, I) =

1 X |Os (i) Ov (i)|
|I| iI |Os (i) Ov (i)|
19

fiCarman & Knoblock

new source, v candidate source definition, D[s ] set
input tuples used test source (s set input attributes source s). Os (i)
denotes set tuples returned new source invoked input tuple i. Ov (i)
corresponding set returned candidate definition. Using relational projection
() selection () operators notation introduced section 2.1, sets
written follows. (Note sc represents output attributes s.)
Os (i) sc (s =i (E[s]))



Ov (i) sc (s =i (EI [v]))

view hypothesis testing information retrieval task, consider recall
number common tuples, divided number tuples produced
source, precision number common tuples divided number tuples
produced definition. measure takes precision recall account
calculating average Jaccard similarity sets. table gives
example score calculated input tuple.
input tuple
iI
ha, bi
hc, di
he, f
hg, hi
hi, ji

actual output
tuples Os (i)
{hx, yi, hx, zi}
{hx, wi, hx, zi}
{hx, wi, hx, yi}



predicted output
tuples Ov (i)
{hx, yi}
{hx, wi, hx, yi}
{hx, wi, hx, yi}
{hx, yi}


Jaccard similarity
tuple
1/2
1/3
1
0
#undef!

first two rows table show inputs predicted actual output
tuples overlap. third row, definition produces exactly set tuples
source modeled thus gets maximum score. fourth row,
definition produced tuple, source didnt, definition penalised.
last row, definition correctly predicted tuples would output source.
score function undefined point. certain perspective definition
score well correctly predicted tuples returned
input, giving high score definition produces tuples dangerous.
may cause overly constrained definitions generate output tuples
score well. time, less constrained definitions better predicting
output tuples average may score poorly. example, consider source returns
weather forecasts zipcodes Los Angeles:
source($zip, temp) :- forecast(zip, tomorrow, temp), UScity(zip, Los Angeles).
consider two candidate definitions source. first returns temperature
zipcode, second returns temperature 0 C:
v1 ($zip, temp) :- forecast(zip, tomorrow, temp).
v2 ($zip, temp) :- forecast(zip, tomorrow, temp), temp < 0 C .
Assume source candidates invoked using 20 different randomly selected
zipcodes. zipcodes, source return output, zipcode
lie outside Los Angeles. first candidate likely return output zipcodes,
second candidate would, source, rarely produce output.
temperature zipcodes greater zero, nothing
20

fiLearning Semantic Definitions Information Sources Internet

whether zipcode Los Angeles. score definitions highly
correctly produce output, system would erroneously prefer second candidate
first (because latter often produces output). prevent happening,
simply ignore inputs definition correctly predicts zero tuples.
setting score average values.
Returning attention table, ignoring last row, overall score
definition would calculated 0.46.
5.1 Partial Definitions
search proceeds toward correct definition service, many semi-complete
(unsafe) definitions generated. definitions produce values
attributes target tuple subset them. example, candidate:
source5($zip1, $dist1, zip2, ) :- source4($zip1, $zip2, dist1).
produces one two output attributes produced source. presents
problem, score defined sets tuples containing output
attributes new source. One solution might wait definitions become
sufficiently long produce outputs, comparing see one best
describes new source. are, however, two reasons would make sense:
space safe definitions large enumerate, thus need compare
partial definitions guide search toward correct definition.
best definition system generate may well partial one, set
known sources may sufficient completely model source.
simplest way compute score partial definition compute function
before, instead using raw source tuples, projecting subset
attributes produced definition. revised score shown below. (Note
projection v\s , denotes subset output attributes
produced view definition v. Note projection distinct, i.e.
multiple instances tuple may produced.)
score2 (s, v, I) =

1 X |v\s (Os (i)) Ov (i)|
|I| iI |v\s (Os (i)) Ov (i)|

revised score useful however, gives unfair advantage definitions
produce output attributes source. far easier
correctly produce subset output attributes produce them. Consider
example two source definitions shown below. two definitions identical except
second returns output distance value dist2, first not:
source5($zip1, $dist1, zip2, )
:- source4($zip1, $zip2, dist2), (dist2, dist1).
source5($zip1, $dist1, zip2, dist2):- source4($zip1, $zip2, dist2), (dist2, dist1).
Since two identical, projection subset case return
number tuples. means definitions would get score although
second definition clearly better first since produces required outputs.
need able penalise partial definitions way attributes
dont produce. One way first calculate size domain |D[a]|
21

fiCarman & Knoblock

missing attributes. example above, missing attribute distance value.
Since distance continuous value, calculating size domain obvious.
approximate size domain by:
|D[distance]|

max min
accuracy

accuracy error-bound distance values. (We discuss error-bounds
section 5.4.) Note cardinality calculation may specific semantic type.
Armed domain size, penalise score definition dividing
product size domains output attributes generated
definition. essence, saying possible values extra attributes
allowed definition. technique similar techniques used learning
without explicit negative examples (Zelle, Thompson, Califf, & Mooney, 1995).
set missing output attributes given expression sc \v, thus penalty
missing attributes size domain tuples scheme, i.e.:
penalty = |D[sc \v]|
Using penalty value calculate new score, takes account missing
attributes. Simply dividing projected score penalty would adhere
intended meaning compensating missing attribute values, thus may skew
results. Instead, derive new score introducing concept typed dom predicates
follows:
dom predicate semantic data-type t, denoted domt , single arity
relation whose extension set domain datatype, i.e. E[domt ] =
D[t]. Similarly, dom predicate scheme A, denoted domA , relation
whose extension E[domA ] = D[A].
Dom predicates introduced Duschka handle problem query reformulation
presence sources binding constraints (Duschka, 1997). (In work
predicates typed, although typing would resulted efficient algorithm.) use convert partial definition v safe (complete) definition
v 0 . simply adding dom predicate end view definition
generates values missing attributes. example above, v 0 would be:
source5($zip1, $dist1, zip2, x) :source4($zip1, $zip2, dist2), (dist2, dist1), domdistance (x).
x new variable type distance. new view definition v 0 safe,
variables head clause appear body. general,
turn unsafe view definition v safe definition v 0 appending dom predicate
domsc \v (x1 , ..., xn ), xi distinguished variable (from head clause)
corresponding output attribute v 0 wasnt bound v. use
complete definition calculate score before:
score3 (s, v, I) = score(s, v 0 , I) =

22

1 X |Os (i) Ov0 (i)|
|I| iI |Os (i) Ov0 (i)|

fiLearning Semantic Definitions Information Sources Internet

rewritten (by expanding denominator) follows:
score3 (s, v, I) =

|Os (i) Ov0 (i)|
1 X
|I| iI |Os (i)| + |Ov0 (i)| |Os (i) Ov0 (i)|

remove references v 0 equation considering:
Ov0 (i) = Ov (i) E[domsc \v ] = Ov (i) D[sc \v]
Thus size set given |Ov0 (i)| = |Ov (i)||D[sc \v]| size intersection
calculated taking projection output attributes produced v:
|Os (i) Ov0 (i)| = |v\s (Os (i) Ov (i) D[sc \v])| = |v\s (Os (i)) Ov (i)|
Substituting cardinalities score function given above, arrive following
equation penalised score:
score3 (s, v, I) =

|v\s (Os (i)) Ov (i)|
1 X
|I| iI |Os (i)| + |Ov (i)||D[sc \v]| |v\s (Os (i)) Ov (i)|

5.2 Binding Constraints
candidate definitions generated search may different binding
constraints target predicate. instance partial definition shown below,
variable zip2 output target source, input source4 :
source5($zip1, $dist1, zip2, ) :- source4($zip1, $zip2, dist1).
logical perspective, order test definition correctly, need invoke
source4 every possible value domain zipcodes. practical
two reasons: firstly, system may complete list zipcodes disposal.
Secondly far importantly, invoking source4 thousands different zipcodes
would take long time would probably result system blocked
use service. instead invoking source thousands times,
approximate score definition sampling domain zipcodes
invoking source using sampled values. compensate sampling
scaling (certain components of) score ratio sampled zipcodes
entire domain. Considering example above, randomly choose sample (denoted
[zipcode]) say 20 values domain zipcodes, set tuples returned
definition need scaled factor |D[zipcode]|/20.
general equation computing scaling factor (denoted SF ) shown below.
Note sampling may need performed set attributes. (Here v \s
denotes input attributes v outputs s.)
SF =

|D[v \s ]|
|[v \s ]|

calculate effect scaling factor overall score follows. denote
set tuples returned definition given sampled input Ov (i). value
23

fiCarman & Knoblock

scaled approximate set tuples would returned
definition invoked possible values additional input attributes:
|Ov (i)| |Ov (i)| SF
Assuming sampling performed randomly domain possible values,
intersection tuples produced source definition scale
way. Thus factor affected scaling score defined previously
|Os (i)|. divide throughout scaling factor new scoring function:
score4 (s, v, I) =

|v\s (Os (i)) Ov (i)|
1 X
|I| iI |Os (i)|/SF + |Ov (i)||D[sc \v]| |v\s (Os (i)) Ov (i)|

problem approach often sampled set values small
result intersect set values returned source, even though larger
sample would intersected way. Thus sampling introduces unfair distortions
score certain definitions, causing perform poorly. example, consider
source5 assume scalability purposes, service places limit
maximum value input radius dist1. (This makes sense, otherwise user could
set input radius cover entire US, tuple every possible zipcode would
need returned.) consider sampling performed above. randomly choose
20 zipcodes set possible zipcodes, chance sample containing
zipcode lies within 300 mile radius particular zipcode (in middle
desert) low. Moreover, even one pair zipcodes (out 20) results successful
invocation, sufficient learning good definition service.
get around problem bias sample that, whenever possible, half
values taken positive examples target (those tuples returned new
source) half taken negative examples (those tuples returned source).
sampling positive negative tuples, guarantee approximation
generated accurate possible given limited sample size. denote set
positive negative samples + [v \s ] [v \s ], use values define
positive total scaling factors shown below. (The numerator positive values
different before, values taken output new source.)
SF + =

|v \s (v\s (Os (i)))|
| + [v \s ]|

total scaling factor value before, calculated slightly differently:
SF =

|D[v \s ]|
| + [v \s ]| + | [v \s ]|

score approximated accordingly taking account new scaling
factors. intersection needs scaled using positive scaling factor:
|v\s (Os (i)) Ov (i)| |v\s (Os (i)) Ov (i)| SF +
new scaling results new function evaluating quality view definition:
score5 (s, v, I) =

|v\s (Os (i)) Ov (i)| SF +
1 X
|I| iI |Os (i)| + |Ov (i)||D[sc \v]| SF |v\s (Os (i)) Ov (i)| SF +
24

fiLearning Semantic Definitions Information Sources Internet

5.3 Favouring Shorter Definitions
derived score comparing data source candidate
produce, define evaluation function eval used Algorithm 1. mentioned
section 4.6, shorter definitions target source preferred longer
possibly less accurate ones. accordance principle, scale score
length definition, favour shorter definitions follows:
eval(v) = length(v) score5 (s, v, I)
length(v) length clause < 1 weighting factor. Setting
weighting factor little less 1 (such 0.95) helps remove logically redundant
definitions, sometimes hard detect, often return almost exactly
score shorter equivalent. discuss problem generating non-redundant
clauses section 6.3.
5.4 Approximating Equality
now, ignored problem deciding whether two tuples produced
target source definition same. Since different sources may serialize data
different ways different levels accuracy, must allow flexibility
values tuples contain. instance, example section 2, distance
values returned source definition match exactly, sufficiently
similar accepted value.
numeric types temperature distance makes sense use error bound (like
0.5 C) percentage error (such 1%) decide two values considered
same. sensing equipment (in case temperature) algorithm (in
case distance) error bound associated values produces.
require error bound numeric type provided problem specification.
(Ideally, bounds would learnt automatically examples.)
certain nominal types company names, values hIBM Corporationi
hInternational Business Machines Corp.i represent value, simplistic equality
checking using exact substring matches sufficient deciding whether two values
correspond entity. case, string edit distances JaroWinkler
score better distinguishing strings representing entity representing
different ones (Bilenko, Mooney, Cohen, Ravikumar, & Fienberg, 2003). machine learning
classifier could trained set examples learn available string
edit distances best distinguishes values type threshold set accepting
pair match. require pair similarity metric threshold (or
combinations metrics) provided problem specification.
cases, enumerated types months year might associated
simple equality checking procedure, values hJanuaryi, hJani h1i
found equal. actual equality procedure used depend semantic type
assume work procedure given problem definition. note
procedure need 100% accurate, provide sufficient level accuracy
guide system toward correct source description. Indeed, equality rules could
generated offline training classifier.
25

fiCarman & Knoblock

Complex types date present bigger problem one considers range
possible serializations, including values h5/4/2006i hThu, 4 May 2006i h2006-05-04i.
cases specialized functions required check equality values
break complex types constituent parts (in case day, month
year ). latter would form part domain model.
cases, deciding whether two values type considered equal
depends type, relations used in. Consider
two relations shown below. first provides latitude longitude coordinates
centroid zipcode, second returns coordinates particular address:
centroid(zipcode, latitude, longitude)
geocode(number, street, zipcode, latitude, longitude)
Given different ways calculating centroid zipcode (including using center
mass center population density) error bound 500 meters might make sense
equating latitude longitude coordinates. geocoding service, hand,
error bound 50 meters may reasonable. general, error bounds
associated set global relations, instead semantic types, could
learnt accordingly. relations contain multiple attributes, problem
deciding whether two tuples refer entity called record linkage (Winkler,
1999). entire field research devoted tackling problem. Due complexity
problem variety techniques developed handle it,
investigate here.

6. Extensions
section discuss extensions basic algorithm needed handling real data
sources, well ways reduce size hypothesis space improve quality
definitions produced.
6.1 Generating Inputs
first step source induction algorithm generate set tuples
represent target relation induction process. words, system must
try invoke new source gather example data. without biasing
induction process easier said done. simplest approach generating input values
select constants random set examples given problem specification.
problem approach cases new source produce
output selected inputs. Instead system may need select values according
distribution domain values order source invoke correctly.
example, consider source providing posts used cars sale certain area. source
takes make car input, returns car details:
usedCars($make, model, year, price, phone)
Although hundred different car manufacturers world,
produce bulk cars. Thus invoking source values Ferrari,
Lotus Aston Martin less likely return tuples, compared
common brands Ford Toyota (unless source providing data sports
cars course). distribution possible values available, system first try
26

fiLearning Semantic Definitions Information Sources Internet

common values, generally, choose values set according
distribution. particular example, might difficult query source
complete set car manufacturers one invocations returns data.
general, set examples may large (such 40,000+ zipcodes US)
number interesting values set (the ones likely return results) may
small, case taking advantage prior knowledge distribution
possible values makes sense. noted execution system
receive lot output data different sources accesses. data recorded
generate distributions possible values different types.
problem generating viable input data new source becomes yet difficult
input required single value tuple values. case system
first try invoke source random combinations attribute values
examples type. Invoking sources (such source5 ) easy
explicit restriction combination input values:
source5($zip, $distance, zip, distance)
cases, geocoding service combination possible input values highly
restricted:
USGeocoder($number, $street, $zipcode, latitude, longitude)
Randomly selecting input values independently one another unlikely result
successful invocations. (In order invocation succeed, randomly generated
tuple must correspond address actually exists.) cases, failing
invoke source number times, system try invoke sources (such
hotel lookup service below), produce tuples containing required attribute types:
HotelSearch($city, hotel, number, street, zipcode)
general, process invoking sources generate input sources chained
set viable inputs generated.
note problem synthesizing viable input data difficult
interesting research problem. combined approach utilizing value distributions
invoking alternative services performs well experiments (see section 7.3), area
future work develop general solution.
6.2 Dealing Sources
order minimise source accesses, expensive terms time
bandwidth, requests individual sources cached local relational database.
implementation means implicit assumption work
output produced services constant duration induction process.
could problematic service modeled provides (near) real-time data
update frequency less time takes induce definition. weather
prediction service, updated hourly, may present much problem, since
difference predicted temperatures may vary slightly one update
next. real-time flight status service providing coordinates given aircraft
every five minutes, caching may problematic location plane vary
greatly takes, example, one hour induce definition. theory one could test
variation systematically periodically invoking source previously
27

fiCarman & Knoblock

successful input tuple see output changed, update caching policy
accordingly.
6.3 Logical Optimisations
Evaluating definitions expensive terms time (waiting sources return data) computation (calculating joins large tables). Thus makes sense
check candidate redundancy evaluating it. decide definitions
redundant, use concept query containment:
query q1 LR,A said contained another query q2 LR,A
database instance I, set tuples returned first query subset
returned second, i.e. EI [q1 ] EI [q2 ]. denote containment
q1 v q2 . Two queries considered logically equivalent q1 v q2 q2 v q1 .
conjunctive queries learnt paper, testing query containment reduces
problem finding containment mapping (Chandra & Merlin, 1977).11 use
test discover logically equivalent definitions following, (which contain
reordering literals):
source($zip, temp):- getCentroid($zip, lat, lon), getConditions($lat, $lon, temp).
source($zip, temp):- getConditions($lat, $lon, temp), getCentroid($zip, lat, lon).
equivalence checking performed efficiently canonical ordering predicate
variable names chosen priori. Whenever logically equivalent definitions discovered, search backtrack, thereby avoiding entire sub-trees equivalent clauses.
Similarly, test skip logically redundant clauses following (which
equivalent shorter definition without second literal):
source($zip, ):- getCentroid($zip, lat, long), getCentroid($zip, lat, ).
Again, redundancy checking performed efficiently (Levy, Mendelzon, Sagiv, &
Srivastava, 1995) resulting little computational overhead search.
6.4 Functional Sources
information may known functionality certain sources expressed
source definitions. example, sources Multiply Concatenate,
implemented locally, known complete. (A source considered complete,
returns tuples implied definition, i.e. E[s] = EI [v].) Whenever
information available, induction system take advantage improve search
efficiency. explain how, define class sources call functional sources,
complete input tuple return exactly one output tuple. slightly
restrictive standard ILP concept determinate literals (Cameron-Jones &
Quinlan, 1994), every input tuple return one output tuple. Multiply
Concatenate examples functional sources. system takes advantage fact
functional sources place restrictions input. Whenever functional source
added candidate definition, score definition doesnt change providing
sources inputs none outputs bound. (The set tuples returned new
11. queries contain interpreted predicates, containment testing little involved (Afrati,
Li, & Mitra, 2004).

28

fiLearning Semantic Definitions Information Sources Internet

definition before, new attributes corresponding outputs
source.) Thus new definition need evaluated, added
queue (of definitions expand) is, becomes particularly advantageous
sources input arity high.
6.5 Constants
Constants often used source descriptions define scope service. Consider
weather service provides reports zipcodes California:
calWeather($zip, $date, temp) :- forecast(zip, date, temp), USstate(zip, California).
mediator receives query asking forecast Chicago, know
source relevant query since Chicago California. Although constants
source descriptions useful, simply introducing hypothesis
language could cause search space grow prohibitively. (For states, branching
factor would 50, zipcodes would excess 40,000.) Obviously generate
test methodology make sense domain semantic type large.
Alternatively, one explicitly check repeated values tuples returned
new source (i.e. constants head clause) join source
definition relations (i.e. constants body clause). example,
definition join source relation hzip, date, tempi definition relation
hzip, date, temp, statei would produce tuples state equal California.
constant could added definition.
source($zip, $date, temp) :- forecast(zip, date, temp), USstate(zip, state).
complicated detection procedures would required discovering constants interpreted predicates (i.e. range restrictions numeric attributes).
6.6 Post-Processing
definition learnt new source, may possible tighten
definition removing logical redundancies unfolding. Consider following
definition involving calls two hotel sources, one check availability
check rating:
source($hotel, address, rating):HotelAvailability($hotel, address, price), HotelRating($hotel, rating, address).
unfolding definition (in terms definitions hotel sources) contains
two references accommodation relation:
source($hotel, address, rating):accommodation(hotel, , address), available(hotel, today, price),
accommodation(hotel, rating, address).
first literal redundant removed unfolding. general,
rules used discover redundancy candidate definitions used remove redundant
literals unfolding. Moreover, since post-processing step needs performed
once, time spent searching complicated forms redundancy.
29

fiCarman & Knoblock

7. Evaluation
section describe evaluation source induction algorithm. first
describe experimental setup used experiments performed. Finally,
compare induction algorithm particular complex schema matching system.
7.1 Experimental Setup
source induction algorithm defined paper implemented system called
eidos, stands Efficiently Inducing Definitions Online Sources. eidos implements techniques optimisations discussed sections 4 6. (Certain
extensions section 6 partially implemented: implementation currently
checks constants head clause perform tightening
definitions.) code written Java MySQL database used caching
results source invocations.
eidos tested 25 different problems involving real services several domains
including hotels, financial data, weather cars. domain model used
problem included 70 different semantic types, ranging common
ones zipcode specific types stock ticker symbols. data model
contained 36 relations (excluding interpreted predicates), used model 33
different services. modeled services publicly available information sources.
note decision use set known sources problem
(regardless domain) important order make sure tests realistic.
decision made problem difficult standard schema matching/mapping
scenario source schema chosen, provides data known
priori relevant output schema.
order give better sense problem setting complexity known
sources available, list ten (ordered arity). Due space limitations dont
show complete list definitions, input/output types source.
Note sources share semantic types latitude longitude, means
search space associated sources alone large.
1

WeatherConditions($city,state,country,latitude,longitude,time,time,timeoffset,
datetime,temperatureF,sky,pressureIn,direction,speedMph,humidity,temperatureF)
2 WeatherForecast($city,state,country,latitude,longitude,timeoffset,day,date,
temperatureF,temperatureF,time,time,sky,direction,speedMph,humidity)
3 GetDistance($latitude,$longitude,$latitude,$longitude,distanceKm)
4 USGeocoder($street,$zipcode,city,state,latitude,longitude)
5 ConvertDMS($latitudeDMS,$longitudeDMS,latitude,longitude)
6 USGSEarthquakes(decimal,timestamp,latitude,longitude)
7 GetAirportCoords($iata,airport,latitude,longitude)
8 CountryCode($latitude,$longitude,countryAbbr)
9 GetCentroid($zipcode,latitude,longitude)
10 Altitude($latitude,$longitude,distanceM)

order induce definitions problem, source (and candidate definition) invoked least 20 times using random inputs. Whenever possible, system
attempted generate 10 positive examples source (invocations source
returned tuples) 10 negative examples (inputs produced output).
30

fiLearning Semantic Definitions Information Sources Internet

ensure search terminated, number iterations algorithm including backtracking steps limited 30. search time limit 20 minutes imposed.
inductive search bias used experiments shown below, weighting factor
(defined section 5.3) 0.9 used direct search toward shorter definitions.
Search Bias
Maximum clause length = 7
Maximum predicate repetition = 2
Maximum variable level = 5
Executable candidates
variable repetition within literal
experiments, different procedures used decide equality values
type discussed section 5.4. equality procedures used different
types listed below. accuracy bounds thresholds used chosen maximize
overall performance learning algorithm. (In practice, meta-learning algorithm
could used determine best accuracy bounds different attribute types.)
semantic types listed below, substring matching (checking one string contained
other) used test equality values.
Types
latitudes, longitudes
distances, speeds, temperatures, prices
humidity, pressure, degrees
decimals
companies, hotels, airports
dates

Equality Procedure
accuracy bound 0.002
accuracy bound 1%
accuracy bound 1.0
accuracy bound 0.1
JaroWinkler score 0.85
specialised equality procedure

experiments run dual core 3.2 GHz Pentium 4 4 GB RAM (although
memory limiting factor tests). system running Windows
2003 Server, Java Runtime Environment 1.5 MySQL 5.0.
7.2 Evaluation Criteria
order evaluate induction system, one would compare problem
definition generated system ideal definition source (denoted vbest
v respectively). words, would evaluation function, rates
quality definition produced respect hand-written definition
source (i.e. quality : vbest v [0, 1]). problem twofold. Firstly,
obvious define similarity function conjunctive queries many
different possibilities exist (see Markov Marinchev, 2000, particular example).
Secondly, working best definition hand, taking account limitations
domain model fact available sources noisy, incomplete, possibly
less accurate, even serialise data different ways, may extremely difficult, even
possible. order evaluate discovered definitions, instead count
number correctly generated attributes definition. attribute said
correctly generated, if:
31

fiCarman & Knoblock

input, definition correctly restricts domain possible values
attribute,
output, definition correctly predicts value given input tuples.
Consider following definition takes two input values returns difference
square root (providing difference positive):
source($A, $B, C, D) :- sum(B, C, A), product(D, D, C), B.
imagine induction system managed learn source returns
difference input output, i.e.:
source($A, $B, C, ) :- sum(B, C, A).
say first input attribute correctly generated input
constrained respect input attribute B (the inequality missing). input B
deemed correctly generated present sum relation (only one input penalised
missing inequality). output C deemed correctly generated respect
inputs, missing attribute isnt generated all. (Note ordering
variables sum relation different, say sum(A, B, C), C would
generated, correctly generated.)
Given definition correctly generated attributes, one define expressions
precision recall attributes contained source definition. define precision
ratio correctly generated attributes total number attributes generated
definition, i.e.:
precision =

# correctly generated attributes
total # generated attributes

define recall ratio generated attributes total number attributes
would generated ideal definition, given sources available. (In
cases sources available generate values attribute case,
attribute included count.)
recall =

# correctly generated attributes
total # attributes generated

Note defined precision recall schema level terms attributes
involved source definition. could defined data level terms
tuples returned source definition. Indeed, Jaccard similarity
used score candidate definitions combination data-level precision recall values.
reason choosing schema level metrics evaluation better reflect
semantic correctness learnt definition, far independent
completeness (amount overlap) known target sources.
Returning example above, precision simple definition learnt would
2/3 recall would 2/4. Note that, product relation available
domain model (in case attribute could never generated), recall
would higher 2/3.
32

fiLearning Semantic Definitions Information Sources Internet

7.3 Experiments
definitions learnt system described below. Overall system performed
well able learn intended definition (ignoring missing join variables
superfluous literals) 19 25 problems.
7.3.1 Geospatial Sources
first set problems involved nine geospatial data sources providing variety location
based information. definitions learnt system listed below. reported
terms source predicates rather domain relations (i.e. unfoldings)
corresponding definitions much shorter. makes easier understand
well search algorithm performing.
1
2
3

4
5
6

7
8
9

GetInfoByZip($zip0,cit1,sta2,_,tim4) :GetTimezone($sta2,tim4,_,_), GetCityState($zip0,cit1,sta2).
GetInfoByState($sta0,cit1,zip2,_,tim4) :GetTimezone($sta0,tim4,_,_), GetCityState($zip2,cit1,sta0).
GetDistanceBetweenZipCodes($zip0,$zip1,dis2) :GetCentroid($zip0,lat1,lon2), GetCentroid($zip1,lat4,lon5),
GetDistance($lat1,$lon2,$lat4,$lon5,dis10), ConvertKm2Mi($dis10,dis2).
GetZipCodesWithin($_,$dis1,_,dis3) :<(dis3,dis1).
YahooGeocoder($str0,$zip1,cit2,sta3,_,lat5,lon6) :USGeocoder($str0,$zip1,cit2,sta3,lat5,lon6).
GetCenter($zip0,lat1,lon2,cit3,sta4) :WeatherConditions($cit3,sta4,_,lat1,lon2,_,_,_,_,_,_,_,_,_,_,_),
GetZipcode($cit3,$sta4,zip0).
Earthquakes($_,$_,$_,$_,lat4,lon5,_,dec7,_) :USGSEarthquakes(dec7,_,lat4,lon5).
USGSElevation($lat0,$lon1,dis2) :ConvertFt2M($dis2,dis1), Altitude($lat0,$lon1,dis1).
CountryInfo($cou0,cou1,cit2,_,_,cur5,_,_,_,_) :GetCountryName($cou0,cou1), GoCurrency(cur5,cou0,_),
WeatherConditions($cit2,_,cou1,_,_,_,_,_,_,_,_,_,_,_,_,_).

first two sources provide information zipcodes, name city,
state timezone. differ binding constraints, first taking
zipcode input, second taking state. second source returns many output
tuples per input value, making harder learn definition, even though two sources
provide logically information. induced definitions best possible given
known sources available. (None provided missing output attribute, telephone area-code.) third source calculates distance miles two zipcodes,
(it source4 section 2). correct definition learnt source,
next source, returned zipcodes within given radius, reasonable definition could learnt within time limit.12 Ignoring binding constraints, intended
12. recall problem 1/4 input attribute dis1 determined correctly
generated attribute (it constrained respect output attribute dis3 ), four attributes
generated (the output attribute dis3 generated < predicate). precision
1/1 dis1 generated attribute, correctly generated.

33

fiCarman & Knoblock

definition third, restriction output distance less
input distance. Thus would far easier eidos learn definition
fourth source terms third. Indeed, new definition third
source added set known sources, system able learn following:
4 GetZipCodesWithin($zip0,$dis1,zip2,dis3) :GetDistanceBetweenZipCodes($zip0,$zip2,dis3), <(dis3,dis1).

ability system improve learning ability time set known
sources increases key benefit approach.
Source five geocoding service provided Yahoo. (Geocoding services map addresses latitude longitude coordinates.) eidos learnt functionality
provided service called USGeocoder. Source six simple service providing
latitude/longitude coordinates city state given zipcode. Interestingly,
system learnt sources coordinates better predicted weather conditions service (discussed section 7.3.3), GetCentroid source third
definition. Note new source definition unfolded contain extraneous predicates related weather information.13 additional predicates interfere
usefulness definition, however, query reformulation algorithm still
use source answer queries regardless. (Thus precision recall scores
affected.) post-processing step remove extraneous predicates possible,
would require additional information domain model.14 seventh source provided
earthquake data within bounding box took input. case, system
discovered source indeed providing earthquake data, (lat4 lon5 coordinates earthquake dec7 magnitude). didnt manage, however, work
input coordinates related output. next source provided elevation
data feet, found sufficiently similar known altitude data metres.
Finally, system learnt definition source providing information countries
currency used, name capital city. Since known sources
available provide information, system ended learning weather reports
available capital country.
Problem
1
2
3
4
5
6
7
8
9

# Candidates
25
24
888
3
50
40
11
15
176

# Invocations
5068
9804
11136
11176
13148
15162
14877
177
28784

Time (s)
85
914
449
25
324
283
18
72
559

log10 (Score)
-1.36
-1.08
-0.75
0.25
-0.45
-7.61
-6.87
-8.58
-5.77

Precision
4/4
4/4
3/3
1/1
6/6
5/5
3/3
3/3
4/4

Recall
4/4
4/4
3/3
1/4
6/6
5/5
3/9
3/3
4/4

13. unfolding shown below. conditions predicate could removed without affecting meaning:
GetCenter($zip0, lat1, lon2, cit3, sta4):- municipality(cit3, sta4, zip2, tim3), country( , cou5, ),
northAmerica(cou5), centroid(zip2, lat1, lon2), conditions(lat1, lon2, , , , , , , , , , , ),
timezone(tim3, , ), municipality(cit3, sta4, zip0, ).
14. particular, universally quantified knowledge would needed domain model, e.g.:
lat, lon x1 , ..., x11 s.t. conditions(lat, long, x1 , ..., x11 )

34

fiLearning Semantic Definitions Information Sources Internet

table shows details regarding search performed learn definitions listed above. problem, shows number candidates generated prior
winning definition, along time number source invocations required
learn definition. (The last two values interpreted caution
highly dependent delay accessing sources, caching data
system.) scores shown fifth column normalised version scoring
function used compare definitions search. (Normalisation involved removing
penalty applied missing outputs.) scores small, logarithm
values shown (hence negative values).15 scores interpreted
confidence system definitions produced. closer value zero,
better definitions ability produce tuples source. see
system far confident definitions one five, latter
ones.16 last two columns give precision recall value problem.
average precision problems 100%. (Note high precision value
expected, given induction algorithm relies finding matching tuples
source definition.) average recall geospatial problems high
84%.
7.3.2 Financial Sources
Two sources tested provided financial data. definitions generated eidos
sources shown below.
10 GetQuote($tic0,pri1,dat2,tim3,pri4,pri5,pri6,pri7,cou8,_,pri10,_,_,pri13,_,com15):YahooFinance($tic0,pri1,dat2,tim3,pri4,pri5,pri6,pri7,cou8),
GetCompanyName($tic0,com15,_,_),Add($pri5,$pri13,pri10),Add($pri4,$pri10,pri1).
11 YahooExchangeRate($_,$cur1,pri2,dat3,_,pri5,pri6) :GetCurrentTime(_,_,dat3,_), GoCurrency(cur1,cou5,pri2),
GoCurrency(_,cou5,pri5), Add($pri2,$pri5,pri12), Add($pri2,$pri6,pri12).

first financial service provided stock quote information, system learnt
source returned exactly information stock market service provided Yahoo.
able work previous days close plus todays change equal
current price. second source provided rate exchange currencies
given input. case, system fare well. unable learn intended
result, involved calculating exchange rate taking ratio values
first second currency.
Problem
10
11

# Candidates
2844
367

# Invocations
16671
16749

Time (s)
387
282

log10 (Score)
-8.13
-9.84

Precision
12/13
1/5

Recall
12/12
1/4

Details regarding search spaces two problems shown above. average
precision recall problems much lower 56% 63% respectively,
system unable learn intended definition second problem.
15. positive value problem 4 results approximation error.
16. Low scores perfect precision recall (problems 6, 8 9) indicate little overlap
target known sources. fact system learns correct definition cases
testimony robustness approach.

35

fiCarman & Knoblock

7.3.3 Weather Sources
Internet, two types weather information services, provide
forecasts coming days, provide details current weather conditions.
experiments, pair services provided Weather.com used learn definitions number weather sources. first set definitions, correspond
sources provide current weather conditions, listed below:
12 NOAAWeather($ica0,air1,_,_,sky4,tem5,hum6,dir7,spe8,_,pre10,tem11,_,_) :GetAirportInfo($ica0,_,air1,cit3,_,_),
WeatherForecast($cit3,_,_,_,_,_,_,_,_,_,_,_,sky4,dir7,_,_),
WeatherConditions($cit3,_,_,_,_,_,_,_,_,tem5,sky4,pre33,_,spe8,hum6,tem11),
ConvertIn2mb($pre33,pre10).
13 WunderGround($sta0,$cit1,tem2,_,_,pre5,pre6,sky7,dir8,spe9,spe10) :WeatherConditions($cit1,sta0,_,_,_,_,_,_,dat8,tem9,sky7,pre5,dir8,spe13,_,tem2),
WeatherForecast($cit1,sta0,_,_,_,_,_,_,tem24,_,_,_,_,_,spe10,_),
ConvertIn2mb($pre5,pre6),<(tem9,tem24),ConvertTime($dat8,_,_,_,_),<(spe9,spe13).
14 WeatherBugLive($_,cit1,sta2,zip3,tem4,_,_,dir7,_,_) :WeatherConditions($cit1,sta2,_,_,_,_,_,_,_,tem4,_,_,dir7,_,_,_),
GetZipcode($cit1,$sta2,zip3).
15 WeatherFeed($cit0,$_,tem2,_,sky4,tem5,_,_,pre8,lat9,_,_) :WeatherConditions($cit0,_,_,lat9,_,_,_,_,_,_,sky4,pre8,dir12,_,_,tem5),
WeatherForecast($cit0,_,_,_,_,_,_,_,_,tem2,_,_,_,dir12,_,_).
16 WeatherByICAO($ica0,air1,cou2,lat3,lon4,_,dis6,_,sky8,_,_,_,_) :Altitude($lat3,$lon4,dis6), GetAirportInfo($ica0,_,air1,cit6,_,cou8),
WeatherForecast($cit6,_,cou8,_,_,_,_,_,_,_,_,_,sky8,_,_,_),
GetCountryName($cou2,cou8).
17 WeatherByLatLon($_,$_,_,_,_,lat5,lon6,_,dis8,_,_,_,_,_,_) :Altitude($lat5,$lon6,dis8).

first problem, system learnt source 12 provided current conditions airports,
checking weather report cities airport located.
particular problem demonstrates advantages learning definitions new
sources described section 3.3. definition learnt, mediator receives
request current conditions airport, generate answer query
executing single call newly modeled source, (without needing find nearby
city). system performed well next three sources (13 15) learning definitions
cover attributes each. last two problems, system
perform well. case source 16, system spent time learning
attributes airport returned (such country, coordinates, elevation,
etc.). last case, system able learn source returning
coordinates along elevation. note different sources may provide
data different levels accuracy. Thus fact system unable learn
definition particular source could simply mean data returned
source wasnt sufficiently accurate system label match.
addition current weather feeds, system run two problems involving
weather forecast feeds. well first problem, matching bar one
attributes (the country) finding order high low temperatures
36

fiLearning Semantic Definitions Information Sources Internet

inverted. well second problem, learning definition source
produced output attributes.
18 YahooWeather($zip0,cit1,sta2,_,lat4,lon5,day6,dat7,tem8,tem9,sky10) :WeatherForecast($cit1,sta2,_,lat4,lon5,_,day6,dat7,tem9,tem8,_,_,sky10,_,_,_),
GetCityState($zip0,cit1,sta2).
19 WeatherBugForecast($_,cit1,sta2,_,day4,sky5,tem6,_) :WeatherForecast($cit1,sta2,_,_,_,tim5,day4,_,tem6,_,tim10,_,sky5,_,_,_),
WeatherConditions($cit1,_,_,_,_,tim10,_,tim5,_,_,_,_,_,_,_,_).

Details regarding number candidates generated order learn definitions
different weather sources shown below. average precision definitions produced
91%, average recall 62%.
Problem
12
13
14
15
16
17
18
19

# Candidates
277
1989
98
199
102
45
119
116

# Invocations
579
426
2499
754
946
7669
13876
14857

Time (s)
233
605
930
292
484
1026
759
1217

log10 (Score)
-2.92
-6.35
-13.37
-6.48
-29.69
-26.71
-5.74
-12.56

Precision
8/9
6/9
5/5
5/6
6/7
3/3
10/10
5/5

Recall
8/11
6/10
5/8
5/10
6/9
3/13
10/11
5/7

7.3.4 Hotel Sources
Definitions learnt sources providing hotel information Yahoo, Google
US Fire Administration. definitions shown below.
20 USFireHotelsByCity($cit0,_,_,sta3,zip4,cou5,_) :HotelsByZip($zip4,_,_,cit0,sta3,cou5).
21 USFireHotelsByZip($zip0,_,_,cit3,sta4,cou5,_) :HotelsByZip($zip0,_,_,cit3,sta4,cou5).
22 YahooHotel($zip0,$_,hot2,str3,cit4,sta5,_,_,_,_,_) :HotelsByZip($zip0,hot2,str3,cit4,sta5,_).
23 GoogleBaseHotels($zip0,_,cit2,sta3,_,_,lat6,lon7,_) :WeatherConditions($cit2,sta3,_,lat6,lon7,_,_,_,_,_,_,_,_,_,_,_),
GetZipcode($cit2,$sta3,zip0).

system performed well three four problems. unable time
allocated discover definition hotel attributes (name, street, latitude longitude) returned Google Web Service. average precision problems
90% average recall 60%.
Problem
20
21
22
23

# Candidates
16
16
43
95

# Invocations
448
1894
3137
4931

Time (s)
48
5
282
1161
37

log10 (Score)
-4.00
-2.56
-2.81
-7.50

Precision
4/4
4/4
5/5
3/5

Recall
4/6
4/6
5/9
3/6

fiCarman & Knoblock

7.3.5 Cars Traffic Sources
last problems system tested pair traffic related Web Services.
first service, provided Yahoo, reported live traffic data (such accidents
construction work) within given radius input zipcode. known sources
available provided information, surprisingly, system unable
learn definition traffic related attributes source. (Instead, system
discovered relationship input zipcode output longitude wasnt
correct, precision problem zero.)
24 YahooTraffic($zip0,$_,_,lat3,lon4,_,_,_) :GetCentroid($zip0,_,lon4), CountryCode($lat3,$lon4,_).
25 YahooAutos($zip0,$mak1,dat2,yea3,mod4,_,_,pri7,_) :GoogleBaseCars($zip0,$mak1,_,mod4,pri7,_,_,yea3),
ConvertTime($dat2,_,dat10,_,_), GetCurrentTime(_,_,dat10,_).

second problem involved classified used-car listing Yahoo took zipcode
car manufacturer input. eidos able learn good definition source,
taking advantage fact cars (defined make, model, year
price) listed sale Googles classified car listing.
Problem
24
25

# Candidates
81
55

# Invocations
29974
405

Time (s)
1065
815

log10 (Score)
-11.21
-5.29

Precision
0/3
6/6

Recall
0/4
6/6

Since system failed first problem (it found incorrect/non-general relationships different attributes), succeeded second problem find best
possible definition, average precision recall problems 50%.
7.3.6 Overall Results
Across 25 problems, eidos managed generate definitions high accuracy (average
precision 88%) large number attributes (average recall 69%). results
promising, especially considering problems involved real data sources
cases small overlap data produced target provided
known sources (as evidenced low logarithmic scores). addition minimal
overlap, many sources provided incomplete tuples (i.e. tuples containing multiple NULL
N/A values) well erroneous inaccurate data, making problem
difficult. high average precision recall lead us believe Jaccard measure
good job distinguishing correct incorrect definitions presence
data sources noisy (inconsistent) incomplete (missing tuples values).
Comparing different domains, one see system performed better
problems fewer input output attributes (such geospatial problems),
expected given resulting search space much smaller.
38

fiLearning Semantic Definitions Information Sources Internet

7.4 Empirical Comparison
demonstrated effectiveness eidos learning definitions real information
services, show system capable handling problems
well-known complex schema matching system.
iMAP system (Dhamanka, Lee, Doan, Halevy, & Domingos, 2004) (discussed
section 8.4) schema matcher learn complex (many-to-one) mappings
concepts source target schema. uses set special purpose searchers
learn different types mappings. eidos system, hand, uses generic
search algorithm solve comparable problem. Since two systems made
perform similar task, show eidos capable running one problem
domains used evaluation iMAP. chose particular domain online cricket
databases one used evaluation involved aligning data
two independent data sources. (All problems involved generating synthetic data
splitting single database source target schema, would
interesting eidos.)
Player statistics two online cricket databases (cricketbase.com cricinfo.com)
used experiments. Since neither sources provided programmatic access
data, statistics data extracted HTML pages inserted
relational database. extraction process involved flattening data relational
model small amount data cleaning. (The resulting tables similar
necessarily exactly used iMAP experiments.) data
two websites used create three data sources representing website. three
sources representing cricinfo.com used learn definitions three sources
representing cricketbase.com. known sources available system, including
functionality splitting apart comma-separated lists, adding multiplying numbers,
on. definitions learnt describe cricketbase services shown below:
1 CricbasePlayers($cou0,nam1,_,dat3,_,unk5,unk6) :CricinfoPlayer($nam1,dat3,_,_,_,lis5,nam6,unk5,unk6), contains($lis5,cou0),
CricinfoTest($nam6,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_).
2 CricbaseTest($_,nam1,cou2,_,_,_,cou6,cou7,dec8,cou9,_,_,_,cou13,cou14,dec15,_,
dec17,cou18,_,_) :CricinfoTest($nam1,_,_,cou2,cou6,cou18,cou7,_,dec8,dec15,_,cou9,_,_,_,cou14,
cou13,_,_,_,_,dec17).
3 CricbaseODI($_,nam1,cou2,_,_,_,cou6,cou7,dec8,cou9,cou10,_,cou12,cou13,_,dec15,
dec16,dec17,cou18,cou19,_) :CricinfoODI($nam1,_,_,cou2,cou6,_,cou10,_,dec8,_,cou7,cou18,cou19,cou9,_,_,
cou13,cou12,dec15,_,dec16,dec17).

first source provided player profiles country. second third sources provided
detailed player statistics two different types cricket (Test One-Day-International
respectively). system easily found best definition first source. definition
involved looking players country list teams played for. eidos
perform quite well second third problems. two reasons
this. Firstly, arity sources much higher many instances
semantic type (count decimal ), making space possible alignments much larger.
39

fiCarman & Knoblock

(Because large search space, longer timeout 40 minutes used.) Secondly,
high frequency null values (the constant N/A) data fields
confused algorithm, made harder discover overlapping tuples
desired attributes.
Problem
1
2
3

# Candidates
199
1162
3114

# Invocations
3762
1517
4299

Time (s)
432
1319
2127

log10 (Score)
-3.95
-4.70
-6.28

Precision
5/5
8/11
8/14

Recall
5/5
8/16
8/16

Details search performed learn definitions shown above. average
precision problems 77% average recall lower 66%.
values comparable quality matchings reported iMAP.17 results
good, considering eidos searches space many-to-many correspondences,
(trying define set target attributes contemporaneously), iMAP searches
spaces one-to-one many-to-one correspondences. Moreover, eidos first invokes
target source generate representative data (a task performed iMAP)
performs generic search reasonable definitions without relying specialised search
algorithms different types attributes (as done iMAP).

8. Related Work
section describe work paper relates research performed
Machine Learning, Database Semantic Web communities. that,
describe early work performed Artificial Intelligence community.
discuss algorithm differs standard ILP techniques, particular
direct application techniques possible problem.
8.1 Early Approach
first work concerned learning models describing operations available
Internet performed (in pre-XML era) problem called category translation
(Perkowitz & Etzioni, 1995; Perkowitz, Doorenbos, Etzioni, & Weld, 1997). problem
consisted incomplete internal world model external information source
goal characterize information source terms world model.
world model consisted set objects O, object belonged certain
category (e.g. people) associated set attributes ha1 (o), ..., (o)i, made
strings objects. simple relational interpretation world model would
consider category relation, object tuple. information
source, meanwhile, operation took single value input returned
single tuple output. category translation problem viewed simplification
source definition induction problem, whereby:
17. actual values precision recall cricket domain quoted, accuracy range
68-92% simple matches (one-to-one correspondences source target fields) 50-86%
complex matches (many-to-one correspondences) across synthetic real problems given.

40

fiLearning Semantic Definitions Information Sources Internet

extensions global relations explicit. (There one source per global
relation, doesnt binding constraints, i.e. R = S.)
information provided sources change time.
new source takes single value input returns single tuple output.
order find solutions instances category translation problem, authors employed variant relational path-finding (Richards & Mooney, 1992), extension
foil algorithm, learn models external source. technique described
paper solving instances source induction problem similar
based foil-like inductive search algorithm.
8.2 Direct Application ILP techniques
Researchers became interested field Inductive Logic Programming early
nineties, resulting number different ILP systems developed including foil
(Cameron-Jones & Quinlan, 1994), progol (Muggleton, 1995) aleph18 . Ideally, one
would apply off-the-shelf ILP systems source definition induction
problem. number issues, however, limit direct applicability systems.
issues summarised follows:





Extensions global relations virtual.
Sources may incomplete respect definitions.
Explicit negative examples target available.
Sources may serialise constants different ways.

first issue fact ILP systems assume extensional
definition target predicate extensional (or cases intentional) definitions
(source) predicates used definition target. words,
assume tables already exist relational database represent
new source known sources. case, need generate tables first
invoking services relevant inputs. One could envisage invoking sources
every possible input using resulting tables perform induction. direct
approach would feasible two reasons. Firstly, complete set possible input
values may known system. Secondly, even possible generate
complete set viable inputs service, may practical query source
large set tuples. Consider source4 section 2, calculates distance
miles two zipcodes. Given 40,000 zipcodes US, generating
extensional representation source would require performing billion
invocations! Performing large number invocations make sense
small number example invocations would suffice characterising functionality
source. paper developed efficient algorithm queries
sources needed order evaluate individual candidate definitions.
second issue regarding incompleteness sources causes problem
candidate evaluated. Since set tuples returned known source may
18. See Aleph Manual Ashwin Srinivasan, available at:
http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/aleph.html

41

fiCarman & Knoblock

subset implied definition, set tuples returned
candidate hypothesis executed sources. means
system tries evaluate hypothesis comparing tuples output
new source, cannot sure tuple produced new source
hypothesis fact logically implied it. fact taken account
evaluation function scoring candidate definitions, discussed section 5.
third issue regarding lack explicit negative examples target predicate
affects evaluation candidate hypotheses. classic approach dealing
problem assume closed world, tuples (over head relation)
explicitly declared positive must negative. Since new source may
fact incomplete respect best possible definition it, assumption
necessarily hold. words, particular tuple produced
candidate definition executed tuple returned new source
necessarily mean candidate definition incorrect.
fourth issue fact data provided different sources may
need reconciled, sense different serialisations (strings representing)
value (such Monday Mon instance) must recognized. Since ILP
systems designed operate single database containing multiple tables,
issue heterogeneity data handled current systems. section 5.4
discussed heterogeneity resolved system.
8.3 Machine Learning Approaches
Since advent services Internet, researchers investigating ways
model automatically. Primarily, interest centered using machine learning
techniques classify input output types service, facilitate service
discovery. & Kushmerick proposed using Support Vector Machine classify
input output attributes different semantic types based metadata interface
descriptions (He & Kushmerick, 2003, 2004). notion semantic types (such
zipcode) opposed syntactic types (like integer ) went way toward defining
functionality source provides. Recently, researchers (Lerman et al., 2006)
proposed use logistic regression assigning semantic types input parameters
based metadata, pattern language assigning semantic types output
parameters based data source produces. work classifying input
output attributes service semantic types forms prerequisite work
article. purposes paper, assumed problem solved.
addition classifying input/output attributes services, & Kushmerick
investigated idea classifying services different service types.
precisely, used classification techniques assign service interfaces different semantic domains (such weather flights) operations interface
provides different classes operation (such weatherForecast flightStatus).
resulting source description (hypothesis) language limited select-project queries,
sufficiently expressive describe many sources available Internet.
According approach, since every operation must characterized particular operation class, operations provide overlapping (non-identical) functionality would
42

fiLearning Semantic Definitions Information Sources Internet

need assigned different classes would operations provide composed functionality (such as, example, operation provides weather flight data).
need exhaustive set operation classes (and accompanying training data) major
limitation approach, shared work described paper, relies
expressive language describing service operations.
One way eliminate need predefined set operation types use unsupervised clustering techniques generate (operation) classes automatically examples
(WSDL documents). idea implemented system called Woogle (Dong et al.,
2004). system clustered service interfaces together using similarity score based
co-occurrence metadata labels. took advantage clusters produced improve keyword-based search Web Services. advantage unsupervised approach
labeled training data required, time-consuming generate.
clustering approaches, however, useful service discovery, suffer limitations
previous approach comes expressiveness.
8.4 Database Approaches
database community long interested problem integrating data
disparate sources. Specifically, areas data warehousing (Widom, 1995) information integration (Wiederhold, 1996), researchers interested resolving semantic
heterogeneity exists different databases data combined
accessed via single interface. schema mapping problem problem determining
mapping relations contained source schema particular relation
target schema. mapping defines transformation used populate
target relation data source schema. Mappings may arbitrarily complex
procedures, general declarative queries SQL Datalog. complexity queries makes schema mapping problem far difficult highly
investigated schema matching problem (Rahm & Bernstein, 2001), involves finding
1-to-1 correspondences fields source target schema.
source definition induction problem viewed type schema mapping
problem, known sources define source schema unknown source
specifies target relation. order solve schema mapping problem, one typically
takes advantage available auxiliary information (including source target data
instances, labels respective schemas, on). problems generally
simpler, however, data (the extensions relations) source target
schema usually explicitly available. source induction, data hidden behind
service interface, binding constraints, data extremely large
even (in case sources providing mathematical functions) infinite. Thus making
problem considerably difficult.
schema integration system CLIO (Yan, Miller, Haas, & Fagin, 2001) helps users
build SQL queries map data source target schema. CLIO, foreign keys
instance data used generate integration rules semi-automatically. Since CLIO
relies heavily user involvement, make sense compare directly
automated system developed paper.
43

fiCarman & Knoblock

Another closely related problem complex schema matching, goal
discover complex (many-to-one) mappings two relational tables XML schemas.
problem far complicated basic (one-to-one) schema matching because:
space possible correspondences relations longer Cartesian
product source target relations, powerset source relation times
target relation.
Many-to-one mappings require mapping function, simple concatenate(x,y,z), arbitrarily complex formula z = x2 + y.
iMAP system (Dhamanka et al., 2004) tries learn many-to-one mappings concepts set source relations target relation. uses set special
purpose searchers learn different types mappings (such mathematical expressions,
unit conversions time/date manipulations). uses meta-heuristic control
search performed different special purpose searchers. one views
source schema functions available use mappings (such concatenate(x,y,z), add(x,y,z), etc.) set known sources source definition induction
problem, complex schema matching source induction problems somewhat
similar. main differences problems are:
data associated source schema explicit (and static) complex schema
matching, hidden (and dynamic) source induction.
general, set known sources source induction problem much larger
(and data provide may less consistent), set mapping functions
source relations complex schema matching problem.
paper develop general framework handling source induction problem.
Since iMAP provides functionality similar system, perform
simple empirical comparison section 7.4.
8.5 Semantic Web Approach
stated goal Semantic Web (Berners-Lee, Hendler, & Lassila, 2001) enable
machine understanding Web resources. done annotating resources
semantically meaningful metadata. Thus work described paper much
line Semantic Web, far attempting discover semantically
meaningful definitions online information sources. De facto standards annotating
services semantic markup around number years. standards
provide service owners metadata language adding declarative statements service
interface descriptions attempt describe semantics service terms
functionality (e.g. book purchase operation) data (e.g. weather forecast)
provides. Work languages related article two perspectives:
viewed alternative approach gaining knowledge semantics
newly discovered source (providing semantic metadata associated it).
Semantic Web Service annotation languages seen target language
semantic descriptions learnt paper.
44

fiLearning Semantic Definitions Information Sources Internet

Web Service already semantically annotated, heterogeneity may still exist
ontology used service provider used consumer, case
learning capabilities described paper may required reconcile differences.
importantly, interested vast number sources semantic
markup currently unavailable. work article complements Semantic
Web community providing way automatically annotating sources semantic
information; thereby relieving service providers burden manually annotating
services. learnt, Datalog source definitions converted Description Logicbased representations used OWL-S (Martin, Paolucci, McIlraith, Burstein,
McDermott, McGuinness, Parsia, Payne, Sabou, Solanki, Srinivasan, & Sycara, 2004)
WSMO (Roman, Keller, Lausen, de Bruijn, Lara, Stollberg, Polleres, Feier, Bussler, &
Fensel, 2005). reason use Datalog paper (rather Description Logics)
mediator-based integration systems rely representation language.

9. Discussion
paper presented completely automatic approach learning definitions
online services. approach exploits definition sources either given
system learned previously. resulting framework significant advance
prior approaches focused learning inputs outputs class
service. demonstrated empirically viability approach.
key contribution article procedure learning semantic definitions
online information services is:
Fully automated : Definitions learnt completely automated manner without
need user intervention.
expressive: query language defining sources conjunctive
queries, far expressive previous attribute-value approaches.
Sufficiently robust: procedure able learn definitions presence noisy
incomplete data, thus sufficiently robust handle real data sources.
Data-access efficient: procedure samples data live sources, invoking
sparingly required, making highly efficient terms source accesses.
Evolving: procedures ability learn definitions improves time new
definition learnt added set known sources.
9.1 Application Scenarios
number different application scenarios system capable learning
definitions online sources. generally involve providing semantic definitions data
integration systems, exploit integrate available sources.
obvious application work would system (depicted left side
Figure 1) crawls Web, searching information sources. Upon finding source,
system would use classifier assign semantic types it, followed inductive
learner generate definition it. definition could used annotate
source Semantic Web, mediator answering queries. Importantly,
entire process could run minimal user involvement.
45

fiCarman & Knoblock

Figure 1: Architecture diagrams three different application scenarios.

challenging application scenario (shown center Figure 1) would involve
real-time service discovery. Consider case mediator unable answer
particular query desired information lies scope sources available.
search performed based missing conjuncts (relation names constants)
query using specialised Web Service search engine, Woogle (Dong et al.,
2004). services returned would annotated semantic types and, possible,
semantic definitions. definitions provided mediator, would complete
query processing return answer user. scenario may seem little farfetched one considers specific example: imagine user interacting geospatial
browser (an online atlas). user turns particular information layer, ski
resorts, source available current field view (of, instance, Italy),
results would displayed. background search could performed new
source discovered, provides ski resorts Europe. relevant data could
displayed, user unaware search performed.
Perhaps likely application scenario (to right Figure 1) source
induction system would mixed initiative one. case human would annotate
different operations service interface semantic definitions. time,
system would attempt induce definitions remaining operations, prompt
user suggestions them. scenario classifier may needed,
since attributes name different operations would likely
semantic type. Moreover, since definitions learnt system may cases
contain erroneous superfluous predicates, user could involved process
checking improving definitions discovered.
46

fiLearning Semantic Definitions Information Sources Internet

9.2 Opportunities Research
number future directions work allow techniques applied
broadly. discuss two directions, improving search algorithm extending
query language.
number known sources grows, search space, necessary develop additional heuristics better direct search toward best definition.
Many heuristic techniques developed ILP community may
applicable source induction problem. pressing perhaps need develop
robust termination condition halting search sufficiently good definition
discovered. number available sources increases, simple timeout used
experiments ineffective certain (more complicated) definitions necessarily
take longer learn others.
Another way increase applicability work extend query language
better describes sources available. Often online sources return complete
set results rather cut list maximum cardinality. example
YahooHotel source described section 7.3.4 returns maximum 20 hotels near given
location, orders according distance. case, recognising specific
ordering tuples produced would useful mediator. second useful
extension query language would ability describe sources using procedural
construct if-then-else. construct needed describe behaviour sources
certain inputs. example, consider YahooGeocoder section 7.3.1, takes
input tuple containing street name, number, zipcode. geocoder unable
locate corresponding address database (because doesnt exist), instead
returning tuples, returns centroid zipcode. Describing behavior
possible using procedural constructs.

Acknowledgments
research based upon work supported part Defense Advanced Research
Projects Agency (DARPA), Department Interior, NBC, Acquisition Services Division, Contract No. NBCHD030010. U.S. Government authorized
reproduce distribute reports Governmental purposes notwithstanding copyright annotation thereon. views conclusions contained herein
authors interpreted necessarily representing official policies
endorsements, either expressed implied, organizations person
connected them.

References
Afrati, F. N., Li, C., & Mitra, P. (2004). containment conjunctive queries using arithmetic comparisions. 9th International Conference Extending Database Technology (EDBT 2004) Heraklion-Crete, Greece.
47

fiCarman & Knoblock

Arens, Y., Knoblock, C. A., & Shen, W.-M. (1996). Query reformulation dynamic
information integration. Journal Intelligent Information Systems - Special Issue
Intelligent Information Integration, 6 (2/3), 99130.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). semantic web. Scientific American,
284 (5), 3443.
Bilenko, M., Mooney, R. J., Cohen, W. W., Ravikumar, P., & Fienberg, S. E. (2003).
Adaptive name matching information integration.. IEEE Intelligent Systems, 18 (5),
1623.
Cameron-Jones, R. M., & Quinlan, J. R. (1994). Efficient top-down induction logic
programs. SIGART Bulletin, 5 (1), 3342.
Carman, M. J., & Knoblock, C. A. (2007). Learning semantic descriptions web information sources. Proceedings Twentieth International Joint Conference
Artificial Intelligence (IJCAI-07) Hyderabad, India.
Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation conjunctive queries
relational data bases. Proceedings 9th ACM Symposium Theory
Computing (STOC), pp. 7790 Boulder, Colorado.
Dhamanka, R., Lee, Y., Doan, A., Halevy, A., & Domingos, P. (2004). imap: Discovering
complex semantic matches database schemas. SIGMOD 04: Proceedings
2004 ACM SIGMOD International Conference Management Data.
Dong, X., Halevy, A. Y., Madhavan, J., Nemes, E., & Zhang, J. (2004). Simlarity search
web services. Proceedings VLDB.
Duschka, O. M. (1997). Query Planning Optimization Information Integration. Ph.D.
thesis, Department Computer Science, Stanford University.
Garcia-Molina, H., Hammer, J., Ireland, K., Papakonstantinou, Y., Ullman, J., & Widom,
J. (1995). Integrating accessing heterogeneous information sources tsimmis.
Proceedings AAAI Symposium Information Gathering, pp. 61-64.
He, A., & Kushmerick, N. (2003). Learning attach semantic metadata web services.
2nd International Semantic Web Conference (ISWC).
He, A., & Kushmerick, N. (2004). Iterative ensemble classification relational data:
case study semantic web services. 15th European Conference Machine
Learning (ECML2004) Pisa, Italy. Springer.
Knoblock, C. A., Minton, S., Ambite, J. L., Ashish, N., Muslea, I., Philpot, A., & Tejada,
S. (2001). ariadne approach web-based information integration. International
Journal Cooperative Information Systems, 10 (1-2), 145169.
Lerman, K., Plangprasopchok, A., & Knoblock, C. A. (2006). Automatically labeling data
used web services. Proceedings 21st National Conference Artificial
Intelligence (AAAI).
48

fiLearning Semantic Definitions Information Sources Internet

Levy, A. Y. (2000). Logic-based techniques data integration. Minker, J. (Ed.), LogicBased Artificial Intelligence. Kluwer Publishers.
Levy, A. Y., Mendelzon, A. O., Sagiv, Y., & Srivastava, D. (1995). Answering queries using
views. Proceedings 14th ACM SIGACT-SIGMOD-SIGART Symposium
Principles Database Systems, pp. 95104 San Jose, Calif.
Markov, Z., & Marinchev, I. (2000). Metric-based inductive learning using semantic height
functions. Proceedings 11th European Conference Machine Learning
(ECML 2000). Springer.
Martin, D., Paolucci, M., McIlraith, S., Burstein, M., McDermott, D., McGuinness, D.,
Parsia, B., Payne, T., Sabou, M., Solanki, M., Srinivasan, N., & Sycara, K. (2004).
Bringing semantics web services: owl-s approach. Proceedings First
International Workshop Semantic Web Services Web Process Composition
(SWSWPC 2004).
Muggleton, S., & Feng, C. (1990). Efficient induction logic programs. Proceedings
1st Conference Algorithmic Learning Theory.
Muggleton, S. (1995). Inverse entailment Progol. New Generation Computing, Special
issue Inductive Logic Programming, 13 (3-4), 245286.
Nedellec, C., Rouveirol, C., Ade, H., Bergadano, F., & Tausend, B. (1996). Declarative
bias ILP. De Raedt, L. (Ed.), Advances Inductive Logic Programming, pp.
82103. IOS Press.
Pazzani, M. J., & Kibler, D. F. (1992). utility knowledge inductive learning.
Machine Learning, 9, 5794.
Perkowitz, M., & Etzioni, O. (1995). Category translation: Learning understand information internet. Proceedings Fourteenth International Joint Conference
Artificial Intelligence (IJCAI-95).
Perkowitz, M., Doorenbos, R. B., Etzioni, O., & Weld, D. S. (1997). Learning understand information internet: example-based approach. Journal Intelligent
Information Systems, 8 (2), 133153.
Pottinger, R., & Halevy, A. Y. (2001). Minicon: scalable algorithm answering queries
using views. VLDB Journal, 10 (2-3).
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Machine
Learning: ECML-93, European Conference Machine Learning, Proceedings, Vol.
667, pp. 320. Springer-Verlag.
Rahm, E., & Bernstein, P. (2001). survey approaches automatic schema matching.
VLDB Journal, 10 (4).
Richards, B. L., & Mooney, R. J. (1992). Learning relations pathfinding. National
Conference Artificial Intelligence, pp. 5055.
49

fiCarman & Knoblock

Roman, D., Keller, U., Lausen, H., de Bruijn, J., Lara, R., Stollberg, M., Polleres, A.,
Feier, C., Bussler, C., & Fensel, D. (2005). Web service modeling ontology. Applied
Ontology, 1 (1), 77106.
Ullman, J. D. (1989). Principles Database Knowledge-Base Systems, Vol. 2. Computer Science Press, Rockville, Maryland.
Weber, I., Tausend, B., & Stahl, I. (1995). Language series revisited: complexity
hypothesis spaces ILP. Proceedings 8th European Conference Machine
Learning, Vol. 912, pp. 360363. Springer-Verlag.
Widom, J. (1995). Research problems data warehousing. CIKM 95: Proceedings
fourth International Conference Information Knowledge Management, pp.
2530. ACM Press.
Wiederhold, G. (1992). Mediators architecture future information systems. Computer, 25 (3), 3849.
Wiederhold, G. (Ed.). (1996). Intelligent Integration Information. Kluwer Academic
Publishers, Boston MA.
Winkler, W. (1999). state record linkage current research problems. Tech. rep.,
Statistical Research Division, U.S. Bureau Census, Washington, DC.
Yan, L. L., Miller, R. J., Haas, L. M., & Fagin, R. (2001). Data-driven understanding
refinement schema mappings. SIGMOD 01: Proceedings 2001 ACM
SIGMOD International Conference Management data.
Zelle, J. M., Thompson, C. A., Califf, M. E., & Mooney, R. J. (1995). Inducing logic
programs without explicit negative examples. Proceedings Fifth International
Workshop Inductive Logic Programming.

50


