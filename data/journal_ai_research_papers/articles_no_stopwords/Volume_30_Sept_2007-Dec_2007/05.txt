Journal Articial Intelligence Research 30 (2007) 659-684

Submitted 06/07; published 12/07

Learning Play Using Low-Complexity Rule-Based Policies:
Illustrations Ms. Pac-Man
Istvn Szita
Andrs Lrincz

szityu@eotvos.elte.hu
andras.lorincz@elte.hu

Dept. Information Systems
Etvs University, Hungary, H-1117

Abstract
article propose method deal certain combinatorial reinforcement learning tasks. demonstrate approach popular Ms. Pac-Man game.
dene set high-level observation action modules, rule-based policies
constructed automatically. policies, actions temporally extended, may
work concurrently. policy agent encoded compact decision list. components list selected large pool rules, either hand-crafted
generated automatically. suitable selection rules learnt cross-entropy
method, recent global optimization algorithm ts framework smoothly. Crossentropy-optimized policies perform better hand-crafted policy, reach score
average human players. argue learning successful mainly (i) policies
may apply concurrent actions thus policy space suciently rich, (ii) search
biased towards low-complexity policies therefore, solutions compact description
found quickly exist.

1. Introduction
last two decades, reinforcement learning (RL) reached mature state,
laid solid foundations. large variety algorithms, including valuefunction-based, direct policy search hybrid methods. reviews subjects,
see, e.g., books Bertsekas Tsitsiklis (1996) Sutton Barto (1998).
basic properties many algorithms relatively well understood, e.g. conditions
convergence, complexity, eect various parameters, although needless say
still lots important open questions. plenty test problems
(like various maze-navigation tasks, pole-balancing, car hill etc.)
capabilities RL algorithms demonstrated, number large-scale RL
applications growing steadily. However, current RL algorithms far
out-of-the-box methods, still need demonstrations showing RL
ecient complex tasks.
think games (including diverse set classical board games, card games, modern computer games, etc.) ideal test environments reinforcement learning. Games
intended interesting challenging human intelligence therefore,
ideal means explore articial intelligence still missing. Furthermore,
games well RL paradigm: goal-oriented sequential decision problems,
decision long-term eects. many cases, hidden information, random
events, unknown environment, known unknown players account (part of) diculty
c 2007 AI Access Foundation. rights reserved.


fiSzita & Lrincz

playing game. circumstances focus reinforcement learning. Games
attractive testing new methods: decision space huge cases,
nding good strategy challenging task.
another great advantage using games test problems: rules games
xed, danger `tailoring task algorithm' i.e., tweak rules
and/or environment meet capabilities proposed RL algorithm
reduced, compared, e.g., various maze navigation tasks.
RL tried many classical games, including checkers (Samuel, 1959), backgammon (Tesauro, 1994), chess (Baxter, Tridgell, & Weaver, 2001). hand,
modern computer games got spotlight recently, many
successful attempts learn AI tools. Notable exceptions are, example, roleplaying game Baldur's Gate (Spronck, Sprinkhuizen-Kuyper, & Postma, 2003), real-time
strategy game Wargus (Ponsen & Spronck, 2004), possibly, Tetris (Szita & Lrincz,
2006). games pose new challenges RL, example, many observations
considered parallel, observation space action space huge.
spirit, decided investigate arcade game Ms. Pac-Man. game
interesting largely unsolved, imposes several important questions
RL, overview Section 8. provide hand-coded high-level actions
observations, task RL learn combine good policy.
apply rule-based policies, easy interpret enable one include
human domain-knowledge easily. learning, apply cross-entropy method,
recently developed general optimization algorithm. show hybrid approach
successful either tabula rasa learning hand-coded strategy alone.
next section introduce Ms. Pac-Man game briey discuss
formalized reinforcement learning task. sections 3 4, shall shortly describe
cross-entropy optimization method rule-based policies, respectively. section 5,
details learning experiments provided, section 6 present results.
Section 7 provides review related literature, nally, section 8 summarize
discuss approach emphasis implications RL problems.

2. Pac-Man Reinforcement Learning
video-game Pac-Man rst released 1979, reached immense success.
considered one popular video games date (Wikipedia, 2006).
player maneuvers Pac-Man maze (see Fig. 1), Pac-Man eats dots
maze. particular maze 174 dots,1 one worth 10 points. level
nished dots eaten. make things dicult, four
ghosts maze try catch Pac-Man, succeed, Pac-Man loses life.
Initially, three lives, gets extra life reaching 10,000 points.
four power-up items corners maze, called power dots (worth
40 points). Pac-Man eats power dot, ghosts turn blue short period (15
seconds), slow try escape Pac-Man. time, Pac-Man
1. maze original Pac-Man game slightly dierent. description applies opensource Pac-Man implementation Courtillat (2001). two versions equivalent terms
complexity entertainment value.

660

fiLearning play Ms. Pac-Man

Figure 1: snapshot Pac-Man game
able eat them, worth 200, 400, 800 1600 points, consecutively. point
values reset 200 time another power dot eaten, player would want
eat four ghosts per power dot. ghost eaten, remains hurry back center
maze ghost reborn. certain intervals, fruit appears near center
maze remains while. Eating fruit worth 100 points.
investigations restricted learning optimal policy rst level,
maximum achievable score 174 10 + 4 40 + 4 (200 + 400 + 800 + 1600) = 13900 plus
100 points time fruit eaten.
original version Pac-Man, ghosts move complex deterministic route,
possible learn deterministic action sequence require observations.
Many patterns found enthusiastic players. Pac-Man's sequels,
notably Ms. Pac-Man, randomness added movement ghosts. way,
single optimal action sequence, observations necessary optimal decision
making. respects, game play mostly unchanged.
implementation, ghosts moved randomly 20% time straight towards
Pac-Man remaining 80%, ghosts may turn back (following Koza, 1992, Chapter
12). emphasize presence randomness, shall refer implementation
Ms. Pac-Man-clone.

2.1 Ms. Pac-Man RL Task
Ms. Pac-Man meets criteria reinforcement learning task. agent make
sequence decisions depend observations. environment stochastic
(because paths ghosts unpredictable). well-dened reward function
(the score eating things), actions inuence rewards collected future.
661

fiSzita & Lrincz

full description state would include (1) whether dots eaten (one
bit dot one power dot), (2) position direction Ms. Pac-Man,
(3) position direction four ghosts, (4) whether ghosts blue (one bit
ghost), so, long remain blue (in range 1 15 seconds) (5)
whether fruit present, time left appears/disappears (6) number
lives left. size resulting state space astronomical, kind function
approximation feature-extraction necessary RL.
action space much smaller, four basic actions: go north/south/east/west. However, typical game consists multiple hundreds steps,
number possible combinations still enormous. indicates need temporally
extended actions.
moderate amount domain knowledge Ms. Pac-Man: one, quite
easy dene high-level observations action modules potentially useful.
hand, constructing well-performing policy seems much dicult. Therefore,
provide mid-level domain knowledge algorithm: use domain knowledge
preprocess state information dene action modules. hand,
role policy search reinforcement learning combine observations modules
rule-based policies nd proper combination.

3. Cross-Entropy Method
goal optimize rule-based policies performing policy search space
legal rule-based policies. search apply cross-entropy method (CEM),
recently published global optimization algorithm (Rubinstein, 1999). aims nd
(approximate) solution global optimization tasks following form

x := arg max f (x).
x

f general objective function (e.g., need assume continuity dierentiability). summarize mechanism method briey (see section 7.2
overview applications).

3.1 Intuitive Description
optimization algorithms maintain single candidate solution x(t) time
step, CEM maintains distribution possible solutions. distribution, solution
candidates drawn random. essentially random guessing, nice trick
turned highly eective optimization method.
3.1.1 Power Random Guessing

Random guessing overly simple `optimization' method: draw many samples
xed distribution g , select best sample estimation optimum.
limit case innitely many samples, random guessing nds global optimum.
two notes here: (i) shown Wolpert Macready (1997),
general problems, uniform random guessing worse method, (ii)
nonetheless, practical problems, uniform random guessing extremely inecient.
662

fiLearning play Ms. Pac-Man

Thus, random guessing safe start with, one proceeds collection
experience, limited much possible.
eciency random guessing depends greatly distribution g
samples drawn. example, g sharply peaked around x , samples
may sucient get good estimate. case opposite, distribution
sharply peaked around x 6= x : tremendous number examples may needed get
good estimate global optimum. Naturally, nding good distribution least
hard nding x .
3.1.2 Improving Efficiency Random Guessing

drawing moderately many samples distribution g , may able give
acceptable approximation x , may still obtain better sampling distribution.
basic idea CEM selects best samples, modies g
becomes peaked around them. Consider example, x 0-1 vector g
Bernoulli distribution coordinate. Suppose drawn 1000 samples
selected 10 best. see majority selected samples, ith coordinate
1, CEM shifts Bernoulli distribution corresponding component towards 1.
Afterwards, next set samples drawn already modied distribution.
idea seems plausible: majority best-scoring samples ith coordinate
1, structure tness landscape, may hope ith
coordinate x 1. follows, describe update rule CEM
formal way sketch derivation.

3.2 Formal Description Cross-Entropy Method
pick g family parameterized distributions, denoted G , describe
algorithm iteratively improves parameters distribution g .
Let N number samples drawn, let samples x(1) , . . . , x(N )
drawn independently distribution g . R, set high-valued samples,

L := {x(i) | f (x(i) ) , 1 N },
provides approximation level set

L := {x | f (x) }.
Let U uniform distribution level set L . large values , distribution peaked around x , would suitable random sampling. raises two
potential problems: (i) large values L contain points (possibly none),
making accurate approximation impossible, (ii) level set L usually member
parameterized distribution family.
rst problem easy avoid choosing lower values . However,
make compromise, setting low would inhibit large improvement steps.
compromise achieved follows: CEM chooses ratio [0, 1] adjusts L
set best N samples. corresponds setting := f (x(N ) ), provided
samples arranged decreasing order values. best N samples called
elite samples. practice, typically chosen range [0.02, 0.1].
663

fiSzita & Lrincz

problem solved changing goal approximation: CEM chooses
distribution g distribution family G approximates best empirical distribution L . best g found minimizing distance G uniform
distribution elite samples. measure distance cross-entropy distance
(often called Kullback-Leibler divergence). cross-entropy distance two distributions
g h dened
Z
g(x)
DCE (g||h) = g(x) log
dx
h(x)
general form cross-entropy method summarized Table 1. known
mild regularity conditions, CE method converges probability 1 (Margolin,
2004). Furthermore, suciently large population, global optimum found
high probability.
input: G
input: g0 G
input: N
input:
input:
0 1,
1 N ,
draw x(i) distribution gt
compute := f (x(i) )
sort -values descending order
t+1 := fN
Et+1 := {x(i) | f (x(i) ) t+1 }
gt+1 := arg mingG DCE (g||Uniform(Et+1 ))
end loop

%
%
%
%
%
%

parameterized distrib. family
initial distribution
population size
selection ratio
number iterations
CEM iteration main loop

% draw N samples
% evaluate
% level set threshold
% get elite samples
% get nearest distrib. G

Table 1: Pseudo-code general cross-entropy method

3.3 Cross-Entropy Method Bernoulli Distribution
many parameterized distribution families, parameters minimum cross-entropy
member computed easily simple statistics elite samples. provide
formulae Bernoulli distributions, needed policy learning procedure
detailed next section. Derivations well list discrete continuous
distributions simple update rules found tutorial de Boer, Kroese,
Mannor, Rubinstein (2004).
Let domain optimization = {0, 1}m , component drawn
independent Bernoulli distributions, i.e., G = Bernoullim . distribution g G parameterized m-dimensional vector p = (p1 , . . . , pm ). using g sampling,
664

fiLearning play Ms. Pac-Man

component j sample x

1, probability pj ;
xj =
0, probability 1 pj .
drawing N samples x(1) , . . . , x(N ) xing threshold value , let E denote set
elite samples, i.e.,
E := {x(i) | f (x(i) ) }
notation, distribution g 0 minimum CE-distance uniform distribution elite set following parameters:

p0 := (p01 , . . . , p0m ),
P
P
(i)
(i)
x(i) E (xj = 1)
x(i) E (xj = 1)
0
P
=
pj :=
N
x(i) E 1

(1)

words, parameters g 0 simply component wise empirical probabilities
1's elite set. derivation rule, see tutorial de Boer et al. (2004).
Changing distribution parameters p p0 coarse, cases,
applying step-size parameter preferable. resulting algorithm summarized
Table 2.
input: p0 = (p0,1 , . . . , p0,m )
input: N
input:
input:
0 1,
1 N ,
draw x(i) Bernoullim (pt )
compute := f (x(i) )
sort -values descending order
t+1 := fN
Et+1 := {x(i) | f (x(i) ) t+1 }
P

(i)
p0j :=
x(i) E (xj = 1) /( N )
pt+1,j := p0j + (1 ) pt,j
end loop

%
%
%
%
%

initial distribution parameters
population size
selection ratio
number iterations
CEM iteration main loop

% draw N samples
% evaluate
%
%
%
%

level set threshold
get elite samples
get parameters nearest distrib.
update step-size

Table 2: Pseudo-code cross-entropy method Bernoulli distributions
need optimize functions = {1, 2, . . . , K}m K > 2.
simplest case, distributions domain parameterized
K parameters:
PK
p = (p1,1 , . . . , p1,K ; . . . ; pm,1 , . . . , pm,K ) 0 pj,k 1 k=1 pj,k = 1 j (this
special case multinomial distribution).
update rule parameters essentially Eq. 1 Bernoulli case:
P
P
(i)
(i)
(i) E (xj = k)
x(i) E (xj = k)
x
0
P
=
.
(2)
pj,k :=
N
x(i) E 1
665

fiSzita & Lrincz

Note constraint

PK

0
k=1 pj,k

= 1 satised automatically j .

4. Rule-Based Policies
basic formulation, rule sentence form [Condition] holds,
[Action]. rule-based policy set rules mechanism breaking ties, i.e.,
decide rule executed, multiple rules satised conditions.
Rule-based policies human-readable, easy include domain knowledge,
able represent complex behaviors. reasons, often used many
areas articial intelligence (see section 7.3 short overview related literature).
order apply rule-based policies Ms. Pac-Man, need specify four things:
(1) possible actions (2) possible conditions
constructed observations, (3) make rules conditions actions,
(4) combine rules policies. answers described following
sections.

4.1 Action Modules
dening action modules Ms. Pac-Man, listed modules easy
implement considered potentially useful (see Table 3). way, kept human work minimum, still managed formalize part domain knowledge
problem. consequence, list action modules means optimal: actions could eective appropriate denition, others
may superuous. example, four dierent modules ghost avoidance:
FromGhost escapes nearest ghost, without considering position
ghosts; ToLowerGhostDensity tries take account inuence multiple ghosts;
FromGhostCenter moves geometrical center ghosts, thus, able avoid
surrounded trapped, but, hand, easily bump ghost
so; nally, ToGhostFreeArea considers whole board search safe
location, agent avoid shepherded ghosts. modules
may strengths weaknesses, possibly combination needed
success. actions, potentially useful, listed
(for example, moving towards fruit).
Note modules exclusive. example, escaping
ghosts, Ms. Pac-Man may prefer route dots eaten, may want
head towards power dot. Without possibility concurrent actions, performance
Ms. Pac-Man agent may reduced considerably (which investigated experimental
section 5.3).
need mechanism conict resolution, dierent action modules may suggest dierent directions. assigning priorities modules. agent
switches action module, decides priority. decision,
learning decision part learning task.2
2. Action priorities learnt indirectly: rule xed priority, action switched
rule, inherits priority. action switched dierent rules
dierent priorities. mechanism described detail section 4.6.

666

fiLearning play Ms. Pac-Man

Table 3: List action modules used rule construction.
Name

Description

ToDot
ToPowerDot
FromPowerDot

Go towards nearest dot.
Go towards nearest power dot.
Go direction opposite nearest power
dot.
Go towards nearest edible (blue) ghost.
Go direction opposite nearest ghost.
Go towards maximally safe junction.
four directions, safety nearest junction estimated direction. Ms. PacMan n steps away junction
nearest ghost k steps away, safety
value junction n k . negative value
means Ms. Pac-Man possibly cannot reach
junction.
Go direction maximizes Euclidean
distance geometrical center ghosts.
Go current direction, choose
random available action (except turning back)
impossible.
go direction cumulative ghost
density decreases fastest. ghost denes
density cloud (with radius = 10 linear decay), cumulative ghost density
calculated.
Choose location board minimum ghost distance largest, head towards
shortest path.

ToEdGhost
FromGhost
ToSafeJunction

FromGhostCenter
KeepDirection
ToLowerGhostDensity

ToGhostFreeArea

667

fiSzita & Lrincz

Table 4: List observations used rule construction. Distances denote length
shortest path, unless noted otherwise. Distance particular object type +
object exists moment.
Name

Description

Constant
NearestDot
NearestPowerDot
NearestGhost
NearestEdGhost
MaxJunctionSafety

Constant 1 value.
Distance nearest dot.
Distance nearest power dot.
Distance nearest ghost.
Distance nearest edible (blue) ghost.
four directions, safety nearest
junction direction estimated, dened
description action ToSafeJunction.
observation returns value maximally safe junction.
Euclidean distance geometrical center
ghosts.
Euclidean distance geometrical center
uneaten dots.
ghost denes density cloud (with radius
= 10 linear decay). Returns value
cumulative ghost density.
travelling salesman distance ghosts:
length shortest route starts
Ms. Pac-Man reaches four ghosts (not
considering movement).

GhostCenterDist
DotCenterDist
GhostDensity
TotalDistToGhosts

implemented following mechanism: decision agent concerns
action modules: agent either switch or, switch action module. is,
principle, agent able use subset action modules, instead selecting
single one time step. Basically, module highest priority decides direction
Ms. Pac-Man. one equally ranked directions, lower-priority
modules checked. direction cannot decided checking switched-on modules
order decreasing priority (for example, module switched on, two directions
ranked equally switched-on modules), random direction chosen.
Ms. Pac-Man make decisions time advances whole grid cell (the
mechanism ensures never stands still), according 25 game ticks approx. 0.2
seconds simulated game time.

4.2 Observations, Conditions Rules
Similarly actions, easily dene list observations potentially useful
decision making. observations descriptions summarized Table 4.
668

fiLearning play Ms. Pac-Man

Modules could improved many ways, example, checking whether
enough time intercept edible ghosts calculating NearestEdGhost taking
consideration movement ghosts calculating NearestGhost, NearestEdGhost
MaxJunctionSafety. kept implementation modules simple possible.
designed reasonable modules, eort made make module denitions
optimal, complete non-redundant.
necessary tools dening conditions rule. typical condition
true observations given range. note status action module
important proper decision making. example, agent may decide
ghost close, switches modules except escape module. Therefore
allow conditions check whether action module `on' `o'.
sake simplicity, conditions restricted form [observation]
< [value], [observation] > [value], [action]+, [action]-, conjunction
terms. example,

(NearestDot<5) (NearestGhost>8) (FromGhost+)
valid condition rules.
conditions actions, rules constructed easily. implementation, rule form [Condition], [Action]. example,

(NearestDot<5) (NearestGhost>8) (FromGhost+)
FromGhostCenter+
valid rule.

4.3 Constructing Policies Rules
Decision lists standard forms constructing policies single rules.
approach pursue here, too. Decision lists simply lists rules, together
mechanism decides order rules checked.
rule priority assigned. agent make decision, checks
rule list starting ones highest priority. conditions rule fullled,
corresponding action executed, decision-making process halts.
Note principle, priority rule dierent priority action
modules. However, sake simplicity, make distinction: rule priority
k switches action module, priority action module taken k .
Intuitively, makes sense: important rule activated, eect
important. rule priority k switches module, executed, regardless
priority module.
may worth noting many possible alternatives ordering rules
actions:

rule could xed priority, part provided domain knowledge
(Spronck, Ponsen, Sprinkhuizen-Kuyper, & Postma, 2006).
priority rule could free parameter learned CEM
method.
669

fiSzita & Lrincz

Instead absolute priorities, agent could learn relative ordering rules
(Timuri, Spronck, & van den Herik, 2007).
order rules could determined heuristic decision mechanism.
example, generality rule e.g., rules few/many conditions large/small domains could taken account. heuristics used linear
classier systems (see e.g. work Bull & Kovacs, 2005)
principle, one would nd interesting solutions using computer minimal
bias `domain knowledge'. regard, eciency simple priority management method satisfactory, experiment priority heuristics.

4.4 Example
Let us consider example shown Table 5. rule-based policy Ms. PacMan agent.
Table 5: hand-coded policy playing Ms. Pac-Man. Bracketed numbers denote
priorities, [1] highest priority.
[1]
[1]
[2]
[2]
[3]
[3]
[3]
[3]










NearestGhost<4 FromGhost+
NearestGhost>7 JunctionSafety>4 FromGhostNearestEdGhost>99 ToEdGhostNearestEdGhost<99 ToEdGhost+
Constant>0 KeepDirection+
FromPowerDot- ToPowerDot+
GhostDensity<1.5 NearestPowerDot<5 FromPowerDot+
NearestPowerDot>10 FromPowerDot-

rst two rules manage ghost avoidance: ghost close, agent
ee, gets safe distance. Ghost avoidance priority
activities. next two rules regulate edible ghost
board, agent chase (the value NearestEdGhost innity (> 99)
edible ghosts, 41 board, are). activity
relatively high priority, eating ghosts worth lots points, must done
blueness ghosts disappears, must done quickly. fth rule says
agent turn back, directions equally good. rule prevents
unnecessary zigzagging (while dots eaten), surprisingly eective.
remaining rules tweak management power dots. Basically, agent prefers eat
power dot. However, blue ghosts board, power dot resets
score counter 200, bad move. Furthermore, ghost density low around
agent, probably hard collect ghosts, preferable
wait eating power dot.
670

fiLearning play Ms. Pac-Man

4.5 Mechanism Decision Making
mechanism decision making depicted Fig 2. short, (hidden) state-space
world Ms. Pac-Man Ghosts. dynamics (hidden) statespace determines vector observations, checked conditions.
conditions rule satised, corresponding action module switched o.
consequence, multiple actions may eect once. example, decision depicted
Fig. 2 sets two actions work together.

Figure 2: Decision-making mechanism Ms. Pac-Man agent. time step t,
agent receives actual observations state action modules.
checks rules script order, executes rst rule satised
conditions.
Initially, action module switched-o state. module switched
on, remains either explicitly switched another module
priority switched replaces it.

4.6 Learning Rule-Based Policies CEM
apply CEM searching space rule-based policies. Learning composed
three phases: (1) generation random policies drawn according current parameter
set, (2) evaluation policies, consists playing game Ms. Pac-Man
measure score, (3) updating parameter set using CEM update rules.
4.6.1 Drawing Random Scripts Predefined Rule-Base

Suppose predened rule-base containing K rules (for example, one listed
Appendix A). policy rule slots. slot lled K rules,
671

fiSzita & Lrincz

left empty. result, policies could contain rules, possibly much less.
rule slot xed priority, too, set {1, 2, 3}.3 priority rule slot
change learning. Learning can, however, push important rule high-priority
slot low-priority one, vice versa.
1 m, slot lled rule rule-base probability pi ,
left empty probability 1 pi . decided slot
PKbe lled,
particular rule j (1 j K ) selected probability qi,j , j=1 qi,j = 1
slot {1, . . . , m}. result, policies could contain rules, possibly much
less. pi values qi,j values learnt simultaneously cross-entropy
method (Table 2), using update rules (1) (2), respectively. gives total
+ K parameters optimize (although eective number parameters much
less, qi,j values unused slots irrelevant). Initial probabilities set
pi = 1/2 qi,j = 1/K .
4.6.2 Drawing Random Rules without Predefined Rule-Base

studied situations lessened domain knowledge; use predened rulebase. Script generation kept same, rule-base K rules generated
randomly. case generated dierent rule-bases rule slots; low
ratio meaningful rules counteracted increased rule variety.
random rule random pair randomly drawn condition set randomly
drawn action. Random condition sets contained 2 conditions. random action constructed follows: action module selected uniformly set modules listed
Table 3, switched probability 50%. construction random condition starts uniformly random selection module either Table 3 Table
4. selected module action, condition [action]- [action]+
equal probability. selected module observation, condition
[observation]<[value] [observation]>[value] equal probability, [value]
selected uniformly ve-element set. values set determined separately observation module follows: played 100 games using xed policy
recorded histogram values observation. Subsequently, ve-element set
determined would split histogram regions equal area. example,
value set NearestGhost {12, 8, 6, 5, 4}.
design random rule generation procedure contains arbitrary elements (e.g.
number conditions rule, number values observation compared
to). intuition behind procedure generate rules suciently versatile,
ratio meaningless rules (e.g. rules unsatisable conditions) large.
However, optimization form done point.

5. Description Experiments
According assumptions, eectiveness described architecture based
three pillars: (1) human domain knowledge provided modules rules; (2)
3. According preliminary experiments, quality learned policy improve increasing
priority set number slots.

672

fiLearning play Ms. Pac-Man

eectiveness optimization algorithm; (3) possibility concurrent actions.
Below, describe set experiments designed test assumptions.

5.1 Full Architecture
rst experiment, random rules used. construction, use modules
dened sections 4.1 4.2. second experiment, rules generated randomly,
hand-coded. case, role learning determine rules
used.
5.1.1 Learning Random Rule Construction

rst experiment, rule-base generated randomly, described section 4.6.2.
number rule slots xed = 100 (priorities distributed evenly), one
containing K = 100 randomly generated rules. values K selected
coarse search parameter space.
parameters CEM follows: population size N = 1000, selection ratio
= 0.05, step size = 0.6.4 values fairly standard CEM,
tried varying them. step, probabilities using rule slot (that
is, values pi , qi,j ) slightly decreased, using decay rate = 0.98.
larger decay rate, useful rules annulled often. hand, smaller
decay aect performance, many superuous rules left policies.
score given policy huge variance due random factors game.
Therefore, obtain reliable tness estimations, score policy averaged
3 subsequent games. Learning lasted 50 episodes, sucient tune
probability close either 0 1. performed 10 parallel training runs. experiment
type denoted CE-RandomRB.
5.1.2 Learning Hand-Coded Rules

second experiment constructed rule-base K = 42 hand-coded rules (shown
Appendix A) thought potentially useful. could placed one
= 30 rule slots.5 parameters experiment identical previous
one. experiment type denoted CE-FixedRB.

5.2 Eect Learning Algorithm
following experiment, compared performance CEM simple stochastic
gradient optimization. single comparison sucient measure eciency
CEM; serves provide point reference. comparison relevant,
algorithms similar complexity move gradually towards
best samples found far. dierence SG maintains single solution
4. Note per-episode learning rate. would correspond per-instance learning rate
0 = /( N ) = 0.012 on-line learning algorithm.
5. contrast previous experiment, rules meaningful potentially useful. Therefore
need large pool rules, much lower used. found algorithm
fairly insensitive choice m; signicant changes performance observed parameter
modied factor 3.

673

fiSzita & Lrincz

candidate time, whereas CEM maintains distribution solutions. Thus, CEM
maintains memory solutions becomes less fragile occasional wrong parameter
changes.
particular form stochastic gradient search following: initial policy
drawn random (consisting 6 rules). that, generated 100 random mutation
current solution candidate step, evaluated obtained policies. bestperforming mutation chosen next solution candidate. Mutations generated
using following procedure: (1) rule, condition changed random new
condition probability 0.05; (2) rule, action changed random new
action probability 0.05. listed parameter values (number rules policy, number
mutated policies, probabilities mutation) results coarse parameter-space
optimization.
number episodes set 500. way, evaluated number
dierent policies (50,000) CEM experiments. random rule-base
xed rule-base experiments repeated using stochastic gradient method, executing
10 parallel training runs. resulting policies denoted SG-RandomRB SGFixedRB, respectively.

5.3 Eect Parallel Actions
According assumptions, possibility parallel actions plays crucial role
success architecture. conrm assumption, repeated previous experiments
concurrent actions disabled. agent switches action module,
action modules switched automatically. experiment types denoted
CE-RandomRB-1action, CE-FixedRB-1action, SG-RandomRB-1action SGFixedRB-1action.

5.4 Baseline Experiments
order isolate assess contribution learning, performed two additional
experiments dierent amounts domain knowledge learning. Furthermore,
asked human subjects play game.
5.4.1 Random Policies

rst non-learning experiment, used rule-base 42 hand-coded rules (identical
rule-base CE-FixedRB). Ten rules selected random, random priorities
assigned them. measured performance policies constructed way.
5.4.2 Hand-Coded Policy

second non-learning experiment, hand-coded rules priorities,
is, hand-coded full policy. policy shown Table 5, constructed
trial-and-error. Naturally, policy constructed knowing results
learning experiments.
674

fiLearning play Ms. Pac-Man

Table 6: Ms. Pac-Man results. See text details. Abbreviations: CE: learning
cross-entropy method, SG: learning stochastic gradient, randomRB:
randomly generated rule-base, fixedRB: xed, hand-coded rule-base, 1action:
one action module work time.
Method

Avg. Score

(25%/75% percentiles)

6382
4135
5449

(6147/6451)
(6682/9369)
(3356/5233)
(4843/6090)

CE-randomRB-1action
CE-fixedRB-1action
SG-randomRB-1action
SG-fixedRB-1action

5417
5631
2267
4415

(5319/5914)
(5705/5982)6
(1770/2694)
(3835/5364)

Random policy
Hand-coded policy
Human play

676
7547
8064

(140/940)
(6190/9045)
(5700/10665)

CE-randomRB
CE-fixedRB
SG-randomRB
SG-fixedRB

8186

5.4.3 Human Play

nal experiment, human subjects asked play rst level Ms. PacMan measured performance. subjects played Pac-Man and/or
similar games before, none experienced player.

6. Experimental Results
Human experiments performed rst level open-source Pac-Man clone
Courtillat (2001). experiments applied Delphi re-implementation
code.
learning experiments, 10 parallel learning runs executed, one 50
episodes. training period sucient tune probabilities close either 0
1, learned policy could determined unambiguously cases. obtained
policy tested playing 50 consecutive games, giving total 500 test games per
experiment. non-learning experiments agents played 500 test games, too, using
random policies hand-coded policy, respectively. human subject played 20
games, giving total 100 test games. Results summarized Table 6. provide
25% 75% percentile values instead variances, distribution scores
highly non-Gaussian.
6. fact average smaller 25% percentile caused highly skewed distribution
scores. games, agent reached score range 5800 300, except games
extremely low score. games aect 25% percentile lowered average
signicantly.

675

fiSzita & Lrincz

[1]
[1]
[2]
[2]
[2]
[3]








NearestGhost<3 FromGhost+
MaxJunctionSafety>3 FromGhostNearestEdGhost>99 ToPowerDot+
NearestEdGhost<99 ToEdGhost+
GhostDensity<1.5 NearestPowerDot<5 FromPowerDot+
Constant>0 ToCenterofDots+

Figure 3: Best policy learned CE-fixedRB. Average score 50 games: 9480.
[1]
[1]
[1]
[2]
[2]
[2]
[3]









MaxJunctionSafety>2.5 ToLowerGhostDensity- FromGhostNearestGhost<6 MaxJunctionSafety<1 FromGhost+
NearestGhost>6 FromGhostCenter- ToEdGhost+
ToEdGhost- CenterOfDots>20 ToEdGhost+
ToEdGhost- NearestEdGhost<99 ToEdGhost+
NearestDot>1 GhostCenterDist>0 KeepDirection+
ToGhostFreeArea- ToDot- ToPowerDot+

Figure 4: Best policy learned CE-randomRB. Average score 50 games: 7199.
Note presence always-true (and thus, superuous) conditions
ToLowerGhostDensity-, FromGhostCenter-, ToGhostFreeArea- ToDot-.

Fig. 3 shows best individual policy learned CE-fixedRB, reaching 9480 points
average. Ghost avoidance given highest priority, turned ghost
close. Otherwise Ms. Pac-Man concentrates eating power dots subsequently
eating blue ghosts. takes care eat power dot blue
ghosts board, otherwise would miss opportunity eat 1600-point
ghost (and possibly several others, too). lowest priority setting, agent looks
ordinary dots, although rule eect previous rules decide
direction (for example, endgame power dots left ghosts
original form).
Policies learnt CE-randomRB behave similarly ones learnt CE-fixedRB,
although behavior somewhat obscured superuous conditions and/or rules,
demonstrated clearly example policy shown Fig. 4. noise generated
random rules, algorithm often fails learn correct priorities various
activities.
eect enabling/disabling concurrent actions signicant. instructive
take look best policy learned CE-fixedRB-1action shown Fig. 5:
agent concentrate eating ghosts, major source reward. However,
cannot use modules necessary ghost avoidance long-term survival.
results show CEM performs signicantly better stochastic gradient
learning. believe, however, dierence could lowered thorough search
parameter space. SG many global optimization methods evolutionary
methods simulated annealing could reach similar performances CEM. According
de Boer et al. (2004) applications cited section 7.2, advantage CEM
676

fiLearning play Ms. Pac-Man

[2] NearestEdGhost>99 ToPowerDot+
[2] NearestEdGhost<99 ToEdGhost+

Figure 5: Best policy learned CE-fixedRB-1action. Average score 50 games:
6041.

maintains distribution solutions reach robust performance
little eort, requiring little tuning parameters: canonical set
parameters (0.01 0.1, 0.5 0.8, population large possible)
performance method robust. claim coincides experiences
parameter-optimization process.
Finally, interesting analyze dierences tactics human
computer players. One fundamental tactic human players try lure
ghosts close Ms. Pac-Man ghosts close other. way,
eaten fast turn blue. behavior evolved
experiments. Besides, tactics CEM chance discover,
lacking appropriate sensors. example, human player (and does) calculate
time remaining blue period, approximate future position ghosts,
on.

7. Related Literature
section, review literature learning Pac-Man game, various components learning architecture: cross-entropy method, rule-based policies,
concurrent actions.

7.1 Previous Work (Ms.) Pac-Man
Variants Pac-Man used previously several studies. direct comparison
performances possible cases, however, simplied versions
game used studies.
Koza (1992) uses Ms. Pac-Man example application genetic programming.
uses dierent score value fruit (worth 2000 points instead 100 points used
here), shape board (and consequently, number dots) dierent,
therefore scores cannot directly compared. However, Koza reports (on p. 355)
Pac Man could scored additional 9000 points captured four monsters
four occasions turned blue. score, one reported,
translates approximately 5000 points scoring system.
Lucas (2005) uses full-scale Ms. Pac-Man game test problem. trains
neural network position evaluator hand-crafted input features. purposes
training, uses evolutionary strategy approach. obtained controller able
reach 4781 116 points, averaged 100 games.
Bonet Stauer (1999) restrict observations 10 10 window centered Ms. PacMan, uses neural network temporal-dierence learning learn reactive con677

fiSzita & Lrincz

troller. series increasingly dicult learning tasks, able teach basic
pellet-collecting ghost-avoidance behaviors greatly simplied versions game:
used simple mazes containing power dots one ghost.
Gallagher Ryan (2003) denes behavior agent parameterized nite
state automata. parameters learnt population-based incremental learning,
evolutionary method similar CEM. run simplied version Pac-Man;
single ghost power dots, takes away complexity game.
Tiong (2002) codes rule-based policies Pac-Man hand, uses learning
improve them. tests, similarly ours, based Pac-Man implementation
Courtillat (2001), limits number ghosts 1. best-performing rule set
reaches 2164 points average maximal 2700. However, results likely
scale well increasing number ghosts: ghost eaten 1.4 times
average (out possible 4 times per game).7

7.2 Cross-Entropy Method
cross-entropy method Rubinstein (1999) general algorithm global optimization
tasks, bearing close resemblance estimation-of-distribution evolutionary methods (see e.g.
paper Muehlenbein, 1998). areas successful application range combinatorial optimization problems optimal buer allocation problem (Allon, Kroese, Raviv,
& Rubinstein, 2005), DNA sequence alignment (Keith & Kroese, 2002) independent process analysis (Szab, Pczos, & Lrincz, 2006).
cross-entropy method several successful reinforcement learning applications, too:
Dambreville (2006) uses CEM learning input-output hierarchical HMM controls
predator agent partially observable grid world; Menache, Mannor, Shimkin (2005)
use radial basis function approximation value function continuous maze navigation task, use CEM adapt parameters basis functions; nally, Mannor,
Rubinstein, Gat (2003) apply CEM policy search simple grid world maze navigation problem. Recently, cross-entropy method applied successfully
game Tetris Szita Lrincz (2006).

7.3 Rule-Based Policies
representation policies rule sequences widespread technique complex problems computer games. example, many Pac-Man-related papers listed
use rule-based representation.
Learning classier systems (Holland, 1986) genetic-algorithm based methods
evolve suitable rules given task. Bull (2004) gives excellent general overview
pointers references. Hayek machine Baum (1996) similar architecture,
agents (corresponding simple rules) dene economical system: make bids
executing tasks hope obtain rewards. Schaul (2005) applies
architecture Sokoban game.
Dynamic scripting (Spronck et al., 2006) another prominent example using
learning rule-based policies. uses hand-coded rule-base reinforcement-learning7. Results cited section 3.6.

678

fiLearning play Ms. Pac-Man

principle determine rules included policy. Dynamic scripting
successful applications state-of-the-art computer games role-playing game
Neverwinter Nights (Spronck et al., 2006) real-time strategy game Wargus (Ponsen
& Spronck, 2004).

7.4 Concurrent Actions
traditional formalizations RL tasks, agent select execute single action
time. work known us handles concurrent actions explicitly Rohanimanesh Mahadevan (2001). formalize RL tasks concurrent actions
framework semi-Markov decision processes present simple grid world demonstrations.

8. Summary Closing Remarks
article proposed method learns play Ms. Pac-Man. dened
set high-level observation action modules following properties: (i) actions
temporally extended, (ii) actions exclusive, may work concurrently.
method uncover action combinations together priorities. Thus, agent
pursue multiple goals parallel.
decision agent concerns whether action module turned (if
o) (if on). Furthermore, decisions depend current observations
may depend state action modules. policy agent represented
list if-then rules priorities. policies easy interpret analyze.
easy incorporate additional human knowledge. cross-entropy method used
learning policies play well. Learning biased towards low-complexity policies,
consequence policy representation applied cross entropy learning
method. CEM, higher complexity solutions harder discover special means
used counteract premature convergence. solutions higher complexities,
noise injection suggested previous work (Szita & Lrincz, 2006). Learned
low complexity policies reached better score hand-coded policy average human
players.
applied architecture potentials handle large, structured observation-
action-spaces, partial observability, temporally extended concurrent actions. Despite
versatility, policy search eective, biased towards low-complexity
policies. properties attractive point view large-scale applications.

8.1 Role Domain Knowledge
demonstrating abilities RL algorithm, desirable learning starts
scratch, contribution learning clearly measurable. However, choices
test problems often misleading: many `abstract' domains contain considerable amount
domain knowledge implicitly. example, consider grid world navigation tasks,
often used class problems tabula rasa learning.
simple version grid world navigation task, state integer uniquely
identies position agent, atomic actions moves grid cells north/south/east/west actual cell. importantly, unique identication
679

fiSzita & Lrincz

position means moves agent change direction agent
task laboratory coordinate framework, sometimes called allocentric coordinates,
egocentric coordinates. concepts north, south, etc. correspond
high-level abstraction, meaning humans only, must considered
part domain knowledge. domain knowledge provided us similar grid
world sense provide high-level observations allocentric form,
`distance nearest ghost d' `Ms. Pac-Man position (11, 2)'. Similarly, action `go
north' action `go towards nearest power dot' essentially level.
implicit presence high-level concepts becomes even apparent move
abstract MDPs `real world'. Consider robotic implementation maze
task: full state information, i.e. state well state environment
available robot. sees local features may see local features
time. obtain exact position, move one unit's length prescribed direction,
robot integrate information movement sensors, optical/radar sensors etc.
information fusion, although necessary, topic reinforcement learning. Thus,
task, great amount domain knowledge needs provided
CE based policy search method could applied.
opinion, role human knowledge selects set observations
actions suit learning algorithm. extra knowledge typically necessary
applications. Nonetheless, numerous (more-or-less successful) approaches exist obtaining
domain knowledge automatically. According one approach, set observations
chosen rich (and redundant) set observations feature selection method.
cross-entropy method seems promising here, (see paper Szita, 2006,
application feature selection brain fMRI data 2006 Pittsburgh Brain Activity
Interpretation Competition). According dierent approach, successful combinations
lower level rules joined higher level concepts/rules. Machine learning
powerful tools here, e.g. arithmetic coding data compression (Witten, Neal, & Cleary,
1987). applied many areas, including writing tool Dasher developed Ward
MacKay (2002). extensions included framework reinforcement
learning.

8.2 Low-Complexity Policies
space legal policies huge (potentially innite), interesting question
search eective huge space. Direct search formidable. think
implicit bias towards low-complexity policies useful studied here.
low-complexity policy, mean following: policy may consist many
rules, cases, applied concurrently. Unused rules
get rewarded get punished unless limit useful rule, eective length
policies biased towards short policies. implicit bias strengthened explicit
one work: absence explicit reinforcement, probability applying rule
decays, indierent rules get wiped quickly. seems promising use frequent low
complexity rule combinations building blocks continued search powerful
still low-complexity policies.
680

fiLearning play Ms. Pac-Man

bias towards short policies reduces eective search space considerably. Moreover, many real-life problems, low-complexity solutions exist (for excellent analysis
possible reasons, see paper Schmidhuber, 1997). Therefore, search concentrated
relevant part policy space, pays less attention complex policies (which
therefore less likely according Occam's razor arguments.)

Acknowledgments
Please send correspondence Andrs Lrincz. authors would thank anonymous reviewers detailed comments suggestions improving presentation
paper. material based upon work supported partially European Oce
Aerospace Research Development, Air Force Oce Scientic Research, Air Force
Research Laboratory, Contract No. FA-073029. research supported
EC FET grant, `New Ties project' contract 003752. opinions, ndings
conclusions recommendations expressed material authors
necessarily reect views European Oce Aerospace Research Development, Air Force Oce Scientic Research, Air Force Research Laboratory, EC,
members EC New Ties project.

Appendix A. Hand-Coded Rule-Base
list rules hand-coded rule-base used experiments.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25



























Constant>0 ToDot+
Constant>0 ToCenterofDots+
NearestGhost<4 FromGhost+
NearestGhost<3 FromGhost+
NearestGhost<5 FromGhost+
NearestGhost>5 FromGhostNearestGhost>6 FromGhostNearestGhost>7 FromGhostConstant>0 ToSafeJunction+
MaxJunctionSafety<3 ToSafeJunction+
MaxJunctionSafety<1 ToSafeJunction+
MaxJunctionSafety>3 ToSafeJunctionMaxJunctionSafety>3 FromGhostMaxJunctionSafety>5 ToSafeJunctionMaxJunctionSafety>5 FromGhostConstant>0 KeepDirection+
Constant>0 ToEdGhost+
NearestGhost<4 ToPowerDot+
NearestEdGhost<99 ToPowerDotNearestEdGhost<99 NearestPowerDot<5 FromPowerDot+
NearestEdGhost<99 FromPowerDot+
NearestEdGhost>99 FromPowerDotNearestEdGhost>99 ToPowerDot+
GhostDensity>1 ToLowerGhostDensity+
GhostDensity<0.5 ToLowerGhostDensity681

fiSzita & Lrincz

26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42



















NearestPowerDot<2 NearestGhost<5 ToPowerDot+
NearestGhost>7 MaxJunctionSafety>4 FromGhostGhostDensity<1.5 NearestPowerDot<5 FromPowerDot+
NearestPowerDot>10 FromPowerDotTotalDistToGhosts>30 FromPowerDot+
MaxJunctionSafety<3 FromGhost+
MaxJunctionSafety<2 FromGhost+
MaxJunctionSafety<1 FromGhost+
MaxJunctionSafety<0 FromGhost+
Constant>0 FromGhostCenter+
NearestGhost<4 FromGhost+
NearestGhost>7 MaxJunctionSafety>4 FromGhostNearestEdGhost>99 ToEdGhostNearestEdGhost<99 ToEdGhost+
FromPowerDot- ToPowerDot+
GhostDensity<1.5 NearestPowerDot<5 FromPowerDot+
NearestPowerDot>10 FromPowerDot-

References
Allon, G., Kroese, D. P., Raviv, T., & Rubinstein, R. Y. (2005). Application crossentropy method buer allocation problem simulation-based environment.
Annals Operations Research, 134, 137151.
Baum, E. B. (1996). Toward model mind laissez-faire economy idiots.
Proceedings 13rd International Conference Machine Learning, pp. 2836.
Baxter, J., Tridgell, A., & Weaver, L. (2001). Machines learn play games, chap.
Reinforcement learning chess, pp. 91116. Nova Science Publishers, Inc.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientic.
Bonet, J. S. D., & Stauer, C. P. (1999). Learning play Pac-Man using incremental
reinforcement learning.. [Online; accessed 09 October 2006].
Bull, L. (2004). Applications Learning Classier Systems, chap. Learning Classier Systems: Brief Introduction, pp. 313. Springer.
Bull, L., & Kovacs, T. (2005). Foundations Learning Classier Systems, chap. Foundations
Learning Classier Systems: Introduction, pp. 314. Springer.
Courtillat, P. (2001). NoN-SeNS Pacman 1.6 C sourcecode.. [Online; accessed 09
October 2006].
Dambreville, F. (2006). Cross-entropic learning machine decision partially
observable universe. Journal Global Optimization. appear.
de Boer, P.-T., Kroese, D. P., Mannor, S., & Rubinstein, R. Y. (2004). tutorial
cross-entropy method. Annals Operations Research, 134, 1967.
682

fiLearning play Ms. Pac-Man

Gallagher, M., & Ryan, A. (2003). Learning play pac-man: evolutionary, rule-based
approach. et. al., R. S. (Ed.), Proc. Congress Evolutionary Computation, pp.
24622469.
Holland, J. H. (1986). Escaping brittleness: possibilities general-purpose learning
algorithms applied parallel rule-based systems. Mitchell, Michalski, & Carbonell
(Eds.), Machine Learning, Articial Intelligence Approach. Volume II, chap. 20,
pp. 593623. Morgan Kaufmann.
Keith, J., & Kroese, D. P. (2002). Sequence alignment rare event simulation. Proceedings 2002 Winter Simulation Conference, pp. 320327.
Koza, J. (1992). Genetic programming: programming computers means
natural selection. MIT Press.
Lucas, S. M. (2005). Evolving neural network location evaluator play Ms. Pac-Man.
IEEE Symposium Computational Intelligence Games, pp. 203210.
Mannor, S., Rubinstein, R. Y., & Gat, Y. (2003). cross-entropy method fast policy
search. 20th International Conference Machine Learning.
Margolin, L. (2004). convergence cross-entropy method. Annals Operations
Research, 134, 201214.
Menache, I., Mannor, S., & Shimkin, N. (2005). Basis function adaptation temporal
dierence reinforcement learning. Annals Operations Research, 134 (1), 215238.
Muehlenbein, H. (1998). equation response selection use prediction.
Evolutionary Computation, 5, 303346.
Ponsen, M., & Spronck, P. (2004). Improving adaptive game AI evolutionary learning.
Computer Games: Articial Intelligence, Design Education.
Rohanimanesh, K., & Mahadevan, S. (2001). Decision-theoretic planning concurrent
temporally extended actions. Proceedings 17th Conference Uncerainty
Articial Intelligence, pp. 472479.
Rubinstein, R. Y. (1999). cross-entropy method combinatorial continuous
optimization. Methodology Computing Applied Probability, 1, 127190.
Samuel, A. L. (1959). studies machine learning using game checkers. IBM
Journal Research Development, 6, 211229.
Schaul, T. (2005). Evolving compact concept-based Sokoban solver. Master's thesis, cole
Polytechnique Fdrale de Lausanne.
Schmidhuber, J. (1997). computer scientist's view life, universe, everything.
Freksa, C., Jantzen, M., & Valk, R. (Eds.), Foundations Computer Science:
Potential - Theory - Cognition, Vol. 1337 Lecture Notes Computer Science, pp.
201208. Springer, Berlin.
683

fiSzita & Lrincz

Spronck, P., Ponsen, M., Sprinkhuizen-Kuyper, I., & Postma, E. (2006). Adaptive game ai
dynamic scripting. Machine Learning, 63 (3), 217248.
Spronck, P., Sprinkhuizen-Kuyper, I., & Postma, E. (2003). Online adaptation computer
game opponent AI. Proceedings 15th Belgium-Netherlands Conference
Articial Intelligence, pp. 291298.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge.
Szab, Z., Pczos, B., & Lrincz, A. (2006). Cross-entropy optimization independent
process analysis. ICA, pp. 909916.
Szita, I. (2006). select 100 voxels best prediction simplistic
approach. Tech. rep., Etvs Lornd University, Hungary.
Szita, I., & Lrincz, A. (2006). Learning Tetris using noisy cross-entropy method. Neural
Computation, 18 (12), 29362941.
Tesauro, G. (1994). TD-Gammon, self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6 (2), 215219.
Timuri, T., Spronck, P., & van den Herik, J. (2007). Automatic rule ordering dynamic
scripting. Third Articial Intelligence Interactive Digital Entertainment
Conference, pp. 4954.
Tiong, A. L. K. (2002). Rule set representation tness functions articial pac man
playing agent. Bachelor's thesis, Department Information Technology Electrical
Engineering.
Ward, D. J., & MacKay, D. J. C. (2002). Fast hands-free writing gaze direction. Nature,
418, 838540.
Wikipedia (2006). Pac-Man Wikipedia, free encyclopedia. Wikipedia. [Online;
accessed 20 May 2007].
Witten, I. A., Neal, R. M., & Cleary, J. G. (1987). Arithmetic coding data compression.
Communications ACM, 30, 520540.
Wolpert, D. H., & Macready, W. G. (1997). free lunch theorems optimization. IEEE
Transactions Evolutionary Computation, 1, 6782.

684


