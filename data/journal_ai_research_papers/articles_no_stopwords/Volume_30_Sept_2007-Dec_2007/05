journal articial intelligence

submitted published

learning play low complexity rule policies
illustrations ms pac man
istvn szita
andrs lrincz

szityu eotvos elte hu
andras lorincz elte hu

dept information systems
etvs university hungary h

abstract
article propose method deal certain combinatorial reinforcement learning tasks demonstrate popular ms pac man game
dene set high level observation action modules rule policies
constructed automatically policies actions temporally extended may
work concurrently policy agent encoded compact decision list components list selected large pool rules hand crafted
generated automatically suitable selection rules learnt cross entropy
method recent global optimization ts framework smoothly crossentropy optimized policies perform better hand crafted policy reach score
average human players argue learning successful mainly policies
may apply concurrent actions thus policy space suciently rich ii search
biased towards low complexity policies therefore solutions compact description
found quickly exist

introduction
last two decades reinforcement learning rl reached mature state
laid solid foundations large variety including valuefunction direct policy search hybrid methods reviews subjects
see e g books bertsekas tsitsiklis sutton barto
basic properties many relatively well understood e g conditions
convergence complexity eect parameters although needless say
still lots important open questions plenty test
maze navigation tasks pole balancing car hill etc
capabilities rl demonstrated number large scale rl
applications growing steadily however current rl far
box methods still need demonstrations showing rl
ecient complex tasks
think games including diverse set classical board games card games modern computer games etc ideal test environments reinforcement learning games
intended interesting challenging human intelligence therefore
ideal means explore articial intelligence still missing furthermore
games well rl paradigm goal oriented sequential decision
decision long term eects many cases hidden information random
events unknown environment known unknown players account part diculty
c ai access foundation rights reserved


fiszita lrincz

playing game circumstances focus reinforcement learning games
attractive testing methods decision space huge cases
nding good strategy challenging task
another great advantage games test rules games
xed danger tailoring task e tweak rules
environment meet capabilities proposed rl
reduced compared e g maze navigation tasks
rl tried many classical games including checkers samuel backgammon tesauro chess baxter tridgell weaver hand
modern computer games got spotlight recently many
successful attempts learn ai tools notable exceptions example roleplaying game baldur gate spronck sprinkhuizen kuyper postma real time
strategy game wargus ponsen spronck possibly tetris szita lrincz
games pose challenges rl example many observations
considered parallel observation space action space huge
spirit decided investigate arcade game ms pac man game
interesting largely unsolved imposes several important questions
rl overview section provide hand coded high level actions
observations task rl learn combine good policy
apply rule policies easy interpret enable one include
human domain knowledge easily learning apply cross entropy method
recently developed general optimization hybrid
successful tabula rasa learning hand coded strategy alone
next section introduce ms pac man game briey discuss
formalized reinforcement learning task sections shall shortly describe
cross entropy optimization method rule policies respectively section
details learning experiments provided section present
section provides review related literature nally section summarize
discuss emphasis implications rl

pac man reinforcement learning
video game pac man rst released reached immense success
considered one popular video games date wikipedia
player maneuvers pac man maze see fig pac man eats dots
maze particular maze dots one worth points level
nished dots eaten make things dicult four
ghosts maze try catch pac man succeed pac man loses life
initially three lives gets extra life reaching points
four power items corners maze called power dots worth
points pac man eats power dot ghosts turn blue short period
seconds slow try escape pac man time pac man
maze original pac man game slightly dierent description applies opensource pac man implementation courtillat two versions equivalent terms
complexity entertainment value



filearning play ms pac man

figure snapshot pac man game
able eat worth points consecutively point
values reset time another power dot eaten player would want
eat four ghosts per power dot ghost eaten remains hurry back center
maze ghost reborn certain intervals fruit appears near center
maze remains eating fruit worth points
investigations restricted learning optimal policy rst level
maximum achievable score plus
points time fruit eaten
original version pac man ghosts move complex deterministic route
possible learn deterministic action sequence require observations
many patterns found enthusiastic players pac man sequels
notably ms pac man randomness added movement ghosts way
single optimal action sequence observations necessary optimal decision
making respects game play mostly unchanged
implementation ghosts moved randomly time straight towards
pac man remaining ghosts may turn back following koza chapter
emphasize presence randomness shall refer implementation
ms pac man clone

ms pac man rl task
ms pac man meets criteria reinforcement learning task agent make
sequence decisions depend observations environment stochastic
paths ghosts unpredictable well dened reward function
score eating things actions inuence rewards collected future


fiszita lrincz

full description state would include whether dots eaten one
bit dot one power dot position direction ms pac man
position direction four ghosts whether ghosts blue one bit
ghost long remain blue range seconds
whether fruit present time left appears disappears number
lives left size resulting state space astronomical kind function
approximation feature extraction necessary rl
action space much smaller four basic actions go north south east west however typical game consists multiple hundreds steps
number possible combinations still enormous indicates need temporally
extended actions
moderate amount domain knowledge ms pac man one quite
easy dene high level observations action modules potentially useful
hand constructing well performing policy seems much dicult therefore
provide mid level domain knowledge use domain knowledge
preprocess state information dene action modules hand
role policy search reinforcement learning combine observations modules
rule policies nd proper combination

cross entropy method
goal optimize rule policies performing policy search space
legal rule policies search apply cross entropy method cem
recently published global optimization rubinstein aims nd
approximate solution global optimization tasks following form

x arg max f x
x

f general objective function e g need assume continuity dierentiability summarize mechanism method briey see section
overview applications

intuitive description
optimization maintain single candidate solution x time
step cem maintains distribution possible solutions distribution solution
candidates drawn random essentially random guessing nice trick
turned highly eective optimization method
power random guessing

random guessing overly simple optimization method draw many samples
xed distribution g select best sample estimation optimum
limit case innitely many samples random guessing nds global optimum
two notes shown wolpert macready
general uniform random guessing worse method ii
nonetheless practical uniform random guessing extremely inecient


filearning play ms pac man

thus random guessing safe start one proceeds collection
experience limited much possible
eciency random guessing depends greatly distribution g
samples drawn example g sharply peaked around x samples
may sucient get good estimate case opposite distribution
sharply peaked around x x tremendous number examples may needed get
good estimate global optimum naturally nding good distribution least
hard nding x
improving efficiency random guessing

drawing moderately many samples distribution g may able give
acceptable approximation x may still obtain better sampling distribution
basic idea cem selects best samples modies g
becomes peaked around consider example x vector g
bernoulli distribution coordinate suppose drawn samples
selected best see majority selected samples ith coordinate
cem shifts bernoulli distribution corresponding component towards
afterwards next set samples drawn already modied distribution
idea seems plausible majority best scoring samples ith coordinate
structure tness landscape may hope ith
coordinate x follows describe update rule cem
formal way sketch derivation

formal description cross entropy method
pick g family parameterized distributions denoted g describe
iteratively improves parameters distribution g
let n number samples drawn let samples x x n
drawn independently distribution g r set high valued samples

l x f x n
provides approximation level set

l x f x
let u uniform distribution level set l large values distribution peaked around x would suitable random sampling raises two
potential large values l contain points possibly none
making accurate approximation impossible ii level set l usually member
parameterized distribution family
rst easy avoid choosing lower values however
make compromise setting low would inhibit large improvement steps
compromise achieved follows cem chooses ratio adjusts l
set best n samples corresponds setting f x n provided
samples arranged decreasing order values best n samples called
elite samples practice typically chosen range


fiszita lrincz

solved changing goal approximation cem chooses
distribution g distribution family g approximates best empirical distribution l best g found minimizing distance g uniform
distribution elite samples measure distance cross entropy distance
often called kullback leibler divergence cross entropy distance two distributions
g h dened
z
g x
dce g h g x log
dx
h x
general form cross entropy method summarized table known
mild regularity conditions ce method converges probability margolin
furthermore suciently large population global optimum found
high probability
input g
input g g
input n
input
input

n
draw x distribution gt
compute f x
sort values descending order
fn
et x f x
gt arg mingg dce g uniform et
end loop








parameterized distrib family
initial distribution
population size
selection ratio
number iterations
cem iteration main loop

draw n samples
evaluate
level set threshold
get elite samples
get nearest distrib g

table pseudo code general cross entropy method

cross entropy method bernoulli distribution
many parameterized distribution families parameters minimum cross entropy
member computed easily simple statistics elite samples provide
formulae bernoulli distributions needed policy learning procedure
detailed next section derivations well list discrete continuous
distributions simple update rules found tutorial de boer kroese
mannor rubinstein
let domain optimization component drawn
independent bernoulli distributions e g bernoullim distribution g g parameterized dimensional vector p p pm g sampling


filearning play ms pac man

component j sample x

probability pj
xj
probability pj
drawing n samples x x n xing threshold value let e denote set
elite samples e
e x f x
notation distribution g minimum ce distance uniform distribution elite set following parameters

p p p
p
p


x e xj
x e xj

p

pj
n
x e



words parameters g simply component wise empirical probabilities
elite set derivation rule see tutorial de boer et al
changing distribution parameters p p coarse cases
applying step size parameter preferable resulting summarized
table
input p p p
input n
input
input

n
draw x bernoullim pt
compute f x
sort values descending order
fn
et x f x
p


p j
x e xj n
pt j p j pt j
end loop







initial distribution parameters
population size
selection ratio
number iterations
cem iteration main loop

draw n samples
evaluate





level set threshold
get elite samples
get parameters nearest distrib
update step size

table pseudo code cross entropy method bernoulli distributions
need optimize functions k k
simplest case distributions domain parameterized
k parameters
pk
p p p k pm pm k pj k k pj k j
special case multinomial distribution
update rule parameters essentially eq bernoulli case
p
p


e xj k
x e xj k
x

p



pj k
n
x e


fiszita lrincz

note constraint

pk


k pj k

satised automatically j

rule policies
basic formulation rule sentence form condition holds
action rule policy set rules mechanism breaking ties e
decide rule executed multiple rules satised conditions
rule policies human readable easy include domain knowledge
able represent complex behaviors reasons often used many
areas articial intelligence see section short overview related literature
order apply rule policies ms pac man need specify four things
possible actions possible conditions
constructed observations make rules conditions actions
combine rules policies answers described following
sections

action modules
dening action modules ms pac man listed modules easy
implement considered potentially useful see table way kept human work minimum still managed formalize part domain knowledge
consequence list action modules means optimal actions could eective appropriate denition others
may superuous example four dierent modules ghost avoidance
fromghost escapes nearest ghost without considering position
ghosts tolowerghostdensity tries take account inuence multiple ghosts
fromghostcenter moves geometrical center ghosts thus able avoid
surrounded trapped hand easily bump ghost
nally toghostfreearea considers whole board search safe
location agent avoid shepherded ghosts modules
may strengths weaknesses possibly combination needed
success actions potentially useful listed
example moving towards fruit
note modules exclusive example escaping
ghosts ms pac man may prefer route dots eaten may want
head towards power dot without possibility concurrent actions performance
ms pac man agent may reduced considerably investigated experimental
section
need mechanism conict resolution dierent action modules may suggest dierent directions assigning priorities modules agent
switches action module decides priority decision
learning decision part learning task
action priorities learnt indirectly rule xed priority action switched
rule inherits priority action switched dierent rules
dierent priorities mechanism described detail section



filearning play ms pac man

table list action modules used rule construction
name

description

todot
topowerdot
frompowerdot

go towards nearest dot
go towards nearest power dot
go direction opposite nearest power
dot
go towards nearest edible blue ghost
go direction opposite nearest ghost
go towards maximally safe junction
four directions safety nearest junction estimated direction ms pacman n steps away junction
nearest ghost k steps away safety
value junction n k negative value
means ms pac man possibly cannot reach
junction
go direction maximizes euclidean
distance geometrical center ghosts
go current direction choose
random available action except turning back
impossible
go direction cumulative ghost
density decreases fastest ghost denes
density cloud radius linear decay cumulative ghost density
calculated
choose location board minimum ghost distance largest head towards
shortest path

toedghost
fromghost
tosafejunction

fromghostcenter
keepdirection
tolowerghostdensity

toghostfreearea



fiszita lrincz

table list observations used rule construction distances denote length
shortest path unless noted otherwise distance particular object type
object exists moment
name

description

constant
nearestdot
nearestpowerdot
nearestghost
nearestedghost
maxjunctionsafety

constant value
distance nearest dot
distance nearest power dot
distance nearest ghost
distance nearest edible blue ghost
four directions safety nearest
junction direction estimated dened
description action tosafejunction
observation returns value maximally safe junction
euclidean distance geometrical center
ghosts
euclidean distance geometrical center
uneaten dots
ghost denes density cloud radius
linear decay returns value
cumulative ghost density
travelling salesman distance ghosts
length shortest route starts
ms pac man reaches four ghosts
considering movement

ghostcenterdist
dotcenterdist
ghostdensity
totaldisttoghosts

implemented following mechanism decision agent concerns
action modules agent switch switch action module
principle agent able use subset action modules instead selecting
single one time step basically module highest priority decides direction
ms pac man one equally ranked directions lower priority
modules checked direction cannot decided checking switched modules
order decreasing priority example module switched two directions
ranked equally switched modules random direction chosen
ms pac man make decisions time advances whole grid cell
mechanism ensures never stands still according game ticks approx
seconds simulated game time

observations conditions rules
similarly actions easily dene list observations potentially useful
decision making observations descriptions summarized table


filearning play ms pac man

modules could improved many ways example checking whether
enough time intercept edible ghosts calculating nearestedghost taking
consideration movement ghosts calculating nearestghost nearestedghost
maxjunctionsafety kept implementation modules simple possible
designed reasonable modules eort made make module denitions
optimal complete non redundant
necessary tools dening conditions rule typical condition
true observations given range note status action module
important proper decision making example agent may decide
ghost close switches modules except escape module therefore
allow conditions check whether action module
sake simplicity conditions restricted form observation
value observation value action action conjunction
terms example

nearestdot nearestghost fromghost
valid condition rules
conditions actions rules constructed easily implementation rule form condition action example

nearestdot nearestghost fromghost
fromghostcenter
valid rule

constructing policies rules
decision lists standard forms constructing policies single rules
pursue decision lists simply lists rules together
mechanism decides order rules checked
rule priority assigned agent make decision checks
rule list starting ones highest priority conditions rule fullled
corresponding action executed decision making process halts
note principle priority rule dierent priority action
modules however sake simplicity make distinction rule priority
k switches action module priority action module taken k
intuitively makes sense important rule activated eect
important rule priority k switches module executed regardless
priority module
may worth noting many possible alternatives ordering rules
actions

rule could xed priority part provided domain knowledge
spronck ponsen sprinkhuizen kuyper postma
priority rule could free parameter learned cem
method


fiszita lrincz

instead absolute priorities agent could learn relative ordering rules
timuri spronck van den herik
order rules could determined heuristic decision mechanism
example generality rule e g rules many conditions large small domains could taken account heuristics used linear
classier systems see e g work bull kovacs
principle one would nd interesting solutions computer minimal
bias domain knowledge regard eciency simple priority management method satisfactory experiment priority heuristics

example
let us consider example shown table rule policy ms pacman agent
table hand coded policy playing ms pac man bracketed numbers denote
priorities highest priority


















nearestghost fromghost
nearestghost junctionsafety fromghostnearestedghost toedghostnearestedghost toedghost
constant keepdirection
frompowerdot topowerdot
ghostdensity nearestpowerdot frompowerdot
nearestpowerdot frompowerdot

rst two rules manage ghost avoidance ghost close agent
ee gets safe distance ghost avoidance priority
activities next two rules regulate edible ghost
board agent chase value nearestedghost innity
edible ghosts board activity
relatively high priority eating ghosts worth lots points must done
blueness ghosts disappears must done quickly fth rule says
agent turn back directions equally good rule prevents
unnecessary zigzagging dots eaten surprisingly eective
remaining rules tweak management power dots basically agent prefers eat
power dot however blue ghosts board power dot resets
score counter bad move furthermore ghost density low around
agent probably hard collect ghosts preferable
wait eating power dot


filearning play ms pac man

mechanism decision making
mechanism decision making depicted fig short hidden state space
world ms pac man ghosts dynamics hidden statespace determines vector observations checked conditions
conditions rule satised corresponding action module switched
consequence multiple actions may eect example decision depicted
fig sets two actions work together

figure decision making mechanism ms pac man agent time step
agent receives actual observations state action modules
checks rules script order executes rst rule satised
conditions
initially action module switched state module switched
remains explicitly switched another module
priority switched replaces

learning rule policies cem
apply cem searching space rule policies learning composed
three phases generation random policies drawn according current parameter
set evaluation policies consists playing game ms pac man
measure score updating parameter set cem update rules
drawing random scripts predefined rule base

suppose predened rule base containing k rules example one listed
appendix policy rule slots slot lled k rules


fiszita lrincz

left empty policies could contain rules possibly much less
rule slot xed priority set priority rule slot
change learning learning however push important rule high priority
slot low priority one vice versa
slot lled rule rule base probability pi
left empty probability pi decided slot
pkbe lled
particular rule j j k selected probability qi j j qi j
slot policies could contain rules possibly much
less pi values qi j values learnt simultaneously cross entropy
method table update rules respectively gives total
k parameters optimize although eective number parameters much
less qi j values unused slots irrelevant initial probabilities set
pi qi j k
drawing random rules without predefined rule base

studied situations lessened domain knowledge use predened rulebase script generation kept rule base k rules generated
randomly case generated dierent rule bases rule slots low
ratio meaningful rules counteracted increased rule variety
random rule random pair randomly drawn condition set randomly
drawn action random condition sets contained conditions random action constructed follows action module selected uniformly set modules listed
table switched probability construction random condition starts uniformly random selection module table table
selected module action condition action action
equal probability selected module observation condition
observation value observation value equal probability value
selected uniformly element set values set determined separately observation module follows played games xed policy
recorded histogram values observation subsequently element set
determined would split histogram regions equal area example
value set nearestghost
design random rule generation procedure contains arbitrary elements e g
number conditions rule number values observation compared
intuition behind procedure generate rules suciently versatile
ratio meaningless rules e g rules unsatisable conditions large
however optimization form done point

description experiments
according assumptions eectiveness described architecture
three pillars human domain knowledge provided modules rules
according preliminary experiments quality learned policy improve increasing
priority set number slots



filearning play ms pac man

eectiveness optimization possibility concurrent actions
describe set experiments designed test assumptions

full architecture
rst experiment random rules used construction use modules
dened sections second experiment rules generated randomly
hand coded case role learning determine rules
used
learning random rule construction

rst experiment rule base generated randomly described section
number rule slots xed priorities distributed evenly one
containing k randomly generated rules values k selected
coarse search parameter space
parameters cem follows population size n selection ratio
step size values fairly standard cem
tried varying step probabilities rule slot
values pi qi j slightly decreased decay rate
larger decay rate useful rules annulled often hand smaller
decay aect performance many superuous rules left policies
score given policy huge variance due random factors game
therefore obtain reliable tness estimations score policy averaged
subsequent games learning lasted episodes sucient tune
probability close performed parallel training runs experiment
type denoted ce randomrb
learning hand coded rules

second experiment constructed rule base k hand coded rules shown
appendix thought potentially useful could placed one
rule slots parameters experiment identical previous
one experiment type denoted ce fixedrb

eect learning
following experiment compared performance cem simple stochastic
gradient optimization single comparison sucient measure eciency
cem serves provide point reference comparison relevant
similar complexity move gradually towards
best samples found far dierence sg maintains single solution
note per episode learning rate would correspond per instance learning rate
n line learning
contrast previous experiment rules meaningful potentially useful therefore
need large pool rules much lower used found
fairly insensitive choice signicant changes performance observed parameter
modied factor



fiszita lrincz

candidate time whereas cem maintains distribution solutions thus cem
maintains memory solutions becomes less fragile occasional wrong parameter
changes
particular form stochastic gradient search following initial policy
drawn random consisting rules generated random mutation
current solution candidate step evaluated obtained policies bestperforming mutation chosen next solution candidate mutations generated
following procedure rule condition changed random
condition probability rule action changed random
action probability listed parameter values number rules policy number
mutated policies probabilities mutation coarse parameter space
optimization
number episodes set way evaluated number
dierent policies cem experiments random rule base
xed rule base experiments repeated stochastic gradient method executing
parallel training runs resulting policies denoted sg randomrb sgfixedrb respectively

eect parallel actions
according assumptions possibility parallel actions plays crucial role
success architecture conrm assumption repeated previous experiments
concurrent actions disabled agent switches action module
action modules switched automatically experiment types denoted
ce randomrb action ce fixedrb action sg randomrb action sgfixedrb action

baseline experiments
order isolate assess contribution learning performed two additional
experiments dierent amounts domain knowledge learning furthermore
asked human subjects play game
random policies

rst non learning experiment used rule base hand coded rules identical
rule base ce fixedrb ten rules selected random random priorities
assigned measured performance policies constructed way
hand coded policy

second non learning experiment hand coded rules priorities
hand coded full policy policy shown table constructed
trial error naturally policy constructed knowing
learning experiments


filearning play ms pac man

table ms pac man see text details abbreviations ce learning
cross entropy method sg learning stochastic gradient randomrb
randomly generated rule base fixedrb xed hand coded rule base action
one action module work time
method

avg score

percentiles










ce randomrb action
ce fixedrb action
sg randomrb action
sg fixedrb action











random policy
hand coded policy
human play









ce randomrb
ce fixedrb
sg randomrb
sg fixedrb



human play

nal experiment human subjects asked play rst level ms pacman measured performance subjects played pac man
similar games none experienced player

experimental
human experiments performed rst level open source pac man clone
courtillat experiments applied delphi implementation
code
learning experiments parallel learning runs executed one
episodes training period sucient tune probabilities close
learned policy could determined unambiguously cases obtained
policy tested playing consecutive games giving total test games per
experiment non learning experiments agents played test games
random policies hand coded policy respectively human subject played
games giving total test games summarized table provide
percentile values instead variances distribution scores
highly non gaussian
fact average smaller percentile caused highly skewed distribution
scores games agent reached score range except games
extremely low score games aect percentile lowered average
signicantly



fiszita lrincz















nearestghost fromghost
maxjunctionsafety fromghostnearestedghost topowerdot
nearestedghost toedghost
ghostdensity nearestpowerdot frompowerdot
constant tocenterofdots

figure best policy learned ce fixedrb average score games
















maxjunctionsafety tolowerghostdensity fromghostnearestghost maxjunctionsafety fromghost
nearestghost fromghostcenter toedghost
toedghost centerofdots toedghost
toedghost nearestedghost toedghost
nearestdot ghostcenterdist keepdirection
toghostfreearea todot topowerdot

figure best policy learned ce randomrb average score games
note presence true thus superuous conditions
tolowerghostdensity fromghostcenter toghostfreearea todot

fig shows best individual policy learned ce fixedrb reaching points
average ghost avoidance given highest priority turned ghost
close otherwise ms pac man concentrates eating power dots subsequently
eating blue ghosts takes care eat power dot blue
ghosts board otherwise would miss opportunity eat point
ghost possibly several others lowest priority setting agent looks
ordinary dots although rule eect previous rules decide
direction example endgame power dots left ghosts
original form
policies learnt ce randomrb behave similarly ones learnt ce fixedrb
although behavior somewhat obscured superuous conditions rules
demonstrated clearly example policy shown fig noise generated
random rules often fails learn correct priorities
activities
eect enabling disabling concurrent actions signicant instructive
take look best policy learned ce fixedrb action shown fig
agent concentrate eating ghosts major source reward however
cannot use modules necessary ghost avoidance long term survival
cem performs signicantly better stochastic gradient
learning believe however dierence could lowered thorough search
parameter space sg many global optimization methods evolutionary
methods simulated annealing could reach similar performances cem according
de boer et al applications cited section advantage cem


filearning play ms pac man

nearestedghost topowerdot
nearestedghost toedghost

figure best policy learned ce fixedrb action average score games


maintains distribution solutions reach robust performance
little eort requiring little tuning parameters canonical set
parameters population large possible
performance method robust claim coincides experiences
parameter optimization process
finally interesting analyze dierences tactics human
computer players one fundamental tactic human players try lure
ghosts close ms pac man ghosts close way
eaten fast turn blue behavior evolved
experiments besides tactics cem chance discover
lacking appropriate sensors example human player calculate
time remaining blue period approximate future position ghosts


related literature
section review literature learning pac man game components learning architecture cross entropy method rule policies
concurrent actions

previous work ms pac man
variants pac man used previously several studies direct comparison
performances possible cases however simplied versions
game used studies
koza uses ms pac man example application genetic programming
uses dierent score value fruit worth points instead points used
shape board consequently number dots dierent
therefore scores cannot directly compared however koza reports p
pac man could scored additional points captured four monsters
four occasions turned blue score one reported
translates approximately points scoring system
lucas uses full scale ms pac man game test trains
neural network position evaluator hand crafted input features purposes
training uses evolutionary strategy obtained controller able
reach points averaged games
bonet stauer restrict observations window centered ms pacman uses neural network temporal dierence learning learn reactive con

fiszita lrincz

troller series increasingly dicult learning tasks able teach basic
pellet collecting ghost avoidance behaviors greatly simplied versions game
used simple mazes containing power dots one ghost
gallagher ryan denes behavior agent parameterized nite
state automata parameters learnt population incremental learning
evolutionary method similar cem run simplied version pac man
single ghost power dots takes away complexity game
tiong codes rule policies pac man hand uses learning
improve tests similarly pac man implementation
courtillat limits number ghosts best performing rule set
reaches points average maximal however likely
scale well increasing number ghosts ghost eaten times
average possible times per game

cross entropy method
cross entropy method rubinstein general global optimization
tasks bearing close resemblance estimation distribution evolutionary methods see e g
muehlenbein areas successful application range combinatorial optimization optimal buer allocation allon kroese raviv
rubinstein dna sequence alignment keith kroese independent process analysis szab pczos lrincz
cross entropy method several successful reinforcement learning applications
dambreville uses cem learning input output hierarchical hmm controls
predator agent partially observable grid world menache mannor shimkin
use radial basis function approximation value function continuous maze navigation task use cem adapt parameters basis functions nally mannor
rubinstein gat apply cem policy search simple grid world maze navigation recently cross entropy method applied successfully
game tetris szita lrincz

rule policies
representation policies rule sequences widespread technique complex computer games example many pac man related papers listed
use rule representation
learning classier systems holland genetic methods
evolve suitable rules given task bull gives excellent general overview
pointers references hayek machine baum similar architecture
agents corresponding simple rules dene economical system make bids
executing tasks hope obtain rewards schaul applies
architecture sokoban game
dynamic scripting spronck et al another prominent example
learning rule policies uses hand coded rule base reinforcement learning cited section



filearning play ms pac man

principle determine rules included policy dynamic scripting
successful applications state art computer games role playing game
neverwinter nights spronck et al real time strategy game wargus ponsen
spronck

concurrent actions
traditional formalizations rl tasks agent select execute single action
time work known us handles concurrent actions explicitly rohanimanesh mahadevan formalize rl tasks concurrent actions
framework semi markov decision processes present simple grid world demonstrations

summary closing remarks
article proposed method learns play ms pac man dened
set high level observation action modules following properties actions
temporally extended ii actions exclusive may work concurrently
method uncover action combinations together priorities thus agent
pursue multiple goals parallel
decision agent concerns whether action module turned
furthermore decisions depend current observations
may depend state action modules policy agent represented
list rules priorities policies easy interpret analyze
easy incorporate additional human knowledge cross entropy method used
learning policies play well learning biased towards low complexity policies
consequence policy representation applied cross entropy learning
method cem higher complexity solutions harder discover special means
used counteract premature convergence solutions higher complexities
noise injection suggested previous work szita lrincz learned
low complexity policies reached better score hand coded policy average human
players
applied architecture potentials handle large structured observation
action spaces partial observability temporally extended concurrent actions despite
versatility policy search eective biased towards low complexity
policies properties attractive point view large scale applications

role domain knowledge
demonstrating abilities rl desirable learning starts
scratch contribution learning clearly measurable however choices
test often misleading many abstract domains contain considerable amount
domain knowledge implicitly example consider grid world navigation tasks
often used class tabula rasa learning
simple version grid world navigation task state integer uniquely
identies position agent atomic actions moves grid cells north south east west actual cell importantly unique identication


fiszita lrincz

position means moves agent change direction agent
task laboratory coordinate framework sometimes called allocentric coordinates
egocentric coordinates concepts north south etc correspond
high level abstraction meaning humans must considered
part domain knowledge domain knowledge provided us similar grid
world sense provide high level observations allocentric form
distance nearest ghost ms pac man position similarly action go
north action go towards nearest power dot essentially level
implicit presence high level concepts becomes even apparent move
abstract mdps real world consider robotic implementation maze
task full state information e state well state environment
available robot sees local features may see local features
time obtain exact position move one unit length prescribed direction
robot integrate information movement sensors optical radar sensors etc
information fusion although necessary topic reinforcement learning thus
task great amount domain knowledge needs provided
ce policy search method could applied
opinion role human knowledge selects set observations
actions suit learning extra knowledge typically necessary
applications nonetheless numerous less successful approaches exist obtaining
domain knowledge automatically according one set observations
chosen rich redundant set observations feature selection method
cross entropy method seems promising see szita
application feature selection brain fmri data pittsburgh brain activity
interpretation competition according dierent successful combinations
lower level rules joined higher level concepts rules machine learning
powerful tools e g arithmetic coding data compression witten neal cleary
applied many areas including writing tool dasher developed ward
mackay extensions included framework reinforcement
learning

low complexity policies
space legal policies huge potentially innite interesting question
search eective huge space direct search formidable think
implicit bias towards low complexity policies useful studied
low complexity policy mean following policy may consist many
rules cases applied concurrently unused rules
get rewarded get punished unless limit useful rule eective length
policies biased towards short policies implicit bias strengthened explicit
one work absence explicit reinforcement probability applying rule
decays indierent rules get wiped quickly seems promising use frequent low
complexity rule combinations building blocks continued search powerful
still low complexity policies


filearning play ms pac man

bias towards short policies reduces eective search space considerably moreover many real life low complexity solutions exist excellent analysis
possible reasons see schmidhuber therefore search concentrated
relevant part policy space pays less attention complex policies
therefore less likely according occam razor arguments

acknowledgments
please send correspondence andrs lrincz authors would thank anonymous reviewers detailed comments suggestions improving presentation
material upon work supported partially european oce
aerospace development air force oce scientic air force
laboratory contract fa supported
ec fet grant ties project contract opinions ndings
conclusions recommendations expressed material authors
necessarily reect views european oce aerospace development air force oce scientic air force laboratory ec
members ec ties project

appendix hand coded rule base
list rules hand coded rule base used experiments




















































constant todot
constant tocenterofdots
nearestghost fromghost
nearestghost fromghost
nearestghost fromghost
nearestghost fromghostnearestghost fromghostnearestghost fromghostconstant tosafejunction
maxjunctionsafety tosafejunction
maxjunctionsafety tosafejunction
maxjunctionsafety tosafejunctionmaxjunctionsafety fromghostmaxjunctionsafety tosafejunctionmaxjunctionsafety fromghostconstant keepdirection
constant toedghost
nearestghost topowerdot
nearestedghost topowerdotnearestedghost nearestpowerdot frompowerdot
nearestedghost frompowerdot
nearestedghost frompowerdotnearestedghost topowerdot
ghostdensity tolowerghostdensity
ghostdensity tolowerghostdensity

fiszita lrincz





































nearestpowerdot nearestghost topowerdot
nearestghost maxjunctionsafety fromghostghostdensity nearestpowerdot frompowerdot
nearestpowerdot frompowerdottotaldisttoghosts frompowerdot
maxjunctionsafety fromghost
maxjunctionsafety fromghost
maxjunctionsafety fromghost
maxjunctionsafety fromghost
constant fromghostcenter
nearestghost fromghost
nearestghost maxjunctionsafety fromghostnearestedghost toedghostnearestedghost toedghost
frompowerdot topowerdot
ghostdensity nearestpowerdot frompowerdot
nearestpowerdot frompowerdot

references
allon g kroese p raviv rubinstein r application crossentropy method buer allocation simulation environment
annals operations
baum e b toward model mind laissez faire economy idiots
proceedings rd international conference machine learning pp
baxter j tridgell weaver l machines learn play games chap
reinforcement learning chess pp nova science publishers inc
bertsekas p tsitsiklis j n neuro dynamic programming athena scientic
bonet j stauer c p learning play pac man incremental
reinforcement learning online accessed october
bull l applications learning classier systems chap learning classier systems brief introduction pp springer
bull l kovacs foundations learning classier systems chap foundations
learning classier systems introduction pp springer
courtillat p non sens pacman c sourcecode online accessed
october
dambreville f cross entropic learning machine decision partially
observable universe journal global optimization appear
de boer p kroese p mannor rubinstein r tutorial
cross entropy method annals operations


filearning play ms pac man

gallagher ryan learning play pac man evolutionary rule
et al r ed proc congress evolutionary computation pp

holland j h escaping brittleness possibilities general purpose learning
applied parallel rule systems mitchell michalski carbonell
eds machine learning articial intelligence ii chap
pp morgan kaufmann
keith j kroese p sequence alignment rare event simulation proceedings winter simulation conference pp
koza j genetic programming programming computers means
natural selection mit press
lucas evolving neural network location evaluator play ms pac man
ieee symposium computational intelligence games pp
mannor rubinstein r gat cross entropy method fast policy
search th international conference machine learning
margolin l convergence cross entropy method annals operations

menache mannor shimkin n basis function adaptation temporal
dierence reinforcement learning annals operations
muehlenbein h equation response selection use prediction
evolutionary computation
ponsen spronck p improving adaptive game ai evolutionary learning
computer games articial intelligence design education
rohanimanesh k mahadevan decision theoretic concurrent
temporally extended actions proceedings th conference uncerainty
articial intelligence pp
rubinstein r cross entropy method combinatorial continuous
optimization methodology computing applied probability
samuel l studies machine learning game checkers ibm
journal development
schaul evolving compact concept sokoban solver master thesis cole
polytechnique fdrale de lausanne
schmidhuber j computer scientist view life universe everything
freksa c jantzen valk r eds foundations computer science
potential theory cognition vol lecture notes computer science pp
springer berlin


fiszita lrincz

spronck p ponsen sprinkhuizen kuyper postma e adaptive game ai
dynamic scripting machine learning
spronck p sprinkhuizen kuyper postma e online adaptation computer
game opponent ai proceedings th belgium netherlands conference
articial intelligence pp
sutton r barto g reinforcement learning introduction mit press
cambridge
szab z pczos b lrincz cross entropy optimization independent
process analysis ica pp
szita select voxels best prediction simplistic
tech rep etvs lornd university hungary
szita lrincz learning tetris noisy cross entropy method neural
computation
tesauro g td gammon self teaching backgammon program achieves masterlevel play neural computation
timuri spronck p van den herik j automatic rule ordering dynamic
scripting third articial intelligence interactive digital entertainment
conference pp
tiong l k rule set representation tness functions articial pac man
playing agent bachelor thesis department information technology electrical
engineering
ward j mackay j c fast hands free writing gaze direction nature

wikipedia pac man wikipedia free encyclopedia wikipedia online
accessed may
witten neal r cleary j g arithmetic coding data compression
communications acm
wolpert h macready w g free lunch theorems optimization ieee
transactions evolutionary computation




