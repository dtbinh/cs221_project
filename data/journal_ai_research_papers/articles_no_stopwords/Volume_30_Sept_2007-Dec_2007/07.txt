Journal Artificial Intelligence Research 30 (2007) 457-500

Submitted 05/07; published 11/07

Using Linguistic Cues Automatic Recognition
Personality Conversation Text
Francois Mairesse

f.mairesse@sheffield.ac.uk

Department Computer Science, University Sheffield
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Marilyn A. Walker

m.a.walker@sheffield.ac.uk

Department Computer Science, University Sheffield
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Matthias R. Mehl

mehl@email.arizona.edu

Department Psychology, University Arizona
1503 E University Blvd. Building 68, Tucson, AZ 85721, USA

Roger K. Moore

r.k.moore@dcs.shef.ac.uk

Department Computer Science, University Sheffield
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Abstract
well known utterances convey great deal information speaker
addition semantic content. One type information consists cues
speakers personality traits, fundamental dimension variation humans.
Recent work explores automatic detection types pragmatic variation
text conversation, emotion, deception, speaker charisma, dominance, point
view, subjectivity, opinion sentiment. Personality affects aspects
linguistic production, thus personality recognition may useful tasks,
addition many potential applications. However, date, little work
automatic recognition personality traits. article reports experimental results
recognition Big Five personality traits, conversation text, utilising
self observer ratings personality. work reports classification results,
experiment classification, regression ranking models. model, analyse
effect different feature sets accuracy. Results show traits, type
statistical model performs significantly better baseline, ranking models
perform best overall. present experiment suggesting ranking models
accurate multi-class classifiers modelling personality. addition, recognition
models trained observed personality perform better models trained using selfreports, optimal feature set depends personality trait. qualitative analysis
learned models confirms previous findings linking language personality,
revealing many new linguistic markers.

1. Introduction
Personality complex attributesbehavioural, temperamental, emotional
mentalthat characterise unique individual.

well known utterances convey great deal information speaker
addition semantic content. One type information consists cues
c
2007
AI Access Foundation. rights reserved.

fiMairesse, Walker, Mehl & Moore

speakers personality traits, fundamental dimension variation humans.
Personality typically assessed along five dimensions known Big Five:
Extraversion vs. Introversion (sociable, assertive, playful vs. aloof, reserved, shy)
Emotional stability vs. Neuroticism (calm, unemotional vs. insecure, anxious)
Agreeableness vs. Disagreeable (friendly, cooperative vs. antagonistic, faultfinding)
Conscientiousness vs. Unconscientious (self-disciplined, organised vs. inefficient, careless)
Openness experience (intellectual, insightful vs. shallow, unimaginative)
five personality traits repeatedly obtained applying factor analyses
various lists trait adjectives used personality description questionnaires (sample
adjectives above) (Norman, 1963; Peabody & Goldberg, 1989; Goldberg, 1990). basis
factor analyses Lexical Hypothesis (Allport & Odbert, 1936), i.e.
relevant individual differences encoded language, important
difference, likely expressed single word. Despite known
limits (Eysenck, 1991; Paunonen & Jackson, 2000), last 50 years Big Five model
become standard psychology experiments using Big Five shown
personality traits influence many aspects task-related individual behaviour. example,
success interpersonal tasks depends personalities participants,
personality traits influence leadership ability (Hogan, Curphy, & Hogan, 1994), general job
performance (Furnham, Jackson, & Miller, 1999), attitude toward machines (Sigurdsson,
1991), sales ability (Furnham et al., 1999), teacher effectiveness (Rushton, Murray, & Erdle,
1987), academic ability motivation (Furnham & Mitchell, 1991; Komarraju &
Karau, 2005). However, date little work automatic recognition
personality traits (Argamon, Dhawle, Koppel, & Pennebaker, 2005; Mairesse & Walker,
2006a, 2006b; Oberlander & Nowson, 2006).
Recent work AI explores methods automatic detection types pragmatic variation text conversation, emotion (Oudeyer, 2002; Liscombe, Venditti, & Hirschberg, 2003), deception (Newman, Pennebaker, Berry, & Richards, 2003;
Enos, Benus, Cautin, Graciarena, Hirschberg, & Shriberg, 2006; Graciarena, Shriberg,
Stolcke, Enos, Hirschberg, & Kajarekar, 2006; Hirschberg, Benus, Brenier, Enos, Friedman, Gilman, Girand, Graciarena, Kathol, Michaelis, Pellom, Shriberg, & Stolcke, 2005),
speaker charisma (Rosenberg & Hirschberg, 2005), mood (Mishne, 2005), dominance
meetings (Rienks & Heylen, 2006), point view subjectivity (Wilson, Wiebe, & Hwa,
2004; Wiebe, Wilson, Bruce, Bell, & Martin, 2004; Wiebe & Riloff, 2005; Stoyanov, Cardie,
& Wiebe, 2005; Somasundaran, Ruppenhofer, & Wiebe, 2007), sentiment opinion
(Turney, 2002; Pang & Lee, 2005; Popescu & Etzioni, 2005; Breck, Choi, & Cardie, 2007).
contrast pragmatic phenomena, may relatively contextualised
short-lived, personality usually considered longer term, stable, aspect
individuals (Scherer, 2003). However, evidence personality interacts with,
affects, aspects linguistic production. example, strong relations
extraversion conscientiousness traits positive affects,
458

fiRecognising Personality Conversation Text

neuroticism disagreeableness various negative affects (Watson & Clark, 1992). Lying leads inconsistencies impressions agreeableness personality trait across modes
(visual vs. acoustic), inconsistencies used cues deception detection
human judges (Heinrich & Borkenau, 1998). Outgoing energetic people (i.e. extravert)
successful deception, apprehensive (i.e. neurotic) individuals
successful (Riggio, Salinas, & Tucker, 1988), individuals score highly agreeableness openness experience traits better detecting deception (Enos
et al., 2006). Features used automatically recognise introversion extraversion
studies important automatically identifying deception (Newman et al., 2003).
Speaker charisma shown correlate strongly extraversion (Bono & Judge,
2004), individuals dominate meetings similar characteristics extraverts,
verbosity (Rienks & Heylen, 2006). Oberlander Nowson (2006) suggest
opinion mining could benefit personality information. Thus evidence suggests
incorporating personality models tasks may improve accuracy.
hypothesise computational recognition user personality could useful many computational applications. Identification leaders using personality
dimensions could useful analysing meetings conversations suspected terrorists (Hogan et al., 1994; Tucker & Whittaker, 2004; Nunn, 2005). Dating websites could
analyse text messages try match personalities increase chances successful
relationship (Donnellan, Conger, & Bryant, 2004). Tutoring systems might effective
could adapt learners personality (Komarraju & Karau, 2005). Automatically
identifying authors personality corpus could improve language generation,
individual differences language affect way concepts expressed (Reiter &
Sripada, 2004). Studies shown users evaluation conversational agents
depends personality (Reeves & Nass, 1996; Cassell & Bickmore, 2003),
suggests requirement systems adapt users personality, humans
(Funder & Sneed, 1993; McLarney-Vesotski, Bernieri, & Rempala, 2006).
applications would possible acquire personality information
asking user author directly (John, Donahue, & Kentle, 1991; Costa & McCrae,
1992), explore whether possible acquire personality models Big Five
personality traits observation individual linguistic outputs text conversation.
date, know two studies besides automatic recognition user
personality (Argamon et al., 2005; Mairesse & Walker, 2006a, 2006b; Oberlander & Nowson,
2006). work applied classification models recognition personality
texts blog postings. knowledge, results presented first
examine recognition personality dialogue (Mairesse & Walker, 2006a, 2006b),
apply regression ranking models allow us model personality recognition
using continuous scales traditional psychology. systematically examine
use different feature sets, suggested psycholinguistic research, report statistically
significant results.
start Section 2 reviewing psychology findings linking personality
language; findings motivate features used learning experiments described
Section 3. Section 3 overviews methods use automatically train personality
models, using conversation written language samples, self-ratings
observer ratings personality traits. explore use classification models (Section 4),
459

fiMairesse, Walker, Mehl & Moore

regression models (Section 5), ranking models (Section 6), effect different
feature sets model accuracy. results show traits, type statistical
model performs significantly better baseline, ranking models perform best
overall. addition, models trained observed personality scores perform better
models trained using self-reports, optimal feature set dependent personality
trait. rules derived features used learned models confirm previous findings
linking language personality, revealing many new linguistic markers. delay
review Argamon et al. (2005) Oberlander Nowson (2006) Section 7,
better compare results own, sum discuss future work
Section 8.

2. Personality Markers Language
believe might possible automatically recognise personality linguistic
cues? Psychologists documented existence cues discovering correlations
range linguistic variables personality traits, across wide range linguistic
levels, including acoustic parameters (Smith, Brown, Strong, & Rencher, 1975; Scherer,
1979), lexical categories (Pennebaker & King, 1999; Pennebaker, Mehl, & Niederhoffer,
2003; Mehl, Gosling, & Pennebaker, 2006; Fast & Funder, 2007), n-grams (Oberlander &
Gill, 2006), speech-act type (Vogel & Vogel, 1986). correlations reported
literature generally weak (see Section 3.3), clear whether features
improve accuracies statistical models unseen subjects. Big Five traits,
extraversion received attention researchers. However, studies focusing
systematically Big Five traits becoming common.
2.1 Markers Extraversion
summarise various findings linking extraversion language cues Table 1,
different levels language production speech, syntax content selection. review Furnham (1990) describes linguistic features linked extraversion traits,
Dewaele Furnham (1999) review studies focusing link extraversion
language learning speech production.
Findings include higher correlation extraversion oral language, especially study involves complex task. Extraverts talk more, louder
repetitively, fewer pauses hesitations, higher speech rates,
shorter silences, higher verbal output, lower type/token ratio less formal language, introverts use broader vocabulary (Scherer, 1979; Furnham, 1990; Gill &
Oberlander, 2002). Extraverts use positive emotion words, show agreements compliments introverts (Pennebaker & King, 1999). Extravert students
learning French second language produce back-channels, implicit style lower lexical richness formal situations. seems complex
task higher level anxiety, easier differentiate introverts
extraverts (Dewaele & Furnham, 1999).
Heylighen Dewaele (2002) note extraversion significantly correlated
contextuality, opposed formality. Contextuality seen high reliance
shared knowledge conversational partners, leading use many deictic
460

fiRecognising Personality Conversation Text

Level
Conversational
behaviour
Topic
selection

Style
Syntax

Lexicon

Speech

Introvert
Listen
Less back-channel behaviour
Self-focused
Problem talk, dissatisfaction
Strict selection
Single topic
semantic errors
self-references
Formal
Many hedges (tentative words)
Many nouns, adjectives, prepositions (explicit)
Elaborated constructions
Many words per sentence
Many articles
Many negations
Correct
Rich
High diversity
Many exclusive inclusive words
social words
positive emotion words
Many negative emotion words
Received accent
Slow speech rate
disfluencies
Many unfilled pauses
Long response latency
Quiet
Low voice quality
Non-nasal voice
Low frequency variability

Extravert
Initiate conversation
back-channel behaviour
self-focused*
Pleasure talk, agreement, compliment
Think loud*
Many topics
Many semantic errors
Many self-references
Informal
hedges (tentative words)
Many verbs, adverbs, pronouns (implicit)
Simple constructions*
words per sentence
articles
negations
Loose*
Poor
Low diversity
exclusive inclusive words
Many social words
Many positive emotion words
negative emotion words
Local accent*
High speech rate
Many disfluencies*
unfilled pauses
Short response latency
Loud
High voice quality
Nasal voice
High frequency variability

Table 1: Summary identified language cues extraversion various production levels, based previous studies Scherer (1979), Furnham (1990), Pennebaker
King (1999), Dewaele Furnham (1999), Gill (2003), Mehl et al. (2006).
Asterisks indicate cue based hypothesis, opposed study
results.

expressions pronouns, verbs, adverbs interjections, whereas formal language
less ambiguous assumes less common knowledge. order measure variation,
Heylighen Dewaele suggest use metric called formality, defined as:
F = (noun freq + adjective freq + preposition freq + article freq - pronoun freq - verb
freq - adverb freq - interjection freq + 100)/2
argue measure important dimension variation
linguistic expressions, shown Bibers factor analysis various genres (Biber, 1988).
addition introversion, authors find formality correlates positively
level education femininity speaker. Situational variables related
use formal language audience size, time span dialogues,
unavailability feedback, difference backgrounds spatial location speakers,
well preceding amount conversation.
461

fiMairesse, Walker, Mehl & Moore

Scherer (1979) shows extraverts perceived talking louder
nasal voice, American extraverts tend make fewer pauses, German extraverts produce pauses introverts. Thus personality markers culture-dependent,
even among western societies.
Oberlander Gill (2006) use content analysis tools n-gram language models
identify markers extravert introvert emails. replicate previous findings
identify new personality markers first person singular pronouns (e.g., dont)
formal greetings (e.g., Hello) introversion, less formal phrases Take care
Hi characterise extraverts.

2.2 Markers Big Five Traits
Pennebaker King (1999) identify many linguistic features associated
Big Five personality traits. use Linguistic Inquiry Word Count (LIWC)
tool count word categories essays written students whose personality
assessed using questionnaire. authors find small significant correlations
linguistic dimensions personality traits. Neurotics use 1st person singular
pronouns, negative emotion words less positive emotion words. hand,
agreeable people express positive fewer negative emotions. use fewer
articles. Conscientious people avoid negations, negative emotion words words reflecting
discrepancies (e.g., would). Finally, openness experience characterised
preference longer words words expressing tentativity (e.g., perhaps maybe),
well avoidance 1st person singular pronouns present tense forms.
Additionally, Mehl et al. (2006) study markers personality perceived observers.
find use words related insight avoidance past tense indicates
openness experience, swearing marks disagreeableness. authors show
linguistic cues vary greatly across gender. example, males perceived
conscientious produce filler words, females dont. Gender differences
found markers self-assessed personality: use 2nd person pronouns indicates
conscientious male, unconscientious female.
Gill Oberlander (2003) study correlates emotional stability: find neurotics use concrete frequent words. However, show observers dont
use cues correctly, observer reports neuroticism correlate negatively selfreports.
Concerning prosody, Smith et al. (1975) show speech rate positively correlated perceived competence (conscientiousness), speech rate inverted-U
relationship benevolence (agreeableness), suggesting need non-linear models.
traits produced findings others. reason might
reflected language, extraversion. However, possible
focus consequence extraversion correlated linguistic cues
analysed easily (e.g., verbosity).
462

fiRecognising Personality Conversation Text

3. Experimental Method
conduct set experiments examine whether automatically trained models
used recognise personality unseen subjects. approach summarised
five steps:
1. Collect individual corpora;
2. Collect associated personality ratings participant;
3. Extract relevant features texts;
4. Build statistical models personality ratings based features;
5. Test learned models linguistic outputs unseen individuals.
following sections describe steps detail.
3.1 Sources Language Personality
Introvert
Ive waking time far.
been, 5 days? Dear me, Ill never
keep up, morning
person all. maybe Ill adjust,
not. want internet access
room, dont yet,
Wed??? think. aint soon
enough, cause got calculus homework [...]

Extravert
really random thoughts.
want best things life.
fear want much!
fall flat face
dont amount anything.
feel born BIG things
earth. knows...
Persian party today.

Neurotic
One friends barged in,
jumped seat. crazy.
tell again.
Im fastidious actually.
certain things annoy me. things
would annoy would actually
annoy normal human being,
know Im freak.

Emotionally stable
excel sport
know push body harder
anyone know, matter test
always push body harder everyone
else. want best matter
sport event.
good love ride
bike.

Table 2: Extracts essays corpus, participants rated extremely introvert,
extravert, neurotic, emotionally stable.

use data Pennebaker King (1999) Mehl et al. (2006) experiments. first corpus contains 2,479 essays psychology students (1.9 million
words), told write whatever comes mind 20 minutes. data
collected analysed Pennebaker King (1999); sample shown Table 2.
463

fiMairesse, Walker, Mehl & Moore

Introvert
- Yeah would kilograms. Yeah see
youre saying.
- Tuesday class. dont know.
- dont know. A16. Yeah, kind cool.
- dont know. cant wait
every night,
know?
- Yeah. dont know. bed
there? Well ok just...

Extravert
- Thats first yogurt experience here.
Really watery. Why?
- Damn. New game.
- Oh.
- Thats rude. That.
- Yeah, he, other.
likes her.
- going end breaking
hes going like.

Unconscientious
- Chinese. Get together.
- tried yell window.
Oh. xxxxs fucking dumb ass. Look
him. Look him, dude. Look him.
wish camera. Hes fucking brushing
t-shirt tooth brush. Get kick
it. Dont steal nothing.

Conscientious
- dont, dont know fact
would imagine historically women
entered prostitution done
so, everyone, majority
extreme desperation think. dont
know, think people understand
desperation dont dont see [...]

Table 3: Extracts EAR corpus, participants rated extremely introvert, extravert, unconscientious, conscientious. participants utterances
shown.

Personality assessed asking student fill Big Five Inventory questionnaire (John et al., 1991), asks participants evaluate 5 point scale well
personality matches series descriptions.
second source data consists conversation extracts recorded using Electronically Activated Recorder (EAR) (Mehl, Pennebaker, Crow, Dabbs, & Price, 2001), collected
Mehl et al. (2006). preserve participants privacy, random snippets conversation recorded. corpus much smaller essays corpus (96 participants
total 97,468 words 15,269 utterances). essays corpus consists
texts, EAR corpus contains sound extracts transcripts. corpus therefore
allows us build models personality recognition speech. participants utterances transcribed (not conversational partners), making impossible
reconstruct whole conversations. Nevertheless, conversation extracts less formal
essays, personality may best observed absence behavioural constraints. Table 4 shows essays corpus much larger EAR corpus,
amount data per subject comparable, i.e. 766 words per subject essays
1,015 EAR corpus. Table 3 shows examples conversations EAR corpus
different personality traits.
personality ratings, EAR corpus contains self-reports ratings 18
independent observers. Psychologists use self-reports facilitate evaluating personality large number participants, large number standard self-report
tests. Observers asked make judgments rating descriptions Big Five
Inventory (John & Srivastava, 1999) 7 point scale (from strongly disagree strongly
464

fiRecognising Personality Conversation Text

Dataset
Source language
Personality reports
Number words
Subjects
Words per subject

Essays
Written
Self reports
1.9 million
2,479
766.4

EAR
Spoken
Self observer
97,468
96
1,015.3

Table 4: Comparison essays EAR corpora.
agree), without knowing participants. Observers divided three groups,
rating one third participants, listening participants entire set sound
files (130 files average). personality assessment based audio recordings,
contain information transcripts (e.g., ambient sounds, including captured conversations). Mehl et al. (2006) report strong inter-observer reliabilities across
Big Five dimensions (intraclass correlations based one-way random effect models: mean
r = 0.84, p < .01). observers ratings averaged participant, produce
final scores used experiments.
Interestingly, average correlations frequency counts psycholinguistic
word categories Big Five personality dimensions considerably larger
EAR corpus student essays studied Pennebaker King. Moreover,
correlations reported Mehl et al. seem higher observer reports
self-reports. Based observation, hypothesise models observed personality
outperform models self-assessed personality.
3.2 Features
features used experiments motivated previous psychological findings
correlations measurable linguistic factors personality traits. Features
divided subsets depending source described subsections below.
total feature set summarised Table 6. experimental results given Sections 4,
5, 6 examine effect feature subset model accuracy.
3.2.1 Content Syntax
extracted set linguistic features essay conversation transcript,
starting frequency counts 88 word categories Linguistic Inquiry Word
Count (LIWC) utility (Pennebaker et al., 2001). features include syntactic (e.g.,
ratio pronouns) semantic information (e.g., positive emotion words),
validated expert judges. LIWC features illustrated Table 5. Pennebaker
King (1999) previously found significant correlations features
Big Five personality traits. Relevant word categories extraversion include social
words, emotion words, first person pronouns, present tense verbs. Mehl et al. (2006)
showed LIWC features extracted EAR corpus significantly correlated
self observer reports personality.
added 14 additional features MRC Psycholinguistic database (Coltheart, 1981), contains statistics 150,000 words, estimates age
465

fiMairesse, Walker, Mehl & Moore

Feature
Anger words
Metaphysical issues
Physical state/function
Inclusive words
Social processes
Family members
Past tense verbs
References friends
Imagery words
Syllables per word
Concreteness
Frequency use

Type
LIWC
LIWC
LIWC
LIWC
LIWC
LIWC
LIWC
LIWC
MRC
MRC
MRC
MRC

Example
hate, kill, pissed
God, heaven, coffin
ache, breast, sleep
with, and, include
talk, us, friend
mom, brother, cousin
walked, were,
pal, buddy, coworker
Low: future, peace - High: table, car
Low: - High: uncompromisingly
Low: patience, candor - High: ship
Low: duly, nudity - High: he,

Table 5: Examples LIWC word categories MRC psycholinguistic features (Pennebaker et al., 2001; Coltheart, 1981). MRC features associate word
numerical value.

acquisition, frequency use, familiarity. introverts take longer reflect
utterances, Heylighen Dewaele (2002) suggest vocabulary richer
precise, implying lower frequency use. MRC feature set previously used
Gill Oberlander (2002), showed extraversion negatively correlated
concreteness. Concreteness indicates neuroticism, well use frequent
words (Gill & Oberlander, 2003). Table 5 shows examples MRC scales. MRC
feature computed averaging feature value words essay conversational extract. Part-of-Speech tags computed identify correct entry
database among set homonyms.
3.2.2 Utterance Type
Various facets personality traits seem depend level initiative speaker
type utterance used (e.g., assertiveness, argumentativeness, inquisitiveness, etc.).
example, extraverts assertive emails (Gill & Oberlander, 2002),
extravert second language learners shown produce back-channel behaviour
(Vogel & Vogel, 1986). therefore introduced features characterising types utterance produced. automatically tagged utterance EAR corpus speech
act categories Walker Whittaker (1990), using heuristic rules based utterances parse tree:
Command: utterance using imperative form, command verb (e.g., must to)
yes/no second person question modal auxiliary can;
Prompt: single word utterance used back-channelling (e.g., Yeah, OK, Huh, etc.);
Question: interrogative utterance isnt command;
Assertion: utterance.
466

fiRecognising Personality Conversation Text

LIWC FEATURES (Pennebaker et al., 2001):
Standard counts:
- Word count (WC), words per sentence (WPS), type/token ratio (Unique), words captured (Dic), words
longer 6 letters (Sixltr), negations (Negate), assents (Assent), articles (Article),
prepositions (Preps), numbers (Number)
- Pronouns (Pronoun): 1st person singular (I), 1st person plural (We), total 1st person (Self), total
2nd person (You), total 3rd person (Other)
Psychological processes:
- Affective emotional processes (Affect): positive emotions (Posemo), positive feelings (Posfeel), optimism
energy (Optim), negative emotions (Negemo), anxiety fear (Anx), anger (Anger),
sadness (Sad)
- Cognitive Processes (Cogmech): causation (Cause), insight (Insight), discrepancy (Discrep), inhibition
(Inhib), tentative (Tentat), certainty (Certain)
- Sensory perceptual processes (Senses): seeing (See), hearing (Hear), feeling (Feel)
- Social processes (Social): communication (Comm), references people (Othref), friends (Friends),
family (Family), humans (Humans)
Relativity:
- Time (Time), past tense verb (Past), present tense verb (Present), future tense verb (Future)
- Space (Space): (Up), (Down), inclusive (Incl), exclusive (Excl)
- Motion (Motion)
Personal concerns:
- Occupation (Occup): school (School), work job (Job), achievement (Achieve)
- Leisure activity (Leisure): home (Home), sports (Sports), television movies (TV), music (Music)
- Money financial issues (Money)
- Metaphysical issues (Metaph): religion (Relig), death (Death), physical states functions (Physcal),
body states symptoms (Body), sexuality (Sexual), eating drinking (Eating), sleeping
(Sleep), Grooming (Groom)
dimensions:
- Punctuation (Allpct): period (Period), comma (Comma), colon (Colon), semi-colon (Semic), question
(Qmark), exclamation (Exclam), dash (Dash), quote (Quote), apostrophe (Apostro), parenthesis
(Parenth), (Otherp)
- Swear words (Swear), nonfluencies (Nonfl), fillers (Fillers)
MRC FEATURES (Coltheart, 1981):
Number letters (Nlet), phonemes (Nphon), syllables (Nsyl), Kucera-Francis written frequency (K-Ffreq), Kucera-Francis number categories (K-F-ncats), Kucera-Francis number samples (K-F-nsamp),
Thorndike-Lorge written frequency (T-L-freq), Brown verbal frequency (Brown-freq), familiarity rating
(Fam), concreteness rating (Conc), imageability rating (Imag), meaningfulness Colorado Norms (Meanc),
meaningfulness Paivio Norms (Meanp), age acquisition (AOA)
UTTERANCE TYPE FEATURES:
Ratio commands (Command), prompts back-channels (Prompt), questions (Question), assertions (Assertion)
PROSODIC FEATURES:
Average, minimum, maximum standard deviation voices pitch Hz (Pitch-mean, Pitch-min,
Pitch-max, Pitch-stddev) intensity dB (Int-mean, Int-min, Int-max, Int-stddev), voiced time (Voiced)
speech rate (Word-per-sec)

Table 6: Description features, feature labels brackets.
evaluated automatic tagger applying set 100 hand-labelled utterances
randomly selected EAR corpus. obtain 88% correct labels, mostly
assertions. Table 7 summarises partition evaluation results speech act
type. speech act, corresponding feature value ratio number
occurrences speech act total number utterances text.
467

fiMairesse, Walker, Mehl & Moore

Label
Assertion
Command
Prompt
Question


Fraction
73.0%
4.3%
7.0%
15.7%
100%

Labelling accuracy
0.95
0.50
0.57
1.00
0.88

Table 7: Partition speech acts automatically extracted EAR corpus,
classification accuracies sample 100 hand-labelled utterances.

3.2.3 Prosody
Personality shown influence speech production. Extraversion associated
variation fundamental frequency (Scherer, 1979), higher voice quality
intensity (Mallory & Miller, 1958), fewer shorter silent pauses (Siegman
& Pope, 1965). Smith et al. (1975) showed speech rate positively correlated
perceived competence (conscientiousness). Interestingly, authors found speech
rate inverted-U relationship benevolence (agreeableness), suggesting need
non-linear models. See Section 3.4.
added prosodic features based audio data EAR conversation extracts.
EAR recorded participants anytime day, necessary automatically remove non-voiced signal. used Praat (Boersma, 2001) compute features
characterising voices pitch intensity (mean, extremas standard deviation),
added estimate speech rate dividing number words voiced time.
important aspect work features extracted without manual
annotation beyond transcription, didnt filter utterances speakers
may captured EAR even though utilised microphone pointing towards
participants head. Although advances speaker recognition techniques might improve
accuracy prosodic features, make assumption noise introduced
surrounding speakers little effect prosodic features, therefore
affect performance statistical models. assumption still remains tested,
personality similarity-attraction effect (Byrne & Nelson, 1965) might influence
personality distribution participants conversational partners.
included features mentioned section (117) models based
EAR corpus. Models computed using essays corpus contain LIWC MRC
features (102), speech acts meaningful dialogues.
3.3 Correlational Analysis
order assess individual features important modelling personality regardless
model used, report previous correlational studies LIWC features
data well analyses new MRC, utterance type prosodic features.
LIWC features already analysed Mehl et al. (2006) EAR dataset,
468

fiRecognising Personality Conversation Text

Pennebaker King (1999) essays.1 Tables 8 11 show features correlating
significantly personality ratings (p < .05, correlations .05 only), combining
together results previous studies new findings provide insight features
likely influence personality recognition models Sections 4.3, 5.3 6.3.
correlation magnitudes Tables 8 9 LIWC MRC features
essays data set show although extraversion well perceived conversations,
isnt strongly reflected written language, correlation magnitudes
essays dataset noticeably low. Table 10 shows word count (WC) important feature modelling extraversion conversation, observer reports
self-reports. Interestingly, marker doesnt hold written language (see Table 9).
markers common observed self-reported extraversion include variation
intensity (Int-stddev), mean intensity (Int-mean), word repetitions (Unique), words
high concreteness (Conc) imageability (Imag). See Table 11.
hand, words related anger, affect, swearing, positive negative emotions (Posemo
Negemo) perceived extravert, dont mark self-assessed extraversion
conversations.
Tables 10 11 show emotional stability, markers hold
self-reports observer reports: high word count low mean pitch (Pitch-mean).
Surprisingly, observed emotional stability associated swearing anger words,
self-assessed ratings. reported Mehl et al. (2006), neurotics expected
produce self-references (Self I). Pennebaker King (1999) show neurotics
use self-references observed essays, well use words related
negative emotions anxiety. Table 11 shows conversations, self-assessed neurotics
tend low constant voice intensity (Int-mean Int-stddev),
markers arent used observers all.
emotional stability expressed differently various datasets, markers
agreeableness consistent: words related swearing (Swear) anger (Anger) indicate self-assessed observed disagreeableness, regardless source language.
See Tables 8, 9 10. Interestingly, Table 11 shows agreeable people
back-channelling (Prompt), suggesting tend listen conversational
partners. observers dont seem take prosody account evaluating agreeableness, Table 11 shows prosodic cues pitch variation (Pitch-stddev)
maximum voice intensity (Max-int) indicate self-assessed disagreeableness.
far markers conscientiousness concerned, Tables 8 10 show
similar agreeableness, unconscientious participants use words related
swearing (Swear), anger (Anger) negative emotions (Negemo), regardless
dataset assessment method. hand, observed conscientiousness associated
words expressing insight, back-channels (Prompt), longer words (Nphon, Nlet, Nsyl
Sixltr) well words acquired late children (AOA), self-assessed
conscientiousness mostly expressed positive feelings (Posfeel) conversations.
avoidance negative language seems main marker conscientiousness
essays, features Table 8 correlate weakly self-reports.
1. correlations differ Pennebaker Kings study use additional student essays
collected following years.

469

fiMairesse, Walker, Mehl & Moore

Trait
LIWC
Achieve
Affect
AllPct
Anger
Anx
Apostro
Article
Assent
Body
Cause
Certain
Cogmech
Comm
Comma
Death
Dic
Excl
Exclam
Family
Feel
Fillers
Friends
Future
Groom
Hear
Home
Humans

Incl
Inhib
Insight
Job
Leisure
Metaph
Motion
Music
Negate
Negemo
Nonfl
Number
Occup
Optim

Othref
Parenth
Period
Physcal
Posemo
Posfeel
Preps
Present
Pronoun
Qmark

Extraversion

.03
.03
-.08**
-.03
-.01
-.08**
-.08**
.01
-.05**
.01
.05*
-.03
-.02
-.02
-.02
.05*
-.01
.00
.05*
-.01
-.04*
.06**
-.02
-.02
-.03
-.01
.04
.05*
.04*
-.03
-.01
.02
-.03
-.01
.03
-.04*
-.08**
-.03
-.03
-.03
.03
.03
.06**
.07**
-.06**
-.05*
-.02
.07**
.07**
.00
.00
.07**
-.06**

Emotional
stability
.01
-.07**
-.04
-.08**
-.14**
-.04
.11**
.02
-.04
-.03
-.01
-.02
.00
.01
-.04
-.09**
.02
-.05*
-.05*
-.09**
.01
-.04*
.01
-.02
.00
-.02
-.02
-.15**
-.01
.02
-.01
.01
.07**
.01
-.01
.06**
-.12**
-.18**
.01
.05*
.05*
.04
-.01
.02
.03
-.03
-.05*
.07**
-.01
.06**
-.12**
-.12**
-.05*

Agreeableness

-.01
-.04
-.01
-.16**
.03
-.02
-.03
.00
-.04*
.00
.03
-.02
-.01
-.02
-.02
.06**
-.02
.06**
.09**
.04
-.01
.02
.02
.01
-.01
.04*
-.03
.05*
.03
-.02
.00
.01
.03
-.01
.05*
-.01
-.11**
-.11**
.01
-.03
.04
.01
.03
.01
-.04*
-.01
-.03
.05*
.03
.04
-.01
.04*
-.04

Conscientiousness

.02
-.06**
-.04
-.14**
.05*
-.06**
.02
-.04
-.04*
-.04
.04*
-.06**
-.05**
-.01
-.06**
.06**
-.01
.00
.04*
.02
-.03
.01
.07**
.01
-.04*
.06**
-.08**
.04
.04*
-.02
-.03
.05**
-.01
-.08**
.03
-.07**
-.07**
-.11**
-.05*
-.02
.09**
.08**
.01
.01
-.01
-.01
-.03
.02
-.02
.08**
-.03
.02
-.06**

Openness
experience
-.07**
.04*
.10**
.06**
-.04
.05**
.11**
.04*
.02
-.05*
.04
.02
.03
.10**
.05*
-.20**
.07**
-.03
-.07**
-.04*
-.01
-.12**
-.04
-.05**
.04*
-.15**
.04
-.14**
-.03
.04*
.05*
-.05**
-.05**
.08**
-.13**
.10**
.01
.04
.02
-.06**
-.18**
-.07**
.01
.06**
.10**
.04
.01
.02
.08**
-.04
-.09**
-.06**
.08**

Table 8: Pearsons correlation coefficients LIWC features personality ratings
essays dataset, based analysis Pennebaker King (1999)
(* = significant p < .05 level, ** = p < .01). features correlate
significantly least one trait shown.

470

fiRecognising Personality Conversation Text

Trait
LIWC (2)
Quote
Relig
Sad
School
See
Self
Semic
Sexual
Sixltr
Sleep
Social
Space
Sports
Swear
Tentat
Time
TV
Unique

WC

WPS

MRC
AOA
Brown-freq
Conc
Fam
Imag
K-F-freq
K-F-ncats
K-F-nsamp
Meanc
Meanp
Nlet
Nphon
Nsyl
T-L-freq

Extraversion

Emotional
stability

Agreeableness

Conscientiousness

Openness
experience

-.05*
.00
.00
.03
.00
.07**
-.03
.07**
-.06**
-.01
.08**
-.02
.01
-.01
-.06**
-.02
-.04
-.05**
.03
.03
.06**
-.01
-.01

-.02
.03
-.12**
.05**
.09**
-.14**
.02
-.02
.06**
-.03
.00
.05*
.09**
.00
-.01
.02
.04*
.10**
.06**
-.06**
.07**
.02
.03

-.01
.00
.00
.06**
.00
.06**
.02
.00
-.05*
-.02
.02
.03
.02
-.14**
-.03
.07**
-.02
-.04*
.02
.01
.04*
.02
-.06**

-.03
-.06**
.01
.10**
-.03
.04*
.00
-.04
.02
.03
-.02
.01
.00
-.11**
-.06**
.09**
-.04*
-.05*
-.01
.02
.01
-.02
-.04*

.09**
.07**
-.01
-.20**
.05**
-.14**
.05**
.09**
.10**
-.08**
.02
-.04
-.05**
.08**
.05*
-.15**
.04
.09**
-.06**
.05*
.04
.06**
.11**

-.01
.05*
.02
.08**
.05*
-.01
.06**
.06**
.06**
.02
-.09**
-.08**
-.07**
.01

.05*
-.06**
-.06**
-.05*
-.04*
.10**
-.04*
-.01
-.10**
-.02
.09**
.08**
.07**
.10**

-.04*
.03
.03
.08**
.05*
.00
.08**
.03
.05**
.05*
-.03
-.03
-.02
.01

.06**
.06**
-.01
.05**
.00
.05*
.07**
.05**
-.01
.00
.00
.01
.04
.06**

.11**
-.07**
-.10**
-.17**
-.08**
.07**
-.12**
-.07**
-.11**
-.04*
.15**
.14**
.13**
.05**

Table 9: Continuation Table 8, i.e. Pearsons correlation coefficients LIWC
MRC features personality ratings essays dataset (* = significant
p < .05 level, ** = p < .01). features correlate significantly
least one trait shown.
Tables 8 9 show openness experience trait yielding highest correlations essays corpus: articles, second person pronouns (You) long words (Sixltr)
indicate openness, non-open participants tend talk occupations (Occup,
Home School) (Self). far conversations concerned, observers
use similar cues openness conscientiousness, insight words, longer words,
back-channels high age acquisition (AOA).
section shows features likely vary depending source language
method assessment personality. analyses help evaluate
usefulness individual features, question features combined
predict personality accurately addressed statistical models.
471

fiMairesse, Walker, Mehl & Moore

Dataset
Trait
LIWC
Affect
Anger
Articles
Assent
Cause
Cogmech
Comm
Dic
Discrep
Eating
Family
Feel
Female
Filler
Friend
Hear
Home
Humans

Inhib
Insight
Metaph
Money
Negemo
Nonfl

Othref
Past
Physcal
Posfeel
Pronoun
Relig
Self
Senses
Sexual
Sixltr
Social
Space
Sports
Swear
Tentat
Unique

WC

Extra

Observer reports
Emot
Agree Consc

Open

Extra

Emot

.40**
.37**
.21*
-.29**
-.13
.04
-.18
-.07
.08
.25*
.26*
.21*
.29**
-.01
.14
-.20
-.02
-.01
.03
.19
.04
.30**
-.02
.36**
-.01
.09
.00
-.19
.30**
.28**
-.02
.30**
.09
-.04
.24*
-.04
-.04
.03
.10
.30**
-.04
-.6**
.06
.63**

.13
.30**
.32**
-.02
-.23*
-.01
-.27**
-.16
-.03
.15
-.23*
.06
-.03
-.19
-.01
-.23*
-.19
.21*
-.41**
.01
-.02
.07
.24*
.18
.05
.02
.05
-.07
.24*
.04
-.30**
.06
-.42**
-.12
.21*
-.04
-.06
.18
.28**
.27**
.15
-.18
.04
.28**

.00
-.14
.14
.03
.00
.23*
-.26*
-.08
.23*
-.11
-.04
.05
-.17
.01
-.14
-.29**
.06
-.12
-.17
.00
.32**
-.02
.01
-.11
.06
-.17
-.22*
-.31**
-.17
.05
-.28**
-.07
-.15
-.26*
-.22*
.24*
-.31**
-.07
-.11
-.17
.30**
-.12
-.05
.20

.05
-.02
.03
-.11
.00
.11
-.01
.02
.10
-.03
.14
.08
.24*
-.05
.20*
-.04
.04
.07
.21*
.02
-.06
.20
-.08
.03
-.02
.02
.02
-.10
-.07
.06
.12
.26*
.25*
.03
-.05
-.20
.06
-.10
.03
-.08
-.14
-.32**
.06
.29**

-.13
.07
.00
-.05
-.09
.01
-.13
-.15
-.01
-.02
-.02
.05
.07
-.13
.01
-.08
-.12
-.03
-.16
.02
-.10
.10
.01
-.05
.17
.04
.13
-.18
-.06
-.14
-.07
.15
-.17
-.10
.04
-.15
.04
.09
.21*
.06
.04
-.22*
.07
.22*

-.20
-.49**
.03
.30**
.03
.24*
-.14
-.17
.13
-.31**
-.12
.03
.04
.04
-.08
-.19
.03
-.01
-.21*
-.22*
.34**
-.10
-.13
-.44**
.09
-.07
-.13
-.25*
-.39**
.05
-.23*
-.09
-.25*
-.18
-.49**
.25*
-.17
-.21*
-.15
-.51**
.26*
-.03
-.08
.10

-.24*
-.56**
-.15
.24*
.15
.20*
.00
-.05
.10
-.43**
-.03
-.03
.03
.20*
-.13
-.07
.04
-.23*
-.08
-.14
.29**
-.26*
-.24*
-.49**
.24*
-.09
-.14
-.18
-.47**
.14
-.17
-.27**
-.13
-.15
-.48**
.30**
-.15
-.24*
-.19
-.61**
.15
-.03
-.11
.07

Self-reports
Agree Consc
-.17
-.30**
.04
.19
.07
.08
.20*
.16
.15
-.10
.26**
-.08
.29**
.20
.05
.13
.29**
-.20
.23*
-.18
.03
-.10
-.22*
-.16
-.03
.05
.07
-.05
-.16
-.07
.19
-.06
.18
.12
-.19
-.01
.12
-.18
-.15
-.28**
.05
-.18
-.05
.18

-.19
-.30**
-.09
-.03
-.02
.00
.12
-.01
.09
-.19
.04
.02
.12
.18
.16
.07
-.03
-.06
.01
-.11
.01
-.09
-.06
-.25*
-.02
.05
.01
.05
-.27**
.23*
.05
-.09
.02
.03
-.23*
.19
.06
.01
-.05
-.29**
.14
-.05
.03
.03

Open
.13
.10
-.04
.08
-.23*
-.06
-.17
-.20*
-.09
-.05
-.14
.02
-.22*
-.08
-.11
-.19
-.07
.01
-.08
-.12
.05
.03
-.15
.10
.17
-.28**
-.19
-.26**
.05
.11
-.21*
.04
-.08
-.14
.04
.03
-.21*
.23*
-.03
.06
.05
-.03
.31**
.06

Table 10: Pearsons correlation coefficients LIWC features personality ratings
EAR dataset, based analysis Mehl et al. (2006) (* = significant
p < .05 level, ** = p < .01). features correlate significantly
least one trait shown.
3.4 Statistical Models
Various systems require different levels granularity modelling personality: might
important cluster users large groups correctly possible, system
might need discriminate individual users. Depending application
adaptation capabilities target system, possible use different types personality
models, depending whether personality modelling treated classification problem,
472

fiRecognising Personality Conversation Text

Dataset
Trait
Prosody
Int-max
Int-mean
Int-stddev
Pitch-max
Pitch-mean
Pitch-min
Pitch-stddev
Voiced
Word-per-sec
MRC
AOA
Brown-freq
Conc
Fam
Imag
K-F-freq
K-F-ncats
K-F-nsamp
Meanc
Nlet
Nphon
Nsyl
T-L-freq
Utterance
type
Assertion
Command
Prompt
Question

Extra

Observer reports
Emot
Agree Consc

Open

Extra

Emot

Self-reports
Agree Consc

.42**
.32**
.40**
.28**
.17
-.17
-.13
.23*
.07

.12
.20
.03
.10
-.45**
-.23*
.13
.27**
-.14

.07
-.02
-.08
.13
.06
-.02
.07
.06
-.12

-.13
-.06
-.12
.05
.04
.08
.03
.03
-.04

.05
.04
-.08
.23*
-.18
-.04
.11
.21*
-.17

.19
.21*
.36**
-.03
.12
.09
-.28**
-.02
.20*

.10
.22*
.28**
-.11
-.25*
-.08
.01
.07
.07

-.25*
-.05
.00
-.10
.07
.21*
-.34**
-.04
.09

-.01
-.16
-.06
-.03
.03
.04
.03
-.03
.02

.14
.03
.10
.01
-.04
.08
-.03
.03
.04

-.23*
-.26*
.24*
-.17
.33**
-.27**
-.24*
-.24*
.29**
-.14
-.12
-.16
-.24*

.01
-.41**
-.05
-.28**
.00
-.04
-.24*
-.20*
-.10
.17
.09
-.04
-.06

.26**
-.08
-.20*
-.24*
-.23*
.07
-.03
-.03
-.18
.25*
.25*
.23*
.06

.26**
.07
-.33**
-.07
-.33**
.17
.08
.16
-.25*
.31**
.36**
.34**
.16

.21*
-.16
-.32**
-.18
-.35**
.16
.00
.20
-.34**
.25*
.28**
.19
.13

-.12
-.04
.23*
-.03
.25*
-.22*
-.01
-.15
.23*
-.23*
-.16
-.13
-.19

.04
-.15
-.10
-.21*
-.09
-.06
-.06
-.04
-.12
.03
.02
-.02
-.07

.05
.14
.01
.17
.01
-.24*
.17
.03
.08
-.18
-.20
-.06
-.18

-.05
.07
-.12
.01
-.06
.05
.05
.08
-.06
.13
.15
.12
.06

.08
-.12
-.02
-.13
-.03
-.01
-.23*
-.17
-.07
.12
.13
.10
-.08

-.05
.00
-.10
.13

-.21*
.01
.07
.22*

-.03
-.08
.36**
-.16

.01
-.20*
.27**
-.11

-.09
.00
.25*
-.04

-.02
.13
-.05
.01

-.06
.21*
.01
-.01

-.09
-.01
.22*
-.02

.21*
.00
-.05
-.24*

-.14
.16
.02
.10

Open

Table 11: Continuation Table 10, i.e. Pearsons correlation coefficients features
personality ratings EAR dataset (* = significant p < .05 level,
** = p < .01). features correlate significantly least one trait
shown.
previous work Argamon et al. (2005) Oberlander Nowson (2006), whether
model personality traits via scalar values actually generated self-reports
observer methods used corpus collection described Section 3.1.
support applications dialogue system adaptation, output generation
limited points extremes personality scale, introvert vs. extravert
language neurotic vs. emotionally stable, develop classification models splitting
subjects two equal size groups.
However, model personality traits scalar values, two choices.
treat personality modelling regression problem ranking problem.
regression models replicate actual scalar values seen personality ratings
data, good argument treating personality ranking problem
definition, personality evaluation assesses relative differences individuals, e.g.
one person described extravert average population not. Moreover,
Freund, Iyer, Schapire, Singer (1998) argue ranking models better fit
learning problems scales arbitrary values (rather reflecting real world
measures).
473

fiMairesse, Walker, Mehl & Moore

classification regression models, use Weka toolbox (Witten & Frank,
2005) training evaluation. order evaluate models personality classification,
compare six different learning algorithms baseline returning majority class.
classification algorithms analysed C4.5 decision tree learning (J48), Nearest
neighbour (k = 1), Naive Bayes (NB), Ripper (JRip), Adaboost (10 rounds boosting)
Support vector machines linear kernels (SMO).
regression, compare five algorithms baseline model returning mean
personality score. focus linear regression model, M5 regression tree, M5
model tree returning linear model, REPTree decision tree, model based Support
vector machines linear kernels (SMOreg). Parameters algorithms set
Wekas default values.
Concerning ranking problem, train personality models Big Five trait
using RankBoost, boosting algorithm ranking (Freund et al., 1998; Schapire, 1999).
Given personality trait model, linguistic features personality scores converted training set ordered pairs examples x, y:
= {(x, y)| x, language samples two individuals,
x higher score personality trait}
example x represented set indicator functions hs (x) 1 m.
indicator functions calculated thresholding feature values (counts) described
Section 3.2. example, one indicator function is:
(

h100 (x) =

1 Word-per-sec(x) 0.73
0 otherwise

h100 (x) = 1 xs average speech rate 0.73 words per second. single parameter associated indicator function, ranking score example
x calculated
X
F (x) =
hs (x)


score used rank various language samples (written text conversation extracts),
goal duplicating ranking found training data, training examples used set parameter values . Training process setting
parameters minimise following loss function:
Loss =

1 X
eval(F (x) F (y))
|T | (x,y)T

eval function returns 1 ranking scores (x, y) pair misordered, 0
otherwise. words, ranking loss percentage misordered pairs,
order predicted scores doesnt match order dictated personality scores
questionnaire.
techniques used work express learned models rules decision
trees, support analysis differences personality models (see Sections 4.3,
5.3 6.3).
474

fiRecognising Personality Conversation Text

4. Classification Results
evaluate binary classification models based essays corpus self-reports
personality, well models based EAR corpus self observer reports.
results averaged 10-fold cross-validation, significance tests done
using two-tailed paired t-test p < .05 level.
4.1 Essays Corpus
Classification results essays corpus self-reports Table 12. Interestingly,
openness experience easiest trait model five classifiers six significantly
outperform baseline four produce best performance trait,
accuracies 62.1% using support vector machines (SMO). Emotional stability
produces second best performance four classifiers six, 57.4% accuracy
SMO model. Conscientiousness hardest trait model two classifiers
significantly outperform baseline, however SMO model performs well best
model extraversion agreeableness, around 55% correct classifications.
find support vector machines generally perform best, Naive Bayes
AdaboostM1 second position. SMO significantly outperforms majority class baseline
trait. J48 decision tree recognising extraversion shown Figure 1,
rule-based JRip model classifying openness experience 58.8% accuracy illustrated
Table 16.

Trait
Base J48
NN
NB
JRIP
Extraversion
50.04 54.44 53.27 53.35 52.70
Emotional stability
50.08 51.09
51.62
56.42 55.90
Agreeableness
50.36 53.51 50.16
53.88 52.63
Conscientiousness
50.57 51.37
52.10
53.80
52.71
Openness experience 50.32 54.24 53.07
59.57 58.85
statistically significant improvement majority
baseline (two-tailed paired t-test, p < .05)

ADA
55.00
55.98
52.71
54.45
59.09
class

SMO
54.93
57.35
55.78
55.29
62.11

Table 12: Classification accuracy two equal size bins essays corpus, using selfreports. Models majority class baseline (Base); J48 decision tree (J48);
Nearest neighbour (NN); Naive Bayes (NB); JRip rule set (JRIP); AdaboostM1
(ADA); Support vector machines (SMO).

Feature set comparison: order evaluate feature set contributes
final result, trained binary classifiers using algorithms producing best overall
results feature set. analyse LIWC MRC features essays
corpus, utterance type prosodic features dont apply written texts. use
Naive Bayes, AdaboostM1 SMO classifiers give best performances
full feature set. Results shown Table 13.
475

fiMairesse, Walker, Mehl & Moore

Articles
7.23

> 7.23

Sexuality

Introvert

0.12

> 0.12
Parentheses

Apostrophes
2.57

> 2.57

17.91

> 17.91


0.64

Sadness

Achievement

Words per sentence

Extravert

1.52
Introvert

> 0.64

0.64

> 1.52
Extravert

1.44
Extravert

Introvert
> 1.44
Introvert

> 0.64
Familiarity

Introvert

> 599.7

599.7

Positive emotions

Introvert

1.66

> 1.66
Grooming

Introvert
0.11

> 0.11

Extravert

Introvert

Figure 1: J48 decision tree binary classification extraversion, based essays
corpus self-reports.

Remarkably, see LIWC features outperform MRC features every
trait, LIWC features always perform slightly better full
feature set. clearly suggests MRC features arent helpful LIWC features
classifying personality written text, however Table 13 shows still
outperform baseline four traits five.
Concerning algorithms, find AdaboostM1 performs best extraversion
(56.3% correct classifications), SMO produces best models traits.
suggests support vector machines promising modelling personality general.
easiest trait model still openness experience, 62.5% accuracy using LIWC
features only.
4.2 EAR Corpus
Classification accuracies EAR corpus Table 14. find extraversion
easiest trait model using observer reports, Naive Bayes AdaboostM1
476

fiRecognising Personality Conversation Text

Feature set
None
LIWC features
MRC features
Classifier
Base
NB
ADA
SMO
NB
ADA
SMO
Set size
0
88
88
88
14
14
14
Extraversion
50.04 52.71
56.34 52.75
52.87 51.45
53.88
Emotional stability
50.08 56.02 55.33 58.20 52.39
52.06
53.52
Agreeableness
50.36 54.12 52.71
56.39 53.03 52.06
53.31
Conscientiousness
50.57 53.92 54.48 55.62 53.03
52.95
53.84
Openness experience 50.32 58.92 58.64 62.52 55.41 56.70 57.47
statistically significant improvement majority class
baseline (two-tailed paired t-test, p < .05)
Table 13: Classification accuracies two equal size bins essays corpus using
majority class baseline (Base), Naive Bayes (NB), AdaboostM1 (ADA) Support Vector Machine (SMO) classifiers, different feature sets. Best model
trait bold.

outperforming baseline accuracy 73.0%. J48 decision tree extraversion 66.8% accuracy shown Figure 2. Emotional stability modelled
comparable success using Naive Bayes classifier, however improvement baseline lower extraversion (22.8% vs. 25.2%) classifiers dont perform
well. Models observed conscientiousness outperform baseline, 67.7% accuracy using Naive Bayes classifier, best model agreeableness produces 61.3%
correct classifications. None models openness experience significantly outperform baseline, suggests openness experience expressed clearly
stream consciousness essays self-reports EAR dataset. Support vector
machines dont perform well essays corpus, probably sparseness dataset. Self-reports much harder model observer reports given
dataset size, none self-report classifiers significantly outperform majority
class baseline.
Feature set comparison: EAR corpus investigated importance 4
feature sets: utterance type, LIWC, MRC, prosodic features. use Naive Bayes
models observer ratings perform best features. Interestingly,
Table 15 shows good classification accuracies extraversion come combination LIWC, MRC prosodic features, outperform baseline own,
dont well 73.0% accuracy full feature set. Moreover, extraversion
trait prosody seems make difference. LIWC features main
indicators emotional stability, although model features still performs better. MRC features important classifying conscientiousness (66.8%),
prosodic features produce best model openness experience 64.6% accuracy,
improving model features. Although utterance type features never outperform baseline own, lack significance could result small
1. Although equal size bins used, baseline accuracies differ 50% random
sampling cross-validation.

477

fiMairesse, Walker, Mehl & Moore

Data
Obs
Obs
Obs
Obs
Obs
Self
Self
Self
Self
Self

Trait Base J48
NN
NB
JRIP ADA
SMO
Extra 47.78 66.78 59.33 73.00 60.44
73.00 65.78
Emot 51.11 62.56 58.22 73.89 56.22
48.78
60.33
Agree 47.78 48.78 51.89 61.33 51.89
52.89
56.33
Consc 47.78 57.67 61.56 67.67 61.56
60.22 57.11
Open 47.78 52.22 46.78 57.00
49.67
50.56
55.89
Extra 47.78 48.78 49.67 57.33
50.56
54.44
49.89
Emot 51.11 45.56 46.78 50.44
46.78
41.89
44.33
Agree 52.22 47.89 50.89 58.33
56.89
55.22
52.33
Consc 51.11 33.44 45.56 39.33
43.11
46.11
53.22
Open 51.11 52.00 42.22 61.44
45.00
56.00
47.78
statistically significant improvement majority class
baseline (two-tailed paired t-test, p < .05)

Table 14: Classification accuracy two equal size bins EAR corpus, observer
ratings (Obs) self-reports (Self). Models majority class baseline (Base)1 ;
J48 decision tree (J48); Nearest neighbour (NN); Naive Bayes (NB); JRip rules
set (JRIP); AdaboostM1 (ADA); Support vector machines (SMO).

Feature set
None Type LIWC
MRC
Prosody
Set size
0
4
88
14
11
Extraversion
47.78
45.67
68.89
68.78
67.56
Emotional stability
51.11
60.22
69.89
60.78
61.78
Agreeableness
47.78
57.56
54.00
58.67
50.44
Conscientiousness
47.78
59.67
60.22
66.78
52.11
Openness experience 47.78
53.11
61.11
54.00
64.56
statistically significant improvement majority class
baseline (two-tailed paired t-test, p < .05)
Table 15: Classification accuracies EAR corpus observer reports using Naive
Bayes classifier, different feature sets (None=baseline, Type=utterance type).
Models performing better full feature set bold.

dataset size, since Section 3.3 showed utterance type features strongly correlate
several personality traits.
4.3 Qualitative Analysis
Decision trees rule-based models easily understood, therefore help
uncover new linguistic markers personality. models replicate previous findings,
link verbosity extraversion (c.f. Word count node Figure 2),
provide many new markers.
478

fiRecognising Personality Conversation Text

Word count
1284

> 1284
Extravert

Metaphysical issues
0.25

> 0.25

Commas
8.72

Articles
> 8.72

Eating

Extravert

3.51

> 3.51

Extravert

Space

0.51

> 0.51

3.22

Introvert

Sad

Extravert

> 3.22
Frequency use

0.15

> 0.15

6072

Introvert

Extravert

Extravert

> 6072
Introvert

Figure 2: J48 decision tree binary classification extraversion, based EAR corpus
observer reports.

#
1
2
3
4
5
6

Ordered rules
(School 1.47) (Motion 1.71) open
(Occup 2.49) (Sixltr 13.11) (School 1.9) (I 10.5) open
(Fam 600.335106) (Friends 0.67) open
(Nlet 3.502543) (Number 1.13) open
(School 0.98) (You 0) (AllPct 13.4) open
feature values Open

Table 16: JRip rule set binary classification openness experience, based
essays corpus.

model self-assessed openness experience detailed Table 16 shows students referring lot school work tend low scores trait (Rules 1, 2
5). expected, avoidance longer words indicative lack cre479

fiMairesse, Walker, Mehl & Moore

ativity/conventionality (Rules 4 5), well use high-familiarity words
references friends (Rule 3).
model observed extraversion Figure 2 shows word count important feature classifying trait observer. model suggests given
low verbosity, extraversion still manifest use words related metaphysical issues together articles, well use many commas.
association extraversion avoidance articles probably reflects use
pronouns common nouns confirms previous findings associating extraversion
implicit language (Heylighen & Dewaele, 2002).
Interestingly, decision tree trained essays corpus Figure 1 self-reported
extraversion differs lot observer model Figure 2. word count
important feature observers, doesnt seem marker self-assessed extraversion
(see Section 3.3), although number words per sentence used discriminate
subset data. hand, self-report model associates introversion
use articles, case observer model. sexual content
doesnt affect observer model, second important feature modelling selfreported extraversion. example, participants using many sex-related words modelled
introvert, unless avoid parentheses words related sadness.

5. Regression Results
trained regression models using corpora. baseline model returning
mean personality scores training set. use relative absolute error
evaluation, ratio models prediction error error produced
baseline. low relative error therefore indicates model performs better
constant mean baseline, 100% relative error implies performance equivalent
baseline. results averaged 10-fold cross-validation, significance
tests done using two-tailed paired t-test p < .05 level.
5.1 Essays Corpus
Regression results essays corpus self-reports Table 17. Paired t-tests
show emotional stability openness experience produce models significantly
improve baseline. classification task, openness experience
easiest trait model using essays: four regression models five outperform baseline. M5 model tree produces best result 93.3% relative error openness
experience (6.7% error decrease), 96.4% relative error emotional stability.
terms correlation model predictions actual ratings, model
emotional stability openness experience produce Pearsons correlation coefficients
0.24 0.33, respectively. Although magnitude improvement seems relatively
small, one needs keep mind difficulty regression task binary classification task: fine-grained personality recognition problem, requiring
association exact scalar value individual.
Feature set comparison: Table 18 provides results comparison LIWC
MRC feature sets using linear regression model, M5 model tree support
480

fiRecognising Personality Conversation Text

Trait
Base
LR
M5R
M5
REP
Extraversion
100.00 99.17
99.31
99.22
99.98
Emotional stability
100.00 96.87
99.75
96.43
99.35
Agreeableness
100.00 98.92
99.86
99.22
99.78
Conscientiousness
100.00 98.68
100.62
98.56
100.47
Openness experience 100.00 93.58
97.68 93.27
99.82
statistically significant improvement mean value
baseline (two-tailed paired t-test, p < .05)

SMO
100.65
98.35
100.28
99.30
94.19

Table 17: Relative error regression models trained essays corpus features.
Models mean value baseline (Base), linear regression (LR); M5 regression tree (M5R), M5 model tree linear models (M5), REPTree (REP)
Support vector machines regression (SMO).

vector machine algorithm regression (SMOreg). Overall, LIWC features perform better
MRC features except extraversion, linear regression model
MRC features produces better results full feature set. traits,
LIWC features perform better full feature set, almost always
significantly outperform baseline. model openness experience produces
lowest relative error, 6.50% improvement baseline.
Feature set
None
LIWC features
MRC features
Regression model
Base
LR
M5
SMO
LR
M5
SMO
Extraversion
100.00 99.39
99.25 100.8
98.79 98.79 99.13
Emotional stability
100.00 96.71 96.42 98.03
99.49
99.54
99.89
Agreeableness
100.00 98.50 98.52 99.52
99.75
99.81
99.31
Conscientiousness
100.00 98.23 98.14 99.46
99.23
99.23
99.16
Openness experience 100.00 93.50 93.70 94.14 97.44 97.44 97.26
statistically significant improvement mean value
baseline (two-tailed paired t-test, p < .05)
Table 18: Relative error regression models trained essays corpus MRC
LIWC feature sets only. Models linear regression (LR); M5 model tree
(M5); Support vector machines regression (SMO). Best models bold.

5.2 EAR Corpus
Regression results EAR corpus Table 19. paired t-test (two-tailed, p < .05)
cross-validation folds shows error reduction significant observed
extraversion (79.9% relative error, i.e. 20.1% error decrease), conscientiousness (14.3% improvement) emotional stability (13.3% improvement). extraversion easiest
trait model observer ratings, models agreeableness openness experience
dont outperform baseline.
481

fiMairesse, Walker, Mehl & Moore

terms correlation model predictions actual ratings, models
extraversion, emotional stability conscientiousness respectively produce Pearsons
correlation coefficients 0.54, 0.47 0.44, significantly outperforming baseline.
correlations relatively high, given average correlations ratings
pair observers 0.54 extraversion, 0.29 emotional stability 0.51
conscientiousness (18 observers, 31 33 data points pair).
Linear regression support vector machines perform poorly, suggesting
require bigger dataset essays corpus. classification task, self-reports
EAR corpus clearly difficult model: none models show significant
improvement baseline.

Data
Obs
Obs
Obs
Obs
Obs
Self
Self
Self
Self
Self

Trait
Base
LR M5R
M5
REP
Extraversion
100.00 179.16
82.16
80.15
79.94
Emotional stability
100.00 302.74
92.03
86.75 100.51
Agreeableness
100.00 242.68
96.73
111.16
99.37
Conscientiousness
100.00 188.18
82.68
90.85
98.08
Openness experience 100.00 333.65 101.64
119.53
102.76
Extraversion
100.00 204.96 104.50
118.44
99.94
Emotional stability
100.00 321.97 104.10
108.39
99.91
Agreeableness
100.00 349.87 106.90
110.84
101.64
Conscientiousness
100.00 177.12 103.39
120.29
107.33
Openness experience 100.00 413.70 107.12
122.68
126.31
statistically significant improvement mean value
baseline (two-tailed paired t-test, p < .05)

SMO
140.05
162.05
173.97
131.75
213.20
176.51
233.19
201.80
124.91
233.01

Table 19: Relative error regression models, observer ratings (Obs) self-reports
(Self) EAR corpus. Models mean value baseline (Base); linear
regression (LR); M5 regression tree (M5R); M5 model tree linear models
(M5); REPTree decision tree (REPT); Support vector machines regression
(SMO). relative error baseline model 100%.

Feature set comparison: trained regression models individual feature set
using observer reports, since self-reports didnt produce significant result using
features. focus three regression tree algorithms perform best
features. Table 20 shows LIWC good predictors observed extraversion,
REPTree outperforms model features 76.4% relative error
(23.6% improvement baseline). LIWC features produce best regression
model conscientiousness (82.1% relative error, 17.9% improvement). Surprisingly,
best model emotional stability contains prosodic features, 85.3% relative
error (14.7% improvement). finding suggests speech cues crucial
perception neuroticism, could explain Gill Oberlander (2003) reported
low correlation self-assessed observed emotional stability using text only.
482

fiRecognising Personality Conversation Text

classification task, utterance type features dont show significant improvement
own.
Set
Model
Extra
Emot
Agree
Consc
Open

Utterance type
LIWC features
MRC features
M5R
M5
REP
M5R
M5
REP
M5R
M5
REP
100.0
103.7
101.8
81.61
77.84
76.38
99.23
102.2
99.69
102.5
103.0
102.6
90.79
109.6
109.6
93.13
96.08
104.4
102.4
102.7
111.1
98.49
111.7
102.5
104.1
112.5
102.2
100.0
95.04
104.1
82.13
96.62
93.50
97.00
102.0
91.24
101.1
99.03
109.9
105.1
129.5
103.7
106.2
111.6
105.5
statistically significant improvement mean value
baseline (two-tailed paired t-test, p < .05)

Prosodic features
M5R
M5
REP
94.07
90.91
88.31
92.24
85.32
97.95
100.0
108.4
108.9
100.0
104.7
101.7
100.1
113.5
99.93

Table 20: Relative error regression models trained EAR corpus individual
feature sets. Models M5 regression tree (M5R); M5 model tree linear
models (M5); REPTree regression tree (REP). Best models bold.

5.3 Qualitative Analysis
Regression trees extraversion conscientiousness Figures 3 4. suggested
correlations Section 3.3, model Figure 3 shows voices pitch
variation intensity play important role modelling extraversion. high verbal
output perceived sign extraversion (see Word Count nodes), confirming previous
findings (Scherer, 1979). hand, low mean pitch combined constant
voice intensity characterises high introverts.
Figure 4 suggests conscientious people use fewer swear words content related
sexuality, preferring longer words. figure shows conscientious
people use fewer pronouns, i.e. explicit style, well words related
communication (e.g., talk share).

6. Ranking Results
Results corpora different feature sets Tables 21 22. models
trained 100 rounds boosting. baseline model ranks extracts randomly,
producing ranking loss 0.5 average (lower better). Results averaged
10-fold cross-validation, significance tests done using two-tailed paired t-test
p < .05 level.
6.1 Essays Corpus
Table 21 shows openness experience produces best ranking model
essays corpus, producing ranking loss 0.39 (lower better). Remarkably, trait
easiest model three recognition tasks corpus.
case conversational data, seems streams consciousness, generally
personal writings, likely exhibit cues relative authors openness experience.
Emotional stability produces second best model ranking loss 0.42, followed
conscientiousness extraversion, model agreeableness produces highest
483

fiMairesse, Walker, Mehl & Moore

Word count
675

> 675
Word count

Mean pitch
231

> 231

Intensity variation
6.39
2.86

3.23

1299

> 1299

3.83

4.24

> 6.39
3.02

Figure 3: M5 regression tree observed extraversion, computed using EAR corpus.
target output ranges 1 5.5, 5.5 means strongly extravert (the
highest value means observer ratings). mean pitch value
expressed Hertz, intensity variation (standard deviation) decibels.

ranking loss. models significantly outperform random ranking baseline,
actual improvement still relatively small.
Feature set
Base
LIWC MRC
Extraversion
0.50 0.44 0.44
0.46
Emotional stability
0.50 0.42 0.42
0.47
Agreeableness
0.50 0.46 0.46
0.48
Conscientiousness
0.50 0.44 0.44
0.47
Openness experience 0.50 0.39 0.39
0.44
statistically significant improvement
random ordering baseline
(two-tailed paired t-test, p < .05)
Table 21: Ranking loss essays corpus 10-fold cross-validation different
feature sets random ordering baseline (Base). Best models bold
(lower better).

Feature set comparison: evaluate features contribute ranking accuracy,
trained ranking model feature set. Table 21 clearly shows LIWC
features contributors model accuracy, inclusion MRC features
doesnt reduce ranking loss trait.
484

fiRecognising Personality Conversation Text

Swear words
0.93

> 0.93
Sexuality words

Pronouns
16.7
4.01

> 16.7
3.63

0.62
Comm. words

1.46
3.15

> 1.46
3.26

> 0.62
Syllables per word
> 1.14

1.14
2.90

Body states words

0.59
2.96

> 0.59
2.98

Figure 4: M5 regression tree observed conscientiousness, computed using EAR corpus. target output ranges 1 7, 7 means strongly conscientious
(Comm. words ratio words related communication).

6.2 EAR Corpus
Concerning EAR corpus, Table 22 reporting experiments using features, shows
models extraversion, agreeableness, conscientiousness, openness experience
better random ranking baseline. Emotional stability difficult trait
model, agreeableness conscientiousness produce best results, ranking
losses 0.31 0.33 respectively.
Feature set
None

LIWC MRC Type
Extraversion
0.50
0.35 0.36
0.45
0.55
Emotional stability
0.50
0.41
0.41
0.39
0.43
Agreeableness
0.50 0.31 0.32
0.44
0.45
Conscientiousness
0.50 0.33 0.36
0.41
0.44
Openness experience
0.50
0.38 0.37
0.41
0.49
statistically significant improvement random
ordering baseline (two-tailed paired t-test, p < .05)

Prosody
0.26
0.45
0.54
0.55
0.44

Table 22: Ranking loss EAR corpus observer reports1 10-fold crossvalidation different feature sets (None=baseline, Type=utterance type). Best
models bold (lower better).

485

fiMairesse, Walker, Mehl & Moore

Feature set comparison: looking individual feature sets, Table 22 shows
LIWC features perform significantly better baseline dimensions emotional stability, emotional stability best predicted MRC features (0.39
ranking loss). Interestingly, prosodic features good predictors extraversion,
lower ranking error full feature set (0.26). model produces best overall
result, 74% chance model detect extravert among two
unseen conversation extracts. previous recognition tasks, utterance type features
never significantly outperform baseline.
6.3 Qualitative Analysis
RankBoost rules indicate impact feature recognition personality
trait magnitude parameter associated feature. Tables 23
25 show rules impact best models, associated
values. feature labels Table 6. example, model extraversion Table 23
confirms previous findings associating trait longer conversations (Rule 5),
high speech rate (Rules 1 4) high pitch (Rules 2, 6 7) (Nass & Lee, 2001).
new markers emerge, high pitch variation introverts (Rules 15, 18
20), contradicting previous findings reported Scherer (1979).
Extraversion model prosodic features
# Positive rules

# Negative rules
1 Word-per-sec 0.73 1.43 11 Pitch-max 636.35
2 Pitch-mean 194.61 0.41 12 Pitch-slope 312.67
3 Voiced 647.35
0.41 13 Int-min 54.30
4 Word-per-sec 2.22 0.36 14 Word-per-sec 1.69
5 Voiced 442.95
0.31 15 Pitch-stddev 115.49
6 Pitch-max 599.88
0.30 16 Pitch-max 637.27
7 Pitch-mean 238.99 0.26 17 Pitch-slope 260.51
8 Int-stddev 6.96
0.24 18 Pitch-stddev 118.10
9 Int-max 85.87
0.24 19 Int-stddev 6.30
10 Voiced 132.35
0.23 20 Pitch-stddev 119.73


-0.05
-0.06
-0.06
-0.06
-0.06
-0.06
-0.12
-0.15
-0.18
-0.47

Table 23: Subset RankBoost model extraversion prosodic features only,
based EAR conversations observer reports. Rows 1-10 represent rules
producing highest score increase, rows 11-20 indicate evidence
end scale, i.e. introversion.

Concerning agreeableness, Rules 1 20 Table 24 suggest agreeable people
use longer words shorter sentences, Rules 2 4 show express
tentativity (with words maybe perhaps) positive emotions (e.g., happy good).
Anger swear words greatly reduce agreeableness score (Rules 12, 13, 18 19),
well use negations (Rule 15).
1. built models self-reports personality based EAR corpus, none significantly
outperforms baseline.

486

fiRecognising Personality Conversation Text

Agreeableness model features
# Positive rules

# Negative rules
1 Nphon 2.66
0.56 11 Fam 601.61
2 Tentat 2.83
0.50 12 Swear 0.41
3 Colon 0.03
0.41 13 Anger 0.92
4 Posemo 2.67 0.32 14 Time 3.71
5 Voiced 584
0.32 15 Negate 3.52
6 Relig 0.43
0.27 16 Fillers 0.54
7 Insight 2.09
0.25 17 Time 3.69
8 Prompt 0.06 0.25 18 Swear 0.61
9 Comma 4.60 0.23 19 Swear 0.45
10 Money 0.38
0.20 20 WPS 6.13


-0.16
-0.18
-0.19
-0.20
-0.20
-0.22
-0.23
-0.27
-0.27
-0.45

Table 24: Best RankBoost model based EAR conversations agreeableness. Rows
1-10 represent rules producing highest score increase, rows 11-20
indicate evidence end scale, i.e. disagreeableness.

Table 25 shows conscientious people talk lot work (Rule 1),
unconscientious people swear lot (Rules 19). Insight words (e.g., think know)
good indicator conscientiousness, well words expressing positive feelings
happy love (Rule 2 3). Interestingly, conscientious people modelled
high variation voice intensity (Rule 4). hand, Rule 20 shows
speaking loud produces opposite effect, well high pitch (Rule 13).
Long utterances indicative low conscientiousness (Rule 12).
rule sets presented contain extreme rules ranking models,
contain many additional personality cues arent identified typical
correlational analysis. example, high speech rate high mean pitch tend
contribute high extraversion ranking Table 23s model, dont correlate
significantly observer ratings, detailed Table 11. Similarly, positive emotion
words (Posemo) avoidance long utterances (WPS) indicate agreeableness
model Table 24, features dont correlate significantly agreeableness
ratings.

7. Related Work
knowledge, two studies automatic recognition personality. studies focused classification written texts based
self-reports, rather using continuous modelling techniques here.
Argamon et al. (2005) use essays corpus Pennebaker King (1999),
results directly comparable ours. work, use top-down approach
feature definition: feature set consists relative frequencies 675 function words
word categories based networks theory Systemic Functional Grammar. However,
simplify task removing middle third dataset, thereby potentially
increasing precision cost reducing recall maximum 67%. train SMO
models top third lower third essays corpus two personality traits
487

fiMairesse, Walker, Mehl & Moore

Conscientiousness model
# Positive rules

#
1 Occup 1.21
0.37 11
2 Insight 2.15
0.36 12
3 Posfeel 0.30
0.30 13
4 Int-stddev 7.83 0.29 14
5 Nlet 3.29
0.27 15
6 Comm 1.20
0.26 16
7 Nphon 2.66
0.25 17
8 Nphon 2.67
0.22 18
9 Nphon 2.76
0.20 19
10 K-F-nsamp 329 0.19 20

features
Negative rules
Swear 0.20
WPS 6.25
Pitch-mean 229
Othref 7.64
Humans 0.83
Swear 0.93
Swear 0.17
Relig 0.32
Swear 0.65
Int-max 86.84


-0.18
-0.19
-0.20
-0.20
-0.21
-0.21
-0.24
-0.27
-0.31
-0.50

Table 25: Best RankBoost model based EAR conversations conscientiousness. Rows
1-10 represent rules producing highest score increase, rows 11-20
indicate evidence end scale, i.e. unconscientiousness.

extraversion emotional stability, achieving accuracies subset data
58% traits.
believe likely personality recognition models need based full
range values useful practical application. Nevertheless, order
direct comparison, removed middle third essays dataset trained
SMO classifier LIWC features. obtain 57% classification accuracy extraversion 60% emotional stability, whereas algorithm applied
whole corpus, obtain accuracies 55% extraversion 57% emotional stability,
significantly outperforming baseline (see Table 12). Using EAR conversational data
observer reports, accuracies SMO models remain 65% extraversion
increase 63% emotional stability (see Table 14).
results suggest feature set combination Argamon et al.
could possibly improve performance, feature sets perform comparably. Using
features, Argamon et al. identify relative frequencies set function words
best predictor extraversion, suggesting refer norms certainty
salient. Concerning emotional stability, feature set characterising appraisal
produces far best results. Appraisal features relative frequencies positive
negative words well frequencies category Attitude network (e.g., affect,
appreciation, judgement, etc.). find neurotics tend use words related
negative appraisal affect, fewer appreciation appraisal words, suggesting
focus personal feelings.
Oberlander Nowson (2006) follow bottom-up feature discovery method training Naive Bayes SMO models four Big Five traits corpus personal
weblogs, using n-gram features extracted dataset. order able compare
Argamon et al., report experiments remove texts non-extreme
personality scores corpus, report experiments applying classification
algorithms seven different ways partitioning whole corpus classes, motivated
approximating continuous modelling approach. Although, results arent directly
488

fiRecognising Personality Conversation Text

comparable based different corpora, report results
use instances dataset, believe discarding test data increases
precision cost making recall unacceptably low.
building Naive Bayes models using frequent bi-grams tri-grams
computed full corpus, Oberlander Nowson (2006) find model agreeableness one outperforming baseline (54% accuracy, level significance
mentioned). keeping n-grams distinctive two extreme sets given
trait, accuracies range 65% extraversion 72% emotional stability. Finally,
applying automatic feature selection algorithm filtered set, accuracies increase range 83% emotional stability 93% agreeableness. testing
whether models generalise different corpus weblogs, Nowson Oberlander
(2007) report binary classification accuracies ranging 55% extraversion 65%
conscientiousness. Interestingly, models trained extreme instances
original corpus seem outperform models trained full corpus, although level
significance mentioned. studies show careful feature selection greatly improves
classification accuracy, n-grams appropriate model self-reports personality, although, Oberlander Nowson point out, features likely overfit.
would therefore interesting test future work whether feature sets used
generalise another dataset.
Oberlander Nowson (2006) report results 3-way 5-way classification,
order approximate finer-grained continuous personality ratings used psychology
(as scalar models present here). obtain maximum 44.7%
extraversion 5 bins, using raw n-grams (baseline 33.8%). results
directly comparable different corpus, different feature
sets. Moreover, provided results multiple classification experiments,
models cannot take account fact different classes part
total ordering, thus resulting models forced ignore importance
features correlate ordering across classes. believe regression
ranking models appropriate finer-grained personality recognition (see Sections
5 6).
evaluate claim, first mapped output best classifier ranking
compared RankBoost models. trained Naive Bayes classifier EAR
corpus observer reports features, using 5 equal size bins.2 test fold
10-fold cross-validation, computed ranking loss produced classifier based
ordering five classes. Results Table 26 show RankBoost significantly
outperforms classifier four traits five (p < .05), improvement close
significance emotional stability (p = 0.12).
RankBoosts goal minimise ranking loss, comparison likely
favour ranking models. Therefore, mapped output RankBoost models
5 classification bins see whether RankBoost could perform well classifier
classification task. divided output ranking 5 bins, containing 20%
slice contiguously ranked instances. Results Table 26 show Naive Bayes
classifier never outperforms RankBoost significantly, ranking model produces
2. Oberlander Nowson use unequal bins defined personality trait using standard deviation
mean, may easier task equal size bins.

489

fiMairesse, Walker, Mehl & Moore

Task
Ranking
Classification
Model
Base NB Rank Base NB Rank
Extraversion
0.50 0.48 0.35
20.0 32.3
32.1
Emotional stability
0.50 0.50 0.41
20.0 21.9
21.9
Agreeableness
0.50 0.50 0.31
20.0 28.4
37.8
Conscientiousness
0.50 0.46 0.33
20.0 34.7
30.3
Openness experience 0.50 0.53 0.38
20.0 19.8
26.8
statistically significant improvement
model (two-tailed t-test, p < .05)
Table 26: Comparison ranking (Rank) classification models (NB) personality ranking classification tasks (5 bins). Evaluation metrics ranking
loss (lower better) classification accuracy (higher better), respectively.
Results averaged 10-fold cross-validation.

better mean accuracy agreeableness (38%) openness experience (27%),
accuracy emotional stability (22%). sum, find ranking models perform
well classification better ranking compared best classifier, thus
modelling personality using continuous models accurate.

8. Discussion Future Work
show personality recognised computers language cues.3
recent work AI explores methods automatic detection types pragmatic
variation text conversation, opinion, emotion, deception, date,
know two studies besides automatic recognition user personality (Argamon et al., 2005; Mairesse & Walker, 2006a, 2006b; Oberlander & Nowson, 2006).
knowledge, results presented first demonstrate statistically significant
results texts recognise personality conversation (Mairesse & Walker, 2006a,
2006b). present first results applying regression ranking models order
model personality recognition using continuous scales traditional psychology.
systematically examine use different feature sets, suggested previous psycholinguistic research. Although features suggested psycholinguistic
literature, reported correlations personality ratings generally weak:
obvious would improve accuracies statistical models unseen subjects.
Computational work modelling personality primarily focused methods
expressing personality virtual agents tutorial systems, concepts related personality politeness, emotion, social intelligence (Walker, Cahn, & Whittaker,
1997; Andre, Klesen, Gebhard, Allen, & Rist, 1999; Lester, Towns, & FitzGerald, 1999;
Wang, Johnson, Mayer, Rizzo, Shaw, & Collins, 2005) inter alia. Studies shown
user evaluations agent personality depend users personality (Reeves & Nass,
1996; Cassell & Bickmore, 2003), suggesting ability model users personality
3. online demo personality recognition tool based models presented paper
downloaded www.dcs.shef.ac.uk/cogsys/recognition.html

490

fiRecognising Personality Conversation Text

required. Models present automatic recognition user personality
one way acquire user model (Chu-Carroll & Carberry, 1994; Thompson, Goker,
& Langley, 2004; Zukerman & Litman, 2001). plan test models user models
context adaptive dialogue system.
Table 27 summarises results personality traits recognition tasks
analysed. clearly emerges extraversion easiest trait model
spoken language, followed emotional stability conscientiousness. Concerning written language, models openness experience produce best results recognition
tasks. see feature selection important, best models
contain small subset full feature set. Prosodic features important modelling observed extraversion, emotional stability openness experience. MRC features
useful models emotional stability, LIWC features beneficial traits.
analysed qualitatively features influence specific models,
recognition tasks, well reporting correlations feature personality
traits Section 3.3.
Although parameters algorithms optimised, bottom
Table 27 seems indicate simple models Naive Bayes regression trees tend
outperform complex ones (e.g., support vector machines), confirming results
Oberlander Nowson (2006). However, experiments larger essays corpus
(more 2,400 texts) show support vector machines boosting algorithms produce higher classification accuracies. therefore likely algorithms would
perform better spoken data trained much larger corpus EAR
dataset, parameters optimised.
hypothesised models observed personality outperform models selfassessed personality. results suggest observed personality may easier
model self-reports, least conversational data. EAR corpus, find many
good results models observed personality, models self-assessed personality
never outperform baseline. may due objective observers using similar cues
models, self-reports personality may influenced factors
desirability trait (Edwards, 1953). Hogan (1982) introduced distinction
agents observers perspective personality assessment. agents
perspective conceptually taps persons identity (or personality inside),
observers perspective contrast taps persons reputation (or personality
outside). facets personality important psychological implications. persons
identity shapes way person experiences world. persons reputation, however,
psychologically less important: determines whether people get hired fired (e.g.,
reputation honesty), get married divorced, get adored stigmatised.
harder assess, observers perspective received comparatively little attention
psychology. Given everyday life people act observers peoples behaviours
time, external perspective naturally high theoretical importance
social relevance (Hogan, 1982).
Recent research exploring issue psychology based Brunswikian Lens
model (Brunswik, 1956), used extensively recent years explain
kernel truth social perception strangers. Use lens model personality
research reflects widely shared assumptions expression personality commu491

fiMairesse, Walker, Mehl & Moore

Task
Baseline

Classification
n/a
none
50%

n/a

Regression
none
0%

n/a

Ranking
none

0.50

1%
4%
2%
2%
7%

Rank
Rank
Rank
Rank
Rank

LIWC
LIWC
LIWC
LIWC
LIWC

0.44
0.42
0.46
0.44
0.39

24%
15%
3%
18%
1%

Rank
Rank
Rank
Rank
Rank

prosody
MRC


LIWC

0.26
0.39
0.31
0.33
0.37

Self-report models trained written data (essays):
Extraversion
Emotional stability
Agreeableness
Conscientiousness
Openness experience

ADA
SMO
SMO
SMO
SMO

LIWC
LIWC
LIWC
LIWC
LIWC

56%
58%
56%
56%
63%

LR
M5
LR
M5
M5

MRC
LIWC
LIWC
LIWC


Observer report models trained spoken data (EAR):
Extraversion
Emotional stability
Agreeableness
Conscientiousness
Openness experience

NB
NB
NB
NB
NB





prosody

73%
74%
61%
68%
65%

REP
M5
M5R
M5R
M5

LIWC
prosody
all*
LIWC
type*

Table 27: Comparison best models trait, three recognition tasks.
table entry contains algorithm, feature set, model performance.
See Sections 3.2 3.4 details. Depending task, evaluation metric either (1) classification accuracy; (2) percentage improvement
regression baseline; (3) ranking loss. Asterisks indicate results arent
significant p < .05 level.

nicatively functional, i.e. (a) latent attributes persons expressed via observable
cues; (b) observers rely observable cues infer latent attributes others; (c) observers use appropriate cues is, implicit assumptions relations
observable cues latent attributes extent accurate. model
useful identifying observable cues mediate convergences judgments
latent attributes direct measures attributes (Scherer, 2003; Heinrich &
Borkenau, 1998).
discrepancies markers self-assessed observed personality,
another issue identification appropriate model given specific application.
gold standard approximated either observer self-reports, however
likely specific trait one type report closer true personality.
hypothesis remains tested traits high visibility (e.g., extraversion)
accurately assessed using observer reports, tend yield higher interjudge agreement (Funder, 1995), low visibility traits (e.g., emotional stability)
better assessed oneself. personality recogniser aiming estimate true personality
would therefore switch observer models self-report models, depending
trait assessment.
Beyond practical applications personality recognition models, work
attempt explore different ways looking relation personality language. looked various personality recognition tasks, applied different learning
492

fiRecognising Personality Conversation Text

methods Section 3.4. tasks vary complexity: ranking model directly
derived regression model, classification model derived either
ranking regression model. type model closer actual relation
language, generally behaviour, personality? personality vary continuously, clusters people similar trait combinations? relation
continuous, classification algorithms never able produce accurate models
two classes, dont take account ordering classes.
ranking models outperform classifiers (see Section 7), given wide range individual
differences reflected literature Big Five (Allport & Odbert, 1936; Norman,
1963; Goldberg, 1990), believe personality varies continuously among members
population, suggesting regression ranking models accurate
long run. hypothesis supported recent work medical research showing
antisocial personality disorder varies continuously (Marcus, Lilienfeld, Edens, & Poythress,
2006). Regression provides detailed model output variables, depending whether absolute differences personality scores meaningful,
relative orderings people matter, ranking may appropriate. Additional
models could tried ranking task, support vector algorithms ordinal regression (Herbrich, Graepel, & Obermayer, 2000). Moreover, future work
assess whether optimising parameters learning algorithms improves performance.
future work, would improve models examine well
perform across dialogue domains. clear whether accuracies high enough
useful. Applications involving speech recognition introduce noise features
except prosodic features, probably reducing model accuracy, since EAR
corpus relatively small, expect training data would improve performance.
Additionally, believe inclusion gender feature would produce better
models, actual language correlates perceived personality shown depend
gender speaker (Mehl et al., 2006). believe future work
investigate combination individual features trait-dependent way. Another issue
poor performance utterance type featuressince significant correlation
results features Section 3.3, unclear features useful
statistical models. could possibly arise small size datasets,
relatively low accuracy hand-crafted automatic tagger, compared
work using supervised learning methods (Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky,
Taylor, Martin, Ess-Dykema, & Meteer, 2000; Webb, Hepple, & Wilks, 2005).
begun test models spoken language generator (Mairesse &
Walker, 2007). future work, plan compare utility models trained out-ofdomain corpora, here, methods training models, terms
utility automatic adaptation output generation dialogue systems.

Acknowledgments
would thank James Pennebaker giving us access essays data.
work partially funded Royal Society Wolfson Research Merit Award Marilyn
Walker, Vice Chancellors studentship Francois Mairesse.
493

fiMairesse, Walker, Mehl & Moore

References
Allport, G. W., & Odbert, H. S. (1936). Trait names: psycho-lexical study. Psychological
Monographs, 47 (1, Whole No. 211), 171220.
Andre, E., Klesen, M., Gebhard, P., Allen, S., & Rist, T. (1999). Integrating models
personality emotions lifelike characters. Proceedings Workshop
Affect Interactions - Towards new Generation Interfaces, pp. 136149.
Argamon, S., Dhawle, S., Koppel, M., & Pennebaker, J. (2005). Lexical predictors
personality type. Proceedings Joint Annual Meeting Interface
Classification Society North America.
Biber, D. (1988). Variation across Speech Writing. Cambridge University Press.
Boersma, P. (2001). Praat, system phonetics computer. Glot International,
5 (9/10), 341345.
Bono, J. E., & Judge, T. A. (2004). Personality transformational transactional
leadership: meta-analysis. Journal Applied Psychology, 89 (5), 901910.
Breck, E., Choi, Y., & Cardie, C. (2007). Identifying expressions opinion context.
Twentieth International Joint Conference Artificial Intelligence (IJCAI).
Brunswik, E. (1956). Perception Representative Design Psychological Experiments. University California Press, Berkeley, CA.
Byrne, D., & Nelson, D. (1965). Attraction linear function proportion positive
reinforcements. Journal Personality Social Psychology, 1, 659663.
Cassell, J., & Bickmore, T. (2003). Negotiated collusion: Modeling social language
relationship effects intelligent agents. User Modeling User-Adapted Interaction,
13, 89132.
Chu-Carroll, J., & Carberry, S. (1994). plan-based model response generation
collaborative task-oriented dialogue. Proceedings 12th National Conference
Artificial Intelligence (AAAI), pp. 799805.
Coltheart, M. (1981). MRC psycholinguistic database. Quarterly Journal Experimental Psychology, 33A, 497505.
Costa, P. T., & McCrae, R. R. (1992). NEO PI-R Professional Manual. Psychological
Assessment Resources, Odessa, FL.
Dewaele, J.-M., & Furnham, A. (1999). Extraversion: unloved variable applied
linguistic research. Language Learning, 49 (3), 509544.
Donnellan, M. B., Conger, R. D., & Bryant, C. M. (2004). Big Five enduring
marriages. Journal Research Personality, 38, 481504.
494

fiRecognising Personality Conversation Text

Edwards, A. L. (1953). relationship judged desirability trait
probability endorsed. Journal Applied Psychology, 37, 9093.
Enos, F., Benus, S., Cautin, R., Graciarena, M., Hirschberg, J., & Shriberg, E. (2006).
Personality factors human deception detection: Comparing human machine
performance. Proceedings ICSLP.
Eysenck, H. J. (1991). Dimensions personality: 16, 5 3? criteria taxonomic
paradigm. Personality Individual Differences, 12 (8), 773790.
Fast, L. A., & Funder, D. C. (2007). Personality manifest word use: Correlations
self-report, acquaintance-report, behavior. Journal Personality Social
Psychology, press.
Freund, Y., Iyer, R., Schapire, R. E., & Singer, Y. (1998). efficient boosting algorithm
combining preferences. Proceedings 15th International Conference
Machine Learning, pp. 170178.
Funder, D. C. (1995). accuracy personality judgment: realistic approach.
Psychological Review, 102, 652670.
Funder, D. C., & Sneed, C. D. (1993). Behavioral manifestations personality: ecological approach judgmental accuracy. Journal Personality Social Psychology,
64 (3), 479490.
Furnham, A. (1990). Language personality. Giles, H., & Robinson, W. (Eds.),
Handbook Language Social Psychology. Winley.
Furnham, A., Jackson, C. J., & Miller, T. (1999). Personality, learning style work
performance. Personality Individual Differences, 27, 11131122.
Furnham, A., & Mitchell, J. (1991). Personality, needs, social skills academic achievement: longitudinal study. Personality Individual Differences, 12, 10671073.
Gill, A., & Oberlander, J. (2003). Perception e-mail personality zero-acquaintance:
Extraversion takes care itself; neuroticism worry. Proceedings 25th
Annual Conference Cognitive Science Society, pp. 456461.
Gill, A. (2003). Personality Language: Projection Perception Personality
Computer-Mediated Communication. Ph.D. thesis, University Edinburgh.
Gill, A. J., & Oberlander, J. (2002). Taking care linguistic features extraversion.
Proceedings 24th Annual Conference Cognitive Science Society, pp.
363368.
Goldberg, L. R. (1990). alternative description personality: Big-Five factor
structure. Journal Personality Social Psychology, 59, 12161229.
Graciarena, M., Shriberg, E., Stolcke, A., Enos, F., Hirschberg, J., & Kajarekar, S. (2006).
Combining prosodic, lexical cepstral systems deceptive speech detection.
Proceedings IEEE ICASSP.
495

fiMairesse, Walker, Mehl & Moore

Heinrich, C. U., & Borkenau, P. (1998). Deception deception detection: role
cross-modal inconsistency. Journal Personality, 66 (5), 687712.
Herbrich, R., Graepel, T., & Obermayer, K. (2000). Large margin rank boundaries
ordinal regression. Smola, A. J., Bartlett, P., Scholkopf, B., & Schuurmans, D.
(Eds.), Advances Large Margin Classifiers, pp. 115132. MIT Press, Cambridge,
MA.
Heylighen, F., & Dewaele, J.-M. (2002). Variation contextuality language:
empirical measure. Context Context, Special issue Foundations Science, 7 (3),
293340.
Hirschberg, J., Benus, S., Brenier, J. M., Enos, F., Friedman, S., Gilman, S., Girand,
C., Graciarena, M., Kathol, A., Michaelis, L., Pellom, B., Shriberg, E., & Stolcke,
A. (2005). Distinguishing deceptive non-deceptive speech. Proceedings
Interspeech2005 - Eurospeech.
Hogan, R. (1982). socioanalytic theory personality. Nebraska Symposium Motivation,
30, 5589.
Hogan, R., Curphy, G. J., & Hogan, J. (1994). know leadership: Effectiveness personality. American Psychologist, 49 (6), 493504.
John, O. P., Donahue, E. M., & Kentle, R. L. (1991). Big Five Inventory: Versions
4a 5b. Tech. rep., Berkeley: University California, Institute Personality
Social Research.
John, O. P., & Srivastava, S. (1999). Big Five trait taxonomy: History, measurement,
theoretical perspectives. Pervin, L. A., & John, O. P. (Eds.), Handbook
personality theory research. New York: Guilford Press.
Komarraju, M., & Karau, S. J. (2005). relationship Big Five personality
traits academic motivation. Personality Individual Differences, 39, 557567.
Lester, J. C., Towns, S. G., & FitzGerald, P. J. (1999). Achieving affective impact: Visual
emotive communication lifelike pedagogical agents. International Journal
Artificial Intelligence Education, 10 (3-4), 278291.
Liscombe, J., Venditti, J., & Hirschberg, J. (2003). Classifying subject ratings emotional
speech using acoustic features. Proceedings Interspeech2003 - Eurospeech.
Mairesse, F., & Walker, M. (2006a). Automatic recognition personality conversation.
Proceedings HLT-NAACL.
Mairesse, F., & Walker, M. (2006b). Words mark nerds: Computational models personality recognition language. Proceedings 28th Annual Conference
Cognitive Science Society, pp. 543548.
Mairesse, F., & Walker, M. (2007). PERSONAGE: Personality generation dialogue.
Proceedings 45th Annual Meeting Association Computational Linguistics (ACL), pp. 496503.
496

fiRecognising Personality Conversation Text

Mallory, P., & Miller, V. (1958). possible basis association voice characteristics
personality traits. Speech Monograph, 25, 255260.
Marcus, D. K., Lilienfeld, S. O., Edens, J. F., & Poythress, N. G. (2006). antisocial
personality disorder continuous categorical? taxometric analysis. Psychological
Medicine, 36 (11), 15711582.
McLarney-Vesotski, A. R., Bernieri, F., & Rempala, D. (2006). Personality perception:
developmental study. Journal Research Personality, 40 (5), 652674.
Mehl, M. R., Gosling, S. D., & Pennebaker, J. W. (2006). Personality natural habitat: Manifestations implicit folk theories personality daily life. Journal
Personality Social Psychology, 90, 862877.
Mehl, M., Pennebaker, J., Crow, M., Dabbs, J., & Price, J. (2001). Electronically
Activated Recorder (EAR): device sampling naturalistic daily activities
conversations. Behavior Research Methods, Instruments, Computers, 33, 517
523.
Mishne, G. (2005). Experiments mood classification blog posts. Proceedings
ACM SIGIR 2005 Workshop Stylistic Analysis Text Information Access.
Nass, C., & Lee, K. (2001). computer-synthesized speech manifest personality? experimental tests recognition, similarity-attraction, consistency-attraction. Journal
Experimental Psychology: Applied, 7 (3), 171181.
Newman, M. L., Pennebaker, J. W., Berry, D. S., & Richards, J. M. (2003). Lying words:
Predicting deception linguistic style. Personality Social Psychology Bulletin,
29, 665675.
Norman, W. T. (1963). Toward adequate taxonomy personality attributes: Replicated
factor structure peer nomination personality rating. Journal Abnormal Social
Psychology, 66, 574583.
Nowson, S., & Oberlander, J. (2007). Identifying bloggers: Towards large scale personality classification personal weblogs. Proceedings International Conference
Weblogs Social Media.
Nunn, S. (2005). Preventing next terrorist attack: theory practice homeland security information systems. Journal Homeland Security Emergency
Management, 2 (3).
Oberlander, J., & Gill, A. J. (2006). Language character: stratified corpus comparison individual differences e-mail communication. Discourse Processes, 42,
239270.
Oberlander, J., & Nowson, S. (2006). Whose thumb anyway? classifying author personality weblog text. Proceedings 44th Annual Meeting Association
Computational Linguistics (ACL).
497

fiMairesse, Walker, Mehl & Moore

Oudeyer, P.-Y. (2002). Novel useful features algorithms recognition emotions
speech. Proceedings 1st International Conference Speech Prosody, pp.
547550.
Pang, B., & Lee, L. (2005). Seeing stars: Exploiting class relationships sentiment
categorization respect rating scales. Proceedings 43rd Annual Meeting
Association Computational Linguistics (ACL), pp. 115124.
Paunonen, S. V., & Jackson, D. N. (2000). beyond Big Five? plenty!. Journal
Personality, 68 (5), 821836.
Peabody, D., & Goldberg, L. R. (1989). determinants factor structures
personality-trait descriptor. Journal Personality Social Psychology, 57 (3),
552567.
Pennebaker, J. W., Francis, M. E., & Booth, R. J. (2001). Inquiry Word Count: LIWC
2001. Lawrence Erlbaum, Mahwah, NJ.
Pennebaker, J. W., & King, L. A. (1999). Linguistic styles: Language use individual
difference. Journal Personality Social Psychology, 77, 12961312.
Pennebaker, J. W., Mehl, M., & Niederhoffer, K. (2003). Psychological aspects natural
language use: words, selves. Annual Review Psychology, 54, 547577.
Popescu, A., & Etzioni, O. (2005). Extracting product features opinions reviews.
Proceedings HTL-EMNLP.
Reeves, B., & Nass, C. (1996). Media Equation. University Chicago Press.
Reiter, E., & Sripada, S. G. (2004). Contextual influences near-synonym choice.
Proceedings International Natural Language Generation Conference, pp. 161
170.
Rienks, R., & Heylen, D. (2006). Dominance detection meetings using easily obtainable
features. Bourlard, H., & Renals, S. (Eds.), Revised Selected Papers 2nd Joint
Workshop Multimodal Interaction Related Machine Learning Algorithms, Vol.
3869 Lecture Notes Computer Science. Springer Verlag.
Riggio, R. E., Salinas, C., & Tucker, J. (1988). Personality deception ability. Personality
Individual Differences, 9 (1), 189191.
Rosenberg, A., & Hirschberg, J. (2005). Acoustic/prosodic lexical correlates charismatic speech. Proceedings Interspeech2005 - Eurospeech.
Rushton, J. P., Murray, H. G., & Erdle, S. (1987). Combining trait consistency learning specificity approaches personality, illustrative data faculty teaching
performance. Personality Individual Differences, 8, 5966.
Schapire, R. (1999). brief introduction boosting. Proceedings Sixteenth International Joint Conference Artificial Intelligence, 2, 14011406.
498

fiRecognising Personality Conversation Text

Scherer, K. R. (1979). Personality markers speech. Scherer, K. R., & Giles, H. (Eds.),
Social markers speech, pp. 147209. Cambridge University Press.
Scherer, K. R. (2003). Vocal communication emotion: review research paradigms.
Speech Communication, 40 (1-2), 227256.
Siegman, A., & Pope, B. (1965). Personality variables associated productivity
verbal fluency initial interview. Proceedings 73rd Annual Conference
American Psychological Association.
Sigurdsson, J. F. (1991). Computer experience, attitudes toward computers personality
characteristics psychology undergraduates. Personality Individual Differences,
12 (6), 617624.
Smith, B. L., Brown, B. L., Strong, W. J., & Rencher, A. C. (1975). Effects speech rate
personality perception. Language Speech, 18, 145152.
Somasundaran, S., Ruppenhofer, J., & Wiebe, J. (2007). Detecting arguing sentiment
meetings. Proceedings 8th SIGdial Workshop Discourse Dialogue.
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin,
R., Ess-Dykema, C. V., & Meteer, M. (2000). Dialogue act modeling automatic
tagging recognition conversational speech. Computational Linguistics, 26 (3),
339371.
Stoyanov, V., Cardie, C., & Wiebe, J. (2005). Multi-perspective question answering using
OpQA corpus. Proceedings HLT-EMNLP.
Thompson, C. A., Goker, M. H., & Langley, P. (2004). personalized system conversational recommendations. Journal Artificial Intelligence Research, 21, 393428.
Tucker, S., & Whittaker, S. (2004). Accessing multimodal meeting data: Systems, problems possibilities. Lecture Notes Computer Science, Machine Learning
Multimodal Interaction, 3361, 111.
Turney, P. D. (2002). Thumbs thumbs down? Semantic orientation applied unspervised classification reviews. Proceedings 40th Annual Meeting
Association Computational Linguistics (ACL), pp. 417424.
Vogel, K., & Vogel, S. (1986). Linterlangue et la personalite de lapprenant. International
Journal Applied Linguistics, 24 (1), 4868.
Walker, M., Cahn, J. E., & Whittaker, S. J. (1997). Improvising linguistic style: Social
affective bases agent personality. Proceedings 1st Conference
Autonomous Agents, pp. 96105.
Walker, M., & Whittaker, S. (1990). Mixed initiative dialogue: investigation
discourse segmentation. Proceedings 28th Annual Meeting Association
Computational Linguistics (ACL), pp. 7078.
499

fiMairesse, Walker, Mehl & Moore

Wang, N., Johnson, W. L., Mayer, R. E., Rizzo, P., Shaw, E., & Collins, H. (2005).
politeness effect: Pedagogical agents learning gains. Frontiers Artificial Intelligence Applications, 125, 686693.
Watson, D., & Clark, L. A. (1992). traits temperament: General specific
factors emotional experience relation five factor model. Journal
Personality, 60 (2), 44176.
Webb, N., Hepple, M., & Wilks, Y. (2005). Error analysis dialogue act classification.
Proceedings 8th International Conference Text, Speech Dialogue.
Wiebe, J., & Riloff, E. (2005). Creating subjective objective sentence classifiers
unannotated texts. Proceedings 6th International Conference Intelligent
Text Processing Computational Linguistics.
Wiebe, J., Wilson, T., Bruce, R., Bell, M., & Martin, M. (2004). Learning subjective
language. Computational Linguistic, 30 (3), 277308.
Wilson, T., Wiebe, J., & Hwa, R. (2004). mad you? finding strong
weak opinion clauses. Proceedings 19th National Conference Artificial
Intelligence (AAAI), pp. 761769.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools
techniques. Morgan Kaufmann, San Francisco, CA.
Zukerman, I., & Litman, D. (2001). Natural language processing user modeling: Synergies limitations. User Modeling User-Adapted Interaction, 11 (1-2), 129158.

500


