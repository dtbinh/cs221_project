Journal Artificial Intelligence Research 25 (2006) 75-118

Submitted 01/05; published 1/06

Approximate Policy Iteration Policy Language Bias:
Solving Relational Markov Decision Processes
Alan Fern

afern@cs.orst.edu
School Electrical Engineering Computer Science, Oregon State University
Sungwook Yoon
sy@purdue.edu
Robert Givan
givan@purdue.edu
School Electrical Computer Engineering, Purdue University

Abstract
study approach policy selection large relational Markov Decision Processes
(MDPs). consider variant approximate policy iteration (API) replaces
usual value-function learning step learning step policy space. advantageous
domains good policies easier represent learn corresponding
value functions, often case relational MDPs interested in.
order apply API problems, introduce relational policy language
corresponding learner. addition, introduce new bootstrapping routine goalbased planning domains, based random walks. bootstrapping necessary
many large relational MDPs, reward extremely sparse, API ineffective
domains initialized uninformed policy. experiments show
resulting system able find good policies number classical planning domains
stochastic variants solving extremely large relational MDPs.
experiments point limitations approach, suggesting future work.

1. Introduction
Many planning domains naturally represented terms objects relations
among them. Accordingly, AI researchers long studied algorithms planning
learning-to-plan relational state action spaces. include, example, classical
STRIPS domains blocks world logistics.
common criticism domains algorithms assumption idealized,
deterministic world model. This, part, led AI researchers study planning
learning within decision-theoretic framework, explicitly handles stochastic environments generalized reward-based objectives. However, work based
explicit propositional state-space models, far demonstrated scalability
large relational domains commonly addressed classical planning.
Intelligent agents must able simultaneously deal complexity arising
relational structure complexity arising uncertainty. primary goal
research move toward agents bridging gap classical
decision-theoretic techniques.
paper, describe straightforward practical method solving large,
relational MDPs. work viewed form relational reinforcement learning
(RRL) assume strong simulation model environment. is, assume
access black-box simulator, provide (relationally represented)
c
2006
AI Access Foundation. rights reserved.

fiFern, Yoon, & Givan

state/action pair receive sample appropriate next-state reward distributions. goal interact simulator order learn policy achieving high
expected reward. separate challenge, considered here, combine work
methods learning environment simulator avoid dependence provided
simulator.
Dynamic-programming approaches finding optimal control policies MDPs (Bellman, 1957; Howard, 1960), using explicit (flat) state space representations, break
state space becomes extremely large. recent work extends algorithms
use propositional (Boutilier & Dearden, 1996; Dean & Givan, 1997; Dean, Givan, &
Leach, 1997; Boutilier, Dearden, & Goldszmidt, 2000; Givan, Dean, & Greig, 2003; Guestrin,
Koller, Parr, & Venkataraman, 2003b) well relational (Boutilier, Reiter, & Price, 2001;
Guestrin, Koller, Gearhart, & Kanodia, 2003a) state-space representations. extensions significantly expanded set approachable problems, yet shown
capacity solve large classical planning problems benchmark problems
used planning competitions (Bacchus, 2001), let alone stochastic variants. One possible reason methods based calculating representing value
functions. familiar STRIPS planning domains (among others), useful value functions
difficult represent compactly, manipulation becomes bottle-neck.
techniques purely deductivethat is, value function guaranteed certain level accuracy. Rather, work, focus inductive
techniques make guarantees practice. existing inductive forms approximate policy iteration (API) utilize machine learning select compactly represented
approximate value functions iteration dynamic programming (Bertsekas & Tsitsiklis, 1996). machine learning algorithm, selection hypothesis space,
space value functions, critical performance. example space used frequently
space linear combinations human-selected feature set.
knowledge, previous work applies form API
benchmark problems classical planning, stochastic variants.1 Again, one
reason high complexity typical value functions large relational
domains, making difficult specify good value-function spaces facilitate learning.
Comparably, often much easier compactly specify good policies, accordingly
good policy spaces learning. observation basis recent work inductive policy selection relational planning domains, deterministic (Khardon, 1999a;
Martin & Geffner, 2000), probabilistic (Yoon, Fern, & Givan, 2002). techniques
show useful policies learned using policy-space bias described generic
(relational) knowledge representation language. incorporate ideas variant API, achieves significant success without representing learning approximate
value functions. course, natural direction future work combine policy-space
techniques value-function techniques, leverage advantages both.
Given initial policy, approach uses simulation technique policy rollout
(Tesauro & Galperin, 1996) generate trajectories improved policy. trajectories given classification learner, searches classifier, policy,
matches trajectory data, resulting approximately improved policy. two
1. Recent work relational reinforcement learning applied STRIPS problems much simpler
goals typical benchmark planning domains, discussed Section 8.

76

fiAPI Policy Language Bias

steps iterated improvement observed. resulting algorithm
viewed form API iteration carried without inducing approximate
value functions.
avoiding value function learning, algorithm helps address representational
challenge applying API relational planning domains. However, another fundamental
challenge that, non-trivial relational domains, API requires form bootstrapping. particular, STRIPS planning domains reward, corresponds
achieving goal condition, sparsely distributed unlikely reached random exploration. Thus, initializing API random uninformed policy, likely result
reward signal hence guidance policy improvement. One approach bootstrapping rely user provide good initial policy heuristic gives guidance
toward achieving reward. Rather, work develop new automatic bootstrapping
approach goal-based planning domains, require user intervention.
bootstrapping approach based idea random-walk problem distributions.
given planning domain, blocks world, distribution randomly generates
problem (i.e., initial state goal) selecting random initial state
executing sequence n random actions, taking goal condition subset
properties resulting state. problem difficulty typically increases n,
small n (short random walks) even random policies uncover reward. Intuitively,
good policy problems walk length n used bootstrap API problems
slightly longer walk lengths. bootstrapping approach iterates idea, starting
random policy small n, gradually increasing walk length
learn policy long random walks. long-random-walk policies clearly capture
much domain knowledge, used various ways. Here, show empirically
policies often perform well problem distributions relational domains used
recent deterministic probabilistic planning competitions.
implementation bootstrapped API approach took second place 3 competitors hand-tailored track 2004 International Probabilistic Planning Competition.2 knowledge first machine-learning based system entered
planning competition, either deterministic probabilistic.
Here, give evaluation system number probabilistic deterministic
relational planning domains, including AIPS-2000 competition benchmarks, benchmarks hand-tailored track 2004 Probabilistic Planning Competition.
results show system often able learn policies domains perform
well long-random-walk problems. addition, policies often perform well
planning-competition problem distributions, comparing favorably state-of-theart planner deterministic domains. experiments highlight number
limitations current system, point interesting directions future work.
remainder paper proceeds follows. Section 2, introduce problem
setup then, Section 3, present new variant API. Section 4, provide
2. Note, however, approach hand-tailored. Rather, given domain definition, system
learns policy offline, automatically, applied problem domain.
entered hand-tailored track track facilitated use offline learning,
providing domains problem generators competition. entrants humanwritten domain.

77

fiFern, Yoon, & Givan

technical analysis algorithm, giving performance bounds policy-improvement
step. Sections 5 6, describe implemented instantiation API approach
relational planning domains. includes description generic policy language
relational domains, classification learner language, novel bootstrapping
technique goal-based domains. Section 7 presents empirical results, finally
Sections 8 9 discuss related work future directions.

2. Problem Setup
formulate work framework Markov Decision Processes (MDPs).
primary motivation develop algorithms relational planning domains, first
describe problem setup approach general, action-simulatorbased MDP representation. Later, Section 5, describe particular representation planning domains
relational MDPs corresponding relational instantiation approach.
Following adapting Kearns, Mansour, Ng (2002) Bertsekas Tsitsiklis
(1996), represent MDP using generative model hS, A, T, R, Ii, finite
set states, finite, ordered set actions, randomized action-simulation
algorithm that, given state action a, returns next state s0 according unknown
probability distribution PT (s0 |s, a). component R reward function maps
real-numbers, R(s, a) representing reward taking action state s,
randomized initial-state algorithm inputs returns state according
unknown distribution P0 (s). sometimes treat (s, a) random variables
distributions P0 () PT (|s, a) respectively.
MDP = hS, A, T, R, Ii, policy (possibly stochastic) mapping
A. value function , denoted V (s), represents expected, cumulative, discounted
reward following policy starting state s, unique solution
V (s) = E[R(s, (s)) + V (T (s, (s)))]

(1)

0 < 1 discount factor. Q-value function Q (s, a) represents
expected, cumulative, discounted reward taking action state following ,
given
Q (s, a) = R(s, a) + E[V (T (s, a))]

(2)

measure quality policy objective function V () = E[V (I)], giving
expected value obtained policy starting randomly drawn initial
state. common objective MDP planning reinforcement learning find
optimal policy = argmax V (). However, automated technique, including one
present here, date able guarantee finding optimal policy relational
planning domains consider, reasonable running time.
well known fact given current policy , define new improved
policy
PI (s) = argmaxaA Q (s, a)

(3)

value function PI guaranteed 1) worse
state s, 2) strictly improve state optimal. Policy iteration
78

fiAPI Policy Language Bias

algorithm computing optimal policies iterating policy improvement (PI)
initial policy reach fixed point, guaranteed optimal policy.
iteration policy improvement involves two steps: 1) Policy evaluation compute
value function V current policy , 2) Policy selection, where, given V
step 1, select action maximizes Q (s, a) state, defining new improved
policy.
Finite Horizons. Since API variant based simulation, must bound
simulation trajectories horizon h, technical analysis Section 4 use notion
finite-horizon discounted reward. h-horizon value function Vh recursively defined

V0 (s) = 0,

Vh (s) = E[R(s, (s)) + Vh1 (T (s, (s)))]

(4)

giving expected discounted reward obtained following h steps s.
(T (s, a))], h-horizon
define h-horizon Q-function Qh (s, a) = R(s, a) + E[Vh1

objective function V h () = E[Vh (I)]. well known, effect using finite
horizon made arbitrarily small. particular, states
actions a, approximation error decreases exponentially h,
|V (s) Vh (s)| h Vmax ,
|Q (s, a) Qh (s, a)| h Vmax ,
Rmax
Vmax =
,
1
Rmax maximum absolute value reward action state.
get |V h () V ()| h Vmax .

3. Approximate Policy Iteration Policy Language Bias
Exact solution techniques, policy iteration, typically intractable large statespace MDPs, arising relational planning domains. section,
introduce new variant approximate policy iteration (API) intended domains.
First, review generic form API used prior work, based learning approximate
value functions. Next, motivated fact value functions often difficult learn
relational domains, describe API variant, avoids learning value functions
instead learns policies directly state-action mappings.
3.1 API Approximate Value Functions
API, described Bertsekas Tsitsiklis (1996), uses combination Monte-Carlo
simulation inductive machine learning heuristically approximate policy iteration
large state-space MDPs. Given current policy , iteration API approximates
policy evaluation policy selection, resulting approximately improved policy .
First, policy-evaluation step constructs training set samples V small
representative set states. sample computed using simulation, estimating V (s)
policy state drawing number sample trajectories starting
79

fiFern, Yoon, & Givan

averaging cumulative, discounted reward along trajectories. Next,
policy-selection step uses function approximator (e.g., neural network) learn
approximation V V based training data. V serves representation
, selects actions using sampled one-step lookahead based V ,
(s) = arg max R(s, a) + E[V (T (s, a))].
aA

common variant procedure learns approximation Q rather V .
API exploits function approximators generalization ability avoid evaluating
state state space, instead directly evaluating small number training states.
Thus, use API assumes states perhaps actions represented factored
form (typically, feature vector) facilitates generalizing properties training data
entire state action spaces. Note case perfect generalization (i.e.,
V (s) = V (s) states s), equal exact policy improvement
PI , thus API simulates exact policy iteration. However, practice, generalization
perfect, typically guarantees policy improvement3 nevertheless,
API often converges usefully (Tesauro, 1992; Tsitsiklis & Van Roy, 1996).
success API procedure depends critically ability represent
learn good value-function approximations. MDPs, arising
relational planning domains, often difficult specify space value functions
learning mechanism facilitate good generalization. example, work relational
reinforcement learning (Dzeroski, DeRaedt, & Driessens, 2001) shown learning
approximate value functions classical domains, blocks world, problematic.4 spite this, often relatively easy compactly specify good policies using
language (relational) state-action mappings. suggests languages may
provide useful policy-space biases learning API. However, prior API methods
based approximating value functions hence leverage biases.
motivation, consider form API directly learns policies without directly
representing approximating value functions.
3.2 Using Policy Language Bias
policy simply classifier, possibly stochastic, maps states actions. API
approach based view, motived recent work casts policy selection
standard classification learning problem. particular, given ability observe
trajectories target policy, use machine learning select policy, classifier,
mimics target closely possible. Khardon (1999b) studied learning setting
provided PAC-like learnability results, showing certain assumptions, small
number trajectories sufficient learn policy whose value close
target. addition, recent empirical work, relational planning domains (Khardon, 1999a;
Martin & Geffner, 2000; Yoon et al., 2002), shown using expressive languages
3. strong assumptions, API shown converge infinite limit near-optimal
value function. See Proposition 6.2 Bertsekas Tsitsiklis (1996).
4. particular, RRL work considered variety value-function representation including relational
regression trees, instance based methods, graph kernels, none generalized well
varying numbers objects.

80

fiAPI Policy Language Bias

specifying state-action mappings, good policies learned sample trajectories
good policies.
results suggest that, given policy , somehow generate trajectories
improved policy, learn approximately improved policy based
trajectories. idea basis approach. Figure 1 gives pseudo-code API
variant, starts initial policy 0 produces sequence approximately
improved policies. iteration involves two primary steps: First, given current
policy , procedure Improved-Trajectories (approximately) generates trajectories
improved policy 0 = PI . Second, trajectories used training data
procedure Learn-Policy, returns approximation 0 . describe
step detail.
Step 1: Generating Improved Trajectories. Given base policy , simulation technique policy rollout (Tesauro & Galperin, 1996; Bertsekas & Tsitsiklis, 1996)
computes approximation improved policy 0 = PI , 0 result
applying one step policy iteration . Furthermore, given state s, policy rollout
computes (s) without need solve 0 states, thus provides
tractable way approximately simulate improved policy 0 large state-space MDPs.
Often 0 significantly better , hence , lead substantially
improved performance small cost. Policy rollout provided significant benefits
number application domains, including example Backgammon (Tesauro & Galperin,
1996), instruction scheduling (McGovern, Moss, & Barto, 2002), network-congestion control
(Wu, Chong, & Givan, 2001), Solitaire (Yan, Diaconis, Rusmevichientong, & Van Roy,
2004).
Policy rollout computes (s), estimate 0 (s), estimating Q (s, a)
action taking maximizing action (s) suggested Equation 3.
Q (s, a) estimated drawing w trajectories length h, trajectory
result starting s, taking action a, following actions selected
h 1 steps. estimate Q (s, a) taken average cumulative
discounted reward along trajectory. sampling width w horizon h specified
user, control trade increased computation time large values,
reduced accuracy small values. Note rollout applies stochastic
deterministic policies due variance Q-value estimates, rollout policy
stochastic even deterministic base policies.
procedure Improved-Trajectories uses rollout generate n length h trajectories
, beginning randomly drawn initial state. Rather recording
states actions along trajectory, store additional information used
policy-learning algorithm. particular, ith element trajectory form
hsi , (si ), Q(si , a1 ), . . . , Q(si , )i, giving ith state si along trajectory, action
selected current (unimproved) policy si , Q-value estimates Q(si , a)
action. Note given Q-value information si learning algorithm
determine approximately improved action (s), maximizing actions, desired.
Step 2: Learn Policy. Intuitively, want Learn-Policy select new policy
closely matches training trajectories. experiments, use relatively simple
learning algorithms based greedy search within space policies specified policylanguage bias. Sections 5.2 5.3 detail policy-language learning bias used
81

fiFern, Yoon, & Givan

technique, associated learning algorithm. Section 4 provide
technical analysis idealized version algorithm, providing guidance regarding
required number training trajectories. note labeling training state
trajectories associated Q-values action, rather simply
best action, enable learner make informed trade-offs, focusing accuracy
states wrong decisions high costs, empirically useful. Also,
inclusion (s) training data enables learner adjust data relative ,
desirede.g., learner uses bias focuses states large improvement
appears possible.
Finally, note API effective, important initial policy
0 provide guidance toward improvement, i.e., 0 must bootstrap API process.
example, goal-based planning domains 0 reach goal sampled
states. Section 6 discuss important issue bootstrapping introduce
new bootstrapping technique.

4. Technical Analysis
section, consider variant policy improvement step main API loop,
learns improved policy given base policy . show select sampling
width w, horizon h, training set size n that, certain assumptions, quality
learned policy close quality 0 policy iteration improvement. Similar
results shown previous forms API based approximate value functions
(Bertsekas & Tsitsiklis, 1996), however, assumptions much different nature.5
analysis divided two parts. First, following Khardon (1999b), consider
sample complexity policy learning. is, consider many trajectories
target policy must observed learner guarantee good approximation
target. Second, show apply result, deterministic policies,
problem learning rollout policies, stochastic. Throughout
assume context MDP = hS, A, T, R, Ii.
4.1 Learning Deterministic Policies
trajectory length h sequence (s0 , a0 , s1 , a1 , . . . , ah1 , sh ) alternating states si
actions ai . say deterministic policy consistent trajectory (s1 , a1 , . . . , sh )
0 < h, (si ) = ai . define Dh distribution set
length h trajectories, Dh (t) probability generates trajectory
= (s0 , a0 , s1 , a1 , . . . , ah1 , sh ) according following process: first draw s0 according
initial state distribution I, draw si+1 (si , (si )) 0 < h. Note
Dh (t) non-zero consistent t.
policy improvement step first generates trajectories rollout policy (see Section 3.2), via procedure Improved-Trajectories, learns approximation
5. particular, Bertsekas Tsitsiklis (1996) assumes bound L norm value function
approximation, i.e., state approximation almost perfect. Rather assume
improved policy 0 comes finite class policies consistent learner.
cases policy improvement guaranteed given additional assumption minimum
Q-advantage MDP (see below).

82

fiAPI Policy Language Bias

API (n, w, h, M, 0 , )
// training set size n, sampling width w, horizon h,
// MDP = hS, {a1 , . . . , }, T, R, Ii, initial policy 0 , discount factor .
0 ;
loop
Improved-Trajectories(n, w, h, M, );
Learn-Policy(T );
satisfied ;
// e.g., change small
Return ;
Improved-Trajectories(n, w, h, M, )
// training set size n, sampling width w,
// horizon h, MDP , current policy
;
repeat n times // generate n trajectories improved policy
nil;
state drawn I; // draw random initial state
= 1 h
hQ(s, a1 ), . . . , Q(s, )i Policy-Rollout(, s, w, h, ); // Q (s, a) estimates
hs, (s), Q(s, a1 ), . . . , Q(s, ))i; // concatenate new sample onto trajectory
action maximizing Q(s, a); // action improved policy state
state sampled (s, a); // simulate action improved policy
t;
Return ;
Policy-Rollout (, s, w, h, )

// Compute Q (s, a) estimates hQ(s, a1 ), . . . , Q(s, )i

// policy , state s, sampling width w, horizon h, MDP
action ai
Q(s, ai ) 0;
repeat w times // Q(s, ai ) average w trajectories
R R(s, ai ); s0 state sampled (s, ai ); // take action ai
= 1 h 1 // take h 1 steps accumulating discounted reward R
R R + R(s0 , (s0 ));
s0 state sampled (s0 , (s0 ))
Q(s, ai ) Q(s, ai ) + R; // include trajectory average
Q(s, ai )

Q(s,ai )
;
w

Return hQ(s, a1 ), . . . , Q(s, )i

Figure 1: Pseudo-code API algorithm. See Section 5.3 instantiation LearnPolicy called Learn-Decision-List.
. Note rollout policy serves stochastic approximation 0 = PI
policy iteration improvement . Thus, Improved-Trajectories viewed at0
tempting draw trajectories Dh , learning step viewed learning
83

fiFern, Yoon, & Givan

0

approximation 0 . Imagining moment draw trajectories Dh ,
fundamental question many trajectories sufficient ensure learned
policy good 0 . Khardon (1999b) studied question case
deterministic policies undiscounted goal-based planning domains (i.e., MDPs
reward received goal states). give straightforward adaptation
main result problem setting general reward functions measure
quality policy V ().
learning-problem formulation similar spirit standard framework Probably Approximately Correct (PAC) learning. particular, assume target
policy comes finite class deterministic policies H. example, H may correspond
set policies described bounded-length decision lists. addition,
assume learner consistenti.e., returns policy H consistent
training trajectories. assumptions, relatively small number
trajectories (logarithmic |H|) sufficient ensure high probability
learned policy good target.
Proposition 1. Let H finite class deterministic policies. H,

set n = 1 ln |H|
trajectories drawn independently Dh , 1 probability
every H consistent trajectories satisfies V () V () 2Vmax ( + h ).
proof proposition Appendix. computational complexity
finding consistent policy depends policy class H. Polynomial-time algorithms
given interesting classes bounded-length decision listshowever,
algorithms typically expensive policy classes consider practice. Rather,
described Section 5.3, use learner based greedy heuristic search, often
works well practice.
assumption target policy comes fixed size class H often
violated. However, pointed Khardon (1999b), straightforward give
extension Proposition 1 setting learner considers increasingly complex
policies consistent one found. case, sample complexity related
encoding size target policy rather size H, thus allowing use
large expressive policy classes without necessarily paying full sample-complexity
price Proposition 1.
4.2 Learning Rollout Policies
proof Proposition 1 relies critically fact policy class H contains
deterministic policies. However, main API loop, target policies computed via
rollout hence stochastic due uncertainty introduced finite sampling. Thus,
cannot directly use Proposition 1 context learning trajectories produced
rollout. deal problem describe variant Improved-Trajectories
reliably generate training trajectories deterministic policy 0 = PI (see
Equation 3), guaranteed improve improvement possible.
Given base policy , first define (s) set actions maximize
Q (s, a). Note 0 (s) = min (s), minimum taken respect action ordering provided MDP. Importantly policy deterministic thus
84

fiAPI Policy Language Bias

generate trajectories it, apply result learn close approximation. order generate trajectories 0 slightly modify Improved-Trajectories.
modification introduced analysis only, experiments based procedures given Figure 1. modification replace action maximization step
Improved-Trajectories (second last statement loop), chooses next
action execute, following two steps
A(, s) {a0 | maxa Q(s, a) Q(s, a0 ) }
min A(, s)
Q(s, a) estimate Qh (s, a) computed policy rollout using sampling width
w, newly introduced parameter.
Note A(, s) = (s), selected action equal 0 (s). condition true every state encountered modified Improved-Trajectories
effectively generate trajectories 0 . Thus, would bound probability
A(, s) 6= (s) small value appropriately choosing sampling width w,
horizon h, . Unfortunately, choice parameters depends MDP.
is, given particular parameter values, MDP event
A(, s) 6= (s) non-negligible probability state. reason first
define Q-advantage MDP show select appropriate parameter values
given lower-bound .
Given MDP policy , let 0 set states 0 iff
two actions a0 Q (s, a) 6= Q (s, a0 ), i.e., actions distinct
Q-values. state 0 define a1 (s) a2 (s) best action second
best action respectively measured Q (s, a). Q-advantage defined =
minsS 0 a1 (s) a2 (s), measures minimum Q-value gap optimal
sub-optimal action state space. Given lower-bound Q-advantage
MDP following proposition indicates select parameter values ensure
A(, s) = (s) high probability.
Proposition 2.


MDP Q-advantage least , 0 < 0 < 1,

h > log


8Vmax

8Vmax



2



w >
=

2

ln

|A|
0

state s, A(, s) = (s) probability least 1 0 .
proof given Appendix. Thus, parameter values satisfying conditions, MDP Q-advantage least guaranteed probability
least 1 0 A(, s) = A(, s). means Improved-Trajectories correctly select action 0 (s) probability least 1 0 . Note proposition
85

fiFern, Yoon, & Givan

agrees intuition h w increase decreasing Q-advantage
increasing Vmax w increase decreasing 0 .6
order generate n length h trajectories 0 , modified Improved-Trajectories
routine must compute set A(, ) n h states, yielding n h opportunities make
error. ensure error made, modified procedure sets sampling width w

. guarantees error free training set created probability
using 0 = 2nh

least 1 2 .
Combining observation assumption 0 H apply Proposition
0
1 follows. First, generate n = 1 ln 2|H|
trajectories using modified Improved
Trajectories routine (with 0 = 2nh
). Next, learn policy trajectories using
consistent learner. know probability generating imperfect training set
bounded 2 , chosen value n, failure probability learner
bounded 2 . Thus, get probability least 1 , learned policy
satisfies V () V ( 0 ) 2Vmax ( + h ), giving approximation guarantee relative
improved policy 0 . summarized following proposition.
Proposition 3. Let H finite class deterministic policies, 0 < < 1, 0 < < 1.
MDP Q-advantage
least , policy PI H, set

1
1
n > ln 2|H|
trajectories produced modified Improved-Trajectories using
parameters satisfying,
=


2

h > log


8Vmax

8Vmax 2 2nh|A|
w >
ln


least 1 probability every H consistent trajectories satisfies
V () V (PI ) 2Vmax ( + h ).




One notable aspect result logarithmic dependence
number actions |A| 1 . However, practical utility hindered dependence
typically known practice, exponentially small
planning horizon. Unfortunately, dependence appears unavoidable type
approach try learn trajectories PI produced rollout.
particular setting parameters, always MDP
small enough Q-advantage, value rollout policy arbitrarily worse
PI .

5. API Relational Planning
work motivated goal solving relational MDPs. particular, interested finding policies relational MDPs represent classical planning domains
6. first glance appears lower-bound h decreases increasing Vmax decreasing .
However, opposite true since base logarithm discount factor, strictly less
one. note since upper-bounded 2Vmax bound h always positive.

86

fiAPI Policy Language Bias

stochastic variants. policies applied problem instance
planning domain, hence viewed form domain-specific control knowledge.
section, first describe straightforward way view classical planning domains
(not single problem instances) relationally factored MDPs. Next, describe
relational policy space policies compactly represented taxonomic decision
lists. Finally, present heuristic learning algorithm policy space.
5.1 Planning Domains MDPs.
say MDP hS, A, T, R, Ii relational defined giving finite
set objects O, finite set predicates P , finite set action types . fact
predicate applied appropriate number objects, e.g., on(a, b) blocks-world
fact. state set facts, interpreted representing true facts state.
state space contains possible sets facts. action action type applied
appropriate number objects, e.g., putdown(a) blocks-world action, action
space set actions.
classical planning domain describes set problem instances related structure,
problem instance gives initial world state goal. example, blocks
world classical planning domain, problem instance specifies initial block
configuration set goal conditions. Classical planners attempt find solutions
specific problem instances domain. Rather, goal solve entire planning domains
finding policy applied problem instances. described below,
straightforward view classical planning domain relational MDP MDP
state corresponds problem instance.
State Action Spaces. classical planning domain specifies set action
types , world predicates W , possible world objects O. Together define
MDP action space. state MDP corresponds single problem instance (i.e.,
world state goal) planning domain specifying current world
goal. achieve letting set relational MDP predicates P = W G,
G set goal predicates. set goal predicates contains predicate
world predicate W , named prepending g onto corresponding
world predicate name (e.g., goal predicate gclear corresponds world predicate
clear). definition P see MDP states sets goal world
facts, indicating true world facts problem instance goal conditions.
important note, described below, MDP actions change world
facts goal facts. Thus, large relational MDP viewed collection
disconnected sub-MDPs, sub-MDP corresponds distinct goal condition.
Reward Function. Given MDP state objective reach another MDP state
goal facts subset corresponding world factsi.e., reach world state
satisfies goal. call states goal states MDP. example,
MDP state
{on-table(a), on(a, b), clear(b), gclear(b)}
goal state blocks-world MDP, would goal state without world fact
clear(b). represent objective reaching goal state quickly defining R assign
reward zero actions taken goal states negative rewards actions
87

fiFern, Yoon, & Givan

states, representing cost taking actions. Typically, classical planning
domains, action costs uniformly -1, however, framework allows cost vary
across actions.
Transition Function. classical planning domain provides action simulator
(e.g., defined STRIPS rules) that, given world state action, returns new world
state. define MDP transition function simulator modified treat goal
states terminal preserve without change goal predicates MDP state. Since
classical planning domains typically large number actions, action definitions
usually accompanied preconditions indicate legal actions given state,
usually legal actions small subset possible actions. assume
treats actions legal no-ops. simplicity, relational MDP definition
explicitly represent action preconditions, however, assume algorithms
access preconditions thus need consider legal actions. example,
restrict rollout legal actions given state.
Initial State Distribution. Finally, initial state distribution program
generates legal problem instances (MDP states) planning domain. example, problem domains planning competitions commonly distributed problem
generators.
definitions, good policy one reach goal states via low-cost
action sequences initial states drawn I. Note policies mappings
problem instances actions thus sensitive goal conditions.
way, learned policies able generalize across different goals. next describe
language representing generalized policies.
5.2 Taxonomic Decision List Policies.
single argument action types, many useful rules planning domains take form
apply action type object class C (Martin & Geffner, 2000). example,
blocks world, pick clear block belongs table table,
logistics world, unload object destination. Using concept
language describing object classes, Martin Geffner (2000) introduced use
decision lists rules useful learning bias, showing promising experiments
deterministic blocks world. motivation, consider policy space similar
one used originally Martin Geffner, generalized handle multiple action
arguments. Also, historical reasons, concept language based upon taxonomic
syntax (McAllester, 1991; McAllester & Givan, 1993), rather description logic
used Martin Geffner.
Comparison Predicates. relational MDPs world goal predicates,
corresponding classical planning domains, often useful polices compare
current state goal. end, introduce new set predicates, called
comparison predicates, derived world goal predicates.
world predicate p corresponding goal predicate gp, introduce new comparison
predicate cp defined conjunction p gp. is, comparison-predicate
fact true corresponding world goal predicates facts true.
88

fiAPI Policy Language Bias

example, blocks world, comparison-predicate fact con(a, b) indicates
b current state goali.e., on(a, b) gon(a, b) true.
Taxonomic Syntax. Taxonomic syntax provides language writing class expressions represent sets objects properties interest serve fundamental
pieces build policies. Class expressions built MDP predicates
(including comparison predicates applicable) variables. policy representation,
variables used denote action arguments, runtime instantiated
objects. simplicity consider predicates arity one two, call
primitive classes relations, respectively. domain contains predicates arity
three more, automatically convert multiple auxiliary binary predicates. Given
list variables X = (x1 , . . . , xk ), class expressions given by,
C[X] ::= C0 | xi | a-thing | C[X] | (R C[X]) | (min R)
R ::= R0 | R 1 | R
C[X] class expression, R relation expression, C0 primitive class, R0
primitive relation, xi variable X. Note that, classical planning domains,
primitive classes relations world, goal, comparison predicates. define depth d(C[X]) class expression C[X] one C[X] either primitive
class, a-thing, variable, (min R), otherwise define d(C[X]) d(R C[X])
d(C[X]) + 1, R relation expression C[X] class expression. given
relational MDP denote Cd [X] set class expressions C[X] depth
less.
Intuitively class expression (R C[X]) denotes set objects related
relation R object set C[X]. expression (R C[X]) denotes
set objects related R chain object C[X]this
constructor important representing recursive concepts (e.g., blocks a).
expression (min R) denotes set objects minimal relation R.
formally, let MDP state = (o1 , . . . , ok ) variable assignment,
assigns object oi variable xi . interpretation C[X] relative
set objects denoted C[X]s,O . primitive class C0 interpreted set
objects predicate symbol C0 true s. Likewise, primitive relation R0
interpreted set object tuples relation R0 holds s. class
expression a-thing denotes set objects s. class expression xi , xi
variable, interpreted singleton set {oi }. interpretation compound
expressions given by,
(C[X])s,O = {o | 6 C[X]s,O }
(R C[X])s,O = {o | o0 C[X]s,O s.t. (o0 , o) Rs,O }
(min R)s,O = {o | o0 s.t. (o, o0 ) Rs,O , 6 o0 s.t. (o0 , o) Rs,O }
(R )s,O = ID {(o1 , ov ) | o2 , . . . , ov1 s.t. (oi , oi+1 ) Rs,O 1 < v}
(R1 )s,O = {(o, o0 ) | (o0 , o) Rs,O }
C[X] class expression, R relation expression, ID identity relation.
examples useful blocks-world concepts, given primitive classes clear, gclear,
holding, con-table, along primitive relations on, gon, con, are:
89

fiFern, Yoon, & Givan

(gon1 holding) depth two, denotes block want block
held.
(on (on gclear)) depth three, denotes blocks currently blocks
want make clear.
(con con-table) depth two, denotes set blocks well constructed
towers. see note block bv class exists
sequence blocks b1 , . . . , bv b1 table goal
current state (i.e. con-table(b1 )) bi+1 bi goal current state
(i.e. con(bi , bi+1 )) 1 < v.
(gon (con con-table)) depth three, denotes blocks belong top
currently well constructed tower.
Decision List Policies represent policies decision lists action-selection rules.
rule form a(x1 , . . . , xk ) : L1 , L2 , . . . Lm , k-argument action type,
Li literals, xi action-argument variables. denote list
action argument variables X = (x1 , . . . , xk ). literal form x C[X],
C[X] taxonomic syntax class expression x action-argument variable.
Given MDP state list action-argument objects = (o1 , . . . , ok ), say
literal xi C[X] true given iff oi C[X]s,O . say rule
R = a(x1 , . . . , xk ) : L1 , L2 , . . . Lm allows action a(o1 , . . . ok ) iff literal rule
true given O. Note literals rule action type a,
possible actions type allowed rule. rule viewed placing mutual
constraints tuples objects action type applied to. Note
single rule may allow actions many actions one type. Given decision list
rules say action allowed list allowed rule list,
previous rule allows actions. Again, decision list may allow actions
multiple actions one type. decision list L MDP defines deterministic policy
[L] MDP. L allows actions state s, [L](s) least7 legal action
s; otherwise, [L](s) least legal action allowed L. important
note since [L] considers legal actions, specified action preconditions,
rules need encode preconditions, allows simpler rules learning.
words, think rule implicitly containing preconditions
action type.
example taxonomic decision list policy consider simple blocks-world domain
goal condition always clear red blocks. primitive classes
domain red, clear, holding, single relation on. following
policy solve problem domain.
putdown(x1 ) : x1 holding
pickup(x1 ) : x1 clear, x1 (on (on red))
7. action ordering relational MDP defined lexicographically terms orderings action
types objects.

90

fiAPI Policy Language Bias

first rule cause agent putdown block held. Otherwise,
block held, find block x1 clear red block (expressed
(on (on red))) pick up. Appendix B gives examples complex policies
learned system experiments.
5.3 Learning Taxonomic Decision Lists
given relational MDP, define Rd,l set action-selection rules
length l literals whose class expression depth d. Also, let
Hd,l denote policy space defined decision lists whose rules Rd,l . Since
number depth-bounded class expressions finite finite number rules,
hence Hd,l finite, though exponentially large. implementation Learn-Policy,
used main API loop, learns policy Hd,l user specified values l.
use Rivest-style decision-list learning approach (Rivest, 1987)an approach
taken Martin Geffner (2000) learning class-based policies. primary difference
Martin Geffner (2000) technique method selecting individual
rules decision list. use greedy, heuristic search, previous work used
exhaustive enumeration approach. difference allows us find rules
complex, potential cost failing find good simple rules enumeration
might discover.
Recall Section 3, training set given Learn-Policy contains trajectories
rollout policy. learning algorithm, however, sensitive trajectory
structure (i.e., order trajectory elements) thus, simplify discussion,
take input learner training set contains union
trajectory elements. means trajectory set contains n length h
trajectories, contain total n h training examples. described Section 3,
training example form hs, (s), Q(s, a1 ), . . . , Q(s, )i, state,
(s) action selected previous policy, Q(s, ai ) Q-value estimate
Q (s, ai ). Note experiments training examples contain values
legal actions state.
Given training set D, natural learning goal find decision-list policy
training example selects action maximum estimated Q-value. learning
goal, however, problematic practice often several best (or close
best) actions measured true Q-function. case, due random sampling,
particular action looks best according Q-value estimates training set
arbitrary. Attempting learn concise policy matches arbitrary actions
difficult best likely impossible.
One approach (Lagoudakis & Parr, 2003) avoiding problem use statistical
tests determine actions clearly best (positive examples) ones
clearly best (negative examples). learner asked find
policy consistent positive negative examples. approach
shown empirical success, potential shortcoming throwing away
Q-value information. particular, may always possible find policy
exactly matches training data. cases, would learner make informed
trade-offs regarding sub-optimal actionsi.e., prefer sub-optimal actions larger
91

fiFern, Yoon, & Givan

Learn-Decision-List (D, d, l, b)
// training set D, concept depth d, rule length l, beam width b
L nil;
(D empty)
R Learn-Rule(D, d, l, b);
{d | R covers d};
L Extend-List(L, R); // add R end list
Return L;
Learn-Rule(D, d, l, b)
// training set D, concept depth d, rule length l, beam width b
action type

// compute rule action type

Ra Beam-Search(D, d, l, b, a);
Return argmaxa Hvalue(Ra , D);
Beam-Search (D, d, l, b, a)
// training set D, concept depth d, rule length l, beam width b, action type
k arity a; X (x1 , . . . , xk );

//

L {(x C) | x X, C Cd [X]}; //

X sequence action-argument variables
construct set depth bounded candidate literals

B0 { a(X) : nil }; 1; // initialize beam single rule literals
loop
G = Bi1 {R Rd,l | R = Add-Literal(R0 , l), R0 Bi1 , l L};
Bi Beam-Select(G, b, D); //

select best b heuristic values

+ 1;
Bi1 = Bi ; //

loop improvement heuristic

Return argmaxRBi Hvalue(R, D) //

return best rule final beam

Figure 2: Pseudo-code learning decision list Hd,l given training data D.
procedure Add-Literal(R, l) simply returns rule literal l added end
rule R. procedure Beam-Select(G, w, D) selects best b rules G different
heuristic values. procedure Hvalue(R, D) returns heuristic value rule R relative
training data described text.

Q-values. motivation, describe cost-sensitive decision-list learner
sensitive full set Q-values D. learning goal roughly find decision
list selects actions large cumulative Q-value training set.
Learning List Rules. say decision list L covers training example
hs, (s), Q(s, a1 ), . . . , Q(s, )i L suggests action state s. Given set training
examples D, search decision list selects actions high Q-value via
iterative set-covering approach carried Learn-Decision-List. Decision-list rules
92

fiAPI Policy Language Bias

constructed one time order list covers training examples.
Pseudo-code algorithm given Figure 2. Initially, decision list null list
cover training examples. iteration, search high quality
rule R quality measured relative set currently uncovered training examples.
selected rule appended current decision-list, training examples newly
covered selected rule removed training set. process repeats
list covers training examples. success approach depends heavily
function Learn-Rule, selects good rule relative uncovered training
examplestypically good rule one selects actions best (or close best)
Q-value covers significant number examples.
Learning Individual Rules. input rule learner Learn-Rule set
training examples, along depth length parameters l, beam width b.
action type a, rule learner calls routine Beam-Search find good rule
Ra Rd,l action type a. Learn-Rule returns rule Ra highest value
measured heuristic, described later section.
given action type a, procedure Beam-Search generates beam B0 , B1 . . .,
Bi set rules Rd,l action type a. sets evolve specializing
rules previous sets adding literals them, guided heuristic function. Search
begins general rule a(X) : nil, allows action type state.
Search iteration produces set Bi contains b rules highest different heuristic
values among following set8
G = Bi1 {R Rd,l | R = Add-Literal(R0 , l), R0 Bi1 , l L}
L set possible literals depth less. set includes
current best rules (those Bi1 ) rule Rd,l formed adding
new literal rule Bi1 . search ends improvement heuristic value
occurs, Bi = Bi1 . Beam-Search returns best rule Bi according
heuristic.
Heuristic Function. training instance hs, (s), Q(s, a1 ), . . . , Q(s, )i, define Q-advantage taking action ai instead (s) state (s, ai ) = Q(s, ai )
Q(s, (s)). Likewise, Q-advantage rule R sum Q-advantages actions
allowed R s. Given rule R set training examples D, heuristic function
Hvalue(R, D) equal number training examples rule covers plus
cumulative Q-advantage rule training examples.9 Using Q-advantage rather
Q-value focuses learner toward instances large improvement previous policy possible. Naturally, one could consider using different weights coverage
Q-advantage terms, possibly tuning weight automatically using validation data.
8. Since many rules Rd,l equivalent, must prevent beam filling semantically
equivalent rules. Rather deal problem via expensive equivalence testing take ad-hoc,
practically effective approach. assume rules coincidentally heuristic
value, ones must equivalent. Thus, construct beams whose members
different heuristic values. choose rules value preferring shorter rules,
arbitrarily.
9. coverage term included, covering zero Q-advantage example
covering it. zero Q-advantage good (e.g., previous policy optimal state).

93

fiFern, Yoon, & Givan

6. Random Walk Bootstrapping
two issues critical success API technique. First, API
fundamentally limited expressiveness policy language strength
learner, dictates ability capture improved policy described training
data iteration. Second, API yield improvement Improved-Trajectories
successfully generates training data describes improved policy. large classical
planning domains, initializing API uninformed random policy typically result
essentially random training data, helpful policy improvement.
example, consider MDP corresponding 20-block blocks world initial
problem distribution generates random initial goal states. case, random
policy unlikely reach goal state within practical horizon time. Hence,
rollout trajectories unlikely reach goal, providing guidance toward learning
improved policy (i.e., policy reliably reach goal).
interested solving large domains this, providing guiding inputs
API critical. Fern, Yoon, Givan (2003), showed bootstrapping API
domain-independent heuristic planner (Hoffmann & Nebel, 2001), API
able uncover good policies blocks world, simplified logistics world (no planes),
stochastic variants. approach, however, limited heuristics ability
provide useful guidance, vary widely across domains.
describe new bootstrapping procedure goal-based planning domains, based
random walks, guiding API toward good policies. planning system,
evaluated Section 7, based integrating procedure API order find
policies goal-based planning domains. non-goal-based MDPs, bootstrapping
procedure directly applied, bootstrapping mechanisms must used
necessary. might include providing initial non-trivial policy, providing heuristic
function, form reward shaping (Mataric, 1994). Below, first describe
idea random-walk distributions. Next, describe use distributions
context bootstrapping API, giving new algorithm LRW-API.
6.1 Random Walk Distributions
Throughout consider MDP = hS, A, T, R, Ii correspond goal-based planning domains, described Section 5.1. Recall state corresponds
planning problem, specifying world state (via world facts) set goal conditions (via
goal facts). use terms MDP state planning problem interchangeably.
Note that, context, distribution planning problems. convenience
denote MDP states tuples = (w, g), w g sets world facts
goal facts respectively.
Given MDP state = (w, g) set goal predicates G, define s|G
MDP state (w, g 0 ) g 0 contains goal facts g applications predicate
G. Given set goal predicates G, define n-step random-walk problem
distribution RW n (M, G) following stochastic algorithm:
1. Draw random state s0 = (w0 , g0 ) initial state distribution I.
94

fiAPI Policy Language Bias

2. Starting s0 take n uniformly random actions10 , giving state sequence (s0 , . . . , sn ),
sn = (wn , g0 ) (recall actions change goal facts). uniformly
random action selection, assume extra no-op action (that change
state) selected fixed probability, reasons explained below.
3. Let g set goal facts corresponding world facts wn , e.g.,
wn = {on(a, b), clear(a)}, g = {gon(a, b), gclear(a)}. Return planning
problem (MDP state) (s0 , g)|G output.
sometimes abbreviate RW n (M, G) RW n G clear context.
Intuitively, perform well distribution policy must able achieve facts
involving goal predicates typically result n-step random walk
initial state. restricting set goal predicates G specify types facts
interested achievinge.g., blocks world may interested
achieving facts involving predicate.
random-walk distributions provide natural way span range problem difficulties. Since longer random walks tend take us initial state, small
n typically expect planning problems generated RW n become
difficult n grows. However, n becomes large, problems generated require far
fewer n steps solvei.e., direct paths initial state
end state long random walk. Eventually, since finite, problem difficulty
stop increasing n.
question raised idea whether, large n, good performance RW n
ensures good performance problem distributions interest domain.
domains, simple blocks world11 , good random-walk performance
seem yield good performance distributions interest. domains,
grid world (with keys locked doors), intuitively, random walk unlikely
uncover problem requires unlocking sequence doors. Indeed, since RW n
insensitive goal distribution underlying planning domain, random-walk
distribution may quite different.
believe good performance long random walks often useful,
addressing one component difficulty many planning benchmarks. successfully
address problems components difficulty, planner need deploy orthogonal technology landmark extraction setting subgoals (Hoffman, Porteous, &
Sebastia, 2004). example, grid world, could automatically set subgoal
possessing key first door, long random-walk policy could provide useful
macro getting key.
purpose developing bootstrapping technique API, limit focus
finding good policies long random walks. experiments, define long
specifying large walk length N . Theoretically, inclusion no-op action
definition RW ensures induced random-walk Markov chain12 aperiodic,
10. practice, select random actions set applicable actions state si , provided
simulator makes possible identify set.
11. blocks world large n, RW n generates various pairs random block configurations, typically
pairing states far apartclearly, policy performs well distribution captured
significant information blocks world.
12. dont formalize chain here, various formalizations work well.

95

fiFern, Yoon, & Givan

thus distribution states reached increasingly long random walks converges
stationary distribution13 . Thus RW = limn RW n well-defined, take
good performance RW goal.
6.2 Random-Walk Bootstrapping
MDP , define [I 0 ] MDP identical initial state
distribution replaced 0 . define success ratio SR(, [I]) [I]
probability solves problem drawn I. treating random variable,
average length AL(, [I]) [I] conditional expectation solution
length problems drawn given solves I. Typically solution length
problem taken number actions, however, action costs uniform,
length taken sum action costs. Note MDP formulation
classical planning domains, given Section 5.1, policy achieves high V ()
high success ratio low average cost.
Given MDP set goal predicates G, system attempts find good
policy [RW N ], N selected large enough adequately approximate
RW , still allowing tractable completion learning. Naively, given initial
random policy 0 , could try apply API directly. However, already discussed,
work general, since interested planning domains RW produces
extremely large difficult problems random policies provide ineffective starting
point.
However, small n (e.g., n = 1), RW n typically generates easy problems,
likely API, starting even random initial policy, reliably find good
policy RW n . Furthermore, expect policy n performs well RW n ,
provide reasonably good, perhaps perfect, guidance problems drawn
RW moderately larger n. Thus, expect able find
good policy RW bootstrapping API initial policy n . suggests natural
iterative bootstrapping technique find good policy large n (in particular, n = N ).
Figure 3 gives pseudo-code procedure LRW-API integrates API
random-walk bootstrapping find policy long-random-walk problem distribution.
Intuitively, algorithm viewed iterating two stages: first, finding
hard enough distribution current policy (by increasing n); and, then, finding good
policy hard distribution using API. algorithm maintains current policy
current walk length n (initially n = 1). long success ratio RWn
success threshold , constant close one, simply iterate steps
approximate policy improvement. achieve success ratio policy ,
if-statement increases n success ratio RW n falls . is,
performs well enough current n-step distribution move distribution
slightly harder. constant determines much harder set small enough
likely used bootstrap policy improvement harder distribution.
(The simpler method increasing n 1 whenever success ratio achieved
13. Markov chain may irreducible, stationary distribution may reached
initial states; however, considering one initial state, described I.

96

fiAPI Policy Language Bias

LRW-API (N, G, n, w, h, M, 0 , )
// max random-walk length N , goal predicates G
// training set size n, sampling width w, horizon h,
// MDP , initial policy 0 , discount factor .
0 ; n 1;
loop
c (n) >
SR

// Find harder n-step distribution .
c (i) < , N none;
n least [n, N ] s.t. SR
0 = [RW n (M, G)];
Improved-Trajectories(n, w, h, 0 , );
Learn-Policy(T );
satisfied
Return ;

c (n) estimates success ratio planning
Figure 3: Pseudo-code LRW-API. SR
domain problems drawn RW n (M, G) drawing set problems returning
fraction solved . Constants described text.

find good policies whenever method does. take much longer, may run
API repeatedly training set already good policy.)
n becomes equal maximum walk length N , n = N future
iterations. important note even find policy good success ratio
RW N may still possible improve average length policy. Thus,
continue API distribution satisfied success ratio
average length current policy.

7. Relational Planning Experiments
section, evaluate LRW-API technique relational MDPs corresponding
deterministic stochastic classical planning domains. first give results number
deterministic benchmark domains, showing promising results comparison stateof-the-art planner (Hoffmann & Nebel, 2001), highlighting limitations
approach. Next, give results several stochastic planning domains including
domain-specific track 2004 International Probabilistic Planning Competition
(IPPC). domain definitions problem generators used experiments
available upon request.
experiments, use policy learner described Section 5.3 learn
taxonomic decision list policies. cases, number training trajectories 100,
policies restricted rules depth bound length bound l. discount
97

fiFern, Yoon, & Givan

factor always one, LRW-API always initialized policy selects
random actions. utilize maximum-walk-length parameter N = 10, 000 set
equal 0.9 0.1 respectively.
7.1 Deterministic Planning Experiments
perform experiments seven familiar STRIPS planning domains including used
AIPS-2000 planning competition, used evaluate TL-Plan Bacchus
Kabanza (2000), Gripper domain. domain standard problem generator
accepts parameters, control size difficulty randomly generated
problems. list domain parameters associated them. detailed
description domains found Hoffmann Nebel (2001).
Blocks World (n) : standard blocks worlds n blocks.
Freecell (s, c, f, l) : version Solitaire suits, c cards per suit, f freecells,
l columns.
Logistics (a,c,l,p) : logistics transportation domain airplanes, c cities, l
locations, p packages.
Schedule (p) : job shop scheduling domain p parts.
Elevator (f, p) : elevator scheduling f floors p people.
Gripper (b) : robotic gripper domain b balls.
Briefcase (i) : transportation domain items.
LRW Experiments. first set experiments evaluates ability LRW-API
find good policies RW . utilize sampling width one rollout, since
deterministic domains. Recall iteration LRW-API compute
(approximately) improved policy may increase walk length n find harder
problem distribution. continued iterating LRW-API observed
improvement. training time per iteration approximately five hours.14 Though
initial training period significant, policy learned used solve new
problems quickly, terminating seconds solution one found, even
large problems.

Figure 4 provides data iteration LRW-API seven domains
indicated parameter settings. first column, domain, indicates
iteration number (e.g., Blocks World run 8 iterations). second column
records walk length n used learning corresponding iteration. third
fourth columns record SR AL policy learned corresponding iteration
14. timing information relatively unoptimized Scheme implementation. reimplementation
C would likely result 5-10 fold speed-up.

98

fin

RW n
SR
AL

iter. #

iter. #

API Policy Language Bias

RW
SR
AL

n

Blocks World (20)
1
2
3
4
5
6
7
8

4
14
54
54
54
54
334
334

0.92
0.94
0.56
0.78
0.88
0.98
0.84
0.99


2.0
5.6
15.0
15.0
33.7
25.1
45.6
37.8

0
0.10
0.17
0.32
0.65
0.90
0.87
1
0.96

5
8
30
30
30
30
30
30
30

0.97
0.97
0.65
0.72
0.90
0.81
0.78
0.90
0.93


1.4
2.7
7.0
7.1
6.7
6.7
6.8
6.9
7.7

0.08
0.26
0.78
0.85
0.85
0.89
0.87
0.89
0.93
1

RW
SR
AL

Logistics (1,2,2,6)
0
41.4
42.8
40.2
47.0
43.9
50.1
43.3
49.0

1
2
3
4
5
6
7
8
9
10

43
44
45

Freecell (4,2,2,4)
1
2
3
4
5
6
7
8
9

RW n
SR
AL

3.6
6.3
7.0
7.0
6.3
6.6
6.8
6.6
7.9
5.4

5
45
45
45
45
45
45
45
45
45

45
45
45

0.86
0.86
0.81
0.86
0.76
0.76
0.86
0.76
0.70
0.81

0.74
0.90
0.92


3.1
6.5
6.9
6.8
6.1
5.9
6.2
6.9
6.1
6.1

6.4
6.9
6.6

0.25
0.28
0.31
0.28
0.28
0.32
0.39
0.31
0.19
0.25

0.25
0.39
0.38
1

11.3
7.2
8.4
8.9
7.8
8.4
9.1
11.0
7.8
7.6

9.0
9.3
9.4
13

0.48
1
1

27
34
36

0
0.2
1
1

0
38
30
28

Schedule (20)
1
2

1
4

0.79
1


1
3.45

Briefcase (10)
Elevator (20,10)
1

20

1

4.0



1
1

26
23

1
1

13
13

1
2
3

5
15
15

0.91
0.89
1


1.4
4.2
3.0

Gripper (10)
1

10

1


3.8

Figure 4: Results iteration LRW-API seven deterministic planning domains.
iteration, show walk length n used learning, along success ratio
(SR) average length (AL) learned policy RW n RW . final
policy shown domain performs = 0.9 SR walks length N = 10, 000
(with exception Logistics), iteration improve performance.
benchmark show SR AL planner problems drawn
RW .

measured 100 problems drawn RW n corresponding value n (i.e.,
distribution used learning). SR exceeds , next iteration seeks
increased walk length n. fifth sixth columns record SR AL
99

fiFern, Yoon, & Givan

policy, measured 100 problems drawn LRW target distribution RW ,
experiments approximated RW N N = 10, 000.
So, example, see Blocks World total 8 iterations,
learn first one iteration n = 4, one iteration n = 14, four iterations
n = 54, two iterations n = 334. point see resulting
policy performs well RW . iterations n = N , shown, showed
improvement policy found iteration eight. domains, observed
improvement iterating n = N , thus show iterations.
note domains except Logistics (see below) achieve policies good performance
RW N learning much shorter RW n distributions, indicating indeed
selected large enough value N capture RW , desired.
General Observations. several domains, learner bootstraps quickly
short random-walk problems, finding policy works well even much longer
random-walk problems. include Schedule, Briefcase, Gripper, Elevator. Typically, large problems domains many somewhat independent subproblems
short solutions, short random walks generate instances different typical
subproblems. domains, best LRW policy found small number
iterations performs comparably RW . note considered
good domain-independent planner domains, consider successful
result.
two domains, Logistics15 Freecell, planner unable find policy
success ratio one RW . believe result limited knowledge representation allowed policies following reasons. First, cannot write good
policies domains within current policy language. example, logistics, one
important concept set containing packages trucks truck
packages goal city. However, domain defined way concept
cannot expressed within language used experiments. Second, final learned
decision lists Logistics Freecell, Appendix B, contain much larger
number specific rules lists learned domains. indicates
learner difficulty finding general rules, within language restrictions,
applicable large portions training data, resulting poor generalization. Third,
success ratio (not shown) sampling-based rollout policy, i.e., improved policy
simulated Improved-Trajectories, substantially higher resulting
learned policy becomes policy next iteration. indicates LearnDecision-List learning much weaker policy sampling-based policy generating
training data, indicating weakness either policy language learning algorithm. example, logistics domain, iteration eight, training data learning
iteration-nine policy generated sampling rollout policy achieves success ratio
0.97 100 training problems drawn RW 45 distribution, learned
iteration-nine policy achieves success ratio 0.70, shown figure iteration
nine. Extending policy language incorporate expressiveness appears
required domains require sophisticated learning algorithm,
point future work.
15. Logistics, planner generates long sequence policies similar, oscillating success ratio
elided table ellipsis space reasons.

100

fiAPI Policy Language Bias


Domain
Blocks


SR AL
0.81
60
0.28 158

Size
(20)
(50)

SR
1
1

AL
54
151

Freecell

(4,2,2,4)
(4,13,4,8)

0.36
0

15


1
0.47

10
112

Logistics

(1,2,2,6)
(3,10,2,30)

0.87
0

6


1
1

6
158

Elevator

(60,30)

1

112

1

98

Schedule

(50)

1

175

1

212

Briefcase

(10)
(50)

1
1

30
162

1
0

29


Gripper

(50)

1

149

1

149

Figure 5: Results standard problem distributions seven benchmarks. Success ratio
(SR) average length (AL) provided policy learned LRW
problem distribution. given domain, learned LRW policy used
problem size shown.

remaining domain, Blocks World, bootstrapping provided increasingly
long random walks appears particularly useful. policies learned walk
lengths 4, 14, 54, 334 increasingly effective target LRW distribution RW .
walks length 54 334, takes multiple iterations master provided level
difficulty beyond previous walk length. Finally, upon mastering walk length 334,
resulting policy appears perform well walk length. learned policy modestly
superior RW success ratio average length.
Evaluation Original Problem Distributions. domain denote
best learned LRW policyi.e., policy, domain, highest
performance RW , shown Figure 4. taxonomic decision lists corresponding
domain given Appendix B. Figure 5 shows performance ,
comparison FF, original intended problem distributions domains.
measured success ratio systems giving time limit 100 seconds solve
problem. attempted select largest problem sizes previously used
evaluation domain-specific planners, either AIPS-2000 Bacchus Kabanza
(2000), well show smaller problem size cases one planners
show performed poorly large size. case, use problem generators
provided domains, evaluate 100 problems size.
Overall, results indicate learned, reactive policies competitive
domain-independent planner FF. important remember policies
learned domain-independent fashion, thus LRW-API viewed general
approach generating domain-specific reactive planners. two domains, Blocks World
101

fiFern, Yoon, & Givan

Briefcase, learned policies substantially outperform success ratio, especially
large domain sizes. three domains, Elevator, Schedule, Gripper, two approaches perform quite similarly success ratio, approach superior average
length Schedule superior average length Elevator.
two domains, Logistics Freecell, substantially outperforms learned policies success ratio. believe partly due inadequate policy language,
discussed above. believe, however, another reason poor performance
long-random-walk distribution RW correspond well standard
problem distributions. seems particularly true Freecell. policy learned
Freecell (4,2,2,4) achieved success ratio 93 percent RW , however, standard distribution achieved 36 percent. suggests RW generates problems
significantly easier standard distribution. supported fact
solutions produced standard distribution average twice long
produced RW . One likely reason easy random walks
end dead states Freecell, actions applicable. Thus random walk
distribution typically produce many problems goals correspond dead
states. standard distribution hand treat dead states goals.
7.2 Probabilistic Planning Experiments
present experiments three probabilistic domains described probabilistic planning domain language PPDDL (Younes, 2003).
Ground Logistics (c, p) : probabilistic version logistics airplanes, c
cities p packages. driving action probability failure domain.
Colored Blocks World (n) : probabilistic blocks world n colored blocks,
goals involve constructing towers certain color patterns. probability
moved blocks fall floor.
Boxworld (c, p) : probabilistic version full logistics c cities p packages.
Transportation actions probability going wrong direction.
Ground Logistics domain originally Boutilier et al. (2001), used
evaluation Yoon et al. (2002). Colored Blocks World Boxworld domains
domains used hand-tailored track IPPC LRW-API technique
entered. hand-tailored track, participants provided problem generators
domain competition allowed incorporate domain knowledge
planner use competition time. provided problem generators LRW-API
learned policies domains, entered competition.
conducted experiments probabilistic domains Yoon et al.
(2002), including variants blocks world variant Ground Logistics,
appeared Fern et al. (2003). However, show results since
qualitatively identical deterministic blocks world results described
Ground Logistics results show below.
three probabilistic domains, conducted LRW experiments using
procedure above. parameters given LRW-API except
102

fin

SR

RW n
AL

iter. #

iter. #

API Policy Language Bias

RW
SR
AL

Boxworld (10,5)
1
10 0.73
4.3
2
10 0.93
2.3
3
20 0.91
4.4
4
40 0.96
6.1
5 170 0.62
30.8
37.9
6 170 0.49
7 170 0.63
29.3
29.1
8 170 0.63
9 170 0.48
36.4
Standard Distribution (15,15)

n

SR

RW n
AL

SR

RW
AL

Ground Logistics (3,4,4,3)
0.03
0.13
0.17
0.31
0.25
0.17
0.21
0.18
0.17
0

61.5
58.4
55.9
50.4
52.2
55.7
55
55.3
55.3


1
5 0.95
2
10 0.97
3 160
1
Standard Distribution

2.71
2.06
6.41
(5,7,7,20)

0.17
0.84
1
1

168.9
17.5
7.2
20

Colored Blocks World (10)
1
2
3
4
5

2 0.86
1.7
5 0.89
8.4
40 0.92
11.7
100 0.76
37.5
100 0.94
20.0
Standard Distribution (50)

0.19
0.81
0.85
0.77
0.95
0.95

93.6
40.8
32.7
38.5
21.9
123

Figure 6: Results iteration LRW-API three probabilistic planning domains.
iteration, show walk length n used learning, along success ratio
(SR) average length (AL) learned policy RW n RW .
benchmark, show performance standard problem distribution policy whose
performance best RW .
sampling width used rollout set w = 10, set 0.85 order
account stochasticity domains. results experiments shown
Figure 6. tables form Figure 4 last row given
domain gives performance standard distribution, i.e., problems drawn
domains problem generator. Colored Blocks World problem generator
produces problems whose goals specified using existential quantifiers. example,
simple goal may exists blocks x x red, blue x y.
Since policy language cannot directly handle existentially quantified goals preprocess
planning problems produced problem generator remove them. done
assigning particular block names existential variables, ensuring static
properties block (in case color) satisfied static properties variable
assigned to. domain, finding assignment trivial, resulting
assignment taken goal, giving planning problem learned policy
applied. Since blocks world states fully connected, resulting goal always
guaranteed achievable.

Boxworld, LRW-API able find good policy RW standard
distribution. Again, deterministic Logistics Freecell, believe
primarily restricted policy languages currently used learner.
Here, domains, see decision list learned Boxworld contains many
specific rules, indicating learner able generalize well beyond
103

fiFern, Yoon, & Givan

training trajectories. Ground Logistics, see LRW-API quickly finds good
policy RW standard distribution.
Colored Blocks World, see LRW-API able quickly find good
policy RW standard distribution. However, unlike deterministic
(uncolored) blocks world, success ratio observed less one, solving 95
percent problems. unclear, LRW-API able find perfect policy.
relatively easy hand-code policy Colored Blocks World using language
learner, hence inadequate knowledge representation answer. predicates
action types domain deterministic counterpart
stochastic variants previously considered. difference apparently
interacts badly learners search bias, causing fail find perfect policy.
Nevertheless, two results, along probabilistic planning results shown
here, indicate good policy expressible language, LRW-API
find good policies complex relational MDPs. makes LRW-API one
techniques simultaneously cope complexity resulting stochasticity
relational structure domains these.

8. Related Work
Boutilier et al. (2001) presented first exact solution technique relational MDPs
based structured dynamic programming. However, practical implementation
approach provided, primarily due need simplification first-order
logic formulas. ideas, however, served basis logic-programming-based
system (Kersting, Van Otterlo, & DeRaedt, 2004) successfully applied blocksworld problems involving simple goals simplified logistics world. style approach
inherently limited domains exact value functions and/or policies
compactly represented chosen knowledge representation. Unfortunately,
generally case types domains consider here, particularly planning
horizon grows. Nevertheless, providing techniques directly reason
MDP model important direction. Note API approach essentially ignores
underlying MDP model, simply interacts MDP simulator black box.
interesting research direction consider principled approximations techniques discover good policies difficult domains. considered
Guestrin et al. (2003a), class-based MDP value function representation
used compute approximate value function could generalize across different sets
objects. Promising empirical results shown multi-agent tactical battle domain.
Presently class-based representation support representation features commonly found classical planning domains (e.g., relational facts
on(a, b) change time), thus directly applicable contexts. However, extending work richer representations interesting direction. ability
reason globally domain may give advantages compared API.
approach closely related work relational reinforcement learning (RRL) (Dzeroski et al., 2001), form online API learns relational value-function approximations. Q-value functions learned form relational decision trees (Q-trees)
used learn corresponding policies (P -trees). RRL results clearly demonstrate
104

fiAPI Policy Language Bias

difficulty learning value-function approximations relational domains. Compared P trees, Q-trees tend generalize poorly much larger. RRL yet demonstrated
scalability problems complex considered hereprevious RRL blocks-world
experiments include relatively simple goals16 , lead value functions much
less complex ones here. reason, suspect RRL would difficulty
domains consider, precisely value-function approximation step
avoid; however, needs experimentally tested.
note, however, API approach advantage using unconstrained
simulator, whereas RRL learns irreversible world experience (pure RL). using
simulator, able estimate Q-values actions training state,
providing us rich training data. Without simulator, RRL able directly
estimate Q-value action training statethus, RRL learns Q-tree
provide estimates Q-value information needed learn P -tree. way, valuefunction learning serves critical role simulator unavailable. believe,
many relational planning problems, possible learn model simulator
world experiencein case, API approach incorporated planning
component RRL. Otherwise, finding ways either avoid learning effectively
learn relational value-functions RRL interesting research direction.
Researchers classical planning long studied techniques learning improve
planning performance. collection survey work learning planning domains see Minton (1993) Zimmerman Kambhampati (2003). Two primary approaches learn domain-specific control rules guiding search-based planners e.g.,
Minton, Carbonell, Knoblock, Kuokka, Etzioni, Gil (1989), Veloso, Carbonell, Perez,
Borrajo, Fink, Blythe (1995), Estlin Mooney (1996), Huang, Selman, Kautz
(2000), Ambite, Knoblock, Minton (2000), Aler, Borrajo, Isasi (2002), and,
closely related, learn domain-specific reactive control policies (Khardon, 1999a; Martin
& Geffner, 2000; Yoon et al., 2002).
Regarding latter, work novel using API iteratively improve stand-alone
control policies. Regarding former, theory, search-based planners iteratively
improved continually adding newly learned control knowledgehowever, difficult avoid utility problem (Minton, 1988), i.e., swamped low utility rules.
Critically, policy-language bias confronts issue preferring simpler policies.
learning approach tied base planner (let alone tied single particular base planner), unlike previous work. Rather, require domain simulator.
ultimate goal systems allow planning large, difficult problems
beyond reach domain-independent planning technology. Clearly, learning
achieve goal requires form bootstrapping almost previous systems
relied human purpose. far, common human-bootstrapping
approach learning small problems. Here, human provides small problem
distribution learner, limiting number objects (e.g., using 2-5 blocks
blocks world), control knowledge learned small problems. approach
work, human must ensure small distribution good control knowledge
small problems good large target distribution. contrast, long16. complex blocks-world goal RRL achieve on(A, B) n block environment.
consider blocks-world goals involve n blocks.

105

fiFern, Yoon, & Givan

random-walk bootstrapping approach applied without human assistance directly
large planning domains. However, already pointed out, goal performing well
LRW distribution may always correspond well particular target problem
distribution.
bootstrapping approach similar spirit bootstrapping framework learning exercises(Natarajan, 1989; Reddy & Tadepalli, 1997). Here, learner provided planning problems, exercises, order increasing difficulty. learning
easier problems, learner able use new knowledge, skills, order bootstrap learning harder problems. work, however, previously relied
human provide exercises, typically requires insight planning domain
underlying form control knowledge planner. work viewed
automatic instantiation learning exercises, specifically designed learning LRW
policies.
random-walk bootstrapping similar approach used Micro-Hillary
(Finkelstein & Markovitch, 1998), macro-learning system problem solving.
work, instead generating problems via random walks starting initial state, random
walks generated backward goal states. approach assumes actions
invertible given set backward actions. assumptions hold,
backward random-walk approach may preferable provided goal
distribution match well goals generated forward random walks.
course, cases forward random walks may preferable. Micro-Hillary
empirically tested N N sliding-puzzle domain; however, discussed work,
remain challenges applying system complex domains parameterized actions recursive structure, familiar STRIPS domains. best
knowledge, idea learning random walks previously explored
context STRIPS planning domains.
idea searching good policy directly policy space rather value-function
space primary motivation policy-gradient RL algorithms. However, algorithms
largely explored context parametric policy spaces. approach
demonstrated impressive success number domains, appears difficult define
policy spaces types planning problem considered here.
API approach viewed type reduction planning reinforcement
learning classification learning. is, solve MDP generating solving
series cost-sensitive classification problems. Recently, several
proposals reducing reinforcement learning classification. Dietterich Wang (2001)
proposed reinforcement learning approach based batch value function approximation.
One proposed approximations enforced learned approximation assign
best action highest value, type classifier learning. Lagoudakis Parr
(2003) proposed classification-based API approach closely related ours. primary difference form classification problem produced iteration.
generate standard multi-class classification problems, whereas generate cost-sensitive
problems. Bagnell, Kakade, Ng, Schneider (2003) introduced closely related algorithm learning non-stationary policies reinforcement learning. specified horizon
time h, approach learns sequence h policies. iteration, policies
held fixed except one, optimized forming classification problem via policy
106

fiAPI Policy Language Bias

rollout17 . Finally, Langford Zadrozny (2004) provide formal reduction reinforcement learning classification, showing -accurate classification learning implies
near-optimal reinforcement learning. approach uses optimistic variant sparse
sampling generate h classification problems, one horizon time step.

9. Summary Future Work
introduced new variant API learns policies directly, without representing
approximate value functions. allowed us utilize relational policy language
learning compact policy representations. introduced new API bootstrapping
technique goal-based planning domains. experiments show LRW-API
algorithm, combines techniques, able find good policies variety
relational MDPs corresponding classical planning domains stochastic variants.
know previous MDP technique successfully applied problems
these.
experiments pointed number weaknesses current approach. First,
bootstrapping technique, based long random walks, always correspond
well problem distribution interest. Investigating automatic bootstrapping
techniques interesting direction, related general problems exploration
reward shaping reinforcement learning. Second, seen limitations
current policy language learner partly responsible failures
system. cases, must either: 1) depend human provide useful features
system, 2) extend policy language develop advanced learning techniques. Policy-language extensions considering include various extensions
knowledge representation used represent sets objects domain (in particular,
route-finding maps/grids), well non-reactive policies incorporate search
decision-making.
consider ever complex planning domains, inevitable brute-force
enumeration approach learning policies trajectories scale. Presently
policy learner, well entire API technique, makes attempt use definition
domain one available. believe developing learner exploit
information bias search good policies important direction future work.
Recently, Gretton Thiebaux (2004) taken step direction using logical
regression (based domain model) generate candidate rules learner. Developing tractable variations approach promising research direction. addition,
exploring ways incorporating domain model approach modelblind approaches critical. Ultimately, scalable AI planning systems need combine
experience stronger forms explicit reasoning.

17. initial state distribution dictated policies previous time steps, held fixed.
Likewise actions selected along rollout trajectories dictated policies future time steps,
held fixed.

107

fiFern, Yoon, & Givan

Acknowledgments
would thank Lin Zhu originally suggesting idea using random walks
bootstrapping. would thank reviewers editors helping
vastly improve paper. work supported part NSF grants 9977981-IIS
0093100-IIS.

Appendix A. Omitted Proofs
Proposition 1. Let H finite class deterministic policies. H,

set n = 1 ln |H|
trajectories drawn independently Dh , 1 probability
every H consistent trajectories satisfies V () V () 2Vmax ( + h ).
Proof: first introduce basic properties notation used below.
deterministic policy , consistent trajectory t, Dh (t) entirely
determined underlying MDP transition dynamics. implies two deterministic policies 0 consistent trajectory Dh (t) = Dh (t).
denote v(t) cumulative discounted reward accumulated executing trajectory
P
t. policy , V h () = Dh (t) v(t) summation taken
length h trajectories (or simply consistent ). Finally set
P
trajectories let Dh () = t0 Dh (t) giving cumulative probability
generating trajectories .
Consider particular H H consistent n trajectories
. let denote set length h trajectories consistent
denote set trajectories consistent . Following Khardon (1999b)
first give standard argument showing high probability Dh () > 1 . see
consider probability consistent n = 1 ln |H|
trajectories


given Dh () 1 . probability occurs (1 )n < en = |H|
.

Thus probability choosing |H| |H|
= . Thus, probability

least 1 know Dh () > 1 . Note Dh () = Dh ().
given condition Dh () > 1 show V h () V h () 2Vmax
considering difference two value functions.
V h () V h () =

X

Dh (t) v(t)



=

X

=

v(t) +

X

(Dh (t) Dh (t)) v(t)



Dh (t)





Dh (t) v(t)



Dh (t)



X

X

v(t) + 0



Dh (t)



) + Dh ( )]

Dh () + 1 Dh ()]

Vmax [Dh (

= Vmax [1

X

2Vmax
108

X

v(t)

Dh (t) v(t)

fiAPI Policy Language Bias

third lines follows since Dh (t) = Dh (t) consistent t.
last line follows substituting assumption Dh () = Dh () > 1 previous
line. Combining result approximation due using finite horizon,
V () V () V h () V h () + 2 h Vmax
get probability least 1 , V () V () 2Vmax ( + h ), completes
proof. 2
Proposition 2.


MDP Q-advantage least , 0 < 0 < 1,

h > log


8Vmax

8Vmax



2



w >
=

2

ln

|A|
0

state s, A(, s) = (s) probability least 1 0 .
Proof: Given real valued random variable X bounded absolute value Xmax
average X w independently drawn samples X,
q additive Chernoff bound states
probability least 1 , |E[X] X| Xmax wln .
Note Qh (s, a) expectation random variable X(s, a) = R(s, a) +

Vh1 (T (s, a)) Q(s, a) simply average w independent samples X(s, a).
0
Chernoff bound tells us probability least 1 |A|
, |Qh (s, a) Q(s, a)|
q

0


Vmax ln |A|ln
, |A| number actions. Substituting choice w
w

get probability least 1 0 , |Qh (s, a) Q(s, a)| < 8 satisfied actions
simultaneously. know |Q (s, a) Qh (s, a)| h Vmax , choice

h gives, |Q (s, a) Qh (s, a)| < 8 . Combining relationships get

probability least 1 0 , |Q (s, a) Q(s, a)| < 4 holds actions simultaneously.
use bound show high probability Q-value estimates

actions (s) within 2 range other, actions outside
range. particular, consider action (s) action a0 .
a0 (s) Q (s, a) = Q (s, a0 ). bound get

|Q(s, a) Q(s, a0 )| < 2 . Otherwise a0 6 (s) assumption MDP
Q-advantage get Q (s, a) Q (s, a0 ) . Using bound implies

Q(s, a) Q(s, a0 ) > 2 . relationships definition A(, s) imply
probability least 1 0 A(, s) = (s). 2

Appendix B. Learned Policies
give final taxonomic-decision-list policies learned domain
experiments. Rather write rules form a(x1 , . . . , xk ) : L1 L2 Lm
109

fiFern, Yoon, & Givan

drop variables head simply write, : L1 L2 Lm . addition
use notation R short-hand (R1 ) R relation. interpreting policies, important remember rule action type a,
preconditions action type implicitly included constraints. Thus, rules
often allow actions legal, actions never considered
system.
Gripper
1. MOVE: (X1 (NOT (GAT (CARRY1 GRIPPER)))) (X2 (NOT (GAT (AT1 AT-ROBBY)))) (X2 (GAT (NOT
(CAT1 ROOM)))) (X1 (CAT BALL))
2. DROP: (X1 (GAT1 AT-ROBBY))
3. PICK: (X1 (GAT1 (GAT (CARRY1 GRIPPER)))) (X1 (GAT1 (NOT AT-ROBBY)))
4. PICK: (X2 (AT (NOT (GAT1 ROOM)))) (X1 (GAT1 (NOT AT-ROBBY)))
5. PICK: (X1 (GAT1 (NOT AT-ROBBY)))
Briefcase
1. PUT-IN: (X1 (GAT1 (NOT IS-AT)))
2. MOVE: (X2 (AT (NOT (CAT1 LOCATION)))) (X2 (NOT (AT (GAT1 CIS-AT))))
3. MOVE: (X2 (GAT IN)) (X1 (NOT (CAT IN)))
4. TAKE-OUT: (X1 (CAT1 IS-AT))
5. MOVE: (X2 GIS-AT)
6. MOVE: (X2 (AT (GAT1 CIS-AT)))
7. PUT-IN: (X1 UNIVERSAL)
Schedule
1. DO-IMMERSION-PAINT: (X1 (NOT (PAINTED1 X2 ))) (X1 (GPAINTED1 X2 ))
2. DO-DRILL-PRESS: (X1 (GHAS-HOLEO1 X3 )) (X1 (GHAS-HOLEW1 X2 ))
3. DO-LATHE: (X1 (NOT (SHAPE1 CYLINDRICAL))) (X1 (GSHAPE1 CYLINDRICAL))
4. DO-DRILL-PRESS: (X1 (GHAS-HOLEW1 X2 ))
5. DO-DRILL-PRESS: (X1 (GHAS-HOLEO1 X3 ))
6. DO-GRIND: (X1 (NOT (SURFACE-CONDITION1 SMOOTH))) (X1 (GSURFACE-CONDITION1 SMOOTH))
7. DO-POLISH: (X1 (NOT (SURFACE-CONDITION1 POLISHED))) (X1 (GSURFACE-CONDITION1 POLISHED))
8. DO-TIME-STEP:
Elevator
1. DEPART: (X2 GSERVED)
2. DOWN: (X2 (DESTIN BOARDED)) (X2 (DESTIN GSERVED))
3. UP: (X2 (DESTIN BOARDED)) (X2 (DESTIN GSERVED)) (X2 (ABOVE (ORIGIN BOARDED))) (X1 (NOT
(DESTIN BOARDED)))
4. BOARD: (X2 (NOT CSERVED)) (X2 GSERVED)
5. UP: (X2 (ORIGIN GSERVED)) (X2 (NOT (DESTIN BOARDED))) (X2 (NOT (DESTIN GSERVED))) (X2
(ORIGIN (NOT CSERVED))) (X2 (ABOVE (DESTIN PASSENGER))) (X1 (NOT (DESTIN BOARDED)))
6. DOWN: (X2 (ORIGIN GSERVED)) (X2 (ORIGIN (NOT CSERVED))) (X1 (NOT (DESTIN BOARDED)))

110

fiAPI Policy Language Bias

7. UP: (X2 (NOT (ORIGIN BOARDED))) (X2 (NOT (DESTIN BOARDED)))
FreeCell
1. SENDTOHOME: (X1 (CANSTACK1 (CANSTACK (SUIT1 (SUIT INCELL))))) (X5 (NOT GHOME))
2. MOVE-B: (X2 (NOT (CANSTACK (ON GHOME)))) (X2 (CANSTACK GHOME)) (X2 (VALUE1 (NOT
COLSPACE))) (X1 (CANSTACK1 (SUIT1 (SUIT BOTTOMCOL))))
3. MOVE: (X1 (CANSTACK1 (ON (CANSTACK1 (ON1 GHOME))))) (X3 (CANSTACK (ON (SUIT1 (SUIT BOTTOMCOL))))) (X1 (ON1 BOTTOMCOL)) (X1 (CANSTACK1 (ON GHOME))) (X3 (ON1 (CANSTACK1
(ON1 (NOT (CANSTACK (VALUE1 CELLSPACE))))))) (X1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL)))))
(X3 (CANSTACK BOTTOMCOL)) (X1 (SUIT1 (SUIT (ON1 (NOT (CANSTACK (VALUE1 CELLSPACE)))))))
(X1 (VALUE1 (NOT COLSPACE))) (0 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X1 (NOT
(CANSTACK1 CHOME)))
4. SENDTOHOME-B: (X4 (NOT GHOME))
5. SENDTOHOME: (X1 (ON1 (CANSTACK (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X5 (NOT GHOME))
6. SENDTOHOME: (X1 (ON1 (ON1 GHOME))) (X1 (CANSTACK1 (NOT GHOME))) (X1 (CANSTACK1 (NOT
(ON1 GHOME)))) (X5 (NOT GHOME))
7. MOVE-B: (X1 (NOT (CANSTACK1 GHOME))) (X2 (VALUE1 (NOT COLSPACE))) (X1 (CANSTACK1
(SUIT1 (SUIT BOTTOMCOL))))
8. SENDTOFREE: (X1 (ON1 (ON1 GHOME))) (X1 (NOT GHOME))
9. SENDTOHOME: (X5 (CANSTACK1 (CANSTACK (ON GHOME)))) (X5 (NOT GHOME))
10. SENDTOHOME: (0 GHOME) (X5 (VALUE1 (NOT COLSPACE))) (X5 (NOT (CANSTACK1 (ON1 (NOT
GHOME))))) (X1 (ON1 (NOT (ON1 GHOME)))) (X5 (NOT GHOME))
11. NEWCOLFROMFREECELL: (X1 GHOME)
12. SENDTOHOME: (X5 (CANSTACK1 (ON GHOME))) (X1 GHOME) (X5 (NOT GHOME))
13. MOVE-B: (X1 (VALUE1 (VALUE HOME))) (X2 (VALUE1 (NOT COLSPACE))) (X1 (CANSTACK1 (SUIT1
(SUIT BOTTOMCOL))))
14. SENDTOHOME: (X1 (CANSTACK1 (ON1 (CANSTACK1 (SUIT1 (SUIT INCELL)))))) (X5 (NOT GHOME))
15. SENDTOHOME: (X1 (ON1 (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X5 (NOT GHOME))
16. SENDTOFREE: (X1 (CANSTACK1 (ON (ON1 GHOME)))) (X1 (SUIT1 (SUIT BOTTOMCOL))) (X1 (ON1
BOTTOMCOL))
17. MOVE: (X3 (ON1 (CANSTACK1 CLEAR))) (X1 (ON1 (CANSTACK (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE))))))) (X3 (NOT GHOME)) (X1 GHOME) (X3 (CANSTACK BOTTOMCOL)) (X3 (ON1
(CANSTACK1 (ON1 (NOT (CANSTACK (VALUE1 CELLSPACE))))))) (X1 (NOT (CANSTACK1 (SUIT1
(SUIT INCELL))))) (X1 (ON1 BOTTOMCOL)) (X1 (SUIT1 (SUIT (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE))))))) (X1 (VALUE1 (NOT COLSPACE))) (X1 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT
INCELL)))))) (X1 (NOT (CANSTACK1 CHOME)))
18. MOVE: (X1 (SUIT1 (SUIT CHOME))) (X3 (NOT GHOME)) (X3 (NOT (ON1 GHOME))) (X1 (ON1
(CANSTACK1 BOTTOMCOL)))
19. SENDTOHOME: (X1 (CANSTACK (ON (CANSTACK (ON GHOME))))) (X1 GHOME) (X5 (NOT GHOME))
20. SENDTOHOME: (X1 (CANSTACK1 (ON (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (SUIT1 (SUIT BOTTOMCOL)))) (X5 (NOT GHOME))
21. SENDTOFREE: (X1 (CANSTACK (ON (CANSTACK (VALUE1 CELLSPACE))))) (X1 (CANSTACK CHOME))
22. SENDTOHOME: (X1 (CANSTACK1 (SUIT1 (SUIT INCELL)))) (X1 (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE))))) (X5 (NOT GHOME))
23. SENDTONEWCOL: (X1 (CANSTACK (CANSTACK1 (ON1 GHOME))))
24. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (CANSTACK GHOME)))
(X1 (NOT (ON1 GHOME))) (X1 (ON1 (NOT (CANSTACK1 (SUIT1 (SUIT INCELL))))))

111

fiFern, Yoon, & Givan

25. SENDTOFREE: (X1 (ON1 (CANSTACK (CANSTACK1 (ON1 GHOME))))) (X1 (NOT (CANSTACK BOTTOMCOL))) (X1 (NOT (CANSTACK1 (CANSTACK (ON GHOME)))))
26. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (NOT (CANSTACK
GHOME))) (X1 (CANSTACK (NOT (SUIT1 (SUIT BOTTOMCOL)))))
27. SENDTOHOME: (X1 (CANSTACK1 (CANSTACK (ON1 GHOME)))) (X1 (ON1 (CANSTACK1 (ON1 (NOT
GHOME))))) (X1 (NOT GHOME)) (X5 (NOT GHOME))
28. SENDTOFREE: (X1 (CANSTACK (ON1 (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (CANSTACK (CANSTACK1
(ON1 GHOME)))) (X1 (NOT GHOME)) (X1 (ON1 (CANSTACK1 (ON1 (NOT (CANSTACK (VALUE1
CELLSPACE)))))))
29. SENDTOFREE: (X1 (CANSTACK CHOME)) (X1 (SUIT1 (SUIT (CANSTACK1 (ON1 GHOME)))))
30. SENDTOHOME: (X1 GHOME) (X1 (SUIT1 (SUIT BOTTOMCOL))) (X1 (CANSTACK1 (NOT (ON1
GHOME)))) (X5 (NOT GHOME))
31. SENDTOFREE: (X1 (CANSTACK1 (ON1 GHOME))) (X1 (CANSTACK1 (ON1 (NOT GHOME))))
32. SENDTOFREE: (X1 (CANSTACK (ON1 GHOME))) (X1 (NOT GHOME)) (X1 (ON1 (CANSTACK1 (ON1
(NOT GHOME)))))
33. SENDTOHOME: (X1 (ON1 (CANSTACK1 BOTTOMCOL))) (X1 (CANSTACK1 (NOT GHOME))) (X5
(NOT GHOME))
34. SENDTOFREE: (X1 (CANSTACK (ON (CANSTACK1 (ON1 (NOT GHOME)))))) (X1 (NOT (SUIT1 (SUIT
BOTTOMCOL)))) (X1 (NOT GHOME))
35. SENDTOHOME: (X1 (NOT (CANSTACK1 GHOME))) (X1 (NOT (SUIT1 (SUIT BOTTOMCOL)))) (X5
(NOT GHOME))
36. SENDTOFREE: (X1 (NOT (ON1 GHOME))) (X1 (CANSTACK (CANSTACK1 (ON1 (NOT GHOME)))))
37. SENDTOFREE-B: (X1 (NOT GHOME))
38. SENDTOFREE: (X1 UNIVERSAL)
Logistics
1. FLY-AIRPLANE: (X1 (IN (GAT1 AIRPORT))) (X1 (NOT (IN (GAT1 (AT AIRPLANE))))) (X3 (NOT (GAT
(IN1 TRUCK)))) (X1 (NOT (IN (GAT1 (NOT AIRPORT)))))
2. LOAD-TRUCK: (X2 (IN (NOT (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (GAT (IN1 TRUCK)))) (X1 (NOT
(CAT1 LOCATION)))
3. DRIVE-TRUCK: (X3 (AT (AT1 (GAT (IN1 TRUCK))))) (X3 (IN-CITY1 (IN-CITY (AT AIRPLANE)))) (X1
(AT1 (NOT (GAT (IN1 TRUCK)))))
4. UNLOAD-TRUCK: (X1 (GAT1 (AT (IN OBJ)))) (X1 (GAT1 (AT OBJ))) (X1 (NOT (GAT1 (AT AIRPLANE)))) (X2 (AT1 (GAT (IN1 TRUCK)))) (X1 (GAT1 (AT TRUCK)))
5. FLY-AIRPLANE: (X3 (GAT (IN1 AIRPLANE))) (X1 (IN (NOT (GAT1 (AT TRUCK))))) (X1 (AT1 (NOT
(GAT (IN1 TRUCK)))))
6. UNLOAD-AIRPLANE: (X2 (NOT (IN (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (AT AIRPLANE)))
7. LOAD-TRUCK: (X2 (IN (NOT (GAT1 LOCATION)))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1 (GAT1
LOCATION))
8. UNLOAD-TRUCK: (X1 (GAT1 (AT TRUCK))) (X2 (AT1 AIRPORT)) (X2 (NOT (IN (GAT1 (NOT AIRPORT))))) (X1 (GAT1 (AT AIRPLANE)))
9. FLY-AIRPLANE: (X3 (AT (AT1 (GAT (IN1 TRUCK))))) (X1 (AT1 (GAT (GAT1 LOCATION)))) (X1
(NOT (AT1 (CAT OBJ))))
10. DRIVE-TRUCK: (X1 (IN (GAT1 LOCATION))) (X1 (AT1 (NOT (GAT (IN1 TRUCK))))) (X1 (AT1 (NOT
(AT AIRPLANE))))
11. UNLOAD-TRUCK: (X2 (AT1 (GAT (GAT1 (NOT AIRPORT))))) (X1 (NOT (GAT1 AIRPORT)))
12. FLY-AIRPLANE: (X3 (NOT (GAT (GAT1 LOCATION)))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))) (X3 (AT
(NOT (GAT1 (AT AIRPLANE))))) (X3 (AT OBJ)) (X1 (NOT (IN (GAT1 AIRPORT)))) (X3 (NOT (AT
(IN OBJ))))

112

fiAPI Policy Language Bias

13. UNLOAD-TRUCK: (X1 (GAT1 AIRPORT))
14. LOAD-TRUCK: (X1 (AT1 (CAT (GAT1 (AT AIRPLANE))))) (X1 (NOT (GAT1 LOCATION)))
15. LOAD-TRUCK: (X1 (GAT1 (CAT (GAT1 (AT AIRPLANE))))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1
(GAT1 (AT (GAT1 (AT AIRPLANE)))))
16. LOAD-TRUCK: (X1 (GAT1 (NOT AIRPORT))) (X1 (NOT (GAT1 (AT TRUCK))))
17. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (AT1 (CAT OBJ)))
18. FLY-AIRPLANE: (X3 (NOT (GAT (AT1 (CAT OBJ))))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))) (X1 (AT1
(GAT (GAT1 (AT TRUCK)))))
19. LOAD-TRUCK: (X1 (GAT1 (AT AIRPLANE))) (X1 (NOT (GAT1 (AT TRUCK)))) (X1 (AT1 (CAT OBJ)))
20. LOAD-AIRPLANE: (X1 (GAT1 AIRPORT)) (X1 (NOT (CAT1 LOCATION))) (X1 (GAT1 (NOT (AT
AIRPLANE)))) (X2 (NOT (IN (GAT1 (NOT AIRPORT)))))
21. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X3 (NOT (AT TRUCK)))
22. LOAD-TRUCK: (X1 (AT1 (CAT (GAT1 (NOT AIRPORT))))) (X1 (GAT1 AIRPORT))
23. DRIVE-TRUCK: (X3 (NOT (AT OBJ))) (X1 (NOT (AT1 (CAT OBJ)))) (X1 (AT1 (GAT (GAT1 LOCATION))))
24. LOAD-TRUCK: (X1 (GAT1 (CAT (CAT1 AIRPORT)))) (X1 (NOT (CAT1 LOCATION)))
25. FLY-AIRPLANE: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (AT1 (AT OBJ)))
26. DRIVE-TRUCK: (X1 (IN OBJ))
27. DRIVE-TRUCK: (X1 (AT1 (GAT (GAT1 AIRPORT)))) (X3 (AT (GAT1 AIRPORT))) (X1 (AT1 (NOT
(AT AIRPLANE))))
28. FLY-AIRPLANE: (X3 (CAT (GAT1 (AT TRUCK)))) (X1 (AT1 (GAT (GAT1 LOCATION))))
29. LOAD-TRUCK: (X1 (GAT1 (AT OBJ))) (X1 (NOT (CAT1 LOCATION)))
30. DRIVE-TRUCK: (X3 (AT (GAT1 (AT AIRPLANE)))) (X1 (NOT (AT1 (CAT OBJ))))
31. DRIVE-TRUCK: (X3 (AT AIRPLANE)) (X3 (AT (GAT1 (AT TRUCK))))
32. UNLOAD-AIRPLANE: (X2 (NOT (AT1 (CAT OBJ)))) (X1 (GAT1 (NOT AIRPORT)))
33. DRIVE-TRUCK: (X3 (AT (GAT1 (AT TRUCK))))
34. LOAD-TRUCK: (X1 (AT1 (NOT AIRPORT))) (X1 (GAT1 AIRPORT))
35. FLY-AIRPLANE: (X3 (AT (GAT1 LOCATION)))
36. FLY-AIRPLANE: (X1 (IN OBJ)) (X3 (NOT (GAT (GAT1 LOCATION)))) (X1 (NOT (IN (GAT1 AIRPORT))))
(X3 (NOT (AT (IN OBJ)))) (X1 (AT1 (GAT (AT1 (CAT OBJ))))))
37. DRIVE-TRUCK: (X1 (AT1 (AT AIRPLANE)))
38. LOAD-AIRPLANE: (X1 (GAT1 (NOT AIRPORT)))
Blocks World
1. STACK: (X2 (GON HOLDING)) (X2 (CON (MIN GON))) (X1 (GON ON-TABLE))
2. PUTDOWN:
3. UNSTACK: (X1 (ON (ON (MIN GON)))) (X2 (CON (ON (MIN GON))))
4. UNSTACK: (X2 (ON1 (GON CLEAR))) (X2 (GON (ON (MIN GON)))) (X1 (ON (GON ON-TABLE)))
(X1 (GON (NOT CLEAR)))
5. PICKUP: (X1 (GON1 (CON (MIN GON)))) (X1 (GON1 CLEAR)) (X1 (GON1 (CON ON-TABLE)))
6. UNSTACK: (X2 (CON (GON1 CLEAR))) (X1 (GON1 (ON (MIN GON)))) (X1 (GON1 (CON
CLEAR)))

113

fiFern, Yoon, & Givan

7. UNSTACK: (X1 (NOT (GON (MIN GON))))
8. UNSTACK: (X2 (GON ON-TABLE)) (X1 (GON1 (CON (MIN GON)))) (X1 (GON1 CLEAR))
9. UNSTACK: (X1 (NOT (CON (MIN GON)))) (X2 (ON (GON1 ON-TABLE))) (X2 (GON (NOT ONTABLE))) (X1 (GON (GON ON-TABLE))) (X1 (GON (NOT CLEAR)))
10. UNSTACK: (X2 (NOT (CON CLEAR))) (X1 (GON1 (CON ON-TABLE)))
11. UNSTACK: (X1 (GON1 CLEAR)) (X1 (ON (ON (MIN GON)))
Ground Logistics
1. LOAD: (X2 (NOT (IN (GIN1 CITY)))) (X1 (NOT (CIN1 CITY))) (X1 (GIN1 CITY))
2. UNLOAD: (X1 (GIN1 X3 ))
3. DRIVE: (X1 (IN (GIN1 X3 )))
4. DRIVE: (X3 (NOT (GIN BLOCK))) (X3 (IN (GIN1 CITY))) (X1 CAR) (X2 CLEAR)
5. DRIVE: (X3 (IN (GIN1 RAIN))) (X1 TRUCK)
Colored Blocks World
1. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GON-TOP-OF1 (ON-TOP-OF BLOCK)))
2. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF1 (CON-TOP-OF1 BLOCK))) (X2 (GON-TOP-OF HOLDING))
(X2 (CON-TOP-OF TABLE))
3. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF BLOCK))) (X1 (ON-TOP-OF (GON-TOP-OF1 TABLE)))
(X2 (GON-TOP-OF (GON-TOP-OF1 BLOCK))) (X2 (NOT (CON-TOP-OF1 BLOCK))) (X2 (ON-TOPOF1 (GON-TOP-OF BLOCK))) (X1 (GON-TOP-OF (GON-TOP-OF1 BLOCK)))
4. PICK-UP-BLOCK-FROM: (X1 (NOT (CON-TOP-OF TABLE))) (X1 (GON-TOP-OF1 (CON-TOP-OF TABLE))) (X1 (GON-TOP-OF (ON-TOP-OF1 BLOCK)))
5. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF1 (ON-TOP-OF1 TABLE))) (X2 (GON-TOP-OF HOLDING)) (X2
(CON-TOP-OF TABLE))
6. PUT-DOWN-BLOCK-ON: (X2 (CON-TOP-OF (ON-TOP-OF BLOCK))) (X1 (GON-TOP-OF1 (GON-TOP-OF1
BLOCK)))
7. PUT-DOWN-BLOCK-ON: (X2 (GON-TOP-OF HOLDING)) (X2 (CON-TOP-OF TABLE))
8. PUT-DOWN-BLOCK-ON: (X2 TABLE)
9. PICK-UP-BLOCK-FROM: (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GON-TOP-OF1 (CON-TOP-OF TABLE)))
10. PICK-UP-BLOCK-FROM: (X1 (GON-TOP-OF1 (CON-TOP-OF1 TABLE))) (X2 TABLE) (X1 (GON-TOP-OF
(GON-TOP-OF BLOCK))) (X1 (GON-TOP-OF (ON-TOP-OF1 TABLE)))
11. PICK-UP-BLOCK-FROM: (X2 (ON-TOP-OF (CON-TOP-OF BLOCK))) (X1 (GON-TOP-OF1 (CON-TOP-OF1
TABLE)))
12. PICK-UP-BLOCK-FROM: (X2 (ON-TOP-OF1 BLOCK)) (X2 (NOT (CON-TOP-OF TABLE))) (X2 (GONTOP-OF (ON-TOP-OF1 BLOCK))) (X2 (GON-TOP-OF (ON-TOP-OF1 BLOCK)))
13. PICK-UP-BLOCK-FROM: (X1 (GON-TOP-OF1 (GON-TOP-OF1 TABLE)))
Boxworld
1. DRIVE-TRUCK: (X2 (GBOX-AT-CITY (BOX-AT-CITY1 X3 ))) (X3 (NOT (CAN-FLY (TRUCK-AT-CITY (NOT
PREVIOUS))))) (X3 (CAN-DRIVE1 PREVIOUS)) (X2 (NOT (CAN-FLY (TRUCK-AT-CITY (NOT PREVIOUS))))) (X3 (NOT (CAN-FLY (BOX-AT-CITY BOX)))) (X2 (CAN-DRIVE (CAN-DRIVE (BOX-AT-CITY BOX))))
(X3 (NOT (CAN-FLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))
2. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 (TRUCK-AT-CITY PREVIOUS))) (X3 (GBOXAT-CITY BOX)) (X3 (NOT (BOX-AT-CITY PREVIOUS))) (X1 (GBOX-AT-CITY1 (CAN-DRIVE1 (CANDRIVE1 (CAN-FLY CITY))))) (X2 (BOX-ON-TRUCK (GBOX-AT-CITY1 PREVIOUS)))
3. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 X3 ))) (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY
(BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))

114

fiAPI Policy Language Bias

4. DRIVE-TRUCK: (X3 (CAN-DRIVE (BOX-AT-CITY PREVIOUS))) (X2 (CAN-FLY (CAN-DRIVE1 (BOX-AT-CITY
BOX)))) (X3 (CAN-DRIVE (CAN-FLY (TRUCK-AT-CITY TRUCK)))) (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY
(BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X2 PREVIOUS) (X2 (CAN-DRIVE (CAN-DRIVE X3 ))) (X3
(NOT (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))) (X3 (NOT (CAN-FLY PREVIOUS)))
(X3 (CAN-DRIVE (NOT (BOX-AT-CITY BOX)))) (X2 (CAN-DRIVE (CAN-DRIVE1 X3 ))) (X3 (CAN-DRIVE
(NOT (TRUCK-AT-CITY TRUCK))))
5. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 (CAN-DRIVE (TRUCK-AT-CITY TRUCK)))) (X3 (NOT
(PLANE-AT-CITY PREVIOUS))) (X3 (CAN-DRIVE (CAN-DRIVE1 (CAN-FLY CITY)))) (X3 (CAN-DRIVE1
(NOT (TRUCK-AT-CITY (NOT PREVIOUS)))))
6. UNLOAD-BOX-FROM-TRUCK-IN-CITY: (X3 (GBOX-AT-CITY (BOX-ON-TRUCK1 TRUCK))) (X3 (NOT (CANFLY (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X1 (GBOX-AT-CITY1 CITY))
7. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 PREVIOUS))) (X3 (CAN-DRIVE (GBOX-AT-CITY
(GBOX-AT-CITY1 PREVIOUS)))) (X3 (NOT (PLANE-AT-CITY PLANE))) (X2 (NOT (CAN-FLY (GBOX-ATCITY (GBOX-AT-CITY1 PREVIOUS)))))
8. FLY-PLANE: (X1 (BOX-ON-PLANE (GBOX-AT-CITY1 X3 )))
9. UNLOAD-BOX-FROM-PLANE-IN-CITY: (X1 (GBOX-AT-CITY1 PREVIOUS))
10. FLY-PLANE: (X2 (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY)))))) (X2
(GBOX-AT-CITY BOX)) (X3 (NOT (PLANE-AT-CITY PREVIOUS))) (X1 (NOT PREVIOUS))
11. LOAD-BOX-ON-PLANE-IN-CITY: (X1 (GBOX-AT-CITY1 (CAN-FLY PREVIOUS))) (X3 (NOT (TRUCK-AT-CITY
(NOT PREVIOUS)))) (X3 (NOT (CAN-DRIVE (TRUCK-AT-CITY (BOX-ON-TRUCK (GBOX-AT-CITY1 CITY))))))
12. DRIVE-TRUCK: (X1 (BOX-ON-TRUCK (GBOX-AT-CITY1 X3 ))) (X2 (NOT (CAN-DRIVE (CAN-FLY PREVIOUS)))) (X2 (CAN-DRIVE1 (CAN-FLY CITY)))
13. LOAD-BOX-ON-TRUCK-IN-CITY: (X1 (GBOX-AT-CITY1 PREVIOUS))

References
Aler, R., Borrajo, D., & Isasi, P. (2002). Using genetic programming learn improve
control knowledge. Artificial Intelligence, 141 (1-2), 2956.
Ambite, J. L., Knoblock, C. A., & Minton, S. (2000). Learning plan rewriting rules.
Artificial Intelligence Planning Systems, pp. 312.
Bacchus, F. (2001). AIPS 00 planning competition. AI Magazine, 22(3)(3), 5762.
Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge planning. Artificial Intelligence, 16, 123191.
Bagnell, J., Kakade, S., Ng, A., & Schneider, J. (2003). Policy search dynamic programming. Proceedings 16th Conference Advances Neural Information
Processing.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamic
programming. Saitta, L. (Ed.), International Conference Machine Learning.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121 (1-2), 49107.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order
MDPs. International Joint Conference Artificial Intelligence.
115

fiFern, Yoon, & Givan

Dean, T., & Givan, R. (1997). Model minimization markov decision processes. National
Conference Artificial Intelligence, pp. 106111.
Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques computing approximately optimal solutions Markov decision processes. Conference Uncertainty
Artificial Intelligence, pp. 124131.
Dietterich, T., & Wang, X. (2001). Batch value function approximation via support vectors.
Proceedings Conference Advances Neural Information Processing.
Dzeroski, S., DeRaedt, L., & Driessens, K. (2001). Relational reinforcement learning. Machine Learning, 43, 752.
Estlin, T. A., & Mooney, R. J. (1996). Multi-strategy learning search control partialorder planning. National Conference Artificial Intelligence.
Fern, A., Yoon, S., & Givan, R. (2003). Approximate policy iteration policy language bias. Proceedings 16th Conference Advances Neural Information
Processing.
Finkelstein, L., & Markovitch, S. (1998). selective macro-learning algorithm
application NxN sliding-tile puzzle. Journal Artificial Intelligence Research,
8, 223263.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions model minimization
Markov decision processes. Artificial Intelligence, 147 (1-2), 163223.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order regression inductive policy
selection. Conference Uncertainty Artificial Intelligence.
Guestrin, C., Koller, D., Gearhart, C., & Kanodia, N. (2003a). Generalizing plans new
environments relational mdps. International Joint Conference Artificial Intelligence.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003b). Efficient solution algorithms
factored mdps. Journal Artificial Intelligence Research, 19, 399468.
Hoffman, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. Journal
Artificial Intelligence Research, 22, 215278.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 263302.
Howard, R. (1960). Dynamic Programming Markov Decision Processes. MIT Press.
Huang, Y.-C., Selman, B., & Kautz, H. (2000). Learning declarative control rules
constraint-based planning. International Conference Machine Learning, pp.
415422.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). sparse sampling algorithm nearoptimal planning large markov decision processes. Machine Learning, 49 (23),
193208.
Kersting, K., Van Otterlo, M., & DeRaedt, L. (2004). Bellman goes relational. Proceedings
Twenty-First International Conference Machine Learning.
116

fiAPI Policy Language Bias

Khardon, R. (1999a). Learning action strategies planning domains. Artificial Intelligence, 113 (1-2), 125148.
Khardon, R. (1999b). Learning take actions. Machine Learning, 35 (1), 5790.
Lagoudakis, M., & Parr, R. (2003). Reinforcement learning classification: Leveraging
modern classifiers. International Conference Machine Learning.
Langford, J., & Zadrozny, B. (2004). Reducing t-step reinforcement learning classification.
http://hunch.net/jl/projects/reductions/RL class/colt submission.ps.
Martin, M., & Geffner, H. (2000). Learning generalized policies planning domains using
concept languages. International Conference Principles Knowledge Representation Reasoning.
Mataric, M. (1994). Reward functions accelarated learning. Proceedings International Conference Machine Learning.
McAllester, D., & Givan, R. (1993). Taxonomic syntax first order inference. Journal
ACM, 40 (2), 246283.
McAllester, D. (1991). Observations cognitive judgements. National Conference
Artificial Intelligence.
McGovern, A., Moss, E., & Barto, A. (2002). Building basic block instruction scheduler
using reinforcement learning rollouts. Machine Learning, 49 (2/3), 141160.
Minton, S. (1988). Quantitative results concerning utility explanation-based learning.
National Conference Artificial Intelligence.
Minton, S. (Ed.). (1993). Machine Learning Methods Planning. Morgan Kaufmann.
Minton, S., Carbonell, J., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. (1989).
Explanation-based learning: problem solving perspective. Artificial Intelligence, 40,
63118.
Natarajan, B. K. (1989). learning exercises. Annual Workshop Computational
Learning Theory.
Reddy, C., & Tadepalli, P. (1997). Learning goal-decomposition rules using exercises.
International Conference Machine Learning, pp. 278286. Morgan Kaufmann.
Rivest, R. (1987). Learning decision lists. Machine Learning, 2 (3), 229246.
Tesauro, G. (1992). Practical issues temporal difference learning. Machine Learning, 8,
257277.
Tesauro, G., & Galperin, G. (1996). On-line policy improvement using monte-carlo search.
Conference Advances Neural Information Processing.
Tsitsiklis, J., & Van Roy, B. (1996). Feature-based methods large scale DP. Machine
Learning, 22, 5994.
Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating
planning learning: PRODIGY architecture. Journal Experimental
Theoretical AI, 7 (1).
Wu, G., Chong, E., & Givan, R. (2001). Congestion control via online sampling. Infocom.
117

fiFern, Yoon, & Givan

Yan, X., Diaconis, P., Rusmevichientong, P., & Van Roy, B. (2004). Solitaire: Man versus
machine. Conference Advances Neural Information Processing.
Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection first-order MDPs.
Conference Uncertainty Artificial Intelligence.
Younes, H. (2003). Extending pddl model stochastic decision processes. Proceedings
International Conference Automated Planning Scheduling Workshop
PDDL.
Zimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: Looking back, taking stock, going forward. AI Magazine, 24(2)(2), 7396.

118


