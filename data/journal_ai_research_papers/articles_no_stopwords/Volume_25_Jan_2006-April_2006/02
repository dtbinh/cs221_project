journal artificial intelligence

submitted published

approximate policy iteration policy language bias
solving relational markov decision processes
alan fern

afern cs orst edu
school electrical engineering computer science oregon state university
sungwook yoon
sy purdue edu
robert givan
givan purdue edu
school electrical computer engineering purdue university

abstract
study policy selection large relational markov decision processes
mdps consider variant approximate policy iteration api replaces
usual value function learning step learning step policy space advantageous
domains good policies easier represent learn corresponding
value functions often case relational mdps interested
order apply api introduce relational policy language
corresponding learner addition introduce bootstrapping routine goalbased domains random walks bootstrapping necessary
many large relational mdps reward extremely sparse api ineffective
domains initialized uninformed policy experiments
resulting system able good policies number classical domains
stochastic variants solving extremely large relational mdps
experiments point limitations suggesting future work

introduction
many domains naturally represented terms objects relations
among accordingly ai researchers long studied
learning plan relational state action spaces include example classical
strips domains blocks world logistics
common criticism domains assumption idealized
deterministic world model part led ai researchers study
learning within decision theoretic framework explicitly handles stochastic environments generalized reward objectives however work
explicit propositional state space far demonstrated scalability
large relational domains commonly addressed classical
intelligent agents must able simultaneously deal complexity arising
relational structure complexity arising uncertainty primary goal
move toward agents bridging gap classical
decision theoretic techniques
describe straightforward practical method solving large
relational mdps work viewed form relational reinforcement learning
rrl assume strong simulation model environment assume
access black box simulator provide relationally represented
c

ai access foundation rights reserved

fifern yoon givan

state action pair receive sample appropriate next state reward distributions goal interact simulator order learn policy achieving high
expected reward separate challenge considered combine work
methods learning environment simulator avoid dependence provided
simulator
dynamic programming approaches finding optimal control policies mdps bellman howard explicit flat state space representations break
state space becomes extremely large recent work extends
use propositional boutilier dearden dean givan dean givan
leach boutilier dearden goldszmidt givan dean greig guestrin
koller parr venkataraman b well relational boutilier reiter price
guestrin koller gearhart kanodia state space representations extensions significantly expanded set approachable yet shown
capacity solve large classical benchmark
used competitions bacchus let alone stochastic variants one possible reason methods calculating representing value
functions familiar strips domains among others useful value functions
difficult represent compactly manipulation becomes bottle neck
techniques purely deductivethat value function guaranteed certain level accuracy rather work focus inductive
techniques make guarantees practice existing inductive forms approximate policy iteration api utilize machine learning select compactly represented
approximate value functions iteration dynamic programming bertsekas tsitsiklis machine learning selection hypothesis space
space value functions critical performance example space used frequently
space linear combinations human selected feature set
knowledge previous work applies form api
benchmark classical stochastic variants one
reason high complexity typical value functions large relational
domains making difficult specify good value function spaces facilitate learning
comparably often much easier compactly specify good policies accordingly
good policy spaces learning observation basis recent work inductive policy selection relational domains deterministic khardon
martin geffner probabilistic yoon fern givan techniques
useful policies learned policy space bias described generic
relational knowledge representation language incorporate ideas variant api achieves significant success without representing learning approximate
value functions course natural direction future work combine policy space
techniques value function techniques leverage advantages
given initial policy uses simulation technique policy rollout
tesauro galperin generate trajectories improved policy trajectories given classification learner searches classifier policy
matches trajectory data resulting approximately improved policy two
recent work relational reinforcement learning applied strips much simpler
goals typical benchmark domains discussed section



fiapi policy language bias

steps iterated improvement observed resulting
viewed form api iteration carried without inducing approximate
value functions
avoiding value function learning helps address representational
challenge applying api relational domains however another fundamental
challenge non trivial relational domains api requires form bootstrapping particular strips domains reward corresponds
achieving goal condition sparsely distributed unlikely reached random exploration thus initializing api random uninformed policy likely
reward signal hence guidance policy improvement one bootstrapping rely user provide good initial policy heuristic gives guidance
toward achieving reward rather work develop automatic bootstrapping
goal domains require user intervention
bootstrapping idea random walk distributions
given domain blocks world distribution randomly generates
e initial state goal selecting random initial state
executing sequence n random actions taking goal condition subset
properties resulting state difficulty typically increases n
small n short random walks even random policies uncover reward intuitively
good policy walk length n used bootstrap api
slightly longer walk lengths bootstrapping iterates idea starting
random policy small n gradually increasing walk length
learn policy long random walks long random walk policies clearly capture
much domain knowledge used ways empirically
policies often perform well distributions relational domains used
recent deterministic probabilistic competitions
implementation bootstrapped api took second place competitors hand tailored track international probabilistic competition knowledge first machine learning system entered
competition deterministic probabilistic
give evaluation system number probabilistic deterministic
relational domains including aips competition benchmarks benchmarks hand tailored track probabilistic competition
system often able learn policies domains perform
well long random walk addition policies often perform well
competition distributions comparing favorably state theart planner deterministic domains experiments highlight number
limitations current system point interesting directions future work
remainder proceeds follows section introduce
setup section present variant api section provide
note however hand tailored rather given domain definition system
learns policy offline automatically applied domain
entered hand tailored track track facilitated use offline learning
providing domains generators competition entrants humanwritten domain



fifern yoon givan

technical analysis giving performance bounds policy improvement
step sections describe implemented instantiation api
relational domains includes description generic policy language
relational domains classification learner language novel bootstrapping
technique goal domains section presents empirical finally
sections discuss related work future directions

setup
formulate work framework markov decision processes mdps
primary motivation develop relational domains first
describe setup general action simulatorbased mdp representation later section describe particular representation domains
relational mdps corresponding relational instantiation
following adapting kearns mansour ng bertsekas tsitsiklis
represent mdp generative model hs r ii finite
set states finite ordered set actions randomized action simulation
given state action returns next state according unknown
probability distribution pt component r reward function maps
real numbers r representing reward taking action state
randomized initial state inputs returns state according
unknown distribution p sometimes treat random variables
distributions p pt respectively
mdp hs r ii policy possibly stochastic mapping
value function denoted v represents expected cumulative discounted
reward following policy starting state unique solution
v e r v



discount factor q value function q represents
expected cumulative discounted reward taking action state following
given
q r e v



measure quality policy objective function v e v giving
expected value obtained policy starting randomly drawn initial
state common objective mdp reinforcement learning
optimal policy argmax v however automated technique including one
present date able guarantee finding optimal policy relational
domains consider reasonable running time
well known fact given current policy define improved
policy
pi argmaxaa q



value function pi guaranteed worse
state strictly improve state optimal policy iteration


fiapi policy language bias

computing optimal policies iterating policy improvement pi
initial policy reach fixed point guaranteed optimal policy
iteration policy improvement involves two steps policy evaluation compute
value function v current policy policy selection given v
step select action maximizes q state defining improved
policy
finite horizons since api variant simulation must bound
simulation trajectories horizon h technical analysis section use notion
finite horizon discounted reward h horizon value function vh recursively defined

v

vh e r vh



giving expected discounted reward obtained following h steps
h horizon
define h horizon q function qh r e vh

objective function v h e vh well known effect finite
horizon made arbitrarily small particular states
actions approximation error decreases exponentially h
v vh h vmax
q qh h vmax
rmax
vmax


rmax maximum absolute value reward action state
get v h v h vmax

approximate policy iteration policy language bias
exact solution techniques policy iteration typically intractable large statespace mdps arising relational domains section
introduce variant approximate policy iteration api intended domains
first review generic form api used prior work learning approximate
value functions next motivated fact value functions often difficult learn
relational domains describe api variant avoids learning value functions
instead learns policies directly state action mappings
api approximate value functions
api described bertsekas tsitsiklis uses combination monte carlo
simulation inductive machine learning heuristically approximate policy iteration
large state space mdps given current policy iteration api approximates
policy evaluation policy selection resulting approximately improved policy
first policy evaluation step constructs training set samples v small
representative set states sample computed simulation estimating v
policy state drawing number sample trajectories starting


fifern yoon givan

averaging cumulative discounted reward along trajectories next
policy selection step uses function approximator e g neural network learn
approximation v v training data v serves representation
selects actions sampled one step lookahead v
arg max r e v
aa

common variant procedure learns approximation q rather v
api exploits function approximators generalization ability avoid evaluating
state state space instead directly evaluating small number training states
thus use api assumes states perhaps actions represented factored
form typically feature vector facilitates generalizing properties training data
entire state action spaces note case perfect generalization e
v v states equal exact policy improvement
pi thus api simulates exact policy iteration however practice generalization
perfect typically guarantees policy improvement nevertheless
api often converges usefully tesauro tsitsiklis van roy
success api procedure depends critically ability represent
learn good value function approximations mdps arising
relational domains often difficult specify space value functions
learning mechanism facilitate good generalization example work relational
reinforcement learning dzeroski deraedt driessens shown learning
approximate value functions classical domains blocks world problematic spite often relatively easy compactly specify good policies
language relational state action mappings suggests languages may
provide useful policy space biases learning api however prior api methods
approximating value functions hence leverage biases
motivation consider form api directly learns policies without directly
representing approximating value functions
policy language bias
policy simply classifier possibly stochastic maps states actions api
view motived recent work casts policy selection
standard classification learning particular given ability observe
trajectories target policy use machine learning select policy classifier
mimics target closely possible khardon b studied learning setting
provided pac learnability showing certain assumptions small
number trajectories sufficient learn policy whose value close
target addition recent empirical work relational domains khardon
martin geffner yoon et al shown expressive languages
strong assumptions api shown converge infinite limit near optimal
value function see proposition bertsekas tsitsiklis
particular rrl work considered variety value function representation including relational
regression trees instance methods graph kernels none generalized well
varying numbers objects



fiapi policy language bias

specifying state action mappings good policies learned sample trajectories
good policies
suggest given policy somehow generate trajectories
improved policy learn approximately improved policy
trajectories idea basis figure gives pseudo code api
variant starts initial policy produces sequence approximately
improved policies iteration involves two primary steps first given current
policy procedure improved trajectories approximately generates trajectories
improved policy pi second trajectories used training data
procedure learn policy returns approximation describe
step detail
step generating improved trajectories given base policy simulation technique policy rollout tesauro galperin bertsekas tsitsiklis
computes approximation improved policy pi
applying one step policy iteration furthermore given state policy rollout
computes without need solve states thus provides
tractable way approximately simulate improved policy large state space mdps
often significantly better hence lead substantially
improved performance small cost policy rollout provided significant benefits
number application domains including example backgammon tesauro galperin
instruction scheduling mcgovern moss barto network congestion control
wu chong givan solitaire yan diaconis rusmevichientong van roy

policy rollout computes estimate estimating q
action taking maximizing action suggested equation
q estimated drawing w trajectories length h trajectory
starting taking action following actions selected
h steps estimate q taken average cumulative
discounted reward along trajectory sampling width w horizon h specified
user control trade increased computation time large values
reduced accuracy small values note rollout applies stochastic
deterministic policies due variance q value estimates rollout policy
stochastic even deterministic base policies
procedure improved trajectories uses rollout generate n length h trajectories
beginning randomly drawn initial state rather recording
states actions along trajectory store additional information used
policy learning particular ith element trajectory form
hsi si q si q si giving ith state si along trajectory action
selected current unimproved policy si q value estimates q si
action note given q value information si learning
determine approximately improved action maximizing actions desired
step learn policy intuitively want learn policy select policy
closely matches training trajectories experiments use relatively simple
learning greedy search within space policies specified policylanguage bias sections detail policy language learning bias used


fifern yoon givan

technique associated learning section provide
technical analysis idealized version providing guidance regarding
required number training trajectories note labeling training state
trajectories associated q values action rather simply
best action enable learner make informed trade offs focusing accuracy
states wrong decisions high costs empirically useful
inclusion training data enables learner adjust data relative
desirede g learner uses bias focuses states large improvement
appears possible
finally note api effective important initial policy
provide guidance toward improvement e must bootstrap api process
example goal domains reach goal sampled
states section discuss important issue bootstrapping introduce
bootstrapping technique

technical analysis
section consider variant policy improvement step main api loop
learns improved policy given base policy select sampling
width w horizon h training set size n certain assumptions quality
learned policy close quality policy iteration improvement similar
shown previous forms api approximate value functions
bertsekas tsitsiklis however assumptions much different nature
analysis divided two parts first following khardon b consider
sample complexity policy learning consider many trajectories
target policy must observed learner guarantee good approximation
target second apply deterministic policies
learning rollout policies stochastic throughout
assume context mdp hs r ii
learning deterministic policies
trajectory length h sequence ah sh alternating states si
actions ai say deterministic policy consistent trajectory sh
h si ai define dh distribution set
length h trajectories dh probability generates trajectory
ah sh according following process first draw according
initial state distribution draw si si si h note
dh non zero consistent
policy improvement step first generates trajectories rollout policy see section via procedure improved trajectories learns approximation
particular bertsekas tsitsiklis assumes bound l norm value function
approximation e state approximation almost perfect rather assume
improved policy comes finite class policies consistent learner
cases policy improvement guaranteed given additional assumption minimum
q advantage mdp see



fiapi policy language bias

api n w h
training set size n sampling width w horizon h
mdp hs r ii initial policy discount factor

loop
improved trajectories n w h
learn policy
satisfied
e g change small
return
improved trajectories n w h
training set size n sampling width w
horizon h mdp current policy

repeat n times generate n trajectories improved policy
nil
state drawn draw random initial state
h
hq q policy rollout w h q estimates
hs q q concatenate sample onto trajectory
action maximizing q action improved policy state
state sampled simulate action improved policy

return
policy rollout w h

compute q estimates hq q

policy state sampling width w horizon h mdp
action ai
q ai
repeat w times q ai average w trajectories
r r ai state sampled ai take action ai
h take h steps accumulating discounted reward r
r r r
state sampled
q ai q ai r include trajectory average
q ai

q ai

w

return hq q

figure pseudo code api see section instantiation learnpolicy called learn decision list
note rollout policy serves stochastic approximation pi
policy iteration improvement thus improved trajectories viewed
tempting draw trajectories dh learning step viewed learning


fifern yoon givan



approximation imagining moment draw trajectories dh
fundamental question many trajectories sufficient ensure learned
policy good khardon b studied question case
deterministic policies undiscounted goal domains e mdps
reward received goal states give straightforward adaptation
main setting general reward functions measure
quality policy v
learning formulation similar spirit standard framework probably approximately correct pac learning particular assume target
policy comes finite class deterministic policies h example h may correspond
set policies described bounded length decision lists addition
assume learner consistenti e returns policy h consistent
training trajectories assumptions relatively small number
trajectories logarithmic h sufficient ensure high probability
learned policy good target
proposition let h finite class deterministic policies h

set n ln h
trajectories drawn independently dh probability
every h consistent trajectories satisfies v v vmax h
proof proposition appendix computational complexity
finding consistent policy depends policy class h polynomial time
given interesting classes bounded length decision listshowever
typically expensive policy classes consider practice rather
described section use learner greedy heuristic search often
works well practice
assumption target policy comes fixed size class h often
violated however pointed khardon b straightforward give
extension proposition setting learner considers increasingly complex
policies consistent one found case sample complexity related
encoding size target policy rather size h thus allowing use
large expressive policy classes without necessarily paying full sample complexity
price proposition
learning rollout policies
proof proposition relies critically fact policy class h contains
deterministic policies however main api loop target policies computed via
rollout hence stochastic due uncertainty introduced finite sampling thus
cannot directly use proposition context learning trajectories produced
rollout deal describe variant improved trajectories
reliably generate training trajectories deterministic policy pi see
equation guaranteed improve improvement possible
given base policy first define set actions maximize
q note min minimum taken respect action ordering provided mdp importantly policy deterministic thus


fiapi policy language bias

generate trajectories apply learn close approximation order generate trajectories slightly modify improved trajectories
modification introduced analysis experiments procedures given figure modification replace action maximization step
improved trajectories second last statement loop chooses next
action execute following two steps
maxa q q
min
q estimate qh computed policy rollout sampling width
w newly introduced parameter
note selected action equal condition true every state encountered modified improved trajectories
effectively generate trajectories thus would bound probability
small value appropriately choosing sampling width w
horizon h unfortunately choice parameters depends mdp
given particular parameter values mdp event
non negligible probability state reason first
define q advantage mdp select appropriate parameter values
given lower bound
given mdp policy let set states iff
two actions q q e actions distinct
q values state define best action second
best action respectively measured q q advantage defined
minss measures minimum q value gap optimal
sub optimal action state space given lower bound q advantage
mdp following proposition indicates select parameter values ensure
high probability
proposition


mdp q advantage least

h log


vmax

vmax







w




ln




state probability least
proof given appendix thus parameter values satisfying conditions mdp q advantage least guaranteed probability
least means improved trajectories correctly select action probability least note proposition


fifern yoon givan

agrees intuition h w increase decreasing q advantage
increasing vmax w increase decreasing
order generate n length h trajectories modified improved trajectories
routine must compute set n h states yielding n h opportunities make
error ensure error made modified procedure sets sampling width w

guarantees error free training set created probability
nh

least
combining observation assumption h apply proposition

follows first generate n ln h
trajectories modified improved
trajectories routine nh
next learn policy trajectories
consistent learner know probability generating imperfect training set
bounded chosen value n failure probability learner
bounded thus get probability least learned policy
satisfies v v vmax h giving approximation guarantee relative
improved policy summarized following proposition
proposition let h finite class deterministic policies
mdp q advantage
least policy pi h set



n ln h
trajectories produced modified improved trajectories
parameters satisfying





h log


vmax

vmax nh
w
ln


least probability every h consistent trajectories satisfies
v v pi vmax h




one notable aspect logarithmic dependence
number actions however practical utility hindered dependence
typically known practice exponentially small
horizon unfortunately dependence appears unavoidable type
try learn trajectories pi produced rollout
particular setting parameters mdp
small enough q advantage value rollout policy arbitrarily worse
pi

api relational
work motivated goal solving relational mdps particular interested finding policies relational mdps represent classical domains
first glance appears lower bound h decreases increasing vmax decreasing
however opposite true since base logarithm discount factor strictly less
one note since upper bounded vmax bound h positive



fiapi policy language bias

stochastic variants policies applied instance
domain hence viewed form domain specific control knowledge
section first describe straightforward way view classical domains
single instances relationally factored mdps next describe
relational policy space policies compactly represented taxonomic decision
lists finally present heuristic learning policy space
domains mdps
say mdp hs r ii relational defined giving finite
set objects finite set predicates p finite set action types fact
predicate applied appropriate number objects e g b blocks world
fact state set facts interpreted representing true facts state
state space contains possible sets facts action action type applied
appropriate number objects e g putdown blocks world action action
space set actions
classical domain describes set instances related structure
instance gives initial world state goal example blocks
world classical domain instance specifies initial block
configuration set goal conditions classical planners attempt solutions
specific instances domain rather goal solve entire domains
finding policy applied instances described
straightforward view classical domain relational mdp mdp
state corresponds instance
state action spaces classical domain specifies set action
types world predicates w possible world objects together define
mdp action space state mdp corresponds single instance e
world state goal domain specifying current world
goal achieve letting set relational mdp predicates p w g
g set goal predicates set goal predicates contains predicate
world predicate w named prepending g onto corresponding
world predicate name e g goal predicate gclear corresponds world predicate
clear definition p see mdp states sets goal world
facts indicating true world facts instance goal conditions
important note described mdp actions change world
facts goal facts thus large relational mdp viewed collection
disconnected sub mdps sub mdp corresponds distinct goal condition
reward function given mdp state objective reach another mdp state
goal facts subset corresponding world factsi e reach world state
satisfies goal call states goal states mdp example
mdp state
table b clear b gclear b
goal state blocks world mdp would goal state without world fact
clear b represent objective reaching goal state quickly defining r assign
reward zero actions taken goal states negative rewards actions


fifern yoon givan

states representing cost taking actions typically classical
domains action costs uniformly however framework allows cost vary
across actions
transition function classical domain provides action simulator
e g defined strips rules given world state action returns world
state define mdp transition function simulator modified treat goal
states terminal preserve without change goal predicates mdp state since
classical domains typically large number actions action definitions
usually accompanied preconditions indicate legal actions given state
usually legal actions small subset possible actions assume
treats actions legal ops simplicity relational mdp definition
explicitly represent action preconditions however assume
access preconditions thus need consider legal actions example
restrict rollout legal actions given state
initial state distribution finally initial state distribution program
generates legal instances mdp states domain example domains competitions commonly distributed
generators
definitions good policy one reach goal states via low cost
action sequences initial states drawn note policies mappings
instances actions thus sensitive goal conditions
way learned policies able generalize across different goals next describe
language representing generalized policies
taxonomic decision list policies
single argument action types many useful rules domains take form
apply action type object class c martin geffner example
blocks world pick clear block belongs table table
logistics world unload object destination concept
language describing object classes martin geffner introduced use
decision lists rules useful learning bias showing promising experiments
deterministic blocks world motivation consider policy space similar
one used originally martin geffner generalized handle multiple action
arguments historical reasons concept language upon taxonomic
syntax mcallester mcallester givan rather description logic
used martin geffner
comparison predicates relational mdps world goal predicates
corresponding classical domains often useful polices compare
current state goal end introduce set predicates called
comparison predicates derived world goal predicates
world predicate p corresponding goal predicate gp introduce comparison
predicate cp defined conjunction p gp comparison predicate
fact true corresponding world goal predicates facts true


fiapi policy language bias

example blocks world comparison predicate fact con b indicates
b current state goali e b gon b true
taxonomic syntax taxonomic syntax provides language writing class expressions represent sets objects properties interest serve fundamental
pieces build policies class expressions built mdp predicates
including comparison predicates applicable variables policy representation
variables used denote action arguments runtime instantiated
objects simplicity consider predicates arity one two call
primitive classes relations respectively domain contains predicates arity
three automatically convert multiple auxiliary binary predicates given
list variables x x xk class expressions given
c x c xi thing c x r c x min r
r r r r
c x class expression r relation expression c primitive class r
primitive relation xi variable x note classical domains
primitive classes relations world goal comparison predicates define depth c x class expression c x one c x primitive
class thing variable min r otherwise define c x r c x
c x r relation expression c x class expression given
relational mdp denote cd x set class expressions c x depth
less
intuitively class expression r c x denotes set objects related
relation r object set c x expression r c x denotes
set objects related r chain object c x
constructor important representing recursive concepts e g blocks
expression min r denotes set objects minimal relation r
formally let mdp state ok variable assignment
assigns object oi variable xi interpretation c x relative
set objects denoted c x primitive class c interpreted set
objects predicate symbol c true likewise primitive relation r
interpreted set object tuples relation r holds class
expression thing denotes set objects class expression xi xi
variable interpreted singleton set oi interpretation compound
expressions given
c x c x
r c x c x rs
min r rs rs
r id ov ov oi oi rs v
r rs
c x class expression r relation expression id identity relation
examples useful blocks world concepts given primitive classes clear gclear
holding con table along primitive relations gon con


fifern yoon givan

gon holding depth two denotes block want block
held
gclear depth three denotes blocks currently blocks
want make clear
con con table depth two denotes set blocks well constructed
towers see note block bv class exists
sequence blocks b bv b table goal
current state e con table b bi bi goal current state
e con bi bi v
gon con con table depth three denotes blocks belong top
currently well constructed tower
decision list policies represent policies decision lists action selection rules
rule form x xk l l lm k argument action type
li literals xi action argument variables denote list
action argument variables x x xk literal form x c x
c x taxonomic syntax class expression x action argument variable
given mdp state list action argument objects ok say
literal xi c x true given iff oi c x say rule
r x xk l l lm allows action ok iff literal rule
true given note literals rule action type
possible actions type allowed rule rule viewed placing mutual
constraints tuples objects action type applied note
single rule may allow actions many actions one type given decision list
rules say action allowed list allowed rule list
previous rule allows actions decision list may allow actions
multiple actions one type decision list l mdp defines deterministic policy
l mdp l allows actions state l least legal action
otherwise l least legal action allowed l important
note since l considers legal actions specified action preconditions
rules need encode preconditions allows simpler rules learning
words think rule implicitly containing preconditions
action type
example taxonomic decision list policy consider simple blocks world domain
goal condition clear red blocks primitive classes
domain red clear holding single relation following
policy solve domain
putdown x x holding
pickup x x clear x red
action ordering relational mdp defined lexicographically terms orderings action
types objects



fiapi policy language bias

first rule cause agent putdown block held otherwise
block held block x clear red block expressed
red pick appendix b gives examples complex policies
learned system experiments
learning taxonomic decision lists
given relational mdp define rd l set action selection rules
length l literals whose class expression depth let
hd l denote policy space defined decision lists whose rules rd l since
number depth bounded class expressions finite finite number rules
hence hd l finite though exponentially large implementation learn policy
used main api loop learns policy hd l user specified values l
use rivest style decision list learning rivest
taken martin geffner learning class policies primary difference
martin geffner technique method selecting individual
rules decision list use greedy heuristic search previous work used
exhaustive enumeration difference allows us rules
complex potential cost failing good simple rules enumeration
might discover
recall section training set given learn policy contains trajectories
rollout policy learning however sensitive trajectory
structure e order trajectory elements thus simplify discussion
take input learner training set contains union
trajectory elements means trajectory set contains n length h
trajectories contain total n h training examples described section
training example form hs q q state
action selected previous policy q ai q value estimate
q ai note experiments training examples contain values
legal actions state
given training set natural learning goal decision list policy
training example selects action maximum estimated q value learning
goal however problematic practice often several best close
best actions measured true q function case due random sampling
particular action looks best according q value estimates training set
arbitrary attempting learn concise policy matches arbitrary actions
difficult best likely impossible
one lagoudakis parr avoiding use statistical
tests determine actions clearly best positive examples ones
clearly best negative examples learner asked
policy consistent positive negative examples
shown empirical success potential shortcoming throwing away
q value information particular may possible policy
exactly matches training data cases would learner make informed
trade offs regarding sub optimal actionsi e prefer sub optimal actions larger


fifern yoon givan

learn decision list l b
training set concept depth rule length l beam width b
l nil
empty
r learn rule l b
r covers
l extend list l r add r end list
return l
learn rule l b
training set concept depth rule length l beam width b
action type

compute rule action type

ra beam search l b
return argmaxa hvalue ra
beam search l b
training set concept depth rule length l beam width b action type
k arity x x xk



l x c x x c cd x

x sequence action argument variables
construct set depth bounded candidate literals

b x nil initialize beam single rule literals
loop
g bi r rd l r add literal r l r bi l l
bi beam select g b

select best b heuristic values


bi bi

loop improvement heuristic

return argmaxrbi hvalue r

return best rule final beam

figure pseudo code learning decision list hd l given training data
procedure add literal r l simply returns rule literal l added end
rule r procedure beam select g w selects best b rules g different
heuristic values procedure hvalue r returns heuristic value rule r relative
training data described text

q values motivation describe cost sensitive decision list learner
sensitive full set q values learning goal roughly decision
list selects actions large cumulative q value training set
learning list rules say decision list l covers training example
hs q q l suggests action state given set training
examples search decision list selects actions high q value via
iterative set covering carried learn decision list decision list rules


fiapi policy language bias

constructed one time order list covers training examples
pseudo code given figure initially decision list null list
cover training examples iteration search high quality
rule r quality measured relative set currently uncovered training examples
selected rule appended current decision list training examples newly
covered selected rule removed training set process repeats
list covers training examples success depends heavily
function learn rule selects good rule relative uncovered training
examplestypically good rule one selects actions best close best
q value covers significant number examples
learning individual rules input rule learner learn rule set
training examples along depth length parameters l beam width b
action type rule learner calls routine beam search good rule
ra rd l action type learn rule returns rule ra highest value
measured heuristic described later section
given action type procedure beam search generates beam b b
bi set rules rd l action type sets evolve specializing
rules previous sets adding literals guided heuristic function search
begins general rule x nil allows action type state
search iteration produces set bi contains b rules highest different heuristic
values among following set
g bi r rd l r add literal r l r bi l l
l set possible literals depth less set includes
current best rules bi rule rd l formed adding
literal rule bi search ends improvement heuristic value
occurs bi bi beam search returns best rule bi according
heuristic
heuristic function training instance hs q q define q advantage taking action ai instead state ai q ai
q likewise q advantage rule r sum q advantages actions
allowed r given rule r set training examples heuristic function
hvalue r equal number training examples rule covers plus
cumulative q advantage rule training examples q advantage rather
q value focuses learner toward instances large improvement previous policy possible naturally one could consider different weights coverage
q advantage terms possibly tuning weight automatically validation data
since many rules rd l equivalent must prevent beam filling semantically
equivalent rules rather deal via expensive equivalence testing take ad hoc
practically effective assume rules coincidentally heuristic
value ones must equivalent thus construct beams whose members
different heuristic values choose rules value preferring shorter rules
arbitrarily
coverage term included covering zero q advantage example
covering zero q advantage good e g previous policy optimal state



fifern yoon givan

random walk bootstrapping
two issues critical success api technique first api
fundamentally limited expressiveness policy language strength
learner dictates ability capture improved policy described training
data iteration second api yield improvement improved trajectories
successfully generates training data describes improved policy large classical
domains initializing api uninformed random policy typically
essentially random training data helpful policy improvement
example consider mdp corresponding block blocks world initial
distribution generates random initial goal states case random
policy unlikely reach goal state within practical horizon time hence
rollout trajectories unlikely reach goal providing guidance toward learning
improved policy e policy reliably reach goal
interested solving large domains providing guiding inputs
api critical fern yoon givan showed bootstrapping api
domain independent heuristic planner hoffmann nebel api
able uncover good policies blocks world simplified logistics world planes
stochastic variants however limited heuristics ability
provide useful guidance vary widely across domains
describe bootstrapping procedure goal domains
random walks guiding api toward good policies system
evaluated section integrating procedure api order
policies goal domains non goal mdps bootstrapping
procedure directly applied bootstrapping mechanisms must used
necessary might include providing initial non trivial policy providing heuristic
function form reward shaping mataric first describe
idea random walk distributions next describe use distributions
context bootstrapping api giving lrw api
random walk distributions
throughout consider mdp hs r ii correspond goal domains described section recall state corresponds
specifying world state via world facts set goal conditions via
goal facts use terms mdp state interchangeably
note context distribution convenience
denote mdp states tuples w g w g sets world facts
goal facts respectively
given mdp state w g set goal predicates g define g
mdp state w g g contains goal facts g applications predicate
g given set goal predicates g define n step random walk
distribution rw n g following stochastic
draw random state w g initial state distribution


fiapi policy language bias

starting take n uniformly random actions giving state sequence sn
sn wn g recall actions change goal facts uniformly
random action selection assume extra op action change
state selected fixed probability reasons explained
let g set goal facts corresponding world facts wn e g
wn b clear g gon b gclear return
mdp state g g output
sometimes abbreviate rw n g rw n g clear context
intuitively perform well distribution policy must able achieve facts
involving goal predicates typically n step random walk
initial state restricting set goal predicates g specify types facts
interested achievinge g blocks world may interested
achieving facts involving predicate
random walk distributions provide natural way span range difficulties since longer random walks tend take us initial state small
n typically expect generated rw n become
difficult n grows however n becomes large generated require far
fewer n steps solvei e direct paths initial state
end state long random walk eventually since finite difficulty
stop increasing n
question raised idea whether large n good performance rw n
ensures good performance distributions interest domain
domains simple blocks world good random walk performance
seem yield good performance distributions interest domains
grid world keys locked doors intuitively random walk unlikely
uncover requires unlocking sequence doors indeed since rw n
insensitive goal distribution underlying domain random walk
distribution may quite different
believe good performance long random walks often useful
addressing one component difficulty many benchmarks successfully
address components difficulty planner need deploy orthogonal technology landmark extraction setting subgoals hoffman porteous
sebastia example grid world could automatically set subgoal
possessing key first door long random walk policy could provide useful
macro getting key
purpose developing bootstrapping technique api limit focus
finding good policies long random walks experiments define long
specifying large walk length n theoretically inclusion op action
definition rw ensures induced random walk markov chain aperiodic
practice select random actions set applicable actions state si provided
simulator makes possible identify set
blocks world large n rw n generates pairs random block configurations typically
pairing states far apartclearly policy performs well distribution captured
significant information blocks world
dont formalize chain formalizations work well



fifern yoon givan

thus distribution states reached increasingly long random walks converges
stationary distribution thus rw limn rw n well defined take
good performance rw goal
random walk bootstrapping
mdp define mdp identical initial state
distribution replaced define success ratio sr
probability solves drawn treating random variable
average length al conditional expectation solution
length drawn given solves typically solution length
taken number actions however action costs uniform
length taken sum action costs note mdp formulation
classical domains given section policy achieves high v
high success ratio low average cost
given mdp set goal predicates g system attempts good
policy rw n n selected large enough adequately approximate
rw still allowing tractable completion learning naively given initial
random policy could try apply api directly however already discussed
work general since interested domains rw produces
extremely large difficult random policies provide ineffective starting
point
however small n e g n rw n typically generates easy
likely api starting even random initial policy reliably good
policy rw n furthermore expect policy n performs well rw n
provide reasonably good perhaps perfect guidance drawn
rw moderately larger n thus expect able
good policy rw bootstrapping api initial policy n suggests natural
iterative bootstrapping technique good policy large n particular n n
figure gives pseudo code procedure lrw api integrates api
random walk bootstrapping policy long random walk distribution
intuitively viewed iterating two stages first finding
hard enough distribution current policy increasing n finding good
policy hard distribution api maintains current policy
current walk length n initially n long success ratio rwn
success threshold constant close one simply iterate steps
approximate policy improvement achieve success ratio policy
statement increases n success ratio rw n falls
performs well enough current n step distribution move distribution
slightly harder constant determines much harder set small enough
likely used bootstrap policy improvement harder distribution
simpler method increasing n whenever success ratio achieved
markov chain may irreducible stationary distribution may reached
initial states however considering one initial state described



fiapi policy language bias

lrw api n g n w h
max random walk length n goal predicates g
training set size n sampling width w horizon h
mdp initial policy discount factor
n
loop
c n
sr

harder n step distribution
c n none
n least n n sr
rw n g
improved trajectories n w h
learn policy
satisfied
return

c n estimates success ratio
figure pseudo code lrw api sr
domain drawn rw n g drawing set returning
fraction solved constants described text

good policies whenever method take much longer may run
api repeatedly training set already good policy
n becomes equal maximum walk length n n n future
iterations important note even policy good success ratio
rw n may still possible improve average length policy thus
continue api distribution satisfied success ratio
average length current policy

relational experiments
section evaluate lrw api technique relational mdps corresponding
deterministic stochastic classical domains first give number
deterministic benchmark domains showing promising comparison stateof art planner hoffmann nebel highlighting limitations
next give several stochastic domains including
domain specific track international probabilistic competition
ippc domain definitions generators used experiments
available upon request
experiments use policy learner described section learn
taxonomic decision list policies cases number training trajectories
policies restricted rules depth bound length bound l discount


fifern yoon givan

factor one lrw api initialized policy selects
random actions utilize maximum walk length parameter n set
equal respectively
deterministic experiments
perform experiments seven familiar strips domains including used
aips competition used evaluate tl plan bacchus
kabanza gripper domain domain standard generator
accepts parameters control size difficulty randomly generated
list domain parameters associated detailed
description domains found hoffmann nebel
blocks world n standard blocks worlds n blocks
freecell c f l version solitaire suits c cards per suit f freecells
l columns
logistics c l p logistics transportation domain airplanes c cities l
locations p packages
schedule p job shop scheduling domain p parts
elevator f p elevator scheduling f floors p people
gripper b robotic gripper domain b balls
briefcase transportation domain items
lrw experiments first set experiments evaluates ability lrw api
good policies rw utilize sampling width one rollout since
deterministic domains recall iteration lrw api compute
approximately improved policy may increase walk length n harder
distribution continued iterating lrw api observed
improvement training time per iteration approximately five hours though
initial training period significant policy learned used solve
quickly terminating seconds solution one found even
large

figure provides data iteration lrw api seven domains
indicated parameter settings first column domain indicates
iteration number e g blocks world run iterations second column
records walk length n used learning corresponding iteration third
fourth columns record sr al policy learned corresponding iteration
timing information relatively unoptimized scheme implementation reimplementation
c would likely fold speed



fin

rw n
sr
al

iter

iter

api policy language bias

rw
sr
al

n

blocks world

























































































rw
sr
al

logistics

























freecell










rw n
sr
al












































































































schedule













briefcase
elevator







































gripper









figure iteration lrw api seven deterministic domains
iteration walk length n used learning along success ratio
sr average length al learned policy rw n rw final
policy shown domain performs sr walks length n
exception logistics iteration improve performance
benchmark sr al planner drawn
rw

measured drawn rw n corresponding value n e
distribution used learning sr exceeds next iteration seeks
increased walk length n fifth sixth columns record sr al


fifern yoon givan

policy measured drawn lrw target distribution rw
experiments approximated rw n n
example see blocks world total iterations
learn first one iteration n one iteration n four iterations
n two iterations n point see resulting
policy performs well rw iterations n n shown showed
improvement policy found iteration eight domains observed
improvement iterating n n thus iterations
note domains except logistics see achieve policies good performance
rw n learning much shorter rw n distributions indicating indeed
selected large enough value n capture rw desired
general observations several domains learner bootstraps quickly
short random walk finding policy works well even much longer
random walk include schedule briefcase gripper elevator typically large domains many somewhat independent subproblems
short solutions short random walks generate instances different typical
subproblems domains best lrw policy found small number
iterations performs comparably rw note considered
good domain independent planner domains consider successful

two domains logistics freecell planner unable policy
success ratio one rw believe limited knowledge representation allowed policies following reasons first cannot write good
policies domains within current policy language example logistics one
important concept set containing packages trucks truck
packages goal city however domain defined way concept
cannot expressed within language used experiments second final learned
decision lists logistics freecell appendix b contain much larger
number specific rules lists learned domains indicates
learner difficulty finding general rules within language restrictions
applicable large portions training data resulting poor generalization third
success ratio shown sampling rollout policy e improved policy
simulated improved trajectories substantially higher resulting
learned policy becomes policy next iteration indicates learndecision list learning much weaker policy sampling policy generating
training data indicating weakness policy language learning example logistics domain iteration eight training data learning
iteration nine policy generated sampling rollout policy achieves success ratio
training drawn rw distribution learned
iteration nine policy achieves success ratio shown figure iteration
nine extending policy language incorporate expressiveness appears
required domains require sophisticated learning
point future work
logistics planner generates long sequence policies similar oscillating success ratio
elided table ellipsis space reasons



fiapi policy language bias


domain
blocks


sr al




size



sr



al



freecell
















logistics
















elevator











schedule











briefcase
















gripper











figure standard distributions seven benchmarks success ratio
sr average length al provided policy learned lrw
distribution given domain learned lrw policy used
size shown

remaining domain blocks world bootstrapping provided increasingly
long random walks appears particularly useful policies learned walk
lengths increasingly effective target lrw distribution rw
walks length takes multiple iterations master provided level
difficulty beyond previous walk length finally upon mastering walk length
resulting policy appears perform well walk length learned policy modestly
superior rw success ratio average length
evaluation original distributions domain denote
best learned lrw policyi e policy domain highest
performance rw shown figure taxonomic decision lists corresponding
domain given appendix b figure shows performance
comparison original intended distributions domains
measured success ratio systems giving time limit seconds solve
attempted select largest sizes previously used
evaluation domain specific planners aips bacchus kabanza
well smaller size cases one planners
performed poorly large size case use generators
provided domains evaluate size
overall indicate learned reactive policies competitive
domain independent planner important remember policies
learned domain independent fashion thus lrw api viewed general
generating domain specific reactive planners two domains blocks world


fifern yoon givan

briefcase learned policies substantially outperform success ratio especially
large domain sizes three domains elevator schedule gripper two approaches perform quite similarly success ratio superior average
length schedule superior average length elevator
two domains logistics freecell substantially outperforms learned policies success ratio believe partly due inadequate policy language
discussed believe however another reason poor performance
long random walk distribution rw correspond well standard
distributions seems particularly true freecell policy learned
freecell achieved success ratio percent rw however standard distribution achieved percent suggests rw generates
significantly easier standard distribution supported fact
solutions produced standard distribution average twice long
produced rw one likely reason easy random walks
end dead states freecell actions applicable thus random walk
distribution typically produce many goals correspond dead
states standard distribution hand treat dead states goals
probabilistic experiments
present experiments three probabilistic domains described probabilistic domain language ppddl younes
ground logistics c p probabilistic version logistics airplanes c
cities p packages driving action probability failure domain
colored blocks world n probabilistic blocks world n colored blocks
goals involve constructing towers certain color patterns probability
moved blocks fall floor
boxworld c p probabilistic version full logistics c cities p packages
transportation actions probability going wrong direction
ground logistics domain originally boutilier et al used
evaluation yoon et al colored blocks world boxworld domains
domains used hand tailored track ippc lrw api technique
entered hand tailored track participants provided generators
domain competition allowed incorporate domain knowledge
planner use competition time provided generators lrw api
learned policies domains entered competition
conducted experiments probabilistic domains yoon et al
including variants blocks world variant ground logistics
appeared fern et al however since
qualitatively identical deterministic blocks world described
ground logistics
three probabilistic domains conducted lrw experiments
procedure parameters given lrw api except


fin

sr

rw n
al

iter

iter

api policy language bias

rw
sr
al

boxworld






















standard distribution

n

sr

rw n
al

sr

rw
al

ground logistics




























standard distribution
















colored blocks world
















standard distribution















figure iteration lrw api three probabilistic domains
iteration walk length n used learning along success ratio
sr average length al learned policy rw n rw
benchmark performance standard distribution policy whose
performance best rw
sampling width used rollout set w set order
account stochasticity domains experiments shown
figure tables form figure last row given
domain gives performance standard distribution e drawn
domains generator colored blocks world generator
produces whose goals specified existential quantifiers example
simple goal may exists blocks x x red blue x
since policy language cannot directly handle existentially quantified goals preprocess
produced generator remove done
assigning particular block names existential variables ensuring static
properties block case color satisfied static properties variable
assigned domain finding assignment trivial resulting
assignment taken goal giving learned policy
applied since blocks world states fully connected resulting goal
guaranteed achievable

boxworld lrw api able good policy rw standard
distribution deterministic logistics freecell believe
primarily restricted policy languages currently used learner
domains see decision list learned boxworld contains many
specific rules indicating learner able generalize well beyond


fifern yoon givan

training trajectories ground logistics see lrw api quickly finds good
policy rw standard distribution
colored blocks world see lrw api able quickly good
policy rw standard distribution however unlike deterministic
uncolored blocks world success ratio observed less one solving
percent unclear lrw api able perfect policy
relatively easy hand code policy colored blocks world language
learner hence inadequate knowledge representation answer predicates
action types domain deterministic counterpart
stochastic variants previously considered difference apparently
interacts badly learners search bias causing fail perfect policy
nevertheless two along probabilistic shown
indicate good policy expressible language lrw api
good policies complex relational mdps makes lrw api one
techniques simultaneously cope complexity resulting stochasticity
relational structure domains

related work
boutilier et al presented first exact solution technique relational mdps
structured dynamic programming however practical implementation
provided primarily due need simplification first order
logic formulas ideas however served basis logic programming
system kersting van otterlo deraedt successfully applied blocksworld involving simple goals simplified logistics world style
inherently limited domains exact value functions policies
compactly represented chosen knowledge representation unfortunately
generally case types domains consider particularly
horizon grows nevertheless providing techniques directly reason
mdp model important direction note api essentially ignores
underlying mdp model simply interacts mdp simulator black box
interesting direction consider principled approximations techniques discover good policies difficult domains considered
guestrin et al class mdp value function representation
used compute approximate value function could generalize across different sets
objects promising empirical shown multi agent tactical battle domain
presently class representation support representation features commonly found classical domains e g relational facts
b change time thus directly applicable contexts however extending work richer representations interesting direction ability
reason globally domain may give advantages compared api
closely related work relational reinforcement learning rrl dzeroski et al form online api learns relational value function approximations q value functions learned form relational decision trees q trees
used learn corresponding policies p trees rrl clearly demonstrate


fiapi policy language bias

difficulty learning value function approximations relational domains compared p trees q trees tend generalize poorly much larger rrl yet demonstrated
scalability complex considered hereprevious rrl blocks world
experiments include relatively simple goals lead value functions much
less complex ones reason suspect rrl would difficulty
domains consider precisely value function approximation step
avoid however needs experimentally tested
note however api advantage unconstrained
simulator whereas rrl learns irreversible world experience pure rl
simulator able estimate q values actions training state
providing us rich training data without simulator rrl able directly
estimate q value action training statethus rrl learns q tree
provide estimates q value information needed learn p tree way valuefunction learning serves critical role simulator unavailable believe
many relational possible learn model simulator
world experiencein case api incorporated
component rrl otherwise finding ways avoid learning effectively
learn relational value functions rrl interesting direction
researchers classical long studied techniques learning improve
performance collection survey work learning domains see minton zimmerman kambhampati two primary approaches learn domain specific control rules guiding search planners e g
minton carbonell knoblock kuokka etzioni gil veloso carbonell perez
borrajo fink blythe estlin mooney huang selman kautz
ambite knoblock minton aler borrajo isasi
closely related learn domain specific reactive control policies khardon martin
geffner yoon et al
regarding latter work novel api iteratively improve stand alone
control policies regarding former theory search planners iteratively
improved continually adding newly learned control knowledgehowever difficult avoid utility minton e swamped low utility rules
critically policy language bias confronts issue preferring simpler policies
learning tied base planner let alone tied single particular base planner unlike previous work rather require domain simulator
ultimate goal systems allow large difficult
beyond reach domain independent technology clearly learning
achieve goal requires form bootstrapping almost previous systems
relied human purpose far common human bootstrapping
learning small human provides small
distribution learner limiting number objects e g blocks
blocks world control knowledge learned small
work human must ensure small distribution good control knowledge
small good large target distribution contrast long complex blocks world goal rrl achieve b n block environment
consider blocks world goals involve n blocks



fifern yoon givan

random walk bootstrapping applied without human assistance directly
large domains however already pointed goal performing well
lrw distribution may correspond well particular target
distribution
bootstrapping similar spirit bootstrapping framework learning exercises natarajan reddy tadepalli learner provided exercises order increasing difficulty learning
easier learner able use knowledge skills order bootstrap learning harder work however previously relied
human provide exercises typically requires insight domain
underlying form control knowledge planner work viewed
automatic instantiation learning exercises specifically designed learning lrw
policies
random walk bootstrapping similar used micro hillary
finkelstein markovitch macro learning system solving
work instead generating via random walks starting initial state random
walks generated backward goal states assumes actions
invertible given set backward actions assumptions hold
backward random walk may preferable provided goal
distribution match well goals generated forward random walks
course cases forward random walks may preferable micro hillary
empirically tested n n sliding puzzle domain however discussed work
remain challenges applying system complex domains parameterized actions recursive structure familiar strips domains best
knowledge idea learning random walks previously explored
context strips domains
idea searching good policy directly policy space rather value function
space primary motivation policy gradient rl however
largely explored context parametric policy spaces
demonstrated impressive success number domains appears difficult define
policy spaces types considered
api viewed type reduction reinforcement
learning classification learning solve mdp generating solving
series cost sensitive classification recently several
proposals reducing reinforcement learning classification dietterich wang
proposed reinforcement learning batch value function approximation
one proposed approximations enforced learned approximation assign
best action highest value type classifier learning lagoudakis parr
proposed classification api closely related primary difference form classification produced iteration
generate standard multi class classification whereas generate cost sensitive
bagnell kakade ng schneider introduced closely related learning non stationary policies reinforcement learning specified horizon
time h learns sequence h policies iteration policies
held fixed except one optimized forming classification via policy


fiapi policy language bias

rollout finally langford zadrozny provide formal reduction reinforcement learning classification showing accurate classification learning implies
near optimal reinforcement learning uses optimistic variant sparse
sampling generate h classification one horizon time step

summary future work
introduced variant api learns policies directly without representing
approximate value functions allowed us utilize relational policy language
learning compact policy representations introduced api bootstrapping
technique goal domains experiments lrw api
combines techniques able good policies variety
relational mdps corresponding classical domains stochastic variants
know previous mdp technique successfully applied

experiments pointed number weaknesses current first
bootstrapping technique long random walks correspond
well distribution interest investigating automatic bootstrapping
techniques interesting direction related general exploration
reward shaping reinforcement learning second seen limitations
current policy language learner partly responsible failures
system cases must depend human provide useful features
system extend policy language develop advanced learning techniques policy language extensions considering include extensions
knowledge representation used represent sets objects domain particular
route finding maps grids well non reactive policies incorporate search
decision making
consider ever complex domains inevitable brute force
enumeration learning policies trajectories scale presently
policy learner well entire api technique makes attempt use definition
domain one available believe developing learner exploit
information bias search good policies important direction future work
recently gretton thiebaux taken step direction logical
regression domain model generate candidate rules learner developing tractable variations promising direction addition
exploring ways incorporating domain model modelblind approaches critical ultimately scalable ai systems need combine
experience stronger forms explicit reasoning

initial state distribution dictated policies previous time steps held fixed
likewise actions selected along rollout trajectories dictated policies future time steps
held fixed



fifern yoon givan

acknowledgments
would thank lin zhu originally suggesting idea random walks
bootstrapping would thank reviewers editors helping
vastly improve work supported part nsf grants iis
iis

appendix omitted proofs
proposition let h finite class deterministic policies h

set n ln h
trajectories drawn independently dh probability
every h consistent trajectories satisfies v v vmax h
proof first introduce basic properties notation used
deterministic policy consistent trajectory dh entirely
determined underlying mdp transition dynamics implies two deterministic policies consistent trajectory dh dh
denote v cumulative discounted reward accumulated executing trajectory
p
policy v h dh v summation taken
length h trajectories simply consistent finally set
p
trajectories let dh dh giving cumulative probability
generating trajectories
consider particular h h consistent n trajectories
let denote set length h trajectories consistent
denote set trajectories consistent following khardon b
first give standard argument showing high probability dh see
consider probability consistent n ln h
trajectories


given dh probability occurs n en h


thus probability choosing h h
thus probability

least know dh note dh dh
given condition dh v h v h vmax
considering difference two value functions
v h v h

x

dh v





x



v

x

dh dh v



dh





dh v



dh



x

x

v



dh



dh

dh dh

vmax dh

vmax

x

vmax


x

v

dh v

fiapi policy language bias

third lines follows since dh dh consistent
last line follows substituting assumption dh dh previous
line combining approximation due finite horizon
v v v h v h h vmax
get probability least v v vmax h completes
proof
proposition


mdp q advantage least

h log


vmax

vmax







w




ln




state probability least
proof given real valued random variable x bounded absolute value xmax
average x w independently drawn samples x
q additive chernoff bound states
probability least e x x xmax wln
note qh expectation random variable x r

vh q simply average w independent samples x

chernoff bound tells us probability least
qh q
q




vmax ln ln
number actions substituting choice w
w

get probability least qh q satisfied actions
simultaneously know q qh h vmax choice

h gives q qh combining relationships get

probability least q q holds actions simultaneously
use bound high probability q value estimates

actions within range actions outside
range particular consider action action
q q bound get

q q otherwise assumption mdp
q advantage get q q bound implies

q q relationships definition imply
probability least

appendix b learned policies
give final taxonomic decision list policies learned domain
experiments rather write rules form x xk l l lm


fifern yoon givan

drop variables head simply write l l lm addition
use notation r short hand r r relation interpreting policies important remember rule action type
preconditions action type implicitly included constraints thus rules
often allow actions legal actions never considered
system
gripper
move x gat carry gripper x gat robby x gat
cat room x cat ball
drop x gat robby
pick x gat gat carry gripper x gat robby
pick x gat room x gat robby
pick x gat robby
briefcase
put x gat
move x cat location x gat cis
move x gat x cat
take x cat
move x gis
move x gat cis
put x universal
schedule
immersion paint x painted x x gpainted x
drill press x ghas holeo x x ghas holew x
lathe x shape cylindrical x gshape cylindrical
drill press x ghas holew x
drill press x ghas holeo x
grind x surface condition smooth x gsurface condition smooth
polish x surface condition polished x gsurface condition polished
time step
elevator
depart x gserved
x destin boarded x destin gserved
x destin boarded x destin gserved x origin boarded x
destin boarded
board x cserved x gserved
x origin gserved x destin boarded x destin gserved x
origin cserved x destin passenger x destin boarded
x origin gserved x origin cserved x destin boarded



fiapi policy language bias

x origin boarded x destin boarded
freecell
sendtohome x canstack canstack suit suit incell x ghome
move b x canstack ghome x canstack ghome x value
colspace x canstack suit suit bottomcol
move x canstack canstack ghome x canstack suit suit bottomcol x bottomcol x canstack ghome x canstack
canstack value cellspace x canstack suit suit incell
x canstack bottomcol x suit suit canstack value cellspace
x value colspace canstack suit suit incell x
canstack chome
sendtohome b x ghome
sendtohome x canstack canstack suit suit incell x ghome
sendtohome x ghome x canstack ghome x canstack
ghome x ghome
move b x canstack ghome x value colspace x canstack
suit suit bottomcol
sendtofree x ghome x ghome
sendtohome x canstack canstack ghome x ghome
sendtohome ghome x value colspace x canstack
ghome x ghome x ghome
newcolfromfreecell x ghome
sendtohome x canstack ghome x ghome x ghome
move b x value value home x value colspace x canstack suit
suit bottomcol
sendtohome x canstack canstack suit suit incell x ghome
sendtohome x canstack ghome x ghome
sendtofree x canstack ghome x suit suit bottomcol x
bottomcol
move x canstack clear x canstack canstack value
cellspace x ghome x ghome x canstack bottomcol x
canstack canstack value cellspace x canstack suit
suit incell x bottomcol x suit suit canstack value
cellspace x value colspace x canstack suit suit
incell x canstack chome
move x suit suit chome x ghome x ghome x
canstack bottomcol
sendtohome x canstack canstack ghome x ghome x ghome
sendtohome x canstack canstack ghome x suit suit bottomcol x ghome
sendtofree x canstack canstack value cellspace x canstack chome
sendtohome x canstack suit suit incell x canstack value
cellspace x ghome
sendtonewcol x canstack canstack ghome
sendtofree x canstack canstack ghome x canstack ghome
x ghome x canstack suit suit incell



fifern yoon givan

sendtofree x canstack canstack ghome x canstack bottomcol x canstack canstack ghome
sendtofree x canstack canstack ghome x canstack
ghome x canstack suit suit bottomcol
sendtohome x canstack canstack ghome x canstack
ghome x ghome x ghome
sendtofree x canstack canstack ghome x canstack canstack
ghome x ghome x canstack canstack value
cellspace
sendtofree x canstack chome x suit suit canstack ghome
sendtohome x ghome x suit suit bottomcol x canstack
ghome x ghome
sendtofree x canstack ghome x canstack ghome
sendtofree x canstack ghome x ghome x canstack
ghome
sendtohome x canstack bottomcol x canstack ghome x
ghome
sendtofree x canstack canstack ghome x suit suit
bottomcol x ghome
sendtohome x canstack ghome x suit suit bottomcol x
ghome
sendtofree x ghome x canstack canstack ghome
sendtofree b x ghome
sendtofree x universal
logistics
fly airplane x gat airport x gat airplane x gat
truck x gat airport
load truck x gat airport x gat gat truck x
cat location
drive truck x gat truck x city city airplane x
gat truck
unload truck x gat obj x gat obj x gat airplane x gat truck x gat truck
fly airplane x gat airplane x gat truck x
gat truck
unload airplane x gat airport x gat airplane
load truck x gat location x gat truck x gat
location
unload truck x gat truck x airport x gat airport x gat airplane
fly airplane x gat truck x gat gat location x
cat obj
drive truck x gat location x gat truck x
airplane
unload truck x gat gat airport x gat airport
fly airplane x gat gat location x gat cat obj x
gat airplane x obj x gat airport x
obj



fiapi policy language bias

unload truck x gat airport
load truck x cat gat airplane x gat location
load truck x gat cat gat airplane x gat truck x
gat gat airplane
load truck x gat airport x gat truck
fly airplane x gat airplane x cat obj
fly airplane x gat cat obj x gat cat obj x
gat gat truck
load truck x gat airplane x gat truck x cat obj
load airplane x gat airport x cat location x gat
airplane x gat airport
fly airplane x gat airplane x truck
load truck x cat gat airport x gat airport
drive truck x obj x cat obj x gat gat location
load truck x gat cat cat airport x cat location
fly airplane x gat airplane x obj
drive truck x obj
drive truck x gat gat airport x gat airport x
airplane
fly airplane x cat gat truck x gat gat location
load truck x gat obj x cat location
drive truck x gat airplane x cat obj
drive truck x airplane x gat truck
unload airplane x cat obj x gat airport
drive truck x gat truck
load truck x airport x gat airport
fly airplane x gat location
fly airplane x obj x gat gat location x gat airport
x obj x gat cat obj
drive truck x airplane
load airplane x gat airport
blocks world
stack x gon holding x con min gon x gon table
putdown
unstack x min gon x con min gon
unstack x gon clear x gon min gon x gon table
x gon clear
pickup x gon con min gon x gon clear x gon con table
unstack x con gon clear x gon min gon x gon con
clear



fifern yoon givan

unstack x gon min gon
unstack x gon table x gon con min gon x gon clear
unstack x con min gon x gon table x gon ontable x gon gon table x gon clear
unstack x con clear x gon con table
unstack x gon clear x min gon
ground logistics
load x gin city x cin city x gin city
unload x gin x
drive x gin x
drive x gin block x gin city x car x clear
drive x gin rain x truck
colored blocks world
pick block x con top table x gon top top block
put block x con top con top block x gon top holding
x con top table
pick block x con top block x top gon top table
x gon top gon top block x con top block x topof gon top block x gon top gon top block
pick block x con top table x gon top con top table x gon top top block
put block x con top top table x gon top holding x
con top table
put block x con top top block x gon top gon top
block
put block x gon top holding x con top table
put block x table
pick block x con top table x gon top con top table
pick block x gon top con top table x table x gon top
gon top block x gon top top table
pick block x top con top block x gon top con top
table
pick block x top block x con top table x gontop top block x gon top top block
pick block x gon top gon top table
boxworld
drive truck x gbox city box city x x fly truck city
previous x drive previous x fly truck city previous x fly box city box x drive drive box city box
x fly truck city box truck gbox city city
unload box truck city x gbox city truck city previous x gboxat city box x box city previous x gbox city drive candrive fly city x box truck gbox city previous
drive truck x box truck gbox city x x drive truck city
box truck gbox city city



fiapi policy language bias

drive truck x drive box city previous x fly drive box city
box x drive fly truck city truck x drive truck city
box truck gbox city city x previous x drive drive x x
truck city box truck gbox city city x fly previous
x drive box city box x drive drive x x drive
truck city truck
load box truck city x gbox city drive truck city truck x
plane city previous x drive drive fly city x drive
truck city previous
unload box truck city x gbox city box truck truck x canfly truck city box truck gbox city city x gbox city city
drive truck x box truck gbox city previous x drive gbox city
gbox city previous x plane city plane x fly gbox atcity gbox city previous
fly plane x box plane gbox city x
unload box plane city x gbox city previous
fly plane x drive truck city box truck gbox city city x
gbox city box x plane city previous x previous
load box plane city x gbox city fly previous x truck city
previous x drive truck city box truck gbox city city
drive truck x box truck gbox city x x drive fly previous x drive fly city
load box truck city x gbox city previous

references
aler r borrajo isasi p genetic programming learn improve
control knowledge artificial intelligence
ambite j l knoblock c minton learning plan rewriting rules
artificial intelligence systems pp
bacchus f aips competition ai magazine
bacchus f kabanza f temporal logics express search control knowledge artificial intelligence
bagnell j kakade ng schneider j policy search dynamic programming proceedings th conference advances neural information
processing
bellman r dynamic programming princeton university press
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific
boutilier c dearden r approximating value trees structured dynamic
programming saitta l ed international conference machine learning
boutilier c dearden r goldszmidt stochastic dynamic programming
factored representations artificial intelligence
boutilier c reiter r price b symbolic dynamic programming first order
mdps international joint conference artificial intelligence


fifern yoon givan

dean givan r model minimization markov decision processes national
conference artificial intelligence pp
dean givan r leach model reduction techniques computing approximately optimal solutions markov decision processes conference uncertainty
artificial intelligence pp
dietterich wang x batch value function approximation via support vectors
proceedings conference advances neural information processing
dzeroski deraedt l driessens k relational reinforcement learning machine learning
estlin mooney r j multi strategy learning search control partialorder national conference artificial intelligence
fern yoon givan r approximate policy iteration policy language bias proceedings th conference advances neural information
processing
finkelstein l markovitch selective macro learning
application nxn sliding tile puzzle journal artificial intelligence

givan r dean greig equivalence notions model minimization
markov decision processes artificial intelligence
gretton c thiebaux exploiting first order regression inductive policy
selection conference uncertainty artificial intelligence
guestrin c koller gearhart c kanodia n generalizing plans
environments relational mdps international joint conference artificial intelligence
guestrin c koller parr r venkataraman b efficient solution
factored mdps journal artificial intelligence
hoffman j porteous j sebastia l ordered landmarks journal
artificial intelligence
hoffmann j nebel b system fast plan generation
heuristic search journal artificial intelligence
howard r dynamic programming markov decision processes mit press
huang c selman b kautz h learning declarative control rules
constraint international conference machine learning pp

kearns j mansour ng sparse sampling nearoptimal large markov decision processes machine learning

kersting k van otterlo deraedt l bellman goes relational proceedings
twenty first international conference machine learning


fiapi policy language bias

khardon r learning action strategies domains artificial intelligence
khardon r b learning take actions machine learning
lagoudakis parr r reinforcement learning classification leveraging
modern classifiers international conference machine learning
langford j zadrozny b reducing step reinforcement learning classification
http hunch net jl projects reductions rl class colt submission ps
martin geffner h learning generalized policies domains
concept languages international conference principles knowledge representation reasoning
mataric reward functions accelarated learning proceedings international conference machine learning
mcallester givan r taxonomic syntax first order inference journal
acm
mcallester observations cognitive judgements national conference
artificial intelligence
mcgovern moss e barto building basic block instruction scheduler
reinforcement learning rollouts machine learning
minton quantitative concerning utility explanation learning
national conference artificial intelligence
minton ed machine learning methods morgan kaufmann
minton carbonell j knoblock c kuokka r etzioni gil
explanation learning solving perspective artificial intelligence

natarajan b k learning exercises annual workshop computational
learning theory
reddy c tadepalli p learning goal decomposition rules exercises
international conference machine learning pp morgan kaufmann
rivest r learning decision lists machine learning
tesauro g practical issues temporal difference learning machine learning

tesauro g galperin g line policy improvement monte carlo search
conference advances neural information processing
tsitsiklis j van roy b feature methods large scale dp machine
learning
veloso carbonell j perez borrajo fink e blythe j integrating
learning prodigy architecture journal experimental
theoretical ai
wu g chong e givan r congestion control via online sampling infocom


fifern yoon givan

yan x diaconis p rusmevichientong p van roy b solitaire man versus
machine conference advances neural information processing
yoon fern givan r inductive policy selection first order mdps
conference uncertainty artificial intelligence
younes h extending pddl model stochastic decision processes proceedings
international conference automated scheduling workshop
pddl
zimmerman kambhampati learning assisted automated looking back taking stock going forward ai magazine




