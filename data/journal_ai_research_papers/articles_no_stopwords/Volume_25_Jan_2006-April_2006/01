journal artificial intelligence

submitted published

decision theoretic non markovian rewards
sylvie thiebaux
charles gretton
john slaney
david price

sylvie thiebaux anu edu au
charles gretton anu edu au
john slaney anu edu au
david price anu edu au

national ict australia
australian national university
canberra act australia

froduald kabanza

kabanza usherbrooke ca

departement dinformatique
universite de sherbrooke
sherbrooke quebec j k r canada

abstract
decision process rewards depend history rather merely current state called decision process non markovian rewards nmrdp decisiontheoretic many desirable behaviours naturally expressed properties execution sequences rather properties states nmrdps form
natural model commonly adopted fully markovian decision process mdp model
tractable solution methods developed mdps directly apply
presence non markovian rewards number solution methods nmrdps
proposed literature exploit compact specification non markovian
reward function temporal logic automatically translate nmrdp equivalent mdp solved efficient mdp solution methods presents
nmrdpp non markovian reward decision process planner software platform
development experimentation methods decision theoretic nonmarkovian rewards current version nmrdpp implements single interface
family methods existing well approaches describe detail include dynamic programming heuristic search structured methods
nmrdpp compare methods identify certain features affect
performance nmrdpps treatment non markovian rewards inspired treatment
domain specific search control knowledge tlplan planner incorporates
special case first international probabilistic competition nmrdpp
able compete perform well domain independent hand coded
tracks search control knowledge latter

c

ai access foundation rights reserved

fithiebaux gretton slaney price kabanza

introduction

markov decision processes mdps widely accepted preferred model
decision theoretic boutilier dean hanks fundamental
assumption behind mdp formulation system dynamics
reward function markovian therefore information needed determine reward
given state must encoded state
requirement easy meet many desirable
behaviours naturally expressed properties execution sequences see e g drummond haddawy hanks bacchus kabanza pistore traverso
typical cases include rewards maintenance property periodic
achievement goal achievement goal within given number steps
request made even simply first achievement goal
becomes irrelevant afterwards
instance consider health care robot assists ederly disabled people
achieving simple goals reminding important tasks e g taking pill
entertaining checking transporting objects e g checking stoves
temperature bringing coffee escorting searching e g glasses
nurse cesta et al domain might want reward robot making
sure given patient takes pill exactly every hours penalise fails
prevent patient within time frame may
reward repeatedly visiting rooms ward given order reporting
detects may receive reward patients request answered
within appropriate time frame etc another example elevator control domain
koehler schuster elevator must get passengers origin
destination efficiently possible attempting satisfying range
conditions providing priority services critical customers domain
trajectories elevator desirable others makes natural encode
assigning rewards trajectories
decision process rewards depend sequence states passed
rather merely current state called decision process non markovian
rewards nmrdp bacchus boutilier grove difficulty nmrdps
efficient mdp solution methods directly apply traditional way
circumvent formulate nmrdp equivalent mdp whose states
augmenting original nmrdp extra information capturing
enough history make reward markovian hand crafting mdp however
difficult general exacerbated fact size mdp
impacts effectiveness many solution methods therefore interest
automating translation mdp starting natural specification nonmarkovian rewards systems dynamics bacchus et al bacchus boutilier
grove focus



fidecision theoretic non markovian rewards

existing approaches
solving nmrdps setting central issue define non markovian reward
specification language translation mdp adapted class mdp solution
methods representations would use type hand
precisely tradeoff effort spent translation e g producing
small equivalent mdp without many irrelevant history distinctions effort required
solve appropriate resolution tradeoff depends type representations
solution methods envisioned mdp instance structured representations
solution methods ability ignore irrelevant information may cope
crude translation state flat representations methods require
sophisticated translation producing mdp small feasible
two previous proposals within line rely past linear temporal
logic pltl formulae specify behaviours rewarded bacchus et al
nice feature pltl yields straightforward semantics non markovian
rewards lends range translations crudest finest two
proposals adopt different translations adapted two different types solution
methods representations first bacchus et al targets classical state
solution methods policy iteration howard generate complete policies
cost enumerating states entire mdp consequently adopts expensive
translation attempts produce minimal mdp contrast second translation
bacchus et al efficient crude targets structured solution methods
representations see e g hoey st aubin hu boutilier boutilier dearden
goldszmidt feng hansen require explicit state enumeration

first contribution provide language translation adapted
another class solution methods proven quite effective dealing large
mdps namely anytime state heuristic search methods lao hansen
zilberstein lrtdp bonet geffner ancestors barto bardtke
singh dean kaelbling kirman nicholson thiebaux hertzberg shoaff
schneider methods typically start compact representation
mdp probabilistic operators search forward initial state
constructing states expanding envelope policy time permits may
produce approximate even incomplete policy explicitly construct explore
fraction mdp neither two previous proposals well suited
solution methods first cost translation performed
prior solution phase annihilates benefits anytime second
size mdp obtained obstacle applicability state
methods since cost translation size mdp
severely impact quality policy obtainable deadline need
appropriate resolution tradeoff two
following main features translation entirely embedded
anytime solution method full control given parts mdp
explicitly constructed explored mdp obtained minimal


fithiebaux gretton slaney price kabanza

minimal size achievable without stepping outside anytime framework e
without enumerating parts state space solution method would necessarily
explore formalise relaxed notion minimality call blind minimality
reference fact require lookahead beyond fringe
appropriate context anytime state solution methods want
minimal mdp achievable without expensive pre processing
rewarding behaviours specified pltl appear
way achieving relaxed notion minimality powerful blind minimality without
prohibitive translation therefore instead pltl adopt variant future linear
temporal logic fltl specification language extend handle rewards
language complex semantics pltl enables natural translation blind minimal mdp simple progression reward formulae moreover
search control knowledge expressed fltl bacchus kabanza fits particularly
nicely framework used dramatically reduce fraction search
space explored solution method
system
second contribution nmrdpp first reported implementation nmrdp solution
methods nmrdpp designed software platform development experimentation common interface given description actions domain nmrdpp
lets user play compare encoding styles non markovian rewards
search control knowledge translations resulting nmrdp mdp
mdp solution methods solving made record
range statistics space time behaviour supports
graphical display mdps policies generated
nmrdpps primary interest treatment non markovian rewards
competitive platform decision theoretic purely markovian rewards
first international probabilistic competition nmrdpp able enrol
domain independent hand coded tracks attempting featuring
contest thanks use search control knowledge scored second place
hand coded track featured probabilistic variants blocks world logistics
surprisingly scored second domain independent subtrack consisting taken blocks world logistic domains
latter released participants prior competition
experimental analysis
third contribution experimental analysis factors affect performance
nmrdp solution methods nmrdpp compared behaviours
influence parameters structure degree uncertainty dynamics
type rewards syntax used described reachability conditions
tracked relevance rewards optimal policy able identify number
general trends behaviours methods provide advice concerning
best suited certain circumstances experiments lead us rule one



fidecision theoretic non markovian rewards

methods systematically underperforming identify issues claim
minimality made one pltl approaches
organisation
organised follows section begins background material mdps
nmrdps existing approaches section describes section
presents nmrdpp sections report experimental analysis approaches section explains used nmrdpp competition section concludes
remarks related future work appendix b gives proofs theorems
material presented compiled series recent conference workshop
papers thiebaux kabanza slaney b gretton price thiebaux
b details logic use represent rewards may found
slaney

background
mdps nmrdps equivalence
start notation definitions given finite set states write
set finite sequences states set possibly infinite
state sequences stands possibly infinite state sequence
natural number mean state index mean prefix
h denotes concatenation
mdps
markov decision process type consider tuple hs pr ri
finite set fully observable states initial state finite set actions
denotes subset actions applicable pr
family probability distributions pr probability
state performing action state r ir reward function
r immediate reward state well known mdp
compactly represented dynamic bayesian networks dean kanazawa
boutilier et al probabilistic extensions traditional languages see e g
kushmerick hanks weld thiebaux et al younes littman
stationary policy mdp function
action executed state value v policy seek
maximise sum expected future rewards infinite horizon discounted
far future occur
v lim e
n

x
n





r



discount factor controlling contribution distant rewards



fithiebaux gretton slaney price kabanza

















r










initial state p false two actions
possible causes transition probability
change probability b
transition probabilities state p true
actions c stay go lead
respectively probability
reward received first time p true
subsequently rewarded state sequences
hs
hs
hs
hs
etc
































































b















c











figure simple nmrdp
nmrdps
decision process non markovian rewards identical mdp except
domain reward function idea process passed
state sequence stage reward r received stage figure
gives example reward function policy nmrdp depends history
mapping value policy expectation
discounted cumulative reward infinite horizon
x

n

v lim e
r
n



e
decision process hs pr ri state let
stand
set state sequences rooted feasible actions
e

pr note definition
e
depend r therefore applies mdps nmrdps
equivalence
clever developed solve mdps cannot directly applied nmrdps
one way dealing translate nmrdp equivalent mdp
expanded state space bacchus et al expanded states mdp
e states short augment states nmrdp encoding additional information
sufficient make reward history independent instance want reward
first achievement goal g nmrdp states equivalent mdp would
carry one extra bit information recording whether g already true e state
seen labelled state nmrdp via function definition
history information dynamics nmrdps markovian actions
probabilistic effects mdp exactly nmrdp following definition
adapted given bacchus et al makes concept equivalent mdp
precise figure gives example



fidecision theoretic non markovian rewards


























































































r








b






c













































































































b



c


























figure mdp equivalent nmrdp figure
initial state state rewarded states

definition mdp hs pr r equivalent nmrdp hs pr ri
exists mapping


pr
exists unique
pr pr
e
e
feasible state sequence


r r
items ensure bijection feasible state sequences nmrdp
feasible e state sequences mdp therefore stationary policy mdp
reinterpreted non stationary policy nmrdp furthermore item ensures
two policies identical values consequently solving nmrdp optimally
reduces producing equivalent mdp solving optimally bacchus et al
proposition let nmrdp equivalent mdp policy
e
let function defined sequence prefixes


j j j policy v v
technically definition allows sets actions different action
differ must inapplicable reachable states nmrdp e states equivalent
mdp practical purposes seen identical



fithiebaux gretton slaney price kabanza

existing approaches
existing approaches nmrdps bacchus et al use temporal logic
past pltl compactly represent non markovian rewards exploit compact
representation translate nmrdp mdp amenable shelf solution
methods however target different classes mdp representations solution methods consequently adopt different styles translations
bacchus et al target state mdp representations equivalent mdp
first generated entirely involves enumeration e states transitions
solved traditional dynamic programming methods
value policy iteration methods extremely sensitive number
states attention paid producing minimal equivalent mdp least number
states first simple translation call pltlsim produces large mdp
post processed minimisation solved another call pltlmin
directly minimal mdp relies expensive pre processing phase
second bacchus et al call pltlstr targets structured
mdp representations transition model policies reward value functions represented compact form e g trees algebraic decision diagrams adds hoey et al
boutilier et al instance probability given proposition state
variable true execution action specified tree whose internal
nodes labelled state variables whose previous values given variable depends whose arcs labelled possible previous values variables
whose leaves labelled probabilities translation amounts augmenting
structured mdp temporal variables tracking relevant properties state
sequences together compact representation dynamics e g trees
previous values relevant variables non markovian reward function
terms variables current values structured solution methods structured
policy iteration spudd run resulting structured mdp neither
translation solution methods explicitly enumerates states
review approaches detail reader referred respective
papers additional information
representing rewards pltl
syntax pltl language chosen represent rewarding behaviours
propositional logic augmented operators previously since see emerson whereas classical propositional logic formula denotes set states subset
pltl formula denotes set finite sequences states subset formula
without temporal modality expresses property must true current state e
last state finite sequence f specifies f holds previous state
state one last f f requires f true point sequence unless point present f held ever since formally
modelling relation stating whether formula f holds finite sequence defined
recursively follows
p iff p p p set atomic propositions



fidecision theoretic non markovian rewards

f iff f
f f iff f f
f iff f
f f iff j j f k j k k f
f f meaning f true
one define useful operators
f meaning f true e g g
g denotes
point fif
set finite sequences ending state g true first time sequence
k f
useful abbreviation k k times ago k iterations modality
ki f f true k last steps fik f ki f f true
k last steps
non markovian reward functions described set pairs ri
pltl reward formula ri real semantics reward assigned
sequence sum ri sequence model let
f denote set reward formulae description reward function bacchus
et al give list behaviours might useful reward together
expression pltl instance f atemporal formula f r rewards
r units achievement f whenever happens markovian reward
f r rewards every state following including achievement f
contrast
f r rewards first occurrence f f fik f r rewards occurrence
f
f every k steps n r rewards nth state independently
properties f f f r rewards occurrence f immediately followed
f f reactive called response formulae describe
achievement f triggered condition command c particularly useful
c r every state f true following first issue
written f
command rewarded alternatively written f f c r
first occurrence f rewarded command common
k c r
reward achievement f within k steps trigger write example f
reward states f holds
theoretical point view known lichtenstein pnueli zuck
behaviours representable pltl exactly corresponding star free regular
languages non star free behaviours pp reward even number states
containing p therefore representable course non regular behaviours
pn q n e g reward taking equal numbers steps left right shall
speculate severe restriction purposes
principles behind translations
three translations mdp pltlsim pltlmin pltlstr rely equivalence f f f f f f decompose temporal modalities
requirement last state sequence requirement
prefix sequence precisely given state formula f one com



fithiebaux gretton slaney price kabanza

pute f formula reg f called regression f regression
property f true finite sequence ending iff
reg f true prefix reg f represents must
true previously f true reg defined follows
reg p iff p otherwise p p
reg f reg f
reg f f reg f reg f
reg f f
reg f f reg f reg f f f
instance take state p holds q take f q p q
meaning q must false step ago must held point
past p must held since q last reg f q p q
f hold previous stage q false p q requirement
still hold p q false reg f indicating f
cannot satisfied regardless came earlier sequence
notational convenience x set formulae write x x x x x
translations exploit pltl representation rewards follows expanded
state e state generated mdp seen labelled set sub f
subformulae reward formulae f negations subformulae must
true paths leading e state sufficient determine current
truth reward formulae f needed compute current reward ideally
small enough enable e contain
subformulae draw history distinctions irrelevant determining reward
one point another note however worst case number distinctions
needed even minimal equivalent mdp may exponential f happens
instance formula k f requires k additional bits information memorising
truth f last k steps
pltlsim
choice bacchus et al consider two cases simple case
call pltlsim mdp obeying properties produced simply labelling
e state set subformulae sub f true sequence leading
e state mdp generated forward starting initial e state labelled
set sub f subformulae true sequence
hs successors e state labelled nmrdp state subformula set
generated follows labelled successor nmrdp
set subformulae sub f reg
instance consider nmrdp shown figure set f qp consists
single reward formula set sub f consists subformulae reward formula
size f reward formula measured length size f set reward formulae
f measured sum lengths formulae f



fidecision theoretic non markovian rewards

start state

p

b

b




b

q

b


p q

b

initial state p q false
p false action independently sets
p q true probability
p q false action b sets q true
probability actions
effect otherwise reward obtained
whenever q p optimal policy
apply b q gets produced making
sure avoid state left hand side
apply p gets produced
apply b indifferently forever

figure another simple nmrdp
negations sub f p q p p q p p q p p q
p equivalent mdp produced pltlsim shown figure
pltlmin
unfortunately mdps produced pltlsim far minimal although could
postprocessed minimisation invoking mdp solution method
expansion may still constitute serious bottleneck therefore bacchus et al consider
complex two phase translation call pltlmin capable producing
mdp satisfying property preprocessing phase iterates states
computes state set l subformulae function l
solution fixpoint equation l f reg l successor
subformulae l candidates inclusion sets labelling respective
e states labelled subsequent expansion phase taking
l l instead sub f sub f subformulae
l exactly relevant way feasible execution sequences starting
e states labelled rewarded leads expansion phase produce minimal
equivalent mdp
figure shows equivalent mdp produced pltlmin nmrdp example
figure together function l labels built observe
mdp smaller pltlsim mdp reach state left hand side
p true q false point tracking values subformulae
q cannot become true reward formula cannot reflected fact
l p contains reward formula
worst case computing l requires space number iterations
exponential f hence question arises whether gain expansion
phase worth extra complexity preprocessing phase one questions
experimental analysis section try answer
pltlstr
pltlstr translation seen symbolic version pltlsim set
added temporal variables contains purely temporal subformulae ptsub f reward
formulae f modality prepended unless already



fithiebaux gretton slaney price kabanza

start state
f f f f f
reward


b



p
f f f f f
reward



q
f f f f f
reward

b

p
f f f f f
reward

following subformulae sub f label
e states
f p
f q
f p
f p
f q p
f p
f q
f p
f p
f q p

b



p
f f f f f
reward


b

p q
f f f f f
reward
b



b

b

p q
f f f f f
reward
b
p q
f f f f f
reward

b

figure equivalent mdp produced pltlsim

start state
f f f
reward

p
f
reward

b

b



b

q
f f f
reward



function l given
l q p p p
l p q p
l q q p p p
l p q q p p p

b



following formulae label e states
f q p
f p
f p
f q p
f p
f p

p q
f f f
reward


b

p q
f f f
reward


b

p q
f f f
reward

b

figure equivalent mdp produced pltlmin



fidecision theoretic non markovian rewards

q

p



prv prv p

prv p



dynamics p





dynamics p





reward

figure adds produced pltlstr prv previously stands
ptsub f ptsub f repeatedly applying equivalence
f f f f f f subformula ptsub f express current
value hence reward formulae function current values formulae
state variables required compact representation transition
reward
nmrdp example figure set purely temporal variables ptsub f
p p identical ptsub f figure shows adds forming
part symbolic mdp produced pltlstr adds describing dynamics
temporal variables e adds describing effects actions b
respective values add describing reward
complex illustration consider example bacchus et al
p q r p q r
f

ptsub f p q r p q r r
set temporal variables used
p q r p q r r
equivalences reward decomposed expressed means
propositions p q temporal variables follows
p q r

p q r p q r
q r p p q r

q p
pltlsim underlying mdp produced pltlstr far minimal
encoded history features even vary one state next however size
problematic state approaches structured solution methods
enumerate states able dynamically ignore variables become
irrelevant policy construction instance solving mdp may


fithiebaux gretton slaney price kabanza

able determine temporal variables become irrelevant situation
track although possible principle costly realised good policy
dynamic analysis rewards contrast pltlmins static analysis bacchus et al
must encode enough history determine reward reachable future
states policy
one question arises circumstances analysis irrelevance structured solution methods especially dynamic aspects really effective
another question experimental analysis try address

fltl forward looking
noted section two key issues facing approaches nmrdps
specify reward functions compactly exploit compact representation
automatically translate nmrdp equivalent mdp amenable chosen
solution method accordingly goals provide reward function specification
language translation adapted anytime state solution methods
brief reminder relevant features methods consider two goals
turn describe syntax semantics language notion formula
progression language form basis translation translation
properties embedding solution method call
fltl finish section discussion features distinguish fltl
existing approaches
anytime state solution methods
main drawback traditional dynamic programming policy iteration
howard explicitly enumerate states reachable
entire mdp interest state solution methods may
produce incomplete policies enumerate fraction states policy iteration
requires
let e denote envelope policy set states reachable
non zero probability initial state policy defined
e say policy complete incomplete otherwise
set states e undefined called fringe policy
fringe states taken absorbing value heuristic common feature
anytime state perform forward search starting
repeatedly expanding envelope current policy one step forward adding one
fringe states provided admissible heuristic values fringe states
eventually converge optimal policy without necessarily needing explore
entire state space fact since operators used compactly represent
state space may even need construct small subset mdp
returning optimal policy interrupted convergence return
possibly incomplete often useful policy
methods include envelope expansion dean et al
deploys policy iteration judiciously chosen larger larger envelopes successive policy seed calculation next recent lao hansen


fidecision theoretic non markovian rewards

zilberstein combines dynamic programming heuristic search
viewed clever implementation particular case envelope expansion
fringe states given admissible heuristic values policy iteration run
convergence envelope expansions clever implementation runs
policy iteration states whose optimal value actually affected fringe
state added envelope another example backtracking forward search
space possibly incomplete policies rooted thiebaux et al performed interrupted point best policy found far returned real time
dynamic programming rtdp barto et al another popular anytime
mdps learning real time korf deterministic domains
asymptotic convergence guarantees rtdp envelope made sample
paths visited frequency determined current greedy policy
transition probabilities domain rtdp run line line given number
steps interrupted variant called lrtdp bonet geffner incorporates
mechanisms focus search states whose value yet converged resulting
convergence speed finite time convergence guarantees
fltl translation present targets anytime although
could used traditional methods value policy iteration
language semantics
compactly representing non markovian reward functions reduces compactly representing
behaviours interest behaviour mean set finite sequences states
subset e g hs hs hs figure recall
reward issued end prefix set behaviours compactly
represented straightforward represent non markovian reward functions mappings
behaviours real numbers shall defer looking section
represent behaviours compactly adopt version future linear temporal logic
fltl see emerson augmented propositional constant intended
read behaviour want reward happened reward received
language fltl begins set basic propositions p giving rise literals
l p p
stand true false respectively connectives classical
temporal modalities next u weak giving formulae
f l f f f f f f u f
weak f u f means f true f ever unlike
commonly used strong imply f eventually true
allows us define useful operator f f u f true
k f k iterations modality f
adopt notations
wk
true exactly k steps k f f f true within next k steps
v
k f ki f f true throughout next k steps
although negation officially occurs literals e formulae negation
normal form nnf allow write formulae involving usual way


fithiebaux gretton slaney price kabanza

provided equivalent nnf every formula equivalent
literal eventualities f true time
expressible restrictions deliberate use notation
logic theorise allocation rewards would indeed need means say
rewards received express features liveness
reward eventually fact mechanism ensuring
rewards given restricted purpose eventualities
negated dollar needed fact including would create technical difficulties
relating formulae behaviours represent
semantics language similar fltl important difference
interpretation constant depends behaviour b want reward
whatever modelling relation must indexed b therefore write
b f mean formula f holds th stage arbitrary sequence
relative behaviour b defining b first step description semantics
b iff b
b
b
b p p p iff p
b p p p iff p
b f f iff b f b f
b f f iff b f b f
b

f

iff b f

b f u f iff k j j k j b f k b f
note except subscript b first rule standard fltl
semantics therefore free formulae keep fltl meaning fltl
say b f iff b f b f iff b f
modelling relation b seen specifying formula holds
reading takes b input next final step use b relation define
formula f behaviour bf represents must rather assume
f holds solve b instance let f p e get rewarded
every time p true would bf set finite sequences ending
state containing p arbitrary f take bf set prefixes
rewarded f hold sequences

definition bf b b f
understand definition recall b contains prefixes end get
reward evaluates true since f supposed describe way rewards
received arbitrary sequence interested behaviours b make
true way make f hold without imposing constraints evolution
world however may many behaviours property take



fidecision theoretic non markovian rewards

intersection ensuring bf reward prefix prefix
every behaviour satisfying f pathological cases see section makes bf
coincide set inclusion minimal behaviour b b f reason
stingy semantics making rewards minimal f actually say rewards
allocated prefixes required truth instance p says
reward given every time p true even though generous distribution
rewards would consistent
examples
intuitively clear many behaviours specified means fltl formulae
simple way general translate past future tense expressions examples used illustrate pltl section expressible
naturally fltl follows
classical goal formula g saying goal p rewarded whenever happens
easily expressed p already noted bg set finite sequences states
p holds last state care p achieved get rewarded
state write p behaviour formula represents
set finite state sequences least one state p holds contrast
formula p u p stipulates first occurrence p rewarded e
specifies behaviour figure reward occurrence p every k
steps write k p k p k
response formulae achievement p triggered command c
write c p reward every state p true following first
issue command reward first occurrence p command write
c p u p bounded variants reward goal achievement
within k steps trigger command write example c k p reward
states p holds
worth noting express simple behaviours involving past tense operators
stipulate reward p true write u p say rewarded
p true since q write q u p
finally often useful reward holding p occurrence q
neatest expression q u p q q
reward normality
fltl therefore quite expressive unfortunately rather expressive
contains formulae describe unnatural allocations rewards instance
may make rewards depend future behaviours rather past may

b b f case free f logical theorem
bf e following normal set theoretic conventions limiting case harm since
free formulae describe attribution rewards
open question whether set representable behaviours fltl pltl
star free regular languages even behaviours little hope
practical translation one exists



fithiebaux gretton slaney price kabanza

leave open choice several behaviours rewarded example
dependence future p stipulates reward p going hold
next call formula reward unstable reward stable f amounts
whether particular prefix needs rewarded order make f true depend
future sequence example open choice behavior reward
p p says reward achievements goal p
reward achievements p determine call formula rewardindeterminate reward determinate f amounts set behaviours
modelling f e b b f unique minimum bf insufficient
small make f true
investigating fltl slaney examine notions reward stability
reward determinacy depth motivate claim formulae rewardstable reward determinate call reward normal precisely
capture notion funny business intuition ask reader
note needed rest reference define
definition f reward normal iff every every b b f iff
every bf b
property reward normality decidable slaney appendix give
simple syntactic constructions guaranteed reward normal formulae
reward abnormal formulae may interesting present purposes restrict attention
reward normal ones indeed stipulate part method reward normal
formulae used represent behaviours naturally formulae section
normal
fltl formula progression
defined language represent behaviours rewarded turn
computing given reward formula minimum allocation rewards states
actually encountered execution sequence way satisfy formula
ultimately wish use anytime solution methods generate state sequences
incrementally via forward search computation best done fly sequence
generated therefore devise incremental model checking
technique normally used check whether state sequence model fltl formula
bacchus kabanza technique known formula progression
progresses pushes formula sequence
progression technique shown essence computes modelling relation b given section however unlike definition b designed
useful states sequence become available one time defers
evaluation part formula refers future point next
state becomes available let state say last state sequence prefix
difficulties inherent use linear time formalisms contexts principle
directionality must enforced shared instance formalisms developed reasoning
actions event calculus ltl action theories see e g calvanese de giacomo
vardi



fidecision theoretic non markovian rewards

generated far let b boolean true iff behaviour b
rewarded let fltl formula f describe allocation rewards possible
futures progression f given b written prog b f formula
describe allocation rewards possible futures next state given
passed crucially function prog markovian depending
current state single boolean value b note prog computable
linear time length f free formulae collapses fltl formula
progression bacchus kabanza regardless value b assume prog
incorporates usual simplification sentential constants f simplifies
f simplifies f etc
fltl progression
prog true

prog false

prog b

prog b

prog b p
iff p otherwise
prog b p
iff p otherwise
prog b f f prog b f prog b f
prog b f f prog b f prog b f
prog b f
f
prog b f u f prog b f prog b f f u f
rew f
prog f

true iff prog false f
prog rew f f

fundamental property prog following b b
property b f iff b prog b f
proof

see appendix b



b function prog seems require b least b input course
progression applied practice f one state time
really want compute appropriate b namely represented
f similarly section turn second step use prog
decide fly whether newly generated sequence prefix bf
allocated reward purpose functions prog rew given
given f function prog defines infinite sequence
formulae hf f obvious way
f f
prog
decide whether prefix rewarded rew first tries progressing
formula boolean flag set false gives consistent
need reward prefix continue without rewarding


fithiebaux gretton slaney price kabanza

know must rewarded order satisfy f case
obtain must progress time boolean flag set
value true sum behaviour corresponding f rew
illustrate behaviour fltl progression consider formula f p u p
stating reward received first time p true let state p
holds prog false f p u p therefore since formula
progressed rew f true reward received prog f prog true f
p u p reward formula fades away affect subsequent
progression steps hand p false prog false f
p u p p u p therefore since formula progressed rew f
false reward received prog f prog false f p u p reward
formula persists subsequent progression steps
following theorem states weak assumptions rewards correctly allocated progression
theorem let f reward normal let hf f progressing
successive states sequence function prog provided
rew iff bf
proof see appendix b



premise theorem f never progresses indeed
means even rewarding suffice make f true something must
gone wrong earlier stage boolean rew made false
made true usual explanation original f reward normal
instance p reward unstable progresses next state p
true regardless f p p rew f false f p
p f however admittedly bizarre possibilities exist
example although p reward unstable substitution instance
progresses steps logically equivalent reward normal
progression method deliver correct minimal behaviour cases
even reward normal cases would backtrack choice values
boolean flags interest efficiency choose allow backtracking instead
raises exception whenever reward formula progresses informs
user sequence caused onus thus placed domain
modeller select sensible reward formulae avoid possible progression
noted worst case detecting reward normality cannot easier
decision fltl expected simple
syntactic criterion reward normality practice however commonsense precautions
avoiding making rewards depend explicitly future tense expressions suffice
keep things normal routine cases generous class syntactically recognisable
reward normal formulae see appendix
reward functions
language defined far able compactly represent behaviours
extension non markovian reward function straightforward represent


fidecision theoretic non markovian rewards

function set fltl ir formulae associated real valued rewards
call reward function specification formula f associated reward r
write f r rewards assumed independent additive
reward function r represented given
x
r bf
definition r
f r

e g p u p q get reward first time p
holds reward first time q holds onwards reward
conditions met otherwise
progress reward function specification compute reward
stages progression defines sequence h reward function
specifications rprog rprog function applies prog
formulae reward function specification
rprog prog f r f r
total reward received stage simply sum real valued rewards
granted progression function behaviours represented formulae
x
r rew f
f r

proceeding way get expected analog theorem states progression
correctly computes non markovian reward functions
theorem let reward normal reward function specification let h
progressing successive states
x sequence function
rprog provided r
r rew f r
f r

proof

immediate theorem



translation mdp
exploit compact representation non markovian reward function reward
function specification translate nmrdp equivalent mdp amenable statebased anytime solution methods recall section e state mdp
labelled state nmrdp history information sufficient determine
immediate reward case compact representation reward function specification
additional information summarised progression
sequence states passed e state form hs
strictly speaking multiset convenience represent set rewards multiple
occurrences formula multiset summed
extend definition reward normality reward specification functions obvious way
requiring reward formulae involved reward normal



fithiebaux gretton slaney price kabanza

state fltl ir reward function specification obtained progression
two e states hs ht equal immediate rewards
progressing semantically equivalent
definition let hs pr ri nmrdp reward function specification representing r e r r see definition translate mdp
hs pr r defined follows
fltl ir
hs
hs
pr rprog

otherwise


hs pr hs undefined
x
r hs
r rew f
hs pr hs hs



f r

reachable
item says e states labelled state reward function specification item
says initial e state labelled initial state original reward
function specification item says action applicable e state applicable
state labelling item explains successor e states probabilities
computed given action applicable e state hs successor e state
labelled successor state via nmrdp progression
probability e state pr nmrdp note
cost computing pr linear computing pr sum lengths
formulae item motivated see section finally since items
leave open choice many mdps differing unreachable states contain
item excludes irrelevant extensions easy translation leads
equivalent mdp defined definition obviously function required
definition given hs proof matter checking conditions
practical implementation labelling one step ahead definition
label initial e state rprog compute current reward current reward specification label progression predecessor reward specifications
current state rather predecessor states apparent
potential reduce number states generated mdp
figure shows equivalent mdp produced fltl version nmrdp
example figure recall example pltl reward formula q p
fltl allocation rewards described p q figure
care needed notion semantic equivalence rewards additive determining
equivalence may involve arithmetic well theorem proving example reward function specification p q equivalent p q p q p q
although one one correspondence formulae two sets



fidecision theoretic non markovian rewards

start state
f
reward

p
f f
reward

p
f f f
reward

b





q
f
reward

b

b

b

b



following formulae label e states
f p q
f q
f q

p q
f f
reward
b
p q
f f f
reward
b
p q
f f f
reward

b

figure equivalent mdp produced fltl
shows relevant formulae labelling e states obtained progression reward
formula note without progressing one step ahead would e states state
p left hand side labelled f f f f f f respectively
blind minimality
size mdp obtained e number e states contains key issue
us amenable state solution methods ideally would
mdp minimal size however know method building minimal
equivalent mdp incrementally adding parts required solution method since
worst case even minimal equivalent mdp larger nmrdp
factor exponential length reward formulae bacchus et al constructing
entirely would nullify interest anytime solution methods
however explain definition leads equivalent mdp exhibiting relaxed
notion minimality amenable incremental construction inspection
may observe wherever e state hs successor hs via action
means order succeed rewarding behaviours described means
execution sequences start going via necessary future
starting succeeds rewarding behaviours described hs
minimal equivalent mdp really execution sequences succeeding
rewarding behaviours described hs must minimal mdp
construction progression introduce e states priori needed
note e state priori needed may really needed may fact
execution sequence available actions exhibits given behaviour



fithiebaux gretton slaney price kabanza

instance consider response formula p k q k e every time trigger p
true rewarded k steps later provided q true obviously whether p
true stage affects way future states rewarded however
transition relation happens property k steps state satisfying p
state satisfying q reached posteriori p irrelevant need
label e states differently according whether p true observe occurrence
example figure leads fltl produce extra state
bottom left figure detect cases would look perhaps quite deep
feasible futures cannot constructing e states fly hence
relaxed notion call blind minimality coincide absolute
minimality
formalise difference true blind minimality purpose
convenient define functions mapping e states e functions
ir intuitively assigning rewards sequences nmrdp starting e recall
definition maps e state mdp underlying nmrdp state
definition let nmrdp let set e states equivalent mdp
f
let e reachable e state let sequence e states

e obtained
e let corresponding sequence
sense j j j define

e



e

r

otherwise

e
r

otherwise

unreachable e state e define e e
note carefully difference former describes rewards assigned
continuations given state sequence latter confines rewards feasible
continuations note well defined despite indeterminacy
choice since clause definition choices lead values
r
theorem let set e states equivalent mdp hs pr ri
minimal iff every e state reachable contains two distinct e states

proof

see appendix b



blind minimality similar except since looking ahead distinction
drawn feasible trajectories others future
definition let set e states equivalent mdp hs pr ri
blind minimal iff every e state reachable contains two distinct estates


fidecision theoretic non markovian rewards

theorem let translation definition blind minimal
equivalent mdp
proof

see appendix b



size difference blind minimal minimal mdps depend
precise interaction rewards dynamics hand making theoretical analyses difficult experimental rather anecdotal however experiments
section computation time point view often preferable work blind minimal mdp invest overhead computing
truly minimal one
finally recall syntactically different semantically equivalent reward function
specifications define e state therefore neither minimality blind minimality
achieved general without equivalence check least complex theorem
proving ltl pratical implementations avoid theorem proving favour embedding fast formula simplification progression regression
means principle approximate minimality blind minimality
appears enough practical purposes
embedded solution construction
blind minimality essentially best achievable anytime state solution methods typically extend envelope one step forward without looking deeper
future translation blind minimal mdp trivially embedded
solution methods line construction mdp method entirely
drives construction parts mdp feels need explore
leave others implicit time short suboptimal even incomplete policy may
returned fraction state expanded state spaces might constructed
note solution method raise exception soon one reward formulae progresses e soon expanded state hs built r
since acts detector unsuitable reward function specifications
extent enabled blind minimality allows dynamic analysis
reward formulae much pltlstr bacchus et al indeed execution
sequences feasible particular policy actually explored solution method contribute analysis rewards policy specifically reward formulae generated
progression given policy determined prefixes execution sequences
feasible policy dynamic analysis particularly useful since relevance
reward formulae particular policies e g optimal policy cannot detected priori
forward chaining planner tlplan bacchus kabanza introduced idea
fltl specify domain specific search control knowledge formula progression
prune unpromising sequential plans plans violating knowledge deterministic
search spaces shown provide enormous time gains leading tlplan
win competition hand tailored track
progression provides elegant way exploit search control knowledge yet
context decision theoretic dramatic reduction



fithiebaux gretton slaney price kabanza

fraction mdp constructed explored therefore substantially better
policies deadline
achieve follows specify via free formula c properties know
must verified paths feasible promising policies simply progress c
alongside reward function specification making e states triples hs ci c free
formula obtained progression prevent solution method applying action
leads control knowledge violated action applicability condition item
definition becomes hs ci iff c changes
straightforward instance effect control knowledge formula p q
remove consideration feasible path p followed q detected
soon violation occurs formula progresses although focuses
non markovian rewards rather dynamics noted free formulae
used express non markovian constraints systems dynamics
incorporated exactly control knowledge
discussion
existing approaches bacchus et al advocate use pltl finite
past specify non markovian rewards pltl style specification describe
past conditions get rewarded fltl describe
conditions present future future states rewarded
behaviours rewards may scheme naturalness thinking one
style depends case letting kids strawberry dessert
good day fits naturally past oriented account rewards whereas
promising may watch movie tidy room indeed making sense
whole notion promising goes naturally fltl one advantage pltl
formulation trivially enforces principle present rewards depend
future states fltl responsibility placed domain modeller best
offer exception mechanism recognise mistakes effects appear
syntactic restrictions hand greater expressive power fltl opens
possibility considering richer class decision processes e g uncertainty
rewards received dessert movie time next week
rains
rate believe fltl better suited pltl solving nmrdps
anytime state solution methods pltlsim translation could easily embedded solution method loses structure original formulae
considering subformulae individually consequently expanded state space easily
becomes exponentially bigger blind minimal one problematic
solution methods consider size severely affects performance solution
quality pre processing phase pltlmin uses pltl formula regression sets
subformulae potential labels possible predecessor states subsequent
generation phase builds mdp representing histories make difference way actually feasible execution sequences rewarded
recover structure original formula best case mdp produced
exponentially smaller blind minimal one however prohibitive cost



fidecision theoretic non markovian rewards

pre processing phase makes unsuitable anytime solution methods consider method pltl regression achieve meaningful relaxed
notion minimality without costly pre processing phase fltl
fltl progression precisely letting solution method resolve
tradeoff quality cost principled way intermediate two extreme
suggestions
structured representation solution methods targeted bacchus et al
differ anytime state solution methods fltl primarily aims particular
require explicit state enumeration non minimality
problematic state approaches virtue size mdp produced
pltlstr translation pltlsim clearly unsuitable anytime state methods
another sense fltl represents middle way combining advantages conferred
state structured approaches e g pltlmin one side pltlstr
former fltl inherits meaningful notion minimality latter
approximate solution methods used perform restricted dynamic analysis
reward formulae particular formula progression enables even state methods
exploit structure fltl space however gap blind
true minimality indicates progression alone insufficient fully exploit
structure hope pltlstr able take advantage full structure
reward function possibility fail exploit even much structure
fltl efficiently empirical comparison three approaches needed answer
question identify domain features favoring one

nmrdpp
first step towards decent comparison different approaches framework
includes non markovian reward decision process planner nmrdpp
platform development experimentation approaches nmrdps
provides implementation approaches described common framework
within single system common input language nmrdpp available line
see http rsise anu edu au charlesg nmrdpp worth noting bacchus et al
report implementation approaches
input language
input language enables specification actions initial states rewards search
control knowledge format action specification essentially
spudd system hoey et al reward specification one formulae
associated name real number formulae pltl fltl
control knowledge given language chosen reward control
knowledge formulae verified sequence states feasible
generated policies initial states simply specified part control knowledge
explicit assignments propositions
would interesting hand use pltlstr conjunction symbolic versions
methods e g symbolic lao feng hansen symbolic rtdp feng hansen zilberstein




fithiebaux gretton slaney price kabanza

action flip
heads
endaction
action tilt
heads heads
endaction
heads
first heads prv pdi heads
seq prv heads prv heads heads
figure input coin example prv previously stands

pdi past diamond stands
instance consider simple example consisting coin showing heads
tails heads two actions performed flip action changes
coin heads tails probability tilt action changes
probability otherwise leaving initial state tails get reward
heads pltl reward
first head written heads
time achieve sequence heads heads tails heads heads heads pltl
input language nmrdp described shown figure
common framework
common framework underlying nmrdpp takes advantage fact nmrdp
solution methods general divided distinct phases preprocessing
expansion solving first two optional
pltlsim preprocessing simply computes set sub f subformulae reward
formulae pltlmin includes computing labels l state
pltlstr preprocessing involves computing set temporal variables well
adds dynamics rewards fltl require preprocessing
expansion optional generation entire equivalent mdp prior solving
whether line expansion sensible depends mdp solution method used
state value policy iteration used mdp needs expanded anyway
hand anytime search structured method used
definitely bad idea experiments often used expansion solely purpose
measuring size generated mdp
solving mdp done number methods currently nmrdpp provides
implementations classical dynamic programming methods namely state value
policy iteration howard heuristic search methods state lao hansen
zilberstein value policy iteration subroutine one structured
method namely spudd hoey et al prime candidates future developments
l rtdp bonet geffner symbolic lao feng hansen symbolic
rtdp feng et al


fidecision theoretic non markovian rewards

load coin nmrdp
pltlstr preprocessing

loadworld coin
preprocess spltl
startcputimer
spudd
stopcputimer
readcputimer

iterationcount

displaydot valuetodot
expected value





report number iterations
display add value function

prv heads

prv prv pdi heads



report solving time

heads

prv heads

prv prv pdi heads

solve mdp spudd



prv heads

prv pdi heads





prv pdi heads





display policy

displaydot policytodot
optimal policy

heads

prv heads

flip







tilt

pltlmin preprocessing
completely expand mdp
report mdp size

preprocess mpltl
expand
domainstatesize
printdomain domain rb
reward
flip

flip

display postcript rendering mdp

tilt

tilt

heads
reward
flip
heads
reward
tilt

flip

tilt

tilt

flip

tilt

flip

reward
tilt

flip

tilt

flip

tilt

tilt

flip
reward
flip

flip

flip

tilt

tilt

heads
reward

solve mdp vi
report number iterations

valit
iterationcount

getpolicy


output policy textual

figure sample session


fithiebaux gretton slaney price kabanza

approaches covered
altogether types preprocessing choice whether expand
mdp solution methods give rise quite number nmrdp approaches including
limited previously mentioned see e g pltlstr combinations possible e g state processing variants incompatible structured
solution methods converse possible principle however present
structured form preprocessing fltl formulae
pltlstr example interesting variant pltlstr obtain
considering additional preprocessing whereby state space explored without explicitly
enumerating produce bdd representation e states reachable start
state done starting bdd representing start e state repeatedly
applying action non zero probabilities converted ones ed
last action adds reachable e states bdd
sure represents reachable e state space used additional control
knowledge restrict search noted without phase pltlstr makes
assumptions start state thus left possible disadvantage similar
structured reachability analysis techniques used symbolic implementation
lao feng hansen however important aspect
temporal variables included bdd
nmrdpp system
nmrdpp controlled command language read file interactively command language provides commands different phases preprocessing
expansion solution methods commands inspect resulting policy value
functions e g rendering via dot labs well supporting
commands timing memory usage sample session coin nmrdp
successively solved pltlstr pltlmin shown figure
nmrdpp implemented c makes use number supporting libraries
particular relies heavily cudd package manipulating adds somenzi
action specification trees converted stored adds system
moreover structured rely heavily cudd add computations
state make use mtl matrix template library matrix
operations mtl takes advantage modern processor features mmx sse
provides efficient sparse matrix operations believe implementations
mdp solution methods comparable state art instance found
implementation spudd comparable performance within factor
reference implementation hoey et al hand believe data
structures used regression progression temporal formulae could optimised

experimental analysis
faced three substantially different approaches easy compare
performance depend domain features varied structure
transition model type syntax length temporal reward formula presence



fidecision theoretic non markovian rewards

rewards unreachable irrelevant optimal policy availability good heuristics
control knowledge etc interactions factors section
report experimental investigation influence factors try
answer questions raised previously
dynamics domain predominant factor affecting performance
type reward major factor
syntax used describe rewards major factor
overall best method
overall worst method
preprocessing phase pltlmin pay compared pltlsim
simplicity fltl translation compensate blind minimality
benefit true minimality outweigh cost pltlmin preprocessing
dynamic analyses rewards pltlstr fltl effective
one analyses powerful rather complementary
cases able identify systematic patterns
section obtained pentium ghz gnu linux machine mb
ram
preliminary remarks
clearly fltl pltlstr great potential exploiting domain specific heuristics control knowledge pltlmin less avoid obscuring therefore
refrained incorporating features experiments running lao
heuristic value state crudest possible sum reward values
performance interpreted light necessarily
reflect practical abilities methods able exploit features
begin general observations one question raised whether
gain pltl expansion phase worth expensive preprocessing performed
pltlmin e whether pltlmin typically outperforms pltlsim definitively answer
question pathological exceptions preprocessing pays found expansion
bottleneck post hoc minimisation mdp produced pltlsim
help much pltlsim therefore little practical interest decided
report performance often order magnitude worse
pltlmin unsurprisingly found pltlstr would typically scale larger state
spaces inevitably leading outperform state methods however effect
uniform structured solution methods sometimes impose excessive memory requirements
makes uncompetitive certain cases example n f large n
features reward formula
executive summary answers executive reader yes yes pltlstr
fltl pltlsim yes yes respectively yes yes respectively



fithiebaux gretton slaney price kabanza

domains
experiments performed four hand coded domains propositions dynamics
random domains hand coded domain n propositions pi dynamics
makes every state possible eventually reachable initial state
propositions false first two domains spudd linear spudd expon
discussed hoey et al two others
intention spudd linear take advantage best case behaviour
spudd proposition pi action ai sets pi true propositions
pj j false spudd expon used hoey et al demonstrate
worst case behaviour spudd proposition pi action ai sets pi
true propositions pj j true sets pi false otherwise
sets latter propositions false third domain called one turn
one turn action per proposition turn pi action probabilistically
succeeds setting pi true pi false turn action similar fourth
domain called complete fully connected reflexive domain proposition pi
action ai sets pi true probability n false otherwise
pj j true false probability note ai cause transition
n states
random domains size n involve n propositions method generating
dynamics detailed appendix c let us summarise saying able
generate random dynamics exhibiting given degree structure given degree
uncertainty lack structure essentially measures bushiness internal part
adds representing actions uncertainty measures bushiness leaves
influence dynamics
interaction dynamics reward certainly affects performance
different approaches though strikingly factors reward type see
found reward scheme varying degree structure
uncertainty generally change relative success different approaches
instance figures average run time methods function
degree structure resp degree uncertainty random size n
reward n state encountered stage n rewarded regardless properties
run time increases slightly degrees significant change relative
performance typical graphs obtain rewards
clearly counterexamples observation exist notable cases
extreme dynamics instance spudd expon domain although small values
n n pltlstr approaches faster others handling reward
n virtually type dynamics encountered perform poorly
reward spudd expon explained fact small fraction
spudd expon states reachable first n steps n steps fltl immediately
recognises reward consequence formula progressed
pltlmin discovers fact expensive preprocessing pltlstr
hand remains concerned prospect reward pltlsim would


n

fltl



fiaverage cpu time sec

decision theoretic non markovian rewards




















structure structured unstructured
fltl

pltlmin

pltlstruct

pltlstruct

figure changing degree structure

average cpu time sec






















uncertainty certain uncertain
fltl

pltlmin

pltlstruct

pltlstruct

figure changing degree uncertainty
influence reward types
type reward appears stronger influence performance dynamics
unsurprising reward type significantly affects size generated mdp
certain rewards make size minimal equivalent mdp increase constant
number states constant factor others make increase factor exponential
length formula table illustrates third column reports size
minimal equivalent mdp induced formulae left hand side
legitimate question whether direct correlation size increase
appropriateness different methods instance might expect state
methods particularly well conjunction reward types inducing small mdp
figures necessarily valid non completely connected nmrdps unfortunately even
completely connected domains appear much cheaper way determine mdp
size generate count states



fithiebaux gretton slaney price kabanza

type
first time pi
pi sequence start state
two consecutive pi
pi n times ago

formula
ni pi
ni pi
ni pi n
n

pi pi
n ni pi

size

n
nk
n

fastest
pltlstr
fltl
pltlstr
pltlstr

slowest
pltlmin
pltlstr
fltl
pltlmin

table influence reward type mdp size method performance

average cpu time sec






















n
approaches prvin
approaches prvout

figure changing syntax
otherwise badly comparison structured methods interestingly
case instance table whose last two columns report fastest slowest
methods range hand coded domains n first row contradicts
expectation moreover although pltlstr fastest last row larger values
n represented table aborts lack memory unlike
methods
obvious observations arising experiments pltlstr nearly
fastest runs memory perhaps interesting
second row expose inability methods pltl deal
rewards specified long sequences events converting reward formula
set subformulae lose information order events
recovered laboriously reasoning fltl progression contrast takes events one
time preserving relevant structure step experimentation led us
observe pltl perform poorly reward specified
k f fik f f true k steps ago within last k
formulae form k f
steps last k steps
influence syntax
unsurprisingly syntax used express rewards affects length
formula major influence run time typical example effect
captured figure graph demonstrates expressing prvout n ni pi


fidecision theoretic non markovian rewards

state count n























n
pltlmin
fltl

figure effect multiple rewards mdp size

total cpu time sec





















n
fltl

pltlmin

pltlstruct

pltlstruct

figure effect multiple rewards run time
prvin ni n pi thereby creating n times temporal subformulae alters
running time pltl methods fltl affected fltl progression requires two
iterations reward formula graph represents averages running
times methods complete domain
serious concern relation pltl approaches handling reward
specifications containing multiple reward elements notably found pltlmin
necessarily produce minimal equivalent mdp situation demonstrate consider set reward formulae f f fn associated
real value r given pltl approaches distinguish unnecessarily past
behaviours lead identical future rewards may occur reward
e state determined truth value f f formula necessarily require
e states distinguish cases f f f f
hold however given specification pltlmin makes distinction example



fithiebaux gretton slaney price kabanza

taking pi figure shows fltl leads mdp whose size times
nmrdp contrast relative size mdp produced pltlmin
linear n number rewards propositions obtained
hand coded domains except spudd expon figure shows run times function
n complete fltl dominates overtaken pltlstr large values
n mdp becomes large explicit exploration practical obtain
minimal equivalent mdp pltlmin bloated reward specification form
ni pi nj j pj r ni pi n r necessary virtue
exponential length adequate solution
influence reachability
approaches claim ability ignore variables irrelevant
condition track unreachable pltlmin detects preprocessing
pltlstr exploits ability structured solution methods ignore fltl ignores progression never exposes however given mechanisms
avoiding irrelevance different expect corresponding differences effects
experimental investigation found differences performance best illustrated looking response formulae assert trigger condition c reached
reward received upon achievement goal g resp within k steps
k c fltl c k g resp
pltl written g k c resp g
c k g
goal unreachable pltl approaches perform well false
goal g lead behavioural distinctions hand constructing
mdp fltl considers successive progressions k g without able detect
unreachable actually fails happen exactly blindness blind
minimality amounts figure illustrates difference performance function
number n propositions involved spudd linear domain reward
form g n c g unreachable
fltl shines trigger unreachable since c never happens formula
progress goal however complicated never tracked generated mdp situation pltl approaches still consider k c subformulae
discover expensive preprocessing pltlmin reachability analysis pltlstr never pltlstr irrelevant illustrated figure
spudd linear reward form g n c c unreachable
dynamic irrelevance
earlier claimed one advantage pltlstr fltl pltlmin pltlsim
former perform dynamic analysis rewards capable detecting irrelevance
variables particular policies e g optimal policy experiments confirm
claim however reachability whether goal triggering condition
response formula becomes irrelevant plays important role determining whether
sometimes speak conditions goals reachable achievable rather feasible
although may temporally extended keep line conventional vocabulary
phrase reachability analysis



fidecision theoretic non markovian rewards

total cpu time sec





















n
fltl

pltlmin

pltlstruct

pltlstruct

figure response formula unachievable goal

total cpu time sec



















n
fltl

pltlmin

pltlstruct

pltlstruct

figure response formula unachievable trigger
pltlstr fltl taken pltlstr able dynamically ignore goal
fltl able dynamically ignore trigger
illustrated figures figures domain considered
n propositions response formula g n c
g c achievable response formula assigned fixed reward study effect
dynamic irrelevance goal figure achievement g rewarded value
r e g r pltl figure hand study effect
dynamic irrelevance trigger achievement c rewarded value r
figures runtime methods r increases
achieving goal resp trigger made less attractive r increases
point response formula becomes irrelevant optimal policy
happens run time pltlstr resp fltl exhibits abrupt durable improvement
figures fltl able pick irrelevance trigger pltlstr able
exploit irrelevance goal expected pltlmin whose analysis static pick


fithiebaux gretton slaney price kabanza

total cpu time sec






















r
pltlmin

pltlstruct

fltl

pltlstruct

average cpu time sec

figure response formula unrewarding goal





















r
pltlmin

fltl

pltlstruct

pltlstruct

figure response formula unrewarding trigger
performs consistently badly note figures pltlstr progressively
takes longer compute r increases value iteration requires additional iterations
converge
summary
experiments artificial domains found pltlstr fltl preferable statebased pltl approaches cases one insists latter strongly
recommend preprocessing fltl technique choice reward requires tracking
long sequence events desired behaviour composed many elements
identical rewards response formulae advise use pltlstr probability
reaching goal low achieving goal costly conversely advise
use fltl probability reaching triggering condition low reaching
costly cases attention paid syntax reward formulae


fidecision theoretic non markovian rewards

particular minimising length indeed could expected found syntax
formulae type non markovian reward encode predominant
factor determining difficulty much features
markovian dynamics domain

concrete example
experiments far focused artificial aimed characterising
strengths weaknesses approaches look concrete example
order give sense size interesting techniques
solve example derived miconic elevator classical benchmark
koehler schuster elevator must get number passengers origin
floor destination initially elevator arbitrary floor passenger
served boarded elevator version one single
action causes elevator service given floor effect unserved
passengers whose origin serviced floor board elevator boarded passengers
whose destination serviced floor unboard become served task plan
elevator movement passengers eventually served
two variants miconic simple variant reward received
time passenger becomes served hard variant elevator attempts
provide range priority services passengers special requirements many passengers
prefer travelling single direction destination certain
passengers might offered non stop travel destination finally passengers
disabilities young children supervised inside elevator
passenger supervisor assigned omit vip conflicting group
services present original hard miconic reward formulae
create additional difficulties
formulation makes use propositions pddl description miconic used international competition dynamic propositions
record floor elevator currently whether passengers served boarded
static propositions record origin destination floors passengers well
categories non stop direct travel supervisor supervised passengers fall however
formulation differs pddl description two interesting ways firstly since
use rewards instead goals able preferred solution even
goals cannot simultaneously satisfied secondly priority services naturally
described terms non markovian rewards able use action description simple hard versions whereas pddl description hard miconic
requires additional actions complex preconditions monitor satisfaction priority service constraints reward schemes miconic encapsulated
four different types reward formula
simple variant reward received first time passenger pi served
experimented stochastic variants miconic passengers small probability
desembarking wrong floor however useful present deterministic
version since closer miconic deterministic benchmark since shown
rewards far crucial impact dynamics relative performance methods



fithiebaux gretton slaney price kabanza

pltl

servedpi servedpi

fltl

servedpi u servedpi

next reward received time non stop passenger pi served one step
boarding elevator
pltl

n onstoppi boardedpi servedpi servedpi

fltl

n onstoppi boardedpi servedpi servedpi

reward received time supervised passenger pi served
accompanied times inside elevator supervisor pj
pltl
fltl

supervisedpi supervisorpj pi servedpi
servedpi boardedpi boardedpj
servedpi u boardedpi supervisedpi boardedpj supervisorpj pi
servedpi servededpi

finally reward received time direct travel passenger pi served
travelled one direction since boarding e g case going
directp
w w servedpi servedpi
j k j atf loork atf loorj boardedpi boardedpi
w w
fltl directpi boardedpi servedpi u j k atf loorj atf loork
servedpi servedpi

pltl

similarly case going
experiments section run dual pentium ghz gnu linux
machine gb ram first experimented simple variant giving reward
time passenger first served figure shows cpu time taken
approaches solve random increasing number n floors
passengers figure shows number states expanded data
point corresponds one random fair structured
ran pltlstr able exploit reachability start state first observation
although pltlstr best small values n quickly runs memory
pltlstr pltlsim need track formulae form servedpi
pltlsim conjecture run memory earlier
second observation attempts pltl minimisation pay much
pltlmin reduced memory tracks fewer subformulae size
mdp produces identical size pltlsim mdp larger
fltl mdp size increase due fact pltl approaches label differently
e states passengers served depending become served
passengers reward formula true e state contrast fltl
implementation progression one step ahead labels e states reward
understand fltl formula observe get reward iff boardedpi supervisedpi
boardedpj supervisorpj pi holds servedpi becomes true recall formula q u p
q q rewards holding p occurrence q



fidecision theoretic non markovian rewards

total cpu time sec




















n
fltl
pltlsim
pltlmin
pltlstr

figure simple miconic run time



state count n
























n
fltl
pltlsim pltlmin

figure simple miconic number expanded states
formulae relevant passengers still need served formulae
progressed gain number expanded states materialises run time gains
resulting fltl eventually taking lead
second experiment illustrates benefits even extremely simple admissible heuristic conjunction fltl heuristic applicable discounted stochastic
shortest path discounts rewards shortest time future
possible simply amounts assigning fringe state value times
number still unserved passengers discounted avoiding floors
passenger waiting destination boarded passenger
figures compare run time number states expanded fltl used
conjunction value iteration valit used conjunction lao


fithiebaux gretton slaney price kabanza

total cpu time sec




















n
fltllao h
fltllao u
fltlvalit

figure effect simple heuristic run time

state count n





















n
fltllao h
fltlvalit fltllao u

figure effect simple heuristic number expanded states
search informed heuristic lao h uninformed lao lao u e lao
heuristic n node included reference point
overhead induced heuristic search seen graphs heuristic search
generates significantly fewer states eventually pays terms run time
final experiment considered hard variant giving reward
service reward non stop travel reward appropriate supervision
reward direct travel regardless number n floors
passengers feature single non stop traveller third passengers require
supervision half passengers care traveling direct cpu time number
states expanded shown figures respectively simple case
pltlsim pltlstr quickly run memory formulae type create
many additional variables track approaches seem


fidecision theoretic non markovian rewards

total cpu time sec


















n
fltl
pltlsim
pltlmin
pltlstruct

figure hard miconic run time

state count n



















n
fltl
pltlsim
pltlmin

figure hard miconic number expanded states
exhibit enough structure help pltlstr fltl remains fastest
seem much due size generated mdp slightly
pltlmin mdp rather overhead incurred minimisation another
observation arising experiment small instances handled
comparison classical version solved state art
optimal classical planners example international competition
propplan planner fourman optimally solved instances hard miconic
passengers floors seconds much less powerful machine



fithiebaux gretton slaney price kabanza

nmrdpp probabilistic competition
report behaviour nmrdpp probabilistic track th international competition ipc since competition feature non markovian
rewards original motivation taking part compare solution methods
implemented nmrdpp markovian setting objective largely underestimated
challenges raised merely getting planner ready competition especially
competition first kind end decided successfully preparing nmrdpp attempt competition one solution method possibly
search control knowledge would honorable
crucial encountered translation ppddl younes
littman probabilistic variant pddl used input language competition nmrdpps add input language translating ppddl adds
possible theory devising translation practical enough need
competition small number variables small quickly generated easily manipulable
adds another matter mtbdd translator kindly made available participants
competition organisers able achieve required efficiency
times translation quick nmrdpp unable use generated adds efficiently consequently implemented state translator top pddl parser
backup opted state solution method since rely adds
could operate translators
version nmrdpp entered competition following
attempt get translation adds mtbdd proves infeasible
abort rely state translator instead
run fltl expansion state space taking search control knowledge account
available break mn complete
run value iteration convergence failing achieve useful e g
expansion complete enough even reach goal state go back step
run many trials possible remaining time following generated policy defined falling back non deterministic search control
policy available
step trying maximise instances original add
nmrdpp version could run intact step decided use lao
run good heuristic often incurs significant overhead compared value
iteration
featured competition classified goal rewardbased goal positive reward received goal
state reached reward action performance may incur usually
negative reward another orthogonal distinction made
given planners mn run whatever computation saw appropriate including parsing pre processing policy generation execute trial runs generated
policy initial state goal state



fidecision theoretic non markovian rewards

domains communicated advance participants domains
latter consisted variants blocks world logistics box world
gave participating planners opportunity exploit knowledge
domain much hand coded deterministic track
decided enroll nmrdpp control knowledge mode domain independent
mode difference two modes first uses fltl search
control knowledge written known domains additional input main concern
writing control knowledge achieve reasonable compromise size
effectiveness formulae blocks world domain two actions
pickup putdown chance dropping block onto table
control knowledge used encoded variant well known gn near optimal strategy
deterministic blocks world slaney thiebaux whenever possible
try putting clear block goal position otherwise put arbitrary clear block
table blocks get dropped table whenever action fails
success probabilities rewards identical across actions optimal policies
essentially made optimal sequences actions deterministic blocks
world little need sophisticated strategy colored blocks
world domain several blocks share color goal refers
color blocks control knowledge selected arbitrary goal state non colored
blocks world consistent colored goal specification used strategy
non colored blocks world performance strategy depends entirely
goal state selected therefore arbitrarily bad
logistics ipc distinguish airports locations within
city trucks drive two locations city planes fly
two airports contrast box world features cities
airport accessible truck priori map truck
plane connections arbitrary goal get packages city origin
city destination moving truck chance resulting reaching one
three cities closest departure city rather intended one size box
world search space turned quite challenging nmrdpp therefore writing
search control knowledge gave optimality consideration favored maximal
pruning helped fact box world generator produces
following structure cities divided clusters composed
least one airport city furthermore cluster least one hamiltonian circuit
trucks follow control knowledge used forced planes one trucks
one cluster idle cluster truck allowed move could
attempt driving along chosen hamiltonian circuit picking dropping parcels
went
planners participating competition shown table planners e g
j j domain specific tuned blocks box worlds use
domain specific search control knowledge learn examples participating
planners domain independent
sophisticated near optimal strategies deterministic blocks world exist see slaney thiebaux
much complex encode might caused time performance



fithiebaux gretton slaney price kabanza

part
c
e
g
g
j
j
j
p
q
r

description
symbolic lao
first order heuristic search fluent calculus
nmrdpp without control knowledge
nmrdpp control knowledge
interpreter hand written classy policies
learns classy policies random walks
version replanning upon failure
mgpt lrtdp automatically extracted heuristics
probaprop conformant probabilistic planner
structured reachability analysis structured pi

reference
feng hansen
karabaev skvortsova


fern et al
fern et al
hoffmann nebel
bonet geffner
onder et al
teichteil konigsbuch fabiani

table competition participants domain specific planners starred
dom
prob
g
j
j
e
j
g
r
p
c
q








bw c nr








bw nc nr







bx nr








expl bw


hanoise


zeno


tire nr



























total











table goal domain specific planners starred entries
percentage runs goal reached blank indicates
planner unable attempt indicates planner
attempted never able achieve goal indicates
unavailable due bug evaluation software couple
initially announced found invalid
dom
prob
j
g
e
j
j
p
c
g
r
q








bw c r




























bw nc r











bx r

















file


tire r














total











table reward domain specific planners starred entries
average reward achieved runs blank indicates
planner unable attempt indicates planner
attempted achieve strictly positive reward indicates
unavailable


fidecision theoretic non markovian rewards

tables competition extracted competition overview younes littman weissmann asmuth
competition web site http www cs rutgers edu mlittman topics ipc pt
first tables concerns goal second reward entries tables represent goal achievement percentage average reward achieved planner versions left column top
two rows planners top part tables domain specific
known domains lie left hand side tables colored blocks world
bw c nr goal version bw c r reward version blocks
non colored blocks world bw nc nr goal version blocks bwnc r reward version blocks box world
bx nr goal bx r reward cities boxes unknown domains lie right hand side tables comprise
expl bw exploding version block blocks world putting
block may destroy object put zeno probabilistic variant zeno travel
domain ipc plane persons cities fuel levels hanoise
probabilistic variant tower hanoi disks rods file
putting files randomly chosen folders tire variant tire world
cities spare tires tire may go flat driving
planner nmrdpp g g version able attempt achieving strictly positive reward even j competition overall
winner able successfully attempt many nmrdpp performed particularly well goal achieving goal runs except expl bw
hanoise tire nr note three goal achievement probability
optimal policy exceed planner outperformed nmrdpp
scale pointed behaves well probabilistic version blocks box
world optimal policies close deterministic
hoffmann analyses reasons heuristic works well traditional benchmarks blocks world logistics hand unable
solve unknown different structure require substantial
probabilistic reasoning although easily solved number participating planners expected large discrepancy version nmrdpp
allowed use search control g domain independent version g
latter performs okay unknown goal domains able solve
known ones fact except none participating domain independent
planners able solve
reward case nmrdpp control knoweldge behaves well known
human encoded policies j performed better without control knowledge nmrdpp unable scale participants
mgpt furthermore nmrdpp appears perform poorly two unknown
cases might due fact fails generate optimal policy suboptimal policies easily high negative score domains see younes et al
r tire know nmrdpp indeed generate suboptimal policy additionally
could nmrdpp unlucky sampling policy evaluation process



fithiebaux gretton slaney price kabanza

tire r particular high variance costs trajectories
optimal policy
alltogether competition suggest control knowledge likely essential solving larger markovian nmrdpp
observed deterministic planners approaches making use control knowledge
quite powerful

conclusion related future work
examined solving decision processes nonmarkovian rewards described existing approaches exploit compact representation reward function automatically translate nmrdp equivalent
process amenable mdp solution methods computational model underlying
framework traced back work relationship linear temporal logic
automata areas automated verification model checking vardi
wolper remaining framework proposed representation
non markovian reward functions translation mdps aimed making best
possible use state anytime heuristic search solution method representation extends future linear temporal logic express rewards translation
effect embedding model checking solution method mdp
minimal size achievable without stepping outside anytime framework consequently
better policies deadline described nmrdpp software platform
implements approaches common interface proved useful tool
experimental analysis system analysis first kind
able identify number general trends behaviours methods
provide advice best suited certain circumstances obvious
reasons analysis focused artificial domains additional work examine
wider range domains practical interest see form take
context ultimately would analysis help nmrdpp automatically select
appropriate method unfortunately difficulty translating
pltl fltl likely nmrdpp would still maintain pltl
fltl version reward formulae
detailed comparison solving nmrdps existing methods bacchus et al found sections two important aspects future
work would help take comparison one settle question appropriateness translation structured solution methods symbolic implementations
solution methods consider e g symbolic lao feng hansen well
formula progression context symbolic state representations pistore traverso
could investigated purpose take advantage greater
expressive power fltl consider richer class decision processes instance
uncertainty rewards received many extensions language
possible adding eventualities unrestricted negation first class reward propositions
quantitative time etc course dealing via progression without backtracking
another matter



fidecision theoretic non markovian rewards

investigate precise relationship line work recent work
temporally extended goals non deterministic domains particular
interest weak temporally extended goals expressible eagle language
dal lago et al temporally extended goals expressible ctl baral
zhao eagle enables expression attempted reachability maintenance goals
form try reach p try maintain p add goals reach p
maintain p already expressible ctl idea generated policy
make every attempt satisfying proposition p furthermore eagle includes recovery goals
form g fail g meaning goal g must achieved whenever goal g fails
cyclic goals form repeat g meaning g achieved cyclically
fails semantics goals given terms variants buchi tree automata
preferred transitions dal lago et al present
symbolic model checking generates policies achieving goals baral zhao
describe ctl alternative framework expressing subset eagle goals
variety others ctl variant ctl allows formulae involving
two types path quantifiers quantifiers tied paths feasible generated
policy usual quantifiers generally tied paths feasible
domain actions baral zhao present
would interesting know whether eagle ctl goals encoded nonmarkovian rewards framework immediate consequence would nmrdpp
could used plan generally would examine respective
merits non deterministic temporally extended goals decision theoretic
non markovian rewards
pure probabilistic setting rewards recent related includes work
controller synthesis probabilistic temporally extended goals expressible
probabilistic temporal logics csl pctl younes simmons baier et al
logics enable expressing statements probability policy satisfying given temporal goal exceeding given threshold instance younes simmons
describe general probabilistic framework involving concurrency continuous time temporally extended goals rich enough model generalised semi markov
processes solution directly comparable presented
another exciting future work area investigation temporal logic formalisms
specifying heuristic functions nmrdps generally search
temporally extended goals good heuristics important solution methods
targeting surely value ought depend history methods
described could applicable description processing heuristics related
extending search control knowledge fully operate
presence temporally extended goals rewards stochastic actions first issue
branching probabilistic logics ctl pctl variants preferred
fltl describing search control knowledge stochastic actions
involved search control often needs refer possible futures even
probabilities another major goalp modality
key specification reusable search control knowledge interpreted respect
would argue hand ctl necessary representing non markovian rewards



fithiebaux gretton slaney price kabanza

fixed reachability goal bacchus kabanza applicable
domains temporally extended goals let alone rewards kabanza thiebaux
present first search control presence temporally extended goals
deterministic domains much remains done system nmrdpp able
support meaningful extension goalp
finally let us mention related work area databases uses similar
pltlstr extend database auxiliary relations containing sufficient information
check temporal integrity constraints chomicki issues somewhat different
raised nmrdps ever one sequence databases matters
size auxiliary relations avoiding making redundant distinctions

acknowledgements
many thanks fahiem bacchus rajeev gore marco pistore ron van der meyden moshe
vardi lenore zuck useful discussions comments well anonymous
reviewers david smith thorough reading excellent
suggestions sylvie thiebaux charles gretton john slaney david price thank national ict australia support nicta funded australian governments
backing australias ability initiative part australian council froduald kabanza supported canadian natural sciences engineering
council nserc

appendix class reward normal formulae
existing decision procedure slaney determining whether formula rewardnormal guaranteed terminate finitely involves construction comparison
automata rather intricate practice therefore useful give simple syntactic
characterisation set constructors obtaining reward normal formulae even though
formulae constructible
say formula material iff contains temporal operators
material formulae boolean combinations atoms
consider four operations behaviours representable formulae fltl firstly
behaviour may delayed specified number timesteps secondly may made
conditional material trigger thirdly may started repeatedly material
termination condition met fourthly two behaviours may combined form
union operations easily realised syntactically corresponding operations
formulae material formula
delay f

f

cond f f
loop f f u
union f f f f
f atemporal formula goalp f true iff f true goal states



fidecision theoretic non markovian rewards

shown slaney set reward normal formulae closed delay
cond material loop material union closure
operations represents class behaviours closed intersection
concatenation well union
many familiar reward normal formulae obtainable applying four operations example p loop cond p sometimes paraphrase necessary
example p q required form antecedent
conditional equivalent p q loop cond p delay cond q
cases easy example formula p u p stipulates reward
first time p happens form suggested capture
behaviour operations requires formula p p u p

appendix b proofs theorems
property b b b f iff b prog b f
proof
induction structure f several base cases fairly trivial
f f nothing prove progress hold
everywhere nowhere respectively f p f holds progresses
holds f hold progresses
hold case f p similar last base case f following
equivalent
b f
b
b
prog b f
b prog b f
induction case f g h following equivalent
b f
b g b h
b prog b g b prog b h induction hypothesis
b prog b g prog b h
b prog b f
induction case f g h analogous case
induction case f g trivial inspection definitions
induction case f g u h f logically equivalent h g g u h
cases holds stage behaviour b iff prog b f holds stage

theorem let f reward normal let hf f progressing
successive states sequence provided rew
iff bf



fithiebaux gretton slaney price kabanza

proof first definition reward normality f reward normal b f iff
bf b next b f progressing f according
b letting bi true iff b cannot lead contradiction
property progression truth preserving
remains b f progressing f according b
must lead eventually proof induction structure f
usual base case f literal atom negated atom trivial
case f g h suppose b f b g b h induction
hypothesis g h progresses eventually hence conjunction
case f g h suppose b f b g b h induction
hypothesis g h progresses eventually suppose without loss generality
g progress h point g progressed
formula g f progressed g simplifies g since g progresses
eventually f
case f g suppose b f let let b b
b g induction hypothesis g progressed according b eventually
reaches progression f according b exactly
first step leads
case f g u h suppose b f j j b g
j b h proceed induction j base case j b g
b h whence main induction hypothesis g h eventually progress
thus h g f progresses eventually f particular f f
establishing base case induction case suppose b g course b h
since f equivalent h g f b f b h b g clearly b f
b previous case therefore b f failure occurs stage j
therefore hypothesis induction j applies f progressed
according b goes eventually f progressed according b goes
similarly

theorem let set e states equivalent mdp hs pr ri
minimal iff every e state reachable contains two distinct e states

proof proof construction canonical equivalent mdp dc let set
e partitioned equivalence classes
finite prefixes state sequences
e r
j iff j
r j let denote equivalence class let e set
equivalence classes let function takes e
j let j pr j
hsi otherwise let j let r r note
following four facts
functions r well defined
dc hs ri equivalent mdp



fidecision theoretic non markovian rewards

equivalent mdp mapping subset states
onto e
satisfies condition every e state reachable contains two
distinct e states iff dc isomorphic

fact amounts j matter
two sequences used define r equivalence class cases
simply j case r special case h
equality rewards extensions
fact matter checking four conditions definition hold
conditions hold trivially construction
e r r
condition says feasible state sequence
given construction condition states
pr
e exists unique j e j

j pr
e required j hs
suppose pr
course required condition reads
hs unique element x e x
x pr
establish existence need hs pr
immediate definition establish uniqueness suppose
x x pr actions since pr
transition probability x nonzero action definition
x hs
fact readily observed let equivalent mdp states
state x x one state
action gives nonzero probability
transition x follows uniqueness part condition definition
together fact transition function probability distribution sums
therefore given finite state sequence one state reached
start state following therefore induces equivalence relation
j iff lead state sequences
feasible may regarded equivalent reachable state
associated nonempty equivalence class finite sequences states working
definitions may observe sub relation j
j hence function takes equivalence class
feasible sequence induces mapping h epimorphism fact
reachable subset states onto e
establish fact must shown case mapping
reversed equivalence class dc corresponds exactly one element


fithiebaux gretton slaney price kabanza

e
suppose contradiction exist sequences j

j following two sequences arrive two different
elements j therefore
e
exists sequence k
r k r j k
contradicts condition j

theorem follows immediately facts
theorem let translation definition blind minimal
equivalent mdp
proof reachability e states obvious constructed
reached e state pair hs state reward function
specification fact hs determines distribution rewards
continuations sequences
reach hs
p
reward f r r bf blind minimal exist
distinct e states hs hs sum makes
semantically equivalent contradicting supposition distinct


appendix c random domains
random domains produced first creating random action specification
defining domain dynamics experiments conducted involved
producing second step random reward specification desired properties
relation generated dynamics
random generation domain dynamics takes parameters number n
propositions domain number actions produced starts
assigning effects action proposition affected exactly one
action example actions propositions first actions may affect
propositions th one affected propositions different
action initial effects continue add effects one time
sufficient proportion state space reachable see proportion reachable parameter
additional effect generated picking random action random
proposition producing random decision diagram according uncertainty
structure parameters
uncertainty parameter probability non zero one value leaf node
uncertainty leaf nodes random values uniform
distribution uncertainty leaf nodes values
equal probability
structure influence parameter probability decision diagram containing
particular proposition influence decision diagrams
none included however



fidecision theoretic non markovian rewards

including propositions unlikely significant structure
decision diagrams depend values propositions
proportion reachable parameter lower bound proportion entire n
state space reachable start state adds behaviour
lower bound reached value running
actions sufficient allow entire state space reachable
reward specification produced regard generated dynamics
specified number rewards reachable specified number unreachable
first decision diagram produced represent states reachable
given domain dynamics next random path taken root
decision diagram true terminal generating attainable reward false
terminal producing unattainable reward propositions encountered
path negated form conjunction reward formula process
repeated desired number reachable unreachable rewards obtained

references
labs graphviz available http www att com
sw tools graphviz
bacchus f boutilier c grove rewarding behaviors proc american
national conference artificial intelligence aaai pp
bacchus f boutilier c grove structured solution methods nonmarkovian decision processes proc american national conference artificial
intelligence aaai pp
bacchus f kabanza f temporally extended goals annals
mathematics artificial intelligence
bacchus f kabanza f temporal logic express search control knowledge
artificial intelligence
baier c groer leucker bollig b ciesinski f controller synthesis
probabilistic systems extended abstract proc ifip international conference
theoretical computer science ifip tcs
baral c zhao j goal specification presence nondeterministic actions
proc european conference artificial intelligence ecai pp
barto bardtke singh learning act real time dynamic programming artificial intelligence
bonet b geffner h labeled rtdp improving convergence real time
dynamic programming proc international conference automated
scheduling icaps pp



fithiebaux gretton slaney price kabanza

bonet b geffner h mgpt probabilistic planner heuristic search
journal artificial intelligence
boutilier c dean hanks decision theoretic structural assumptions computational leverage journal artificial intelligence
vol pp
boutilier c dearden r goldszmidt stochastic dynamic programming
factored representations artificial intelligence
calvanese de giacomo g vardi reasoning actions ltl action theories proc international conference principles
knowledge representation reasoning kr pp
cesta bahadori g c grisetti g giuliani loochi l leone g nardi
oddi pecora f rasconi r saggase scopelliti robocare
project cognitive systems care elderly proc international conference
aging disability independence icadi
chomicki j efficient checking temporal integrity constraints bounded
history encoding acm transactions database systems
dal lago u pistore traverso p language extended
goals proc american national conference artificial intelligence aaai pp

dean kaelbling l kirman j nicholson time constraints stochastic domains artificial intelligence
dean kanazawa k model reasoning persistance causation
computational intelligence
drummond situated control rules proc international conference
principles knowledge representation reasoning kr pp
emerson e temporal modal logic handbook theoretical computer
science vol b pp elsevier mit press
feng z hansen e symbolic lao search factored markov decision processes proc american national conference artificial intelligence aaai pp

feng z hansen e zilberstein symbolic generalization line
proc conference uncertainty artificial intelligence uai pp
fern yoon givan r learning domain specific knowledge random
walks proc international conference automated scheduling
icaps pp
fourman propositional proc aips workshop model theoretic
approaches pp


fidecision theoretic non markovian rewards

gretton c price thiebaux implementation comparison solution
methods decision processes non markovian rewards proc conference
uncertainty artificial intelligence uai pp
gretton c price thiebaux b nmrdpp system decision theoretic
non markovian rewards proc icaps workshop
uncertainty incomplete information pp
haddawy p hanks representations decision theoretic utility
functions deadline goals proc international conference principles
knowledge representation reasoning kr pp
hansen e zilberstein lao heuristic search finds solutions
loops artificial intelligence
hoey j st aubin r hu boutilier c spudd stochastic
decision diagrams proc conference uncertainty artificial intelligence uai
pp
hoffmann j local search topology benchmarks theoretical analysis
proc international conference ai scheduling aips pp
hoffmann j nebel b system fast plan generation
heuristic search journal artificial intelligence
howard r dynamic programming markov processes mit press cambridge

kabanza f thiebaux search control temporally extended
goals proc international conference automated scheduling
icaps pp
karabaev e skvortsova heuristic search solving firstorder mdps proc conference uncertainty artificial intelligence uai
pp
koehler j schuster k elevator control proc
international conference ai scheduling aips pp
korf r real time heuristic search artificial intelligence
kushmerick n hanks weld probabilistic
artificial intelligence
lichtenstein pnueli zuck l glory past proc conference
logics programs pp lncs
onder n whelan g c li l engineering conformant probabilistic planner
journal artificial intelligence



fithiebaux gretton slaney price kabanza

pistore traverso p model checking extended goals
non deterministic domains proc international joint conference artificial intelligence ijcai pp
slaney j semi positive ltl uninterpreted past operator logic journal
igpl
slaney j thiebaux blocks world revisited artificial intelligence

somenzi f
cudd cu decision diagram package
ftp vlsi colorado edu pub

available

teichteil konigsbuch f fabiani p symbolic heuristic policy iteration structured decision theoretic exploration proc icaps workshop uncertainty autonomous systems
thiebaux hertzberg j shoaff w schneider stochastic model
actions plans anytime uncertainty international journal
intelligent systems
thiebaux kabanza f slaney j anytime state solution methods
decision processes non markovian rewards proc conference uncertainty
artificial intelligence uai pp
thiebaux kabanza f slaney j b model checking decisiontheoretic non markovian rewards proc ecai workshop modelchecking artificial intelligence mochart pp
vardi automated verification graph logic automata proc international joint conference artificial intelligence ijcai pp invited

wolper p relation programs computations temporal
logic proc temporal logic specification lncs pp
younes h l littman ppddl extension pddl expressing
domains probabilistic effects tech rep cmu cs school
computer science carnegie mellon university pittsburgh pennsylvania
younes h l littman weissmann asmuth j first probabilistic
track international competition journal artificial intelligence
vol pp
younes h simmons r g policy generation continuous time stochastic
domains concurrency proc international conference automated
scheduling icaps pp




