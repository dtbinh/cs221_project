Journal Artificial Intelligence Research 25 (2006) 17-74

Submitted 12/04; published 01/06

Decision-Theoretic Planning non-Markovian Rewards
Sylvie Thiebaux
Charles Gretton
John Slaney
David Price

Sylvie.Thiebaux@anu.edu.au
Charles.Gretton@anu.edu.au
John.Slaney@anu.edu.au
David.Price@anu.edu.au

National ICT Australia &
Australian National University
Canberra, ACT 0200, Australia

Froduald Kabanza

kabanza@usherbrooke.ca

Departement dInformatique
Universite de Sherbrooke
Sherbrooke, Quebec J1K 2R1, Canada

Abstract
decision process rewards depend history rather merely current state called decision process non-Markovian rewards (NMRDP). decisiontheoretic planning, many desirable behaviours naturally expressed properties execution sequences rather properties states, NMRDPs form
natural model commonly adopted fully Markovian decision process (MDP) model.
tractable solution methods developed MDPs directly apply
presence non-Markovian rewards, number solution methods NMRDPs
proposed literature. exploit compact specification non-Markovian
reward function temporal logic, automatically translate NMRDP equivalent MDP solved using efficient MDP solution methods. paper presents
nmrdpp(Non-Markovian Reward Decision Process Planner), software platform
development experimentation methods decision-theoretic planning nonMarkovian rewards. current version nmrdpp implements, single interface,
family methods based existing well new approaches describe detail. include dynamic programming, heuristic search, structured methods. Using
nmrdpp, compare methods identify certain problem features affect
performance. nmrdpps treatment non-Markovian rewards inspired treatment
domain-specific search control knowledge TLPlan planner, incorporates
special case. First International Probabilistic Planning Competition, nmrdpp
able compete perform well domain-independent hand-coded
tracks, using search control knowledge latter.

c
2006
AI Access Foundation. rights reserved.

fiThiebaux, Gretton, Slaney, Price & Kabanza

1. Introduction
1.1 Problem
Markov decision processes (MDPs) widely accepted preferred model
decision-theoretic planning problems (Boutilier, Dean, & Hanks, 1999). fundamental
assumption behind MDP formulation system dynamics
reward function Markovian. Therefore, information needed determine reward
given state must encoded state itself.
requirement always easy meet planning problems, many desirable
behaviours naturally expressed properties execution sequences (see e.g., Drummond, 1989; Haddawy & Hanks, 1992; Bacchus & Kabanza, 1998; Pistore & Traverso,
2001). Typical cases include rewards maintenance property, periodic
achievement goal, achievement goal within given number steps
request made, even simply first achievement goal
becomes irrelevant afterwards.
instance, consider health care robot assists ederly disabled people
achieving simple goals reminding important tasks (e.g. taking pill),
entertaining them, checking transporting objects (e.g. checking stoves
temperature bringing coffee), escorting them, searching (e.g. glasses
nurse) (Cesta et al., 2003). domain, might want reward robot making
sure given patient takes pill exactly every 8 hours (and penalise fails
prevent patient within time frame!), may
reward repeatedly visiting rooms ward given order reporting
problem detects, may receive reward patients request answered
within appropriate time-frame, etc. Another example elevator control domain
(Koehler & Schuster, 2000), elevator must get passengers origin
destination efficiently possible, attempting satisfying range
conditions providing priority services critical customers. domain,
trajectories elevator desirable others, makes natural encode
problem assigning rewards trajectories.
decision process rewards depend sequence states passed
rather merely current state called decision process non-Markovian
rewards (NMRDP) (Bacchus, Boutilier, & Grove, 1996). difficulty NMRDPs
efficient MDP solution methods directly apply them. traditional way
circumvent problem formulate NMRDP equivalent MDP, whose states
result augmenting original NMRDP extra information capturing
enough history make reward Markovian. Hand crafting MDP however
difficult general. exacerbated fact size MDP
impacts effectiveness many solution methods. Therefore, interest
automating translation MDP, starting natural specification nonMarkovian rewards systems dynamics (Bacchus et al., 1996; Bacchus, Boutilier,
& Grove, 1997). problem focus on.

18

fiDecision-Theoretic Planning non-Markovian Rewards

1.2 Existing Approaches
solving NMRDPs setting, central issue define non-Markovian reward
specification language translation MDP adapted class MDP solution
methods representations would use type problems hand.
precisely, tradeoff effort spent translation, e.g. producing
small equivalent MDP without many irrelevant history distinctions, effort required
solve it. Appropriate resolution tradeoff depends type representations
solution methods envisioned MDP. instance, structured representations
solution methods ability ignore irrelevant information may cope
crude translation, state-based (flat) representations methods require
sophisticated translation producing MDP small feasible.
two previous proposals within line research rely past linear temporal
logic (PLTL) formulae specify behaviours rewarded (Bacchus et al., 1996, 1997).
nice feature PLTL yields straightforward semantics non-Markovian
rewards, lends range translations crudest finest. two
proposals adopt different translations adapted two different types solution
methods representations. first (Bacchus et al., 1996) targets classical state-based
solution methods policy iteration (Howard, 1960) generate complete policies
cost enumerating states entire MDP. Consequently, adopts expensive
translation attempts produce minimal MDP. contrast, second translation
(Bacchus et al., 1997) efficient crude, targets structured solution methods
representations (see e.g., Hoey, St-Aubin, Hu, & Boutilier, 1999; Boutilier, Dearden, &
Goldszmidt, 2000; Feng & Hansen, 2002), require explicit state enumeration.
1.3 New Approach
first contribution paper provide language translation adapted
another class solution methods proven quite effective dealing large
MDPs, namely anytime state-based heuristic search methods LAO* (Hansen &
Zilberstein, 2001), LRTDP (Bonet & Geffner, 2003), ancestors (Barto, Bardtke, &
Singh, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1995; Thiebaux, Hertzberg, Shoaff,
& Schneider, 1995). methods typically start compact representation
MDP based probabilistic planning operators, search forward initial state,
constructing new states expanding envelope policy time permits. may
produce approximate even incomplete policy, explicitly construct explore
fraction MDP. Neither two previous proposals well-suited
solution methods, first cost translation (most performed
prior solution phase) annihilates benefits anytime algorithms, second
size MDP obtained obstacle applicability state-based
methods. Since cost translation size MDP results
severely impact quality policy obtainable deadline, need
appropriate resolution tradeoff two.
approach following main features. translation entirely embedded
anytime solution method, full control given parts MDP
explicitly constructed explored. MDP obtained minimal,
19

fiThiebaux, Gretton, Slaney, Price & Kabanza

minimal size achievable without stepping outside anytime framework, i.e.,
without enumerating parts state space solution method would necessarily
explore. formalise relaxed notion minimality, call blind minimality
reference fact require lookahead (beyond fringe).
appropriate context anytime state-based solution methods, want
minimal MDP achievable without expensive pre-processing.
rewarding behaviours specified PLTL, appear
way achieving relaxed notion minimality powerful blind minimality without
prohibitive translation. Therefore instead PLTL, adopt variant future linear
temporal logic (FLTL) specification language, extend handle rewards.
language complex semantics PLTL, enables natural translation blind-minimal MDP simple progression reward formulae. Moreover,
search control knowledge expressed FLTL (Bacchus & Kabanza, 2000) fits particularly
nicely framework, used dramatically reduce fraction search
space explored solution method.
1.4 New System
second contribution nmrdpp, first reported implementation NMRDP solution
methods. nmrdpp designed software platform development experimentation common interface. Given description actions domain, nmrdpp
lets user play compare various encoding styles non-Markovian rewards
search control knowledge, various translations resulting NMRDP MDP,
various MDP solution methods. solving problem, made record
range statistics space time behaviour algorithms. supports
graphical display MDPs policies generated.
nmrdpps primary interest treatment non-Markovian rewards,
competitive platform decision-theoretic planning purely Markovian rewards.
First International Probabilistic Planning Competition, nmrdpp able enrol
domain-independent hand-coded tracks, attempting problems featuring
contest. Thanks use search control-knowledge, scored second place
hand-coded track featured probabilistic variants blocks world logistics
problems. surprisingly, scored second domain-independent subtrack consisting problems taken blocks world logistic domains.
latter problems released participants prior competition.
1.5 New Experimental Analysis
third contribution experimental analysis factors affect performance
NMRDP solution methods. Using nmrdpp, compared behaviours
influence parameters structure degree uncertainty dynamics,
type rewards syntax used described them, reachability conditions
tracked, relevance rewards optimal policy. able identify number
general trends behaviours methods provide advice concerning
best suited certain circumstances. experiments lead us rule one

20

fiDecision-Theoretic Planning non-Markovian Rewards

methods systematically underperforming, identify issues claim
minimality made one PLTL approaches.
1.6 Organisation Paper
paper organised follows. Section 2 begins background material MDPs,
NMRDPs, existing approaches. Section 3 describes new approach Section 4
presents nmrdpp. Sections 5 6 report experimental analysis various approaches. Section 7 explains used nmrdpp competition. Section 8 concludes
remarks related future work. Appendix B gives proofs theorems.
material presented compiled series recent conference workshop
papers (Thiebaux, Kabanza, & Slaney, 2002a, 2002b; Gretton, Price, & Thiebaux, 2003a,
2003b). Details logic use represent rewards may found 2005 paper
(Slaney, 2005).

2. Background
2.1 MDPs, NMRDPs, Equivalence
start notation definitions. Given finite set states, write
set finite sequences states S, set possibly infinite
state sequences. stands possibly infinite state sequence
natural number, mean state index , (i) mean prefix
h0 , . . . , . ; 0 denotes concatenation 0 .
2.1.1 MDPs
Markov decision process type consider 5-tuple hS, s0 , A, Pr, Ri,
finite set fully observable states, s0 initial state, finite set actions
(A(s) denotes subset actions applicable S), {Pr(s, a, ) | S, A(s)}
family probability distributions S, Pr(s, a, s0 ) probability
state s0 performing action state s, R : 7 IR reward function
R(s) immediate reward state s. well known MDP
compactly represented using dynamic Bayesian networks (Dean & Kanazawa, 1989;
Boutilier et al., 1999) probabilistic extensions traditional planning languages (see e.g.,
Kushmerick, Hanks, & Weld, 1995; Thiebaux et al., 1995; Younes & Littman, 2004).
stationary policy MDP function : 7 A, (s) A(s)
action executed state s. value V policy s0 , seek
maximise, sum expected future rewards infinite horizon, discounted
far future occur:
V (s0 ) = lim E
n

X
n





R(i ) | , 0 = s0

i=0

0 < 1 discount factor controlling contribution distant rewards.

21

fiThiebaux, Gretton, Slaney, Price & Kabanza

.............................
........
...
....
...
....
.

.....................
...
.... 0 ....
.
....
...................

@
R



..................................
....
.......
...
...
..

initial state s0 , p false two actions
possible: causes transition s1 probability
0.1, change probability 0.9, b
transition probabilities 0.5. state s1 , p true,
actions c (stay go) lead s1 s0
respectively probability 1.
reward received first time p true,
subsequently. is, rewarded state sequences are:
hs0 , s1
hs0 , s0 , s1
hs0 , s0 , s0 , s1
hs0 , s0 , s0 , s0 , s1
etc.



..
..
...
...
...
..
....
...
.
.
.
.
.
.......
.
.
.
.
.
.
.
.
.
.
..................................
................................
...
...
...
...
...
...
....
.
..
..
..
..
...
..
...
.
..
...
.
....
..
.... .................. .......
....... ...
... .......
.....
.
... 1 .....
.
....
...................



0.9

b
6@

1.0

0.1

0.5

0.5

~s =
I.....
@
...
..
c

.....
...
....
............

...
..

1.0 ..................

Figure 1: Simple NMRDP
2.1.2 NMRDPs
decision process non-Markovian rewards identical MDP except
domain reward function . idea process passed
state sequence (i) stage i, reward R((i)) received stage i. Figure 1
gives example. reward function, policy NMRDP depends history,
mapping A. before, value policy expectation
discounted cumulative reward infinite horizon:
X

n

V (s0 ) = lim E
R((i)) | , 0 = s0
n

i=0

e
decision process = hS, s0 , A, Pr, Ri state S, let D(s)
stand
set state sequences rooted feasible actions D, is:
e
D(s)
= { | 0 = A(i ) Pr(i , a, i+1 ) > 0}. Note definition
e
D(s) depend R therefore applies MDPs NMRDPs.
2.1.3 Equivalence
clever algorithms developed solve MDPs cannot directly applied NMRDPs.
One way dealing problem translate NMRDP equivalent MDP
expanded state space (Bacchus et al., 1996). expanded states MDP
(e-states, short) augment states NMRDP encoding additional information
sufficient make reward history-independent. instance, want reward
first achievement goal g NMRDP, states equivalent MDP would
carry one extra bit information recording whether g already true. e-state
seen labelled state NMRDP (via function Definition 1 below)
history information. dynamics NMRDPs Markovian, actions
probabilistic effects MDP exactly NMRDP. following definition,
adapted given Bacchus et al. (1996), makes concept equivalent MDP
precise. Figure 2 gives example.

22

fiDecision-Theoretic Planning non-Markovian Rewards


..............................
.......
..
....
...
....
.



.................................
....
.......
...
...
..

...................
....
.
..... 0 ....
.... 2 ....
..
..
................
...
..
...
...
....
...
.
.
.
.
.
.......
.
.
.
.
.
.
.
.
.
.
.
.
.
..............................
................................
.
...
...
...
...
...
..
....
.
..
..
..
..
...
.
..
...
.
.
...
.
.
....
..
.... ................. .......
.
....... ...
..... 0 ..............
...
.
.
.
.... 3 ...
...............

@
R ?




0.9

0.1

b
6@

1.0

~s =

c....

@
....
.
.
.....
...
....
...........

1.0

0.5

..... ................
...........
......
...
...
...
....
.
.
..
.............................. ....
.
.
.
.
.
..
.
.
.
.
.
.
..........
..
.....
.
.
.
.
.
.
.
.
.
.
.
.
...
.
...
.
.
.
.
...
....................
....................
...
...
.
.
..... 0 ....
..... 0 ....
.... 1 ....
.... 0 ....
................
...............
...
....
......
.
....
......
........
...
............... ..........................
...
..........
.
..
...
...
..
..
...
.
.
.
....
.
.
.
.
.......
............. ...................

0.5

0.5

0.5...........



b

/

c

@







...
..
..
....
............

0.1





@


0.9

Figure 2: MDP Equivalent NMRDP Figure 1. (s00 ) = (s02 ) = s0 . (s01 ) =
(s03 ) = s1 . initial state s00 . State s01 rewarded; states not.

Definition 1 MDP D0=hS 0 , s00 , A0 , Pr0, R0 equivalent NMRDP = hS, s0 , A, Pr, Ri
exists mapping : 0 7 that:1
1. (s00 ) = s0 .
2. s0 0 , A0 (s0 ) = A( (s0 )).
3. s1 , s2 S, A(s1 ) Pr(s1 , a, s2 ) > 0, s01 0
(s01 ) = s1 , exists unique s02 0 , (s02 ) = s2 ,
a0 A0 (s01 ), Pr0 (s01 , a0 , s02 ) = Pr(s1 , a0 , s2 ).
e 0 ) 0
e 0 (s0 ) (0 ) =
4. feasible state sequence D(s
0

i, have: R0 (0i ) = R((i)) i.
Items 13 ensure bijection feasible state sequences NMRDP
feasible e-state sequences MDP. Therefore, stationary policy MDP
reinterpreted non-stationary policy NMRDP. Furthermore, item 4 ensures
two policies identical values, consequently, solving NMRDP optimally
reduces producing equivalent MDP solving optimally (Bacchus et al., 1996):
Proposition 1 Let NMRDP, D0 equivalent MDP it, 0 policy
e 0 ) ((i)) = 0 (0 ),
D0 . Let function defined sequence prefixes (i) D(s

0
j (j ) = j . policy V (s0 ) = V0 (s00 ).
1. Technically, definition allows sets actions A0 different, action
differ must inapplicable reachable states NMRDP e-states equivalent
MDP. practical purposes, A0 seen identical.

23

fiThiebaux, Gretton, Slaney, Price & Kabanza

2.2 Existing Approaches
existing approaches NMRDPs (Bacchus et al., 1996, 1997) use temporal logic
past (PLTL) compactly represent non-Markovian rewards exploit compact
representation translate NMRDP MDP amenable off-the-shelf solution
methods. However, target different classes MDP representations solution methods, consequently, adopt different styles translations.
Bacchus et al. (1996) target state-based MDP representations. equivalent MDP
first generated entirely involves enumeration e-states transitions
them. Then, solved using traditional dynamic programming methods
value policy iteration. methods extremely sensitive number
states, attention paid producing minimal equivalent MDP (with least number
states). first simple translation call pltlsim produces large MDP
post-processed minimisation solved. Another, call pltlmin,
directly results minimal MDP, relies expensive pre-processing phase.
second approach (Bacchus et al., 1997), call pltlstr, targets structured
MDP representations: transition model, policies, reward value functions represented compact form, e.g. trees algebraic decision diagrams (ADDs) (Hoey et al.,
1999; Boutilier et al., 2000). instance, probability given proposition (state
variable) true execution action specified tree whose internal
nodes labelled state variables whose previous values given variable depends, whose arcs labelled possible previous values (> ) variables,
whose leaves labelled probabilities. translation amounts augmenting
structured MDP new temporal variables tracking relevant properties state
sequences, together compact representation (1) dynamics, e.g. trees
previous values relevant variables, (2) non-Markovian reward function
terms variables current values. Then, structured solution methods structured
policy iteration SPUDD algorithm run resulting structured MDP. Neither
translation solution methods explicitly enumerates states.
review approaches detail. reader referred respective
papers additional information.
2.2.1 Representing Rewards PLTL
syntax PLTL, language chosen represent rewarding behaviours,
propositional logic, augmented operators (previously) (since) (see Emerson, 1990). Whereas classical propositional logic formula denotes set states (a subset
S), PLTL formula denotes set finite sequences states (a subset ). formula
without temporal modality expresses property must true current state, i.e.,
last state finite sequence. f specifies f holds previous state (the
state one last). f1 f2 , requires f2 true point sequence, and, unless point present, f1 held ever since. formally,
modelling relation |= stating whether formula f holds finite sequence (i) defined
recursively follows:
(i) |= p iff p , p P, set atomic propositions

24

fiDecision-Theoretic Planning non-Markovian Rewards

(i) |= f iff (i) 6|= f
(i) |= f1 f2 iff (i) |= f1 (i) |= f2
(i) |= f iff > 0 (i 1) |= f
(i) |= f1 f2 iff j i, (j) |= f2 k, j < k i, (k) |= f1
- f > f meaning f true
S, one define useful operators
- f meaning f always true. E.g, g
- g denotes
point, fif
set finite sequences ending state g true first time sequence.
- k f
useful abbreviation k (k times ago), k iterations modality,
ki=1 f (f true k last steps), fik f ki=1 f (f true
k last steps).
Non-Markovian reward functions described set pairs (fi : ri )
PLTL reward formula ri real, semantics reward assigned
sequence sum ri sequence model . Below, let
F denote set reward formulae description reward function. Bacchus
et al. (1996) give list behaviours might useful reward, together
expression PLTL. instance, f atemporal formula, (f : r) rewards
r units achievement f whenever happens. Markovian reward.
- f : r) rewards every state following (and including) achievement f ,
contrast (
- f : r) rewards first occurrence f . (f fik f : r) rewards occurrence
(f
f every k steps. (n : r) rewards nth state, independently
properties. (2 f1 f2 f3 : r) rewards occurrence f1 immediately followed
f2 f3 . reactive planning, so-called response formulae describe
achievement f triggered condition (or command) c particularly useful.
- c : r) every state f true following first issue
written (f
command rewarded. Alternatively, written (f (f c) : r)
first occurrence f rewarded command. common
- k c : r)
reward achievement f within k steps trigger; write example (f
reward states f holds.
theoretical point view, known (Lichtenstein, Pnueli, & Zuck, 1985)
behaviours representable PLTL exactly corresponding star-free regular
languages. Non star-free behaviours (pp) (reward even number states
containing p) therefore representable. Nor, course, non-regular behaviours
pn q n (e.g. reward taking equal numbers steps left right). shall
speculate severe restriction purposes planning.
2.2.2 Principles Behind Translations
three translations MDP (pltlsim, pltlmin, pltlstr) rely equivalence f1 f2 f2 (f1 (f1 f2 )), decompose temporal modalities
requirement last state sequence (i), requirement
prefix (i 1) sequence. precisely, given state formula f , one com-

25

fiThiebaux, Gretton, Slaney, Price & Kabanza

pute in2 O(||f ||) new formula Reg(f, s) called regression f s. Regression
property that, > 0, f true finite sequence (i) ending = iff
Reg(f, s) true prefix (i 1). is, Reg(f, s) represents must
true previously f true now. Reg defined follows:
Reg(p, s) = > iff p otherwise, p P
Reg(f, s) = Reg(f, s)
Reg(f1 f2 , s) = Reg(f1 , s) Reg(f2 , s)
Reg(f, s) = f
Reg(f1 f2 , s) = Reg(f2 , s) (Reg(f1 , s) (f1 f2 ))
instance, take state p holds q not, take f = (q) (p q),
meaning q must false 1 step ago, must held point
past p must held since q last did. Reg(f, s) = q (p q), is,
f hold now, previous stage, q false p q requirement
still hold. p q false s, Reg(f, s) = , indicating f
cannot satisfied, regardless came earlier sequence.
notational convenience, X set formulae write X X{x | x X}.
translations exploit PLTL representation rewards follows. expanded
state (e-state) generated MDP seen labelled set Sub(F )
subformulae reward formulae F (and negations). subformulae must
(1) true paths leading e-state, (2) sufficient determine current
truth reward formulae F , needed compute current reward. Ideally
(3) small enough enable that, i.e. contain
subformulae draw history distinctions irrelevant determining reward
one point another. Note however worst-case, number distinctions
needed, even minimal equivalent MDP, may exponential ||F ||. happens
instance formula k f , requires k additional bits information memorising
truth f last k steps.
2.2.3 pltlsim
choice s, Bacchus et al. (1996) consider two cases. simple case,
call pltlsim, MDP obeying properties (1) (2) produced simply labelling
e-state set subformulae Sub(F ) true sequence leading
e-state. MDP generated forward, starting initial e-state labelled
s0 set 0 Sub(F ) subformulae true sequence
hs0 i. successors e-state labelled NMRDP state subformula set
generated follows: labelled successor s0 NMRDP
set subformulae { 0 Sub(F ) | |= Reg( 0 , s0 )}.
instance, consider NMRDP shown Figure 3. set F = {qp} consists
single reward formula. set Sub(F ) consists subformulae reward formula,
2. size ||f || reward formula measured length size ||F || set reward formulae
F measured sum lengths formulae F .

26

fiDecision-Theoretic Planning non-Markovian Rewards

start_state
a(0.16)
p

a(1) b(1)

a(0.04) b(0.2)
a(0.16)

a(0.64)

b(0.8)

q

a(0.2) b(1)

a(0.8)
p, q

a(1) b(1)

initial state, p q false.
p false, action independently sets
p q true probability 0.8.
p q false, action b sets q true
probability 0.8. actions
effect otherwise. reward obtained
whenever q p. optimal policy
apply b q gets produced, making
sure avoid state left-hand side,
apply p gets produced,
apply b indifferently forever.

Figure 3: Another Simple NMRDP
negations, Sub(F ) = {p, q, p, p, q p, p, q, p, p, (q
p)}. equivalent MDP produced pltlsim shown Figure 4.
2.2.4 pltlmin
Unfortunately, MDPs produced pltlsim far minimal. Although could
postprocessed minimisation invoking MDP solution method,
expansion may still constitute serious bottleneck. Therefore, Bacchus et al. (1996) consider
complex two-phase translation, call pltlmin, capable producing
MDP satisfying property (3). Here, preprocessing phase iterates states
S, computes, state s, set l(s) subformulae, function l
solution fixpoint equation l(s) = F {Reg( 0 , s0 ) | 0 l(s0 ), s0 successor s}.
subformulae l(s) candidates inclusion sets labelling respective
e-states labelled s. is, subsequent expansion phase above, taking
0 l(s0 ) 0 l(s0 ) instead 0 Sub(F ) 0 Sub(F ). subformulae
l(s) exactly relevant way feasible execution sequences starting
e-states labelled rewarded, leads expansion phase produce minimal
equivalent MDP.
Figure 5 shows equivalent MDP produced pltlmin NMRDP example
Figure 3, together function l labels built. Observe
MDP smaller pltlsim MDP: reach state left-hand side
p true q false, point tracking values subformulae,
q cannot become true reward formula cannot either. reflected fact
l({p}) contains reward formula.
worst case, computing l requires space, number iterations S,
exponential ||F ||. Hence question arises whether gain expansion
phase worth extra complexity preprocessing phase. one questions
experimental analysis Section 5 try answer.
2.2.5 pltlstr
pltlstr translation seen symbolic version pltlsim. set
added temporal variables contains purely temporal subformulae PTSub(F ) reward
formulae F , modality prepended (unless already there): = { |

27

fiThiebaux, Gretton, Slaney, Price & Kabanza

start_state
f6,f7,f8,f9,f10
Reward=0
a(0.16)

a(0.04) b(0.2)

a(0.16)

p
f1,f7,f8,f9,f10
Reward=0

a(0.64)

q
f2,f6,f8,f9,f10
Reward=0

a(1) b(1)

p
f1,f3,f4,f7,f10
Reward=0

following subformulae Sub(F ) label
e-states:
f1 : p
f2 : q
f3 : p
f4 : p
f5 : q p
f6 : p
f7 : q
f8 : p
f9 : p
f10 : (q p)

a(0.2) b(1)

a(0.8)

p
f1,f3,f7,f9,f10
Reward=0
a(1)

b(0.8)

p, q
f1,f2,f8,f9,f10
Reward=0
b(1)

a(1)

a(1) b(1)

b(1)

p, q
f1,f2,f3,f9,f10
Reward=0
a(1) b(1)
p, q
f1,f2,f3,f4,f5
Reward=1

a(1) b(1)

Figure 4: Equivalent MDP Produced pltlsim

start_state
f4,f5,f6
Reward=0
a(0.16)
p
f4
Reward=0

a(1) b(1)

a(0.04) b(0.2)

a(0.16)

b(0.8)

q
f4,f5,f6
Reward=0

a(0.64)

function l given by:
l({}) = {q p, p, p}
l({p}) = {q p}
l({q}) = {q p, p, p}
l({p, q}) = {q p, p, p}

a(0.2) b(1)

a(0.8)

following formulae label e-states:
f1 : q p
f2 : p
f3 : p
f4 : (q p)
f5 : p
f6 : p

p, q
f3,f4,f5
Reward=0
a(1)

b(1)

p, q
f2,f3,f4
Reward=0
a(1)

b(1)

p, q
f1,f2,f3
Reward=1

a(1) b(1)

Figure 5: Equivalent MDP Produced pltlmin

28

fiDecision-Theoretic Planning non-Markovian Rewards

q

p

1.00

prv prv p

prv p

0.00

1. dynamics p

1.00

0.00

2. dynamics p

0.00

1.00

3. reward

Figure 6: ADDs Produced pltlstr. prv (previously) stands
PTSub(F ), 6= 0 } { | PTSub(F )}. repeatedly applying equivalence
f1 f2 f2 (f1 (f1 f2 )) subformula PTSub(F ), express current
value, hence reward formulae, function current values formulae
state variables, required compact representation transition
reward models.
NMRDP example Figure 3, set purely temporal variables PTSub(F ) =
{p, p}, identical PTSub(F ). Figure 6 shows ADDs forming
part symbolic MDP produced pltlstr: ADDs describing dynamics
temporal variables, i.e., ADDs describing effects actions b
respective values, ADD describing reward.
complex illustration, consider example (Bacchus et al., 1997)
- (p (q r))} {> (p (q r))}
F = {

PTSub(F ) = {> (p (q r)), p (q r), r}
set temporal variables used
= {t1 : (> (p (q r))), t2 : (p (q r)), t3 : r}
Using equivalences, reward decomposed expressed means
propositions p, q temporal variables t1 , t2 , t3 follows:
> (p (q r))

(p (q r)) (> (p (q r)))
(q r) (p (p (q r))) t1

(q t3 ) (p t2 ) t1
pltlsim, underlying MDP produced pltlstr far minimal
encoded history features even vary one state next. However, size
problematic state-based approaches, structured solution methods
enumerate states able dynamically ignore variables become
irrelevant policy construction. instance, solving MDP, may
29

fiThiebaux, Gretton, Slaney, Price & Kabanza

able determine temporal variables become irrelevant situation
track, although possible principle, costly realised good policy.
dynamic analysis rewards contrast pltlmins static analysis (Bacchus et al.,
1996) must encode enough history determine reward reachable future
states policy.
One question arises circumstances analysis irrelevance structured solution methods, especially dynamic aspects, really effective.
another question experimental analysis try address.

3. fltl: Forward-Looking Approach
noted Section 1 above, two key issues facing approaches NMRDPs
specify reward functions compactly exploit compact representation
automatically translate NMRDP equivalent MDP amenable chosen
solution method. Accordingly, goals provide reward function specification
language translation adapted anytime state-based solution methods.
brief reminder relevant features methods, consider two goals
turn. describe syntax semantics language, notion formula
progression language form basis translation, translation
itself, properties, embedding solution method. call approach
fltl. finish section discussion features distinguish fltl
existing approaches.
3.1 Anytime State-Based Solution Methods
main drawback traditional dynamic programming algorithms policy iteration
(Howard, 1960) explicitly enumerate states reachable s0
entire MDP. interest state-based solution methods, may
produce incomplete policies, enumerate fraction states policy iteration
requires.
Let E() denote envelope policy , set states reachable
(with non-zero probability) initial state s0 policy. defined
E(), say policy complete, incomplete otherwise.
set states E() undefined called fringe policy.
fringe states taken absorbing, value heuristic. common feature
anytime state-based algorithms perform forward search, starting s0
repeatedly expanding envelope current policy one step forward adding one
fringe states. provided admissible heuristic values fringe states,
eventually converge optimal policy without necessarily needing explore
entire state space. fact, since planning operators used compactly represent
state space, may even need construct small subset MDP
returning optimal policy. interrupted convergence, return
possibly incomplete often useful policy.
methods include envelope expansion algorithm (Dean et al., 1995),
deploys policy iteration judiciously chosen larger larger envelopes, using successive policy seed calculation next. recent LAO algorithm (Hansen
30

fiDecision-Theoretic Planning non-Markovian Rewards

& Zilberstein, 2001) combines dynamic programming heuristic search
viewed clever implementation particular case envelope expansion algorithm,
fringe states given admissible heuristic values, policy iteration run
convergence envelope expansions, clever implementation runs
policy iteration states whose optimal value actually affected new fringe
state added envelope. Another example backtracking forward search
space (possibly incomplete) policies rooted s0 (Thiebaux et al., 1995), performed interrupted, point best policy found far returned. Real-time
dynamic programming (RTDP) (Barto et al., 1995) another popular anytime algorithm
MDPs learning real-time (Korf, 1990) deterministic domains,
asymptotic convergence guarantees. RTDP envelope made sample
paths visited frequency determined current greedy policy
transition probabilities domain. RTDP run on-line, off-line given number
steps interrupted. variant called LRTDP (Bonet & Geffner, 2003) incorporates
mechanisms focus search states whose value yet converged, resulting
convergence speed finite time convergence guarantees.
fltl translation present targets anytime algorithms, although
could used traditional methods value policy iteration.
3.2 Language Semantics
Compactly representing non-Markovian reward functions reduces compactly representing
behaviours interest, behaviour mean set finite sequences states
(a subset ), e.g. {hs0 , s1 i, hs0 , s0 , s1 i, hs0 , s0 , s0 , s1 . . .} Figure 1. Recall
reward issued end prefix (i) set. behaviours compactly
represented, straightforward represent non-Markovian reward functions mappings
behaviours real numbers shall defer looking Section 3.6.
represent behaviours compactly, adopt version future linear temporal logic
(FLTL) (see Emerson, 1990), augmented propositional constant $, intended
read behaviour want reward happened reward received now.
language $FLTL begins set basic propositions P giving rise literals:
L ::= P | P | > | | $
> stand true false, respectively. connectives classical
, temporal modalities (next) U (weak until), giving formulae:
F ::= L | F F | F F | F | F U F
weak: f1 U f2 means f1 true f2 is, ever. Unlike
commonly used strong until, imply f2 eventually true.
allows us define useful operator (always): f f U (f always true
k f k iterations modality (f
on). adopt notations
Wk
true exactly k steps), k f i=1 f (f true within next k steps),
V
k f ki=1 f (f true throughout next k steps).
Although negation officially occurs literals, i.e., formulae negation
normal form (NNF), allow write formulae involving usual way,
31

fiThiebaux, Gretton, Slaney, Price & Kabanza

provided equivalent NNF. every formula equivalent,
literal $ eventualities (f true time)
expressible. restrictions deliberate. use notation
logic theorise allocation rewards, would indeed need means say
rewards received express features liveness (always,
reward eventually), fact using mechanism ensuring
rewards given be, restricted purpose eventualities
negated dollar needed. fact, including would create technical difficulties
relating formulae behaviours represent.
semantics language similar FLTL, important difference:
interpretation constant $ depends behaviour B want reward
(whatever is), modelling relation |= must indexed B. therefore write
(, i) |=B f mean formula f holds i-th stage arbitrary sequence ,
relative behaviour B. Defining |=B first step description semantics:
(, i) |=B $ iff (i) B
(, i) |=B >
(, i) 6|=B
(, i) |=B p, p P, iff p
(, i) |=B p, p P, iff p 6
(, i) |=B f1 f2 iff (, i) |=B f1 (, i) |=B f2
(, i) |=B f1 f2 iff (, i) |=B f1 (, i) |=B f2
(, i) |=B

f

iff (, + 1) |=B f

(, i) |=B f1 U f2 iff k (j, j k (, j) 6|=B f2 ) (, k) |=B f1
Note except subscript B first rule, standard FLTL
semantics, therefore $-free formulae keep FLTL meaning. FLTL,
say |=B f iff (, 0) |=B f , |=B f iff |=B f .
modelling relation |=B seen specifying formula holds,
reading takes B input. next final step use |=B relation define,
formula f , behaviour Bf represents, must rather assume
f holds, solve B. instance, let f (p $), i.e., get rewarded
every time p true. would Bf set finite sequences ending
state containing p. arbitrary f , take Bf set prefixes
rewarded f hold sequences:

Definition 2 Bf {B | |=B f }
understand Definition 2, recall B contains prefixes end get
reward $ evaluates true. Since f supposed describe way rewards
received arbitrary sequence, interested behaviours B make $
true way make f hold without imposing constraints evolution
world. However, may many behaviours property, take

32

fiDecision-Theoretic Planning non-Markovian Rewards

intersection,3 ensuring Bf reward prefix prefix
every behaviour satisfying f . pathological cases (see Section 3.4), makes Bf
coincide (set-inclusion) minimal behaviour B |=B f . reason
stingy semantics, making rewards minimal, f actually say rewards
allocated prefixes required truth. instance, (p $) says
reward given every time p true, even though generous distribution
rewards would consistent it.
3.3 Examples
intuitively clear many behaviours specified means $FLTL formulae.
simple way general translate past future tense expressions,4 examples used illustrate PLTL Section 2.2 expressible
naturally $FLTL, follows.
classical goal formula g saying goal p rewarded whenever happens
easily expressed: (p $). already noted, Bg set finite sequences states
p holds last state. care p achieved get rewarded
state on, write (p $). behaviour formula represents
set finite state sequences least one state p holds. contrast,
formula p U (p $) stipulates first occurrence p rewarded (i.e.
specifies behaviour Figure 1). reward occurrence p every k
steps, write (( k+1 p k p) k+1 $).
response formulae, achievement p triggered command c,
write (c (p $)) reward every state p true following first
issue command. reward first occurrence p command, write
(c (p U (p$))). bounded variants reward goal achievement
within k steps trigger command, write example (c k (p $)) reward
states p holds.
worth noting express simple behaviours involving past tense operators.
stipulate reward p always true, write $ U p. say rewarded
p true since q was, write (q ($ U p)).
Finally, often find useful reward holding p occurrence q.
neatest expression q U ((p q) (q $)).
3.4 Reward Normality
$FLTL therefore quite expressive. Unfortunately, rather expressive,
contains formulae describe unnatural allocations rewards. instance,
may make rewards depend future behaviours rather past, may
3.
B |=B f , case $-free f logical theorem,
Bf i.e. following normal set-theoretic conventions. limiting case harm, since
$-free formulae describe attribution rewards.
4. open question whether set representable behaviours $FLTL PLTL,
star-free regular languages. Even behaviours same, little hope
practical translation one exists.

33

fiThiebaux, Gretton, Slaney, Price & Kabanza

leave open choice several behaviours rewarded.5 example
dependence future p $, stipulates reward p going hold
next. call formula reward-unstable. reward-stable f amounts
whether particular prefix needs rewarded order make f true depend
future sequence. example open choice behavior reward
(p $) (p $) says either reward achievements goal p
reward achievements p determine which. call formula rewardindeterminate. reward-determinate f amounts set behaviours
modelling f , i.e. {B | |=B f }, unique minimum. not, Bf insufficient (too
small) make f true.
investigating $FLTL (Slaney, 2005), examine notions reward-stability
reward-determinacy depth, motivate claim formulae rewardstable reward-determinate call reward-normal precisely
capture notion funny business. intuition ask reader
note, needed rest paper. reference then, define:
Definition 3 f reward-normal iff every every B , |=B f iff
every i, (i) Bf (i) B.
property reward-normality decidable (Slaney, 2005). Appendix give
simple syntactic constructions guaranteed result reward-normal formulae.
reward-abnormal formulae may interesting, present purposes restrict attention
reward-normal ones. Indeed, stipulate part method reward-normal
formulae used represent behaviours. Naturally, formulae Section 3.3
normal.
3.5 $FLTL Formula Progression
defined language represent behaviours rewarded, turn
problem computing, given reward formula, minimum allocation rewards states
actually encountered execution sequence, way satisfy formula.
ultimately wish use anytime solution methods generate state sequences
incrementally via forward search, computation best done fly, sequence
generated. therefore devise incremental algorithm based model-checking
technique normally used check whether state sequence model FLTL formula
(Bacchus & Kabanza, 1998). technique known formula progression
progresses pushes formula sequence.
progression technique shown Algorithm 1. essence, computes modelling relation |=B given Section 3.2. However,unlike definition |=B , designed
useful states sequence become available one time, defers
evaluation part formula refers future point next
state becomes available. Let state, say , last state sequence prefix (i)
5. difficulties inherent use linear-time formalisms contexts principle
directionality must enforced. shared instance formalisms developed reasoning
actions Event Calculus LTL action theories (see e.g., Calvanese, De Giacomo, &
Vardi, 2002).

34

fiDecision-Theoretic Planning non-Markovian Rewards

generated far, let b boolean true iff (i) behaviour B
rewarded. Let $FLTL formula f describe allocation rewards possible
futures. progression f given b, written Prog(b, s, f ), new formula
describe allocation rewards possible futures next state, given
passed s. Crucially, function Prog Markovian, depending
current state single boolean value b. Note Prog computable
linear time length f , $-free formulae, collapses FLTL formula
progression (Bacchus & Kabanza, 1998), regardless value b. assume Prog
incorporates usual simplification sentential constants >: f simplifies
, f > simplifies f , etc.
Algorithm 1 $FLTL Progression
Prog(true, s, $)
= >
Prog(false, s, $)
=
Prog(b, s, >)
= >
Prog(b, s, )
=
Prog(b, s, p)
= > iff p otherwise
Prog(b, s, p)
= > iff p 6 otherwise
Prog(b, s, f1 f2 ) = Prog(b, s, f1 ) Prog(b, s, f2 )
Prog(b, s, f1 f2 ) = Prog(b, s, f1 ) Prog(b, s, f2 )
Prog(b, s, f )
= f
Prog(b, s, f1 U f2 ) = Prog(b, s, f2 ) (Prog(b, s, f1 ) f1 U f2 )
Rew(s, f )
$Prog(s, f )

= true iff Prog(false, s, f ) =
= Prog(Rew(s, f ), s, f )

fundamental property Prog following. b ((i) B):
Property 1 (, i) |=B f iff (, + 1) |=B Prog(b, , f )
Proof:

See Appendix B.



|=B , function Prog seems require B (or least b) input, course
progression applied practice f one new state time ,
really want compute appropriate B, namely represented
f . So, similarly Section 3.2, turn second step, use Prog
decide fly whether newly generated sequence prefix (i) Bf
allocated reward. purpose functions $Prog Rew, given
Algorithm 1. Given f , function $Prog Algorithm 1 defines infinite sequence
formulae hf0 , f1 , . . .i obvious way:
f0 = f
fi+1 = $Prog(i , )
decide whether prefix (i) rewarded, Rew first tries progressing
formula boolean flag set false. gives consistent result,
need reward prefix continue without rewarding (i), result
35

fiThiebaux, Gretton, Slaney, Price & Kabanza

know (i) must rewarded order satisfy f . case,
obtain fi+1 must progress again, time boolean flag set
value true. sum up, behaviour corresponding f {(i)|Rew(i , )}.
illustrate behaviour $FLTL progression, consider formula f = p U (p $)
stating reward received first time p true. Let state p
holds, Prog(false, s, f ) = ( p U (p $)) . Therefore, since formula
progressed , Rew(s, f ) true reward received. $Prog(s, f ) = Prog(true, s, f ) =
> ( p U (p $)) >, reward formula fades away affect subsequent
progression steps. If, hand, p false s, Prog(false, s, f ) = (>
p U (p$)) p U (p$)). Therefore, since formula progressed , Rew(s, f )
false reward received. $Prog(s, f ) = Prog(false, s, f ) = p U (p$), reward
formula persists subsequent progression steps.
following theorem states weak assumptions, rewards correctly allocated progression:
Theorem 1 Let f reward-normal, let hf0 , f1 , . . .i result progressing
successive states sequence using function $Prog. Then, provided
, Rew(i , ) iff (i) Bf .
Proof: See Appendix B



premise theorem f never progresses . Indeed =
i, means even rewarding (i) suffice make f true, something must
gone wrong: earlier stage, boolean Rew made false
made true. usual explanation original f reward-normal.
instance p $, reward unstable, progresses next state p
true there: regardless 0 , f0 = p $ = p $, Rew(0 , f0 ) = false, f1 = p,
p 1 f2 = . However, (admittedly bizarre) possibilities exist:
example, although p $ reward-unstable, substitution instance > $,
progresses steps, logically equivalent $ reward-normal.
progression method deliver correct minimal behaviour cases
(even reward-normal cases) would backtrack choice values
boolean flags. interest efficiency, choose allow backtracking. Instead,
algorithm raises exception whenever reward formula progresses , informs
user sequence caused problem. onus thus placed domain
modeller select sensible reward formulae avoid possible progression .
noted worst case, detecting reward-normality cannot easier
decision problem $FLTL expected simple
syntactic criterion reward-normality. practice, however, commonsense precautions
avoiding making rewards depend explicitly future tense expressions suffice
keep things normal routine cases. generous class syntactically recognisable
reward-normal formulae, see Appendix A.
3.6 Reward Functions
language defined far, able compactly represent behaviours.
extension non-Markovian reward function straightforward. represent
36

fiDecision-Theoretic Planning non-Markovian Rewards

function set6 $FLTL IR formulae associated real valued rewards.
call reward function specification. formula f associated reward r ,
write (f : r) . rewards assumed independent additive,
reward function R represented given by:
X
{r | (i) Bf }
Definition 4 R ((i)) =
(f :r)

E.g, {p U (p $) : 5.2, (q $) : 7.3}, get reward 5.2 first time p
holds, reward 7.3 first time q holds onwards, reward 12.5
conditions met, 0 otherwise.
Again, progress reward function specification compute reward
stages . before, progression defines sequence h0 , 1 , . . .i reward function
specifications, i+1 = RProg(i , ), RProg function applies Prog
formulae reward function specification:
RProg(s, ) = {(Prog(s, f ) : r) | (f : r) }
Then, total reward received stage simply sum real-valued rewards
granted progression function behaviours represented formulae :
X
{r | Rew(i , f )}
(f :r)i

proceeding way, get expected analog Theorem 1, states progression
correctly computes non-Markovian reward functions:
Theorem 2 Let reward-normal7 reward function specification, let h0 , 1 . . .i
result progressing successive states
X sequence using function
RProg. Then, provided ( : r) 6 i,
{r | Rew(i , f )} = R ((i)).
(f :r)i

Proof:

Immediate Theorem 1.



3.7 Translation MDP
exploit compact representation non-Markovian reward function reward
function specification translate NMRDP equivalent MDP amenable statebased anytime solution methods. Recall Section 2 e-state MDP
labelled state NMRDP history information sufficient determine
immediate reward. case compact representation reward function specification
0 , additional information summarised progression 0
sequence states passed through. e-state form hs, i,
6. Strictly speaking, multiset, convenience represent set, rewards multiple
occurrences formula multiset summed.
7. extend definition reward-normality reward specification functions obvious way,
requiring reward formulae involved reward normal.

37

fiThiebaux, Gretton, Slaney, Price & Kabanza

state, $FLTL IR reward function specification (obtained progression).
Two e-states hs, ht, equal = t, immediate rewards same,
results progressing semantically equivalent.8
Definition 5 Let = hS, s0 , A, Pr, Ri NMRDP, 0 reward function specification representing R (i.e., R0 = R, see Definition 4). translate MDP
D0 = hS 0 , s00 , A0 , Pr0 , R0 defined follows:
1. 0 2$FLTL IR
2. s00 = hs0 , 0
3. A0 (hs, i) = A(s)
Pr(s, a, s0 ) 0 = RProg(s, )
0
otherwise
0
0
6 (hs, i), Pr (hs, i, a, ) undefined
X
5. R0 (hs, i) =
{r | Rew(s, f )}
4. A0 (hs, i), Pr0 (hs, i, a, hs0 , 0 i) =



(f :r)

6. s0 0 , s0 reachable A0 s00 .
Item 1 says e-states labelled state reward function specification. Item
2 says initial e-state labelled initial state original reward
function specification. Item 3 says action applicable e-state applicable
state labelling it. Item 4 explains successor e-states probabilities
computed. Given action applicable e-state hs, i, successor e-state
labelled successor state s0 via NMRDP progression
s. probability e-state Pr(s, a, s0 ) NMRDP. Note
cost computing Pr0 linear computing Pr sum lengths
formulae . Item 5 motivated (see Section 3.6). Finally, since items 15
leave open choice many MDPs differing unreachable states contain,
item 6 excludes irrelevant extensions. easy show translation leads
equivalent MDP, defined Definition 1. Obviously, function required
Definition1 given (hs, i) = s, proof matter checking conditions.
practical implementation, labelling one step ahead definition:
label initial e-state RProg(s0 , 0 ) compute current reward current reward specification label progression predecessor reward specifications
current state rather predecessor states. apparent below,
potential reduce number states generated MDP.
Figure 7 shows equivalent MDP produced $FLTL version NMRDP
example Figure 3. Recall example, PLTL reward formula q p.
$FLTL, allocation rewards described ((p q) $). figure
8. Care needed notion semantic equivalence. rewards additive, determining
equivalence may involve arithmetic well theorem proving. example, reward function specification {(p $ : 3), (q $ : 2)} equivalent {((p q) $ : 5), ((p q) $ : 3), ((p q) $ : 2)}
although one-one correspondence formulae two sets.

38

fiDecision-Theoretic Planning non-Markovian Rewards

start_state
f1
Reward=0
a(0.16)
p
f1,f2
Reward=0
a(1)
p
f1,f2,f3
Reward=0

a(0.04) b(0.2)

a(0.16)

a(0.64)

q
f1
Reward=0

b(1)

a(1) b(1)

b(0.8)

a(0.2) b(1)

a(0.8)

following formulae label e-states:
f1 : ((p q) $)
f2 : q $
f3 : q $

p, q
f1,f2
Reward=0
a(1) b(1)
p, q
f1,f2,f3
Reward=0
a(1) b(1)
p, q
f1,f2,f3
Reward=1

a(1) b(1)

Figure 7: Equivalent MDP Produced fltl
shows relevant formulae labelling e-states, obtained progression reward
formula. Note without progressing one step ahead, would 3 e-states state
{p} left-hand side, labelled {f1 }, {f1 , f2 }, {f1 , f2 , f3 }, respectively.
3.8 Blind Minimality
size MDP obtained, i.e. number e-states contains key issue
us, amenable state-based solution methods. Ideally, would
MDP minimal size. However, know method building minimal
equivalent MDP incrementally, adding parts required solution method. since
worst case even minimal equivalent MDP larger NMRDP
factor exponential length reward formulae (Bacchus et al., 1996), constructing
entirely would nullify interest anytime solution methods.
However, explain, Definition 5 leads equivalent MDP exhibiting relaxed
notion minimality, amenable incremental construction. inspection,
may observe wherever e-state hs, successor hs0 , 0 via action a,
means order succeed rewarding behaviours described means
execution sequences start going s0 via a, necessary future
starting s0 succeeds rewarding behaviours described 0 . hs,
minimal equivalent MDP, really execution sequences succeeding
rewarding behaviours described , hs0 , 0 must minimal MDP.
is, construction progression introduce e-states priori needed.
Note e-state priori needed may really needed: may fact
execution sequence using available actions exhibits given behaviour.

39

fiThiebaux, Gretton, Slaney, Price & Kabanza

instance, consider response formula (p ( k q k $)), i.e., every time trigger p
true, rewarded k steps later provided q true then. Obviously, whether p
true stage affects way future states rewarded. However,
transition relation happens property k steps state satisfying p,
state satisfying q reached, posteriori p irrelevant, need
label e-states differently according whether p true observe occurrence
example Figure 7, leads fltl produce extra state
bottom left Figure. detect cases, would look perhaps quite deep
feasible futures, cannot constructing e-states fly. Hence
relaxed notion call blind minimality always coincide absolute
minimality.
formalise difference true blind minimality. purpose,
convenient define functions mapping e-states e functions
IR intuitively assigning rewards sequences NMRDP starting (e). Recall
Definition 1 maps e-state MDP underlying NMRDP state.
Definition 6 Let NMRDP. Let 0 set e-states equivalent MDP D0
f0 (s0 )
D. Let e reachable e-state 0 . Let 0 (i) sequence e-states
0
e 0 ) obtained
0 (i) = e. Let (i) corresponding sequence D(s
sense that, j i, (j) = (0j ). , define

(e) : 7



(e) : 7

R((i 1); ) 0 =
0
otherwise

e i)
R((i 1); ) D(
0
otherwise

unreachable e-state e, define (e)() (e)() 0 .
Note carefully difference . former describes rewards assigned
continuations given state sequence, latter confines rewards feasible
continuations. Note well-defined despite indeterminacy
choice 0 (i), since clause 4 Definition 1, choices lead values
R.
Theorem 3 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.
D0 minimal iff every e-state 0 reachable 0 contains two distinct e-states s01
s02 (s01 ) = (s02 ) (s01 ) = (s02 ).
Proof:

See Appendix B.



Blind minimality similar, except that, since looking ahead, distinction
drawn feasible trajectories others future s:
Definition 7 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.
D0 blind minimal iff every e-state 0 reachable 0 contains two distinct estates s01 s02 (s01 ) = (s02 ) (s01 ) = (s02 ).
40

fiDecision-Theoretic Planning non-Markovian Rewards

Theorem 4 Let D0 translation Definition 5. D0 blind minimal
equivalent MDP D.
Proof:

See Appendix B.



size difference blind-minimal minimal MDPs depend
precise interaction rewards dynamics problem hand, making theoretical analyses difficult experimental results rather anecdotal. However, experiments
Section 5 6 show computation time point view, often preferable work blind-minimal MDP invest overhead computing
truly minimal one.
Finally, recall syntactically different semantically equivalent reward function
specifications define e-state. Therefore, neither minimality blind minimality
achieved general without equivalence check least complex theorem
proving LTL. pratical implementations, avoid theorem proving favour embedding (fast) formula simplification progression regression algorithms.
means principle approximate minimality blind minimality,
appears enough practical purposes.
3.9 Embedded Solution/Construction
Blind minimality essentially best achievable anytime state-based solution methods typically extend envelope one step forward without looking deeper
future. translation blind-minimal MDP trivially embedded
solution methods. results on-line construction MDP: method entirely
drives construction parts MDP feels need explore,
leave others implicit. time short, suboptimal even incomplete policy may
returned, fraction state expanded state spaces might constructed.
Note solution method raise exception soon one reward formulae progresses , i.e., soon expanded state hs, built ( : r) ,
since acts detector unsuitable reward function specifications.
extent enabled blind minimality, approach allows dynamic analysis
reward formulae, much pltlstr (Bacchus et al., 1997). Indeed, execution
sequences feasible particular policy actually explored solution method contribute analysis rewards policy. Specifically, reward formulae generated
progression given policy determined prefixes execution sequences
feasible policy. dynamic analysis particularly useful, since relevance
reward formulae particular policies (e.g. optimal policy) cannot detected priori.
forward-chaining planner TLPlan (Bacchus & Kabanza, 2000) introduced idea
using FLTL specify domain-specific search control knowledge formula progression
prune unpromising sequential plans (plans violating knowledge) deterministic
search spaces. shown provide enormous time gains, leading TLPlan
win 2002 planning competition hand-tailored track. approach based
progression, provides elegant way exploit search control knowledge, yet
context decision-theoretic planning. results dramatic reduction

41

fiThiebaux, Gretton, Slaney, Price & Kabanza

fraction MDP constructed explored, therefore substantially better
policies deadline.
achieve follows. specify, via $-free formula c0 , properties know
must verified paths feasible promising policies. simply progress c0
alongside reward function specification, making e-states triples hs, , ci c $-free
formula obtained progression. prevent solution method applying action
leads control knowledge violated, action applicability condition (item
3 Definition 5) becomes: A0 (hs, , ci) iff A(s) c 6= (the changes
straightforward). instance, effect control knowledge formula (p q)
remove consideration feasible path p followed q. detected
soon violation occurs, formula progresses . Although paper focuses
non-Markovian rewards rather dynamics, noted $-free formulae
used express non-Markovian constraints systems dynamics,
incorporated approach exactly control knowledge.
3.10 Discussion
Existing approaches (Bacchus et al., 1996, 1997) advocate use PLTL finite
past specify non-Markovian rewards. PLTL style specification, describe
past conditions get rewarded now, $FLTL describe
conditions present future future states rewarded.
behaviours rewards may scheme, naturalness thinking one
style depends case. Letting kids strawberry dessert
good day fits naturally past-oriented account rewards, whereas
promising may watch movie tidy room (indeed, making sense
whole notion promising) goes naturally $FLTL. One advantage PLTL
formulation trivially enforces principle present rewards depend
future states. $FLTL, responsibility placed domain modeller. best
offer exception mechanism recognise mistakes effects appear,
syntactic restrictions. hand, greater expressive power $FLTL opens
possibility considering richer class decision processes, e.g. uncertainty
rewards received (the dessert movie) (some time next week,
rains).
rate, believe $FLTL better suited PLTL solving NMRDPs
using anytime state-based solution methods. pltlsim translation could easily embedded solution method, loses structure original formulae
considering subformulae individually. Consequently, expanded state space easily
becomes exponentially bigger blind-minimal one. problematic
solution methods consider, size severely affects performance solution
quality. pre-processing phase pltlmin uses PLTL formula regression find sets
subformulae potential labels possible predecessor states, subsequent
generation phase builds MDP representing histories make difference way actually feasible execution sequences rewarded.
recover structure original formula, best case, MDP produced
exponentially smaller blind-minimal one. However, prohibitive cost

42

fiDecision-Theoretic Planning non-Markovian Rewards

pre-processing phase makes unsuitable anytime solution methods. consider method based PLTL regression achieve meaningful relaxed
notion minimality without costly pre-processing phase. fltl approach based
$FLTL progression precisely that, letting solution method resolve
tradeoff quality cost principled way intermediate two extreme
suggestions above.
structured representation solution methods targeted Bacchus et al. (1997)
differ anytime state-based solution methods fltl primarily aims at, particular
require explicit state enumeration all. Here, non-minimality
problematic state-based approaches. virtue size MDP produced,
pltlstr translation is, pltlsim, clearly unsuitable anytime state-based methods.9
another sense, too, fltl represents middle way, combining advantages conferred
state-based structured approaches, e.g. pltlmin one side, pltlstr
other. former fltl inherits meaningful notion minimality. latter,
approximate solution methods used perform restricted dynamic analysis
reward formulae. particular, formula progression enables even state-based methods
exploit structure $FLTL space. However, gap blind
true minimality indicates progression alone insufficient always fully exploit
structure. hope pltlstr able take advantage full structure
reward function, possibility fail exploit even much structure
fltl, efficiently. empirical comparison three approaches needed answer
question identify domain features favoring one other.

4. NMRDPP
first step towards decent comparison different approaches framework
includes all. Non-Markovian Reward Decision Process Planner, nmrdpp,
platform development experimentation approaches NMRDPs.
provides implementation approaches described common framework,
within single system, common input language. nmrdpp available on-line,
see http://rsise.anu.edu.au/~charlesg/nmrdpp. worth noting Bacchus et al.
(1996, 1997) report implementation approaches.
4.1 Input language
input language enables specification actions, initial states, rewards, search
control-knowledge. format action specification essentially
SPUDD system (Hoey et al., 1999). reward specification one formulae,
associated name real number. formulae either PLTL $FLTL.
Control knowledge given language chosen reward. Control
knowledge formulae verified sequence states feasible
generated policies. Initial states simply specified part control knowledge
explicit assignments propositions.
9. would interesting, hand, use pltlstr conjunction symbolic versions
methods, e.g. Symbolic LAO* (Feng & Hansen, 2002) Symbolic RTDP (Feng, Hansen, & Zilberstein,
2003).

43

fiThiebaux, Gretton, Slaney, Price & Kabanza

action flip
heads (0.5)
endaction
action tilt
heads (heads (0.9) (0.1))
endaction
heads =
[first, 5.0]? heads ~prv (pdi heads)
[seq, 1.0]? (prv^2 heads) (prv heads) ~heads
Figure 8: Input Coin Example. prv (previously) stands
-.
pdi (past diamond) stands
instance, consider simple example consisting coin showing either heads
tails (heads). two actions performed. flip action changes
coin show heads tails 50% probability. tilt action changes 10%
probability, otherwise leaving is. initial state tails. get reward 5.0
- heads PLTL) reward 1.0
first head (this written heads
time achieve sequence heads, heads, tails (2 heads heads heads PLTL).
input language, NMRDP described shown Figure 8.
4.2 Common framework
common framework underlying nmrdpp takes advantage fact NMRDP
solution methods can, general, divided distinct phases preprocessing,
expansion, solving. first two optional.
pltlsim, preprocessing simply computes set Sub(F ) subformulae reward
formulae. pltlmin, includes computing labels l(s) state s.
pltlstr, preprocessing involves computing set temporal variables well
ADDs dynamics rewards. fltl require preprocessing.
Expansion optional generation entire equivalent MDP prior solving.
Whether off-line expansion sensible depends MDP solution method used.
state-based value policy iteration used, MDP needs expanded anyway.
If, hand, anytime search algorithm structured method used,
definitely bad idea. experiments, often used expansion solely purpose
measuring size generated MDP.
Solving MDP done using number methods. Currently, nmrdpp provides
implementations classical dynamic programming methods, namely state-based value
policy iteration (Howard, 1960), heuristic search methods: state-based LAO* (Hansen &
Zilberstein, 2001) using either value policy iteration subroutine, one structured
method, namely SPUDD (Hoey et al., 1999). Prime candidates future developments
(L)RTDP (Bonet & Geffner, 2003), symbolic LAO* (Feng & Hansen, 2002), symbolic
RTDP (Feng et al., 2003).
44

fiDecision-Theoretic Planning non-Markovian Rewards

load coin NMRDP
pltlstr preprocessing

> loadWorld(coin)
> preprocess(sPltl)
> startCPUtimer
> spudd(0.99, 0.0001)
> stopCPUtimer
> readCPUtimer
1.22000
> iterationCount
1277
> displayDot(valueToDot)
Expected value

18.87

18.62

report number iterations
display ADD value function

(prv heads)

(prv (prv pdi heads))

23.87

report solving time

heads

(prv heads)

(prv (prv pdi heads))

solve MDP SPUDD(, )

23.62

(prv^2 heads)

(prv pdi heads)

18.25

23.15

(prv pdi heads)

19.25

24.15

display policy

> displayDot(policyToDot)
Optimal policy

heads

(prv heads)

flip

>
>
>
6
>

tilt

pltlmin preprocessing
completely expand MDP
report MDP size

preprocess(mPltl)
expand
domainStateSize
printDomain ("") | show-domain.rb
Reward=0
flip(0.5)

flip(0.5)

display postcript rendering MDP

tilt(0.9)

tilt(0.1)

heads
Reward=5
flip(0.5)
heads
Reward=0
tilt(0.1)

flip(0.5)

tilt(0.9)

tilt(0.9)

flip(0.5)

tilt(0.1)

flip(0.5)

Reward=1
tilt(0.9)

flip(0.5)

tilt(0.9)

flip(0.5)

tilt(0.1)

tilt(0.1)

flip(0.5)
Reward=0
flip(0.5)

flip(0.5)

flip(0.5)

tilt(0.9)

tilt(0.1)

heads
Reward=0

solve MDP VI(, )
report number iterations

> valIt(0.99, 0.0001)
> iterationCount
1277
> getPolicy
...

output policy (textual)

Figure 9: Sample Session
45

fiThiebaux, Gretton, Slaney, Price & Kabanza

4.3 Approaches covered
Altogether, various types preprocessing, choice whether expand,
MDP solution methods, give rise quite number NMRDP approaches, including,
limited previously mentioned (see e.g. pltlstr(a) below). combinations possible. E.g., state-based processing variants incompatible structured
solution methods (the converse possible principle, however). Also, present
structured form preprocessing $FLTL formulae.
pltlstr(a) example interesting variant pltlstr, obtain
considering additional preprocessing, whereby state space explored (without explicitly
enumerating it) produce BDD representation e-states reachable start
state. done starting BDD representing start e-state, repeatedly
applying action. Non-zero probabilities converted ones result or-ed
last result. action adds reachable e-states BDD,
sure represents reachable e-state space. used additional control
knowledge restrict search. noted without phase pltlstr makes
assumptions start state, thus left possible disadvantage. Similar
structured reachability analysis techniques used symbolic implementation
LAO* (Feng & Hansen, 2002). However, important aspect
temporal variables included BDD.
4.4 nmrdpp System
nmrdpp controlled command language, read either file interactively. command language provides commands different phases (preprocessing,
expansion, solution) methods, commands inspect resulting policy value
functions, e.g. rendering via DOT (AT&T Labs-Research, 2000), well supporting
commands timing memory usage. sample session, coin NMRDP
successively solved pltlstr pltlmin shown Figure 9.
nmrdpp implemented C++, makes use number supporting libraries.
particular, relies heavily CUDD package manipulating ADDs (Somenzi,
2001): action specification trees converted stored ADDs system,
moreover structured algorithms rely heavily CUDD ADD computations.
state-based algorithms make use MTL Matrix Template Library matrix
operations. MTL takes advantage modern processor features MMX SSE
provides efficient sparse matrix operations. believe implementations
MDP solution methods comparable state art. instance, found
implementation SPUDD comparable performance (within factor 2)
reference implementation (Hoey et al., 1999). hand, believe data
structures used regression progression temporal formulae could optimised.

5. Experimental Analysis
faced three substantially different approaches easy compare,
performance depend domain features varied structure
transition model, type, syntax, length temporal reward formula, presence

46

fiDecision-Theoretic Planning non-Markovian Rewards

rewards unreachable irrelevant optimal policy, availability good heuristics
control-knowledge, etc, interactions factors. section,
report experimental investigation influence factors try
answer questions raised previously:10
1. dynamics domain predominant factor affecting performance?
2. type reward major factor?
3. syntax used describe rewards major factor?
4. overall best method?
5. overall worst method?
6. preprocessing phase pltlmin pay, compared pltlsim?
7. simplicity fltl translation compensate blind-minimality,
benefit true minimality outweigh cost pltlmin preprocessing?
8. dynamic analyses rewards pltlstr fltl effective?
9. one analyses powerful, rather complementary?
cases all, able identify systematic patterns. results
section obtained using Pentium4 2.6GHz GNU/Linux 2.4.20 machine 500MB
ram.
5.1 Preliminary Remarks
Clearly, fltl pltlstr(a) great potential exploiting domain-specific heuristics control-knowledge; pltlmin less so. avoid obscuring results, therefore
refrained incorporating features experiments. running LAO*,
heuristic value state crudest possible (the sum reward values
problem). Performance results interpreted light necessarily
reflect practical abilities methods able exploit features.
begin general observations. One question raised whether
gain PLTL expansion phase worth expensive preprocessing performed
pltlmin, i.e. whether pltlmin typically outperforms pltlsim. definitively answer
question: pathological exceptions, preprocessing pays. found expansion
bottleneck, post-hoc minimisation MDP produced pltlsim
help much. pltlsim therefore little practical interest, decided
report results performance, often order magnitude worse
pltlmin. Unsurprisingly, found pltlstr would typically scale larger state
spaces, inevitably leading outperform state-based methods. However, effect
uniform: structured solution methods sometimes impose excessive memory requirements
makes uncompetitive certain cases, example n f , large n,
features reward formula.
10. executive summary answers executive reader. 1. no, 2. yes, 3. yes, 4. pltlstr
fltl, 5. pltlsim, 6. yes, 7. yes no, respectively, 8. yes, 9. yes, respectively.

47

fiThiebaux, Gretton, Slaney, Price & Kabanza

5.2 Domains
Experiments performed four hand-coded domains (propositions + dynamics)
random domains. hand-coded domain n propositions pi , dynamics
makes every state possible eventually reachable initial state
propositions false. first two domains, spudd-linear spudd-expon
discussed Hoey et al. (1999); two others own.
intention spudd-linear take advantage best case behaviour
SPUDD. proposition pi , action ai sets pi true propositions
pj , 1 j < false. spudd-expon, used Hoey et al. (1999) demonstrate
worst case behaviour SPUDD. proposition pi , action ai sets pi
true propositions pj , 1 j < true (and sets pi false otherwise),
sets latter propositions false. third domain, called on/off, one turn-on
one turn-off action per proposition. turn-on-pi action probabilistically
succeeds setting pi true pi false. turn-off action similar. fourth
domain, called complete, fully connected reflexive domain. proposition pi
action ai sets pi true probability i/(n + 1) (and false otherwise)
pj , j 6= true false probability 0.5. Note ai cause transition
2n states.
Random domains size n involve n propositions. method generating
dynamics detailed appendix C. Let us summarise saying able
generate random dynamics exhibiting given degree structure given degree
uncertainty. Lack structure essentially measures bushiness internal part
ADDs representing actions, uncertainty measures bushiness leaves.
5.3 Influence Dynamics
interaction dynamics reward certainly affects performance
different approaches, though strikingly factors reward type (see
below). found reward scheme, varying degree structure
uncertainty generally change relative success different approaches.
instance, Figures 10 11 show average run time methods function
degree structure, resp. degree uncertainty, random problems size n = 6
reward n > (the state encountered stage n rewarded, regardless properties11 ).
Run-time increases slightly degrees, significant change relative
performance. typical graphs obtain rewards.
Clearly, counterexamples observation exist. notable cases
extreme dynamics, instance spudd-expon domain. Although small values
n, n = 6, pltlstr approaches faster others handling reward
n > virtually type dynamics encountered, perform poorly
reward spudd-expon. explained fact small fraction
spudd-expon states reachable first n steps. n steps, fltl immediately
recognises reward consequence, formula progressed >.
pltlmin discovers fact expensive preprocessing. pltlstr,
hand, remains concerned prospect reward, pltlsim would.
11.

n $

$FLTL

48

fiAverage CPU time (sec)

Decision-Theoretic Planning non-Markovian Rewards

30
25
20
15
10
5

0.1

0.3

0.5

0.7

0.9

1.1

Structure (0:Structured, ... 1:Unstructured)
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 10: Changing Degree Structure

Average CPU time (sec)

35
25
20
15
10
5

0

0.2

0.4

0.6

0.8

1

1.2

Uncertainty (0:Certain, ... 1:Uncertain)
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 11: Changing Degree Uncertainty
5.4 Influence Reward Types
type reward appears stronger influence performance dynamics.
unsurprising, reward type significantly affects size generated MDP:
certain rewards make size minimal equivalent MDP increase constant
number states constant factor, others make increase factor exponential
length formula. Table 1 illustrates this. third column reports size
minimal equivalent MDP induced formulae left hand side.12
legitimate question whether direct correlation size increase
(in)appropriateness different methods. instance, might expect state-based
methods particularly well conjunction reward types inducing small MDP
12. figures necessarily valid non-completely connected NMRDPs. Unfortunately, even
completely connected domains, appear much cheaper way determine MDP
size generate count states.

49

fiThiebaux, Gretton, Slaney, Price & Kabanza

type
first time pi
pi sequence start state
two consecutive pi
pi n times ago

formula
- ni=1 pi )
(ni=1 pi ) (
(ni=1 pi ) n >
n1
i=1
(pi pi+1 )
n ni=1 pi

size
O(1)||S||
O(n)||S||
O(nk )||S||
O(2n )||S||

fastest
pltlstr(a)
fltl
pltlstr
pltlstr

slowest
pltlmin
pltlstr
fltl
pltlmin

Table 1: Influence Reward Type MDP Size Method Performance

Average CPU time (sec)

1000
600
400
200

2

2.5

3

3.5

4

4.5

5

5.5

n
APPROACHES prvIn
APPROACHES prvOut

Figure 12: Changing Syntax
otherwise badly comparison structured methods. Interestingly, always
case. instance, Table 1 whose last two columns report fastest slowest
methods range hand-coded domains 1 n 12, first row contradicts
expectation. Moreover, although pltlstr fastest last row, larger values
n (not represented table), aborts lack memory, unlike
methods.
obvious observations arising experiments pltlstr nearly
always fastest runs memory. Perhaps interesting results
second row, expose inability methods based PLTL deal
rewards specified long sequences events. converting reward formula
set subformulae, lose information order events,
recovered laboriously reasoning. $FLTL progression contrast takes events one
time, preserving relevant structure step. experimentation led us
observe PLTL based algorithms perform poorly reward specified using
- k f , fik f (f true k steps ago, within last k
formulae form k f ,
steps, last k steps).
5.5 Influence Syntax
Unsurprisingly, find syntax used express rewards, affects length
formula, major influence run time. typical example effect
captured Figure 12. graph demonstrates re-expressing prvOut n (ni=1 pi )
50

fiDecision-Theoretic Planning non-Markovian Rewards

State count/(2^n)

11
9
7
5
3
1
0

2

4

6

8

10

12

14

n
PLTLMIN
FLTL

Figure 13: Effect Multiple Rewards MDP size

Total CPU time (sec)

1500
1000
500

0

2

4

6

8

10

12

14

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 14: Effect Multiple Rewards Run Time
prvIn ni=1 n pi , thereby creating n times temporal subformulae, alters
running time PLTL methods. fltl affected $FLTL progression requires two
iterations reward formula. graph represents averages running
times methods, complete domain.
serious concern relation PLTL approaches handling reward
specifications containing multiple reward elements. notably found pltlmin
necessarily produce minimal equivalent MDP situation. demonstrate, consider set reward formulae {f1 , f2 , . . . , fn }, associated
real value r. Given this, PLTL approaches distinguish unnecessarily past
behaviours lead identical future rewards. may occur reward
e-state determined truth value f1 f2 . formula necessarily require
e-states distinguish cases {f1 >, f2 } {f1 , f2 >}
hold; however, given specification, pltlmin makes distinction. example,

51

fiThiebaux, Gretton, Slaney, Price & Kabanza

taking = pi , Figure 13 shows fltl leads MDP whose size 3 times
NMRDP. contrast, relative size MDP produced pltlmin
linear n, number rewards propositions. results obtained
hand-coded domains except spudd-expon. Figure 14 shows run-times function
n complete. fltl dominates overtaken pltlstr(A) large values
n, MDP becomes large explicit exploration practical. obtain
minimal equivalent MDP using pltlmin, bloated reward specification form
{( ni=1 (pi nj=1,j6=i pj ) : r), . . . , ( ni=1 pi : n r)} necessary, which, virtue
exponential length, adequate solution.
5.6 Influence Reachability
approaches claim ability ignore variables irrelevant
condition track unreachable:13 pltlmin detects preprocessing,
pltlstr exploits ability structured solution methods ignore them, fltl ignores progression never exposes them. However, given mechanisms
avoiding irrelevance different, expect corresponding differences effects.
experimental investigation, found differences performance best illustrated looking response formulae, assert trigger condition c reached
reward received upon achievement goal g in, resp. within, k steps.
- k c, $FLTL, (c k (g $)), resp.
PLTL, written g k c, resp. g
(c k (g $))
goal unreachable, PLTL approaches perform well. always false,
goal g lead behavioural distinctions. hand, constructing
MDP, fltl considers successive progressions k g without able detect
unreachable actually fails happen. exactly blindness blind
minimality amounts to. Figure 15 illustrates difference performance function
number n propositions involved spudd-linear domain, reward
form g n c, g unreachable.
fltl shines trigger unreachable. Since c never happens, formula
always progress itself, goal, however complicated, never tracked generated MDP. situation PLTL approaches still consider k c subformulae,
discover, expensive preprocessing pltlmin, reachability analysis pltlstr(a), never pltlstr, irrelevant. illustrated Figure 16,
spudd-linear reward form g n c, c unreachable.
5.7 Dynamic Irrelevance
Earlier claimed one advantage pltlstr fltl pltlmin pltlsim
former perform dynamic analysis rewards capable detecting irrelevance
variables particular policies, e.g. optimal policy. experiments confirm
claim. However, reachability, whether goal triggering condition
response formula becomes irrelevant plays important role determining whether
13. sometimes speak conditions goals reachable achievable rather feasible,
although may temporally extended. keep line conventional vocabulary
phrase reachability analysis.

52

fiDecision-Theoretic Planning non-Markovian Rewards

Total CPU time (sec)

350
250
150
100
50

2

4

6

8

10

12

14

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 15: Response Formula Unachievable Goal

Total CPU time (sec)

350
250
150
100
50

1

3

5

7

9

11

n
FLTL

PLTLMIN

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 16: Response Formula Unachievable Trigger
pltlstr fltl approach taken: pltlstr able dynamically ignore goal,
fltl able dynamically ignore trigger.
illustrated Figures 17 18. figures, domain considered
on/off n = 6 propositions, response formula g n c before,
g c achievable. response formula assigned fixed reward. study effect
dynamic irrelevance goal, Figure 17, achievement g rewarded value
r (i.e. (g : r) PLTL). Figure 18, hand, study effect
dynamic irrelevance trigger achievement c rewarded value r.
figures show runtime methods r increases.
Achieving goal, resp. trigger, made less attractive r increases
point response formula becomes irrelevant optimal policy.
happens, run-time pltlstr resp. fltl, exhibits abrupt durable improvement.
figures show fltl able pick irrelevance trigger, pltlstr able
exploit irrelevance goal. expected, pltlmin whose analysis static pick
53

fiThiebaux, Gretton, Slaney, Price & Kabanza

Total CPU time (sec)

200
150
100
50

0

50

100

150

200

250

300

350

r
PLTLMIN

PLTLSTRUCT

FLTL

PLTLSTRUCT (A)

Average CPU time (sec)

Figure 17: Response Formula Unrewarding Goal
200
150
100
50

0

50

100

150

200

250

300

350

r
PLTLMIN

FLTL

PLTLSTRUCT

PLTLSTRUCT(A)

Figure 18: Response Formula Unrewarding Trigger
either performs consistently badly. Note figures, pltlstr progressively
takes longer compute r increases value iteration requires additional iterations
converge.
5.8 Summary
experiments artificial domains, found pltlstr fltl preferable statebased PLTL approaches cases. one insists using latter, strongly
recommend preprocessing. fltl technique choice reward requires tracking
long sequence events desired behaviour composed many elements
identical rewards. response formulae, advise use pltlstr probability
reaching goal low achieving goal costly, conversely, advise
use fltl probability reaching triggering condition low reaching
costly. cases, attention paid syntax reward formulae
54

fiDecision-Theoretic Planning non-Markovian Rewards

particular minimising length. Indeed, could expected, found syntax
formulae type non-Markovian reward encode predominant
factor determining difficulty problem, much features
Markovian dynamics domain.

6. Concrete Example
experiments far focused artificial problems aimed characterising
strengths weaknesses various approaches. look concrete example
order give sense size interesting problems techniques
solve. example derived Miconic elevator classical planning benchmark
(Koehler & Schuster, 2000). elevator must get number passengers origin
floor destination. Initially, elevator arbitrary floor passenger
served boarded elevator. version problem, one single
action causes elevator service given floor, effect unserved
passengers whose origin serviced floor board elevator, boarded passengers
whose destination serviced floor unboard become served. task plan
elevator movement passengers eventually served.14
two variants Miconic. simple variant, reward received
time passenger becomes served. hard variant, elevator attempts
provide range priority services passengers special requirements: many passengers
prefer travelling single direction (either down) destination, certain
passengers might offered non-stop travel destination, finally, passengers
disabilities young children supervised inside elevator
passenger (the supervisor) assigned them. omit VIP conflicting group
services present original hard Miconic problem, reward formulae
create additional difficulties.
formulation problem makes use propositions PDDL description Miconic used 2000 International Planning Competition: dynamic propositions
record floor elevator currently whether passengers served boarded,
static propositions record origin destination floors passengers, well
categories (non-stop, direct-travel, supervisor, supervised) passengers fall in. However,
formulation differs PDDL description two interesting ways. Firstly, since
use rewards instead goals, able find preferred solution even
goals cannot simultaneously satisfied. Secondly, priority services naturally
described terms non-Markovian rewards, able use action description simple hard versions, whereas PDDL description hard miconic
requires additional actions (up, down) complex preconditions monitor satisfaction priority service constraints. reward schemes Miconic encapsulated
four different types reward formula.
1. simple variant, reward received first time passenger Pi served:
14. experimented stochastic variants Miconic passengers small probability
desembarking wrong floor. However, find useful present results deterministic
version since closer Miconic deterministic planning benchmark since, shown
before, rewards far crucial impact dynamics relative performance methods.

55

fiThiebaux, Gretton, Slaney, Price & Kabanza

PLTL:

ServedPi ServedPi

$FLTL:

ServedPi U (ServedPi $)

2. Next, reward received time non-stop passenger Pi served one step
boarding elevator:
PLTL:

N onStopPi BoardedPi ServedPi ServedPi

$FLTL:

((N onStopPi BoardedPi ServedPi ServedPi ) $)

3. Then, reward received time supervised passenger Pi served
accompanied times inside elevator supervisor15 Pj :
PLTL:
$FLTL:

SupervisedPi SupervisorPj Pi ServedPi
ServedPi fi(BoardedPi BoardedPj )
ServedPi U ((BoardedPi SupervisedPi (BoardedPj SupervisorPj Pi )
ServedPi ) (ServededPi $))

4. Finally, reward received time direct travel passenger Pi served
travelled one direction since boarding, e.g., case going up:
DirectP
W W ServedPi ServedPi
(( j k>j (AtF loork AtF loorj )) (BoardedPi BoardedPi ))
W W
$FLTL: ((DirectPi BoardedPi ) (ServedPi U ((( j k>i AtF loorj AtF loork )
ServedPi ) (servedPi $))))

PLTL:

similarly case going down.
Experiments section run Dual Pentium4 3.4GHz GNU/Linux 2.6.11
machine 1GB ram. first experimented simple variant, giving reward
50 time passenger first served. Figure 19 shows CPU time taken
various approaches solve random problems increasing number n floors
passengers, Figure 20 shows number states expanded so. data
point corresponds one random problem. fair structured approach,
ran pltlstr(a) able exploit reachability start state. first observation
although pltlstr(a) best small values n, quickly runs memory.
pltlstr(a) pltlsim need track formulae form ServedPi
pltlsim not, conjecture run memory earlier.
second observation attempts PLTL minimisation pay much here.
pltlmin reduced memory tracks fewer subformulae, size
MDP produces identical size pltlsim MDP larger
fltl MDP. size increase due fact PLTL approaches label differently
e-states passengers served, depending become served
(for passengers, reward formula true e-state). contrast, fltl
implementation progression one step ahead labels e-states reward
15. understand $FLTL formula, observe get reward iff (BoardedPi SupervisedPi )
(BoardedPj SupervisorPj Pi ) holds ServedPi becomes true, recall formula q U ((p
q) (q $)) rewards holding p occurrence q.

56

fiDecision-Theoretic Planning non-Markovian Rewards

Total CPU time (sec)

7000
4000
2000
1000

2

4

6

8

10

12

14

n
FLTL
PLTLSIM
PLTLMIN
PLTLSTR(A)

Figure 19: Simple Miconic - Run Time

45

State count/(2^n)

40
35
30
25
20
15
10
5
0
2

4

6

8

10

12

14

n
FLTL
PLTLSIM, PLTLMIN

Figure 20: Simple Miconic - Number Expanded States
formulae relevant passengers still need served, formulae
progressed >. gain number expanded states materialises run time gains,
resulting fltl eventually taking lead.
second experiment illustrates benefits using even extremely simple admissible heuristic conjunction fltl. heuristic applicable discounted stochastic
shortest path problems, discounts rewards shortest time future
possible. simply amounts assigning fringe state value 50 times
number still unserved passengers (discounted once), results avoiding floors
passenger waiting destination boarded passenger.
Figures 21 22 compare run time number states expanded fltl used
conjunction value iteration (valIt) used conjunction LAO*
57

fiThiebaux, Gretton, Slaney, Price & Kabanza

Total CPU time (sec)

35000
20000
10000
5000

2

4

6

8

10

12

14

n
FLTLLAO(h)
FLTLLAO(u)
FLTLvalIt

Figure 21: Effect Simple Heuristic Run Time

State count/(2^n)

50
40
30
20
10
0
2

4

6

8

10

12

14

n
FLTLLAO(h)
FLTLvalIt,FLTLLAO(u)

Figure 22: Effect Simple Heuristic Number Expanded States
search informed heuristic (LAO(h)). Uninformed LAO* (LAO*(u), i.e. LAO*
heuristic 50 n node) included reference point show
overhead induced heuristic search. seen graphs, heuristic search
generates significantly fewer states eventually pays terms run time.
final experiment, considered hard variant, giving reward 50
service (1), reward 2 non-stop travel (2), reward 5 appropriate supervision
(3), reward 10 direct travel (2). Regardless number n floors
passengers, problems feature single non-stop traveller, third passengers require
supervision, half passengers care traveling direct. CPU time number
states expanded shown Figures 23 24, respectively. simple case,
pltlsim pltlstr quickly run memory. Formulae type (2) (3) create
many additional variables track approaches, problem seem
58

fiDecision-Theoretic Planning non-Markovian Rewards

Total CPU time (sec)

14000
8000
4000
2000

2

3

4

5

6

7

n
FLTL
PLTLSIM
PLTLMIN
PLTLSTRUCT(A)

Figure 23: Hard Miconic - Run Time

State count/(2^n)

100
80
60
40
20
0
2

3

4

5

6

7

n
FLTL
PLTLSIM
PLTLMIN

Figure 24: Hard Miconic - Number Expanded States
exhibit enough structure help pltlstr. fltl remains fastest. Here,
seem much due size generated MDP slightly
pltlmin MDP, rather overhead incurred minimisation. Another
observation arising experiment small instances handled
comparison classical planning version problem solved state art
optimal classical planners. example, 2000 International Planning Competition,
PropPlan planner (Fourman, 2000) optimally solved instances hard Miconic
20 passengers 40 floors 1000 seconds much less powerful machine.

59

fiThiebaux, Gretton, Slaney, Price & Kabanza

7. nmrdpp Probabilistic Planning Competition
report behaviour nmrdpp probabilistic track 4th International Planning Competition (IPC-4). Since competition feature non-Markovian
rewards, original motivation taking part compare solution methods
implemented nmrdpp Markovian setting. objective largely underestimated
challenges raised merely getting planner ready competition, especially
competition first kind. end, decided successfully preparing nmrdpp attempt problems competition using one solution method (and possibly
search control knowledge), would honorable result.
crucial problem encountered translation PPDDL (Younes &
Littman, 2004), probabilistic variant PDDL used input language competition, nmrdpps ADD-based input language. translating PPDDL ADDs
possible theory, devising translation practical enough need
competition (small number variables, small, quickly generated, easily manipulable
ADDs) another matter. mtbdd, translator kindly made available participants
competition organisers, always able achieve required efficiency.
times, translation quick nmrdpp unable use generated ADDs efficiently. Consequently, implemented state-based translator top PDDL parser
backup, opted state-based solution method since rely ADDs
could operate translators.
version nmrdpp entered competition following:
1. Attempt get translation ADDs using mtbdd, proves infeasible,
abort rely state-based translator instead.
2. Run fltl expansion state space, taking search control knowledge account
available. Break 10mn complete.
3. Run value iteration convergence. Failing achieve useful result (e.g.
expansion complete enough even reach goal state), go back step 2.
4. Run many 30 trials possible remaining time,16 following generated policy defined, falling back non-deterministic search control
policy available.
Step 1 trying maximise instances original ADD-based
nmrdpp version could run intact. Step 3, decided use LAO*
run good heuristic, often incurs significant overhead compared value
iteration.
problems featured competition classified goal-based rewardbased problems. goal-based problems, (positive) reward received goal
state reached. reward-based problems, action performance may incur (usually
negative) reward. Another orthogonal distinction made problems
16. given problem, planners 15mn run whatever computation saw appropriate (including parsing, pre-processing, policy generation any), execute 30 trial runs generated
policy initial state goal state.

60

fiDecision-Theoretic Planning non-Markovian Rewards

domains communicated advance participants domains
were. latter consisted variants blocks world logistics (or box world)
problems, gave participating planners opportunity exploit knowledge
domain, much hand-coded deterministic planning track.
decided enroll nmrdpp control-knowledge mode domain-independent
mode. difference two modes first uses FLTL search
control knowledge written known domains additional input. main concern
writing control knowledge achieve reasonable compromise size
effectiveness formulae. blocks world domain, two actions
pickup-from putdown-to 25% chance dropping block onto table,
control knowledge used encoded variant well-known GN1 near-optimal strategy
deterministic blocks world planning (Slaney & Thiebaux, 2001): whenever possible,
try putting clear block goal position, otherwise put arbitrary clear block
table. blocks get dropped table whenever action fails,
success probabilities rewards identical across actions, optimal policies
problem essentially made optimal sequences actions deterministic blocks
world little need sophisticated strategy.17 colored blocks
world domain, several blocks share color goal refers
color blocks, control knowledge selected arbitrary goal state non-colored
blocks world consistent colored goal specification, used strategy
non-colored blocks world. performance strategy depends entirely
goal-state selected therefore arbitrarily bad.
Logistics problems IPC-2 distinguish airports locations within
city; trucks drive two locations city planes fly
two airports. contrast, box world features cities,
airport, accessible truck. priori, map truck
plane connections arbitrary. goal get packages city origin
city destination. Moving truck 20% chance resulting reaching one
three cities closest departure city rather intended one. size box
world search space turned quite challenging nmrdpp. Therefore, writing
search control knowledge, gave optimality consideration favored maximal
pruning. helped fact box world generator produces problems
following structure. Cities divided clusters, composed
least one airport city. Furthermore cluster least one hamiltonian circuit
trucks follow. control knowledge used forced planes one, trucks
one cluster idle. cluster, truck allowed move could
attempt driving along chosen hamiltonian circuit, picking dropping parcels
went.
planners participating competition shown Table 2. Planners E, G2,
J1, J2 domain-specific: either tuned blocks box worlds, use
domain-specific search control knowledge, learn examples. participating
planners domain-independent.
17. sophisticated near-optimal strategies deterministic blocks world exist (see Slaney & Thiebaux,
2001), much complex encode might caused time performance problems.

61

fiThiebaux, Gretton, Slaney, Price & Kabanza

Part.
C
E*
G1
G2*
J1*
J2*
J3
P
Q
R

Description
symbolic LAO*
first-order heuristic search fluent calculus
nmrdpp without control knowledge
nmrdpp control knowledge
interpreter hand written classy policies
learns classy policies random walks
version replanning upon failure
mgpt: lrtdp automatically extracted heuristics
ProbaProp: conformant probabilistic planner
structured reachability analysis structured PI

Reference
(Feng & Hansen, 2002)
(Karabaev & Skvortsova, 2005)
paper
paper
(Fern et al., 2004)
(Fern et al., 2004)
(Hoffmann & Nebel, 2001)
(Bonet & Geffner, 2005)
(Onder et al., 2006)
(Teichteil-Konigsbuch & Fabiani, 2005)

Table 2: Competition Participants. Domain-specific planners starred
dom
prob
G2*
J1*
J2*
E*
J3
G1
R
P
C
Q

5
100
100
100
100
100

bw-c-nr
8
11
100 100
100 100
100 100
100 100
100 100

bw-nc-nr
8
100
100
100
100
100

bx-nr
5-10 10-10
100 100
100
100
100
67
100

expl-bw
11

hanoise
5-3

zeno
1-2-3-7

tire-nr
30-4

9



50
57


100
90
100
100
3

23
30
30
53
?
23

100

3

total
600
600
567
400
632
180
177
153
100
26

Table 3: Results Goal-Based Problems. Domain-specific planners starred. Entries
percentage runs goal reached. blank indicates
planner unable attempt problem. indicates planner
attempted problem never able achieve goal. ? indicates
result unavailable (due bug evaluation software, couple
results initially announced found invalid).
dom
prob
J1*
G2*
E*
J2*
J3
P
C
G1
R
Q

5
497
495
496
497
496

bw-c-r
8
11
487 481
486 480
492 486
486 482
487 482

5
494
495
495
495
494
494
495
495
494
180

8
489
490
490
490
490
488

bw-nc-r
11 15 18
21
480 470 462 458
480 468 352 286
480 468
481
466 397

455
459

bx-r
5-10 10-10 10-15
419
317
129
438 376

376
425
184


346



279

file
30-4

tire-r
30-4

36
58



?




11

total
5183
4846
2459
4229
4475
2087
495
495
494
191

Table 4: Results Reward-Based Problems. Domain-specific planners starred. Entries
average reward achieved 30 runs. blank indicates
planner unable attempt problem. indicates planner
attempted problem achieve strictly positive reward. ? indicates
result unavailable.
62

fiDecision-Theoretic Planning non-Markovian Rewards

Tables 3 4 show results competition, extracted competition overview paper (Younes, Littman, Weissmann, & Asmuth, 2005)
competition web site http://www.cs.rutgers.edu/~mlittman/topics/ipc04-pt/.
first tables concerns goal-based problems second reward-based problems. entries tables represent goal-achievement percentage average reward achieved various planner versions (left-column) various problems (top
two rows). Planners top part tables domain-specific. Problems
known domains lie left-hand side tables. colored blocks world problems
bw-c-nr (goal-based version) bw-c-r (reward version) 5, 8, 11 blocks.
non-colored blocks world problems bw-nc-nr (goal-based version) 8 blocks, bwnc-r (reward-based version) 5, 8, 11, 15, 18, 21 blocks. box world problems
bx-nr (goal-based) bx-r (reward-based), 5 10 cities 10 15 boxes. Problems unknown domains lie right hand side tables. comprise:
expl-bw, exploding version 11 block blocks world problem putting
block may destroy object put on, zeno, probabilistic variant zeno travel
domain problem IPC-3 1 plane, 2 persons, 3 cities 7 fuel levels, hanoise,
probabilistic variant tower hanoi problem 5 disks 3 rods, file, problem
putting 30 files 5 randomly chosen folders, tire, variant tire world problem
30 cities spare tires 4 them, tire may go flat driving.
planner nmrdpp G1 G2 version, able attempt problems, achieving strictly positive reward 4 them. even (J3), competition overall
winner, able successfully attempt many problems. nmrdpp performed particularly well goal-based problems, achieving goal 100% runs except expl-bw,
hanoise, tire-nr (note three problems, goal achievement probability
optimal policy exceed 65%). planner outperformed nmrdpp
scale. pointed before, behaves well probabilistic version blocks box
world optimal policies close deterministic problem
Hoffmann (2002) analyses reasons heuristic works well traditional planning benchmarks blocks world logistics. hand, unable
solve unknown problems different structure require substantial
probabilistic reasoning, although problems easily solved number participating planners. expected, large discrepancy version nmrdpp
allowed use search control (G2) domain-independent version (G1).
latter performs okay unknown goal-based domains, able solve
known ones. fact, except ff, none participating domain-independent
planners able solve problems.
reward-based case, nmrdpp control knoweldge behaves well known
problems. human-encoded policies (J1) performed better. Without control knowledge nmrdpp unable scale problems, participants
mgpt are. Furthermore nmrdpp appears perform poorly two unknown problems.
cases, might due fact fails generate optimal policy: suboptimal policies easily high negative score domains (see Younes et al., 2005).
r-tire, know nmrdpp indeed generate suboptimal policy. Additionally,
could nmrdpp unlucky sampling-based policy evaluation process:

63

fiThiebaux, Gretton, Slaney, Price & Kabanza

tire-r particular, high variance costs various trajectories
optimal policy.
Alltogether, competition results suggest control knowledge likely essential solving larger problems (Markovian not) nmrdpp, that,
observed deterministic planners, approaches making use control knowledge
quite powerful.

8. Conclusion, Related, Future Work
paper, examined problem solving decision processes nonMarkovian rewards. described existing approaches exploit compact representation reward function automatically translate NMRDP equivalent
process amenable MDP solution methods. computational model underlying
framework traced back work relationship linear temporal logic
automata areas automated verification model-checking (Vardi, 2003;
Wolper, 1987). remaining framework, proposed new representation
non-Markovian reward functions translation MDPs aimed making best
possible use state-based anytime heuristic search solution method. representation extends future linear temporal logic express rewards. translation
effect embedding model-checking solution method. results MDP
minimal size achievable without stepping outside anytime framework, consequently
better policies deadline. described nmrdpp, software platform
implements approaches common interface, proved useful tool
experimental analysis. system analysis first kind.
able identify number general trends behaviours methods
provide advice best suited certain circumstances. obvious
reasons, analysis focused artificial domains. Additional work examine
wider range domains practical interest, see form results take
context. Ultimately, would analysis help nmrdpp automatically select
appropriate method. Unfortunately, difficulty translating
PLTL $FLTL, likely nmrdpp would still maintain PLTL
$FLTL version reward formulae.
detailed comparison approach solving NMRDPs existing methods (Bacchus et al., 1996, 1997) found Sections 3.10 5. Two important aspects future
work would help take comparison further. One settle question appropriateness translation structured solution methods. Symbolic implementations
solution methods consider, e.g. symbolic LAO* (Feng & Hansen, 2002), well
formula progression context symbolic state representations (Pistore & Traverso,
2001) could investigated purpose. take advantage greater
expressive power $FLTL consider richer class decision processes, instance
uncertainty rewards received when. Many extensions language
possible: adding eventualities, unrestricted negation, first-class reward propositions,
quantitative time, etc. course, dealing via progression without backtracking
another matter.

64

fiDecision-Theoretic Planning non-Markovian Rewards

investigate precise relationship line work recent work
planning temporally extended goals non-deterministic domains. particular
interest weak temporally extended goals expressible Eagle language
(Dal Lago et al., 2002), temporally extended goals expressible -CTL* (Baral &
Zhao, 2004). Eagle enables expression attempted reachability maintenance goals
form try-reach p try-maintain p, add goals do-reach p
do-maintain p already expressible CTL. idea generated policy
make every attempt satisfying proposition p. Furthermore, Eagle includes recovery goals
form g1 fail g2 , meaning goal g2 must achieved whenever goal g1 fails,
cyclic goals form repeat g, meaning g achieved cyclically
fails. semantics goals given terms variants Buchi tree automata
preferred transitions. Dal Lago et al. (2002) present planning algorithm based
symbolic model-checking generates policies achieving goals. Baral Zhao
(2004) describe -CTL*, alternative framework expressing subset Eagle goals
variety others. -CTL* variant CTL* allows formulae involving
two types path quantifiers: quantifiers tied paths feasible generated
policy, usual, quantifiers generally tied paths feasible
domain actions. Baral Zhao (2004) present planning algorithm.
would interesting know whether Eagle -CTL* goals encoded nonMarkovian rewards framework. immediate consequence would nmrdpp
could used plan them. generally, would examine respective
merits non-deterministic planning temporally extended goals decision-theoretic
planning non-Markovian rewards.
pure probabilistic setting (no rewards), recent related research includes work
planning controller synthesis probabilistic temporally extended goals expressible
probabilistic temporal logics CSL PCTL (Younes & Simmons, 2004; Baier et al.,
2004). logics enable expressing statements probability policy satisfying given temporal goal exceeding given threshold. instance, Younes Simmons
(2004) describe general probabilistic planning framework, involving concurrency, continuous time, temporally extended goals, rich enough model generalised semi-Markov
processes. solution algorithms directly comparable presented here.
Another exciting future work area investigation temporal logic formalisms
specifying heuristic functions NMRDPs generally search problems
temporally extended goals. Good heuristics important solution methods
targeting, surely value ought depend history. methods
described could applicable description processing heuristics. Related
problem extending search control knowledge fully operate
presence temporally extended goals, rewards, stochastic actions. first issue
branching probabilistic logics CTL PCTL variants preferred
FLTL describing search control knowledge, stochastic actions
involved, search control often needs refer possible futures even
probabilities.18 Another major problem GOALP modality,
key specification reusable search control knowledge interpreted respect
18. would argue, hand, CTL necessary representing non-Markovian rewards.

65

fiThiebaux, Gretton, Slaney, Price & Kabanza

fixed reachability goal19 (Bacchus & Kabanza, 2000), such, applicable
domains temporally extended goals, let alone rewards. Kabanza Thiebaux (2005)
present first approach search control presence temporally extended goals
deterministic domains, much remains done system nmrdpp able
support meaningful extension GOALP.
Finally, let us mention related work area databases uses similar approach
pltlstr extend database auxiliary relations containing sufficient information
check temporal integrity constraints (Chomicki, 1995). issues somewhat different
raised NMRDPs: ever one sequence databases, matters
size auxiliary relations avoiding making redundant distinctions.

Acknowledgements
Many thanks Fahiem Bacchus, Rajeev Gore, Marco Pistore, Ron van der Meyden, Moshe
Vardi, Lenore Zuck useful discussions comments, well anonymous
reviewers David Smith thorough reading paper excellent
suggestions. Sylvie Thiebaux, Charles Gretton, John Slaney, David Price thank National ICT Australia support. NICTA funded Australian Governments
Backing Australias Ability initiative, part Australian Research Council. Froduald Kabanza supported Canadian Natural Sciences Engineering Research
Council (NSERC).

Appendix A. Class Reward-Normal Formulae
existing decision procedure (Slaney, 2005) determining whether formula rewardnormal guaranteed terminate finitely, involves construction comparison
automata rather intricate practice. therefore useful give simple syntactic
characterisation set constructors obtaining reward-normal formulae even though
formulae constructible.
say formula material iff contains $ temporal operators
is, material formulae boolean combinations atoms.
consider four operations behaviours representable formulae $FLTL. Firstly,
behaviour may delayed specified number timesteps. Secondly, may made
conditional material trigger. Thirdly, may started repeatedly material
termination condition met. Fourthly, two behaviours may combined form
union. operations easily realised syntactically corresponding operations
formulae. material formula:
delay[f ] =

f

cond[m, f ] = f
loop[m, f ] = f U
union[f1 , f2 ] = f1 f2
19. f atemporal formula, GOALP(f ) true iff f true goal states.

66

fiDecision-Theoretic Planning non-Markovian Rewards

shown (Slaney, 2005) set reward-normal formulae closed delay,
cond (for material m), loop (for material m) union, closure
{$} operations represents class behaviours closed intersection
concatenation well union.
Many familiar reward-normal formulae obtainable $ applying four operations. example, (p $) loop[, cond[p, $]]. Sometimes paraphrase necessary.
example, ((p q) $) required form antecedent
conditional, equivalent (p (q $)) loop[, cond[p, delay[cond[q, $]]]].
cases easy. example formula p U (p$) stipulates reward
first time p happens form suggested. capture
behaviour using operations requires formula (p $) ( (p $) U p).

Appendix B. Proofs Theorems
Property 1 b ((i) B), (, i) |=B f iff (, + 1) |=B Prog(b, , f ).
Proof:
Induction structure f . several base cases, fairly trivial.
f = > f = nothing prove, progress hold
everywhere nowhere respectively. f = p f holds progresses >
holds i+1 f hold progresses
hold i+1 . case f = p similar. last base case, f = $. following
equivalent:
(, i) |=B f
(i) B
b
Prog(b, , f ) = >
(, + 1) |=B Prog(b, , f )
Induction case 1: f = g h. following equivalent:
(, i) |=B f
(, i) |=B g (, i) |=B h
(, + 1) |=B Prog(b, , g) (, + 1) |=B Prog(b, , h) (by induction hypothesis)
(, + 1) |=B Prog(b, , g) Prog(b, , h)
(, + 1) |=B Prog(b, , f )
Induction case 2: f = g h. Analogous case 1.
Induction case 3: f = g. Trivial inspection definitions.
Induction case 4: f = g U h. f logically equivalent h (g (g U h)
cases 1, 2 3 holds stage behaviour B iff Prog(b, , f ) holds stage i+1.

Theorem 1 Let f reward-normal, let hf0 , f1 , . . .i result progressing
successive states sequence . Then, provided , Rew(i , )
iff (i) Bf .

67

fiThiebaux, Gretton, Slaney, Price & Kabanza

Proof: First, definition reward-normality, f reward-normal |=B f iff
i, (i) Bf (i) B. Next, |=B f progressing f according
B (that is, letting bi true iff (i) B) cannot lead contradiction
Property 1, progression truth-preserving.
remains, then, show 6|=B f progressing f according B
must lead eventually . proof induction structure f
usual base case f literal (an atom, negated atom >, $) trivial.
Case f = g h. Suppose 6|=B f . either 6|=B g 6|=B h, induction
hypothesis either g h progresses eventually , hence conjunction.
Case f = g h. Suppose 6|=B f . 6|=B g 6|=B h, induction
hypothesis g h progresses eventually . Suppose without loss generality
g progress h does. point g progressed
formula g 0 f progressed g 0 simplifies g 0 . Since g 0 progresses
eventually, f .
Case f = g. Suppose 6|=B f . Let = 0 ; let B 0 = {|0 ; B}.
6|=B 0 g, induction hypothesis g progressed according B 0 eventually
reaches . progression f according B exactly
first step, leads .
Case f = g U h. Suppose 6|=B f . j (, j) 6|=B g
j, (, i) 6|=B h. proceed induction j. base case j = 0, 6|=B g
6|=B h whence main induction hypothesis g h eventually progress
. Thus h (g f 0 ) progresses eventually f 0 , particular f 0 = f ,
establishing base case. induction case, suppose |=B g (and course 6|=B h).
Since f equivalent h (g f ) 6|=B f , 6|=B h |=B g, clearly 6|=B f .
B 0 previous case, therefore, 6|=B 0 f failure occurs stage j 1
. Therefore hypothesis induction j applies, f progressed
according B 0 goes eventually , f progressed according B goes
similarly .

Theorem 3 Let 0 set e-states equivalent MDP D0 = hS, s0 , A, Pr, Ri.
D0 minimal iff every e-state 0 reachable 0 contains two distinct e-states s01
s02 (s01 ) = (s02 ) (s01 ) = (s02 ).
Proof: Proof construction canonical equivalent MDP Dc . Let set
e 0 ) partitioned equivalence classes,
finite prefixes state sequences D(s
e 0 ), R(1(i); ) =
1(i) 2(j) iff 1i = 2j 1(i); D(s
R(2(j); ). Let [(i)] denote equivalence class (i). Let E set
equivalence classes. Let function takes [(i)] E A(i ).
(i) (j) A([(i)]), let ([(i)], a, [(j)]) Pr(i , a, s) [(j)] =
[(i); hsi]. Otherwise let ([(i)], a, [(j)]) = 0. Let R([(i)]) R((i)). note
following four facts:
1. functions A, R well-defined.
2. Dc = hE, [hs0 i], A, , Ri equivalent MDP ([(i)]) = .

68

fiDecision-Theoretic Planning non-Markovian Rewards

3. equivalent MDP D00 mapping subset states
D00 onto E.
4. D0 satisfies condition every e-state 0 reachable 0 contains two
distinct e-states s01 s02 (s01 ) = (s02 ) (s01 ) = (s02 ) iff Dc isomorphic
D0 .
fact 1 amounts 1(i) 2(j) matter
two sequences used define A, R equivalence class. cases
simply 1i = 2j . case R, special case = h1i
equality rewards extensions.
Fact 2 matter checking four conditions Definition 1 hold. these,
conditions 1 ( ([s0 ]) = s0 ) 2 (A([(i)]) = A(i )) hold trivially construction.
e 0 ), R([(i)]) = R((i))
Condition 4 says feasible state sequence D(s
i. given construction. Condition 3 states:
s1 , s2 S, A(s1 ) Pr(s1 , a, s2 ) > 0,
e 0 ) = s1 , exists unique [(j)] E, j = s2 ,
(i) D(s
A([(i)]), ([(i)], a, [[j]]) = Pr(s1 , a, s2 ).
e 0 ) = s1 . required (j) (i); hs2 i,
Suppose Pr(s1 , , s2 ) > 0, (i) D(s
course A([(i)]) = A(i ), required condition reads:
[(i); hs2 i] unique element X E (X) = s2
A(i ), ([(i)], a, X) = Pr(s1 , a, s2 ).
establish existence, need A(i ) ([(i)], a, [(i); hs2 i]) = Pr(i , a, s2 ),
immediate definition above. establish uniqueness, suppose
(X) = s2 ([(i)], a, X) = Pr(s1 , a, s2 ) actions A(i ). Since Pr(s1 , , s2 ) >
0, transition probability [(i)] X nonzero action, definition
, X [(i); hs2 i].
Fact 3 readily observed. Let equivalent MDP D. states s1
s2 D, state X (X) = s1 one state
(Y ) = s2 action A(s1 ) gives nonzero probability
transition X . follows uniqueness part condition 3 Definition 1
together fact transition function probability distribution (sums 1).
Therefore given finite state sequence (i) one state reached
start state following (i). Therefore induces equivalence relation
: (i) (j) iff lead state (the sequences
feasible may regarded equivalent ). reachable state
associated nonempty equivalence class finite sequences states D. Working
definitions, may observe sub-relation (if (i) (j)
(i) (j)). Hence function takes equivalence class
feasible sequence (i) [(i)] induces mapping h (an epimorphism fact)
reachable subset states onto E.
establish Fact 4, must shown case D0 mapping
reversed, equivalence class [(i)] Dc corresponds exactly one element
69

fiThiebaux, Gretton, Slaney, Price & Kabanza

e 0)
D0 . Suppose (for contradiction). exist sequences 1(i) 2(j) D(s
0
1(i) 2(j) following two sequences s0 arrive two different
elements s01 s02 D0 (s01 ) = 1i = 2j = (s02 ) (s01 ) 6= (s02 ). Therefore
e
exists sequence (k) D(s)
R(1(i 1); (k)) 6= R(2(j 1); (k)).
contradicts condition 1(i) 2(j).

Theorem 3 follows immediately facts 14.
Theorem 4 Let D0 translation Definition 5. D0 blind minimal
equivalent MDP D.
Proof: Reachability e-states obvious, constructed
reached. e-state pair hs, state reward function
specification. fact, = (hs, i) determines distribution rewards
continuations sequences
reach hs, i. is, 0 = s,
P
reward (f :r) {r | Bf }. D0 blind minimal, exist
distinct e-states hs, hs, 0 sum . makes
0 semantically equivalent, contradicting supposition distinct.


Appendix C. Random Problem Domains
Random problem domains produced first creating random action specification
defining domain dynamics. experiments conducted20 involved
producing, second step, random reward specification desired properties
relation generated dynamics.
random generation domain dynamics takes parameters number n
propositions domain number actions produced, starts
assigning effects action proposition affected exactly one
action. example, 5 actions 14 propositions, first 4 actions may affect
3 propositions each, 5th one 2, affected propositions different.
action initial effects, continue add effects one time,
sufficient proportion state space reachable see proportion reachable parameter
below. additional effect generated picking random action random
proposition, producing random decision diagram according uncertainty
structure parameters below:
Uncertainty parameter probability non zero/one value leaf node.
uncertainty 1 result leaf nodes random values uniform
distribution. uncertainty 0 result leaf nodes values 0 1
equal probability.
Structure (or influence) parameter probability decision diagram containing
particular proposition. influence 1 result decision diagrams
20. None included paper, however.

70

fiDecision-Theoretic Planning non-Markovian Rewards

including propositions (and unlikely significant structure), 0
result decision diagrams depend values propositions.
Proportion Reachable parameter lower bound proportion entire 2n
state space reachable start state. algorithm adds behaviour
lower bound reached. value 1 result algorithm running
actions sufficient allow entire state space reachable.
reward specification produced regard generated dynamics
specified number rewards reachable specified number unreachable.
First, decision diagram produced represent states reachable
not, given domain dynamics. Next, random path taken root
decision diagram true terminal generating attainable reward, false
terminal producing unattainable reward. propositions encountered
path, negated not, form conjunction reward formula. process
repeated desired number reachable unreachable rewards obtained.

References
AT&T Labs-Research (2000). Graphviz. Available http://www.research.att.com/
sw/tools/graphviz/.
Bacchus, F., Boutilier, C., & Grove, A. (1996). Rewarding behaviors. Proc. American
National Conference Artificial Intelligence (AAAI), pp. 11601167.
Bacchus, F., Boutilier, C., & Grove, A. (1997). Structured solution methods nonMarkovian decision processes. Proc. American National Conference Artificial
Intelligence (AAAI), pp. 112117.
Bacchus, F., & Kabanza, F. (1998). Planning temporally extended goals. Annals
Mathematics Artificial Intelligence, 22, 527.
Bacchus, F., & Kabanza, F. (2000). Using temporal logic express search control knowledge
planning. Artificial Intelligence, 116 (1-2).
Baier, C., Groer, M., Leucker, M., Bollig, B., & Ciesinski, F. (2004). Controller synthesis
probabilistic systems (extended abstract). Proc. IFIP International Conference
Theoretical Computer Science (IFIP TCS).
Baral, C., & Zhao, J. (2004). Goal specification presence nondeterministic actions.
Proc. European Conference Artificial Intelligence (ECAI), pp. 273277.
Barto, A., Bardtke, S., & Singh, S. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72, 81138.
Bonet, B., & Geffner, H. (2003). Labeled RTDP: Improving convergence real-time
dynamic programming. Proc. International Conference Automated Planning
Scheduling (ICAPS), pp. 1221.

71

fiThiebaux, Gretton, Slaney, Price & Kabanza

Bonet, B., & Geffner, H. (2005). mGPT: probabilistic planner based heuristic search.
Journal Artificial Intelligence Research, 24, 933944.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
Vol. 11, pp. 194.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic dynamic programming
factored representations. Artificial Intelligence, 121 (1-2), 49107.
Calvanese, D., De Giacomo, G., & Vardi, M. (2002). Reasoning actions planning LTL action theories. Proc. International Conference Principles
Knowledge Representation Reasoning (KR), pp. 493602.
Cesta, A., Bahadori, S., G, C., Grisetti, G., Giuliani, M., Loochi, L., Leone, G., Nardi, D.,
Oddi, A., Pecora, F., Rasconi, R., Saggase, A., & Scopelliti, M. (2003). RoboCare
project. Cognitive systems care elderly. Proc. International Conference
Aging, Disability Independence (ICADI).
Chomicki, J. (1995). Efficient checking temporal integrity constraints using bounded
history encoding. ACM Transactions Database Systems, 20 (2), 149186.
Dal Lago, U., Pistore, M., & Traverso, P. (2002). Planning language extended
goals. Proc. American National Conference Artificial Intelligence (AAAI), pp.
447454.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning time constraints stochastic domains. Artificial Intelligence, 76, 3574.
Dean, T., & Kanazawa, K. (1989). model reasoning persistance causation.
Computational Intelligence, 5, 142150.
Drummond, M. (1989). Situated control rules. Proc. International Conference
Principles Knowledge Representation Reasoning (KR), pp. 103113.
Emerson, E. A. (1990). Temporal modal logic. Handbook Theoretical Computer
Science, Vol. B, pp. 9971072. Elsevier MIT Press.
Feng, Z., & Hansen, E. (2002). Symbolic LAO search factored Markov decision processes. Proc. American National Conference Artificial Intelligence (AAAI), pp.
455460.
Feng, Z., Hansen, E., & Zilberstein, S. (2003). Symbolic generalization on-line planning.
Proc. Conference Uncertainty Artificial Intelligence (UAI), pp. 209216.
Fern, A., Yoon, S., & Givan, R. (2004). Learning domain-specific knowledge random
walks. Proc. International Conference Automated Planning Scheduling
(ICAPS), pp. 191198.
Fourman, M. (2000). Propositional planning. Proc. AIPS Workshop Model-Theoretic
Approaches Planning, pp. 1017.
72

fiDecision-Theoretic Planning non-Markovian Rewards

Gretton, C., Price, D., & Thiebaux, S. (2003a). Implementation comparison solution
methods decision processes non-Markovian rewards. Proc. Conference
Uncertainty Artificial Intelligence (UAI), pp. 289296.
Gretton, C., Price, D., & Thiebaux, S. (2003b). NMRDPP: system decision-theoretic
planning non-Markovian rewards. Proc. ICAPS Workshop Planning
Uncertainty Incomplete Information, pp. 4856.
Haddawy, P., & Hanks, S. (1992). Representations decision-theoretic planning: Utility
functions deadline goals. Proc. International Conference Principles
Knowledge Representation Reasoning (KR), pp. 7182.
Hansen, E., & Zilberstein, S. (2001). LAO : heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: stochastic planning using
decision diagrams. Proc. Conference Uncertainty Artificial Intelligence (UAI),
pp. 279288.
Hoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.
Proc. International Conference AI Planning Scheduling (AIPS), pp. 92100.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Howard, R. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge,
MA.
Kabanza, F., & Thiebaux, S. (2005). Search control planning temporally extended
goals. Proc. International Conference Automated Planning Scheduling
(ICAPS), pp. 130139.
Karabaev, E., & Skvortsova, O. (2005). Heuristic Search Algorithm Solving FirstOrder MDPs. Proc. Conference Uncertainty Artificial Intelligence (UAI),
pp. 292299.
Koehler, J., & Schuster, K. (2000). Elevator control planning problem. Proc.
International Conference AI Planning Scheduling (AIPS), pp. 331338.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.
Artificial Intelligence, 76, 239286.
Lichtenstein, O., Pnueli, A., & Zuck, L. (1985). glory past. Proc. Conference
Logics Programs, pp. 196218. LNCS, volume 193.
Onder, N., Whelan, G. C., & Li, L. (2006). Engineering conformant probabilistic planner.
Journal Artificial Intelligence Research, 25, 115.

73

fiThiebaux, Gretton, Slaney, Price & Kabanza

Pistore, M., & Traverso, P. (2001). Planning model-checking extended goals
non-deterministic domains. Proc. International Joint Conference Artificial Intelligence (IJCAI-01), pp. 479484.
Slaney, J. (2005). Semi-positive LTL uninterpreted past operator. Logic Journal
IGPL, 13, 211229.
Slaney, J., & Thiebaux, S. (2001). Blocks world revisited. Artificial Intelligence, 125,
119153.
Somenzi, F. (2001).
CUDD: CU Decision Diagram Package.
ftp://vlsi.colorado.edu/pub/.

Available

Teichteil-Konigsbuch, F., & Fabiani, P. (2005). Symbolic heuristic policy iteration algorithms structured decision-theoretic exploration problems. Proc. ICAPS workshop Planning Uncertainty Autonomous Systems.
Thiebaux, S., Hertzberg, J., Shoaff, W., & Schneider, M. (1995). stochastic model
actions plans anytime planning uncertainty. International Journal
Intelligent Systems, 10 (2), 155183.
Thiebaux, S., Kabanza, F., & Slaney, J. (2002a). Anytime state-based solution methods
decision processes non-Markovian rewards. Proc. Conference Uncertainty
Artificial Intelligence (UAI), pp. 501510.
Thiebaux, S., Kabanza, F., & Slaney, J. (2002b). model-checking approach decisiontheoretic planning non-Markovian rewards. Proc. ECAI Workshop ModelChecking Artificial Intelligence (MoChArt-02), pp. 101108.
Vardi, M. (2003). Automated verification = graph, logic, automata. Proc. International Joint Conference Artificial Intelligence (IJCAI), pp. 603606. Invited
paper.
Wolper, P. (1987). relation programs computations models temporal
logic. Proc. Temporal Logic Specification, LNCS 398, pp. 75123.
Younes, H. L. S., & Littman, M. (2004). PPDDL1.0: extension PDDL expressing
planning domains probabilistic effects. Tech. rep. CMU-CS-04-167, School
Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania.
Younes, H. L. S., Littman, M., Weissmann, D., & Asmuth, J. (2005). first probabilistic
track International Planning Competition. Journal Artificial Intelligence
Research, Vol. 24, pp. 851887.
Younes, H., & Simmons, R. G. (2004). Policy generation continuous-time stochastic
domains concurrency. Proc. International Conference Automated Planning
Scheduling (ICAPS), pp. 325333.

74


