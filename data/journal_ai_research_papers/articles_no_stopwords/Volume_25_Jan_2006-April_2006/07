Journal Artificial Intelligence Research 25 (2006) 425456

Submitted 12/04; published 4/06

Logical Hidden Markov Models
Kristian Kersting
Luc De Raedt

kersting@informatik.uni-freiburg.de
deraedt@informatik.uni-freiburg.de

Institute Computer Science
Albert-Ludwigs-Universitat Freiburg
Georges-Koehler-Allee 079
D-79110 Freiburg, Germany

Tapani Raiko

tapani.raiko@hut.fi

Laboratory Computer Information Science
Helsinki University Technology
P.O. Box 5400
FIN-02015 HUT, Finland

Abstract
Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov models
deal sequences structured symbols form logical atoms, rather flat
characters.
note formally introduces LOHMMs presents solutions three central inference problems LOHMMs: evaluation, likely hidden state sequence parameter estimation. resulting representation algorithms experimentally evaluated
problems domain bioinformatics.

1. Introduction
Hidden Markov models (HMMs) (Rabiner & Juang, 1986) extremely popular analyzing sequential data. Application areas include computational biology, user modelling,
speech recognition, empirical natural language processing, robotics. Despite successes, HMMs major weakness: handle sequences flat, i.e., unstructured symbols. Yet, many applications symbols occurring sequences structured. Consider, e.g., sequences UNIX commands, may parameters
emacs lohmms.tex, ls, latex lohmms.tex, . . .Thus, commands essentially structured.
Tasks considered UNIX command sequences include prediction
next command sequence (Davison & Hirsh, 1998), classification command
sequence user category (Korvemaker & Greiner, 2000; Jacobs & Blockeel, 2001),
anomaly detection (Lane, 1999). Traditional HMMs cannot easily deal type
structured sequences. Indeed, applying HMMs requires either 1) ignoring structure
commands (i.e., parameters), 2) taking possible parameters explicitly
account. former approach results serious information loss; latter leads
combinatorial explosion number symbols parameters HMM
consequence inhibits generalization.
sketched problem HMMs akin problem dealing structured examples traditional machine learning algorithms studied fields inductive logic programming (Muggleton & De Raedt, 1994) multi-relational learnc
2006
AI Access Foundation. rights reserved.

fiKersting, De Raedt, & Raiko

ing (Dzeroski & Lavrac, 2001). paper, propose (inductive) logic programming
framework, Logical HMMs (LOHMMs), upgrades HMMs deal structure.
key idea underlying LOHMMs employ logical atoms structured (output state)
symbols. Using logical atoms, UNIX command sequence represented
emacs(lohmms.tex), ls, latex(lohmms.tex), . . . two important motivations
using logical atoms symbol level. First, variables atoms allow one make
abstraction specific symbols. E.g., logical atom emacs(X, tex) represents files X
LATEX user tex could edit using emacs. Second, unification allows one share information among states. E.g., sequence emacs(X, tex), latex(X, tex) denotes
file used argument Emacs LATEX.
paper organized follows. reviewing logical preliminaries, introduce
LOHMMs define semantics Section 3; Section 4, upgrade basic
HMM inference algorithms use LOHMMs; investigate benefits LOHMMs
Section 5: show LOHMMs strictly expressive HMMs,
design order magnitude smaller corresponding propositional
instantiations, unification yield models, better fit data. Section 6,
empirically investigate benefits LOHMMs real world data. concluding,
discuss related work Section 7. Proofs theorems found Appendix.

2. Logical Preliminaries
first-order alphabet set relation symbols r arity 0, written r/m,
set functor symbols f arity n 0, written f/n. n = 0 f called constant,
= 0 p called propositional variable. (We assume least one constant
given.) atom r(t1 , . . . , tn ) relation symbol r followed bracketed n-tuple
terms ti . term variable V functor symbol f(t1 , . . . , tk ) immediately followed
bracketed k-tuple terms ti . Variables written upper-case, constant, functor predicate symbols lower-case. symbol denote anonymous variables
read treated distinct, new variables time encountered. iterative
clause formula form H B H (called head) B (called body) logical
atoms. substitution = {V1 /t1 , . . . , Vn /tn }, e.g. {X/tex}, assignment terms ti
variables Vi . Applying substitution term, atom clause e yields instantiated term, atom, clause e occurrences variables V simultaneously
replaced term ti , e.g. ls(X) emacs(F, X){X/tex} yields ls(tex) emacs(F, tex).
substitution called unifier finite set atoms singleton. unifier
called general unifier (MGU) if, unifier S, exists
substitution = . term, atom clause E called ground contains
variables, i.e., vars(E) = . Herbrand base , denoted hb , set
ground atoms constructed predicate functor symbols . set G (A)
atom consists ground atoms belong hb .

3. Logical Hidden Markov Models
logical component traditional HMM corresponds Mealy machine (Hopcroft
& Ullman, 1979), i.e., finite state machine output symbols associated
426

fiLogical Hidden Markov Models

transitions. essentially propositional representation symbols used
represent states output symbols flat, i.e. structured. key idea underlying
LOHMMs replace flat symbols abstract symbols. abstract symbol
definition logical atom. abstract represents set ground, i.e.,
variable-free atoms alphabet , denoted G (A). Ground atoms play
role traditional symbols used HMMs.
Example 1 Consider alphabet 1 constant symbols tex, dvi, hmm1,
lohmm1, relation symbols emacs/2, ls/1, xdvi/1, latex/2. atom
emacs(File, tex) represents set {emacs(hmm1, tex), emacs(lohmm1, tex)}. assume
alphabet typed avoid useless instantiations emacs(tex, tex)).
use atoms instead flat symbols allows us analyze logical structured sequences
emacs(hmm1, tex), latex(hmm1, tex), xdvi(hmm1, dvi).


Definition 1 Abstract transition expressions form p : H
B p [0, 1],
H, B atoms. variables implicitly assumed universally quantified,
i.e., scope variables single abstract transition.
atoms H B represent abstract states represents abstract output symbol.

semantics abstract transition p : H
B one one states
G (B), say BB , one go probability p one states G (HB ), say HB H ,
emitting symbol G (OB H ), say OB H .
latex(File)

Example 2 Consider c 0.8 : xdvi(File, dvi) latex(File, tex). general
H, B share predicate. due nature running example. Assume state latex(hmm1, tex), i.e.
B = {File/hmm1}. c specifies probability 0.8 next state
G1 (xdvi(hmm1, dvi)) = {xdvi(hmm1, dvi)} ( i.e., probability 0.8
next state xdvi(hmm1, dvi)), one symbols G 1 (latex(hmm1)) =
{latex(hmm1)} ( i.e., latex(hmm1)) emitted. Abstract states might
complex latex(file(FileStem, FileExtension), User)
example simple H empty. situation becomes complicated substitutions empty. Then, resulting
state output symbol sets necessarily singletons. Indeed, transilatex(File)

tion 0.8 : emacs(File0 , dvi) latex(File, tex) resulting state set would
G1 (emacs(File0 , dvi)) = {emacs(hmm1, tex), emacs(lohmm1, tex)}. Thus transition
non-deterministic two possible resulting states. therefore need
mechanism assign probabilities possible alternatives.
Definition 2 selection distribution specifies abstract state observation
symbol alphabet distribution ( | A) G (A).
continue example, let (emacs(hmm1, tex) | emacs(File0 , tex)) = 0.4
(emacs(lohmm1, tex) | emacs(File0 , tex)) = 0.6. would probability 0.4 0.8 = 0.32 next state emacs(hmm1, tex) 0.48
emacs(lohmm1, tex).
427

fiKersting, De Raedt, & Raiko



Taking account, meaning abstract transition p : H
B summarized follows. Let BB G (B), HB H G (HB ) OB H G (OB H ).
model makes transition state BB HB H emits symbol OB H probability
p (HB H | HB ) (OB H | OB H ).

(1)

represent , probabilistic representation - principle - used, e.g. Bayesian
network Markov chain. Throughout remainder present paper, however,
use nave Bayes approach. precisely, associate argument
r/m
r/m
relation r/m finite domain Di
constants probability distribution Pi

r/m
Di . Let vars(A) = {V1 , . . . , Vl } variables occurring atom r/m,
let = {V1 /s1 , . . . Vl /sl } substitution grounding A. Vj considered
r/m
random variable domain Darg(Vj ) argument arg(Vj ) appears first in. Then,
Q
r/m
(A | A) = lj=1 Parg(Vj ) (sj ). E.g. (emacs(hmm1, tex) | emacs(F, E)), computed
emacs/2

emacs/2

product P1
(hmm1) P2
(tex).
Thus far semantics single abstract transition defined. LOHMM
usually consists multiple abstract transitions creates complication.
Example 3 Consider

emacs(File)

0.8 : latex(File, tex) emacs(File, tex)



emacs(File)

0.4 : dvi(File) emacs(File, User).
two abstract transitions make
conflicting statements state resulting emacs(hmm1, tex). Indeed, according
first transition, probability 0.8 resulting state latex(hmm1, tex)
according second one assigns 0.4 xdvi(hmm1).
essentially two ways deal situation. one hand, one might want
combine normalize two transitions assign probability 23 respectively 13 .
hand, one might want one rule firing. paper, chose
latter option allows us consider transitions independently, simplifies
learning, yields locally interpretable models. employ subsumption (or generality) relation among B-parts two abstract transitions. Indeed, B-part
first transition B1 = emacs(File, tex) specific second transition B2 = emacs(File, User) exists substitution = {User/tex}
B2 = B1 , i.e., B2 subsumes B1 . Therefore G1 (B1 ) G1 (B2 ) first transition
regarded informative second one. therefore preferred
second one starting emacs(hmm1, tex). say first transition specific second one. Remark generality relation imposes
partial order set transitions. considerations lead strategy
considering maximally specific transitions apply state order determine
successor states. implements kind exception handling default reasoning
akin Katzs (1987) back-off n-gram models. back-off n-gram models,
detailed model deemed provide sufficiently reliable information current
context used. is, one encounters n-gram sufficiently reliable,
back-off use (n 1)-gram; reliable either back-off level n 2, etc.
conflict resolution strategy work properly provided bodies maximally specific transitions (matching given state) represent abstract state.
428

fiLogical Hidden Markov Models

start

ls : 0.4

0.45

0.55
emacs(F) : 0.7
ls(U0 )

emacs(F, U)
emacs(F) : 0.3

ls : 0.6
emacs(F) : 0.3

emacs(F0 , U)

latex(F) : 0.2
emacs(F) : 0.1

latex(F) : 0.2
latex(F, tex)

emacs(F, tex)
emacs(F) : 0.6

latex(F) : 0.6

Figure 1: logical hidden Markov model.

enforced requiring generality relation B-parts closed
greatest lower bound (glb) predicate, i.e., pair B1 , B2 bodies,
= mgu(B1 , B2 ) exists, another body B (called lower bound) subsumes B1
(therefore B2 ) subsumed B1 , B2 , lower bound
subsumed B. E.g., body second abstract transition example
emacs(hmm1, User) set abstract transitions would closed glb.
Finally, order specify prior distribution states, assume finite set
clauses form p : H start using distinguished start symbol p
probability LOHMM start state G (H).
able formally define logical hidden Markov models.
Definition 3 logical hidden Markov model (LOHMM) tuple (, , , )
logical alphabet, selection probability , set abstract transitions,
set abstract transitions encoding prior distribution. Let B set atoms
occur body parts transitions . assume B closed glb require
X
B B :
p = 1.0
(2)

p:H
B

probabilities p clauses sum 1.0 .

HMMs special cases LOHMMs contains relation symbols arity
zero selection probability irrelevant. Thus, LOHMMs directly generalize HMMs.
LOHMMs represented graphically. Figure 1 contains example. underlying language 2 consists 1 together constant symbol denotes
user employ LATEX. graphical notation, nodes represent abstract states
black tipped arrows denote abstract transitions. White tipped arrows used represent meta knowledge. precisely, white tipped, dashed arrows represent generality
subsumption ordering abstract states. follow transition abstract state
outgoing white tipped, dotted arrow dotted arrow always followed.
Dotted arrows needed abstract state occur different cirlatex(File)
cumstances. Consider transition p : latex(File0 , User0 ) latex(File, User).
429

fiKersting, De Raedt, & Raiko

0.6

start

0.45

em(F, U)



em(F, t)

state

abstract state

abstract state
0.4

ls


ls(t)

em(f1 )

em(f1 , t)

1.0

la(F, t)

la(f1 , t)

abstract state

state

ls(U0 )

em(f2 )

state abstract state

0.6

la(f1 )

0.7

em(f2 , o)
state



em(F, U)
abstract state

em(F0 , U)
abstract state

Figure 2: Generating

observation
sequence
emacs(hmm1), latex(hmm1),
emacs(lohmm1), ls LOHMM Figure 1. command emacs
abbreviated em, f1 denotes filename hmm1, f2 represents lohmm1, denotes
tex user, user. White tipped solid arrows indicate selections.

Even though atoms head body transition syntactically different
represent abstract state. accurately represent meaning transition
cannot use black tipped arrow latex(File, User) itself, would actulatex(File)

ally represent abstract transition p : latex(File, User) latex(File, User).
Furthermore, graphical representation clarifies LOHMMs generative models. Let us explain model Figure 1 would generate observation sequence
emacs(hmm1), latex(hmm1), emacs(lohmm1), ls (cf. Figure 2). chooses initial abstract state, say emacs(F, U). Since variables F U uninstantiated, model
samples state emacs(hmm1, tex) G2 using . indicated dashed arrow, emacs(F, tex) specific emacs(F, U). Moreover, emacs(hmm1, tex) matches
emacs(F, tex). Thus, model enters emacs(F, tex). Since value F already
instantiated previous abstract state, emacs(hmm1, tex) sampled probability
1.0. Now, model goes latex(F, tex), emitting emacs(hmm1) abstract
observation emacs(F) already fully instantiated. Again, since F already instantiated,
latex(hmm1, tex) sampled probability 1.0. Next, move emacs(F 0 , U), emitting latex(hmm1). Variables F0 U emacs(F0 , U) yet bound; so, values, say
lohmm1 others, sampled . dotted arrow brings us back emacs(F, U).
variables implicitly universally quantified abstract transitions, scope
variables restricted single abstract transitions. turn, F treated distinct,
new variable, automatically unified F0 , bound lohmm1. contrast,
variable U already instantiated. Emitting emacs(lohmm1), model makes transition
ls(U0 ). Assume samples tex U0 . Then, remains ls(U0 ) probability
0.4 . Considering possible samples, allows one prove following theorem.
Theorem 1 (Semantics) logical hidden Markov model language defines
discrete time stochastic process, i.e., sequence random variables hX it=1,2,... ,
domain
Xt hb() hb(). induced probability measure Cartesian product
N
hb() hb() exists unique > 0 limit .
concluding section, let us address design choices underlying LOHMMs.
First, LOHMMs introduced Mealy machines, i.e., output symbols
associated transitions. Mealy machines fit logical setting quite intuitively
directly encode conditional probability P (O, S0 |S) making transition S0
430

fiLogical Hidden Markov Models

emitting observation O. Logical hidden Markov models define distribution
X
P (O, S0 |S) =
p (S0 | HB ) (O | O0 B H )
O0
p:HB
O0

sum runs abstract transitions H B B specific S.
Observations correspond (partially) observed proof steps and, hence, provide information
shared among heads bodies abstract transitions. contrast, HMMs usually
introduced Moore machines. Here, output symbols associated states implicitly
assuming S0 independent. Thus, P (O, S0 | S) factorizes P (O | S) P (S0 | S).
makes difficult observe information shared among heads bodies.
turn, Moore-LOHMMs less intuitive harder understand. detailed
discussion issue, refer Appendix B essentially show
propositional case Mealy- Moore-LOHMMs equivalent.
Second, nave Bayes approach selection distribution reduces model complexity expense lower expressivity: functors neglected variables
treated independently. Adapting expressive approaches interesting future line
research. instance, Bayesian networks allow one represent factorial HMMs (Ghahramani & Jordan, 1997). Factorial HMMs viewed LOHMMs, hidden
states summarized 2 k-ary abstract state. first k arguments encode k
state variables, last k arguments serve memory previous joint state.
i-th argument conditioned + k-th argument. Markov chains allow one
sample compound terms finite depth s(s(s(0))) model e.g. misspelled
filenames. akin generalized HMMs (Kulp, Haussler, Reese, & Eeckman, 1996),
node may output finite sequence symbols rather single symbol.
Finally, LOHMMs introduced present paper specify probability distribution sequences given length. Reconsider LOHMM Figure 1. Already probabilities observation sequences length 1, i.e., ls, emacs(hmm1),

P
emacs(lohmm1)) sum 1. precisely, > 0 holds x1 ,...,xt P (X1 =
x1 , . .P
. , Xt P
= xt ) = 1.0 . order model distribution sequences variable length,
i.e., t>0 x1 ,...,xt P (X1 = x1 , . . . , Xt = xt ) = 1.0 may add distinguished end state.
end state absorbing whenever model makes transition state,
terminates observation sequence generated.

4. Three Inference Problems LOHMMs
HMMs, three inference problems interest. Let LOHMM let
= O1 , O2 , . . . , OT , > 0, finite sequence ground observations:
(1) Evaluation: Determine probability P (O | ) sequence generated
model .
(2) likely state sequence: Determine hidden state sequence
likely produced observation sequence O, i.e. = arg maxS P (S | O, ) .
(3) Parameter estimation: Given set = {O1 , . . . , Ok } observation sequences, determine likely parameters abstract transitions selection
| ) .
distribution , i.e. = arg max P (O
431

fiPSfrag replacements
Kersting, De Raedt, & Raiko

sc(1)
abstract selection abstract
transition
transition

selection

abstract selection
sc(2)
transition

sc(Y)
ls(o)
ls(o)

ls(o)

ls(U)
ls(t)

hc(1)

ls(o)
ls(t)

ls(t)

hc(2)

ls(t)
ls(U)

start

hc(X)

...

ls(U)
em(f2,o)

sc(Z)

em(f1,o)

em(F,U)

em(f1,t)

em(F,o)

em(f2,t)

latex(f1,t)

latex(f1,t) latex(f2,t)

em(F,U)

O1

em(F,U)

O2
O2

abstract state

S2

S1

S0

em(F,o)

states

Figure 3: Trellis induced LOHMM Figure 1. sets reachable states time
0, 1, . . . denoted S0 , S1 , . . . contrast HMMs, additional
layer states sampled abstract states.

address problems turn upgrading existing solutions
HMMs. realized computing grounded trellis Figure 3. possible
ground successor states given state computed first selecting applicable
abstract transitions applying selection probabilities (while taking account
substitutions) ground resulting states. two-step factorization coalesced
one step HMMs.
evaluate O, consider probability partial observation sequence 1 , O2 , . . . , Ot
(ground) state time t, 0 < , given model = (, , , )
(S) := P (O1 , O2 , . . . , Ot , qt = | )
qt = denotes system state time t. HMMs, (S) computed using dynamic programming approach. = 0, set 0 (S) = P (q0 = | ) ,
i.e., 0 (S) probability starting state and, > 0, compute (S) based
t1 (S0 ):
1: S0 := {start}
2: = 1, 2, . . . ,
3:
St =
4:
foreach St1
5:
6:
7:
8:
9:

/* initialize set reachable states*/
/* initialize set reachable states clock t*/


foreach maximally specific p : H
B s.t. B = mgu(S, B) exists
foreach S0 = HB H G (HB ) s.t. Ot1 unifies OB H
S0 6 St
St := St {S0 }
(S0 ) := 0.0

(S0 ) := (S0 ) + t1 (S) p
P
11: return P (O | ) = SST (S)

10:

432

(S0 | HB ) (Ot1 | OB H )

fiLogical Hidden Markov Models

assume sake simplicity start abstract transition p : H
start . Furthermore, boxed parts specify differences HMM formula:
unification taken account.
P
Clearly, HMMs P (O | ) = SST (S) holds. computational complexity
forward procedure O(T (|B| + g)) = O(T s2 ) = maxt=1,2,...,T |St | ,
maximal number outgoing abstract transitions regard abstract state,
g maximal number ground instances abstract state. completely
analogous manner, one devise backward procedure compute
(S) = P (Ot+1 , Ot+2 , . . . , OT | qt = S, ) .
useful solving Problem (3).
forward procedure, straightforward adapt Viterbi algorithm
solution Problem (2), i.e., computing likely state sequence. Let (S)
denote highest probability along single path time accounts first
observations ends state S, i.e.,
(S) =

max

S0 ,S1 ,...,St1

P (S0 , S1 , . . . , St1 , St = S, O1 , . . . , Ot1 |M ) .

procedure finding likely state sequence basically follows forward procedure. Instead summing ground transition probabilities line 10, maximize
them. precisely, proceed follows:
1:S0 := {start}
/* initialize set reachable states*/
2: = 1, 2, . . . ,
3:
St =
/* initialize set reachable states clock t*/
foreach St1
4:

5:
foreach maximally specific p : H
B s.t. B = mgu(S, B) exists
foreach S0 = HB H G (HB ) s.t. Ot1 unifies OB H
6:
S0 6 St
7:
8:
St := St {S0 }
(S, S0 ) := 0.0
9:
10:
(S, S0 ) := (S, S0 ) + t1 (S) p (S0 | HB ) (Ot1 | OB H )
11:
foreach S0 St
12:
(S0 ) = maxSSt1 (S, S0 )
13:
(S0 ) = arg maxSSt1 (S, S0 )
Here, (S, S0 ) stores probability making transition S0 (S0 ) (with
1 (S) = start states S) keeps track state maximizing probability along
single path time accounts first observations ends state 0 .
likely hidden state sequence computed
ST +1 = arg max +1 (S)


St

=

SST +1
(St+1 )

= T, 1, . . . , 1 .

One consider problem (2) abstract level. Instead considering
contributions different abstract transitions single ground transition state
433

fiKersting, De Raedt, & Raiko

state S0 line 10, one might consider likely abstract transition only.
realized replacing line 10 forward procedure
(S0 ) := max(t (S0 ), t1 (S) p (S0 | HB ) (Ot1 | OB H )) .
solves problem finding (20 ) likely state abstract transition
sequence:
Determine sequence states abstract transitions GT =
S0 , T0 , S1 , T1 , S2 , . . . , ST , TT , ST+1 exists substitutions Si+1
Si Ti likely produced observation sequence O, i.e.
GT = arg maxGT P (GT | O, ) .
Thus, logical hidden Markov models pose new types inference problems.
parameter estimation, estimate maximum likelihood transition
probabilities selection distributions. estimate former, upgrade well-known
Baum-Welch algorithm (Baum, 1972) estimating maximum likelihood parameters
HMMs probabilistic context-free grammars.
HMMs, Baum-Welch algorithm computes improved estimate p tranO
sition probability (ground) transition p : H
B taking ratio
p= P

(T)
H0

O0

B

(T0 )

(3)

expected number (T) times making transitions time given
model observation sequence O, total number times transitions
made B time given O.
Basically applies abstract transition. However,
little bit careful direct access (T). Let (gcl, T)
GO
probability following abstract transition via ground instance gcl p : GH GB
time t, i.e.,
(gcl, T) =

(GB) p t+1 (GH)
(GH | HB ) (Ot1 | OB H ) ,
P (O | )

(4)

B , H forward procedure (see above) P (O | ) probability
model generated sequence O. Again, boxed terms constitute main
difference corresponding HMM formula. order apply Equation (3) compute
improved estimates probabilities associated abstract transitions, set
(T) =


X
t=1

(T) =

X
X

(gcl, T)

t=1 gcl

inner sum runs ground instances T.
leads following re-estimation method, assume sets
reachable states reused computations - -values:
434

fiLogical Hidden Markov Models

1:
2:
3:
4:
5:
6:
7:
8:
9:

/* initialization expected counts */
foreach
(T) := /* 0 using pseudocounts */
/* compute expected counts */
= 0, 1, . . . ,
foreach St


foreach max. specific p : H
B s.t. B = mgu(S, B) exists
foreach S0 = HB H G (HB ) s.t. S0 St+1 mgu(Ot , OB H ) exists

(T) := (T) + (S) p t+1 (S0 ) P (O | ) (S0 | HB ) (Ot1 | OB H )

Here, equation (4) found line 9. line 3, set pseudocounts small samplesize regularizers. methods avoid biased underestimate probabilities even
zero probabilities m-estimates (see e.g., Mitchell, 1997) easily adapted.
estimate selection probabilities, recall follows nave Bayes scheme. Therefore, estimated probability domain element domain ratio
number times selected number times d0 selected.
procedure computing -values thus reused.
Altogether, Baum-Welch algorithm works follows: converged, (1) estimate abstract transition probabilities, (2) selection probabilities. Since
instance EM algorithm, increases likelihood data every update,
according McLachlan Krishnan (1997), guaranteed reach stationary
point. standard techniques overcome limitations EM algorithms applicable.
computational complexity (per iteration) O(k ( + d)) = O(k s2 + k d)
k number sequences, complexity computing -values (see above),
sum sizes domains associated predicates. Recently, Kersting
Raiko (2005) combined Baum-Welch algorithm structure search model
selection logical hidden Markov models using inductive logic programming (Muggleton
& De Raedt, 1994) refinement operators. refinement operators account different
abstraction levels explored.

5. Advantages LOHMMs
section, investigate benefits LOHMMs: (1) LOHMMs strictly
expressive HMMs, (2), using abstraction, logical variables unification
beneficial. specifically, (2), show
(B1) LOHMMs design smaller propositional instantiations,
(B2) unification yield better log-likelihood estimates.
5.1 Expressivity LOHMMs
Whereas HMMs specify probability distributions regular languages, LOHMMs specify
probability distributions expressive languages.

435

fiKersting, De Raedt, & Raiko

Theorem 2 (consistent) probabilistic context-free grammar (PCFG) G
language L exists LOHMM s.t. PG (w) = PM (w) w L.
proof (see Appendix C) makes use abstract states unbounded depth.
precisely, functors used implement stack. Without functors, LOHMMs cannot
encode PCFGs and, Herbrand base finite, proven always
exists equivalent HMM.
Furthermore, functors allowed, LOHMMs strictly expressive PCFGs.
specify probability distributions languages context-sensitive:
1.0 :
stack(s(0), s(0))

0.8 :
stack(s(X), s(X))


0.2 : unstack(s(X), s(X))

b
1.0 :
unstack(X, Y)

c
1.0 :
unstack(s(0), Y)

end
1.0 :
end

start
stack(X, X)
stack(X, X)
unstack(s(X), Y)
unstack(s(0), s(Y))
unstack(s(0), s(0))

LOHMM defines distribution {an bn cn | n > 0}.
Finally, use logical variables enables one deal identifiers. Identifiers
special types constants denote objects. Indeed, recall UNIX command
sequence emacs lohmms.tex, ls, latex lohmms.tex, . . . introduction. filename
lohmms.tex identifier. Usually, specific identifiers matter rather
fact object occurs multiple times sequence. LOHMMs easily deal
identifiers setting selection probability constant arguments
identifiers occur. Unification takes care necessary variable bindings.
5.2 Benefits Abstraction Variables Unification
Reconsider domain UNIX command sequences. Unix users oftenly reuse newly created directory subsequent commands mkdir(vt100x), cd(vt100x), ls(vt100x) .
Unification allow us elegantly employ information allows us specify that, observing created directory, model makes transition state
newly created directory used:
p1 : cd(Dir, mkdir) mkdir(Dir, com)



p2 : cd( , mkdir) mkdir(Dir, com)

first transition followed, cd command move newly created directory;
second transition followed, specified directory cd move to. Thus,
LOHMM captures reuse created directories argument future commands.
Moreover, LOHMM encodes simplest possible case show benefits unification. time, observation sequence uniquely determines state sequence,
functors used. Therefore, left abstract output symbols associated
abstract transitions. total, LOHMM U , modelling reuse directories, consists
542 parameters still covers 451000 (ground) states, see Appendix
complete model. compression number parameters supports (B1).
empirically investigate benefits unification, compare U variant N
U variables shared, i.e., unification used instance
436

fiLogical Hidden Markov Models

first transition allowed, see Appendix D. N 164 parameters less U .
computed following zero-one win function
(


1 log PU (O) log PN (O) > 0
f (O) =
0 otherwise
leave-one-out cross-validated Unix shell logs collected Greenberg (1988). Overall,
data consists 168 users four groups: computer scientists, nonprogrammers, novices
others. 300000 commands logged average 110 sessions
per user. present results subset data. considered computer
scientist sessions least single mkdir command appears. yield 283 logical
sequences total 3286 ground atoms. LOO win 81.63%. LOO statistics
favor U :

U
N

training
O)
O) log PPU (O
log P (O
O)
N (O
11361.0
1795.3
13157.0

test
log P (O) log PPNU (O)
(O)
42.8
7.91
50.7

Thus, although U 164 parameters N , shows better generalization performance. result supports (B2). pattern often found U 1
0.15 : cd(Dir, mkdir) mkdir(Dir, com)



0.08 : cd( , mkdir) mkdir(Dir, com)

favoring changing directory made. knowledge cannot captured N
0.25 : cd( , mkdir) mkdir(Dir, com).
results clearly show abstraction variables unification beneficial
applications, i.e., (B1) (B2) hold.

6. Real World Applications
intentions investigate whether LOHMMs applied real world
domains. precisely, investigate whether benefits (B1) (B2)
exploited real world application domains. Additionally, investigate whether
(B3) LOHMMs competitive ILP algorithms utilize unification
abstraction variables,
(B4) LOHMMs handle tree-structured data similar PCFGs.
aim, conducted experiments two bioinformatics application domains: protein
fold recognition (Kersting, Raiko, Kramer, & De Raedt, 2003) mRNA signal structure
detection (Horvath, Wrobel, & Bohnebeck, 2001). application domains multiclass
problems five different classes each.
1. sum probabilities (0.15 + 0.08 = 0.23 6= 0.25) use pseudo counts
subliminal non-determinism (w.r.t. abstract states) U , i.e., case first
transition fires, second one fires.

437

fiKersting, De Raedt, & Raiko

6.1 Methodology
order tackle multiclass problem LOHMMs, followed plug-in estimate
approach. Let {c1 , c2 , . . . , ck } set possible classes. Given finite set training
examples {(xi , yi )}ni=1 X {c1 , c2 , . . . , cn }, one tries find f : X {c1 , c2 , . . . , ck }
f (x) = arg

max

c{c1 ,c2 ,...,ck }

P (x | M, c ) P (c) .

(5)

low approximation error training data well unseen examples.
Equation (5), denotes model structure classes, c denotes
maximum likelihood parameters class c estimated training examples
yi = c only, P (c) prior class distribution.
implemented Baum-Welch algorithm (with pseudocounts m, see line 3) maximum likelihood parameter estimation using Prolog system Yap-4.4.4. experiments,
set = 1 let Baum-Welch algorithm stop change log-likelihood
less 0.1 one iteration next. experiments ran Pentium-IV
3.2 GHz Linux machine.
6.2 Protein Fold Recognition
Protein fold recognition concerned proteins fold nature, i.e., threedimensional structures. important problem biological functions proteins
depend way fold. common approach use database searches find proteins (of known fold) similar newly discovered protein (of unknown fold). facilitate
protein fold recognition, several expert-based classification schemes proteins
developed group current set known protein structures according similarity
folds. instance, structural classification proteins (Hubbard, Murzin, Brenner, & Chotia, 1997) (SCOP) database hierarchically organizes proteins according
structures evolutionary origin. machine learning perspective, SCOP induces
classification problem: given protein unknown fold, assign best matching group
classification scheme. protein fold classification problem investigated
Turcotte, Muggleton, Sternberg (2001) based inductive logic programming
(ILP) system PROGOL Kersting et al. (2003) based LOHMMs.
secondary structure protein domains2 elegantly represented logical sequences. example, secondary structure Ribosomal protein L4 represented
st(null, 2), he(right, alpha, 6), st(plus, 2), he(right, alpha, 4), st(plus, 2),
he(right, alpha, 4), st(plus, 3), he(right, alpha, 4), st(plus, 1), he(hright, alpha, 6)
Helices certain type, orientation length he(HelixType, HelixOrientation, Length),
strands certain orientation length st(StrandOrientation, Length) atoms
logical predicates. application traditional HMMs sequences requires one
either ignore structure helices strands, results loss information,
take possible combinations (of arguments orientation length) account,
leads combinatorial explosion number parameters
2. domain viewed sub-section protein appears number distantly related
proteins fold independently rest protein.

438

fiLogical Hidden Markov Models

end
Block B length 3

Block s(B) length 2

Dynamics within block

Dynamics within block

block(B, s(P))

block(s(B), s(P))

block(B, P)

block(s(B), P)
Transition next block

Transition next block

block(s(B), s(0))

block(B, s(s(s(0))))

block(B, 0)

block(s(B), 0)

Figure 4: Scheme left-to-right LOHMM block model.
results reported Kersting et al. (2003) indicate LOHMMs well-suited
protein fold classification: number parameters LOHMM order
magnitude smaller number corresponding HMM (120 versus approximately
62000) generalization performance, 74% accuracy, comparable Turcotte
et al.s (2001) result based ILP system Progol, 75% accuracy. Kersting et al.
(2003), however, cross-validate results investigate common
bioinformatics impact primary sequence similarity classification accuracy.
instance, two commonly requested ASTRAL subsets subset sequences
less 95% identity (95 cut) less 40% identity
(40 cut). Motivated this, conducted following new experiments.
data consists logical sequences secondary structure protein domains.
work Kersting et al. (2003), task predict one five populated
SCOP folds alpha beta proteins (a/b): TIM beta/alpha-barrel (fold 1), NAD(P)binding Rossmann-fold domains (fold 2), Ribosomal protein L4 (fold 23), Cysteine hydrolase
(fold 37), Phosphotyrosine protein phosphatases I-like (fold 55). class a/b
proteins consists proteins mainly parallel beta sheets (beta-alpha-beta units).
data extracted automatically ASTRAL dataset version 1.65 (Chandonia,
Hon, Walker, Lo Conte, P.Koehl, & Brenner, 2004) 95 cut 40 cut.
work Kersting et al. (2003), consider strands helices only, i.e., coils
isolated strands discarded. 95 cut, yields 816 logical sequences consisting
total 22210 ground atoms. number sequences classes listed 293,
151, 87, 195, 90. 40 cut, yields 523 logical sequences consisting total
14986 ground atoms. number sequences classes listed 182, 100, 66, 122,
53.
LOHMM structure: used LOHMM structure follows left-to-right block topology,
see Figure 4, model blocks consecutive helices (resp. strands). Block
size s, say 3, model remain block = 3 time steps. similar
idea used model haplotypes (Koivisto, Perola, Varilo, Hennah, Ekelund, Lukk,
Peltonen, Ukkonen, & Mannila, 2002; Koivisto, Kivioja, Mannila, Rastas, & Ukkonen,
2004). contrast common HMM block models (Won, Prugel-Bennett, & Krogh, 2004),
439

fiKersting, De Raedt, & Raiko

transition parameters shared within block one ensure model
makes transition next state s(Block ) end block; example
exactly 3 intra-block transitions. Furthermore, specific abstract transitions
helix types strand orientations model priori distribution, intra-
inter-block transitions. number blocks sizes chosen according
empirical distribution sequence lengths data beginning
ending protein domains likely captured detail. yield following block
structure
1 2

...

19 20

27 28

...

40 41

46 47

61 62

76 77

numbers denote positions within protein domains. Furthermore, note
last block gathers remaining transitions. blocks modelled using
hidden abstract states
hc(HelixType, HelixOrientation, Length, Block ) sc(StrandOrientation, Length, Block ) .
Here, Length denotes number consecutive bases structure element consists of.
length discretized 10 bins original lengths uniformally
distributed. total, LOHMM 295 parameters. corresponding HMM without
parameter sharing 65200 parameters. clearly confirms (B1).
Results: performed 10-fold cross-validation. 95 cut dataset, accuracy
76% took approx. 25 minutes per cross-validation iteration; 40 cut, accuracy
73% took approx. 12 minutes per cross-validation iteration. results validate
Kersting et al.s (2003) results and, turn, clearly show (B3) holds. Moreover,
novel results 40 cut dataset indicate similarities detected LOHMMs
protein domain structures accompanied high sequence similarity.
6.3 mRNA Signal Structure Detection
mRNA sequences consist bases (guanine, adenine, uracil, cytosine) fold intramolecularly form number short base-paired stems (Durbin, Eddy, Krogh, & Mitchison,
1998). base-paired structure called secondary structure, cf. Figures 5 6.
secondary structure contains special subsequences called signal structures responsible special biological functions, RNA-protein interactions cellular transport.
function signal structure class based common characteristic binding
site class elements. elements necessarily identical similar.
vary topology (tree structure), size (number constituting bases), base
sequence.
goal experiments recognize instances signal structures classes
mRNA molecules. first application relational learning recognize signal structure class mRNA molecules described works Bohnebeck, Horvath,
Wrobel (1998) Horvath et al. (2001), relational instance-based learner
RIBL applied. dataset 3 used similar one described Horvath
3. dataset described work Horvath et al. (2001) could obtain
original dataset. compare smaller data set used Horvath et al., consisted

440

fiLogical Hidden Markov Models

et al. (2001). consisted 93 mRNA secondary structure sequences. precisely,
composed 15 5 SECIS (Selenocysteine Insertion Sequence), 27 IRE (Iron Responsive
Element), 36 TAR (Trans Activating Region) 10 histone stem loops constituting five
classes.
secondary structure composed different building blocks stacking region,
hairpin loops, interior loops etc. contrast secondary structure proteins forms
chains, secondary structure mRNA forms tree. trees easily handled
using HMMs, mRNA secondary structure data challenging proteins.
Moreover, Horvath et al. (2001) report making tree structure available RIBL
background knowledge influence classification accuracy. precisely,
using simple chain representation RIBL achieved 77.2% leave-one-out cross-validation
(LOO) accuracy whereas using tree structure background knowledge RIBL achieved
95.4% LOO accuracy.
followed Horvath et al.s experimental setup, is, adapted data representations LOHMMs compared chain model tree model.

Chain Representation: chain representation (see Figure 5),
signal
structures

described

single(TypeSingle, Position, Acid )

helical(TypeHelical , Position, Acid , Acid ).
Depending type, structure element represented either single/3 helical/4.
first argument
TypeSingle (resp.
TypeHelical ) specifies type structure element, i.e.,
single, bulge3, bulge5, hairpin (resp. stem). argument Position position sequence element within corresponding structure element counted down,
i.e.4 , {n13 (0), n12 (0), . . . , n1 (0)}. maximal position set 13
maximal position observed data. last argument encodes observed nucleotide
(pair).
used LOHMM structure follows left-to-right block structure shown
Figure 4. underlying idea model blocks consecutive helical structure elements. hidden states modelled using single(TypeSingle, Position, Acid , Block )
helical(TypeHelical , Position, Acid , Acid , Block ). Block consecutive helical (resp. single) structure elements, model remain Block transition
single element. transition single (resp. helical) element occurs Position
n(0). positions n(Position), transitions helical (resp. single)
structure elements helical (resp. single) structure elements Position capturing dynamics nucleotide pairs (resp. nucleotides) within structure elements. instance,

66 signal structures close data set. larger data set (with 400 structures) Horvath
et al. report error rate 3.8% .
4. nm (0) shorthand recursive application functor n 0 times, i.e., position m.

441

fiKersting, De Raedt, & Raiko

helical(stem, n(0), c, g).
helical(stem, n(n(0)), c, g).
helical(stem, n(n(n(0))), c, g).
single(bulge5, n(0), a).
single(bulge5, n(n(0)), a).
single(bulge5, n(n(n(0))), g).
helical(stem, n(0), c, g).
helical(stem, n(n(0)), c, g).
single(bulge5, n(0), a).
helical(stem, n(0), a, a).
helical(stem, n(n(0)), u, a).
helical(stem, n(n(n(0))), u, g).
helical(stem, n(n(n(n(0)))), u, a).
helical(stem, n(n(n(n(n(0))))), c, a).
helical(stem, n(n(n(n(n(n(0)))))), u, a).
helical(stem, n(n(n(n(n(n(n(0))))))), a, u).

u


u
c
c
c

g
g
g



g


c
c

single(hairpin, n(n(n(0))), a).
single(hairpin, n(n(0)), u).
single(hairpin, n(0), u).

single(bulge3, n(0), a).

g
g



u
u
u
c
u




g



u

Figure 5: chain representation SECIS signal structure. ground atoms
ordered clockwise starting helical(stem, n(n(n(n(n(n(n(0))))))), a, u)
lower left-hand side corner.

transitions block n(0) position n(n(0))
pa :he(stem,n(0),X,Y)

: he(stem, n(0), X, Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))
pb :he(stem,n(0),X,Y)

b:

he(stem, n(0), Y, X, n(0)) he(stem, n(n(0)), X, Y, n(0)))

c:

he(stem, n(0), X, , n(0)) he(stem, n(n(0)), X, Y, n(0)))

d:

he(stem, n(0), , Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))

e:

he(stem, n(0), , , n(0)) he(stem, n(n(0)), X, Y, n(0)))

pc :he(stem,n(0),X,Y)

pd :he(stem,n(0),X,Y)
pe :he(stem,n(0),X,Y)

total, 5 possible blocks maximal number blocks consecutive
helical structure elements observed data. Overall, LOHMM 702 parameters.
contrast, corresponding HMM 16600 transitions validating (B1).
Results: LOO test log-likelihood 63.7, EM iteration took average
26 seconds.
Without unification-based transitions b-d, i.e., using abstract transitions
pa :he(stem,n(0),X,Y)

: he(stem, n(0), X, Y, n(0)) he(stem, n(n(0)), X, Y, n(0)))
e:

pe :he(stem,n(0),X,Y)

he(stem, n(0), , , n(0)) he(stem, n(n(0)), X, Y, n(0))),

model 506 parameters. LOO test log-likelihood 64.21, EM iteration took average 20 seconds. difference LOO test log-likelihood statistically
significant (paired t-test, p = 0.01).
Omitting even transition a, LOO test log-likelihood dropped 66.06,
average time per EM iteration 18 seconds. model 341 parameters.
difference average LOO log-likelihood statistically significant (paired t-test, p = 0.001).
results clearly show unification yield better LOO test log-likelihoods, i.e.,
(B2) holds.
442

fiLogical Hidden Markov Models

nucleotide pair((c, g)).
nucleotide pair((c, g)).
nucleotide pair((c, g)).
helical(s(s(s(s(s(0))))), s(s(s(0))), [c], stem, n(n(n(0)))).
nucleotide(a).
nucleotide(a).
nucleotide(g).
single(s(s(s(s(0)))), s(s(s(0))), [], bulge5, n(n(n(0)))).
nucleotide pair((c, g)).
nucleotide pair((c, g)).
helical(s(s(s(0))), s(0), [c, c, c], stem, n(n(0))).
nucleotide(a).
single(s(s(0)), s(0), [], bulge5, n(0)).
nucleotide pair((a, a)).
nucleotide pair((u, a)).
nucleotide pair((u, g)).
nucleotide pair((u, a)).
nucleotide pair((c, a)).
nucleotide pair((u, a)).
nucleotide pair((a, u)).
helical(s(0), 0, [c, c], stem, n(n(n(n(n(n(n(0)))))))).

u


u
c
c
c

g
g
g



g


c
c

g
g

single(s(s(s(s(s(s(0)))))), s(s(s(s(s(0))))),
[], hairpin, n(n(n(0)))).
nucleotide(a).
nucleotide(u).
nucleotide(u).

single(s(s(s(s(s(s(s(0))))))), s(s(s(0))),
[], bulge3, n(0)).
nucleotide(a).



u
u
u
c
u




g



u

0
s(0)
s(s(0))s(s(s(0)))
s(s(s(s(0))))
s(s(s(s(s(s(s(0)))))))
s(s(s(s(s(0))))
s(s(s(s(s(s(0))))))

root(0, root, [c]).

Figure 6: tree representation SECIS signal structure. (a) logical sequence,
i.e., sequence ground atoms representing SECIS signal structure.
ground atoms ordered clockwise starting root(0, root, [c]) lower
left-hand side corner. (b) tree formed secondary structure elements.

Tree Representation: tree representation (see Figure 6 (a)), idea capture
tree structure formed secondary structure elements, see Figure 6 (b).
training instance described sequence ground facts
root(0, root, #Children),
helical(ID, ParentID, #Children, Type, Size),
nucleotide pair(BasePair ),
single(ID, ParentID, #Children, Type, Size),
nucleotide(Base) .
Here, ID ParentID natural numbers 0, s(0), s(s(0)), . . . encoding childparent relation, #Children denotes number5 children [], [c], [c, c], . . ., Type
type structure element stem, hairpin, . . ., Size natural number
0, n(0), n(n(0)), . . . Atoms root(0, root, #Children) used root topology.
maximal #Children 9 maximal Size 13 maximal value
observed data.
trees easily handled using HMMs, used LOHMM basically
encodes PCFG. Due Theorem 2, possible. used LOHMM structure
found Appendix E. processes mRNA trees in-order. Unification used
parsing tree. chain representation, used Position argument hidden
states encode dynamics nucleotides (nucleotide pairs) within secondary structure
5. Here, use Prolog short hand notation [] lists. list either constant [] representing
empty list, compound term functor ./2 two arguments, respectively head
tail list. Thus [a, b, c] compound term .(a, .(b, .(c, []))).

443

fiKersting, De Raedt, & Raiko

elements. maximal Position 13. contrast chain representation,
nucleotide pairs (a, u) treated constants. Thus, argument BasePair
consists 16 elements.
Results: LOO test log-likelihood 55.56. Thus, exploiting tree structure
yields better probabilistic models. average, EM iteration took 14 seconds. Overall,
result shows (B4) holds.
Although Baum-Welch algorithm attempts maximize different objective function, namely likelihood data, interesting compare LOHMMs RIBL
terms classification accuracy.
Classification Accuracy: chain representation, LOO accuracies
LOHMMs 99% (92/93). considerable improvement RIBLs 77.2% (51/66)
LOO accuracy representation. tree representation, LOHMM
achieved LOO accuracy 99% (92/93). comparable RIBLs LOO accuracy
97% (64/66) kind representation.
Thus, already chain LOHMMs show marked increases LOO accuracy compared RIBL (Horvath et al., 2001). order achieve similar LOO accuracies, Horvath
et al. (2001) make tree structure available RIBL background knowledge.
LOHMMs, significant influence LOO test log-likelihood,
LOO accuracies. clearly supports (B3). Moreover, according Horvath et al.,
mRNA application considered success terms application domain,
although primary goal experiments. exist alternative
parameter estimation techniques models, covariance models (Eddy &
Durbin, 1994) pair hidden Markov models (Sakakibara, 2003), might
used well basis comparison. However, LOHMMs employ (inductive) logic programming principles, appropriate compare systems within paradigm
RIBL.

7. Related Work
LOHMMs combine two different research directions. one hand, related
several extensions HMMs probabilistic grammars. hand,
related recent interest combining inductive logic programming principles
probability theory (De Raedt & Kersting, 2003, 2004).
first type approaches, underlying idea upgrade HMMs probabilistic
grammars represent structured state spaces.
Hierarchical HMMs (Fine, Singer, & Tishby, 1998), factorial HMMs (Ghahramani &
Jordan, 1997), HMMs based tree automata (Frasconi, Soda, & Vullo, 2002) decompose state variables smaller units. hierarchical HMMs states
HMMs, factorial HMMs factored k state variables depend one
another observation, tree based HMMs represented probability
distributions defined tree structures. key difference LOHMMs
approaches employ logical concept unification. Unification essential
444

fiLogical Hidden Markov Models

allows us introduce abstract transitions, consist detailed
states. experimental evidence shows, sharing information among abstract states
means unification lead accurate model estimation. holds relational Markov models (RMMs) (Anderson, Domingos, & Weld, 2002) LOHMMs
closely related. RMMs, states different types, type described
different set variables. domain variable hierarchically structured.
main differences LOHMMs RMMs RMMs either support
variable binding unification hidden states.
equivalent HMMs context-free languages probabilistic context-free grammars (PCFGs). HMMs, consider sequences logical atoms
employ unification. Nevertheless, formal resemblance Baum-Welch
algorithms LOHMMs PCFGs. case LOHMM encodes PCFG
algorithms identical theoretical point view. re-estimate parameters
ratio expected number times transition (resp. production) used
expected number times transition (resp. production) might used. proof
Theorem 2 assumes PCFG given Greibach normal form6 (GNF) uses
pushdown automaton parse sentences. grammars GNF, pushdown automata
common parsing. contrast, actual computations Baum-Welch algorithm
PCFGs, called Inside-Outside algorithm (Baker, 1979; Lari & Young, 1990),
usually formulated grammars Chomsky normal form7 . Inside-Outside algorithm
make use efficient CYK algorithm (Hopcroft & Ullman, 1979) parsing strings.
alternative learning PCFGs strings learn structured data
skeletons, derivation trees nonterminal nodes removed (Levy &
Joshi, 1978). Skeletons exactly set trees accepted skeletal tree automata (STA).
Informally, STA, given tree input, processes tree bottom up, assigning
state node based states nodes children. STA accepts tree iff
assigns final state root tree. Due automata-based characterization
skeletons derivation trees, learning problem (P)CFGs reduced
problem STA. particular, STA techniques adapted learning tree
grammars (P)CFGs (Sakakibara, 1992; Sakakibara et al., 1994) efficiently.
PCFGs extended several ways. closely related LOHMMs
unification-based grammars extensively studied computational linguistics. Examples (stochastic) attribute-value grammars (Abney, 1997), probabilistic feature grammars (Goodman, 1997), head-driven phrase structure grammars (Pollard & Sag,
1994), lexical-functional grammars (Bresnan, 2001). learning within frameworks, methods undirected graphical models used; see work Johnson (2003)
description recent work. key difference LOHMMs nonterminals replaced structured, complex entities. Thus, observation sequences
flat symbols atoms modelled. Goodmans probabilistic feature grammars
exception. treat terminals nonterminals vectors features. abstraction
made, i.e., feature vectors ground instances, unification employed.
6. grammar GNF iff productions form aV variable, exactly one
terminal V string none variables.
7. grammar CNF iff every production form B, C A, B C variables,
terminal.

445

fiKersting, De Raedt, & Raiko

con

mkdir
con
mkdir

mv

ls

cd

mv
con

vt100x

vt100x

ls
new

vt100x

vt100x

vt100x

new

(a)

cd

vt100x

(b)

vt100x

vt100x

Figure 7: (a) atom logical sequence mkdir(vt100x), mv(new, vt100x),
ls(vt100x), cd(vt100x) forms tree. shaded nodes denote shared labels
among trees. (b) sequence represented single tree. predicate con/2 represents concatenation operator.

Therefore, number parameters needs estimated becomes easily large,
data sparsity serious problem. Goodman applied smoothing overcome problem.
LOHMMs generally related (stochastic) tree automata (see e.g., Carrasco, Oncina, Calera-Rubio, 2001). Reconsider Unix command sequence
mkdir(vt100x), mv(new, vt100x), ls(vt100x), cd(vt100x) . atom forms tree, see
Figure 7 (a), and, indeed, whole sequence atoms forms (degenerated) tree,
see Figure 7 (b). Tree automata process single trees vertically, e.g., bottom-up. state
automaton assigned every node tree. state depends node label
states associated siblings node. focus sequential
domains. contrast, LOHMMs intended learning sequential domains.
process sequences trees horizontally, i.e., left right. Furthermore, unification
used share information consecutive sequence elements. Figure 7 (b)
illustrates, tree automata employ information allowing higher-order
transitions, i.e., states depend node labels states associated
predecessors 1, 2, . . . levels tree.
second type approaches, attention devoted developing highly
expressive formalisms, e.g. PCUP (Eisele, 1994), PCLP (Riezler, 1998), SLPs (Muggleton, 1996), PLPs (Ngo & Haddawy, 1997), RBNs (Jaeger, 1997), PRMs (Friedman,
Getoor, Koller, & Pfeffer, 1999), PRISM (Sato & Kameya, 2001), BLPs (Kersting & De
Raedt, 2001b, 2001a), DPRMs (Sanghai, Domingos, & Weld, 2003). LOHMMs
seen attempt towards downgrading highly expressive frameworks. Indeed, applying main idea underlying LOHMMs non-regular probabilistic grammar, i.e., replacing
flat symbols atoms, yields principle stochastic logic programs (Muggleton, 1996).
consequence, LOHMMs represent interesting position expressiveness scale.
Whereas retain essential logical features expressive formalisms,
seem easier understand, adapt learn. akin many contemporary consid446

fiLogical Hidden Markov Models

erations inductive logic programming (Muggleton & De Raedt, 1994) multi-relational
data mining (Dzeroski & Lavrac, 2001).

8. Conclusions
Logical hidden Markov models, new formalism representing probability distributions
sequences logical atoms, introduced solutions three central
inference problems (evaluation, likely state sequence parameter estimation)
provided. Experiments demonstrated unification improve generalization
accuracy, number parameters LOHMM order magnitude smaller
number parameters corresponding HMM, solutions presented
perform well practice LOHMMs possess several advantages traditional
HMMs applications involving structured sequences.
Acknowledgments authors thank Andreas Karwath Johannes Horstmann
interesting collaborations protein data; Ingo Thon interesting collaboration
analyzing Unix command sequences; Saul Greenberg providing Unix command sequence data. authors would thank anonymous reviewers comments considerably improved paper. research partly supported
European Union IST programme contract numbers IST-2001-33053 FP6-508861
(Application Probabilistic Inductive Logic Programming (APrIL) II). Tapani Raiko
supported Marie Curie fellowship DAISY, HPMT-CT-2001-00251.

Appendix A. Proof Theorem 1
Let = (, , , ) LOHMM. show specifies time discrete stochastic
process, i.e., sequence random variables hXt it=1,2,... , domains random
variable Xt hb(), Herbrand base , define immediate state operator
TM -operator current emission operator EM -operator.
Definition 4 (TM -Operator, EM -Operator ) operators TM : 2hb 2hb EM :
2hb 2hb


TM (I) = {HB H | (p : H
B) : BB I, HB H G (H)}


EM (I) = {OB H | (p : H
B) : BB I, HB G G (H)
OB H G (O)}
i+1
({start}))
= 1, 2, 3, . . ., set TM
({start}) := TM (TM
1
TM ({start}) := TM ({start}) specifies state set clock forms random varii ({start}) specifies possible symbols emitted transitioning
able Yi . set UM
+ 1. forms variable Ui . Yi (resp. Ui ) extended random
variable Zi (resp. Ui ) hb :

P (Zi = z) =



({start})
0.0 : z 6 TM
P (Yi = z) : otherwise

447

fiKersting, De Raedt, & Raiko

PSfrag replacements

Z1

Z2

Z3

U1

U2

...

U3

Figure 8: Discrete time stochastic process induced LOHMM. nodes Z Ui
represent random variables hb .

Figure 8 depicts influence relation among Zi Ui . Using standard arguments
probability theory noting
P (Ui = Ui | Zi+1 = zi+1 , Zi = zi ) =
P (Zi+1 | Zi ) =

X

P (Zi+1 , ui | Zi )

P (Zi+1 = zi+1 , Ui = ui | Zi )
P
ui P (Zi+1 , ui | Zi )

ui

probability distributions due equation (1), easy show Kolmogorovs extension theorem (see Bauer, 1991; Fristedt
Gray, 1997) holds. Thus,
Nt
specifies unique probability distribution
(Z
Ui ) > 0

i=1
limit .


Appendix B. Moore Representations LOHMMs
HMMs, Moore representations, i.e., output symbols associated states Mealy
representations, i.e., output symbols associated transitions, equivalent.
appendix, investigate extend holds LOHMMs.
Let L Mealy-LOHMM according definition 3. following, derive
notation equivalent LOHMM L0 Moore representation abstract
transitions abstract emissions (see below). predicate b/n L extended b/n+
1 L0 . domains first n arguments b/n. last argument
store observation emitted. precisely, abstract transition
o(v1 ,...,vk )

p : h(w1 , . . . , wl ) b(u1 , . . . , un )
L, abstract transition
p : h(w1 , . . . , wl , o(v01 , . . . , v0k )) b(u1 , . . . , un , )
L0 . primes o(v01 , . . . , v0k ) denote replaced free 8 variables o(v1 , . . . , vk )
distinguished constant symbol, say #. Due this, holds
(h(w1 , . . . , wl )) = (h(w1 , . . . , wl , o(v01 , . . . , v0k ))) ,
8. variable X vars(o(v1 , . . . , vk )) free iff X 6 vars(h(w1 , . . . , wl )) vars(b(u1 , . . . , un )).

448

(6)

fiLogical Hidden Markov Models

L0 output distribution specified using abstract emissions expressions
form
1.0 : o(v1 , . . . , vk ) h(w1 , . . . , wl , o(v01 , . . . , v0k )) .
(7)
semantics abstract transition L0
state S0t G0 (b(u1 , . . . , un , )) system make transition
S0t+1 G0 (h(w1 , . . . , wl , o(v01 , . . . , v0k ))) probability
p (S0t+1 | h(w1 , . . . , wl , o(v01 , . . . , v0k )) | S0t )


state

(8)

S0t = mgu(S0t , b(u1 , . . . , un , )). Due Equation (6), Equation (8) rewritten

p (S0t+1 | h(w1 , . . . , wl ) | S0t ) .
Due equation (7), system emit output symbol ot+1 G0 (o(v1 , . . . , vk ))
state S0t+1 probability
(ot+1 | o(v1 , . . . , vk )S0t+1 S0t )
S0t+1 = mgu(h(w1 , . . . , wl , o(v01 , . . . , v0k )), S0t+1 ). Due construction L0 ,
exists triple (St , St+1 , Ot+1 ) L triple (S0t , S0t+1 , Ot+1 ), > 0, L0 (and vise
versa). Hence,both LOHMMs assign overall transition probability.
L L0 differ way initialize sequences h(S0t , S0t+1 , Ot+1 it=0,2...,T (resp.
h(St , St+1 , Ot+1 it=0,2...,T ). Whereas L starts state S0 makes transition S1
emitting O1 , Moore-LOHMM L0 supposed emit symbol O0 S00 making
transition S01 . compensate using prior distribution. existence
correct prior distribution L0 seen follows. L, finitely many
states reachable time = 1, i.e, PL (q0 = S) > 0 holds finite set ground
states S. probability PL (q0 = s) computed similar 1 (S). set = 1 line
6, neglecting condition Ot1 line 10, dropping (Ot1 | OB H ) line 14.
Completely listing states S1 together PL (q0 = S), i.e., PL (q0 = S) : start ,
constitutes prior distribution L0 .
argumentation basically followed approach transform Mealy machine
Moore machine (see e.g., Hopcroft Ullman, 1979). Furthermore, mapping
Moore-LOHMM introduced present section Mealy-LOHMM straightforward.

Appendix C. Proof Theorem 2
Let terminal alphabet N nonterminal alphabet. probabilistic context-free
grammar (PCFG) G consists distinguished start symbol N plus finite set

productions
P form p : X , X N , (N ) p [0, 1].
X N , :X p = 1. PCFG defines stochastic process sentential forms states,
leftmost rewriting steps transitions. denote single rewriting operation
grammar single arrow . result one ore rewriting operations
able rewrite (N ) sequence (N ) nonterminals terminals,
write . probability rewriting product probability
449

fiKersting, De Raedt, & Raiko

values associated productions used derivation. assume G consistent, i.e.,
sum probabilities derivations sum 1.0.
assume PCFG G Greibach normal form. follows Abney
et al.s (1999) Theorem 6 G consistent. Thus, every production P G
form p : X aY1 . . . Yn n 0. order encode G LOHMM ,
introduce (1) non-terminal symbol X G constant symbol nX (2)
terminal symbol G constant symbol t. production P G, include

abstract transition form p : stack([nY1 , . . . , nYn |S])
stack([nX|S]), n > 0,

p : stack(S)
stack([nX|S]), n = 0. Furthermore, include 1.0 : stack([s]) start
end
1.0 : end stack([]). straightforward prove induction G
equivalent.


Appendix D. Logical Hidden Markov Model Unix Command
Sequences
LOHMMs described model Unix command sequences triggered mkdir.
aim, transformed original Greenberg data sequence logical atoms
com, mkdir(Dir, LastCom), ls(Dir, LastCom), cd(Dir, Dir, LastCom), cp(Dir, Dir, LastCom)
mv(Dir, Dir, LastCom). domain LastCom {start, com, mkdir, ls, cd, cp, mv}.
domain Dir consisted argument entries mkdir, ls, cd, cp, mv original
dataset. Switches, pipes, etc. neglected, paths made absolute. yields
212 constants domain Dir. original commands, mkdir, ls, cd,
cp, mv, represented com. mkdir appear within 10 time steps
command C {ls, cd, cp,mv}, C represented com. Overall, yields
451000 ground states covered Markov model.
unification LOHMM U basically implements second order Markov model, i.e.,
probability making transition depends upon current state previous
state. 542 parameters following structure:
com start.
mkdir(Dir, start) start.

com com.
mkdir(Dir, com) com.
end com.

Furthermore, C {start, com}
mkdir(Dir, com)
mkdir( , com)
com
end
ls(Dir, mkdir)
ls( , mkdir)
cd(Dir, mkdir)









mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).

cd( , mkdir)
cp( , Dir, mkdir)
cp(Dir, , mkdir)
cp( , , mkdir)
mv( , Dir, mkdir)
mv(Dir, , mkdir)
mv( , , mkdir)

450









mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).
mkdir(Dir,C).

fiLogical Hidden Markov Models

together C {mkdir, ls, cd, cp, mv} C 1 {cd, ls} (resp.
C2 {cp, mv})
mkdir(Dir, com)
mkdir( , com)
com
end
ls(Dir,C1 )
ls( ,C1 )
cd(Dir,C1 )
cd( ,C1 )
cp( , Dir,C1 )
cp(Dir, ,C1 )
cp( , ,C1 )
mv( , Dir,C1 )
mv(Dir, ,C1 )
mv( , ,C1 )
















C1 (Dir,C). mkdir( , com)
com
C1 (Dir,C).
C1 (Dir,C).
end
ls(From,C2 )
C1 (Dir,C).
ls(To,C2 )
C1 (Dir,C).
C1 (Dir,C).
ls( ,C2 )
C1 (Dir,C).
cd(From,C2 )
C1 (Dir,C).
cd(To,C2 )
C1 (Dir,C).
cd( ,C2 )
C1 (Dir,C). cp(From, ,C2 )
C1 (Dir,C).
cp( , To,C2 )
C1 (Dir,C).
cp( , ,C2 )
C1 (Dir,C). mv(From, ,C2 )
mv( , To,C2 )
C1 (Dir,C).
mv( , ,C2 )

C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).
C2 (From, To,C).

states fully observable, omitted output symbols associated
clauses, and, sake simplicity, omitted associated probability values.
unification LOHMM N variant U variables shared

mkdir( , com) cp(From, To,C).
ls(
com cp(From, To,C).
cd(
end cp(From, To,C). cp( ,
mv( ,

, cp)
, cp)
, cp)
, cp)






cp(From, To,C).
cp(From, To,C).
cp(From, To,C).
cp(From, To,C).

transitions affected, N 164 parameters less U , i.e., 378.

Appendix E. Tree-based LOHMM mRNA Sequences
LOHMM processes nodes mRNA trees in-order. structure LOHMM
shown end section. copies shaded parts. Terms
abbreviated using starting alphanumerical; tr stands tree, helical, si
single, nuc nucleotide, nuc p nucleotide pair.
domain #Children covers maximal branching factor found data, i.e.,
{[c], [c, c], . . . , [c, c, c, c, c, c, c, c, c]}; domain Type consists types occurring
data, i.e., {stem, single, bulge3, bulge5, hairpin}; Size, domain covers
maximal length secondary structure element data, i.e., longest sequence
consecutive bases respectively base pairs constituting secondary structure element.
length encoded {n1 (0), n2 (0), . . . , n13 (0)} nm (0) denotes recursive
application functor n times. Base BasePair , domains 4 bases
respectively 16 base pairs. total, 491 parameters.
451

fimy start
1.0 : root(0, root, X)

Copies tr(Id, [c], [Pa [C]|R]), tr(Id, [c, c], [Pa [C]|R]),
tr(Id, [c, c, c], [Pa [C]|R])
0.25 : he(s(Id), Pa, [], T, L)
tr(Id, , [Pa [C]|R])

tr(0, X, [0 X])

tr(Id, [c, c, c], [Pa [C1, C2|Cs]|R])

0.25 : he(s(Id), Pa, B, T, L)

0.25 : he(s(Id), Pa, [], T, L)
0.25 : he(s(Id), Pa, B, T, L)
tr(Id, , [Pa [C1, C2|Cs]|R])
0.25 : si(s(Id), Pa, B, T, L)
0.25 : si(s(Id), Pa, [], T, L)

0.25 : si(s(Id), Pa, B, T, L)
0.25 : si(s(Id), Pa, [], T, L)
se(T, L, s(Id), B, [s(Id) B|R])

se(T, L, s(Id), [], R)

Copies tr(Id, [c], [Pa [C1, C2|Cs]|R]), tr(Id, [c, c], [Pa [C1, C2|Cs]|R]),

tree
model

se(T, L, s(Id), [], [Pa [C2|Cs]|R]) se(T, L, s(Id), B, [s(Id) B, Pa [C2|Cs]|R])

Copies type single, bulge3, bulge5
Copies n(n(0)) n(n(n(0)))

Copies length sequence n(n(0)), n(n(n(0))), n(n(n(n(0))))
se(stem, n(A), Id, B, S)

se(hairpin, n(A), Id, B, S)

Copies nuc p(a, g), . . . , nuc p(u, u)

Copies nuc(g), nuc(c), nuc(u)
0.25 : nuc(a)

0.0625 : nuc p(a, a)

se(hairpin, A, Id, B, S)

se(hairpin, n(0), Id, B, S)

se(stem, A, Id, B, S)

se(hairpin, n(0), s( ), , [])

0.25 : nuc(a)

se(stem, n(0), s( ), , [])

0.0625 : nuc p(a, a)

0.25 : nuc(a)

se(stem, n(0), Id, B, S)

0.0625 : nuc p(a, a)

Copies nuc p(a, g), . . . , nuc p(u, u)

Copies nuc(g), nuc(c), nuc(u)
end

tr(Id, B, S)

sequence
model

Kersting, De Raedt, & Raiko

Figure 9: mRNA LOHMM structure. symbol denotes anonymous variables
read treated distinct, new variables time encountered.
copies shaded part. Terms abbreviated using starting
alphanumerical; tr stands tree, se structure element, helical,
si single, nuc nucleotide, nuc p nucleotide pair.

References

452

Abney, S. (1997). Stochastic Attribute-Value Grammars. Computational Linguistics, 23 (4),
597618.

1.0

start

fiLogical Hidden Markov Models

Abney, S., McAllester, D., & Pereira, F. (1999). Relating probabilistic grammars automata. Proceedings 37th Annual Meeting Association Computational
Linguistics (ACL-1999), pp. 542549. Morgan Kaufmann.
Anderson, C., Domingos, P., & Weld, D. (2002). Relational Markov Models Application Adaptive Web Navigation. Proceedings Eighth International
Conference Knowledge Discovery Data Mining (KDD-2002), pp. 143152 Edmonton, Canada. ACM Press.
Baker, J. (1979). Trainable grammars speech recognition. Speech communication
paper presented th 97th Meeting Acoustical Society America, pp. 547550
Boston, MA.
Bauer, H. (1991). Wahrscheinlichkeitstheorie (4. edition). Walter de Gruyter, Berlin, New
York.
Baum, L. (1972). inequality associated maximization technique statistical estimation probabilistic functions markov processes. Inequalities, 3, 18.
Bohnebeck, U., Horvath, T., & Wrobel, S. (1998). Term comparison first-order similarity
measures. Proceedings Eigth International Conference Inductive Logic
Programming (ILP-98), Vol. 1446 LNCS, pp. 6579. Springer.
Bresnan, J. (2001). Lexical-Functional Syntax. Blackwell, Malden, MA.
Carrasco, R., Oncina, J., & Calera-Rubio, J. (2001). Stochastic inference regular tree
languages. Machine Learning, 44 (1/2), 185197.
Chandonia, J., Hon, G., Walker, N., Lo Conte, L., P.Koehl, & Brenner, S. (2004).
ASTRAL compendium 2004. Nucleic Acids Research, 32, D189D192.
Davison, B., & Hirsh, H. (1998). Predicting Sequences User Actions. Predicting
Future: AI Approaches Time-Series Analysis, pp. 512. AAAI Press.
De Raedt, L., & Kersting, K. (2003). Probabilistic Logic Learning. ACM-SIGKDD Explorations: Special issue Multi-Relational Data Mining, 5 (1), 3148.
De Raedt, L., & Kersting, K. (2004). Probabilistic Inductive Logic Programming.
Ben-David, S., Case, J., & Maruoka, A. (Eds.), Proceedings 15th International
Conference Algorithmic Learning Theory (ALT-2004), Vol. 3244 LNCS, pp.
1936 Padova, Italy. Springer.
Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological sequence analysis:
Probabilistic models proteins nucleic acids. Cambridge University Press.
Dzeroski, S., & Lavrac, N. (Eds.). (2001). Relational data mining. Springer-Verlag, Berlin.
Eddy, S., & Durbin, R. (1994). RNA sequence analysis using covariance models. Nucleic
Acids Res., 22 (11), 20792088.
453

fiKersting, De Raedt, & Raiko

Eisele, A. (1994). Towards probabilistic extensions contraint-based grammars.
Dorne, J. (Ed.), Computational Aspects Constraint-Based Linguistics Decription-II.
DYNA-2 deliverable R1.2.B.
Fine, S., Singer, Y., & Tishby, N. (1998). hierarchical hidden markov model: analysis
applications. Machine Learning, 32, 4162.
Frasconi, P., Soda, G., & Vullo, A. (2002). Hidden markov models text categorization
multi-page documents. Journal Intelligent Information Systems, 18, 195217.
Friedman, N., Getoor, L., Koller, D., & Pfeffer, A. (1999). Learning probabilistic relational
models. Proceedings Sixteenth International Joint Conference Artificial Intelligence (IJCAI-1999), pp. 13001307. Morgan Kaufmann.
Fristedt, B., & Gray, L. (1997). Modern Approach Probability Theory. Probability
applications. Birkhauser Boston.
Ghahramani, Z., & Jordan, M. (1997). Factorial hidden Markov models. Machine Learning,
29, 245273.
Goodman, J. (1997). Probabilistic feature grammars. Proceedings Fifth International Workshop Parsing Technologies (IWPT-97) Boston, MA, USA.
Greenberg, S. (1988). Using Unix: collected traces 168 users. Tech. rep., Dept.
Computer Science, University Calgary, Alberta.
Hopcroft, J., & Ullman, J. (1979). Introduction Automata Theory, Languages,
Computation. Addison-Wesley Publishing Company.
Horvath, T., Wrobel, S., & Bohnebeck, U. (2001). Relational Instance-Based learning
Lists Terms. Machine Learning, 43 (1/2), 5380.
Hubbard, T., Murzin, A., Brenner, S., & Chotia, C. (1997). SCOP : structural classification
proteins database. NAR, 27 (1), 236239.
Jacobs, N., & Blockeel, H. (2001). Learning Shell: Automated Macro Construction.
User Modeling 2001, pp. 3443.
Jaeger, M. (1997). Relational Bayesian networks. Proceedings Thirteenth Conference Uncertainty Artificial Intelligence (UAI), pp. 266273. Morgan Kaufmann.
Katz, S. (1987). Estimation probabilities sparse data hte language model component speech recognizer. IEEE Transactions Acoustics, Speech, Signal
Processing (ASSP), 35, 400401.
Kersting, K., & De Raedt, L. (2001a). Adaptive Bayesian Logic Programs. Rouveirol,
C., & Sebag, M. (Eds.), Proceedings 11th International Conference Inductive
Logic Programming (ILP-01), Vol. 2157 LNAI, pp. 118131. Springer.
454

fiLogical Hidden Markov Models

Kersting, K., & De Raedt, L. (2001b). Towards Combining Inductive Logic Programming
Bayesian Networks. Rouveirol, C., & Sebag, M. (Eds.), Proceedings
11th International Conference Inductive Logic Programming (ILP-01), Vol. 2157
LNAI, pp. 118131. Springer.
Kersting, K., & Raiko, T. (2005). Say EM Selecting Probabilistic Models Logical
Sequences. Bacchus, F., & Jaakkola, T. (Eds.), Proceedings 21st Conference
Uncertainty Artificial Intelligence, UAI 2005, pp. 300307 Edinburgh, Scotland.
Kersting, K., Raiko, T., Kramer, S., & De Raedt, L. (2003). Towards discovering structural signatures protein folds based logical hidden markov models. Altman,
R., Dunker, A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings Pacific Symposium Biocomputing (PSB-03), pp. 192203 Kauai, Hawaii, USA. World
Scientific.
Koivisto, M., Kivioja, T., Mannila, H., Rastas, P., & Ukkonen, E. (2004). Hidden Markov
Modelling Techniques Haplotype Analysis. Ben-David, S., Case, J., & Maruoka,
A. (Eds.), Proceedings 15th International Conference Algorithmic Learning Theory (ALT-04), Vol. 3244 LNCS, pp. 3752. Springer.
Koivisto, M., Perola, M., Varilo, T., Hennah, W., Ekelund, J., Lukk, M., Peltonen, L.,
Ukkonen, E., & Mannila, H. (2002). MDL method finding haplotype blocks
estimating strength haplotype block boundaries. Altman, R., Dunker,
A., Hunter, L., Jung, T., & Klein, T. (Eds.), Proceedings Pacific Symposium
Biocomputing (PSB-02), pp. 502513. World Scientific.
Korvemaker, B., & Greiner, R. (2000). Predicting UNIX command files: Adjusting user
patterns. Adaptive User Interfaces: Papers 2000 AAAI Spring Symposium,
pp. 5964.
Kulp, D., Haussler, D., Reese, M., & Eeckman, F. (1996). Generalized Hidden Markov
Model Recognition Human Genes DNA. States, D., Agarwal, P.,
Gaasterland, T., Hunter, L., & Smith, R. (Eds.), Proceedings Fourth International Conference Intelligent Systems Molecular Biology,(ISMB-96), pp. 134
142 St. Louis, MO, USA. AAAI.
Lane, T. (1999). Hidden Markov Models Human/Computer Interface Modeling.
Rudstrom, A. (Ed.), Proceedings IJCAI-99 Workshop Learning Users,
pp. 3544 Stockholm, Sweden.
Lari, K., & Young, S. (1990). estimation stochastic context-free grammars using
inside-outside algorithm. Computer Speech Language, 4, 3556.
Levy, L., & Joshi, A. (1978). Skeletal structural descriptions. Information Control,
2 (2), 192211.
McLachlan, G., & Krishnan, T. (1997). EM Algorithm Extensions. Wiley, New
York.
455

fiKersting, De Raedt, & Raiko

Mitchell, T. M. (1997). Machine Learning. McGraw-Hill Companies, Inc.
Muggleton, S. (1996). Stochastic logic programs. De Raedt, L. (Ed.), Advances
Inductive Logic Programming, pp. 254264. IOS Press.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods.
Journal Logic Programming, 19 (20), 629679.
Ngo, L., & Haddawy, P. (1997). Answering queries context-sensitive probabilistic
knowledge bases. Theoretical Computer Science, 171, 147177.
Pollard, C., & Sag, I. (1994). Head-driven Phrase Structure Grammar. University
Chicago Press, Chicago.
Rabiner, L., & Juang, B. (1986). Introduction Hidden Markov Models. IEEE ASSP
Magazine, 3 (1), 416.
Riezler, S. (1998). Statistical inference probabilistic modelling constraint-based
nlp. Schrder, B., Lenders, W., & und T. Portele, W. H. (Eds.), Proceedings
4th Conference Natural Language Processing (KONVENS-98). CoRR
cs.CL/9905010.
Sakakibara, Y. (1992). Efficient learning context-free grammars positive structural
examples. Information Computation, 97 (1), 2360.
Sakakibara, Y. (2003). Pair hidden markov models tree structures. Bioinformatics,
19 (Suppl.1), i232i240.
Sakakibara, Y., Brown, M., Hughey, R., Mian, I., Sjolander, K., & Underwood, R. (1994).
Stochastic context-free grammars tRNA modelling. Nucleic Acids Research,
22 (23), 51125120.
Sanghai, S., Domingos, P., & Weld, D. (2003). Dynamic probabilistic relational models.
Gottlob, G., & Walsh, T. (Eds.), Proceedings Eighteenth International Joint
Conference Artificial Intelligence (IJCAI-03), pp. 992997 Acapulco, Mexico. Morgan Kaufmann.
Sato, T., & Kameya, Y. (2001). Parameter learning logic programs symbolic-statistical
modeling. Journal Artificial Intelligence Research (JAIR), 15, 391454.
Scholkopf, B., & Warmuth, M. (Eds.). (2003). Learning Parsing Stochastic UnificationBased Grammars, Vol. 2777 LNCS. Springer.
Turcotte, M., Muggleton, S., & Sternberg, M. (2001). effect relational background
knowledge learning protein three-dimensional fold signatures. Machine Learning,
43 (1/2), 8195.
Won, K., Prugel-Bennett, A., & Krogh, A. (2004). Block Hidden Markov Model Biological Sequence Analysis. Negoita, M., Howlett, R., & Jain, L. (Eds.), Proceedings
Eighth International Conference Knowledge-Based Intelligent Information
Engineering Systems (KES-04), Vol. 3213 LNCS, pp. 6470. Springer.

456


