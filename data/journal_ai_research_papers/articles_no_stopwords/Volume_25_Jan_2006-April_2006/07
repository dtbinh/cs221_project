journal artificial intelligence

submitted published

logical hidden markov
kristian kersting
luc de raedt

kersting informatik uni freiburg de
deraedt informatik uni freiburg de

institute computer science
albert ludwigs universitat freiburg
georges koehler allee
freiburg germany

tapani raiko

tapani raiko hut

laboratory computer information science
helsinki university technology
p box
fin hut finland

abstract
logical hidden markov lohmms upgrade traditional hidden markov
deal sequences structured symbols form logical atoms rather flat
characters
note formally introduces lohmms presents solutions three central inference lohmms evaluation likely hidden state sequence parameter estimation resulting representation experimentally evaluated
domain bioinformatics

introduction
hidden markov hmms rabiner juang extremely popular analyzing sequential data application areas include computational biology user modelling
speech recognition empirical natural language processing robotics despite successes hmms major weakness handle sequences flat e unstructured symbols yet many applications symbols occurring sequences structured consider e g sequences unix commands may parameters
emacs lohmms tex ls latex lohmms tex thus commands essentially structured
tasks considered unix command sequences include prediction
next command sequence davison hirsh classification command
sequence user category korvemaker greiner jacobs blockeel
anomaly detection lane traditional hmms cannot easily deal type
structured sequences indeed applying hmms requires ignoring structure
commands e parameters taking possible parameters explicitly
account former serious information loss latter leads
combinatorial explosion number symbols parameters hmm
consequence inhibits generalization
sketched hmms akin dealing structured examples traditional machine learning studied fields inductive logic programming muggleton de raedt multi relational learnc

ai access foundation rights reserved

fikersting de raedt raiko

ing dzeroski lavrac propose inductive logic programming
framework logical hmms lohmms upgrades hmms deal structure
key idea underlying lohmms employ logical atoms structured output state
symbols logical atoms unix command sequence represented
emacs lohmms tex ls latex lohmms tex two important motivations
logical atoms symbol level first variables atoms allow one make
abstraction specific symbols e g logical atom emacs x tex represents files x
latex user tex could edit emacs second unification allows one share information among states e g sequence emacs x tex latex x tex denotes
file used argument emacs latex
organized follows reviewing logical preliminaries introduce
lohmms define semantics section section upgrade basic
hmm inference use lohmms investigate benefits lohmms
section lohmms strictly expressive hmms
design order magnitude smaller corresponding propositional
instantiations unification yield better fit data section
empirically investigate benefits lohmms real world data concluding
discuss related work section proofs theorems found appendix

logical preliminaries
first order alphabet set relation symbols r arity written r
set functor symbols f arity n written f n n f called constant
p called propositional variable assume least one constant
given atom r tn relation symbol r followed bracketed n tuple
terms ti term variable v functor symbol f tk immediately followed
bracketed k tuple terms ti variables written upper case constant functor predicate symbols lower case symbol denote anonymous variables
read treated distinct variables time encountered iterative
clause formula form h b h called head b called body logical
atoms substitution v vn tn e g x tex assignment terms ti
variables vi applying substitution term atom clause e yields instantiated term atom clause e occurrences variables v simultaneously
replaced term ti e g ls x emacs f x x tex yields ls tex emacs f tex
substitution called unifier finite set atoms singleton unifier
called general unifier mgu unifier exists
substitution term atom clause e called ground contains
variables e vars e herbrand base denoted hb set
ground atoms constructed predicate functor symbols set g
atom consists ground atoms belong hb

logical hidden markov
logical component traditional hmm corresponds mealy machine hopcroft
ullman e finite state machine output symbols associated


filogical hidden markov

transitions essentially propositional representation symbols used
represent states output symbols flat e structured key idea underlying
lohmms replace flat symbols abstract symbols abstract symbol
definition logical atom abstract represents set ground e
variable free atoms alphabet denoted g ground atoms play
role traditional symbols used hmms
example consider alphabet constant symbols tex dvi hmm
lohmm relation symbols emacs ls xdvi latex atom
emacs file tex represents set emacs hmm tex emacs lohmm tex assume
alphabet typed avoid useless instantiations emacs tex tex
use atoms instead flat symbols allows us analyze logical structured sequences
emacs hmm tex latex hmm tex xdvi hmm dvi


definition abstract transition expressions form p h
b p
h b atoms variables implicitly assumed universally quantified
e scope variables single abstract transition
atoms h b represent abstract states represents abstract output symbol

semantics abstract transition p h
b one one states
g b say bb one go probability p one states g hb say hb h
emitting symbol g ob h say ob h
latex file

example consider c xdvi file dvi latex file tex general
h b share predicate due nature running example assume state latex hmm tex e
b file hmm c specifies probability next state
g xdvi hmm dvi xdvi hmm dvi e probability
next state xdvi hmm dvi one symbols g latex hmm
latex hmm e latex hmm emitted abstract states might
complex latex file filestem fileextension user
example simple h empty situation becomes complicated substitutions empty resulting
state output symbol sets necessarily singletons indeed transilatex file

tion emacs file dvi latex file tex resulting state set would
g emacs file dvi emacs hmm tex emacs lohmm tex thus transition
non deterministic two possible resulting states therefore need
mechanism assign probabilities possible alternatives
definition selection distribution specifies abstract state observation
symbol alphabet distribution g
continue example let emacs hmm tex emacs file tex
emacs lohmm tex emacs file tex would probability next state emacs hmm tex
emacs lohmm tex


fikersting de raedt raiko



taking account meaning abstract transition p h
b summarized follows let bb g b hb h g hb ob h g ob h
model makes transition state bb hb h emits symbol ob h probability
p hb h hb ob h ob h



represent probabilistic representation principle used e g bayesian
network markov chain throughout remainder present however
use nave bayes precisely associate argument
r
r
relation r finite domain di
constants probability distribution pi

r
di let vars v vl variables occurring atom r
let v vl sl substitution grounding vj considered
r
random variable domain darg vj argument arg vj appears first
q
r
lj parg vj sj e g emacs hmm tex emacs f e computed
emacs

emacs

product p
hmm p
tex
thus far semantics single abstract transition defined lohmm
usually consists multiple abstract transitions creates complication
example consider

emacs file

latex file tex emacs file tex



emacs file

dvi file emacs file user
two abstract transitions make
conflicting statements state resulting emacs hmm tex indeed according
first transition probability resulting state latex hmm tex
according second one assigns xdvi hmm
essentially two ways deal situation one hand one might want
combine normalize two transitions assign probability respectively
hand one might want one rule firing chose
latter option allows us consider transitions independently simplifies
learning yields locally interpretable employ subsumption generality relation among b parts two abstract transitions indeed b part
first transition b emacs file tex specific second transition b emacs file user exists substitution user tex
b b e b subsumes b therefore g b g b first transition
regarded informative second one therefore preferred
second one starting emacs hmm tex say first transition specific second one remark generality relation imposes
partial order set transitions considerations lead strategy
considering maximally specific transitions apply state order determine
successor states implements kind exception handling default reasoning
akin katzs back n gram back n gram
detailed model deemed provide sufficiently reliable information current
context used one encounters n gram sufficiently reliable
back use n gram reliable back level n etc
conflict resolution strategy work properly provided bodies maximally specific transitions matching given state represent abstract state


filogical hidden markov

start

ls




emacs f
ls u

emacs f u
emacs f

ls
emacs f

emacs f u

latex f
emacs f

latex f
latex f tex

emacs f tex
emacs f

latex f

figure logical hidden markov model

enforced requiring generality relation b parts closed
greatest lower bound glb predicate e pair b b bodies
mgu b b exists another body b called lower bound subsumes b
therefore b subsumed b b lower bound
subsumed b e g body second abstract transition example
emacs hmm user set abstract transitions would closed glb
finally order specify prior distribution states assume finite set
clauses form p h start distinguished start symbol p
probability lohmm start state g h
able formally define logical hidden markov
definition logical hidden markov model lohmm tuple
logical alphabet selection probability set abstract transitions
set abstract transitions encoding prior distribution let b set atoms
occur body parts transitions assume b closed glb require
x
b b
p


p h
b

probabilities p clauses sum

hmms special cases lohmms contains relation symbols arity
zero selection probability irrelevant thus lohmms directly generalize hmms
lohmms represented graphically figure contains example underlying language consists together constant symbol denotes
user employ latex graphical notation nodes represent abstract states
black tipped arrows denote abstract transitions white tipped arrows used represent meta knowledge precisely white tipped dashed arrows represent generality
subsumption ordering abstract states follow transition abstract state
outgoing white tipped dotted arrow dotted arrow followed
dotted arrows needed abstract state occur different cirlatex file
cumstances consider transition p latex file user latex file user


fikersting de raedt raiko



start



em f u



em f

state

abstract state

abstract state


ls


ls

em f

em f



la f

la f

abstract state

state

ls u

em f

state abstract state



la f



em f
state



em f u
abstract state

em f u
abstract state

figure generating

observation
sequence
emacs hmm latex hmm
emacs lohmm ls lohmm figure command emacs
abbreviated em f denotes filename hmm f represents lohmm denotes
tex user user white tipped solid arrows indicate selections

even though atoms head body transition syntactically different
represent abstract state accurately represent meaning transition
cannot use black tipped arrow latex file user would actulatex file

ally represent abstract transition p latex file user latex file user
furthermore graphical representation clarifies lohmms generative let us explain model figure would generate observation sequence
emacs hmm latex hmm emacs lohmm ls cf figure chooses initial abstract state say emacs f u since variables f u uninstantiated model
samples state emacs hmm tex g indicated dashed arrow emacs f tex specific emacs f u moreover emacs hmm tex matches
emacs f tex thus model enters emacs f tex since value f already
instantiated previous abstract state emacs hmm tex sampled probability
model goes latex f tex emitting emacs hmm abstract
observation emacs f already fully instantiated since f already instantiated
latex hmm tex sampled probability next move emacs f u emitting latex hmm variables f u emacs f u yet bound values say
lohmm others sampled dotted arrow brings us back emacs f u
variables implicitly universally quantified abstract transitions scope
variables restricted single abstract transitions turn f treated distinct
variable automatically unified f bound lohmm contrast
variable u already instantiated emitting emacs lohmm model makes transition
ls u assume samples tex u remains ls u probability
considering possible samples allows one prove following theorem
theorem semantics logical hidden markov model language defines
discrete time stochastic process e sequence random variables hx
domain
xt hb hb induced probability measure cartesian product
n
hb hb exists unique limit
concluding section let us address design choices underlying lohmms
first lohmms introduced mealy machines e output symbols
associated transitions mealy machines fit logical setting quite intuitively
directly encode conditional probability p making transition


filogical hidden markov

emitting observation logical hidden markov define distribution
x
p
p hb b h

p hb


sum runs abstract transitions h b b specific
observations correspond partially observed proof steps hence provide information
shared among heads bodies abstract transitions contrast hmms usually
introduced moore machines output symbols associated states implicitly
assuming independent thus p factorizes p p
makes difficult observe information shared among heads bodies
turn moore lohmms less intuitive harder understand detailed
discussion issue refer appendix b essentially
propositional case mealy moore lohmms equivalent
second nave bayes selection distribution reduces model complexity expense lower expressivity functors neglected variables
treated independently adapting expressive approaches interesting future line
instance bayesian networks allow one represent factorial hmms ghahramani jordan factorial hmms viewed lohmms hidden
states summarized k ary abstract state first k arguments encode k
state variables last k arguments serve memory previous joint state
th argument conditioned k th argument markov chains allow one
sample compound terms finite depth model e g misspelled
filenames akin generalized hmms kulp haussler reese eeckman
node may output finite sequence symbols rather single symbol
finally lohmms introduced present specify probability distribution sequences given length reconsider lohmm figure already probabilities observation sequences length e ls emacs hmm

p
emacs lohmm sum precisely holds x xt p x
x p
xt p
xt order model distribution sequences variable length
e x xt p x x xt xt may add distinguished end state
end state absorbing whenever model makes transition state
terminates observation sequence generated

three inference lohmms
hmms three inference interest let lohmm let
ot finite sequence ground observations
evaluation determine probability p sequence generated
model
likely state sequence determine hidden state sequence
likely produced observation sequence e arg maxs p
parameter estimation given set ok observation sequences determine likely parameters abstract transitions selection

distribution e arg max p


fipsfrag replacements
kersting de raedt raiko

sc
abstract selection abstract
transition
transition

selection

abstract selection
sc
transition

sc
ls
ls

ls

ls u
ls

hc

ls
ls

ls

hc

ls
ls u

start

hc x



ls u
em f

sc z

em f

em f u

em f

em f

em f

latex f

latex f latex f

em f u



em f u




abstract state







em f

states

figure trellis induced lohmm figure sets reachable states time
denoted contrast hmms additional
layer states sampled abstract states

address turn upgrading existing solutions
hmms realized computing grounded trellis figure possible
ground successor states given state computed first selecting applicable
abstract transitions applying selection probabilities taking account
substitutions ground resulting states two step factorization coalesced
one step hmms
evaluate consider probability partial observation sequence ot
ground state time given model
p ot qt
qt denotes system state time hmms computed dynamic programming set p q
e probability starting state compute

start


st

foreach st






initialize set reachable states
initialize set reachable states clock


foreach maximally specific p h
b b mgu b exists
foreach hb h g hb ot unifies ob h
st
st st


p
p
return p sst





hb ot ob h

filogical hidden markov

assume sake simplicity start abstract transition p h
start furthermore boxed parts specify differences hmm formula
unification taken account
p
clearly hmms p sst holds computational complexity
forward procedure b g maxt st
maximal number outgoing abstract transitions regard abstract state
g maximal number ground instances abstract state completely
analogous manner one devise backward procedure compute
p ot ot ot qt
useful solving
forward procedure straightforward adapt viterbi
solution e computing likely state sequence let
denote highest probability along single path time accounts first
observations ends state e


max

st

p st st ot

procedure finding likely state sequence basically follows forward procedure instead summing ground transition probabilities line maximize
precisely proceed follows
start
initialize set reachable states


st
initialize set reachable states clock
foreach st



foreach maximally specific p h
b b mgu b exists
foreach hb h g hb ot unifies ob h

st


st st



p hb ot ob h

foreach st

maxsst

arg maxsst
stores probability making transition
start states keeps track state maximizing probability along
single path time accounts first observations ends state
likely hidden state sequence computed
st arg max


st



sst
st



one consider abstract level instead considering
contributions different abstract transitions single ground transition state


fikersting de raedt raiko

state line one might consider likely abstract transition
realized replacing line forward procedure
max p hb ot ob h
solves finding likely state abstract transition
sequence
determine sequence states abstract transitions gt
st tt st exists substitutions si
si ti likely produced observation sequence e
gt arg maxgt p gt
thus logical hidden markov pose types inference
parameter estimation estimate maximum likelihood transition
probabilities selection distributions estimate former upgrade well known
baum welch baum estimating maximum likelihood parameters
hmms probabilistic context free grammars
hmms baum welch computes improved estimate p trano
sition probability ground transition p h
b taking ratio
p p


h



b





expected number times making transitions time given
model observation sequence total number times transitions
made b time given
basically applies abstract transition however
little bit careful direct access let gcl
go
probability following abstract transition via ground instance gcl p gh gb
time e
gcl

gb p gh
gh hb ot ob h
p



b h forward procedure see p probability
model generated sequence boxed terms constitute main
difference corresponding hmm formula order apply equation compute
improved estimates probabilities associated abstract transitions set



x




x
x

gcl

gcl

inner sum runs ground instances
leads following estimation method assume sets
reachable states reused computations values


filogical hidden markov











initialization expected counts
foreach
pseudocounts
compute expected counts

foreach st


foreach max specific p h
b b mgu b exists
foreach hb h g hb st mgu ot ob h exists

p p hb ot ob h

equation found line line set pseudocounts small samplesize regularizers methods avoid biased underestimate probabilities even
zero probabilities estimates see e g mitchell easily adapted
estimate selection probabilities recall follows nave bayes scheme therefore estimated probability domain element domain ratio
number times selected number times selected
procedure computing values thus reused
altogether baum welch works follows converged estimate abstract transition probabilities selection probabilities since
instance em increases likelihood data every update
according mclachlan krishnan guaranteed reach stationary
point standard techniques overcome limitations em applicable
computational complexity per iteration k k k
k number sequences complexity computing values see
sum sizes domains associated predicates recently kersting
raiko combined baum welch structure search model
selection logical hidden markov inductive logic programming muggleton
de raedt refinement operators refinement operators account different
abstraction levels explored

advantages lohmms
section investigate benefits lohmms lohmms strictly
expressive hmms abstraction logical variables unification
beneficial specifically
b lohmms design smaller propositional instantiations
b unification yield better log likelihood estimates
expressivity lohmms
whereas hmms specify probability distributions regular languages lohmms specify
probability distributions expressive languages



fikersting de raedt raiko

theorem consistent probabilistic context free grammar pcfg g
language l exists lohmm pg w pm w w l
proof see appendix c makes use abstract states unbounded depth
precisely functors used implement stack without functors lohmms cannot
encode pcfgs herbrand base finite proven
exists equivalent hmm
furthermore functors allowed lohmms strictly expressive pcfgs
specify probability distributions languages context sensitive

stack


stack x x


unstack x x

b

unstack x

c

unstack

end

end

start
stack x x
stack x x
unstack x
unstack
unstack

lohmm defines distribution bn cn n
finally use logical variables enables one deal identifiers identifiers
special types constants denote objects indeed recall unix command
sequence emacs lohmms tex ls latex lohmms tex introduction filename
lohmms tex identifier usually specific identifiers matter rather
fact object occurs multiple times sequence lohmms easily deal
identifiers setting selection probability constant arguments
identifiers occur unification takes care necessary variable bindings
benefits abstraction variables unification
reconsider domain unix command sequences unix users oftenly reuse newly created directory subsequent commands mkdir vt x cd vt x ls vt x
unification allow us elegantly employ information allows us specify observing created directory model makes transition state
newly created directory used
p cd dir mkdir mkdir dir com



p cd mkdir mkdir dir com

first transition followed cd command move newly created directory
second transition followed specified directory cd move thus
lohmm captures reuse created directories argument future commands
moreover lohmm encodes simplest possible case benefits unification time observation sequence uniquely determines state sequence
functors used therefore left abstract output symbols associated
abstract transitions total lohmm u modelling reuse directories consists
parameters still covers ground states see appendix
complete model compression number parameters supports b
empirically investigate benefits unification compare u variant n
u variables shared e unification used instance


filogical hidden markov

first transition allowed see appendix n parameters less u
computed following zero one win function



log pu log pn
f
otherwise
leave one cross validated unix shell logs collected greenberg overall
data consists users four groups computer scientists nonprogrammers novices
others commands logged average sessions
per user present subset data considered computer
scientist sessions least single mkdir command appears yield logical
sequences total ground atoms loo win loo statistics
favor u

u
n

training

log ppu
log p

n




test
log p log ppnu





thus although u parameters n shows better generalization performance supports b pattern often found u
cd dir mkdir mkdir dir com



cd mkdir mkdir dir com

favoring changing directory made knowledge cannot captured n
cd mkdir mkdir dir com
clearly abstraction variables unification beneficial
applications e b b hold

real world applications
intentions investigate whether lohmms applied real world
domains precisely investigate whether benefits b b
exploited real world application domains additionally investigate whether
b lohmms competitive ilp utilize unification
abstraction variables
b lohmms handle tree structured data similar pcfgs
aim conducted experiments two bioinformatics application domains protein
fold recognition kersting raiko kramer de raedt mrna signal structure
detection horvath wrobel bohnebeck application domains multiclass
five different classes
sum probabilities use pseudo counts
subliminal non determinism w r abstract states u e case first
transition fires second one fires



fikersting de raedt raiko

methodology
order tackle multiclass lohmms followed plug estimate
let c c ck set possible classes given finite set training
examples xi yi ni x c c cn one tries f x c c ck
f x arg

max

c c c ck

p x c p c



low approximation error training data well unseen examples
equation denotes model structure classes c denotes
maximum likelihood parameters class c estimated training examples
yi c p c prior class distribution
implemented baum welch pseudocounts see line maximum likelihood parameter estimation prolog system yap experiments
set let baum welch stop change log likelihood
less one iteration next experiments ran pentium iv
ghz linux machine
protein fold recognition
protein fold recognition concerned proteins fold nature e threedimensional structures important biological functions proteins
depend way fold common use database searches proteins known fold similar newly discovered protein unknown fold facilitate
protein fold recognition several expert classification schemes proteins
developed group current set known protein structures according similarity
folds instance structural classification proteins hubbard murzin brenner chotia scop database hierarchically organizes proteins according
structures evolutionary origin machine learning perspective scop induces
classification given protein unknown fold assign best matching group
classification scheme protein fold classification investigated
turcotte muggleton sternberg inductive logic programming
ilp system progol kersting et al lohmms
secondary structure protein domains elegantly represented logical sequences example secondary structure ribosomal protein l represented
st null right alpha st plus right alpha st plus
right alpha st plus right alpha st plus hright alpha
helices certain type orientation length helixtype helixorientation length
strands certain orientation length st strandorientation length atoms
logical predicates application traditional hmms sequences requires one
ignore structure helices strands loss information
take possible combinations arguments orientation length account
leads combinatorial explosion number parameters
domain viewed sub section protein appears number distantly related
proteins fold independently rest protein



filogical hidden markov

end
block b length

block b length

dynamics within block

dynamics within block

block b p

block b p

block b p

block b p
transition next block

transition next block

block b

block b

block b

block b

figure scheme left right lohmm block model
reported kersting et al indicate lohmms well suited
protein fold classification number parameters lohmm order
magnitude smaller number corresponding hmm versus approximately
generalization performance accuracy comparable turcotte
et al ilp system progol accuracy kersting et al
however cross validate investigate common
bioinformatics impact primary sequence similarity classification accuracy
instance two commonly requested astral subsets subset sequences
less identity cut less identity
cut motivated conducted following experiments
data consists logical sequences secondary structure protein domains
work kersting et al task predict one five populated
scop folds alpha beta proteins b tim beta alpha barrel fold nad p binding rossmann fold domains fold ribosomal protein l fold cysteine hydrolase
fold phosphotyrosine protein phosphatases fold class b
proteins consists proteins mainly parallel beta sheets beta alpha beta units
data extracted automatically astral dataset version chandonia
hon walker lo conte p koehl brenner cut cut
work kersting et al consider strands helices e coils
isolated strands discarded cut yields logical sequences consisting
total ground atoms number sequences classes listed
cut yields logical sequences consisting total
ground atoms number sequences classes listed

lohmm structure used lohmm structure follows left right block topology
see figure model blocks consecutive helices resp strands block
size say model remain block time steps similar
idea used model haplotypes koivisto perola varilo hennah ekelund lukk
peltonen ukkonen mannila koivisto kivioja mannila rastas ukkonen
contrast common hmm block prugel bennett krogh


fikersting de raedt raiko

transition parameters shared within block one ensure model
makes transition next state block end block example
exactly intra block transitions furthermore specific abstract transitions
helix types strand orientations model priori distribution intra
inter block transitions number blocks sizes chosen according
empirical distribution sequence lengths data beginning
ending protein domains likely captured detail yield following block
structure


















numbers denote positions within protein domains furthermore note
last block gathers remaining transitions blocks modelled
hidden abstract states
hc helixtype helixorientation length block sc strandorientation length block
length denotes number consecutive bases structure element consists
length discretized bins original lengths uniformally
distributed total lohmm parameters corresponding hmm without
parameter sharing parameters clearly confirms b
performed fold cross validation cut dataset accuracy
took approx minutes per cross validation iteration cut accuracy
took approx minutes per cross validation iteration validate
kersting et al turn clearly b holds moreover
novel cut dataset indicate similarities detected lohmms
protein domain structures accompanied high sequence similarity
mrna signal structure detection
mrna sequences consist bases guanine adenine uracil cytosine fold intramolecularly form number short base paired stems durbin eddy krogh mitchison
base paired structure called secondary structure cf figures
secondary structure contains special subsequences called signal structures responsible special biological functions rna protein interactions cellular transport
function signal structure class common characteristic binding
site class elements elements necessarily identical similar
vary topology tree structure size number constituting bases base
sequence
goal experiments recognize instances signal structures classes
mrna molecules first application relational learning recognize signal structure class mrna molecules described works bohnebeck horvath
wrobel horvath et al relational instance learner
ribl applied dataset used similar one described horvath
dataset described work horvath et al could obtain
original dataset compare smaller data set used horvath et al consisted



filogical hidden markov

et al consisted mrna secondary structure sequences precisely
composed secis selenocysteine insertion sequence ire iron responsive
element tar trans activating region histone stem loops constituting five
classes
secondary structure composed different building blocks stacking region
hairpin loops interior loops etc contrast secondary structure proteins forms
chains secondary structure mrna forms tree trees easily handled
hmms mrna secondary structure data challenging proteins
moreover horvath et al report making tree structure available ribl
background knowledge influence classification accuracy precisely
simple chain representation ribl achieved leave one cross validation
loo accuracy whereas tree structure background knowledge ribl achieved
loo accuracy
followed horvath et al experimental setup adapted data representations lohmms compared chain model tree model

chain representation chain representation see figure
signal
structures

described

single typesingle position acid

helical typehelical position acid acid
depending type structure element represented single helical
first argument
typesingle resp
typehelical specifies type structure element e
single bulge bulge hairpin resp stem argument position position sequence element within corresponding structure element counted
e n n n maximal position set
maximal position observed data last argument encodes observed nucleotide
pair
used lohmm structure follows left right block structure shown
figure underlying idea model blocks consecutive helical structure elements hidden states modelled single typesingle position acid block
helical typehelical position acid acid block block consecutive helical resp single structure elements model remain block transition
single element transition single resp helical element occurs position
n positions n position transitions helical resp single
structure elements helical resp single structure elements position capturing dynamics nucleotide pairs resp nucleotides within structure elements instance

signal structures close data set larger data set structures horvath
et al report error rate
nm shorthand recursive application functor n times e position



fikersting de raedt raiko

helical stem n c g
helical stem n n c g
helical stem n n n c g
single bulge n
single bulge n n
single bulge n n n g
helical stem n c g
helical stem n n c g
single bulge n
helical stem n
helical stem n n u
helical stem n n n u g
helical stem n n n n u
helical stem n n n n n c
helical stem n n n n n n u
helical stem n n n n n n n u

u


u
c
c
c

g
g
g



g


c
c

single hairpin n n n
single hairpin n n u
single hairpin n u

single bulge n

g
g



u
u
u
c
u




g



u

figure chain representation secis signal structure ground atoms
ordered clockwise starting helical stem n n n n n n n u
lower left hand side corner

transitions block n position n n
pa stem n x

stem n x n stem n n x n
pb stem n x

b

stem n x n stem n n x n

c

stem n x n stem n n x n



stem n n stem n n x n

e

stem n n stem n n x n

pc stem n x

pd stem n x
pe stem n x

total possible blocks maximal number blocks consecutive
helical structure elements observed data overall lohmm parameters
contrast corresponding hmm transitions validating b
loo test log likelihood em iteration took average
seconds
without unification transitions b e abstract transitions
pa stem n x

stem n x n stem n n x n
e

pe stem n x

stem n n stem n n x n

model parameters loo test log likelihood em iteration took average seconds difference loo test log likelihood statistically
significant paired test p
omitting even transition loo test log likelihood dropped
average time per em iteration seconds model parameters
difference average loo log likelihood statistically significant paired test p
clearly unification yield better loo test log likelihoods e
b holds


filogical hidden markov

nucleotide pair c g
nucleotide pair c g
nucleotide pair c g
helical c stem n n n
nucleotide
nucleotide
nucleotide g
single bulge n n n
nucleotide pair c g
nucleotide pair c g
helical c c c stem n n
nucleotide
single bulge n
nucleotide pair
nucleotide pair u
nucleotide pair u g
nucleotide pair u
nucleotide pair c
nucleotide pair u
nucleotide pair u
helical c c stem n n n n n n n

u


u
c
c
c

g
g
g



g


c
c

g
g

single
hairpin n n n
nucleotide
nucleotide u
nucleotide u

single
bulge n
nucleotide



u
u
u
c
u




g



u









root root c

figure tree representation secis signal structure logical sequence
e sequence ground atoms representing secis signal structure
ground atoms ordered clockwise starting root root c lower
left hand side corner b tree formed secondary structure elements

tree representation tree representation see figure idea capture
tree structure formed secondary structure elements see figure b
training instance described sequence ground facts
root root children
helical id parentid children type size
nucleotide pair basepair
single id parentid children type size
nucleotide base
id parentid natural numbers encoding childparent relation children denotes number children c c c type
type structure element stem hairpin size natural number
n n n atoms root root children used root topology
maximal children maximal size maximal value
observed data
trees easily handled hmms used lohmm basically
encodes pcfg due theorem possible used lohmm structure
found appendix e processes mrna trees order unification used
parsing tree chain representation used position argument hidden
states encode dynamics nucleotides nucleotide pairs within secondary structure
use prolog short hand notation lists list constant representing
empty list compound term functor two arguments respectively head
tail list thus b c compound term b c



fikersting de raedt raiko

elements maximal position contrast chain representation
nucleotide pairs u treated constants thus argument basepair
consists elements
loo test log likelihood thus exploiting tree structure
yields better probabilistic average em iteration took seconds overall
shows b holds
although baum welch attempts maximize different objective function namely likelihood data interesting compare lohmms ribl
terms classification accuracy
classification accuracy chain representation loo accuracies
lohmms considerable improvement ribls
loo accuracy representation tree representation lohmm
achieved loo accuracy comparable ribls loo accuracy
kind representation
thus already chain lohmms marked increases loo accuracy compared ribl horvath et al order achieve similar loo accuracies horvath
et al make tree structure available ribl background knowledge
lohmms significant influence loo test log likelihood
loo accuracies clearly supports b moreover according horvath et al
mrna application considered success terms application domain
although primary goal experiments exist alternative
parameter estimation techniques covariance eddy
durbin pair hidden markov sakakibara might
used well basis comparison however lohmms employ inductive logic programming principles appropriate compare systems within paradigm
ribl

related work
lohmms combine two different directions one hand related
several extensions hmms probabilistic grammars hand
related recent interest combining inductive logic programming principles
probability theory de raedt kersting
first type approaches underlying idea upgrade hmms probabilistic
grammars represent structured state spaces
hierarchical hmms fine singer tishby factorial hmms ghahramani
jordan hmms tree automata frasconi soda vullo decompose state variables smaller units hierarchical hmms states
hmms factorial hmms factored k state variables depend one
another observation tree hmms represented probability
distributions defined tree structures key difference lohmms
approaches employ logical concept unification unification essential


filogical hidden markov

allows us introduce abstract transitions consist detailed
states experimental evidence shows sharing information among abstract states
means unification lead accurate model estimation holds relational markov rmms anderson domingos weld lohmms
closely related rmms states different types type described
different set variables domain variable hierarchically structured
main differences lohmms rmms rmms support
variable binding unification hidden states
equivalent hmms context free languages probabilistic context free grammars pcfgs hmms consider sequences logical atoms
employ unification nevertheless formal resemblance baum welch
lohmms pcfgs case lohmm encodes pcfg
identical theoretical point view estimate parameters
ratio expected number times transition resp production used
expected number times transition resp production might used proof
theorem assumes pcfg given greibach normal form gnf uses
pushdown automaton parse sentences grammars gnf pushdown automata
common parsing contrast actual computations baum welch
pcfgs called inside outside baker lari young
usually formulated grammars chomsky normal form inside outside
make use efficient cyk hopcroft ullman parsing strings
alternative learning pcfgs strings learn structured data
skeletons derivation trees nonterminal nodes removed levy
joshi skeletons exactly set trees accepted skeletal tree automata sta
informally sta given tree input processes tree bottom assigning
state node states nodes children sta accepts tree iff
assigns final state root tree due automata characterization
skeletons derivation trees learning p cfgs reduced
sta particular sta techniques adapted learning tree
grammars p cfgs sakakibara sakakibara et al efficiently
pcfgs extended several ways closely related lohmms
unification grammars extensively studied computational linguistics examples stochastic attribute value grammars abney probabilistic feature grammars goodman head driven phrase structure grammars pollard sag
lexical functional grammars bresnan learning within frameworks methods undirected graphical used see work johnson
description recent work key difference lohmms nonterminals replaced structured complex entities thus observation sequences
flat symbols atoms modelled goodmans probabilistic feature grammars
exception treat terminals nonterminals vectors features abstraction
made e feature vectors ground instances unification employed
grammar gnf iff productions form av variable exactly one
terminal v string none variables
grammar cnf iff every production form b c b c variables
terminal



fikersting de raedt raiko

con

mkdir
con
mkdir

mv

ls

cd

mv
con

vt x

vt x

ls


vt x

vt x

vt x





cd

vt x

b

vt x

vt x

figure atom logical sequence mkdir vt x mv vt x
ls vt x cd vt x forms tree shaded nodes denote shared labels
among trees b sequence represented single tree predicate con represents concatenation operator

therefore number parameters needs estimated becomes easily large
data sparsity serious goodman applied smoothing overcome
lohmms generally related stochastic tree automata see e g carrasco oncina calera rubio reconsider unix command sequence
mkdir vt x mv vt x ls vt x cd vt x atom forms tree see
figure indeed whole sequence atoms forms degenerated tree
see figure b tree automata process single trees vertically e g bottom state
automaton assigned every node tree state depends node label
states associated siblings node focus sequential
domains contrast lohmms intended learning sequential domains
process sequences trees horizontally e left right furthermore unification
used share information consecutive sequence elements figure b
illustrates tree automata employ information allowing higher order
transitions e states depend node labels states associated
predecessors levels tree
second type approaches attention devoted developing highly
expressive formalisms e g pcup eisele pclp riezler slps muggleton plps ngo haddawy rbns jaeger prms friedman
getoor koller pfeffer prism sato kameya blps kersting de
raedt b dprms sanghai domingos weld lohmms
seen attempt towards downgrading highly expressive frameworks indeed applying main idea underlying lohmms non regular probabilistic grammar e replacing
flat symbols atoms yields principle stochastic logic programs muggleton
consequence lohmms represent interesting position expressiveness scale
whereas retain essential logical features expressive formalisms
seem easier understand adapt learn akin many contemporary consid

filogical hidden markov

erations inductive logic programming muggleton de raedt multi relational
data mining dzeroski lavrac

conclusions
logical hidden markov formalism representing probability distributions
sequences logical atoms introduced solutions three central
inference evaluation likely state sequence parameter estimation
provided experiments demonstrated unification improve generalization
accuracy number parameters lohmm order magnitude smaller
number parameters corresponding hmm solutions presented
perform well practice lohmms possess several advantages traditional
hmms applications involving structured sequences
acknowledgments authors thank andreas karwath johannes horstmann
interesting collaborations protein data ingo thon interesting collaboration
analyzing unix command sequences saul greenberg providing unix command sequence data authors would thank anonymous reviewers comments considerably improved partly supported
european union ist programme contract numbers ist fp
application probabilistic inductive logic programming april ii tapani raiko
supported marie curie fellowship daisy hpmt ct

appendix proof theorem
let lohmm specifies time discrete stochastic
process e sequence random variables hxt domains random
variable xt hb herbrand base define immediate state operator
tm operator current emission operator em operator
definition tm operator em operator operators tm hb hb em
hb hb


tm hb h p h
b bb hb h g h


em ob h p h
b bb hb g g h
ob h g

start
set tm
start tm tm

tm start tm start specifies state set clock forms random varii start specifies possible symbols emitted transitioning
able yi set um
forms variable ui yi resp ui extended random
variable zi resp ui hb

p zi z



start
z tm
p yi z otherwise



fikersting de raedt raiko

psfrag replacements

z

z

z

u

u



u

figure discrete time stochastic process induced lohmm nodes z ui
represent random variables hb

figure depicts influence relation among zi ui standard arguments
probability theory noting
p ui ui zi zi zi zi
p zi zi

x

p zi ui zi

p zi zi ui ui zi
p
ui p zi ui zi

ui

probability distributions due equation easy kolmogorovs extension theorem see bauer fristedt
gray holds thus
nt
specifies unique probability distribution
z
ui


limit


appendix b moore representations lohmms
hmms moore representations e output symbols associated states mealy
representations e output symbols associated transitions equivalent
appendix investigate extend holds lohmms
let l mealy lohmm according definition following derive
notation equivalent lohmm l moore representation abstract
transitions abstract emissions see predicate b n l extended b n
l domains first n arguments b n last argument
store observation emitted precisely abstract transition
v vk

p h w wl b u un
l abstract transition
p h w wl v v k b u un
l primes v v k denote replaced free variables v vk
distinguished constant symbol say due holds
h w wl h w wl v v k
variable x vars v vk free iff x vars h w wl vars b u un





filogical hidden markov

l output distribution specified abstract emissions expressions
form
v vk h w wl v v k

semantics abstract transition l
state g b u un system make transition
g h w wl v v k probability
p h w wl v v k


state



mgu b u un due equation equation rewritten

p h w wl
due equation system emit output symbol ot g v vk
state probability
ot v vk
mgu h w wl v v k due construction l
exists triple st st ot l triple ot l vise
versa hence lohmms assign overall transition probability
l l differ way initialize sequences h ot resp
h st st ot whereas l starts state makes transition
emitting moore lohmm l supposed emit symbol making
transition compensate prior distribution existence
correct prior distribution l seen follows l finitely many
states reachable time e pl q holds finite set ground
states probability pl q computed similar set line
neglecting condition ot line dropping ot ob h line
completely listing states together pl q e pl q start
constitutes prior distribution l
argumentation basically followed transform mealy machine
moore machine see e g hopcroft ullman furthermore mapping
moore lohmm introduced present section mealy lohmm straightforward

appendix c proof theorem
let terminal alphabet n nonterminal alphabet probabilistic context free
grammar pcfg g consists distinguished start symbol n plus finite set

productions
p form p x x n n p
x n x p pcfg defines stochastic process sentential forms states
leftmost rewriting steps transitions denote single rewriting operation
grammar single arrow one ore rewriting operations
able rewrite n sequence n nonterminals terminals
write probability rewriting product probability


fikersting de raedt raiko

values associated productions used derivation assume g consistent e
sum probabilities derivations sum
assume pcfg g greibach normal form follows abney
et al theorem g consistent thus every production p g
form p x ay yn n order encode g lohmm
introduce non terminal symbol x g constant symbol nx
terminal symbol g constant symbol production p g include

abstract transition form p stack ny nyn
stack nx n

p stack
stack nx n furthermore include stack start
end
end stack straightforward prove induction g
equivalent


appendix logical hidden markov model unix command
sequences
lohmms described model unix command sequences triggered mkdir
aim transformed original greenberg data sequence logical atoms
com mkdir dir lastcom ls dir lastcom cd dir dir lastcom cp dir dir lastcom
mv dir dir lastcom domain lastcom start com mkdir ls cd cp mv
domain dir consisted argument entries mkdir ls cd cp mv original
dataset switches pipes etc neglected paths made absolute yields
constants domain dir original commands mkdir ls cd
cp mv represented com mkdir appear within time steps
command c ls cd cp mv c represented com overall yields
ground states covered markov model
unification lohmm u basically implements second order markov model e
probability making transition depends upon current state previous
state parameters following structure
com start
mkdir dir start start

com com
mkdir dir com com
end com

furthermore c start com
mkdir dir com
mkdir com
com
end
ls dir mkdir
ls mkdir
cd dir mkdir









mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c

cd mkdir
cp dir mkdir
cp dir mkdir
cp mkdir
mv dir mkdir
mv dir mkdir
mv mkdir











mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c
mkdir dir c

filogical hidden markov

together c mkdir ls cd cp mv c cd ls resp
c cp mv
mkdir dir com
mkdir com
com
end
ls dir c
ls c
cd dir c
cd c
cp dir c
cp dir c
cp c
mv dir c
mv dir c
mv c
















c dir c mkdir com
com
c dir c
c dir c
end
ls c
c dir c
ls c
c dir c
c dir c
ls c
c dir c
cd c
c dir c
cd c
c dir c
cd c
c dir c cp c
c dir c
cp c
c dir c
cp c
c dir c mv c
mv c
c dir c
mv c

c c
c c
c c
c c
c c
c c
c c
c c
c c
c c
c c
c c
c c
c c
c c

states fully observable omitted output symbols associated
clauses sake simplicity omitted associated probability values
unification lohmm n variant u variables shared

mkdir com cp c
ls
com cp c
cd
end cp c cp
mv

cp
cp
cp
cp






cp c
cp c
cp c
cp c

transitions affected n parameters less u e

appendix e tree lohmm mrna sequences
lohmm processes nodes mrna trees order structure lohmm
shown end section copies shaded parts terms
abbreviated starting alphanumerical tr stands tree helical si
single nuc nucleotide nuc p nucleotide pair
domain children covers maximal branching factor found data e
c c c c c c c c c c c c domain type consists types occurring
data e stem single bulge bulge hairpin size domain covers
maximal length secondary structure element data e longest sequence
consecutive bases respectively base pairs constituting secondary structure element
length encoded n n n nm denotes recursive
application functor n times base basepair domains bases
respectively base pairs total parameters


fimy start
root root x

copies tr id c pa c r tr id c c pa c r
tr id c c c pa c r
id pa l
tr id pa c r

tr x x

tr id c c c pa c c cs r

id pa b l

id pa l
id pa b l
tr id pa c c cs r
si id pa b l
si id pa l

si id pa b l
si id pa l
se l id b id b r

se l id r

copies tr id c pa c c cs r tr id c c pa c c cs r

tree
model

se l id pa c cs r se l id b id b pa c cs r

copies type single bulge bulge
copies n n n n n

copies length sequence n n n n n n n n n
se stem n id b

se hairpin n id b

copies nuc p g nuc p u u

copies nuc g nuc c nuc u
nuc

nuc p

se hairpin id b

se hairpin n id b

se stem id b

se hairpin n

nuc

se stem n

nuc p

nuc

se stem n id b

nuc p

copies nuc p g nuc p u u

copies nuc g nuc c nuc u
end

tr id b

sequence
model

kersting de raedt raiko

figure mrna lohmm structure symbol denotes anonymous variables
read treated distinct variables time encountered
copies shaded part terms abbreviated starting
alphanumerical tr stands tree se structure element helical
si single nuc nucleotide nuc p nucleotide pair

references



abney stochastic attribute value grammars computational linguistics




start

filogical hidden markov

abney mcallester pereira f relating probabilistic grammars automata proceedings th annual meeting association computational
linguistics acl pp morgan kaufmann
anderson c domingos p weld relational markov application adaptive web navigation proceedings eighth international
conference knowledge discovery data mining kdd pp edmonton canada acm press
baker j trainable grammars speech recognition speech communication
presented th th meeting acoustical society america pp
boston
bauer h wahrscheinlichkeitstheorie edition walter de gruyter berlin
york
baum l inequality associated maximization technique statistical estimation probabilistic functions markov processes inequalities
bohnebeck u horvath wrobel term comparison first order similarity
measures proceedings eigth international conference inductive logic
programming ilp vol lncs pp springer
bresnan j lexical functional syntax blackwell malden
carrasco r oncina j calera rubio j stochastic inference regular tree
languages machine learning
chandonia j hon g walker n lo conte l p koehl brenner
astral compendium nucleic acids
davison b hirsh h predicting sequences user actions predicting
future ai approaches time series analysis pp aaai press
de raedt l kersting k probabilistic logic learning acm sigkdd explorations special issue multi relational data mining
de raedt l kersting k probabilistic inductive logic programming
ben david case j maruoka eds proceedings th international
conference algorithmic learning theory alt vol lncs pp
padova italy springer
durbin r eddy krogh mitchison g biological sequence analysis
probabilistic proteins nucleic acids cambridge university press
dzeroski lavrac n eds relational data mining springer verlag berlin
eddy durbin r rna sequence analysis covariance nucleic
acids res


fikersting de raedt raiko

eisele towards probabilistic extensions contraint grammars
dorne j ed computational aspects constraint linguistics decription ii
dyna deliverable r b
fine singer tishby n hierarchical hidden markov model analysis
applications machine learning
frasconi p soda g vullo hidden markov text categorization
multi page documents journal intelligent information systems
friedman n getoor l koller pfeffer learning probabilistic relational
proceedings sixteenth international joint conference artificial intelligence ijcai pp morgan kaufmann
fristedt b gray l modern probability theory probability
applications birkhauser boston
ghahramani z jordan factorial hidden markov machine learning

goodman j probabilistic feature grammars proceedings fifth international workshop parsing technologies iwpt boston usa
greenberg unix collected traces users tech rep dept
computer science university calgary alberta
hopcroft j ullman j introduction automata theory languages
computation addison wesley publishing company
horvath wrobel bohnebeck u relational instance learning
lists terms machine learning
hubbard murzin brenner chotia c scop structural classification
proteins database nar
jacobs n blockeel h learning shell automated macro construction
user modeling pp
jaeger relational bayesian networks proceedings thirteenth conference uncertainty artificial intelligence uai pp morgan kaufmann
katz estimation probabilities sparse data hte language model component speech recognizer ieee transactions acoustics speech signal
processing assp
kersting k de raedt l adaptive bayesian logic programs rouveirol
c sebag eds proceedings th international conference inductive
logic programming ilp vol lnai pp springer


filogical hidden markov

kersting k de raedt l b towards combining inductive logic programming
bayesian networks rouveirol c sebag eds proceedings
th international conference inductive logic programming ilp vol
lnai pp springer
kersting k raiko say em selecting probabilistic logical
sequences bacchus f jaakkola eds proceedings st conference
uncertainty artificial intelligence uai pp edinburgh scotland
kersting k raiko kramer de raedt l towards discovering structural signatures protein folds logical hidden markov altman
r dunker hunter l jung klein eds proceedings pacific symposium biocomputing psb pp kauai hawaii usa world
scientific
koivisto kivioja mannila h rastas p ukkonen e hidden markov
modelling techniques haplotype analysis ben david case j maruoka
eds proceedings th international conference algorithmic learning theory alt vol lncs pp springer
koivisto perola varilo hennah w ekelund j lukk peltonen l
ukkonen e mannila h mdl method finding haplotype blocks
estimating strength haplotype block boundaries altman r dunker
hunter l jung klein eds proceedings pacific symposium
biocomputing psb pp world scientific
korvemaker b greiner r predicting unix command files adjusting user
patterns adaptive user interfaces papers aaai spring symposium
pp
kulp haussler reese eeckman f generalized hidden markov
model recognition human genes dna states agarwal p
gaasterland hunter l smith r eds proceedings fourth international conference intelligent systems molecular biology ismb pp
st louis mo usa aaai
lane hidden markov human computer interface modeling
rudstrom ed proceedings ijcai workshop learning users
pp stockholm sweden
lari k young estimation stochastic context free grammars
inside outside computer speech language
levy l joshi skeletal structural descriptions information control

mclachlan g krishnan em extensions wiley
york


fikersting de raedt raiko

mitchell machine learning mcgraw hill companies inc
muggleton stochastic logic programs de raedt l ed advances
inductive logic programming pp ios press
muggleton de raedt l inductive logic programming theory methods
journal logic programming
ngo l haddawy p answering queries context sensitive probabilistic
knowledge bases theoretical computer science
pollard c sag head driven phrase structure grammar university
chicago press chicago
rabiner l juang b introduction hidden markov ieee assp
magazine
riezler statistical inference probabilistic modelling constraint
nlp schrder b lenders w und portele w h eds proceedings
th conference natural language processing konvens corr
cs cl
sakakibara efficient learning context free grammars positive structural
examples information computation
sakakibara pair hidden markov tree structures bioinformatics
suppl
sakakibara brown hughey r mian sjolander k underwood r
stochastic context free grammars trna modelling nucleic acids

sanghai domingos p weld dynamic probabilistic relational
gottlob g walsh eds proceedings eighteenth international joint
conference artificial intelligence ijcai pp acapulco mexico morgan kaufmann
sato kameya parameter learning logic programs symbolic statistical
modeling journal artificial intelligence jair
scholkopf b warmuth eds learning parsing stochastic unificationbased grammars vol lncs springer
turcotte muggleton sternberg effect relational background
knowledge learning protein three dimensional fold signatures machine learning

k prugel bennett krogh block hidden markov model biological sequence analysis negoita howlett r jain l eds proceedings
eighth international conference knowledge intelligent information
engineering systems kes vol lncs pp springer




