Journal Artificial Intelligence Research 17 (2002) 171-228

Submitted 3/02; published 9/02

Towards Adjustable Autonomy Real World
scerri@isi.edu
pynadath@isi.edu
tambe@usc.edu

Paul Scerri
David V. Pynadath
Milind Tambe
Information Sciences Institute Computer Science Department
University Southern California
4676 Admiralty Way, Marina del Rey, CA 90292 USA

Abstract

Adjustable autonomy refers entities dynamically varying autonomy, transferring decision-making control entities (typically agents transferring control
human users) key situations. Determining whether transfers-of-control
occur arguably fundamental research problem adjustable autonomy. Previous work investigated various approaches addressing problem often
focused individual agent-human interactions. Unfortunately, domains requiring collaboration teams agents humans reveal two key shortcomings previous
approaches. First, approaches use rigid one-shot transfers control result
unacceptable coordination failures multiagent settings. Second, ignore costs (e.g.,
terms time delays effects actions) agent's team due transfers-ofcontrol.
remedy problems, article presents novel approach adjustable autonomy, based notion transfer-of-control strategy. transfer-of-control strategy
consists conditional sequence two types actions: (i) actions transfer decisionmaking control (e.g., agent user vice versa) (ii) actions change
agent's pre-specified coordination constraints team members, aimed minimizing
miscoordination costs. goal high-quality individual decisions made
minimal disruption coordination team. present mathematical model
transfer-of-control strategies. model guides informs operationalization
strategies using Markov Decision Processes, select optimal strategy, given
uncertain environment costs individuals teams. approach
carefully evaluated, including via use real-world, deployed multi-agent system
assists research group daily activities.
1.

Introduction

Exciting, emerging application areas ranging intelligent homes (Lesser et al., 1999),
routine organizational coordination (Pynadath et al., 2000), electronic commerce (Collins
et al., 2000a), long-term space missions (Dorais et al., 1998) utilize decision-making
skills agents humans. new applications brought forth increasing
interest agents' adjustable autonomy (AA), i.e., entities dynamically adjusting
level autonomy based situation (Mulsiner & Pell, 1999). Many exciting
applications deployed unless reliable AA reasoning central component.
AA, entity need make decisions autonomously; rather choose reduce
autonomy transfer decision-making control users agents,
c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiScerri, Pynadath & Tambe
expected net benefit (Dorais et al., 1998; Barber, Goel, & Martin, 2000a;
Hexmoor & Kortenkamp, 2000).
central problem AA determine whether transfers decision-making
control occur. key challenge balance two potentially con icting goals.
one hand, ensure highest-quality decisions made, agent transfer control human user (or another agent) whenever user superior decision-making
expertise.1 hand, interrupting user high costs user may
unable make communicate decision, thus transfers-of-control minimized. Previous work examined several different techniques attempt balance
two con icting goals thus address transfer-of-control problem. example,
one technique suggests decision-making control transferred expected
utility higher expected utility making autonomous decision
(Horvitz, Jacobs, & Hovel, 1999). second technique uses uncertainty sole rationale
deciding control, forcing agent relinquish control user
whenever uncertainty high (Gunderson & Martin, 1999). Yet techniques transfer
control user erroneous autonomous decision could cause significant harm (Dorais
et al., 1998) agent lacks capability make decision (Ferguson, Allen, &
Miller, 1996).
Unfortunately, previous approaches transfer-of-control reasoning indeed
previous work AA, focused domains involving single agent single
user, isolated interactions entities. applied interacting teams
agents humans, interaction agent human impacts interaction entities, techniques lead dramatic failures. particular,
presence entities team members introduces third goal maintaining coordination (in addition two goals already mentioned above), previous
techniques fail address. Failures occur two reasons. Firstly, previous techniques
ignore team related factors, costs team due incorrect decisions due
delays decisions transfers-of-control. Secondly (and importantly),
techniques use one-shot transfers-of-control, rigidly committing one two choices: (i)
transfer control wait input (choice H ) (ii) act autonomously (choice A). However,
given interacting teams agents humans, either choice lead costly failures
entity control fails make report decision way maintains coordination.
instance, human user might unable provide required input due temporary communication failure; may cause agent fail part joint action,
joint action may dependent user's input. hand, forcing
less capable entity make decision simply avoid miscoordination lead poor
decisions significant consequences. Indeed, seen Section 2.2, applied
rigid transfer-of-control decision-making domain involving teams agents users,
failed dramatically.
Yet, many emerging applications involve multiple agents multiple humans acting
cooperatively towards joint goals. address shortcomings previous AA work
domains, article introduces notion transfer-of-control strategy. transfer-ofcontrol strategy consists pre-defined, conditional sequence two types actions: (i)
1. AA problem general involves transferring control one entity another, paper,
typically focus interactions involving autonomous agents human users.

172

fiTowards Adjustable Autonomy Real World
actions transfer decision-making control (e.g., agent user vice versa);
(ii) actions change agent's pre-specified coordination constraints team members,
rearranging activities needed (e.g., reordering tasks buy time make decision).
agent executes strategy performing actions order, transferring control
specified entity changing coordination required, point time
entity currently control exercises control makes decision. Thus, previous
choices H two many different possibly complex transfer-ofcontrol strategies. instance, ADAH strategy implies agent initially attempts
make autonomous decision. agent makes decision autonomously strategy
execution ends there. However, chance unable make decision
timely manner, perhaps computational resources busy higher
priority tasks. avoid miscoordination agent executes action changes
coordination constraints activity. example, action could inform
agents coordinated action delayed, thus incurring cost inconvenience
others buying time make decision. still cannot make decision,
eventually take action H , transferring decision-making control user waiting
response. general, strategies involve available entities contain many
actions change coordination constraints. strategies may useful singleagent single-human settings, particularly critical general multiagent settings,
discussed below.
Transfer-of-control strategies provide exible approach AA complex systems
many actors. enabling multiple transfers-of-control two (or more) entities,
rather rigidly committing one entity (i.e., H ), strategy attempts provide
highest quality decision, avoiding coordination failures. particular, multiagent
setting often uncertainty whether entity make decision
so, e.g., user may fail respond, agent may able make decision
expected communication channel may fail. strategy addresses uncertainty
planning multiple transfers control cover contingencies. instance,
ADH strategy, agent ultimately transfers control human attempt ensure
response provided case agent unable act. Furthermore, explicit
coordination-change actions, i.e., actions, reduce miscoordination effects, cost,
better decisions made. Finally, since utility transferring control changing
coordination dependent actions taken afterwards, agent must plan strategy
advance find sequence actions maximizes team benefits. example,
reacting current situation repeatedly taking giving control strategy
ADHADH : : : may costly planning ahead, making bigger coordination
change, using shorter ADH strategy. developed decision theoretic model
strategies, allows expected utility strategy calculated and, hence,
strategies compared.
Thus, key AA problem select right strategy, i.e., one provides
benefit high-quality decisions without risking significant costs interrupting user
miscoordination team. Furthermore, agent must select right strategy despite
significant uncertainty. Markov decision processes (MDPs) (Puterman, 1994) natural
choice implementing reasoning explicitly represent costs, benefits
uncertainty well lookahead examine potential consequences sequences
173

fiScerri, Pynadath & Tambe
actions. Section 4, general reward function presented MDP results
agent carefully balancing risks incorrect autonomous decisions, potential miscoordination
costs due changing coordination team members. Detailed experiments
performed MDP, key results follows. relative importance
central factors, cost miscoordination, varied resulting MDP policies
varied desirable way, i.e., agent made decisions autonomously cost
transferring control entities increased. experiments reveal phenomenon
reported literature: agent may act autonomously coordination
change costs either low high, \middle" range, agent tends act
less autonomously.
research conducted context real-world multi-agent system,
called Electric Elves (E-Elves) (Chalupsky, Gil, Knoblock, Lerman, Oh, Pynadath, Russ,
& Tambe, 2001; Pynadath et al., 2000), used six months
University Southern California, Information Sciences Institute. E-Elves assists
group researchers project assistant daily activities, providing exciting
opportunity test AA ideas real environment. Individual user proxy agents called
Friday (from Robinson Crusoe's servant Friday) act team assist rescheduling
meetings, ordering meals, finding presenters day-to-day activities. course
several months, MDP-based AA reasoning used around clock E-Elves,
making many thousands autonomy decisions. Despite unpredictability user's
behavior agent's limited sensing abilities, MDP consistently made sensible
AA decisions. Moreover, many times agent performed several transfers-of-control
cope contingencies user responding. One lesson learned actually
deploying system sometimes users wished uence AA reasoning, e.g.,
ensure control transferred particular circumstances. allow users
uence AA reasoning, safety constraints introduced allow users prevent
agents taking particular actions ensuring take particular actions.
safety constraints provide guarantees behavior AA reasoning, making
basic approach generally applicable and, particular, making applicable
domains mistakes serious consequences.
rest article organized follows. Section 2 gives detailed description
AA problem presents Electric Elves motivating example application. Section
3 presents formal model transfer-of-control strategies AA. (Readers interested
mathematical details may wish skip Section 3.) operationalization
strategies via MDPs described Section 4. Section 5, results detailed
experiments presented. Section 6 looks related work, including earlier AA work
analyzed within strategies framework. Section 7 gives summary article.
Finally, Section 8 outlines areas work could extended make applicable
applications.
2.

Adjustable Autonomy { Problem

general AA problem previously formally defined literature, particularly multiagent context. following, formal definition problem given
clearly define task AA reasoning. team, may consist entirely
174

fiTowards Adjustable Autonomy Real World
agents include humans, joint activity, ff. entity team works
cooperatively joint activity. agent, A, role, , team. Depending
specific task, roles need performed successfully order
joint activity succeed. primary goal agent success
pursues performing . Performing requires one non-trivial decisions
made. make decision, d, agent draw upon n entities set
E = fe1 : : : en g, typically includes agent itself. entity E (e.g., human
user) capable making decision d. entities E necessarily part
team performing ff. Different agents users differing abilities make decisions
due available computational resources, access relevant information, etc. Coordination
constraints, , exist roles members team. example,
various roles might need executed simultaneously certain order
combined quality total cost. critical facet successful completion joint task
ff, given jointness, ensure coordination team members maintained,
i.e., violated. Thus, describe AA problem instance tuple:
hA; ff; ; ; d; E i.
AA perspective, agent take two types actions decision, d.
First, transfer control entity E capable making decision. general,
restrictions when, often long decision-making control
transferred particular entity. Typically, agent transfer decision-making
control itself. general, assume agent transfers control,
guarantee exact time response exact quality decision made
entity control transferred. fact, cases know whether
entity able make decision even whether entity know
decision-making control, e.g., control transferred via email, agent may know
user actually read email.
second type action agent take request changes coordination constraints, , team members. coordination change gives agent
possibility changing requirements surrounding decision made, e.g.,
required timing, cost quality decision, may allow better fulfill responsibilities. coordination change might involve reordering delaying tasks may
involve changing roles, may dramatic change team pursues
completely different way. Changing coordination cost, may better
incur cost violate coordination constraints, i.e., incur miscoordination costs.
Miscoordination team members occur many reasons, e.g., constraint
limits total cost joint task might violated one team member incurs higher
expected cost team members reduce costs. article,
primarily concerned constraints related timing roles, e.g., ordering constraints requirements simultaneous execution. turn, usually requires
agent guards delayed decisions although require decision
made soon.
Thus, AA problem agent, given problem instance, hA; ff; ; ; d; E i,
choose transfer-of-control coordination-change actions maximizes overall
expected utility team. remainder section describe concrete, real175

fiScerri, Pynadath & Tambe
world domain AA (Section 2.1) initial failed approach motivates solution
(Section 2.2).

2.1 Electric Elves
research initiated response issues arose real application
resulting approach extensively tested day-to-day running application.
Electric Elves (E-Elves) project USC/ISI deploy agent organization
support daily activities human organization (Pynadath et al., 2000; Chalupsky
et al., 2001). believe application fairly typical future generation applications involving teams agents humans. operation human organization
requires performance many everyday tasks ensure coherence organizational
activities, e.g., monitoring status activities, gathering information keeping everyone informed changes activities. Teams software agents aid organizations
accomplishing tasks, facilitating coherent functioning rapid, exible response
crises. number underlying AI technologies support E-Elves, e.g., technologies
devoted agent-human interactions, agent coordination, accessing multiple heterogeneous
information sources, dynamic assignment organizational tasks, deriving information
organization members (Chalupsky et al., 2001). technologies useful,
AA fundamental effective integration E-Elves day-to-day running
real organization and, hence, focus paper.
basic design E-Elves shown Figure 1(a). agent proxy called
Friday (after Robinson Crusoes' man-servant Friday) acts behalf user
agent team. design Friday proxies discussed detail (Tambe, Pynadath,
Chauvat, Das, & Kaminka, 2000) (where referred TEAMCORE proxies).
Currently, Friday perform several tasks user. user delayed meeting,
Friday reschedule meeting, informing Fridays, turn inform users.
research presentation slot open, Friday may respond invitation present
behalf user. Friday order user's meals (see Figure 2(a)) track
user's location, posting Web page. Friday communicates users using wireless
devices, personal digital assistants (PALM VIIs) WAP-enabled mobile phones,
via user workstations. Figure 1(b) shows PALM VII connected Global Positioning
Service (GPS) device, tracking users' locations enabling wireless communication
Friday user. Friday's team behavior based teamwork model,
called STEAM (Tambe, 1997). STEAM encodes enforces constraints roles
required success joint activity, e.g., meeting attendees arrive
meeting simultaneously. role within team needs filled, STEAM requires
team member assigned responsibility role. find best suited person,
team auctions role, allowing consider combination factors assign
best suited user. Friday bid behalf user, indicating whether user
capable and/or willing fill particular role. Figure 2(b) shows tool allows users
view auctions progress intervene desire. auction progress, Jay
Modi's Friday bid Jay capable giving presentation, unwilling
so. Paul Scerri's agent highest bid eventually allocated role.

176

fiTowards Adjustable Autonomy Real World

Friday

Friday

Friday

Friday

(a)

(b)

Figure 1: (a) Overall E-Elves architecture, showing Friday agents interacting users.
(b)Palm VII communicating users GPS device detecting
location.

177

fiScerri, Pynadath & Tambe
AA critical success E-Elves since, despite range sensing devices,
Friday considerable uncertainty user's intentions even location; hence,
Friday always appropriate information make correct decisions.
hand, user required information, Friday cannot continually ask
user input, since interruptions disruptive time-consuming. four
decisions E-Elves AA reasoning applied: (i) whether user attend
meeting time; (ii) whether close auction role; (iii) whether user willing
perform open team role; (iv) order lunch. paper,
focus AA reasoning two decisions: whether user attend meeting
time whether close auction role. decision whether user
attend meeting time often used dicult decisions Friday
faces. brie describe decision close auction later show insight
provided model strategies led significant reduction amount code
required implement AA reasoning decision. decision volunteer user
meeting similar earlier decisions, omitted brevity; decision order
lunch currently implemented simpler fashion (at least yet) illustrative
full set complexities.
central decision Friday, describe terms problem formulation,
hA; ff; ; ; d; E i, whether user attend meeting currently scheduled meeting time. case, Friday agent, A. joint activity, ff, meeting
attendees attend meeting simultaneously. Friday acts proxy user, hence
role, , ensure user arrives currently scheduled meeting time.
coordination constraint, , Friday's role roles Fridays
occur simultaneously, i.e., users must attend currently scheduled time.
attendee arrives late, all, time attendees wasted;
hand, delaying meeting disruptive users' schedules. decision, d, whether
user attend meeting could made either Friday user, i.e.,
E = fuser; Fridayg. Clearly, user often better placed make decision.
However, Friday transfers control user decision, must guard miscoordination, i.e., attendees wait, waiting user response.
decisions potentially costly, e.g., incorrectly telling attendees user
attend, Friday avoid taking autonomously. buy time
user make decision gather information, Friday could change
coordination constraints action. Friday several different actions disposal, including delaying meeting different lengths time, well able
cancel meeting entirely. user request action, e.g., via dialog box
Figure 5(a), buy time make meeting. user decides required,
Friday conduit Fridays (and hence users) informed.
Friday must select sequence actions, either transferring control user, delaying
cancelling meeting autonomously announcing user attend,
maximize utility team.
second AA decision look decision close auction open
role assign user role.2 case, joint activity, ff, group research
2. roles submitting bids auction AA decisions simpler, hence
focus here.

178

fiTowards Adjustable Autonomy Real World

(a)

(b)

Figure 2: (a) Friday transferring control user decision whether order lunch.
(b) E-Elves auction monitoring tool.
meeting role, , auctioneer. Users always submit bids
role immediately; fact, bids may spread several days, users might
bid all. specific decision, d, focus whether close auction
assign role continue waiting incoming bids. individual team members
provide bids, auctioneer agent human team leader decides presenter based
input (E = fuser; auctioneer agentg). team expects willing presenter
high-quality research presentation, means presenter need time
prepare. Thus, coordination constraint, capable, willing user must
allocated role enough time prepare presentation. Despite individually
responsible actions, agent team may reach highly undesirable decision, e.g., assigning
user week week, hence advantage getting human team leader's
input. agent faces uncertainty (e.g., better bids come in?), costs (i.e., later
assignment, less time presenter prepare), needs consider
possibility human team leader special preference
presentation particular meeting. transferring control, agent allows
human team leader make assignment. decision, coordination-change action,
D, would reschedule research meeting. However, relative cost cancelling
meeting, cost rescheduling high rescheduling useful action.

2.2 Decision-Tree Approach
One logical avenue attack AA problem E-Elves apply approach
used previously reported, successful meeting scheduling system, particular CAP
(Mitchell, Caruana, Freitag, McDermott, & Zabowski, 1994). CAP, Friday learned
user preferences using C4.5 decision-tree learning (Quinlan, 1993). Friday recorded values
dozen carefully selected attributes user's preferred action (identified asking
179

fiScerri, Pynadath & Tambe
user) whenever make decision. Friday used data learn decision
tree encoded autonomous decision making. AA, Friday asked user
wanted decisions taken autonomously future. responses, Friday
used C4.5 learn second decision tree encoded rules transferring control.
Thus, second decision tree indicated Friday act autonomously, would
take action suggested first decision tree. Initial tests C4.5 approach
promising (Tambe et al., 2000), key problem soon became apparent.
Friday encountered decision learned transfer control user,
would wait indefinitely user make decision, even though inaction caused
miscoordination teammates. particular, team members would arrive
meeting location, waiting response user's Friday, would end
completely wasting time response arrived. address problem, user
respond within fixed time limit (five minutes), Friday took autonomous action.
Although performance improved, resulting system deployed 24/7 led
dramatic failures, including:
1. Example 1: Tambe's (a user) Friday incorrectly cancelled meeting division
director Friday over-generalized training examples.
2. Example 2: Pynadath's (another user) Friday incorrectly cancelled group's weekly
research meeting time-out forced choice (incorrect) autonomous
action.
3. Example 3: Friday delayed meeting almost 50 times, time 5 minutes.
correctly applying learned rule ignoring nuisance rest
meeting participants.
4. Example 4: Tambe's Friday automatically volunteered presentation,
actually unwilling. Friday over-generalized examples
timeout occurred took undesirable autonomous action.
Clearly, team context, rigidly transferring control one agent (user) failed. Furthermore, using time-out rigidly transferred control back agent,
capable making high-quality decision, failed. particular, agent needed
better avoid taking risky decisions explicitly considering costs (example 1),
take lower cost actions delay meetings buy user time respond (example 2
4). Furthermore, example 3 showed, agent needed plan ahead, avoid taking
costly sequences actions could replaced single less costly action (example
3). theory, using C4.5 Friday might eventually able learn rules would
successfully balance costs deal uncertainty handle special cases
on, large amount training data would required.
3.

Strategies Adjustable Autonomy

avoid rigid one-shot transfers control allow team costs considered,
introduce notion transfer-of-control strategy, defined follows:
180

fiTowards Adjustable Autonomy Real World
Definition 3.1 transfer-of-control strategy pre-defined, conditional sequence two

types actions: (i) actions transfer decision-making control (e.g., agent
user agents, vice versa) (ii) actions change agent's pre-specified
coordination constraints team members, aimed minimizing miscoordination costs.

agent executes transfer-of-control strategy performing specified actions
sequence, transferring control specified entity changing coordination required,
point time entity currently control exercises control
makes decision. Considering multi-step strategies allows agent exploit decisionmaking sources considered risky exploit without possibility retaking control.
example, control could transferred capable always available decision
maker taken back decision made serious miscoordination occurred.
complex strategies, potentially involving several coordination changes, give agent
option try several decision-making sources exible getting input
high-quality decision makers. result, transfer-of-control strategies specifically allow
agent avoid costly errors, enumerated previous section.3
Given AA problem instance, hA; ff; ; ; d; E , agent transfer decision-making
control decision entity ei 2 E , denote transfer-of-control
action symbol representing entity, i.e., transferring control ei denoted
ei . agent transfers decision-making control, may stipulate limit time
wait response entity. capture additional stipulation,
denote transfer-of-control actions time limit, e.g., ei (t) represents ei
decision-making control maximum time t. action two possible outcomes:
either ei responds time makes decision, respond decision
remains unmade time t. addition, agent mechanism
change coordination constraints (denoted D) change expected timing
decision. action changes coordination constraints, , team members.
action associated value, Dvalue , specifies magnitude (i.e., much
alleviated temporal pressure), cost, Dcost , specifies price paid
making change. concatenate actions specify complete transferof-control strategy. instance, strategy H (5)A would specify agent first
relinquishes control asks entity H (denoting H uman user). user responds
decision within five minutes, need go further. not,
agent proceeds next transfer-of-control action sequence. example,
next action, A, specifies agent make decision complete task.
transfers control occur case. define space possible
strategies following regular expression:

= (E R)((E R) + D)

(1)

(E R) possible combinations entity maximum time.
readability, frequently omit time specifications transfer-ofcontrol actions instead write order agent transfers control among
3. domains, may make sense attempt get input one entity once, hence
requiring strategies actions might executed parallel. However, work, first
step, consider strategies. Furthermore, relevant domains hand.

181

fiScerri, Pynadath & Tambe
entities executes Ds (e.g., often write HA instead H (5)A). time
specifications omitted, assume transfers happen optimal times,4 i.e.,
times lead highest expected utility. consider strategies
sequence actions different timings strategy, agent O(jE jk )
possible strategies select from, k maximum length strategy jE j
number entities. Thus, agent wide range options, even practical
considerations lead reasonable upper bound k jE j. agent must select
strategy maximizes overall expected utility ff.
rest section, present mathematical model transfer-of-control strategies AA use model guide search solution. Moreover, model
provides tool predicting performance various strategies, justifying use
explaining observed phenomena use. Section 3.1 presents model AA
strategies detail. Section 3.2 reveals key properties complex strategies, including dominance relationships among strategies. Section 3.3 examines E-Elves application
light model, make specific predictions properties successful
AA approach reasoning application class have. predictions shape
operationalization strategies Section 4.

3.1 Mathematical Model Strategies
transfer-of-control model presented section allows calculation expected
utility (EU) individual strategies, thus allowing strategies compared. calculation strategy's EU considers four elements: likely relative quality different
entities' decisions; probability getting response entity particular time;
cost delaying decision; costs benefits changing coordination constraints. parameters might modeled similar manner, experience
E-Elves AA work suggests parameters critical ones
across wide range joint activities.
first element model expected quality entity's decision. general,
capture quality entity's decision time functions EQ = fEQde (t) :
R ! Rg. quality decision ects probability entity make
\appropriate" decision costs incurred decision wrong. expected quality
decision calculated decision theoretic way, multiplying probability
outcome, i.e., decision, utility decision, i.e., cost benefit
decision. example, higher probability entity make mistake,
lower quality, even lower mistakes might costly. quality decision
entity make vary time information available changes
time \think". second element model probability entity
make decision control transferred it. functions, P = fP>e (t) : R ! [0; 1]g,
represent continuous probability distributions time
entity e respond.
R t0 ei
is, probability ei respond time t0 0 P> (t)dt.
third element model representation cost inappropriate timing
decision. general, making decision particular point time incurs
4. best time transfer control found, e.g., differentiating expected utility equation
Section 3.1 solving 0.

182

fiTowards Adjustable Autonomy Real World
cost function time, t, coordination constraints, ,
team members. stated earlier, focus cases constraint violations due delays
making decisions. Thus, cost due violation constraints caused
making decision point time. write wait-cost function :
W = f (; t) returns cost making decision particular point time
given coordination constraints, . miscoordination cost fundamental aspect
model given emphasis multiagent domains. called \wait cost" models
miscoordination arises team \waits" entity make ultimate
decision. domains E-Elves, team incurs wait costs situations (for
example) meeting attendees assembled meeting room time
meeting, kept waiting without input decision Friday (potentially
cannot provide high-quality decision, get input user). Notice
different roles lead different wait cost functions, since delays performance
different roles different effects team. assume
point time, , costs accrue, i.e., f (; t) = f (; ).
deadline, , maximum cost due inappropriate timing decision
incurred. Finally, assume that, general, , wait cost function nondecreasing, ecting idea bigger violations constraints lead higher wait costs.
final element model coordination-change action, D, moves agent
away deadline hence reduces wait costs incurred.
model effect letting W function Dvalue (rather t)
action fixed cost, Dcost , incurred immediately upon execution.
example, E-Elves domain, suppose time meeting, Friday delays
meeting 15 minutes (D action). Then, following time period, incur
relatively low cost making decision 15 minutes meeting (t Dvalue ),
rather relatively high cost making decision time meeting.
Other, possibly complex, models action could used.
use four elements compute EU arbitrary strategy, s. utility
derived decision made time entity control quality
entity's decision minus costs incurred waiting t, i.e., EUedc (t) = EQdec (t)
W (t). coordination-change action taken effect utility.
coordination change value Dvalue taken time , incurred wait cost
W (). Then, t, wait cost incurred W (t Dvalue ) W ( Dvalue ).
Thus, action taken time cost Dcost value Dvalue ,
utility decision time (t > ) is: EUedc (t) = EQdec (t) W () W ( Dvalue ) +
W (t Dvalue) Dcost. calculate EU entire strategy, multiply response
probability mass function's value instant EU receiving response
instant, integrate products. Hence, EU strategy given
problem instance, hA; ff; ; ; d; E , is:
Z 1
h
A;ff;;

;d;E

EUs
=
P (t)EUedc (t) :dt
(2)
0 >
strategy involves several actions, need ensure probability response
function wait-cost calculation ect control situation point
strategy. example, user, H , control time t, P> (t) ect H's
183

fiScerri, Pynadath & Tambe

W (0)

EUAd = EQdA (0)
EUed

EUeA

=

=



Z

0
Z

0

EUedDeA =

P>(t) (EQde (t)
P>(t) (EQde (t)

W (t)):dt +
W (t)):dt +

1
P>(t) (EQde (t)


Z

1

Z



R

(3)

W (D)):dt (4)

P>(t):dt (EQda (T )

W (T )) (5)


0
0 P> (t)(EQe (t) W (t)):dt +

P>(t)(EQe (t) W () + W ( Dvalue ) W (t Dvalue ) Dcost ):dt +
R1

P> (t)(EQA (t) W () + W ( Dvalue ) W (T Dvalue ) Dcost ):dt

(6)

RT

Table 1: General AA EU equations sample transfer control strategies.
probability responding t, i.e., P>H (t0 ). end, break integral
Equation 2 separate terms, term representing one segment strategy,
e.g., strategy UA would one term U control another
control.
Using basic technique writing EU calculations, write
specific equations arbitrary transfer-of-control strategies. Equations 3-6 Table 1
show EU equations strategies A, e , eA e DeA respectively. equations
assume agent, A, make decision instantaneously (or least, delay
significant enough affect overall value decision). equations created
writing integral segments strategy, described above.
time agent takes control e , time occurs.
One write equations complex strategies way. Notice
equations make assumptions particular functions.
Given EU strategy calculated, AA problem agent reduces
finding following transfer-of-control strategy maximize EU. Formally,
agent's problem is:

Axiom 3.1 problem hA; ff; ; ; d; E , agent must select 2 8s0 2
S; s0 6= s; EUshA;ff;;;d;E EUsh0A;ff;;;d;E

184

fiTowards Adjustable Autonomy Real World

5
0
-5
0.1

w 0.2

0.3

0.4

1.2
0.8p

Figure 3: Graph comparing EU two strategies, H DA (solid line) H (dashed line)
given particular instantiation model constant expected decisionmaking quality, exponentially rising wait costs, Markovian response probabilities. p parameter P>(t) function, higher p meaning longer
expected response time. w parameter W (t) function higher w
meaning rapidly accruing wait costs.

3.2 Dominance Relationships among Strategies
agent could potentially find strategy highest EU examining
every strategy S, computing EU, selecting strategy highest value.
example, consider problem domains constant expected decision-making quality,
exponentially rising wait costs, Markovian response probabilities. Figure 3 shows
graph EU two strategies (H DA H ) given particular model instantiation.
Notice that, different response probabilities rates wait cost accrual, one strategy
outperforms other, neither strategy dominant entire parameter space.
EU strategy dependent timing transfers control, turn
depend relative quality entities' decision making. Appendix provides
detailed analysis.
Fortunately, evaluate compare every candidate
exhaustive search find optimal strategy. instead use analytical methods
draw general conclusions relative values different candidate strategies.
particular, present three Lemmas show domain-level conditions
particular strategy types superior others. Lemmas lead us the, perhaps
surprising, conclusion complex strategies necessarily superior single-shot
strategies, even multi-agent context; fact, particular strategy dominates
strategies across domains.
Let us first consider AA subproblem whether agent ever take back
control another entity. show that, certain conditions, agent
always eventually take back control, strategy selection process ignore
strategies agent (i.e., strategies ending A). agent's
goal strike right balance waiting indefinitely user response
185

fiScerri, Pynadath & Tambe
taking risky autonomous action. Informally, agent reasons eventually
make decision expected cost continued waiting exceeds difference
user's decision quality own. formally, agent eventually take back
decision-making control iff, time t:
Z
P> (t0 )W (t0 ):dt0 W (t) > EQdU (t) EQdA (t)
(7)


left-hand side calculates future expected wait costs right-hand side
calculates extra utility gained getting response user. result
leads following general conclusion strategies end giving control back
agent:
Lemma 1: 2 isRa strategy ending e 2 E , s0 sA, EUsd0 > EUsd iff
8e 2 E; 9t < P>(t0 )W (t0 ):dt0 W (t) > EQde (t) EQdA(t)
Lemma 1 says if, point time, expected cost indefinitely leaving
control hands user exceeds difference quality agent's
user's decisions, strategies ultimately give agent control dominate
not. Thus, rate wait cost accrual increases difference
relative quality decision-making abilities decreases user's probability response
decreases, strategies agent eventually takes back control dominate.
key consequence Lemma (in opposite direction) that, rate costs accrue
accelerate, probability response stays constant (i.e., Markovian),
agent indefinitely leave control user (if user originally
given control), since expected wait cost change time. Hence, even
agent faced situation potentially high total wait costs, optimal strategy
may one-shot strategy handing control waiting indefinitely,
expected future wait costs point time relatively low. Thus, Lemma 1 isolates
condition consider appending transfer-of-control action
strategy.
perform similar analysis identify conditions
include action strategy. agent incentive changing coordination
constraints via action due additional time made available getting highquality response entity. However, overall value action depends
number factors (e.g., cost taking action timing subsequent
transfers control). calculate expected value comparing EU
strategy without D. useful increased expected value
strategy greater cost, Dcost .
Lemma 2: sR2 s0 included EUsd0 > EUsd iff
R
P> (t0 )W (t):dt0
P>(t0 )W (tjD):dt0 > Dcost
illustrate consequences Lemma 2 considering specific problem model
Appendix (i.e., P> (t) = exp , W (t) = ! exp!t , EQde (t) = c, candidate strategies
iff ( ! )! exp ( !) (1 exp !Dvalue ) >
eA e DA). case, EUedDA > EUeA
Dcost. Figure 4 plots value action vary rate wait cost accumulation,
w, parameter Markovian response probability function, p. graph shows
186

fiTowards Adjustable Autonomy Real World

Value
0.16
0.12
0.08
0.04
0
-0.04
0.1 0.2
w0.3 0.4 0.5

1
0.75
0.5 p
0.25

Figure 4: value action particular model (P> (t) = exp
EQde (t) = c).

,

W (t) = ! exp!t ,

benefit highest probability response neither low
high. probability response low, user unlikely respond,
even given extra time; hence, agent incurred Dcost benefit.
little value probability response high, user likely
respond shortly D, meaning little effect (the effect
wait costs action taken). Overall, according Lemma 2, points
graph goes Dcost , agent include action, and, points,
not. Figure 4 demonstrates value action specific subclass problem
domains, extend conclusion general case well. instance,
specific model exponential wait costs, models wait costs grow
slowly, fewer situations Lemma 2's criterion holds (i.e.,
useful). Thus, Lemma 2 allows us eliminate strategies consideration, based
evaluation criterion particular domain interest.
Given Lemma 2's evaluation adding single action strategy, natural
ask whether second, third, etc. action would increase EU even further. words,
complex strategy better simple one, even complex strategy even
better? answer \not necessarily".

8K 2 N; 9W 2 W; 9P 2 P; 9EQ 2 EQ optimal strategy
actions.
Informally, Lemma 3 says cannot fix single, optimal number actions,
every possible number actions, potential domain (i.e., combination
Lemma 3:

K

wait-cost, response-probability, expected-quality functions) number
actions justified optimal. Consider situation cost
function number Ds date (i.e., cost K th f (K )). example,
E-Elves' meeting case, cost delaying meeting third time much
higher cost first delay, since delay successively annoying
meeting participants. Hence, test usefulness K th strategy,
187

fiScerri, Pynadath & Tambe
given specific model Appendix I, is:

!

exp exp! )
(8)
f (K ) < !(exp Dvalue! 1) ( exp


Depending nature f (K ), Equation 8 hold number Ds, so,
K , conditions strategy K Ds optimal. instance,
Section 5.3, show maximum length optimal strategy random
configuration 25 entities usually less eight actions.
Equation 8 illustrates value additional limited changing Dcost ,
Lemma 3 shows us factors affect value additional D.
example, even constant Dcost , value additional depends many
actions agent performs. Figure 4 shows value depends
rate wait costs accrue. rate wait cost accrual accelerates time (e.g.,
exponential model), action slows acceleration, rendering second action
less useful (since wait costs accruing slowly). Notice Ds become
valueless deadline, wait costs stop accruing.
Taken together, Lemmas 1-3 show particular transfer-of-control strategy dominates others across domains. Moreover, different strategies, single-shot
strategies arbitrarily complex strategies, appropriate different situations, although
range situations particular transfer-of-control action provides benefit
quite narrow. Since strategy might low EU set parameters, choosing
wrong strategy lead poor results. hand, understand
parameter configuration intended application domain, Lemmas 1-3 provide useful
tools focusing search optimal transfer-of-control strategy. Lemmas
used off-line substantially reduce space strategies need searched
find optimal strategy. However, general may many strategies finding
optimal strategy may possible feasible.

3.3 Model Predictions E-Elves
section, use model predict properties successful approach AA
E-Elves. Using approximate functions probability response, wait cost,
expected decision quality, calculate EU various strategies determine
types strategies going useful. Armed knowledge, predict
key properties successful implementation.
key feature E-Elves user mobile. moves around environment, probability responding requests decisions changes drastically, e.g.,
likely respond workstation. calculate EU different strategies,
need know P>(t), means need estimate response probabilities
model change user moves around. Friday communicates via
workstation dialog box, user respond, average, five minutes. However,
Friday communicates via Palm pilot average user response time hour. Users
generally take longer decide whether want present research meeting, taking
approximately two days average. So, function P>(t) average value
5 minutes user oce, average one hour user contacted
via Palm pilot average two days decision whether present
188

fiTowards Adjustable Autonomy Real World
research meeting. necessary estimate relative quality user, EQdU (t),
Friday's decision making, EQdA (t). assume user's decision-making EQdU (t)
high respect Friday's, EQdA (t). uncertainty user intentions makes
hard Friday consistently make correct decisions time user
arrive meetings, although sensors (e.g., GPS device) give indication
user's location. dealing important meetings, cost Friday's errors
higher. Thus, cases, decision-making quality user Friday
similar, i.e., EQUd (t) EQAd (t); cases, order magnitude
difference, i.e., EQUd (t) 10 EQAd (t). wait cost function, W (t), much larger
big meetings small increase rapidly attendees wait longer meeting
room. Finally, cost delays, i.e., Dcost , vary order magnitude.
particular, cost rescheduling meetings varies greatly, e.g., cost rescheduling
small informal meetings colleagues far less rescheduling full lecture room
5 PM Friday.
parameters laid show parameters vary decision decision.
specific decision, use Markovian response probabilities (e.g., user
oce, average response time five minutes), exponentially increasing wait costs,
constant decision-making quality (though changes decision decision) calculate
EU interesting strategies. Calculating EU different strategies using values
different parameters shown allows us draw following conclusions (Table 5
Section 5.3 presents quantitative illustration predictions):

strategy e used, since combinations user location
meeting importance EU strategy low.

Multiple strategies required, since different user locations meeting importance different strategies optimal.

Since quite different strategies required user different locations,
AA reasoning need change strategies user changes location.

strategy reasonable EU possible parameter instantiations, hence always
using strategy occasionally cause dramatic failures.

decisions, strategies end agent taking decision, since strategies
ending user control generally low EU.

predictions provide important guidance successful solution AA
E-Elves. particular, make clear approach must exibly choose
different strategies adjust depending meeting type user location.
Section 2.2 described unsuccessful C4.5 approach AA E-Elves identified
several reasons mistakes occurred. particular, rigidly transferring control
one entity ignoring potential team costs involved agent's decision highlighted
reasons dramatic mistakes Friday's autonomy reasoning. Reviewing C4.5
approach light notion strategies, see Friday learned one strategy
stuck strategy. particular, originally, Friday would wait indefinitely user
response, i.e., would follow strategy e , learned transfer control. shown later
189

fiScerri, Pynadath & Tambe
Table 5, strategy low EU. fixed-length timeout introduced,
Friday would follow strategy e (5)A. strategy high EU EQUd (t) EQAd (t)
low EU EQUd (t) 10 EQAd (t). Thus, model explains phenomenon
observed practice.
hand, use model understand C4.5's failure case
mean never useful AA. Different strategies required
certain parameters (like probability response wait cost) change significantly.
applications parameters change dramatically decision decision,
one particular strategy may always appropriate. applications, C4.5 might learn
right strategy small amount training data perform acceptably well.
4.

Operationalizing Strategies MDPs

formalized problem AA selection transfer-of-control strategy highest EU. need operational mechanism allows agent
perform selection. One major conclusion previous section different
strategies dominate different situations, applications E-Elves require mechanism(s) selecting strategies situation-sensitive fashion. particular,
mechanism must exibly change strategies situation changes. required mechanism must represent utility function specified expected decision qualities,
EQ, costs violating coordination constraints, W, coordination-change cost,
Dcost. Finally, mechanism must represent uncertainty entity responses
look ahead possible responses (or lack thereof) may occur future.
MDPs natural means performing decision-theoretic planning required find
best transfer-of-control strategy. MDP policies provide mapping agent's
state optimal transfer control strategy. encoding parameters model
AA strategies MDP, MDP effectively becomes detailed implementation
model and, hence, assumes properties. use standard algorithms (Puterman,
1994) find optimal MDP policy and, hence, optimal strategies follow
state.
simplify exposition, well illustrate generality resulting MDP,
section describes mapping AA strategies MDP four subsections.
particular, Section 4.1 provides direct mapping strategies abstract MDP. Section
4.2 fills state features enable concrete realization reward function,
still maintaining domain-independent view. Thus, section completely defines general
MDP AA potentially reusable across broad class domains. Section 4.3 illustrates
implemented instantiation MDP E-Elves. Section 4.4 addresses practical
issues operationalizing MDPs domains E-Elves.

4.1 Abstract MDP Representation AA Problem
MDP representation's fundamental state features capture state control:

controlling-entity entity currently decision-making control.
ei -response response ei made agent's requests input.
190

fiTowards Adjustable Autonomy Real World
Original State Action
Destination State
ectrl time
ectrl ei -response
time
ej
tk
ei
ei
yes
tk+1
ej
tk
ei
ei

tk+1
ei
tk
wait
ei
yes
tk+1
ei
tk
wait
ei

tk+1
ei
tk

ei

tk Dvalue

Probability
1
1

R tk+1 ei
tkR P> (t)dt
tk+1 ei
tk P> (t)dt
R tk+1
ei
tkR P> (t)dt
tk+1 ei
tk P> (t)dt

1

Table 2: Transition probability function AA MDP. ectrl controlling-entity.



time current time, typically discretized ranging 0 deadline,
| i.e., set ft0 = 0; t1 ; t2 ; : : : ; tn = g.

ei -response null time = , agent terminal state. former
case, decision value ei -response.
specify set actions MDP representation = E [fD; waitg.
set actions subsumes set entities, E , since agent transfer decision-making
control one entities. action coordination-change action
changes coordination constraints, discussed earlier. \wait" action puts transferring control making autonomous decision, without changing coordination
team. agent reason \wait" best action when, time, situation
likely change put agent position improved autonomous decision
transfer-of-control, without significant harm. example, E-Elves domain, times
closer meeting, users generally make accurate determinations whether
arrive time, hence sometimes useful wait meeting long
time off.
transition probabilities (specified Table 2) represent effects actions
distribution effects (i.e., ensuing state world). If, state
time = tk , agent chooses action transfers decision-making control entity,
ei , agent itself, outcome state controlling-entity = ei
time = tk+1 . two possible outcomes ei -response: either entity responds
decision transition (producing terminal state), not,
derive probability distribution two P. \wait" action similar
branch, except controlling-entity remains unchanged. Finally, action occurs
instantaneously, time controlling entity respond, resulting
state effectively moves earlier time (e.g., tk tk Dvalue ).
derive reward function MDP straightforward fashion
strategy model. Table 3 presents complete specification reward function.
transitions take time, i.e., transferring control receiving response
(Table 3, row 1) \wait" (Table 3, row 2), agent incurs wait cost interval.
transitions agent performs D, agent incurs cost action (Table 3,
row 3). terminal states response ei , agent derives expected quality
entity's decision (Table 3, row 4). policy maximizes reward agent
expects receive according AA MDP model correspond exactly optimal
191

fiScerri, Pynadath & Tambe
controlling-entity time ei -response Action
ej
tk

ei
ei
tk

wait
ei
tk


ei
tk
yes

Reward

W (k + 1) W (k)
W (k + 1) W (k)
Dcost
EQdei (tk )

Table 3: Reward function AA MDP.
transfer-of-control strategy. Note reward function described abstract
fashion|for example, specify compute agent's expected quality
decision, EQAd (t).

4.2 MDP Representation AA Problem within Team Context
given high-level description MDP implementing notion
transfer-of-control strategies AA. remainder section provides detailed
look MDP broad class AA domains (including E-Elves) agent
acts behalf user filling role, , within context team activity, ff.
reward function compares EU different strategies, finding optimal one
current state. facilitate calculation, need represent parameters used
model. introduce following state features capture aspects AA
problem team context:

team-orig-expect- team originally expected fulfilling .
team-expect- team's current expectations fulfilling role implies.
agent-expect- agent's (probabilistic) estimation fulfilled.
\other attributes" encapsulate aspects joint activity affected
decision.

add specific features generic AA state features already
presented, overall state, within MDP representation decision d, tuple:

hcontrolling-entity; team-orig-expect-; team-expect-; agent-expect-; ff-status;
ei -response; time; attributesi
example, meeting scenario, team-orig-expect- could \Meet 3pm", teamexpect- could \Meet 3:15pm" user requested delay, agent-expect- could
\Meet 3:30pm" agent believes user make rescheduled meeting.
transition probability function AA MDP team context includes
underlying AA transition probabilities Table 3, must include probabilities
new state features. particular, addition temporal effect
action described Section 4.1, additional effect coordination ff.
action changes value team-expect- feature (in domain-dependent
192

fiTowards Adjustable Autonomy Real World
deterministic way). actions affect team's expectations. team-orig-expect-
feature change; include simplify definition reward function.
transition probabilities agent-expect- ff-specific features domain-specific.
provide example transition probabilities Section 4.3.
final part MDP representation reward function. team AA MDP
framework uses reward function breaks function Table 3 follows:

R(s; a) = f (team-orig-expect-(s); team-expect- (s); agent-expect- (s);
ff-status (s); time(s); a)
X
=
EQde (time(s)) e -response

(9)

e 2E nfAg

1 f1 (k team-orig-expect- (s) team-expect- (s) k)
21 f21 (time(s))
22 f22 (k team-expect-(s) agent-expect-(s) k)
+3 f3 (ff-status (s)) + 4 f4 (a)

(10)

first component reward function captures value getting response
decision-making entity agent itself. Notice one entity actually
respond, one e -response non-zero. corresponds EQed (t) function
used model bottom row Table 3. f1 function ects inherent
value performing role team originally expected, hence deterring agent
taking costly coordination changes unless gain indirect value
so. corresponds Dcost mathematical model third row Table 3.
f21 corresponds second row Table 3, represents wait cost function,
W (t), model. component encourages agent keep team members
informed role's status (e.g., making decision taking explicit action),
rather causing wait without information. Functions f22 f3 represent
quality agent's decision, represented QAd (t). standard MDP algorithms
compute expectation agent's reward, expectation quality
produce desired EQAd (t) fourth row Table 3. first quality function, f22 ,
ects value keeping team's understanding role performed
accordance agent expects user actually perform role. agent
receives reward role performed exactly team expects,
uncertainty agent's expectation, errors possible. f22 represents costs
come errors. second quality component, f3 , uences overall reward based
successful completion joint activity, encourages agent take actions
maximize likelihood joint activity succeeds. desire joint
task succeed implicit mathematical model must explicitly represented
MDP. component, f4 , augments first row Table 3 account additional
costs transfer-of-control actions. particular, f4 broken follows:
(

f4 (a) =

q(e ) 2 E
0
otherwise
193

(11)

fiScerri, Pynadath & Tambe
function q(e ) represents cost transferring control particular entity, e.g.,
cost WAP phone message user. Notice, detailed, domain-specific costs
appear directly model.
Given MDP's state space, actions, transition probabilities, reward function,
agent use value iteration generate policy P : ! specifies optimal
action state (Puterman, 1994). agent executes policy taking
action policy dictates every state finds itself. policy
may include several transfers control coordination-change actions. particular
series actions depends activities user. interpret policy
contingent combination many transfer-of-control strategies, strategy follow
chosen depending user's status (i.e., agent-expect-).

4.3 Example: E-Elves MDPs
example AA MDP generic delay MDP, instantiated
meeting Friday may act behalf user. Recall decision, d, whether
let meeting attendees wait user begin meeting. joint activity,
ff, meeting agent role, , ensuring user attends
meeting scheduled time. coordination constraints, , attendees
arrive meeting location simultaneously effect action delay
cancel meeting.
delay MDP's state representation, team-orig-expect- originally-scheduledmeeting-time, since attendance originally scheduled meeting time team
originally expects user best possible outcome. team-expect- timerelative-to-meeting, may increase meeting delayed. ff-status becomes statusof-meeting. agent-expect- represented explicitly; instead, user-location used
observable heuristic user likely attend meeting. example,
user away department shortly meeting begin unlikely
attending time, all. state features, total state space contains
2800 states individual meeting, large number states arising
fine-grained discretization time.
general reward function mapped delay MDP reward function following way.
(

g(N; ff) N < 4
(12)
1
otherwise
N number times meeting rescheduled g function takes
account factors number meeting attendees, size meeting delay
time originally scheduled meeting time. function effectively forbids
agent ever performing 4 actions.
delay MDP, functions, f21 f22 , correspond cost making
meeting attendees wait, merge single function, f2 . expect
consolidation possible similar domains team's expectations relate
f1 =

194

fiTowards Adjustable Autonomy Real World
temporal aspect role performance.
(

f2 =

h(late; ff) late > 0
0
otherwise

(13)

late difference scheduled meeting time time user
arrives meeting room. late probabilistically calculated MDP based
user's current location model user's behavior.
8
>
<

rff + ruser user attends
f3 = > rff
meeting takes place, user attend
: 0
otherwise

(14)

value, rff , models inherent value ff, value ruser models user's
individual value ff.
f4 given previously Equation 11. cost communicating user
depends medium used communicate. example, higher cost
communicating via WAP phone via workstation dialog box.
users asked input, assumed that, respond, response
\correct", i.e., user says delay meeting 15 minutes, assume
user arrive time re-scheduled meeting. user asked front
his/her workstation, dialog one shown Figure 5 popped up, allowing user
select action taken. expected quality agent's decision calculated
considering agent's proposed decision possible outcomes decision.
example, agent proposes delaying meeting 15 minutes, calculation
decision quality includes probability benefits user actually arrive
15 minutes originally scheduled meeting time, probability costs
user arrives originally scheduled meeting time, etc.

(a)

(b)

Figure 5: (a) Dialog box delaying meetings. (b) small portion delay MDP.
delay MDP represents probabilities change user location (e.g.,
oce meeting location) occur given time interval. Figure 5(b) shows portion
195

fiScerri, Pynadath & Tambe
state space, showing user-response, user location features. transition
labeled \delay n" corresponds action \delay n minutes". figure shows
multiple transitions due \ask" (i.e., transfer control user) \wait" actions,
relative probability outcome represented thickness arrow.
state transitions correspond uncertainty associated user's response (e.g.,
agent performs \ask" action, user may respond specific information may
respond all, leaving agent effectively \wait"). One possible policy produced
delay MDP, subclass meetings, specifies \ask" state S0 Figure 5(b)
(i.e., agent gives autonomy). world reaches state S3, policy specifies
\wait". However, agent reaches state S5, policy chooses \delay 15",
agent executes autonomously. terms strategies, sequence actions
H D.
Earlier, described another AA decision E-Elves, namely whether close
auction open team role. Here, brie describe key aspects mapping
decision MDP. auction must closed time user prepare
meeting, sucient time given interested users submit bids
human team leader choose particular user. team-orig-expect- (s) highquality presenter selected enough time prepare. action, hence
team-expect- (s) = team-orig-expect- (s). agent-expect-(s) whether agent believes
high-quality bid believes bid arrive time user allocated
role. agent's decision quality, EQdA (t), function number bids
submitted quality bids, e.g., team members submitted
bids one user's bid stands out, agent confidently choose user
presentation. Thus, ff-status primarily quality best bid far difference
quality bid second-best bid. critical component
reward function Equation 10 2 component, gives reward agent
fulfills users' expectation willing presenter high-quality presentation.

4.4 User-Specified Constraints
standard MDP algorithms provide agent optimal policies subject encoded probabilities reward function. Thus, agent designer access correct
models entities' (e.g., human users E-Elves) decision qualities probabilities response, agent select best possible transfer-of-control strategy.
However, possible entities accurate information
abilities agent designer. exploit knowledge, entity could
communicate model quality decision probability response directly
agent designer. Unfortunately, typical entity unlikely able express
knowledge form MDP reward function transition probabilities. agent
could potentially learn additional knowledge interactions
entities domain. However, learning may require arbitrarily large number
interactions, take place without benefit entities' inside
knowledge.
alternative, provide language constraints allows entities
directly immediately communicate inside information agent. constraint
196

fiTowards Adjustable Autonomy Real World

Figure 6: Screenshot tool entering constraints. constraint displayed forbids
transferring control (i.e., forces transfer) five minutes meeting
teammates previously given information user's attendance
meeting.
language provides entities simple way inform agent specific properties
needs. entity use constraint forbid agent entering specific states
performing specific actions specific states. constraints directly communicated
user via tool shown Figure 6. instance, figure shown user
forbidding agent autonomous action five minutes meeting. define
forbidden-action constraints set, Cfa , element constraint
boolean function, cfa : !ft; f g. Similarly, define forbidden-state constraints
set, Cfs , elements, cfs : !ft; f g. constraint returns particular domain
element (either state state-action pair, appropriate), constraint applies
given element. example, forbidden-action constraint, cfa , forbids action
performed state cfa (s; a) = t.
provide probabilistic semantics, suitable MDP context, first provide
notation. Denote probability agent ever arrive state sf following
jP ). Then, define semantics
policy, P , initial state si Pr(si !
f
jP ) = 0. semantics given
forbidden-state constraint cfs requiring Pr(si !
f
^P (s )=ajP ) = 0
forbidden-action constraint, cfa , bit complex, requiring Pr(si!
f
f
(i.e., cfa forbids agent entering state sf performing action a).
cases, aggregation constraints may forbid actions state sf . case,
conjunction allows agent still satisfy forbidden-action constraints avoiding sf
(i.e., state sf becomes forbidden). state, sf , becomes indirectly forbidden
fashion, action potentially leads agent ancestor state
sf likewise becomes forbidden. Hence, effect forbidding constraints propagate
backward state space, affecting state/action pairs beyond cause
immediate violations.
197

fiScerri, Pynadath & Tambe
forbidding constraints powerful enough entity communicate wide
range knowledge decision quality probability response agent.
instance, E-Elves users forbidden agents rescheduling meetings
lunch time. so, users provide feature specification states want
forbid, meeting-time =12 PM. specification generates forbidden-state
constraint, cfs , true state, s, meeting-time =12 PM s. constraint
effectively forbids agent performing action would result state
meeting-time =12PM. Similarly, users forbidden autonomous actions certain
states providing specification actions want forbid, e.g., action 6=\ask".
generates forbidden-action constraint, cfa , true state/action pair,
(s; a), 6=\ask". example, user might specify constraint states
oce, time meeting know
always make decisions case. Users easily create complicated constraints
specifying values multiple features, well using comparison functions
= (e.g., 6=, >).
Analogous forbidding constraints, introduce required-state requiredaction constraints, defined sets, Crs Cra , respectively. interpretation provided
required-state constraint symmetric, opposite forbidden-state
jP ) = 1. Thus, state, agent must eventually reach
constraint: Pr(si !
f
^P (s )=ajP ) = 1.
required state, sf . Similarly, required-action constraint, Pr(si!
f
f
users specify constraints forbidding counterparts (i.e., specifying values relevant state features action, appropriate). addition,
requiring constraints propagate backward. Informally, forbidden constraints focus
locally specific states actions, required constraints express global properties
states.
resulting language allows agent exploit synergistic interactions
initial model transfer-of-control strategies entity-specified constraints. example,
forbidden-action constraint prevents agent taking autonomous action
particular state equivalent user specifying agent must transfer control
user state. AA terms, user instructs agent consider transferof-control strategies violate constraint. exploit pruning strategy
space user, extended standard value iteration consider constraint
satisfaction generating optimal strategies. Appendix II provides description
novel algorithm finds optimal policies respecting user constraints. appendix
includes proof algorithm's correctness.
5. Experimental Results

section presents experimental results aimed validating claims made previous sections. particular, experiments aim show utility complex transfer-ofcontrol strategies effectiveness MDPs technique operationalization.
Section 5.1 details use E-Elves daily activities Section 5.2 discusses
pros cons living working assistance Fridays. Section 5.3 shows
characteristics strategies type domain (in particular, different strategies
198

fiTowards Adjustable Autonomy Real World
used practice). Finally, Section 5.4 describes detailed experiments illustrate
characteristics AA MDP.

5.1 E-Elves Daily Use
E-Elves system heavily used ten users research group ISI, June
2000 December 2000.5 Friday agents ran continuously, around clock, seven
days week. exact number agents running varied period execution,
usually five ten Friday agents individual users, capability matcher (with proxy),
interest matcher (with proxy). Occasionally, temporary Friday agents operated
behalf special guests short-term visitors.
Daily Counts Exchanged Messages
No. Messages

300
250
200
150
100
50
0
Jun Jul Aug Sep Oct Nov Dec
Date

Figure 7: Number daily coordination messages exchanged proxies seven-month
period.
Figure 7 plots number daily messages exchanged Fridays seven months
(June December, 2000). size daily counts ects large amount
coordination necessary manage various activities, high variability illustrates
dynamic nature domain (note low periods vacations final exams).
Figure 8(a) illustrates number meetings monitored user. seven
months, nearly 700 meetings monitored. users fewer 20 meetings,
others 250. users 50% meetings delayed (this includes
regularly scheduled meetings cancelled, instance due travel). Figure 8(b)
shows usually 50% delayed meetings autonomously delayed.
graph, repeated delays single meeting counted once. graphs show
5. user base system greatly reduced period due personnel relocations
student graduations, remains use smaller number users.

199

fiUser Delays vs. Autonomous Delays

Meetings Monitored vs. Meetings Delayed
400
350
300
250
200
150
100
50
0

140

Number Meetings

ito

ramanan

tambe

nair

scerri

modi

pynadath

jungh

Total Delays
Human Delays

120

Monitored
Delayed

kulkarni

Number Meetings

Scerri, Pynadath & Tambe

100
80
60
40
20
0
1

Users

2

3

4

5

6

7

8

Users

(a)

(b)

Figure 8: (a) Monitored vs. delayed meetings per user. (b) Meetings delayed autonomously
vs. hand.
agents acting autonomously large number instances, but, equally importantly,
humans often intervening, indicating critical importance adjustable autonomy
Friday agents.
seven-month period, presenter USC/ISI's TEAMCORE research group
presentations decided using auctions. Table 4 shows summary auction results.
Column 1 (\Date") shows dates research presentations. Column 2 (\No.
Bids") shows total number bids received decision. key feature
auction decisions made without 9 users entering bids; fact, one case,
4 bids received. Column 3 (\Best bid") shows winning bid. winner typically
bid < 1; 1 >, i.e., indicating user represents capable willing
presentation | high-quality bid. Interestingly, winner July 27 made
bid < 0; 1 >, i.e., capable willing. team able settle winner
despite bid highest possible, illustrating exibility. Finally, columns
4 (\Winner") 5 (\Method") show auction outcome. `H' column 5 indicates
auction decided human, `A' indicates decided autonomously. five
seven auctions, user automatically selected presenter. two manual
assignments due exceptional circumstances group (e.g., first-time visitor),
illustrating need AA.
Date
No. bids Best bid Winner Method
Jul 6, 2001
7
1,1
Scerri
H
Jul 20, 2001
9
1,1
Scerri

Jul 27, 2001
7
0,1
Kulkarni

Aug 3, 2001
8
1,1
Nair

Aug 3, 2001
4
1,1
Tambe

Sept 19, 2001
6
-,Visitor
H
Oct 31, 2001
7
1,1
Tambe

Table 4: Results auctioning research presentation slot.
200

fiTowards Adjustable Autonomy Real World
5.2 Evaluating Pros Cons E-Elves Use
general effectiveness E-Elves shown several observations.
E-Elves' operation, group members exchanged email messages announce
meeting delays. Instead, Fridays autonomously informed users delays, thus reducing
overhead waiting delayed members. Second, overhead sending emails recruit
announce presenter research meetings assumed agent-run auctions. Third,
web page, Friday agents post users' location, commonly used avoid
overhead trying track users manually. Fourth, mobile devices kept users
informed remotely changes schedules, enabling remotely delay
meetings, volunteer presentations, order meals, etc. Users began relying Friday
heavily order lunch one local \Subway" restaurant owner even suggested: \. . .
computers getting order food. . . might think marketing
them!!". Notice daily use E-Elves number different users occurred
MDP implementation AA replaced unreliable C4.5 implementation.
However, agents ensured users spent less time daily coordination (and
miscoordination), price paid. One issue users felt
less privacy location continually posted web monitored
agent. Another issue security private information credit card numbers
used ordering lunch. users adjusted agents monitor daily activities,
users adjusted behavior around agent. One example
behavior users preferring minute two early meeting lest
agent decide late delay meeting. general, since agents never made
catastrophically bad decisions users felt comfortable using agent frequently
took advantage services.
emphatic evidence success MDP approach that, since replacing
C4.5 implementation, agents never repeated catastrophic mistakes
enumerated Section 2.2. particular, Friday avoids errors error 3 Section
2.2 selecting strategy single, large action, higher EU
strategy many small Ds (e.g., DDDD). Friday avoids error 1, large cost
associated erroneous cancel action significantly penalizes EU cancellation.
Friday instead chooses higher-EU strategy first transfers control user
taking action autonomously. Friday avoids errors errors 2 4 selecting
strategies situation-sensitive manner. instance, agent's decision-making
quality low (i.e., high risk), agent perform coordination-change action
allow time user response agent get information.
words, exibly uses strategies e DeA, rather always using e (5)A strategy
discussed Section 2.2. indicates reasonably appropriate strategy chosen
situation. Although current agents occasionally make mistakes, errors
typically order transferring control user minutes earlier may
necessary. Thus, agents' decisions reasonable, though always optimal.6
6. inherent subjectivity user feedback makes determination optimality dicult.

201

fiScerri, Pynadath & Tambe
5.3 Strategy Evaluation
previous section looked application MDP approach E-Elves
address strategies particular. section, specifically examine strategies
E-Elves. show Fridays indeed follow strategies strategies followed
ones predicted model. show model led insight that,
turn, led dramatic simplification one part implementation. Finally, show
use strategies limited E-Elves application showing empirically
that, random configurations entities, optimal strategy one
transfer-of-control action 70% cases.
Figure 9 shows frequency distribution number actions taken per meeting
(this graph omits \wait" actions). number actions taken meeting corresponds
length part strategy followed (the strategy may longer,
decision made actions taken). graph shows MDP
followed complex strategies real world followed different strategies
different times. graph bears model's predictions different strategies would
required good solution AA problem E-Elves domain.
Table 5 shows EU values computed model strategy selected
MDP. Recall MDP explicitly models users' movements locations,
model assumes users move. Hence, order accurate
comparison model MDP's results, focus cases
user's location change (i.e., probability response constant).
EU values calculated using parameter values set Section 3.3. Notice,
MDP often perform Ds transferring control buy time reduce
uncertainty. model abstraction domain, actions, changes
user location, captured. Except slight discrepancy first case
match MDP's behavior model's predictions exact, provided
ignore actions beginning MDP strategies. Thus, despite model
considerably abstracted domain high correlation MDP
policies model's suggested strategies. Moreover, general properties policies
predicted model borne exactly. particular, recall model
predicted different strategies would required, strategy e would used,
generally strategies ending would best | properties MDP policies.
model predicts parameters vary greatly sucient find
single optimal strategy follow strategy situation. MDP
decision close auction instance E-Elves. pattern
behavior followed every time open role needs filled team. consistency
arises wait cost (since meetings same)
pattern incoming bids reasonably consistent (variations individuals' behavior
cancel look team whole). model predicts
parameters change, find optimal strategy parameters
execute strategy every time. However, since MDP worked effectively
meeting AA, MDP chosen implementing auction AA.
realized parameters vary greatly, concluded MDP could replaced
simple implementation optimal strategy. verify hypothesis, replaced
202

fiTowards Adjustable Autonomy Real World

No. meetings

No. actions per meeting
200
180
160
140
120
100
80
60
40
20
0
0

2

4
6
8
No. actions

10

12

Figure 9: frequency distribution number steps taken AA strategy
meeting scenario. actions taken meeting, meeting
cancelled Friday started AA reasoning.

Location

e
eA e DA MDP
Small meeting, active participant
oce
14.8 -277 41.9 42.05 DDe DA
@ dept. 14.8 -6E7 31.4 28.0 DDeA
@ meet loc. 14.8 -2E5 39.2 39.1
eA
Large meeting, passive participant
oce
14.6 -7E12 30.74 30.65 DDeA
@ dept. 14.6 -2E17 14.6 7.7
DDeA
@ meet loc. 14.5 -7E14 25.1 23.5
eA
Table 5: EU values simple strategies calculated model. last column
shows strategy actually followed MDP.

203

fiScerri, Pynadath & Tambe
Date No. Bids MDP eA
7/20/00
9
25% 26%
7/27/00
7
14% 20%
8/3/00
8
29% 23%
Table 6: Auction results. \MDP" column shows percentage available auction
time remaining MDP chose close auction. \eA" column
shows percentage available auction time remaining strategy eA,
EQde (t) proportional number bids received (\No. Bids" column),
would closed auction.

general MDP code three simple lines code implementing eA strategy,
determined optimal particular parameters problem. Using log files
recorded actual auctions reported (Scerri, Pynadath, & Tambe, 2001),
experimentally verified MDP eA strategy produced result.
Table 6 shows percentage available auction time remaining (e.g., auction
opened four days role performed, closing auction one day
would correspond 25%) MDP version eA version code closed
auction. number bids used estimate agent's expected decision quality.
timing auction closing close, certainly within hours. result
precisely MDP strategy implementations, MDP
implementation reactive incoming bids strategy implementation.
confirm need strategies phenomenon unique particular
settings E-Elves, experiment run randomly generated configurations
entities. wait cost configuration increased exponentially, rate
accrual varying configuration configuration. configurations contained
3 25 entities, randomly chosen Markovian response probabilities randomly
chosen, constant, decision-making quality. cost value action
randomly selected. configuration, agent could respond instantly,
lower decision quality entities. configuration,
optimal transfer-of-control strategy found. Figure 10(a) shows percentage optimal
strategies (z-axis) length (y-axis \jOpt. Strat.j"), separated according
rate wait costs accrued (x-axis, \Wait Cost Param"). figure shows
rate wait cost accrues low, optimal strategies length
one, agent handing control entity highest decision-making
quality. rate wait cost accrual high, strategies length two,
agent brie giving best decision maker opportunity make decision
taking back control acting wait costs became high. intermediate
values wait cost parameter, considerably variation length
optimal strategy. Figure 10(b) shows percentage optimal strategies length
wait cost parameter 0.12 (i.e., slice Figure 10(a)). Hence, strategies
often contained several transfers control several coordination changes. Thus,
experiment shows complex transfer-of-control strategies useful, E-Elves,
204

fiTowards Adjustable Autonomy Real World
range domains, especially wait costs neither negligible
accruing fast.
Strategy Lengths w = 0.12
35

% Opt. Strats.

30
25
% Opt. Strats.

100
90
80
70
60
50
40
30
20
10
01

2

20
15
10

3

|Opt. Strat.|

4

5

6

7

8 0

0.35 0.4
0.25 0.3
0.15 0.2
0.1
Wait
Cost
Param
0.05

5
0
1

(a)

2

3

4
5
|Opt. Strat.|

6

7

8

(b)

Figure 10: (a) Percentage optimal strategies certain length, broken according fast wait costs accruing. (b) Percentage optimal strategies
certain length wait cost parameter = 0.12.
Thus, shown MDP produces strategies Friday follows
strategies practice. Moreover, strategies followed ones predicted model.
practical use, followed prediction model, i.e., MDP
required auctions, able substantially reduce complexity one part
system. Finally, showed need strategies specifically phenomenon
E-Elves domain.

5.4 MDP Experiments
Experience using MDP approach AA E-Elves indicates effective
making reasonable AA decisions. However, order determine whether MDPs
generally useful tool AA reasoning, systematic experiments required.
section, present systematic experiments determine important properties
MDPs AA. MDP reward function designed result optimal strategy
followed state.
experiments, vary one parameters weights
different factors Equation 10. MDP instantiated range values
parameter policy produced value. case, total policy
defined 2800 states. policy analyzed determine basic properties
policy. particular, counted number states policy specifies ask,
delay, say user attending say user attending. statistics
show broadly policy changes parameters change, e.g., whether Friday gives
autonomy less cost coordination change increased. first
aim experiments simply confirm policies change desired expected
way parameters reward function changed. instance, Friday's expected
decision quality increased, states makes autonomous
205

fiScerri, Pynadath & Tambe
decision. Secondly, practical perspective critical understand sensitive
MDP policies small variations parameters, sensitivity would mean
small variations parameter values significantly impact MDP performance.
Finally, experiments reveal interesting phenomena.
first experiment looks effect 1 parameter Equation 10, represented delay MDP implementation team repair cost (function g Equation
12), policies produced delay MDP. parameter determines averse Friday changing coordination constraints. Figure 11 shows properties
policy change team repair cost value varied. x-axis gives value
team repair cost, y-axis gives number times action appears
policy. Figure 11(a) shows number times Friday ask user input.
number times transfer control exhibits interesting phenomenon: number
asks maximum intermediate value parameter. low values,
Friday \confidently" (i.e., decision quality high) make decisions autonomously,
since cost errors low, hence less value relinquishing autonomy.
high team repair costs, Friday \confidently" decide autonomously make
coordination change. intermediate region Friday uncertain needs
call user's decision making often. Furthermore, cost delaying
meeting increases, Friday delay meeting less (Figure 11(b)) tell team
user attending often (Figure 11(d)). so, Friday gives user less time
arrive meeting, choosing instead announce user attending.
Essentially, Friday's decision quality become close enough user's decision quality
asking user worth risk respond cost asking
input. Except jump value zero non-zero value,
number times Friday says user attending change (Figure 11(c)).
delay MDP use E-Elves team repair cost parameter set two. Around
value policy changes little, hence slight changes parameter lead
large changes policy.
second experiment, vary 2 parameter Equation 10, implemented
delay MDP variable team wait cost (function h Equation 13).
factor determines heavily Friday weigh differences
team expects user fulfill role user actually fulfill role.
particular, determines cost team members wait meeting room
user. Figure 12 shows changes policy parameter varied (again
x-axis shows value parameter y-axis shows number times
action appears policy). graph number times agent asks
policy (Figure 12(a)), exhibits phenomena 1 parameter varied,
i.e., increasing decreasing parameter increases. graphs show that,
cost teammates' time increases, Friday acts autonomously often (Figure 12(bd)). Friday asks whenever potential costs asking lower potential costs
errors makes { cost time waiting user decision increases, balance
tips towards acting. Notice phenomenon number asks increasing
decreasing occurs way 1 parameter; however, occurs
slightly different reason. case, waiting costs low, Friday's decision-making
quality high acts autonomously. waiting costs high, Friday cannot
206

fiTowards Adjustable Autonomy Real World

Number delays policy

68
66
64
62
60
58
56
54
52
50
48

# delays

# asks

Number asks policy

0

2
4
6
8
"Team repair cost" weight

140
130
120
110
100
90
80
70
60
50
40
30

10

0

(a)

Number Attending messages policy
# Attending

# attending

140
135
130
125
120
115
110
105
100
95
90
2
4
6
8
"Team repair cost" weight

10

(b)

Number Attending messages policy

0

2
4
6
8
"Team repair cost" weight

10

70
60
50
40
30
20
10
0
0

(c)

2
4
6
8
"Team repair cost" weight

(d)

Figure 11: Properties MDP policy team repair cost varied.

207

10

fiScerri, Pynadath & Tambe
afford risk user respond quickly, acts autonomously (despite
decision quality low). Figure 12(b) shows number delay actions taken
Friday increases, states meeting already delayed twice.
indicates normally expensive third delay meeting starts
become worthwhile cost teammates wait meeting room high.
delay MDP, value 1 used 2 . decision transfer control (i.e., ask)
particularly sensitive changes parameter around value|again, slight
changes significant impact.
Number Asks policy

Number Delays policy

70
# delays

# asks

50
40
30

100
80
60
40
20
0

20
0

2

4

6

8

10

0

"Cost teammates time" weight

4

6

8

10

(b)

Number Attending messages policy
30
# Attending

Number Attending messages policy
260
240
220
200
180
160
140
120
100
80
0

2

"Cost teammates time" weight

(a)

# Attending

Total
1st Delay
2nd Delay
3rd Delay

120

60

2
4
6
8
10
"Cost teammates time" weight

25
20
15
10
5
0
0

(c)

2
4
6
8
10
"Cost teammates time" weight

(d)

Figure 12: Properties MDP policy teammate time cost varied. (b) shows
number times meeting delayed states yet
delayed, delayed already, delayed
twice already.
third experiment, value 3 , weight joint task, varied
(Figure 13). E-Elves, value joint task includes value user
meeting value meeting without user. experiment, value
208

fiTowards Adjustable Autonomy Real World
meeting without user varied. Figure 13 shows policy changes value
meeting without user changes (again x-axis shows value parameter
y-axis shows number times action appears policy). graphs
show significantly instability values. large changes
result simultaneous change utility taking key actions expected
quality Friday's decision making, e.g., utility saying user attending much
higher meeting low value without user. current delay MDP,
value set 0.25, part graph insensitive small changes
parameter.
three experiments above, specific E-Elves parameters regions
graph small changes parameter lead significant changes policy.
However, regions graphs policy change dramatically small
changes parameter. indicates domains, parameters different
E-Elves, policies sensitive small changes parameters.

180
160
140
120
100
80
60
40
20
0
-10

Number delays policy
120
100
# delays

# asks

Number asks policy

80
60
40

-8

-6
-4
-2
0
Joint activity weight

20
-10

2

-8

(a)

Number Attending messages policy
20
# attending

# attending

180
160
140
120
-8

-6
-4
-2
0
Joint activity weight

2

(b)

Number Attending messages policy
200

100
-10

-6
-4
-2
0
Joint activity weight

15
10
5
0
-10

2

(c)

-8

-6
-4
-2
Joint activity weight

0

2

(d)

Figure 13: Properties MDP policy importance successful joint task
varied.
209

fiScerri, Pynadath & Tambe
experiments show three important properties MDP approach AA.
First, changing parameters reward function generally lead changes
policy expected desired. Second, value parameters uenced
policy, effect AA reasoning often reasonably small, suggesting small
errors model affect users greatly. Finally, interesting phenomena
number asks reaching peak intermediate values parameters revealed.
three previous experiments examined behavior MDP changes
parameters reward function changed. another experiment, central
domain-level parameter affecting behavior MDP, i.e., probability getting
user response cost getting response (corresponding f4 ), varied. Figure
14 shows number times Friday chooses ask (y-axis) varies
expected time get user response (x-axis) cost (each line
graph represents different cost). MDP performs expected, choosing ask
often cost low and/or likely get prompt response. Notice
that, cost low enough, Friday sometimes choose ask user even
long expected response time. Conversely, expected response time suciently
high, Friday assume complete autonomy. graph shows distinct
change number asks point (depending cost), outside change
point graphs relatively at. key reason fairly rapid change number
asks often difference quality Friday's user's decision
making fairly small range. mean response time increases, expected wait
costs increase, eventually becoming high enough Friday decide act autonomously
instead asking.

# Asks

Number Asks Policy
70
60
50
40
30
20
10
0
0.01

Cost = 0.0001
Cost = 0.2
Cost = 1.0

0.1
1
10
Mean Response Time

100

Figure 14: Number ask actions policy mean response time (in minutes) varied.
x-axis uses logarithmic scale.
conclude section quantitative illustration impact constraints
strategy selection. experiment, merged user-specified constraints
E-Elves users, resulting set 10 distinct constraints. started unconstrained
210

fiTowards Adjustable Autonomy Real World

Figure 15: (a) Number possible strategies (logarithmic). (b) Time required strategy
generation.
instance delay MDP added constraints one time, counting strategies
satisfied applied constraints. repeated experiments expanded
instances delay MDP, increased initial state space increasing
frequency decisions (i.e., adding values time-relative-to-meeting feature).
expansion results three new delay MDPs, artificial, uenced
real delay MDP. Figure 15a displays results (on logarithmic scale), line
corresponds original delay MDP (2760 states), lines B (3320 states), C (3880
states), (4400 states) correspond expanded instances. data point
mean five different orderings constraint addition. four MDPs, constraints
substantially reduce space possible agent behaviors. instance, original
delay MDP, applying 10 constraints eliminated 1180 2760 original states
consideration, reduced mean number viable actions per acceptable state
3.289 2.476. end result 50% reduction size (log10 ) strategy space.
hand, constraints alone provide complete strategy, since
plots stay well 0, even 10 constraints. Since none individual users
able/willing provide 10 constraints, cannot expect anyone add enough constraints
completely specify entire strategy. Thus, MDP representation associated
policy selection algorithms still far redundant.
constraints' elimination behaviors decreases time required strategy
selection. Figure 15b plots total time constraint propagation value iteration
four MDPs Figure 15a (averaged five constraint orderings).
data point mean five separate iterations, total 25 iterations
per data point. values zero-constraint case correspond standard value iteration without constraints. savings value iteration restricted strategy space
dramatically outweigh cost pre-propagating additional constraints. addition,
savings increase size MDP. original delay MDP (A),
28% reduction policy-generation time, largest MDP (D), 53%
reduction. Thus, introduction constraints provide dramatic acceleration
agent's strategy selection.

211

fiScerri, Pynadath & Tambe
6.

Related Work

discussed related work Section 1. section adds discussion.
Section 6.1, examine two representative AA systems { detailed experimental
results presented { explain results via model. illustrates
potential applicability model systems. Section 6.2, examine AA
systems areas related work, meta-reasoning, conditional planning
anytime algorithms.

6.1 Analyzing AA Work Using Strategy Model
Goodrich, Olsen, Crandall, Palmer (2001) report tele-operated teams robots,
user's high-level reasoning robots' low-level skills required
achieve task. Within domain, examined effect user neglect
robot performance. idea user neglect similar idea entities taking time
make decisions; case, user \neglects" robot, joint task takes longer
perform. domain, coordination constraint user input must arrive
robot work low-level actions needs perform. Four control systems
tested robot, giving different amount autonomy robot,
performance measured user neglect varied.
Although quite distinct E-Elves system, mapping Goodrich's team robots
AA problem formulation provides interesting insights. system
interesting feature entity robot call decision, i.e., user,
part team. Changing autonomy robot effectively changes nature
coordination constraints user robot. Figure 16 shows performance
(y-axis) four control policies amount user neglect increased (x-axis).
experiments showed higher robot autonomy allowed operator \neglect"
robot without serious impact performance.
notion transfer-of-control strategies used qualitatively predict
behavior observed practice, even though Goodrich et al. (2001) use
notion strategies. lowest autonomy control policy used Goodrich et al. (2001)
pure tele-operation one. Since robot cannot resort decision making,
represent control policy strategy U , i.e., control indefinitely hands
user. second control policy allows user specify waypoints on-board
intelligence works details getting waypoints. Since robot highlevel decision-making ability, strategy simply give control user. However,
since coordination robot user abstract, i.e., coordination
constraints looser, wait cost function less severe. human giving less
detailed guidance fully tele-operated case (which good according
(Goodrich et al., 2001)), hence use lower value expected quality user
decision. denote approach Uw p distinguish fully tele-operated case.
next control policy allows robot choose waypoints given user
inputs regions interest. robot accept waypoints user. ability
robot calculate waypoints modeled D, since effectively changes
coordination entities, removing user's need give waypoints. model
control policy strategy U DU . final control policy full autonomy, i.e., A.
212

fiTowards Adjustable Autonomy Real World
Performance



UDU
U wp

U

Neglect

(a)
Goodrich robot operation EU
60
40
EU

20
0
-20
-40
-60
2

1.5

(b)

1
p

0.5

0

Figure 16: Goodrich al's various control strategies plotted neglect. (a) Experimental results. Thinner lines represent control systems intelligence
autonomy. (b) Results theoretically derived model strategies presented article (p parameter probability response function).
Robot decision making inferior user, hence robot's decision quality less
user's. graphs four strategies, plotted probability response
parameter (getting smaller right, match \neglect" Goodrich et al graph)
shown Figure 16. Notice shape graph theoretically derived model,
shown Figure 16(b), qualitatively shape experimentally derived
graph, Figure 16(a). Hence, theory predicted qualitatively performance
found experimentation.
common assumption earlier AA work entity asked
decision make decision promptly, hence strategies handling contingency
213

fiScerri, Pynadath & Tambe
lack response required. example, Horvitz's (1999) work using
decision theory aimed developing general, theoretical models AA reasoning
user workstation. prototype system, called LookOut, helping users manage
calendars implemented test ideas (Horvitz, 1999). Although systems
distinctly different E-Elves, mapping problem formulation allows us
analyze utility approaches across range domains without implement
approach domains.
critical difference Horvitz's work work LookOut
address possibility receiving (timely) response. Thus, complex strategies
required. typical case LookOut, agent three options: take
action, take action, engage dialog. central factor uencing
decision whether user particular goal action would aid, i.e., user
goal, action useful, he/she goal, action
disruptive. Choosing act act corresponds pursuing strategy A.7 Choosing
seek user input corresponds strategy U . Figure 17(a) shows graph different
options plotted probability user goal (corresponds Figure 6
Horvitz (1999)). agent's expected decision quality, EQdA (t) derived Equation
2 Horvitz (1999). (In words, Horvitz's model performs detailed calculations
expected decision quality.) model predicts selection strategies
Horvitz does, i.e., choosing strategy EQdA (t) low, U otherwise (assuming
two strategies available). However, model predicts something
Horvitz consider, i.e., rate wait costs accrue becomes
non-negligible choice simple. Figure 17(b) shows EU two
strategies changes rate wait costs accruing increased. fact optimal
strategy varies wait cost suggests Horvitz's approach would immediately
appropriate domain wait costs non-negligible, e.g., would need
modified many multi-agent settings.

6.2 Approaches AA
Several different approaches taken core problem whether
transfer decision-making control. example, Hexmoor examines much time agent
AA reasoning (Hexmoor, 2000). Similarly, Dynamic Adaptive Autonomy
framework, group agents allocates votes amongst themselves, hence defining amount
uence agent decision thus, definition, autonomy
agent respect decision (Barber, Martin, & Mckay, 2000b). related
application meeting scheduling Cesta, Collia, D'Aloisi (1998) taken approach
providing powerful tools users constrain monitor behavior proxy
agents, agents explicitly reason relinquishing control user.
least work done multiagent context, possibility multiple
transfers control considered.
Complementing work, researchers focused issues architectures
AA. instance, AA interface 3T architecture (Bonasso, Firby, Gat, Kortenkamp,
7. consider choosing act autonomous decision, hence categorize way autonomous action

214

fiTowards Adjustable Autonomy Real World

Horvitzs EU Calculations Wait Cost
1

EU

0
-1
-2
0

0.2
0.4
0.6
0.8
Probability User Goal

1

(a)

Horvitzs EU Calculations Wait Cost
0.4

EU

0.2
0
-0.2
-0.4
-0.6
0

0.05 0.1 0.15 0.2 0.25 0.3

(b)

w

Figure 17: EU different agent options. solid (darkest) line shows EU taking
autonomous action, dashed (medium dark) line shows EU autonomously deciding act dotted line shows EU transferring
control user. (a) Plotted probability user goal,
wait cost. (b) plotted wait cost, fixed probability user goal.
Miller, & Slack, 1997) implemented solve human-machine interaction problems
experienced number NASA projects (Brann, Thurman, & Mitchell, 1996).
experiences showed interaction system required way
deliberative layer detailed control actuators. AA controls layers
encapsulated referred 3T's fourth layer { interaction layer
215

fiScerri, Pynadath & Tambe
(Schreckenghost, 1999). similar area AA technology required safety-critical
intelligent software, controlling nuclear power plants oil refineries (Musliner
& Krebsbach, 1999). work resulted system called AEGIS (Abnormal Event
Guidance Information System) combines human agent capabilities rapid
reaction emergencies petro-chemical refining plant. AEGIS features shared task
representation users intelligent system work (Goldman,
Guerlain, Miller, & Musliner, 1997). key hypothesis work model needs
multiple levels abstraction user interact level see fit.
Interesting work Fong, Thorpe, Baur (2002) extended idea tele-operated
robotics re-defining relationship robot user collaborative one,
rather traditional master-slave configuration. particular, robot treats
human resource perform perceptual cognitive functions robot
determines cannot adequately perform. However, yet work looked
possibility user available provide input required, would require
robot perform complex transfer-of-control reasoning.
previous work AA ignored complex strategies AA, work
research fields potentially relevant. example, research issues addressed fields mixed-initiative decision-making (Collins, Bilot, Gini, & Mobasher,
2000b), anytime algorithms (Zilberstein, 1996), multi-processor scheduling (Stankovic, Ramamritham, & Cheng, 1985), meta-reasoning (Russell & Wefald, 1989), game theory (Fudenberg & Tirole, 1991), contingency plans (Draper, Hanks, & Weld, 1994; Peot &
Smith, 1992) have, least superficial, similarities AA problem. However,
turns core assumptions focus research areas different
enough algorithms developed related fields directly applicable
AA problem.
mixed-initiative decision making human user assumed continually available
(Collins et al., 2000b; Ferguson & Allen, 1998), negating need reasoning
likelihood response. Furthermore, often little time pressure coordination
constraints. Thus, basic problem transferring control human
agent common mixed-initiative decision making AA, assumptions
quite different leading distinct solutions. Likewise, related research fields make
distinctly different assumptions lead distinctly different solutions. instance,
contingency planning (Draper et al., 1994; Peot & Smith, 1992) deals problem
creating plans deal critical developments environment. Strategies related
contingency planning plans deal specific contingency
entity making decision manner maintains coordination. However, contingency planning, key diculty creating plans. contrast, AA, creating
strategies straightforward key diculty choosing strategies.
contribution recognizing need strategies addressing AA problem, instantiating strategies via MDPs, development general, domain-independent
reward function leads MDP choosing optimal strategy particular situation.
Similarly, another related research area meta-reasoning (Russell & Wefald, 1989).
Meta-reasoning work looks online reasoning computation. type meta-reasoning,
closely related AA, chooses sequences computations different ex216

fiTowards Adjustable Autonomy Real World
pected quality running time, subject constraint choosing highest-quality
sequence computations possible (because takes long) (Russell & Wefald,
1989). idea treat computations actions \meta-reason" EU
certain combinations computation (base-level) actions. output metareasoning sequence computations executed sequence. AA parallels metareasoning consider reasoning transferring control entities reasoning
selecting computations, i.e., think entities computations. However, AA,
aim one entity make high-quality decision, meta-reasoning, aim
sequence computations high quality. Moreover, meta-reasoning
assumption computations guaranteed return timely result executed,
apply AA. Finally, meta-reasoning looks sequence computations use
fixed amount time, AA reasons trading extra time better decision
(possibly buying time action). Thus, algorithms developed meta-reasoning
applicable AA.
Another research area conceptual similarity AA field anytime algorithms (Zilberstein, 1996). anytime algorithm quickly finds initial solution
incrementally tries improve solution stopped. AA problem similar
assume agent make immediate decision, problem
property solution always available (an important property anytime
algorithm). However, case general, i.e., agent always
answer. Furthermore, anytime algorithms generally need deal multiple,
distributed entities, opportunity change coordination (i.e., using
action).
Multi-processor scheduling looks assigning tasks nodes order meet certain
time constraints (Stankovic et al., 1985). entities thought \nodes", AA
assigning tasks nodes. multiprocessor scheduling, quality
computation performed nodes usually assumed equal, i.e., nodes
homogeneous. Thus, reasoning trades quality time required,
AA. Moreover, deadlines externally imposed multi-processor scheduling algorithms,
rather exibly reasoned AA. Multi-processor scheduling algorithms
sometimes deal node rejecting task cannot fulfill time constraints
network failures. However, AA problem focuses failure get response
central issue load balancing auxiliary issue, multi-processor scheduling
opposite focus. difference focus leads algorithms developed
multiprocessor scheduling community well suited AA (and vice versa).
7.

Conclusions

Adjustable autonomy critical success real-world agent systems allows
agent leverage skills, resources decision-making abilities entities,
human agent. Previous work addressed AA context single-agent
single-human scenarios, solutions scale increasingly complex multiagent systems. particular, previous work used rigid, one-shot transfers control
consider team costs and, importantly, consider possibility costly

217

fiScerri, Pynadath & Tambe
miscoordination team members. Indeed, applied rigid transfer-of-control
approach multi-agent context, failed dramatically.
article makes three key contributions enable application AA
complex multiagent domains. First, article introduces notion transfer-of-control
strategy. transfer-of-control strategy consists conditional sequence two types
actions: (i) actions transfer decision-making control (ii) actions change
agent's pre-specified coordination constraints team members, aimed minimizing
miscoordination costs. strategies allow agents plan sequences transfer-of-control
actions. Thus, strategy allows agent transfer control entities best able make
decisions, buy time decisions made still avoid miscoordination | even
entity control transferred fails make decision. Additionally,
introduced idea changing coordination constraints mechanism giving
agent opportunity provide high-quality decisions, showed changes
can, cases, effective way increasing team's expected utility.
second contribution article mathematical model AA strategies
allows us calculate expected utility strategies. model shows
complex strategies indeed better single-shot strategies situations,
always superior. fact, analysis showed particular strategy dominates
whole space AA decisions; instead, different strategies optimal different
situations.
third contribution article operationalization notion transferof-control strategies via Markov Decision Processes general reward function
leads MDP find optimal strategies multiagent context. general, domainindependent reward function allow approach potentially applied
multi-agent domains. implemented, applied, tested MDP approach AA reasoning real-world application supporting researchers daily activities. Daily use
showed MDP approach effective balancing need avoid risky autonomous
decisions potential costly miscoordination. Furthermore, detailed experiments
showed policies produced MDPs desirable properties, transferring control user less often probability getting timely response low.
Finally, practical experience system revealed users require ability manipulate AA reasoning agents. end, introduced constraint language
allows user limit range behavior MDP exhibit. presented
algorithm processing constraints, showed desirable property
reducing time takes find optimal policies.
8.

Future Work

model AA presented article suciently rich model wide variety
interesting applications. However, key factors modeled
current formulation required domains. One key issue allow agent
factor AA reasoning agents AA reasoning. instance,
Elves domain, one agent likely decide delay meeting, another agent may
wait decision avoid asking user. Conversely, agent take
back control decision knows another agent going continue waiting user input,
218

fiTowards Adjustable Autonomy Real World
might continue wait input. interactions substantially increase
complexity reasoning agent needs perform. article, assumed
agent finding transfer-of-control strategy single, isolated decision.
general, many decisions made agent able
ignore interactions decisions. example, transferring control many
decisions user, reduces probability getting prompt response them.
Reasoning interactions add complexity required reasoning
agent.
Another focus future work generalizing AA decision making allow
types constraints | coordination constraints | taken account.
would turn require generalization concept action include types
stop-gap actions may lead different types strategies agent could pursue.
Additionally, transfer-of-control actions could generalized allow parts decision
transferred, e.g., allow input received user without transferring total
control him/her, allow actions could performed collaboratively. Similarly,
actions reversible, agent could make decision allow user reverse
it. hope generalizations would improve applicability adjustable
autonomy research complex domains.
Acknowledgments

research supported DARPA award no. F30602-98-2-0108. effort
managed Air Force Research Labs/Rome site. article unifies, generalizes, significantly extends approaches described previous conference papers (Scerri et al., 2001;
Scerri, Pynadath, & Tambe, 2002; Pynadath & Tambe, 2001). thank colleagues,
especially, Craig Knoblock, Yolanda Gil, Hans Chalupsky Tom Russ collaborating
Electric Elves project. would thank JAIR reviewers
useful comments.

219

fiScerri, Pynadath & Tambe
Appendix A: Example Instantiation Model

Appendix, present detailed look one possible instantiation AA model.
use instantiation calculate EU commonly used strategies show
EU varies parameters rate wait cost accrual time
transfers control performed. instantiation, agent, A, one entity
call decision (i.e., user U ), hence E = fA; U g. W (t), use following
function:
(

!t
W (t) = !! exp
exp! otherwise

(15)

exponential wait cost function ects idea big delay much worse
small one. polynomial similar function could used exponential
used since makes mathematics cleaner. probability response use:
P>(t) = exp . Markovian response probability ects entity likely
respond next point time previous point. users moving around dynamic environment, turns reasonable approximation.
entities' decision-making quality constant time, particular, EQdA (t) =
EQdU (t) = . Assuming constant decision-making quality always accurate
dynamic environment since information available entity may change (hence uencing
ability make decision) however, decisions involving static facts preferences
decision-making quality relatively constant. functions coarse approximation range interesting applications, including E-Elves. Table 7 shows resulting
instantiated equations simple strategies (For convenience let = !). Figures
18(a) (b) show graphically EU eA strategy varies along different axes (w
parameter wait cost function, higher w means faster accruing wait costs
p parameter response probability function, higher p means faster response).
Notice EU depends transfer time (T) much (the user's
decision quality). Figure 18(d) shows value (as discussed earlier).
Figure 18(c) compares EU e DeA e strategies. complex
transfer-of-control strategy (i.e., transfers control makes), atter
EU graph plotted wait cost (w) response probability (p) parameters.
particular, fall-off wait costs high probability response low
dramatic complex strategy.

Appendix B: Constraint Propagation Algorithm Correctness

Section 4.4, examined need user-specified constraints conjunction
MDP-based approach strategies. must thus extend standard MDP policy
evaluation algorithms support evaluation strategies accounting
standard quantitative reward function new qualitative constraints. appendix
provides novel algorithm developed evaluate strategies accounting
220

fiTowards Adjustable Autonomy Real World

5
4.5
4
3.5
3
0

20
15
10
5
0
-5

1.5
0.1

w 0.2

0.3

0.5

1 p

0

10

(a)

T20

30

40

(b)
Value
0.16
0.12
0.08
0.04
0
-0.04

5
0
-5
0.1

w 0.2

0.3

4

20
16
12
beta
8

0.4

1.2
0.8p

0.1 0.2
w0.3 0.4 0.5

(c)

1
0.75
0.5 p
0.25

(d)

Figure 18: Equation 17, i.e., strategy eA plotted (a) ! (i.e., w, rate
wait costs accrue) (i.e., p likelihood response) (b) (transfer
time)and beta (the user's decision quality). (c) Comparing strategies e DeA
e (dotted line e ). (d) value D.

221

fiScerri, Pynadath & Tambe


EUed = exp !(

= ! exp
EUeA

(

1) + exp



1)

!
+fi


(ff

fi)

(16)

!
+fi


(17)

EUedDeAt =
(18)
!

value
! (exp 1) + (1 exp ) + ! exp
(exp exp ) +


(Dcost )(exp exp ) + ! exp! (exp !Dvalue 1)(exp exp )
exp (Dcost + !(exp! exp!( Dvalue ) + exp!(T Dvalue ) ))
Table 7: Instantiated AA EU equations simple transfer control strategies.
both. present detailed proof algorithm's output correct strategy
(i.e., strategy highest expected utility, subject user-specified constraints).
standard MDP value iteration algorithm, value strategy particular
state single number, expected utility U . addition two types
constraints, value tuple hF; N; U i. F represents strategy's ability satisfy
forbidding constraints; therefore, boolean indicating whether state forbidden
not. N represents strategy's ability satisfy necessary constraints; therefore,
set requiring constraints satisfied. traditional value iteration,
U expected reward. instance, value state, V (s) = htrue; fcrs g; 0:3i,
executing policy state achieve expected value 0.3 satisfy
required-state constraint crs . However, guaranteed satisfy requiredstate, required-action, constraints. addition, forbidden, nonzero
probability violating forbidden-action forbidden-state constraint. record
forbidding constraints policy violates, since violating one equally
bad. record requiring constraints policy satisfies, since satisfying
constraints preferable satisfying them. Therefore, size
value function grows linearly number requiring constraints, independent
number forbidding constraints.
Following form standard value iteration, initialize value function
states considering immediate value strategy given state, without
lookahead. precisely:

V 0 (s)

*

_

c2Cfs

+

c(s); fc 2 Crs jc(s)g ; RS (s)

(19)

Thus, state forbidden forbidden-state constraints immediately apply,
satisfies required-state constraints immediately apply. standard value
iteration, expected utility value reward function state.
222

fiTowards Adjustable Autonomy Real World
value iteration, must define updated value function V t+1 refinement
previous iteration's value function, V . States become forbidden V t+1
violate constraints directly successors forbidden according V .
States satisfy requirements satisfy directly successors satisfy
requirement. simplify following expressions, define 0 set
successors: fs0 2 jMssa 0 > 0g. following expression provides precise definition
iterative step:
*

_
_
_
max
c(s) _
c(s; a) _
F 0;
a2A c2C

0
0
0
c2Cfa V (s )=hF ;N ;U 0 i;s0 2S 0
fs
\
fc 2 Crsjc(s)g [ fc 2 Cra jc(s; a)g [ N 0;
V (s0 )=hF 0 ;N 0 ;U 0 i;s0 2S 0
+
X
RS (s) + R(s; a) + Mssa 0 U 0
(20)
V (s0 )=hF 0 ;N 0 ;U 0 i;s0 2S 0
standard value iteration, iterative step specifies maximization possible choices action. However, two additional components represent value
strategy respect constraints, longer obvious comparison
function use evaluating candidate actions. Therefore, perform maximization
using following preference ordering, x means preferable x:
ht; N; U

f; N 0; U 0ff
hF; N; U
F; N 0 N; Uff0
hF; N; U F; N; U 0 > U

V t+1 (s)

words, satisfying forbidden constraint takes highest priority, satisfying
requiring constraints second, increasing expected value last. define optimal
action, P (s), action, a, final V (s) expression maximized.
Despite various set operations Equation 20, time complexity iteration
step exceeds standard value iteration linear factor, namely number
constraints, jCfs j + jCfa j + jCrsj + jCra j. eciency derives fact
constraints satisfied/violated independently other. determination whether
single constraint satisfied/violated requires time standard value
iteration, hence overall linear increase time complexity.
expected value lowest priority, separate iterative step
Equation 20 two phases: constraint propagation value iteration.
constraint-propagation phase, compute first two components value function, hF; N; i. value-iteration phase computes third component, h; ; U i,
standard value iteration. However, ignore state/action pairs that, according
results constraint propagation, violate forbidding constraint (ht; N; i) requiring constraint (hf; N Crs [ Cra ; i). component-wise independence
Equation 20, two-phase algorithm computes identical value function original,
single-phase version (over state/action pairs satisfy constraints).
rest Appendix provide proof correctness modified value
iteration policy. Given policy, P , constructed according algorithm, must
223

fiScerri, Pynadath & Tambe
show agent following P obey constraints specified user. agent
begins state, 2 , must prove satisfy constraints
V (s) = hf; Cra [ Crs ; U i. prove results forbidding requiring constraints
separately.

Theorem 1 agent following policy, P , value function, V , generated Section 4.4, state 2 violate forbidding constraint probability zero
V (s) = hf; N; U (for U N ).
Proof: prove theorem induction subspaces states, classified

\close" violating forbidding constraint. precisely, partition
state space, , subsets, Sk , defined contain states violate forbidding
constraint minimum k state transitions. words, S0 contains states
violate forbidding constraint directly; S1 contains states violate
forbidding constraints themselves, successor state (following transition
probability function, P ) (i.e., successor state S0 ); S2 contains states
violate forbidding constraints, successors do,
least one successor state successor state (i.e., successor state
S1 ); etc. jS j nonempty subsets mutually exclusive sequence.
make partition exhaustive, special subset, S1 , contains states
agent never violate forbidding constraint following P . first show, induction
k, 8s 2 Sk (0 k jS j), V (s) = ht; N; U i, required theorem.
Basis step (S0): definition, agent violate forbidding constraint 2 S0 .
Therefore, either 9c 2 Cfs c(s) = 9c 2 Cfa c(s; P (s)) = t,
know, Equation 20, V (s) = ht; N; U i.
Inductive step (Sk ; 1 k jS j): Assume, induction hypothesis, 8s0 2
Sk 1 , V (s0 ) = ht; N 0 ; U 0 i. definition Sk , state, 2 Sk , least one
successor state, s0 2 Sk 1 . Then, according Equation 20, V (s) = ht; N; U i,
disjunction 0 must include s0 , F 0 = t.
Therefore, induction, know 2 Sk (0 k jS j), V (s) = ht; N; U i.
show 8s 2 S1 , V (s) = hf; N; U i. prove, induction t, that,
state, 2 S1, V (s) = hf; N; U i.
Basis step (V 0 ): definition, 2
S1 , thereff cannot exist c 2 Cfs
c(s) = t. Then, Equation 19, V 0 (s) = f; N 0 ; U 0 .
Inductive step (V ; > 0): Assume,
inductive
hypothesis, that, s0 2 S1 ,

V 1 (s0 ) = hf; N 0 ; U 0 i. know V (s) = f; N ; U three disjunctions
Equation 20 false. first false, described basis step. second term
similarly false, since, definition S1, cannot exist c 2 Cfa
c(s; P (s)) = t. evaluating third term, first note 0 S1. words,
successor states S1 (if successor s0 2 Sk finite k,
2 Sk+1). Since successors S1 , know, inductive hypothesis,
disjunction V 1 successors
fffalse. Therefore, three disjunctive


terms Equation 20 false, V (s) = f; N ; U .
Therefore, induction, know 2 S1 , V (s) = hf; N; U i. definition
state partition, two results prove theorem required. 2
224

fiTowards Adjustable Autonomy Real World
Theorem 2 agent following policy, P , value function, V , generated described
Section 4.4, state 2 satisfy every requiring constraint
probability one V (s) = hF; Cra [ Crs ; U (for U F ).
Proof Sketch: proof parallels Theorem 1, state partition, Sk ,
k corresponds maximum number transitions satisfying requiring
constraint. However, here, states S1 violate constraint, rather

satisfy it. cycles state space prevent guarantee satisfying requiring
constraint within fixed number transitions, although probability satisfaction
limit may 1. current constraint semantics, decided
situation fails satisfy constraint, algorithm behaves accordingly. cycles
effect handling forbidding constraints, where, saw Theorem 1,
need consider minimum -length trajectory. 2
proofs two theorems operate independently, policy-specified action
satisfy constraints, action exists. precedence forbidding constraints
requiring ones effect optimal action states. However,
con icting forbidding requiring constraints state, preference ordering
causes agent choose policy satisfies forbidding constraint violates
requiring constraint. agent make opposite choice simply change
preference ordering Section 4.4. Regardless choice, Theorems 1 2,
agent use value function, V , identify existence violation
notify user violation possible constraint con ict.
References

Barber, K., Goel, A., & Martin, C. (2000a). Dynamic adaptive autonomy multi-agent
systems. Journal Experimental Theoretical Artificial Intelligence, 12 (2), 129{
148.
Barber, K. S., Martin, C., & Mckay, R. (2000b). communication protocol supporting
dynamic autonomy agreements. Proceedings PRICAI 2000 Workshop Teams
Adjustable Autonomy, pp. 1{10, Melbourne, Australia.
Bonasso, R., Firby, R., Gat, E., Kortenkamp, D., Miller, D., & Slack, M. (1997). Experiences architecture intelligent reactive agents. Journal Experimental
Theorectical Artificial Intelligence, 9 (1), 237{256.
Brann, D., Thurman, D., & Mitchell, C. (1996). Human interaction lights-out automation: field study. Proceedings 1996 Symposium Human Interaction
Complex Systems, pp. 276{283, Dayton, USA.
Cesta, A., Collia, M., & D'Aloisi, D. (1998). Tailorable interactive agents scheduling
meetings. Lecture Notes AI, Proceedings AIMSA'98, No. 1480, pp. 153{166.
Springer Verlag.
Chalupsky, H., Gil, Y., Knoblock, C., Lerman, K., Oh, J., Pynadath, D., Russ, T., & Tambe,
M. (2001). Electric Elves: Applying agent technology support human organizations.
International Conference Innovative Applications AI, pp. 51{58.
225

fiScerri, Pynadath & Tambe
Collins, J., Bilot, C., Gini, M., & Mobasher, B. (2000a). Mixed-initiative decision-support
agent-based automated contracting. Proceedings International Conference
Autonomous Agents (Agents'2000).
Collins, J., Bilot, C., Gini, M., & Mobasher, B. (2000b). Mixed-initiative decision support
agent-based automated contracting. Proceedings International Conference
Autonomous Agents (Agents'2000), pp. 247{254.
Dorais, G., Bonasso, R., Kortenkamp, D., Pell, B., & Schreckenghost, D. (1998). Adjustable
autonomy human-centered autonomous systems mars. Proceedings
First International Conference Mars Society, pp. 397{420.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning information gathering
contingent execution. Hammond, K. (Ed.), Proc. Second International Conference Artificial Intelligence Planning Systems, pp. 31{37, University Chicago,
Illinois. AAAI Press.
Ferguson, G., Allen, J., & Miller, B. (1996). TRAINS-95 : Towards mixed-initiative
planning assistant. Proceedings Third Conference Artificial Intelligence
Planning Systems, pp. 70{77.
Ferguson, G., & Allen, J. (1998). TRIPS : intelligent integrated problem-solving assistant. Proceedings Fifteenth National Conference Artificial Intelligence(AAAI98), pp. 567{573, Madison, WI, USA.
Fong, T., Thorpe, C., & Baur, C. (2002). Robot partner: Vehicle teleoperation collaborative control. Workshop Multi-Robot Systems, Naval Research Laboratory,
Washington, D.C.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press, Cambridge, Massachusetts.
Goldman, R., Guerlain, S., Miller, C., & Musliner, D. (1997). Integrated task representation indirect interaction. Working Notes AAAI Spring Symposium
Computational Models Mixed-Initiative Interaction.
Goodrich, M., Olsen, D., Crandall, J., & Palmer, T. (2001). Experiments adjustable
autonomy. Hexmoor, H., Castelfranchi, C., Falcone, R., & Cox, M. (Eds.), Proceedings IJCAI Workshop Autonomy, Delegation Control: Interacting
Intelligent Agents.
Gunderson, J., & Martin, W. (1999). Effects uncertainty variable autonomy maintainance robots. Agents'99 Workshop Autonomy Control Software, pp. 26{34.
Hexmoor, H. (2000). cognitive model situated autonomy. Proceedings PRICAI2000, Workshop Teams Adjustable Autonomy, pp. 11{20, Melbourne, Australia.
Hexmoor, H., & Kortenkamp, D. (2000). Introduction autonomy control software. Journal
Experiemental Theoretical Artificial Intelligence, 12 (2), 123{128.
Horvitz, E. (1999). Principles mixed-initiative user interfaces. Proceedings ACM
SIGCHI Conference Human Factors Computing Systems (CHI'99), pp. 159{166,
Pittsburgh, PA.
226

fiTowards Adjustable Autonomy Real World
Horvitz, E., Jacobs, A., & Hovel, D. (1999). Attention-sensitive alerting. Proceedings
Conference Uncertainty Artificial Intelligence (UAI'99), pp. 305{313, Stockholm, Sweden.
Lesser, V., Atighetchi, M., Benyo, B., Horling, B., Raja, A., Vincent, R., Wagner, T., Xuan,
P., & Zhang, S. (1999). UMASS intelligent home project. Proceedings
Third Annual Conference Autonomous Agents, pp. 291{298, Seattle, USA.
Mitchell, T., Caruana, R., Freitag, D., McDermott, J., & Zabowski, D. (1994). Experience
learning personal assistant. Communications ACM, 37 (7), 81{91.
Mulsiner, D., & Pell, B. (1999). Call papers: AAAI spring symposium adjustable
autonomy. www.aaai.org.
Musliner, D., & Krebsbach, K. (1999). Adjustable autonomy procedural control
refineries. AAAI Spring Symposium Agents Adjustable Autonomy, pp.
81{87, Stanford, California.
Peot, M. A., & Smith, D. E. (1992). Conditional nonlinear planning. Hendler, J. (Ed.),
Proc. First International Conference Artificial Intelligence Planning Systems, pp.
189{197, College Park, Maryland. Morgan Kaufmann.
Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons.
Pynadath, D., Tambe, M., Arens, Y., Chalupsky, H., Gil, Y., Knoblock, C., Lee, H., Lerman,
K., Oh, J., Kamachandran, S., Rosenbloom, P., & Russ, T. (2000). Electric-elves:
Immersing agent organization human organization. Proceedings
AAAI Fall Symposium Socially Intelligent Agents { Human Loop.
Pynadath, D., & Tambe, M. (2001). Revisiting Asimov's first law: response call
arms. Intelligent Agents VIII Proceedings International workshop Agents,
Theories, Architectures Languages (ATAL'01).
Quinlan, J. R. (1993). C4.5: Programs machine learning. Morgan Kaufmann, San
Mateo, CA.
Russell, S. J., & Wefald, E. (1989). Principles metareasoning. Brachman, R. J.,
Levesque, H. J., & Reiter, R. (Eds.), KR'89: Principles Knowledge Representation
Reasoning, pp. 400{411. Morgan Kaufmann, San Mateo, California.
Scerri, P., Pynadath, D., & Tambe, M. (2001). Adjustable autonomy real-world multiagent environments. Proceedings Fifth International Conference Autonomous Agents (Agents'01), pp. 300{307.
Scerri, P., Pynadath, D., & Tambe, M. (2002). elf acted autonomously: Towards
theory adjustable autonomy. First International Joint Conference Autonomous Agents Multi-Agent Systems (AAMAS'02).
Schreckenghost, D. (1999). Human interaction control software supporting adjustable
autonomy. Musliner, D., & Pell, B. (Eds.), Agents Adjustable Autonomy,
AAAI 1999 Spring Symposium Series, pp. 116{119.
Stankovic, J., Ramamritham, K., & Cheng, S. (1985). Evaluation exible task scheduling algorithm distributed hard real-time system. IEEE Transactions Computers, 34 (12), 1130{1143.
227

fiScerri, Pynadath & Tambe
Tambe, M. (1997). Towards exible teamwork. Journal Artificial Intelligence Research
(JAIR), 7, 83{124.
Tambe, M., Pynadath, D. V., Chauvat, N., Das, A., & Kaminka, G. A. (2000). Adaptive
agent integration architectures heterogeneous team members. Proceedings
International Conference MultiAgent Systems, pp. 301{308.
Zilberstein, S. (1996). Using anytime algorithms intelligent systems. AI Magazine, 17 (3),
73{83.

228


