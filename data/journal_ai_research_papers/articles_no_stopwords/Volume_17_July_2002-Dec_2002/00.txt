Journal Artificial Intelligence Research 17 (2002) 1-33

Submitted 8/01; published 7/02

Critical Assessment
Benchmark Comparison Planning
Adele E. Howe
Eric Dahlman

Computer Science Department
Colorado State University, Fort Collins, CO 80523

howe@cs.colostate.edu
dahlman@cs.colostate.edu

Abstract
Recent trends planning research led empirical comparison becoming commonplace. field started settle methodology comparisons,
obvious practical reasons requires running subset planners subset problems.
paper, characterize methodology examine eight implicit assumptions
problems, planners metrics used many comparisons. problem assumptions are: PR1) performance general purpose planner
penalized/biased executed sampling problems domains, PR2) minor syntactic
differences representation affect performance, PR3) problems solvable STRIPS capable planners unless require ADL. planner assumptions are:
PL1) latest version planner best one use, PL2) default parameter settings
approximate good performance, PL3) time cut-offs unduly bias outcome.
metrics assumptions are: M1) performance degrades similarly planner run
degraded runtime environments (e.g., machine platform) M2) number plan
steps distinguishes performance. find assumptions supported
empirically; particular, planners affected differently assumptions.
conclude call community devote research resources improving state
practice especially enhancing available benchmark problems.

1. Introduction
recent years, comparative evaluation become increasingly common demonstrating
capabilities new planners. Planners directly compared
problems taken set domains. result, recent advances planning
translated dramatic increases size problems solved (Weld,
1999), empirical comparison highlighted improvements.
Comparative evaluation planning significantly uenced expedited
Artificial Intelligence Planning Scheduling (AIPS) conference competitions.
competitions dual effect highlighting progress field providing
relatively unbiased comparison state-of-the-art planners. individual researchers
compare planners others, include fewer planners fewer test problems
time constraints.
support first competition 1998 (McDermott, 2000), Drew McDermott defined,
contributions organizing committee, shared problem/domain definition
language, PDDL (McDermott et al., 1998) (Planning Domain Definition Language). Using

c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHowe & Dahlman

common language means planners' performance directly compared, without
entailing hand translation factoring different representational capabilities.
second benefit, lack translation (or least human accomplished translation) meant performance could compared large number problems
domains1. fact, five competition planners given large number problems
(170 problems ADL track 165 STRIPS track) within seven domains,
including one domain planner developers never seen prior competition.
first competition generated large collection benchmarks: seven domains used
competition plus 21 considered use. 28 domains available
ftp://ftp.cs.yale.edu/pub/mcdermott/domains/. second competition added three
novel domains set.
third major benefit competitions appear motivated researchers develop systems others use. number entrants went five
first competition 16 second. Additionally, 1998 competitors six
sixteen 2000 competitors made code available web sites. Thus, others
perform comparisons.
paper, describe current practice comparative evaluation evolved
since AIPS competitions critically examine underlying assumptions
practice. summarize existing evidence assumptions describe
experimental tests others previously considered. assumptions
organized three groups concerning critical decisions experiment design:
problems tested, planners included performance metrics collected.
Comparisons (as part competitions specific researchers) proven enormously useful motivating progress field. goal understand assumptions
readers know far comparative results generalized. contrast
competitions, community cannot legislate fairness individual researcher's comparative evaluations, readers may able identify cases results viewed
either skeptically confidence. Thus, conclude paper observations
call considerably research new problems, metrics methodologies
support planner evaluation.
contrast competitions, goal declare winner. goal
critique individual studies. Consequently, draw attention away
possible interpretation, whenever possible, report results using letter designators
assigned randomly planners.

2. Planning Competitions Direct Comparisons

Recently, AIPS competitions spurred considerable interest comparative evaluation. roots comparative planner evaluation go back considerably further, however.
Although researchers able run side-by-side comparisons planners
1. solve particular planning problem (i.e., construct sequence actions transform initial state
goal state), planners require domain theory problem description. domain theory represents
abstract actions executed environment; typically, domain descriptions include
variables instantiated specific objects values. Multiple problems defined
domain; problem descriptions require initial state description, goal state association
domain.

2

fiA Critical Assessment Benchmark Comparison Planning

others, able demonstrate performance planner well-known problems, could viewed de facto benchmarks. Sussman's anomaly (Sussman, 1973)
Blocksworld premier planning benchmark problem domain many years;
every planner needed \cut teeth" it.
researchers tired Blocksworld, many called additional benchmark problems
environments. Mark Drummond, Leslie Kaelbling Stanley Rosenschein organized
workshop benchmarks metrics (Drummond, Kaelbling, & Rosenschein, 1990).
Testbed environments, Martha Pollack's TileWorld (Pollack & Ringuette, 1990)
Steve Hanks's TruckWorld (Hanks, Nguyen, & Thomas, 1993), used comparing
algorithms within planners. 1992, UCPOP (Penberthy & Weld, 1992) distributed
large set problems (117 problems 21 domains) demonstration purposes.
1995, Barry Fox Mark Ringer set planning scheduling benchmarks web page
(http://www.newosoft.com/~benchmrx/) collect problem definitions, emphasis
manufacturing applications. Recently, PLANET (a coordinating organization European planning scheduling researchers) proposed planning benchmark collection
initiative (http://planet.dfki.de).
Clearly, benchmark problems become well-established means demonstrating
planner performance. However, practice known benefits pitfalls; Hanks, Pollack
Cohen (1994) discuss detail context agent architecture design.
benefits include providing metrics comparison supporting experimental control.
pitfalls include lack generality results potential benchmarks
unduly uence next generation solutions. words, researchers construct
solutions excel benchmarks, regardless whether benchmarks accurately
represent desired real applications.
obtain benefits listed benchmarks, problems often idealized
simplified versions real problems. Cohen (1991) points , research papers
AI, least AAAI conference, exploit benchmark problems; yet relate
benchmarks target tasks. may significant problem; example, study
owshop scheduling2 benchmarks, found performance standard benchmark set generalize performance problems realistic structure (Watson,
Barbulescu, Howe, & Whitley, 1999). study Blocksworld problems found
best known Blocksworld benchmark problems atypical require short
plans solution optimal solutions easy find (Slaney & Thiebaux, 2001).
spite diculties, benchmark problems AIPS competitions considerably uenced comparative planner evaluations. example, AIPS 2000 conference proceedings (Chien, Kambhampati, & Knoblock, 2000), papers improvements classical planning (12 44 papers conference) relied heavily
comparative evaluation using benchmark problems; papers concerned scheduling,
specific applications, theoretical analyses special extensions standard paradigm
(e.g., POMDP, sensing). 12 classical papers, six used problems AIPS98
competition benchmark set, six used problems Kautz Selman's distribution
problems blackbox (Kautz, 2002) three added problems
well. paper showed results subset problems benchmark distributions
2. Scheduling area related planning actions already known, sequence still
needs determined. Flowshop scheduling type manufacturing scheduling problem.

3

fiHowe & Dahlman

(e.g., Drew McDermott's first competition) logistics, blocksworld, rocket
gripper domains popular (used 11, 7, 5 5 papers, respectively). availability planners competition exploited; eight papers compared
systems AIPS98 planners: blackbox, STAN, IPP HSP (in 5, 3, 3 1
papers, respectively).

3. Assumptions Direct Comparison
canonical planner evaluation experiment follows procedure Table 1. procedure
designed compare performance new planner previous state art
highlight superior performance set cases new planner. exact form
experiment depends purpose, e.g., showing superiority class problem
highlighting effect design decision.
1. Select and/or construct subset planner domains
2. Construct problem set by:
running large set benchmark problems
selecting problems desirable features
varying facet problem increase diculty (e.g., number blocks)
3. Select planners are:
representative state art problems
similar distinct new planner, depending point comparison advance new planner
available able parse problems
4. Run problems planners using default parameters setting upper limit
time allowed
5. Record problems solved, many plan steps/actions solution
much CPU time required either solve problem, fail time
Table 1: Canonical comparative planner evaluation experiment.
protocol depends three selections: problems, planners evaluation metrics.
simply practical even desirable run available planners available
problems. Thus, one needs make informed decisions select. purpose
paper examine assumptions underlying decisions help make
informed. Every planner comparison adopt every one assumptions,
assumptions ones commonly found planner comparisons. example,
comparisons designed specific purpose (e.g., show scale-up certain problems
suitability planner logistics problems) carefully select particular types
problems benchmark sets.
4

fiA Critical Assessment Benchmark Comparison Planning

Problems Many planning systems developed solve particular type planning
problem explore specific type algorithmic variation. Consequently, one would expect
perform better problems developed. Even
designed specific purpose, test set used development may
subtly biased development. community knows planner performance depends
problem features, general, how, why. Researchers tend design
planners general purpose. Consequently, comparisons assume
performance general-purpose planner penalized/biased
executed sampling problems domains (problem assumption 1).

community knows problem representation uences planner performance.
example, benchmark problem sets include many versions Blocksworld problems, designed different planner developers. versions vary problem representation,
minor apparently syntactic changes (e.g., clauses ordered within operators,
initial conditions goals, whether information extraneous) changes ecting addition domain knowledge (e.g., constraints included whether
variables typed). Consequently, comparisons assume
syntactic representational modifications either matter affect planner equally (problem assumption 2).

PDDL includes field, :requirements, capabilities required planner solve
problem. PDDL1.0 defined 21 values :requirements field; base/default requirement :strips, meaning STRIPS derived add delete sets action effects. :adl
(from Pednault's Action Description Language) requires variable typing, disjunctive preconditions, equality built-in predicate, quantified preconditions conditional effects
addition :strips capability. Yet, many planners either ignore :requirements
field reject problem specifies :adl (ignoring many requirements
could cause trouble). Thus, comparisons assume
problems benchmark set solvable STRIPS planner unless
require :adl (problem assumption 3).

Planners wonderful trend making planners publicly available led dilemma
determining use configure them. problem compounded
longevity planner projects; projects produced multiple versions.
Consequently, comparisons tend assume
latest version planner best (planner assumption 1).

planners may include parameters. example, blackbox planner allows
user define strategy applying different solution methods. Researchers expect
parameters affect performance. Consequently, comparisons assume
default parameter settings approximate good performance (planner assumption
2).
5

fiHowe & Dahlman

Experiments invariably use time cut-offs concluding planning yet found
solution declared failure. Many planners would need exhaustively search large space
declare failure. practical reasons, time threshold set determine
halt planner, failure declared time-out reached. Thus, comparisons
assume
one picks suciently high time-out threshold, highly unlikely
solution would found slightly time granted (planner
assumption 3).

Metrics Ideally, performance would measured based well planner

job (i.e., constructing `best' possible plan solve problem) eciently
so. planner shown solve possible problems, basic
metric performance number percentage problems actually solved within
allowed time. metric commonly reported competitions. However, research
papers tend report directly typically test relatively small number
problems.
Eciency clearly function memory effort. Memory size limited
hardware. Effort measured CPU time, preferably always platform
language. problems CPU time well known: programmer skill
varies; research code designed fast prototyping fast execution; numbers
literature cannot compared newer numbers due processor speed improvements.
However, CPU times regenerated experimenter's environment one assumes

performance degrades similarly reductions capabilities runtime
environment (e.g., CPU speed, memory size) (metric assumption 1).

words, experimenter user system expect code
optimized particular compiler/operating system/hardware configuration,
perform similarly moved another compatible environment.
commonly reported comparison metric computation time. second
number steps actions (for planners allow parallel execution) plan. Although
planning seeks solutions achieving goals, goals defined terms states
world, lend well general measures quality. fact, quality likely
problem dependent (e.g., resource cost, amount time execute, robustness),
number plan steps favored. Comparisons assume
number steps resulting plan varies planner solutions approximates quality (metric assumption 2).

comparison, competitions especially, unenviable task determining
trade-off combine three metrics (number solved, time, number steps). Thus,
number steps matter, comparison could simplified.
converted assumption testable question. either summarized
literature question ran experiment test it.
6

fiA Critical Assessment Benchmark Comparison Planning

3.1 Experimental Setup
key issues examined previously, directly indirectly. those,
simply summarize results subsections follow. However, open
questions. those, ran seven well known planners large set 2057 benchmark
problems. planners accept PDDL representation, although built-in
translators PDDL internal representation others rely translators
added. several versions planner available, included (for total
13 planners). basic problem set comprises UCPOP benchmarks, AIPS98
2000 competition test sets additional problem set developed specific application.
exception permuted problems (see section Problem Assumption
2 specifics), problems run 440 MHz Ultrasparc 10s 256 Megabytes
memory running SunOS 2.8. Whenever possible, versions compiled developers
used; source code available, compiled systems according
developers' instructions. planners written Common Lisp run Allegro
Common Lisp version 5.0.1. planners compiled GCC (EGCS version
2.91.66). planner given 30 minute limit wall clock time3 find solution;
however, times reported run times returned operating system.
3.1.1 Planners

planners called primitive-action planners (Wilkins & desJardins,
2001), planners require relatively limited domain knowledge construct plans
simple action descriptions. AIPS98 competition required planners accept
PDDL, majority planners used study competition entrants later
versions thereof 4 . common language facilitated comparison planners without address effects translation step. two exceptions UCPOP
Prodigy; however, representations similar PDDL translated automatically. planners represent five different approaches planning: plan graph analysis,
planning satisfiability, planning heuristic search, state-space planning learning
partial order planning. possible, used multiple versions planner,
necessarily recent. conducted study period time (almost 1.5 years), froze set early on; comparing performance declare
winner think lack recent versions undermined results
testing assumptions.

IPP (Koehler, Nebel, Hoffmann, & Dimopoulos, 1997) extends Graphplan (Blum &

Furst, 1997) algorithm accept richer plan description language. early versions,
language subset ADL extends STRIPS formalism Graphplan
allow conditional universally quantified effects operators. version 4.0,
negation handled via introduction new predicates negated preconditions
3. used actual time lightly loaded machines occasionally system would thrash due
inadequate memory resulting little progress considerable time.
4. used BUS system manager running planners (Howe, Dahlman, Hansen, Scheetz, &
von Mayrhauser, 1999), implemented AIPS98 competition planners. facilitated
running many different planners, somewhat bias included.

7

fiHowe & Dahlman

corresponding mutual exclusion rules; subsequent versions handle directly (Koehler,
1999). used AIPS98 version IPP well later 4.0 version.
SGP (Sensory Graph Plan) (Weld, Anderson, & Smith, 1998) extends Graphplan
richer domain description language, primarily focusing uncertainty sensing.
IPP, transformation performed using expansion techniques remove
quantification. SGP directly supports negated preconditions conditional effects.
SGP tends slower (it implemented Common Lisp instead C)
Graphplan based planners. used SGP version 1.0b.
STAN (STate ANalysis) (Fox & Long, 1999) extends Graphplan algorithm part
adding preprocessor (called TIM) infer type information problem domain.
information used within planning algorithm reduce size search
space Graphplan algorithm would search. STAN incorporated optimized data
structures (bit vectors planning graph) help avoid many redundant calculations performed Graphplan. Additionally, STAN maintains wave front graph
construction track remaining goals limit graph construction. Subsequent versions
incorporated analyses (e.g., symmetry exploitation) additional simpler planning engine. Four versions STAN tested: AIPS98 competition version, version
3.0, version 3.0s development snapshot version 4.0.
blackbox (Kautz & Selman, 1998) converts planning problems Boolean satisfiability
problems, solved using variety different techniques. user indicates
techniques tried order. constructing satisfiability problem,
blackbox uses planning graph constructed Graphplan. blackbox, used
version 2.5 version 3.6b.
HSP (Heuristic Search Planner) (Bonet & Geffner, 1999) based heuristic search.
planner uses variation hill-climbing random restarts solve planning problems.
heuristic based using Graphplan algorithm solve relaxed form
planning problem. study, used version 1.1, algorithmic refinement
version entered AIPS98 competition, version 2.0.
Prodigy 5 (The Prodigy Research Group, 1992) combines state-space planning backward chaining goal state. plan construction consists head-plan
totally ordered actions starting initial state tail-plan partially ordered
actions related goal state. Although ocially entered competition, informal results presented AIPS98 competition suggested Prodigy performed well
comparison entrants. used Prodigy version 4.0.
UCPOP (Barrett, Golden, Penberthy, & Weld, 1993) Partial Order Causal Link
planner. decision include UCPOP based several factors. First,
expand quantifiers negated preconditions; domains, expansion
grounding operators great make problem insolvable. Second, UCPOP
based significantly different algorithm interest recently resurfaced.
used UCPOP version 4.1.
5. thank Eugene Fink code translates PDDL Prodigy.

8

fiA Critical Assessment Benchmark Comparison Planning

Source
# Domains # Problems
Benchmarks
50
293
AIPS 1998
6
202
AIPS 2000
5
892
Developers
1
13
Application
3
72
Table 2: Summary problems testing set: source problems, number
domains problems within domains.
3.1.2 Test Problems

Following standard practice, experiments require planners solve commonly available
benchmark problems AIPS competition problems. addition, test assumptions uence domains (assumption PR1) representations problems
(assumption PR2), include permuted benchmark problems application problems. section describes set problems domains study,
focusing source composition.
problems require STRIPS capabilities (i.e., add delete lists). chose
least common denominator several reasons. First, capable planners still handle
STRIPS requirements; thus, maximized number planners could included
experiment. Also, surprisingly, problems type available. Second,
examining assumptions evaluation, including effect required capabilities
performance. propose duplicate effort competitions singling
planners distinction, rather, purpose determine factors differentially
affect planners.
bulk problems came AIPS98 AIPS 2000 problem sets
set problems distributed PDDL specification. remaining problems
solicited several sources. source counts problems domains
summarized Table 2.
Benchmark Problems preponderance problems planning test sets \toy
problems": well-known synthetic problems designed test attribute planners.
Blocksworld domain long included evaluation well known,
subgoal interactions supports constructing increasingly complex problems
(e.g., towers blocks). benchmark problems simplified versions realistic
planning problems, e.g., tire, refrigerator repair logistics domains. used
set included UCPOP planner. problems contributed large number
people include multiple encodings problems/domains, especially Blocksworld.
AIPS Competitions: 1998 2000 first AIPS competition, Drew McDermott solicited problems competitors well constructing own,
mystery domain, semantically useless names objects operators.
Problems generated domain automatically. competition included 155
problems six domains: robot movement grid, gripper balls
9

fiHowe & Dahlman

moved rooms robot two grippers, logistics transporting packages, organizing snacks movie watching, two mystery domains, disguised logistics
problems.
format 1998 competition required entrants execute 140 problems
first round. problems, 52 could solved planner. round two,
planners executed 15 new problems three domains, one included
first round.
2000 competition attracted 15 competitors three tracks: STRIPS, ADL
hand-tailored track. required performance problems five domains: logistics,
Blocksworld, parts machining, Freecell (a card game), Miconic-10 elevator control.
domains determined organizing committee, Fahiem Bacchus
chair, represented somewhat broader range. chose problems Untyped
STRIPS track set.
scientific standpoint, one interesting conclusions competitions observed trade-offs performance. Planners appeared excel different
problems, either solving set finding solution faster. 1998, IPP solved
problems found shorter plans round two; STAN solved problems fastest;
HSP solved problems round one; blackbox solved problems fastest
round one. 2000, awards given two groups distinguished planners across
different categories planners (STRIPS, ADL hand tailored), according
judges, \it impossible say one planner best"(Bacchus, 2000);
TalPlanner highest distinguished planner group. graphs performance show differences computation time relative planners problem
scale-up. However, planner failed solve problems, makes trends
harder interpret (the computation time graphs gaps).
purpose competitions showcase planner technology
succeeded admirably. planners solved much harder problems could
accomplished years past. trend planners handling increasingly dicult
problems, competition test sets may become historical interest tracking field's
progress.
Problems Solicited Planner Developers asked planner developers
problems used development. One developer, Maria Fox, sent us domain
(Sodor, logistics application) set problems used. would
included domains problems received others.
Applications Miconic elevator domain AIPS2000 competition
derived actual planning application. domain problems extremely
simplified (e.g., removing arithmetic).
add another realistic problem comparison, included one planning application set test domains: generating cases test software interface.
similarities software interface test cases plans, developed system,
several years ago, automatically generating interface test cases using AI planner.
system designed generate test cases user interface Storage Technology's robot tape library (Howe, von Mayrhauser, & Mraz, 1997). interface (i.e.,
commands interface) coded domain theory. example, mount com10

fiA Critical Assessment Benchmark Comparison Planning

mand/action's description required drive empty effect changing
position tape mounted changing status tape drive. Problems
described initial states tape library (e.g., tapes resident,
status devices software controller) goal states human operator might
wish achieve.
time, found simplest problems could generated using
planners available. included application part knew would
challenge. part test set, include three domain theories (different ways
coding application involving 8-11 operators) twenty-four problems domain.
included 24 wanted include enough problems see effect,
many overly bias results. problems relatively simple, requiring
movement one tape coupled status changes,
still dicult could solved original system.

3.2 Problem Assumptions
General-purpose planners exhibit differential capabilities domains sometimes even
problems within domain. Thus, selection problem set would seem critical
evaluation. example, many problems benchmark sets variants logistics
problems; thus, general-purpose planner actually tailored logistics may appear
better overall current benchmarks. section, empirically examine
possible problem set factors may uence performance results.

Problem Assumption 1: Extent Performance General Purpose
Planners Biased Toward Particular Problems/Domains? Although planners
developed general purpose, competitions previous studies shown
planners excel different domains/problems. Unfortunately, community yet
good understanding planner well particular domain. studied
impact problem selection performance two ways.
First, assessed whether performance might positively biased toward problems
tested development. developer6 asked indicate domains used
development. compared planner's performance development
problems (i.e., development set) problems remaining complete test set
(rest). ran 2x2 2 tests comparing number problems solved versus failed
development test sets. included number solved failed analysis
timed-out problems made difference results7.
results analysis summarized Table 3; Figure 1 graphically displays
ratio successes failures development problems. planners
except C performed significantly better development problems. suggests
planners tailored (intentionally not) particular types problems
tend better test sets biased accordingly. example, one
6. decided studying planners way representations
development problems PDDL.
7. One planner exception rule; one case, planner timed far frequently
non-development problems.

11

fiHowe & Dahlman

Development
Planner Sol. Fail

48
56
B
42
34
C
30
0
G
43
35
H
52
9

113
20
J
114
24
K
37
56
L
63
32

Rest
Sol. Fail
2
P
207 1026 51.70 0.001
226 929 51.27 0.001
549 16
0.13 0.722
233 924 49.56 0.001
234 655 91.41 0.001
328 920 187.72 0.001
388 949 157.62 0.001
203 987 27.82 0.001
358 846 52.13 0.001

Table 3: 2 results comparing outcome development versus problems.
planners set, STAN, designed emphasis logistics problems (Fox &
Long, 1999).

Figure 1: Histogram ratios success/failures development problems
planners.
analysis introduces variety biases. developers tended give us short
lists probably really representative actually used. set used
moving target, rather stationary suggests. set problems included
experimentation publication may different still. Consequently, second
part, broadened question determine effect different subsets problems
12

fiA Critical Assessment Benchmark Comparison Planning

n
5
10
20
30

0
0
0
0
0

1
1
3
0
0

2
2
0
0
0

3
0
0
0
0

Rank Dominance
4 5 6 7
5 7 10 4
4 10 6 7
1 3 8 7
1 1 9 6

8 9 10 Total Pairs
10 18 21
78
5 23 20
78
11 8 40
78
9 8 44
78

Table 4: Rank dominance counts 10 samples domains domain sizes (n) five
30.
performance. 10 trials, randomly selected n domains (and companion
problems) form problem set. counted many problems could
solved planner ranked relative performance planner. Thus,
value n, obtained 10 planner rankings. focused rankings problems
solved two reasons: First, domain includes different number problems, making
count problems variable across trials. Second, relative ranking gets
heart whether one planner might considered improvement another.
tested values 5, 10, 20 30 n (30 half domains disposal).
give sense variability size, n = 5, problems solved trial
varied 11 64. assess changes rankings across trials, computed rank
dominance pairs planners; rank dominance defined number trials
planner x's rank lower planner y's (note: ties would count toward neither
planner). 13 planners study resulted 78 dominance pairings. relative
ranking two planners stable, one would expect one always dominate
other, i.e., rank dominance 10.
Table 4 shows number pairs value (0-10) rank dominance
four values n. given pair, used highest number rank dominance
pair, e.g., one always lower rank, pair's rank dominance 10
five, five. ties, maximum less five.
data suggest even picking half domains, rankings completely
stable: 56% pairings, one always dominates, 22% 0.3 greater chance
switching relative ranking. values degrade n decreases 27% always
dominating n = 5.

Problem Assumption 2: Syntactic Representation Differences Affect
Performance? Although well known planners' performance depends

representation (Joslin & Pollack, 1994; Srinivasan & Howe, 1995), two recent developments
planner research suggest effect needs better understood. First, common
representation, i.e., PDDL, may bias performance. planners rely pre-processing
step convert PDDL native representation, step usually requires making
arbitrary choices ordering coding. Second, advantage planners based
Graphplan supposed less vulnerable minor changes representa13

fiHowe & Dahlman

Planner

B
C

E
F
G
H

J
K
L


None Subset
65 315
30
70 295
45
318 74
18
202 169
39
111 132
167
112 138
160
70 295
45
91 290
29
109 134
167
150 124
136
60 305
45
112 284
14
212 148
50

Table 5: number problems planners able solve all, none
subset permutations.
tion. Although reasoning claim sound, exigencies implementation may
require re-introduction representation sensitivity.
evaluate sensitivity representation, ten permutations problem
AIPS2000 set generated, resulting 4510 permuted problems. permutations
constructed randomly reordering preconditions operator definitions
order definitions operators within domain definition.
limited number problems study ten permutations problems would prohibitive. selected AIPS2000 problems attention
recently developed benchmark set. Even within set, domains
permuted would result different domains transformation used. purposes investigation, limited set modifications
permutations preconditions operators known affect planners practical considerations limited number permutations could
executed. Finally, expediency, ran permutations smaller number faster
platforms expedited throughput computation time factor
study.
analyze data, divided performance permutations problems
three groups based whether planner able solve permutations,
none permutations subset permutations. planner insensitive
minor representational changes, subset count zero. results
Table 5, see planners affected permutation operation.
susceptibility permuting problem strongly planner dependent (2 = 1572:16,
P < 0:0001), demonstrating planners vulnerable others.
examining number Subset column, one assess degree susceptibility. planners sensitive reorderings, even relied Graphplan
14

fiA Critical Assessment Benchmark Comparison Planning

0
0
165
166
152
145
0
0
138
130
0
13
169

35
8
216
196
199
185
8
46
169
160
8
24
212

0
0
163
139
157
150
0
0
138
130
0
16
149

0
0
2
0
0
0
0
0
0
0
0
0
2

255
268
561
279
384
376
276
285
441
502
240
421
372

8
0
197
180
168
165
0
17
139
130
0
13
180

Pre.
8

0
0
169
164
162
157
0
0
138
130
0
19
168

Pre.
Safety
Strips
Typing

0
0
5
3
1
0
0
0
0
0
0
0
0

9


B
C

E
F
G
H

J
K
L


Feature

Axioms
Cond. Eff.
Dis. Pre.
Equality

Planner

0
0
160
139
149
145
0
0
138
130
0
13
151

Table 6: number problems claiming require PDDL feature solved
planner.

methodology. sensitive E, F, J (which included Graphplan based
planners 40% problems mixed results permutations) C
L least sensitive (3-4% affected).

Problem Assumption 3: Performance Depend PDDL Requirements
Features? planners intended handle STRIPS problems.

problems test set claim require features STRIPS; one would expect
planners would able handle problems. addition,
planners claim able handle given feature may well planners.
Table 6 shows effects feature requirements ability solve problems. data
table based features specified :requirements list PDDL
definition domain.
verify requirements accurate necessary; thus, problem
may solvable ignoring part PDDL syntax understood,
problem may mislabeled designer. evident cases planner
support given feature still appears able solve corresponding
problem. planners, e.g., older versions STAN, reject problem requires
STRIPS without trying solve it; ADL problem makes use
STRIPS features would attempted.
guidance planner use when, results must viewed
skepticism. example, would appear based results planner might
15

fiHowe & Dahlman

good choice problems conditional effects able solve many
problems. would mistake, since planner cannot actually handle types
problems. cases, problems claim require ADL, fact, make
use STRIPS subset.
Clearly, certain problems solved specific planners. instance, C
planners able handle safety constraints, based data,
C, E appear handle domain axioms. half planners trouble
typed problems. gaps appear due problems translation
native representation.

3.3 Planners

Publicly available, general-purpose planners tend large programs developed
period years enhanced include additional features time. Thus, several versions
likely available, versions likely features turned
on/off via parameter settings.
authors release later versions planning systems, general assumption
newer versions outperform predecessors. However, may
case practice. instance, planner could better optimized toward specific class
problem turn hurts performance problems. Also, advanced
capabilities, even unused, may incur overhead solution problems.
comparison purposes, one use latest version? First, tested
question study comparing multiple versions four planners. Second,
planner relies parameter settings tune performance. Some, blackbox,
many parameters. Others none. Comparisons tend use default published
parameter settings people usually understand effects parameters
tuning extremely time consuming. practice undermine fair
comparison?
Planner Assumption 1: Latest Version Best? study, compared
performance multiple versions four planners (labeled section W, X,
Z, larger version numbers indicating subsequent versions). considered two criteria
improvement: outcome planning computation time solved problems.
outcome planning one of: solved, failed timed-out. criterion, statistically
analyzed data superior performance one versions. outcome results
planners summarized Table 7. table shows, rarely new version
result problems solved. Z improved number test problems
solved subsequent versions.
check whether differences outcome significant, ran 2x3 2 tests
planner version independent variable outcome dependent. Table 8 summarizes
results 2 analysis. Z, compared version successor only.
differences significant except transition Z 2 3 (this expected
two versions extremely similar).
Another planner performance metric, evaluated, speed solution.
analysis, limited comparison problems solved
versions planner. classified problem whether later version solved
16

fiA Critical Assessment Benchmark Comparison Planning

Planner Version Solved Failed Timeout Solved?
W
1
286
664
533
W
2
255
1082
147
+
X
1
502
973
3
X
2
441
940
103
+

1
387
750
339

2
382
771
329
+
Z
1
240
1043
201
Z
2
276
959
248
*
Z
3
268
963
252
+
Z
4
421
878
184
*
Table 7: Version performance: counts outcome change number solved.
old
new
Planner Version Version 2
P
W
1
2
320.96 .0001
X
1
2
98.84 .0001

1
2
.46
.79
Z
1
2
10.96 .004
Z
2
3
.158 .924
Z
3
4
48.50 .0001
Table 8: 2 results comparing versions planner.
problem faster, slower, time preceding version. results
Table 9, see planners improved average speed solution
subsequent versions, exception Z (transition 1 2 versions). However,
Z increase number problems solved versions.
Planner Old New Faster Slower Total
W
1
2
161
61
30
252
X
1
2
295
126
0
421

1
2
222
82
53
357
Z
1
2
84
121
30
235
Z
2
3
131
84
53
268
Z
3
4
115
92
21
228
Table 9: Improvements execution speed across versions. Faster column counts
number cases new version solved problem faster; Slower specifies
cases new version took longer solve given problem.
17

fiHowe & Dahlman

Planner Assumption 2: Parameter Settings Matter Fair Comparison?

planner set, three obvious, easily manipulable parameters: Blackbox, HSP
UCPOP. blackbox extensive set parameters control everything
much trace information print sequence solver applications. HSP's function
varied include (or not) loop detection, change search heuristic vary
number paths expand. UCPOP, user change strategies governing node
orderings aw selection.
run experiments assumption planners
parameters clear literature parameters matter.
Blackbox relies heavily random restarts trying alternative SAT solvers. Kautz
Selman (1999), authors blackbox carefully study aspects blackbox's design
demonstrate differential performance using different SAT solvers; propose hypotheses
performance differences working better models performance variation.
heart HSP heuristic search. Thus, performance varies depending
heuristics. Experiments HSP (a planner builds ideas
HSP) shown importance heuristic selection search space expansion,
computation time problem scale (Haslum & Geffner, 2000; Hoffmann & Nebel,
2001).
HSP, heuristic search critical UCPOP's performance. set studies
explored alternative settings aw selection heuristics employed UCPOP (Joslin &
Pollack, 1994; Srinivasan & Howe, 1995; Genevini & Schubert, 1996), producing dramatic
improvements domains heuristics. Pollack et al. (1997) confirmed,
good default strategy could derived, performance best
circumstances.
Thus, parameters control fundamental aspects algorithms,
search strategies, role parameters comparisons cannot easily dismissed.

Planner Assumption 3: Time Cut-offs Unfair? Planners often admit

failure. Instead, planner stops used allotted time found
solution. setting time threshold requirement planner execution.
comparison, one might always wonder whether enough time allotted fair {
perhaps solution almost found execution terminated.
determine whether cut-off 30 minutes fair, examined distribution
times declared successes failures8. Across planners problem set,
found distributions skewed (approximately log normal long right tails)
planners quick declare success failure, going so.
Table 10 shows max, mean, median standard deviation success failure times
planners. differences mean median indicate distribution
skew, low standard deviations relative observed max times. max time
shows rare occasions planners might make decision within 2 minutes
cut-off.
8. separated two usually observed significant difference distributions time
succeed time fail { half planners quick succeed slow fail, half
reversed relationship.

18

fiA Critical Assessment Benchmark Comparison Planning

Planner

B
C

E
F
G
H

J
K
L


Successes
Max Mean Median
667.9 34.0
1.3
1608.5 38.5
0.5
1455.4 89.9
1.6
481.0 17.8
1.1
1076 26.2
0.1
1282.4 44.4
0.1
1456.2 44.6
0.7
657.7 29.58
1.4
1713.8 115.4
0.2
1596.5 43.6
4.3
1110.5 31.0
0.32
1611.9 54.4
2.0
1675.3 53.4
1.45

Sd
98.7
182.8
244.6
77.4
126.8
126.8
188.5
80.6
303.1
127.4
121.8
180.9
196.5

Failures
Max Mean Median
1116.4 44.9
4.9
1692.0 45.6
17.8
1.4
0.4
0.13
713.6 26.3
1.1
1622.8 286.9
260.6
1188.4 22.3
0.2
1196.5 43.8
16.7
1080.6 93.8
1.4
50.6
5.1
4.9
1796 11.0
11.0
1298.8 27.7
12.1
847.1 124.1
68.4
1.6
0.9
0.8

Sd
128.8
96.8
0.4
122.6
189.1
104.8
78.5
162.1
6.3
57.9
65.2
164.8
0.4

Table 10: Max, mean, median standard deviations (Sd) computation times
success failure planner.

table show, observed distributions show,
values greater half time cut-off. Figures 2 3 display
distributions planner F, means middle set planners
quite typical distributions. Consequently, least problems, cut-off 15
minutes (900 seconds) would significantly change results.

300

200

100

0
0

104 208 312 416 520 624 728 832 936 1040 1144 1248
success.time

Figure 2: Histogram times, seconds, planner F succeed.
19

fiHowe & Dahlman

600

400

200

0
0

96

192 288 384 480 576 672 768 864 960 1056 1152
fail.time

Figure 3: Histogram times, seconds, planner F fail.

3.4 Performance Metrics

comparisons emphasize number problems solved CPU time completion metrics. Often, problems organized increasing diculty show scale-up.
Comparing based metrics leaves lot open interpretation. example,
planners designed find optimal plan, measured number steps either
parallel sequential plan. Consequently, planners may require computation.
Thus, ignoring plan quality, planners may unfairly judged. hypothesize
hardware software platform tests vary results. planner
developed machine 1GB memory, likely performance degrade
less. key issue whether effect less uniform across set planners.
section, examine two issues: execution platform effect plan
quality.

Metric Assumption 1: Performance Vary Planners Run
Different Hardware Platforms? Often planner run competition

someone else's lab, hardware software platforms differ platform used
development. Clearly, slowing processor speed slow planning,
requiring higher cut-offs. Reduction memory may well change set problems
solved increase processing time due increased swapping. Changing
hardware configuration may change way memory cached organized, favoring
planners' internal representations others. Changing compilers could affect
amount type optimizations code. exact effects probably unknown.
assumption changes affect planners less equally.
test this, ran planners less powerful, lower memory machine compared
results two platforms: base Sun Ultrasparc 10/440 256mb memory
Ultrasparc 1/170 128mb memory. operating system compilers
versions machines. problems run platforms.
followed much methodology comparison planner versions: comparing
number problems solved time solution. Table 11 shows results
measured problems solved, failed timed-out planner two platforms.
20

fiA Critical Assessment Benchmark Comparison Planning

Planner Platform Solved Failed Timed-Out 2
p % Reduction

Ultra 1
94
383
27
Ultra 10
95
389
20 1.09 .58
1
B
Ultra 1
121
346
37
Ultra 10
121
353
30 0.80 .67
0
C
Ultra 1
354
7
143
Ultra 10
367
7
130 0.85 .65
4

Ultra 1
218
59
227
Ultra 10
217
59
228 0.01 .998
-.4
E
Ultra 1
280
145
79
Ultra 10
284
150
70 0.66 .72
1
F
Ultra 1
277
155
72
Ultra 10
284
154
66 0.35 .84
2
G
Ultra 1
120
347
37
Ultra 10
121
352
31 0.57 .75
1
H
Ultra 1
116
350
38
Ultra 10
122
338
44 0.80 .67
7

Ultra 1
265
201
38
Ultra 10
274
201
29 1.36 .51
3
J
Ultra 1
280
220
4
Ultra 10
285
217
2 0.73 .69
2
K
Ultra 1
108
370
26
Ultra 10
108
368
28 0.08 .96
0
L
Ultra 1
149
339
16
Ultra 10
150
341
13 0.32 .85
1

Ultra 1
250
65
189
Ultra 10
258
66
180 0.35 .84
3
Table 11: Number problems solved, failed timed-out planner two
hardware platforms. Last column percentage reduction number
solved faster slower platforms.

21

fiHowe & Dahlman

Planner

B
C

E
F
G
H

J
K
L


Faster
# Mean
92
5.18
120
4.02
294
31.89
177
11.02
275
2.68
271
14.86
117
5.02
115
6.86
261
25.73
280
42.24
107
15.26
148
16.81
194
32.72

Slower
Sd # Mean
30.76 1
10.01 0
101.71 60
0.29
82.82 39
0.23
12.27 1
72.44 0
17.17 1
25.24 0
119.97 0
138.16 0
75.42 0
98.54 1
139.73 56
0.30

Sd
0.14
0.14

0.18

Total
1
1
0
1
4
6
2
1
4
0
1
0
0

94
121
354
217
280
277
120
116
265
280
108
149
250

Table 12: Improvements execution speed moving slower faster platform. Counts
problems solved platforms. faster slower,
mean standard deviation (Sd) difference provided.
before, looked change time solution. Table 12 shows time
solution changes planner. surprisingly, faster processor memory
nearly always lead better performance. Somewhat surprisingly, difference far less
doubling might expected; mean differences much less
mean times faster processor (see Table 10 mean solution times).
Also, effect seems vary planners. Based counts, Lisp-based
planners appear less susceptible trend (the ones sometimes faster
slower platform). However, advantages small, affecting primarily
smaller problems. think effect due need load Lisp image
startup centralized server; thus, computation time small problems
dominated network delay. Older versions planners appear less sensitive
switch platform.
study, platforms make little difference results, despite
doubling processor speed doubling memory. However, two platforms
underpowered compared development platforms planners.
chose platforms differed characteristics (processor speed
memory amount) access 20 identically configured machines.
really observe difference, 1GB9 memory may needed.
Recent trends planning technology exploited cheap memory: translations
propositional representations, compilation problems built-in caching memory
management techniques. Thus, planners designed trade-off memory time;
9. propose figure amount requested participants AIPS 2000
planning competition.

22

fiA Critical Assessment Benchmark Comparison Planning

planners understandably affected memory limitations problems.
Given results study, considered performing careful study memory
artificially limiting memory planners
access enough suciently large machines likely make difference could
devise scheme fairly across planners (which implemented
different languages require different software run-time environments).
Another important factor may memory architecture/management. planners
include memory managers, map better hardware platforms
others (e.g., HSP uses linear organization appears fit well Intel's memory
architecture).

Metric Assumption 2: Number Plan Steps Vary? Several researchers

examined issue measuring plan quality directing planning based it, e.g.,
(Perez, 1995; Estlin & Mooney, 1997; Rabideau, Englehardt, & Chien, 2000). number
steps plan rather weak measure plan quality, far, one
widely used primitive-action planning.
expect planners sacrifice quality (as measured plan length) speed.
Thus, ignoring even measure plan quality may unfair planners.
check whether appears factor problem set, counted plan length
plans returned output compared lengths across planners.
planners construct parallel plans, adopted general definition:
sequential plan length. compared plan lengths returned planner
every successfully solved problem.
found 11% problems solved one planner (not necessarily
one). planners found equal length solutions 62% remained (493
problems). calculated standard deviation (SD) plan length solutions
problem analyzed SDs. found minimum observed SD 0.30,
maximum 63.30, mean 2.43 standard deviation 5.45. Thirteen
cases showed SDs higher 20. Obviously, cases involved fairly long plans (up
165 steps); cases problems logistics gripper domains.
check whether planners favored minimal lengths, counted number
cases planner found shortest length plan (ties attributed
planners) variance plan length. Table 13 lists results.
planners find shortest length plans one third problems. Planner F
designed optimize plan length, shows results. one exception,
older planners rarely find shortest plans.

4. Interpretation Results Recommendations
previous section presented summarization analysis planner runs.
section, ect results mean empirical comparison planners;
summarize results recommend partial solutions. possible guarantee
fairness propose magic formula performing evaluations, state
practice general certainly improved. propose three general recommendations
12 recommendations targeted specific assumptions.
23

fiHowe & Dahlman

Planner Count

178
B
169
C
0

161
E
5
F
319
G
171
H
176

222
J
0
K
159
L
151

283
Table 13: Number plans planner found shortest plan. data
include problems different length plans found.
Many targeted recommendations amount requesting problem planner developers precise requirements expectations contributions.
planners extremely complex time consuming build, documentation may inadequate determine subsequent version differs previous
conditions (e.g., parameter settings, problem types) planner fairly
compared. current positive trend making planners available, behooves
developer include information distribution system.
sweeping recommendation shift research focus away developing
best general-purpose planner. Even competitions, planners identified
superior ones designed specific classes problems, e.g., IPP.
competitions done great job exciting interest encouraging development
public availability planners incorporate representation.
However, advance research, informative comparative evaluations
designed specific purpose { test hypothesis prediction
performance planner10. experimental hypothesis focuses analysis often
leads naturally justified design decisions experiment itself. example, Hoffmann Nebel, authors Fast-Forward (FF) system, state introduction
JAIR paper FF's development motivated specific set benchmark
domains; system heuristic, designed heuristics fit expectations/needs domains (Hoffmann & Nebel, 2001). Additionally, part
evaluation, compare specific system system commonalities
point various advantages disadvantages design decisions specific
10. Paul Cohen advocated experimental methodology artificial intelligence based
hypotheses, predictions models considerable detail; see Cohen (1991, 1995).

24

fiA Critical Assessment Benchmark Comparison Planning

problems. Follow-up work researchers comparing systems
well-defined starting point comparison.

Recommendation 1: Experiments driven hypotheses. Re-

searchers precisely articulate advance experiments expectations new planner augmentations existing planner add
state art. expectations turn justify selection
problems, planners metrics form core comparative
evaluation.
general issue whether results accurate. reported results
output planners. planner stated output successful,
took face value. However, examining output, determined
claims successful solution erroneous { proposed solution would work.
way ensure output correct solution checker. Drew McDermott
used solution checker AIPS98 competition. However, planners
provide output compatible format checker. Thus, another concern
comparative evaluation output needs cross-checked.
declaring winner (i.e., planner exhibited superior performance), think
lack solution checker casts serious doubt results. part,
concerned factors cause observed success rates change.

Recommendation 2: input standardized PDDL, output
standardized, least format returned plans.

Another general issue whether benchmark sets representative space
interesting planning problems. test directly (in fact, sure
one could so), clustering results observations others planning
community suggest set biased toward logistics problems. Additionally, many
problems getting dated longer distinguish performance. researchers
begun formally analyze problem set, either service building improved
planners (e.g., Hoffmann & Nebel, 2001) better understand planning problems.
example, related area scheduling, group identified distinctive patterns
topology search spaces different types classical scheduling problems
related topology performance algorithms (Watson, Beck, Barbulescu, Whitley, &
Howe, 2001). Within planning, Hoffmann examined topology local search spaces
small problems benchmark collection found simple structure
respect well-known relaxations (Hoffmann, 2001). Additionally, worked
partial taxonomy, based three characteristics, analyzed domains. Helmert
analyzed computational complexity subclass benchmarks, transportation
problems, identified key features affect diculty problems (Helmert,
2001).

Recommendation 3: benchmark problem sets eval-

uated over-hauled. Problems easily solved removed.
Researchers study benchmark problems/domains classify
25

fiHowe & Dahlman

problem types key characteristics. Developers contribute application problems realistic versions evolving set.
remainder section describes recommendations improving state
art planner comparisons.

Problem Assumption 1: General Purpose Planners Biased Toward Particular Problems/Domains? set problems planner developed
strong effect performance planner. either effect
unintentional over-specialization result concerted effort part
developers optimize system solve specific problem. one exception, every
planner fared better tailored subset problems (training set). Consequently,
must conclude choice subset problems may well affect outcome
comparison.
fair planner comparison must account likely biases problem set. Good
performance certain class problems imply good performance general.
large performance differential planners targeted problem domain (i.e., well
focus problems poorly others) may well indicate developers
succeeded optimizing performance planner.
Recommendation 4: Problem sets constructed highlight
designers' expectations superior performance planner,
specific selection criteria.
hand, goal demonstrate across board performance,
results randomly selecting domains suggests biases mitigated.
Recommendation 5: highlighting performance \general" problems
goal, problem set selected randomly benchmark
domains.

Problem Assumption 2: Syntactic Representation Differences Affect
Performance? Many studies, including this, shown planners may sensitive

representational features. representations translated automatically
mean performance unaffected. algorithm
theoretically insensitive factor mean practice is.
planners showed sensitivity permuted problems, degree sensitivity varied.
outcome suggests translators even minor variations problem descriptions
impact outcome used care, especially sensitivity
focus study planner vulnerable effect.
Recommendation 6: Representation translators avoided using
native versions problems testing multiple versions problems necessary.
many planner developers participating AIPS competitions, become
less issue.
importantly, researchers explicitly testing effect alternative phrasings planning problems determine sensitivity performance separate
effects advice/tuning essence problem.
26

fiA Critical Assessment Benchmark Comparison Planning

Recommendation 7: Studies consider role minor syntactic vari-

ations performance include permuted problems (i.e., initial conditions,
goals, preconditions actions) problem sets demonstrate robustness, provide opportunity learning protect developers
accidentally over-fitting algorithm set test problems.

Problem Assumption 3: Performance Depend PDDL Requirements
Features? planners perform quite advertised expected given

problem features. discrepancy could many possible causes: problems incorrectly
specified, planners less sensitivity thought, solutions correct, etc.
example, many problems benchmark set designed competitions
even intended widely used may specified carefully enough.

Recommendation 8: problems contributed benchmark set,
developers verify requirements stated description
problem correctly ect subset features needed. Planner evaluators
use problems match planner's capabilities.

Depending cause, results skewed, e.g., planner may unfairly
maligned unable solve problem specifically designed solve.
recommendation addresses gaps specification problem set,
mismatches capabilities specifiable PDDL planners possess
remain.

Recommendation 9: Planner developers develop vocabulary
planner's capabilities, PDDL ags, specify expected
capabilities planner's distribution.

Planner Assumption 1: Latest Version Best? results suggest

new versions run faster, often solve problems. Thus, newest version may
represent \best" (depending definition) performance class planner.
competitions fields, e.g., automatic theorem proving community, require
previous year's best performer compete well; advantage establishing
baseline performance well allowing comparison focus may shift
time.

Recommendation 10: primary evaluation metric speed, newer

version may best competition. number problems solved one
wishes establish progress made, may worth running
older version well. recommendation 9 followed,
evaluators select version based guidance.

Planner Assumption 2: Effect Parameter Settings? Perfor-

mance planners vary parameter settings. Unfortunately, often
dicult figure set parameters properly, changing settings makes
dicult compare results across experiments. Generally, issue
27

fiHowe & Dahlman

developers users tend rely default parameter settings. Unfortunately,
sometimes developers exploit alternative settings experiments, complicating
later comparison.

Recommendation 11: planner includes parameters, developer
guide users settings. not, default settings
used developers others experiments facilitate comparison.

Planner Assumption 3: Time Cut-offs Unfair? found little benefit
increasing time cut-offs beyond 15 minutes problems.

Recommendation 12: total computation time bottleneck, run

problems separate batches, incrementally increasing time cut-off
runs including unresolved problems subsequent runs.
additional problems solved run, stop.

Metric Assumption 1: Alternative Platforms Lead Different Performance? experiments, performance vary much expected.
result suggests researchers general developing specific hardware/software
configurations, recent trends suggest otherwise, least regards memory. Again,
systems research prototypes, behooves developer clear
his/her expectations anyone subsequently using system accommodate requests studies.

Recommendation 13: factors planner design, researchers

must clearly state hardware/software requirements planners,
design based platform assumptions. Additionally, careful study memory versus time trade-offs undertaken, given recent trends memory exploitation.

Metric Assumption 2: Number Plan Steps Vary? certainly can.

one neglects quality measures, planners penalized efforts declare
best planner.

Recommendation 14: expedite generalizing across studies, reports
describe performance terms solved (how many types),
much time required quality solutions. Tradeoffs reported, possible, e.g., 12% increase computation time
30% decrease plan length. Additionally, design goal find
optimal solution, compare planners design goal.

Good metrics plan quality sorely needed. latest specification PDDL
specification supports definition problem-specific metrics (Fox & Long, 2002);
metrics indicate whether total-time (a new concept supported specification action
durations) specified functions minimized maximized. addition
excellent start, general metrics plan-length total-time
needed expedite comparisons across problems.
28

fiA Critical Assessment Benchmark Comparison Planning

Recommendation 15: Developing good metrics valuable research contri-

bution. Researchers consider worthwhile project, conference organizers reviewers encourage papers topic, planner developers
implement planners responsive new quality metrics (i.e.,
support tunable heuristics evaluation criteria).

5. Conclusions

Fair evaluation comparison planners hard. Many apparently benign factors exert
significant effects performance. Superior performance one planner another
problem neither intentionally designed solve may explained minor
representational features. However, comparative analysis general problems practical
importance practical create specialized solution every problem.
analyzed effects experiment design decisions empirical comparison
planners made recommendations ameliorating effects decisions.
recommendations common sense suggestions improving current
methodology.
expand beyond current methodology require least two substantive changes.
First, field needs question whether trying show performance
planning problems general. shift general comparisons focused comparisons (on
problem class mechanism hypothesis testing) could produce significant advances
understanding planning.
Second, benchmark problem sets require attention. Many problems
discarded simple show much. domains far removed
real applications. may time revisit testbeds. example, several researchers
robotics constructed interactive testbed comparing motion planning algorithms
(Piccinocchi, Ceccarelli, Piloni, & Bicchi, 1997). testbed consists user interface
defining new problems, collection well-known algorithms simulator testing
algorithms specific problems. Thus, user design his/her problems compare performance various algorithms (including own) via web site.
testbed affords several advantages current paradigm static benchmark problems
developer conducted comparisons, particular, replicability extendability
test set. Alternatively, challenging problem sets developed modifying deployed
applications (Wilkins & desJardins, 2001; Engelhardt, Chien, Barrett, Willis, & Wilklow,
2001).
recent years, planning community significantly improved size planning
problems solved reasonable time advanced state art
empirical comparison systems. interpret results empirical comparisons
understand motivate development planning, community
needs understand effects empirical methodology itself. purpose
paper understanding initiate dialogue methodology
used.

29

fiHowe & Dahlman

Acknowledgments
research partially supported Career award National Science
Foundation IRI-9624058 grant Air Force Oce Scientific Research F4962000-1-0144. U.S. Government authorized reproduce distribute reprints
Governmental purposes notwithstanding copyright notation thereon.
grateful reviewers careful reading well-considered comments
submitted version; hope done justice suggestions.

References

Bacchus,
F.
(2000).
AIPS-2000
planning
competition.
http://www.cs.toronto.edu/aips2000/SelfContainedAIPS-2000.ppt.
Barrett, A., Golden, K., Penberthy, S., & Weld, D. (1993). UCPOP User's Manual. Dept.
Computer Science Engineering, University Washington, Seattle, WA. TR
93-09-06.
Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial
Intelligence Journal, 90 (1-2), 225{279.
Bonet, B., & Geffner, H. (1999). Planning heuristic search: New results. Proceedings
Fifth European Conference Planning (ECP-99) Durham, UK.
Chien, S., Kambhampati, S., & Knoblock, C. A. (Eds.)(2000). Proceedings Fifth
International Conference Artificial Intelligence Planning Scheduling (AIPS
2000). AAAI Press, Breckenridge, CO.
Cohen, P. R. (1991). survey eighth national conference artificial intelligence:
Pulling together pulling apart? AI Magazine, 12 (1), 16{41.
Cohen, P. R. (1995). Empirical Methods Artificial Intelligence. MIT Press.
Drummond, M. E., Kaelbling, L. P., & Rosenschein, S. J. (1990). Collected notes
benchmarks metrics workshop. Artificial intelligence branch FIA-91-06, NASA
Ames Research Center.
Engelhardt, B., Chien, S., Barrett, T., Willis, J., & Wilklow, C. (2001). data-chaser
citizen explorer benchmark problem sets. Proceedings Sixth European
Conference Planning (ECP 01) Toledo, Spain.
Estlin, T. A., & Mooney, R. J. (1997). Learning improve ecicency quality
planning. Proceedings Fifteenth International Joint Conference Artificial
Intelligence, pp. 1227{1233, Nagoya, Japan.
Fox, M., & Long, D. (1999). ecient implementation plan-graph STAN.
Journal Artificial Intelligence Research, 10, 87{115.
Fox, M., & Long, D. (2002). PDDL2.1: extension PDDL expressing temporal
planning domains. Available http://www.dur.ac.uk/d.p.long/pddl2.ps.gz.
30

fiA Critical Assessment Benchmark Comparison Planning

Genevini, A., & Schubert, L. (1996). Accelerating partial-order planners: techniques
effective search control pruning. Journal Artificial Intelligence Research, 5,
95{137.
Hanks, S., Nguyen, D., & Thomas, C. (1993). beginner's guide truckworld simulator. Dept. Computer Science Engineering UW-CSE-TR 93-06-09, University
Washington.
Hanks, S., Pollack, M. E., & Cohen, P. R. (1994). Benchmarks, test beds, controlled
experimentation design agent architectures. AI Magazine, 17{42.
Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Proceedings Fifth International Conference Artificial Intelligence Planning
Scheduling (AIPS 2000), pp. 140{149, Breckenridge, CO. AAAI Press.
Helmert, M. (2001). complexity planning transportation domains. 6th
European Conference Planning (ECP'01), Lecture Notes Artificial Intelligence,
New York, Springer-Verlag.
Hoffmann, J. (2001). Local search topology planning benchmarks: empirical analysis.
Proceedings 17th International Joint Conference Artificial Intelligence
Seattle, WA, USA.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253{302.
Howe, A. E., Dahlman, E., Hansen, C., Scheetz, M., & von Mayrhauser, A. (1999). Exploiting competitive planner performance. Proceedings Fifth European Conference
Planning, Durham, UK.
Howe, A. E., von Mayrhauser, A., & Mraz, R. T. (1997). Test case generation AI
planning problem. Automated Software Engineering, 4 (1), 77{106.
Joslin, D., & Pollack, M. (1994). Least-cost aw repair: plan refinement strategy
partial-order planning. Proceedings Twelfth National Conference Artificial
Intelligence, pp. 1004{1009, Seattle, WA.
Kautz, H., & Selman, B. (1998). BLACKBOX: new approach application
theorem proving problem solving. Working notes AIPS98 Workshop
Planning Combinatorial Search, Pittsburgh, PA.
Kautz,
H.
blackbox:
SAT technology planning system.
http://www.cs.washington.edu/homes/kautz/blackbox/index.html.
Kautz, H., & Selman, B. (1999). Unifying SAT-based graph-based planning. Proceedings Sixteenth International Joint Conference Artificial Intelligence, Stockholm, Sweden.
Koehler, J. (1999). Handling conditional effects negative goals IPP. Tech. rep.
128, Institute Computer Science, Albert Ludwigs University, Freiburg, Germany.
31

fiHowe & Dahlman

Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs
ADL subset. Proceedings Fourth European Conference Planning.
McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &
Wilkins, D. (1998). Planning Domain Definition Language.
McDermott, D. (2000). 1998 AI planning systems competition. AI Magazine, 21 (2),
35{56.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: sound, complete, partial order planner
adl. Proceedings Third International Conference Knowledge Representation Reasoning, pp. 103{114.
Perez, M. A. (1995). Learning Search Control Knowledge Improve Plan Quality. Ph.D.
thesis, Carnegie-Mellon University.
Piccinocchi, S., Ceccarelli, M., Piloni, F., & Bicchi, A. (1997). Interactive benchmark
planning algorithms web. Proceedings IEEE International Conference
Robotics Automation.
Pollack, M. E., & Ringuette, M. (1990). Introducing Tileworld: Experimentally evaluating agent architectures. Proceedings Eight National Conference Artificial
Intelligence, pp. 183{189, Boston, MA.
Pollack, M., Joslin, D., & Paolucci, M. (1997). Flaw selection strategies partial-order
planning. Journal Artificial Intelligence Research, 6, 223{262.
Rabideau, G., Englehardt, B., & Chien, S. (2000). Using generic prferences incrementally
improve plan quality. Proceedings Fifth International Conference Artificial
Intelligence Planning Scheduling (AIPS 2000), Breckenridge, CO.
Slaney, J., & Thiebaux, S. (2001). Blocks world revisited. Artificial Intelligence Journal,
125 (1-2), 119{153.
Srinivasan, R., & Howe, A. E. (1995). Comparison methods improving search eciency
partial-order planner. Proceedings 14th International Joint Conference
Artificial Intelligence, pp. 1620{1626, Montreal, Canada.
Sussman, G. A. (1973). computational model skill acquisition. Tech. rep. Memo no.
AI-TR-297, MIT AI Lab.
Prodigy Research Group (1992). PRODIGY 4.0; manual tutorial. School
Computer Science 92-150, Carnegie Mellon University.
Watson, J., Barbulescu, L., Howe, A., & Whitley, L. D. (1999). Algorithm performance
problem structure ow-shop scheduling. Proceedings Sixteenth National
Conference Artificial Intelligence (AAAI-99), Orlando, FL.
Watson, J., Beck, J., Barbulescu, L., Whitley, L. D., & Howe, A. (2001). Toward descriptive model local search cost job-shop scheduling. Proceedings Sixth
European Conference Planning (ECP'01), Toledo, Spain.
32

fiA Critical Assessment Benchmark Comparison Planning

Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan handle uncertainty
sensing actions. Proceedings Fifteenth National Conference Artificial
Intelligence Madison, WI.
Weld, D. S. (1999). Recent advances AI planning. AI Magazine, 20 (2), 93{122.
Wilkins, D. E., & desJardins, M. (2001). call knowledge-based planning. AI Magazine,
22 (1), 99{115.

33


