Journal Artificial Intelligence Research 17 (2002) 289-308

Submitted 6/02; published 10/02

Unified Model Structural Organization
Language Music
Rens Bod

RENS@ILLC.UVA .NL

Institute Logic, Language Computation
University Amsterdam, Nieuwe Achtergracht 166
1018 WV Amsterdam, NETHERLANDS,
School Computing, University Leeds
LS2 9JT Leeds, UK

Abstract
general model predict perceived phrase structure language
music? usually assumed humans separate faculties language
music, work focuses commonalities rather differences
modalities, aiming finding deeper "faculty". key idea perceptual system
strives simplest structure (the "simplicity principle"), biased
likelihood previous structures (the "likelihood principle"). present series dataoriented parsing (DOP) models combine two principles tested
Penn Treebank Essen Folksong Collection. experiments show (1)
combination two principles outperforms use either them, (2) exactly
model parameter setting achieves maximum accuracy language
music. argue results suggest interesting parallel linguistic
musical structuring.

1. Introduction: Problem Structural Organization
widely accepted human cognitive system tends organize perceptual information
hierarchical descriptions conveniently represented tree structures. Tree
structures used describe linguistic perception (e.g. Wundt, 1901; Chomsky,
1965), musical perception (e.g. Longuet-Higgins, 1976; Lerdahl & Jackendoff, 1983)
visual perception (e.g. Palmer, 1977; Marr, 1982). Yet, little attention paid
commonalities different forms perception question whether
exists general, underlying mechanism governs perceptual organization. paper
studies exactly question: acknowledging differences perceptual
modalities, general model predict perceived tree structure sensory
input? studying question, use empirical methodology: model
might hypothesize tested manually analyzed benchmarks
linguistically annotated Penn Treebank (Marcus et al. 1993) musically annotated
Essen Folksong Collection (Schaffrath, 1995). argue general model
structural organization language, music vision, carry experiments
linguistic musical benchmarks, since benchmark visual tree structures currently
available, best knowledge.
Figure 1 gives three simple examples linguistic, musical visual information
corresponding tree structures printed (these examples resp. taken Martin et al.
1987, Lerdahl & Jackendoff, 1983, Dastani, 1998).

2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBOD

List sales products 1973

NP
NP
NP
V

DT

PP
N

P

PP
N

P

N

List sales products 1973

Figure 1: Examples linguistic, musical visual input tree structures
Thus, tree structure describes parts input combine constituents
constituents combine representation whole input. Note linguistic
tree structure labeled syntactic categories, whereas musical visual tree
structures unlabeled. language syntactic constraints
words combined larger constituents (e.g. English determiner combined
noun precedes noun, expressed rule NP DT N),
music (and lesser extent vision) restrictions: principle note
may combined note.
Apart differences, fundamental commonality: perceptual input
undergoes process hierarchical structuring found input itself. main
problem thus: derive perceived tree structure given input?
problem trivial may illustrated fact inputs assigned
following, alternative tree structures:


NP
PP

NP
V

DT

N

P

PP
N

P

N

List sales products 1973

Figure 2: Alternative tree structures inputs Figure 1
alternative structures possible perceived. linguistic tree
structure Figure 1 corresponds meaning different tree Figure 2.
two musical tree structures correspond different groupings motifs. two
visual structures correspond different visual Gestalts. alternative tree
structures possible, plausible: correspond structures
actually perceived human cognitive system.
phenomenon input may assigned different structural organizations
known ambiguity problem. problem one hardest problems modeling
human perception. Even language, phrase-structure grammar may specify
words combined constituents, ambiguity problem notoriously hard (cf.
Manning & Schtze, 1999). Charniak (1997: 37) argues many sentences Wall
Street Journal one million different parse trees. ambiguity problem
290

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

musical input even harder, since virtually constraints notes may
combined constituents. Talking rhythm perception music, Longuet-Higgins
Lee (1987) note "Any given sequence note values principle infinitely ambiguous,
ambiguity seldom apparent listener.".
following Section, discuss two principles traditionally proposed
solve ambiguity: likelihood principle simplicity principle. Section 3,
argue new integration two principles within data-oriented parsing framework.
hypothesis human cognitive system strives simplest structure generated
shortest derivation, biased frequency previously
perceived structures. Section 4, go computational aspects model.
Section 5, discuss linguistic musical test domains. Section 6 presents empirical
investigation comparison model. Finally, Section 7, give discussion
approach go combinations simplicity likelihood proposed
literature.

2. Two principles: Likelihood Simplicity
predict set possible tree structures tree actually
perceived human cognitive system? field visual perception, two competing
principles traditionally proposed govern structural organization. first, initiated
Helmholtz (1910), advocates likelihood principle: perceptual input organized
probable structure. second, initiated Wertheimer (1923) developed
Gestalt psychologists, advocates simplicity principle: perceptual system
viewed finding simplest rather probable structure (see Chater, 1999,
overview). two principles used linguistic musical structuring.
following, briefly review principles modality.
2.1 Likelihood
likelihood principle particularly influential field natural language
processing (see Manning Schtze, 1999, review). field, appropriate
tree structure sentence assumed likely structure. likelihood tree
computed probabilities parts (e.g. phrase-structure rules) turn
estimated large manually analyzed language corpus, i.e. treebank. State-of-the-art
probabilistic parsers Collins (2000), Charniak (2000) Bod (2001a) obtain around
90% precision recall Penn Wall Street Journal treebank (Marcus et al. 1993).
likelihood principle applied musical perception, e.g. Raphael (1999)
Bod (2001b/c). probabilistic natural language processing, probable musical
tree structure computed probabilities rules fragments taken large
annotated musical corpus. musical benchmark used models
Essen Folksong Collection (Schaffrath, 1995).
vision science, huge interest probabilistic models (e.g. Hoffman, 1998;
Kersten, 1999). Mumford (1999) even seen fit declare Dawning Stochasticity.
Unfortunately, visual treebanks currently available.
2.2 Simplicity
simplicity principle long tradition field visual perception psychology (e.g.
Restle, 1970; Leeuwenberg, 1971; Simon, 1972; Buffart et al. 1983; van der Helm, 2000).
291

fiBOD

field, visual pattern formalized constituent structure means visual coding
language based primitive elements line segments angles. Perception
described process selecting simplest structure corresponding "shortest
encoding" visual pattern.
notion simplicity applied music perception. Collard et al. (1981) use
coding language Leeuwenberg (1971) predict metrical structure four preludes
Bach's Well-Tempered Clavier. well-known music perception theory
proposed Lerdahl Jackendoff (1983). theory contains two kinds rules: "wellformedness rules" "preference rules". role well-formedness rules define
kinds formal objects (grouping structures) theory employs. grouping structures
listener actually hears, described preference rules describe Gestaltpreferences kind identified Wertheimer (1923), therefore seen
embodiment simplicity principle.
Notions simplicity exist language processing. example, Frazier (1978)
viewed arguing parser prefers simplest structure containing minimal
attachments. Bod (2000a) defines simplest tree structure sentence structure
generated smallest number subtrees given treebank.

3. Combining Likelihood Simplicity
key idea current paper principles play role perceptual organization,
albeit rather different ones: simplicity principle general cognitive preference
economy, likelihood principle probabilistic bias due previous perceptual
experiences. Informally stated, working hypothesis human cognitive system
strives simplest structure generated shortest derivation,
biased frequency previously perceived structures (some combinations
simplicity likelihood discussed Section 7). formally instantiate working
hypothesis, first need model defines set possible structures input.
paper, chosen model defines set phrase-structures input
basis treebank previously analyzed input, known Data-Oriented
Parsing DOP model (see Bod, 1998; Collins & Duffy, 2002). DOP learns grammar
extracting subtrees given treebank combines subtrees analyze fresh input.
chosen DOP (1) uses subtrees arbitrary size, thereby capturing nonlocal dependencies, (2) obtained competitive results various benchmarks
(Bod, 2001a/b; Collins & Duffy, 2002). following, first review DOP model
discuss use likelihood simplicity principles approach. Next, show
two principles combined instantiate working hypothesis.
3.1 Data-Oriented Parsing
Section, illustrate DOP model linguistic example (for rigorous definition
DOP, reader referred Bod, 1998). come back musical examples
Section 5. Suppose given following extremely small linguistic treebank two
trees resp. wanted dress rack saw dog telescope
(actual treebanks contain tens thousands trees, cf. Marcus et al. 1993):

292

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC





NP

VP

V

NP



wanted



PP
NP

saw

NP

dress P

VP
V

PP

NP


VP

NP

NP

P

dog telescope

rack

Figure 3: example treebank
DOP model parse new sentence, e.g. saw dress telescope,
combining subtrees treebank means substitution operation (indicated ):







VP

NP



NP

PP
NP

P

dress



=

telescope



VP



PP
NP

V

VP

NP

VP

PP
NP

V
saw

saw

P

NP

dress telescope

Figure 4: Parsing sentence combining subtrees Figure 3
Thus substitution operation combines two subtrees substituting second subtree
leftmost nonlexical leaf node first subtree (the result may combined
third subtree, etc.). combination subtrees results tree structure
whole sentence called derivation. Since many different subtrees, various
sizes, typically many different derivations produce, however, tree;
instance:





NP


VP

NP


VP
V
saw

dress

P

VP

NP


PP
NP



=

NP

VP
V

telescope

PP
NP

P

NP

saw dress telescope

Figure 5: different derivation produces parse tree
293

fiBOD

interesting case occurs different derivations produce different
parse trees. happens sentence ambiguous; example, DOP produces
following alternative parse tree saw dress telescope:




NP

VP

V

V
saw


P



=

PP

NP

NP

telescope

NP

VP

V

NP

saw
NP


PP

PP

NP


dress

dress

P

NP

telescope

Figure 6: different derivation produces different parse tree
3.2 Likelihood-DOP
Bod (1993), DOP enriched likelihood principle predict perceived tree
structure set possible structures. model, call Likelihood-DOP,
computes probable tree input occurrence-frequencies subtrees.
probability subtree t, P(t), computed number occurrences t, | |, divided
total number occurrences treebank-subtrees root label t.
Let r(t) return root label t. may write:
P(t) =

|t|

t': r(t')= r( t)

| t' |

probability derivation t1...tn computed product probabilities
subtrees ti:
P(t1...tn ) =

P(ti)

seen, may different derivations generate parse tree.
probability parse tree thus sum probabilities distinct derivations. Let
tid i-th subtree derivation produces tree T, probability given

P(T) =

P(tid)

294

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

parsing sentence s, interested trees assigned s,
denote Ts. best parse tree, Tbest, according Likelihood-DOP tree
maximizes probability Ts:
Tbest = arg max P(Ts)
Ts

Thus Likelihood-DOP computes probability tree sum products,
product corresponds probability certain derivation generating tree.
distinguishes Likelihood-DOP statistical parsing models identify exactly
one derivation parse tree thus compute probability tree one
product probabilities (e.g. Charniak, 1997; Collins, 1999; Eisner, 1997). Likelihood-DOP's
probability model allows including counts subtrees wide range sizes: everything
counts single-level rules counts entire trees.
Note subtree probabilities Likelihood-DOP directly estimated
relative frequencies treebank-trees. relative-frequency estimator obtains
competitive results several domains (Bonnema et al. 1997; Bod, 2001a; De Pauw, 2000),
maximize likelihood training data (Johnson, 2002).
may hidden derivations relative-frequency estimator cannot deal with. 1
estimation procedures take account hidden derivations maximize
likelihood training data. example, Bod (2000b) presents Likelihood-DOP model
estimates subtree probabilities maximum likelihood re-estimation procedure
based expectation-maximization algorithm (Dempster et al. 1977). However, since
relative frequency estimator far outperformed estimator (see
Bod et al. 2002b), stick relative frequency estimator current paper.
3.3 Simplicity-DOP
Likelihood-DOP justice preference humans display simplest
structure generated shortest derivation input. Bod (2000a), simplest tree
structure input defined tree constructed smallest number
subtrees treebank. refer model Simplicity-DOP. Instead
producing probable parse tree input, Simplicity-DOP thus produces parse
tree generated shortest derivation consisting fewest treebank-subtrees,
independent probabilities subtrees. define length derivation d,
L(d), number subtrees d; thus = t1...tn L(d) = n. Let derivation
results parse tree T, best parse tree, Tbest, according Simplicity-DOP
tree produced derivation minimal length:
Tbest = arg min L(d Ts )
Ts

Section 3.2, Ts parse tree sentence s. example, given treebank Figure
3, simplest parse tree saw dress telescope given Figure 5, since
1 subtrees restricted depth 1 relative frequency estimator coincide

maximum likelihood estimator. depth-1 DOP model corresponds stochastic context-free
grammar. well-known DOP models allow subtrees greater depth outperform depth-1
DOP models (Bod, 1998; Collins & Duffy, 2002).

295

fiBOD

parse tree generated derivation two treebank-subtrees,
parse tree Figure 6 (and parse tree) needs least three treebank-subtrees
generated. 2
shortest derivation may unique: happen different parse trees
sentence generated minimal number treebank-subtrees (also
probable parse tree may unique, never happens practice). case
back frequency ordering subtrees. is, subtrees root label
assigned rank according frequency treebank: frequent subtree (or
subtrees) root label gets rank 1, second frequent subtree gets rank 2, etc.
Next, rank (shortest) derivation computed sum ranks
subtrees involved. derivation smallest sum, highest rank, taken final
best derivation producing final best parse tree Simplicity-DOP (see Bod, 2000a).
performed one little adjustment rank subtree. adjustment averages
rank subtree ranks sub-subtrees. is, instead simply taking
rank subtree, compute rank subtree (arithmetic) mean ranks
sub-subtrees (including subtree itself). effect technique
redresses low-ranked subtree contains high-ranked sub-subtrees.
Simplicity-DOP Likelihood-DOP obtain rather similar parse accuracy
Wall Street Journal Essen Folksong Collection (in terms precision/recall -- see
Section 6), best trees predicted two models quite match. suggests
combined model, justice simplicity likelihood, may boost accuracy.
3.4 Combining Likelihood-DOP Simplicity-DOP: SL-DOP LS-DOP
underlying idea combining likelihood simplicity human perceptual system
searches simplest tree structure (generated shortest derivation)
biased likelihood tree structure. is, instead selecting simplest tree
per se, combined model selects simplest tree among n likeliest trees, n
free parameter. course ways combine simplicity likelihood
within DOP framework. straightforward alternative would select
probable tree among n simplest trees, suggesting perceptual system
searching probable structure among simplest ones. refer
first combination simplicity likelihood (which selects simplest among n
likeliest trees) Simplicity-Likelihood-DOP SL-DOP, second combination
(which selects likeliest among n simplest trees) Likelihood-Simplicity-DOP LSDOP. Note n=1, Simplicity-Likelihood-DOP equal Likelihood-DOP, since
one probable tree select from, Likelihood-Simplicity-DOP equal
Simplicity-DOP, since one simplest tree select from. Moreover, n gets large,
SL-DOP converges Simplicity-DOP LS-DOP converges Likelihood-DOP.
varying parameter n, able compare Likelihood-DOP, Simplicity-DOP
several instantiations SL-DOP LS-DOP.

2 One might argue straightforward metric simplicity would return parse tree

smallest number nodes (rather smallest number treebank-subtrees). metric
known perform quite badly (see Manning & Schtze, 1999; Bod, 2000a).

296

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

4. Computational Issues
Bod (1993) showed standard chart parsing techniques applied Likelihood-DOP.
treebank-subtree converted context-free rule r lefthand side r
corresponds root label righthand side r corresponds frontier labels
t. Indices link rules original subtrees maintain subtree's internal
structure probability. rules used create derivation forest sentence
(using chart parser -- see Charniak, 1993), probable parse computed
sampling sufficiently large number random derivations forest ("Monte Carlo
disambiguation", see Bod, 1998). technique successfully applied parsing
ATIS portion Penn Treebank (Marcus et al. 1993), extremely time consuming.
mainly number random derivations sampled reliably
estimate probable parse increases exponentially sentence length (see
Goodman, 2002). therefore questionable whether Bod's sampling technique scaled
larger domains Wall Street Journal (WSJ) portion Penn Treebank.
Goodman (1996) showed Likelihood-DOP reduced compact stochastic
context-free grammar (SCFG) contains exactly eight SCFG rules node
training set trees. Although Goodman's method still allow efficient computation
probable parse (in fact, problem computing probable parse
Likelihood-DOP NP-hard -- see Sima'an, 1996), method allow efficient
computation "maximum constituents parse", i.e. parse tree likely
largest number correct constituents. Unfortunately, Goodman's SCFG reduction method
beneficial indeed subtrees used, maximum parse accuracy usually
obtained restricting subtrees. example, Bod (2001a) shows "optimal"
subtree set achieving highest parse accuracy WSJ obtained restricting
maximum number words subtree 12 restricting maximum depth
unlexicalized subtrees 6. Goodman (2002) shows subtree restrictions,
subtree depth, may incorporated reduction method, found reduction
method optimal subtree set.
paper therefore use Bod's subtree-to-rule conversion method LikelihoodDOP, use Bod's Monte Carlo sampling technique derivation forests,
turned computationally prohibitive. Instead, use well-known Viterbi
optimization algorithm chart parsing (cf. Charniak, 1993; Manning & Schtze, 1999)
allows computing k probable derivations input cubic time. Using
algorithm, estimate probable parse tree input 10,000
probable derivations, summing probabilities derivations generate tree.
Although approach guarantee probable parse tree actually found,
shown Bod (2000a) perform least well estimation probable
parse Monte Carlo techniques ATIS corpus. Moreover, approach known
obtain significantly higher accuracy selecting parse tree generated single
probable derivation (Bod, 1998; Goodman, 2002), therefore consider
paper.
Simplicity-DOP, first convert treebank-subtrees rewrite rules
Likelihood-DOP. Next, simplest tree, i.e. shortest derivation, efficiently
computed Viterbi optimization way probable derivation, provided
assign rules equal probabilities, case shortest derivation equal
probable derivation. seen follows: rule probability p
probability derivation involving n rules equal p n, since 0<p<1 derivation
297

fiBOD

fewest rules greatest probability. experiments Section 6, give
rule probability mass equal 1/R, R number distinct rules derived Bod's
method. mentioned 3.3, shortest derivation may unique. case
compute shortest derivations input apply ranking scheme
derivations. ranks shortest derivations computed summing ranks
subtrees involve. shortest derivation smallest sum subtree ranks
taken produce best parse tree.
SL-DOP LS-DOP, compute either n likeliest n simplest trees means
Viterbi optimization. Next, either select simplest tree among n likeliest ones (for
SL-DOP) likeliest tree among n simplest ones (for LS-DOP). experiments, n
never larger 1,000.

5. Test Domains
linguistic test domain used Wall Street Journal (WSJ) portion Penn
Treebank (Marcus et al. 1993). portion contains approx. 50,000 sentences
manually annotated perceived linguistic tree structures using predefined set
lexico-syntactic labels. Since WSJ extensively used described
literature (cf. Manning & Schtze, 1999; Charniak, 2000; Collins, 2000; Bod, 2001a),
go here.
musical test domain used European folksongs Essen Folksong
Collection (Schaffrath, 1995; Huron, 1996), correspond approx. 6,200 folksongs
manually enriched perceived musical grouping structures. Essen
Folksong Collection previously used Bod (2001b) Temperley (2001) test
musical parsers. current paper presents first experiments Likelihood-DOP,
Simplicity-DOP, SL-DOP LS-DOP collection. Essen folksongs
represented staff notation encoded Essen Associative Code (ESAC).
pitch encodings ESAC resemble "solfege": scale degree numbers used replace
movable syllables "do", "re", "mi", etc. Thus 1 corresponds "do", 2 corresponds "re", etc.
Chromatic alterations represented adding either "#" "b" number.
plus ("+") minus ("-") signs added number note falls resp.
principle octave (thus -1, 1 +1 refer al "do", different octaves).
Duration represented adding period underscore number. period (".")
increases duration 50% underscore ("_") increases duration 100%;
one underscore may added number. number duration indicator,
duration corresponds smallest value. Thus pitches ESAC encoded integers
1 7 possibly preceded followed symbols octave, chromatic alteration
duration. pitch encoding treated atomic symbol, may simple "1"
complex "+2#_.". pause represented 0, possibly followed duration
indicators, treated atomic symbol. loudness timbre indicators used
ESAC.
Phrase boundaries indicated hard returns ESAC. phrases unlabeled (cf.
Section 1 paper). Yet make ESAC annotations readable DOP models,
added three basic labels phrase structures: label "S" whole song,
label "P" phrase, label "N" atomic symbol. way, obtained
conventional tree structures could directly employed DOP models parse new
input. use label "N" distinguishes annotations previous work (Bod,
298

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

2001b/c) used labels song phrase ("S" "P"). addition "N"
enhances productivity robustness musical parsing model, although leads
much larger number subtrees.
example, assume simple melody consisting two phrases, (1 2) (2 3),
tree structure given Figure 7.

P

P

N

N

N

N

1

2

2

3

Figure 7: Example musical tree structure consisting two phrases
Subtrees extracted tree structure include following:

P
N

P
P

N

N

N

N

2

3

3

1

Figure 8: subtrees extracted tree figure 7
Thus first subtree indicates phrase starting note 1, followed exactly one
(unspecified) note, phrase followed exactly one (unspecified) phrase.
subtrees used parse new musical input way explained
linguistic parsing Section 3.

6. Experimental Evaluation Comparison
evaluate DOP models, used blind testing method randomly divides
treebank training set test set, strings test set parsed
means subtrees training set. applied standard PARSEVAL metrics
precision recall compare proposed parse tree P corresponding correct test
set parse tree follows (cf. Black et al. 1991):
# correct constituents P

# correct constituents P

Precision =

Recall =

# constituents P

# constituents

constituent P "correct" exists constituent label spans
atomic symbols (i.e. words notes).3 Since precision recall obtain rather
3 precision recall scores computed using "evalb" program (available via

http://www.cs.nyu.edu/cs/projects/proteus/evalb/)

299

fiBOD

different results (see Bod, 2001b), often balanced single measure
performance, known F-score (see Manning & Schtze, 1999):
F-score =

2 Precision Recall
Precision + Recall

experiments, divided treebanks (i.e. WSJ Essen Folksong
Collection) 10 training/test set splits: 10% WSJ used test material time
(sentences 40 words), Essen Folksong Collection test sets 1,000 folksongs
used time. words test set unknown training set,
guessed categories using statistics word-endings, hyphenation capitalization
(cf. Bod, 2001a); unknown notes. previous work (Bod, 2001a), limited
maximum size subtrees depth 14, used random samples 400,000 subtrees
depth > 1 14.4 Next, restricted maximum number atomic symbols
subtree 12 maximum depth unlexicalized subtrees 6. subtrees
smoothed technique described Bod (1998: 85-94) based simple Good-Turing
estimation (Good, 1953).
Table 1 shows mean F-scores obtained SL-DOP LS-DOP language
music various values n. Recall n=1, SL-DOP equal Likelihood-DOP
LS-DOP equal Simplicity-DOP.

n
1
5
10
11
12
13
14
15
20
50
100
1,000

SL-DOP

LS-DOP

(simplest among n likeliest)

(likeliest among n simplest)

Language

Music

Language

Music

87.9%
89.3%
90.2%
90.2%
90.2%
90.2%
90.2%
90.2%
90.0%
88.7%
86.8%
85.6%

86.0%
86.8%
87.2%
87.3%
87.3%
87.3%
87.2%
87.2%
86.9%
85.6%
84.3%
84.3%

85.6%
86.1%
87.0%
87.0%
87.0%
87.0%
87.0%
87.0%
87.1%
87.4%
87.9%
87.9%

84.3%
85.5%
85.7%
85.7%
85.7%
85.7%
85.7%
85.7%
85.7%
86.0%
86.0%
86.0%

Table 1: F-scores obtained SL-DOP LS-DOP language music

4 random subtree samples selected first exhaustively computing complete set

subtrees (this computationally prohibitive). Instead, particular depth > 1 sampled
subtrees randomly selecting node random tree training set, selected
random expansions node subtree particular depth obtained. repeated
procedure 400,000 times depth > 1 14.

300

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

Table shows increase accuracy SL-DOP LS-DOP
value n increases 1 11. accuracy SL-DOP decreases n=13
converges Simplicity-DOP (i.e. LS-DOP n=1), accuracy LS-DOP continues
increase converges Likelihood-DOP (i.e. SL-DOP n=1). highest accuracy
obtained SL-DOP 11 n 13, language music. Thus SL-DOP outperforms
Likelihood-DOP Simplicity-DOP, selection simplest structure
top likeliest ones turns promising model selection likeliest
structure top simplest ones. According paired t-testing, accuracy
improvement SL-DOP n=11 SL-DOP n=1 (when equal Likelihood-DOP)
statistically significant language (p<.0001) music (p<.006).
surprising SL-DOP reaches highest accuracy small value n.
even surprising exactly model (with parameter setting) obtains
maximum accuracy language music. model embodies idea
perceptual system strives simplest structure searches among
probable structures.
compare results language others, tested SL-DOP n=11
standard division WSJ, uses sections 2 21 training (approx. 40,000
sentences) section 23 testing (2416 sentences 100 words) (see e.g. Manning &
Schtze, 1999; Charniak, 2000; Collins, 2000). division, SL-DOP achieved F-score
90.7% best previous models obtained F-score 89.7% (Collins, 2000; Bod,
2001a). terms error reduction, SL-DOP improves 9.6% models.
common report accuracy sentences 40 words WSJ, SLDOP obtained F-score 91.8%.
musical results compared Bod (2001b/c), tested three probabilistic
parsing models increasing complexity training/test set splits Essen
Folksong Collection. best results obtained hybrid DOP-Markov parser:
80.7% F-score. significantly worse best result 87.3% obtained SL-DOP
splits Essen folksongs. difference may explained fact
hybrid DOP-Markov parser Bod (2001b/c) takes account context higher
nodes tree sister nodes, DOP models presented
current paper take subtree account (almost) arbitrary width depth, thereby
covering larger amount musical context. Moreover, mentioned Section 5, models
Bod (2001b/c) use label "N" notes; instead, Markov approach used
parse new sequences notes.
would interesting compare musical results melodic parser
Temperley (2001), uses system preference rules similar Lerdahl Jackendoff
(1983), evaluated Essen Folksong Collection.
tested several test sets 1,000 randomly selected folksongs, Temperley used one test
set 65 folksongs moreover cleaned eliminating folksongs irregular
meter (Temperley, 2001: 74). therefore difficult compare results Temperley's;
yet, noteworthy Temperley's parser correctly identified 75.5% phrase
boundaries. Although lower 87.3% obtained SL-DOP, Temperley's parser
"trained" previously analyzed examples model (though note
Temperley's results obtained tuning optimal phrase length parser
average phrase length Essen Folksong Collection).
perhaps mentioned parsing models trained treebanks widely
used natural language processing, still rather uncommon musical processing.
301

fiBOD

musical parsing models, including Temperley's, employ rule-based approach
parsing based combination low-level rules -- "prefer phrase boundaries
large intervals" -- higher-level rules -- "prefer phrase boundaries changes
harmony". low-level rules usually based well-known Gestalt principles
proximity similarity (Wertheimer, 1923), prefer phrase boundaries larger
intervallic distances. However, Bod (2001c) shown Gestalt principles
predict incorrect phrase boundaries number folksongs, higher-level
phenomena cannot alleviate incorrect predictions. folksongs contain phrase
boundary falls large pitch time interval (which called
jump-phrases) rather intervals -- would predicted Gestalt principles.
Moreover, musical factors, melodic parallelism, meter harmony, predict
exactly incorrect phrase boundaries cases (see Bod, 2001b/c details).
conjectured jump-phrases inherently memory-based, reflecting idiomdependent pitch contours (cf. Huron, 1996; Snyder, 2000), best captured
memory-based model tries mimic musical experience listener
certain culture (Bod, 2001c).

7. Discussion Conclusion
seen combination simplicity likelihood quite rewarding linguistic
musical structuring, suggesting interesting parallel two modalities. Yet, one
may question whether model massively memorizes re-uses previously perceived
structures cognitive plausibility. Although question important want
claim cognitive relevance model, appears evidence people store
various kinds previously heard fragments, language (Jurafsky, 2002) music
(Saffran et al. 2000). people store fragments arbitrary size, proposed DOP?
overview article, Jurafsky (2002) reports large body psycholinguistic evidence
showing people store lexical items bigrams, frequent phrases
even whole sentences. case sentences, people store idiomatic sentences,
"regular" high-frequency sentences.5 Thus, least language
evidence humans store fragments arbitrary size provided fragments
certain minimal frequency. suggests humans need always parse new input
rules grammar, productively re-use previously analyzed fragments.
Yet, evidence people store fragments hear, suggested DOP.
high-frequency fragments seem memorized. However, human perceptual
faculty needs learn fragments stored, initially need keep track
fragments (with possibility forgetting them) otherwise frequencies never
accumulate. results model continuously incrementally updates fragment
memory given new input -- correspondence DOP approach,
approaches (cf. Daelemans, 1999; Scha et al. 1999; Spiro, 2002).
acknowledge importance rule-based system acquiring fragment memory,
substantial memory available may efficient construct tree means
already parsed fragments constructing entirely means rules. many cognitive
5 results derived differences reaction times sentence recognition

frequency (whole) test sentences varied, variables, lexical frequency,
bigram frequency, plausibility, syntactic/semantic complexity, etc., kept constant.

302

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

activities advantageous store results, immediately retrieved
memory, rather computing time scratch. shown,
example, manual reaches (Rosenbaum et al. 1992), arithmetic operations (Rickard et al.
1994), word formation (Baayen et al. 1997), mention few. linguistic musical
parsing may exception this.
stressed experiments reported paper limited least two
respects. First, musical test domain rather restricted. wide variety linguistic
treebanks currently available (see Manning & Schtze, 1999), number musical
treebanks extremely limited. thus need larger richer annotated musical
corpora covering broader domains. development annotated corpora may timeconsuming, experience natural language processing shown worth
effort, since corpus-based parsing systems dramatically outperform grammar-based parsing
systems. second limitation experiments evaluated parse
results rather parse process. is, assessed accurately
models mimic input-output behavior human annotator, without investigating
process annotator arrived perceived structures. unlikely humans
process perceptual input computing 10,000 likely derivations using random samples
400,000 subtrees current paper. Yet, many applications suffices
know perceived structure rather process led structure.
seen combination simplicity likelihood predicts perceived structure
high degree accuracy.
proposals integrating principles simplicity likelihood
human perception (see Chater, 1999 review). Chater notes context
Information Theory (Shannon, 1948), principles simplicity likelihood identical.
context, simplicity principle interpreted minimizing expected length encode
message i, log2 p bits, leads result maximizing
probability i. used information-theoretical definition simplest structure
Simplicity-DOP, would return structure Likelihood-DOP, improved
results would obtained combination two. hand, defining
simplest structure one generated smallest number subtrees, independent
probabilities, created notion simplicity provably different notion
likely structure, which, combined Likelihood-DOP, obtained improved results.
Another integration two principles may provided notion Minimum
Description Length MDL (cf. Rissanen, 1978). MDL principle viewed
preferring statistical model allows shortest encoding training data.
relevant encoding consists two parts: first part encodes model data,
second part encodes data terms model (in bit length). MDL closely related
stochastic complexity (Rissanen, 1989) Kolmogorov complexity (Li Vitanyi, 1997),
used natural language processing estimating parameters stochastic
grammar (e.g. Osborne, 1999). leave open research question whether
MDL successfully used estimating parameters DOP's subtrees. However,
since MDL known give asymptotically results maximum likelihood estimation
(MLE) (Rissanen, 1989), application DOP may lead unproductive model.
maximum likelihood estimator assign training set trees empirical
frequencies, assign 0 weight trees (see Bonnema, 2002 proof).
would result model generate training data strings.
Johnson (2002) argues may overlearning problem rather problem
303

fiBOD

MLE per se, standard methods, cross-validation regularization, would seem
principle ways avoid overlearning. leave issue future
investigation.
idea general underlying model language music uncontroversial.
linguistics usually assumed humans separate language faculty, Lerdahl
Jackendoff (1983) argued separate music faculty. work propose
separate faculties exist, wants focus commonalities rather
differences faculties, aiming finding deeper "faculty" may hold
perception general. hypothesis perceptual system strives simplest
structure searches among likeliest structures.

Acknowledgements
Thanks Aline Honingh, Remko Scha, Neta Spiro, Menno van Zaanen three anonymous
reviewers excellent comments. preliminary version paper presented
keynote talk LCG workshop ("Learning Computational Grammars", Tbingen, 2001).

References
Baayen, R. H., Dijkstra, T. & Schreuder, R. (1997). Singular Plurals Dutch: Evidence
Parallel Dual-Route Model. Journal Memory Language, 37, 94-117.
Black, E., Abney, S., Flickinger, D., Gnadiec, C., Grishman, R., Harrison, P., Hindle, D.,
Ingria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B.
& Strzalkowski, T. (1991). Procedure Quantitatively Comparing Syntactic
Coverage English, Proceedings DARPA Speech Natural Language
Workshop, Pacific Grove, Morgan Kaufmann.
Bod, R. (1993). Using Annotated Language Corpus Virtual Stochastic Grammar.
Proceedings AAAI-93, Menlo Park, Ca.
Bod, R. (1998). Beyond Grammar: Experience-Based Theory Language. Stanford:
CSLI Publications (Lecture notes number 88).
Bod, R. (2000a). Parsing Shortest Derivation. Proceedings COLING-2000,
Saarbrcken, Germany.
Bod, R. (2000b). Combining Semantic Syntactic Structure Language Modeling.
Proceedings ICSLP-2000, Beijing, China.
Bod, R. (2001a). Minimal Set Fragments Achieves Maximal Parse
Accuracy? Proceedings ACL'2001, Toulouse, France.
Bod, R. (2001b). Memory-Based Model Music Analysis. Proceedings International
Computer Music Conference (ICMC'2001), Havana, Cuba.
Bod, R. (2001c). Memory-Based Models Melodic Analysis: Challenging Gestalt
Principles. Journal New Music Research, 31(1), 26-36. (available
http://staff.science.uva.nl/~rens/jnmr01.pdf)
304

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

Bod, R., Hay, J. & Jannedy, S. (Eds.) (2002a). Probabilistic Linguistics. Cambridge,
MIT Press. (in press)
Bod, R., Scha, R. & Sima'an, K. (Eds.) (2002b). Data-Oriented Parsing. Stanford, CSLI
Publications. (in press)
Bonnema, R. (2002). Probability Models DOP. Bod et al. (2002b).
Bonnema, R., Bod, R. & Scha, R. (1997). DOP Model Semantic Interpretation,
Proceedings ACL/EACL-97, Madrid, Spain.
Buffart, H., Leeuwenberg, E. & Restle , F. (1983). Analysis Ambiguity Visual Pattern
Completion. Journal Experimental Psychology: Human Perception
Performance. 9, 980-1000.
Charniak, E. (1993). Statistical Language Learning, Cambridge, MIT Press.
Charniak, E. (1997). Statistical Techniques Natural Language Parsing, AI Magazine,
Winter 1997, 32-43.
Charniak, E. (2000). Maximum-Entropy-Inspired Parser. Proceedings ANLPNAACL'2000, Seattle, Washington.
Chater, N. (1999). Search Simplicity: Fundamental Cognitive Principle?
Quarterly Journal Experimental Psychology, 52A(2), 273-302.
Chomsky, N. (1965). Aspects Theory Syntax, Cambridge, MIT Press.
Collard, R., Vos, P. & Leeuwenberg, E. (1981). Melody Tells Metre Music .
Zeitschrift fr Psychologie. 189, 25-33.
Collins, M. (1999). Head-Driven Statistical Models Natural Language Parsing, PhDthesis, University Pennsylvania, PA.
Collins, M. (2000). Discriminative Reranking Natural Language Parsing, Proceedings
ICML-2000, Stanford, Ca.
Collins, M. & Duffy, N. (2002). New Ranking Algorithms Parsing Tagging: Kernels
Discrete Structures, Voted Perceptron. Proceedings ACL'2002,
Philadelphia, PA.
Daelemans, W. (1999). Introduction Special Issue Memory-Based Language
Processing. Journal Experimental Theoretical Artificial Intelligence 11(3),
287-296.
Dastani, M. (1998). Languages Perception. ILLC Dissertation Series 1998-05, University
Amsterdam.
Dempster, A., Laird, N. & Rubin, D. (1977). Maximum Likelihood Incomplete Data via
EM Algorithm, Journal Royal Statistical Society, 39, 1-38.

305

fiBOD

De Pauw, G. (2000). Aspects Pattern-matching Data-Oriented Parsing, Proceedings
COLING-2000, Saarbrcken, Germany.
Eisner, J. (1997). Bilexical Grammars Cubic-Time Probabilistic Parser, Proceedings
Fifth International Workshop Parsing Technologies, Boston, Mass.
Frazier, L. (1978). Comprehending Sentences: Syntactic Parsing Strategies. PhD.
Thesis, University Connecticut.
Good, I. (1953). Population Frequencies Species Estimation Population
Parameters, Biometrika 40, 237-264.
Goodman, J. (1996). Efficient Algorithms Parsing DOP Model, Proceedings
Empirical Methods Natural Language Processing, Philadelphia, PA.
Goodman, J. (2002). Efficient Parsing DOP PCFG-Reductions. Bod et al. 2002b.
von Helmholtz, H. (1910). Treatise Physiological Optics (Vol. 3), Dover, New York.
Hoffman, D. (1998). Visual Intelligence. New York, Norton & Company, Inc.
Huron, D. (1996). Melodic Arch Western Folksongs. Computing Musicology 10, 223.
Johnson, M. (2002). DOP Estimation Method Biased Inconsistent. Computational
Linguistics, 28, 71-76.
Jurafsky, D. (2002). Probabilistic Modeling Psycholinguistics: Comprehension
Production. Bod et al. 2002a. (available http://www.colorado.edu/ling/jurafsky/
prob.ps)
Kersten, D. (1999). High-level vision statistical inference. Gazzaniga , S. (Ed.), New
Cognitive Neurosciences, Cambridge, MIT Press.
Leeuwenberg, E. (1971). Perceptual Coding Language Perceptual Auditory
Patterns. American Journal Psychology. 84, 307-349.
Lerdahl, F. & Jackendoff, R. (1983). Generative Theory Tonal Music. Cambridge,
MIT Press.
Li, M. & Vitanyi, P. (1997). Introduction Kolmogorov Complexity
Applications (2nd ed.). New York, Springer.
Longuet-Higgins, H. (1976). Perception Melodies. Nature 263, 646-653.
Longuet-Higgins, H. Lee, C. (1987). Rhythmic Interpretation Monophonic Music.
Mental Processes: Studies Cognitive Science, Cambridge, MIT Press.
Manning, C. & Schtze, H. (1999). Foundations Statistical Natural Language
Processing. Cambridge, MIT Press.
Marcus, M., Santorini, B., & Marcinkiewicz, M. (1993). Building Large Annotated Corpus
English: Penn Treebank, Computational Linguistics 19(2).
306

fiA UNIFIED MODEL STRUCTURAL ORGANIZATION LANGUAGE MUSIC

Marr, D. (1982). Vision. San Francisco, Freeman.
Martin, W., Church, K. & Patil, R. (1987). Preliminary Analysis Breadth-first Parsing
Algorithm: Theoretical Experimental Results. Bolc, L. (Ed.), Natural Language
Parsing Systems, Springer Verlag, Berlin.
Mumford, D. (1999). dawning age stochasticity. Based lecture
Accademia Nazionale dei Lincei. (available http://www.dam.brown.edu/people/
mumford/Papers/Dawning.ps)
Osborne, M. (1999). Minimal description length-based induction definite clause grammars
noun phrase identification. Proceedings EACL Workshop Computational
Natural Language Learning. Bergen, Norway.
Palmer, S. (1977). Hierarchical Structure Perceptual Representation. Cognitive
Psychology, 9, 441-474.
Raphael, C. (1999). Automatic Segmentation Acoustic Musical Signals Using Hidden
Markov Models. IEEE Transactions Pattern Analysis Machine Intelligence,
21(4), 360-370.
Restle , F. (1970). Theory Serial Pattern Learning: Structural Trees. Psychological
Review, 86, 1-24.
Rickard, T., Healy, A. & Bourne Jr., E. (1994). cognitive structure basic arithmetic
skills: Operation, order symbol transfer effects. Journal Experimental
Psychology: Learning, Memory Cognition, 20, 1139-1153.
Rissanen, J. (1978). Modeling shortest data description. Automatica, 14, 465-471.
Rissanen, J. (1989). Stochastic Complexity Statistical Inquiry. Series Computer
Science - Volume 15. World Scientific, 1989.
Rosenbaum, D., Vaughan, J., Barnes, H. & Jorgensen, M. (1992). Time course movement
planning: Selection handgrips object manipulation. Journal Experimental
Psychology: Learning, Memory Cognition, 18, 1058-1073.
Saffran, J., Loman, M. & Robertson, R. (2000). Infant Memory Musical Experiences.
Cognition, 77, B16-23.
Scha, R., Bod, R. & Sima'an, K. (1999). Memory-Based Syntactic Analysis. Journal
Experimental Theoretical Artificial Intelligence, 11(3), 409-440.
Schaffrath, H. (1995). Essen Folksong Collection Humdrum Kern Format. D.
Huron (ed.). Menlo Park, CA: Center Computer Assisted Research
Humanities.
Shannon, C. (1948). Mathematical Theory Communication. Bell System Technical
Journal. 27, 379-423, 623-656.
Sima'an, K. (1996). Computational Complexity Probabilistic Disambiguation means
Tree Grammars. Proceedings COLING-96, Copenhagen, Denmark.
307

fiBOD

Simon, H. (1972). Complexity Representation Patterned Sequences Symbols.
Psychological Review. 79, 369-382.
Snyder, B. (2000). Music Memory. Cambridge, MIT Press.
Spiro, N. (2002). Combining Grammar-based Memory-based Models Perception
Time Signature Phase. Anagnostopoulou, C., Ferrand, M. & Smaill, A. (Eds.).
Music Artificial Intelligence, Lecture Notes Artificial Intelligence, Vol. 2445,
Springer-Verlag, 186-197.
Temperley, D. (2001). Cognition Basic Musical Structures. Cambridge, MIT
Press.
Wertheimer, M. (1923). Untersuchungen zur Lehre von der Gestalt. Psychologische
Forschung 4, 301-350.
Wundt, W. (1901). Sprachgeschichte und Sprachpsychologie. Engelmann, Leipzig.

308


