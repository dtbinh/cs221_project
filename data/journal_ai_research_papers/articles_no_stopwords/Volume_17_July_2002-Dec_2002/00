journal artificial intelligence

submitted published

critical assessment
benchmark comparison
adele e howe
eric dahlman

computer science department
colorado state university fort collins co

howe cs colostate edu
dahlman cs colostate edu

abstract
recent trends led empirical comparison becoming commonplace field started settle methodology comparisons
obvious practical reasons requires running subset planners subset
characterize methodology examine eight implicit assumptions
planners metrics used many comparisons assumptions pr performance general purpose planner
penalized biased executed sampling domains pr minor syntactic
differences representation affect performance pr solvable strips capable planners unless require adl planner assumptions
pl latest version planner best one use pl default parameter settings
approximate good performance pl time cut offs unduly bias outcome
metrics assumptions performance degrades similarly planner run
degraded runtime environments e g machine platform number plan
steps distinguishes performance assumptions supported
empirically particular planners affected differently assumptions
conclude call community devote resources improving state
practice especially enhancing available benchmark

introduction
recent years comparative evaluation become increasingly common demonstrating
capabilities planners planners directly compared
taken set domains recent advances
translated dramatic increases size solved weld
empirical comparison highlighted improvements
comparative evaluation significantly uenced expedited
artificial intelligence scheduling aips conference competitions
competitions dual effect highlighting progress field providing
relatively unbiased comparison state art planners individual researchers
compare planners others include fewer planners fewer test
time constraints
support first competition mcdermott drew mcdermott defined
contributions organizing committee shared domain definition
language pddl mcdermott et al domain definition language

c ai access foundation morgan kaufmann publishers rights reserved

fihowe dahlman

common language means planners performance directly compared without
entailing hand translation factoring different representational capabilities
second benefit lack translation least human accomplished translation meant performance could compared large number
domains fact five competition planners given large number
adl track strips track within seven domains
including one domain planner developers never seen prior competition
first competition generated large collection benchmarks seven domains used
competition plus considered use domains available
ftp ftp cs yale edu pub mcdermott domains second competition added three
novel domains set
third major benefit competitions appear motivated researchers develop systems others use number entrants went five
first competition second additionally competitors six
sixteen competitors made code available web sites thus others
perform comparisons
describe current practice comparative evaluation evolved
since aips competitions critically examine underlying assumptions
practice summarize existing evidence assumptions describe
experimental tests others previously considered assumptions
organized three groups concerning critical decisions experiment design
tested planners included performance metrics collected
comparisons part competitions specific researchers proven enormously useful motivating progress field goal understand assumptions
readers know far comparative generalized contrast
competitions community cannot legislate fairness individual researcher comparative evaluations readers may able identify cases viewed
skeptically confidence thus conclude observations
call considerably metrics methodologies
support planner evaluation
contrast competitions goal declare winner goal
critique individual studies consequently draw attention away
possible interpretation whenever possible report letter designators
assigned randomly planners

competitions direct comparisons

recently aips competitions spurred considerable interest comparative evaluation roots comparative planner evaluation go back considerably however
although researchers able run side side comparisons planners
solve particular e construct sequence actions transform initial state
goal state planners require domain theory description domain theory represents
abstract actions executed environment typically domain descriptions include
variables instantiated specific objects values multiple defined
domain descriptions require initial state description goal state association
domain



fia critical assessment benchmark comparison

others able demonstrate performance planner well known could viewed de facto benchmarks sussman anomaly sussman
blocksworld premier benchmark domain many years
every planner needed cut teeth
researchers tired blocksworld many called additional benchmark
environments mark drummond leslie kaelbling stanley rosenschein organized
workshop benchmarks metrics drummond kaelbling rosenschein
testbed environments martha pollack tileworld pollack ringuette
steve hanks truckworld hanks nguyen thomas used comparing
within planners ucpop penberthy weld distributed
large set domains demonstration purposes
barry fox mark ringer set scheduling benchmarks web page
http www newosoft com benchmrx collect definitions emphasis
manufacturing applications recently planet coordinating organization european scheduling researchers proposed benchmark collection
initiative http planet dfki de
clearly benchmark become well established means demonstrating
planner performance however practice known benefits pitfalls hanks pollack
cohen discuss detail context agent architecture design
benefits include providing metrics comparison supporting experimental control
pitfalls include lack generality potential benchmarks
unduly uence next generation solutions words researchers construct
solutions excel benchmarks regardless whether benchmarks accurately
represent desired real applications
obtain benefits listed benchmarks often idealized
simplified versions real cohen points papers
ai least aaai conference exploit benchmark yet relate
benchmarks target tasks may significant example study
owshop scheduling benchmarks found performance standard benchmark set generalize performance realistic structure watson
barbulescu howe whitley study blocksworld found
best known blocksworld benchmark atypical require short
plans solution optimal solutions easy slaney thiebaux
spite diculties benchmark aips competitions considerably uenced comparative planner evaluations example aips conference proceedings chien kambhampati knoblock papers improvements classical papers conference relied heavily
comparative evaluation benchmark papers concerned scheduling
specific applications theoretical analyses special extensions standard paradigm
e g pomdp sensing classical papers six used aips
competition benchmark set six used kautz selman distribution
blackbox kautz three added
well showed subset benchmark distributions
scheduling area related actions already known sequence still
needs determined flowshop scheduling type manufacturing scheduling



fihowe dahlman

e g drew mcdermott first competition logistics blocksworld rocket
gripper domains popular used papers respectively availability planners competition exploited eight papers compared
systems aips planners blackbox stan ipp hsp
papers respectively

assumptions direct comparison
canonical planner evaluation experiment follows procedure table procedure
designed compare performance planner previous state art
highlight superior performance set cases planner exact form
experiment depends purpose e g showing superiority class
highlighting effect design decision
select construct subset planner domains
construct set
running large set benchmark
selecting desirable features
varying facet increase diculty e g number blocks
select planners
representative state art
similar distinct planner depending point comparison advance planner
available able parse
run planners default parameters setting upper limit
time allowed
record solved many plan steps actions solution
much cpu time required solve fail time
table canonical comparative planner evaluation experiment
protocol depends three selections planners evaluation metrics
simply practical even desirable run available planners available
thus one needs make informed decisions select purpose
examine assumptions underlying decisions help make
informed every planner comparison adopt every one assumptions
assumptions ones commonly found planner comparisons example
comparisons designed specific purpose e g scale certain
suitability planner logistics carefully select particular types
benchmark sets


fia critical assessment benchmark comparison

many systems developed solve particular type
explore specific type algorithmic variation consequently one would expect
perform better developed even
designed specific purpose test set used development may
subtly biased development community knows planner performance depends
features general researchers tend design
planners general purpose consequently comparisons assume
performance general purpose planner penalized biased
executed sampling domains assumption

community knows representation uences planner performance
example benchmark sets include many versions blocksworld designed different planner developers versions vary representation
minor apparently syntactic changes e g clauses ordered within operators
initial conditions goals whether information extraneous changes ecting addition domain knowledge e g constraints included whether
variables typed consequently comparisons assume
syntactic representational modifications matter affect planner equally assumption

pddl includes field requirements capabilities required planner solve
pddl defined values requirements field base default requirement strips meaning strips derived add delete sets action effects adl
pednault action description language requires variable typing disjunctive preconditions equality built predicate quantified preconditions conditional effects
addition strips capability yet many planners ignore requirements
field reject specifies adl ignoring many requirements
could cause trouble thus comparisons assume
benchmark set solvable strips planner unless
require adl assumption

planners wonderful trend making planners publicly available led dilemma
determining use configure compounded
longevity planner projects projects produced multiple versions
consequently comparisons tend assume
latest version planner best planner assumption

planners may include parameters example blackbox planner allows
user define strategy applying different solution methods researchers expect
parameters affect performance consequently comparisons assume
default parameter settings approximate good performance planner assumption



fihowe dahlman

experiments invariably use time cut offs concluding yet found
solution declared failure many planners would need exhaustively search large space
declare failure practical reasons time threshold set determine
halt planner failure declared time reached thus comparisons
assume
one picks suciently high time threshold highly unlikely
solution would found slightly time granted planner
assumption

metrics ideally performance would measured well planner

job e constructing best possible plan solve eciently
planner shown solve possible basic
metric performance number percentage actually solved within
allowed time metric commonly reported competitions however
papers tend report directly typically test relatively small number

eciency clearly function memory effort memory size limited
hardware effort measured cpu time preferably platform
language cpu time well known programmer skill
varies code designed fast prototyping fast execution numbers
literature cannot compared newer numbers due processor speed improvements
however cpu times regenerated experimenter environment one assumes

performance degrades similarly reductions capabilities runtime
environment e g cpu speed memory size metric assumption

words experimenter user system expect code
optimized particular compiler operating system hardware configuration
perform similarly moved another compatible environment
commonly reported comparison metric computation time second
number steps actions planners allow parallel execution plan although
seeks solutions achieving goals goals defined terms states
world lend well general measures quality fact quality likely
dependent e g resource cost amount time execute robustness
number plan steps favored comparisons assume
number steps resulting plan varies planner solutions approximates quality metric assumption

comparison competitions especially unenviable task determining
trade combine three metrics number solved time number steps thus
number steps matter comparison could simplified
converted assumption testable question summarized
literature question ran experiment test


fia critical assessment benchmark comparison

experimental setup
key issues examined previously directly indirectly
simply summarize subsections follow however open
questions ran seven well known planners large set benchmark
planners accept pddl representation although built
translators pddl internal representation others rely translators
added several versions planner available included total
planners basic set comprises ucpop benchmarks aips
competition test sets additional set developed specific application
exception permuted see section assumption
specifics run mhz ultrasparc megabytes
memory running sunos whenever possible versions compiled developers
used source code available compiled systems according
developers instructions planners written common lisp run allegro
common lisp version planners compiled gcc egcs version
planner given minute limit wall clock time solution
however times reported run times returned operating system
planners

planners called primitive action planners wilkins desjardins
planners require relatively limited domain knowledge construct plans
simple action descriptions aips competition required planners accept
pddl majority planners used study competition entrants later
versions thereof common language facilitated comparison planners without address effects translation step two exceptions ucpop
prodigy however representations similar pddl translated automatically planners represent five different approaches plan graph analysis
satisfiability heuristic search state space learning
partial order possible used multiple versions planner
necessarily recent conducted study period time almost years froze set early comparing performance declare
winner think lack recent versions undermined
testing assumptions

ipp koehler nebel hoffmann dimopoulos extends graphplan blum

furst accept richer plan description language early versions
language subset adl extends strips formalism graphplan
allow conditional universally quantified effects operators version
negation handled via introduction predicates negated preconditions
used actual time lightly loaded machines occasionally system would thrash due
inadequate memory resulting little progress considerable time
used bus system manager running planners howe dahlman hansen scheetz
von mayrhauser implemented aips competition planners facilitated
running many different planners somewhat bias included



fihowe dahlman

corresponding mutual exclusion rules subsequent versions handle directly koehler
used aips version ipp well later version
sgp sensory graph plan weld anderson smith extends graphplan
richer domain description language primarily focusing uncertainty sensing
ipp transformation performed expansion techniques remove
quantification sgp directly supports negated preconditions conditional effects
sgp tends slower implemented common lisp instead c
graphplan planners used sgp version b
stan state analysis fox long extends graphplan part
adding preprocessor called tim infer type information domain
information used within reduce size search
space graphplan would search stan incorporated optimized data
structures bit vectors graph help avoid many redundant calculations performed graphplan additionally stan maintains wave front graph
construction track remaining goals limit graph construction subsequent versions
incorporated analyses e g symmetry exploitation additional simpler engine four versions stan tested aips competition version version
version development snapshot version
blackbox kautz selman converts boolean satisfiability
solved variety different techniques user indicates
techniques tried order constructing satisfiability
blackbox uses graph constructed graphplan blackbox used
version version b
hsp heuristic search planner bonet geffner heuristic search
planner uses variation hill climbing random restarts solve
heuristic graphplan solve relaxed form
study used version algorithmic refinement
version entered aips competition version
prodigy prodigy group combines state space backward chaining goal state plan construction consists head plan
totally ordered actions starting initial state tail plan partially ordered
actions related goal state although ocially entered competition informal presented aips competition suggested prodigy performed well
comparison entrants used prodigy version
ucpop barrett golden penberthy weld partial order causal link
planner decision include ucpop several factors first
expand quantifiers negated preconditions domains expansion
grounding operators great make insolvable second ucpop
significantly different interest recently resurfaced
used ucpop version
thank eugene fink code translates pddl prodigy



fia critical assessment benchmark comparison

source
domains
benchmarks


aips


aips


developers


application


table summary testing set source number
domains within domains
test

following standard practice experiments require planners solve commonly available
benchmark aips competition addition test assumptions uence domains assumption pr representations
assumption pr include permuted benchmark application section describes set domains study
focusing source composition
require strips capabilities e add delete lists chose
least common denominator several reasons first capable planners still handle
strips requirements thus maximized number planners could included
experiment surprisingly type available second
examining assumptions evaluation including effect required capabilities
performance propose duplicate effort competitions singling
planners distinction rather purpose determine factors differentially
affect planners
bulk came aips aips sets
set distributed pddl specification remaining
solicited several sources source counts domains
summarized table
benchmark preponderance test sets toy
well known synthetic designed test attribute planners
blocksworld domain long included evaluation well known
subgoal interactions supports constructing increasingly complex
e g towers blocks benchmark simplified versions realistic
e g tire refrigerator repair logistics domains used
set included ucpop planner contributed large number
people include multiple encodings domains especially blocksworld
aips competitions first aips competition drew mcdermott solicited competitors well constructing
mystery domain semantically useless names objects operators
generated domain automatically competition included
six domains robot movement grid gripper balls


fihowe dahlman

moved rooms robot two grippers logistics transporting packages organizing snacks movie watching two mystery domains disguised logistics

format competition required entrants execute
first round could solved planner round two
planners executed three domains one included
first round
competition attracted competitors three tracks strips adl
hand tailored track required performance five domains logistics
blocksworld parts machining freecell card game miconic elevator control
domains determined organizing committee fahiem bacchus
chair represented somewhat broader range chose untyped
strips track set
scientific standpoint one interesting conclusions competitions observed trade offs performance planners appeared excel different
solving set finding solution faster ipp solved
found shorter plans round two stan solved fastest
hsp solved round one blackbox solved fastest
round one awards given two groups distinguished planners across
different categories planners strips adl hand tailored according
judges impossible say one planner best bacchus
talplanner highest distinguished planner group graphs performance differences computation time relative planners
scale however planner failed solve makes trends
harder interpret computation time graphs gaps
purpose competitions showcase planner technology
succeeded admirably planners solved much harder could
accomplished years past trend planners handling increasingly dicult
competition test sets may become historical interest tracking field
progress
solicited planner developers asked planner developers
used development one developer maria fox sent us domain
sodor logistics application set used would
included domains received others
applications miconic elevator domain aips competition
derived actual application domain extremely
simplified e g removing arithmetic
add another realistic comparison included one application set test domains generating cases test software interface
similarities software interface test cases plans developed system
several years ago automatically generating interface test cases ai planner
system designed generate test cases user interface storage technology robot tape library howe von mayrhauser mraz interface e
commands interface coded domain theory example mount com

fia critical assessment benchmark comparison

mand action description required drive empty effect changing
position tape mounted changing status tape drive
described initial states tape library e g tapes resident
status devices software controller goal states human operator might
wish achieve
time found simplest could generated
planners available included application part knew would
challenge part test set include three domain theories different ways
coding application involving operators twenty four domain
included wanted include enough see effect
many overly bias relatively simple requiring
movement one tape coupled status changes
still dicult could solved original system

assumptions
general purpose planners exhibit differential capabilities domains sometimes even
within domain thus selection set would seem critical
evaluation example many benchmark sets variants logistics
thus general purpose planner actually tailored logistics may appear
better overall current benchmarks section empirically examine
possible set factors may uence performance

assumption extent performance general purpose
planners biased toward particular domains although planners
developed general purpose competitions previous studies shown
planners excel different domains unfortunately community yet
good understanding planner well particular domain studied
impact selection performance two ways
first assessed whether performance might positively biased toward
tested development developer asked indicate domains used
development compared planner performance development
e development set remaining complete test set
rest ran x tests comparing number solved versus failed
development test sets included number solved failed analysis
timed made difference
analysis summarized table figure graphically displays
ratio successes failures development planners
except c performed significantly better development suggests
planners tailored intentionally particular types
tend better test sets biased accordingly example one
decided studying planners way representations
development pddl
one planner exception rule one case planner timed far frequently
non development



fihowe dahlman

development
planner sol fail



b


c


g


h





j


k


l



rest
sol fail

p











table comparing outcome development versus
planners set stan designed emphasis logistics fox
long

figure histogram ratios success failures development
planners
analysis introduces variety biases developers tended give us short
lists probably really representative actually used set used
moving target rather stationary suggests set included
experimentation publication may different still consequently second
part broadened question determine effect different subsets


fia critical assessment benchmark comparison

n





























rank dominance






total pairs









table rank dominance counts samples domains domain sizes n five

performance trials randomly selected n domains companion
form set counted many could
solved planner ranked relative performance planner thus
value n obtained planner rankings focused rankings
solved two reasons first domain includes different number making
count variable across trials second relative ranking gets
heart whether one planner might considered improvement another
tested values n half domains disposal
give sense variability size n solved trial
varied assess changes rankings across trials computed rank
dominance pairs planners rank dominance defined number trials
planner x rank lower planner note ties would count toward neither
planner planners study resulted dominance pairings relative
ranking two planners stable one would expect one dominate
e rank dominance
table shows number pairs value rank dominance
four values n given pair used highest number rank dominance
pair e g one lower rank pair rank dominance
five five ties maximum less five
data suggest even picking half domains rankings completely
stable pairings one dominates greater chance
switching relative ranking values degrade n decreases
dominating n

assumption syntactic representation differences affect
performance although well known planners performance depends

representation joslin pollack srinivasan howe two recent developments
planner suggest effect needs better understood first common
representation e pddl may bias performance planners rely pre processing
step convert pddl native representation step usually requires making
arbitrary choices ordering coding second advantage planners
graphplan supposed less vulnerable minor changes representa

fihowe dahlman

planner

b
c

e
f
g
h

j
k
l


none subset



























table number planners able solve none
subset permutations
tion although reasoning claim sound exigencies implementation may
require introduction representation sensitivity
evaluate sensitivity representation ten permutations
aips set generated resulting permuted permutations
constructed randomly reordering preconditions operator definitions
order definitions operators within domain definition
limited number study ten permutations would prohibitive selected aips attention
recently developed benchmark set even within set domains
permuted would different domains transformation used purposes investigation limited set modifications
permutations preconditions operators known affect planners practical considerations limited number permutations could
executed finally expediency ran permutations smaller number faster
platforms expedited throughput computation time factor
study
analyze data divided performance permutations
three groups whether planner able solve permutations
none permutations subset permutations planner insensitive
minor representational changes subset count zero
table see planners affected permutation operation
susceptibility permuting strongly planner dependent
p demonstrating planners vulnerable others
examining number subset column one assess degree susceptibility planners sensitive reorderings even relied graphplan


fia critical assessment benchmark comparison





















































































pre
















pre
safety
strips
typing


















b
c

e
f
g
h

j
k
l


feature

axioms
cond eff
dis pre
equality

planner















table number claiming require pddl feature solved
planner

methodology sensitive e f j included graphplan
planners mixed permutations c
l least sensitive affected

assumption performance depend pddl requirements
features planners intended handle strips

test set claim require features strips one would expect
planners would able handle addition
planners claim able handle given feature may well planners
table shows effects feature requirements ability solve data
table features specified requirements list pddl
definition domain
verify requirements accurate necessary thus
may solvable ignoring part pddl syntax understood
may mislabeled designer evident cases planner
support given feature still appears able solve corresponding
planners e g older versions stan reject requires
strips without trying solve adl makes use
strips features would attempted
guidance planner use must viewed
skepticism example would appear planner might


fihowe dahlman

good choice conditional effects able solve many
would mistake since planner cannot actually handle types
cases claim require adl fact make
use strips subset
clearly certain solved specific planners instance c
planners able handle safety constraints data
c e appear handle domain axioms half planners trouble
typed gaps appear due translation
native representation

planners

publicly available general purpose planners tend large programs developed
period years enhanced include additional features time thus several versions
likely available versions likely features turned
via parameter settings
authors release later versions systems general assumption
newer versions outperform predecessors however may
case practice instance planner could better optimized toward specific class
turn hurts performance advanced
capabilities even unused may incur overhead solution
comparison purposes one use latest version first tested
question study comparing multiple versions four planners second
planner relies parameter settings tune performance blackbox
many parameters others none comparisons tend use default published
parameter settings people usually understand effects parameters
tuning extremely time consuming practice undermine fair
comparison
planner assumption latest version best study compared
performance multiple versions four planners labeled section w x
z larger version numbers indicating subsequent versions considered two criteria
improvement outcome computation time solved
outcome one solved failed timed criterion statistically
analyzed data superior performance one versions outcome
planners summarized table table shows rarely version
solved z improved number test
solved subsequent versions
check whether differences outcome significant ran x tests
planner version independent variable outcome dependent table summarizes
analysis z compared version successor
differences significant except transition z expected
two versions extremely similar
another planner performance metric evaluated speed solution
analysis limited comparison solved
versions planner classified whether later version solved


fia critical assessment benchmark comparison

planner version solved failed timeout solved
w




w





x




x
















z




z





z





z





table version performance counts outcome change number solved
old

planner version version
p
w



x








z



z



z



table comparing versions planner
faster slower time preceding version
table see planners improved average speed solution
subsequent versions exception z transition versions however
z increase number solved versions
planner old faster slower total
w






x













z






z






z






table improvements execution speed across versions faster column counts
number cases version solved faster slower specifies
cases version took longer solve given


fihowe dahlman

planner assumption parameter settings matter fair comparison

planner set three obvious easily manipulable parameters blackbox hsp
ucpop blackbox extensive set parameters control everything
much trace information print sequence solver applications hsp function
varied include loop detection change search heuristic vary
number paths expand ucpop user change strategies governing node
orderings aw selection
run experiments assumption planners
parameters clear literature parameters matter
blackbox relies heavily random restarts trying alternative sat solvers kautz
selman authors blackbox carefully study aspects blackbox design
demonstrate differential performance different sat solvers propose hypotheses
performance differences working better performance variation
heart hsp heuristic search thus performance varies depending
heuristics experiments hsp planner builds ideas
hsp shown importance heuristic selection search space expansion
computation time scale haslum geffner hoffmann nebel

hsp heuristic search critical ucpop performance set studies
explored alternative settings aw selection heuristics employed ucpop joslin
pollack srinivasan howe genevini schubert producing dramatic
improvements domains heuristics pollack et al confirmed
good default strategy could derived performance best
circumstances
thus parameters control fundamental aspects
search strategies role parameters comparisons cannot easily dismissed

planner assumption time cut offs unfair planners often admit

failure instead planner stops used allotted time found
solution setting time threshold requirement planner execution
comparison one might wonder whether enough time allotted fair
perhaps solution almost found execution terminated
determine whether cut minutes fair examined distribution
times declared successes failures across planners set
found distributions skewed approximately log normal long right tails
planners quick declare success failure going
table shows max mean median standard deviation success failure times
planners differences mean median indicate distribution
skew low standard deviations relative observed max times max time
shows rare occasions planners might make decision within minutes
cut
separated two usually observed significant difference distributions time
succeed time fail half planners quick succeed slow fail half
reversed relationship



fia critical assessment benchmark comparison

planner

b
c

e
f
g
h

j
k
l


successes
max mean median



























sd














failures
max mean median






























sd














table max mean median standard deviations sd computation times
success failure planner

table observed distributions
values greater half time cut figures display
distributions planner f means middle set planners
quite typical distributions consequently least cut
minutes seconds would significantly change











success time

figure histogram times seconds planner f succeed


fihowe dahlman













fail time

figure histogram times seconds planner f fail

performance metrics

comparisons emphasize number solved cpu time completion metrics often organized increasing diculty scale
comparing metrics leaves lot open interpretation example
planners designed optimal plan measured number steps
parallel sequential plan consequently planners may require computation
thus ignoring plan quality planners may unfairly judged hypothesize
hardware software platform tests vary planner
developed machine gb memory likely performance degrade
less key issue whether effect less uniform across set planners
section examine two issues execution platform effect plan
quality

metric assumption performance vary planners run
different hardware platforms often planner run competition

someone else lab hardware software platforms differ platform used
development clearly slowing processor speed slow
requiring higher cut offs reduction memory may well change set
solved increase processing time due increased swapping changing
hardware configuration may change way memory cached organized favoring
planners internal representations others changing compilers could affect
amount type optimizations code exact effects probably unknown
assumption changes affect planners less equally
test ran planners less powerful lower memory machine compared
two platforms base sun ultrasparc mb memory
ultrasparc mb memory operating system compilers
versions machines run platforms
followed much methodology comparison planner versions comparing
number solved time solution table shows
measured solved failed timed planner two platforms


fia critical assessment benchmark comparison

planner platform solved failed timed
p reduction

ultra



ultra




b
ultra



ultra




c
ultra



ultra





ultra



ultra




e
ultra



ultra




f
ultra



ultra




g
ultra



ultra




h
ultra



ultra





ultra



ultra




j
ultra



ultra




k
ultra



ultra




l
ultra



ultra





ultra



ultra




table number solved failed timed planner two
hardware platforms last column percentage reduction number
solved faster slower platforms



fihowe dahlman

planner

b
c

e
f
g
h

j
k
l


faster
mean



























slower
sd mean

















sd





total




























table improvements execution speed moving slower faster platform counts
solved platforms faster slower
mean standard deviation sd difference provided
looked change time solution table shows time
solution changes planner surprisingly faster processor memory
nearly lead better performance somewhat surprisingly difference far less
doubling might expected mean differences much less
mean times faster processor see table mean solution times
effect seems vary planners counts lisp
planners appear less susceptible trend ones sometimes faster
slower platform however advantages small affecting primarily
smaller think effect due need load lisp image
startup centralized server thus computation time small
dominated network delay older versions planners appear less sensitive
switch platform
study platforms make little difference despite
doubling processor speed doubling memory however two platforms
underpowered compared development platforms planners
chose platforms differed characteristics processor speed
memory amount access identically configured machines
really observe difference gb memory may needed
recent trends technology exploited cheap memory translations
propositional representations compilation built caching memory
management techniques thus planners designed trade memory time
propose figure amount requested participants aips
competition



fia critical assessment benchmark comparison

planners understandably affected memory limitations
given study considered performing careful study memory
artificially limiting memory planners
access enough suciently large machines likely make difference could
devise scheme fairly across planners implemented
different languages require different software run time environments
another important factor may memory architecture management planners
include memory managers map better hardware platforms
others e g hsp uses linear organization appears fit well intel memory
architecture

metric assumption number plan steps vary several researchers

examined issue measuring plan quality directing e g
perez estlin mooney rabideau englehardt chien number
steps plan rather weak measure plan quality far one
widely used primitive action
expect planners sacrifice quality measured plan length speed
thus ignoring even measure plan quality may unfair planners
check whether appears factor set counted plan length
plans returned output compared lengths across planners
planners construct parallel plans adopted general definition
sequential plan length compared plan lengths returned planner
every successfully solved
found solved one planner necessarily
one planners found equal length solutions remained
calculated standard deviation sd plan length solutions
analyzed sds found minimum observed sd
maximum mean standard deviation thirteen
cases showed sds higher obviously cases involved fairly long plans
steps cases logistics gripper domains
check whether planners favored minimal lengths counted number
cases planner found shortest length plan ties attributed
planners variance plan length table lists
planners shortest length plans one third planner f
designed optimize plan length shows one exception
older planners rarely shortest plans

interpretation recommendations
previous section presented summarization analysis planner runs
section ect mean empirical comparison planners
summarize recommend partial solutions possible guarantee
fairness propose magic formula performing evaluations state
practice general certainly improved propose three general recommendations
recommendations targeted specific assumptions


fihowe dahlman

planner count


b

c



e

f

g

h



j

k

l



table number plans planner found shortest plan data
include different length plans found
many targeted recommendations amount requesting planner developers precise requirements expectations contributions
planners extremely complex time consuming build documentation may inadequate determine subsequent version differs previous
conditions e g parameter settings types planner fairly
compared current positive trend making planners available behooves
developer include information distribution system
sweeping recommendation shift focus away developing
best general purpose planner even competitions planners identified
superior ones designed specific classes e g ipp
competitions done great job exciting interest encouraging development
public availability planners incorporate representation
however advance informative comparative evaluations
designed specific purpose test hypothesis prediction
performance planner experimental hypothesis focuses analysis often
leads naturally justified design decisions experiment example hoffmann nebel authors fast forward system state introduction
jair development motivated specific set benchmark
domains system heuristic designed heuristics fit expectations needs domains hoffmann nebel additionally part
evaluation compare specific system system commonalities
point advantages disadvantages design decisions specific
paul cohen advocated experimental methodology artificial intelligence
hypotheses predictions considerable detail see cohen



fia critical assessment benchmark comparison

follow work researchers comparing systems
well defined starting point comparison

recommendation experiments driven hypotheses

searchers precisely articulate advance experiments expectations planner augmentations existing planner add
state art expectations turn justify selection
planners metrics form core comparative
evaluation
general issue whether accurate reported
output planners planner stated output successful
took face value however examining output determined
claims successful solution erroneous proposed solution would work
way ensure output correct solution checker drew mcdermott
used solution checker aips competition however planners
provide output compatible format checker thus another concern
comparative evaluation output needs cross checked
declaring winner e planner exhibited superior performance think
lack solution checker casts serious doubt part
concerned factors cause observed success rates change

recommendation input standardized pddl output
standardized least format returned plans

another general issue whether benchmark sets representative space
interesting test directly fact sure
one could clustering observations others
community suggest set biased toward logistics additionally many
getting dated longer distinguish performance researchers
begun formally analyze set service building improved
planners e g hoffmann nebel better understand
example related area scheduling group identified distinctive patterns
topology search spaces different types classical scheduling
related topology performance watson beck barbulescu whitley
howe within hoffmann examined topology local search spaces
small benchmark collection found simple structure
respect well known relaxations hoffmann additionally worked
partial taxonomy three characteristics analyzed domains helmert
analyzed computational complexity subclass benchmarks transportation
identified key features affect diculty helmert


recommendation benchmark sets eval

uated hauled easily solved removed
researchers study benchmark domains classify


fihowe dahlman

types key characteristics developers contribute application realistic versions evolving set
remainder section describes recommendations improving state
art planner comparisons

assumption general purpose planners biased toward particular domains set planner developed
strong effect performance planner effect
unintentional specialization concerted effort part
developers optimize system solve specific one exception every
planner fared better tailored subset training set consequently
must conclude choice subset may well affect outcome
comparison
fair planner comparison must account likely biases set good
performance certain class imply good performance general
large performance differential planners targeted domain e well
focus poorly others may well indicate developers
succeeded optimizing performance planner
recommendation sets constructed highlight
designers expectations superior performance planner
specific selection criteria
hand goal demonstrate across board performance
randomly selecting domains suggests biases mitigated
recommendation highlighting performance general
goal set selected randomly benchmark
domains

assumption syntactic representation differences affect
performance many studies including shown planners may sensitive

representational features representations translated automatically
mean performance unaffected
theoretically insensitive factor mean practice
planners showed sensitivity permuted degree sensitivity varied
outcome suggests translators even minor variations descriptions
impact outcome used care especially sensitivity
focus study planner vulnerable effect
recommendation representation translators avoided
native versions testing multiple versions necessary
many planner developers participating aips competitions become
less issue
importantly researchers explicitly testing effect alternative phrasings determine sensitivity performance separate
effects advice tuning essence


fia critical assessment benchmark comparison

recommendation studies consider role minor syntactic vari

ations performance include permuted e initial conditions
goals preconditions actions sets demonstrate robustness provide opportunity learning protect developers
accidentally fitting set test

assumption performance depend pddl requirements
features planners perform quite advertised expected given

features discrepancy could many possible causes incorrectly
specified planners less sensitivity thought solutions correct etc
example many benchmark set designed competitions
even intended widely used may specified carefully enough

recommendation contributed benchmark set
developers verify requirements stated description
correctly ect subset features needed planner evaluators
use match planner capabilities

depending cause skewed e g planner may unfairly
maligned unable solve specifically designed solve
recommendation addresses gaps specification set
mismatches capabilities specifiable pddl planners possess
remain

recommendation planner developers develop vocabulary
planner capabilities pddl ags specify expected
capabilities planner distribution

planner assumption latest version best suggest

versions run faster often solve thus newest version may
represent best depending definition performance class planner
competitions fields e g automatic theorem proving community require
previous year best performer compete well advantage establishing
baseline performance well allowing comparison focus may shift
time

recommendation primary evaluation metric speed newer

version may best competition number solved one
wishes establish progress made may worth running
older version well recommendation followed
evaluators select version guidance

planner assumption effect parameter settings perfor

mance planners vary parameter settings unfortunately often
dicult figure set parameters properly changing settings makes
dicult compare across experiments generally issue


fihowe dahlman

developers users tend rely default parameter settings unfortunately
sometimes developers exploit alternative settings experiments complicating
later comparison

recommendation planner includes parameters developer
guide users settings default settings
used developers others experiments facilitate comparison

planner assumption time cut offs unfair found little benefit
increasing time cut offs beyond minutes

recommendation total computation time bottleneck run

separate batches incrementally increasing time cut
runs including unresolved subsequent runs
additional solved run stop

metric assumption alternative platforms lead different performance experiments performance vary much expected
suggests researchers general developing specific hardware software
configurations recent trends suggest otherwise least regards memory
systems prototypes behooves developer clear
expectations anyone subsequently system accommodate requests studies

recommendation factors planner design researchers

must clearly state hardware software requirements planners
design platform assumptions additionally careful study memory versus time trade offs undertaken given recent trends memory exploitation

metric assumption number plan steps vary certainly

one neglects quality measures planners penalized efforts declare
best planner

recommendation expedite generalizing across studies reports
describe performance terms solved many types
much time required quality solutions tradeoffs reported possible e g increase computation time
decrease plan length additionally design goal
optimal solution compare planners design goal

good metrics plan quality sorely needed latest specification pddl
specification supports definition specific metrics fox long
metrics indicate whether total time concept supported specification action
durations specified functions minimized maximized addition
excellent start general metrics plan length total time
needed expedite comparisons across


fia critical assessment benchmark comparison

recommendation developing good metrics valuable contri

bution researchers consider worthwhile project conference organizers reviewers encourage papers topic planner developers
implement planners responsive quality metrics e
support tunable heuristics evaluation criteria

conclusions

fair evaluation comparison planners hard many apparently benign factors exert
significant effects performance superior performance one planner another
neither intentionally designed solve may explained minor
representational features however comparative analysis general practical
importance practical create specialized solution every
analyzed effects experiment design decisions empirical comparison
planners made recommendations ameliorating effects decisions
recommendations common sense suggestions improving current
methodology
expand beyond current methodology require least two substantive changes
first field needs question whether trying performance
general shift general comparisons focused comparisons
class mechanism hypothesis testing could produce significant advances
understanding
second benchmark sets require attention many
discarded simple much domains far removed
real applications may time revisit testbeds example several researchers
robotics constructed interactive testbed comparing motion
piccinocchi ceccarelli piloni bicchi testbed consists user interface
defining collection well known simulator testing
specific thus user design compare performance including via web site
testbed affords several advantages current paradigm static benchmark
developer conducted comparisons particular replicability extendability
test set alternatively challenging sets developed modifying deployed
applications wilkins desjardins engelhardt chien barrett willis wilklow

recent years community significantly improved size
solved reasonable time advanced state art
empirical comparison systems interpret empirical comparisons
understand motivate development community
needs understand effects empirical methodology purpose
understanding initiate dialogue methodology
used



fihowe dahlman

acknowledgments
partially supported career award national science
foundation iri grant air force oce scientific f u government authorized reproduce distribute reprints
governmental purposes notwithstanding copyright notation thereon
grateful reviewers careful reading well considered comments
submitted version hope done justice suggestions

references

bacchus
f

aips

competition
http www cs toronto edu aips selfcontainedaips ppt
barrett golden k penberthy weld ucpop user manual dept
computer science engineering university washington seattle wa tr

blum l furst l fast graph analysis artificial
intelligence journal
bonet b geffner h heuristic search proceedings
fifth european conference ecp durham uk
chien kambhampati knoblock c eds proceedings fifth
international conference artificial intelligence scheduling aips
aaai press breckenridge co
cohen p r survey eighth national conference artificial intelligence
pulling together pulling apart ai magazine
cohen p r empirical methods artificial intelligence mit press
drummond e kaelbling l p rosenschein j collected notes
benchmarks metrics workshop artificial intelligence branch fia nasa
ames center
engelhardt b chien barrett willis j wilklow c data chaser
citizen explorer benchmark sets proceedings sixth european
conference ecp toledo spain
estlin mooney r j learning improve ecicency quality
proceedings fifteenth international joint conference artificial
intelligence pp nagoya japan
fox long ecient implementation plan graph stan
journal artificial intelligence
fox long pddl extension pddl expressing temporal
domains available http www dur ac uk p long pddl ps gz


fia critical assessment benchmark comparison

genevini schubert l accelerating partial order planners techniques
effective search control pruning journal artificial intelligence

hanks nguyen thomas c beginner guide truckworld simulator dept computer science engineering uw cse tr university
washington
hanks pollack e cohen p r benchmarks test beds controlled
experimentation design agent architectures ai magazine
haslum p geffner h admissible heuristics optimal proceedings fifth international conference artificial intelligence
scheduling aips pp breckenridge co aaai press
helmert complexity transportation domains th
european conference ecp lecture notes artificial intelligence
york springer verlag
hoffmann j local search topology benchmarks empirical analysis
proceedings th international joint conference artificial intelligence
seattle wa usa
hoffmann j nebel b system fast plan generation
heuristic search journal artificial intelligence
howe e dahlman e hansen c scheetz von mayrhauser exploiting competitive planner performance proceedings fifth european conference
durham uk
howe e von mayrhauser mraz r test case generation ai
automated software engineering
joslin pollack least cost aw repair plan refinement strategy
partial order proceedings twelfth national conference artificial
intelligence pp seattle wa
kautz h selman b blackbox application
theorem proving solving working notes aips workshop
combinatorial search pittsburgh pa
kautz
h
blackbox
sat technology system
http www cs washington edu homes kautz blackbox index html
kautz h selman b unifying sat graph proceedings sixteenth international joint conference artificial intelligence stockholm sweden
koehler j handling conditional effects negative goals ipp tech rep
institute computer science albert ludwigs university freiburg germany


fihowe dahlman

koehler j nebel b hoffmann j dimopoulos extending graphs
adl subset proceedings fourth european conference
mcdermott ghallab howe knoblock c ram veloso weld
wilkins domain definition language
mcdermott ai systems competition ai magazine

penberthy j weld ucpop sound complete partial order planner
adl proceedings third international conference knowledge representation reasoning pp
perez learning search control knowledge improve plan quality ph
thesis carnegie mellon university
piccinocchi ceccarelli piloni f bicchi interactive benchmark
web proceedings ieee international conference
robotics automation
pollack e ringuette introducing tileworld experimentally evaluating agent architectures proceedings eight national conference artificial
intelligence pp boston
pollack joslin paolucci flaw selection strategies partial order
journal artificial intelligence
rabideau g englehardt b chien generic prferences incrementally
improve plan quality proceedings fifth international conference artificial
intelligence scheduling aips breckenridge co
slaney j thiebaux blocks world revisited artificial intelligence journal

srinivasan r howe e comparison methods improving search eciency
partial order planner proceedings th international joint conference
artificial intelligence pp montreal canada
sussman g computational model skill acquisition tech rep memo
ai tr mit ai lab
prodigy group prodigy manual tutorial school
computer science carnegie mellon university
watson j barbulescu l howe whitley l performance
structure ow shop scheduling proceedings sixteenth national
conference artificial intelligence aaai orlando fl
watson j beck j barbulescu l whitley l howe toward descriptive model local search cost job shop scheduling proceedings sixth
european conference ecp toledo spain


fia critical assessment benchmark comparison

weld anderson c smith extending graphplan handle uncertainty
sensing actions proceedings fifteenth national conference artificial
intelligence madison wi
weld recent advances ai ai magazine
wilkins e desjardins call knowledge ai magazine





