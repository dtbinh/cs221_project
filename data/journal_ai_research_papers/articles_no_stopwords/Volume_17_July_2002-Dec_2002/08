journal artificial intelligence

submitted published

unified model structural organization
language music
rens bod

rens illc uva nl

institute logic language computation
university amsterdam nieuwe achtergracht
wv amsterdam netherlands
school computing university leeds
ls jt leeds uk

abstract
general model predict perceived phrase structure language
music usually assumed humans separate faculties language
music work focuses commonalities rather differences
modalities aiming finding deeper faculty key idea perceptual system
strives simplest structure simplicity principle biased
likelihood previous structures likelihood principle present series dataoriented parsing dop combine two principles tested
penn treebank essen folksong collection experiments
combination two principles outperforms use exactly
model parameter setting achieves maximum accuracy language
music argue suggest interesting parallel linguistic
musical structuring

introduction structural organization
widely accepted human cognitive system tends organize perceptual information
hierarchical descriptions conveniently represented tree structures tree
structures used describe linguistic perception e g wundt chomsky
musical perception e g longuet higgins lerdahl jackendoff
visual perception e g palmer marr yet little attention paid
commonalities different forms perception question whether
exists general underlying mechanism governs perceptual organization
studies exactly question acknowledging differences perceptual
modalities general model predict perceived tree structure sensory
input studying question use empirical methodology model
might hypothesize tested manually analyzed benchmarks
linguistically annotated penn treebank marcus et al musically annotated
essen folksong collection schaffrath argue general model
structural organization language music vision carry experiments
linguistic musical benchmarks since benchmark visual tree structures currently
available best knowledge
figure gives three simple examples linguistic musical visual information
corresponding tree structures printed examples resp taken martin et al
lerdahl jackendoff dastani

ai access foundation morgan kaufmann publishers rights reserved

fibod

list sales products

np
np
np
v

dt

pp
n

p

pp
n

p

n

list sales products

figure examples linguistic musical visual input tree structures
thus tree structure describes parts input combine constituents
constituents combine representation whole input note linguistic
tree structure labeled syntactic categories whereas musical visual tree
structures unlabeled language syntactic constraints
words combined larger constituents e g english determiner combined
noun precedes noun expressed rule np dt n
music lesser extent vision restrictions principle note
may combined note
apart differences fundamental commonality perceptual input
undergoes process hierarchical structuring found input main
thus derive perceived tree structure given input
trivial may illustrated fact inputs assigned
following alternative tree structures


np
pp

np
v

dt

n

p

pp
n

p

n

list sales products

figure alternative tree structures inputs figure
alternative structures possible perceived linguistic tree
structure figure corresponds meaning different tree figure
two musical tree structures correspond different groupings motifs two
visual structures correspond different visual gestalts alternative tree
structures possible plausible correspond structures
actually perceived human cognitive system
phenomenon input may assigned different structural organizations
known ambiguity one hardest modeling
human perception even language phrase structure grammar may specify
words combined constituents ambiguity notoriously hard cf
manning schtze charniak argues many sentences wall
street journal one million different parse trees ambiguity


fia unified model structural organization language music

musical input even harder since virtually constraints notes may
combined constituents talking rhythm perception music longuet higgins
lee note given sequence note values principle infinitely ambiguous
ambiguity seldom apparent listener
following section discuss two principles traditionally proposed
solve ambiguity likelihood principle simplicity principle section
argue integration two principles within data oriented parsing framework
hypothesis human cognitive system strives simplest structure generated
shortest derivation biased frequency previously
perceived structures section go computational aspects model
section discuss linguistic musical test domains section presents empirical
investigation comparison model finally section give discussion
go combinations simplicity likelihood proposed
literature

two principles likelihood simplicity
predict set possible tree structures tree actually
perceived human cognitive system field visual perception two competing
principles traditionally proposed govern structural organization first initiated
helmholtz advocates likelihood principle perceptual input organized
probable structure second initiated wertheimer developed
gestalt psychologists advocates simplicity principle perceptual system
viewed finding simplest rather probable structure see chater
overview two principles used linguistic musical structuring
following briefly review principles modality
likelihood
likelihood principle particularly influential field natural language
processing see manning schtze review field appropriate
tree structure sentence assumed likely structure likelihood tree
computed probabilities parts e g phrase structure rules turn
estimated large manually analyzed language corpus e treebank state art
probabilistic parsers collins charniak bod obtain around
precision recall penn wall street journal treebank marcus et al
likelihood principle applied musical perception e g raphael
bod b c probabilistic natural language processing probable musical
tree structure computed probabilities rules fragments taken large
annotated musical corpus musical benchmark used
essen folksong collection schaffrath
vision science huge interest probabilistic e g hoffman
kersten mumford even seen fit declare dawning stochasticity
unfortunately visual treebanks currently available
simplicity
simplicity principle long tradition field visual perception psychology e g
restle leeuwenberg simon buffart et al van der helm


fibod

field visual pattern formalized constituent structure means visual coding
language primitive elements line segments angles perception
described process selecting simplest structure corresponding shortest
encoding visual pattern
notion simplicity applied music perception collard et al use
coding language leeuwenberg predict metrical structure four preludes
bach well tempered clavier well known music perception theory
proposed lerdahl jackendoff theory contains two kinds rules wellformedness rules preference rules role well formedness rules define
kinds formal objects grouping structures theory employs grouping structures
listener actually hears described preference rules describe gestaltpreferences kind identified wertheimer therefore seen
embodiment simplicity principle
notions simplicity exist language processing example frazier
viewed arguing parser prefers simplest structure containing minimal
attachments bod defines simplest tree structure sentence structure
generated smallest number subtrees given treebank

combining likelihood simplicity
key idea current principles play role perceptual organization
albeit rather different ones simplicity principle general cognitive preference
economy likelihood principle probabilistic bias due previous perceptual
experiences informally stated working hypothesis human cognitive system
strives simplest structure generated shortest derivation
biased frequency previously perceived structures combinations
simplicity likelihood discussed section formally instantiate working
hypothesis first need model defines set possible structures input
chosen model defines set phrase structures input
basis treebank previously analyzed input known data oriented
parsing dop model see bod collins duffy dop learns grammar
extracting subtrees given treebank combines subtrees analyze fresh input
chosen dop uses subtrees arbitrary size thereby capturing nonlocal dependencies obtained competitive benchmarks
bod b collins duffy following first review dop model
discuss use likelihood simplicity principles next
two principles combined instantiate working hypothesis
data oriented parsing
section illustrate dop model linguistic example rigorous definition
dop reader referred bod come back musical examples
section suppose given following extremely small linguistic treebank two
trees resp wanted dress rack saw dog telescope
actual treebanks contain tens thousands trees cf marcus et al



fia unified model structural organization language music





np

vp

v

np



wanted



pp
np

saw

np

dress p

vp
v

pp

np


vp

np

np

p

dog telescope

rack

figure example treebank
dop model parse sentence e g saw dress telescope
combining subtrees treebank means substitution operation indicated







vp

np



np

pp
np

p

dress





telescope



vp



pp
np

v

vp

np

vp

pp
np

v
saw

saw

p

np

dress telescope

figure parsing sentence combining subtrees figure
thus substitution operation combines two subtrees substituting second subtree
leftmost nonlexical leaf node first subtree may combined
third subtree etc combination subtrees tree structure
whole sentence called derivation since many different subtrees
sizes typically many different derivations produce however tree
instance





np


vp

np


vp
v
saw

dress

p

vp

np


pp
np





np

vp
v

telescope

pp
np

p

np

saw dress telescope

figure different derivation produces parse tree


fibod

interesting case occurs different derivations produce different
parse trees happens sentence ambiguous example dop produces
following alternative parse tree saw dress telescope




np

vp

v

v
saw


p





pp

np

np

telescope

np

vp

v

np

saw
np


pp

pp

np


dress

dress

p

np

telescope

figure different derivation produces different parse tree
likelihood dop
bod dop enriched likelihood principle predict perceived tree
structure set possible structures model call likelihood dop
computes probable tree input occurrence frequencies subtrees
probability subtree p computed number occurrences divided
total number occurrences treebank subtrees root label
let r return root label may write
p



r r



probability derivation tn computed product probabilities
subtrees ti
p tn

p ti

seen may different derivations generate parse tree
probability parse tree thus sum probabilities distinct derivations let
tid th subtree derivation produces tree probability given

p

p tid



fia unified model structural organization language music

parsing sentence interested trees assigned
denote ts best parse tree tbest according likelihood dop tree
maximizes probability ts
tbest arg max p ts
ts

thus likelihood dop computes probability tree sum products
product corresponds probability certain derivation generating tree
distinguishes likelihood dop statistical parsing identify exactly
one derivation parse tree thus compute probability tree one
product probabilities e g charniak collins eisner likelihood dop
probability model allows including counts subtrees wide range sizes everything
counts single level rules counts entire trees
note subtree probabilities likelihood dop directly estimated
relative frequencies treebank trees relative frequency estimator obtains
competitive several domains bonnema et al bod de pauw
maximize likelihood training data johnson
may hidden derivations relative frequency estimator cannot deal
estimation procedures take account hidden derivations maximize
likelihood training data example bod b presents likelihood dop model
estimates subtree probabilities maximum likelihood estimation procedure
expectation maximization dempster et al however since
relative frequency estimator far outperformed estimator see
bod et al b stick relative frequency estimator current
simplicity dop
likelihood dop justice preference humans display simplest
structure generated shortest derivation input bod simplest tree
structure input defined tree constructed smallest number
subtrees treebank refer model simplicity dop instead
producing probable parse tree input simplicity dop thus produces parse
tree generated shortest derivation consisting fewest treebank subtrees
independent probabilities subtrees define length derivation
l number subtrees thus tn l n let derivation
parse tree best parse tree tbest according simplicity dop
tree produced derivation minimal length
tbest arg min l ts
ts

section ts parse tree sentence example given treebank figure
simplest parse tree saw dress telescope given figure since
subtrees restricted depth relative frequency estimator coincide

maximum likelihood estimator depth dop model corresponds stochastic context free
grammar well known dop allow subtrees greater depth outperform depth
dop bod collins duffy



fibod

parse tree generated derivation two treebank subtrees
parse tree figure parse tree needs least three treebank subtrees
generated
shortest derivation may unique happen different parse trees
sentence generated minimal number treebank subtrees
probable parse tree may unique never happens practice case
back frequency ordering subtrees subtrees root label
assigned rank according frequency treebank frequent subtree
subtrees root label gets rank second frequent subtree gets rank etc
next rank shortest derivation computed sum ranks
subtrees involved derivation smallest sum highest rank taken final
best derivation producing final best parse tree simplicity dop see bod
performed one little adjustment rank subtree adjustment averages
rank subtree ranks sub subtrees instead simply taking
rank subtree compute rank subtree arithmetic mean ranks
sub subtrees including subtree effect technique
redresses low ranked subtree contains high ranked sub subtrees
simplicity dop likelihood dop obtain rather similar parse accuracy
wall street journal essen folksong collection terms precision recall see
section best trees predicted two quite match suggests
combined model justice simplicity likelihood may boost accuracy
combining likelihood dop simplicity dop sl dop ls dop
underlying idea combining likelihood simplicity human perceptual system
searches simplest tree structure generated shortest derivation
biased likelihood tree structure instead selecting simplest tree
per se combined model selects simplest tree among n likeliest trees n
free parameter course ways combine simplicity likelihood
within dop framework straightforward alternative would select
probable tree among n simplest trees suggesting perceptual system
searching probable structure among simplest ones refer
first combination simplicity likelihood selects simplest among n
likeliest trees simplicity likelihood dop sl dop second combination
selects likeliest among n simplest trees likelihood simplicity dop lsdop note n simplicity likelihood dop equal likelihood dop since
one probable tree select likelihood simplicity dop equal
simplicity dop since one simplest tree select moreover n gets large
sl dop converges simplicity dop ls dop converges likelihood dop
varying parameter n able compare likelihood dop simplicity dop
several instantiations sl dop ls dop

one might argue straightforward metric simplicity would return parse tree

smallest number nodes rather smallest number treebank subtrees metric
known perform quite badly see manning schtze bod



fia unified model structural organization language music

computational issues
bod showed standard chart parsing techniques applied likelihood dop
treebank subtree converted context free rule r lefthand side r
corresponds root label righthand side r corresponds frontier labels
indices link rules original subtrees maintain subtree internal
structure probability rules used create derivation forest sentence
chart parser see charniak probable parse computed
sampling sufficiently large number random derivations forest monte carlo
disambiguation see bod technique successfully applied parsing
atis portion penn treebank marcus et al extremely time consuming
mainly number random derivations sampled reliably
estimate probable parse increases exponentially sentence length see
goodman therefore questionable whether bod sampling technique scaled
larger domains wall street journal wsj portion penn treebank
goodman showed likelihood dop reduced compact stochastic
context free grammar scfg contains exactly eight scfg rules node
training set trees although goodman method still allow efficient computation
probable parse fact computing probable parse
likelihood dop np hard see sima method allow efficient
computation maximum constituents parse e parse tree likely
largest number correct constituents unfortunately goodman scfg reduction method
beneficial indeed subtrees used maximum parse accuracy usually
obtained restricting subtrees example bod shows optimal
subtree set achieving highest parse accuracy wsj obtained restricting
maximum number words subtree restricting maximum depth
unlexicalized subtrees goodman shows subtree restrictions
subtree depth may incorporated reduction method found reduction
method optimal subtree set
therefore use bod subtree rule conversion method likelihooddop use bod monte carlo sampling technique derivation forests
turned computationally prohibitive instead use well known viterbi
optimization chart parsing cf charniak manning schtze
allows computing k probable derivations input cubic time
estimate probable parse tree input
probable derivations summing probabilities derivations generate tree
although guarantee probable parse tree actually found
shown bod perform least well estimation probable
parse monte carlo techniques atis corpus moreover known
obtain significantly higher accuracy selecting parse tree generated single
probable derivation bod goodman therefore consider

simplicity dop first convert treebank subtrees rewrite rules
likelihood dop next simplest tree e shortest derivation efficiently
computed viterbi optimization way probable derivation provided
assign rules equal probabilities case shortest derivation equal
probable derivation seen follows rule probability p
probability derivation involving n rules equal p n since p derivation


fibod

fewest rules greatest probability experiments section give
rule probability mass equal r r number distinct rules derived bod
method mentioned shortest derivation may unique case
compute shortest derivations input apply ranking scheme
derivations ranks shortest derivations computed summing ranks
subtrees involve shortest derivation smallest sum subtree ranks
taken produce best parse tree
sl dop ls dop compute n likeliest n simplest trees means
viterbi optimization next select simplest tree among n likeliest ones
sl dop likeliest tree among n simplest ones ls dop experiments n
never larger

test domains
linguistic test domain used wall street journal wsj portion penn
treebank marcus et al portion contains approx sentences
manually annotated perceived linguistic tree structures predefined set
lexico syntactic labels since wsj extensively used described
literature cf manning schtze charniak collins bod
go
musical test domain used european folksongs essen folksong
collection schaffrath huron correspond approx folksongs
manually enriched perceived musical grouping structures essen
folksong collection previously used bod b temperley test
musical parsers current presents first experiments likelihood dop
simplicity dop sl dop ls dop collection essen folksongs
represented staff notation encoded essen associative code esac
pitch encodings esac resemble solfege scale degree numbers used replace
movable syllables mi etc thus corresponds corresponds etc
chromatic alterations represented adding b number
plus minus signs added number note falls resp
principle octave thus refer al different octaves
duration represented adding period underscore number period
increases duration underscore increases duration
one underscore may added number number duration indicator
duration corresponds smallest value thus pitches esac encoded integers
possibly preceded followed symbols octave chromatic alteration
duration pitch encoding treated atomic symbol may simple
complex pause represented possibly followed duration
indicators treated atomic symbol loudness timbre indicators used
esac
phrase boundaries indicated hard returns esac phrases unlabeled cf
section yet make esac annotations readable dop
added three basic labels phrase structures label whole song
label p phrase label n atomic symbol way obtained
conventional tree structures could directly employed dop parse
input use label n distinguishes annotations previous work bod


fia unified model structural organization language music

b c used labels song phrase p addition n
enhances productivity robustness musical parsing model although leads
much larger number subtrees
example assume simple melody consisting two phrases
tree structure given figure

p

p

n

n

n

n









figure example musical tree structure consisting two phrases
subtrees extracted tree structure include following

p
n

p
p

n

n

n

n









figure subtrees extracted tree figure
thus first subtree indicates phrase starting note followed exactly one
unspecified note phrase followed exactly one unspecified phrase
subtrees used parse musical input way explained
linguistic parsing section

experimental evaluation comparison
evaluate dop used blind testing method randomly divides
treebank training set test set strings test set parsed
means subtrees training set applied standard parseval metrics
precision recall compare proposed parse tree p corresponding correct test
set parse tree follows cf black et al
correct constituents p

correct constituents p

precision

recall

constituents p

constituents

constituent p correct exists constituent label spans
atomic symbols e words notes since precision recall obtain rather
precision recall scores computed evalb program available via

http www cs nyu edu cs projects proteus evalb



fibod

different see bod b often balanced single measure
performance known f score see manning schtze
f score

precision recall
precision recall

experiments divided treebanks e wsj essen folksong
collection training test set splits wsj used test material time
sentences words essen folksong collection test sets folksongs
used time words test set unknown training set
guessed categories statistics word endings hyphenation capitalization
cf bod unknown notes previous work bod limited
maximum size subtrees depth used random samples subtrees
depth next restricted maximum number atomic symbols
subtree maximum depth unlexicalized subtrees subtrees
smoothed technique described bod simple good turing
estimation good
table shows mean f scores obtained sl dop ls dop language
music values n recall n sl dop equal likelihood dop
ls dop equal simplicity dop

n













sl dop

ls dop

simplest among n likeliest

likeliest among n simplest

language

music

language

music





















































table f scores obtained sl dop ls dop language music

random subtree samples selected first exhaustively computing complete set

subtrees computationally prohibitive instead particular depth sampled
subtrees randomly selecting node random tree training set selected
random expansions node subtree particular depth obtained repeated
procedure times depth



fia unified model structural organization language music

table shows increase accuracy sl dop ls dop
value n increases accuracy sl dop decreases n
converges simplicity dop e ls dop n accuracy ls dop continues
increase converges likelihood dop e sl dop n highest accuracy
obtained sl dop n language music thus sl dop outperforms
likelihood dop simplicity dop selection simplest structure
top likeliest ones turns promising model selection likeliest
structure top simplest ones according paired testing accuracy
improvement sl dop n sl dop n equal likelihood dop
statistically significant language p music p
surprising sl dop reaches highest accuracy small value n
even surprising exactly model parameter setting obtains
maximum accuracy language music model embodies idea
perceptual system strives simplest structure searches among
probable structures
compare language others tested sl dop n
standard division wsj uses sections training approx
sentences section testing sentences words see e g manning
schtze charniak collins division sl dop achieved f score
best previous obtained f score collins bod
terms error reduction sl dop improves
common report accuracy sentences words wsj sldop obtained f score
musical compared bod b c tested three probabilistic
parsing increasing complexity training test set splits essen
folksong collection best obtained hybrid dop markov parser
f score significantly worse best obtained sl dop
splits essen folksongs difference may explained fact
hybrid dop markov parser bod b c takes account context higher
nodes tree sister nodes dop presented
current take subtree account almost arbitrary width depth thereby
covering larger amount musical context moreover mentioned section
bod b c use label n notes instead markov used
parse sequences notes
would interesting compare musical melodic parser
temperley uses system preference rules similar lerdahl jackendoff
evaluated essen folksong collection
tested several test sets randomly selected folksongs temperley used one test
set folksongs moreover cleaned eliminating folksongs irregular
meter temperley therefore difficult compare temperley
yet noteworthy temperley parser correctly identified phrase
boundaries although lower obtained sl dop temperley parser
trained previously analyzed examples model though note
temperley obtained tuning optimal phrase length parser
average phrase length essen folksong collection
perhaps mentioned parsing trained treebanks widely
used natural language processing still rather uncommon musical processing


fibod

musical parsing including temperley employ rule
parsing combination low level rules prefer phrase boundaries
large intervals higher level rules prefer phrase boundaries changes
harmony low level rules usually well known gestalt principles
proximity similarity wertheimer prefer phrase boundaries larger
intervallic distances however bod c shown gestalt principles
predict incorrect phrase boundaries number folksongs higher level
phenomena cannot alleviate incorrect predictions folksongs contain phrase
boundary falls large pitch time interval called
jump phrases rather intervals would predicted gestalt principles
moreover musical factors melodic parallelism meter harmony predict
exactly incorrect phrase boundaries cases see bod b c details
conjectured jump phrases inherently memory reflecting idiomdependent pitch contours cf huron snyder best captured
memory model tries mimic musical experience listener
certain culture bod c

discussion conclusion
seen combination simplicity likelihood quite rewarding linguistic
musical structuring suggesting interesting parallel two modalities yet one
may question whether model massively memorizes uses previously perceived
structures cognitive plausibility although question important want
claim cognitive relevance model appears evidence people store
kinds previously heard fragments language jurafsky music
saffran et al people store fragments arbitrary size proposed dop
overview article jurafsky reports large body psycholinguistic evidence
showing people store lexical items bigrams frequent phrases
even whole sentences case sentences people store idiomatic sentences
regular high frequency sentences thus least language
evidence humans store fragments arbitrary size provided fragments
certain minimal frequency suggests humans need parse input
rules grammar productively use previously analyzed fragments
yet evidence people store fragments hear suggested dop
high frequency fragments seem memorized however human perceptual
faculty needs learn fragments stored initially need keep track
fragments possibility forgetting otherwise frequencies never
accumulate model continuously incrementally updates fragment
memory given input correspondence dop
approaches cf daelemans scha et al spiro
acknowledge importance rule system acquiring fragment memory
substantial memory available may efficient construct tree means
already parsed fragments constructing entirely means rules many cognitive
derived differences reaction times sentence recognition

frequency whole test sentences varied variables lexical frequency
bigram frequency plausibility syntactic semantic complexity etc kept constant



fia unified model structural organization language music

activities advantageous store immediately retrieved
memory rather computing time scratch shown
example manual reaches rosenbaum et al arithmetic operations rickard et al
word formation baayen et al mention linguistic musical
parsing may exception
stressed experiments reported limited least two
respects first musical test domain rather restricted wide variety linguistic
treebanks currently available see manning schtze number musical
treebanks extremely limited thus need larger richer annotated musical
corpora covering broader domains development annotated corpora may timeconsuming experience natural language processing shown worth
effort since corpus parsing systems dramatically outperform grammar parsing
systems second limitation experiments evaluated parse
rather parse process assessed accurately
mimic input output behavior human annotator without investigating
process annotator arrived perceived structures unlikely humans
process perceptual input computing likely derivations random samples
subtrees current yet many applications suffices
know perceived structure rather process led structure
seen combination simplicity likelihood predicts perceived structure
high degree accuracy
proposals integrating principles simplicity likelihood
human perception see chater review chater notes context
information theory shannon principles simplicity likelihood identical
context simplicity principle interpreted minimizing expected length encode
message log p bits leads maximizing
probability used information theoretical definition simplest structure
simplicity dop would return structure likelihood dop improved
would obtained combination two hand defining
simplest structure one generated smallest number subtrees independent
probabilities created notion simplicity provably different notion
likely structure combined likelihood dop obtained improved
another integration two principles may provided notion minimum
description length mdl cf rissanen mdl principle viewed
preferring statistical model allows shortest encoding training data
relevant encoding consists two parts first part encodes model data
second part encodes data terms model bit length mdl closely related
stochastic complexity rissanen kolmogorov complexity li vitanyi
used natural language processing estimating parameters stochastic
grammar e g osborne leave open question whether
mdl successfully used estimating parameters dop subtrees however
since mdl known give asymptotically maximum likelihood estimation
mle rissanen application dop may lead unproductive model
maximum likelihood estimator assign training set trees empirical
frequencies assign weight trees see bonnema proof
would model generate training data strings
johnson argues may overlearning rather


fibod

mle per se standard methods cross validation regularization would seem
principle ways avoid overlearning leave issue future
investigation
idea general underlying model language music uncontroversial
linguistics usually assumed humans separate language faculty lerdahl
jackendoff argued separate music faculty work propose
separate faculties exist wants focus commonalities rather
differences faculties aiming finding deeper faculty may hold
perception general hypothesis perceptual system strives simplest
structure searches among likeliest structures

acknowledgements
thanks aline honingh remko scha neta spiro menno van zaanen three anonymous
reviewers excellent comments preliminary version presented
keynote talk lcg workshop learning computational grammars tbingen

references
baayen r h dijkstra schreuder r singular plurals dutch evidence
parallel dual route model journal memory language
black e abney flickinger gnadiec c grishman r harrison p hindle
ingria r jelinek f klavans j liberman marcus roukos santorini b
strzalkowski procedure quantitatively comparing syntactic
coverage english proceedings darpa speech natural language
workshop pacific grove morgan kaufmann
bod r annotated language corpus virtual stochastic grammar
proceedings aaai menlo park ca
bod r beyond grammar experience theory language stanford
csli publications lecture notes number
bod r parsing shortest derivation proceedings coling
saarbrcken germany
bod r b combining semantic syntactic structure language modeling
proceedings icslp beijing china
bod r minimal set fragments achieves maximal parse
accuracy proceedings acl toulouse france
bod r b memory model music analysis proceedings international
computer music conference icmc havana cuba
bod r c memory melodic analysis challenging gestalt
principles journal music available
http staff science uva nl rens jnmr pdf


fia unified model structural organization language music

bod r hay j jannedy eds probabilistic linguistics cambridge
mit press press
bod r scha r sima k eds b data oriented parsing stanford csli
publications press
bonnema r probability dop bod et al b
bonnema r bod r scha r dop model semantic interpretation
proceedings acl eacl madrid spain
buffart h leeuwenberg e restle f analysis ambiguity visual pattern
completion journal experimental psychology human perception
performance
charniak e statistical language learning cambridge mit press
charniak e statistical techniques natural language parsing ai magazine
winter
charniak e maximum entropy inspired parser proceedings anlpnaacl seattle washington
chater n search simplicity fundamental cognitive principle
quarterly journal experimental psychology
chomsky n aspects theory syntax cambridge mit press
collard r vos p leeuwenberg e melody tells metre music
zeitschrift fr psychologie
collins head driven statistical natural language parsing phdthesis university pennsylvania pa
collins discriminative reranking natural language parsing proceedings
icml stanford ca
collins duffy n ranking parsing tagging kernels
discrete structures voted perceptron proceedings acl
philadelphia pa
daelemans w introduction special issue memory language
processing journal experimental theoretical artificial intelligence

dastani languages perception illc dissertation series university
amsterdam
dempster laird n rubin maximum likelihood incomplete data via
em journal royal statistical society



fibod

de pauw g aspects pattern matching data oriented parsing proceedings
coling saarbrcken germany
eisner j bilexical grammars cubic time probabilistic parser proceedings
fifth international workshop parsing technologies boston mass
frazier l comprehending sentences syntactic parsing strategies phd
thesis university connecticut
good population frequencies species estimation population
parameters biometrika
goodman j efficient parsing dop model proceedings
empirical methods natural language processing philadelphia pa
goodman j efficient parsing dop pcfg reductions bod et al b
von helmholtz h treatise physiological optics vol dover york
hoffman visual intelligence york norton company inc
huron melodic arch western folksongs computing musicology
johnson dop estimation method biased inconsistent computational
linguistics
jurafsky probabilistic modeling psycholinguistics comprehension
production bod et al available http www colorado edu ling jurafsky
prob ps
kersten high level vision statistical inference gazzaniga ed
cognitive neurosciences cambridge mit press
leeuwenberg e perceptual coding language perceptual auditory
patterns american journal psychology
lerdahl f jackendoff r generative theory tonal music cambridge
mit press
li vitanyi p introduction kolmogorov complexity
applications nd ed york springer
longuet higgins h perception melodies nature
longuet higgins h lee c rhythmic interpretation monophonic music
mental processes studies cognitive science cambridge mit press
manning c schtze h foundations statistical natural language
processing cambridge mit press
marcus santorini b marcinkiewicz building large annotated corpus
english penn treebank computational linguistics


fia unified model structural organization language music

marr vision san francisco freeman
martin w church k patil r preliminary analysis breadth first parsing
theoretical experimental bolc l ed natural language
parsing systems springer verlag berlin
mumford dawning age stochasticity lecture
accademia nazionale dei lincei available http www dam brown edu people
mumford papers dawning ps
osborne minimal description length induction definite clause grammars
noun phrase identification proceedings eacl workshop computational
natural language learning bergen norway
palmer hierarchical structure perceptual representation cognitive
psychology
raphael c automatic segmentation acoustic musical signals hidden
markov ieee transactions pattern analysis machine intelligence

restle f theory serial pattern learning structural trees psychological
review
rickard healy bourne jr e cognitive structure basic arithmetic
skills operation order symbol transfer effects journal experimental
psychology learning memory cognition
rissanen j modeling shortest data description automatica
rissanen j stochastic complexity statistical inquiry series computer
science world scientific
rosenbaum vaughan j barnes h jorgensen time course movement
selection handgrips object manipulation journal experimental
psychology learning memory cognition
saffran j loman robertson r infant memory musical experiences
cognition b
scha r bod r sima k memory syntactic analysis journal
experimental theoretical artificial intelligence
schaffrath h essen folksong collection humdrum kern format
huron ed menlo park ca center computer assisted
humanities
shannon c mathematical theory communication bell system technical
journal
sima k computational complexity probabilistic disambiguation means
tree grammars proceedings coling copenhagen denmark


fibod

simon h complexity representation patterned sequences symbols
psychological review
snyder b music memory cambridge mit press
spiro n combining grammar memory perception
time signature phase anagnostopoulou c ferrand smaill eds
music artificial intelligence lecture notes artificial intelligence vol
springer verlag
temperley cognition basic musical structures cambridge mit
press
wertheimer untersuchungen zur lehre von der gestalt psychologische
forschung
wundt w sprachgeschichte und sprachpsychologie engelmann leipzig




